<doc id="71575" url="https://en.wikipedia.org/wiki?curid=71575" title="Free-space optical communication">
Free-space optical communication

Free-space optical communication (FSO) is an optical communication technology that uses light propagating in free space to wirelessly transmit data for telecommunications or computer networking.
"Free space" means air, outer space, vacuum, or something similar. This contrasts with using solids such as optical fiber cable or an optical transmission line.
The technology is useful where the physical connections are impractical due to high costs or other considerations.
History.
Optical communications, in various forms, have been used for thousands of years. The Ancient Greeks used a coded alphabetic system of signalling with torches developed by Cleoxenus, Democleitus and Polybius. In the modern era, semaphores and wireless solar telegraphs called heliographs were developed, using coded signals to communicate with their recipients.
In 1880 Alexander Graham Bell and his assistant Charles Sumner Tainter created the Photophone, at Bell's newly established Volta Laboratory in Washington, DC. Bell considered it his most important invention. The device allowed for the transmission of sound on a beam of light. On June 3, 1880, Bell conducted the world's first wireless telephone transmission between two buildings, some 213 meters (700 feet) apart.
Its first practical use came in military communication systems many decades later, first for optical telegraphy. German colonial troops used Heliograph telegraphy transmitters during the 1904/05 Herero Genocide in German South-West Africa (today's Namibia) as did British, French, US or Ottoman signals. During the trench warfare of World War I when wire communications were often cut, German signals used three types of optical Morse transmitters called "", the intermediate type for distances of up to 4 km (2.5 miles) at daylight and of up to 8 km (5 miles) at night, using red filters for undetected communications. Optical telephone communications were tested at the end of the war, but not introduced at troop level. In addition, special blinkgeräts were used for communication with airplanes, balloons, and tanks, with varying success.
A major technological step was to replace the Morse code by modulating optical waves in speech transmission. Carl Zeiss Jena developed the " 80/80" (literal translation: optical speaking device) that the German army used in their World War II anti-aircraft defense units, or in bunkers at the Atlantic Wall.
The invention of lasers in the 1960s revolutionized free space optics. Military organizations were particularly interested and boosted their development. However the technology lost market momentum when the installation of optical fiber networks for civilian uses was at its peak.
Many simple and inexpensive consumer remote controls use low-speed communication using infrared (IR) light. This is known as consumer IR technologies.
A recently declassified 1987 Pentagon report reveals free-space lasers have been mounted on Israeli F-15 fighter jets for the purposes of surveillance, missile-tracking, and targeted weaponry.
Usage and technologies.
Free-space point-to-point optical links can be implemented using infrared laser light, although low-data-rate communication over short distances is possible using LEDs. Infrared Data Association (IrDA) technology is a very simple form of free-space optical communications. On the communications side the FSO technology is considered as a part of the Optical Wireless Communications applications. Free-space optics can be used for communications between spacecraft, but this has not been put into practice.
Current market demands.
The demand for a high-speed (10 GBps+) and long range (3 – 5 km) FSO system is apparent in the market place. 
Useful distances.
The reliability of FSO units has always been a problem for commercial telecommunications. Consistently, studies find too many dropped packets and signal errors over small ranges (400 to 500 meters). This is from both independent studies, such as in the Czech republic, as well as formal internal nationwide studies, such as one conducted by MRV FSO staff. Military based studies consistently produce longer estimates for reliability, projecting the maximum range for terrestrial links is of the order of . All studies agree the stability and quality of the link is highly dependent on atmospheric factors such as rain, fog, dust and heat.
Extending the useful distance.
The main reason terrestrial communications have been limited to non-commercial telecommunications functions is fog. Fog consistently keeps FSO laser links over 500 meters from achieving a year-round bit error rate of 99.999%. Several entities are continually attempting to overcome these key disadvantages to FSO communications and field a system with a better quality of service. DARPA has sponsored over $130 million USD in research towards this effort, with the ORCA and ORCLE programs.
Other non-government groups are fielding tests to evaluate different technologies that some claim have the ability to address key FSO adoption challenges. As of October 2014, none have fielded a working system that addresses the most common atmospheric events.
FSO research from 1998-2006 in the private sector totaled $407.1 million, divided primarily among 4 start-up companies. All four failed to deliver products that would meet telecommunications quality and distance standards:
One private company published a paper on Nov 20,2014, claiming they had achieved commercial reliability (99.999% availability) in extreme fog. There is no indication this product is currently commercially available.
Extraterrestrial.
The massive advantages of laser communication in space have multiple space agencies racing to develop a stable space communication platform, with many significant demonstrations and achievements. To date (18 December 2014), "no laser communication system is in use in space." See Laser communication in space
Demonstrations in Space:
See Laser communication in space for references and original wiki information.
The first gigabit laser-based communication was achieved by the European Space Agency and called the European Data Relay System (EDRS) on November 28, 2014. The initial images have just been demonstrated, and a working system is expected to be in place in the 2015-2016 time frame.
NASA's OPALS announced a breakthrough in space-to-ground communication December 9, 2014, uploading 175 megabytes in 3.5 seconds. Their system is also able to re-acquire tracking after the signal was lost due to cloud cover.
In January 2013, NASA used lasers to beam an image of the Mona Lisa to the Lunar Reconnaissance Orbiter roughly 390,000 km (240,000 mi) away. To compensate for atmospheric interference, an error correction code algorithm similar to that used in CDs was implemented.
A two-way distance record for communication was set by the Mercury laser altimeter instrument aboard the MESSENGER spacecraft, and was able to communicate across a distance of 24 million km (15 million miles), as the craft neared Earth on a fly-by in May, 2005. The previous record had been set with a one-way detection of laser light from Earth, by the Galileo probe, of 6 million km in 1992.
Quote from Laser Communication in Space Demonstrations (EDRS)
LEDs.
In 2001, Twibright Labs released Ronja Metropolis, an open source DIY 10Mbps full duplex LED FSO over 1.4 km 
In 2004, a Visible Light Communication Consortium was formed in Japan. This was based on work from researchers that used a white LED-based space lighting system for indoor local area network (LAN) communications. These systems present advantages over traditional UHF RF-based systems from improved isolation between systems, the size and cost of receivers/transmitters, RF licensing laws and by combining space lighting and communication into the same system. In January 2009 a task force for visible light communication was formed by the Institute of Electrical and Electronics Engineers working group for wireless personal area network standards known as IEEE 802.15.7. A trial was announced in 2010 in St. Cloud, Minnesota.
Amateur radio operators have achieved significantly farther distances using incoherent sources of light from high-intensity LEDs. One reported in 2007. However, physical limitations of the equipment used limited bandwidths to about 4 kHz. The high sensitivities required of the detector to cover such distances made the internal capacitance of the photodiode used a dominant factor in the high-impedance amplifier which followed it, thus naturally forming a low-pass filter with a cut-off frequency in the 4 kHz range. From the other side use of lasers radiation source allows to reach very high data rates which are comparable to fiber communications.
Projected data rates and future data rate claims vary. A low-cost white LED (GaN-phosphor) which could be used for space lighting can typically be modulated up to 20 MHz. Data rates of over 100 Mbit/s can be easily achieved using efficient modulation schemes and Siemens claimed to have achieved over 500 Mbit/s in 2010. Research published in 2009 used a similar system for traffic control of automated vehicles with LED traffic lights.
In September 2013, pureLiFi, the Edinburgh start-up working on Li-Fi, also demonstrated high speed point-to-point connectivity using any off-the-shelf LED light bulb. In previous work, high bandwidth specialist LEDs have been used to achieve the high data rates. The new system, the Li-1st, maximizes the available optical bandwidth for any LED device, thereby reducing the cost and improving the performance of deploying indoor FSO systems.
Engineering details.
Typically, best use scenarios for this technology are:
The light beam can be very narrow, which makes FSO hard to intercept, improving security. In any case, it is comparatively easy to encrypt any data traveling across the FSO connection for additional security. FSO provides vastly improved electromagnetic interference (EMI) behavior compared to using microwaves.
Range limiting factors.
For terrestrial applications, the principal limiting factors are: 
These factors cause an attenuated receiver signal and lead to higher bit error ratio (BER). To overcome these issues, vendors found some solutions, like multi-beam or multi-path architectures, which use more than one sender and more than one receiver. Some state-of-the-art devices also have larger fade margin (extra power, reserved for rain, smog, fog). To keep an eye-safe environment, good FSO systems have a limited laser power density and support laser classes 1 or 1M. Atmospheric and fog attenuation, which are exponential in nature, limit practical range of FSO devices to several kilometres.

</doc>
<doc id="71576" url="https://en.wikipedia.org/wiki?curid=71576" title="475 BC">
475 BC

__NOTOC__
Year 475 BC was a year of the pre-Julian Roman calendar. At the time, it was known as the Year of the Consulship of Poplicola and Rutilus (or, less frequently, year 279 "Ab urbe condita"). The denomination 475 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Arts.
</onlyinclude>

</doc>
<doc id="71578" url="https://en.wikipedia.org/wiki?curid=71578" title="394 BC">
394 BC

__NOTOC__
Year 394 BC was a year of the pre-Julian Roman calendar. At the time, it was known as the Year of the Tribunate of Camillus, Poplicola, Medullinus, Albinus, Mamercinus and Scipio (or, less frequently, year 360 "Ab urbe condita"). The denomination 394 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Greece.
</onlyinclude>

</doc>
<doc id="71579" url="https://en.wikipedia.org/wiki?curid=71579" title="393 BC">
393 BC

__NOTOC__
Year 393 BC was a year of the pre-Julian Roman calendar. At the time, it was known as the Year of the Consulship of Poplicola and Cornelius (or, less frequently, year 361 "Ab urbe condita"). The denomination 393 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Literature.
</onlyinclude>

</doc>
<doc id="71580" url="https://en.wikipedia.org/wiki?curid=71580" title="392 BC">
392 BC

__NOTOC__
Year 392 BC was a year of the pre-Julian Roman calendar. At the time, it was known as the Year of the Consulship of Poplicola and Capitolinus (or, less frequently, year 362 "Ab urbe condita"). The denomination 392 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Art.
</onlyinclude>

</doc>
<doc id="71581" url="https://en.wikipedia.org/wiki?curid=71581" title="Ribavirin">
Ribavirin

Ribavirin is an anti-viral drug used for severe RSV infection; hepatitis C infection, including if persistent, and often in combination with peginterferon alfa-2b or peginterferon alfa-2a; as well as some other viral infections.
It is a guanosine (ribonucleic) analog used to stop viral RNA synthesis and viral mRNA capping, thus, it is a nucleoside inhibitor. Ribavirin is a prodrug, which when metabolized resembles purine RNA nucleotides. In this form it interferes with RNA metabolism required for viral replication. How it exactly affects viral replication is unknown; many mechanisms have been proposed for this but none of these has been proven to date. Multiple mechanisms may be responsible for its actions.
It is on the World Health Organization's List of Essential Medicines, a list of the most important medication needed in a basic health system.
Medical uses.
Ribavirin is used primarily to treat hepatitis C and viral hemorrhagic fevers (which is an orphan indication in most countries). In this former indication the oral (capsule or tablet) form of ribavirin is used in combination with pegylated interferon alfa. Including in people coinfected with hepatitis B, HIV and in the pediatric population. Statins may improve this combination's efficacy in treating hepatitis C. Ribavirin is the only known treatment for a variety of viral hemorrhagic fevers, including Lassa fever, Crimean-Congo hemorrhagic fever, Venezuelan hemorrhagic fever, and Hantavirus infection, although data regarding these infections are scarce and the drug might be effective only in early stages. It is noted by the USAMRIID that "Ribavirin has poor in vitro and in vivo activity against the filoviruses (Ebola and Marburg) and the flaviviruses (dengue, yellow fever, Omsk hemorrhagic fever, and Kyasanur forest disease)" The aerosol form has been used in the past to treat respiratory syncytial virus-related diseases in children, although the evidence to support this is rather weak.
It has been used (in combination with ketamine, midazolam, and amantadine) in treatment of rabies.
Experimental data indicate that ribavirin may have useful activity against Canine distemper. Ribavirin has also been used as a treatment for herpes simplex virus. One small study found that ribavirin treatment reduced the severity of herpes outbreaks and promoted recovery, as compared with placebo treatment. Another study found that ribavirin potentiated the antiviral effect of acyclovir.
There has also been some interest in its possible use as a treatment for cancers, especially acute myeloid leukemia.
Adverse effects.
Ribavirin should not be given with zidovudine because of the increased risk of anemia; concurrent use with didanosine should likewise be avoided because of an increased risk of mitochondrial toxicity.
Mechanisms of action.
RNA viruses.
Ribavirin's carboxamide group can make the native nucleoside drug resemble adenosine or guanosine, depending on its rotation. For this reason, when ribavirin is incorporated into RNA, as a base analog of either adenine or guanine, it pairs equally well with either uracil or cytosine, inducing mutations in RNA-dependent replication in RNA viruses. Such hypermutation can be lethal to RNA viruses.
DNA viruses.
Neither of these mechanisms explains ribavirin's effect on many DNA viruses, which is more of a mystery, especially given the complete inactivity of ribavirin's 2' deoxyribose analogue, which suggests that the drug functions only as an RNA nucleoside mimic, and never a DNA nucleoside mimic. Ribavirin 5'-monophosphate inhibits cellular inosine monophosphate dehydrogenase, thereby depleting intracellular pools of GTP.
History.
In 1972 it was reported that ribavirin was active against a variety of RNA and DNA viruses in culture and in animals, without undue toxicity.
Derivatives.
Ribavirin is possibly best viewed as a ribosyl purine analogue with an incomplete purine 6-membered ring. This structural resemblance historically prompted replacement of the 2' nitrogen of the triazole with a carbon (which becomes the 5' carbon in an imidazole), in an attempt to partly "fill out" the second ring--- but to no great effect. Such 5' imidazole riboside derivatives show antiviral activity with 5' hydrogen or halide, but the larger the substituent, the smaller the activity, and all proved less active than ribavirin. Note that two natural products were already known with this imidazole riboside structure: substitution at the 5' carbon with OH results in pyrazomycin/pyrazofurin, an antibiotic with antiviral properties but unacceptable toxicity, and replacement with an amino group results in the natural purine synthetic precursor 5-aminoimidazole-4-carboxamide-1-β-D-ribofuranoside (AICAR), which has only modest antiviral properties.
Taribavirin (viramidine).
The most successful ribavirin derivative to date is the 3-carboxamidine derivative of the parent 3-carboxamide, first reported in 1973 by J.T.Witkowski et al., and now called taribavirin (former names "viramidine" and "ribamidine"). This drug shows a similar spectrum of antiviral activity to ribavirin, which is not surprising as it is now known to be a pro-drug for ribavirin. Viramidine, however, has useful properties of less erythrocyte-trapping and better liver-targeting than ribavirin. The first property is due to viramidine's basic amidine group which inhibits drug entry into RBCs, and the second property is probably due to increased concentration of the enzymes which convert amidine to amide, in liver tissue. Viramidine is in phase III human trials and may one day be used in place of ribavirin, at least against certain kinds of viral hepatitis. Viramidine's slightly superior toxicological properties may eventually cause it to replace ribavirin in all uses of ribavirin.

</doc>
<doc id="71582" url="https://en.wikipedia.org/wiki?curid=71582" title="391 BC">
391 BC

__NOTOC__
Year 391 BC was a year of the pre-Julian Roman calendar. At the time, it was known as the Year of the Tribunate of Flavus, Medullinus, Camerinus, Fusus, Mamercinus and Mamercinus (or, less frequently, year 363 "Ab urbe condita"). The denomination 391 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Roman Republic.
</onlyinclude>

</doc>
<doc id="71583" url="https://en.wikipedia.org/wiki?curid=71583" title="390 BC">
390 BC

__NOTOC__
Year 390 BC was a year of the pre-Julian Roman calendar. At the time, it was known as the Year of the Tribunate of Ambustus, Longus, Ambustus, Fidenas, Ambustus and Cornelius (or, less frequently, year 364 "Ab urbe condita"). The denomination 390 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Architecture.
</onlyinclude>

</doc>
<doc id="71585" url="https://en.wikipedia.org/wiki?curid=71585" title="Medieval metal">
Medieval metal

Medieval metal or medieval rock is a subgenre of folk metal that blends hard rock or heavy metal music with medieval folk music. Medieval metal is mostly restricted to Germany where it is known as Mittelalter-Metal or Mittelalter-Rock. The genre emerged from the middle of the 1990s with contributions from Subway to Sally, In Extremo and Schandmaul. The style is characterised by the prominent use of a wide variety of traditional folk and medieval instruments.
History.
Precursors.
The medieval folk band Corvus Corax, was formed in 1989 and released a debut album in the same year.
The group relies on period instruments that include the cister, hurdy-gurdy, biniou, buccina, davul, riq and cornetto curvo with the most prominent being the shawm and bagpipes. They are also known for their use of source material, adopting melodies from medieval literature written in an old system of notation called neumes but otherwise using their own interpretations for arrangements and the rhythm. They describe their approach as "louder, dirtier and more powerful than any interpretation of medieval music before." The result has been associated more with medieval taverns and pubs rather than the royal courts or church.
While medieval metal is a German phenomenon, one of the inspirations for the genre is the English folk metal band Skyclad. Formed in 1990 as a thrash metal band, they added violins from session musician Mike Evans on several tracks from their debut album, "The Wayward Sons of Mother Earth", with the song "The Widdershins Jig" acclaimed as "particularly significant" and "a certain first in the realms of Metal". The band added a full time violinist to their ranks and has since been credited not only as the originators and pioneers of folk metal but also as a direct inspiration for medieval metal bands.
Origins.
The East German band Subway to Sally was formed in 1992 as a folk rock band, singing in English and incorporating Irish and Scottish influences in their music. With their second album "MCMXCV" released in 1995, the band adopted a "more traditional approach" and started singing in German. Taking Skyclad as an influence, Subway to Sally performs a blend of hard rock and heavy metal music "enriched with medieval melodies enmeshed in the songs via bagpipes, hurdy-gurdy, lute, mandoline, shalm , fiddle and flute" and combined with "romantic-symbolic German-speaking poetry" in their lyrics. With chart success in their native Germany, they have since been credited as the band "that set off the wave of what is known as medieval rock."
In the year 1994, a concert was organised in Berlin that featured a collaboration between a rock band known as Noah and members of the aforementioned medieval group Corvus Corax. The result of this mix of medieval and rock music saw the group Noah turning into In Extremo. They began with two acoustic medieval albums before releasing a metal album "Weckt die Toten!" in 1998. They have since found chart success in Germany with their "medieval style stage garb and unashamed usage of such bizarre instruments as the Scottish bagpipes."
Corvus Corax also joined in the fray with the release of an EP in 1996 that featured metal music with bagpipes. The EP was titled Tanzwut and the group has since continued exploring medieval metal as a side project by that name. Their style blends not only medieval music and heavy metal but also industrial and electronic beats.
The year 1999 also saw the release of Schandmaul's debut album. Describing themselves as the "minstrels of today," the Bavarian outfit employs a musical arsenal that includes the bagpipes, barrel-organ, shawm, violin and mandolin. Like Subway to Sally and In Extremo, Schandmaul has experienced chart success in their native Germany. Other groups that also emerged during the late 1990s and early 2000s included Letzte Instanz, Morgenstern and Schattentantz.
Musical characteristics.
Like its parent genre, medieval rock features the same typical instruments found in heavy metal music: guitars, bass, drums and vocalist. Bands in the genre are known to supplement their sound with a wide range of folk and traditional instruments. Woodwind instruments like the bagpipes, flutes and shawm can be found in the music of Corvus Corax, Tanzwut, In Extremo, Schandmaul, Morgernstern, Schattentantz and Subway to Sally while string instruments like the violin, lute, hurdy-gurdy, cello, harp and mandolin are employed by Subway to Sally, In Extremo, Schandmaul, Morgernstern, and Schattentantz.

</doc>
<doc id="71589" url="https://en.wikipedia.org/wiki?curid=71589" title="Infrared Data Association">
Infrared Data Association

The Infrared Data Association (IrDA) is an industry-driven interest group that was founded in 1993 by around 50 companies. IrDA provides specifications for a complete set of protocols for wireless infrared communications, and the name "IrDA" also refers to that set of protocols. The main reason for using IrDA had been wireless data transfer over the “last one meter” using point-and-shoot principles. Thus, it has been implemented in portable devices such as mobile telephones, laptops, cameras, printers, medical devices. Main characteristics of this kind of wireless optical communication is physically secure data transfer, line-of-sight (LOS) and very low bit error rate (BER) that makes it very efficient.
Specifications.
IrPHY.
The mandatory IrPHY (Infrared Physical Layer Specification) is the physical layer of the IrDA specifications. It comprises optical link definitions, modulation, coding, cyclic redundancy check (CRC) and the framer. Different data rates use different modulation/coding schemes:
Further characteristics are:
The frame size depends on the data rate mostly and varies between 64 B and 64 kB. Additionally, bigger blocks of data can be transferred by sending multiple frames consecutively. This can be adjusted with a parameter called "window size" (1–127). Finally, data blocks up to 8 MB can be sent at once. Combined with a low bit error rate of generally <, that communication could be very efficient compared to other wireless solutions.
IrDA transceivers communicate with infrared pulses (samples) in a cone that extends at least 15 degrees half angle off center. The IrDA physical specifications require the lower and upper limits of irradiance such that a signal is visible up to one meter away, but a receiver is not overwhelmed with brightness when a device comes close. In practice, there are some devices on the market that do not reach one meter, while other devices may reach up to several meters. There are also devices that do not tolerate extreme closeness. The typical sweet spot for IrDA communications is from away from a transceiver, in the center of the cone. IrDA data communications operate in half-duplex mode because while transmitting, a device’s receiver is blinded by the light of its own transmitter, and thus full-duplex communication is not feasible. The two devices that communicate simulate full-duplex communication by quickly turning the link around. The primary device controls the timing of the link, but both sides are bound to certain hard constraints and are encouraged to turn the link around as fast as possible.
IrLAP.
The mandatory IrLAP (Infrared Link Access Protocol) is the second layer of the IrDA specifications. It lies on top of the IrPHY layer and below the IrLMP layer. It represents the data link layer of the OSI model.
The most important specifications are:
On the IrLAP layer the communicating devices are divided into a "primary device" and one or more "secondary devices". The primary device controls the secondary devices. Only if the primary device requests a secondary device to send, is it allowed to do so.
IrLMP.
The mandatory IrLMP (Infrared Link Management Protocol) is the third layer of the IrDA specifications. It can be broken down into two parts. 
First, the LM-MUX (Link Management Multiplexer), which lies on top of the IrLAP layer. Its most important achievements are:
Second, the LM-IAS (Link Management Information Access Service), which provides a list, where service providers can register their services so other devices can access these services by querying the LM-IAS.
Tiny TP.
The optional Tiny TP (Tiny Transport Protocol) lies on top of the IrLMP layer. It provides:
IrCOMM.
The optional IrCOMM (Infrared Communications Protocol) lets the infrared device act like either a serial or parallel port. It lies on top of the IrLMP layer.
OBEX.
The optional OBEX (Object Exchange) provides the exchange of arbitrary data objects (e.g., vCard, vCalendar or even applications) between infrared devices. It lies on top of the Tiny TP protocol, so Tiny TP is mandatory for OBEX to work.
IrLAN.
The optional IrLAN (Infrared Local Area Network) provides the possibility to connect an infrared device to a local area network. There are three possible methods:
As IrLAN lies on top of the Tiny TP protocol, the Tiny TP protocol must be implemented for IrLAN to work.
IrSimple.
IrSimple achieves at least 4 to 10 times faster data transmission speeds by improving the efficiency of the infrared IrDA protocol. A 500 KB normal picture from a cell phone can be transferred within 1 second.
IrSimpleShot.
One of the primary targets of IrSimpleShot (IrSS) is to allow the millions of IrDA-enabled camera phones to wirelessly transfer pictures to printers, printer kiosks and flat-panel TVs.
Reception.
IrDA was popular on PDAs, laptops and some desktops from the late 1990s through the early 2000s. However, it has been displaced by other wireless technologies such as Wi-Fi and Bluetooth, favored because they don't need a direct line of sight and can therefore support hardware like mice and keyboards. It is still used in some environments where interference makes radio-based wireless technologies unusable.
An attempt was made to revive IrDA around 2005

</doc>
<doc id="71601" url="https://en.wikipedia.org/wiki?curid=71601" title="Portuguese colonization of the Americas">
Portuguese colonization of the Americas

Portugal was the leading country in the European exploration of the world in the 15th century. The Treaty of Tordesillas in 1494 divided the Earth outside Europe into Castilian and Portuguese global territorial hemispheres for exclusive conquest and colonization. Portugal colonized parts of South America (mostly Brazil), but also made some unsuccessful attempts to colonize North America in present-day Canada.
Settlements in North America.
Based on the Treaty of Tordesillas, the Portuguese Crown claimed it had territorial rights in the area visited by John Cabot in 1497 and 1498. To that end, in 1499 and 1500, the Portuguese mariner João Fernandes Lavrador visited the northeast Atlantic coast and Greenland, which accounts for the appearance of "Labrador" on topographical maps of the period. Subsequently, in 1501 and 1502 the Corte-Real brothers explored and charted Greenland and what is today the Canadian province of Newfoundland and Labrador, claiming these lands as part of the Portuguese Empire. Fragmentary evidence also suggests a previous expedition in 1473 by João Vaz Corte-Real, their father, with other Europeans, to "Terra Nova do Bacalhau" ("Newfoundland of the Codfish") in North America. The possible voyage of 1473 and several other possible earlier pre-Columbian expeditions throughout the 15th century and before 1473, ordered from the Azores and Continental Portugal, to the area of North America, remain as matters of great controversy for scholars, and based in brief, often unclear as its precise destinations, or fragmentary, historical documents. In 1506, King Manuel I of Portugal created taxes for the cod fisheries in Newfoundland waters. João Álvares Fagundes and Pêro de Barcelos established fishing outposts in Newfoundland and Nova Scotia around 1521; however, these were later abandoned, with the Portuguese colonizers focusing their efforts on South America. Although the Portuguese-founded town of Portugal Cove-St. Philip's, Newfoundland and Labrador, Canada remains important as a cultural center, nowadays.
Colonization of Brazil.
In April 1500, the second Portuguese India Armada, headed by Pedro Álvares Cabral, with a crew of expert captains, including Bartolomeu Dias and Nicolau Coelho, encountered the Brazilian coast as it swung westward in the Atlantic while performing a large "volta do mar" to avoid becalming in the Gulf of Guinea. On 21 April 1500 a mountain was seen and was named "Monte Pascoal", and on the 22 April Cabral landed on the coast, in Porto Seguro. Believing the land to be an island, he named it Ilha de Vera Cruz (Island of the True Cross). The previous expedition of Vasco da Gama to India already recorded several signs of land near its western open Atlantic Ocean route, in 1497. It has also been suggested that Duarte Pacheco Pereira may have discovered the coasts of Brazil in 1498, possible its northeast, but the exact area of the expedition and the explored regions remain unclear. On the other hand, some historians have suggested that the Portuguese may have encountered the South American bulge earlier while sailing the "volta do mar" (in the Southwest Atlantic), hence the insistence of John II in moving the line west of Tordesillas in 1494—so his landing in Brazil may not have been an accident; although John's motivation may have simply been to increase the chance of claiming new lands in the Atlantic. From the east coast, the fleet then turned eastward to resume the journey to the southern tip of Africa and India. Landing in the New World and reaching Asia, the expedition connected four continents for the first time in history.
In 1501–1502 an expedition led by Gonçalo Coelho (or André Gonçalves and/or Gaspar de Lemos), sailed south along the coast of South America to the bay of present-day Rio de Janeiro. Among his crew was the Florentine Amerigo Vespucci. According to Vespucci, the expedition reached the latitude "South Pole elevation 52° S" in the "cold" latitudes of what is now Patagonia, near the Strait of Magellan, before turning back. Vespucci wrote that they headed toward the southwest-south, following "a long, unbending coastline". This seems controversial, since he changed part of his description in the subsequent letter (stating that around 32° S, they made a shift to open sea, to south-southeast), maintaining, however, that they reached a similar 50° S latitude.
Amerigo Vespucci participated as observer in four Spanish and Portuguese exploratory voyages. The expeditions became widely known in Europe after two accounts attributed to him, published between 1502 and 1504. His last two voyages to the east and southern east coasts of South America, by Portugal, especially the expedition of 1501-1502 to Brazil and beyond, and its meeting with Cabral`s ships and men (who had touched the South American, African and Asian continents) on the African coast, at Bezeguiche (the bay of Dakar, Senegal), listening the accounts of its sailors (then returning to Portugal), were the most decisive for his "New World" hypothesis. Vespucci suggested that the newly discovered lands (especially what is today South America/Brazil) were not the Indies but a "New World", the "Mundus novus", Latin title of a contemporary document based on Vespucci letters to Lorenzo di Pierfrancesco de' Medici, which had become widely popular in Europe.
Around 1508 or 1511-1512, Portuguese captains reached and explored the River Plate estuary in the present-day Uruguay and Argentina, and went as far south as the present-day Gulf of San Matias at 42°S (recorded in the "Newen Zeytung auss Pressilandt" meaning "New Tidings from the Land of Brazil").
Some historians have attributed this voyage to Coelho and Vespucci years before, but a good part of historians and researchers, through the sparse and comparative documentation, identify the captains and the experienced pilot of the "India run" ("the best Pilot of Portugal" and a "best friend" of the Fugger's Agent), with Diogo Ribeiro, Estevão Frois and the pilot João de Lisboa. The explorers also reported that after going by the 40th parallel to south, along the coast, they found a "land" or "point extending into the sea", and further south, a Gulf.
This and the following expeditions of Cristóvão Jacques to the River Plate and into the Parana River in 1521; and of Martim Afonso de Sousa and his brother Pero Lopes de Sousa, in 1530-1532, from the Amazon river, to Lagoa dos Patos and to the rivers Plate and Paraná, reinforced and demonstrated Portuguese interest in the River Plate.
Permanent habitation in Brazil did not begin until São Vicente was founded in 1532 by Martim Afonso de Sousa, although temporary trading posts were established earlier to collect brazilwood, used as a dye. São Vicente, by its "democratic" municipal prerogatives (in the tradition of Portuguese municipalism since the Middle Ages) and by the general elections to its first "Câmara" (City Council) on August 22, 1532, is symbolically considered the birthplace of democracy in the Americas.
From 1534 to 1536, 15 Captaincy colonies were created in Portuguese America. The captaincies were autonomous, and mostly private, colonies of the Portuguese Empire, each owned and run by a Captain-major.
In 1549, due to their failure and limited success, the Captaincy Colonies of Brazil were united into the Governorate General of Brazil. The captaincy colonies were reorganized as provincial districts to the Governorate. The captaincies continued to be ruled by their hereditary captain-majors but they now reported to the Governor-General of Brazil. The new system was implemented so that Portuguese America could be managed correctly and provide a steady and wealthy income for the Portuguese Empire. The capital of the new governorate established its capital at São Salvador and the first Jesuits arrived the same year.
With permanent settlement came the establishment of the sugar cane industry and its intensive labor demands which were met with Native and later African slaves.
From 1565 through 1567, Mem de Sá, the third Governor General of Brazil, successfully destroyed a ten-year-old French colony called France Antarctique, at Guanabara Bay. He and his nephew, Estácio de Sá, then founded the city of Rio de Janeiro in March 1567.
In 1621, Philip II of Portugal divided the Governorate General of Brazil into two separate and autonomous colonies, the State of Maranhão and the State of Brazil. Regarding this period it is preferable to refer to "Portuguese America" rather than "Portuguese Brazil" or "Colonial Brazil", as the states were two separate colonies, each with their own governor general and government.
Between 1630 and 1654, the Netherlands came to control part of Brazil's Northeast region, with their capital in Recife. The Portuguese won a significant victory in the Second Battle of Guararapes in 1649. By 1654, the Netherlands had surrendered and returned control of all Brazilian land to the Portuguese.
In 1751, the State of Maranhão was restructured into State of Grão-Pará and Maranhão, with a new capital and government.
In 1772, the State of Grão-Pará and Maranhão was split into two new states, the State of Grão-Pará and Rio Negro and the State of Maranhão and Piauí. The new states would fare poorly and only last 3 years.
In 1775, the three colonies of Portuguese America (the State of Brazil, the State of Maranhão and Piauí; and the State of Grão-Pará and Rio Negro) were united into a singular colony, under the State of Brazil. This arrangement would last until the end of Colonial Brazil. As a result, Brazil did not split into several countries, as happened to its Spanish-speaking neighbors.
Caribbean merchants.
The "Early Navigators" practically have been to the entire Caribbean, from The Bahamas to Jamaica. Papiamento, one of the languages spoken in the islands, is a mixture of Portuguese and African languages.
Portuguese merchants have been trading in the West Indies. To such an extent, that, for instance, for the Portuguese town of Póvoa de Varzim, most of its seafarers dying abroad, most of the deaths occurred in the Route of the Antilles, in the West Indies. At the turn of the 17th century, with the union with Castile, the Spanish kings favored the free movement of the people, and other lands of the New World, such as Peru and the Gulf of Mexico, were open to the Portuguese merchants.
Colonization of Uruguay.
The Portuguese founded the first Uruguayan city, Colônia do Sacramento.

</doc>
<doc id="71605" url="https://en.wikipedia.org/wiki?curid=71605" title="D block">
D block

D block or variant, may mean:

</doc>
<doc id="71608" url="https://en.wikipedia.org/wiki?curid=71608" title="Document management system">
Document management system

A document management system (DMS) is a system (based on computer programs in the case of the management of digital documents) used to track, manage and store documents and reduce paper. Most are capable of keeping a record of the various versions created and modified by different users (history tracking). The term has some overlap with the concepts of content management systems. It is often viewed as a component of enterprise content management (ECM) systems and related to digital asset management, document imaging, workflow systems and records management systems.
History.
Beginning in the 1980s, a number of vendors began developing software systems to manage paper-based documents. These systems dealt with paper documents, which included not only printed and published documents, but also photographs, prints, etc..
Later developers began to write a second type of system which could manage electronic documents, i.e., all those documents, or files, created on computers, and often stored on users' local file-systems. The earliest electronic document management (EDM) systems managed either proprietary file types, or a limited number of file formats. Many of these systems later became known as document imaging systems, because they focused on the capture, storage, indexing and retrieval of image file formats. EDM systems evolved to a point where systems could manage any type of file format that could be stored on the network. The applications grew to encompass electronic documents, collaboration tools, security, workflow, and auditing capabilities.
These systems enabled an organization to capture faxes and forms, to save copies of the documents as images, and to store the image files in the repository for security and quick retrieval (retrieval made possible because the system handled the extraction of the text from the document in the process of capture, and the text-indexer function provided text-retrieval capabilities).
While many EDM systems store documents in their native file format (Microsoft Word or Excel, PDF), some web-based document management systems are beginning to store content in the form of html. These policy management systems require content to be imported into the system. However, once content is imported, the software (ex. Corona Document Management System) acts like a search engine so users can find what they are looking for faster. The html format allows for better application of search capabilities such as full-text searching and stemming.
Components.
Document management systems commonly provide storage, versioning, metadata, security, as well as indexing and retrieval capabilities. Here is a description of these components:
Standardization.
Many industry associations publish their own lists of particular document control standards that are used in their particular field. Following is a list of some of the relevant ISO documents. Divisions ICS 01.140.10 and 01.140.20. The ISO has also published a series of standards regarding the technical documentation, covered by the division of 01.110.
Document control.
Government regulations require that companies working in certain industries control their documents. These industries include accounting (for example: 8th EU Directive, Sarbanes–Oxley Act), food safety (e.g., Food Safety Modernization Act), ISO (mentioned above), medical-device manufacturing (FDA), manufacture of blood, human cells, and tissue products (FDA), healthcare ("JCAHO"), and information technology ("ITIL").
Particular industries are held to a higher level of documentation control, for privacy, warranty and insurance purposes. For example, in the construction industry, it is common to hold project documentation on record and adequately controlled for warranty periods ranging on average for 5 to 10 years. An information systems strategy plan (ISSP) can shape organisational information systems over medium to long-term periods.
Documents stored in a document management system—such as procedures, work instructions, and policy statements—provide evidence of documents under control. Failing to comply can cause fines, the loss of business, or damage to a business's reputation.
The following are important aspects of document control:
Integrated DM.
Integrated document management comprises the technologies, tools, and methods used to capture, manage, store, preserve, deliver and dispose of 'documents' across an enterprise. In this context 'documents' are any of a myriad of information assets including images, office documents, graphics, and drawings as well as the new electronic objects such as Web pages, email, instant messages, and video.

</doc>
<doc id="71611" url="https://en.wikipedia.org/wiki?curid=71611" title="IBM 3720">
IBM 3720

The IBM 3720 was a communications controller (front-end processor) made by IBM, suitable for use with IBM System/390. The 3720, introduced in 1986, was capable of supporting up to 60 communications lines, and was a smaller version of the 3725. Official service support was withdrawn in 1999 in favour of the IBM 3745.
The IBM 3720 is unrelated to the similarly-numbered IBM 3270 display terminal system.

</doc>
<doc id="71617" url="https://en.wikipedia.org/wiki?curid=71617" title="Infant mortality">
Infant mortality

Infant mortality refers to deaths of young children, typically those less than one year of age. It is measured by the infant mortality rate (IMR), which is the number of deaths of children under one year of age per 1000 live births.
The leading causes of infant mortality are birth asphyxia, pneumonia, term birth complications, neonatal infection, diarrhea, malaria, measles and malnutrition. Many factors contribute to infant mortality, such as the mother's level of education, environmental conditions, and political and medical infrastructure. Improving sanitation, access to clean drinking water, immunization against infectious diseases, and other public health measures can help reduce high rates of infant mortality.
Child mortality is the death of a child before the child's fifth birthday, measures as the Under-5 Child Mortality Rate (U5MR). National statistics sometimes group these two mortality rates together. Globally, ten million infants and children die each year before their fifth birthday; 99% of these deaths occur in developing nations.
Infant mortality rate was an indicator used to monitor progress towards the Fourth Goal of the Millennium Development Goals of the United Nations for the year 2015. It is now a target in the Sustainable Development Goals for Goal Number 3 ("Ensure healthy lives and promote well-being for all at all ages").
Classification.
Infant mortality rate (IMR) is the number of deaths of children less than one year of age per 1000 live births. The rate for a given region is the number of children dying under one year of age, divided by the number of live births during the year, multiplied by 1,000.
Forms of infant mortality:
Causes.
Leading causes of congenital infant mortality are malformations, sudden infant death syndrome, maternal complications during pregnancy, and accidents and unintentional injuries. Environmental and social barriers prevent access to basic medical resources and thus contribute to an increasing infant mortality rate; 99% of infant deaths occur in developing countries, and 86% of these deaths are due to infections, premature births, complications during delivery, and perinatal asphyxia and birth injuries. Greatest percentage reduction of infant mortality occurs in countries that already have low rates of infant mortality.
Common causes are preventable with low-cost measures. In the United States, a primary determinant of infant mortality risk is infant birth weight with lower birth weights increasing the risk of infant mortality. The determinants of low birth weight include socio-economic, psychological, behavioral and environmental factors.
Medical.
Causes of infant mortality that are related to medical conditions include: low birth weight, sudden infant death syndrome, malnutrition and infectious diseases, including neglected tropical diseases.
Low birth weight.
Low birth weight makes up 60–80% of the infant mortality rate in developing countries. "The New England Journal of Medicine" stated that "The lowest mortality rates occur among infants weighing . For infants born weighing or less, the mortality rate rapidly increases with decreasing weight, and most of the infants weighing or less die. As compared with normal-birth-weight infants, those with low weight at birth are almost 40 times more likely to die in the neonatal period; for infants with very low weight at birth the relative risk of neonatal death is almost 200 times greater." Infant mortality due to low birth weight is usually a direct cause stemming from other medical complications such as preterm birth, poor maternal nutritional status, lack of prenatal care, maternal sickness during pregnancy, and an unhygienic home environments. Along with birth weight, period of gestation makes up the two most important predictors of an infant's chances of survival and their overall health.
According to the "New England Journal of Medicine", "in the past two decades, the infant mortality rate (deaths under one year of age per thousand live births) in the United States has declined sharply." Low birth weights from African American mothers remain twice as high as that of white women. LBW may be the leading cause of infant deaths, and it is greatly preventable. Although it is preventable, the solutions may not be the easiest but effective programs to help prevent LBW are a combination of health care, education, environment, mental modification and public policy, influencing a culture supporting lifestyle. Preterm birth is the leading cause of newborn deaths worldwide. Even though America excels past many other countries in the care and saving of premature infants, the percentage of American woman who deliver prematurely is comparable to those in developing countries. Reasons for this include teenage pregnancy, increase in pregnant mothers over the age of thirty-five, increase in the use of in-vitro fertilization which increases the risk of multiple births, obesity and diabetes. Also, women who do not have access to health care are less likely to visit a doctor, therefore increasing their risk of delivering prematurely.
Sudden infant death syndrome.
Thousands of infant deaths per year are classified as Sudden infant death syndrome (SIDS). According to the Mayo Clinic, SIDS is the unexplained death, usually during sleep, of a seemingly healthy baby. Although the direct cause of SIDS remains unknown, many doctors believe that there are several factors that put babies at an increased risk of SIDS, including: babies sleeping on their stomachs, exposure to cigarette smoke in the womb or after birth, sleeping in bed with parents, premature birth, being a twin or triplet, being born to a teen mother, and also living in poverty settings. Although the cause is unknown and currently cannot be explained, doctors have come to the conclusion that SIDS is most likely to occur between 2 and 4 months and most deaths occur in the winter time. Recommended precautions include ensuring that infants sleep on their backs, controlling the temperature of the bedroom, employing a crib without toys or excess bedding, and breastfeeding.
Malnutrition.
Malnutrition frequently accompanies these diseases, and is a primary factor contributing to the complications of both diarrhea and pneumonia, although the causal links and mechanisms remain unclear. Factors other nutrition also influence the incidence of diarrhea, including socioeconomic status, disruption of traditional lifestyles, access to clean water and sanitation facilities, age and breast-feeding status.
Protein energy malnutrition and micronutrient deficiency are two reasons for stunted growth in children under five years old in the least developed countries. Malnutrition leads to diarrhea and dehydration, and ultimately death. Millions of women in developing countries are stunted due to a history of childhood malnutrition. Women's bodies are thus underdeveloped, and their chances of surviving childbirth decrease. Due to underdeveloped bodies, the probability of an obstructed pregnancy increases. Protein-energy deficiency results in low-quality breast milk that provides less energy and other nutrition.
Vitamin A deficiency can lead to stunted growth, blindness, and increased mortality due to the lack of nutrients in the body. Two hundred and fifty million infants are affected by Vitamin A deficiency. Among women in developing countries, 40% have iron deficiency anemia, which increases maternal and infant mortality rates, chances of stillbirth, cases of low birth weight babies, premature delivery, and probability of fetal brain damage. One way to prevent Vitamin A deficiency is to educate the mother on the many benefits of breastfeeding. Breast milk is a natural source of Vitamin A, and supplies the suckling infant with enough Vitamin A.
Infectious diseases.
Babies born in low to middle income countries in sub-Saharan Africa and southern Asia are at the highest risk of neonatal death. Bacterial infections of the bloodstream, lungs, and the brain's covering (meningitis) are responsible for 25% of neonatal deaths. Newborns can acquire infections during birth from bacteria that are present in their mother's reproductive tract. The mother may not be aware of the infection, or she may have an untreated pelvic inflammatory disease or sexually transmitted disease. These bacteria can move up the vaginal canal into the amniotic sac surrounding the baby. Maternal blood-borne infection is another route of bacterial infection from mother to baby. Neonatal infection is also more likely with the premature rupture of the membranes (PROM) of the amniotic sac.
Seven out of ten childhood deaths are due to infectious diseases: acute respiratory infection, diarrhea, measles, and malaria. Acute respiratory infection such as pneumonia, bronchitis, and bronchiolitis account for 30% of childhood deaths; 95% of pneumonia cases occur in the developing world. Diarrhea is the second-largest cause of childhood mortality in the world, while malaria causes 11% of childhood deaths. Measles is the fifth-largest cause of childhood mortality. Folic acid for mothers is one way to combat iron deficiency. A few public health measures used to lower levels of iron deficiency anemia include iodize salt or drinking water, and include vitamin A and multivitamin supplements into a mother's diet. A deficiency of this vitamin causes certain types of anemia (low red blood cell count).
Environmental.
Infant mortality rate can be a measure of a nation's health and social condition. It is a composite of a number of component rates which have their separate relationship with various social factors and can often be seen as an indicator to measure the level of socioeconomic disparity within a country.
Organic water pollution is a better indicator of infant mortality than health expenditures per capita. Water contaminated with various pathogens houses a host of parasitic and microbial infections. Infectious disease and parasites are carried via water pollution from animal wastes. Areas of low socioeconomic status are more prone to inadequate plumbing infrastructure, and poorly maintained facilities. The burning of inefficient fuels doubles the rate of children under 5 years old with acute respiratory tract infections. Climate and geography often play a role in sanitation conditions. For example, the inaccessibility of clean water exacerbates poor sanitation conditions.
People who live in areas where particulate matter (PM) air pollution is higher tend to have more health problems across the board. Short-term and long-term effects of ambient air pollution are associated with an increased mortality rate, including infant mortality. Air pollution is consistently associated with post neonatal mortality due to respiratory effects and sudden infant death syndrome. Specifically, air pollution is highly associated with SIDs in the United States during the post-neonatal stage. High infant mortality is exacerbated because newborns are a vulnerable subgroup that is affected by air pollution. Newborns who were born into these environments are no exception. Women who are exposed to greater air pollution on a daily basis who are pregnant should be closely watched by their doctors, as well as after the baby is born. Babies who live in areas with less air pollution have a greater chance of living until their first birthday. As expected, babies who live in environments with more air pollution are at greater risk for infant mortality. Areas that have higher air pollution also have a greater chance of having a higher population density, higher crime rates and lower income levels, all of which can lead to higher infant mortality rates.
The key pollutant for infant mortality rates is carbon monoxide. Carbon monoxide is a colorless, odorless gas that does great harm especially to infants because of their immature respiratory system.
Another major pollutant is second-hand smoke, which is a pollutant that can have detrimental effects on a fetus. According to the "American Journal of Public Health", "in 2006, more than 42 000 Americans died of second hand smoke-attributable diseases, including more than 41 000 adults and nearly 900 infants ... fully 36% of the infants who died of low birth weight caused by exposure to maternal smoking in utero were Blacks, as were 28% of those dying of respiratory distress syndrome, 25% dying of other respiratory conditions, and 24% dying of sudden infant death syndrome." "The American Journal of Epidemiology" also stated that "Compared with nonsmoking women having their first birth, women who smoked less than one pack of cigarettes per day had a 25% greater risk of mortality, and those who smoked one or more packs per day had a 56% greater risk. Among women having their second or higher birth, smokers experienced 30% greater mortality than nonsmokers."
Modern research in the United States on racial disparities in infant mortality suggests a link between the institutionalized racism that pervades the environment and high rates of African American infant mortality. In synthesis of this research, it has been observed that "African American infant mortality remains elevated due to the social arrangements that exist between groups and the lifelong experiences responding to the resultant power dynamics of these arrangements."
Socio-economic factors.
Social class is a major factor in infant mortality, both historically and today. Between 1912 and 1915, the Children's Bureau in the United States examined data across eight cities and nearly 23,000 live births. They discovered that lower incomes tend to correlate with higher infant mortality. In cases where the father had no income, the rate of infant mortality was 357% more than that for the highest income earners ($1,250+). Differences between races were also apparent. African-American mothers experience infant mortality at a rate 44% higher than average; however, research indicates that socio-economic factors do not totally account for the racial disparities in infant mortality.
While infant mortality is normally negatively correlated with GDP, there may indeed be some opposing short-term effects from a recession. A recent study by "The Economist" showed that economic slowdowns reduce the amount of air pollution, which results in a lower infant mortality rate. In the late 1970s and early 1980s, the recession's impact on air quality is estimated to have saved around 1,300 US babies. It is only during deep recessions that infant mortality increases. According to Norbert Schady and Marc-François Smitz, recessions when GDP per capita drops by 15% or more increase infant mortality.
Social class dictates which medical services are available to an individual. Disparities due to socioeconomic factors have been exacerbated by advances in medical technology. Developed countries, most notably the United States, have seen a divergence between those living in poverty who cannot afford medical advanced resources, leading to an increased chance of infant mortality, and others.
War.
In policy, there is a lag time between realization of a problem's possible solution and actual implementation of policy solutions. Infant mortality rates are related to war, political unrest, and government corruption.
In most cases, war-affected areas will experience a significant increase in infant mortality rates.. Having a War taking place where a woman is planning on having a baby is not only stressful on the mother and fetus, but also has several detrimental effects. Commondreams.org reported that "in the years since 1990, Iraq has seen its child mortality rate soar by 125 per cent, the highest increase of any country in the world. Its rate of deaths of children under five now matches that of Mauritania." Also, "Figures collated by the charity show that in 1990 Iraq's mortality rate for under-fives was 50 per 1,000 live births. In 2005 it was 125. While many other countries have higher rates – Angola, Somalia and the Democratic Republic of Congo, for instance, all have rates above 200 – the increase in Iraq is higher than elsewhere" according to commondreams.org. The primary causes of the increase are external factors such as murder and abuse. However, many other significant factors influence infant mortality rates in war-torn areas. Health care systems in developing countries in the midst of war often collapse. Attaining basic medical supplies and care becomes increasingly difficult. During the Yugoslav Wars in the 1990s Bosnia experienced a 60% decrease in child immunizations. Preventable diseases can quickly become epidemic given the medical conditions during war.
Many developing countries rely on foreign aid for basic nutrition. Transport of aid becomes significantly more difficult in times of war. In most situations the average weight of a population will drop substantially. Expecting mothers are affected even more by lack of access to food and water. During the Yugoslav Wars in Bosnia the number of premature babies born increased and the average birth weight decreased.
There have been several instances in recent years of systematic rape as a weapon of war. Women who become pregnant as a result of war rape face even more significant challenges in bearing a healthy child. Studies suggest that women who experience sexual violence before or during pregnancy are more likely to experience infant death in their children.
Causes of infant mortality in abused women range from physical side effects of the initial trauma to psychological effects that lead to poor adjustment to society. Many women who became pregnant by rape in Bosnia were isolated from their hometowns making life after childbirth exponentially more difficult.
Medicine and biology.
Developing countries have a lack of access to affordable and professional health care resources, and skilled personnel during deliveries. Countries with histories of extreme poverty also have a pattern of epidemics, endemic infectious diseases, and low levels of access to maternal and child healthcare.
The American Academy of Pediatrics recommends that infants need multiple doses of vaccines such as diphtheria-tetanus-acellular pertussis vaccine, Haemophilus influenzae type b (Hib) vaccine, Hepatitis B (HepB) vaccine, inactivated polio vaccine (IPV), and pneumococcal vaccine (PCV). Research was conducted by the Institute of Medicine's Immunization Safety Review Committee concluded that there is no relationship between these vaccines and risk of SIDS in infants. This tells us that not only is it extremely necessary for every child to get these vaccines to prevent serious diseases, but there is no reason to believe that if your child does receive an immunization that it will have any effect on their risk of SIDS.
Economics.
Political modernization perspective, the neo-classical economic theory that scarce goods are most effectively distributed to the market, say that the level of political democracy influences the rate of infant mortality. Developing nations with democratic governments tend to be more responsive to public opinion, social movements, and special interest groups for issues like infant mortality. In contrast, non-democratic governments are more interested in corporate issues and less so in health issues. Democratic status effects the dependency a nation has towards its economic state via export, investments from multinational corporations and international lending institutions.
Levels of socioeconomic development and global integration are inversely related to a nation's infant mortality rate. Dependency perspective occurs in a global capital system. A nation's internal impact is highly influenced by its position in the global economy and has adverse effects on the survival of children in developing countries. Countries can experience disproportionate effects from its trade and stratification within the global system. It aids in the global division of labor, distorting the domestic economy of developing nations. The dependency of developing nations can lead to a reduce rate of economic growth, increase income inequality inter- and intra-national, and adversely affects the wellbeing of a nation's population. A collective cooperation between economic countries plays a role in development policies in the poorer, peripheral, countries of the world.
These economic factors present challenges to governments' public health policies. If the nation's ability to raise its own revenues is compromised, governments will lose funding for its health service programs, including services that aim in decreasing infant mortality rates. Peripheral countries face higher levels of vulnerability to the possible negative effects of globalization and trade in relation to key countries in the global market.
Even with a strong economy and economic growth (measured by a country's gross national product), the advances of medical technologies may not be felt by everyone, lending itself to increasing social disparities.
Cultural.
High rates of infant mortality occur in developing countries where financial and material resources are scarce and there is a high tolerance to high number of infant deaths. There are circumstances where a number of developing countries to breed a culture where situations of infant mortality such as favoring male babies over female babies are the norm. In developing countries such as Brazil, infant mortality rates are commonly not recorded due to failure to register for death certificates. Failure to register is mainly due to the potential loss of time and money and other indirect costs to the family. Even with resource opportunities such as the 1973 Public Registry Law 6015, which allowed free registration for low-income families, the requirements to qualify hold back individuals who are not contracted workers.
Another cultural reason for infant mortality, such as what is happening in Ghana, is that "besides the obvious, like rutted roads, there are prejudices against wives or newborns leaving the house." Because of this it is making it even more difficult for the women and newborns to get the treatment that is available to them and that is needed.
Cultural influences and lifestyle habits in the United States can account for some deaths in infants throughout the years. It has been reported that cultures other than white have an increased chance of experiencing infant mortality. According to the Journal of the American Medical Association "the post neonatal mortality risk (28 to 364 days) was highest among continental Puerto Ricans" compared to babies of the non-Hispanic race. Examples of this include teenage pregnancy, obesity, diabetes and smoking. All are possible causes of premature births, which is the second highest cause of infant mortality. Ethnic differences experienced in the United States are accompanied by higher prevalence of behavioral risk factors and sociodemographic challenges that each ethnic group faces.
Gender favoritism.
Historically, males have had higher infant mortality rates than females. The difference between male and female infant mortality rates have been dependent on environmental, social, and economic conditions. More specifically, males are biologically more vulnerable to infections and conditions associated with prematurity and development. Before 1970, the reasons for male infant mortality were due to infections, and chronic degenerative diseases. However, since 1970, certain cultures emphasizing males has led to a decrease in the infant mortality gap between males and females. Also, medical advances have resulted in a growing number of male infants surviving at higher rates than females due to the initial high infant mortality rate of males.
Genetic components results in newborn females being biologically advantaged when it comes to surviving their first birthday. Males, biologically, have lower chances of surviving infancy in comparison to female babies. As infant mortality rates saw a decrease on a global scale, the gender most affected by infant mortality changed from males experiences a biological disadvantage, to females facing a societal disadvantage. Some developing nations have social and cultural patterns that reflects adult discrimination to favor boys over girls for their future potential to contribute to the household production level. A country's ethnic composition, homogeneous versus heterogeneous, can explain social attitudes and practices. Heterogeneous level is a strong predictor in explaining infant mortality.
Birth spacing.
Birth spacing is the time between births. Births spaced at least three years apart from one another are associated with the lowest rate of mortality. The longer the interval between births, the lower the risk for having any birthing complications, and infant, childhood and maternal mortality. Higher rates of pre-term births, and low birth weight are associated with birth to conception intervals of less than six months and abortion to pregnancy interval of less than six months. Shorter intervals between births increase the chances of chronic and general under-nutrition; 57% of women in 55 developing countries reported birth spaces shorter than three years; 26% report birth spacing of less than two years. Only 20% of post-partum women report wanting another birth within two years; however, only 40% are taking necessary steps such as family planning to achieve the birth intervals they want.
Unplanned pregnancies and birth intervals of less than twenty-four months are known to correlate with low birth weights and delivery complications. Also, women who are already small in stature tend to deliver smaller than average babies, perpetuating a cycle of being underweight.
Education.
The mother's educational attainment and literacy are correlated with age of first pregnancy, and probability that the mother attain prenatal and postnatal care. Mothers with a secondary education have a higher probability of waiting until a later age to get pregnant. Once pregnant, they are also more likely to get prenatal and postnatal care, and deliver their child in the presence of a skilled attendant. Women who finish at least a primary-level education have improved nutrition, medical care, information access, and economic independence. Infants reap benefits such as healthy environments, improved nutrition, and medical care. Mothers with some level of education have a higher probability to breastfeeding. The duration of breastfeeding has the potential to influence the birth space. Women without any educational background tend to have children at an earlier age, thus their bodies are not yet mature enough to carry and deliver a child.
Prevention.
Millennium Development Goals were created to improve the health and well being of people worldwide. Its fourth goal is to decrease the number of mortalities within the infant and childhood population by two thirds, meaning it will decrease mortality from 95 to 31 deaths per 1000. Countries slow to abide by the Millennium Development Goal by 2015 are projected to have difficulty in reaching goal four.
Public health.
Reductions in infant mortality are possible in any stage of a country's development. Rate reductions are evidence that a country is advancing in human knowledge, social institutions and physical capital. Governments can reduce the mortality rates by addressing the combined need for education (such as universal primary education), nutrition, and access to basic maternal and infant health services. A policy focus has the potential to aid those most at risk for infant and childhood mortality allows rural, poor and migrant populations.
Reducing chances of babies being born at low birth weights and contracting pneumonia can be accomplished by improving air quality. Improving hygiene can prevent infant mortality. Home-based technology to chlorinate, filter, and solar disinfection for organic water pollution could reduce cases of diarrhea in children by up to 48%. Improvements in food supplies and sanitation has been shown to work in the United States' most vulnerable populations, one being African Americans. Overall, women's health status need to remain high.
Simple behavioral changes, such as hand washing with soap, can significantly reduce the rate of infant mortality from respiratory and diarrheal diseases. According to UNICEF, hand washing with soap before eating and after using the toilet can save more lives of children than any single vaccine or medical intervention, by cutting deaths from diarrhea and acute respiratory infections.
Future problems for mothers and babies can be prevented. It is important that women of reproductive age adopt healthy behaviors in everyday life, such as taking folic acid, maintaining a healthy diet and weight, being physically active, avoiding tobacco use, and avoiding excessive alcohol and drug use. If women follow some of the above guidelines, later complications can be prevented to help decrease the infant mortality rates. Attending regular prenatal care check-ups will help improve the baby's chances of being delivered in safer conditions and surviving.
Focusing on preventing preterm and low birth weight deliveries throughout all populations can help to eliminate cases of infant mortality and decrease health care disparities within communities. In the United States, these two goals have decreased infant mortality rates on a regional population, it has yet to see further progress on a national level.
Medical treatments.
Technological advances in medicine would decrease the infant mortality rate and an increased access to such technologies could decrease racial and ethnic disparities. It has been shown that technological determinants are influenced by social determinants. Those who cannot afford to utilize advances in medicine tend to show higher rates of infant mortality. Technological advances has, in a way, contributed to the social disparities observed today. Providing equal access has the potential to decrease socioeconomic disparities in infant mortality. Specifically, Cambodia is facing issues with a disease that is unfortunately killing infants. The symptoms only last 24 hours and the result is death. As stated if technological advances were increased in countries it would make it easier to find the solution to diseases such as this.
Cultural changes.
Educated females practice a healthier lifestyle. The more educated a woman is the more likely she is to seek out care, give birth in the presence of a skilled attendant, breastfeed, and understand the consequences of HIV/AIDS. More educated women tend to decrease infant mortality rate by reducing their fertility. Improving women's health and social status is one way to ameliorate infant mortality. Status should rise for females seeking out education. Providing women access to family planning centers can educate mothers on how to plan ahead for their families. Educational means can also teach mothers on the beneficial health practices such as breastfeeding. Government recognizing birth space as a possible health intervention is now working towards making affordable contraception available.
Economic policies.
Granting women employment raises their status and autonomy. Having a gainful employment can raise the perceived worth of females. This can lead to an increase in the number of women getting an education and a decrease in the number of female infanticide. In the social modernization perspective, education leads to development. Higher number of skilled workers means more earning and further economic growth. According to the economic modernization perspective, this is one type economic growth viewed as the driving force behind the increase in development and standard of living in a country. This is further explained by the modernization theory- economic development promotes physical wellbeing. As economy rises, so do technological advances and thus, medical advances in access to clean water, health care facilities, education, and diet. These changes may decrease infant mortality.
Economically, governments could reduce infant mortality by building and strengthening capacity in human resources. Increasing human resources such as physicians, nurses, and other health professionals will increase the number of skilled attendants and the number of people able to give out immunized against diseases such as measles. Increasing the number of skilled professionals is negatively correlated with maternal, infant, and childhood mortality. Between 1960 and 2000, the infant mortality rate decreased by half as the number of physicians increased by four folds. With the addition of one physician to every 1000 persons in a population, infant mortality will reduce by 30%.
Differences in measurement.
The infant mortality rate correlates very strongly with, and is among the best predictors of, state failure. IMR is therefore also a useful indicator of a country's level of health or development, and is a component of the physical quality of life index.
However, the method of calculating IMR often varies widely between countries, and is based on how they define a live birth and how many premature infants are born in the country. Reporting of infant mortality rates can be inconsistent, and may be understated, depending on a nation's live birth criterion, vital registration system, and reporting practices. The reported IMR provides one statistic which reflects the standard of living in each nation. Changes in the infant mortality rate reflect social and technical capacities of a nation's population. The World Health Organization (WHO) defines a live birth as any infant born demonstrating independent signs of life, including breathing, heartbeat, umbilical cord pulsation or definite movement of voluntary muscles. This definition is used in Austria, for example. The WHO definition is also used in Germany, but with one slight modification: muscle movement is not considered to be a sign of life. Many countries, however, including certain European states (e.g. France) and Japan, only count as live births cases where an infant breathes at birth, which makes their reported IMR numbers somewhat lower and increases their rates of perinatal mortality. In the Czech Republic and Bulgaria, for instance, requirements for live birth are even higher.
Although many countries have vital registration systems and certain reporting practices, there are many inaccuracies, particularly in undeveloped nations, in the statistics of the number of infants dying. Studies have shown that comparing three information sources (official registries, household surveys, and popular reporters) that the "popular death reporters" are the most accurate. Popular death reporters include midwives, gravediggers, coffin builders, priests, and others—essentially people who knew the most about the child's death. In developing nations, access to vital registries, and other government-run systems which record births and deaths, is difficult for poor families for several reasons. These struggles force stress on families, and make them take drastic measures in unofficial death ceremonies for their deceased infants. As a result, government statistics will inaccurately reflect a nation's infant mortality rate. Popular death reporters have first-hand information, and provided this information can be collected and collated, can provide reliable data which provide a nation with accurate death counts and meaningful causes of deaths that can be measured/studied.
UNICEF uses a statistical methodology to account for reporting differences among countries:
Another challenge to comparability is the practice of counting frail or premature infants who die before the normal due date as miscarriages (spontaneous abortions) or those who die during or immediately after childbirth as stillborn. Therefore, the quality of a country's documentation of perinatal mortality can matter greatly to the accuracy of its infant mortality statistics. This point is reinforced by the demographer Ansley Coale, who finds dubiously high ratios of reported stillbirths to infant deaths in Hong Kong and Japan in the first 24 hours after birth, a pattern that is consistent with the high recorded sex ratios at birth in those countries. It suggests not only that many female infants who die in the first 24 hours are misreported as stillbirths rather than infant deaths, but also that those countries do not follow WHO recommendations for the reporting of live births and infant deaths.
Another seemingly paradoxical finding, is that when countries with poor medical services introduce new medical centers and services, instead of declining, the reported IMRs often increase for a time. This is mainly because improvement in access to medical care is often accompanied by improvement in the registration of births and deaths. Deaths that might have occurred in a remote or rural area, and not been reported to the government, might now be reported by the new medical personnel or facilities. Thus, even if the new health services reduce the actual IMR, the reported IMR may increase.
Collecting the accurate statistics of infant mortality rate could be an issue in some rural communities in developing countries. In those communities, some other alternative methods for calculating infant mortality rate are emerged, for example, popular death reporting and household survey.
The country-to-country variation in child mortality rates is huge, and growing wider despite the progress. Among the world's roughly 200 nations, only Somalia showed no decrease in the under-5 mortality rate over the past two decades.The lowest rate in 2011 was in Singapore, which had 2.6 deaths of children under age 5 per 1,000 live births. The highest was in Sierra Leone, which had 185 child deaths per 1,000 births. The global rate is 51 deaths per 1,000 births. For the United States, the rate is eight per 1,000 births.
Infant mortality rate (IMR) is not only a group of statistic but instead it is a reflection of the socioeconomic development and effectively represents the presence of medical services in the countries. IMR is an effective resource for the health department to make decision on medical resources reallocation. IMR also formulates the global health strategies and help evaluate the program success. The existence of IMR helps solve the inadequacies of the other vital statistic systems for global health as most of the vital statistic systems usually neglect the infant mortality statistic number from the poor. There are certain amounts of unrecorded infant deaths in the rural area as they do not have information about infant mortality rate statistic or do not have the concept about reporting early infant death.
Europe and America.
The exclusion of any high-risk infants from the denominator or numerator in reported IMRs can be problematic for comparisons. Many countries, including the United States, Sweden and Germany, count an infant exhibiting any sign of life as alive, no matter the month of gestation or the size, but according to United States some other countries differ in these practices. All of the countries named adopted the WHO definitions in the late 1980s or early 1990s, which are used throughout the European Union. However, in 2009, the US CDC issued a report that stated that the American rates of infant mortality were affected by the United States' high rates of premature babies compared to European countries. It also outlined the differences in reporting requirements between the United States and Europe, noting that France, the Czech Republic, Ireland, the Netherlands, and Poland do not report all live births of babies under 500 g and/or 22 weeks of gestation. However, the differences in reporting are unlikely to be the primary explanation for the United States' relatively low international ranking. Rather, the report concluded that primary reason for the United States’ higher infant mortality rate when compared with Europe was the United States’ much higher percentage of preterm births. There are a number of factors which may account for this higher rate of preterm births, which include obesity or poor prenatal care.
The U.S. National Institute of Child Health & Human Development (NICHD) has made great strides in lowering U.S. infant mortality rates. Since the institute was created the U.S. infant mortality rate has dropped 70%, in part due to their research.
Russia.
Until the 1990s, Russia and the Soviet Union did not count, as a live birth or as an infant death, extremely premature infants (less than 1,000 g, less than 28 weeks gestational age, or less than 35 cm in length) that were born alive (breathed, had a heartbeat, or exhibited voluntary muscle movement) but failed to survive for at least seven days. Although such extremely premature infants typically accounted for only about 0.5% of all live-born children, their exclusion from both the numerator and the denominator in the reported IMR led to an estimated 22%–25% lower reported IMR. In some cases, too, perhaps because hospitals or regional health departments were held accountable for lowering the IMR in their catchment area, infant deaths that occurred in the 12th month were "transferred" statistically to the 13th month (i.e., the second year of life), and thus no longer classified as an infant death.
Brazil.
In certain rural developing areas, such as northeastern Brazil, infant births are often not recorded in the first place, resulting in the discrepancies between the infant mortality rate (IMR) and the actual amount of infant deaths. Access to vital registry systems for infant births and deaths is an extremely difficult and expensive task for poor parents living in rural areas. Government and bureaucracies tend to show an insensitivity to these parents and their recent suffering from a lost child, and produce broad disclaimers in the IMR reports that the information has not been properly reported, resulting in these discrepancies. Little has been done to address the underlying structural problems of the vital registry systems in respect to the lack of reporting from parents in rural areas, and in turn has created a gap between the official and popular meanings of child death. It is also argued that the bureaucratic separation of vital death recording from cultural death rituals is to blame for the inaccuracy of the infant mortality rate (IMR). Vital death registries often fail to recognize the cultural implications and importance of infant deaths. It is not to be said that vital registry systems are not an accurate representation of a region's socio-economic situation, but this is only the case if these statistics are valid, which is unfortunately not always the circumstance. "Popular death reporters" is an alternative method for collecting and processing statistics on infant and child mortality. Many regions may benefit from "popular death reporters" who are culturally linked to infants may be able to provide more accurate statistics on the incidence of infant mortality. According to ethnographic data, "popular death reporters" refers to people who had inside knowledge of "anjinhos", including the grave-digger, gatekeeper, midwife, popular healers etc. —— all key participants in mortuary rituals. By combining the methods of household surveys, vital registries, and asking "popular death reporters" this can increase the validity of child mortality rates, but there are many barriers that can reflect the validity of our statistics of infant mortality. One of these barriers are political economic decisions. Numbers are exaggerated when international funds are being doled out; and underestimated during reelection.
The bureaucratic separation of vital death reporting and cultural death rituals stems in part due to structural violence. Individuals living in rural areas of Brazil need to invest large capital for lodging and travel in order to report infant birth to a Brazilian Assistance League office. The negative financial aspects deters registration, as often individuals are of lower income and cannot afford such expenses. Similar to the lack of birth reporting, families in rural Brazil face difficult choices based on already existing structural arrangements when choosing to report infant mortality. Financial constraints such as reliance on food supplementations may also lead to skewed infant mortality data.
In developing countries such as Brazil the deaths of impoverished infants are regularly unrecorded into the countries vital registration system; this causes a skew statistically. Culturally validity and contextual soundness can be used to ground the meaning of mortality from a statistical standpoint. In northeast Brazil they have accomplished this standpoint while conducting an ethnographic study combined with an alternative method to survey infant mortality. These types of techniques can develop quality ethnographic data that will ultimately lead to a better portrayal of the magnitude of infant mortality in the region. Political economic reasons have been seen to skew the infant mortality data in the past when governor Ceara devised his presidency campaign on reducing the infant mortality rate during his term in office. By using this new way of surveying, these instances can be minimized and removed, overall creating accurate and sound data.
Epidemiology.
Global trends.
For the world, and for both less developed countries (LDCs) and more developed countries (MDCs), IMR declined significantly between 1960 and 2001. According to the State of the World's Mothers report by Save the Children, the world IMR declined from 126 in 1960 to 57 in 2001.
However, IMR was, and remains, higher in LDCs. In 2001, the IMR for LDCs (91) was about 10 times as large as it was for MDCs (8). On average, for LDCs, the IMR is 17 times as higher than that of MDCs. Also, while both LDCs and MDCs made significant reductions in infant mortality rates, reductions among less developed countries are, on average, much less than those among the more developed countries.
A difference of almost 100 times separate countries with the highest and lowest reported infant mortality rates. The top and bottom five countries by this measure (taken from The World Factbook's 2012 estimates) are shown below.
Yet one has to keep in mind that according to Guillot, Gerland, Pelletier and Saabneh "birth histories, however, are subject to a number of errors, including omission of deaths and age misreporting errors." Not to say this information is incorrect, but to be aware that in other countries the numbers may not be fully accurate due to those reasons.
In the United States.
The infant mortality rate in the U.S. decreased by 2.3% to a historic low of 582 infant deaths per 100,000 live births in 2014.
In the 1850s, the infant mortality rate in the United States was estimated at 216.8 per 1,000 babies born for whites and 340.0 per 1,000 for African Americans, but rates have significantly declined in the West in modern times. This declining rate has been mainly due to modern improvements in basic health care, technology, and medical advances. In the last three decades, infant mortality overall has also decreased considerably. In the last century, the infant mortality rate has decreased by 93%. Overall, the rates have decreased drastically from 20 deaths in 1970 to 6.9 deaths in 2003 (per every 1000 live births). In 2003, the leading causes of infant mortality in the United States were congenital anomalies, disorders related to immaturity, SIDS, and maternal complications. Babies born with low birth weight increased to 8.1% while cigarette smoking during pregnancy declined to 10.2%. This reflected the amount of low birth weights concluding that 12.4% of births from smokers were low birth weights opposing to 7.7% of such births from non-smokers. According to the "New York Times", "the main reason for the high rate is preterm delivery, and there was a 10% increase in such births from 2000 to 2006." Between 2007 and 2011, however, the preterm birth rate has decreased every year. In 2011 there was a 11.73% rate of babies born before the 37th week of gestation, down from a high of 12.80% in 2006.
Economic expenditures on L&D and neonatal care are relatively high in the United States. A conventional birth averages 9,775 USD with a C-section costing 15,041 USD. Preterm births in the U.S. have been estimated to cost 51,600 USD per child, with a total yearly cost of 26.2 billion USD. Despite this spending, several reports state that infant mortality rate in the United States is significantly higher than in other developed nations. Estimates vary; the CIA's "World Factbook" ranks the U.S. 55th internationally in 2014, with a rate of 6.17, while the UN figures from 2005-2010 place the U.S. 34th.
Aforementioned differences in measurement could play a substantial role in the disparity between the U.S. and other nations. A non-viable live birth in the U.S. could be registered as a stillbirth in similarly developed nations like Japan, Sweden, Norway, Ireland, the Netherlands, and France – thereby avoiding the infant death classification altogether. Neonatal intensive care is also more likely to be applied in the U.S. to marginally viable infants, although such interventions have been found to increase both costs and disability. A study following the implementation of the Born Alive Infant Protection Act of 2002 found universal resuscitation of infants born between 20–23 weeks increased the neonatal spending burden by 313.3 million while simultaneously decreasing quality-adjusted life years by 329.3.
The vast majority of research conducted in the late 20th and early 21st century indicates that African-American infants are more than twice as likely to die in their first year of life than are white infants. Although following a decline from 13.63 to 11.46 deaths per 1000 live births from 2005 to 2010, non-Hispanic black mothers continued to report a rate 2.2 times as high as that for non-Hispanic white mothers. The improved rate of 11.46 still places U.S. non-Hispanic black mothers on par with Russia, which ranks 66th internationally.
Contemporary research findings have demonstrated that nationwide racial disparities in infant mortality are linked to the experiential state of the mother and that these disparities cannot be totally accounted for by socio-economic, behavioral or genetic factors. The Hispanic paradox, an effect observed in other health indicators, appears in the infant mortality rate, as well. Hispanic mothers see an IMR comparable to non-Hispanic white mothers, despite lower educational attainment and economic status. A study in North Carolina, for example, concluded that "white women who did not complete high school have a lower infant mortality rate than black college graduates." According to Mustillo's CARDIA (Coronary Artery Risk Development in Young Adults) study, "self reported experiences of racial discrimination were associated with pre-term and low-birthweight deliveries, and such experiences may contribute to black-white disparities in prenatal outcomes." Likewise, dozens of population-based studies indicate that "the subjective, or perceived experience of racial discrimination is strongly associated with an increased risk of infant death and with poor health prospects for future generations of African Americans."
Society and culture.
Other meanings.
In reliability engineering, "infant mortality" refers to the failures that occur in the first part of the bathtub curve.
See also.
Related statistical categories:

</doc>
<doc id="71621" url="https://en.wikipedia.org/wiki?curid=71621" title="ICOMP (index)">
ICOMP (index)

iCOMP for Intel Comparative Microprocessor Performance was an index published by Intel used to measure the relative performance of its microprocessors.
There were three revisions of the iCOMP index. Version 1.0 was benchmarked against the 486SX 25, while version 2.0 was benchmarked against the Pentium 120.

</doc>
<doc id="71624" url="https://en.wikipedia.org/wiki?curid=71624" title="JPEG File Interchange Format">
JPEG File Interchange Format

The JPEG File Interchange Format (JFIF) is an image file format standard. It is a format for exchanging JPEG encoded files compliant with the JPEG Interchange Format (JIF) standard. It solves some of JIF's limitations in regard to simple JPEG encoded file interchange. As with all JIF compliant files, image data in JFIF files is compressed using the techniques in the JPEG standard, hence JFIF is sometimes referred to as "JPEG/JFIF".
Purpose.
JFIF defines a number of details that are left unspecified by the JPEG Part 1 standard (ISO/IEC IS 10918-1, ITU-T Recommendation T.81):
Component sample registration.
JPEG allows multiple components (such as Y, Cb, and Cr) to have different resolutions, but it does not define how those differing sample arrays should be aligned. The JFIF standard requires samples to be sited "interstitially" — meaning the decoder can treat each component array as representing an array of equal-sized rectangular pixels sampled in their centers, with each array having the same exterior boundaries as the image. This is convenient for computer users, but is not the alignment used in MPEG-2 and most video applications.
Resolution and aspect ratio.
The JPEG standard does not include any method of coding the resolution or aspect ratio of an image. JFIF provides resolution or aspect ratio information using an application segment extension to JPEG. It uses Application Segment #0, with a segment header of 'JFIF\x00', and specifies that this must be the first segment in the file, hence making it simple to recognise a JFIF file. Exif images recorded by digital cameras generally do not include this segment, but typically comply in all other respects with the JFIF standard.
Color space.
JPEG does not define which color encoding is to be used for images. JFIF defines the color model to be used: either Y for greyscale, or YCbCr as defined by CCIR 601. Since this is not an absolute color space — unless an ICC profile, colorspace metadata, or an sRGB tag is provided and interpreted – a decoded JFIF image will be in a device-dependent RGB colorspace. Hence, JFIF does not by itself provide a mechanism for accurately transporting color-managed images across the Internet.
File format structure.
A JFIF file consists of a sequence of markers or marker segments. The markers are defined in part 1 of the JPEG Standard. Each marker consists of two bytes: an codice_1 byte followed by a byte which is not equal to codice_2 or codice_1 and specifies the type of the marker. Some markers stand alone, but most indicate the start of a marker segment that contains data bytes according to the following pattern:
codice_4
The bytes "s1" and "s2" are taken together to represent a big-endian 16-bit integer specifying the length of the following "data bytes" plus the 2 bytes used to represent the length. In other words, "s1" and "s2" specify the number of the following "data bytes" as 256 * "s1" + "s2" - 2.
According to part 1 of the JPEG standard, applications can use APP marker segments and define an application specific meaning of the data. In the JFIF standard, the following APP marker segments are defined:
They are described below.
The JFIF standard requires that the JFIF APP0 marker segment immediately follows the SOI marker. If a JFIF extension APP0 marker segment is used, it must immediately follow the JFIF APP0 marker segment. So a JFIF file will have the following structure:
JFIF APP0 marker segment.
In the mandatory JFIF APP0 marker segment the parameters of the image are specified. Optionally an uncompressed thumbnail can be embedded.
JFIF extension APP0 marker segment.
Immediately following the JFIF APP0 marker segment may be a JFIF extension APP0 marker segment. This segment may only be present for JFIF versions 1.02 and above. It allows to embed a thumbnail image in 3 different formats.
The thumbnail data depends on the thumbnail format as follows:
Compatibility.
The newer Exchangeable image file format (Exif) is comparable to JFIF, but the two standards are mutually incompatible. This is because both standards specify that their particular application segment (APP0 for JFIF, APP1 for Exif) must immediately follow the SOI marker. In practice, many programs and digital cameras produce files with both application segments included. This will not affect the image decoding for most decoders, but poorly designed JFIF or Exif parsers may not recognise the file properly.
JFIF is compatible with Adobe Photoshop's JPEG "Information Resource Block" extensions, and IPTC Information Interchange Model metadata, since JFIF does not preclude other application segments, and the Photoshop extensions are not required to be the first in the file. However, Photoshop generally saves CMYK buffers as four-component "Adobe JPEGs" that are not conformant with JFIF. Since these files are not in a YCbCr color space, they are typically not decodable by Web browsers and other Internet software.
History.
The standard was established on March 1, 1991 in a meeting at C-Cube Microsystems involving representatives of many companies, including C-Cube Microsystems, Radius, NeXT, Storm Tech, the PD JPEG group, Sun, and Handmade Software. The standard appears to have lost ownership, since C-Cube Microsystems is now defunct, and further development of the standard is dead. The latest version is v1.02, published September 1, 1992.
JFIF has been standardized by ISO/IEC 10918-5:2013, ITU-T T.871 and Ecma International TR/98.
In 1996, RFC 2046 specified that the image format used for transmitting JPEG images across the internet should be JFIF. The MIME type of "image/jpeg" must be encoded as JFIF. In practice, however, virtually all Internet software can decode any baseline "JIF" image that uses Y or YCbCr components, whether it is JFIF compliant or not.

</doc>
<doc id="71626" url="https://en.wikipedia.org/wiki?curid=71626" title="Bill Reid">
Bill Reid

William Ronald "Bill" Reid, Jr., OBC ( – ) (Haida) was a Canadian artist whose works include jewelry, sculpture, screen-printing, and paintings. Some of his major works were featured on the Canadian $20 banknote of the Canadian Journey series (2004–2012).
Biography.
Early years.
William Ronald Reid, Jr., called Bill, was born in Victoria, British Columbia; his father was American William Ronald Reid, Sr., of Scottish-German descent and his mother, Sophie Gladstone Reid, was from the Kaadaas gaah Kiiguwaay, Raven/Wolf Clan of T'anuu, more commonly known as the Haida, one of the First Nations of the Pacific coast.
Reid developed a keen interest in Haida art while working as a radio announcer in Toronto for CBC Radio, where he also studied jewelry making at the Ryerson Institute of Technology. His maternal grandfather first taught him about Haida art. He had himself been trained by Charles Edenshaw, a Haida artist of great renown.
In 1951, Reid returned to Vancouver, where he eventually established a studio on Granville Island. He became greatly interested in the works of Edenshaw, working to understand the symbolism of his work, much of which had been lost along with many Haida traditions. During this time Reid also worked on salvaging artifacts, including many intricately carved totem poles, which were then moldering in abandoned village sites. He assisted in the partial reconstruction of a Haida village in the University of British Columbia Museum of Anthropology.
At the age of 29, Reid married a woman named Jane. They had two children together.
Working in the traditional forms and modern media (usually gold, silver and argillite), Reid began by making jewelry. He gradually explored larger sculptures in bronze, red cedar and Nootka Cypress (yellow cedar), usually portraying figures, animals, and scenes from folklore. He intended to express his ancestors' visual traditions into a contemporary form.
Major works and awards.
Reid's most popular works are three large bronze sculptures, two depicting a canoe filled with human and animal figures: one black, "The Spirit of Haida Gwaii", is at the Canadian Embassy, Washington, D.C., in the United States; and one green, "The Jade Canoe", is at Vancouver International Airport, in British Columbia. The third, "Chief of the Undersea World", depicts a breaching orca and is installed at the Vancouver Aquarium. Plaster casts of these sculptures are held by the Canadian Museum of History in Gatineau, Canada.
Legacy and honours.
Reid received many honours in his life, including honorary degrees from the University of British Columbia, the University of Toronto, the University of Victoria, the University of Western Ontario, York University, and Trent University. He received the National Aboriginal Achievement Award for Lifetime Achievement in 1994, and was made a member of the Order of British Columbia and an Officer of France's Order of Arts and Letters. He was made a member of the Royal Canadian Academy of Arts.
On 30 April 1996 Canada Post issued 'The Spirit of Haida Gwaii, 1986-1991, Bill Reid' in the Masterpieces of Canadian art series. The stamp was designed by Pierre-Yves Pelletier based on the sculpture "The Spirit of Haida Gwaii" (1991) by William Ronald Reid in the Canadian Embassy, Washington, United States. The 90¢ stamps are perforated 12.5 x 13 and were printed by Ashton-Potter Limited.
Two of his sculptures, "Raven and the First Men" and "Spirit of Haida Gwaii", are prominently featured on the $20 note in the Bank of Canada's new "Canadian Journey" (2004) issue, paired with a quotation from author Gabrielle Roy.
Later years.
Reid participated in the blockades of logging roads which helped save the rain forests of Gwaii Haanas (South Moresby). He stopped work on the sculpture in Washington during this period to protest the destruction of the forests of Haida Gwaii.
Having dedicated the later part of his life to the creation of new works and these tasks of curation, Reid died on 13 March 1998, of Parkinson's disease, in Vancouver. In July 1998 friends and relatives paddled Lootaas, a large cedar canoe carved by Reid for Expo 86, on a two-day journey along the Pacific coast to bring his ashes to Tanu Island in Haida Gwaii, the site of his mother's village of New Clew.

</doc>
<doc id="71627" url="https://en.wikipedia.org/wiki?curid=71627" title="Integrated geography">
Integrated geography

Integrated geography (also, integrative geography, environmental geography or human–environment geography) is the branch of geography that describes and explains the spatial aspects of interactions between human individuals or societies and their natural environment, called coupled human–environment systems.
Origins.
It requires an understanding of the dynamics of physical geography, as well as the ways in which human societies conceptualize the environment (human geography). Thus, to a certain degree, it may be seen as a successor of "Physische Anthropogeographie" (English: "physical anthropogeography")—a term coined by University of Vienna geographer Albrecht Penck in 1924—and geographical cultural or human ecology (Harlan H. Barrows 1923). Integrated geography in the United States is principally influenced by the schools of Carl O. Sauer (Berkeley), whose perspective was rather historical, and Gilbert F. White (Chicago), who developed a more applied view.
Focus.
The links between human and physical geography were once more readily apparent than they are today. As human experience of the world is increasingly mediated by technology, the relationships have often become obscured. Thereby, integrated geography represents a critically important set of analytical tools for assessing the impact of human presence on the environment by measuring the result of human activity on natural landforms and cycles. It hence is considered the third branch of geography, as compared to physical and human geography 

</doc>
<doc id="71630" url="https://en.wikipedia.org/wiki?curid=71630" title="Unicity distance">
Unicity distance

In cryptography, unicity distance is the length of an original ciphertext needed to break the cipher by reducing the number of possible spurious keys to zero in a brute force attack. That is, after trying every possible key, there should be just one decipherment that makes sense, i.e. expected amount of ciphertext needed to determine the key completely, assuming the underlying message has redundancy.
Consider an attack on the ciphertext string "WNAIW" encrypted using a Vigenère cipher with a five letter key. Conceivably, this string could be deciphered into any other string — RIVER and WATER are both possibilities for certain keys. This is a general rule of cryptanalysis: with no additional information it is impossible to decode this message.
Of course, even in this case, only a certain number of five letter keys will result in English words. Trying all possible keys we will not only get RIVER and WATER, but SXOOS and KHDOP as well. The number of "working" keys will likely be very much smaller than the set of all possible keys. The problem is knowing which of these "working" keys is the right one; the rest are spurious.
Relation with key size and possible plaintexts.
In general, given particular assumptions about the size of the key and the number of possible messages, there is an average ciphertext length where there is only one key (on average) that will generate a readable message. In the example above we see only upper case Roman characters, so if we assume that the plaintext has this form, then there are 26 possible letters for each position in the string. Likewise if we assume five-character upper case keys, there are K=265 possible keys, of which the majority will not "work".
A tremendous number of possible messages, N, can be generated using even this limited set of characters: N = 26L, where L is the length of the message. However only a smaller set of them is readable plaintext due to the rules of the language, perhaps M of them, where M is likely to be very much smaller than N. Moreover M has a one-to-one relationship with the number of keys that work, so given K possible keys, only K × (M/N) of them will "work". One of these is the correct key, the rest are spurious.
Since M/N gets arbitrarily small as the length L of the message increases, there is eventually some L that is large enough to make the number of spurious keys equal to zero. Roughly speaking, this is the L that makes KM/N=1. This L is the unicity distance.
Relation with key entropy and plaintext redundancy.
The unicity distance can equivalently be defined as the minimum amount of ciphertext required to permit a computationally unlimited adversary to recover the unique encryption key.
The expected unicity distance can then be shown to be:
where "U" is the unicity distance, "H"("k") is the entropy of the key space (e.g. 128 for 2128 equiprobable keys, rather less if the key is a memorized pass-phrase). "D" is defined as the plaintext redundancy in bits per character.
Now an alphabet of 32 characters can carry 5 bits of information per character (as 32 = 25). In general the number of bits of information per character is , where "N" is the number of characters in the alphabet and is the binary logarithm. So for English each character can convey bits of information.
However the average amount of actual information carried per character in meaningful English text is only about 1.5 bits per character. So the plain text redundancy is "D" = 4.7 − 1.5 = 3.2.
Basically the bigger the unicity distance the better. For a one time pad of unlimited size, given the unbounded entropy of the key space, we have formula_2, which is consistent with the one-time pad being theoretically unbreakable.
Unicity distance of substitution cipher.
For a simple substitution cipher, the number of possible keys is , the number of ways in which the alphabet can be permuted. Assuming all keys are equally likely, bits. For English text , thus .
So given 28 characters of ciphertext it should be theoretically possible to work out an English plaintext and hence the key.
Practical application.
Unicity distance is a useful theoretical measure, but it doesn't say much about the security of a block cipher when attacked by an adversary with real-world (limited) resources. Consider a block cipher with a unicity distance of three ciphertext blocks. Although there is clearly enough information for a computationally unbounded adversary to find the right key (simple exhaustive search), this may be computationally infeasible in practice.
The unicity distance can be increased by reducing the plaintext redundancy. One way to do this is to deploy data compression techniques prior to encryption, for example by removing redundant vowels while retaining readability. This is a good idea anyway, as it reduces the amount of data to be encrypted.
Another way to increase the unicity distance is to increase the number of possible valid sequences in the files as it is read. Since if for at least the first several blocks any bit pattern can effectively be part of a valid message then the unicity distance has not been reached. This is possible on long files when certain bijective string sorting permutations are used, such as the many variants of bijective Burrows–Wheeler transforms.
Ciphertexts greater than the unicity distance can be assumed to have only one meaningful decryption. Ciphertexts shorter than the unicity distance may have multiple plausible decryptions. Unicity distance is not a measure of how much ciphertext is required for cryptanalysis, but how much ciphertext is required for there to be only one reasonable solution for cryptanalysis.

</doc>
<doc id="71632" url="https://en.wikipedia.org/wiki?curid=71632" title="SIDS">
SIDS

SIDS may refer to:

</doc>
<doc id="71635" url="https://en.wikipedia.org/wiki?curid=71635" title="Cytomegalovirus">
Cytomegalovirus

Cytomegalovirus (from the Greek "cyto-", "cell", and "megalo-", "large") is a genus of viruses in the order Herpesvirales, in the family Herpesviridae, in the subfamily Betaherpesvirinae. Human and monkeys serve as natural hosts. There are currently eight species in this genus including the type species human herpesvirus 5 (HHV-5). Diseases associated with HHV-5 include mononucleosis, and pneumonias. It is typically abbreviated as CMV.
The species that infects humans is commonly known as human CMV (HCMV) or human herpesvirus-5 (HHV-5), and is the most studied of all cytomegaloviruses. Within "Herpesviridae", CMV belongs to the "Betaherpesvirinae" subfamily, which also includes the genera "Muromegalovirus" and "Roseolovirus (HHV-6 and HHV-7)". It is related to other herpesviruses within the subfamilies of "Alphaherpesvirinae" that includes herpes simplex viruses (HSV)-1 and -2 and varicella-zoster virus (VZV), and the "Gammaherpesvirinae" subfamily that includes Epstein–Barr virus.
All herpesviruses share a characteristic ability to remain latent within the body over long periods. Although they may be found throughout the body, CMV infections are frequently associated with the salivary glands in humans and other mammals. Other CMV viruses are found in several mammal species, but species isolated from animals differ from HCMV in terms of genomic structure, and have not been reported to cause human disease.
Taxonomy.
Group: dsDNA
Species.
Several species of "Cytomegalovirus" have been identified and classified for different mammals. The most studied is "Human cytomegalovirus" (HCMV), which is also known as "Human herpesvirus 5" (HHV-5). Other primate CMV species include "Chimpanzee cytomegalovirus" (CCMV) that infects chimpanzees and orangutans, and "Simian cytomegalovirus" (SCCMV) and "Rhesus cytomegalovirus" (RhCMV) that infect macaques; CCMV is known as both "Panine herpesvirus 2" (PaHV-2) and "Pongine herpesvirus-4" (PoHV-4). SCCMV is called "Cercopithecine herpesvirus-5" (CeHV-5) and RhCMV, "Cercopithecine herpesvirus 8" (CeHV-8). A further two viruses found in the night monkey are tentatively placed in the "Cytomegalovirus" genus, and are called "Herpesvirus aotus 1" and "Herpesvirus aotus 3". Rodents also have viruses previously called cytomegaloviruses that are now reclassified under the genus "Muromegalovirus"; this genus contains "Mouse cytomegalovirus" (MCMV) is also known as "Murid herpesvirus 1" (MuHV-1) and the closely related "Murid herpesvirus 2" (MuHV-2) that is found in rats. In addition, there many other viral species with the name "Cytomegalovirus" identified in distinct mammals that are as yet not completely classified; these were predominantly isolated from primates and rodents.
Structure.
Viruses in Cytomegalovirus are enveloped, with icosahedral, Spherical to pleomorphic, and Round geometries, and T=16 symmetry. The diameter is around 150-200 nm. Genomes are linear and non-segmented, around 200kb in length.
Life Cycle.
Viral replication is nuclear, and is lysogenic. Entry into the host cell is achieved by attachment of the viral glycoproteins to host receptors, which mediates endocytosis. Replication follows the dsDNA bidirectional replication model. DNA templated transcription, with some alternative splicing mechanism is the method of transcription. Translation takes place by leaky scanning. The virus exits the host cell by nuclear egress, and budding.
Human and monkeys serve as the natural host. Transmission routes are contact, urine, and saliva.
Genetic engineering.
The CMV promoter is commonly included in vectors used in genetic engineering work conducted in mammalian cells, as it is a strong promoter and drives constitutive expression of genes under its control.

</doc>
<doc id="71643" url="https://en.wikipedia.org/wiki?curid=71643" title="ASGP">
ASGP

ASGP may refer to:

</doc>
<doc id="71644" url="https://en.wikipedia.org/wiki?curid=71644" title="Association of State Green Parties">
Association of State Green Parties

The Association of State Green Parties was an organization of state Green Parties in the United States between 1996 and 2001. In 2001, it evolved into the Green Party of the United States.
Founding.
In the aftermath of the first Green presidential campaign in 1996, 62 Greens from 30 states gathered over the weekend of November 16–17, 1996 to found the Association of State Green Parties (ASGP). The meeting was held at the historic Glen-Ora Farm in Middleburg, Virginia where John Kennedy had his weekend retreat in his administration's early days (rented to the president by the mother of ASGP meeting host and Nader supporter Elaine Broadhead.)
Green Parties from 13 states were the founding members, and approved an initial set of bylaws that set out the ASGP's purpose: (1) Assist in the development of State Green Parties and (2) Create a legally structured national Green Party. The founding meeting also established a national newsletter Green Pages, which carries forward today as the newspaper of the GPUS. The founding editor was Mike Feinstein.
Subsequent ASGP meetings occurred in Portland, OR (April 5–6, 1997), Topsham, ME (October 3–5, 1997),Santa Fe, NM (April 24–26, 1998), Moodus, CT (June 5–6, 1999) and Hiawassee, GA (December 8–10, 2000). Ralph Nader appeared in Moodus to talk about running for president the next year.
The concept of the ASGP came out of the 1991 national Greens Gathering in Elkins, West Virginia, where a committee was tasked with examining what an eventual Green Party might look it. The committee produced a report with contributions from six authors, among them Greg Gerritt from Maine. Gerritt's suggestion was for the creation of an Association of State Green Parties based on sovereign state parties, essentially as he argued, how all parties end up structured in the US. The reaction within the GPUSA was, according to Gerritt, to throw him out of the GPUSA. But that did not kill the concept. Instead it was shared by those involved in the establishment of the Green Politics Network in 1992, and what was founded in 1996 in Middleburg very closely reflected the proposal Gerritt originally submitted.
ASGP and the Greens/Green Party USA.
The ASGP was predominantly focused on establishing state Green Parties and running and electing Greens for public office, even while its member state parties and the individual Greens involved remained involved in issue activism. From 1997 to 1999, as more state Green Parties continued to form, a highly competitive environment between the ASGP and the Greens/Green Party USA (G/GPUSA) began to develop in terms of who would affiliate with which organization, and ultimately who would become the definitive national Green Party. To some extent this divide reflected philosophical differences within U.S. Greens that went back to the late 1980s and early 1990s, around how a Green political party should be organized - based upon state parties, or upon dues-paying local Green groups.
In December 1999, Santa Monica, CA Green City Councilmember Mike Feinstein and New York Green Howie Hawkins met in New Platz, New York during the state meeting of the Green Party of New York State and crafted the 'Plan for a Single National Green Party, which was more generally known as the Feinstein/Hawkins Proposal, meant to create a single national Green Party from among the ASGP and GPUSA by Earth Day, April 2000. The proposal found quick support within the ASGP, but not within the Greens/GPUSA in time for Earth Day.
Boston Proposal.
Instead it would be in October 2000 that the Feinstein/Hawkins proposal was revisited, negotiated further and renamed the 'Boston Proposal' or 'Boston Agreement' (so named because it was negotiated in Boston on October 1–2 in the days before the first 2000 presidential debate).
The negotiators for the Association of State Green Parties were Tony Affigne, David Cobb, Robert Franklin, Greg Gerritt, Anne Goeke, Stephen Herrick, and Tom Sevigny; and for the Greens/Green Party USA: Starlene Rankin, John Stith, Jeff Sutter, Steve Welzer, Rich Whitney, and Julia Willebrand.
The Boston Proposal was approved was by the ASGP at its December 2000 meeting in Hiawasee, GA, but did not pass at the July 2001 G/GPUSA Congress in Carbondale, CO. This caused a schism in G/GPUSA membership from which it never recovered. The next week on July 29 in Santa Barbara, CA the ASGP voted to become the Green Party of the United States, held a press conference in Santa Monica on July 30 to announce it, and was subsequently recognized as having National Committee status by the Federal Election Commission.
Text of the Boston Proposal: JOINT PROPOSAL OF NEGOTIATING COMMITTEES OF THE ASSOCIATION OF STATE GREEN PARTIES and THE GREENS/GREEN PARTY USA
The Negotiating Committees of the Association of State Green Parties and the Greens/Green Party USA, meeting on October 1–2, 2000, have agreed unanimously to recommend to their respective organizations the following proposal for the creation of a new national Green Party, to be called "Green Party of the United States," and a National Committee of the Green Party, which will apply to the Federal Election Commission for official recognition as a national political party.
1. The Greens/Green Party USA may continue as an independent organization but will cease functioning as a political party and will adopt a new name that will omit the word, "party." It will relinquish the domain name, "www.greenparty.org" to the new party.
2. The Green Party shall leave the question of state dues entirely to the state parties and shall neither encourage state parties to collect dues nor discourage state parties from collecting dues.
3. The new Green Party shall have a paid "sustaining members" category of membership and shall encourage members of all state parties to become sustaining members of the national organization.
4. State parties in the new Green Party shall have written democratic by-laws and at least one convention a year (whether it be a general membership assembly or a delegate assembly elected by locals and members-at-large).
5. State Green Parties shall make a good faith effort, where reasonable, to have delegates to the National Committee elected by clusters of local groups. The basis of representation to the National Committee shall be one person, one vote, and in the United States, Congressional Districts are a reasonable approximation of equal population areas. Local groups within a Congressional District or Districts shall come together for the purpose of electing delegates. (Statement of intent: The exact number of delegates and the size of the areas to be used for selecting these delegates is currently under study and will be determined by the ASGP.)
6. Representation to the National Committee shall be based on the principle of one delegate per specified number of Congressional Districts, provided that there is a requisite level of Green Party activity, electoral and/or non-electoral, being conducted in those Congressional Districts.
7. The Accreditation Criteria of the Green Party National Committee shall include the following in addition to the criteria currently used by the ASGP:
a. Evidence of commitment to, and good faith efforts to achieve, gender balance in party leadership and representation.
b. Evidence of good faith efforts to empower individuals and groups from oppressed communities, through, for example, leadership responsibilities, identity caucuses and alliances with community-based organizations, and endorsements of issues and policies.
8. The Green Party National Committee shall reserve seats for representatives of nationally organized caucuses for traditionally disempowered groups, provided that each caucus shall demonstrate a total membership of at least 100 people, from at least 15 member states, use democratic procedures and choose to send representatives to the National Committee.
9. The G/GPUSA Negotiating Committee recommends to the affiliates of G/GPUSA that a mail referendum of the Green Congress be initiated. The referendum shall ask the Congress to authorize the G/GPUSA National Committee to certify whether the ASGP, at its December 2000 meeting, amends its bylaws to reflect the terms of this joint proposal. The mail referendum will also authorize the G/GPUSA National Committee to (a) adopt a new name that will omit the word "party"; (b) relinquish the domain name "www.greenparty.org" to the new party; and (c) support the request of the ASGP for an advisory opinion on the national committee status of the new party, provided that it does in fact certify that the ASGP has amended its bylaws in accordance with this proposal.
ASGP and Nader 2000 presidential campaign.
In September 1998, the New Mexico Green Party proposed that an ASGP Presidential Exploratory Committee be established for the 2000 elections. The ASGP Coordinating Committee passed the proposal on October 30, and on December 20 the ASGP Steering Committee appointed a seven-person committee, chaired by then Texas Green David Cobb. On February 22, 1999 the Committee sent this letter and questionnaire to prospective presidential and vice presidential candidates, asking if they were interested in running on the Green Party ticket in 2000 and if so, how would they envision conducing the campaign: Wendell Berry, Jerry Brown, Lester Brown, Noam Chomsky, Ron Daniels, Ron Dellums, Lani Guinier, Dan Hamburg, Woody Harrelson, Paul Hawken, Jim Hightower, Molly Ivins, Winona LaDuke, Bill McKibben, Cynthia McKinney, Carol Miller, Toni Morrison, Ralph Nader, Ron Ouellette (requested the questionnaire), John Robbins and Jan Schlichtmann. On May 10 the committee also sent the letter and questionnaire to: Harry Belafonte, Julian Bond, Joceyln Elders, Kurt Schmoke, Studs Terkel, Myrlie Evers-Williams and General Lee Butler.
Brown, McKibben, Chomsky, Guinier, Hawken, Miller wrote back declining, but all graciously thanking the ASGP for its outreach, and offering sympathetic statements of support for the Green Party project.
Nader also replied: "If I seek the nomination - a decision that will not be made until next year- and receive that designation, I will pursue a dedicated and thorough campaign that meets the Federal Election Commission requirements. Such an active campaign will have the objective of strengthening our nation’s democracy by strengthening the Green Party movement at the local, state and national levels; by emphasizing the problems of, and remedies for, the excessive concentration of corporate power and wealth in our country, by highlighting the important tools of democracy needed for the American people as voters/citizens, workers, consumers, taxpayers, and small savers/investors. If there are Greens who support my seeking the nomination, I encourage them to expand the number of volunteers and increase the time spent working to build the Green Party this year in order to advance the Party’s “Key Values” and to increase the likelihood of ballot access in all fifty states."
International.
In April 2001, the ASGP was represented at the founding congress of the Global Greens by Mike Feinstein, Annie Goeke and John Rensenbrink. The ASGP was also a founding member party of the Federacíon de Partidos Verdes de las Americas in 1998.

</doc>
<doc id="71649" url="https://en.wikipedia.org/wiki?curid=71649" title="Challenge-Handshake Authentication Protocol">
Challenge-Handshake Authentication Protocol

In computing, the Challenge-Handshake Authentication Protocol (CHAP) authenticates a user or network host to an authenticating entity. That entity may be, for example, an Internet service provider. CHAP is specified in RFC 1994.
CHAP provides protection against replay attacks by the peer through the use of an incrementally changing identifier and of a variable challenge-value. CHAP requires that both the client and server know the plaintext of the secret, although it is never sent over the network. Thus, CHAP provides better security as compared to Password Authentication Protocol (PAP) which is vulnerable for both these reasons. The MS-CHAP variant does not require either peer to know the plaintext and does not transmit it, but has been broken.
Working cycle.
CHAP is an authentication scheme used by Point to Point Protocol (PPP) servers to validate the identity of remote clients. CHAP periodically verifies the identity of the client by using a three-way handshake. This happens at the time of establishing the initial link (LCP), and may happen again at any time afterwards. The verification is based on a shared secret (such as the client user's password).
CHAP packets.
The ID chosen for the random challenge is also used in the corresponding response, success, and failure packets. A new challenge with a new ID must be different from the last challenge with another ID. If the success or failure is lost, the same response can be sent again, and it triggers the same success or failure indication. For MD5 as hash the response value is codice_1, the MD5 for the concatenation of ID, secret, and challenge.

</doc>
<doc id="71653" url="https://en.wikipedia.org/wiki?curid=71653" title="Assyrian language">
Assyrian language

Assyrian language may refer to:

</doc>
<doc id="71663" url="https://en.wikipedia.org/wiki?curid=71663" title="Password Authentication Protocol">
Password Authentication Protocol

A password authentication protocol (PAP) is an authentication protocol that uses a password.
PAP is used by Point to Point Protocol to validate users before allowing them access to server resources. Almost all network operating system remote servers support PAP.
PAP transmits unencrypted ASCII passwords over the network and is therefore considered insecure. It is used as a last resort when the remote server does not support a stronger authentication protocol, like CHAP or EAP (the latter is actually a framework).
Password-based authentication is the protocol where two entities share a password in advance and use the password as the basis of authentication. Existing password authentication schemes can be categorized into two types: weak-password authentication schemes and strong-password authentication schemes. When compared to strong-password schemes, weak-password schemes tend to have lighter computational overhead, the designs are simpler, and implementation is easier, making them especially suitable for some constrained environments.
PAP Packets.
PAP packet embedded in a PPP frame. The protocol field has a value of
C023 (hex).

</doc>
<doc id="71669" url="https://en.wikipedia.org/wiki?curid=71669" title="PPP">
PPP

PPP or ppp may refer to:

</doc>
<doc id="71691" url="https://en.wikipedia.org/wiki?curid=71691" title="Dinner">
Dinner

Dinner usually refers to the most significant and important meal of the day, which can be the noon or the evening meal. However, the term "dinner" can have many different meanings depending on the culture; it may mean a meal of any size eaten at any time of day. Historically, it referred to the first meal of the day, eaten around noon, and is still sometimes used for a noontime meal, particularly if it is a large or main meal. The meaning as the evening meal, generally the largest of the day, is becoming standard in many parts of the English-speaking world. 
Etymology.
The word is from the Old French () "disner", meaning "dine", from the stem of Gallo-Romance "desjunare" ("to break one's fast"), from Latin "dis-" (which indicates the opposite of an action) + Late Latin "ieiunare" ("to fast"), from Latin "ieiunus" ("fasting, hungry"). The Romanian word "dejun" and the French "déjeuner" retain this etymology and to some extent the meaning (whereas the Spanish word "desayuno" and Portuguese "desjejum" are related but are exclusively used for breakfast). Eventually, the term shifted to referring to the heavy main meal of the day, even if it had been preceded by a breakfast meal (or even both breakfast and lunch).
History.
In Europe, the fashionable hour for dinner began to be incrementally postponed during the 18th century, to two and three in the afternoon, until at the time of the First French Empire an English traveler to Paris remarked upon the "abominable habit of dining as late as seven in the evening".
Time of day.
In many modern usages, the term "dinner" refers to the evening meal, which is now often the most significant meal of the day in English-speaking cultures. When this meaning is used, the preceding meals are usually referred to as breakfast, lunch and tea. In some areas, the tradition of using "dinner" to mean the most important meal of the day regardless of time of day leads to a variable name for meals depending on the combination of their size and the time of day, while in others meal names are fixed based on the time they are consumed.
The divide between different meanings of "dinner" is not cut-and-dried based on either geography or socioeconomic class. However, the use of the term dinner for the midday meal is strongest among working-class people, especially in the English Midlands, North of England and the central belt of Scotland. Even in systems in which dinner is the meal usually eaten at the end of the day, an individual dinner may still refer to a main or more sophisticated meal at any time in the day, such as a banquet, feast, or a special meal eaten on a Sunday or holiday, such as Christmas dinner or Thanksgiving dinner. At such a dinner the people who dine together may be formally dressed and consume food with an array of utensils. These dinners are often divided into three or more courses. Appetizers consisting of options such as soup, salad etc., precede the main course, which is followed by the dessert.
A survey by Jacob's Creek, an Australian winemaker, found the average evening meal time in the U.K. to be 7:47pm.
Dinner parties.
A dinner party is a social gathering at which people congregate to eat dinner.
Ancient Rome.
During the times of Ancient Rome, a dinner party was referred to as a "convivia", and was a significant event for Roman emperors and senators to congregate and discuss their relations. The Romans often ate and were also very fond of fish sauce called liquamen (also known as Garum) during said parties.
England.
In greater London, England (–), dinner parties were sometimes formal occasions that included printed invitations and formal RSVPs. The food served at these parties ranged from large, extravagant food displays and several meal courses to more simple fare and food service. Activities sometimes included singing and poetry reciting, among others.

</doc>
<doc id="71712" url="https://en.wikipedia.org/wiki?curid=71712" title="First Intifada">
First Intifada

The First Intifada or First Palestinian Intifada (also known as simply as "the intifada" or "intifadah") was a Palestinian uprising against the Israeli occupation of the West Bank and Gaza, which lasted from December 1987 until the Madrid Conference in 1991, though some date its conclusion to 1993, with the signing of the Oslo Accords. The uprising began on 9 December, in the Jabalia refugee camp after an Israeli Defense Forces' (IDF) truck collided with a civilian car, killing four Palestinians. In the wake of the incident, a protest movement arose, involving a two-fold strategy of resistance and civil disobedience, consisting of general strikes, boycotts of Israeli Civil Administration institutions in the Gaza Strip and the West Bank, an economic boycott consisting of refusal to work in Israeli settlements on Israeli products, refusal to pay taxes, refusal to drive Palestinian cars with Israeli licenses, graffiti, barricading, and widespread throwing of stones and Molotov cocktails at the IDF and its infrastructure within the Palestinian territories. Israel, deploying some 80,000 soldiers and initially firing live rounds, killed a large number of Palestinians. In the first 13 months, 332 Palestinians and 12 Israelis were killed. Given the high proportion of children, youths and civilians killed, it then adopted a policy of 'might, power, and beatings,' namely "breaking Palestinians' bones". The global diffusion of images of soldiers beating adolescents with clubs then led to the adoption of firing semi-lethal plastic bullets. In the intifada's first year, Israeli security forces killed 311 Palestinians, of which 53 were under the age of 17. Over the first two years, according to Save the Children, an estimated 7% of all Palestinians under 18 years of age suffered injuries from shootings, beatings, or tear gas. Over six years the IDF killed an estimated 1,162–1,204 Palestinians. Between 23,600-29,900 Palestinian children required medical treatment from IDF beatings in the first 2 years. 100 Israeli civilians and 60 IDF personnel were killed often by militants outside the control of the Intifada’s UNLU, and more than 1,400 Israeli civilians and 1,700 soldiers were injured. Intra-Palestinian violence was also a prominent feature of the Intifada, with widespread executions of an estimated 822 Palestinians killed as alleged Israeli collaborators,(1988–April 1994). At the time Israel reportedly obtained information from some 18,000 Palestinians who had been compromised, although fewer than half had any proven contact with the Israeli authorities.
The ensuing Second Intifada took place from September 2000 to 2005.
General causes.
According to Mubarak Awad, a Palestinian American clinical psychologist, the Intifada was a protest against Israeli repression including "beatings, shootings, killings, house demolitions, uprooting of trees, deportations, extended imprisonments, and detentions without trial". 
After Israel's capture of the West Bank, Jerusalem, Sinai Peninsula and Gaza Strip from Jordan and Egypt in the Six-Day War in 1967, frustration grew among Palestinians in the Israeli-occupied territories. Israel opened its labor market to Palestinians in the newly occupied territories. Palestinians were recruited mainly to do unskilled or semi-skilled labor jobs Israelis did not want. By the time of the Intifada, over 40 percent of the Palestinian work force worked in Israel daily. Additionally, Israeli confiscation of Palestinian land, high birth rates in the Palestinian territories and the limited allocation of land for new building and agriculture created conditions marked by growing population density and rising unemployment, even for those with university degrees. At the time of the Intifada, only one in eight college-educated Palestinians could find degree-related work. Couple this with an expansion of a Palestinian university system catering to people from refugee camps, villages, and small towns generating new Palestinian elite from a lower social strata that was more activist and confrontational with Israel.
The Israeli Labor Party's Yitzhak Rabin, the then Defense Minister, added deportations in August 1985 to Israel's "Iron Fist" policy of cracking down on Palestinian nationalism. This, which led to 50 deportations in the following 4 years, was accompanied by economic integration and increasing Israeli settlements such that the Jewish settler population in the West Bank alone nearly doubled from 35,000 in 1984 to 64,000 in 1988, reaching 130,000 by the mid nineties. Referring to the developments, Israeli minister of Economics and Finance, Gad Ya'acobi, stated that "a creeping process of "de facto" annexation" contributed to a growing militancy in Palestinian society.
During the 1980s a number of mainstream Israeli politicians referred to policies of transferring the Palestinian population out of the territories leading to Palestinian fears that Israel planned to evict them. Public statements calling for transfer of the Palestinian population were made by Deputy Defense minister Michael Dekel, Cabinet Minister Mordechai Tzipori and government Minister Yosef Shapira among others. Describing the causes of the Intifada, Benny Morris refers to the "all-pervading element of humiliation", caused by the protracted occupation which he says was "always a brutal and mortifying experience for the occupied" and was "founded on brute force, repression and fear, collaboration and treachery, beatings and torture chambers, and daily intimidation, humiliation, and manipulation"
Background.
While the immediate cause for the First Intifada is generally dated to a truck incident involving several Palestinian fatalities at the Erez Crossing in December 1987, Mazin Qumsiyeh argues, against Donald Neff, that it began with multiple youth demonstrations earlier in the preceding month. Some sources consider that the perceived IDF failure in late November 1987 to stop a Palestinian guerrilla operation, the Night of the Gliders, in which six Israeli soldiers were killed, helped catalyze local Palestinians to rebel.
Mass demonstrations had occurred a year earlier when, after two Gaza students at Birzeit University had been shot by Israeli soldiers on campus on 4 December 1986, the Israelis responded with harsh punitive measures, involving summary arrest, detention and systematic beatings of handcuffed Palestinian youths, ex-prisoners and activists, some 250 of whom were detained in four cells inside a converted army camp, known popularly as Ansar 11, outside Gaza city. A policy of deportation was introduced to intimidate activists in January 1987. Violence simmered as a schoolboy from Khan Yunis was shot dead by Israelis soldiers pursuing him in jeep. Over the summer the IDF's Lieutenant Ron Tal, who was responsible for guarding detainees at Ansar 11, was shot dead at point-blank range while stuck in a Gaza traffic jam. A curfew forbidding Gaza residents from leaving their homes was imposed for three days, during the Muslim feast of Eid al-Adha. In two incidents on 1 and 6 October 1987, respectively, the IDF ambushed and killed seven Gaza men, reportedly affiliated with Islamic Jihad, who had escaped from prison in May. Some days later, a 17-year-old schoolgirl, Intisar al-'Attar, was shot in the back while in her schoolyard in Deir al-Balah by a settler in the Gaza Strip. The Arab summit in Amman in November 1987 focused on the Iran–Iraq War, and the Palestinian issue was shunted to the sidelines for the first time in years.
Leadership and aims.
The Intifada was not initiated by any single individual or organization. Local leadership came from groups and organizations affiliated with the PLO that operated within the Occupied Territories; Fatah, the Popular Front, the Democratic Front and the Palestine Communist Party. The PLO's rivals in this activity were the Islamic organizations, Hamas and Islamic Jihad as well as local leadership in cities such as Beit Sahour and Bethlehem. However, the uprising was predominantly led by community councils led by Hanan Ashrawi, Faisal Husseini and Haidar Abdel-Shafi, that promoted independent networks for education (underground schools as the regular schools were closed by the military in reprisal for the uprising), medical care, and food aid. The Unified National Leadership of the Uprising (UNLU) gained credibility where the Palestinian society complied with the issued communiques. There was a collective commitment to abstain from lethal violence, a notable departure from past practice, which, according to Shalev arose from a calculation that recourse to arms would lead to an Israeli bloodbath and undermine the support they had in Israeli liberal quarters. The PLO and its chairman Yassir Arafat had also decided on an unarmed strategy, in the expectation that negotiations at that time would lead to an agreement with Israel. Pearlman attributes the non-violent character of the uprising to the movement's internal organization and its capillary outreach to neighborhood committees that ensured that lethal revenge would not be the response even in the face of Israeli state repression. Hamas and Islamic Jihad cooperated with the leadership at the outset, and throughout the first year of the uprising conducted no armed attacks, except for the stabbing of a soldier in October 1988, and the detonation of two roadside bombs, which had no impact.
Leaflets publicizing the uprising's aims demanded the complete withdrawal of Israel from the territories it had occupied in 1967: the lifting of curfews and checkpoints; it appealed to Palestinians to join in civic resistance, while asking them not to employ arms, since military resistance would only invited devastating retaliation from Israel; it also called for the establishment of the Palestinian state on the West Bank and the Gaza Strip, abandoning the standard rhetorical calls, still current at the time, for the "liberation" of all of Palestine.
The Intifada.
Israel's drive into the occupied territories had occasioned spontaneous acts of resistance, but the administration, pursuing an "iron fist" policy of deportations, demolition of homes, collective punishment, curfews and the suppression of political institutions, was confident that Palestinian resistance was exhausted. The assessment that the unrest would collapse proved to be mistaken.
On 8 December 1987, an Israeli army tank transporter crashed into a row of cars containing Palestinians returning from working in Israel, at the Erez checkpoint. Four Palestinians, three of them residents of the Jabalya refugee camp, the largest of the eight refugee camps in the Gaza Strip, were killed and seven others seriously injured. The traffic incident was witnessed by hundreds of Palestinian labourers returning home from work. The funerals, attended by 10,000 people from the camp that evening, quickly led to a large demonstration. Rumours swept the camp that the incident was an act of intentional retaliation for the stabbing to death of an Israeli businessman, killed while shopping in Gaza two days earlier. Following the throwing of a petrol bomb at a passing patrol car in the Gaza Strip on the following day, Israeli forces, firing with live ammunition and tear gas canisters into angry crowds, shot one young Palestinian dead and wounded 16 others.
On 9 December, several popular and professional Palestinian leaders held a press conference in West Jerusalem with the Israeli League for Human and Civil Rights in response to the deterioration of the situation. While they convened, reports came in that demonstrations at the Jabalya camp were underway and that a 17-year-old youth had been shot to death after throwing a petrol bomb at Israeli soldiers. She would later become known as the first martyr of the intifada. Protests rapidly spread into the West Bank and East Jerusalem. Youths took control of neighbourhoods, closed off camps with barricades of garbage, stone and burning tires, meeting soldiers who endeavoured to break through with petrol bombs. Palestinian shopkeepers closed their businesses, and labourers refused to turn up to their work in Israel. Israel defined these activities as 'riots', and justified the repression as necessary to restore 'law and order'. Within days the occupied territories were engulfed in a wave of demonstrations and commercial strikes on an unprecedented scale. Specific elements of the occupation were targeted for attack: military vehicles, Israeli buses and Israeli banks. None of the dozen Israeli settlements were attacked and there were no Israeli fatalities from stone-throwing at cars at this early period of the outbreak. Equally unprecedented was the extent of mass participation in these disturbances: tens of thousands of civilians, including women and children. The Israeli security forces used the full panoply of crowd control measures to try and quell the disturbances: cudgels, nightsticks, tear gas, water cannons, rubber bullets, and live ammunition. But the disturbances only gathered momentum.
Soon there was widespread rock-throwing, road-blocking and tire burning throughout the territories. By 12 December, six Palestinians had died and 30 had been injured in the violence. The next day, rioters threw a gasoline bomb at the U.S. consulate in East Jerusalem though no one was hurt.
The Israeli response to the Palestinian uprising was harsh. The IDF killed many Palestinians at the beginning of the Intifada, the majority killed during demonstrations and riots. Since initially a high proportion of those killed were civilians and youths, Yitzhak Rabin adopted a fallback policy of 'might, power and beatings'. Israel used mass arrests of Palestinians, engaged in collective punishments like closing down West Bank universities for most years of the uprising, and West Bank schools for a total of 12 months. Round-the-clock curfews were imposed over 1600 times in just the first year. Communities were cut off from supplies of water, electricity and fuel. At any one time, 25,000 Palestinians would be confined to their homes. Trees were uprooted on Palestinians farms, and agricultural produce blocked from being sold. In the first year over 1,000 Palestinians had their homes either demolished or blocked up. Settlers also engaged in private attacks on Palestinians. Palestinian refusals to pay taxes were met with confiscations of property and licenses, new car taxes, and heavy fines for any family whose members had been identified as stone-throwers.
Casualties.
In the first year in the Gaza Strip alone, 142 Palestinians were killed, while no Israelis died. 77 were shot dead, and 37 died from tear-gas inhalation. 17 died from beatings at the hand of Israeli police or soldiers.
During the whole six-year intifada, the Israeli army killed from 1,162-1,204 (or 1,284) Palestinians,241/332 being children. From 57,000 to 120,000 were arrested. 481 were deported while 2,532 had their houses razed to the ground. Between December 1987 and June 1991, 120,000 were injured, 15,000 arrested and 1,882 homes demolished. One journalistic calculation reports that in the Gaza Strip alone from 1988 to 1993, some 60,706 Palestinians suffered injuries from shootings, beatings or tear gas. In the first five weeks alone, 35 Palestinians were killed and some 1,200 wounded, a casualty rate that only energized the uprising by drawing more Palestinians into participating. B'Tselem calculated 179 Israelis killed, while official Israeli statistics place the total at 200 over the same period. 3,100 Israelis, 1,700 of them soldiers, and 1,400 civilians suffered injuries.
By 1990 Ktzi'ot Prison in the Negev held approximately one out of every 50 West Bank and Gazan males older than 16 years. Gerald Kaufman remarked: "of Israel as well as foes have been shocked and saddened by that country's response to the disturbances." In an article in the London Review of Books, John Mearsheimer and Stephen Walt asserted that IDF soldiers were given truncheons and encouraged to break the bones of Palestinian protesters. The Swedish branch of Save the Children estimated that "23,600 to 29,900 children required medical treatment for their beating injuries in the first two years of the Intifada", one third of whom were children under the age of ten years.
Israel adopted a policy of arresting key representatives of Palestinian institutions. After lawyers in Gaza went on strike to protest their inability to visit their detained clients, Israel detained the deputy head of its association without trial for six months. Dr. Zakariya al-Agha, the head of the Gaza Medical Association, was likewise arrested and held for a similar period of detention, as were several women active in Women's Work Committees. During Ramadan, many camps in Gaza were placed under curfew for weeks, impeding residents from buying food, and Al-Shati, Jabalya and Burayj were subjected to saturation bombing by tear gas. During the first year of the Intifada, the total number of casualties in the camps from such bombing totalled 16.
Intra-communal violence.
Between 1988 and 1992, intra-Palestinian violence claimed the lives of nearly 1,000. By June 1990, according to Benny Morris, "Intifada seemed to have lost direction. A symptom of the PLO's frustration was the great increase in the killing of suspected collaborators." Roughly 18,000 Palestinians, compromised by Israeli intelligence, are said to have give information to the other side. Collaborators were threatened with death or ostracism unless they desisted, and if their collaboration with the Occupying Power continued, were executed by special troops such as the "Black Panthers" and "Red Eagles". An estimated 771 (according to Associated Press) to 942 (according to the IDF) Palestinians were executed on suspicion of collaboration during the span of the Intifada.
Other notable events.
On 16 April 1988, a leader of the PLO, Khalil al-Wazir, "nom de guerre" Abu Jihad or 'Father of the Struggle', was assassinated in Tunis by an Israeli commando squad. Israel claimed he was the 'remote-control "main organizer" of the revolt', and perhaps believed that his death would break the back of the intifada. During the mass demonstrations and mourning in Gaza that followed, two of the main mosques of Gaza were raided by the IDF and worshippers were beaten and tear-gassed. In total between 11 and 15 Palestinians were killed during the demonstrations and riots in Gaza and West Bank that followed al-Wazir's death. In June of that year, the Arab League agreed to support the intifada financially at the 1988 Arab League summit. The Arab League reaffirmed its financial support in the 1989 summit.
Israeli defense minister Yitzhak Rabin's response was: "We will teach them there is a price for refusing the laws of Israel." When time in prison did not stop the activists, Israel crushed the boycott by imposing heavy fines and seizing and disposing of equipment, furnishings, and goods from local stores, factories and homes.
On 8 October 1990, 22 Palestinians were killed by Israeli police during the Temple Mount riots. This led the Palestinians to adopt more lethal tactics, with three Israeli civilians and one IDF soldier stabbed in Jerusalem and Gaza two weeks later. Incidents of stabbing persisted.
The Israeli state apparatus carried out contradictory and conflicting policies that were seen to have injured Israel's own interests, such as the closing of educational establishments (putting more youths onto the streets) and issuing the Shin Bet list of collaborators. Suicide bombings by Palestinian militants started on 16 April 1993 with the Mehola Junction bombing, carried at the end of the Intifada.
United Nations.
The large number of Palestinian casualties provoked international condemnation. In subsequent resolutions, including 607 and 608, the Security Council demanded Israel cease deportations of Palestinians. In November 1988, Israel was condemned by a large majority of the UN General Assembly for its actions against the intifada. The resolution was repeated in the following years.
Failing Security Council.
On 17 February 1989, the UN Security Council unanimously but for US condemned Israel for disregarding Security Council resolutions, as well as for not complying with the fourth Geneva Convention. The United States, put a veto on a draft resolution which would have strongly deplored it. On 9 June, the US again put a veto on a resolution. On 7 November, the US vetoed a third draft resolution, condemning alleged Israeli violations of human rights
On 14 October 1990, Israel openly declared that it would not abide Security Council Resolution 672 and refused to receive a delegation of the Secretary-General, which would investigate Israeli violence. The following Resolution 673 made little impression and Israel kept on obstructing UN investigations.
Outcomes.
The Intifada was recognized as an occasion where the Palestinians acted cohesively and independently of their leadership or assistance of neighbouring Arab states.
The Intifada broke the image of Jerusalem as a united Israeli city. There was unprecedented international coverage, and the Israeli response was criticized in media outlets and international fora.
The success of the Intifada gave Arafat and his followers the confidence they needed to moderate their political programme: At the meeting of the Palestine National Council in Algiers in mid-November 1988, Arafat won a majority for the historic decision to recognise Israel's legitimacy; to accept all the relevant UN resolutions going back to 29 November 1947; and to adopt the principle of a two-state solution.
Jordan severed its residual administrative and financial ties to the West Bank in the face of sweeping popular support for the PLO. The failure of the "Iron Fist" policy, Israel's deteriorating international image, Jordan cutting legal and administrative ties to the West Bank, and the U.S.'s recognition of the PLO as the representative of the Palestinian people forced Rabin to seek an end to the violence though negotiation and dialogue with the PLO.
In the diplomatic sphere, the PLO opposed the Gulf War in Iraq. Afterwards, the PLO was isolated diplomatically, with Kuwait and Saudi Arabia cutting off financial support, and 300,000-400,000 Palestinians fled or were expelled from Kuwait before and after the war. The diplomatic process led to the Madrid Conference and the Oslo Accords.
The impact on the Israeli services sector, including the important Israeli tourist industry, was notably negative.

</doc>
<doc id="71717" url="https://en.wikipedia.org/wiki?curid=71717" title="Second Intifada">
Second Intifada

The Second Intifada, also known as the Al-Aqsa Intifada ( ""; "Intifādat El-Aqtzah"), was the second Palestinian uprising against Israel – a period of intensified Israeli-Palestinian violence. It started in September 2000, when Ariel Sharon made a visit to the Temple Mount, seen by Palestinians as highly provocative; and Palestinian demonstrators, throwing stones at police, were dispersed by the Israeli army, using tear gas and rubber bullets.
Both parties caused high numbers of casualties among civilians as well as combatants: the Palestinians by numerous suicide bombings and gunfire; the Israelis by tank and gunfire and air attacks, by numerous targeted killings, and by harsh reactions to demonstrations. The death toll, including both military and civilian, is estimated to be about 3,000 Palestinians and 1,000 Israelis, as well as 64 foreigners.
Some consider the Sharm el-Sheikh Summit on February 8, 2005 to be the end of the "Second Intifada", when President Mahmoud Abbas and Prime Minister Ariel Sharon agreed that all Palestinians would stop all acts of violence against all Israelis everywhere and, in parallel, that Israel would cease all its military activity against all Palestinians everywhere. They reaffirmed their commitment to the Roadmap for Peace. However, the violence did not stop in the following years.
Etymology.
Second Intifada refers to a second Palestinian uprising, following the first Palestinian uprising, which occurred between December 1987 and 1993. "Intifada" () translates into English as "uprising." It is an Arabic word meaning "the shaking off." The term refers to a revolt against the Israeli occupation of the Palestinian Territories.
Al-Aqsa Intifada refers to the Al-Aqsa Mosque, the place where the intifada started. It is the name of a mosque, constructed in the 8th century CE at Al-Haram Al-Sharif, also known as the Temple Mount in the Old City of Jerusalem, a location considered the holiest site in Judaism and third holiest in Islam.
The Intifada is sometimes called the Oslo War (מלחמת אוסלו) by some Israelis who consider it to be the result of concessions made by Israel following the Oslo Accords, and Arafat's War, after the late Palestinian leader whom some blamed for starting it. Others have named what they consider disproportionate response to what was initially a popular uprising by unarmed demonstrators as the reason for the escalation of the Intifada into an all out war.
Background.
Oslo Accords.
Under the Oslo Accords, signed in 1995, Israel committed to the phased withdrawal of its forces from parts of the Gaza Strip and West Bank, and affirmed the Palestinian right to self-government within those areas through the creation of a Palestinian Authority. For their part, the Palestine Liberation Organization formally recognized Israel and committed to adopting responsibility for internal security in population centers in the areas evacuated. Palestinian self-rule was to last for a five-year interim period during which a permanent agreement would be negotiated. However, the realities on the ground left both sides deeply disappointed with the Oslo process. Israelis and Palestinians have blamed each other for the failure of the Oslo peace process. In the five years immediately following the signing of the Oslo accords, 405 Palestinians and 256 Israelis were killed, which for the latter represented a casualty count higher than that of the previous fifteen years combined (216, 172 of whom were killed during the First Intifada).
From 1996 Israel made extensive contingency plans and preparations, collectively code-named "Musical Charm," in the eventuality that peace talks might fail. In 1998, after concluding that the 5-year plan stipulated in the Oslo Talks would not be completed, the IDF implemented an Operation Field of Thorns plan to conquer towns in Area C, and some areas of Gaza, and military exercises at regimental level were carried out in April 2000 to that end. Palestinian preparations were defensive, and small scale, more to reassure the local population than to cope with an eventual attack from Israel. The intensity of these operations led one Brigadier General, Zvi Fogel to wonder whether Israel's military preparations would not turn out to be a self-fulfilling prophecy.
In 1995, Shimon Peres took the place of Yitzhak Rabin, who had been assassinated by Yigal Amir, a Jewish extremist opposed to the Oslo peace agreement. In the 1996 elections, Israelis elected a right-wing coalition led by the Likud candidate, Benjamin Netanyahu who was followed in 1999 by the Labor Party leader Ehud Barak.
Camp David Summit.
From July 11 to 25, 2000, the Middle East Peace Summit at Camp David was held between United States President Bill Clinton, Israeli Prime Minister Ehud Barak, and Palestinian Authority Chairman Yasser Arafat. The talks ultimately failed with each side blaming the other. There were four principal obstacles to agreement: territory, Jerusalem and the Temple Mount, refugees and the right of return, and Israeli security concerns. Disappointment at the situation over the summer led to a significant fracturing of the PLO as many Fatah factions abandoned it to join Hamas and Islamic Jihad.
On September 13, 2000, Yasser Arafat and the Palestinian Legislative Council postponed the planned unilateral declaration of an independent Palestinian state.
Continued settlement.
While Peres had limited settlement construction at the request of US Secretary of State, Madeleine Albright, Netanyahu continued construction within existing Israeli settlements and put forward plans for the construction of a new neighborhood, Har Homa, in East Jerusalem. However, he fell far short of the Shamir government's 1991–92 level and refrained from building new settlements, although the Oslo agreements stipulated no such ban. Construction of Housing Units Before Oslo: 1991–92: 13,960, After Oslo: 1994–95: 3,840, 1996–1997: 3,570.
With the aim of marginalizing the settlers' more militant wing, Barak courted moderate settler opinion, securing agreement for the dismantlement of 12 new outposts that had been constructed since the Wye River Agreement of November 1998, but the continued expansion of existing settlements with plans for 3,000 new houses in the West Bank drew strong condemnation from the Palestinian leadership. Though construction within existing settlements was permitted under the Oslo agreements, Palestinian supporters contend that any continued construction was contrary to its spirit, prejudiced the outcome of final status negotiations, and undermined Palestinian confidence in Barak's desire for peace.
Timeline.
2000.
The Middle East Peace Summit at Camp David from July 11 to 25, 2000, took place between United States President Bill Clinton, Israeli Prime Minister Ehud Barak, and Palestinian Authority Chairman Yasser Arafat. It failed with the latter two blaming each other for the failure of the talks. There were four principal obstacles to agreement: territory, Jerusalem and the Temple Mount, Palestinian refugees and the right of return and Israeli security concerns.
Sharon visits Temple Mount.
On September 28, Israeli opposition leader Ariel Sharon together with a Likud party delegation surrounded by hundreds of Israeli riot police, visited the Temple Mount. Al-Aqsa Mosque is part of the compound and is widely considered the third holiest site in Islam. Israel asserted its control by incorporating East Jerusalem into Jerusalem in 1980, and the compound is the holiest site in Judaism. Sharon was only permitted to enter the compound after the Israeli Interior Minister had received assurances from the Palestinian Authority's security chief that no problems would arise if he made the visit. Sharon did not actually go into the Al-Aqsa Mosque and went during normal tourist hours. Colin Shindler writes, "Shlomo Ben-Ami, the Minister of Internal security, was told by Israeli intelligence that there was no concerted risk of violence. This was implicitly confirmed by Jibril Rajoub, the Palestinian head of Preventive Security on the West Bank, who told Ben-Shlomo that Sharon could visit the Haram, but not enter a mosque on security grounds."
Shortly after Sharon left the site, angry demonstrations by Palestinian Jerusalemites outside erupted into rioting. The person in charge of the waqf at the time, Abu Qteish, was later indicted by Israel for using a loud-speaker to call on Palestinians to defend Al-Aqsa at the time, which action Israeli authorities claimed was responsible for the subsequent stone-throwing in the direction of the Wailing Wall. Israeli police responded with tear gas and rubber bullets, while protesters hurled stones and other missiles, injuring 25 policemen, of whom one was seriously injured and had to be taken to hospital. At least three Palestinians were wounded by rubber bullets.
The stated purpose for Sharon's visit of the compound was to assert the right of all Israelis to visit the Temple Mount; however, according to Likud spokesman Ofir Akunis, the purpose was to "show that under a Likud government Temple Mount will remain under Israeli sovereignty." Ehud Barak in the Camp David negotiations had insisted that East Jerusalem, where the Haram was located, would remain under complete Israeli sovereignty. In response to accusations by Ariel Sharon of government readiness to concede the site to the Palestinians, the Israeli government gave Sharon permission to visit the area. When alerted of his intentions, senior Palestinian figures, such as Yasser Arafat, Saeb Erekat, and Faisal Husseini all asked Sharon to call off his visit.
The Palestinians, some 10 days earlier, had just observed their annual memorial day for the Sabra and Shatila massacre. The Kahan Commission had concluded that Ariel Sharon, who was Defense Minister during the Sabra and Shatila massacre, was found to bear personal responsibility "for ignoring the danger of bloodshed and revenge" and "not taking appropriate measures to prevent bloodshed." Sharon's negligence in protecting the civilian population of Beirut, which had come under Israeli control amounted to a "non-fulfillment of a duty with which the Defence Minister was charged", and it was recommended that Sharon be dismissed as Defence Minister. Sharon initially refused to resign, but after the death of an Israeli after a peace march, Sharon did resign as Defense minister, but remained in the Israeli cabinet.
The Palestinians condemned Sharon's visit to the Temple Mount as a provocation and an incursion, as were his armed bodyguards that arrived on the scene with him. Critics claim that Sharon knew that the visit could trigger violence, and that the purpose of his visit was political. According to one observer, Sharon, in walking on the temple Mount, was “skating on the thinnest ice in the Arab-Israeli conflict." According to Yossef Bodansky,
Clinton's proposal [...] included explicit guarantees that Jews would have the right to visit and pray in and around the Temple Mount... Once Sharon was convinced that Jews had free access to the Temple Mount, there would be little the Israeli religious and nationalist Right could do to stall the peace process. When Sharon expressed interest in visiting the Temple Mount, Barak ordered GSS chief Ami Ayalon to approach Jibril Rajoub with a special request to facilitate a smooth and friendly visit. [...] Rajoub promised it would be smooth as long as Sharon would refrain from entering any of the mosques or praying publicly. [...] Just to be on the safe side, Barak personally approached Arafat and once again got assurances that Sharon's visit would be smooth as long as he did not attempt to enter the Holy Mosques. [...]
A group of Palestinian dignitaries came to protest the visit, as did three Arab Knesset Members. With the dignitaries watching from a safe distance, the Shabab (youth mob) threw rocks and attempted to get past the Israeli security personnel and reach Sharon and his entourage. [...] Still, Sharon's deportment was quiet and dignified. He did not pray, did not make any statement, or do anything else that might be interpreted as offensive to the sensitivities of Muslims. Even after he came back near the Wailing Wall under the hail of rocks, he remained calm. "I came here as one who believes in coexistence between Jews and Arabs," Sharon told the waiting reporters. "I believe that we can build and develop together. This was a peaceful visit. Is it an instigation for Israeli Jews to come to the Jewish people's holiest site?"
Shlomo Ben-Ami, the then acting Israeli Foreign Minister, has maintained, however, that he received Palestinian assurances that no violence would occur, provided that Ariel Sharon not enter one of the mosques.
According to "The New York Times", many in the Arab world, including Egyptians, Palestinians, Lebanese and Jordanians, point to Sharon's visit as the beginning of the Second Intifada and derailment of the peace process. According to Juliana Ochs, Sharon's visit 'symbolically instigated' the second intifada.
First days of the Intifada.
On September 29, 2000, the day after Sharon's visit, following Friday prayers, large riots broke out around the Old City of Jerusalem. After Palestinians on the Temple Mount threw rocks over the Western Wall at Jewish worshippers, Israeli police fired back. The switch to live ammunition occurred when the chief of Jerusalem’s police force was knocked unconscious by a rock. Police then switched to live ammunition, killing four Palestinian youths. Up to 200 Palestinians and police were injured. Another three Palestinians were killed in the Old City and on the Mount of Olives. By the end of the day, 7 Palestinians had been killed and 300 had been wounded. 70 Israeli policemen were also injured in the clashes.
In the days that followed, demonstrations erupted all over the West Bank and Gaza. Israeli police responded with live fire and rubber-coated bullets. In the first five days, at least 47 Palestinians were killed, and 1,885 were wounded. In Paris, as Jacques Chirac attempted to mediate between the parties, he protested to Barak that the ratio of Palestinian and Israeli killed and wounded on one day were such that he could not convince anyone the Palestinians were the aggressors. He also told Barak that "continu(ing) to fire from helicopters on people throwing rocks" and refusing an international inquiry was tantamount to rejecting Arafat's offer to participate in trilateral negotiations. On September 27, an Israeli soldier was killed and another lightly wounded in a bombing by Palestinian militants near the Gaza Strip settlement of Netzarim. Two days later, Palestinian police officer Nail Suleiman opened fire on an Israel Border Police jeep during a joint patrol in the West Bank city of Qalqiliyah, killing Supt. Yosef Tabeja. During the first few days of riots, the IDF fired approximately 1.3 million bullets.
According to Amnesty International the early Palestinian casualties were those taking part in demonstrations or bystanders. Amnesty further states that approximately 80% of the Palestinians killed during the first month were in demonstrations where Israeli security services lives were not in danger.
On September 30, 2000, the death of Muhammad al-Durrah, a Palestinian boy shot dead while sheltering behind his father in an alley in the Gaza Strip was caught on video. Initially the boy's death and his father's wounding was attributed to Israeli soldiers. The scene assumed iconic status, as it was shown around the world and repeatedly broadcast on Arab television. The Israeli army initially assumed responsibility for the killing and apologized, and only retracted 2 months later, when an internal investigation cast doubt on the original version, and controversy subsequently raged as to whether indeed the IDF had fired the shots or Palestinian factions were responsible for the fatal gunshots.
October 2000 events.
The "October 2000 events" refers to several days of disturbances and clashes inside Israel, mostly between Arab citizens and the Israel police. The events also saw large-scale rioting by both Arabs and Jews. Twelve Arab citizens of Israel and a Palestinian from the Gaza Strip were killed by Israeli police, while an Israeli Jew was killed when his car was hit by a rock on the Tel-Aviv-Haifa freeway. During the first month of the Intifada, 141 Palestinians were killed and 5,984 were wounded while just 12 Israelis were killed and 65 wounded.
A general strike and demonstrations across northern Israel began on October 1 and continued for several days. In some cases, the demonstrations escalated into clashes with the Israeli police involving rock-throwing, firebombing, and live-fire. Policemen used tear-gas and opened fire with rubber-coated bullets and later live ammunition in some instances, many times in contravention of police protocol governing riot-dispersion. This use of live ammunition was directly linked with many of the deaths by the Or Commission.
On October 8, thousands of Jewish Israelis participated in violent acts in Tel Aviv and elsewhere, some throwing stones at Arabs, destroying Arab property and chanting "Death to the Arabs."
Following the riots, a high degree of tension between Jewish and Arab citizens and distrust between the Arab citizens and police were widespread. An investigation committee, headed by Supreme Court Justice Theodor Or, reviewed the violent riots and found that the police were poorly prepared to handle such riots and charged major officers with bad conduct. The Or Commission reprimanded Prime Minister Ehud Barak and recommended Shlomo Ben-Ami, then the Internal Security Minister, not serve again as Minister of Public Security. The committee also blamed Arab leaders and Knesset members for contributing to inflaming the atmosphere and making the violence more severe.
Ramallah lynching and Israeli response.
On October 12, PA police arrested two Israeli reservists who had accidentally entered Ramallah, where in the preceding weeks a hundred Palestinians had been killed, many of them minors. Rumors quickly spread that Israeli undercover agents were in the building, and an angry crowd of more than 1,000 Palestinian mourners gathered in front of the station calling for their death. Both soldiers were beaten, stabbed, and disembowelled, and one body was set on fire. An Italian television crew captured the killings on video and then broadcast his tape internationally. A British journalist had his camera destroyed by rioters as he attempted to take a picture. The brutality of the killings shocked the Israeli public. In response, Israel launched a series of retaliatory airstrikes against Palestinian Authority targets in the West Bank and Gaza Strip. The police station where the lynching had taken place was evacuated and destroyed in these operations. Israel later tracked down and arrested those responsible for killing the soldiers.
November and December.
Clashes between Israeli forces and Palestinians increased sharply on November 1, when three Israeli soldiers and six Palestinians were killed, and four IDF soldiers and 140 Palestinians were wounded. In subsequent days, casualties increased as the IDF attempted to restore order, with clashes occurring every day in November. A total of 122 Palestinians and 22 Israelis were killed. On November 27, the first day of Ramadan, Israel eased restrictions on the passage of goods and fuel through the Karni crossing. That same day, the Jerusalem settlement of Gilo came under Palestinian heavy machine gun fire from Beit Jala. Israel tightened restrictions a week later, and Palestinians continued to clash with the IDF and Israeli settlers, with a total of 51 Palestinians and 8 Israelis killed in December.
2001.
The Taba Summit between Israel and the Palestinian Authority was held from January 21 to 27, 2001, at Taba in the Sinai peninsula. Israeli prime minister Ehud Barak and Palestinian President Yasser Arafat came closer to reaching a final settlement than any previous or subsequent peace talks yet ultimately failed to achieve their goals.
On January 17, 2001, Israeli teenager Ofir Rahum was murdered after being lured into Ramallah by a 24-year-old Palestinian, Mona Jaud Awana, a member of Fatah's Tanzim. She had contacted Ofir on the internet and engaged in an online romance with him for several months. She eventually convinced him to drive to Ramallah to meet her, where he was instead ambushed by three Palestinian gunmen and shot over fifteen times. Awana was later arrested in a massive military and police operation, and imprisoned for life. Five other Israelis were killed in January, along with eighteen Palestinians.
Ariel Sharon, at the time from the Likud party, ran against Ehud Barak from the Labour party. Sharon was elected Israeli Prime Minister February 6, 2001 in the 2001 special election to the Prime Ministership. Sharon refused to meet in person with Yasser Arafat.
Violence in March resulted in the deaths of 8 Israelis, mostly civilians, and 26 Palestinians. In Hebron, a Palestinian sniper killed ten-month-old Israeli baby Shalhevet Pass. The murder shocked the Israeli public. According to the Israel police investigation the sniper aimed deliberately at the baby.
On April 30, 2001, seven Palestinian militants were killed in an explosion, one of them a participant in Ofir Rahum's murder. The IDF refused to confirm or deny Palestinian accusations that it was responsible.
On May 7, 2001, the IDF naval commandos captured the vessel "Santorini", which sailed in international waters towards Palestinian Authority-controlled Gaza. The ship was laden with weaponry. The Israeli investigation that followed alleged that the shipment had been purchased by Ahmed Jibril's Popular Front for the Liberation of Palestine - General Command (PFLP-GC). The ship's value and that of its cargo was estimated at $10 million. The crew was reportedly planning to unload the cargo of weapons-filled barrels—carefully sealed and waterproofed along with their contents—at a prearranged location off the Gaza coast, where the Palestinian Authority would recover it.
On May 8, 2001, two Israeli teenagers, Yaakov "Koby" Mandell (13) and Yosef Ishran (14) were kidnapped while hiking near their village. Their bodies were discovered the next morning in a cave near where they lived. "USA Today" reported that, according to the police, both boys had "been bound, stabbed and beaten to death with rocks." The newspaper continued, "The walls of the cave in the Judean Desert were covered with the boys' blood, reportedly smeared there by the killers."
On May 18, 2001, Israel for the first time since 1967 used warplanes to attack targets in the territories. In the past, airstrikes had been carried out with helicopter gunships. 12 Palestinians were killed in these attacks on Palestinian Authority security targets.
On June 1, 2001, an Islamic Jihad suicide bomber detonated himself in the Tel Aviv coastline Dolphinarium dancing club. Twenty-one Israeli civilians, most of them high school students, were killed and 132 injured. The attack significantly hampered American attempts to negotiate cease-fire.
A total of 469 Palestinians and 199 Israelis were killed in 2001. Amnesty International's report on the first year of the Intifada states:
Palestinian terrorists committed a number of suicide attacks later in 2001, among them the Sbarro restaurant massacre with 15 civilian casualties (including 7 children), the Nahariya train station suicide bombing and the Pardes Hanna bus bombing, both with 3 civilian casualties, the Ben Yehuda Street bombing with 11 civilian deaths, many of them children, and the Haifa bus 16 suicide bombing with 15 civilian casualties.
2002.
In January 2002, the IDF Shayetet 13 naval commandos captured the "Karine A", a freighter carrying weapons from Iran towards Israel, believed to be intended for Palestinian militant use against Israel. It was discovered that top officials in the Palestinian Authority were involved in the smuggling, with the Israelis pointing the finger towards Yasser Arafat as also being involved.
Palestinians launched a spate of suicide bombings and attacks, aimed mostly at civilians, against Israel. On March 3, a Palestinian sniper killed 10 Israeli soldiers and settlers and wounded 4 at a checkpoint near Ofra, using an M1 Carbine. He was later arrested and sentenced to life imprisonment. The rate of the attacks increased, and was at its highest in March 2002. In addition to numerous shooting and grenade attacks, that month saw 15 suicide bombings carried out in Israel, an average of one bombing every two days. The high rate of attacks caused widespread fear throughout Israel and serious disruption of daily life throughout the country. March 2002 became known in Israel as "Black March." The wave of suicide bombings culminated with the Passover massacre in Netanya on March 27, in which 30 people were killed at the Park Hotel while celebrating Passover. In total, around 130 Israelis, mostly civilians, were killed in Palestinian attacks during March 2002.
On 12 March United Nations Security Council Resolution 1397 was passed, which reaffirmed a Two-state solution and laid the groundwork for a Road map for peace. Arab leaders, whose constituencies were exposed to detailed television coverage of the violence in the conflict, set out a comprehensive Arab Peace Initiative, which was outlined by Saudi Arabia on 28 March. Arafat endorsed the proposal, while Israel reacted coolly, virtually ignoring it.
On March 29, Israel launched Operation Defensive Shield, which lasted until May 3. The IDF made sweeping incursions throughout the West Bank, and into numerous Palestinian cities. The UN estimated that 497 Palestinians were killed and 1,447 wounded by the Israeli response from March 1 to May 7, although B'Tselem registered 240 killed. Most of the casualties were members of Palestinian security forces and militant groups. Israeli forces also arrested 4,258 Palestinians during the operation. Israeli casualties during the operation totaled 30 dead and 127 wounded. The operation culminated with the recapturing of Palestinian Authority controlled areas.
Jenin.
Between April 2 and 11, a siege and fierce fighting took place in Jenin, a Palestinian refugee camp. The camp was targeted during Operation Defensive Shield after Israel determined that it had "served as a launch site for numerous terrorist attacks against both Israeli civilians and Israeli towns and villages in the area." The Jenin battle became a flashpoint for both sides. Eventually, The battle was won by the IDF, after it operated a dozen of Caterpillar D9 armored bulldozers who cleared Palestinian booby traps, detonated explosive charges, razed buildings and gun-posts and proved impervious to attacks by Palestinian militants.
During the IDF's operations in the camp, Palestinian sources alleged that a massacre of hundreds of people had taken place. A senior Palestinian Authority official alleged in mid-April that some 500 had been killed. During the fighting in Jenin, Israeli officials had also initially estimated hundreds of Palestinian deaths, but later said they expected the Palestinian toll to reach "45 to 55." In the ensuing controversy, Israel blocked the United Nations from conducting the first-hand inquiry unanimously sought by the Security Council, but the UN nonetheless felt able to dismiss claims of a massacre in its report, which said there had been approximately 52 deaths, criticizing both sides for placing Palestinian civilians at risk. Based on their own investigations, Amnesty International and Human Rights Watch charged that some IDF personnel in Jenin had committed war crimes but also confirmed that no massacre had been committed by the IDF. Both human rights organizations called for official inquiries; the IDF disputed the charges.
After the battle, most sources, including the IDF and Palestinian Authority, placed the Palestinian death toll at 52–56; HRW said this total consisted of at least 27 militants and 22 civilians, while the IDF said that 48 militants and 5 civilians had been killed. According to Human Rights Watch, 140 buildings had been destroyed. The IDF reported that 23 Israeli soldiers had been killed and 75 wounded.
Bethlehem.
From April 2 to May 10, a stand-off developed at the Church of the Nativity in Bethlehem. IDF soldiers surrounded the church while Palestinian civilians, militants, and priests were inside. During the siege, IDF snipers killed 8 militants inside the church and wounded more than 40 people. The stand-off was resolved by the deportation to Europe of 13 Palestinian militants whom the IDF had identified as terrorists, and the IDF ended its 38-day stand-off with the militants inside the church.
2003.
Following an Israeli intelligence report stating that Yasir Arafat had paid $20,000 to al-Aqsa Martyrs' Brigades, the United States demanded democratic reforms in the Palestinian Authority, as well the appointment of a prime minister independent of Arafat. On March 13, 2003, following U.S. pressure, Arafat appointed Mahmoud Abbas as Palestinian prime minister.
Following the appointment of Abbas, the U.S. administration promoted the Road map for peace—the Quartet's plan to end the Israeli-Palestinian conflict by disbanding militant organizations, halting settlement activity and establishing a democratic and peaceful Palestinian state. The first phase of the plan demanded that the Palestinian Authority suppress guerrilla and terrorist attacks and confiscate illegal weapons. Unable or unwilling to confront militant organizations and risk civil war, Abbas tried to reach a temporary cease-fire agreement with the militant factions and asked them to halt attacks on Israeli civilians.
On May 20, Israeli naval commandos intercepted another vessel, the "Abu Hassan", on course to the Gaza Strip from Lebanon. It was loaded with rockets, weapons, and ammunition. Eight crew members on board were arrested including a senior Hezbollah member.
On June 29, 2003, a temporary armistice was unilaterally declared by Fatah, Hamas and Islamic Jihad, which declared a ceasefire and halt to all attacks against Israel for a period of three months. Violence decreased somewhat in the following month but suicide bombings against Israeli civilians continued as well as Israeli operations against militants.
Four Palestinians, three of them militants, were killed in gun battles during an IDF raid of Askar near Nablus involving tanks and armoured personnel carriers (APCs); an Israeli soldier was killed by one of the militants. Nearby Palestinians claimed a squad of Israeli police disguised as Palestinian labourers opened fire on Abbedullah Qawasameh as he left a Hebron mosque. YAMAM, the Israeli counter-terrorism police unit that performed the operation stated that Qawasemah opened fire on them as they attempted to arrest him.
On August 19, Hamas coordinated a suicide attack on a crowded bus in Jerusalem killing 23 Israeli civilians, including 7 children. Hamas claimed it was a retaliation for the killing of five Palestinians (including Hamas leader Abbedullah Qawasameh) earlier in the week. U.S. and Israeli media outlets frequently referred to the bus bombing as shattering the quiet and bringing an end to the ceasefire.
Following the Hamas bus attack, Israeli Defence Forces were ordered to kill or capture all Hamas leaders in Hebron and the Gaza Strip. The plotters of the bus suicide bombing were all captured or killed and Hamas leadership in Hebron was badly damaged by the IDF. Strict curfews were enforced in Nablus, Jenin, and Tulkarem; the Nablus lockdown lasted for over 100 days. In Nazlet 'Issa, over 60 shops were destroyed by Israeli civil administration bulldozers. The Israeli civil administration explained that the shops were demolished because they were built without a permit. Palestinians consider Israeli military curfews and property destruction to constitute collective punishment against innocent Palestinians.
Unable to rule effectively under Arafat, Abbas resigned in September 2003. Ahmed Qurei (Abu Ala) was appointed to replace him. The Israeli government gave up hope for negotiated settlement to the conflict and pursued a unilateral policy of physically separating Israel from Palestinian communities by beginning construction on the Israeli West Bank barrier. Israel claims the barrier is necessary to prevent Palestinian attackers from entering Israeli cities. Palestinians claim the barrier separates Palestinian communities from each other and that the construction plan is a de facto annexation of Palestinian territory.
Following an October 4 suicide bombing in Maxim restaurant, Haifa, which claimed the lives of 21 Israelis, Israel claimed that Syria and Iran sponsored the Islamic Jihad and Hezbollah, and were responsible for the terrorist attack. The day after the Maxim massacre, IAF warplanes bombed an alleged former Palestinian training base at Ain Saheb, Syria, which had been mostly abandoned since the 1980s. Munitions being stored on the site were destroyed, and a civilian guard was injured.
2004.
In response to a repeated shelling of Israeli communities with Qassam rockets and mortar shells from Gaza, the IDF operated mainly in Rafah – to search and destroy smuggling tunnels used by militants to obtain weapons, ammunition, fugitives, cigarettes, car parts, electrical goods, foreign currency, gold, drugs, and cloth from Egypt. Between September 2000 and May 2004, ninety tunnels connecting Egypt and the Gaza Strip were found and destroyed. Raids in Rafah left many families homeless. Israel's official stance is that their houses were captured by militants and were destroyed during battles with IDF forces. Many of these houses are abandoned due to Israeli incursions and later destroyed. According to Human Rights Watch, over 1,500 houses were destroyed to create a large buffer zone in the city, many "in the absence of military necessity", displacing around sixteen thousand people.
On February 2, 2004, Israeli Prime Minister Ariel Sharon announced his plan to transfer all the Jewish settlers from the Gaza Strip. The Israeli opposition dismissed his announcement as "media spin" but the Israeli Labour Party said it would support such a move. Sharon's right-wing coalition partners National Religious Party and National Union rejected the plan and vowed to quit the government if it were implemented. Yossi Beilin, peace advocate and architect of the Oslo Accords and the Geneva Accord, also rejected the proposed withdrawal plan. He claimed that withdrawing from the Gaza Strip without a peace agreement would reward terror.
Following the declaration of the disengagement plan by Ariel Sharon and as a response to suicide attacks on Erez crossing and Ashdod seaport (10 people were killed), the IDF launched a series of armored raids on the Gaza Strip (mainly Rafah and refugee camps around Gaza), killing about 70 Hamas militants. On March 22, 2004, an Israeli helicopter gunship killed Hamas leader Sheikh Ahmed Yassin, along with his two bodyguards and nine bystanders, and on April 17, after several failed attempts by Hamas to commit suicide bombings and a successful one that killed an Israeli policeman, Yassin's successor, Abdel Aziz al-Rantissi was killed in an almost identical way, along with a bodyguard and his son Mohammed.
The fighting in Gaza Strip escalated severely in May 2004 after several failed attempts to attack Israeli checkpoints such as Erez crossing and Karni crossing. On May 2, Palestinian militants attacked and shot dead a pregnant woman and her four young daughters. Amnesty International classified it as a crime against humanity and stated that it "reiterates its call on all Palestinian armed groups to put an immediate end to the deliberate targeting of Israeli civilians, in Israel and in the Occupied Territories". Additionally, on May 11 and 12, Palestinian militants destroyed two IDF M-113 APCs, killing 13 soldiers and mutilating their bodies. The IDF launched two raids to recover the bodies in which about 20–40 Palestinians were killed and great damage was caused to structures in the Zaitoun neighbourhood in Gaza and in south-west Rafah.
Subsequently, on May 18 the IDF launched Operation Rainbow with a stated aim of striking the terror infrastructure of Rafah, destroying smuggling tunnels, and stopping a shipment of SA-7 missiles and improved anti-tank weapons. A total of 41 Palestinian militants and 12 civilians were killed in the operation, and about 45–56 Palestinian structures were demolished. Israeli tanks shelled hundreds of Palestinian protesters approaching their positions, killing 10. The protesters had disregarded Israeli warnings to turn back. This incident led to a worldwide outcry against the operation.
On September 29, after a Qassam rocket hit the Israeli town of Sderot and killed two Israeli children, the IDF launched Operation Days of Penitence in the north of the Gaza Strip. The operation's stated aim was to remove the threat of Qassam rockets from Sderot and kill the Hamas militants launching them. The operation ended on October 16, leaving widespread destruction and more than 100 Palestinians dead, at least 20 of whom were under the age of 16. Thirteen-year-old Iman Darweesh Al Hams was killed by the IDF when she strayed into a closed military area: the commander was accused of allegedly firing his automatic weapon at her dead body deliberately to verify the death. The act was investigated by the IDF, but the commander was cleared of all wrongdoing, and more recently, was fully vindicated when a Jerusalem district court found the claim to be libelous, ruled that NIS 300,000 be paid by the journalist and TV company responsible for the report, an additional NIS 80,000 to be paid in legal fees and required the journalist and television company to air a correction. According to Palestinian medics, Israeli forces killed at least 62 militants and 42 other Palestinians believed to be civilians. According to a count performed by "Haaretz", 87 militants and 42 civilians were killed. Palestinian refugee camps were heavily damaged by the Israeli assault. The IDF announced that at least 12 Qassam launchings had been thwarted and many terrorists hit during the operation.
On October 21, the Israeli Air Force killed Adnan al-Ghoul, a senior Hamas bomb maker and the inventor of the Qassam rocket.
On November 11, Yasser Arafat died in Paris.
Escalation in Gaza began amid the visit of Mahmoud Abbas to Syria in order to achieve a Hudna between Palestinian factions and convince Hamas leadership to halt attacks against Israelis. Hamas vowed to continue the armed struggle sending numerous Qassam rockets into open fields near Nahal Oz, and hitting a kindergarten in Kfar Darom with an anti-tank missile.
On December 9 five Palestinians weapon smugglers were killed and two were arrested in the border between Rafah and Egypt. Later that day, Jamal Abu Samhadana and two of his bodyguards were injured by a missile strike. In the first Israeli airstrike against militants in weeks, an unmanned Israeli drone plane launched one missile at Abu Samahdna's car as it traveled between Rafah and Khan Younis in the southern Gaza Strip. It was the fourth attempt on Samhadana's life by Israel. Samhadana is one of two leaders of the Popular Resistance Committees and one of the main forces behind the smuggling tunnels. Samhadana is believed to be responsible for the blast against an American diplomatic convoy in Gaza that killed three Americans.
On December 10, in response to Hamas firing mortar rounds into the Neveh Dekalim settlement in the Gaza Strip and wounding four Israelis (including an 8-year-old boy), Israeli soldiers fired at the Khan Younis refugee camp (the origin of the mortars) killing a 7-year-old girl. An IDF source confirmed troops opened fire at Khan Younis, but said they aimed at Hamas mortar crews.
The largest attack since the death of Yasser Arafat claimed the lives of five Israeli soldiers on December 12, wounding ten others. Approximately 1.5 tons of explosives were detonated in a tunnel under an Israeli military-controlled border crossing on the Egyptian border with Gaza near Rafah, collapsing several structures and damaging others. The explosion destroyed part of the outpost and killed three soldiers. Two Palestinian militants then penetrated the outpost and killed two other Israeli soldiers with gunfire. It is believed that Hamas and a new Fatah faction, the "Fatah Hawks", conducted the highly organized and coordinated attack. A spokesman, "Abu Majad", claimed responsibility for the attack in the name of the Fatah Hawks claiming it was in retaliation for "the assassination" of Yasser Arafat, charging he was poisoned by Israel.
2005.
Palestinian presidential elections were held on January 9, and Mahmoud Abbas (Abu Mazen) was elected as the president of the PA. His platform was of a peaceful negotiation with Israel and non-violence to achieve Palestinian objectives. Although Abbas called on militants to halt attacks against Israel, he promised them protection from Israeli incursions and did not advocate disarmament by force.
Violence continued in the Gaza Strip, and Ariel Sharon froze all diplomatic and security contacts with the Palestinian National Authority. Spokesman Assaf Shariv declared that "Israel informed international leaders today that there will be no meetings with Abbas until he makes a real effort to stop the terror." The freezing of contacts came less than one week after Mahmoud Abbas was elected, and the day before his inauguration. Palestinian negotiator Saeb Erekat, confirming the news, declared "You cannot hold Mahmoud Abbas accountable when he hasn't even been inaugurated yet."
Following international pressure and Israeli threat of wide military operation in the Gaza Strip, Abbas ordered Palestinian police to deploy in the northern Gaza Strip to prevent Qassam rocket and mortar shelling over Israeli settlement. Although attacks on Israelis did not stop completely, they decreased sharply. On February 8, 2005, at the Sharm el-Sheikh Summit of 2005, Sharon and Abbas declared a mutual truce between Israel and the Palestinian National Authority. They shook hands at a four-way summit that also included Jordan and Egypt at Sharm al-Sheikh. However, Hamas and Islamic Jihad said the truce is not binding for their members. Israel has not withdrawn its demand to dismantle terrorist infrastructure before moving ahead in the Road map for peace.
Many warned that truce is fragile, and progress must be done slowly while observing that the truce and quiet are kept. On February 9–10 night, a barrage of 25–50 Qassam rockets and mortar shells hit Neve Dekalim settlement, and another barrage hit at noon. Hamas said it was in retaliation for an attack in which one Palestinian was killed near an Israeli settlement. As a response to the mortar attack, Abbas ordered the Palestinian security forces to stop such attacks in the future. He also fired senior commanders in the Palestinian security apparatus. On February 10, Israeli security forces arrested Maharan Omar Shucat Abu Hamis, a Palestinian resident of Nablus, who was about to launch a bus suicide attack in the French Hill in Jerusalem.
On February 13, 2005, Abbas entered into talks with the leaders of the Islamic Jihad and the Hamas, for them to rally behind him and respect the truce. Ismail Haniyah, a senior leader of the group Hamas said that "its position regarding calm will continue unchanged and Israel will bear responsibility for any new violation or aggression."
In the middle of June, Palestinian factions intensified bombardment over the city of Sderot with improvised Qassam rockets. Palestinian attacks resulted in 2 Palestinians and 1 Chinese civilian killed by a Qassam, and 2 Israelis were killed. The wave of attacks lessened support for the disengagement plan among the Israeli public. Attacks on Israel by the Islamic Jihad and the al-Aqsa Martyrs' Brigades increased in July, and on July 12, a suicide bombing hit the coastal city of Netanya, killing 5 civilians. On July 14, Hamas started to shell Israeli settlements inside and outside the Gaza Strip with dozens of Qassam rockets, killing an Israeli woman. On July 15, Israel resumed its "targeted killing" policy, killing 7 Hamas militants and bombing about 4 Hamas facilities. The continuation of shelling rockets over Israeli settlements, and street battles between Hamas militants and Palestinian policemen, threatened to shatter the truce agreed in the Sharm el-Sheikh Summit of 2005. The Israeli Defence Force also started to build up armored forces around the Gaza Strip in response to the shelling.
End of the Intifada.
The ending date of the Second Intifada is disputed, as there was no definite event that brought it to an end. Some commentators such as Sever Plocker consider the intifada to have ended in late 2004. With the sickness and then death of Yasser Arafat in November 2004, the Palestinians lost their internationally recognised leader of the previous three decades, after which the intifada lost momentum and lead to internal fighting between Palestinian factions (most notably the Fatah–Hamas conflict), as well as conflict within Fatah itself. Israel's unilateral disengagement from the Gaza Strip, announced in June 2004 completed in August 2005, is also cited, for instance by Ramzy Baroud, as signalling the end of the intifada. Palestinian President Mahmoud Abbas vowed in the days leading to the Sharm el-Sheikh Summit of 2005 that it would mark the end of the intifada. The summit resulted in Abbas declaring violence would come to an end, and Ariel Sharon agreed to release 900 Palestinian prisoners and withdraw from West Bank towns, some consider this to be the official end of the Second Intifada, although sporadic violence still continued outside PA control or condolence.
Cause of the Second Intifada.
The Second Intifada started in September 2000, when Ariel Sharon made a visit to the Temple Mount, also known as Al-Haram Al-Sharif, an area sacred to both Jews and Muslims. This visit was seen by Palestinians as highly provocative; and Palestinian demonstrators, throwing stones at police, were dispersed by the Israeli army, using tear gas and rubber bullets. On September 28, Sharon, a Likud party candidate for Israeli Prime Minister, entered the Temple Mount, accompanied by over 1,000 security guards. He stated on that day, "the Temple Mount is in our hands and will remain in our hands. It is the holiest site in Judaism and it is the right of every Jew to visit the Temple Mount." A riot broke out among Palestinians at the site, resulting in clashes between Israeli forces and the protesting crowd. Some believe it started a day later on Friday 29 September a day of prayers, when an Israeli police and military presence was introduced and there were major clashes and deaths.
The Mitchell Report.
The Sharm el-Sheikh Fact-Finding Committee (an investigatory committee set up to look into the causes behind the breakdown in the peace process, chaired by George J. Mitchell) published its report in May 2001.
In the Mitchell Report, the government of Israel asserted that:
The immediate catalyst for the violence was the breakdown of the Camp David negotiations on July 25, 2000, and the "widespread appreciation in the international community of Palestinian responsibility for the impasse". In this view, Palestinian violence was planned by the PA leadership, and was aimed at "provoking and incurring Palestinian casualties as a means of regaining the diplomatic initiative".
The Palestine Liberation Organization, according to the same report, denied that the Intifada was planned, and asserted that "Camp David represented nothing less than an attempt by Israel to extend the force it exercises on the ground to negotiations."
The report also stated:
From the perspective of the PLO, Israel responded to the disturbances with excessive and illegal use of deadly force against demonstrators; behavior which, in the PLO's view, reflected Israel's contempt for the lives and safety of Palestinians. For Palestinians, the widely seen images of Muhammad al-Durrah in Gaza on September 30, shot as he huddled behind his father, reinforced that perception.
The Mitchell report concluded:
The Sharon visit did not cause the "Al-Aqsa Intifada". But it was poorly timed and the provocative effect should have been foreseen; indeed it was foreseen by those who urged that the visit be prohibited.
and also:
We have no basis on which to conclude that there was a deliberate plan by the PA to initiate a campaign of violence at the first opportunity; or to conclude that there was a deliberate plan by the of Israel to respond with lethal force.
Views on the Second Intifada.
Palestinians have claimed that Sharon's visit was the beginning of the Second Intifada, while others have claimed that Yasser Arafat had pre-planned the uprising.
Some, like Bill Clinton, say that tensions were high due to failed negotiations at the Camp David Summit in July 2000. They note that there were Israeli casualties as early as September 27; this is the Israeli "conventional wisdom", according to Dr. Jeremy Pressman, and the view expressed by the Israeli Foreign Ministry. Most mainstream media outlets have taken the view that the Sharon visit was the spark that triggered the rioting at the start of the Second Intifada. In the first five days of rioting and clashes after the visit, Israeli police and security forces killed 47 Palestinians and wounded 1885, while Palestinians killed 5 Israelis.
Palestinians view the Second Intifada as part of their ongoing struggle for national liberation and an end to Israeli occupation, whereas many Israelis consider it to be a wave of Palestinian terrorism instigated and pre-planned by then Palestinian leader Yasser Arafat.
Some have claimed that Yasser Arafat and the Palestinian Authority (PA) had pre-planned the Intifada. They often quote a speech made in December 2000 by Imad Falouji, the PA Communications Minister at the time, where he explains that the Intifada had been planned since Arafat's return from the Camp David Summit in July, far in advance of Sharon's visit. He stated that the Intifada "was carefully planned since the return of (Palestinian President) Yasser Arafat from Camp David negotiations rejecting the U.S. conditions". David Samuels quotes Mamduh Nofal, former military commander of the Democratic Front for the Liberation of Palestine, who supplies more evidence of pre-September 28 military preparations. Nofal recounts that Arafat "told us, Now we are going to the fight, so we must be ready". Barak as early as May had drawn up contingency plans to halt any intifada in its tracks by the extensive use of IDF snipers, a tactic that resulted in the high number of casualties among Palestinians during the first days of rioting.
Support for the idea that Arafat planned the Intifadah comes from Hamas leader Mahmoud al-Zahar, who said in September 2010 that when Arafat realized that the Camp David Summit in July 2000 would not result in the meeting of all of his demands, he ordered Hamas as well as Fatah and the Aqsa Martyrs Brigades, to launch "military operations" against Israel. al-Zahar is corroborated by Mosab Hassan Yousef, son of the Hamas founder and leader, Sheikh Hassan Yousef, who claims that the Second Intifada was a political maneuver premeditated by Arafat. Yousef claims that "Arafat had grown extraordinarily wealthy as the international symbol of victimhood. He wasn't about to surrender that status and take on the responsibility of actually building a functioning society."
Arafat's widow Suha Arafat reportedly said on Dubai television in December 2012 that her husband had planned the uprising.
"Immediately after the failure of the Camp David I met him in Paris upon his return... Camp David had failed, and he said to me, 'You should remain in Paris.' I asked him why, and he said, 'Because I am going to start an intifada. They want me to betray the Palestinian cause. They want me to give up on our principles, and I will not do so,'" the research institute [MEMRI translated Suha as saying.
Israel began a unilateral pullout from Lebanon in compliance with UN Resolution 425 (1978) in May 2000, which was declared completed on June 16. The move was widely interpreted by Arabs as an Israeli defeat and was to exercise a profound influence on tactics adopted in the Al Aqsa Uprising. the PLO official Farouk Kaddoumi told reporters: "We are optimistic. Hezbollah's resistance can be used as an example for other Arabs seeking to regain their rights." Many Palestinian officials have gone on record as saying that the intifada had been planned long in advance to put pressure on Israel. It is disputed however whether Arafat himself gave direct orders for the outbreak, though he did not intervene to put a break on it A personal advisor to Arafat, Manduh Nufal, claimed in early 2001 that the Palestinian Authority had played a crucial role in the outbreak of the Intifada. Israeli’s military response demolished a large part of the infrastructure built by the PA during the years following the Oslo Accords in preparation for a Palestinian state. This infrastructure included the legitimate arming of Palestinian forces for the first time: some 90 paramilitary camps had been set up to train Palestinian youths in armed conflict. Some 40,000 armed and trained Palestinians existed in the occupied territories.
On September 29, 2001 Marwan Barghouti, the leader of the Fatah Tanzim in an interview to "Al-Hayat", described his role in the lead up to the intifada.
I knew that the end of September was the last period (of time) before the explosion, but when Sharon reached the al-Aqsa Mosque, this was the most appropriate moment for the outbreak of the intifada... The night prior to Sharon's visit, I participated in a panel on a local television station and I seized the opportunity to call on the public to go to the al-Aqsa Mosque in the morning, for it was not possible that Sharon would reach al-Haram al-Sharif just so, and walk away peacefully. I finished and went to al-Aqsa in the morning... We tried to create clashes without success because of the differences of opinion that emerged with others in the al-Aqsa compound at the time... After Sharon left, I remained for two hours in the presence of other people, we discussed the manner of response and how it was possible to react in all the cities (bilad) and not just in Jerusalem. We contacted all (the Palestinian) factions.
Barghouti also went on record as stating that the example of Hezbollah and Israel's withdrawal from Lebanon was a factor which contributed to the Intifada.
According to Nathan Thrall, from Elliott Abrams's inside accounts of negotiations between 2001 and 2005, it would appear to be an inescapable conclusion that violence played an effective role in shaking Israeli complacency and furthering Palestinian goals: the U.S. endorsed the idea of a Palestinian State, Ariel Sharon became the first Israeli Prime Minister to affirm the same idea, and even spoke of Israel's "occupation", and the bloodshed was such that Sharon also decided to withdraw from Gaza, an area he long imagined Israel keeping. However, Zakaria Zubeidi, former Palestinian militant leader of the Al-Aqsa Martyrs' Brigades, considers the Intifada to be a total failure that achieved nothing for the Palestinians.
Casualties.
The casualty data for the Second Intifada has been reported by a variety of sources and though there is general agreement regarding the overall number of dead, the statistical picture is blurred by disparities in how different types of casualties are counted and categorized.
The sources do not vary widely over the data on Israeli casualties. B'Tselem reports that 1,053 Israelis were killed by Palestinian attacks through April 30, 2008. Israeli journalist Ze'ev Schiff reported similar numbers citing the Shin Bet as his source in an August 2004 "Haaretz" article where he noted:
The number of Israeli fatalities in the current conflict with the Palestinians exceeded 1,000 last week. Only two of the country's wars – the War of Independence and the Yom Kippur War – have claimed more Israeli lives than this intifada, which began on September 29, 2000. In the Six-Day War, 803 Israelis lost their lives, while the War of Attrition claimed 738 Israeli lives along the borders with Egypt, Syria and Lebanon.
There is little dispute as to the total number of Palestinians killed by Israelis. B'Tselem reports that through April 30, 2008, there were 4,745 Palestinians killed by Israeli security forces, and 44 Palestinians killed by Israeli civilians. B'Tselem also reports 577 Palestinians killed by Palestinians through April 30, 2008.
Between September 2000 and January 2005, 69 percent of Israeli fatalities were male, while over 95 percent of the Palestinian fatalities were male. "Remember These Children" reports that as of February 1, 2008, 119 Israeli children, age 17 and under, had been killed by Palestinians. Over the same time period, 982 Palestinian children, age 17 and under, were killed by Israelis.
Combatant versus noncombatant deaths.
Regarding the numbers of Israeli civilian versus combatant deaths, B'Tselem reports that through April 30, 2008 there were 719 Israeli civilians killed and 334 Israeli security force personnel killed. In other words, 31.7% of those killed were Israeli security force personnel, while 68.3% were civilians.
B'Tselem reports that through April 30, 2008, out of 4,745 Palestinians killed by Israeli security forces, there were 1,671 "Palestinians who took part in the hostilities and were killed by Israeli security forces", or 35.2%. According to their statistics, 2,204 of those killed by Israeli security forces "did not take part in the hostilities", or 46.4%. There were 870 (18.5%) who B'Tselem defines as "Palestinians who were killed by Israeli security forces and it is not known if they were taking part in the hostilities".
The B'Tselem casualties breakdown's reliability was questioned and its methodology has been heavily criticized by a variety of institutions and several groups and researchers, most notably Jerusalem Center for Public Affairs's senior researcher, retired IDF lieutenant colonel Jonathan Dahoah-Halevi, who claimed that B'Tselem repeatedly classifies terror operatives and armed combatants as "uninvolved civilians", but also criticized the Israeli government for not collecting and publishing casualty data. Caroline B. Glick, deputy managing editor of "The Jerusalem Post" and former advisor to Benjamin Netanyahu, pointed to several instances where, she claimed, B'Tselem had misrepresented Palestinian rioters or terrorists as innocent victims, or where B'Tselem failed to report when an Arab allegedly changed his testimony about an attack by settlers. The Committee for Accuracy in Middle East Reporting in America (CAMERA), which said that B'Tselem repeatedly classified Arab combatants and terrorists as civilian casualties.
The Israeli International Policy Institute for Counter-Terrorism (IPICT), on the other hand, in a "Statistical Report Summary" for September 27, 2000, through January 1, 2005, indicates that 56% (1542) of the 2773 Palestinians killed by Israelis were combatants. According to their data, an additional 406 Palestinians were killed by actions of their own side. 22% (215) of the 988 Israelis killed by Palestinians were combatants. An additional 22 Israelis were killed by actions of their own side.
IPICT counts "probable combatants" in its total of combatants. From their full report in September 2002:
A 'probable combatant' is someone killed at a location and at a time during which an armed confrontation was going on, who appears most likely – but not certain – to have been an active participant in the fighting. For example, in many cases where an incident has resulted in a large number of Palestinian casualties, the only information available is that an individual was killed when Israeli soldiers returned fire in response to shots fired from a particular location. While it is possible that the person killed had not been active in the fighting and just happened to be in the vicinity of people who were, it is reasonable to assume that the number of such coincidental deaths is not particularly high. Where the accounts of an incident appear to support such a coincidence, the individual casualty has been given the benefit of the doubt, and assigned a non-combatant status.
In the same 2002 IPICT full report there is a pie chart (Graph 2.9) that lists the IPICT combatant breakdown for Palestinian deaths through September 2002. Here follow the statistics in that pie chart used to come up with the total combatant percentage through September 2002:
On August 24, 2004, "Haaretz" reporter Ze'ev Schiff published casualty figures based on Shin Bet data. The "Haaretz" article reported: "There is a discrepancy of two or three casualties with the figures tabulated by the Israel Defense Forces."
Here is a summary of the figures presented in the article:
The article does not say whether those killed were combatants or not. Here is a quote:
The Palestinian security forces – for example, Force 17, the Palestinian police, General Intelligence, and the counter security apparatus – have lost 334 of its members during the current conflict, the Shin Bet figures show.
As a response to IDF statistics about Palestinian casualties in the West Bank, the Israeli human rights organization B'Tselem reported that two thirds of the Palestinians killed in 2004 did not participate in the fighting.
Prior to 2003, B'Tselem's methodology differentiated between civilians and members of Palestinian military groups, rather than between combatants and non-combatants, leading to criticism from some pro-Israel sources. B'Tselem no longer uses the term "civilian" and instead describes those killed as "participating" or "not participating in fighting at the time of death".
Others argue that Palestinian National Authority has, throughout the Intifada, placed unarmed men, women, children and the elderly in the line of fire, and that announcing the time and place of anti-occupation demonstrations via television, radio, sermons, and calls from mosque loudspeaker systems is done for this purpose.
In 2009, historian Benny Morris stated in his retrospective book "One States, Two States" that about one third of the Palestinian deaths up to 2004 had been civilians.
Palestinians killed by Palestinians.
B'Tselem reports that through April 30, 2008, there were 577 Palestinians killed by Palestinians. Of those, 120 were "Palestinians killed by Palestinians for suspected collaboration with Israel". B'Tselem maintains a list of deaths of Palestinians killed by Palestinians with details about the circumstances of the deaths. Some of the many causes of death are crossfire, factional fighting, kidnappings, collaboration, etc.
Concerning the killing of Palestinians by other Palestinians, a January 2003 "The Humanist" magazine article reports:
For over a decade the PA has violated Palestinian human rights and civil liberties by routinely killing civilians—including collaborators, demonstrators, journalists, and others—without charge or fair trial. Of the total number of Palestinian civilians killed during this period by both Israeli and Palestinian security forces, 16 percent were the victims of Palestinian security forces.
... According to Freedom House's annual survey of political rights and civil liberties, "Freedom in the World 2001–2002", the chaotic nature of the Intifada along with strong Israeli reprisals has resulted in a deterioration of living conditions for Palestinians in Israeli-administered areas. The survey states:
"Civil liberties declined due to: shooting deaths of Palestinian civilians by Palestinian security personnel; the summary trial and executions of alleged collaborators by the Palestinian Authority (PA); extra-judicial killings of suspected collaborators by militias; and the apparent official encouragement of Palestinian youth to confront Israeli soldiers, thus placing them directly in harm's way."
Internal Palestinian violence has been called an "Intra'fada" during this Intifada and the previous one.
Aftermath.
On January 25, 2006, the Palestinians held general elections for the Palestinian Legislative Council. The Islamist group Hamas won with an unexpected majority of 74 seats, compared to 45 seats for Fatah and 13 for other parties and independents. Hamas is officially declared as a terrorist organization by the United States and the European Union and its gaining control over the Palestinian Authority (such as by forming the government) would jeopardize international funds to the PA, by laws forbidding sponsoring of terrorist group.
On June 9, seven members of the Ghalia family were killed on a Gaza beach. The cause of the explosion remains uncertain. Nevertheless, in response, Hamas declared an end to its commitment to a ceasefire declared in 2005 and announced the resumption of attacks on Israelis. Palestinians blame an Israeli artillery shelling of nearby locations in the northern Gaza Strip for the deaths, while an Israeli military inquiry cleared itself from the charges.
On June 25, a military outpost was attacked by Palestinian militants and a gunbattle followed that left 2 Israeli soldiers and 3 Palestinian militants dead. Corporal Gilad Shalit, an Israeli soldier, was captured and Israel warned of an imminent military response if the soldier was not returned unharmed. In the early hours of June 28 Israeli tanks, APCs and troops entered the Gaza strip just hours after the air force had taken out two main bridges and the only powerstation in the strip, effectively shutting down electricity and water. Operation Summer Rains commenced, the first major phase of the Gaza–Israel conflict, which continues to run independently of the intifada.
On November 26, 2006, a truce was implemented between Israel and the Palestinian Authority. A January 10, 2007, Reuters article reports: "Hamas has largely abided by a November 26 truce which has calmed Israeli-Palestinian violence in Gaza."
An intensification of the Gaza–Israel conflict, the Gaza war, occurred on December 27, 2008 (11:30 a.m. local time; 09:30 UTC) when Israel launched a military campaign codenamed "Operation Cast Lead" () targeting the members and infrastructure of Hamas in response to the numerous rocket attacks upon Israel from the Gaza Strip. The operation has been termed the "Gaza massacre" () by Hamas leaders and much of the media in the Arab World.
On Saturday, January 17, 2009, Israel announced a unilateral ceasefire, conditional on elimination of further rocket and mortar attacks from Gaza, and began withdrawing over the next several days. Hamas later announced its own ceasefire, with its own conditions of complete withdrawal and opening of border crossings. A reduced level of mortar fire originating in Gaza continues, though Israel has so far not taken this as a breach of the ceasefire. The frequency of the attacks can be observed in the thumbnailed graph. The data corresponds to the article "Timeline of the 2008–2009 Israel–Gaza conflict", using mainly "Haaretz" news reports from the February 1 up to the February 28. The usual IDF responses are airstrikes on weapon smuggling tunnels.
Deaths in 2006.
The violence continued on both sides throughout 2006. On December 27 the Israeli Human Rights Organization B'Tselem released its annual report on the Intifada. According to which, 660 Palestinians, a figure more than three times the number of Palestinians killed in 2005, and 23 Israelis, have been killed in 2006. From a December 28 "Haaretz" article: "According to the report, about half of the Palestinians killed, 322, did not take part in the hostilities at the time they were killed. 22 of those killed were targets of assassinations, and 141 were minors." 405 of 660 Palestinians were killed in the 2006 Israel-Gaza conflict, which lasted from June 28 till November 26.
Tactics.
Palestinian tactics ranged from mass protests and general strikes, similar to the First Intifada, to armed attacks on Israeli soldiers, security forces, police, and civilians. Methods of attack include suicide bombings, launching rockets and mortars into Israel, kidnapping of both soldiers and civilians, including children, shootings, assassination, stabbings, stonings, and lynchings.
Israeli tactics included curbing Palestinians' movements through the setting up of checkpoints and the enforcement of strict curfews in certain areas. Infrastructural attacks against Palestinian Authority targets such as police and prisons was another method to force the Palestinian Authority to repress the anti-Israeli protests and attacks on Israeli targets .
Palestinians.
Militant groups involved in violence include Hamas, Palestinian Islamic Jihad, Popular Front for the Liberation of Palestine (PFLP) and the al-Aqsa Martyrs' Brigades. They waged a high-intensity campaign of guerrilla warfare against Israeli military and civilian targets inside Israel and in the occupied territory, utilizing tactics such as ambushes, sniper attacks, and suicide bombings. Military equipment was mostly imported, while some light arms, hand grenades and explosive belts, assault rifles, and Qassam rockets were indigenously produced. They also increased use of remote-controlled landmines against Israeli armor, a tactic that was highly popular among the poorly armed groups. Car bombs were often used against "lightly hardened" targets such as Israeli armored jeeps and checkpoints. Also, more than 1,500 Palestinian drive-by shootings killed 75 people in only the first year of the Intifada.
Among the most effective Palestinian tactics was the suicide bombing ("see List"). Conducted as a single or double bombing, suicide bombings were generally conducted against "soft" targets, or "lightly hardened" targets (such as checkpoints) to try to raise the cost of the war to Israelis and demoralize the Israeli society. Most suicide bombing attacks (although not all) targeted civilians, and conducted on crowded places in Israeli cities, such as public transport, restaurants, and markets.
One recent development is the use of suicide bombs carried by children. Unlike most suicide bombings, the use of these not only earned condemnation from the United States and from human rights groups such as Amnesty International, but also from many Palestinians and much of the Middle East press. The youngest Palestinian suicide bomber was 16-year-old Issa Bdeir, a high school student from the village of Al Doha, who shocked his friends and family when he blew himself up in a park in Rishon LeZion, killing a teenage boy and an elderly man. The youngest attempted suicide bombing was by a 14-year-old captured by soldiers at the Huwwara checkpoint before managing to do any harm.
In May 2004, Israel Defense minister Shaul Mofaz claimed that United Nations Relief and Works Agency for Palestine Refugees in the Near East's ambulances were used to take the bodies of dead Israeli soldiers in order to prevent the Israel Defense Forces from recovering their dead. Reuters has provided video of healthy armed men entering ambulance with UN markings for transport. UNRWA initially denied that its ambulances carry militants but later reported that the driver was forced to comply with threats from armed men. UNRWA still denies that their ambulances carried body parts of dead Israeli soldiers.
In August 2004, Israel said that an advanced explosives-detection device employed by the IDF at the Hawara checkpoint near Nablus discovered a Palestinian ambulance had transported explosive material.
Some of the Palestinian reaction to Israeli policy in the West Bank and Gaza Strip has consisted of non-violent protest, primarily in and near the village of Bil'in. Groups such as the Palestinian Centre for Rapprochement, which works out of Beit Sahour, formally encourage and organize non-violent resistance. Other groups, such as the International Solidarity Movement openly advocate non-violent resistance. Some of these activities are done in cooperation with internationals and Israelis, such as the weekly protests against the Israeli West Bank Barrier carried out in villages like Bi'lin, Biddu and Budrus. This model of resistance has spread to other villages like Beit Sira, Hebron, Saffa, and Ni'lein. During the Israeli re-invasion of Jenin and Nablus, "A Call for a Non-violent Resistance Strategy in Palestine" was issued by two Palestinian Christians in May 2002.
Non-violent tactics have sometimes been met with Israeli military force. For example, Amnesty International notes that "10-year-old Naji Abu Qamer, 11-year-old Mubarak Salim al-Hashash and 13-year-old Mahmoud Tariq Mansour were among eight unarmed demonstrators killed in the early afternoon of May 19, 2004 in Rafah, in the Gaza Strip, when the Israeli army open fire on a non-violent demonstration with tank shells and a missile launched from a helicopter gunship. Dozens of other unarmed demonstrators were wounded in the attack." According to Israeli army and government officials, the tanks shelled a nearby empty building and a helicopter fired a missile in a nearby open space in order to deter the demonstrators from proceeding towards Israeli army positions.
Israel.
The Israel Defense Forces (IDF) countered Palestinian attacks with incursions against militant targets into the West Bank and Gaza Strip, adopting highly effective urban combat tactics. The IDF stressed the safety of their troops, using such heavily armored equipment as the Merkava heavy tank and armored personnel carriers, and carried out airstrikes with various military aircraft including F-16s, drone aircraft and helicopter gunships to strike militant targets. Much of the ground fighting was conducted house-to-house by well-armed and well-trained infantry. Due to its superior training, equipment, and numbers, the IDF had the upper hand during street fighting. Palestinian armed groups suffered heavy losses during combat, but the operations were often criticized internationally due to the civilian casualties often caused. Palestinian metalworking shops and other business facilities suspected by Israel of being used to manufacture weapons are regularly targeted by airstrikes, as well as Gaza Strip smuggling tunnels.
Israeli Caterpillar D9 armored bulldozers were routinely employed to detonate booby traps and IEDs, to demolish houses along the border with Egypt that were used for shooting at Israeli troops, to create "buffer zones", and to support military operations in the West Bank. Until February 2005, Israel had in place a policy to demolish the family homes of suicide bombers after giving them a notice to evacuate. Due to the considerable number of Palestinians living in single homes, the large quantity of homes destroyed, and collateral damage from home demolitions, it became an increasingly controversial tactic. Families began providing timely information to Israeli forces regarding suicide bombing activities in order to prevent the demolition of their homes, although families doing so risked being executed or otherwise punished for collaboration, either by the Palestinian Authority or extrajudicially by Palestinian militants. The IDF committee studying the issue recommended ending the practice because the policy was not effective enough to justify its costs to Israel's image internationally and the backlash it created among Palestinians.
With complete ground and air superiority, mass arrests were regularly conducted by Israeli military and police forces; at any given time, there were about 6,000 Palestinian prisoners detained in Israeli prisons, about half of them held temporarily without a final indictment, in accordance with Israeli law.
The tactic of military "curfew" – long-term lockdown of civilian areas – was used extensively by Israel throughout the Intifada. The longest curfew was in Nablus, which was kept under curfew for over 100 consecutive days, with generally under two hours per day allowed for people to get food or conduct other business.
Security Checkpoints and roadblocks were erected inside and between Palestinian cities, subjecting all people and vehicles to security inspection for free passage. Israel defended those checkpoints as being necessary to stop militants and limit the ability to move weapons around. However some Palestinian, Israeli and International observers and organizations have criticized the checkpoints as excessive, humiliating, and a major cause of the humanitarian situation in the Occupied Territories. Transit could be delayed by several hours, depending on the security situation in Israel. Sniper towers were used extensively in the Gaza Strip before the Israeli pullout.
The Israeli intelligence services Shin Bet and Mossad penetrated Palestinian militant organizations by relying on moles and sources within armed groups, tapping communication lines, and aerial reconnaissance. within the groups the Israeli Security Forces (IDF, Magav, police YAMAM and Mistaravim SF units) to thwart hundreds of suicide bombings by providing real-time warnings and reliable intelligence reports, and a list of Palestinians marked for targeted killings.
Israel extensively used "targeted killings", the assassinations of Palestinian leaders involved in perpetrating attacks against Israelis, to eliminate imminent threats and to deter others from following suit, relying primarily on airstrikes and covert operations by Shin Bet to carry them out. Israel has been criticized for the use of helicopter gunships in urban assassinations, which often results in civilian casualties. Israel in turn has criticized what it describes as a practice of militant leaders hiding among civilians in densely populated areas, thus turning them into unwitting human shields. In one of the most controversial killings, the Mossad (Israeli foreign intelligence service) allegedly killed Hamas leader Mahmoud al-Mabhouh in Dubai, using forged passports to slip agents into Dubai. Throughout the Intifada, the Palestinian leadership suffered heavy losses through targeted killings.
The practice has been widely condemned as extrajudicial executions by the international community, while the Israeli High Court ruled that it is a legitimate measure of self-defense against terrorism. Many criticize the targeted killings for placing civilians at risk, though its supporters believe it reduces civilian casualties on both sides.
In response to repeated rocket attacks from the Gaza Strip, the Israeli Navy imposed a maritime blockade on the area. Israel also sealed the border and closed Gaza's airspace in coordination with Egypt, and subjected all humanitarian supplies entering the Strip to security inspection before transferring them through land crossings. Construction materials were declared banned due to their possible use to build bunkers. The blockade has been internationally criticized as a form of "collective punishment" against Gaza's civilian population.
Although Israel's tactics also have been condemned internationally, Israel insists they are vital for security reasons in order to thwart terrorist attacks. Some cite figures, such as those published in Haaretz newspaper, to prove the effectiveness of these methods (Graph 1: Thwarted attacks (yellow) vs successful attacks (red) – Graph 2: Suicide bombing within the "green line" per quarter).
International involvement.
The international community has long taken an involvement in the Israeli-Palestinian conflict, and this involvement has only increased during the al-Aqsa Intifada. Israel currently receives $3 billion in annual military aid from the United States, excluding loan guarantees. Even though Israel is a developed industrial country, it has remained as the largest annual recipient of US foreign assistance since 1976. It is also the only recipient of US economic aid that does not have to account for how it is spent. The Palestinian Authority receives $100 million annually in military aid from the United States and $2 billion in global financial aid, including "$526 million from Arab League, $651 million from the European Union, $300 million from the US and about $238 million from the World Bank". According to the United Nations, the Palestinian territories are among the leading humanitarian aid recipients.
Additionally, private groups have become increasingly involved in the conflict, such as the International Solidarity Movement on the side of the Palestinians, and the American Israel Public Affairs Committee on the side of the Israelis.
In the 2001 and 2002 Arab League Summits, the Arab states pledged support for the Second Intifada just as they had pledged support for the First Intifada in two consecutive summits in the late 1980s.
Effects on Oslo Accords.
Since the start of the al-Aqsa Intifada and its emphasis on suicide bombers deliberately targeting civilians riding public transportation (buses), the Oslo Accords are viewed with increasing disfavor by the Israeli public.
In May 2000, seven years after the Oslo Accords and five months before the start of the al-Aqsa Intifada, a survey by the Tami Steinmetz Center for Peace Research at the Tel Aviv University found that 39% of all Israelis support the Accords and that 32% believe that the Accords will result in peace in the next few years. In contrast, the May 2004 survey found that 26% of all Israelis support the Accords and 18% believe that the Accords will result in peace in the next few years; decreases of 13% and 16% respectively. Furthermore, later survey found that 80% of all Israelis believe the Israel Defense Forces have succeeded in dealing with the al-Aqsa Intifada militarily.
Economic costs.
Israel.
The Israeli commerce has experienced much hardship, in particular because of the sharp drop in tourism and risk aversion increase. Monetary policy decisions and money holdings behaviours sjanged significantly during this period. A representative of Israel's Chamber of Commerce has estimated the cumulative economic damage caused by the crisis at 150 to 200 billion Shekels, or 35 to 45 billion US $ – against an annual GDP of 122 billion dollars in 2002. As the suicide bombings were sharply decreasing after 2005 following IDF's and Shin-Bet's efforts, the Israeli economy has recovered.
Palestinian Authority.
The Office of the United Nations Special Coordinator in the Occupied Territories (UNSCO) estimates the damage done to the Palestinian economy at over 1.1 billion dollars in the first quarter of 2002, compared to an annual GDP of 4.5 billion dollars.
See also.
Directly connected to the Second Intifada and its aftermath:

</doc>
<doc id="71734" url="https://en.wikipedia.org/wiki?curid=71734" title="Polygraph">
Polygraph

A polygraph, popularly referred to as a lie detector, measures and records several physiological indices such as blood pressure, pulse, respiration, and skin conductivity while the subject is asked and answers a series of questions. The belief underpinning the use of the polygraph is that deceptive answers will produce physiological responses that can be differentiated from those associated with non-deceptive answers; the polygraph is one of several devices used for lie detection.
The polygraph was invented in 1921 by John Augustus Larson, a medical student at the University of California at Berkeley and a police officer of the Berkeley Police Department in Berkeley, California. The polygraph was on the "Encyclopædia Britannica" 2003 list of greatest inventions, described as inventions that "have had profound effects on human life for better or worse."
The efficacy of polygraphs is debated in the scientific community. In 2001, a significant fraction of the scientific community considered polygraphy to be pseudoscience. In 2002, a review by the National Academies of Science found that in populations untrained in countermeasures, polygraph testing can discriminate lying from truth telling at rates above chance, though below perfection. These results apply only to specific events and not to screening where it is assumed that polygraph would work less well. Effectiveness may also be worsened by countermeasures.
In some countries polygraphs are used as an interrogation tool with criminal suspects or candidates for sensitive public or private sector employment. US law enforcement and federal government agencies such as the FBI and the CIA and many police departments such as the LAPD use polygraph examinations to interrogate suspects and screen new employees. Within the US federal government, a polygraph examination is also referred to as a psychophysiological detection of deception (PDD) examination.
Polygraph testing is designed to analyze the physiological reactions of subjects. However, research has indicated that there is no specific physiological reaction associated with lying and that the brain activity and mechanisms associated with lying are unknown, making it difficult to identify factors that separate liars from truth tellers. Polygraph examiners also prefer to use their own individual scoring method, as opposed to computerized techniques, as they may more easily defend their own evaluations.
The validity of polygraph testing is again called in to question with the relevant-irrelevant testing technique, designed to gauge reactions of subjects against crime questions and other non-crime related questions. Studies have indicated that this questioning technique is not ideal, as many innocent subjects exert a heightened physiological reaction to the crime relevant questions.
The control question test, also known as the probable lie test, was developed to combat the issues with the relevant-irrelevant testing method. Although the relevant questions in the probable lie test are used to obtain a reaction from liars, it can also gain a reaction from the innocent subject who is afraid of false detection. The physiological reactions that "distinguish" liars, may also occur in individuals who fear a false detection, or feel passionately that they did not commit the crime. Therefore, although a physiological reaction may be occurring, the reasoning behind the response may be different. Further examination of the probable lie test has indicated that it is biased against innocent subjects. Those who are unable to think of a lie related to the relevant question, will automatically fail the test.
Polygraph examiners, or polygraphers, are licensed or regulated in some jurisdictions. The American Polygraph Association sets standards for courses of training of polygraph operators, though it does not certify individual examiners.
Testing procedure.
The examiner typically begins polygraph test sessions with a pre-test interview to gain some preliminary information which will later be used to develop diagnostic questions. Then the tester will explain how the polygraph is supposed to work, emphasizing that it can detect lies and that it is important to answer truthfully. Then a "stim test" is often conducted: the subject is asked to deliberately lie and then the tester reports that he was able to detect this lie. Guilty subjects are likely to become more anxious when they are reminded of the test's validity. However, there are risks of innocent subjects being equally or more anxious than the guilty. Then the actual test starts. Some of the questions asked are "irrelevant" or IR ("Is your name Fred?"), others are "diagnostic" questions, and the remainder are the "relevant questions", or RQ, that the tester is really interested in. The different types of questions alternate. The test is passed if the physiological responses to the diagnostic questions are larger than those during the relevant questions (RQ).
Criticisms have been given regarding the validity of the administration of the Control Question Technique (CQT). The CQT may be vulnerable to being conducted in an interrogation-like fashion. This kind of interrogation style would elicit a nervous response from innocent and guilty suspects alike. There are several other ways of administrating the questions.
An alternative is the Guilty Knowledge Test (GKT), or the Concealed Information Test (CIT), which is being used in Japan. The administration of this test is given to prevent potential errors that may arise from the questioning style. The test is usually conducted by a tester with no knowledge of the crime or circumstances in question. The administrator tests the participant on their knowledge of the crime that would not be known to an innocent person. For example: "Was the crime committed with a .45 or a 9 mm?" The questions are in multiple choice and the participant is rated on how they react to the correct answer. If they react strongly to the guilty information, then proponents of the test believe that it is likely that they know facts relevant to the case. This administration is considered more valid by supporters of the test because it contains many safeguards to avoid the risk of the administrator influencing the results.
Validity.
Polygraphy is widely criticized. Despite claims of 90% validity by polygraph advocates, the National Research Council has found no evidence of effectiveness. The utility among sex offenders is also poor with insufficient evidence to support accuracy or improved outcomes in this population.
Even using the high estimates of the polygraph's accuracy, false positives occur, and these people suffer the consequences of "failing" the polygraph. In the 1998 Supreme Court case, "United States v. Scheffer", the majority stated that "There is simply no consensus that polygraph evidence is reliable" and "Unlike other expert witnesses who testify about factual matters outside the jurors' knowledge, such as the analysis of fingerprints, ballistics, or DNA found at a crime scene, a polygraph expert can supply the jury only with another opinion..." In 2005 the 11th Circuit Court of Appeals stated that "polygraphy did not enjoy general acceptance from the scientific community". In 2001 William Iacono, Professor of Psychology and Neuroscience concluded that
Although the CQT Question Test may be useful as an investigative aid and tool to induce confessions, it does not pass muster as a scientifically credible test. CQT theory is based on naive, implausible assumptions indicating (a) that it is biased against innocent individuals and (b) that it can be beaten simply by artificially augmenting responses to control questions. Although it is not possible to adequately assess the error rate of the CQT, both of these conclusions are supported by published research findings in the best social science journals (Honts et al., 1994; Horvath, 1977; Kleinmuntz & Szucko, 1984; Patrick & Iacono, 1991). Although defense attorneys often attempt to have the results of friendly CQTs admitted as evidence in court, there is no evidence supporting their validity and ample reason to doubt it. Members of scientific organizations who have the requisite background to evaluate the CQT are overwhelmingly skeptical of the claims made by polygraph proponents.
Summarizing the consensus in psychological research, professor David W. Martin, PhD, from North Carolina State University, states that people have tried to use the polygraph for measuring human emotions, but there is simply no royal road to (measuring) human emotions. Therefore, since one cannot reliably measure human emotions (especially when one has an interest in hiding his/her emotions), the idea of valid detection of truth or falsehood through measuring respiratory rate, blood volume, pulse rate and galvanic skin response is a mere pretense. Psychologists cannot ascertain what emotions one has, with or without the use of polygraph.
Polygraphs measure arousal, which can be caused by anxiety, anxiety disorders such as PTSD, nervousness, fear, confusion, hypoglycemia, psychosis, depression, substance induced (nicotine, stimulants), substance withdrawal state (alcohol withdrawal) or other emotions; polygraphs do not measure "lies." A polygraph cannot differentiate anxiety caused by dishonesty and anxiety caused by something else.
The polygraph is inherently subjective. It relies heavily on interpretation by the examiner, so human error (which could be caused by examiner inexperience) and bias can result in the examiner drawing the wrong conclusion.
National Academy of Sciences.
The accuracy of the polygraph has been contested almost since the introduction of the device. In 2003, the National Academy of Sciences (NAS) issued a report entitled "The Polygraph and Lie Detection". The NAS found that the majority of polygraph research was "unreliable, unscientific and biased", concluding that 57 of the approximately 80 research studies that the American Polygraph Association relies on to come to their conclusions were significantly flawed. These studies did show that specific-incident polygraph testing, in a person untrained in counter-measures, could discern the truth at "a level greater than chance, yet short of perfection". However, due to several flaws, the levels of accuracy shown in these studies "are almost certainly higher than actual polygraph accuracy of specific-incident testing in the field".
When polygraphs are used as a screening tool (in national security matters and for law enforcement agencies for example) the level of accuracy drops to such a level that "Its accuracy in distinguishing actual or potential security violators from innocent test takers is insufficient to justify reliance on its use in employee security screening in federal agencies." The NAS concluded that the polygraph "...may have some utility" but that there is "little basis for the expectation that a polygraph test could have extremely high accuracy.".
The NAS conclusions paralleled those of the earlier United States Congress Office of Technology Assessment report "Scientific Validity of Polygraph Testing: A Research Review and Evaluation". Similarly, a report to Congress by the Moynihan Commission on Government Secrecy on national security concluded that " The few Government-sponsored scientific research reports on polygraph validity (as opposed to its utility), especially those focusing on the screening of applicants for employment,
indicate that the polygraph is neither scientifically valid nor especially effective beyond its ability to generate admissions..".
Countermeasures.
Several countermeasures designed to pass polygraph tests have been described. Asked how he passed the polygraph test, Aldrich Ames explained that he sought advice from his Soviet handler and received the simple instruction to: "Get a good night's sleep, and rest, and go into the test rested and relaxed. Be nice to the polygraph examiner, develop a rapport, and be cooperative and try to maintain your calm." Additionally, Ames explained, "There's no special magic...Confidence is what does it. Confidence and a friendly relationship with the examiner...rapport, where you smile and you make him think that you like him."
Other suggestions for countermeasures include for the subject to mentally record the control and relevant questions as the examiner reviews them prior to commencing the interrogation. Once the interrogation begins, the subject is then supposed to carefully control their breathing during the relevant questions, and to try to artificially increase their heart rate during the control questions, such as by thinking of something scary or exciting or by pricking themselves with a pointed object concealed somewhere on their body. In this way the results will not show a significant reaction to any of the relevant questions.
There are two types of countermeasures: General State (intending to alter the physiological or psychological state of the examinee for the length of the test), and Specific Point (intending to alter the physiological or psychological state of the examinee at specific periods during the examination, either to increase or decrease responses during critical examination periods).
Usage.
Law enforcement agencies and intelligence agencies in the United States are by far the biggest users of polygraph technology. In the United States alone most federal law enforcement agencies either employ their own polygraph examiners or use the services of examiners employed in other agencies. In 1978 Richard Helms, the 8th Director of Central Intelligence, stated that:<br>
Susan McCarthy of "Salon" said in 2000 that "The polygraph is an American phenomenon, with limited use in a few countries, such as Canada, Israel and Japan"
North America.
Canada.
In Canada, the 1987 decision of "R. v. Béland", the Supreme Court of Canada rejected the use of polygraph results as evidence in court. This decision did not, however, affect the use of the polygraph in criminal investigations. The polygraph is regularly used as a forensic tool in the investigation of criminal acts and sometimes employed in the screening of employees for government organizations.
In the province of Ontario, the use of polygraphs is not permitted by the use of an employer. A police force does have the authorization to use a polygraph in the course of the investigation of an offence.
United States.
, polygraph testimony was admitted by stipulation in 19 states, and was subject to the discretion of the trial judge in federal court. The use of polygraph in court testimony remains controversial, although it is used extensively in post-conviction supervision, particularly of sex offenders. In "Daubert v. Merrell Dow Pharmaceuticals" (1993), the old Frye standard was lifted and all forensic evidence, including polygraph, had to meet the new Daubert standard in which "underlying reasoning or methodology is scientifically valid and properly can be applied to the facts at issue." While polygraph tests are commonly used in police investigations in the US, no defendant or witness can be forced to undergo the test. In "United States v. Scheffer" (1998), the U.S. Supreme Court left it up to individual jurisdictions whether polygraph results could be admitted as evidence in court cases. Nevertheless, it is used extensively by prosecutors, defense attorneys, and law enforcement agencies. In the States of Massachusetts, Maryland, New Jersey, Delaware and Iowa it is illegal for any employer to order a polygraph either as conditions to gain employment, or if an employee has been suspected of wrongdoing. The Employee Polygraph Protection Act of 1988 (EPPA) generally prevents employers from using lie detector tests, either for pre-employment screening or during the course of employment, with certain exemptions. As of 2013, about 70,000 job applicants are polygraphed by the federal government on an annual basis.
In the United States, the State of New Mexico admits polygraph testing in front of juries under certain circumstances. In many other states, polygraph examiners are permitted to testify in front of judges in various types of hearings (Motion to Revoke Probation, Motion to Adjudicate Guilt).
In 2010 the NSA produced a video explaining its polygraph process. The video, ten minutes long, is titled "The Truth About the Polygraph" and was posted to the website of the Defense Security Service. Jeff Stein of the "Washington Post" said that the video portrays "various applicants, or actors playing them -- it’s not clear -- describing everything bad they had heard about the test, the implication being that none of it is true." AntiPolygraph.org argues that the NSA-produced video omits some information about the polygraph process; it produced a video responding to the NSA video. George Maschke, the founder of the website, accused the NSA polygraph video of being "Orwellian".
In 2013 the U.S. federal government had begun indicting individuals who stated that they were teaching methods on how to defeat a polygraph test. During one of those investigations, upwards of 30 federal agencies were involved in investigations of almost 5000 people who had various degrees of contact with those being prosecuted or who had purchased books or DVDs on the topic of beating polygraph tests.
Asia.
India.
In 2008, an Indian court adopted the brain electrical oscillations signature test as evidence to convict a woman, who was accused of murdering her fiance. It is the first time that the result of polygraph was used as evidence in court.
On May 5, 2010, The Supreme Court of India declared use of narcoanalysis, brain mapping and polygraph tests on suspects as illegal and against the constitution. Article 20(3) of the Indian Constitution-"No person accused of any offence shall be compelled to be a witness against himself." Polygraph tests are still legal if the defendant requests one.
Israel.
The Supreme Court of Israel, in Civil Appeal 551/89 (Menora Insurance Vs. Jacob Sdovnik), ruled that as the polygraph has not been recognized as a reliable device. In other decisions, polygraph results were ruled inadmissible in criminal trials. It has not been clearly decided if polygraph results are inadmissible as evidence in a civil trial.
In the private discipline, some insurance companies attempt to include a clause in insurance contracts, in which the beneficiary agrees that polygraph results be admissible as evidence. In such cases, where the beneficiary has willingly agreed to such a clause, signed the contract, and taken the test, the courts will honor the contract, and take the polygraph results into consideration. However, it is common practice for lawyers to advise people who signed such contracts to refuse to take the test. Depending on whether or not the beneficiary signed an agreements clause, and whether the test was already taken or not, such a refusal usually has no ill effects; at worst, the court will simply order the person to take the test as agreed. At best, the court will cancel the clause and release the person from taking the test, or rule the evidence inadmissible.
Europe.
In most European jurisdictions, polygraphs are not considered reliable evidence and are not generally used by law enforcement. Courts themselves do not order or facilitate polygraph testing. In most cases, polygraph tests are voluntarily taken by a defendant in order to substantiate his or her defense.
Germany.
The Federal Court of Justice of Germany has ruled that polygraph evidence is inherently inconclusive and not admissible in court. Motions by prosecution or defense for polygraph tests to be exercised will be declined under any circumstance.
Lithuania.
The polygraph was used for the first time in Lithuania in 1992. From the 2004 surveys on the use of the Ewent Knowledge Test (new version of the Concealed Information Test) for criminal investigation.
Oceania.
Australia.
The High Court of Australia has not yet considered the admissibility of polygraph evidence. However, the New South Wales District Court rejected the use of the device in a criminal trial. In "Raymond George Murray" 1982 7A Crim R48 Sinclair DCJ refused to admit polygraph evidence tending to support the defense. The judge rejected the evidence because
The Court cited, with approval, the Canadian case of "Phillion v R" 1978 1SCR 18.
Lie detector evidence is currently inadmissible in New South Wales courts under the "Lie Detectors Act". Under the same act, it is also illegal to use lie detectors for the purpose of granting employment, insurance, financial accommodation, and several other purposes for which lie detectors may be used in other jurisdictions.
Security clearances.
Contrary to popular belief, a security clearance may not be revoked based solely on polygraph results. However, a person's access to sensitive information may be denied if the polygraph results are not favorable. In addition, persons being considered for a government position or job may be denied the employment, if the position specifically requires successful completion of a polygraph examination. It is difficult to precisely determine the effectiveness of polygraph results for the detection or deterrence of spying. It is inadmissible as evidence in most federal courts and military courts martial. The polygraph is more often used as a deterrent to espionage rather than detection. One exception to this was the case of Harold James Nicholson, a CIA employee later convicted of spying for Russia. In 1995, Nicholson had undergone his periodic five year reinvestigation where he showed a strong probability of deception on questions regarding relationships with a foreign intelligence unit. This polygraph test later launched an investigation which resulted in his eventual arrest and conviction. In most cases, however, polygraphs are more of a tool to "scare straight" those who would consider espionage. Jonathan Pollard was advised by his Israeli handlers that he was to resign his job from American intelligence if he was ever told he was subject to a polygraph test. Likewise, John Anthony Walker was advised to by his handlers not to engage in espionage until he had been promoted to the highest position for which a polygraph test was not required, to refuse promotion to higher positions for which polygraph tests were required, and to retire when promotion was mandated. As part of his plea bargain agreement for his case of espionage for the Soviet Union, Robert Hanssen would be made to undergo a polygraph at any time as part of damage assessment. In Hanssen's 25-year career with the FBI, not once was he made to undergo a polygraph. He later said that if he had been ordered to, he may have thought twice about espionage.
Alternatively, the use of polygraph testing, where it causes desperation over dismissal for past dishonesty, may encourage spying. For example, Edward Lee Howard was dismissed from the CIA when, during a polygraph screening, he truthfully answered a series of questions admitting to minor crimes such as petty theft and drug abuse. In retaliation for his perceived unjust punishment for minor offenses, he later sold his knowledge of CIA operations to the Soviet Union.
It is also worth noting that polygraph tests may not deter espionage. From 1945 to the present, at least six Americans had been committing espionage while they successfully passed polygraph tests. Two of the most notable cases of two men who created a false negative result with the polygraphs were Larry Wu-Tai Chin and Aldrich Ames. Ames was given two polygraph examinations while with the CIA, the first in 1986 and the second in 1991. The CIA reported that he passed both examinations after experiencing initial indications of deception. According to a Senate investigation, an FBI review of the first examination concluded that the indications of deception were never resolved. The Senate committee reported that the second examination, at a time when Ames was under suspicion, resulted in indications of deception and a retest a few days later with a different examiner. The second examiner concluded that there were no further indications of deception. In the CIA's analysis of the second exam, they were critical of their own failure to convey to their examiner the existing suspicions that were not addressed in the examination. Also, Cuban spy, Ana Belen Montes passed a counterintelligence scope polygraph test administered by DIA in 1994.
Despite these errors, in August 2008, the US Defense Intelligence Agency announced that it would subject each of its 5,700 prospective and current employees to polygraph testing at least once annually. This expansion of polygraph screening at DIA occurred while DIA polygraph managers ignored documented technical problems discovered in the Lafayette computerized polygraph system. DIA uses computerized Lafayette polygraph systems for routine counterintelligence testing. The impact of the technical flaws within the Lafayette system on the analysis of recorded physiology and on the final polygraph test evaluation is currently unknown.
In 2012, a McClatchy investigation found that the National Reconnaissance Office was possibly breaching ethical and legal boundaries by encouraging its polygraph examiners to extract personal and private information from DoD personnel during polygraph tests that purported to be limited to counterintelligence issues. Allegations of abusive polygraph practices were brought forward by former NRO polygraph examiners.
Alternative tests.
Most polygraph researchers have focused more the exam's predictive value on a subject’s guilt. However, there have been no empirical theories established to explain how a polygraph measures deception. Recent research indicates that Functional Magnetic Resonance Imaging (fMRI) may benefit in explaining the psychological correlations of polygraph exams. It could also explain which parts of the brain are active when subjects use artificial memories. Most brain activity occurs in both sides of the prefrontal cortex, which is linked to response inhibition. This indicates that deception may involve inhibition of truthful responses. Recalling artificial memories are known to activate the posterior cingulate cortex. However, fMRIs are limited by being expensive, immobile, and having inconsistent lying responses. Some researchers believe that reaction time (RT) based tests may replace polygraphs in concealed information detection. RT based tests differ from polygraphs in stimulus presentation duration, and can be conducted without physiological recording as subject response time is measured via computer. However, researchers have found limitations to these tests as subjects voluntarily control their reaction time, deception can still occur within the response deadline, and the test itself lacks physiological recording.
History.
Earlier societies utilized elaborate methods of lie detection which mainly involved torture; for instance, the Middle Ages used boiling water to detect liars as it was believed honest men would withstand it better than liars. Early devices for lie detection include an 1895 invention of Cesare Lombroso used to measure changes in blood pressure for police cases, a 1904 device by Vittorio Benussi used to measure breathing, and an abandoned project by American William Marston which used blood pressure to examine German prisoners of war (POWs). Marston’s machine indicated a strong positive correlation between systolic blood pressure and lying.
Marston wrote a second paper on the concept in 1915, when finishing his undergraduate studies. He entered Harvard Law School and graduated in 1918, re-publishing his earlier work in 1917. Marston's main inspiration for the device was his wife, Elizabeth Holloway Marston. "According to Marston’s son, it was his mother Elizabeth, Marston’s wife, who suggested to him that 'When she got mad or excited, her blood pressure seemed to climb'" (Lamb, 2001). Although Elizabeth is not listed as Marston’s collaborator in his early work, Lamb, Matte (1996), and others refer directly and indirectly to Elizabeth’s work on her husband’s deception research. She also appears in a picture taken in his polygraph laboratory in the 1920s (reproduced in Marston, 1938)."
Despite his predecessor's contributions, Marston styled himself the "father of the polygraph" .
Marston remained the device's primary advocate, lobbying for its use in the courts. In 1938 he published a book, "The Lie Detector Test," wherein he documented the theory and use of the device. In 1938 he appeared in advertising by the Gillette company claiming that the polygraph showed Gillette razors were better than the competition.
A device recording both blood pressure and breathing was invented in 1921 by Dr. John Augustus Larson of the University of California and first applied in law enforcement work by the Berkeley Police Department under its nationally renowned police chief August Vollmer. Further work on this device was done by Leonarde Keeler. As Larson's protege, Keeler updated the device by making it portable and added the galvanic skin response to it in 1939. His device was then purchased by the FBI, and served as the prototype of the modern polygraph.
Several devices similar to Keeler's polygraph version included the Berkeley Psychograph, a blood pressure-pulse-respiration recorder developed by C. D. Lee in 1936 and the Darrow Behavior Research Photopolygraph, which was developed and intended solely for behavior research experiments.
A device which recorded muscular activity accompanying changes in blood pressure was developed in 1945 by John E. Reid, who claimed that greater accuracy could be obtained by making these recordings simultaneously with standard blood pressure-pulse-respiration recordings.
Society and culture.
Lie detection has a long history in mythology and fairy tales; the polygraph has allowed modern fiction to use a device more easily seen as scientific and plausible. Notable instances of polygraph usage include uses in crime and espionage themed television shows and some daytime television talk shows, cartoons and films. The most notable polygraph TV show is "Lie Detector", which first aired in the 1950s created and hosted by Ralph Andrews. In the 1960s Andrews produced a series of specials hosted by Melvin Belli. In the 1970s the show was hosted by Jack Anderson. In 1998 TV producer Mark Phillips with his Mark Phillips Philms & Telephision put "Lie Detector" back on the air on the FOX Network—on that program Dr. Ed Gelb with host Marcia Clark questioned Mark Fuhrman about the allegation that he "planted the bloody glove". Later Phillips produced "Lie Detector" as a series for PAX/ION—some of the guests included Paula Jones, Reverend Paul Crouch accuser Lonny Ford, Ben Rowling, Jeff Gannon and Swift Boat Vet, Steve Garner.
FOX has taken this one step further with their game show "The Moment of Truth" which pits people's honesty against their own sense of modesty, propriety and other values. Contestants are given a polygraph test administered by a polygraph expert in a pre-screening session during which they answer over 50 questions. Later, they must sit in front of a studio audience that including their friends and family for the televised portion of the show. They need only answer 21 answers truthfully "as determined by the polygraph" to win $500,000. The questions get more personal or revealing as they advance. Most polygraph experts caution that the techniques used on "Moment of Truth" do not conform to accepted methods of polygraphy.
Daytime talk shows, such as Maury Povich and Steve Wilkos, frequently use polygraphs to tell if someone is cheating on their significant other.
In one "MacGyver" episode 'Slow Death', MacGyver assists the Indian tribesmen by improvising a polygraph to weed out the crooked doctor. This is made possible by using an analog sphygmomanometer to monitor blood pressure change, and an electronic alarm clock to detect sweat. To test its reliability, MacGyver asked a passenger on the train a few 'placebo' questions. The culprit was only discovered when he was trying to hide his crime, thus his sweat triggered the alarm clock and blood pressure climbed up.
In the movie "Harsh Times" the protagonist, played by actor Christian Bale, is caught trying to "beat" a polygraph test during a pre-employment screening for a federal law enforcement job. He stores a tack in the toe of his shoe and uses the pain sensation to mask his true apprehension of certain questions. The polygrapher is immediately suspicious and threatens to terminate the test.
In the movie "Ocean's 13", one of the characters beats a polygraph test by stepping on a tack when answering truthfully, which supposedly raises the polygraph's readings for the truthful answers so they equal the deceptive ones.
In the television series "Profit", there is a memorable sequence at the end of episode "Healing" where the eponymous character, Jim Profit, manages to fool a polygraph. He does that by putting a nail through the sole of his shoe and pushing it inside of his heel while answering every question in order to even out the readings. This scene is very graphic, especially for its time, 1996. During a voice over, Profit explains the theory behind the polygraph and the flaws he intends to exploit in it.
In the 20th episode of "The Americans", "Arpanet", Nina, a KGB double agent, misleads her FBI handler after receiving coaching on how to beat the polygraph from Oleg, her Soviet superior. Oleg describes the machine as being similar to a camera in that it doesn't know if the subject's smiles convey genuine happiness. He also uses the asp killing Cleopatra as a metaphor, stating it only killed her when she moved. One technique suggested to Nina is visualizing her KGB superior in the room, as well as clenching her anus. She appears to utilize and benefit from these techniques as she passes the test.
In episode 93 of the USA popular science show "MythBusters", they attempted to fool the polygraph by using pain to try to increase the readings when answering truthfully (so the machine will supposedly interpret the truthful and non-truthful answers as the same.) They also attempted to fool the polygraph by thinking happy thoughts when lying and thinking stressful thoughts when telling the truth to try to confuse the machine. However, neither technique was successful for a number of reasons. Michael Martin correctly identified each guilty and innocent subject. The show also noted the opinion that, when done properly, polygraphs are correct 80–99% of the time.
Hand-held lie detector for U.S. military.
A hand-held lie detector is being deployed by the U.S. Department of Defense according to a report in 2008 by investigative reporter Bill Dedman of msnbc.com. The Preliminary Credibility Assessment Screening System, or PCASS, captures less physiological information than a polygraph, and uses an algorithm, not the judgment of a polygraph examiner, to render a decision whether it believes the person is being deceptive or not. The device will be used first in Afghanistan by U.S. Army troops. The Department of Defense ordered its use be limited to non-U.S. persons, in overseas locations only.
Notable cases.
Polygraphy has been faulted for failing to trap known spies such as double-agent Aldrich Ames, who passed two polygraph tests while spying for the Soviet Union. However Ames did fail several tests while at the CIA that were never acted on. Other spies who passed the polygraph include Karl Koecher, Ana Belen Montes, and Leandro Aragoncillo. However, CIA spy Harold James Nicholson failed his polygraph examinations, which aroused suspicions that led to his eventual arrest. Polygraph examination and background checks failed to detect Nada Nadim Prouty, who was not a spy but was convicted for improperly obtaining US citizenship and using it to obtain a restricted position at the FBI. 
The polygraph also failed to catch Gary Ridgway, the "Green River Killer". Another suspect allegedly failed a given lie detector test, whereas Ridgway passed. Ridgway passed a polygraph in 1984; he confessed almost 20 years later when confronted with DNA evidence. In the interim he had killed seven additional women.
Conversely, innocent people have been known to fail polygraph tests. In Wichita, Kansas in 1986, because he failed two polygraph tests (one police administered, the other given by an expert that he had hired), Bill Wegerle had to live under a cloud of suspicion of murdering his wife Vicki Wegerle, although he was neither arrested nor convicted of her death. In March 2004, evidence surfaced connecting her death to the serial killer known as BTK, and in 2005 DNA evidence from the Wegerle murder confirmed that the BTK Killer was Dennis Rader and established Bill Wegerle's innocence.
Prolonged polygraph examinations are sometimes used as a tool by which confessions are extracted from a defendant, as in the case of Richard Miller, who was persuaded to confess largely by polygraph results combined with appeals from a religious leader.
In the high-profile disappearance of 7-year-old Danielle van Dam of San Diego in 2002, police suspected neighbor David Westerfield; he became the prime suspect when he allegedly failed a polygraph test. He was ultimately tried and convicted of kidnapping and first degree murder.

</doc>
<doc id="71735" url="https://en.wikipedia.org/wiki?curid=71735" title="Hugo Steinhaus">
Hugo Steinhaus

Władysław Hugo Dionizy Steinhaus (January 14, 1887 – February 25, 1972) was a Polish mathematician and educator. Steinhaus obtained his PhD under David Hilbert at Göttingen University in 1911 and later became a professor at the Jan Kazimierz University in Lwów (now Lviv, Ukraine), where he helped establish what later became known as the Lwów School of Mathematics. He is credited with "discovering" mathematician Stefan Banach, with whom he gave a notable contribution to functional analysis through the Banach–Steinhaus theorem. After World War II Steinhaus played an important part in the establishment of the mathematics department at Wrocław University and in the revival of Polish mathematics from the destruction of the war.
Author of around 170 scientific articles and books, Steinhaus has left his legacy and contribution in many branches of mathematics, such as functional analysis, geometry, mathematical logic, and trigonometry. Notably he is regarded as one of the early founders of game theory and probability theory which led to later development of more comprehensive approaches by other scholars.
Early life and studies.
Steinhaus was born on January 14, 1887 in Jasło, Austria-Hungary to a family with Jewish roots. His father, Bogusław, was a local industrialist, owner of a brick factory and a merchant. His mother was Ewelina, née Lipschitz. Hugo's uncle, Ignacy Steinhaus, was an activist in the "Koło Polskie" (Polish Circle), and a deputy to the Galician Diet, the regional assembly of the Kingdom of Galicia and Lodomeria.
Hugo finished his studies at the gymnasium in Jasło in 1905. His family wanted him to become an engineer but he was drawn to abstract mathematics and began to study the works of famous contemporary mathematicians on his own. In the same year he began studying philosophy and mathematics at the University of Lemberg. In 1906 he transferred to Göttingen University. At that University he received his Ph.D. in 1911, having written his doctoral dissertation under the supervision of David Hilbert. The title of his thesis was "Neue Anwendungen des Dirichlet'schen Prinzips" ("New applications to Dirichlet's principle").
At the start of World War I Steinhaus returned to Poland and served in Józef Piłsudski's Polish Legion, after which he lived in Kraków.
Academic career.
Interwar Poland.
During the 1916-1917 period and before Poland had regained its full independence, which occurred in 1918, Steinhaus worked in Kraków for the Ministry of the Interior in the ephemeral puppet state of Kingdom of Poland.
In 1917 he started to work at the University of Lemberg (later "Jan Kazimierz University" in Poland) and acquired his habilitation qualification in 1920. In 1921 he became a "profesor nadzwyczajny" (associate professor) and in 1925 "profesor zwyczajny" (full professor) at the same university. During this time he taught a course on the then cutting edge theory of Lebesgue integration, one of the first such courses offered outside France.
While in Lwów, Steinhaus co-founded the Lwów School of Mathematics and was active in the circle of mathematicians associated with the Scottish cafe, although, according to Stanislaw Ulam, for the circle's gatherings, Steinhaus would have generally preferred a more upscale tea shop down the street.
World War II.
In September 1939 after Nazi Germany and the Soviet Union both invaded and occupied Poland, as a fulfillment of the Molotov–Ribbentrop Pact they had signed earlier, Lwów initially came under Soviet occupation. Steinhaus considered escaping to Hungary but ultimately decided to remain in Lwów. The Soviets reorganized the university to give it a more Ukrainian character, but they did appoint Stefan Banach (Steinhaus's student) as the dean of the mathematics department and Steinhaus resumed teaching there. The faculty of the department at the school were also strengthened by several Polish refugees from German occupied Poland. According to Steinhaus, during the experience of this period, he "acquired a insurmountable physical disgust in regard to all sorts of Soviet administrators, politicians and commissars"
During the interwar period and the time of the Soviet occupation, Steinhaus contributed ten problems to the famous "Scottish Book", including the last one, recorded shortly before Lwów was captured by the Nazis in 1941, during Operation Barbarossa.
Steinhaus, because of his Jewish background, spent the Nazi occupation in hiding, first among friends in Lwów, then in the small towns of Osiczyna, near Zamość and Berdechów, near Kraków. The Polish anti-Nazi resistance provided him with false documents of a forest ranger who had died sometime earlier, by the name of Grzegorz Krochmalny. Under this name he taught clandestine classes (higher education was forbidden for Poles under the German occupation). Worried about the possibility of imminent death if captured by Germans, Steinhaus, without access to any scholarly material, reconstructed from memory and recorded all the mathematics he knew, in addition to writing other voluminous memoirs, of which only a little part has been published.
Also while in hiding, and cut off from reliable news on the course of the war, Steinhaus devised a statistical means of estimating for himself the German casualties at the front based on sporadic obituaries published in the local press. The method relied on the relative frequency with which the obituaries stated that the soldier who died was someone's son, someone's "second son", someone's "third son" and so on.
According to his student and biographer, Mark Kac, Steinhaus told him that the happiest day of his life were the twenty four hours between the time that the Germans left occupied Poland and the Soviets had not yet arrived ("They had left, and they had not yet come").
After World War II.
In the last days of World War II Steinhaus, still in hiding, heard a rumor that University of Lwów was to be transferred to the city of Breslau (Wrocław), which Poland was to acquire as a result of the Potsdam agreement (Lwów became part of Soviet Ukraine). Although initially he had doubts, he turned down offers for faculty positions in Łódź and Lublin and made his way to the city where he began teaching at University of Wrocław. While there, he revived the idea behind the "Scottish Book" from Lwów, where prominent and aspiring mathematicians would write down problems of interest along with prizes to be awarded for their solution, by starting the "New Scottish Book". It was also most likely Steinhaus who preserved the original "Scottish Book" from Lwów throughout the war and subsequently sent it to Stanisław Ulam, who translated it into English.
With Steinhaus' help, Wrocław University became renowned for mathematics, much as the University of Lwów had been.
Later, in the 1960s, Steinhaus served as a visiting professor at the University of Notre Dame (1961–62) and the University of Sussex (1966).
Mathematical contributions.
Steinhaus authored over 170 works. Unlike his student, Stefan Banach, who tended to specialize narrowly in the field of functional analysis, Steinhaus made contributions to a wide range of mathematical sub-disciplines, including geometry, probability theory, functional analysis, theory of trigonometric and Fourier series as well as mathematical logic. He also wrote in the area of applied mathematics and enthusiastically collaborated with engineers, geologists, economists, physicians, biologists and, in Kac's words, "even lawyers".
Probably his most notable contribution to functional analysis was the 1927 proof of the Banach–Steinhaus theorem, given along with Stefan Banach, which is now one of the fundamental tools in this branch of mathematics.
His interest in games led him to propose an early formal definition of a strategy, anticipating John von Neumann's more complete treatment of a few years later. Consequently, he is considered an early founder of modern game theory. As a result of his work on infinite games Steinhaus, together with another of his students, Jan Mycielski, proposed the Axiom of determinacy.
Steinhaus was also an early contributor to, and co-founder of, probability theory, which at the time was in its infancy and not even considered an actual part of mathematics. He provided the first axiomatic measure-theoretic description of coin-tossing, which was to influence the full axiomatization of probability by the Russian mathematician Andrey Kolmogorov a decade later. Steinhaus was also the first to offer precise definitions of what it means for two events to be "independent", as well as for what it means for a random variable to be "uniformly distributed".
While in hiding during World War II, Steinhaus worked on the fair cake-cutting problem: how to divide a heterogeneous resource among several people with different preferences such that every person believes he received a proportional share. Steinhaus' work has initiated the modern research of the fair cake-cutting problem.
Steinhaus was also the first person to conjecture the ham-sandwich theorem, and one of the first to propose the method of k-means clustering.
Legacy.
Steinhaus is said to have "discovered" the Polish mathematician Stefan Banach in 1916, after he overheard someone utter the words "Lebesgue integral" while in a Kraków park (Steinhaus referred to Banach as his "greatest mathematical discovery"). Together with Banach and the other participant of the park discussion, Otto Nikodym, Steinhaus started the "Mathematical Society of Kraków", which later evolved into the Polish Mathematical Society. He was a member of "PAU" (the Polish Academy of Learning) and "PAN" (the Polish Academy of Sciences), "PTM" (the Polish Mathematical Society), the "Wrocławskie Towarzystwo Naukowe" (Wrocław Scientific Society) as well as many international scientific societies and science academies.
Steinhaus also published one of the first articles in "Fundamenta Mathematicae", in 1921. He also co-founded "Studia Mathematica" along with Stefan Banach (1929), and "Zastosowania matematyki" (Applications of Mathematics, 1953), "Colloquium Mathematicum", and "Monografie Matematyczne" (Mathematical Monographs).
He received honorary doctorate degrees from Warsaw University (1958), Wrocław Medical Academy (1961), Poznań University (1963) and Wrocław University (1965).
Steinhaus had full command of several foreign languages and was interestingly, known, for his aphorisms, to the point that a booklet of his most famous ones in Polish, French and Latin has been published posthumously.
In 2002, the Polish Academy of Sciences and Wrocław University sponsored "2002, The Year of Hugo Steinhaus", to celebrate his contributions to Polish and world science.
Notable mathematician Mark Kac, Steinhaus's student, wrote:

</doc>
<doc id="71750" url="https://en.wikipedia.org/wiki?curid=71750" title="Fruit machine (homosexuality test)">
Fruit machine (homosexuality test)

"Fruit machine" is a term for a device developed in Canada that was supposed to be able to identify gay men (derogatorily referred to as "fruits"). The subjects were made to view pornography, and the device measured the diameter of the pupils of the eyes (pupillary response test), perspiration, and pulse for a supposed erotic response.
The "fruit machine" was employed in Canada in the 1950s and 1960s during a campaign to eliminate all gay men from the civil service, the Royal Canadian Mounted Police (RCMP), and the military. A substantial number of workers did lose their jobs. Although funding for the "fruit machine" project was cut off in the late 1960s, the investigations continued, and the RCMP collected files on over 9,000 "suspected" gay people.
The chair was like one from a dentist's office. It had a pulley with a camera going towards the pupils. There was a black box in front of it that showed pictures. The pictures ranged from the mundane to sexually explicit photos of men and women. It had previously been determined that the pupils would dilate in relation to the amount of interest in the picture. This was called the pupillary response test. 
People were told the machine was to rate stress. After knowledge of its real purpose became widespread, few people volunteered for it.
Faulty test parameters.
There were many problems with the "fruit machine." To begin with, the pupillary response test was based on fatally flawed assumptions: that visual stimuli would give an involuntary reaction able to be measured scientifically; that homosexuals and heterosexuals would respond to these stimuli differently; and that there were only two types of sexuality. There was also the problem of physiology. The researchers failed to take into account the varying sizes of the pupils and the differing distances between the eyes. Other problems that existed were that the pictures of the subjects' eyes had to be taken from an angle, as the camera would have blocked the subjects' view of the photographs if it were placed directly in front. Also, the amount of light coming from the photographs changed with each slide, causing the subjects' pupils to dilate in a way that was unrelated to their interest in the picture. Finally, the dilation of the pupils was also exceedingly difficult to measure, as the change was often smaller than one millimeter.
The idea was based on a study done by an American university professor, which measured the sizes of the subjects' pupils as they walked through the aisles of grocery stores.
In popular culture.
Brian Drader's 1998 play "The Fruit Machine" juxtaposes the fruit machine project with a parallel storyline about contemporary homophobia.

</doc>
<doc id="71753" url="https://en.wikipedia.org/wiki?curid=71753" title="Tuskegee Airmen">
Tuskegee Airmen

The Tuskegee Airmen 
is the popular name of a group of African-American military pilots (fighter and bomber) who fought in World War II. Officially, they formed the 332nd Fighter Group and the 477th Bombardment Group of the United States Army Air Forces. The name also applies to the navigators, bombardiers, mechanics, instructors, crew chiefs, nurses, cooks and other support personnel for the pilots.
The Tuskegee Airmen were the first African-American military aviators in the United States Armed Forces. During World War II, black Americans in many U.S. states were still subject to the Jim Crow laws and the American military was racially segregated, as was much of the federal government. The Tuskegee Airmen were subjected to discrimination, both within and outside the army. All black military pilots who trained in the United States trained at Moton Field, the Tuskegee Army Air Field, and were educated at Tuskegee University, located near Tuskegee, Alabama; the group included five Haitians from the Haitian Air Force (Alix Pasquet, Raymond Cassagnol, Pelissier Nicolas, Ludovic Audant, and Eberle Guilbaud). There was also one pilot from Port of Spain, Trinidad, Eugene Theodore.
Although the 477th Bombardment Group trained with North American B-25 Mitchell bombers, they never served in combat. The 99th Pursuit Squadron (later, 99th Fighter Squadron) was the first black flying squadron, and the first to deploy overseas (to North Africa in April 1943, and later to Sicily and Italy). The 332nd Fighter Group, which originally included the 100th, 301st, and 302nd Fighter Squadrons, was the first black flying group. The group deployed to Italy in early 1944. In June 1944, the 332nd Fighter Group began flying heavy bomber escort missions, and in July 1944, the 99th Fighter Squadron was assigned to the 332nd Fighter Group, which then had four fighter squadrons.
The 99th Fighter Squadron was initially equipped with Curtiss P-40 Warhawk fighter-bomber aircraft. The 332nd Fighter Group and its 100th, 301st and 302nd Fighter Squadrons were equipped for initial combat missions with Bell P-39 Airacobras (March 1944), later with Republic P-47 Thunderbolts (June–July 1944), and finally with the aircraft with which they became most commonly associated, the North American P-51 Mustang (July 1944). When the pilots of the 332nd Fighter Group painted the tails of their P-47s and later, P-51s, red, the nickname "Red Tails" was coined. The red markings that distinguished the Tuskegee Airmen included red bands on the noses of P-51s as well as a red rudder, the P-51B and D Mustangs flew with similar color schemes, with red propeller spinners, yellow wing bands and all-red tail surfaces.
Origins.
Background.
Before the Tuskegee Airmen, no African American had been a U.S. military pilot. In 1917, African-American men had tried to become aerial observers, but were rejected. African American Eugene Bullard served in the French air service during World War I, because he was not allowed to serve in an American unit. Instead, Bullard returned to infantry duty with the French.
The racially motivated rejections of World War I African-American recruits sparked over two decades of advocacy by African Americans who wished to enlist and train as military aviators. The effort was led by such prominent civil rights leaders as Walter White of the National Association for the Advancement of Colored People, labor union leader A. Philip Randolph, and Judge William H. Hastie. Finally, on 3 April 1939, Appropriations Bill Public Law 18 was passed by Congress containing an amendment designating funds for training African-American pilots. The War Department managed to put the money into funds of civilian flight schools willing to train black Americans.
War Department tradition and policy mandated the segregation of African Americans into separate military units staffed by white officers, as had been done previously with the 9th Cavalry, 10th Cavalry, 24th Infantry Regiment and 25th Infantry Regiment. When the appropriation of funds for aviation training created opportunities for pilot cadets, their numbers diminished the rosters of these older units. In 1941, the War Department and the Army Air Corps, under pressure — three months before its transformation into the USAAF — constituted the first all-black flying unit, the 99th Pursuit Squadron.
Due to the restrictive nature of selection policies, the situation did not seem promising for African Americans since, in 1940, the U.S. Census Bureau reported there were only 124 African-American pilots in the nation. The exclusionary policies failed dramatically when the Air Corps received an abundance of applications from men who qualified, even under the restrictive requirements. Many of the applicants already had participated in the late-December 1938 unveiled Civilian Pilot Training Program (CPTP), Tuskegee University had participated since 1939.
Testing.
The U.S. Army Air Corps had established the Psychological Research Unit 1 at Maxwell Army Air Field, Montgomery, Alabama, and other units around the country for aviation cadet training, which included the identification, selection, education, and training of pilots, navigators, and bombardiers. Psychologists employed in these research studies and training programs used some of the first standardized tests to quantify IQ, dexterity and leadership qualities to select and train the best-suited personnel for the roles of bombardier, navigator, and pilot. The Air Corps determined that the existing programs would be used for all units, including all-black units. At Tuskegee, this effort continued with the selection and training of the Tuskegee Airmen. The War Department set up a system to accept only those with a level of flight experience or higher education which ensured that only the most able and intelligent African-American applicants were able to join.
The First Lady's flight.
The budding flight program at Tuskegee received a publicity boost when First Lady Eleanor Roosevelt inspected it in March 1941, and flew with African-American chief civilian instructor C. Alfred "Chief" Anderson. Anderson, who had been flying since 1929, and was responsible for training thousands of rookie pilots, took his prestigious passenger on a half-hour flight in a Piper J-3 Cub. After landing, she cheerfully announced, "Well, you can fly all right."
The subsequent brouhaha over the First Lady's flight had such an impact it is often mistakenly cited as the start of the CPTP at Tuskegee, even though the program was already five months old. Eleanor Roosevelt used her position as a trustee of the Julius Rosenwald Fund to arrange a loan of $175,000 to help finance the building of Moton Field.
Formation.
On September 11, 1941, the 99th Pursuit Squadron was activated at Chanute Field in Rantoul, Illinois. A cadre of 271 enlisted men was trained in aircraft ground support trades at Chanute, beginning in July 1941; the skills being taught were so technical that setting up segregated classes was deemed impossible. This small number of enlisted men became the core of other black squadrons forming at Tuskegee and Maxwell Fields in Alabama.
The Tuskegee program officially began June 1941 with the 99th Pursuit Squadron at Tuskegee University. The unit consisted of 47 officers and 429 enlisted men, and was backed by an entire service arm. After primary training at Moton Field, they were moved to the nearby Tuskegee Army Air Field, about to the west for conversion training onto operational types. Consequently, Tuskegee Army Air Field became the only Army installation performing three phases of pilot training (basic, advanced, and transition) at a single location. Initial planning called for 500 personnel in residence at a time. By mid-1942, over six times that many were stationed at Tuskegee, even though only two squadrons were training there.
Tuskegee Army Airfield was similar to already-existing airfields reserved for training white pilots, such as Maxwell Field, only distant. African-American contractor McKissack and McKissack, Inc. was in charge of the contract. The company's 2,000 workmen, the Alabama Works Progress Administration, and the U.S. Army built the airfield in only six months. Booker Conley, a student at Tuskegee, drafted the architectural plans for the hangars where aircraft would be housed. The construction was budgeted at $1,663,057. The airmen were placed under the command of Captain Benjamin O. Davis, Jr., one of only two black line officers then serving.
During training, Tuskegee Army Air Field was commanded first by Major James Ellison. Ellison made great progress in organizing the construction of the facilities needed for the military program at Tuskegee. However, he was transferred on 12 January 1942, reputedly because of his insistence that his African-American sentries and Military Police had police authority over local Caucasian civilians.
His successor, Colonel Frederick von Kimble, then oversaw operations at the Tuskegee airfield. Contrary to new Army regulations, Kimble maintained segregation on the field in deference to local customs in the state of Alabama, a policy that was resented by the airmen. Later that year, the Air Corps replaced Kimble. His replacement had been the director of instruction at Tuskegee Army Airfield, Major Noel F. Parrish. Counter to the prevalent racism of the day, Parrish was fair and open-minded and petitioned Washington to allow the Tuskegee Airmen to serve in combat.
The strict racial segregation the U.S. Army required gave way in the face of the requirements for complex training in technical vocations. Typical of the process was the development of separate African-American flight surgeons to support the operations and training of the Tuskegee Airmen. Before the development of this unit, no U.S. Army flight surgeons had been black. Training of African-American men as aviation medical examiners was conducted through correspondence courses until 1943, when two black physicians were admitted to the U.S. Army School of Aviation Medicine at Randolph Field, Texas. This was one of the earliest racially integrated courses in the U.S. Army. Seventeen flight surgeons served with the Tuskegee Airmen from 1941 through 1949. At that time, the typical tour of duty for a U.S. Army flight surgeon was four years. Six of these physicians lived under field conditions during operations in North Africa, Sicily, and Italy. The chief flight surgeon to the Tuskegee Airmen was Vance H. Marchbanks, Jr., M.D., who was a childhood friend of Benjamin Davis.
The accumulation of washed-out cadets at Tuskegee and the propensity of other commands to "dump" African-American personnel on the post exacerbated the difficulties of administering Tuskegee. A shortage of jobs for them made these enlisted men a drag on Tuskegee's housing and culinary departments. Trained officers were also left idle, as the plan to shift African-American officers into command slots stalled, and white officers not only continued to hold command, but were joined by additional white officers assigned to the post. One rationale behind the non-assignment of trained African-American officers was stated by the commanding officer of the Army Air Forces, General Henry "Hap" Arnold: "Negro pilots cannot be used in our present Air Corps units since this would result in Negro officers serving over white enlisted men creating an impossible social situation."
Combat assignment.
The 99th was finally considered ready for combat duty by April 1943. It shipped out of Tuskegee on 2 April, bound for North Africa, where it would join the 33rd Fighter Group and its commander, Colonel William W. Momyer. Given little guidance from battle-experienced pilots, the 99th's first combat mission was to attack the small strategic volcanic island of Pantelleria in the Mediterranean Sea to clear the sea lanes for the Allied invasion of Sicily in July 1943. The air assault on the island began on 30 May 1943. The 99th flew its first combat mission on 2 June. The surrender of the garrison of 11,121 Italians and 78 Germans due to air attack was the first of its kind. The 99th then moved on to Sicily and received a Distinguished Unit Citation for its performance in combat.
By the end of February 1944, more graduates were ready for combat, and the all-black 332nd Fighter Group had been sent overseas with three fighter squadrons: The 100th, 301st and 302nd. Under the command of Colonel Davis, the squadrons were moved to mainland Italy, where the 99th Fighter Squadron, assigned to the group on 1 May 1944, joined them on 6 June at Ramitelli Airfield, near Termoli, on the Adriatic coast. From Ramitelli, the 332nd Fighter Group escorted Fifteenth Air Force heavy strategic bombing raids into Czechoslovakia, Austria, Hungary, Poland and Germany.
Flying escort for heavy bombers, the 332nd earned an impressive combat record. The Allies called these airmen "Red Tails" or "Red-Tail Angels," because of the distinctive crimson unit identification marking predominantly applied on the tail section of the unit's aircraft.
A B-25 bomb group, the 477th Bombardment Group, was forming in the U.S., but was not able to complete its training in time to see action. The 99th Fighter Squadron after its return to the United States became part of the 477th, redesignated the 477th Composite Group.
Active air units.
The only black air units that saw combat during the war were the 99th Pursuit Squadron and the 332nd Fighter Group. The dive-bombing and strafing missions under Lieutenant Colonel Davis, Jr. were considered to be highly successful.
In May 1942, the 99th Pursuit Squadron was renamed the 99th Fighter Squadron. It earned three Distinguished Unit Citations (DUC) during World War II. The DUCs were for operations over Sicily from 30 May – 11 June 1943, Monastery Hill near Cassino from 12–14 May 1944, and for successfully fighting off German jet aircraft on 24 March 1945. The mission was the longest bomber escort mission of the Fifteenth Air Force throughout the war. The 332nd also flew missions in Sicily, Anzio, Normandy, the Rhineland, the Po Valley and Rome-Arno and others. Pilots of the 99th once set a record for destroying five enemy aircraft in under four minutes.
The Tuskegee Airmen shot down three German jets in a single day. On 24 March 1945, 43 P-51 Mustangs led by Colonel Benjamin O. Davis escorted B-17 bombers over into Germany and back. The bombers’ target, a massive Daimler-Benz tank factory in Berlin, was heavily defended by "Luftwaffe" aircraft, included Fw 190 radial propeller fighters, Me 163 "Komet" rocket-powered fighters and 25 of the much more formidable Me 262s, history’s first operational jet fighter. Pilots Charles Brantley, Earl Lane and Roscoe Brown all shot down German jets over Berlin that day. For the mission, the 332nd Fighter Group earned a Distinguished Unit Citation.
Individual pilots of the 332nd Fighter Group earned 96 Distinguished Flying Crosses. Their missions took them over Italy and enemy occupied parts of central and southern Europe. Their operational aircraft were, in succession: Curtiss P-40 Warhawk, Bell P-39 Airacobra, Republic P-47 Thunderbolt and North American P-51 Mustang fighter aircraft.
Tuskegee Airmen bomber units.
Formation.
With African-American fighter pilots being trained successfully, the Army Air Force now came under political pressure from the NAACP and other civil rights organizations to organize a bomber unit. There could be no defensible argument that the quota of 100 African-American pilots in training at one time, or 200 per year out of a total of 60,000 American aviation cadets in annual training, represented the service potential of 13 million African Americans.
On 13 May 1943, the 616th Bombardment Squadron was established as the initial subordinate squadron of the 477th Bombardment Group. The squadron was activated on 1 July 1943, only to be inactivated on 15 August 1943. By September 1943, the number of washed-out cadets on base had surged to 286, with few of them working. In January 1944, the 477th Bombardment Group was reactivated. At the time, the usual training cycle for a bombardment group took three to four months. The 477th would eventually contain four medium bomber squadrons. Slated to comprise 1,200 officers and enlisted men, the unit would operate 60 North American B-25 Mitchell bombers. The 477th would go on to encompass three more bomber squadrons–the 617th Bombardment Squadron, the 618th Bombardment Squadron, and the 619th Bombardment Squadron. The 477th was anticipated to be ready for action in November 1944.
The home field for the 477th was Selfridge Field, located outside Detroit, however, other bases would be used for various types of training courses. Twin-engine pilot training began at Tuskegee while transition to multi-engine pilot training was at Mather Field, California. Some ground crews trained at Mather before rotating to Inglewood, California. Gunners learned to shoot at Eglin Field, Florida. Bombers-navigators learned their trades at Hondo Army Air Field and Midland Air Field, Texas, or at Roswell, New Mexico. Training of the new African-American crewmen also took place at Sioux Falls, South Dakota; Lincoln, Nebraska and Scott Field, Belleville, Illinois. Once trained, the air and ground crews would be spliced into a working unit at Selfridge.
Command difficulties.
The new group's first Commanding Officer was Colonel Robert Selway, who had also commanded the 332nd Fighter Group before it deployed for combat overseas. Like his ranking officer, Major General Frank O'Driscoll Hunter from Georgia, Selway was a racial segregationist. Hunter was blunt about it, saying such things as, "...racial friction will occur if colored and white pilots are trained together." He backed Selway's violations of Army Regulation 210-10, which forbade segregation of air base facilities. They segregated base facilities so thoroughly they even drew a line in the base theater and ordered separate seating by races. When the audience sat in random patterns as part of "Operation Checkerboard", the movie was halted to make men return to segregated seating. African-American officers petitioned base Commanding Officer William Boyd for access to the only officer's club on base. Lieutenant Milton Henry entered the club and personally demanded his club rights; he was court-martialled for this, and discharged.
Subsequently, Colonel Boyd denied club rights to African Americans although General Hunter stepped in and promised a separate but equal club would be built for black airmen. The 477th was transferred to Godman Field, Kentucky before the club was built. They had spent five months at Selfridge but found themselves on a base a fraction of Selfridge's size, with no air-to-ground gunnery range, and deteriorating runways that were too short for B-25 landings. Colonel Selway took on the second role of Commanding Officer of Godman Field. In that capacity, he ceded Godman Field's officer club to African-American airmen. Caucasian officers used the whites-only clubs at nearby Fort Knox, much to the displeasure of African-American officers.
Another irritant was a professional one for African-American officers. They observed a steady flow of white officers through the command positions of the group and squadrons; these officers stayed just long enough to be "promotable" before transferring out at their new rank. This seemed to take about four months. In an extreme example, 22-year-old Robert Mattern was promoted to captain, transferred into squadron command in the 477th days later, and left a month later as a major. He was replaced by another Caucasian officer. Meanwhile, no Tuskegee Airmen held command.
On 15 March 1945, the 477th was transferred to Freeman Field, near Seymour, Indiana. The white population of Freeman Field was 250 officers and 600 enlisted men. Superimposed on it were 400 African-American officers and 2,500 enlisted men of the 477th and its associated units. Freeman Field had a firing range, usable runways, and other amenities useful for training. African-American airmen would work in proximity with white ones; both would live in a public housing project adjacent to the base. Colonel Selway turned the non-commissioned officers out of their club and turned it into a second officers club. He then classified all white personnel as cadre, and all African Americans as trainees. One officers club became the cadre's club. The old Non-Commissioned Officers Club, promptly sarcastically dubbed "Uncle Tom's Cabin", became the trainee's officers club. At least four of the trainees had flown combat in Europe as fighter pilots, and had about four years in service. Four others had completed training as pilots, bombardiers and navigators, and may have been the only triply qualified officers in the entire Air Corps. Several of the Tuskegee Airmen had logged over 900 flight hours by this time. Nevertheless, by Colonel Selway's fiat, they were trainees.
Off-base was no better; many businesses in Seymour would not serve African Americans. A local laundry would not wash their clothes, yet willingly laundered those of captured German soldiers.
In early April 1945, the 118th Base Unit transferred in from Godman Field; its African-American personnel held orders that specified they were base cadre, not trainees. On 5 April, officers of the 477th peaceably tried to enter the whites-only Officer's Club. Selway had been tipped off by a phone call, and had the assistant provost marshal and base billeting manager stationed at the door to refuse the 477th officers entry. The latter, a major, ordered them to leave, and took their names as a means of arresting them when they refused. It was the beginning of the Freeman Field Mutiny.
In the wake of the Freeman Field Mutiny, the 616th and 619th were disbanded and the returned 99th Fighter Squadron assigned to the 477th on 22 June 1945; it was re-designated the 477th Composite Group as a result. On 1 July 1945, Colonel Robert Selway was relieved of the Group's command; he was replaced by Colonel Benjamin O. Davis, Jr. A complete sweep of Selway's white staff followed, with all vacated jobs filled by African-American officers. The war ended before the 477th Composite Group could get into action. The 618th Bombardment Squadron was disbanded on 8 October 1945. On 13 March 1946, the two-squadron group, supported by the 602nd Engineer Squadron (later renamed 602nd Air Engineer Squadron), the 118th Base Unit, and a band, moved to its final station, Lockbourne Field. The 617th Bombardment Squadron and the 99th Fighter Squadron disbanded on 1 July 1947, ending the 477th Composite Group. It would be reorganized as the 332nd Fighter Wing.
War accomplishments.
In all, 992 pilots were trained in Tuskegee from 1941 to 1946. 355 were deployed overseas, and 84 lost their lives in accidents or combat. The toll included 68 pilots killed in action or accidents, 12 killed in training and non-combat missions and 32 captured as prisoners of war.
The Tuskegee Airmen were credited by higher commands with the following accomplishments:
Awards and decorations included:
Controversy over escort record.
On 24 March 1945, during the war, the "Chicago Defender" said that no bomber escorted by the Tuskegee Airmen had ever been lost to enemy fire, under the headline: "332nd Flies Its 200th Mission Without Loss"; the article was based on information supplied by the 15th Air Force.
This statement was repeated for many years, and not publicly challenged, partly because the mission reports were classified for a number of years after the war. In 2004, William Holton, who was serving as the historian of the Tuskegee Airmen Incorporated, conducted research into wartime action reports. Alan Gropman, a professor at the National Defense University, disputed the initial refutations of the no-loss myth, and said he researched more than 200 Tuskegee Airmen mission reports and found no bombers were lost to enemy fighters. Dr. Daniel Haulman of the Air Force Historical Research Agency conducted a reassessment of the history of the unit in 2006 and early 2007. His subsequent report, based on after-mission reports filed by both the bomber units and Tuskegee fighter groups, as well as missing air crew records and witness testimony, documented 25 bombers shot down by enemy fighter aircraft while being escorted by the Tuskegee Airmen.
In a subsequent article, "The Tuskegee Airmen and the Never Lost a Bomber Myth," published in "The Alabama Review" and also by New South Books as an e-book, and included in a more comprehensive study regarding misconceptions about the Tuskegee Airmen released by AFHRA in July 2013, Haulman documented 27 bombers shot down by enemy aircraft while those bombers were being escorted by the 332nd Fighter Group. This total included 15 B-17s of the 483rd Bombardment Group shot down during a particularly savage air battle with an estimated 300 German fighters on 18 July 1944 that also resulted in nine kill credits and the award of five Distinguished Flying Crosses to members of the 332nd. Of the 179 bomber escort missions the 332nd Fighter Group flew for the Fifteenth Air Force, the group encountered enemy aircraft on 35 of those missions and lost bombers to enemy aircraft on only seven, and the total number of bombers lost was 27. By comparison, the average number of bombers lost by the other P-51 fighter groups of the Fifteenth Air Force during the same period was 46.
A number of examples of the fighter group's losses exist in the historical record. A mission report states that on 26 July 1944: "1 B-24 seen spiraling out of formation in T/A (target area) after attack by E/A (enemy aircraft). No chutes seen to open." The Distinguished Flying Cross citation awarded to Colonel Benjamin O. Davis for the mission on 9 June 1944 noted he "so skillfully disposed his squadrons that in spite of the large number of enemy fighters, the bomber formation suffered only a few losses." William Holloman was reported by the "Times" as saying his review of records confirmed bombers had been lost. Holloman was a member of Tuskegee Airmen Inc., a group of surviving Tuskegee pilots and their supporters, who also taught Black Studies at the University of Washington and chaired the Airmen's history committee. According to the 28 March 2007 Air Force report, some bombers under 332nd Fighter Group escort protection were even shot down on the day the "Chicago Defender" article was published. The mission reports, however, do credit the group for not losing a bomber on an escort mission for a six-month period between September 1944 and March 1945, albeit when Luftwaffe contacts were far fewer than earlier.
Postwar.
Contrary to negative predictions from some quarters, Tuskegee Airmen were some of the best pilots in the U.S. Army Air Forces due to a combination of pre-war experience and the personal drive of those accepted for training. Nevertheless, the Tuskegee Airmen continued to have to fight racism. Their combat record did much to quiet those directly involved with the group, notably bomber crews who often requested them for escort, but other units continued to harass these airmen.
In 1949, the 332nd entered the annual U. S. Continental Gunnery Meet in Las Vegas, Nevada. The competition included shooting aerial targets, shooting targets on the ground and dropping bombs on targets. Flying the long range Republic P-47N Thunderbolt (built for the long range escort mission in the Pacific theatre of World War II), the 332nd Fighter Wing took first place in the conventional fighter class. The pilots were Capt. Alva Temple, Lts. Harry Stewart, James Harvey III and Herbert Alexander. Lt. Harvey said, "We had a perfect score. Three missions, two bombs per plane. We didn't guess at anything, we were good." They received congratulations from the Governor of Ohio, and Air Force commanders across the nation.
After segregation in the military was ended in 1948 by President Harry S. Truman with Executive Order 9981, the veteran Tuskegee Airmen now found themselves in high demand throughout the newly formed United States Air Force. Some taught in civilian flight schools, such as the black-owned Columbia Air Center in Maryland.
On 11 May 1949, "Air Force Letter 35.3" was published, which mandated that black Airmen be screened for reassignment to formerly all-white units according to qualifications.
Tuskegee Airmen were instrumental in postwar developments in aviation. Edward A. Gibbs was a civilian flight instructor in the U.S. Aviation Cadet Program at Tuskegee during its inception. He later became the founder of Negro Airmen International, an association joined by many airmen. USAF General Daniel "Chappie" James Jr. (then Lt.) was an instructor of the 99th Pursuit Squadron, later a fighter pilot in Europe, and, in 1975, became the first African American to reach the rank of four-star general.
In 2005, seven Tuskegee Airmen, including Lieutenant Colonel Herbert Carter, Colonel Charles McGee, group historian Ted Johnson, and Lieutenant Colonel Lee Archer, flew to Balad, Iraq, to speak to active duty airmen serving in the current incarnation of the 332nd, which was reactivated as the 332nd Air Expeditionary Group in 1998 and made part of the 332nd Air Expeditionary Wing. "This group represents the linkage between the 'greatest generation' of airmen and the 'latest generation' of airmen," said Lt. Gen. Walter E. Buchanan III, commander of the Ninth Air Force and US Central Command Air Forces.
No one knows how many of the original 996 pilots and about 16,000 ground personnel are still alive. Many of the surviving members of the Tuskegee Airmen participate annually in the Tuskegee Airmen Convention, which is hosted by Tuskegee Airmen, Inc.
Legacy and honors.
On 29 March 2007, the Tuskegee Airmen were collectively awarded a Congressional Gold Medal at a ceremony in the U.S. Capitol rotunda. The medal is currently on display at the Smithsonian Institution.
The airfield where the airmen trained is now the Tuskegee Airmen National Historic Site.
Thurgood Marshall, the future Supreme Court justice, got his start defending Tuskegee bomber trainees. The 477th Bombardment Group was formed in 1944 to extend the so-called "Tuskegee experiment" by allowing black aviators to serve on bomber crews. The aim was to send pilots—many of them veterans of the original Tuskegee fighter group—back to the States for training on B-25 bombers. While in Indiana, some of the African-American officers were arrested and charged with mutiny after entering an all-white officers’ club. Marshall, then a young lawyer, represented the 100 black officers who had landed in jail as a result of the confrontation. The men were soon released (although one was later convicted of violent conduct and fined).
Other members of the Tuskegee Airmen have made contributions in the world of business. Eugene Winslow founded Afro-Am Publishing in Chicago, Illinois, which published "Great Negroes Past and Present" in 1963.
Daniel "Chappie" James Jr. started his career in the early 1940s at Tuskegee, joining the Army Air Corps in July 1943. After the war ended, James stayed in what became the Air Force and flew missions in both Korea and Vietnam. In 1969, James was put in command of Wheelus Air Base outside of Tripoli.
Three Tuskegee airmen went on to become generals. For keeping his cool in the face of Qaddafi’s troops, James was appointed a brigadier general by President Nixon. However, he was not the only graduate of the "Tuskegee experiment" to make flag rank. James followed in the footsteps of Benjamin O. Davis Jr., the original commander of the 332nd Fighter Group and the first black general in the U.S. Air Force. Another Tuskegee aviator, Lucius Theus, retired a major general after dedicating most of his 36-year career in the Air Force to improving the military’s bureaucracy, helping to implement a direct deposit system for service members.
In 2006, California Congressman Adam Schiff and Missouri Congressman William Lacy Clay, Jr., led the initiative to create a commemorative postage stamp to honor the Tuskegee Airmen.
The 99th Flying Training Squadron flies T-1A Jayhawks and, in honor of the Tuskegee Airmen, painted the tops of the tails of their aircraft red.
On 1 August 2008, Camp Creek Parkway, a portion of State Route 6 in south Fulton County and in the City of East Point near Atlanta, Georgia, was officially renamed in honor of the Tuskegee Airmen. The road is a highway that serves as the main artery into Hartsfield-Jackson International Airport.
The Heinz History Center in Pittsburgh presented an award to several Western Pennsylvania Tuskegee veterans, as well as suburban Sewickley, Pennsylvania dedicated a memorial to the seven from that municipality. Also, an exhibit was established at Pittsburgh International Airport in Concourse A.
On 9 December 2008, the Tuskegee Airmen were invited to attend the inauguration of Barack Obama, the first African American elected as President. Retired Lt. William Broadwater, 82, of Upper Marlboro, Maryland, a Tuskegee Airman, summed up the feeling. "The culmination of our efforts and others was this great prize we were given on Nov. 4. Now we feel like we've completed our mission." More than 180 airmen attended the 20 January 2009 inauguration.
The Tuskegee Airmen Memorial was erected at Walterboro Army Airfield, South Carolina, in honor of the Tuskegee Airmen, their instructors, and ground support personnel who trained at the Walterboro Army Airfield during World War II.
In the 2010 Rose Parade, the city of West Covina, California paid tribute to the "service and commitment of the Tuskegee Airmen" with a float, entitled ""Tuskegee Airmen—A Cut Above"", which featured a large bald eagle, two replica World War II ""Redtail"" fighter aircraft and historical images of some of the airmen who served. The float won the mayor's trophy as the most outstanding city entry—national or international.
In June 1998, the Ohio Army and Air National guard opened a jointly operated dining hall. They dedicated the new dining facility called the "Red Tail Dining Facility" to the Tuskegee Airmen. The facility is operated at the Rickenbacker ANG base outside of Columbus Ohio.
In January 2012, MTA Regional Bus Operations officially changed the name of its 100th Street depot in New York City to the Tuskegee Airmen Depot.
In 2012, George Lucas produced "Red Tails", a film based on the experiences of the Tuskegee Airmen.
In 2012, Aldine Independent School District in Harris County, TX, built Benjamin O. Davis High School in honor of Benjamin O. Davis Jr.

</doc>
<doc id="71757" url="https://en.wikipedia.org/wiki?curid=71757" title="Roberta Bondar">
Roberta Bondar

Roberta Bondar (; born December 4, 1945) is Canada's first female astronaut and the first neurologist in space. Following more than a decade as NASA's head of space medicine, Bondar became a consultant and speaker in the business, scientific, and medical communities.
Bondar has received many honours including the Order of Canada, the Order of Ontario, the NASA Space Medal, over 22 honorary degrees and induction into the Canadian Medical Hall of Fame.
Early life and education.
Roberta or Dr. Bondar was born in Sault Ste. Marie, Ontario, on December 4, 1945. Her father is of Ukrainian descent and her mother is of English descent. Bondar, as a child, enjoyed science. She loved the annual science fairs at her classes, and frequently set off experiments in her parents' basement as a child.
Bondar graduated from Sir James Dunn High School in Sault Ste. Marie, and holds a Bachelor of Science in zoology and agriculture from the University of Guelph (1968), a Master of Science in experimental pathology from the University of Western Ontario (1971), a Doctor of Philosophy in neuroscience from the University of Toronto (1974), and a Doctor of Medicine from McMaster University (1977). McMaster University is in Hamilton, Ontario.
Career.
In 1981, Bondar became a fellow of the Royal College of Physicians and Surgeons of Canada in neurology. Bondar also has certification in sky diving and parachuting. A celebrated landscape photographer, Bondar studied professional nature photography at the Brooks Institute of Photography, Santa Barbara, California.
Bondar began astronaut training in 1984, and in 1992 was designated Payload Specialist for the first International Microgravity Laboratory Mission (IML-1). Bondar flew on the NASA Space Shuttle Discovery during Mission STS-42, January 22–30, 1992, during which she performed experiments in the Spacelab.
After her astronaut career, Bondar led an international team of researchers at NASA for more than a decade, examining data obtained from astronauts on space missions to better understand the mechanisms underlying the body's ability to recover from exposure to space.
She also pursued her interests in photography, with emphasis on natural environments. She is the author of four photo essay books featuring her photography of the Earth, including "Passionate Vision" (2000), which covered Canada's national parks.
Bondar has also been a consultant and speaker to diverse organizations, drawing on her expertise as an astronaut, physician, scientific researcher, photographer, author, environment interpreter and team leader. Respected for her expertise and commentary, Bondar has been a guest of television and radio networks throughout the U.S. and Canada. She is featured in the IMAX movie "Destiny in Space", and has also co-anchored the Discovery Channel's coverage of space shuttle launches.
Bondar served two terms as the Chancellor of Trent University, from 2003 to 2009.
In 2009, Bondar registered The Roberta Bondar Foundation as a not-for-profit charity. The foundation centers on environmental awareness.
Honors, awards, and tributes.
On June 28, 2011, it was announced that Bondar would receive a star on Canada's Walk of Fame and would be inducted on October 1 at Elgin Theatre in Toronto. She was the first astronaut to receive the honour.
In her hometown of Sault Ste. Marie, the Roberta Bondar Park pavilion bears her name, as does the marina beside the park and the Ontario government building at 70 Foster Drive. Bondar has also been honoured with a marker on Sault Ste. Marie's Walk of Fame.
In 1996, the Dr. Roberta Bondar Public School was opened in Ajax, Ontario and Roberta Bondar Public School was opened in Ottawa. In 2005, another public school named Roberta Bondar Public School opened in Brampton, Ontario. There are also Dr. Roberta Bondar Elementary School in Abbotsford, British Columbia, and Dr. Roberta Bondar Public School in Maple (Vaughan), Ontario.

</doc>
<doc id="71758" url="https://en.wikipedia.org/wiki?curid=71758" title="Hollow-point bullet">
Hollow-point bullet

A hollow-point bullet is an expanding bullet that has a pit or hollowed out shape in its tip often intended to cause the bullet to expand upon entering a target in order to decrease penetration and disrupt more tissue as it travels through the target. It is also used for controlled penetration, where over-penetration could cause collateral damage (such as aboard an aircraft). In target shooting, they are used for greater accuracy and reduction of smoke, fouling, and lead vapor exposure, as hollow point bullets have an enclosed base while traditional bullets have an exposed lead base. In killing, hollow points are designed to increase in diameter once within the target, thus maximizing tissue damage and blood loss or shock, and to remain inside the target, thereby transferring all of the kinetic energy to the target (whereas some fraction would remain in the bullet if it passed through instead). Both of these goals are meant to maximize stopping power. Jacketed hollow points (JHPs) or plated hollow points are covered in a coating of harder metal (usually a copper alloy or copper coated steel) to increase bullet strength and to prevent fouling the barrel with lead stripped from the bullet. The term hollow-cavity bullet is used to describe a hollow point where the hollow is unusually large, sometimes dominating the volume of the bullet, and causes extreme expansion or fragmentation on impact. 
Plastic-tipped bullets are a type of (rifle) bullet meant to confer the aerodynamic advantage of the spitzer bullet (for example, see very-low-drag bullet) and the stopping power of hollow point bullets.
Mechanism of action.
Expansion.
When a hollow-point hunting bullet strikes a soft target, the pressure created in the pit forces the material (usually lead) around the inside edge to expand outwards, increasing the axial diameter of the projectile as it passes through. This process is commonly referred to as "mushrooming", because the resulting shape, a widened, rounded nose on top of a cylindrical base, typically resembles a mushroom.
The greater frontal surface area of the expanded bullet limits its depth of penetration into the target, and causes more extensive tissue damage along the wound path. Many hollow-point bullets, especially those intended for use at high velocity in centerfire rifles, are "jacketed", i.e. a portion of the lead-cored bullet is wrapped in a thin layer of harder metal, such as copper or mild steel. This jacket provides additional strength to the bullet, and can help prevent it from leaving deposits of lead inside the bore. In "controlled expansion" bullets, the jacket and other internal design characteristics help to prevent the bullet from breaking apart; a fragmented bullet will not penetrate as far.
Accuracy.
For bullets designed for target shooting, some such as the Sierra "Matchking" incorporate a cavity in the nose, called the meplat. This allows the manufacturer to maintain a greater consistency in tip shape and thus aerodynamic properties among bullets of the same design, at the expense of a slightly decreased ballistic coefficient and higher drag. The result is a slightly decreased overall accuracy between bullet trajectory and barrel direction, as well as an increased susceptibility to wind drift, but closer grouping of subsequent shots due to bullet consistency, creating a perceived accuracy for the shooter.
The manufacturing process of hollow-point bullets also produces a flat, uniformly-shaped base on the bullet which allegedly increases accuracy by providing a more consistent piston surface for the expanding gases of the cartridge.
Match or target hollow-point bullets are designed for precision "target" use, and no consideration is given to their expansion or other terminal ballistic performance. The United States military claims to use hollow-point bullets in some sniper rifles for their exceptional accuracy at long ranges, and claim that the hollow-point does not result in wounds significantly different from full metal jacket ammunition in practice.
Hollow-point boat-tail.
A hollow-point boat-tail bullet is a match-grade bullet design that uses the concept of a teardrop-shaped tail to give it a lower drag coefficient and make it produce less turbulence in its wake. Only the base of the bullet has a boat tail-like shape – the meplat is still pointed. Some hollow-point boat-tail bullets with longer, more aerodynamic profiles are known as very-low-drag bullets.
Testing.
Terminal ballistics testing of hollow point bullets is generally performed in ballistic gelatin, or some other medium intended to simulate tissue and cause a hollow point bullet to expand. Test results are generally given in terms of expanded diameter, penetration depth, and weight retention. Expanded diameter is an indication of the size of the wound cavity, penetration depth shows if vital organs could be reached by the bullet, and weight retention indicates how much of the bullet mass fragmented and separated from the main body of the bullet. How these different factors are interpreted is subject to the intended use of the bullet, and what constitutes "Good" or "Terrible" performance is subject to disagreement.
History.
Solid lead bullets, when cast from a soft alloy, will often deform and provide some expansion if they hit the target at a high velocity. This, combined with the limited velocity and penetration attainable with muzzleloading firearms, meant there was little need for extra expansion.
The first hollow-point bullets were marketed in the late 19th century as "express bullets", and were hollowed out to reduce the bullet's mass and provide higher velocities. In addition to providing increased velocities, the hollow also turned out to provide significant expansion, especially when the bullets were cast in a soft lead alloy. Originally intended for rifles, the popular .32-20, .38-40, and .44-40 calibers could also be fired in revolvers.
With the advent of smokeless powder, velocities increased, and bullets got smaller, faster, and lighter. These new bullets (especially in rifles) needed to be jacketed to handle the conditions of firing. The new full metal jacket bullets tended to penetrate straight through a target and produce little damage. This led to the development of the soft point bullet and later jacketed hollow-point bullets at the British arsenal in Dum Dum, near Calcutta around 1890. Designs included the .303" Mk III, IV & V and the .455" Mk III "Manstopper" cartridges. Although such bullet designs were quickly outlawed for use in warfare (in 1898, the Germans complained they breached the Laws of War), they steadily gained ground among hunters due to the ability to control the expansion of the new high velocity cartridges. In modern ammunition, the use of hollow points is primarily limited to handgun ammunition, which tends to operate at much lower velocities than rifle ammunition (on the order of versus over 2,000 feet per second). At rifle velocities, a hollow point is not needed for reliable expansion and most rifle ammunition makes use of tapered jacket designs to achieve the mushrooming effect. At the lower handgun velocities, hollow point designs are generally the only design which will expand reliably.
Modern hollowpoint bullet designs use many different methods to provide controlled expansion, including:
Legality.
The Hague Convention of 1899, Declaration III, prohibited the use in "international warfare" of bullets that easily expand or flatten in the body.
This is often incorrectly believed to be prohibited in the Geneva Conventions, but it significantly predates those conventions, and is in fact a continuance of the St. Petersburg Declaration of 1868, which banned exploding projectiles of less than 400 grams, as well as weapons designed to aggravate injured soldiers or make their death inevitable. NATO members do not use small arms ammunition that are prohibited by the Hague Convention and the United Nations.
Despite the ban on military use, hollow-point bullets are one of the most common types of bullets used by civilians and police, which is due largely to the reduced risk of bystanders being hit by over-penetrating or ricocheted bullets, and the increased speed of incapacitation.
In many jurisdictions, even ones such as the United Kingdom, where expanding ammunition is generally prohibited, it is illegal to hunt certain types of game with ammunition that does "not" expand. Some target ranges forbid full metal jacket ammunition, due to its greater tendency to damage metal targets and backstops.
United Kingdom.
All expanding ammunition, including hollow point bullets, falls under Section 5 (prohibited weapons) of the Firearms Act 1968 and so is illegal to possess or transfer without the written permission of the Home Secretary. The law is in conflict with 'Schedule 2' of 'The Deer Act 1991', prohibiting the use of any ammunition other than soft-nosed or hollow-point bullets for the killing of deer. Because of this conflict, Section 5A(4) allows the police licensing authority to add a condition to a firearm certificate for possession of expanding ammunition for:
United States.
The United States is one of few major powers that did not agree to IV,3 of the Hague Convention of 1899, thus able to use this kind of ammunition in warfare. The US Army has mentioned that they consider using the ammunition for sidearms, plans set in 2018.
The state of New Jersey bans possession of hollow point bullets by civilians except for ammunition possessed at one's dwelling, premises or other land owned or possessed, or for, while and traveling to and from hunting with a hunting license if otherwise legal for the particular game. The law also requires all hollow point ammunition to be transported directly from the place of purchase to one's home or premises, or hunting area, or by members of a rifle or pistol club directly to a place of target practice, or directly to an authorized target range from the place of purchase or one's home or premises.
Winchester Black Talon controversy.
In early 1992, Winchester introduced the "Black Talon", a newly designed hollow-point handgun bullet which used a specially designed, reverse tapered jacket. The jacket was cut at the hollow to intentionally weaken it, and these cuts allowed the jacket to open into six petals upon impact. The thick jacket material kept the tips of the jacket from bending as easily as a normal thickness jacket. The slits that weakened the jacket left triangular shapes in the tip of the jacket, and these triangular sections of jacket would end up pointing out after expansion, leading to the "Talon" name. The bullets were coated with a black colored, paint-like lubricant called "Lubalox," and loaded into nickel-plated brass cases, which made them visually stand out from other ammunition. While actual performance of the Black Talon rounds was not significantly better than any other comparable high performance hollow-point ammunition, the reverse taper jacket did provide reliable expansion under a wide range of conditions, and many police departments adopted the round.
Winchester's "Black Talon" product name was eventually used against them. After a high profile shooting at a large law firm at 101 California Street, San Francisco in 1993, media response against Winchester was swift. "This bullet kills you better", says one report; "its six razorlike claws unfold on impact, expanding to nearly three times the bullet's diameter".
A concern was raised by the American College of Emergency Physicians that the sharp edges of the jacket could penetrate surgical gloves, increasing the risk of blood-borne infections being transmitted to medical personnel treating the gunshot wound. While plausible, there are no recorded cases of such an infection occurring in relation to the Black Talon bullets.
Winchester responded to the media criticism of the Black Talon line by removing it from the commercial market and only selling it to law enforcement distributors. Winchester has since discontinued the sale of the Black Talon entirely, although Winchester does manufacture very similar ammunition, the Ranger T-Series and the Supreme Elite Bonded PDX1.
Terminology.
The hollow point and soft-nosed bullets are both sometimes also referred to as dum-dums, so named after the British arsenal at Dum Dum, in present north Kolkata, India, where it is said jacketed, expanding bullets were first developed. This term is rare among shooters in North America, but can still be found in use, usually in the news media and sensational popular fiction.
Recreational shooters sometimes refer to hollow points as "JHPs", from the common manufacturer's abbreviation for "Jacketed Hollow Point".

</doc>
<doc id="71762" url="https://en.wikipedia.org/wiki?curid=71762" title="John Philip Holland">
John Philip Holland

John Philip Holland () (24 February 184112 August 1914) was an Irish engineer who developed the first submarine to be formally commissioned by the U.S. Navy, and the first Royal Navy submarine, the "Holland 1". 
Early life.
John Philip Holland, Jr., the second of four siblings, all boys, was born in a coastguard cottage in Liscannor, County Clare, Ireland where his father, John, Sr., was a member of the British Coastguard Service. His mother, a native Irish speaker from Liscannor, Máire Ní Scannláin (aka Mary Scanlon), was John Holland's second wife, his first, Anne Foley Holland, believed to be a native of Kilkee, died in 1835. The area was heavily Irish-speaking and Holland learned English properly only when he attended the local English-speaking St Macreehy's National School, and from 1858, in the Christian Brothers in Ennistymon.
Holland joined the Irish Christian Brothers in Limerick and taught in Limerick (CBS Sexton Street) and many other centres in the country including North Monastery CBS in Cork City, St Joseph's CBS (Drogheda) and as the first Mathematics teacher in Colaiste Ris (also Dundalk). Due to ill health, he left the Christian Brothers in 1873. Holland emigrated to the United States in 1873. Initially working for an engineering firm, he returned to teaching again for a further six years in St. John's Catholic school in Paterson, New Jersey.
Development of submarine designs.
While a teacher in Cork, Holland read an account of the battle between the ironclads Monitor and Merrimack during the American Civil War. He realized that the best way to attack such ships would be through an attack beneath the waterline. He drew a design, but when he attempted to obtain funding, he was turned away. After his arrival in the United States, Holland slipped and fell on an icy Boston street and broke a leg. While recuperating from the injury in a hospital, he used his time to refine his submarine designs and was encouraged by Isaac Whelan, a priest.
In 1875, his first submarine designs were submitted for consideration by the U.S. Navy, but turned down as unworkable. The Fenians, however, continued to fund Holland's research and development expenses at a level that allowed him to resign from his teaching post. In 1881, "Fenian Ram" was launched, but soon after, Holland and the Fenians parted company on bad terms over the issue of payment within the Fenian organization, and between the Fenians and Holland. The submarine is now preserved at Paterson Museum, New Jersey.
Holland continued to improve his designs and worked on several experimental boats, prior to his successful efforts with a privately built type, launched on 17 May 1897. This was the first submarine having power to run submerged for any considerable distance, and the first to combine electric motors for submerged travel and gasoline engines for use on the surface. She was purchased by the U.S. Navy, on 11 April 1900, after rigorous tests and was commissioned on 12 October 1900 as USS "Holland". Six more of her type were ordered and built at the Crescent Shipyard in Elizabeth, New Jersey. The company that emerged from under these developments was called The Electric Boat Company, founded on 7 February 1899. Isaac Leopold Rice became the company's first President with Elihu B. Frost acting as vice president and chief financial officer. This company eventually evolved into the major defense contractor General Dynamics.
The USS "Holland" design was also adopted by others, including the Royal Navy in developing the . The Imperial Japanese Navy employed a modified version of the basic design for their first five submarines, although these submarines were at least 10 feet longer at about 63 feet. These submarines were also developed at the Fore River Ship and Engine Company in Quincy, Massachusetts. Holland also designed the "Holland II" and "Holland III" prototypes. The Royal Navy 'Holland 1' is on display at the Submarine Museum, Gosport, England.
Death.
After spending 57 of his 74 years working with submersibles, John Philip Holland died on 12 August 1914 in Newark, New Jersey. He is interred at the Holy Sepulchre Cemetery in Totowa, New Jersey.
Memorial.
A monument stands at the gates of Scholars Townhouse, Drogheda (the former building of the Christian Brothers school where Holland taught) in commemoration of his work. It was unveiled in a ceremony on 14 June 2014 as part of the Irish Maritime Festival. The ceremony was attended by Drogheda Town Council as well as representatives of the US, British and Japanese governments.

</doc>
<doc id="71763" url="https://en.wikipedia.org/wiki?curid=71763" title="USS Holland (SS-1)">
USS Holland (SS-1)

USS "Holland" (SS-1) was the United States Navy's first modern commissioned submarine, although not the first military submarine of the United States, which was the 1775 submersible "Turtle". The boat was originally laid down as Holland VI, and launched on 17 May 1897.
Design and construction.
The work was done at (Ret.) Navy Lieutenant Lewis Nixon's Crescent Shipyard of Elizabeth, New Jersey for John Holland's Holland Torpedo Boat Company, which became the Electric Boat company in 1899. The craft was built under the supervision of John Philip Holland, who designed the vessel and her details. The keel to this craft was laid at Nixon's Crescent Shipyard with both men present. The two men worked together using many of John Holland's proven concepts and patents to make the submarine a reality, each man complementing the other's contributions to the development of the modern submarine.
"Holland VI" included many features that submarines of the early 20th century would exhibit, albeit in later, more advanced forms. She had both an internal combustion engine (specifically, a 4-stroke Otto gasoline engine) for running on the surface and an electric motor for submerged operation. She had a reloadable 18 inch (457 mm) torpedo tube and an 8.4 inch (210 mm) pneumatic dynamite gun in the bow (the dynamite gun's projectiles were called "aerial torpedoes"). A second dynamite gun in the stern was removed in 1900, prior to delivery to the Navy. There was a conning tower from which the boat and her weapons could be directed. Finally, she had all the necessary ballast and trim tanks to make precise changes in depth and attitude underwater.
Service.
"Holland VI" eventually proved her validity and worthiness as a warship and was ultimately purchased by the U.S. government for the sum of $150,000 on 11 April 1900. She was considered to be the first truly successful craft of her type. The United States Government soon ordered more submarines from Holland's company, which were to be known as the . These became America's first fleet of underwater naval vessels.
"Holland VI" was modified after her christening, and was renamed USS "Holland" (SS-1) when she was commissioned by the US Navy on 12 October 1900, at Newport, Rhode Island, with Lieutenant Harry H. Caldwell in command.
"Holland" was the first commissioned submarine in the US Navy and is the first of the unbroken line of submarines in the Navy. She was the third submarine to be owned by the Navy, however. (The first submarine was "Propeller" (also known as "Alligator") and the second was "Intelligent Whale".)
On 16 October 1900, in order to be kept serviceable throughout the winter, "Holland" left Newport under tow of the tug "Leyden" for Annapolis, Maryland, where she was used to train midshipmen of the United States Naval Academy, as well as officers and enlisted men ordered there to receive training vital in preparing for the operation of other submarines being built for the Fleet.
"Holland" proved valuable for experimental purposes in collecting data for submarines under construction or contemplation. Her surface run, from Annapolis to Norfolk, Virginia from 8–10 January 1901, provided useful data on her performance underway over an extended period.
"Holland" (briefly) and five "Plunger"-class Holland-type submarines were based in New Suffolk, New York on the North Fork of Long Island from 1899–1905, prompting the hamlet to claim to be the first submarine base in the United States.
Except for the period from 15 June to 1 October 1901, which was passed training cadets at the Naval Torpedo Station, Newport, Rhode Island, "Holland" remained at Annapolis as a training submarine until 17 July 1905, when she was decommissioned.
"Holland" finished her career at Norfolk, Virginia. Her name was struck from the Naval Vessel Register on 21 November 1910. This revolutionary submarine was sold as scrap to Henry A. Hitner & Sons of Philadelphia on 18 June 1913 for $100. Her purchaser was required to put up $5,000 bond as assurance that the submarine would be broken up and not used as a ship.
About 1915 the hulk of the "Holland", stripped of her external fittings, was sold to Peter J. Gibbons. As of October, 1916 she was on display in Philadelphia. In May 1917 she was moved to the Bronx, New York as a featured attraction at the Bronx International Exposition of Science, Arts and Industries.
"Holland" was on display for several years in Paterson, New Jersey until she was finally scrapped in 1932.
Legacy.
The success of the submarine was instrumental in the founding of the Electric Boat Company, now known as the Electric Boat Division of General Dynamics Corporation. This company, therefore, can trace its origins to the formation of John Philip Holland's original company and the revolutionary submarines that were developed at this shipyard.

</doc>
<doc id="71777" url="https://en.wikipedia.org/wiki?curid=71777" title="Satyendra Nath Bose">
Satyendra Nath Bose

Satyendra Nath Bose (Bengali: সত্যেন্দ্রনাথ বসু), FRS (1 January 1894 – 4 February 1974) was a Bengali physicist specialising in mathematical physics. He is best known for his work on quantum mechanics in the early 1920s, providing the foundation for Bose–Einstein statistics and the theory of the Bose–Einstein condensate. A Fellow of the Royal Society, he was awarded India's second highest civilian award, the Padma Vibhushan in 1954 by the Government of India.
The class of particles that obey Bose–Einstein statistics, bosons, was named after Bose by Paul Dirac.
A self-taught scholar and a polyglot, he had a wide range of interests in varied fields including physics, mathematics, chemistry, biology, mineralogy, philosophy, arts, literature, and music. He served on many research and development committees in sovereign India.
Early life.
Bose was born in Calcutta (now Kolkata), the eldest of seven children. He was the only son, with six sisters after him. His ancestral home was in village Bara Jagulia, in the district of Nadia, in the state of West Bengal. His schooling began at the age of five, near his home. When his family moved to Goabagan, he was admitted to the New Indian School. In the final year of school, he was admitted to the Hindu School. He passed his entrance examination (matriculation) in 1909 and stood fifth in the order of merit. He next joined the intermediate science course at the Presidency College, Calcutta, where he was taught by illustrious teachers such as Jagadish Chandra Bose, Sarada Prasanna Das, and Prafulla Chandra Ray. Naman Sharma and Meghnad Saha, from Dacca (Dhaka), joined the same college two years later. Prasanta Chandra Mahalanobis and Sisir Kumar Mitra were few years senior to Bose. Satyendra Nath Bose chose mixed (applied) mathematics for his BSc and passed the examinations standing first in 1913 and again stood first in the MSc mixed mathematics exam in 1915. It is said that his marks in the MSc examination created a new record in the annals of the University of Calcutta, which is yet to be surpassed.
After completing his MSc, Bose joined the University of Calcutta as a research scholar in 1916 and started his studies in the theory of relativity. It was an exciting era in the history of scientific progress. Quantum theory had just appeared on the horizon and important results had started pouring in.
His father, Surendranath Bose, worked in the Engineering Department of the East Indian Railway Company. In 1914, age 20, Satyendra Nath Bose married Ushabati Ghosh, the 11-year-old daughter of a prominent Calcutta physician. They had nine children, but two died in early childhood. When he died in 1974, he left behind his wife, two sons, and five daughters.
As a polyglot, he was well versed in several languages such as Bengali, English, French, German and Sanskrit as well as the poetry of Lord Tennyson, Rabindranath Tagore and Kalidasa. He could also play the "esraj", a musical instrument similar to a violin. He was actively involved in running night schools that came to be known as the Working Men's Institute.
Research career.
Bose attended Hindu School in Calcutta, and later attended Presidency College, also in Calcutta, earning the highest marks at each institution, while fellow student and future astrophysicist Meghnad Saha came second. He came in contact with teachers such as Jagadish Chandra Bose, Prafulla Chandra Ray and Naman Sharma who provided inspiration to aim high in life. From 1916 to 1921, he was a lecturer in the physics department of the University of Calcutta. Along with Saha, Bose prepared the first book in English based on German and French translations of original papers on Einstein's special and general relativity in 1919. In 1921, he joined as Reader of the department of Physics of the recently founded University of Dhaka (in present-day Bangladesh). Bose set up whole new departments, including laboratories, to teach advanced courses for MSc and BSc honours and taught thermodynamics as well as James Clerk Maxwell's theory of electromagnetism.
Satyendra Nath Bose, along with Saha, presented several papers in theoretical physics and pure mathematics from 1918 onwards. In 1924, while working as a Reader (Professor without a chair) at the Physics Department of the University of Dhaka, Bose wrote a paper deriving Planck's quantum radiation law without any reference to classical physics by using a novel way of counting states with identical particles. This paper was seminal in creating the very important field of quantum statistics. Though not accepted at once for publication, he sent the article directly to Albert Einstein in Germany. Einstein, recognising the importance of the paper, translated it into German himself and submitted it on Bose's behalf to the prestigious "Zeitschrift für Physik". As a result of this recognition, Bose was able to work for two years in European X-ray and crystallography laboratories, during which he worked with Louis de Broglie, Marie Curie, and Einstein.
Bose–Einstein statistics.
While presenting a lecture at the reputable University of Dhaka on the theory of radiation and the ultraviolet catastrophe, Bose intended to show his students that the contemporary theory was inadequate, because it predicted results not in accordance with experimental results.
In the process of describing this discrepancy, Bose for the first time took the position that the Maxwell–Boltzmann distribution would not be true for microscopic particles, where fluctuations due to Heisenberg's uncertainty principle will be significant. Thus he stressed the probability of finding particles in the phase space, each state having volume , and discarding the distinct position and momentum of the particles.
Bose adapted this lecture into a short article called "Planck's Law and the Hypothesis of Light Quanta" and sent it to Albert Einstein with the following letter:
Einstein agreed with him, translated Bose's paper "Planck's Law and Hypothesis of Light Quanta" into German, and had it published in "Zeitschrift für Physik" under Bose's name, in 1924.
(1) There are three outcomes. What is the probability of producing two heads?
(2) Since the coins are distinct, there are two outcomes which produce a head and a tail. The probability of two heads is one-quarter.
The reason Bose's interpretation produced accurate results was that since photons are indistinguishable from each other, one cannot treat any two photons having equal energy as being two distinct identifiable photons. By analogy, if in an alternate universe coins were to behave like photons and other bosons, the probability of producing two heads would indeed be one-third (tail-head = head-tail).
Bose's interpretation is now called Bose–Einstein statistics. This result derived by Bose laid the foundation of quantum statistics, as acknowledged by Einstein and Dirac. When Einstein met Bose face-to-face, he asked him whether he had been aware that he had invented a new type of statistics, and he very candidly said that no, he wasn't that familiar with Boltzmann's statistics and didn't realize that he was doing the calculations differently. He was equally candid with anyone who asked.
Bose–Einstein condensate.
Einstein also did not at first realize how radical Bose's departure was, and in his first paper after Bose he was guided, like Bose, by the fact that the new method gave the right answer. But after Einstein's second paper using Bose's method in which he predicted the Bose–Einstein condensate ("pictured left"), he started to realize just how radical it was, and he compared it to wave/particle duality, saying that some particles didn't behave exactly like particles. Bose had already submitted his article to the British Journal "Philosophical Magazine", which rejected it, before he sent it to Einstein. It is not known why it was rejected.
Einstein adopted the idea and extended it to atoms. This led to the prediction of the existence of phenomena which became known as Bose–Einstein condensate, a dense collection of bosons (which are particles with integer spin, named after Bose), which was demonstrated to exist by experiment in 1995.
Dhaka.
After his stay in Europe, Bose returned to Dhaka in 1926. He did not have a doctorate, and so ordinarily, under the prevailing regulations, he would not be qualified for the post of Professor he applied for, but Einstein recommended him. He was then made Head of the Department of Physics at Dhaka University. He continued guiding and teaching at Dhaka University.
Bose designed equipment himself for a X-ray crystallography laboratory. He set up laboratories and libraries to make the department a center of research in X-ray spectroscopy, X-ray diffraction, magnetic properties of matter, optical spectroscopy, wireless, and unified field theories. He also published an equation of state for real gases with Meghnad Saha. He was also the Dean of the Faculty of Science at Dhaka University until 1945.
Calcutta.
When the partition of India became imminent (1947), he returned to Calcutta and taught there until 1956. He insisted every student to design his own equipment using local materials and local technicians. He was made professor emeritus on his retirement. He then became Vice-Chancellor of Visva-Bharati University in Shanti Niketan. He returned to the University of Calcutta to continue research in nuclear physics and complete earlier works in organic chemistry. In subsequent years, he worked in applied research such as extraction of helium in hot springs of Bakreshwar.
Other fields.
Apart from physics, he did some research in biotechnology and literature (Bengali and English). He made deep studies in chemistry, geology, zoology, anthropology, engineering and other sciences. Being Bengali, he devoted a lot of time to promoting Bengali as a teaching language, translating scientific papers into it, and promoting the development of the region.
Honours.
In 1937, Rabindranath Tagore dedicated his only book on science, Visva–Parichay,to Satyendra Nath Bose. Bose was honoured with title Padma Vibhushan by the Indian Government in 1954. In 1959, he was appointed as the National Professor, the highest honour in the country for a scholar, a position he held for 15 years. In 1986, the S.N. Bose National Centre for Basic Sciences was established by an act of Parliament, Government of India, in Salt Lake, Calcutta.
Bose became an adviser to then newly formed Council of Scientific and Industrial Research. He was the President of Indian Physical Society and the National Institute of Science. He was elected General President of the Indian Science Congress. He was the Vice-President and then the President of Indian Statistical Institute. In 1958, he became a Fellow of the Royal Society. He was nominated as member of Rajya Sabha.
Partha Ghose has stated that
Legacy.
Although several Nobel Prizes were awarded for research related to the concepts of the boson, Bose–Einstein statistics and Bose–Einstein condensate, Bose himself was not awarded a Nobel Prize.
In his book "The Scientific Edge", physicist Jayant Narlikar observed: 

</doc>
<doc id="71779" url="https://en.wikipedia.org/wiki?curid=71779" title="Kirchhoff's laws">
Kirchhoff's laws

There are several Kirchhoff's laws, all named after Gustav Kirchhoff:

</doc>
<doc id="71789" url="https://en.wikipedia.org/wiki?curid=71789" title="Mara Jade">
Mara Jade

Mara Jade Skywalker is a fictional character in "Star Wars" Expanded Universe books, comic books, and computer games. She is also wife to Luke Skywalker, and mother to Ben Skywalker. In the different video games, she is voiced by Heidi Shannon, Edie Mirman and Kath Soucie.
Concept and development.
Mara Jade was introduced in Timothy Zahn's "Heir to the Empire" (1991). Zahn imagined Jade as a strong, complex female character, which he found was lacking in the "Star Wars" universe. While competent, she is also flawed. When the "Thrawn Trilogy" ended in 1993 with "The Last Command", Zahn thought it was the last book for which he would develop the character. When Zahn was asked to write another novel, he established two goals: "to end the war between the New Republic and the Empire, and to get Luke Skywalker and Mara together." Although Lucasfilm initially resisted the idea of Luke marrying Mara, they eventually acquiesced.
Compared to "Star Wars" sole iconic female character, Princess Leia, Zahn said, "Mara has a sharper and more sarcastic manner, and of course, she had to go through the painful realization that her service had been to an evil cause. But they're both women who are strong without sacrificing their femininity, a balance which I think some authors have trouble writing. Bear in mind too, that Leia was one of the first people in the New Republic who decided Mara could be trusted, which perhaps says something about their understanding of each other."
In a 1998 "Star Wars Insider" poll of fans' favorite "Star Wars" characters, Mara Jade was the only Expanded Universe character to break the top 20.
Depiction.
Mara Jade is introduced as smuggler Talon Karrde's second-in-command in "Heir to the Empire". The novel establishes that she was previously an "Emperor's Hand" — a special agent — for Emperor Palpatine. Several stories depict Jade before the events in "Heir to the Empire", showing her training under Palpatine and executing his orders. The Thrawn trilogy depicts Mara's first confrontation with Luke Skywalker, whom she has sworn to kill to avenge Palpatine's death. This was reinforced by the Emperor's final telepathic command to assassinate Luke, which had plagued her thoughts since his death; upon learning this situation, Luke vowed to help Mara break Palpatine's hold on her, regardless of the danger she posed to him. Towards the end of the trilogy, she turns against the Empire and resolves her anger toward Luke and silences the command without killing Luke, instead killing Luke's "clone" — Luuke Skywalker — who had been made by a corrupt Jedi to challenge Luke. In doing so, she took Anakin Skywalker's blue lightsaber (which Luke was using), and used it instead of Luke's green lightsaber, which she kept because she did not trust Luke. Luke recognizes in Mara an underdeveloped affinity for the Force; although she initially resists Jedi training, she eventually becomes a Jedi Master.
Luke and Mara develop a strong bond in Zahn's "The Hand of Thrawn Duology"; he proposes marriage, and the two wed in Michael A. Stackpole's graphic novel "Union". She delivers a son, Ben, during "The New Jedi Order" series. In the "Legacy of the Force" series, Mara becomes suspicious of her nephew, Jacen Solo, when he sends Ben on several ethically dubious missions. Upon learning that Jacen has become a Sith apprentice, Mara vows to kill him, but Jacen ultimately kills "her" in "Sacrifice". She later appears as a Force ghost, first to Ben and then to Luke, in "Revelation". She also appears as a Force ghost to Cade Skywalker in the "" comics, set more than a century after the "Star Wars" films.
Other appearances.
Mara was portrayed by model Shannon McRandle (as Shannon Baksa) on several cards in the Star Wars Customizable Card Game. Meeting her was also a mission objective in the now defunct massively multiplayer online game "Star Wars Galaxies". She is a controllable character in ",". ', ' and '. She also narrates the Imperial campaign in '. 
Reception.
Mara Jade was chosen by IGN as the 19th top "Star Wars" character. IGN's Jesse Schedeen also listed the character as the top 10th "Star Wars" hero, noting that she "entered the Expanded Universe early, and she's stuck around for so long because she's just a great character". UGO Networks called the character the seventh top "Star Wars" Expanded Universe character, calling her complex.
Merchandise.
Mara Jade was one of the first Expanded Universe characters to receive a Hasbro action figure and a Gentle Giant bust. The first Hasbro figure did not use Shannon McRandle's likeness, but the Gentle Giant product did. Additionally, a Mara Jade's lightsaber replica was produced in a limited edition run by Master Replicas and the item rapidly sold out. In August 2007, a second Mara Jade action figure was released by Hasbro as part of their Comic Packs line, consisting of two figures and a comic book reprint. Jade was packed with a Luke Skywalker figure, and issue #5 of Heir to the Empire by Dark Horse Comics. Another Mara Jade action figure has been released as part of Hasbro's 4" Black Series, in 2013.
Wizards of the Coast produced five Mara Jade figures for Star Wars Miniatures collectible board game. The first set, Rebel Storm contained an Imperial version, Mara Jade, Emperor's Hand. This was later reprinted in a full reprint set, Rebels and Imperials. The next one was a New Republic version, Mara Jade, Jedi in Alliance and Empire, a 30th anniversary set. The fourth was an older New Republic version, Mara Jade Skywalker in the Legacy of the Force set. The fifth was an Imperial version again, Arica in the Imperial Entanglements set.
Books.
By "Timothy Zahn"
Thrawn trilogy
Hand of Thrawn series
Comic.
By "Karen Traviss"
Legacy of the Force

</doc>
<doc id="71791" url="https://en.wikipedia.org/wiki?curid=71791" title="Memoirs of a Fox-Hunting Man">
Memoirs of a Fox-Hunting Man

Memoirs of a Fox-Hunting Man is a novel by Siegfried Sassoon, first published in 1928 by Faber and Faber. It won both the Hawthornden Prize and the James Tait Black Memorial Prize, being immediately recognised as a classic of English literature. In the years since its first appearance, it has regularly been a set text for British schoolchildren.
Background.
Prior to its publication, Sassoon's reputation rested entirely on his poetry, mostly written during and about World War I. Only ten years after the war ended, after some experience of journalism, did he feel ready to branch out into prose. So uncertain was he of the wisdom of this move that he elected to publish "Memoirs of a Fox-Hunting Man" anonymously. It is a depiction of his early years presented in the form of an autobiographical novel, with false names being given to the central characters, including Sassoon himself, who appears as "George Sherston," and his mother ("Aunt Evelyn").
Plot.
The story is a series of episodes in the youth of George Sherston, ranging from his first attempts to learn to ride to his experiences in winning point-to-point races. The title is somewhat misleading, as the book is mainly concerned with a series of landmark events in Sherston/Sassoon's childhood and youth, and his encounters with various comic characters. "The Flower-Show Match," an account of an annual village cricket match - an important fixture for those involved - in which young Sherston plays a significant part, was later published separately by Faber as a self-contained story. The book as a whole is a frequently humorous work, in which fox-hunting, one of Sassoon's major interests, comes to represent the young man's innocent frame of mind in the years before war broke out. The book ends with his enlistment in a local regiment. The story is continued in two sequels: "Memoirs of an Infantry Officer" and "Sherston's Progress".

</doc>
<doc id="71792" url="https://en.wikipedia.org/wiki?curid=71792" title="Daniel Boone">
Daniel Boone

Daniel Boone (September 26, 1820) was an American pioneer, explorer, woodsman, and frontiersman, whose frontier exploits made him one of the first folk heroes of the United States. Boone is most famous for his exploration and settlement of what is now Kentucky, which was then part of Virginia, but on the other side of the mountains from the settled areas. As a young adult, Boone supplemented his farm income by hunting and trapping game, and selling their pelts in the fur market. Through this occupational interest, Boone first learned the easy routes to the area. Despite some resistance from American Indian tribes such as the Shawnee, in 1775, Boone blazed his Wilderness Road through the Cumberland Gap in the Appalachian Mountains from North Carolina and Tennessee into Kentucky. There, he founded the village of Boonesborough, Kentucky, one of the first American settlements west of the Appalachians. Before the end of the 18th century, more than 200,000 Americans migrated to Kentucky/Virginia by following the route marked by Boone.
Boone was a militia officer during the Revolutionary War (1775–83), which, in Kentucky, was fought primarily between the American settlers and the British-aided Indians. Boone was captured by Shawnee warriors in 1778. He escaped and alerted Boonesborough that the Shawnees were planning an attack. Although heavily outnumbered, Americans repulsed the Shawnee warriors in the Siege of Boonesborough.
Boone was elected to the first of his three terms in the Virginia General Assembly, during the Revolutionary War, and fought in the Battle of Blue Licks in 1782. Blue Licks, a Shawnee victory over the Patriots, was one of the last battles of the Revolutionary War, coming after the main fighting ended in October 1781.
Following the war, Boone worked as a surveyor and merchant, but fell deeply into debt through failed Kentucky land speculation. Frustrated with the legal problems resulting from his land claims, in 1799, Boone emigrated to eastern Missouri, where he spent most of the last two decades of his life (1800–20).
Boone remains an iconic figure in American history. He was a legend in his own lifetime, especially after an account of his adventures was published in 1784 by John Filson, making him famous across Europe as the typical all-American frontiersman. An American edition made him equally famous across the United States. After his death, he was frequently the subject of heroic tall tales and works of fiction. His adventures—real and legendary—were influential in creating the archetypal Western hero of American folklore. In American popular culture, he is remembered as one of the foremost early frontiersmen. The epic Daniel Boone mythology often overshadows the historical details of his life.
Youth.
Daniel Boone was of English and Welsh ancestry. Because the Gregorian calendar was adopted during his lifetime, Boone's birth date is sometimes given as November 2, 1734 (the "New Style" date), although Boone used the October date. The Boone family belonged to the Religious Society of Friends, called "Quakers", and were persecuted in England for their dissenting beliefs. Daniel's father, Squire (his first name, not a title) Boone (1696–1765) emigrated from the small town of Bradninch, Devon (near Exeter, England) to Pennsylvania in 1713, to join William Penn's colony of dissenters. Squire Boone's parents, George Boone III and Mary Maugridge, followed their son to Pennsylvania in 1717, and in 1720 built a log cabin at Boonecroft.
In 1720, Squire Boone, who worked primarily as a weaver and a blacksmith, married Sarah Morgan (1700–77). Sarah's family were Quakers from Wales, and had settled in Towamencin Township, Montgomery County, Pennsylvania in 1708. In 1731, the Boones moved to the Oley Valley, near the modern city of Reading. There they built a log cabin, partially preserved today as the Daniel Boone Homestead. Daniel Boone was born there, the sixth of 11 children.
Daniel Boone spent his early years on what was then the edge of the Pennsylvania frontier. Several Lenape Indian villages were nearby. The pacifist Pennsylvania Quakers had good relations with the Indians, but the steady growth of the white population compelled many Indians to move further west. Boone was given his first rifle at the age of 12, as families depended on hunting for much of their food. He learned to hunt from both local settlers and the Lenape. Folk tales have often emphasized Boone's skills as a hunter. In one story, the young Boone was hunting in the woods with some other boys, when the howl of a panther scattered all but Boone. He calmly cocked his rifle and shot the predator through the heart just as it leaped at him. The validity of this claim is contested, but the story was told so often that it became part of his popular image.
In Boone's youth, his family became a source of controversy in the local Quaker community when two of the oldest children married outside the endogamous community, in present-day Lower Gwynedd Township, Pennsylvania. In 1742, Boone's parents were compelled to publicly apologize after their eldest child, Sarah, married John Willcockson, a "worldling" (non-Quaker). Because the young couple had "kept company", they were considered "married without benefit of clergy". When the Boones' oldest son Israel married a "worldling" in 1747, Squire Boone stood by him. Both men were expelled from the Quakers; Boone's wife continued to attend monthly meetings with their younger children.
Yadkin River Valley, North Carolina.
In 1750, Squire Boone sold his land and moved the family to North Carolina. Daniel Boone did not attend church again. He identified as a Christian and had all of his children baptized. The Boones eventually settled on the Yadkin River, in what is now Davie County, about two miles (3 km) west of Mocksville. This was in the western backwoods area.
Because he grew up on the frontier, Boone had little formal education, but deep knowledge of the woods. According to one family tradition, a schoolteacher once expressed concern over Boone's education, but Boone's father said, "Let the girls do the spelling and Dan will do the shooting." Boone received some tutoring from family members, though his spelling remained unorthodox. The historian John Mack Faragher cautions that the folk image of Boone as semiliterate is misleading, and argues that he "acquired a level of literacy that was the equal of most men of his times." Boone regularly took reading material with him on his hunting expeditions—the Bible and "Gulliver's Travels" were favorites. He was often the only literate person in groups of frontiersmen. Boone would sometimes entertain his hunting companions by reading to them around the evening campfire.
French and Indian War.
After the French and Indian War (1754–1763) broke out between the French and British, and their respective Indian allies, North Carolina Governor Matthew Rowan called up a militia, into whose service Daniel volunteered. He served under Captain Hugh Waddell on the North Carolina frontier. Waddell's unit was assigned to serve in the command of General Edward Braddock in 1755, and Boone acted as a wagoner, along with his cousin Daniel Morgan, who would later be a key general in the American Revolution. In the Battle of the Monongahela, the denouement of the campaign and a bitter defeat for the British, Boone narrowly escaped death when the baggage wagons were assaulted by Indian troops. Boone remained critical of Braddock's blunders for the rest of his life.
While on the campaign, Boone met John Finley, a packer who worked for George Croghan in the trans-Appalachian fur trade. Finley first interested Boone in the abundance of game and other natural wonders of the Ohio Valley. Finley took Boone on his first fateful hunting trip to Kentucky 12 years later.
Marriage and family.
Boone returned home and on August 14, 1756, he married Rebecca Bryan, a neighbor in the Yadkin River Valley whose brother married one of Boone's sisters. The couple initially lived in a cabin on his father's farm. They eventually had 10 children. His son, Nathan Boone, was the first white man born in Kentucky.
Boone supported his growing family in these years as a market hunter and trapper, collecting pelts for the fur trade. Almost every autumn, Boone would go on "long hunts", extended expeditions into the wilderness lasting weeks or months. Boone went alone or with a small group of men, accumulating hundreds of deer skins in the autumn, and trapping beaver and otter over the winter. The hunt followed a network of bison migration trails, known as the Medicine Trails. When the long hunters returned in the spring, they sold their take to commercial fur traders.
Such frontiersmen often carved messages on trees or wrote their names on cave walls, and Boone's name or initials have been found in many places. One on a tree in present Washington County, Tennessee reads "D. Boon Cilled a. Bar a bear on tree in the year 1760". A similar carving, preserved in the museum of the Filson Historical Society in Louisville, Kentucky, reads "D. Boon Kilt a Bar, 1803." The inscriptions may also be among numerous forgeries of the famous trapper, part of a long tradition of phony Boone relics.
Cherokee conflict, temporary move to Virginia.
In 1758, a conflict erupted between the British forces and the Cherokee, their allies in the French and Indian War (which continued in other parts of the continent). After the Yadkin River Valley was raided by Cherokee, the Boones and many other families fled north to Culpeper County, Virginia. Boone served in the North Carolina militia during this "Cherokee Uprising". His militia expeditions deep into Cherokee territory beyond the Blue Ridge Mountains separated him from his wife for about two years.
In 1762, Boone, his wife and four children moved back to the Yadkin River Valley from Culpeper. By the mid-1760s, with peace made with the Cherokee, colonial immigration into the area increased. The competition of new settlers decreased the amount of game available. Boone had difficulty making ends meet; he was often taken to court for nonpayment of debts. He sold his land to pay off creditors. After his father's death in 1765, Boone traveled with his brother Squire and a group of men to Florida, which had become British territory after the end of the war, to look into the possibility of settling there. According to a family story, Boone purchased land near Pensacola, but Rebecca refused to move so far away from her friends and family. The Boones moved to a more remote area of the Yadkin River Valley, and Boone began to hunt westward into the Blue Ridge Mountains.
Kentucky.
Boone first reached Kentucky in the fall of 1767 while on a long hunt with his brother Squire Boone, Jr. Boone's first steps in Kentucky were near present-day Elkhorn City. While on the Braddock expedition years earlier, Boone had heard about the fertile land and abundant game of Kentucky from fellow wagoner John Findley, who had visited Kentucky to trade with American Indians. Boone and Findley happened to meet again, and Findley encouraged Boone with more tales of Kentucky. At the same time, news had arrived about the Treaty of Fort Stanwix, in which the Iroquois had ceded their claim to Kentucky to the British. This, as well as the unrest in North Carolina due to the Regulator Movement, likely prompted Boone to extend his exploration.
On May 11, 1769, Boone began a two-year hunting expedition in Kentucky. On December 22, 1769, a fellow hunter and he were captured by a party of Shawnees, who confiscated all of their skins and told them to leave and never return. The Shawnees had not signed the Stanwix treaty, and since they regarded Kentucky as their hunting ground, they considered white hunters there to be poachers. Boone, however, continued hunting and exploring Kentucky until his return to North Carolina in 1771, and returned to hunt there again in the autumn of 1772.
On September 25, 1773, Boone packed up his family and, with a group of about 50 immigrants, began the first attempt by British colonists to establish a settlement in Kentucky. Boone was still an obscure hunter and trapper at the time; the most prominent member of the expedition was William Russell, a well-known Virginian and future brother-in-law of Patrick Henry. On October 9, Boone's eldest son James and a small group of men and boys who had left the main party to retrieve supplies were attacked by a band of Delawares, Shawnees, and Cherokees. Following the Treaty of Fort Stanwix, American Indians in the region had been debating what to do about the influx of settlers. This group had decided, in the words of historian John Mack Faragher, "to send a message of their opposition to settlement". James Boone and William Russell's son Henry were captured and gruesomely tortured to death. The brutality of the killings sent shock waves along the frontier, and Boone's party abandoned its expedition.
The massacre was one of the first events in what became known as Dunmore's War, a struggle between Virginia and, primarily, Shawnees of the Ohio Country for control of what is now West Virginia and Kentucky. In the summer of 1774, Boone volunteered to travel with a companion to Kentucky to notify surveyors there about the outbreak of war. The two men journeyed more than in two months to warn those who had not already fled the region. Upon his return to Virginia, Boone helped defend colonial settlements along the Clinch River, earning a promotion to captain in the militia, as well as acclaim from fellow citizens. After the brief war, which ended soon after Virginia's victory in the Battle of Point Pleasant in October 1774, the Shawnees relinquished their claims to Kentucky.
Following Dunmore's War, Richard Henderson, a prominent judge from North Carolina, hired Boone to travel to the Cherokee towns in present North Carolina and Tennessee and inform them of an upcoming meeting. In the 1775 treaty, Henderson purchased the Cherokee claim to Kentucky to establish a colony called Transylvania. Afterwards, Henderson hired Boone to blaze what became known as the Wilderness Road, which went through the Cumberland Gap and into central Kentucky. Along with a party of about 30 workers, Boone marked a path to the Kentucky River, where he founded Boonesborough. Other settlements, notably Harrodsburg, were also established at this time. Despite occasional Indian attacks, Boone returned to the Clinch Valley and brought his family and other settlers to Boonesborough on September 8, 1775.
American Revolution.
Violence in Kentucky increased with the outbreak of the American Revolutionary War (1775–1783). Native Americans who were unhappy about the loss of Kentucky in treaties saw the war as a chance to drive out the colonists. Isolated settlers and hunters became the frequent target of attacks, convincing many to abandon Kentucky. By late spring of 1776, fewer than 200 colonists remained in Kentucky, primarily at the fortified settlements of Boonesborough, Harrodsburg, and Logan's Station.
On July 14, 1776, Boone's daughter Jemima and two other teenaged girls were captured outside Boonesborough by an Indian war party, who carried the girls north towards the Shawnee towns in the Ohio country. Boone and a group of men from Boonesborough followed in pursuit, finally catching up with them two days later. Boone and his men ambushed the Indians while they were stopped for a meal, rescuing the girls and driving off their captors. The incident became the most celebrated event of Boone's life. James Fenimore Cooper created a fictionalized version of the episode in his classic novel "The Last of the Mohicans" (1826).
In 1777, Henry Hamilton, a British Lieutenant Governor of Canada, began to recruit American Indian war parties to raid the Kentucky settlements. On April 24, Shawnee Indians led by Chief Blackfish attacked Boonesborough. Boone was shot in the ankle while outside the fort, but he was carried back inside amid a flurry of bullets by Simon Kenton, a recent arrival at Boonesborough. Kenton became Boone's close friend, as well as a legendary frontiersman in his own right.
While Boone recovered, Shawnees kept up their attacks outside Boonesborough, destroying the surrounding cattle and crops. With the food supply running low, the settlers needed salt to preserve what meat they had, so in January 1778, Boone led a party of 30 men to the salt springs on the Licking River. On February 7, 1778, when Boone was hunting meat for the expedition, he was surprised and captured by warriors led by Blackfish. Because Boone's party was greatly outnumbered, Boone returned the next day with Blackfish and persuaded his men to surrender rather than put up a fight.
Blackfish wanted to continue to Boonesborough and capture it, since it was now poorly defended, but Boone convinced him that the women and children were not hardy enough to survive a winter trek. Instead, Boone promised that Boonesborough would surrender willingly to the Shawnees the following spring. Boone did not have an opportunity to tell his men that he was bluffing to prevent an immediate attack on Boonesborough, however. Boone pursued this strategy so convincingly that many of his men concluded that he had switched his loyalty to the British.
Boone and his men were taken to Blackfish's town of Chillicothe, where they were made to run the gauntlet. As was their custom, the Shawnees adopted some of the prisoners into the tribe to replace fallen warriors; the remainder were taken to Hamilton in Detroit. Boone was adopted into a Shawnee family at Chillicothe, perhaps into the family of Chief Blackfish himself, and given the name Sheltowee (Big Turtle). On June 16, 1778, when he learned Blackfish was about to return to Boonesborough with a large force, Boone eluded his captors and raced home, covering the to Boonesborough in five days on horseback and, after his horse gave out, on foot.
During Boone's absence, his wife and children (except for Jemima) had returned to North Carolina, assuming he was dead. Upon his return to Boonesborough, some of the men expressed doubts about Boone's loyalty, since after surrendering the salt-making party, he had apparently lived quite happily among the Shawnees for months. Boone responded by leading a preemptive raid against the Shawnees across the Ohio River, and then by helping to successfully defend Boonesborough against a 10-day siege led by Blackfish, which began on September 7, 1778.
After the siege, Captain Benjamin Logan and Colonel Richard Callaway—both of whom had nephews who were still captives surrendered by Boone—brought charges against Boone for his recent activities. In the court martial that followed, Boone was found "not guilty", and was even promoted after the court heard his testimony. Despite this vindication, Boone was humiliated by the court martial, and he rarely spoke of it.
After the trial, Boone returned to North Carolina to bring his family back to Kentucky. In the autumn of 1779, a large party of emigrants came with him, including (according to tradition) the family of Abraham Lincoln's grandfather. Rather than remain in Boonesborough, Boone founded the nearby settlement of Boone's Station. He began earning money at this time by locating good land for other settlers. Transylvania land claims had been invalidated after Virginia created Kentucky County, so settlers needed to file new land claims with Virginia. In 1780, Boone collected about $20,000 in cash from various settlers and traveled to Williamsburg to purchase their land warrants. While he was sleeping in a tavern during the trip, the cash was stolen from his room. Some of the settlers forgave Boone the loss; others insisted he repay the stolen money, which took him several years to do.
A popular image of Boone which emerged in later years is that of the backwoodsman who had little affinity for "civilized" society, moving away from places like Boonesborough when they became "too crowded". In reality, however, Boone was a leading citizen of Kentucky at this time. When Kentucky was divided into three Virginia counties in November 1780, Boone was promoted to lieutenant colonel in the Fayette County militia. In April 1781, he was elected as a representative to the Virginia General Assembly, which was held in Richmond. In 1782, he was elected sheriff of Fayette County.
Meanwhile, the American Revolutionary War continued. Boone joined General George Rogers Clark's invasion of the Ohio country in 1780, fighting in the Battle of Piqua on August 7. In October, when Boone was hunting with his brother Ned, Shawnees shot and killed Ned. Apparently thinking that they had killed Daniel Boone, the Shawnees beheaded Ned and took the head home as a trophy. In 1781, Boone traveled to Richmond to take his seat in the legislature, but British dragoons under Banastre Tarleton captured Boone and several other legislators near Charlottesville. The British released Boone on parole several days later. During Boone's term, Cornwallis surrendered at Yorktown in October 1781, but the fighting continued in Kentucky unabated. Boone returned to Kentucky and in August 1782 fought in the Battle of Blue Licks, in which his son Israel was killed. In November 1782, Boone took part in another Clark expedition into Ohio, the last major campaign of the war.
Businessman on the Ohio River.
After the Revolution, Boone resettled in Limestone (renamed Maysville, Kentucky in 1786), then a booming Ohio River port. In 1787, he was elected to the Virginia state assembly as a representative from Bourbon County. In Maysville, he kept a tavern and worked as a surveyor, horse trader, and land speculator. He was initially prosperous, owning seven slaves by 1787, a relatively large number for Kentucky at the time. Boone became a celebrity while living in Maysville. In 1784, on his 50th birthday, historian John Filson published "The Discovery, Settlement And present State of Kentucke", a book which included a chronicle of Boone's adventures.
The Revolutionary War had ended, but the border war with American Indians north of the Ohio River resumed with the Northwest Indian War. In September 1786, Boone took part in a military expedition into the Ohio Country led by Benjamin Logan. Back in Limestone, Boone housed and fed Shawnees who were captured during the raid, and helped to negotiate a truce and prisoner exchange. Although the war escalated and would not end until the American victory at the Battle of Fallen Timbers in 1794, the 1786 expedition was the last time Boone saw military action.
Boone began to have financial troubles while living in Maysville. According to the later folk image, Boone the trailblazer was too unsophisticated for the civilization which followed him and which eventually defrauded him of his land. Boone was not the simple frontiersman of legend, however: he engaged in land speculation on a large scale, buying and selling claims to tens of thousands of acres. The land market in frontier Kentucky was chaotic, and Boone's ventures ultimately failed because his investment strategy was faulty and because his sense of honor made him reluctant to profit at someone else's expense. According to Faragher, "Boone lacked the ruthless instincts that speculation demanded."
Frustrated with the legal hassles that went with land speculation, in 1788, Boone moved upriver to Point Pleasant, Virginia (now West Virginia). There he operated a trading post and occasionally worked as a surveyor's assistant. When Virginia created Kanawha County in 1789, Boone was appointed lieutenant colonel of the county militia. In 1791, he was elected to the Virginia legislature for the third time. He contracted to provide supplies for the Kanawha militia, but his debts prevented him from buying goods on credit, so he closed his store and returned to hunting and trapping.
In 1795, Rebecca and he moved back to Kentucky, living in present Nicholas County on land owned by their son Daniel Morgan Boone. The next year, Boone applied to Isaac Shelby, the first governor of the new state of Kentucky, for a contract to widen the Wilderness Road into a wagon route, but the contract was awarded to someone else. Meanwhile, lawsuits over conflicting land claims continued to make their way through the Kentucky courts. Boone's remaining land claims were sold off to pay legal fees and taxes, but he no longer paid attention to the process. In 1798, a warrant was issued for Boone's arrest after he ignored a summons to testify in a court case, although the sheriff never found him. That same year, the Kentucky assembly named Boone County in his honor.
Missouri.
Having endured legal and financial setbacks, Boone sought to make a fresh start by leaving the United States. In 1799, he moved his extended family to what is now St. Charles County, Missouri, but was then part of Spanish Louisiana. The Spanish, eager to promote settlement in the sparsely populated region, did not enforce the official requirement that all immigrants had to be Roman Catholic. The Spanish governor appointed Boone "syndic" (judge and jury) and commandant (military leader) of the Femme Osage district, The many anecdotes of Boone's tenure as syndic suggest he sought to render fair judgments rather than to strictly observe the letter of the law.
Boone served as syndic and commandant until 1804, when Missouri became part of the United States following the Louisiana Purchase. Because Boone's land grants from the Spanish government had been largely based on verbal agreements, he once again lost his land claims. In 1809, he petitioned Congress to restore his Spanish land claims, which was finally done in 1814. Boone sold most of this land to repay old Kentucky debts. When the War of 1812 came to Missouri, Boone's sons Daniel Morgan Boone and Nathan Boone took part, but by that time Boone was much too old for militia duty.
Boone spent his final years in Missouri, often in the company of children and grandchildren, where he continued to hunt and trap as much as his health and energy levels permitted. According to one story, in 1810 or later, Boone went with a group on a long hunt as far west as the Yellowstone River, a remarkable journey at his age, if true. In 1816, a United States officer at Fort Osage, on the Missouri, wrote:
Stories were told of Boone making one last visit to Kentucky to pay off his creditors, although some or all of these tales may be folklore. American painter John James Audubon claimed to have gone hunting with Boone in the woods of Kentucky around 1810. Years later, Audubon painted a portrait of Boone, supposedly from memory, although skeptics have noted the similarity of this painting to the well-known portraits by Chester Harding. Boone's family insisted he never returned to Kentucky after 1799, although some historians believe Boone visited his brother Squire near Kentucky in 1810 and have therefore reported Audubon's story as factual.
Death.
Daniel Boone died of natural causes, other sources, from accute indigestion, on September 26, 1820, at Nathan Boone's home on Femme Osage Creek, 2-1/2 months short of his 86th birthday. His last words were, "I'm going now. My time has come." He was buried next to Rebecca, who had died on March 18, 1813. The graves, which were unmarked until the mid-1830s, were near Jemima (Boone) Callaway's home on Tuque Creek, about two miles (3 km) from the present-day Marthasville, Missouri. In 1845, the Boones' remains were supposedly disinterred and reburied in a new cemetery Frankfort Cemetery in Frankfort, Kentucky. Resentment in Missouri about the disinterment grew over the years, and a legend arose that Boone's remains never left Missouri. According to this story, Boone's tombstone in Missouri had been inadvertently placed over the wrong grave, but no one had ever corrected the error. Boone's relatives in Missouri, displeased with the Kentuckians who came to exhume Boone, kept quiet about the mistake, and they allowed the Kentuckians to dig up the wrong remains. No contemporary evidence indicates this actually happened, but in 1983, a forensic anthropologist examined a crude plaster cast of Boone's skull made before the Kentucky reburial and announced it might be the skull of an African American. Black slaves had also been buried at Tuque Creek, so it is possible the wrong remains were mistakenly removed from the crowded graveyard. Both the Frankfort Cemetery in Kentucky and the Old Bryan Farm graveyard in Missouri claim to have Boone's remains.
Cultural legacy.
Daniel Boone remains an iconic figure in American history, although his status as an early American folk hero and later as a subject of fiction have tended to obscure the actual details of his life. Boone is commonly remembered as a hunter, pioneer, and "Indian-fighter", though most people are uncertain when he lived or exactly what he did. Several places in the United States are named for him, including the Daniel Boone National Forest, the Sheltowee Trace Trail, the town of Boone, North Carolina, various settlements carrying the name of "Boonville", and seven counties: Boone County, Illinois, Boone County, Indiana, Boone County, Nebraska, Boone County, West Virginia, Boone County, Missouri, Boone County, Arkansas, and Boone County, Kentucky. Schools across the United States are named for Daniel Boone, including schools in Birdsboro, Pennsylvania, Douglassville, Pennsylvania, Richmond, Kentucky, Wentzville, Missouri, Warrenton, Missouri, Gray, Tennessee, and Chicago.
Boone's name has long been synonymous with the American outdoors. For example, the Boone and Crockett Club was a conservationist organization founded by Theodore Roosevelt in 1887, and the Sons of Daniel Boone was the precursor of the Boy Scouts of America.
Daniel Boone was honored with a 6-cent stamp in the American Folklore Series on September 26, 1968, at Frankfort, Kentucky, where he was buried. He was a famous frontiersman in the development of Virginia, Kentucky and the trans-Appalachian west. A wall of roughly-hewn boards displays the tools of Boone's trade—a Pennsylvania rifle, a powder horn, and a knife. The pipe tomahawk represents that the Shawnees had adopted Boone. His name and birth date were carved on the wall.
The U.S. Navy's Polaris submarine , was named for Boone. This nuclear submarine was decommissioned in 1994, and has since been scrapped. She was a member of a class of 41 submarines, all of which were named for great Americans from history, including the , two other noteworthy frontiersmen of the Great West.
Emergence as a legend.
Boone emerged as a legend in large part because of land speculator John Filson's "The Adventures of Colonel Daniel Boon", part of his book "The Discovery, Settlement And present State of Kentucke". First published in 1784, Filson's book was a pamphlet primarily intended to popularize Kentucky to immigrants. It was soon translated into French and German, and made Boone famous in America and Europe. Based on interviews with Boone, Filson's book contained a mostly factual account of Boone's adventures from the exploration of Kentucky through the American Revolution. However, because the real Boone was a man of few words, Filson invented florid, philosophical dialogue for this "autobiography". Subsequent editors cut some of these passages and replaced them with more plausible—but still spurious—ones. Often reprinted, Filson's book established Boone as one of the first popular heroes of the United States.
Like John Filson, Timothy Flint also interviewed Boone, and his "Biographical Memoir of Daniel Boone, the First Settler of Kentucky" (1833) became one of the best-selling biographies of the 19th century. Flint greatly embellished Boone's adventures, doing for Boone what Parson Weems did for George Washington. In Flint's book, Boone fought hand-to-paw with a bear, escaped from Indians by swinging on vines (as Tarzan would later do), and so on. Although Boone's family thought the book was absurd, Flint greatly influenced the popular conception of Boone, since these tall tales were recycled in countless dime novels and books aimed at young boys.
Symbol and stereotype.
Thanks to Filson's book, in Europe, Boone became a symbol of the "natural man" who lives a virtuous, uncomplicated existence in the wilderness. This was most famously expressed in Lord Byron's epic poem "Don Juan" (1822), which devoted a number of stanzas to Boone, including this one:
Byron's poem celebrated Boone as someone who found happiness by turning his back on civilization. In a similar vein, many folk tales depicted Boone as a man who migrated to more remote areas whenever civilization crowded in on him. In a typical anecdote, when asked why he was moving to Missouri, Boone supposedly replied, "I want more elbow room!" Boone rejected such an interpretation of his life, however. "Nothing embitters my old age," he said late in life, like "the circulation of absurd stories that I retire as civilization advances…."
Existing simultaneously with the image of Boone as a refugee from society was, paradoxically, the popular portrayal of him as civilization's trailblazer. Boone was celebrated as an agent of Manifest Destiny, a pathfinder who tamed the wilderness, paving the way for the extension of American civilization. In 1852, critic Henry Tuckerman dubbed Boone "the Columbus of the woods", comparing Boone's passage through the Cumberland Gap to Christopher Columbus's voyage to the New World. In popular mythology, Boone became the first to explore and settle Kentucky, opening the way for countless others to follow. In fact, other Americans had explored and settled Kentucky before Boone, as debunkers in the 20th century often pointed out, but Boone came to symbolize them all, making him what historian Michael Lofaro called "the founding father of westward expansion".
In the 19th century, when Native Americans were being displaced from their lands and confined on reservations, Boone's image was often reshaped into the stereotype of the belligerent, Indian-hating frontiersman which was then popular. In John A. McClung's "Sketches of Western Adventure" (1832), for example, Boone was portrayed as longing for the "thrilling excitement of savage warfare." Boone was transformed in the popular imagination into someone who regarded Indians with contempt and had killed scores of the "savages". The real Boone disliked bloodshed, however. According to historian John Bakeless, there is no record that Boone ever scalped Indians, unlike other frontiersmen of the era. Boone once told his son Nathan that he was certain of having killed only one Indian, during the battle at Blue Licks, although he believed others might have died from his bullets in other battles. Even though Boone had lost two sons in wars with Indians, he respected Indians and was respected by them. In Missouri, Boone often went hunting with the very Shawnees who had captured and adopted him decades earlier. Some 19th-century writers regarded Boone's sympathy for Indians as a character flaw and therefore altered his words to conform to contemporary attitudes.
In fiction.
Boone's adventures, real and mythical, formed the basis of the archetypal hero of the American West, popular in 19th-century novels and 20th-century films. The main character of James Fenimore Cooper's Leatherstocking Tales, the first of which was published in 1823, bore striking similarities to Boone; even his name, Nathaniel Bumppo, echoed Daniel Boone's name. As mentioned above, "The Last of the Mohicans" (1826), Cooper's second Leatherstocking novel, featured a fictionalized version of Boone's rescue of his daughter. After Cooper, other writers developed the Western hero, an iconic figure which began as a variation of Daniel Boone.
In the 20th century, Boone was featured in numerous comic strips, radio programs, and films, where the emphasis was usually on action and melodrama rather than historical accuracy. These are little remembered today; probably the most noteworthy is the 1936 film "Daniel Boone", with George O'Brien playing the title role. Horn in the West, an outdoor drama performed annually in Boone, North Carolina since 1952, is a fictional account of the lives of settlers whom Daniel Boone had led into the Appalachian Mountains.
Daniel Boone was the subject of a TV series that ran on NBC from 1964 to 1970. In the popular theme song for the series, Boone was described as a "big man" in a "coonskin cap", and the "rippin'est, roarin'est, fightin'est man the frontier ever knew!" This did not describe the real Daniel Boone, who was not a big man and did not wear a coonskin cap. Boone was portrayed this way because Fess Parker, the tall actor who played Boone, was essentially reprising his role as Davy Crockett from an earlier TV series. That Boone could be portrayed the same way as Crockett, another American frontiersman with a very different persona, was another example of how Boone's image could be reshaped to suit popular tastes.
External links.
Primary material
Other material

</doc>
<doc id="71796" url="https://en.wikipedia.org/wiki?curid=71796" title="Scot (disambiguation)">
Scot (disambiguation)

A Scot is a member of an ethnic group indigenous to Scotland, derived from the Latin name of Irish raiders, the "Scoti".
Scot may also refer to:

</doc>
<doc id="71798" url="https://en.wikipedia.org/wiki?curid=71798" title="Les Halles">
Les Halles

Les Halles de Paris, usually simply Les Halles (, "The Halls"), was the name given to Paris's central fresh food market. Located in the heart of the city, it was demolished in 1971 and replaced with the Forum des Halles, a modern shopping mall built largely underground and directly connected to the massive RER and métro transit hub of Châtelet-Les-Halles. The shopping mall welcomes visitors daily.
Since 2010, a major reconstruction of the mall is under progress. The new version is planned to be inaugurated by 2016. The mall remains open during works. In 2013, the Forum des Halles was still the second most visited shopping mall in France with 39.2 million visitors.
History.
The wholesale market.
Les Halles was the traditional central market of Paris. In 1183, King Philippe II Auguste enlarged the marketplace in Paris and built a shelter for the merchants, who came from all over to sell their wares. The church of Saint-Eustache was constructed in the 16th century. The circular Halle aux Blés (Corn Exchange), designed by Nicolas Le Camus de Mézières, was built between 1763 and 1769 at the west end of Les Halles. Its circular central court was later covered with a dome, and it was converted into the Bourse de Commerce in 1889. In the 1850s, Victor Baltard designed the famous glass and iron buildings, Les Halles, which would last until the 1970s. Les Halles was known as the "Belly of Paris", as it was called by Émile Zola in his novel "Le Ventre de Paris", which is set in the busy marketplace of the 19th century.
Major conversion.
Unable to compete in the new market economy and in need of massive repairs, the colourful ambience once associated with the bustling area of merchant stalls disappeared in 1971, when Les Halles was dismantled; the wholesale market was relocated to the suburb of Rungis. Two of the glass and cast iron market pavilions were dismantled and re-erected elsewhere; one in the Paris suburb of Nogent-sur-Marne, the other in Yokohama, Japan.
The site was to become the point of convergence of the RER, a network of new express underground lines which was completed in the 1960s. Three lines leading out of the city to the south, east and west were to be extended and connected in a new underground station. For several years, the site of the markets was an enormous open pit, nicknamed "le trou des Halles" ("trou" = hole), regarded as an eyesore at the foot of the historic church of Saint-Eustache.
Construction was completed in 1977 on Châtelet-Les-Halles, Paris's new urban railway hub. The Forum des Halles, a partially underground multiple story commercial and shopping center, opened at the east end of the site in 1979 and remains there today. A public garden covering four hectares opened in 1986. Many of the surrounding streets were pedestrianized.
Paris Les Halles: an urban transit hub to redevelop.
Gare de Châtelet – Les Halles is Paris's most used rail station, serving 750,000 travelers on an average weekday. The buildings and their surroundings have been criticized for their design. In 2002 Mayor Bertrand Delanoë announced that the City of Paris would begin public consultations regarding the remodeling of the area, calling Les Halles "a soulless, architecturally bombastic concrete jungle".
A design competition for the Forum and gardens was held, with entries from Jean Nouvel, Winy Maas, David Mangin, and Rem Koolhaas. Mangin's design for the gardens, which proposed replacing the landscaped mounds and paths of the 1980s design with a simplified pattern of east-west pedestrian promenades and a large central lawn, was selected. The plan also includes extending the pedestrianized area further east to include all the streets bordering the gardens. Another competition was held for the redesign of the Forum. Ten teams submitted plans, and the proposal by Patrick Berger and Jacques Anziutti was selected in 2007. Their design includes a large undulating glass canopy which will cover the redesigned Forum. STIF and RATP began plans for the remodeling of the Châtelet-Les-Halles station in 2007, and the following year Berger and Anziutti were awarded a contract for redesign of the station.
The station redesign includes new entrances on Rue Berger, Rue Rambuteau, and Place Marguerite de Navarre, an expanded RER concourse, and improved pedestrian circulation. Construction began in 2010 on a project which includes the gardens, Forum, and station, and is scheduled to continue through 2016. The clients are the City of Paris, RATP, which operates the Paris Metro, and La Société Civile du Forum des Halles de Paris, which operates the Forum.
In film.
Part of the actual demolition of the site is featured in the 1974 film "Touche pas à la femme blanche" ("Don't Touch the White Woman!"), which iconoclastically restages General Custer's 'last stand' in a distinctly French context in and around the area.
In 1977, Roberto Rossellini made a 54-minute documentary film that testified to the public's response to the demolition of Les Halles and the construction of Centre Georges Pompidou. "The result was a sceptical vision rather than a pure celebration."
The open-air market and Baltard's pavilions were digitally reconstructed for the 2004 film "Un long dimanche de fiançailles" ("A Very Long Engagement"), which was set after the First World War.

</doc>
<doc id="71802" url="https://en.wikipedia.org/wiki?curid=71802" title="Major League Soccer">
Major League Soccer

Major League Soccer (MLS) is a professional soccer league, sanctioned by U.S. Soccer, that represents the sport's highest level in both the United States and Canada. MLS constitutes one of the major professional sports leagues of the United States and Canada. The league is composed of 20 teams—17 in the U.S. and 3 in Canada. The MLS regular season runs from March to October, with each team playing 34 games; the team with the best record is awarded the Supporters' Shield. The post season includes twelve teams competing in the MLS Cup Playoffs through November and December, culminating in the championship game, the MLS Cup. MLS teams also play in other domestic competitions against teams from other divisions in the U.S. Open Cup and in the Canadian Championship. MLS teams also compete against continental rivals in the CONCACAF Champions League.
Major League Soccer was founded in 1993 as part of the United States' successful bid to host the 1994 FIFA World Cup. The first season took place in 1996 with ten teams. MLS experienced financial and operational struggles in its first few years: The league lost millions of dollars, teams played in mostly empty American football stadiums, and two teams folded in 2002. Since then, MLS has expanded to 20 teams, owners built soccer-specific stadiums, average MLS attendance exceeds that of the National Hockey League (NHL) and National Basketball Association (NBA), MLS secured national TV contracts, and the league is now profitable.
Instead of operating as an association of independently owned teams, MLS is a single entity in which each team is owned and controlled by the league's investors. The investor-operators control their teams as owners control teams in other leagues, and are commonly (but inaccurately) referred to as the team's owners. The league has a fixed membership, like most sports leagues in the United States and Canada, which makes it one of the world's few soccer leagues that does not use promotion and relegation, a practice that is uncommon in the two countries. MLS headquarters is located in New York City.
Competition format.
Major League Soccer's regular season runs from March to October. Teams are divided into the Eastern and Western Conferences. Teams play 34 games in an unbalanced schedule: 24 matches against teams within their conference, plus 10 matches against teams from the other conference. Midway through the season, teams break for the annual All-Star Game, a friendly game between the league's finest players and a major club from a different league. At the end of the regular season, the team with the highest point total is awarded the Supporters' Shield.
Unlike other soccer leagues around the world, the MLS regular season is followed by the 12-team MLS Cup Playoffs in November, ending with the MLS Cup championship final in early December.
Although some commentators have argued that playoffs reduce the importance of the regular season, Commissioner Don Garber has explained "Our purpose is to have a valuable competition, and that includes having playoffs that are more meaningful."
Major League Soccer's spring-to-autumn schedule results in scheduling conflicts with the FIFA calendar and with summertime international tournaments such as the World Cup and the Gold Cup, causing several players to miss some MLS matches.
While MLS has looked into changing to an autumn-to-spring format, there are no current plans to do so. If the league were to change its schedule, a winter break would be needed, especially with several teams in colder climates, which some believe would lead to a disadvantage. It would also have to compete with the more popular National Football League (NFL), National Hockey League (NHL), and National Basketball Association (NBA).
Other competitions.
MLS teams also play in other competitions. Every year, up to five MLS teams play in the CONCACAF Champions League against other clubs from the CONCACAF region (Mexico, Central America, and the Caribbean). Two U.S.-based MLS teams qualify based on MLS regular-season results: the winner of the Supporters' Shield and the winner of the other conference. The third U.S. team to qualify is the winner of the MLS Cup. A fourth U.S.-based MLS team can qualify via the U.S. Open Cup, where U.S. based teams compete against lower division U.S. clubs. Canadian MLS clubs play against lower division Canadian clubs in the Canadian Championship for the one Champions League spot allocated to Canada. No MLS club has won the Champions League since it began its current format in 2008, with Mexican clubs dominating the competition, but MLS teams have twice reached the final: Real Salt Lake in 2011 and the Montreal Impact in 2015.
Teams.
MLS's 20 teams are divided between the Eastern and Western Conference. Each club is allowed up to 30 players on its first team roster. All 30 players are eligible for selection to each 18-player game-day squad during the regular season and playoffs.
Since the 2005 season, MLS has added many new clubs. During this period of expansion, Los Angeles became the first two-team market, and the league pushed into Canada in 2007. The league will expand from 20 teams today to 22 teams in 2017 with the additions of Atlanta and either Los Angeles or Minnesota, and then to 23 teams in 2018 with the addition of Minnesota or Los Angeles, depending which team joins the league the preceding year. The league plans to have 24 teams by 2020.
In the history of MLS, twenty-three different clubs have competed in the league, with ten having won at least one MLS Cup, and ten winning at least one Supporters' Shield. The same club has won both trophies six times.
Several teams compete annually for secondary MLS rivalry cups that are typically contested by two teams, usually geographic rivals (e.g., Portland vs. Seattle vs. Vancouver). Each cup is awarded to the team with the better regular-season record in games played between the two teams. The concept is comparable to minor trophies played for by American college football teams.
Beginning with the 2015 season, teams are aligned as follows:
History.
Major League Soccer is the most recent of a series of men's premier professional national soccer leagues established in the United States and Canada.
The predecessor of MLS was the North American Soccer League (NASL), which played from 1968 until 1984.
Establishment.
In 1988, in exchange for FIFA awarding the right to host the 1994 World Cup, U.S. Soccer promised to establish a Division 1 professional soccer league. In 1993, U.S. Soccer selected Major League Professional Soccer (the precursor to MLS) as the exclusive Division 1 professional soccer league. Major League Soccer was officially formed in February 1995 as a limited liability company.
MLS began play in 1996 with ten teams. The first game was held on April 6, 1996, as the San Jose Clash defeated D.C. United before 31,000 fans at Spartan Stadium in San Jose in a game broadcast on ESPN. The league had generated some buzz by managing to lure some marquee players from the 1994 World Cup to play in MLS—including U.S. stars such as Alexi Lalas, Tony Meola and Eric Wynalda, and foreign players such as Mexico's Jorge Campos and Colombia's Carlos Valderrama.
D.C. United won the MLS Cup in three of the league's first four seasons. The league added its first two expansion teams in 1998—the Miami Fusion and the Chicago Fire; the Chicago Fire won its first title in its inaugural season.
After its first season, MLS suffered from a decline in attendance. The league's low attendance was all the more apparent in light of the fact that eight of the original ten teams played in large American football stadiums.
One aspect that had alienated fans was that MLS experimented with rules deviations in its early years in an attempt to "Americanize" the sport. The league implemented the use of shootouts to resolve tie games. MLS also used a countdown clock and halves ended when the clock reached 0:00. The league realized that the rule changes had alienated some traditional soccer fans while failing to draw new American sports fans, and the shootout and countdown clock were eliminated after the 1999 season.
The league's quality was cast into doubt when the U.S. men's national team, which was made up largely of MLS players, finished in last place at the 1998 World Cup.
Major League Soccer lost an estimated $250 million during its first five years, and more than $350 million between its founding and 2004.
The league's financial problems led to Commissioner Doug Logan being replaced by Garber, a former NFL executive, in August 1999.
MLS announced in January 2002 that it had decided to contract the Tampa Bay Mutiny and Miami Fusion, leaving the league with ten teams.
Despite the financial problems, though, MLS did have some accomplishments that would set the stage for the league's resurgence. Columbus Crew Stadium was built in 1999, becoming MLS's first soccer-specific stadium. This began a trend among MLS teams to construct their own venues instead of leasing American football stadiums.
In 2000, the league won an antitrust lawsuit, Fraser v. Major League Soccer, that the players had filed in 1996. The court ruled that MLS's policy of centrally contracting players and limiting player salaries through a salary cap and other restrictions were a legal method for the league to maintain solvency and competitive parity.
Resurgence.
The 2002 FIFA World Cup, in which the United States unexpectedly made the quarterfinals, coincided with a resurgence in American soccer and MLS. MLS Cup 2002 drew 61,316 spectators to Gillette Stadium, the largest attendance in an MLS Cup final. MLS limited teams to three substitutions per game in 2003, and adopted International Football Association Board (IFAB) rules in 2005.
MLS underwent a transition in the years leading up to the 2006 World Cup. After marketing itself on the talents of American players, the league lost some of its homegrown stars to prominent European leagues. For example, Tim Howard was transferred to Manchester United for $4 million in one of the most lucrative contract deals in league history. Many more American players did make an impact in MLS. In 2005, Jason Kreis became the first player to score 100 career MLS goals.
The league's financial stabilization plan included teams moving out of large American football stadiums and into soccer-specific stadiums. From 2003 to 2008, the league oversaw the construction of six additional soccer-specific stadiums, largely funded by owners such as Lamar Hunt and Phil Anschutz, so that by the end of 2008, a majority of teams were now in soccer-specific stadiums.
It was also in this era that MLS expanded for the first time since 1998. Real Salt Lake and Chivas USA began play in 2005, with Chivas USA becoming the second club in Los Angeles. By 2006 the San Jose Earthquakes owners, players and a few coaches moved to Texas to become the expansion Houston Dynamo, after failing to build a stadium in San Jose. The Dynamo became an expansion team, leaving their history behind for a new San Jose ownership group that formed in 2007.
Arrival of Designated Players.
In 2007 the league expanded beyond the United States' borders into Canada with the Toronto FC expansion team. Major League Soccer took steps to further raise the level of play by adopting the Designated Player Rule, which helped bring international stars into the league.
The 2007 season witnessed the MLS debut of David Beckham. Beckham's signing had been seen as a coup for American soccer, and was made possible by the Designated Player Rule. Players such as Cuauhtémoc Blanco (Chicago Fire) and Juan Pablo Ángel (New York Red Bulls), are some of the first Designated Players who made major contributions to their clubs.
The departures of Clint Dempsey and Jozy Altidore, coupled with the return of former U.S. national team stars Claudio Reyna and Brian McBride, highlighted the exchange of top prospects to Europe for experienced veterans to MLS.
By 2008, San Jose had returned to the league under new ownership, and in 2009, the expansion side Seattle Sounders FC began play in MLS. The 2010 season ushered in an expansion franchise in the Philadelphia Union and their new PPL Park stadium. The 2010 season also brought the opening of the New York Red Bulls' soccer-specific stadium, Red Bull Arena, and the debut of French striker Thierry Henry.
The 2011 season brought further expansion with the addition of the Vancouver Whitecaps FC, the second Canadian MLS franchise, and the Portland Timbers. Real Salt Lake reached the finals of the 2010–11 CONCACAF Champions League.
During the 2011 season, the Galaxy signed another international star in Republic of Ireland all-time leading goalscorer Robbie Keane. MLS drew an average attendance of 17,872 in 2011, higher than the average attendances of the NBA and NHL. In 2012, the Montreal Impact became the league's 19th franchise and the third in Canada, and made their home debut in front of a crowd of 58,912, while the New York Red Bulls added Australian all-time leading goalscorer Tim Cahill.
2013–present.
In 2013, MLS introduced New York City FC as its 20th team, and Orlando City Soccer Club as its 21st team, both of which would begin playing in 2015.
In 2013, the league implemented its "Core Players" initiative, allowing teams to retain key players using retention funds instead of losing the players to foreign leagues. Among the first high-profile players re-signed in 2013 using retention funds were U.S. national team regulars Graham Zusi and Matt Besler.
Beginning in summer of 2013 and continuing in the run up to the 2014 World Cup, MLS began signing U.S. stars based abroad, including Clint Dempsey from the English Premier League to Seattle, DaMarcus Beasley from the Liga MX to Houston, Jermaine Jones from the German Bundesliga to New England and Michael Bradley who returned from Italy to join Toronto who also signed England International Striker Jermain Defoe. By the 2014 season, fifteen of the nineteen MLS head coaches had previously played in MLS. By 2013, the league's popularity had increased to the point where MLS was as popular as Major League Baseball among 12- to 17-year-olds, as reported by the 2013 Luker on Trends ESPN poll, having jumped in popularity since the 2010 World Cup.
In 2014, the league announced Atlanta United FC as the 22nd team to start playing in 2017. Even though New York City FC and Orlando City were not set to begin play until 2015, each team made headlines during the summer 2014 transfer window by announcing their first Designated Players – Spain's leading scorer David Villa and Chelsea's leading scorer Frank Lampard to New York, and Ballon d'Or winner Kaká to Orlando. The 2014 World Cup featured 21 MLS players on World Cup rosters and a record 11 MLS players playing for foreign teams – including players from traditional powerhouses Brazil (Júlio César), playing for Toronto FC on loan from Queens Park Rangers FC, and Spain (David Villa), on loan to Melbourne City FC from New York City FC; in the U.S. v. Germany match the U.S. fielded a team with seven MLS starters.
On September 18, 2014, MLS unveiled their new logo as part of the "MLS Next" branding initiative. In addition to the new crest logo, MLS teams display versions in their own colors that are displayed on their jerseys at every game. This change represents the first time that the MLS logo has been changed since the league's inception. Chivas USA folded following the 2014 season, while New York City FC and Orlando City SC joined the league in 2015 as the 19th and 20th teams. Sporting Kansas City and the Houston Dynamo moved from the Eastern Conference to the Western Conference in 2015 to make two 10-team conferences.
In early 2015, the league announced that two teams — Los Angeles FC and Minnesota United — would join MLS in either 2017 or 2018. The 20th season of MLS saw the arrivals of several players who have starred at the highest levels of European club soccer and in international soccer: Giovanni Dos Santos, Kaká, Andrea Pirlo, Frank Lampard, Steven Gerrard, Didier Drogba, David Villa, and Sebastian Giovinco. On December 6, 2015, MLS announced its intent to expand to 28 teams.
League championships.
MLS Cup titles and Supporters' Shield Wins
Organization.
Ownership.
Major League Soccer operates under a single-entity structure in which teams and player contracts are centrally owned by the league. Each team has an investor-operator that is a shareholder in the league. In order to control costs, MLS shares revenues and holds players contracts instead of players contracting with individual teams. In "Fraser v. Major League Soccer", a lawsuit filed in 1996 and decided in 2002, the league won a legal battle with its players in which the court ruled that MLS was a single entity that can lawfully centrally contract for player services. The court also ruled that even absent their collective bargaining agreement, players could opt to play in other leagues if they were unsatisfied.
Having multiple clubs owned by a single owner was a necessity in the league's first ten years. At one time Phil Anschutz's AEG owned six MLS clubs and Lamar Hunt's Hunt Sports owned three franchises. In order to attract additional investors, in 2002 the league announced changes to the operating agreement between the league and its teams to improve team revenues and increase the incentives to be an individual club owner. These changes included granting owners the rights to a certain number of players they develop through their club's academy system each year, sharing the profits of Soccer United Marketing, and being able to sell individual club jersey sponsorships.
As MLS appeared to be on the brink of overall profitability in 2006 and developed significant expansion plans, MLS announced that it wanted each club to have a distinct operator. The league has attracted new ownership that have injected more money into the league. Examples include Red Bull's purchase of the MetroStars from AEG in 2006 for over $100 million.
The league now has 20 investor-operators for its 20 clubs. Hunt Sports owns only one team (FC Dallas). AEG owns the LA Galaxy and retained a 50% interest in the Houston Dynamo until December 2015. For the 2014 season, the league owned the former Chivas USA club, which had suffered from mismanagement and poor financial results under its individual operator relationship. The league eventually dissolved the team, in favor of awarding rights to a second soccer club in the Los Angeles area to a new ownership group on October 30, 2014.
Player acquisition and salaries.
The average salary for MLS players is $283,000, lower than the average salaries in England's second-tier Football League Championship ($344,000 in 2011), Holland's Eredivisie ($445,000), or Mexico's Liga MX ($418,000). The league's minimum player salary will increase in 2016 from $60,000 to $62,500 for most players, and roster players #25–28 will see their minimum salary increase from $50,000 to $51,500.
MLS salaries are limited by a salary cap, which MLS has had in place since the league's inception in 1996. The purpose of the salary cap is to prevent the team's owners from unsustainable spending on player salaries—a practice that had doomed the North American Soccer League during the 1980s—and to prevent a competitive imbalance among teams. The salary cap survived a legal challenge by the players in the "Fraser v. Major League Soccer" lawsuit. The 2016 salary cap is increasing from $3.49 million to $3.66 million per team.
Teams may augment their squads by signing players from other leagues. MLS has two transfer windows—the primary pre-season transfer window lasts three months from mid February until mid May, and the secondary mid season transfer window runs one month from early July to early August. All MLS teams have a limited number of international roster slots that they can use to sign non-domestic players. However MLS teams regularly obtain green cards for their non-domestic players in order to qualify them for domestic status and free up international roster slots. In 2015 48.97% of MLS players were born outside of the U.S. and Canada, with players from 58 countries represented.
MLS has also introduced various initiatives and rules intended to improve quality of players while still maintaining the salary cap. Rules concerning Designated Players, Generation Adidas players, home grown players, and allocation money all allow for additional wage spending that is exempt from the salary cap. These initiatives have brought about an increase in on-field competition.
The designated player (DP) rule allows teams to sign a limited number of players whose salary exceeds the maximum cap, each DP player only counts as $457,500 (the maximum non-DP salary) against the cap in 2016. Instituted in 2007, England's David Beckham was the first signing under the DP rule. The DP rule has led to large income inequality in MLS with top DPs earning as much as 180 times more than a player earning the league minimum. In the 2013 season 21% of the league's wage spending went to just 5 players, this stretched to 29% on the top 6 players in the 2014 season.
The league's "Core Players" initiative allows teams to re-sign players using retention funds that do not count against the salary cap. Retention funds were implemented in 2013 as a mechanism for MLS to retain key players; among the first high-profile players re-signed using retention funds were U.S. national team regulars Graham Zusi and Matt Besler. MLS teams can also obtain allocation money, which is money that the team can use on player salaries that does not count against the cap, and teams can earn allocation money in several ways, such as from the transfer fees earned by selling players to teams in other leagues. MLS teams can also use Targeted Allocation Money (often referred to as TAM), an initiative announced in 2015. Teams can use TAM funds to attract high-profile players by "buying down" contracts of players to below the Designated Player level. High-profile players for which TAM funds were used include Omar Gonzalez. 
The league operates a Generation Adidas program, which is a joint venture between MLS and U.S. Soccer that encourages young American players to enter MLS. The Generation Adidas program has been in place since 1997, and has introduced players such as Landon Donovan, Clint Dempsey, Tim Howard and Michael Bradley into MLS. Players under the Home Grown Player rule are signed to Generation Adidas contracts, all players on Generation Adidas contracts are "off budget players" and their salaries do not count against the cap.
MLS has required all of its teams to operate youth development programs since 2008. MLS roster rules allow teams to sign an unlimited number players straight from their academies and bypassing the draft process. There is also supplementary salary budget made by MLS only for homegrown players that are registered using senior roster slots called homegrown player funds. One of the most prominent and lucrative examples of success in "home-grown" development was Jozy Altidore, who rose to prominence as a teenager in MLS before his record transfer fee $10 million move to Villarreal in Spain in 2008. The various MLS teams' development academies play matches in a U.S. Soccer developmental league against youth academies from other leagues such as the Division II North American Soccer League (NASL) and Division III USL Pro, the latter of which has now rebranded itself as the United Soccer League.
MLS formerly operated a reserve league which gave playing time to players who were not starters for their MLS teams. The Reserve League was formed in 2005, and operated through 2014 (with the exception of the 2009 & 2010 seasons). MLS began integrating its Reserve League with the league then known as USL Pro in 2013, and after the 2014 season folded the Reserve League, with MLS now requiring all teams to either affiliate with a USL team or field a reserve side in that league.
Stadiums.
Since 1999, the league has overseen the construction of twelve stadiums specifically designed for soccer. The development of soccer-specific stadiums owned by the teams has generated a better gameday experience for the fans. The soccer-specific stadiums have yielded positive financial results as teams were no longer required to pay to rent out facilities and gained control over revenue streams such as concessions, parking, naming rights, and the ability to host non MLS events. Several teams have doubled their season-tickets following the team's move into a soccer-specific stadium.
The establishment of soccer-specific stadiums is considered the key to the league and the ability of teams to turn a profit. In 2006, Tim Leiweke, then CEO of Anschutz Entertainment Group, described the proliferation of soccer-specific stadiums as the turning point for MLS.
Columbus Crew owner Lamar Hunt started this trend in 1999 by constructing Columbus Crew Stadium, now known as Mapfre Stadium, as MLS's first soccer-specific stadium. The Los Angeles Galaxy followed four years later with the opening of The Home Depot Center, now StubHub Center, in 2003. FC Dallas opened Pizza Hut Park, now Toyota Stadium, in 2005, and the Chicago Fire began playing their home games in Toyota Park in 2006. The 2007 season brought the opening of Dick's Sporting Goods Park for the Colorado Rapids and BMO Field for Toronto FC.
Near the end of the 2008 season, Rio Tinto Stadium became the home of Real Salt Lake, which meant that for the first time in MLS history a majority of MLS's teams (8 out of 14) played in soccer-specific stadiums. Red Bull Arena, the new home of the New York Red Bulls opened for the start of the 2010 season, and the Philadelphia Union opened PPL Park in June 2010, midway through their inaugural season.
The following season, in 2011, the Portland Timbers made their MLS debut in a newly renovated Jeld-Wen Field, now renamed Providence Park, which was originally a multi-purpose venue but turned into a soccer-specific facility. Also in 2011, Sporting Kansas City moved to new Livestrong Sporting Park.
The Houston Dynamo relocated to their new home at BBVA Compass Stadium in 2012. In the same year, the Montreal Impact joined the league in an expanded Stade Saputo, which reopened June 2012, when renovations pushed the seating capacity to over 20,000. The Impact has used Olympic Stadium for early season matches and for games that require a larger capacity. The San Jose Earthquakes, who had played at Buck Shaw Stadium from 2008 until 2014, opened their new Avaya Stadium before the 2015 season.
The development of additional MLS stadiums is in progress. The Orlando City SC expansion team intends to begin constructing Orlando City Stadium, a soccer-specific stadium, in 2014 to be completed in 2015, while temporarily playing at the renovated Florida Citrus Bowl Stadium in their inaugural year.
Three teams have announced their desire to build a soccer-specific stadium, although these teams have not finalized the stadium site and received all necessary government approvals. D.C. United plays home games at a former NFL and Major League Baseball venue, RFK Stadium; in 2013, D.C. United announced the signing of a public-private partnership term sheet to build a 25,000-seat new soccer stadium in Washington, D.C., and a final deal was reached in late 2014. The New York City FC expansion team will play their games at Yankee Stadium, a Major League Baseball venue, although they intend to move into a soccer specific stadium in the future. The New England Revolution play home games at a National Football League venue, Gillette Stadium, but are currently in discussion with the City of Boston regarding a potential soccer-specific stadium in South Boston.
Several remaining clubs play in stadiums not originally built for MLS and have not announced plans to move. The Seattle Sounders play at CenturyLink Field, a dual-purpose facility used for both American football and soccer. The Vancouver Whitecaps FC joined the league with Portland in 2011 and temporary held matches at Empire Field before moving into the refurbished BC Place in October 2011, a retractable-roof stadium that hosts Canadian football as well as soccer.
Media coverage.
United States.
As of the 2015 season, MLS matches are broadcast nationally by ESPN networks and Fox Sports in English, and Univision networks in Spanish under an eight-year contract. Each broadcaster has a window for national regular season matches, with UniMas airing a game on Friday nights in Spanish and additional matches on Univision Deportes Network, and ESPN and Fox Sports 1 airing games on Sunday evenings in English. ESPN, FS1, and Univision will share in coverage of the playoffs, while ESPN and FS1 will alternate broadcasting the MLS Cup final in English. In total, at least 125 matches will be aired per-season across all three networks, and the three contracts have an average estimated value of $90 million per season—five times larger than the average $18 million value of the previous contracts with ESPN, Univision, and NBC Sports. 7. Matches not televised nationally are broadcast regionally, often by regional sports networks, such as the LA Galaxy and Time Warner Cable SportsNet.
From 2012 to 2014, MLS matches were previously broadcast by NBC Sports, with 40 matches per year—primarily on NBCSN, and select matches broadcast on the NBC network. The move from Fox Soccer to the more widely distributed NBCSN proved successful, with viewership numbers doubling for the 2012 season over those of Fox Soccer.
Canada.
Coverage of MLS expanded into Canada in 2007 with the addition of Toronto FC.
Currently, English-language national MLS broadcast rights in Canada are through the TSN networks with a six-year deal for the 2011–2016 seasons. TSN and TSN2 broadcast a minimum of 30 games during each season, all featuring at least one Canadian team. French-language sister networks RDS and RDS2 have similar broadcast rights. The networks also carry additional games not involving Canadian teams. GolTV Canada carries selected all-U.S. MLS matchups.
As in the United States, the individual Canadian teams also have separate broadcast deals for games not aired under the TSN/RDS national contract. TSN and Sportsnet split coverage of Toronto FC non-national games (TSN and Sportsnet's parent companies own a joint majority stake in the team through Maple Leaf Sports & Entertainment), TVA Sports airs Montreal Impact games, and TSN broadcasts the Vancouver Whitecaps in a separate deal.
International.
MLS signed an international television contract in 2008 through 2013 with sports media company MP & Silva. The figure is reportedly an "eight-figure deal." MP & Silva explained that high-profile, international players who were lured to MLS by the Designated Player Rule have raised the popularity of MLS in international markets.
ESPN International purchased the rights to broadcast MLS in the U.K. in 2009, and other ESPN networks around the world also broadcast games. MLS also entered into a four-year contract with Sky Sports to broadcast two MLS matches per week in the UK from 2015–2019. Eurosport will also broadcast MLS between 2015 and 2019, with four matches per week being screened live to its continental audience.
Video games.
Major League Soccer is a playable league in both the "FIFA" and the "Football Manager" series. The league made its video game debut in 1999 with FIFA 2000. In 2001, Konami released ESPN MLS ExtraTime 2002, which, to date, is the only soccer title to be based solely on the MLS. The league made its first appearance in the management series "Football Manager 2005" in 2004.
Profitability and revenues.
Major League Soccer began to demonstrate positive signs of long-term profitability as early as 2004 with the single-entity ownership structure, salary cap, and the media and marketing umbrella Soccer United Marketing (SUM) all contributing towards MLS's financial security. As soccer-specific stadiums are built, ownership expands, and television coverage increases, MLS has seen its revenues increase while controlling costs.
Television coverage and revenue have increased since the league's early years. In 2006, MLS reached an 8-year TV deal with ESPN spanning the 2007–2014 seasons, and marked the first time that MLS earned rights fees, reported to be worth $7–8 million annually. In September 2012 the league extended its distribution agreement with London-based Media rights agency MP & Silva until 2014 in a deal worth $10 million annually. Total league TV revenues are over $40 million annually. In 2011, MLS earned $150 million when it sold a 25% stake in SUM.
In early 2005, MLS signed a 10-year, $150 million sponsorship deal with Adidas. In 2007, MLS teams started selling ad space on the front of jerseys to go along with the league-wide sponsorship partners who had already been advertising on the back of club jerseys, following the practice of international sport, specifically soccer. MLS established a floor of $500,000 per shirt sponsorship, with the league receiving a flat fee of $200,000 per deal. As of July 2014, sixteen teams had signed sponsorship deals to have company logos placed on the front of their jerseys (and another team is directly owned by its shirt sponsor), and the league average from jersey sponsors was about $2.4 million. As of February 2016, all teams in MLS have jersey sponsors.
The Los Angeles Galaxy made a profit in 2003 in their first season at The Home Depot Center, and FC Dallas turned a profit after moving into Pizza Hut Park in 2005. For each season between 2006–2009, two to three MLS clubs (generally clubs with a soccer-specific stadium) were reported as profitable by the league.
By 2012, the league had shown a marked improvement in its financial health. In November 2013, "Forbes" published its first valuation of MLS teams since 2008, and revealed that ten of the league's nineteen teams earned an operating profit in 2012, while two broke even and seven had a loss. Forbes estimated that the league's collective annual revenues were $494 million, and that the league's collective annual profit was $34 million. Forbes valued the league's franchises to be worth $103 million on average, almost three times as much as the $37 million average valuation in 2008. The Seattle Sounders FC franchise was named the most valuable at $175 million, a 483% gain over the $30 million league entrance fee it paid in 2009.
The trend in increased team values has continued with MLS teams seeing a strong 52% increase in franchise values from 2012 to 2014. In August 2015, Forbes released the updated list of MLS franchise values with the most profitable team weighing in at $245 million and the least at $105 million. The average value jumped from $103 to $157 million.
Rules and officials.
MLS follows the rules and standards of the International Football Association Board (IFAB).
The playoff extra time structure follows IFAB standards: two full 15-minute periods, followed by a penalty shootout if necessary.
Away goals apply to the playoff stage of the competition, but do not apply to overtime in the second leg of any two-legged playoff series.
U.S. Soccer hired the first full-time professional referees in league history in 2007 as part of the league's "Game First" initiatives.
Major League Soccer has been implementing fines and suspensions since the 2011 season for simulation (diving) through its Disciplinary Committee, which reviews plays after the match. The first player fined under the new rule was Charlie Davies, fined $1,000 for intentionally deceiving match officials.
Team names.
Originally, in the style of other U.S. sports leagues, teams were given nicknames at their creation. Examples include the Columbus Crew, the San Jose Clash and the Los Angeles Galaxy. Several of the club names in MLS originated with earlier professional soccer clubs, such as the 1970s-era NASL team names San Jose Earthquakes, Seattle Sounders, Portland Timbers and Vancouver Whitecaps.
D.C. United and Miami Fusion F.C. were the only two MLS teams to adopt European naming conventions during the 1990s. However, European-style names have increased in MLS, with expansion teams such as Real Salt Lake and Toronto FC, in addition to 2015 entrants New York City FC and Orlando City S.C., along with several re-brandings such as the Dallas Burn (now FC Dallas) and Kansas City Wizards (now Sporting Kansas City).
The beverage company Red Bull GmbH owns the New York Red Bulls as well as other sports teams.
Player records.
Statistics below are for all-time leaders. Statistics are for regular season only. Bold indicates active MLS players.
Updated April 17, 2016
Player records (active).
Statistics below are for all-time leaders who are still playing. Statistics are for regular season only.
Updated April 17, 2016
MLS awards.
At the conclusion of each season, the league presents several awards for outstanding achievements, mostly to players, but also to coaches, referees, and teams. The finalists in each category are determined by voting from MLS players, team employees, and the media.

</doc>
<doc id="71807" url="https://en.wikipedia.org/wiki?curid=71807" title="List of rulers of Monaco">
List of rulers of Monaco

The following is a list of rulers of Monaco. Most belong to the House of Grimaldi; exceptions, which consist primarily of the principality's administrators under periods of foreign occupation, are noted.
History.
The House of Grimaldi, descended from Otto Canella, a Genoese statesman, and taking their name from his son Grimaldo, were an ancient and prominent Guelphic Genoese family. Members of this family, in the course of the civil strife in Genoa between the Guelphs and Ghibellines, were banned from Genoa in 1271 and took refuge in Monaco.
François Grimaldi seized the Rock of Monaco in 1297, starting the Grimaldi dynasty, under the sovereignty of the Republic of Genoa. The Grimaldis acquired Menton in 1346 and Roquebrune in 1355, enlarging their possessions. These two towns (some 95% of the country's territory) were eventually ceded to France in 1861, according to the Franco-Monegasque Treaty.
The Grimaldis used the title of Lord until 1612. Then, Lord Honoré II started using the title of Prince, thereby becoming the first Prince of Monaco. Afterwards, Honoré II secured recognition of his independent sovereignty from Spain in 1633, and then from France by the Treaty of Péronne in 1641. Since then the area has remained under the control of the Grimaldi family to the present day, except when under French control from February 24, 1793 to May 17, 1814.

</doc>
<doc id="71808" url="https://en.wikipedia.org/wiki?curid=71808" title="Women's United Soccer Association">
Women's United Soccer Association

The Women's United Soccer Association, often abbreviated to the WUSA, was the world's first women's soccer league in which all the players were paid as professionals. Founded in February 2000, the league began its first season in April 2001 with eight teams in the United States. The league suspended operations on September 15, 2003, shortly after the end of its third season, after making cumulative losses of around US $100 million.
History.
Establishment.
As a result of the US Women's National Team's (US WNT) first-place showing in the 1999 FIFA Women's World Cup, a seemingly viable market for the sport germinated.
Feeding on the momentum of their victory, the twenty US WNT players, in partnership with John Hendricks of the Discovery Channel, sought out the investors, markets, and players necessary to form the eight-team league. The twenty founding players were: 
Michelle Akers, Brandi Chastain, Tracy Ducar, Lorrie Fair, Joy Fawcett, Danielle Fotopoulos, Julie Foudy, Mia Hamm, Kristine Lilly, Shannon MacMillan, Tiffeny Milbrett, Carla Overbeck, Cindy Parlow, Christie Pearce, Tiffany Roberts, Briana Scurry, Kate (Markgraf) Sobrero, Tisha Venturini, Saskia Webber and Sara Whalen.
Initial investment in the league was provided by the following:
The US Soccer Federation approved membership of the league as a sanctioned Division 1 women's professional soccer league on August 18, 2000.
Organization.
Media coverage.
At various times, games were televised on TNT, CNNSI, ESPN2, PAX TV, and various local and regional sports channels.
Teams.
The WUSA franchises were located in Philadelphia; Boston; New York City; Washington, D.C.; Cary, N.C.; Atlanta; San Jose, Ca.; and San Diego:
For the inaugural season, each roster primarily consisted of players from the United States, although up to four international players were allowed on each team's roster. Among the international players were China's Sun Wen, Pu Wei, Fan Yunjie, Zhang Ouying, Gao Hong, Zhao Lihong, and Bai Jie; Germany's Birgit Prinz, Conny Pohlers, Steffi Jones and Maren Meinert; Norway's Hege Riise, Unni Lehn, and Dagny Mellgren; Brazil's Sissi, Kátia and Pretinha; and Canada's Charmaine Hooper, Sharolta Nonen, and Christine Latham.
The league also hosted singular talents from nations which were not at the forefront of women's soccer, such as Maribel Dominguez of Mexico, Homare Sawa of Japan, Julie Fleeting of Scotland, Cheryl Salisbury of Australia, Marinette Pichon of France and Kelly Smith of England.
WUSA Awards.
Founders Cup champions.
The Founders Cup (named in honor of the 20 founding players) was awarded to the winner of a four-team, single-elimination postseason playoff.
WUSA's sudden death overtime was 15 minutes long (2-seven and a half minute periods) and only used in the play-offs.
League suspension.
The WUSA played for three full seasons, suspending operations on September 15, 2003, shortly after the conclusion of the third season. Neither television ratings nor attendance met forecasts, while the league spent its initial $40 million budget, planned to last five years, by the end of the first season.
Even though the players took salary cuts of up to 30% for the final season, with the founding players (who also held an equity stake in the league) taking the largest cuts, that was not enough to bring expenses under control.
In the hopes of an eventual relaunch of the league, all rights to team names, logos, and similar properties were preserved. Efforts to line up new sources of capital and operating funds continued.
In June 2004, the WUSA held two "WUSA Festivals" in Los Angeles and Blaine, Minnesota, featuring matches between reconstituted WUSA teams (often with marquee players borrowed from other teams), in order to maintain the league in the public eye and sustain interest in women's professional soccer.
With the WUSA on hiatus, the Women's Premier Soccer League (WPSL) and the W-League regained their status as the premier women's soccer leagues in the United States, and many former WUSA players joined those teams.
A new women's professional soccer league in the United States called Women's Professional Soccer started in 2009. However, that league suspended operations in January 2012.

</doc>
<doc id="71812" url="https://en.wikipedia.org/wiki?curid=71812" title="Accrington F.C.">
Accrington F.C.

Accrington Football Club was an English football club from Accrington, Lancashire, who were one of the founder members of The Football League.
History.
Accrington F.C. was formed following a meeting at a local public house in 1876. The "Owd Reds" played at Accrington Cricket Club's ground in Thorneyholme Road, still in use for that sport today.
The club was part of the revolt against the Football Association in 1884 over professionalism, after being expelled from the FA the previous year for paying a player. They were one of the original twelve teams forming the Football League on 17 April 1888. Accrington's best season was in 1889–90, when it finished sixth in the table. However, in the 1892–93 season the team finished fifteenth (of 16) and was relegated after losing a test match 1–0 against Sheffield United at Trent Bridge. Accrington then resigned from the league rather than play in the Second Division, becoming the first of the founding Football League clubs to leave the League.
Shortly afterwards, Accrington F.C. suffered financial problems, which eventually led to its demise. The club continued outside the league until 1896, when it finally folded following a 12–0 defeat on 14 January against Darwen in the Lancashire Senior Cup.
Accrington did not have a Football League team again until in 1921–22 the Lancashire Combination league's Accrington Stanley (formerly a local rival), became a member as part of a major expansion of the league.
International players.
During its short life, the club had three players selected for England:

</doc>
<doc id="71821" url="https://en.wikipedia.org/wiki?curid=71821" title="Chapel Hill, North Carolina">
Chapel Hill, North Carolina

Chapel Hill is a city in Orange County, North Carolina (with some eastern portions in Durham County), and the home of the University of North Carolina at Chapel Hill and UNC Health Care. The population was 57,233 at the 2010 census; Chapel Hill is the 16th-largest municipality in North Carolina.
Chapel Hill, Durham, and Raleigh make up the three corners of the Research Triangle, so named in 1959 with the creation of Research Triangle Park, a research park between Durham and Raleigh. Chapel Hill is one of the central cities of the Durham-Chapel Hill MSA, which in turn is part of the Raleigh-Durham-Chapel Hill Combined Statistical Area, with a population of 1,998,808.
History.
The area was the home place of early settler William Barbee of Middlesex County, Virginia, whose 1753 grant of 585 acres from the Earl of Granville was the first of two land grants in what is now the Chapel Hill-Durham area. Though William Barbee died shortly after establishing himself and his family in North Carolina, one of his eight children, Christopher Barbee, became an important contributor to his father's adopted community and to the fledgling University of North Carolina.
Chapel Hill sits atop a hill which was originally occupied by a small Anglican "chapel of ease", built in 1752, known as New Hope Chapel. The Carolina Inn now occupies the site of the original chapel. In 1819, the town was founded to serve the University of North Carolina and grew up around it. The town was chartered in 1851, and its main street, Franklin Street, was named in memory of Benjamin Franklin.
In 1968, only a year after its schools became fully integrated, Chapel Hill became the first predominantly white municipality in the South to elect an African American mayor, Howard Lee. Lee served from 1969 until 1975 and, among other things, helped establish Chapel Hill Transit, the town's bus system. Some 30 years later, in 2002, legislation was passed to make the local buses free of fares to all riders, leading to a large increase in ridership; the buses are financed through Chapel Hill and Carrboro city taxes, federal grants, and UNC student fees. Several hybrid and articulated buses have been added recently. All buses carry GPS transmitters to report their location in real time to a tracking web site. Buses can transport bicycles and have wheelchair lifts.
In 1993, the town celebrated its bicentennial, which resulted in the establishment of the Chapel Hill Museum. This cultural community resource "exhibiting the character and characters of Chapel Hill, North Carolina" includes among its permanent exhibits "Alexander Julian", "History of the Chapel Hill Fire Department", "Chapel Hill's 1914 Fire Truck", "The James Taylor Story", "Farmer/James Pottery", and "The Paul Green Legacy".
On February 10, 2015, three Muslim college students were killed in their home, Finley Forest Condominiums, next to the Friday Center for Continuing Education, University of North Carolina (UNC). Their next-door neighbor, Craig Stephen Hicks, was arrested by police and identified as the main suspect. The incident gained national and international attention due to its islamophobic character.
In addition to the Carolina Inn, the Beta Theta Pi Fraternity House, Chapel Hill Historic District, Chapel Hill Town Hall, Chapel of the Cross, Gimghoul Neighborhood Historic District, Alexander Hogan Plantation, Old Chapel Hill Cemetery, Old East, University of North Carolina, Playmakers Theatre, Rocky Ridge Farm Historic District, and West Chapel Hill Historic District are listed on the National Register of Historic Places.
Geography and climate.
Chapel Hill is located in the southeast corner of Orange County. It is bounded on the west by the town of Carrboro and on the northeast by the city of Durham. However, most of Chapel Hill's borders are adjacent to unincorporated portions of Orange and Durham Counties rather than shared with another municipality. According to the United States Census Bureau, the town has a total area of , of which is land and is covered by water.
Demographics.
Durham, North Carolina, is the core of the four-county Durham-Chapel Hill MSA, which has a population of 504,357 as of Census 2010. The US Office of Management and Budget also includes Chapel Hill as a part of the Raleigh-Durham-Cary Combined Statistical Area, which has a population of 1,749,525 as of Census 2010. Effective June 6, 2003, the Office of Management and Budget redefined the Federal Statistical Areas and dismantled what had been for decades the Raleigh-Durham-Chapel Hill MSA, and split them into two separate MSAs, though the region still functions as a single metropolitan area.
According to the 2010 U.S. Census, 57,233 people in 20,564 households resided in Chapel Hill. The population density was 2,687 people per square mile (1037/km²). The racial composition of the town was 72.8% White, 9.7% African American, 0.3% Native American, 11.9% Asian, 0.02% Pacific Islander, 2.7% some other race, and 2.7% of two or more races. About 6.4% of the population was Hispanic or Latino of any race.
Of the 20,564 households, 51.1% were families, 26.2% of all households had children under the age of 18 living with them, 40.2% were headed by married couples living together, 8.2% had a female householder with no husband present, and 48.9% were not families. About 30.6% of all households were made up of individuals, and 7.7% had someone living alone who was 65 years of age or older. The average household size was 2.35 and the average family size was 2.98.
In the town, the population was distributed as 17.4% under the age of 18, 31.5% from 18 to 24, 23.6% from 25 to 44, 18.4% from 45 to 64, and 9.2% who were 65 years of age or older. The median age was 25.6 years. For every 100 females, there were 87.2 males. For every 100 females age 18 and over, there were 83.6 males.
According to estimates released by the U.S. Census Bureau, over the three-year period of 2005 through 2007, the median income for a household in the town was $51,690, and for a family was $91,049. Males had a median income of $50,258 versus $32,917 for females. The "per capita" income for the town was $35,796. About 8.6% of families and 19.8% of the population were below the poverty line, including 8.6% of those under age 18 and 5.6% of those age 65 or over.
Chapel Hill is North Carolina's best-educated city, proportionately, with 77% of adult residents (25 and older) holding an associate degree or higher, and 73% of adults possessing a baccalaureate degree or higher.
Government.
Chapel Hill uses a council-manager form of government. The community elects a mayor and eight council members. Mayors serve two-year terms, and council members serve staggered four-year terms. Mayor Mark Kleinschmidt, a former town council member, was re-elected to a second term in November 2011 and a third term in 2013.
Two years prior, in 2009, he had made history by being elected the first openly gay mayor of Chapel Hill, succeeding outgoing four-term Mayor Kevin Foy.
The town adopted its flag in 1990. According to flag-designer Spring Davis, the blue represents the town and the University of North Carolina (whose colors are Carolina blue and white); the green represents "environmental awareness"; and the "townscape" in the inverted chevron represents "a sense of home, friends, and community."
The current version of the town's seal, adopted in 1989, is in the process of being replaced with a similar but simpler version. All versions of the seal, dating back to the 1930s, depict Athena, the Greek goddess of wisdom and protector of cities.
School system.
The Chapel Hill-Carrboro school district covers most of the towns of Chapel Hill and Carrboro, along with portions of unincorporated Orange County, and is recognized for its academic strengths. East Chapel Hill High School, Carrboro High School, and Chapel Hill High School have all received national recognition for excellence, with "Newsweek" in 2008 ranking East Chapel Hill High as the 88th-best high school in the nation, and the highest-ranked standard public high school in North Carolina. The state's main youth orchestra, Piedmont Youth Orchestra, is based in Chapel Hill.
Culture.
Though Chapel Hill is a principle city of a large metropolitan area, it retains a relatively small-town feel. Combined with its close neighbor, the Chapel Hill-Carrboro area has roughly 85,000 residents. Many large murals can be seen painted on the buildings. Most of these murals were painted by UNC alumnus Michael Brown. Also, for more than 30 years Chapel Hill has sponsored the annual street fair, Festifall, in October. The fair offer booths to artists, craftsmakers, nonprofits, and food vendors. Performance space is also available for musicians, martial artists, and other groups. The fair is attended by tens of thousands each year.
A variety of corporations are headquartered in Chapel Hill. Health insurance provider Blue Cross and Blue Shield of North Carolina was one of the town's 10 largest employers. Technology companies USAT Corp and Realtime Ops have made Chapel Hill their headquarters location. Journalistic, Inc., the publisher of the nationally acclaimed magazines "Fine Books & Collections", "QSR", and "FSR" recently relocated from Durham to Chapel Hill. New companies are selecting the town as their base of operations such as the service company Alpha Install.
The Morehead Planetarium was the first planetarium built on a U.S. college campus. When it opened in 1949, it was one of six planetariums in the nation and has remained an important town landmark. During the Mercury, Gemini, and Apollo programs, astronauts were trained there. One of the town's hallmark features is the giant sundial, located in the rose gardens in front of the planetarium on Franklin Street.
Influences of the university are seen throughout the town, even in the fire departments. Each fire station in Chapel Hill has a fire engine (numbers 31, 32, 33, 34, and 35) that is Carolina blue. These engines are also decorated with different UNC decals, including a firefighter Rameses.
Chapel Hill also has some new urbanist village communities, such as Meadowmont Village and Southern Village. Meadowmont and Southern Village both have shopping centers, green space where concerts and movies take place, community pools, and schools. Also, a traditional-style mall with a mix of national and local retailers is located at University Place.
In 2009, Chapel Hill ranked no. 3 on "Newsmax" magazine's list of the "Top 25 Most Uniquely American Cities and Towns", a piece written by current CBS News travel editor Peter Greenberg. According to the magazine, Greenberg based the rankings on a variety of features, such as quality of schools and proximity to medical care, as well as culture, hospitality, and scenic beauty.
Food.
Hailed as one of America's Foodiest Small Towns by "Bon Appétit", Chapel Hill is rapidly becoming a hot spot for pop American cuisine. Among the restaurants noted nationally are A Southern Season, Foster's Market (Martha Stewart's Living), Mama Dip's (Food Network's "$40 A Day With Rachael Ray"), Crook's Corner, Sunrise Biscuit Kitchen (The Splendid Table), caffè Driade (Food Network's "$40 A Day With Rachael Ray"), and Lantern Restaurant ("Food & Wine" magazine, "Southern Living" magazine, etc.).
Many unique lunch spots in Chapel Hill have received rave reviews, including on Franklin Street, Sage on Weaver Dairy Road, and Fiesta Grill on Hwy 54.
Music.
Chapel Hill also has a vibrant music scene. Classical composers on the faculty of UNC Chapel Hill include Allen Anderson, Stefan Litwin, and Lee Weisert. In the realm of popular music, Alternative States, Remington Brown, Archers of Loaf, Squirrel Nut Zippers, James Taylor, George Hamilton IV, Southern Culture on the Skids, Superchunk, Polvo, Ben Folds Five, and more recently Porter Robinson, are among the most notable musical artists and acts whose careers began in Chapel Hill. The town has also been a center for the modern revival of old-time music with such bands as the Ayr Mountaineers, Hollow Rock String band, the Tug Creek Ramblers, Two Dollar Pistols, the Fuzzy Mountain String band, Big Fat Gap and the Red Clay Ramblers. Chapel Hill was also the founding home of now Durham-based Merge Records. Bruce Springsteen has made a point to visit the town on four occasions. His most recent appearance was on September 15, 2003, at Kenan Memorial Stadium with the E Street Band. U2 also performed at Kenan on the first American date of their 1983 War Tour, where Bono famously climbed up to the top of the stage, during pouring rain and lightning, holding up a white flag for peace. The 2011 John Craigie song, "Chapel Hill", is about the singer's first visit there.One song from "Dirty", a Sonic Youth album, is named after the town.
Sports.
The University of North Carolina has been very successful at college basketball and women's soccer, and a passion for these sports has been a distinctive feature of the town's culture, fueled by the rivalry among North Carolina's four ACC teams: the North Carolina Tar Heels, the Duke Blue Devils, the NC State Wolfpack, and the Wake Forest Demon Deacons.
The two largest sports venues in the town both house UNC teams. The Dean Smith Center is home to the men's basketball team, while Kenan Memorial Stadium is home to the football team. In addition, Chapel Hill is also home to Carmichael Arena which formerly housed the UNC men's basketball team, and currently is home to the women's team, and to Fetzer Field, home to men's and women's soccer and lacrosse teams.
Many walking/biking trails are in Chapel Hill NC. Some of these include Battle Branch Trail and Bolin Creek Trail.

</doc>
<doc id="71825" url="https://en.wikipedia.org/wiki?curid=71825" title="Burton United F.C.">
Burton United F.C.

Burton United Football Club was a football club based in Burton upon Trent in England. The club was established in 1901 by a merger of Burton Swifts and Burton Wanderers, and folded in 1910. Between 1901 and 1907 the club were members of the Football League.
History.
The club was formed in 1901 when Burton Swifts merged with Burton Wanderers. Swifts had played in the Football League since 1892, whilst Wanderers had been Football League members between 1894 and 1897 but were now members of the Midland League. It was unusual for a smaller town such as Burton to host two Football League clubs so it was hoped the merger would see an upturn in fortunes for the United club.
The new club took Swifts' place in the Second Division of the Football League and took over residence at their Peel Croft home. However, the new club failed to achieve success, and never finished above 10th place or made it past the first round of the FA Cup. In the 1904–05 and 1905–06 seasons the club finished in second last place, and in 1906–07 finished bottom. At the end of the season the club failed to win re-election, losing their Football League place to Fulham. Following the re-election meeting Burslem Port Vale resigned from the League, and although United applied to take their place, it was instead offered to and accepted by Oldham Athletic.
The club dropped into the Birmingham & District League, and in 1909 also became members of the Southern League. At the end of the 1909–10 season the club finished bottom of the Birmingham & District League and second bottom of their Southern League division, and subsequently merged with Burton All Saints, who had been established around the same time as United. They became Burton Town in 1924. They ceased playing in 1940, and were merged into Burton Albion shortly after they were founded in 1950.

</doc>
<doc id="71827" url="https://en.wikipedia.org/wiki?curid=71827" title="Aberdare Athletic F.C.">
Aberdare Athletic F.C.

Aberdare Athletic Football Club were a Welsh football club founded in 1893 and based in Aberdare. They joined the Football League in 1921 but were replaced by Torquay United after failing to be re-elected in 1927.
History.
Founded in 1893, Aberdare were Welsh Cup runners-up, in 1903–04 1904–05 and 1922/23. In 1920–21 they joined the Welsh Section of the Southern League and duly finished runners-up in their first season. That gained them entry to the Football League Third Division South in time for 1921–22.
Aberdare spent six seasons in the League, with their best season being 1921–22, when they finished 8th. That same year, 1926, Aberdare merged with nearby Aberaman Athletic; the first team continued to compete in the Football League under the name Aberdare Athletic, while the reserve team played in the Welsh League under the name "Aberdare & Aberaman Athletic".
However, in the next season, 1926–27 Aberdare Athletic finished bottom of the Third Division South and failed to gain re-election to the league; Torquay United took their place. The merged club fully renamed themselves as Aberdare & Aberaman Athletic, and rejoined the Southern League.
However, the merged club only survived for another year, and in 1928 the Aberaman faction split away from the club to re-form Aberaman Athletic, while the Aberdare half folded. After World War II, in 1945 a reformed Aberdare & Aberaman Athletic was formed, but this side also split into two, in 1947; Aberdare Town FC club continue to this day, and play in the Welsh Football League. 
The club had several different team colours during their existence. Their membership of the Football League coincided with that of a team from another Welsh town Merthyr Town.
Managers.
The following were managers around the time the team was Football League side:
History.
Welsh Football League

</doc>
<doc id="71831" url="https://en.wikipedia.org/wiki?curid=71831" title="Corned beef">
Corned beef

Corned beef is a salt-cured beef product. The term comes from the treatment of the meat with large grained rock salt, also called "corns" of salt. It features as an ingredient in many cuisines. Most recipes include nitrates or nitrites, which convert the natural hemoglobin in beef to methaemoglobin, giving a pink color. It has been argued that nitrates reduce the risk of dangerous botulism during curing. Beef cured with salt only has a gray color, and is sometimes called "New England corned beef". Often sugar and spices are also added to recipes for corned beef.
It was popular during both World Wars, when fresh meat was rationed. Corned beef remains popular in the United Kingdom and countries with British culinary traditions and is commonly used in sandwiches, corned beef hash or eaten with chips and pickles. It also remains especially popular in Canada in a variety of dishes, perhaps most prominently Montreal smoked meat.
History.
Although the exact beginnings of corned beef are unknown, it most likely came about when people began preserving meat through salt-curing. Evidence of its legacy is apparent in numerous cultures, including Ancient Europe and the Middle East. The word "corn" derives from Old English, and is used to describe any small, hard particles or grains. In the case of "corned beef", the word may refer to the coarse granular salts used to cure the beef. The word corned may also refer to the corns of potassium nitrate, also known as saltpetre, which were formerly used to preserve the meat.
19th-century Atlantic trade.
Although the practice of curing beef was found locally in many cultures, the industrial production of corned beef started in the British Industrial Revolution. Irish corned beef was used and traded extensively from the 17th century to the mid-19th century for British civilian consumption and as provisions for the British naval fleets and North American armies due to its non-perishable nature. The product was also traded to the French for use in Caribbean sugar plantations as sustenance for the colonists and the slave laborers. The 17th-century British industrial processes for corned beef did not distinguish between different cuts of beef beyond the tough and undesirable parts such as the beef necks and shanks. Rather, the grading was done by the weight of the cattle into "small beef", "cargo beef", and "best mess beef", the former being the worst and the latter the best. Much of the undesirable portions and lower grades were traded to the French, while better parts were saved for British consumption or shipped to British colonies.
Ireland produced a significant amount of the corned beef in the Atlantic trade from local cattle and salt imported from the Iberian Peninsula and southwestern France. Coastal cities, such as Dublin, Belfast, and Cork, created vast beef curing and packing industries, with Cork producing half of Ireland's annual beef exports in 1668. Although the production and trade of corned beef as a commodity was a source of great wealth for the colonial nations of Britain and France (who were participating in the Atlantic slave trade), in the colonies themselves the product was looked upon with disdain due to its association with poverty and slavery.
Increasing corned beef production to satisfy the rising populations of the industrialised areas of Great Britain and Atlantic trade worsened the effects of the Irish Famine and the Great Potato Famine: 
Despite being a major producer of beef, most of the people of Ireland during this period consumed little of the meat produced, in either fresh or salted form, due to its prohibitive cost. This was because most of the farms and its produce were owned by wealthy Anglo-Irish landlords and that most of the population were from families of poor tenant farmers, and that most of the corned beef was exported.
The lack of beef or corned beef in the Irish diet is especially true in Northern Ireland and areas away from the major centres for corned beef production. However, individuals living in these production centres such as Cork did consume the product to a certain extent. The majority of Irish that resided in Ireland at the time mainly consumed dairy products and meats such as pork or salt pork, bacon and cabbage being a notable example of a traditional Irish meal.
20th century to present.
Although it ceased to be an important commodity in the 20th century Atlantic trade due in part to the abolition of slavery, corned beef production and its canned form remained important as a food source during World War II. Much of the canned corned beef came from Fray Bentos in Uruguay, with over 16 million cans exported in 1943. Even now, significant amounts of the global canned corned beef supply comes from South America. Today, around 80% of the global tinned corned beef supply originates from Brazil.
Cultural associations.
In North America corned beef dishes are associated with traditional Irish cuisine. However, there is considerable debate about the association of corned beef with Ireland. Mark Kurlansky, in his book "Salt", states that the Irish produced a salted beef around the Middle Ages that was the "forerunner of what today is known as Irish corned beef" and in the 17th century the English named the Irish salted beef "corned beef". 
Some say it was not until the wave of 18th century Irish immigration to the United States that much of the ethnic Irish first began to consume corned beef dishes as seen today. The popularity of corned beef compared to bacon among the immigrant Irish may have been due to corned beef being considered a luxury product in their native land, while it was cheaply and readily available in America.
In Ireland today, the serving of corned beef is geared toward tourist consumption and most Irish in Ireland do not identify the ingredient as native cuisine.
The Jewish population produced similar koshered cured beef product made from the brisket which the Irish immigrants purchased as corned beef from Jewish butchers. This may have been facilitated by the close cultural interactions and collaboration of these two diverse cultures in the USA's main 19th and 20th century immigrant port of entry, New York City.
Regions.
North America.
In North America, corned beef typically comes in two forms, a cut of beef (usually brisket, but sometimes round or silverside) cured or pickled in a seasoned brine, cooked, and canned, or tinned.
Corned beef is often purchased ready to eat in delicatessens. It is the key ingredient in the grilled Reuben sandwich, consisting of corned beef, Swiss cheese, sauerkraut, and Thousand Island or Russian dressing on rye bread.
Corned beef hash is commonly served with eggs for breakfast.
Smoking corned beef, typically with a generally similar spice mix, produces smoked meat (or "smoked beef") such as pastrami.
In both the United States and Canada, corned beef is sold in cans in minced form. It is sold this way in Puerto Rico and Uruguay.
Saint Patrick's Day.
In the United States, consumption of corned beef is often associated with Saint Patrick's Day. Corned beef is not considered an Irish national dish, and the connection with Saint Patrick's Day specifically originates as part of Irish-American culture, and is often part of their celebrations in North America.
Corned beef was used as a substitute for bacon by Irish-American immigrants in the late 19th century. Corned beef and cabbage is the Irish-American variant of the Irish dish of bacon and cabbage.
A similar dish is the New England boiled dinner, consisting of corned beef, cabbage, and root vegetables such as carrots, turnips, and potatoes, which is popular in New England and another similar dish, Jiggs dinner is popular in parts of Atlantic Canada.
United Kingdom.
In the United Kingdom, corned beef refers to the variety made from finely minced corned beef in a small amount of gelatin (bully beef; from the French "bouilli" "boiled"), and is sold in distinctive, oblong cans, just as in the United States and Canada, or in slices from supermarkets. It is mainly imported from Argentina, Brazil, or Uruguay. Bully beef and hardtack biscuits were the main field rations of the British Army from the Boer War to World War II. It is commonly served sliced in a corned beef sandwich. Hash and hotch-potch, in which potatoes and corned beef are stewed together, are also made. Tinned corned beef is also used in mainland Europe.
The U.S. version of corned beef is known in the UK as salt beef.
Ireland.
The appearance of corned beef in Irish cuisine dates to the 12th century in the poem "Aislinge Meic Con Glinne" or "The Vision of MacConglinne". Within the text, it is described as a delicacy a king uses to purge himself of the "demon of gluttony". Cattle, valued as a bartering tool, were only eaten when no longer able to provide milk or to work. The corned beef as described in this text was a rare and valued dish, given the value and position of cattle within the culture, as well as the expense of salt, and was unrelated to the corned beef eaten today.
Israel.
In Israel, corned beef, or "loof", has been the traditional field-ration of the Israeli army (Israel Defense Forces). As a result of universal conscription, it was said that Israel has been force-feeding "Loof — a colloquially corrupt short form of 'meatloaf' — to its citizens since the nation's founding." While some sources state that Loof was developed by the IDF in the 1940s, as a form of British 'bully beef', it actually pre-dated the state of Israel as a component of Jewish organizations' relief packages sent to Israel by groups such as Hadassah.

</doc>
<doc id="71833" url="https://en.wikipedia.org/wiki?curid=71833" title="Kazakh language">
Kazakh language

Kazakh (natively , Қазақша, "", "Qazaqşa", ; pronounced ) is a Turkic language belonging to the Kipchak (or Northwestern Turkic) branch, closely related to Nogai, Kyrgyz, and especially Karakalpak. Kazakh is the official language of the Republic of Kazakhstan and a significant minority language in the Ili Kazakh Autonomous Prefecture in the Xinjiang Uyghur Autonomous Prefecture of the People's Republic of China and in the Bayan-Ölgii Province of Mongolia. Kazakh is also spoken by many ethnic Kazakhs through the former Soviet Union (approximately 5,000,000 in the Russian Federation according to the 2002 Russian Census), Afghanistan, Iran, Turkey, and Germany.
Like other Turkic languages, Kazakh is an agglutinative language, and it employs vowel harmony.
Geographic distribution.
The Kazakh language has its speakers (mainly Kazakhs) spread over a vast territory from the Tian Shan mountains to the western shore of Caspian Sea. Kazakh is the official state language of Kazakhstan, in which nearly 10 million speakers are reported to live (based on the CIA World Factbook's estimates for population and percentage of Kazakh speakers). In the People's Republic of China, more than one million ethnic Kazakhs and Kazakh speakers reside in the Ili Kazakh Autonomous Prefecture within the Xinjiang Uyghur Autonomous Region.
Writing system.
Today, Kazakh is written in Cyrillic in Kazakhstan and Mongolia, while more than one million Kazakh-speakers in China use an Arabic-derived alphabet similar to the one that is used to write Uyghur.
The oldest known written records of languages closely related to Kazakh were written in the Orkhon script. However, it is not believed that any of these varieties were direct predecessors of Kazakh. Modern Kazakh has historically been written using versions of the Latin, Cyrillic and Arabic scripts.
In October 2006, Nursultan Nazarbayev, the President of Kazakhstan, brought up the topic of using the Latin alphabet instead of the Cyrillic alphabet as the official script for Kazakh in Kazakhstan. A Kazakh government study released in September 2007 said that Kazakhstan could feasibly switch to a Latin script over a 10- to 12-year period, for a cost of $300 million. The transition was halted temporarily on December 13, 2007, with President Nazarbayev declaring: “For 70 years the Kazakhstanis read and wrote in Cyrillic. More than 100 nationalities live in our state. Thus we need stability and peace. We should be in no hurry in the issue of alphabet transformation”. However, on January 30, 2015, the Minister of Culture and Sports Arystanbek Mukhamediuly announced that a transition plan was underway, with specialists working on the orthography in order to accommodate the phonological aspects of the language.
Phonology.
Kazakh exhibits tongue-root vowel harmony, with some words of recent foreign origin (usually of Russian or Arabic origin) as exceptions. There is also a system of rounding harmony which resembles that of Kyrgyz, but which does not apply as strongly and is not reflected in the orthography.
Consonants.
The following chart depicts the consonant inventory of standard Kazakh; many of the sounds, however, are allophones of other sounds or appear only in recent loan-words. The 18 consonant phonemes listed by Vajda are in bold—since these are phonemes, their listed place and manner of articulation are very general, and will vary from what is shown. The borrowed phonemes , , , and , only occur in recent mostly Russian borrowings, and are shown in parentheses [ ] in the table below.
In the table, the elements left of a divide are voiceless, while those to the right are voiced.
Vowels.
Kazakh has a system of nine phonemic vowels, three of which are diphthongs. The rounding contrast and generally only occur as phonemes in the first syllable of a word, but do occur later allophonically; see the section on harmony below for more information.
According to Vajda, the front/back quality of vowels is actually one of neutral versus retracted tongue root.
Per convention, rounded vowels are presented to the right of their unrounded counterparts. Phonetic values are paired with the corresponding character in Kazakh's Cyrillic alphabet.
Morphology and Syntax.
Kazakh is generally verb-final, though various permutations on SOV (subject–object–verb) word order can be used. Inflectional and derivational morphology, both verbal and nominal, in Kazakh, exists almost exclusively in the form of agglutinative suffixes. Kazakh is a nominative-accusative, head-final, left-branching, dependent-marking language.
Pronouns.
Kazakh has eight personal pronouns:
The declension of the pronouns is outlined in the following chart. Singular pronouns (with the exception of сіз, which used to be plural) exhibit irregularities, while plural pronouns don't. Irregular forms are highlighted in bold.
In addition to the pronouns, there are several more sets of morphemes dealing with person.
Tense, aspect and mood.
Kazakh may express different combinations of tense, aspect, and mood through the use of various verbal morphology or through a system of auxiliary verbs, many of which might better be considered light verbs. For example, the (imperfect) present tense in Kazakh bears different aspectual information depending on whether basic present-tense morphology is used, or one of (commonly) four verbs is used:
In addition to this aspectual distinction, Kazakh also exhibits a number of lexicalized pairs of varying converbs. For example, verbs of motion in Kazakh that are rendered in any kind of progressive tense are obligatorily realized with the helping verb "жатыр", 'to lie', however the verb must take a unique participial form, "-а/е/й." For example, "I am going [right now," would be rendered in Kazakh as "мен бара жатырмын", and not *"мен барып жатырмын."
Annotated Text with Gloss.
State Anthem of the Republic of Kazakhstan (2006 – current)

</doc>
<doc id="71836" url="https://en.wikipedia.org/wiki?curid=71836" title="Uzbek language">
Uzbek language

Uzbek is a Turkic language and the official language of Uzbekistan. It has 27 million native speakers and is spoken by the Uzbeks in Uzbekistan and elsewhere in Central Asia. Uzbek belongs to the Eastern Turkic, or Karluk, branch of the Turkic language family. External influences include Persian, Arabic and Russian. One of the most noticeable distinctions of Uzbek from other Turkic languages is the rounding of the vowel to or , a feature that was influenced by Persian.
Name.
In the language itself, Uzbek is ' or '. In Cyrillic, the same names are written ' and '; in Arabic script, and .
History.
Turkic speakers have probably settled in the Amu-Darya, Syr-Darya and Zeravshan river basins since at least AD 600–700, gradually ousting or assimilating the speakers of Eastern Iranian languages who previously inhabited Soghdiana, Bactria and Chorasmia. The first Turkic dynasty in the region was that of the Karakhanids in the 9th–12th centuries AD, who were a confederation of Karluks, Chigil, Yaghma and other tribes.
Uzbek can be considered the direct descendant or a latter form of Chagatay, the language of great Turkic Central Asian literary development in the realm of Chagatai Khan, Timur (Tamerlane), and the Timurids (including the early Mughal rulers of India). The language was championed by Mir Ali-Shir Nava'i in the 15th and 16th centuries. Nava'i was the greatest representative of Chagatai language literature. He significantly contributed to the development of the Chagatay language and its direct descendant Uzbek and is widely considered to be the founder of Uzbek literature. Ultimately based on the Karluk variant of the Turkic languages, Chagatay contained large numbers of Persian and Arabic loanwords. By the 19th century it was rarely used for literary composition, but disappeared only in the early 20th century.
The term "Uzbek" as applied to language has meant different things at different times. Prior to 1921 "Uzbek" and "Sart" were considered to be different dialects:
In Khiva, Sarts spoke a highly Persianised form of Oghuz Turkic. After 1921 the Soviet regime abolished the term "Sart" as derogatory, and decreed that henceforth the entire settled Turkic population of Turkestan would be known as "Uzbeks", even though many had no Uzbek tribal heritage.
The standard written language that was chosen for the new republic in 1924, however, despite the protests of Uzbek Bolsheviks such as Faizullah Khojaev, was not pre-revolutionary "Uzbek" but the "Sart" language of the Samarkand region. All three dialects continue to exist within modern, spoken Uzbek. Edward A. Allworth argued that this "badly distorted the literary history of the region" and was used to give authors such as the 15th century author Ali-Shir Nava'i an Uzbek identity.
Number of speakers.
Estimates of the number of speakers of Uzbek vary widely. The Swedish encyclopedia "Nationalencyklopedin" estimates the number of native speakers to be 26 million,
and the "CIA World Factbook" estimates 25 million. Other sources estimate the number of speakers of Uzbek to be 21 million in Uzbekistan, 3.4 million in Afghanistan, 900,000 in Tajikistan, 800,000 in Kyrgyzstan, 500,000 in Kazakhstan, 300,000 in Turkmenistan, and 300,000 in Russia.
Loan words.
The influence of Islam, and by extension, Arabic, is evident in Uzbek loanwords. There is also a residual influence of Russian, from the time when Uzbek speakers were under tsarist and Soviet rule. Uzbek vocabulary has also been heavily influenced by Persian through its historic roots.
Dialects.
The Uzbek language has many dialects, varying widely from region to region. However, there is a commonly understood dialect which is used in mass media and in most printed materials. Among the most-widespread dialects are the Tashkent dialect, Afghan dialect, the Ferghana dialect, the Khorezm dialect, the Chimkent-Turkestan dialect, and the Surkhandarya dialect.
Writing systems.
Uzbek has been written in a variety of scripts throughout history:
Despite the official status of the Latin script in Uzbekistan, the use of Cyrillic is still widespread, especially in advertisements and signs. In newspapers, scripts may be mixed, with headlines in Latin and articles in Cyrillic. The Arabic script is no longer used in Uzbekistan except symbolically in limited texts or for the academic studies of Old Uzbek.
In the western Chinese region of Xinjiang, where there is a Uzbek minority, the Cyrillic is still used. However, the Uyghur Arabic script is also used sometimes.

</doc>
<doc id="71837" url="https://en.wikipedia.org/wiki?curid=71837" title="Qazaq">
Qazaq

Qazaq may refer to:

</doc>
