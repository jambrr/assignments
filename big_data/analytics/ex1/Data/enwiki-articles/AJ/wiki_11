<doc id="66997" url="https://en.wikipedia.org/wiki?curid=66997" title="Epidemiology">
Epidemiology

Epidemiology is the study and analysis of the patterns, causes, and effects of health and disease conditions in defined populations. It is the cornerstone of public health, and shapes policy decisions and evidence-based practice by identifying risk factors for disease and targets for preventive healthcare. Epidemiologists help with study design, collection, and statistical analysis of data, amend interpretation and dissemination of results (including peer review and occasional systematic review). Epidemiology has helped develop methodology used in clinical research, public health studies, and, to a lesser extent, basic research in the biological sciences.
Major areas of epidemiological study include disease etiology, transmission, outbreak investigation, disease surveillance, forensic epidemiology and screening, biomonitoring, and comparisons of treatment effects such as in clinical trials. Epidemiologists rely on other scientific disciplines like biology to better understand disease processes, statistics to make efficient use of the data and draw appropriate conclusions, social sciences to better understand proximate and distal causes, and engineering for exposure assessment.
Etymology.
"Epidemiology", literally meaning "the study of what is upon the people", is derived , suggesting that it applies only to human populations. However, the term is widely used in studies of zoological populations (veterinary epidemiology), although the term "epizoology" is available, and it has also been applied to studies of plant populations (botanical or plant disease epidemiology).
The distinction between "epidemic" and "endemic" was first drawn by Hippocrates, to distinguish between diseases that are "visited upon" a population (epidemic) from those that "reside within" a population (endemic). The term "epidemiology" appears to have first been used to describe the study of epidemics in 1802 by the Spanish physician Villalba in "Epidemiología Española". Epidemiologists also study the interaction of diseases in a population, a condition known as a syndemic.
The term epidemiology is now widely applied to cover the description and causation of not only epidemic disease, but of disease in general, and even many non-disease, health-related conditions, such as high blood pressure and obesity. Therefore, this epidemiology is based upon how the pattern of the disease cause changes in the function of everyone.
History.
The Greek physician Hippocrates, (See information regarding the African doctor Imhotep, whom Hippocrates called the true father of medicine. Imhotep) is known as the father of medicine, sought a logic to sickness; he is the first person known to have examined the relationships between the occurrence of disease and environmental influences. Hippocrates believed sickness of the human body to be caused by an imbalance of the four humors (air, fire, water and earth “atoms”). The cure to the sickness was to remove or add the humor in question to balance the body. This belief led to the application of bloodletting and dieting in medicine. He coined the terms "endemic" (for diseases usually found in some places but not in others) and "epidemic" (for diseases that are seen at some times but not others).
Modern era.
In the middle of the 16th century, a doctor from Verona named Girolamo Fracastoro was the first to propose a theory that these very small, unseeable, particles that cause disease were alive. They were considered to be able to spread by air, multiply by themselves and to be destroyable by fire. In this way he refuted Galen's miasma theory (poison gas in sick people). In 1543 he wrote a book "De contagione et contagiosis morbis", in which he was the first to promote personal and environmental hygiene to prevent disease. The development of a sufficiently powerful microscope by Anton van Leeuwenhoek in 1675 provided visual evidence of living particles consistent with a germ theory of disease.
Wu Youke (1582-1652) developed the concept that some diseases were caused by transmissible agents, which he called liqi (pestilential factors). His book Wenyi Lun (Treatise on Acute Epidemic Febrile Diseases) can be regarded as the main etiological work that brought forward the concept, ultimately attributed to Westerners, of germs as a cause of epidemic diseases (source: http://baike.baidu.com/view/143117.htm). His concepts are still considered in current scientific research in relation to Traditional Chinese Medicine studies (see: http://apps.who.int/medicinedocs/en/d/Js6170e/4.html).
Another pioneer, Thomas Sydenham (1624–1689), was the first to distinguish the fevers of Londoners in the later 1600s. His theories on cures of fevers met with much resistance from traditional physicians at the time. He was not able to find the initial cause of the smallpox fever he researched and treated.
John Graunt, a haberdasher and amateur statistician, published "Natural and Political Observations ... upon the Bills of Mortality" in 1662. In it, he analysed the mortality rolls in London before the Great Plague, presented one of the first life tables, and reported time trends for many diseases, new and old. He provided statistical evidence for many theories on disease, and also refuted some widespread ideas on them.
John Snow is famous for his investigations into the causes of the 19th century cholera epidemics, and is also known as the father of (modern) epidemiology. He began with noticing the significantly higher death rates in two areas supplied by Southwark Company. His identification of the Broad Street pump as the cause of the Soho epidemic is considered the classic example of epidemiology. Snow used chlorine in an attempt to clean the water and removed the handle; this ended the outbreak. This has been perceived as a major event in the history of public health and regarded as the founding event of the science of epidemiology, having helped shape public health policies around the world. However, Snow’s research and preventive measures to avoid further outbreaks were not fully accepted or put into practice until after his death.
Other pioneers include Danish physician Peter Anton Schleisner, who in 1849 related his work on the prevention of the epidemic of neonatal tetanus on the Vestmanna Islands in Iceland. Another important pioneer was Hungarian physician Ignaz Semmelweis, who in 1847 brought down infant mortality at a Vienna hospital by instituting a disinfection procedure. His findings were published in 1850, but his work was ill received by his colleagues, who discontinued the procedure. Disinfection did not become widely practiced until British surgeon Joseph Lister 'discovered' antiseptics in 1865 in light of the work of Louis Pasteur.
In the early 20th century, mathematical methods were introduced into epidemiology by Ronald Ross, Janet Lane-Claypon, Anderson Gray McKendrick, and others.
Another breakthrough was the 1954 publication of the results of a British Doctors Study, led by Richard Doll and Austin Bradford Hill, which lent very strong statistical support to the link between tobacco smoking and lung cancer.
In the late 20th century, with advancement of biomedical sciences, a number of molecular markers in blood, other biospecimens and environment were identified as predictors of development or risk of a certain disease. Epidemiology research to examine the relationship between these biomarkers analyzed at the molecular level, and disease was broadly named “molecular epidemiology”. Specifically, "genetic epidemiology" has been used for epidemiology of germline genetic variation and disease. Genetic variation is typically determined using DNA from peripheral blood leukocytes. Since the 2000s, genome-wide association studies (GWAS) have been commonly performed to identify genetic risk factors for many diseases and health conditions.
While most molecular epidemiology studies are still using conventional disease diagnosis and classification systems, it is increasingly recognized that disease evolution represents inherently heterogeneous processes differing from person to person. Conceptually, each individual has a unique disease process different from any other individual (“the unique disease principle”), considering uniqueness of the exposome (a totality of endogenous and exogenous / environmental exposures) and its unique influence on molecular pathologic process in each individual. Studies to examine the relationship between an exposure and molecular pathologic signature of disease (particularly cancer) became increasingly common throughout the 2000s. However, the use of molecular pathology in epidemiology posed unique challenges including lack of research guidelines and standardized statistical methodologies, and paucity of interdisciplinary experts and training programs. Furthermore, the concept of disease heterogeneity appears to conflict with the long-standing premise in epidemiology that individuals with the same disease name have similar etiologies and disease processes. To resolve these issues and advance population health science in the era of molecular precision medicine, “molecular pathology” and “epidemiology” was integrated to create a new interdisciplinary field of “molecular pathological epidemiology” (MPE), defined as “epidemiology of molecular pathology and heterogeneity of disease”. In MPE, investigators analyze the relationships between; (A) environmental, dietary, lifestyle and genetic factors; (B) alterations in cellular or extracellular molecules; and (C) evolution and progression of disease. A better understanding of heterogeneity of disease pathogenesis will further contribute to elucidate etiologies of disease. The MPE approach can be applied to not only neoplastic diseases but also non-neoplastic diseases. The concept and paradigm of MPE have become widespread in the 2010s.
Types of studies.
Epidemiologists employ a range of study designs from the observational to experimental and generally categorized as descriptive, analytic (aiming to further examine known associations or hypothesized relationships), and experimental (a term often equated with clinical or community trials of treatments and other interventions). In observational studies, nature is allowed to “take its course”, as epidemiologists observe from the sidelines. Conversely, in experimental studies, the epidemiologist is the one in control of all of the factors entering a certain case study. Epidemiological studies are aimed, where possible, at revealing unbiased relationships between exposures such as alcohol or smoking, biological agents, stress, or chemicals to mortality or morbidity. The identification of causal relationships between these exposures and outcomes is an important aspect of epidemiology. Modern epidemiologists use informatics as a tool.
Observational studies have two components, descriptive and analytical. Descriptive observations pertain to the “who, what, where and when of health-related state occurrence”. However, analytical observations deal more with the ‘how’ of a health-related event. Experimental epidemiology contains three case types: randomized controlled trials (often used for new medicine or drug testing), field trials (conducted on those at a high risk of conducting a disease), and community trials (research on social originating diseases).
The term 'epidemiologic triad' is used to describe the intersection of "Host", "Agent", and "Environment" in analyzing an outbreak.
Case series.
Case-series may refer to the qualitative study of the experience of a single patient, or small group of patients with a similar diagnosis, or to a statistical technique comparing periods during which patients are exposed to some factor with the potential to produce illness with periods when they are unexposed.
The former type of study is purely descriptive and cannot be used to make inferences about the general population of patients with that disease. These types of studies, in which an astute clinician identifies an unusual feature of a disease or a patient's history, may lead to formulation of a new hypothesis. Using the data from the series, analytic studies could be done to investigate possible causal factors. These can include case control studies or prospective studies. A case control study would involve matching comparable controls without the disease to the cases in the series. A prospective study would involve following the case series over time to evaluate the disease's natural history.
The latter type, more formally described as self-controlled case-series studies, divide individual patient follow-up time into exposed and unexposed periods and use fixed-effects Poisson regression processes to compare the incidence rate of a given outcome between exposed and unexposed periods. This technique has been extensively used in the study of adverse reactions to vaccination, and has been shown in some circumstances to provide statistical power comparable to that available in cohort studies.
Case-control studies.
Case-control studies select subjects based on their disease status. It is a retrospective study. A group of individuals that are disease positive (the "case" group) is compared with a group of disease negative individuals (the "control" group). The control group should ideally come from the same population that gave rise to the cases. The case-control study looks back through time at potential exposures that both groups (cases and controls) may have encountered. A 2×2 table is constructed, displaying exposed cases (A), exposed controls (B), unexposed cases (C) and unexposed controls (D). The statistic generated to measure association is the odds ratio (OR), which is the ratio of the odds of exposure in the cases (A/C) to the odds of exposure in the controls (B/D), i.e. OR = (AD/BC).
If the OR is significantly greater than 1, then the conclusion is "those with the disease are more likely to have been exposed," whereas if it is close to 1 then the exposure and disease are not likely associated. If the OR is far less than one, then this suggests that the exposure is a protective factor in the causation of the disease.
Case-control studies are usually faster and more cost effective than cohort studies, but are sensitive to bias (such as recall bias and selection bias). The main challenge is to identify the appropriate control group; the distribution of exposure among the control group should be representative of the distribution in the population that gave rise to the cases. This can be achieved by drawing a random sample from the original population at risk. This has as a consequence that the control group can contain people with the disease under study when the disease has a high attack rate in a population.
A major drawback for case control studies is that, in order to be considered to be statistically significant, the minimum number of cases required at the 95% confidence interval is related to the odds ratio by the equation:
total cases = (a+c) = (1.96)^2×(1+N)×(1÷ln(OR))^2×((OR+2√OR+1)÷√OR)≈15.5×(1+N)×(1÷ln(OR))^2
where N = the ratio of cases to controls.
As the odds ratio approached 1, approaches 0; rendering case control studies all but useless for low odds ratios. For instance, for an odds ratio of 1.5 and cases = controls, the table shown above would look like this:
For an odds ratio of 1.1:
Cohort studies.
Cohort studies select subjects based on their exposure status. The study subjects should be at risk of the outcome under investigation at the beginning of the cohort study; this usually means that they should be disease free when the cohort study starts. The cohort is followed through time to assess their later outcome status. An example of a cohort study would be the investigation of a cohort of smokers and non-smokers over time to estimate the incidence of lung cancer. The same 2×2 table is constructed as with the case control study. However, the point estimate generated is the relative risk (RR), which is the probability of disease for a person in the exposed group, "P"e = "A" / ("A" + "B") over the probability of disease for a person in the unexposed group, "P""u" = "C" / ("C" + "D"), i.e. "RR" = "P"e / "P"u.
As with the OR, a RR greater than 1 shows association, where the conclusion can be read "those with the exposure were more likely to develop disease."
Prospective studies have many benefits over case control studies. The RR is a more powerful effect measure than the OR, as the OR is just an estimation of the RR, since true incidence cannot be calculated in a case control study where subjects are selected based on disease status. Temporality can be established in a prospective study, and confounders are more easily controlled for. However, they are more costly, and there is a greater chance of losing subjects to follow-up based on the long time period over which the cohort is followed.
Cohort studies also are limited by the same equation for number of cases as for cohort studies, but, if the base incidence rate in the study population is very low, the number of cases required is reduced by ½.
Causal inference.
Although epidemiology is sometimes viewed as a collection of statistical tools used to elucidate the associations of exposures to health outcomes, a deeper understanding of this science is that of discovering "causal" relationships.
"Correlation does not imply causation" is a common theme for much of the epidemiological literature. For epidemiologists, the key is in the term inference. Correlation is a necessary but not sufficient criteria for inference of causation. Epidemiologists use gathered data and a broad range of biomedical and psychosocial theories in an iterative way to generate or expand theory, to test hypotheses, and to make educated, informed assertions about which relationships are causal, and about exactly how they are causal.
Epidemiologists emphasize that the "one cause – one effect" understanding is a simplistic mis-belief. Most outcomes, whether disease or death, are caused by a chain or web consisting of many component causes (Rothman et al., 2008). Causes can be distinguished as necessary, sufficient or probabilistic conditions. If a necessary condition can be identified and controlled (e.g., antibodies to a disease agent, energy in an injury), the harmful outcome can be avoided (Robertson, 2015).
Bradford Hill criteria.
In 1965, Austin Bradford Hill proposed a series of considerations to help assess evidence of causation, which have come to be commonly known as the "Bradford Hill criteria". In contrast to the explicit intentions of their author, Hill's considerations are now sometimes taught as a checklist to be implemented for assessing causality. Hill himself said "None of my nine viewpoints can bring indisputable evidence for or against the cause-and-effect hypothesis and none can be required "sine qua non"."
Legal interpretation.
Epidemiological studies can only go to prove that an agent could have caused, but not that it did cause, an effect in any particular case:
"Epidemiology is concerned with the incidence of disease in populations and does not address the question of the cause of an individual's disease. This question, sometimes referred to as specific causation, is beyond the domain of the science of epidemiology. Epidemiology has its limits at the point where an inference is made that the relationship between an agent and a disease is causal (general causation) and where the magnitude of excess risk attributed to the agent has been determined; that is, epidemiology addresses whether an agent can cause a disease, not whether an agent did cause a specific plaintiff's disease."
In United States law, epidemiology alone cannot prove that a causal association does not exist in general. Conversely, it can be (and is in some circumstances) taken by US courts, in an individual case, to justify an inference that a causal association does exist, based upon a balance of probability.
The subdiscipline of forensic epidemiology is directed at the investigation of specific causation of disease or injury in individuals or groups of individuals in instances in which causation is disputed or is unclear, for presentation in legal settings.
Population-based health management.
Epidemiological practice and the results of epidemiological analysis make a significant contribution to emerging population-based health management frameworks.
Population-based health management encompasses the ability to:
Modern population-based health management is complex, requiring a multiple set of skills (medical, political, technological, mathematical etc.) of which epidemiological practice and analysis is a core component, that is unified with management science to provide efficient and effective health care and health guidance to a population. This task requires the forward looking ability of modern risk management approaches that transform health risk factors, incidence, prevalence and mortality statistics (derived from epidemiological analysis) into management metrics that not only guide how a health system responds to current population health issues, but also how a health system can be managed to better respond to future potential population health issues.
Examples of organizations that use population-based health management that leverage the work and results of epidemiological practice include Canadian Strategy for Cancer Control, Health Canada Tobacco Control Programs, Rick Hansen Foundation, Canadian Tobacco Control Research Initiative.
Each of these organizations use a population-based health management framework called Life at Risk that combines epidemiological quantitative analysis with demographics, health agency operational research and economics to perform:
Validity: precision and bias.
Different fields in epidemiology have different levels of validity. One way to assess the validity of findings is the ratio of false-positives (claimed effects that are not correct) to false-negatives (studies which fail to support a true effect). To take the field of genetic epidemiology, candidate-gene studies produced over 100 false-positive findings for each false-negative. By contrast genome-wide association appear close to the reverse, with only one false positive for every 100 or more false-negatives. This ratio has improved over time in genetic epidemiology as the field has adopted stringent criteria. By contrast other epidemiological fields have not required such rigorous reporting and are much less reliable as a result.
Random error.
Random error is the result of fluctuations around a true value because of sampling variability. Random error is just that: random. It can occur during data collection, coding, transfer, or analysis. Examples of random error include: poorly worded questions, a misunderstanding in interpreting an individual answer from a particular respondent, or a typographical error during coding. Random error affects measurement in a transient, inconsistent manner and it is impossible to correct for random error.
There is random error in all sampling procedures. This is called sampling error.
Precision in epidemiological variables is a measure of random error. Precision is also inversely related to random error, so that to reduce random error is to increase precision. Confidence intervals are computed to demonstrate the precision of relative risk estimates. The narrower the confidence interval, the more precise the relative risk estimate.
There are two basic ways to reduce random error in an epidemiological study. The first is to increase the sample size of the study. In other words, add more subjects to your study. The second is to reduce the variability in measurement in the study. This might be accomplished by using a more precise measuring device or by increasing the number of measurements.
Note, that if sample size or number of measurements are increased, or a more precise measuring tool is purchased, the costs of the study are usually increased. There is usually an uneasy balance between the need for adequate precision and the practical issue of study cost.
Systematic error.
A systematic error or bias occurs when there is a difference between the true value (in the population) and the observed value (in the study) from any cause other than sampling variability. An example of systematic error is if, unknown to you, the pulse oximeter you are using is set incorrectly and adds two points to the true value each time a measurement is taken. The measuring device could be precise but not accurate. Because the error happens in every instance, it is systematic. Conclusions you draw based on that data will still be incorrect. But the error can be reproduced in the future (e.g., by using the same mis-set instrument).
A mistake in coding that affects "all" responses for that particular question is another example of a systematic error.
The validity of a study is dependent on the degree of systematic error. Validity is usually separated into two components:
Selection bias.
Selection bias occurs when study subjects are selected or become part of the study as a result of a third, unmeasured variable which is associated with both the exposure and outcome of interest. For instance, it has repeatedly been noted that cigarette smokers and non smokers tend to differ in their study participation rates. (Sackett D cites the example of Seltzer et al., in which 85% of non smokers and 67% of smokers returned mailed questionnaires.) It is important to note that such a difference in response will not lead to bias if it is not also associated with a systematic difference in outcome between the two response groups.
Information bias.
Information bias is bias arising from systematic error in the assessment of a variable. An example of this is recall bias. A typical example is again provided by Sackett in his discussion of a study examining the effect of specific exposures on fetal health: "in questioning mothers whose recent pregnancies had ended in fetal death or malformation (cases) and a matched group of mothers whose pregnancies ended normally (controls) it was found that 28% of the former, but only 20% of the latter, reported exposure to drugs which could not be substantiated either in earlier prospective
interviews or in other health records". In this example, recall bias probably occurred as a result of women who had had miscarriages having an apparent tendency to better recall and therefore report previous exposures.
Confounding.
Confounding has traditionally been defined as bias arising from the co-occurrence or mixing of effects of extraneous factors, referred to as confounders, with the main effect(s) of interest. A more recent definition of confounding invokes the notion of "counterfactual" effects. According to this view, when one observes an outcome of interest, say Y=1 (as opposed to Y=0), in a given population A which is entirely exposed (i.e. exposure "X" = 1 for every unit of the population) the risk of this event will be "R"A1. The counterfactual or unobserved risk "R"A0 corresponds to the risk which would have been observed if these same individuals had been unexposed (i.e. "X" = 0 for every unit of the population). The true effect of exposure therefore is: "R"A1 − "R"A0 (if one is interested in risk differences) or "R"A1/"R"A0 (if one is interested in relative risk). Since the counterfactual risk "R"A0 is unobservable we approximate it using a second population B and we actually measure the following relations: "R"A1 − "R"B0 or "R"A1/"R"B0. In this situation, confounding occurs when "R"A0 ≠ "R"B0. (NB: Example assumes binary outcome and exposure variables.)
Some epidemiologists prefer to think of confounding separately from common categorizations of bias since, unlike selection and information bias, confounding stems from real causal effects.
The profession.
To date, few universities offer epidemiology as a course of study at the undergraduate level. Many epidemiologists are physicians, or hold graduate degrees such as a Master of Public Health (MPH), Master of Science of Epidemiology (MSc.). Doctorates include the Doctor of Public Health (DrPH), Doctor of Pharmacy (PharmD), Doctor of Philosophy (PhD), Doctor of Science (ScD), Doctor of Social Work (DSW), Doctor of Clinical Practice (DClinP), Doctor of Podiatric Medicine (DPM), Doctor of Veterinary Medicine (DVM), Doctor of Nursing Practice (DNP), Doctor of Physical Therapy (DPT), or for clinically trained physicians, Doctor of Medicine (MD) or Bachelor of Medicine and Surgery (MBBS or MBChB) and Doctor of Osteopathic Medicine (DO).
As public health/health protection practitioners, epidemiologists work in a number of different settings. Some epidemiologists work 'in the field'; i.e., in the community, commonly in a public health/health protection service and are often at the forefront of investigating and combating disease outbreaks. Others work for non-profit organizations, universities, hospitals and larger government entities such as the Centers for Disease Control and Prevention (CDC), the Health Protection Agency, the World Health Organization (WHO), or the Public Health Agency of Canada. Epidemiologists can also work in for-profit organizations such as pharmaceutical and medical device companies in groups such as market research or clinical development.

</doc>
<doc id="67000" url="https://en.wikipedia.org/wiki?curid=67000" title="Stephen R. Donaldson">
Stephen R. Donaldson

Stephen Reeder Donaldson (born May 13, 1947) is an American fantasy, science fiction and mystery novelist, most famous for "The Chronicles of Thomas Covenant", his ten-novel fantasy series. His work is characterized by psychological complexity, conceptual abstractness, moral bleakness, and the use of an arcane vocabulary, and has attracted critical praise for its "imagination, vivid characterizations, and fast pace." He earned his bachelor's degree from The College of Wooster and a Master's degree from Kent State University. He currently resides in New Mexico.
In the United Kingdom he is usually called "Stephen Donaldson" (without the "R").
Personal life.
Donaldson spent part of his youth in India, where he attended what is now the Kodaikanal International School. He was attending Kent State University as a graduate student at the time of the Kent State shootings on May 4, 1970. Though he was not on campus at the time of the shootings, his apartment was one and a half blocks away, and he was forced to live under martial law for three days afterwards. Donaldson does not like to discuss the incident, as he finds the memories disturbing.
Donaldson is a fan of opera, and has said that he "love that direct expression of passionate emotion in beautiful sound". In 1994, he gained a black belt in Shotokan karate.
Major influences.
Donaldson is part of the generation of fantasy authors which came to prominence in the 1970s and early 1980s. Like that of many of his peers, his writing is heavily influenced by the works of J. R. R. Tolkien. However, Donaldson's stories show a wide range of other influences, including Mervyn Peake, C. S. Lewis, Robert E. Howard, and the operas of Richard Wagner. Donaldson is also a great fan of Roger Zelazny's "Amber" novels, which were a direct inspiration for his own "Mordant's Need" series. Also, in the "Gradual Interview" section of his website, Donaldson mentions his extensive study of Joseph Conrad, Henry James and William Faulkner to further develop his narrative style.
"The Chronicles of Thomas Covenant".
Donaldson's most celebrated series is "The Chronicles of Thomas Covenant", which centers on a cynical leper, shunned by society, who is destined to become the heroic savior of an alternative Earth. Covenant struggles against the tyrannical Lord Foul, who intends to break the physical universe in order to escape his bondage and wreak revenge upon his arch enemy, The Creator.
The "Chronicles" were originally published as two trilogies of novels between 1977 and 1983. According to his current publisher, Putnam's, those two series sold more than 10 million copies. A third series, "The Last Chronicles of Thomas Covenant", began publication in 2004 with the novel "The Runes of the Earth". With the second book of that series, "Fatal Revenant", Donaldson again attained bestseller status when the book reached number 12 on the "New York Times" Bestseller List in October 2007.
The Gap Cycle.
A science fiction epic set in a future where humans have pushed far out into space in the attempt to replace depleted resources, "The Gap Cycle" follows two concurrent story arcs. The first concerns an ensign in the United Mining Companies Police (UMCP), Morn Hyland, who is attempting simply to stay alive after being captured by a marauder named Angus Thermopyle. The second follows the Byzantine political maneuvering of the head of the UMCP, Warden Dios, as he attempts to thwart the machinations of his boss, Holt Fasner, who is the CEO of United Mining Companies (UMC) and the most powerful man in human space.
Each of the epics takes place against the backdrop of a threat to human survival itself from an alien species called the Amnion who use genetic mutation as a way to assimilate and overcome. Trade in raw materials (mostly ores) is carried out with the Amnion in exchange for technology, by both the UMC and illegals. Some illegals trade in Amnion territorial space, referred to as "forbidden space", out of bounds to the UMCP by treaty.
Donaldson wrote the series in part to be a reworking of Wagner's Ring Cycle. The "Gap" of the title refers to the faster-than-light drives used by the space vessels in order to cross great distances, an instantaneous occurrence similar to the notion of "folding" space.
The Gap series.
The 2008 reprinting of the series combines "The Real Story" and "Forbidden Knowledge" into a single volume. According to Donaldson's website, this was done at his request.
Other works.
Early work.
Donaldson has stated that, when he was younger, he wrote two fan-fiction novellas: one based on Marvel Comics' Thor, and the other based on Joseph Conrad's "Heart of Darkness". These have never been published. As Donaldson grew older, he discovered that the sensation that he was "making it all up" himself was necessary for his imagination to work well. He now regards these early novellas as failed experiments in the process of discovering himself as a writer. He feels the same way about a play he wrote, whose performance at Kent State University convinced him that he was "not cut out to be a playwright".
"The Man Who".
"The Man Who" is a series of mystery novels written by Donaldson and published under the pseudonym Reed Stephens, derived from his full name, "Stephen Reeder Donaldson". Donaldson "always hated" writing under a false name, but was forced to do so by his publisher, Ballantine Books, who had a firm belief in "category publishing" and thought that readers would feel betrayed if books of such different genres were published under the name of a single author. However, the books sold poorly even when they were re-printed under Donaldson's name by Tor/Forge Books.
Donaldson has indicated that he intends to write at least one more "The Man Who" novel after completion of "The Last Chronicles".
Awards.
Awards referenced from 

</doc>
<doc id="67003" url="https://en.wikipedia.org/wiki?curid=67003" title="Carrie (novel)">
Carrie (novel)

Carrie is an American epistolary novel and author Stephen King's first published novel, released on April 5, 1974, with an approximate first print-run of 30,000 copies. Set primarily in the then-future year of 1979, it revolves around the eponymous Carrietta N. "Carrie" White, a misfit and bullied high school girl who uses her newly discovered telekinetic powers to exact revenge on those who torment her, while in the process causing one of the worst local disasters in American history. King has commented that he finds the work to be "raw" and "with a surprising power to hurt and horrify." It is one of the most frequently banned books in United States schools. Much of the book is written in an epistolary structure, using newspaper clippings, magazine articles, letters, and excerpts from books to tell how Carrie destroyed the fictional town of Chamberlain, Maine while exacting revenge on her sadistic classmates.
Several adaptations of "Carrie" have been released, including a 1976 feature film, a 1988 Broadway musical, a , a 2002 television movie, and a 2013 feature film remake.
King’s works self-consciously and conspicuously shadow the major European and American Gothic writers and works, echoing and repeating their themes, motifs and rhetoric, drawing (for example) on the American sense of Gothic place and on onomastic and other textual resources.The themes in Carrie represented in every event that happens in the novel.
The book is dedicated to King's wife Tabitha: "This is for Tabby, who got me into it – and then bailed me out of it."
King's 1979 novel "The Dead Zone" mentions the book in connection with a fire at another high school prom.
Plot summary.
Carietta "Carrie" White is a 16-year-old girl from Chamberlain, Maine. Her mother, Margaret, a fanatical Christian fundamentalist, has a vindictive and unstable personality, and over the years has ruled Carrie harshly with repeated threats of damnation, as well as occasional physical abuse. Carrie does not fare much better at her school where her frumpy looks and lack of friends make her the butt of ridicule.
At the beginning of the novel, Carrie has her first period while showering after a physical education class; the terrified Carrie has no understanding of menstruation as her mother never told her about it. Her classmates use the event as an opportunity to taunt her; led by Chris Hargensen, they throw tampons and sanitary napkins at her. When gym teacher Miss Desjardin happens upon the scene, she at first berates Carrie for her stupidity but is horrified when she realizes that Carrie has no idea what has happened to her. She helps her clean up and tries to explain. Carrie's mother shows no sympathy for her first encounter with what she calls "the woman's curse".
Miss Desjardin, still incensed over the locker room incident and ashamed at her initial disgust with Carrie, wants all the girls who taunted Carrie suspended and banned from attending the school prom, but the principal instead punishes the girls by giving them several detentions. When Chris, after an altercation with Miss Desjardin, refuses to appear for the detention, she is suspended and barred from the prom and tries to get her father, a prominent local lawyer, to intimidate the school principal into reinstating her privileges.
Carrie gradually discovers her telekinetic powers, which she had apparently possessed since birth, but had not had conscious control over after her infancy, though she remembers several incidents from throughout her life. Carrie practices her powers in secret, developing strength, and also finds that she is somewhat telepathic.
Meanwhile, Sue Snell, another popular girl who had earlier teased Carrie, begins to feel remorseful about her participation in the locker room antics. With the prom fast approaching, Sue convinces her boyfriend, Tommy Ross, one of the most popular boys in the school, to ask Carrie to the prom. Carrie is suspicious but accepts his offer, and makes a red velvet gown. Carrie's mother won't hear of her daughter doing anything so "carnal" as attending a school dance, as she believes that sex in any form is sinful, even after marriage. She also reveals that she knows about Carrie's telekinetic powers, which she considers a form of witchcraft; it seems that they appear every third generation in her family. Carrie, however, is tired of hearing that everything is a sin; she wants a normal life and sees the prom as a new beginning.
The prom initially goes well for Carrie; Tommy's friends are welcoming and Tommy finds himself attracted towards her. Chris Hargensen, still furious, devises her own revenge with her boyfriend Billy Nolan: they fill two buckets full of pig's blood and suspend the buckets over the stage. They rig Carrie's election as prom queen and empty the buckets on Carrie's and Tommy's heads. Tommy is knocked unconscious by one of the buckets and dies within minutes, and he and Carrie are both drenched in blood. Nearly everyone in attendance, even some of the teachers, begin laughing at Carrie, who is finally pushed over the edge. She leaves the building in agonized humiliation, remembers her telekinesis, and decides to use it for vengeance. Initially planning only to lock all the doors and turn on the sprinklers, Carrie remembers the electrical equipment set up for the sound system but turns the sprinklers on anyway. Watching through the windows, she witnesses the deaths of two students and a school official by electrocution, and decides to kill everyone, causing a massive fire that destroys the school and traps almost everyone inside.
Walking home, she burns almost all of Downtown Chamberlain by breaking power lines and exploding gas stations. A side-effect of her telekinesis is "broadcast" telepathy, which causes the city's inhabitants to become aware that the carnage was caused by Carrie White even if they do not know who she is. Carrie returns home to confront her mother, who believes Carrie has been possessed by Satan and that the only way to save her is to kill her. Revealing that Carrie's conception was a result of what may have been marital rape although she admits she enjoyed the sex, she stabs Carrie in the shoulder with a kitchen knife, but Carrie kills her mother by stopping her heart.
Mortally wounded but still alive, Carrie makes her way to a roadhouse where she sees Chris and Billy leaving; after Billy attempts to run her over, she telekinetically takes control of the vehicle and wrecks the car, killing them both. Sue Snell, who has been following Carrie's telepathic "broadcast," finds Carrie collapsed in the parking lot. The two have a brief telepathic conversation. Though Carrie had believed that Sue and Tommy had set her up for the prank, Carrie realizes that Sue is innocent and has never felt real animosity towards her. Carrie forgives her and dies.
One of the few survivors of the fire at the prom is Miss Desjardin, who resigns shortly afterwards, believing that she might have prevented the catastrophe if she had reached out more to Carrie. The principal also resigns. The surviving seniors attend a grim graduation ceremony.
Fictional transcripts of Congressional hearings and a final "White Committee" report are shown; at the end, the report concludes that at least there are no others like Carrie so events like these will not happen again. However, the final document in the book is a cheery letter from an Appalachian woman to her sister, talking about her daughter's telekinetic powers and reminiscing about her grandmother, who had similar abilities.
Publication history.
"Carrie" was actually King's fourth novel, but it was the first one to be published. It was written while he was living in a trailer, on a portable typewriter (on which he also wrote "Misery") that belonged to his wife Tabitha. It began as a short story intended for "Cavalier" magazine, but King tossed the first three pages of his work in the garbage. Of King's published short stories at the time, he recalled,
His wife fished the pages out of the garbage and encouraged him to finish the story; he followed her advice and expanded it into a novel. King said, "I persisted because I was dry and had no better ideas… my considered opinion was that I had written the world's all-time loser."
Carrie is based on a composite of two girls Stephen King observed while attending grade school and high school. Of one of them, he recalled:
King says he wondered what it would have been like to have been raised by such a mother, and based the story itself on a reversal of the Cinderella fairy tale. According to one biography of King, later the girl "married a man who was as odd as her, had kids, and eventually killed herself."
Carrie's telekinesis resulted from King's earlier reading about this topic. At the time of publication, King was working as a high school English teacher at Hampden Academy and barely making ends meet. To cut down on expenses, King had the phone company remove the telephone from his house. As a result, when King received word that the book was chosen for publication, his phone was out of service. Doubleday editor William Thompson (who would eventually become King's close friend), sent a telegram to King's house which read: "Carrie Officially A Doubleday Book. $2,500 Advance Against Royalties. Congrats, Kid - The Future Lies Ahead, Bill." It has been presumed that King drew inspiration from his time as a teacher. New American Library bought the paperback rights for $400,000, which, according to King's contract with Doubleday, was split between them. King eventually quit the teaching job after receiving the publishing payment. The hardback sold a mere 13,000 copies; the paperback, released a year later, sold over 1 million copies in its first year. In King's book, "On Writing", he mentions that he wrote all of "Carrie" in only about two weeks.
King recalls, ""Carrie" was written after "Rosemary's Baby," but before "The Exorcist", which really opened up the field. I didn't expect much of "Carrie". I thought, 'Who'd want to read a book about a poor little girl with menstrual problems?' I couldn't believe I was writing it."
Recalling, King was not confident in the beginning of the novel since he could not relate to Carrie's problems and doubted the significance of the novel. With the support of his wife he decided to proceed with his writings. King structured his novel in that in a way of multiple self-conscious narrators, having three narrators reinforces the novel's warning against the limitations of reason and the potential for abuse in the product of reason.

</doc>
<doc id="67007" url="https://en.wikipedia.org/wiki?curid=67007" title="Edain">
Edain

In the fiction of J. R. R. Tolkien, the Edain were men (humans) who made their way into Beleriand in the First Age, and were friendly to the Elves.
The Sindarin word "Edain" (the second syllable is pronounced as English "dine" rather than English "dane"; the stress falls on the first syllable), singular "Adan" (Quenya Atani, "Atan") literally meant "Second People", and originally referred to all Men, but later it only applied to the Men of Beleriand and their descendants. The Quenya term "Atani" kept its old meaning.
They were divided in three large houses, or tribes:
The Bëorians and Marachians shared a common tongue and were known to each other before settling in Beleriand. The tongue of the Haladin was alien to them.
The House of Bëor was nearly wiped out by Morgoth, and the remainder of its people merged with the Hadorians to become the Númenóreans. It would seem that the Haladin of Beleriand were completely wiped out, or at least disappeared as a separate people.
The Half-elven Elros was heir to the lordship of all three houses of the Edain, and chose to become one of the Edain. He became the first King of Númenor.
When the Númenóreans returned to Middle-earth in the Second Age, they encountered many Men who were obviously related to the Atani: they classified these Men as "Middle Men", and established friendly relations with them. Examples are the Rohirrim, the Men of Dale, and the Breelanders.
Other Men, such as the Dunlendings, were not recognised as Middle Men because they were related to the Haladin rather than Bëorians or Marachians, and they were hostile to Númenor.
A fourth kind of Men came with the Second House, and called themselves "Drughu". This name was adopted in Sindarin as Drúedain or "Drû-folk". They were a strange people, living with the Haladin (and possibly related to them) in the forest of Brethil, some even apparently made it to Númenor, but they died out or had left before the Akallabêth. In the Third Age, their kin were known as the "Woses" of Drúadan forest.

</doc>
<doc id="67008" url="https://en.wikipedia.org/wiki?curid=67008" title="Beleriand">
Beleriand

In J. R. R. Tolkien's fictional legendarium, Beleriand was a region in northwestern Middle-earth during the First Age. Events in Beleriand are described chiefly in his work "The Silmarillion", which tells the story of the early ages of Middle-earth in a style similar to the epic hero tales of Nordic literature. Beleriand also appears in the works "The Book of Lost Tales", "The Children of Húrin", and in the epic poems of "The Lays of Beleriand".
Internal setting.
Geography.
Originally, the name belonged only to the area around the Bay of Balar, but in time, the name was applied to the entire land. Beleriand was originally inhabited by Elves, and later also by Men. To the west and south it had a long shore with the Great Sea Belegaer, to the north were the highland regions of Hithlum, Dorthonion and the hills of Himring, to the east the Ered Luin reached nearly to the sea. The land of Nevrast in the northwest was sometimes considered part of Beleriand.
The River Sirion, the chief river of Beleriand, running north to south, divided it into West Beleriand and East Beleriand. Crossing it east to west was a series of hills and a sudden drop in elevation known as Andram, the Long Wall. The river sank into the ground at the Fens of Sirion, and re-emerged below the Andram at the Gates of Sirion. To the east of the Long Wall, was the River Gelion and its six tributaries draining the Ered Luin, in an area known as Ossiriand, or the Land of Seven Rivers. The River Brithon and the River Nenning were the two lesser rivers of the western land of Falas.
In volume IV of the "History of Middle-earth" are the early maps of Beleriand, then still called Broseliand, showing the elevation of the land by use of contour lines.
In the northwest of Beleriand, north of the Firth of Drengist and between Ered Lómin (the Echoing Mountains) and the shore of Belegaer (the Great Sea), was a region called "Lammoth". Lammoth means "the Great Echo", and it is so named because it is where Morgoth and Ungoliant fled after the darkening of Valinor and Morgoth's theft of the Silmarils. Ungoliant lusted for the Silmarils and she attacked Morgoth in order to get them. He let out a great cry, which echoed throughout the north of Middle-earth. As it is told in the Silmarillion:
"Ungoliant had grown great, and less by the power that had gone out of him; and she rose against him... Then Morgoth sent forth a terrible cry, that echoed in the mountains. Therefore that region was called Lammoth, for the echoes of his voice dwelt there ever after, so that any who cried aloud in that land awoke them, and all the waste between the hills and the sea was filled with a clamour as of voices in anguish. The cry of Morgoth in that hour was the greatest and most dreadful that was ever heard in the northern world."
In "Of Tuor and his Coming to Gondolin" in "Unfinished Tales", the name instead refers to the acoustic properties of the location and the natural reverberations they cause. When Fëanor landed there in the First Age ""the voices of his host were swelled to a mighty clamour"" by the Echoing Mountains.
Realms of Beleriand.
Arvernien.
Arvernien is the southernmost region of Beleriand, bordered on the east by the Mouths of Sirion.
The Mouths were the refuge of the remnants of Eldar and Edain of Beleriand after the Nírnaeth Arnoediad and the Sack of Menegroth.
This region was home to surviving Sindar of Doriath after that land was destroyed. Some Noldor from Nargothrond and Gondolin, as well as the few survivors of the Three Houses of the Edain, also settled near the Mouths of Sirion. The first rulers of this region were Tuor of the Edain and Idril of Gondolin. Later their son Eärendil Half-elven, married the Half-elven Elwing, Dior's daughter and survivor of the Sack of Doriath. Elros and Elrond, the sons of Eärendil and Elwing, were born in Arvernien.
Elwing possessed the Silmaril of Beren and Lúthien. The refugees built many ships, sailing across the seas and to the nearby Isle of Balar where Círdan had fled with the survivors of Eglarest and Brithombar.
Eärendil built the ship Vingilótë and sailed far seeking the hidden land of Valinor to ask for the pardon of the Valar. But though his journeys carried him to many shores, Eärendil was unsuccessful. While he was at sea, the surviving Sons of Fëanor attacked the Mouths of Sirion to reclaim the Silmaril. They killed many people but were almost all slain themselves, save for Maedhros and Maglor. Elwing cast herself into the Sea with the Silmaril, but was saved by the Vala Ulmo and sent to Eärendil. Maglor later repented, and raised Elros and Elrond as his foster children.
With the power of the Silmaril, Eärendil and Elwing found a passage to Valinor, where he pleaded on behalf of Elves and Men with the Valar.
In one version of the "Silmarillion" mythology, the chief army of the Valar landed at the Mouths of Sirion during the War of Wrath. By the end of the War, most of the survivors of Beleriand gathered at the Mouths and the Isle of Balar, and many of them went to Lindon until the Elves were summoned to Valinor, and the Edain to the new isle of Númenor.
Doriath.
Doriath is the realm of the Sindar, the Grey Elves of King Thingol in Beleriand. Along with the other great forests of Tolkien's legendarium such as Mirkwood, Fangorn and Lothlórien it serves as the central stage in the theatre of its time, the First Age. On this stage many of the notable characters and events appear such as: The Geste of Beren and Lúthien from "The Lays of Beleriand", parts of "The Children of Húrin" and, of course, "The Silmarillion". It is called the 'Fenced Land' because Melian, the queen of that land, put a girdle of enchantment about it, so that none can enter without the leave of King Thingol.
East Beleriand.
Himlad (Sindarin: 'cool plain') was a land in northeast Beleriand situated between the rivers Celon and Aros. On its northern border to Lothlann were the hills of Himring and the March of Maedhros. Along with the Pass of Aglon, Himlad was held by Celegorm and Curufin until the Dagor Bragollach, after which it remained uninhabited. 
Thargelion (S. 'beyond Gelion') was the land east of the river Gelion and north of the river Ascar, and therefore not counted as part of Ossiriand. After the Exile of the Noldor this was the land of Caranthir, one of the Sons of Fëanor, and after him it was often called Dor Caranthir. The Second House of Men briefly dwelt here until they were nearly wiped out in an Orc-raid.
Maglor's Gap was a lowland region lying between Himring and the Blue Mountains, the widest break in the northern mountain-fences of Beleriand. It was guarded against the forces of Morgoth in the early First Age by Maglor, second son of Fëanor. During Dagor Aglareb the Gap was breached by Orcs however they were pushed back. During the Dagor Bragollach, Glaurung came through the pass and the Elves were defeated; Maglor fled to Himring, along with most survivors of the battle in that region and Thargelion was deserted.
Falas.
The Falas was the realm of Círdan the Shipwright and his people in the years of Starlight and the First Age of Sun, Sindarin Elves who were known as the Falathrim. They lived in two great walled havens, Eglarest at the mouth of the River Nenning, and to the north of that Brithombar at the mouth of the River Brithon. The Havens were besieged during the First Battle of Beleriand, but during the Dagor-nuin-Giliath the Orcs that besieged the cities went north to fight the Noldor, and were all slain. After 45 F.A. West Beleriand was ruled by Finrod Felagund who ruled from Nargothrond, and Círdan was his ally.
The Havens of the Falas held out during the later Battles of Beleriand until they were finally destroyed in F.A. 473, and Círdan's people fled to the Mouths of Sirion and the Isle of Balar.
Gondolin.
Gondolin was a secret city of Elves in the north of Beleriand. As recounted in "The Silmarillion", the Vala Ulmo, the Lord of Waters, revealed the location of the Vale of Tumladen to the Noldorin Lord Turgon in a dream. Under this divine guidance, Turgon travelled from his kingdom in Nevrast and found the vale. Within the Echoriath, the Encircling Mountains, just west of Dorthonion and east of the River Sirion, lay a round level plain with sheer walls on all sides and a ravine and tunnel leading out to the southwest known as the Hidden Way. In the middle of the vale there was a steep hill which was called Amon Gwareth, the "Hill of Watching". There Turgon decided to found a great city, designed after the city of Tirion in Valinor that the Noldor had left when they went into exile, that would be protected by the mountains and hidden from the Dark Lord Morgoth.
Turgon and his people built Gondolin in secret. After it was completed, he took with him to dwell in the hidden city his entire people in Nevrast — almost a third of the Noldor of Fingolfin's House — as well as nearly three quarters of the northern Sindar.
Hithlum.
Hithlum is the region north of Beleriand near the Helcaraxë. It was separated from Beleriand proper by the Ered Wethrin mountain chain, and was named after the sea mists which formed there at times: "Hithlum" is Sindarin for "Mist-shadow"; its Quenya counterpart is Hísilómë. Hithlum was subdivided in Mithrim, where the High Kings of the Noldor had their halls, and Dor-lómin, which later became a fief of the House of Hador. Hithlum was cold and rainy, but quite fertile. The Noldor landed here in the "Firth of Drengist" and first camped at the shores of Lake Mithrim.
The Ered Wethrin ("Mountains of Shadow") formed the southern and eastern wall, and had only a few passes; as such they formed a natural defensive line. The western wall was formed by the Ered Lómin or "Echoing Mountains", which curved north-westward to Helcaraxë. The land of Lammoth lay west of the Ered Lómin and was not part of Beleriand or Hithlum. The land of Nevrast was separated from Hithlum by the southern part of the Ered Lómin range. Nevrast was usually seen as part of Hithlum, but its climate was that of Beleriand.
Later in the First Age, Hithlum was continually under attack by Morgoth, finally being lost after the Nírnaeth Arnoediad. The Hadorians were scattered, killed, or enslaved, the Noldor were enslaved in Morgoth's mines if they could not flee in time, and Morgoth trapped the Easterlings there. Hithlum was completely destroyed during the War of Wrath.
March of Maedhros.
When the Sons of Fëanor went east after Thingol became aware of the Kinslaying at Alqualondë, a great fortress was built on the hill of Himring in northeast Beleriand. It was the chief stronghold of Maedhros, from which he guarded the northeastern border region that became known as the March of Maedhros. The Hill of Himring is described as "wide-shouldered, bare of trees, and flat upon its summit, surrounded by many lesser hills". To the east of them was Maglor's Gap and Ered Luin; to the west the Pass of Aglon, which Curufin and Celegorm guarded. In the wooded hills around Himring were the springs of the rivers Celon and Little Gelion.
The fortress upon Himring was the only in the East Beleriand to stand firm through the Dagor Bragollach, and many survivors from the surrounding regions, including Maedhros' brother Maglor, rallied there. But in the Battle of Unnumbered Tears the Sons of Fëanor were utterly defeated; the March of Maedhros was no more, and the Hill of Himring was garrisoned by the soldiers of Angband.
After the Drowning of Beleriand during the War of Wrath, the peak of Himring remained above the waves as the Isle of Himring.
Nargothrond.
Nargothrond (S. Narog-Ost-Rond, 'The great underground fortress on the river Narog'), called Nulukkhizdīn by the Dwarves, was the stronghold built by Finrod Felagund, delved into the banks of the river Narog in Beleriand, and the lands to the north (the Talath Dirnen or Guarded Plain) ruled by the city. Inspired by Menegroth in Doriath, and seeking a hidden place from which to be safe from the forces of Morgoth, Finrod established it in the early years of the First Age, in the Caverns of Narog beneath the forested hills of Taur-en-Faroth on the western bank of Narog. The original denizens of this huge cave system had been the Noegyth Nibin, the so-called 'Petty-dwarves', but whether they were driven out of their homes by Finrod's people, or earlier by the nearby Sindar, is not known.
Finrod ruled Nargothrond until he joined Beren in his quest for the Silmaril, and the regency passed to his nephew (or brother) Orodreth. Later, Túrin Turambar came to Nargothrond and became one of its greatest warriors, but he also persuaded the people to fight openly against Morgoth (the bridge was built at this time), which eventually led to its sack by the army of the dragon Glaurung. Glaurung then used Nargothrond as his lair until his death not long afterwards at Túrin's hands, after which the caves were claimed by Mîm, the last of the Petty-dwarves, until he himself was slain by Húrin, Túrin's father. After Húrin's deed, the caves were probably completely abandoned, as they fall out of recorded history in Tolkien's fiction, but they were certainly drowned and lost along with the rest of Beleriand at the end of the First Age.
Nevrast.
Nevrast is a coastal region in the north of Beleriand. The name means "Hither Shore" in Sindarin, as opposed to the "Distant Shore" of Aman and was originally applied to all the shores of Beleriand (although Nevrast was usually not included in West Beleriand). Nevrast was the centre of an Elven kingdom of Turgon for about a century until ca. 125 F.A., when the people began their trek to Gondolin. Turgon's capital (and it seems the only city of Nevrast) was Vinyamar. The land was not permanently inhabited after that, and indeed was utterly abandoned until Tuor came there, guided by Ulmo. Nevrast was the first part of Beleriand the Noldor settled, but was previously inhabited by Sindar. Nevrast's population was soon very mixed, far more so than any other region of Beleriand.
Ossiriand.
Ossiriand, or "Land of Seven Rivers", was the most eastern region of Beleriand during the First Age, lying between the Ered Luin and the river Gelion.
The Seven Rivers were, from north to south:
Along the northern shore of the Ascar ran the Dwarf-road to Nogrod. Ossiriand was a green and forested land, and it was not populated by the Sindar. In the early First Age before the rise of the Moon, a part of the Telerin Elven people called Nandor entered Ossiriand under their leader Denethor, and were given permission by Thingol to settle the lands. After them the land was often renamed Lindon, for "The Singers", after the old clan-name of the Telerin which the Nandor still used in their tongue. They became known as the Laiquendi, or Green Elves.
North of Ossiriand lay the land of Thargelion, and south of the river Adurant later lay the "Land of the Dead that Live", where Lúthien and Beren lived their second lives.
Ossiriand was the only part of Beleriand that survived the War of Wrath, but Belegaer the Great Sea broke through the mountain chain at the former riverbed of Ascar, creating the Gulf of Lhûn. In the Second Age and Third Age the former lands of Ossiriand and Thargelion were known as Lindon, where Gil-galad and Círdan ruled.
Dor Daedeloth.
Dor Daedeloth, or "Land of the Shadow of Dread", lay around the fortress of Angband and on both sides of the Ered Engrin. It was here that the Orcs and other creatures of Morgoth lived and bred. Dor Daedeloth lay north of Ard-galen, the great grassy plain north of Beleriand.
The march of the Noldor in early First Age was halted there, when Fëanor was mortally wounded by Balrogs. The Noldor then encircled the land (at least in the south), starting the Siege of Angband. 
History.
At the end of the First Age of Middle-earth , Beleriand was broken in the War of Wrath by the angelic beings, the Valar against the demonic Morgoth (himself a Vala fallen into evil). As the inhabitants of Beleriand, including masterless Orcs, beasts of Angband, Elves, Men and Dwarves, fled, much of Beleriand sank in the sea. Only a small section of East Beleriand remained, and was known after as Lindon, in the Northwest of Middle-earth of the Second and Third Age. Other parts of East Beleriand survived into the Second Age, but were completely destroyed along with the island kingdom of Númenor. One reference to a part that was not destroyed was made in "The Silmarillion" to the places of death of Túrin Turambar, Morwen and Nienor. Fulfilling a prophecy, the graves of Túrin Turambar and Morwen survived as the island Tol Morwen. Likewise, a part of Dorthonion became Tol Fuin, and Himring became an island.
Of note is what Galadriel said to Treebeard at their parting in Isengard, "Not in Middle-earth, nor until the lands that lie under the wave are lifted up again. Then in the willow-meads of Tasarinan we may meet in the spring. Farewell!" This seems to imply that Beleriand may be existent again, in Arda Healed.
Concept and creation.
Beleriand had many different names in Tolkien's early writings:

</doc>
<doc id="67013" url="https://en.wikipedia.org/wiki?curid=67013" title="King John (play)">
King John (play)

King John, a history play by William Shakespeare, dramatises the reign of John, King of England (ruled 1199–1216), son of Henry II of England and Eleanor of Aquitaine and father of Henry III of England. It is believed to have been written in the mid-1590s but was not published until it appeared in the First Folio in 1623.
Synopsis.
King John receives an ambassador from France, who demands, on pain of war, that he renounce his throne in favour of his nephew, Arthur, whom the French King, Philip, believes to be the rightful heir to the throne.
John adjudicates an inheritance dispute between Robert Falconbridge and his older brother Philip the Bastard, during which it becomes apparent that Philip is the illegitimate son of King Richard I. Queen Eleanor, mother to both Richard and John, recognises the family resemblance and suggests that he renounce his claim to the Falconbridge land in exchange for a knighthood. John knights the Bastard under the name Richard.
In France, King Philip and his forces besiege the English-ruled town of Angiers, threatening attack unless its citizens support Arthur. Philip is supported by Austria, who is believed to have killed King Richard. The English contingent arrives; Eleanor trades insults with Constance, Arthur's mother. Kings Philip and John stake their claims in front of Angiers' citizens, but to no avail: their representative says that they will support the rightful king, whoever that turns out to be.
The Bastard proposes that England and France unite to punish the rebellious citizens of Angiers, at which point they propose an alternative: Philip's son, Louis the Dauphin, should marry John's niece Blanche, a scheme that gives John a stronger claim to the throne, while Louis gains territory for France. Though a furious Constance accuses Philip of abandoning Arthur, Louis and Blanche are married.
Cardinal Pandolf arrives from Rome bearing a formal accusation that John has disobeyed the pope and appointed an archbishop contrary to his desires. John refuses to recant, whereupon he is excommunicated. Pandolf pledges his support for Louis, though Philip is hesitant, having just established family ties with John. Pandolf brings him round by pointing out that his links to the church are older and firmer.
War breaks out; Austria is beheaded by the Bastard in revenge for his father's death; and both Angiers and Arthur are captured by the English. Eleanor is left in charge of English possessions in France, while the Bastard is sent to collect funds from English monasteries. John orders Hubert to kill Arthur. Pandolf suggests to Louis that he now has as strong a claim to the English throne as Arthur (and indeed John), and Louis agrees to invade England.
Hubert finds himself unable to kill Arthur. John's nobles urge Arthur's release. John agrees, but is wrong-footed by Hubert's announcement that Arthur is dead. The nobles, believing he was murdered, defect to Louis' side. The Bastard reports that the monasteries are unhappy about John's attempt to seize their gold. Hubert has a furious argument with John, during which he reveals that Arthur is still alive. John, delighted, sends him to report the news to the nobles.
Arthur dies jumping from a castle wall. (It is open to interpretation whether he deliberately kills himself or just makes a risky escape attempt.) The nobles believe he was murdered by John, and refuse to believe Hubert's entreaties. John attempts to make a deal with Pandolf, swearing allegiance to the Pope in exchange for Pandolf's negotiating with the French on his behalf. John orders the Bastard, one of his few remaining loyal subjects, to lead the English army against France.
While John's former noblemen swear allegiance to Louis, Pandolf explains John's scheme, but Louis refuses to be taken in by it. The Bastard arrives with the English army and threatens Louis, but to no avail. War breaks out with substantial losses on each side, including Louis' reinforcements, who are drowned during the sea crossing. Many English nobles return to John's side after a dying French nobleman, Melun, warns them that Louis plans to kill them after his victory.
John is poisoned by a disgruntled monk. His nobles gather around him as he dies. The Bastard plans the final assault on Louis' forces, until he is told that Pandolf has arrived with a peace treaty. The English nobles swear allegiance to John's son Prince Henry, and the Bastard reflects that this episode has taught that internal bickering could be as perilous to England's fortunes as foreign invasion.
Sources.
King John is closely related to an anonymous history play, "The Troublesome Reign of King John" (c. 1589), the "masterly construction" but infelicitious expression of which led Peter Alexander to argue that Shakespeare's was the earlier play. Honigmann elaborated these arguments, both in his preface to the second Arden edition of King John, and in his 1982 monograph on Shakespeare's influence on his contemporaries. The majority view, however, first advanced in a rebuttal of Honigmann's views by Kenneth Muir, holds that the "Troublesome Reign" antedates King John by a period of several years; and that the skilful plotting of the Troublesome Reign is neither unparalleled in the period, nor proof of Shakespeare's involvement.
Shakespeare derived from "Holinshed's Chronicles" certain verbal collocations and points of action. Honigmann discerned in the play the influence of John Foxe's "Acts and Monuments", Matthew Paris' "Historia Maior", and the "Wakefield Chronicle", but Muir demonstrated that this apparent influence could be explained by the priority of the "Troublesome Reign", which contains similar or identical matter.
Date and text.
The date of composition is unknown, but must lie somewhere between 1587, the year of publication of the second, revised edition of "Holinshed's Chronicles", upon which Shakespeare drew for this and other plays, and 1598, when "King John" was mentioned among Shakespeare's plays in the "Palladis Tamia" of Francis Meres. The editors of the Oxford Shakespeare conclude from the play's incidence of rare vocabulary, use of colloquialisms in verse, pause patterns, and infrequent rhyming that the play was composed in 1596, after "Richard II" but before "Henry IV, Part I".
"King John" is one of only two plays by Shakespeare that are entirely written in verse, the other being "Richard II".
Performance history.
The earliest known performance took place in 1737, when John Rich staged a production at the Theatre Royal, Drury Lane. In 1745, the year of the Jacobite rebellion, competing productions were staged by Colley Cibber at Covent Garden and David Garrick at Drury Lane. Charles Kemble's 1823 production made a serious effort at historical accuracy, inaugurating the 19th century tradition of striving for historical accuracy in Shakespearean production. Other successful productions of the play were staged by William Charles Macready (1842) and Charles Kean (1846). Twentieth century revivals include Robert B. Mantell's 1915 production (the last production to be staged on Broadway) and Peter Brook's 1945 staging, featuring Paul Scofield as the Bastard.
In the Victorian era, "King John" was one of Shakespeare's most frequently staged plays, in part because its spectacle and pageantry were congenial to Victorian audiences. "King John," however, has decreased in popularity: it is now one of Shakespeare's least-known plays and stagings of it are very rare. It has been staged four times on Broadway, the last time in 1915. It has also been staged five times from 1953 to 2014 at the Stratford Shakespeare Festival.
Herbert Beerbohm Tree made a silent film version in 1899 entitled "King John". It is a short film consisting of the King's death throes in Act V, Scene vii and is the earliest surviving film adaptation of a Shakespearean play. "King John" has been produced for television twice: in 1951 with Donald Wolfit and in 1984 with Leonard Rossiter as part of the BBC Television Shakespeare series of adaptations.
George Orwell specifically praised it in 1942 for its view of politics: "When I had read it as a boy it seemed to me archaic, something dug out of a history book and not having anything to do with our own time. Well, when I saw it acted, what with its intrigues and doublecrossings, non-aggression pacts, quislings, people changing sides in the middle of a battle, and what-not, it seemed to me extraordinarily up to date."
Selected modern revivals.
The Royal Shakespeare Company based in Stratford-upon-Avon presented two productions of "King John": in 2006 directed by Josie Rourke as part of their Complete Works Festival and in 2012 directed by Maria Aberg who cast a woman, Pippa Nixon, in the role of the Bastard. The Company's 1974–5 production was heavily rewritten by director John Barton, who included material from "The Troublesome Reign of King John," John Bale "King Johan" (thought to be Shakespeare's own sources) and other works.
In 2008, the Hudson Shakespeare Company of New Jersey produced "King John" as part of their annual Shakespeare in the Parks series. Director Tony White set the action in the medieval era but used a multi-ethnic and gender swapping cast. The roles of Constance and Dauphin Lewis were portrayed by African American actors Tzena Nicole Egblomasse and Jessie Steward and actresses Sharon Pinches and Allison Johnson were used in several male roles. Another notable departure for the production is the depiction of King John himself. Often portrayed as an ineffectual king, actor Jimmy Pravasilis portrayed a headstrong monarch sticking to his guns on his right to rule and his unwillingness to compromise became the result of his downfall.
New York's Theater for a New Audience presented a "remarkable" in-the-round production in 2000, emphasising Faulconbridge's introduction to court realpolitik to develop the audience's own awareness of the characters' motives. The director was Karin Coonrod.
In 2012, Bard on the Beach in Vancouver, British Columbia put on a production. It was also performed as part of the 2013 season at the Utah Shakespeare Festival, recipient of America's Outstanding Regional Theatre Tony Award (2000), presented by the American Theatre Wing and the League of American Theatres and Producers.
The play was presented at Shakespeare's Globe, directed by James Dacre, as part of the summer season 2015, the 800th anniversary year of Magna Carta.

</doc>
<doc id="67015" url="https://en.wikipedia.org/wiki?curid=67015" title="Love's Labour's Lost">
Love's Labour's Lost

"Love's Labour's Lost" is one of William Shakespeare's early comedies, believed to have been written in the mid-1590s for a performance at the Inns of Court before Queen Elizabeth I. It follows the King of Navarre and his three companions as they attempt to forswear the company of women for three years of study and fasting, and their subsequent infatuation with the Princess of Aquitaine and her ladies. In an untraditional ending for a comedy, the play closes with the death of the Princess's father, and all weddings are delayed for a year. The play draws on themes of masculine love and desire, reckoning and rationalization, and reality versus fantasy.
Though first published in quarto in 1598, the play's title page suggests a revision of an earlier version of the play. While there are no obvious sources for the play's plot, the four main characters are loosely based on historical figures. The use of apostrophes in the play's title varies in early editions, though it is most commonly given as "Love's Labour's Lost".
The historical personages portrayed and the political situation in Europe relating to the setting and action of the play were familiar to Shakespeare's audiences. Scholars suggest that the play lost popularity as these historical and political portrayals of Navarre's court became dated and less accessible to theatergoers of later generations. The play's sophisticated wordplay, pedantic humour and dated literary allusions may also be reasons for its relative obscurity, as compared with Shakespeare's more popular works. "Love's Labour's Lost" was staged rarely in the 19th century, but it has been seen more often in the 20th and 21st centuries, with productions by both the Royal Shakespeare Company and the National Theatre, among others. It has also been adapted as a musical, an opera, for radio and television and as a musical film.
"Love's Labour's Lost" features the longest scene (5.2), the longest single word 'honorificabilitudinitatibus' (5.1.39–40), and (depending on editorial choices) the longest speech (4.2.284–361) in all of Shakespeare's plays (see "Date and Text" below).
Synopsis.
Ferdinand, King of Navarre, and his three noble companions, the lords Berowne, Dumaine, and Longaville, take an oath not to give in to the company of women. They devote themselves to three years of study and fasting; Berowne agrees somewhat more hesitantly than the others. The King declares that no woman should come within a mile of the court. Don Adriano de Armado, a Spaniard visiting the court, comes to tell the King of a tryst between Costard and Jaquenetta. After the King sentences Costard, Don Armado confesses his own love for Jaquenetta to his page, Moth. Don Armado writes Jaquenetta a letter and asks Costard to deliver it.
The Princess of France and her ladies arrive, wishing to speak to the King regarding the cession of Aquitaine, but must ultimately make their camp outside the court due to the decree. In visiting the Princess and her ladies at their camp, the King falls in love with the Princess, as do the lords with the ladies. Berowne gives Costard a letter to deliver to the lady Rosaline, which Costard switches with Don Armado's letter that was meant for Jaquenetta. Jaquenetta consults two scholars, Holofernes and Sir Nathaniel, who conclude that the letter is written by Berowne and instruct her to tell the King.
The King and his lords lie in hiding and watch one another as each subsequently reveals their feelings of love. The King ultimately chastises the lords for breaking the oath, but Berowne reveals that the King is likewise in love with the Princess. Jaquenetta and Costard enter with Berowne's letter and accuse him of treason. Berowne confesses to breaking the oath, explaining that the only study worthy of mankind is that of love, and he and the other men collectively decide to relinquish the vow. Arranging for Holofernes to entertain the ladies later, the men then dress as Muscovites and court the ladies in disguise. Boyet, having overheard their planning, helps the ladies trick the men by disguising themselves as each other. When the lords return as themselves, the ladies taunt them and expose their ruse.
The men apologize, and when all identities are righted, they watch Holofernes, Sir Nathaniel, Costard, Moth, and Don Armado present the Nine Worthies. The four lords – as well as the ladies' courtier Boyet – heckle the play, and Don Armado and Costard almost come to blows when Costard reveals mid-pageant that Don Armado has got Jaquenetta pregnant. Their spat is interrupted by news that the Princess's father has died. The Princess makes plans to leave at once, and she and her ladies, readying for mourning, declare that the men must wait a year and a day to prove their loves lasting. Don Armado announces he will swear a similar oath to Jaquenetta and then presents the nobles with a song.
Sources.
"Love's Labour's Lost" is, along with Shakespeare's "The Tempest", a play without any obvious sources. Some possible influences on "Love's Labour's Lost" can be found in the early plays of John Lyly, Robert Wilson's "The Cobbler's Prophecy" (c.1590) and Pierre de la Primaudaye's "L'Academie française" (1577). Michael Dobson and Stanley Wells comment that it has often been conjectured that the plot derives from "a now lost account of a diplomatic visit made to Henry in 1578 by Catherine de Medicis and her daughter Marguerite de Valois, Henry's estranged wife, to discuss the future of Aquitaine, but this is by no means certain."
The four main male characters are all loosely based on historical figures; Navarre is based on Henry of Navarre (who later became King Henry IV of France), Berowne on Charles de Gontaut, duc de Biron, Dumain on Charles, duc de Mayenne and Longaville on Henri I d'Orléans, duc de Longueville. Biron in particular was well known in England because Robert Devereux, 2nd Earl of Essex, had joined forces with Biron's army in support of Henry in 1591. Albert Tricomi states that "the play's humorous idealization could remain durable as long as the French names of its principal characters remained familiar to Shakespeare's audiences. This means that the witty portrayal of Navarre's court could remain reasonably effective until the assassination of Henry IV in 1610. ... Such considerations suggest that the portrayals of Navarre and the civil-war generals presented Elizabethan audiences not with a mere collection of French names in the news, but with an added dramatic dimension which, once lost, helps to account for the eclipse "Love's Labour's Lost" soon underwent."
Critics have attempted to draw connections between notable Elizabethan English persons and the characters of Don Armado, Moth, Sir Nathaniel, and Holofernes, with little success.
Date and text.
Most modern scholars believe the play was written in 1595 or 1596, making it contemporaneous with "Romeo and Juliet" and "A Midsummer Night's Dream". "Love's Labour's Lost" was first published in quarto in 1598 by the bookseller Cuthbert Burby. The title page states that the play was "Newly corrected and augmented by W. Shakespere," which has suggested to some scholars a revision of an earlier version. The play next appeared in print in the First Folio in 1623, with a later quarto in 1631. "Love's Labour's Won" is considered by some to be a lost sequel.
"Love’s Labour’s Lost" features the longest scene in all of Shakespeare’s plays (5.2), which, depending upon formatting and editorial decisions, ranges from around 920 lines to just over 1000 lines. The First Folio records the scene at 942 lines.
The play also features the single longest word in all of Shakespeare's plays: "honorificabilitudinitatibus", spoken by Costard at 5.1.30.
The speech given by Berowne at 4.3.284–361 is potentially the longest in all of Shakespeare's plays, depending on editorial choices. Shakespeare critic and editor Edward Capell has pointed out that certain passages within the speech seem to be redundant and argues that these passages represent a first draft which was not adequately corrected before going to print. Specifically, lines 291–313 are “repeated in substance” further in the speech and are sometimes omitted by editors. With no omissions, the speech is 77 lines and 588 words.
Analysis and criticism.
Title.
The title is normally given as "Love's Labour's Lost". The use of apostrophes varies in early editions. In its first 1598 quarto publication it appears as "Loues labors loſt". In the 1623 First Folio it is "Loues Labour's Lost" and in the 1631 edition it is "Loues Labours Lost". In the Third Folio it appears for the first time with the modern punctuation and spelling as "Love's Labour's Lost". Critic John Hale wrote that the title could be read as "love's labour is lost" or "the lost labours of love" depending on punctuation. Hale suggests that the witty alliteration of the title is in keeping with the pedantic nature of the play. In 1935 Frances Yates asserted that the title derived from a line in John Florio's "His firste Fruites" (1578): "We neede not speak so much of loue, al books are ful of lou, with so many authours, that it were labour lost to speake of Loue", a source from which Shakespeare also took the untranslated Venetian proverb "Venetia, Venetia/Chi non ti vede non ti pretia" (LLL 4.2.92–93) ("Venice, Venice, Who does not see you cannot praise you").
Reputation.
"Love's Labour's Lost" abounds in sophisticated wordplay, puns, and literary allusions and is filled with clever pastiches of contemporary poetic forms. Critic and historian John Pendergast states that "perhaps more than any other Shakespearean play, it explores the power and limitations of language, and this blatant concern for language led many early critics to believe that it was the work of a playwright just learning his art." It is often assumed that the play was written for performance at the Inns of Court, whose students would have been most likely to appreciate its style. It has never been among Shakespeare's most popular plays, probably because its pedantic humour and linguistic density are extremely demanding of contemporary theatregoers. The satirical allusions of Navarre's court are likewise inaccessible, "having been principally directed to fashions of language that have long passed away, and consequently little understood, rather than in any great deficiency of invention."
Themes.
Masculine desire.
Masculine desire structures the play and helps to shape its action. The men's sexual appetite manifests in their desire for fame and honour; the notion of women as dangerous to masculinity and intellect is established early on. The King and his Lords' desires for their idealized women are deferred, confused, and ridiculed throughout the play. As the play comes to a close, their desire is deferred yet again, resulting in an increased exaltation of the women.
Critic Mark Breitenberg commented that the use of idealistic poetry, popularized by Petrarch, effectively becomes the textualized form of the male gaze. In describing and idealizing the ladies, the King and his Lords exercise a form of control over women they love. Don Armado also represents masculine desire through his relentless pursuit of Jacquenetta. The theme of desire is heightened by the concern of increasing female sexuality throughout the Renaissance period and the subsequent threat of cuckoldry. Politics of love, marriage, and power are equally forceful in shaping the thread of masculine desire that drives the plot.
Reckoning and rationalization.
The term 'reckoning' is used in its multiple meanings throughout the Shakespeare canon. In "Love's Labour's Lost" in particular, it is often used to signify a moral judgement; most notably, the idea of a final reckoning as it relates to death. Though the play entwines fantasy and reality, the arrival of the messenger to announce the death of the Princess's father ultimately brings this notion to a head. Scholar Cynthia Lewis suggested that the appearance of the final reckoning is necessary in reminding the lovers of the seriousness of marriage. The need to settle the disagreement between Navarre and France likewise suggests an instance of reckoning, though this particular reckoning is settled offstage. This is presented in stark contrast to the final scene, in which the act of reckoning cannot be avoided. In acknowledging the consequences of his actions, Don Armado is the only one to deal with his reckoning in a noble manner. The Lords and the King effectively pass judgement on themselves, revealing their true moral character when mocking the players during the representation of the Nine Worthies.
Similar to reckoning is the notion of rationalization, which provides the basis for the swift change in the ladies' feelings for the men. The ladies are able to talk themselves into falling in love with the men due to the rationalization of the men's purported flaws. Lewis concluded that "the proclivity to rationalize a position, a like, or a dislike, is linked in "Love's Labour's Lost" with the difficulty of reckoning absolute value, whose slipperiness is indicated throughout the play."
Reality versus fantasy.
Critic Joseph Westlund wrote that "Love's Labour's Lost" functions as a "prelude to the more extensive commentary on imagination in "A Midsummer Night's Dream"." There are several plot points driven by fantasy and imagination throughout the play. The Lords and the King's declaration of abstinence is a fancy that falls short of achievement. This fantasy rests on the men's idea that the resulting fame will allow them to circumvent death and oblivion, a fantastical notion itself. Within moments of swearing their oath, it becomes clear that their fantastical goal is unachievable given the reality of the world, the unnatural state of abstinence itself, and the arrival of the Princess and her ladies. This juxtaposition ultimately lends itself to the irony and humour in the play.
The commoners represent the theme of reality and achievement versus fantasy via their production regarding the Nine Worthies. Like the men's fantastical pursuit of fame, the play within a play represents the commoners' concern with fame. The relationship between the fantasy of love and the reality of worthwhile achievement, a popular Renaissance topic, is also utilized throughout the play. Don Armado attempts to reconcile these opposite desires using Worthies who fell in love as model examples. Time is suspended throughout the play and is of little substance to the plot. The Princess, though originally "craving quick dispatch," quickly falls under the spell of love and abandons her urgent business. This suggests that the majority of the action takes place within a fantasy world. Only with the news of the Princess's father's death are time and reality reawakened.
Music.
Unlike many of Shakespeare’s plays, music plays a role only in the final scene of "Love's Labour's Lost". The songs of spring and winter, titled "Ver and Hiems" and "The Cuckoo and the Owl", respectively, occur near the end of the play. Given the critical controversy regarding the exact dating of "Love's Labour's Lost", there is some indication that "the songs belong to the 1597 additions."
Different interpretations of the meaning of these songs include: optimistic commentary for the future, bleak commentary regarding the recent announcement of death, or an ironic device by which to direct the King and his Lords towards a new outlook on love and life. In keeping with the theme of time as it relates to reality and fantasy, these are seasonal songs that restore the sense of time to the play. Due to the opposing nature of the two songs, they can be viewed as a debate on the opposing attitudes on love found throughout the play. Catherine McLay comments that the songs are functional in their interpretation of the central themes in "Love's Labour's Lost". McLay also suggests that the songs negate what many consider to be a "heretical" ending for a comedy. The songs, a product of traditional comedic structure, are a method by which the play can be " within the periphery of the usual comic definition."
Critic Thomas Berger states that, regardless of the meaning of these final songs, they are important in their contrast with the lack of song throughout the rest of the play. In cutting themselves off from women and the possibility of love, the King and his Lords have effectively cut themselves off from song. Song is allowed into the world of the play at the beginning of Act III, after the Princess and her ladies have been introduced and the men begin to fall in love. Moth’s song "Concolinel" indicates that the vows will be broken. In Act I, Scene II, Moth recites a poem but fails to sing it. Don Armado insists that Moth sing it twice, but he does not. Berger infers that a song was intended to be inserted at this point, but was never written. Had a song been inserted at this point of the play, it would have followed dramatic convention of the time, which often called for music between scenes.
Performance history.
The earliest recorded performance of the play occurred at Christmas in 1597 at the Court before Queen Elizabeth. A second performance is recorded to have occurred in 1605, either at the house of the Earl of Southampton or at that of Robert Cecil, Lord Cranborne. The first known production after Shakespeare's era was not until 1839, at the Theatre Royal, Covent Garden, with Madame Vestris as Rosaline. "The Times" was unimpressed, stating: "The play moved very heavily. The whole dialogue is but a string of brilliant conceits, which, if not delivered well, are tedious and unintelligible. The manner in which it was played last night destroyed the brilliancy completely, and left a residuum of insipidity which was encumbered rather than relieved by the scenery and decorations." The only other performances of the play recorded in England in the 19th century were at Sadler's Wells in 1857 and the St. James's Theatre in 1886.
Notable 20th-century British productions included a 1936 staging at the Old Vic featuring Michael Redgrave as Ferdinand and Alec Clunes as Berowne. In 1949, the play was given at the New Theatre with Redgrave in the role of Berowne. The cast of a 1965 Royal Shakespeare Company production included Glenda Jackson, Janet Suzman and Timothy West. In 1968, the play was staged by Laurence Olivier for the National Theatre, with Derek Jacobi as the Duke and Jeremy Brett as Berowne. The Royal Shakespeare Company produced the play again in 1994. The critic Michael Billington wrote in his review of the production: "The more I see "Love's Labour's Lost", the more I think it Shakespeare's most beguiling comedy. It both celebrates and satisfies linguistic exuberance, explores the often painful transition from youth to maturity, and reminds us of our common mortality."
In late summer 2005, an adaptation of the play was staged in the Dari language in Kabul, Afghanistan by a group of Afghan actors, and was reportedly very well received.
A 2009 staging by Shakespeare's Globe theatre, with artistic direction by Dominic Dromgoole, toured internationally. Ben Brantley, in "The New York Times", called the production, seen at Pace University, "sophomoric". He postulated that the play itself "may well be the first and best example of a genre that would flourish in less sophisticated forms five centuries later: the college comedy."
In 2014, the Royal Shakespeare Company completed a double-feature in which "Love's Labour's Lost", set on the eve of the First World War, is followed by "Much Ado About Nothing" (re-titled "Love's Labour's Won"). Dominic Cavendish of the "Telegraph" called it "the most blissfully entertaining and emotionally involving RSC offering I’ve seen in ages" and remarked that "Parallels between the two works – the sparring wit, the sex-war skirmishes, the shift from showy linguistic evasion to heart-felt earnestness – become persuasively apparent."
Adaptations.
Literature.
Alfred Tennyson's poem "The Princess" (and, by extension, Gilbert and Sullivan's comic opera "Princess Ida") is speculated by Gerhard Joseph to have been inspired by "Love's Labour's Lost".
Thomas Mann in his novel "Doctor Faustus" (1943) has the fictional German composer Adrian Leverkühn attempt to write an opera on the story of the play.
Musical theatre, opera and plays.
An opera of the same title as the play was composed by Nicolas Nabokov, with a libretto by W. H. Auden and Chester Kallman, and first performed in 1973.
In the summer of 2013, The Public Theater in New York City presented a musical adaptation of the play as part of their Shakespeare in the Park programming. This production marked the first new Shakespeare-based musical to be produced at the Delacorte Theater in Central Park since the 1971 mounting of "The Two Gentlemen of Verona" with music by Galt MacDermot. The adaptation of "Love's Labour's Lost" featured a score by "Bloody Bloody Andrew Jackson" collaborators Michael Friedman and Alex Timbers. Timbers also directed the production, which starred Daniel Breaker, Colin Donnell, Rachel Dratch, and Patti Murin, among others. It is being licensed for future productions by Music Theatre International. 
Marc Palmieri's 2015 play "The Groundling", a farce the NY Times referred to as "half comedy and half tragedy" was billed as a "meditation on the meaning of the final moments of "Love's Labour's Lost"".
Film, television and radio.
Kenneth Branagh's 2000 film adaptation relocated the setting to the 1930s and attempted to make the play more accessible by turning it into a musical. The film was a box office disappointment.
The play was one of the last works to be recorded for the BBC Television Shakespeare project, broadcast in 1985. The production set events in the eighteenth century, the costumes and sets being modelled on the paintings of Watteau. This was the only instance in the project of a work set in a period after Shakespeare's death. The play is featured in an episode of the British TV show, "Doctor Who". The episode, entitled "The Shakespeare Code" focuses on Shakespeare himself and a hypothetical follow-up play, "Love's Labour's Won", whose final scene is used as a portal for alien witches to invade Earth. All copies of this play disappear along with the witches.
BBC Radio 3 aired a radio adaptation on 16 December 1946, directed by Noel Illif, with music by Gerald Finzi scored for a small chamber orchestra. The cast included Paul Scofield. The music was subsequently converted into an orchestral suite. BBC Radio 3 aired another radio adaptation on 22 February 1979, directed by David Spenser, with music by Derek Oldfield. The cast included Michael Kitchen as Ferdinand; John McEnery as Berowne; Anna Massey as the Princess of France; Eileen Atkins as Rosaline; and Paul Scofield as Don Adriano.
Two independent filmmakers in Austin, Texas are currently in post-production for a new film adaptation of Love's Labour's Lost, set in a modern-day boarding school.
From 16 July 2015, a vlog adaptation titled "Lovely Little Losers" airs on YouTube, created by The Candle Wasters.

</doc>
<doc id="67017" url="https://en.wikipedia.org/wiki?curid=67017" title="Saigon (disambiguation)">
Saigon (disambiguation)

Saigon is the former name of Ho Chi Minh City.
Saigon may also refer to:

</doc>
<doc id="67018" url="https://en.wikipedia.org/wiki?curid=67018" title="Eugene O'Neill">
Eugene O'Neill

Eugene Gladstone O'Neill (October 16, 1888 – November 27, 1953) was an American playwright and Nobel laureate in Literature. His poetically titled plays were among the first to introduce into American drama techniques of realism earlier associated with Russian playwright Anton Chekhov, Norwegian playwright Henrik Ibsen, and Swedish playwright August Strindberg. The drama "Long Day's Journey Into Night" is often numbered on the short list of being among the finest American plays in the 20th century, alongside Tennessee Williams' "A Streetcar Named Desire" and Arthur Miller's "Death of a Salesman".
O'Neill's plays were among the first to include speeches in American vernacular and involve characters on the fringes of society. They struggle to maintain their hopes and aspirations, but ultimately slide into disillusionment and despair. Of his very few comedies, only one is well-known ("Ah, Wilderness!"). Nearly all of his other plays involve some degree of tragedy and personal pessimism.
Early life.
O'Neill was born in a hotel, the Barrett House, at Broadway and 43rd Street, on what was then Longacre Square (now Times Square). A commemorative plaque was first dedicated there in 1957. The site is now occupied by 1500 Broadway, which houses offices, retail, and ABC Studios.
He was the son of Irish immigrant actor James O'Neill and Mary Ellen Quinlan, who was also of Irish descent. Because his father was often on tour with a theatrical company, accompanied by Eugene's mother, O'Neill was sent to St. Aloysius Academy for Boys, a Catholic boarding school in the Riverdale section of the Bronx, where he found his only solace in books. His father suffered from alcoholism; his mother from an addiction to morphine, prescribed to relieve the pains of the difficult birth of her third son, Eugene.
O'Neill spent his summers at the Monte Cristo Cottage in New London, Connecticut. He attended Princeton University for one year. Accounts vary as to why he left. He may have been dropped for attending too few classes, been suspended for "conduct code violations," or "for breaking a window", or according to a more concrete but possibly apocryphal account, because he threw "a beer bottle into the window of Professor Woodrow Wilson", the future president of the United States.
O'Neill spent several years at sea, during which he suffered from depression and alcoholism. Despite this, he had a deep love for the sea and it became a prominent theme in many of his plays, several of which are set on board ships like those on which he worked. O'Neill joined the Marine Transport Workers Union of the Industrial Workers of the World (IWW), which was fighting for improved living conditions for the working class using quick 'on the job' direct action. O'Neill's parents and elder brother Jamie (who drank himself to death at the age of 45) died within three years of one another, not long after he had begun to make his mark in the theater.
Career.
After his experience in 1912–13 at a sanatorium where he was recovering from tuberculosis, he decided to devote himself full-time to writing plays (the events immediately prior to going to the sanatorium are dramatized in his masterpiece, "Long Day's Journey into Night"). O'Neill had previously been employed by the "New London Telegraph", writing poetry as well as reporting.
In the fall of 1914, he entered Harvard University to attend a course in dramatic technique given by Professor George Baker. He left after one year and did not complete the course.
During the 1910s O'Neill was a regular on the Greenwich Village literary scene, where he also befriended many radicals, most notably Communist Labor Party of America founder John Reed. O'Neill also had a brief romantic relationship with Reed's wife, writer Louise Bryant. O'Neill was portrayed by Jack Nicholson in the 1981 film "Reds", about the life of John Reed.
His involvement with the Provincetown Players began in mid-1916. O'Neill is said to have arrived for the summer in Provincetown with "a trunk full of plays." Susan Glaspell describes what was probably the first ever reading of "Bound East for Cardiff" which took place in the living room of Glaspell and her husband George Cram Cook's home on Commercial Street, adjacent to the wharf (pictured) that was used by the Players for their theater. Glaspell writes in "The Road to the Temple", "So Gene took "Bound East for Cardiff" out of his trunk, and Freddie Burt read it to us, Gene staying out in the dining-room while reading went on. He was not left alone in the dining-room when the reading had finished." The Provincetown Players performed many of O'Neill's early works in their theaters both in Provincetown and on MacDougal Street in Greenwich Village. Some of these early plays began downtown and then moved to Broadway.
O'Neill's first published play, "Beyond the Horizon", opened on Broadway in 1920 to great acclaim, and was awarded the Pulitzer Prize for Drama. His first major hit was "The Emperor Jones", which ran on Broadway in 1920 and obliquely commented on the U.S. occupation of Haiti that was a topic of debate in that year's presidential election. His best-known plays include "Anna Christie" (Pulitzer Prize 1922), "Desire Under the Elms" (1924), "Strange Interlude" (Pulitzer Prize 1928), "Mourning Becomes Electra" (1931), and his only well-known comedy, "Ah, Wilderness!", a wistful re-imagining of his youth as he wished it had been. In 1936 he received the Nobel Prize for Literature after he had been nominated that year by Henrik Schück, member of the Swedish Academy. After a ten-year pause, O'Neill's now-renowned play "The Iceman Cometh" was produced in 1946. The following year's "A Moon for the Misbegotten" failed, and it was decades before coming to be considered as among his best works.
He was also part of the modern movement to partially revive the classical heroic mask from ancient Greek theatre and Japanese Noh theatre in some of his plays, such as "The Great God Brown" and "Lazarus Laughed."
Family life.
O'Neill was married to Kathleen Jenkins from October 2, 1909 to 1912, during which time they had one son, Eugene O'Neill, Jr. (1910–1950). In 1917, O'Neill met Agnes Boulton, a successful writer of commercial fiction, and they married on April 12, 1918. They lived in a home owned by her parents in Point Pleasant, New Jersey after their marriage. The years of their marriage—during which the couple lived in Connecticut and Bermuda and had two children, Shane and Oona—are described vividly in her 1958 memoir "Part of a Long Story". They divorced in 1929, after O'Neill abandoned Boulton and the children for the actress Carlotta Monterey (born San Francisco, California, December 28, 1888; died Westwood, New Jersey, November 18, 1970). O'Neill and Carlotta married less than a month after he officially divorced his previous wife.
In 1929, O'Neill and Monterey moved to the Loire Valley in central France, where they lived in the Château du Plessis in Saint-Antoine-du-Rocher, Indre-et-Loire. During the early 1930s they returned to the United States and lived in Sea Island, Georgia, at a house called "Casa Genotta". He moved to Danville, California in 1937 and lived there until 1944. His house there, "Tao House", is today the Eugene O'Neill National Historic Site.
In their first years together, Monterey organized O'Neill's life, enabling him to devote himself to writing. She later became addicted to potassium bromide, and the marriage deteriorated, resulting in a number of separations, although they never divorced.
In 1943, O'Neill disowned his daughter Oona for marrying the English actor, director and producer Charlie Chaplin when she was 18 and Chaplin was 54. He never saw Oona again.
He also had distant relationships with his sons. Eugene O'Neill, Jr., a Yale classicist, suffered from alcoholism and committed suicide in 1950 at the age of 40. Shane O'Neill became a heroin addict and moved into the family home in Bermuda, "Spithead," with his new wife, where he supported himself by selling off the furnishings. He was disowned by his father before also committing suicide (by jumping out of a window) a number of years later. Oona ultimately inherited Spithead and the connected estate (subsequently known as the Chaplin Estate). In 1950 O'Neill joined The Lambs, the famed theater club.
Illness and death.
After suffering from multiple health problems (including depression and alcoholism) over many years, O'Neill ultimately faced a severe Parkinsons-like tremor in his hands which made it impossible for him to write during the last 10 years of his life; he had tried using dictation but found himself unable to compose in that way. While at Tao House, O’Neill had intended to write a cycle of 11 plays chronicling an American family since the 1800s. Only two of these, "A Touch of the Poet" and "More Stately Mansions" were ever completed. As his health worsened, O’Neill lost inspiration for the project and wrote three largely autobiographical plays, "The Iceman Cometh", "Long Day's Journey Into Night", and "A Moon for the Misbegotten". He managed to complete "Moon for the Misbegotten" in 1943, just before leaving Tao House and losing his ability to write. Drafts of many other uncompleted plays were destroyed by Carlotta at Eugene’s request.
O'Neill died in Room 401 of the Sheraton Hotel on Bay State Road in Boston, on November 27, 1953, at the age of 65. As he was dying, he whispered his last words: "I knew it. I knew it. Born in a hotel room and died in a hotel room." (The building later became the Shelton Hall dormitory at Boston University. There is an urban legend perpetuated by students that O'Neill's spirit haunts the room and dormitory.) A revised analysis of his autopsy report shows that, contrary to the previous diagnosis, he did not have Parkinson's disease, but a late-onset cerebellar cortical atrophy.
Dr. Harry Kozol, the lead prosecuting expert of the Patty Hearst trial, treated O'Neill during these last years of illness. He also was present for O'Neill's death and announced the fact to the public.
O'Neill is interred in the Forest Hills Cemetery in Boston's Jamaica Plain neighborhood.
In 1956 Carlotta arranged for his autobiographical masterpiece "Long Day's Journey Into Night" to be published, although his written instructions had stipulated that it not be made public until 25 years after his death. It was produced on stage to tremendous critical acclaim and won the Pulitzer Prize in 1957. This last play is widely considered to be his finest. Other posthumously-published works include "A Touch of the Poet" (1958) and "More Stately Mansions" (1967).
The United States Postal Service honored O'Neill with a Prominent Americans series (1965–1978) $1 postage stamp.
Influence on African American actors.
O’Neill had major influence on African American actors, in particular Paul Leroy Robeson. O’Neill and Robeson worked on three productions together: "All God's Chillun Got Wings" (1924), "The Emperor Jones" (1924), and "The Hairy Ape" (1931). O’Neill's "The Emperor Jones" broke social barriers and defied conventions of the day as the first American play to feature an African-American central character portrayed in a serious manner.
Legacy.
In Warren Beatty's 1981 film "Reds", O'Neill Is portrayed by Jack Nicholson, who was nominated for the Academy Award for Best Supporting Actor for his performance.
George C. White founded the Eugene O'Neill Theatre Center in Waterford, Connecticut in 1964.
Eugene O'Neill is a member of the American Theater Hall of Fame.
O'Neill is mentioned by Tony Stark in "" (2015), referencing the long running times of his works.
Museums and collections.
O'Neill's home in New London, Monte Cristo Cottage, was made a National Historic Landmark in 1971. His home in Danville, California, near San Francisco, was preserved as the Eugene O'Neill National Historic Site in 1976.
Connecticut College maintains the Louis Sheaffer Collection, consisting of material collected by the O'Neill biographer. The principal collection of O'Neill papers is at Yale University. The Eugene O'Neill Theater Center in Waterford, Connecticut fosters the development of new plays under his name.
There is also a Theatre in New York City named after him located at 230 West 49th Street in midtown-Manhattan. The Eugene O'Neill Theatre has housed musicals and plays such as "Yentl", "Annie", "Grease", "Spring Awakening", and "The Book of Mormon".
Work.
One-act plays.
The Glencairn Plays, all of which feature characters on the fictional ship "Glencairn"—filmed together as "The Long Voyage Home":
Other one-act plays include:

</doc>
<doc id="67019" url="https://en.wikipedia.org/wiki?curid=67019" title="SSM-N-8 Regulus">
SSM-N-8 Regulus

The SSM-N-8A Regulus was a ship- and submarine-launched, nuclear-armed turbojet-powered cruise missile deployed by the United States Navy from 1955 to 1964. Its barrel-shaped fuselage resembled that of numerous fighter aircraft designs of the era, but without a cockpit. When the missile was ready for launch, it was fitted with two large booster rockets on the aft end of the fuselage.
History.
Design and development.
In October 1943, Chance Vought Aircraft Company signed a study contract for a range missile to carry a warhead. The project stalled for four years, however, until May 1947, when the United States Army Air Forces awarded Martin Aircraft Company a contract for a turbojet powered subsonic missile, the Matador. The Navy saw Matador as a threat to its role in guided missiles and, within days, started a Navy development program for a missile that could be launched from a submarine and use the same J33 engine as the Matador. In August 1947, the specifications for the project, now named "Regulus," were issued: Carry a warhead, to a range of , at Mach 0.85, with a circular error probable (CEP) of 0.5% of the range. At its extreme range the missile had to hit within of its target 50% of the time.
Regulus development was preceded by Navy experiments with the JB-2 Loon missile, a close derivative of the German V-1 flying bomb, beginning in the last year of World War II. Submarine testing was performed from 1947 to 1953, with and converted as test platforms, initially carrying the missile unprotected, thus unable to submerge until after launch.
Regulus was designed to be long, in wingspan, in diameter, and would weigh between . After launch, it would be guided toward its target by two control stations, usually submarines with guidance equipment. (Later, with the "Trounce" system (Tactical Radar Omnidirectional Underwater Navigational Control Equipment), one submarine could guide it). Army-Navy competition complicated both the Matador's and the Regulus' developments. The missiles looked alike and used the same engine. They had nearly identical performances, schedules, and costs. Under pressure to reduce defense spending, the United States Department of Defense ordered the Navy to determine if Matador could be adapted for their use. The Navy concluded that the Navy's Regulus could perform the Navy mission better.
Regulus had some advantages over Matador. It required only two guidance stations while Matador required three. It could also be launched quicker, as Matador's boosters had to be fitted while the missile was on the launcher while Regulus was stowed with its boosters attached. Finally, Chance Vought built a recoverable version of the missile, so that even though a Regulus test vehicle was more expensive to build, Regulus was cheaper to use over a series of tests. The Navy program continued, and the first Regulus flew in March 1951.
Due to its size and regulations concerning oversize loads on highways, Chance Vought collaborated with a firm that specialized in trucking oversize loads to develop a special tractor trailer combination which could move a Regulus I missile.
Regulus II.
A second generation supersonic Vought SSM-N-9 Regulus II cruise missile with a range of and a speed of Mach 2 was developed and successfully tested, but the program was canceled in favor of the UGM-27 Polaris nuclear ballistic missile.
The Regulus II missile was a completely new design with improved guidance and double the range, and was intended to replace the Regulus I missile. Regulus II-equipped submarines and ships would have been fitted with the Ships Inertial Navigation System (SINS), allowing the missiles to be aligned accurately before take-off.
Forty-eight test flights of Regulus II prototypes were carried out, 30 of which were successful, 14 partially successful and only four failures. A production contract was signed in January 1958 and the only submarine launch was carried out from the in September 1958.
Due to the high cost of the Regulus II (approximately one million dollars each), budgetary pressure, and the emergence of the UGM-27 Polaris SLBM (submarine-launched ballistic missile), the Regulus II program was canceled on 18 December 1958. At the time of cancellation Vought had completed 20 Regulus II missiles with 27 more on the production line. Production of Regulus I missiles continued until January 1959 with delivery of the 514th missile, and it was withdrawn from service in August 1964.
Ships fitted with Regulus.
The first launch from a submarine occurred in July 1953 from the deck of , a World War II fleet boat modified to carry Regulus. "Tunny" and her sister boat were the United States's first nuclear deterrent patrol submarines. They were joined in 1958 by two purpose-built Regulus submarines, , , and, later, by the nuclear-powered . So that no target would be left uncovered, four Regulus missiles had to be at sea at any given time. Thus, "Barbero" and "Tunny", each of which carried two Regulus missiles, patrolled simultaneously. "Growler" and "Grayback", with four missiles, or "Halibut", with five, could patrol alone. These five submarines made 40 Regulus strategic deterrent patrols between October 1959 and July 1964, when they were relieved by the s carrying the Polaris missile system. "Barbero" also earned the distinction of launching the only delivery of missile mail.
Regulus was deployed by the U.S. Navy in 1955 in the Pacific on board the cruiser . In 1956, three more followed: , , and . These four s each carried three Regulus missiles on operational patrols in the Western Pacific. "Macon"’s last Regulus patrol was in 1958, "Toledo"’s in 1959, "Helena"’s in 1960, and "Los Angeles"’s in 1961.
Ten aircraft carriers were configured to operate Regulus missiles (though only six ever actually launched one). did not deploy with the missile but conducted the first launch of a Regulus from a warship. also did not deploy but was involved in two demonstration launches. and each conducted one test launch. deployed to the Mediterranean carrying three Regulus missiles. deployed once to the Western Pacific with four missiles in 1955. "Lexington", "Hancock", , and were involved in the development of the Regulus Assault Mission (RAM) concept. RAM converted the Regulus cruise missiles into an unmanned aerial vehicle (UAV): Regulus missiles would be launched from cruisers or submarines, and once in flight, guided to their targets by carrier-based pilots with remote control equipment.
Replacement and legacy.
Production of Regulus was phased out in January 1959 with delivery of the 514th missile, and it was removed from service in August 1964. A number of the obsolete missiles were expended as targets at Eglin Air Force Base, Florida. Regulus not only provided the first nuclear strategic deterrence force for the United States Navy during the first years of the Cold War and especially during the Cuban Missile Crisis, preceding the Polaris missiles, Poseidon missiles, and Trident missiles that followed, but it also was the forerunner of the Tomahawk cruise missile.
Surviving examples.
The following museums in the United States have Regulus missiles on display as part of their collections:

</doc>
<doc id="67022" url="https://en.wikipedia.org/wiki?curid=67022" title="Lionel Barrymore">
Lionel Barrymore

Lionel Barrymore (born Lionel Herbert Blythe; April 28, 1878 – November 15, 1954) was an American actor of stage, screen and radio as well as a film director. He won an Academy Award for Best Actor for his performance in "A Free Soul" (1931), and remains best known to modern audiences for the role of the villainous Mr. Potter character in Frank Capra's 1946 film "It's a Wonderful Life". He is also particularly remembered as Ebenezer Scrooge in annual broadcasts of "A Christmas Carol" during his last two decades. He was a member of the theatrical Barrymore family.
Early life.
Lionel Barrymore was born Lionel Herbert Blythe in Philadelphia, the son of actors Georgiana Drew Barrymore and Maurice Barrymore. He was the elder brother of Ethel and John Barrymore, the uncle of John Drew Barrymore and Diana Barrymore and the great-uncle of Drew Barrymore, among other members of the Barrymore family. He attended private schools as a child, including the Art Students League of New York. While raised a Roman Catholic, Barrymore attended the Episcopal Academy in Philadelphia. 
He was married twice, to actresses Doris Rankin and Irene Fenwick, a one-time lover of his brother, John. Doris's sister Gladys was married to Lionel's uncle Sidney Drew, which made Gladys both his aunt and sister-in-law. Doris Rankin bore Lionel two daughters, Ethel Barrymore II (1908 – 1910)and Mary Barrymore (1916 – 1917). Neither child survived infancy. Barrymore never truly recovered from the deaths of his girls, and their loss undoubtedly strained his marriage to Doris Rankin, which ended in 1923. Years later, Barrymore developed a fatherly affection for Jean Harlow, who was born about the same time as his daughters. When Harlow died in 1937, Barrymore and Clark Gable mourned her as though she had been family.
Stage career.
Although reluctant to follow his parents' career, Barrymore appeared together with his formidable grandmother Louisa Lane Drew on tour and in a stage production of "The Rivals" at the age of 15. He later recounted that "I didn't want to act. I wanted to paint or draw. The theater was not in my blood, I was related to the theater by marriage only; it was merely a kind of "in-law" of mine I had to live with." Nevertheless, he soon found success on stage in character roles and continued to act, although he still wanted to become a painter and also to compose music. He appeared on Broadway in his early twenties with his uncle John Drew Jr. in such plays as "The Second in Command" (1901) and "The Mummy and the Hummingbird" (1902), the latter of which won him critical acclaim. Both were produced by Charles Frohman, who produced other plays for Barrymore and his siblings, John and Ethel. "The Other Girl" in 1903–04 was a long-running success for Barrymore. In 1905, he appeared with John and Ethel in a pantomime, starring as the title character in "Pantaloon" and playing another character in the other half of the bill, "Alice Sit-by-the-Fire".
In 1906, after a series of disappointing appearances in plays, Barrymore and his first wife, the actress Doris Rankin, left their stage careers and travelled to Paris, where he trained as an artist. He did not achieve success as a painter, and in 1909 he returned to the US. In December of that year, he returned to the stage in "The Fires of Fate", in Chicago, but left the production later that month after suffering an attack of nerves about the forthcoming New York opening. The producers gave appendicitis as the reason for his sudden departure. Nevertheless, he was soon back on Broadway in "The Jail Bird" in 1910 and continued his stage career with several more plays. He also joined his family troupe, from 1910, in their vaudeville act, where he was happy not to worry as much about memorizing lines. From 1912 to 1917, he was away from the stage again while he established his film career, but after the First World War, he had several successes on Broadway, where he established his reputation as a dramatic and character actor, often performing together with his wife. He proved his talent in such plays as "Peter Ibbetson" (1917) (with brother John), "The Copperhead" (1918) (with Doris), "The Jest" (1919) (again with John) and "The Letter of the Law" (1920). Lionel gave a short-lived performance as MacBeth in 1921 opposite veteran actress Julia Arthur as Lady MacBeth, but the production encountered strongly negative criticsm. His last stage success was in "Laugh, Clown, Laugh", in 1923, with his second wife, Irene Fenwick; they met while acting together in "The Claw" the previous year, and after they fell in love he divorced his first wife. He also received negative notices in three productions in a row in 1925. After these, he never again appeared on stage.
Film career.
Barrymore began making films about 1911 with D.W. Griffith at the Biograph Studios. There are claims that he made an earlier film with Griffith called "The Paris Hat" (1908), but no such motion picture is known to exist. Lionel and Doris were in Paris in 1908, where Lionel attended art school and where their first baby, Ethel, was born. Lionel confirms in his autobiography, "We Barrymores", that he and Doris were in France when Bleriot flew the English Channel on July 25, 1909. Barrymore made "The Battle" (1911), "The New York Hat" (1912), "Friends" and "Three Friends" (1913). In 1915 he co-starred with Lillian Russell in a movie called "Wildfire", one of the legendary Russell's few film appearances. He also was involved in writing and directing at Biograph. The last silent film he directed, "Life's Whirlpool" (Metro Pictures 1917), starred his sister, Ethel. He acted in more than 60 silent films with Griffth.
In 1920, Barrymore reprised his stage role in the film adaptation of "The Copperhead". Before the formation of Metro-Goldwyn-Mayer in 1924, Barrymore forged a good relationship with Louis B. Mayer early on at Metro Pictures. He made several silent features for Metro, most of them now lost. In 1923, Barrymore and Fenwick went to Italy to film "The Eternal City" for Metro Pictures in Rome, combining work with their honeymoon. He occasionally freelanced, returning to Griffith in 1924 to film "America". In 1924, he also went to Germany to star in British producer-director Herbert Wilcox's Anglo-German co-production "Decameron Nights", filmed at UFA's Babelsberg studios outside of Berlin. In 1925, he left New York for Hollywood. He starred as Frederick Harmon in director Henri Diamant-Berger's drama "Fifty-Fifty" (1925) opposite Hope Hampton and Louise Glaum, and made several more freelance motion pictures, including "The Bells" (Chadwick Pictures 1926) with a then-unknown Boris Karloff. His last film for Griffith was in 1928's "Drums of Love".
Prior to his marriage to Irene, Barrymore and his brother John engaged in a dispute over the issue of Irene's chastity in the wake of her having been one of John's lovers. The brothers didn't speak again for two years and weren't seen together until the premiere of John's film "Don Juan" in 1926, by which time they had patched up their differences. After 1926, Barrymore worked almost exclusively for Metro-Goldwyn-Mayer, appearing opposite such luminaries as John Gilbert, Lon Chaney, Sr., Jean Harlow, Wallace Beery, Marie Dressler, Greta Garbo, Clark Gable, Spencer Tracy, his brother John and sister Ethel. His first talking picture was "The Lion and the Mouse"; his stage experience allowed him to excel in delivering the dialogue in sound films.
On the occasional loan-out, Barrymore had a big success with Gloria Swanson in 1928's "Sadie Thompson" and the aforementioned Griffith film, "Drums of Love". In 1929, he returned to directing films. During this early and imperfect sound film period, he directed the controversial "His Glorious Night" with John Gilbert, "Madame X" starring Ruth Chatterton, and "The Rogue Song", Laurel & Hardy's first color film. Barrymore returned to acting in front of the camera in 1931. In that year, he won an Academy Award for his role as an alcoholic lawyer in "A Free Soul" (1931), after being considered in 1930 for Best Director for "Madame X". He could play many characters, like the evil Rasputin in the 1932 "Rasputin and the Empress" (in which he co-starred with siblings John and Ethel) and the ailing Oliver Jordan in "Dinner at Eight" (1933 – also with John, although they had no scenes together).
During the 1930s and 1940s, he became stereotyped as a grouchy but sweet elderly man in such films as "The Mysterious Island" (1929), "Grand Hotel" (1932, with John Barrymore), "Captains Courageous" (1937), "You Can't Take It with You" (1938), "On Borrowed Time" (1939, with Cedric Hardwicke), "Duel in the Sun" (1946), and "Key Largo" (1948).
In a series of Doctor Kildare movies in the 1930s and 1940s, he played the irascible Doctor Gillespie, a role he repeated in an MGM radio series that debuted in New York in 1950 and was later syndicated. He also played the title role in the 1940s radio series, "Mayor of the Town". Barrymore had broken his hip in an accident, hence he played Gillespie in a wheelchair. Later, his worsening arthritis kept him in the chair. The injury also precluded his playing Ebenezer Scrooge in the 1938 MGM film version of "A Christmas Carol", a role Barrymore played every year but two (1936, replaced by brother John Barrymore and 1938, replaced by Orson Welles) on the radio from 1934 through 1953. He also had a role with Clark Gable in Lone Star in 1952. His final film appearance was a cameo in "Main Street to Broadway", an MGM musical comedy released in 1953. His sister Ethel also appeared in the film.
Perhaps his best known role, thanks to perennial Christmastime replays on television, was Mr. Potter, the miserly and mean-spirited banker in "It's a Wonderful Life" (1946) opposite James Stewart. The role suggested that of the "unreformed" stage of Barrymore's "Scrooge" characterization. Lionel's wife, Irene, died on Christmas Eve of 1936 and Lionel did not perform his annual Scrooge that year. John filled in as Scrooge for his grieving brother.
Politics.
Barrymore registered for the draft during World War II despite his age and disability, to encourage others to enlist in the military. He loathed the income tax. He expressed an interest in appearing on television in the 1950s but felt compelled to remain loyal to his old friend and employer, Louis B. Mayer and MGM.
Barrymore was a Republican. In 1944, he attended the massive rally organized by David O. Selznick in the Los Angeles Coliseum in support of the Dewey-Bricker ticket as well as Governor Earl Warren of California, who would become Dewey's running mate in 1948 and later the Chief Justice of the United States. The gathering drew 93,000, with Cecil B. DeMille as the master of ceremonies and with short speeches by Hedda Hopper and Walt Disney. Among the others in attendance were Ann Sothern, Ginger Rogers, Randolph Scott, Adolphe Menjou, Gary Cooper, Eddy Arnold, William Bendix, and Walter Pidgeon.
Medical issues.
Several sources argue that arthritis alone confined Barrymore to a wheelchair. Film historian Jeanine Basinger says that his arthritis was serious by at least 1928, when Barrymore made "Sadie Thompson". Screenwriter Anita Loos claimed that the arthritis was so bad by 1929, Barrymore was taking large quantities of morphine. Film historian David Wallace says it was "well known" that Barrymore was "addicted" to morphine due to arthritis by 1929, when Louis B. Mayer hired Barrymore to direct "Redemption" (a film from which Barrymore was removed). A history of Oscar-winning actors, however, says Barrymore was only "suffering" from arthritis, not crippled by it. Marie Dressler biographer Matthew Kennedy notes that when Barrymore won his Best Actor Oscar award in 1930, the arthritis was still so minor that it only made him limp a little as he went on stage to accept the honor. Barrymore can be seen being quite physical in late silent films like "The Thirteenth Hour" and "West of Zanzibar", where he can be seen climbing out of a window.
Paul Donnelly says Barrymore's inability to walk was caused by a drawing table falling on him in 1936, breaking Barrymore's hip. Barrymore tripped over a cable while filming "Saratoga" in 1937 and broke his hip again. (Film historian Robert A. Osborne says Barrymore also suffered a broken kneecap.) The injury was painful enough that Donnelly, quoting Barrymore, says that Louis B. Mayer bought Barrymore $400 worth of cocaine every day to help him cope with the pain and allow him to sleep. Author David Schwartz says the hip fracture never healed, which was why Barrymore could not walk, while MGM historian John Douglas Eames claims that the injury was "crippling". Barrymore himself said in 1951, that it was breaking his hip twice that kept him in the wheelchair. He said he had no other problems, and that the hip healed well, but it made walking exceptionally difficult. Film historian Allen Eyles reached the same conclusion.
Lew Ayres biographer Lesley Coffin and Louis B. Mayer biographer Scott Eyman argue that it was the combination of the broken hip and Barrymore's worsening arthritis that put him in a wheelchair. Barrymore family biographer Margot Peters, however, says that Barrymore's friends Gene Fowler and James Doane both said Barrymore's arthritis was caused by syphilis, which they say he contracted in 1925. Eyman, however, explicitly rejects this hypothesis.
Whatever the cause of his disability, Barrymore's performance in "Captains Courageous" in 1937 was one of the last times he would be seen standing and walking unassisted. Afterward, Barrymore was able to get about for a short period of time on crutches even though he was in great pain. During the filming of 1938's "You Can't Take It With You", the pain of standing with crutches was so severe that Barrymore required hourly shots of painkillers. By 1938, Barrymore used a wheelchair exclusively and never walked again. He could, however, stand for short periods of time such as at his brother's funeral.
Composer; graphic artist; novelist.
Barrymore also composed music. His works ranged from solo piano pieces to large-scale orchestral works, such as "Tableau Russe," which was performed twice in "Dr. Kildare's Wedding Day" (1941), first by Nils Asther on piano and later by a full symphony orchestra. His piano compositions, "Scherzo Grotesque" and "Song Without Words", were published by G. Schirmer in 1945. He composed a "memoriam" for his late brother John, which was later performed by the Philadelphia Orchestra. He also composed the theme song of the radio program "Mayor of the Town".
Barrymore was a skillful graphic artist, creating etchings and drawings. For years, he maintained an artist's shop and studio attached to his home in Los Angeles. Some of his etchings were included in the "Hundred Prints of the Year".
He wrote a historical novel, "Mr. Cantonwine: A Moral Tale" (1953).
Death.
Barrymore died on November 15, 1954 from a heart attack in Van Nuys, California, and was entombed in the Calvary Cemetery in East Los Angeles, California. He is honored with two stars on the Hollywood Walk of Fame, in the motion picture and radio categories. He is also a member of the American Theatre Hall of Fame, along with his siblings, Ethel and John.

</doc>
<doc id="67023" url="https://en.wikipedia.org/wiki?curid=67023" title="Túpac Amaru Revolutionary Movement">
Túpac Amaru Revolutionary Movement

The Túpac Amaru Revolutionary Movement (, abbreviated MRTA) was a Peruvian radical group which started in the early 1980s. The group was led by Víctor Polay Campos until he was sentenced to 32 years imprisonment in 1992 and by Néstor Cerpa Cartolini ("Comrade Evaristo") until his death in 1997.
The MRTA took its name in homage to Túpac Amaru II, an 18th-century rebel leader who was himself named after his ancestor Túpac Amaru, the last indigenous leader of the Inca people. MRTA was designated a terrorist organization by the Peruvian government, the US Department of State and the European Parliament but was later removed from the United States State Department list of Foreign Terrorist Organizations on October 5, 2001.
At the height of its strength, the movement had several hundred active members. Its stated goals were to establish a socialist state and rid the country of all imperialist elements.
Origins.
The MRTA originated in 1980 from the merging of the Marxist-Leninist Revolutionary Socialist Party and the militant faction of the Revolutionary Left Movement, "MIR El Militante" (MIR-EM). The former gathered several ex-members of the Peruvian armed forces that participated in the leftist dictatorial government of Juan Velasco Alvarado (1968-1975), and the latter represented a subdvision of the Revolutionary Left Movement, a Castroist guerrilla faction which was defeated in 1965.
The MRTA attempted to ally with other leftist organizations following the first democratic elections in Peru after a military government period (1968–1980).
Operations.
The first action by the MRTA occurred on 31 May 1982, when five of its members, including Victor Polay Campos and Jorge Talledo Feria (members of the Central Committee) robbed a bank in La Victoria, Lima. During the hold up, Talledo was killed by friendly fire and became the first loss of the movement.
On 31 May 1989, a group of six guerrillas shot dead eight transvestites and homosexuals in a bar in the city of Tarapoto. The weekly "Cambio", official organ of the MRTA, claimed responsibility, accusing the police of protecting "these social ills, which were used to corrupt youth." This was reported by the Truth and Reconciliation Commission (TRC) in its final report (August 28, 2003). The date has been considered by the Peruvian LGBT movement as a historical landmark.
Peru's counterterrorist program diminished the group's ability to carry out terrorist attacks, and the MRTA suffered from infighting as well as violent clashes with Maoist rival Shining Path, the imprisonment or deaths of senior leaders, and loss of leftist support. ln 2001, several MRTA members remained imprisoned in Bolivia.
On 6 July 1992, MRTA fighters staged a raid on the town of Jaen, Peru, a jungle town located in the northern department of Cajamarca. One policeman, Eladio Garcia Tello, responded to the calls for help. After an intense shootout, the guerrillas were driven out of the town. Eladio Garcia perished in the firefight.
Its last major action resulted in the 1997 Japanese embassy hostage crisis. In December 1996, 14 MRTA members occupied the Japanese Ambassador's residence in Lima, holding 72 hostages for more than four months. Under orders from then-President Alberto Fujimori, armed forces stormed the residence in April 1997, rescuing all but one of the remaining hostages and killing all 14 MRTA militants. Fujimori was publicly acclaimed for the decisive action, but the affair was later tainted by subsequent revelations that at least three, and perhaps as many as eight, of the MRTistas were summarily executed after they surrendered.
Trials and convictions.
In a case that attracted international attention, Lori Berenson, a former MIT student and U.S. socialist activist living in Lima, was arrested on 30 November 1995, by the police and accused of collaborating with the MRTA. She was subsequently sentenced by a military court to life imprisonment (later reduced to 20 years by a civilian court).
In September 2003, four Chilean defendants were retried and convicted of membership in the Túpac Amaru Revolutionary Movement and participation in an attack on the Peru–North American Cultural Institute and a kidnapping-murder in 1993.
On 22 March 2006, Víctor Polay, the guerrilla leader of the MRTA, was found guilty by a Peruvian court on nearly 30 crimes committed during the late 1980s and early 1990s.
Truth and Reconciliation Commission.
Peru's Truth and Reconciliation Commission determined that the group was responsible for 1.5% of the deaths investigated. In its final findings published in 2003, the Commission observed:

</doc>
<doc id="67024" url="https://en.wikipedia.org/wiki?curid=67024" title="MRTA">
MRTA

MRTA may refer for:

</doc>
<doc id="67025" url="https://en.wikipedia.org/wiki?curid=67025" title="Arnold Schoenberg">
Arnold Schoenberg

Arnold Schoenberg or Schönberg (; 13 September 187413 July 1951) was an Austrian composer and painter. He was associated with the expressionist movement in German poetry and art, and leader of the Second Viennese School. With the rise of the Nazi Party, by 1938 Schoenberg's works were labelled as degenerate music because he was Jewish ; he moved to the United States in 1934.
Schoenberg's approach, both in terms of harmony and development, has been one of the most influential of 20th-century musical thought. Many European and American composers from at least three generations have consciously extended his thinking, whereas others have passionately reacted against it.
Schoenberg was known early in his career for simultaneously extending the traditionally opposed German Romantic styles of Brahms and Wagner. Later, his name would come to personify innovations in atonality (although Schoenberg himself detested that term) that would become the most polemical feature of 20th-century art music. In the 1920s, Schoenberg developed the twelve-tone technique, an influential compositional method of manipulating an ordered series of all twelve notes in the chromatic scale. He also coined the term developing variation and was the first modern composer to embrace ways of developing motifs without resorting to the dominance of a centralized melodic idea.
Schoenberg was also a painter, an important music theorist, and an influential teacher of composition; his students included Alban Berg, Anton Webern, Hanns Eisler, Egon Wellesz, and later John Cage, Lou Harrison, Earl Kim, Leon Kirchner, and other prominent musicians. Many of Schoenberg's practices, including the formalization of compositional method and his habit of openly inviting audiences to think analytically, are echoed in avant-garde musical thought throughout the 20th century. His often polemical views of music history and aesthetics were crucial to many significant 20th-century musicologists and critics, including Theodor W. Adorno, Charles Rosen and Carl Dahlhaus, as well as the pianists Artur Schnabel, Rudolf Serkin, Eduard Steuermann and Glenn Gould.
Schoenberg's archival legacy is collected at the Arnold Schönberg Center in Vienna.
Biography.
Early life.
Arnold Schoenberg was born into a lower middle-class Jewish family in the Leopoldstadt district (in earlier times a Jewish ghetto) of Vienna, at "Obere Donaustraße 5". His father Samuel, a native of Bratislava, was a shopkeeper, and his mother Pauline was native of Prague. Arnold was largely self-taught. He took only counterpoint lessons with the composer Alexander Zemlinsky, who was to become his first brother-in-law .
In his twenties, Schoenberg earned a living by orchestrating operettas, while composing his own works, such as the string sextet "Verklärte Nacht" ("Transfigured Night") (1899). He later made an orchestral version of this, which became one of his most popular pieces. Both Richard Strauss and Gustav Mahler recognized Schoenberg's significance as a composer; Strauss when he encountered Schoenberg's "Gurre-Lieder", and Mahler after hearing several of Schoenberg's early works.
Strauss turned to a more conservative idiom in his own work after 1909, and at that point dismissed Schoenberg. Mahler adopted him as a protégé and continued to support him, even after Schoenberg's style reached a point Mahler could no longer understand. Mahler worried about who would look after him after his death. Schoenberg, who had initially despised and mocked Mahler's music, was converted by the "thunderbolt" of Mahler's "Third Symphony", which he considered a work of genius. Afterward he "spoke of Mahler as a saint" (; ).
In 1898 Schoenberg converted to Christianity in the Lutheran church. According to MacDonald (2008, 93) this was partly to strengthen his attachment to Western European cultural traditions, and partly as a means of self-defence "in a time of resurgent anti-Semitism". In 1933, after long meditation, he returned to Judaism, because he realised that "his racial and religious heritage was inescapable", and to take up an unmistakable position on the side opposing Nazism. He would self-identify as a member of the Jewish religion later in life .
In October 1901, he married Mathilde Zemlinsky, the sister of the conductor and composer Alexander von Zemlinsky, with whom Schoenberg had been studying since about 1894. He and Mathilde had two children, Gertrud (1902–1947) and Georg (1906–1974). Gertrud would marry Schoenberg's pupil Felix Greissle in 1921 . During the summer of 1908, his wife Mathilde left him for several months for a young Austrian painter, Richard Gerstl. This period marked a distinct change in Schoenberg's work. It was during the absence of his wife that he composed "You lean against a silver-willow" (), the thirteenth song in the cycle "Das Buch der Hängenden Gärten", Op. 15, based on the collection of the same name by the German mystical poet Stefan George. This was the first composition without any reference at all to a key . Also in this year, he completed one of his most revolutionary compositions, the String Quartet No. 2, whose first two movements, though chromatic in color, use traditional key signatures, yet whose final two movements, also settings of George, daringly weaken the links with traditional tonality. Both movements end on tonic chords, and the work is not fully non-tonal. Breaking with previous string-quartet practice, it incorporates a soprano vocal line.
During the summer of 1910, Schoenberg wrote his "Harmonielehre" ("Theory of Harmony", Schoenberg 1922), which remains one of the most influential music-theory books. From about 1911, Schoenberg belonged to a circle of artists and intellectuals who included Lene Schneider-Kainer, Franz Werfel, Herwarth Walden and the latter's wife, Else Lasker-Schüler.
In 1910 he met Edward Clark, an English music journalist then working in Germany. Clark became his sole English student, and in his later capacity as a producer for the BBC he was responsible for introducing many of Schoenberg's works, and Schoenberg himself, to Britain (as well as Webern, Berg and others).
Another of his most important works from this atonal or pantonal period is the highly influential "Pierrot Lunaire", Op. 21, of 1912, a novel cycle of expressionist songs set to a German translation of poems by the Belgian-French poet Albert Giraud. Utilizing the technique of "Sprechstimme", or melodramatically spoken recitation, the work pairs a female vocalist with a small ensemble of five musicians. The ensemble, which is now commonly referred to as the Pierrot ensemble, consists of flute (doubling on piccolo), clarinet (doubling on bass clarinet), violin (doubling on viola), violoncello, speaker, and piano.
Wilhelm Bopp, director of the Vienna Conservatory from 1907, wanted a break from the stale environment personified for him by Robert Fuchs and Hermann Graedener. Having considered many candidates, he offered teaching positions to Schoenberg and Franz Schreker in 1912. At the time Schoenberg lived in Berlin. He was not completely cut off from the Vienna Conservatory, having taught a private theory course a year earlier. He seriously considered the offer, but he declined. Writing afterward to Alban Berg, he cited his "aversion to Vienna" as the main reason for his decision, while contemplating that it might have been the wrong one financially, but having made it he felt content. A couple of months later he wrote to Schreker suggesting that it might have been a bad idea for him as well to accept the teaching position .
World War I.
World War I brought a crisis in his development. Military service disrupted his life when at the age of 42 he was in the army. He was never able to work uninterrupted or over a period of time, and as a result he left many unfinished works and undeveloped "beginnings". On one occasion, a superior officer demanded to know if he was "this notorious Schoenberg, then"; Schoenberg replied: "Beg to report, sir, yes. Nobody wanted to be, someone had to be, so I let it be me" (according to Norman , this is a reference to Schoenberg's apparent "destiny" as the "Emancipator of Dissonance").
In what Ross calls an "act of war psychosis," Schoenberg drew comparisons between Germany's assault on France and his assault on decadent bourgeois artistic values. In August 1914, while denouncing the music of Bizet, Stravinsky and Ravel, he wrote: "Now comes the reckoning! Now we will throw these mediocre kitschmongers into slavery, and teach them to venerate the German spirit and to worship the German God" .
The deteriorating relation between contemporary composers and the public led him to found the Society for Private Musical Performances ("Verein für musikalische Privataufführungen" in German) in Vienna in 1918. He sought to provide a forum in which modern musical compositions could be carefully prepared and rehearsed, and properly performed under conditions protected from the dictates of fashion and pressures of commerce. From its inception through 1921, when it ended because of economic reasons, the Society presented 353 performances to paid members, sometimes at the rate of one per week. During the first year and a half, Schoenberg did not let any of his own works be performed . Instead, audiences at the Society's concerts heard difficult contemporary compositions by Scriabin, Debussy, Mahler, Webern, Berg, Reger, and other leading figures of early 20th-century music .
Development of the twelve-tone method.
Later, Schoenberg was to develop the most influential version of the dodecaphonic (also known as twelve-tone) method of composition, which in French and English was given the alternative name serialism by René Leibowitz and Humphrey Searle in 1947. This technique was taken up by many of his students, who constituted the so-called Second Viennese School. They included Anton Webern, Alban Berg and Hanns Eisler, all of whom were profoundly influenced by Schoenberg. He published a number of books, ranging from his famous "Harmonielehre" (Theory of Harmony) to "Fundamentals of Musical Composition" , many of which are still in print and used by musicians and developing composers.
Schoenberg viewed his development as a natural progression, and he did not deprecate his earlier works when he ventured into serialism. In 1923 he wrote to the Swiss philanthropist Werner Reinhart: "For the present, it matters more to me if people understand my older works ... They are the natural forerunners of my later works, and only those who understand and comprehend these will be able to gain an understanding of the later works that goes beyond a fashionable bare minimum. I do not attach so much importance to being a musical bogey-man as to being a natural continuer of properly-understood good old tradition!" (; quoted in )
His first wife died in October 1923, and in August of the next year Schoenberg married Gertrud Kolisch (1898–1967), sister of his pupil, the violinist Rudolf Kolisch (; ). She wrote the libretto for Schoenberg's one-act opera "Von heute auf morgen" under the pseudonym Max Blonda. At her request Schoenberg's (ultimately unfinished) piece, "Die Jakobsleiter" was prepared for performance by Schoenberg's student Winfried Zillig. After her husband's death in 1951 she founded Belmont Music Publishers devoted to the publication of his works . Arnold used the notes G and E (German: Es, i.e., "S") for "Gertrud Schoenberg", in the "Suite", for septet, Op. 29 (1925) (see musical cryptogram).
Following the 1924 death of composer Ferruccio Busoni, who had served as Director of a Master Class in Composition at the Prussian Academy of Arts in Berlin, Schoenberg was appointed to this post the next year, but because of health problems was unable to take up his post until 1926. Among his notable students during this period were the composers Roberto Gerhard, Nikos Skalkottas, and Josef Rufer.
Along with his twelve-tone works, 1930 marks Schoenberg's return to tonality, with numbers 4 and 6 of the Six Pieces for Male Chorus Op.35, the other pieces being dodecaphonic .
Third Reich and move to America.
Schoenberg continued in his post until the Nazis came to power under Adolf Hitler in 1933. While vacationing in France, he was warned that returning to Germany would be dangerous. Schoenberg formally reclaimed membership in the Jewish religion at a Paris synagogue, then traveled with his family to the United States . However, this happened only after his attempts to move to Britain came to nothing. He enlisted the aid of his former student and great champion Edward Clark, a senior producer with the BBC, in helping him gain a British teaching post or even a British publisher, but to no avail.
His first teaching position in the United States was at the Malkin Conservatory in Boston. He moved to Los Angeles, where he taught at the University of Southern California and the University of California, Los Angeles, both of which later named a music building on their respective campuses Schoenberg Hall (; ). He was appointed visiting professor at UCLA in 1935 on the recommendation of Otto Klemperer, music director and conductor of the Los Angeles Philharmonic Orchestra; and the next year was promoted to professor at a salary of $5,100 per year, which enabled him in either May 1936 or 1937 to buy a Spanish Revival house at 116 North Rockingham in Brentwood Park, near the UCLA campus, for $18,000. This address was directly across the street from Shirley Temple's house, and there he befriended fellow composer (and tennis partner) George Gershwin. The Schoenbergs were able to employ domestic help and began holding Sunday afternoon gatherings that were known for excellent coffee and Viennese pastries. Frequent guests included Otto Klemperer (who studied composition privately with Schoenberg beginning in April 1936), Edgard Varèse, Joseph Achron, Louis Gruenberg, Ernst Toch, and, on occasion, well-known actors such as Harpo Marx and Peter Lorre (; ; ; ; ; ; ). Composers Leonard Rosenman and George Tremblay studied with Schoenberg at this time.
After his move to the United States in 1934 (Steinberg 1995, 463), the composer used the alternative spelling of his surname "Schoenberg", rather than "Schönberg", in what he called "deference to American practice" , though according to one writer he first made the change a year earlier .
He lived there the rest of his life, but at first he was not settled. In around 1934, he applied for a position of teacher of harmony and theory at the New South Wales State Conservatorium in Sydney. The Director, Edgar Bainton, rejected him for being Jewish and for having "modernist ideas and dangerous tendencies". Schoenberg also at one time explored the idea of emigrating to New Zealand. His secretary and student (and nephew of Schoenberg's mother-in-law Henriette Kolisch), was Richard (Dick) Hoffmann Jr, Viennese-born but who lived in New Zealand 1935–47, and Schoenberg had since childhood been fascinated with islands, and with New Zealand in particular, possibly because of the beauty of the postage stamps issued by that country .
During this final period, he composed several notable works, including the difficult Violin Concerto, Op. 36 (1934/36), the "Kol Nidre", Op. 39, for chorus and orchestra (1938), the "Ode to Napoleon Buonaparte", Op. 41 (1942), the haunting Piano Concerto, Op. 42 (1942), and his memorial to the victims of the Holocaust, "A Survivor from Warsaw", Op. 46 (1947). He was unable to complete his opera "Moses und Aron" (1932/33), which was one of the first works of its genre written completely using dodecaphonic composition. Along with twelve-tone music, Schoenberg also returned to tonality with works during his last period, like the Suite for Strings in G major (1935), the Chamber Symphony No. 2 in E minor, Op. 38 (begun in 1906, completed in 1939), the Variations on a Recitative in D minor, Op. 40 (1941). During this period his notable students included John Cage and Lou Harrison.
In 1941, he became a citizen of the United States.
Later years and death.
Schoenberg's superstitious nature may have triggered his death. The composer had triskaidekaphobia (the fear of the number 13), and according to friend Katia Mann, he feared he would die during a year that was a multiple of 13 (quoted in ). He dreaded his sixty-fifth birthday in 1939 so much that a friend asked the composer and astrologer Dane Rudhyar to prepare Schoenberg's horoscope. Rudhyar did this and told Schoenberg that the year was dangerous, but not fatal.
But in 1950, on his seventy-sixth birthday, an astrologer wrote Schoenberg a note warning him that the year was a critical one: 7 + 6 = 13 (Nuria Schoenberg-Nono, quoted in ). This stunned and depressed the composer, for up to that point he had only been wary of multiples of 13 and never considered adding the digits of his age. He died on Friday, 13 July 1951, shortly before midnight. Schoenberg had stayed in bed all day, sick, anxious and depressed. His wife Gertrud reported in a telegram to her sister-in-law Ottilie the next day that Arnold died at 11:45 pm, 15 minutes before midnight . In a letter to Ottilie dated 4 August 1951, Gertrud explained, "About a quarter to twelve I looked at the clock and said to myself: another quarter of an hour and then the worst is over. Then the doctor called me. Arnold's throat rattled twice, his heart gave a powerful beat and that was the end" .
Schoenberg's ashes were later interred at the Zentralfriedhof in Vienna on 6 June 1974 .
Music.
Schoenberg's significant compositions in the repertory of modern art music extend over a period of more than 50 years. Traditionally they are divided into three periods though this division is arguably arbitrary as the music in each of these periods is considerably varied. The idea that his twelve-tone period "represents a stylistically unified body of works is simply not supported by the musical evidence" , and important musical characteristics—especially those related to motivic development—transcend these boundaries completely. The first of these periods, 1894–1907, is identified in the legacy of the high-Romantic composers of the late nineteenth century, as well as with "expressionist" movements in poetry and art. The second, 1908–1922, is typified by the abandonment of key centers, a move often described (though not by Schoenberg) as "free atonality". The third, from 1923 onward, commences with Schoenberg's invention of dodecaphonic, or "twelve-tone" compositional method. Schoenberg's best-known students, Hanns Eisler, Alban Berg, and Anton Webern, followed Schoenberg faithfully through each of these intellectual and aesthetic transitions, though not without considerable experimentation and variety of approach.
First period: Late Romanticism.
Beginning with songs and string quartets written around the turn of the century, Schoenberg's concerns as a composer positioned him uniquely among his peers, in that his procedures exhibited characteristics of both Brahms and Wagner, who for most contemporary listeners, were considered polar opposites, representing mutually exclusive directions in the legacy of German music. Schoenberg's Six Songs, Op. 3 (1899–1903), for example, exhibit a conservative clarity of tonal organization typical of Brahms and Mahler, reflecting an interest in balanced phrases and an undisturbed hierarchy of key relationships. However, the songs also explore unusually bold incidental chromaticism, and seem to aspire to a Wagnerian "representational" approach to motivic identity. The synthesis of these approaches reaches an apex in his "Verklärte Nacht", Op. 4 (1899), a programmatic work for string sextet that develops several distinctive "leitmotif"-like themes, each one eclipsing and subordinating the last. The only motivic elements that persist throughout the work are those that are perpetually dissolved, varied, and re-combined, in a technique, identified primarily in Brahms's music, that Schoenberg called "developing variation." Schoenberg's procedures in the work are organized in two ways simultaneously; at once suggesting a Wagnerian narrative of motivic ideas, as well as a Brahmsian approach to motivic development and tonal cohesion.
Second period: Free atonality.
Schoenberg's music from 1908 onward experiments in a variety of ways with the absence of traditional keys or tonal centers. His first explicitly atonal piece was the second string quartet, Op. 10, with soprano. The last movement of this piece has no key signature, marking Schoenberg's formal divorce from diatonic harmonies. Other important works of the era include his song cycle "Das Buch der Hängenden Gärten", Op. 15 (1908–1909), his Five Orchestral Pieces, Op. 16 (1909), the influential "Pierrot Lunaire", Op. 21 (1912), as well as his dramatic "Erwartung", Op. 17 (1909). The urgency of musical constructions lacking in tonal centers, or traditional dissonance-consonance relationships, however, can be traced as far back as his Chamber Symphony No. 1, Op. 9 (1906), a work remarkable for its tonal development of whole-tone and quartal harmony, and its initiation of dynamic and unusual ensemble relationships, involving dramatic interruption and unpredictable instrumental allegiances; many of these features would typify the timbre-oriented chamber music aesthetic of the coming century.
Third period: Twelve-tone and tonal works.
In the early 1920s, he worked at evolving a means of order that would make his musical texture simpler and clearer. This resulted in the "method of composing with twelve tones which are related only with one another" , in which the twelve pitches of the octave (unrealized compositionally) are regarded as equal, and no one note or tonality is given the emphasis it occupied in classical harmony. He regarded it as the equivalent in music of Albert Einstein's discoveries in physics. Schoenberg announced it characteristically, during a walk with his friend Josef Rufer, when he said, "I have made a discovery which will ensure the supremacy of German music for the next hundred years" (Stuckenschmidt 1977, 277). This period included the "Variations for Orchestra", Op. 31 (1928); "Piano Pieces", Opp. 33a & b (1931), and the Piano Concerto, Op. 42 (1942). Contrary to his reputation for strictness, Schoenberg's use of the technique varied widely according to the demands of each individual composition. Thus the structure of his unfinished opera "Moses und Aron" is unlike that of his "Fantasy for Violin and Piano", Op. 47 (1949).
Ten features of Schoenberg's mature twelve-tone practice are characteristic, interdependent, and interactive :
Reception and legacy.
First works.
After some early difficulties, Schoenberg began to win public acceptance with works such as the tone poem "Pelleas und Melisande" at a Berlin performance in 1907. At the Vienna première of the "Gurre-Lieder" in 1913, he received an ovation that lasted a quarter of an hour and culminated with Schoenberg's being presented with a laurel crown (Rosen 1996, 4; Stuckenschmidt 1977, 184).
Nonetheless, much of his work was not well received. His "Chamber Symphony No. 1" premièred unremarkably in 1907. However, when it was played again in the "Skandalkonzert" on 31 March 1913, (which also included works by Berg, Webern and Zemlinsky), "one could hear the shrill sound of door keys among the violent clapping, and in the second gallery the first fight of the evening began." Later in the concert, during a performance of the "Altenberg Lieder" by Berg, fighting broke out after Schoenberg interrupted the performance to threaten removal by the police of any troublemakers .
Twelve-tone period.
According to Ethan Haimo, understanding of Schoenberg's twelve-tone work has been difficult to achieve owing in part to the "truly revolutionary nature" of his new system, misinformation disseminated by some early writers about the system's "rules" and "exceptions" that bear "little relation to the most significant features of Schoenberg's music", the composer's secretiveness, and the widespread unavailability of his sketches and manuscripts until the late 1970s. During his life, he was "subjected to a range of criticism and abuse that is shocking even in hindsight" .
Schoenberg criticized Igor Stravinsky's new neoclassical trend in the poem "Der neue Klassizismus" (in which he derogates Neoclassicism, and obliquely refers to Stravinsky as "Der kleine Modernsky"), which he used as text for the third of his "Drei Satiren", Op. 28 .
Schoenberg's serial technique of composition with twelve notes became one of the most central and polemical issues among American and European musicians during the mid- to late-twentieth century. Beginning in the 1940s and continuing to the present day, composers such as Pierre Boulez, Karlheinz Stockhausen, Luigi Nono and Milton Babbitt have extended Schoenberg's legacy in increasingly radical directions. The major cities of the United States (e.g., Los Angeles, New York, and Boston) have had historically significant performances of Schoenberg's music, with advocates such as Babbitt in New York and the Franco-American conductor-pianist Jacques-Louis Monod. Schoenberg's students have been influential teachers at major American universities: Leonard Stein at USC, UCLA and CalArts; Richard Hoffmann at Oberlin; Patricia Carpenter at Columbia; and Leon Kirchner and Earl Kim at Harvard. Musicians associated with Schoenberg have had a profound influence upon contemporary music performance practice in the US (e.g., Louis Krasner, Eugene Lehner and Rudolf Kolisch at the New England Conservatory of Music; Eduard Steuermann and Felix Galimir at the Juilliard School). In Europe, the work of Hans Keller, , and René Leibowitz has had a measurable influence in spreading Schoenberg's musical legacy outside of Germany and Austria. His pupil and assistant Max Deutsch, who later became a professor of music, was also a conductor who made a recording of three "master works" Schoenberg with the Orchestre de la Suisse Romande, released posthumously in late 2013. This recording includes short lectures by Deutsch on each of the pieces .
Criticism.
In the 1920s, Ernst Krenek criticized a certain unnamed brand of contemporary music (presumably Schoenberg and his disciples) as "the self-gratification of an individual who sits in his studio and invents rules according to which he then writes down his notes." Schoenberg took offense at this masturbatory metaphor and answered that Krenek "wishes for only whores as listeners" .
Allen Shawn has noted that, given Schoenberg's living circumstances, his work is usually "defended" rather than listened to, and that it is difficult to experience it "apart" from the ideology that surrounds it . Richard Taruskin asserts that Schoenberg committed what he terms a "poietic fallacy", the conviction that what matters most (or all that matters) in a work of art is the making of it, the maker's input, and that the listener's pleasure must not be the composer's primary objective . Taruskin also criticizes the ideas of measuring Schoenberg's value as a composer in terms of his influence on other artists, the overrating of technical innovation, and the restriction of criticism to matters of structure and craft while derogating other approaches as vulgarian .
Personality and extramusical interests.
Schoenberg was a painter of considerable ability, whose pictures were considered good enough to exhibit alongside those of Franz Marc and Wassily Kandinsky as fellow members of the expressionist Blue Rider group.
He was interested in Hopalong Cassidy films, which Paul Buhle and David Wagner (2002, v–vii) attribute to the films' left-wing screenwriters—a rather odd claim in light of Schoenberg's statement that he was a "bourgeois" turned monarchist .
Schoenberg experienced triskaidekaphobia (the fear of the number 13), which possibly began in 1908 with the composition of the thirteenth song of the song cycle "Das Buch der Hängenden Gärten" Op. 15 . "Moses und Aron" was originally spelled "Moses und Aaron", but when he realised this contained 13 letters, he changed it . His superstitious nature may have triggered his death. According to friend Katia Mann, he feared he would die during a year that was a multiple of 13 (quoted in ).

</doc>
<doc id="67027" url="https://en.wikipedia.org/wiki?curid=67027" title="Túpac Amaru">
Túpac Amaru

Túpac Amaru or Thupa Amaro (Quechua: "Thupaq Amaru") (1545–1572) was the last indigenous monarch (Sapa Inca) of the Neo-Inca State, remnants of the Inca Empire in Vilcabamba, Peru. He was executed by the Spanish.
Accession.
Following the Spanish conquest of the Inca Empire in the 1530s, a few members of the royal family established the small independent Neo-Inca State in Vilcabamba, which was located in the relatively inaccessible Upper Amazon to the northeast of Cusco. The founder of this state was Manco Inca Yupanqui (also known as Manco Cápac II), who had initially allied himself with the Spanish, then led an unsuccessful war against them before establishing himself in Vilcabamba in 1540. After a Spanish attack in 1544 in which Manco Inca Yupanqui was killed, his son Sayri Tupac assumed the title of "Sapa Inca" (emperor, literally "only Inca"), before accepting Spanish authority in 1558, moving to Cuzco, and dying (perhaps by poison) in 1561. He was succeeded in Vilcabamba by his brother Titu Cusi, who himself died in 1571. Túpac Amaru, another brother of the two preceding emperors, then succeeded to the title in Vilcabamba.
Final war with and capture by Spanish.
At this time the Spanish were still unaware of the death of the previous "Sapa Inca" (Titu Cusi) and had routinely sent two ambassadors to continue ongoing negotiations being held with Titu Cusi. They were both killed on the border by an Inca captain.
Using the justification that the Incas had "broken the inviolate law observed by all nations of the world regarding ambassadors" the new Viceroy, Francisco de Toledo, Count of Oropesa, decided to attack and conquer Vilcabamba. He declared war on April 14, 1572. The first engagement of the war commenced in the Vilcabamba valley on June 1. The Inca people attacked first with much spirit despite being only lightly armed. Again and again, they attempted to lift the siege held by the Spanish and their native allies but each time they were forced to retreat. On June 24 the Spanish entered Vilcabamba to find it deserted and the "Sapa Inca" gone. The city had been entirely destroyed, and the last remnants of the Inca Empire, the Neo-Inca State now officially ceased to exist.
Túpac Amaru had left the previous day with a party of about 100 and headed west into the lowland forests. The group, which included his generals and family members, had then split up into smaller parties in an attempt to avoid capture.
Three groups of Spanish soldiers pursued them. One group captured Titu Cusi's son and wife. A second returned with military prisoners along with gold, silver and other precious jewels. The third group returned with Túpac Amaru's two brothers, other relatives and several of his generals. The "Sapa Inca" and his commander remained at large.
Following this, a group of forty hand-picked soldiers under Martín García Óñez de Loyola set out to pursue them. They followed the Masahuay river for 170 miles, where they found an Inca warehouse with quantities of gold and the Inca's tableware. The Spanish captured a group of Chunco Indians and compelled them to tell them what they had seen, and if they had seen the "Sapa Inca". They reported that he had gone down river, by boat, to a place called Momorí. The Spaniards then constructed five rafts and pursued them.
At Momorí, they discovered that Tupac Amaru had escaped by land. They followed with the help of the Manarí Indians, who advised which path the Inca had followed and reported that Túpac was slowed by his wife, who was about to give birth. After a fifty-mile march, they saw a campfire around nine o'clock at night. They found the "Sapa Inca" Túpac Amaru and his wife warming themselves. They assured them that no harm would come to them and secured their surrender. Túpac Amaru was arrested.
The captives were brought back to the ruins of Vilcabamba and together they were all marched into Cuzco on September 21. The invaders also brought the mummified remains of Manco Cápac and Titu Cusi and a gold statue of Punchao, a representation of the Inca's lineage containing the mortal remains of the hearts of the deceased Incas. These sacred items were then destroyed.
Execution.
The five captured Inca generals received a summary trial and were sentenced to death by hanging. Several had already died of torture or disease.
The trial of the "Sapa Inca" himself began a couple of days later. Túpac Amaru was convicted of the murder of the priests in Vilcabamba. Túpac Amaru was sentenced to be beheaded. It was reported in various sources that numerous Catholic clerics, convinced of Túpac Amaru's innocence, pleaded to no avail, on their knees, that the Inca be sent to Spain for a trial instead of being executed.
Some have argued that Viceroy Toledo, in executing a head of state recognized by the Spanish as an independent king, exceeded his authority and committed a crime within the political ideas of his own time. Other claims have been made to the contrary — that Túpac Amaru was in rebellion (his predecessors having allegedly accepted Spanish authority), that Toledo had tried peaceful means to settle differences, that three of his ambassadors to the Inca were murdered, and that Túpac Amaru subsequently raised an army to resist the colonial army. The King of Spain, Philip II, disapproved of the execution.
An eyewitness report from the day recalls him riding a mule with hands tied behind his back and a rope around his neck. Other witnesses reported there were great crowds and the "Sapa Inca" was surrounded by hundreds of guards with lances. In front of the Cathedral of Santo Domingo in the central square of Cuzco a black-draped scaffold had been erected. Reportedly 10,000 to 15,000 witnesses were present. 
Túpac Amaru mounted the scaffold accompanied by the Bishop of Cuzco. As he did, it was reported by the same witnesses that a "multitude of Indians, who completely filled the square, saw that lamentable spectacle knew that their lord and Inca was to die, they deafened the skies, making them reverberate with their cries and wailing."
As reported by Baltasar de Ocampa and Friar Gabriel de Oviedo, Prior of the Dominicans at Cuzco, both eyewitnesses, the "Sapa Inca" raised his hand to silence the crowds, and his last words were;
"Ccollanan Pachacamac ricuy auccacunac yahuarniy hichascancuta."
"Mother Earth, witness how my enemies shed my blood."
Descendants.
Nearly forty years after the conquest of Peru began with the execution of Atahualpa, the conquest ended with the execution of his nephew. The Spanish Viceroy rounded up the royal descendants. Several dozen, including Túpac Amaru's three-year-old son, were banished to Mexico, Chile, Panama and elsewhere. Some of them were allowed to return home.
Túpac Amaru's memory, however, lived on and would become personified in an important late eighteenth century insurgency that was rooted in aspirations toward a revival of Inca status vis-a-vis the Spanish administration. In 1780, José Gabriel Condorcanqui (Túpac Amaru II), who claimed to be a direct descendant of Túpac Amaru, led an indigenous uprising against continued Spanish presence in Peru alongside his wife Micaela Bastidas. Condorcanqui's rebellion emerged in response to new Bourbon Reforms implemented by the Spanish crown, which included incremental increases in levels of taxation upon indigenous populations - such as the alcabala or sales tax. Túpac Amaru II's rebellion was sparked when he (Condorcanqui) captured and killed the Spanish corregidor Antonio Arriaga in November of 1780.
Namesake.
School number 239 in Poland, located in Warsaw, Złota 72, had been named in honor of Tupac Amaru, but closed in 2001.

</doc>
<doc id="67028" url="https://en.wikipedia.org/wiki?curid=67028" title="Umbrella">
Umbrella

An umbrella or parasol is a folding canopy supported by wooden or metal ribs, which is mounted on a wooden, metal or plastic pole. It is designed to protect a person against rain or sunlight. The word "umbrella" typically refers to a device used for protection from rain. The word "parasol" usually refers to an item designed to protect from the sun. Often the difference is the material used for the canopy; some parasols are not waterproof. Umbrella canopies may be made of fabric or flexible plastic.
Umbrellas and parasols are primarily hand-held portable devices sized for personal use. The largest hand-portable umbrellas are golf umbrellas. Umbrellas can be divided into two categories: fully collapsible umbrellas, in which the metal pole supporting the canopy retracts, making the umbrella small enough to fit in a handbag, and non-collapsible umbrellas in which the support pole cannot retract; only the canopy can be collapsed. Another distinction can be made between manually operated umbrellas and spring-loaded automatic umbrellas which spring open at the press of a button.
Hand-held umbrellas have some type of handle, either a wooden or plastic cylinder or a bent "crook" handle (like the handle of a cane). Umbrellas are available in a range of price and quality points, ranging from inexpensive, modest quality models sold at discount stores to expensive, finely made, designer-labeled models. Larger parasols capable of blocking the sun for several people are often used as fixed or semi-fixed devices, used with patio tables or other outdoor furniture, or as points of shade on a sunny beach. The collapsible/folding umbrella, the direct predecessor to the modern umbrella, originated in China. These Chinese umbrellas were internally supported with bendable, retractable, and extendable joints as well as sliding levers similar to those in use today.
Parasols are sometimes called sunshades. An umbrella may also be called a brolly ("UK slang"), parapluie ("nineteenth century, French origin"), rainshade, gamp ("British, informal, dated"), bumbershoot ("American slang").
Etymology.
The word "parasol" (Spanish or French) is a combination of "para", meaning to stop or to shield, and "sol", meaning sun. "Parapluie" (French) similarly consists of "para" combined with "pluie", which means rain (which in turn derives from "pluvia", the Latin word for rain). Hence, a "parasol" shields from sunlight while a "parapluie" shields from rain. ("Parachute" means "shield from fall".)
The word "umbrella" evolved from the Latin "umbella" (an umbel is a flat-topped rounded flower) or "umbra", meaning shaded or shadow (the Latin word, in turn, derives from the Ancient Greek "ómbros" [όμβρος]).
In Britain, umbrellas were sometimes referred to as "gamps" after the character Mrs. Gamp in the Charles Dickens novel "Martin Chuzzlewit", although this usage is now obscure. Mrs. Gamp's character was well known for carrying an umbrella.
"Brolly" is a slang word for umbrella, used often in Britain, Ireland, New Zealand, Australia, South Africa, and Kenya.
"Bumbershoot" is a fanciful Americanism from the late 19th century.
History.
Ancient China.
In all written records, the oldest reference to a collapsible umbrella dates to the year 21 AD, when Wang Mang (r. 9–23) had one designed for a ceremonial four-wheeled carriage. The 2nd century commentator Fu Qian added that this collapsible umbrella of Wang Mang's carriage had bendable joints which enabled them to be extended or retracted. A 1st century collapsible umbrella has since been recovered from the tomb of Wang Guang at Lelang Commandery in the Korean Peninsula, illustrated in a work by Harada and Komai. However, the Chinese collapsible umbrella is perhaps a concept that is yet centuries older than Wang's tomb. Zhou Dynasty bronze castings of complex bronze socketed hinges with locking slides and bolts—which could have been used for parasols and umbrellas—were found in an archeological site of Luoyang, dated to the 6th century BC.
An even older source on the umbrella is perhaps the ancient book of Chinese ceremonies, called "Zhou Li" ("The Rites of Zhou"), dating 2400 years ago, which directs that upon the imperial cars the dais should be placed. The figure of this dais contained in "Zhou-Li", and the description of it given in the explanatory commentary of Lin-hi-ye, both identify it with an umbrella. The latter describes the dais to be composed of 28 arcs, which are equivalent to the ribs of the modern instrument, and the staff supporting the covering to consist of two parts, the upper being a rod 3/18 of a Chinese foot in circumference, and the lower a tube 6/10 in circumference, into which the upper half is capable of sliding and closing.
The Chinese character for umbrella is 傘 ("sǎn") and is a pictograph resembling the modern umbrella in design. Some investigators have supposed that its invention was first created by tying large leaves to bough-like ribs (the branching out parts of an umbrella). Others assert that the idea was probably derived from the tent, which remains in form unaltered to the present day. However, the tradition existing in China is that it originated in standards and banners waving in the air, hence the use of the umbrella was often linked to high-ranking (though not necessarily royalty) in China. On at least one occasion, twenty-four umbrellas were carried before the Emperor when he went out hunting. The umbrella served in this case as a defense against rain rather than sun. The Chinese design was later brought to Japan via Korea and also introduced to Persia and the Western world via the Silk Road. The Chinese and Japanese traditional parasol, often used near temples, remains similar to the original ancient Chinese design.
A late Song Dynasty Chinese divination book that was printed in about 1270 AD features a picture of a collapsible umbrella that is exactly like the modern umbrella of today's China.
Middle East.
In the sculptures at Nineveh the parasol appears frequently. Austen Henry Layard gives a picture of a bas-relief representing a king in his chariot, with an attendant holding a parasol over his head. It has a curtain hanging down behind, but is otherwise exactly like those in use today. It is reserved exclusively for the monarch (who was bald), and is never carried over any other person.
In Persia the parasol is repeatedly found in the carved work of Persepolis, and Sir John Malcolm has an article on the subject in his 1815 "History of Persia." In some sculptures, the figure of a king appears attended by a servant, who carries over his head an umbrella, with stretchers and runner complete. In other sculptures on the rock at Taghe-Bostan, supposed to be not less than twelve centuries old, a deer-hunt is represented, at which a king looks on, seated on a horse, and having an umbrella borne over his head by an attendant.
Ancient Egypt.
In Egypt, the parasol is found in various shapes. In some instances it is depicted as a flagellum, a fan of palm-leaves or coloured feathers fixed on a long handle, resembling those now carried behind the Pope in processions. Gardiner Wilkinson, in his work on Egypt, has an engraving of an Ethiopian princess travelling through Upper Egypt in a chariot; a kind of umbrella fastened to a stout pole rises in the centre, bearing a close affinity to what are now termed chaise umbrellas. According to Wilkinson's account, the umbrella was generally used throughout Egypt, partly as a mark of distinction, but more on account of its useful than its ornamental qualities. In some paintings on a temple wall, a parasol is held over the figure of a god carried in procession.
Ancient Greece.
In Greece, the parasol ("skiadeion"), was an indispensable adjunct to a lady of fashion in the late 5th century BC. Aristophanes mentions it among the common articles of female use; they could apparently open and close. Pausanias describes a tomb near Triteia in Achaia decorated with a 4th-century BC painting ascribed to Nikias; it depicted the figure of a woman, "and by her stood a female slave, bearing a parasol". For a man to carry one was considered a mark of effeminacy. In Aristophanes' "Birds", Prometheus uses one as a comical disguise.
Cultural changes among the Aristoi of Greece eventually led to a brief period - between 505 and 470BC - where men used parasols. Vase iconography bears witness to a transition from men carrying swords, then spears, then staffs, then parasols, to eventually nothing. The parasol, at that time of its fashion, displayed the luxury of the user's lifestyle. During the period of their usage, Greek style was inspired by the Persian and Lydian nobility’s way of dressing: loose robes, long decorated hair, gold, jewellery, and perfume.
It also had religious significance. In the Scirophoria, the feast of Athene Sciras, a white parasol was borne by the priestesses of the goddess from the Acropolis to the Phalerus. In the feasts of Dionysos the umbrella was used, and in an old bas-relief the same god is represented as descending "ad inferos" with a small umbrella in his hand. In the Panathenæa, the daughters of the Metics, or foreign residents, carried parasols over the heads of Athenian women as a mark of inferiority.
Ancient Rome.
From Greece it is probable that the use of the parasol passed to Rome, where it seems to have been usually used by women, while it was the custom even for effeminate men to defend themselves from the heat by means of the "Umbraculum", formed of skin or leather, and capable of being lowered at will. There are frequent references to the umbrella in the Roman Classics, and it appears that it was, not unlikely, a post of honour among maid-servants to bear it over their mistresses. Allusions to it are tolerably frequent in the poets. (Ovid Fast. lib. ii., 1. 31 I.; Martial, lib. xi., ch. 73.; lib. xiv, ch. 28, 130; Juvenal, ix., 50.; Ovid Ars. Am., ii., 209). From such mentions the umbrella does not appear to have been used as a defence from rain; this is curious enough, for it is known that the theatres were protected by the "velarium" or awning, which was drawn across the arena whenever a sudden shower came on. Possibly the expense bestowed in the decoration of the "umbraculum" was a reason for its not being applied to such use.
According to Gorius, the umbrella came to Rome from the Etruscans who came to Rome for protection, and certainly it appears not infrequently on Etruscan vases and pottery, as also on later gems and rubies. One gem, figured by Pacudius, shows an umbrella with a bent handle, sloping backwards. Strabo describes a sort of screen or umbrella worn by Spanish women, but this is not like a modern umbrella.
Ancient India.
The Sanskrit epic Mahabharata (about 4th century BC) relates the following legend: Jamadagni was a skilled bow shooter, and his devoted wife Renuka would always recover each of his arrows immediately. One time however, it took her a whole day to fetch the arrow, and she later blamed the heat of the sun for the delay. The angry Jamadagni shot an arrow at the sun. The sun begged for mercy and offered Renuka an umbrella.
Jean Baptiste Tavernier, in his 17th century book "Voyage to the East", says that on each side of the Mogul's throne were two umbrellas, and also describes the hall of the King of Ava was decorated with an umbrella. The chháta of the Indian and Burmese princes is large and heavy, and requires a special attendant, who has a regular position in the royal household. In Ava it seems to have been part of the king's title, that he was "King of the white elephant, and Lord of the twenty-four umbrellas." In 1855 the King of Burma directed a letter to the Marquis of Dalhousie in which he styles himself "His great, glorious, and most excellent Majesty, who reigns over the kingdoms of Thunaparanta, Tampadipa, and all the great umbrella-wearing chiefs of the Eastern countries".
Siam.
Simon de la Loubère, who was Envoy Extraordinary from the French King to the King of Siam in 1687 and 1688, wrote an account entitled a "New Historical Relation of the Kingdom of Siam", which was translated in 1693 into English. According to his account the use of the umbrella was granted to only some of the subjects by the king. An umbrella with several circles, as if two or three umbrellas were fastened on the same stick, was permitted to the king alone; the nobles carried a single umbrella with painted cloths hanging from it.
The Talapoins (who seem to have been a sort of Siamese monks) had umbrellas made of a palm-leaf cut and folded, so that the stem formed a handle.
Aztec Empire.
The At district of Tenochtitlan was reported to have used an umbrella made from feathers and gold as its "pantli", an identifying marker that is the equivalent of a modern flag. The "pantli" was carried by the army general.
Europe.
The extreme paucity of allusions to umbrellas throughout the Middle Ages shows that they were not in common use. In an old romance, "The Blonde of Oxford", a jester makes fun of a nobleman for being out in the rain without his cloak. "Were I a rich man", says he, "I would bear my house about with me". It appears that people depended on cloaks, not umbrellas, for protection against storms.
17th century.
Thomas Wright, in his "Domestic Manners of the English", gives a drawing from the Harleian MS., No. 604, which represents an Anglo-Saxon gentleman walking out attended by his servant, the servant carrying an umbrella with a handle that slopes backwards, so as to bring the umbrella over the head of the person in front. It probably could not be closed, but otherwise it looks like an ordinary umbrella, and the ribs are represented distinctly.
The use of the parasol and umbrella in France and England was adopted, probably from China, about the middle of the seventeenth century. At that period, pictorial representations of it are frequently found, some of which exhibit the peculiar broad and deep canopy belonging to the large parasol of the Chinese Government officials, borne by native attendants.
John Evelyn, in his "Diary" for June 22, 1664, mentions a collection of rarities shown to him by "Thompson", a Roman Catholic priest, sent by the Jesuits of Japan and China to France. Among the curiosities were "fans like those our ladies use, but much larger, and with long handles, strangely carved and filled with Chinese characters", which is evidently a description of the parasol.
In Thomas Coryat's "Crudities", published in 1611, about a century and a half prior to the general introduction of the umbrella into England, is a reference to a custom of riders in Italy using umbrellas:
And many of them doe carry other fine things of a far greater price, that will cost at the least a duckat, which they commonly call in the Italian tongue umbrellas, that is, things which minister shadowve to them for shelter against the scorching heate of the sunne. These are made of leather, something answerable to the forme of a little cannopy, & hooped in the inside with divers little wooden hoopes that extend the umbrella in a pretty large compasse. They are used especially by horsemen, who carry them in their hands when they ride, fastening the end of the handle upon one of their thighs, and they impart so large a shadow unto them, that it keepeth the heate of the sunne from the upper parts of their bodies.
In John Florio's "A WORLD of Words" (1598), the Italian word Ombrella is translated
a fan, a canopie. also a testern or cloth of state for a prince. also a kind of round fan or shadowing that they vse to ride with in sommer in Italy, a little shade. Also a bonegrace for a woman. Also the husk or cod of any seede or corne. also a broad spreding bunch, as of fenell, nill, or elder bloomes.
In Randle Cotgrave's "Dictionary of the French and English Tongues" (1614), the French Ombrelle is translated
An umbrello; a (fashion of) round and broad fanne, wherewith the Indians (and from them our great ones) preserve themselves from the heat of a scorching sunne; and hence any little shadow, fanne, or thing, wherewith women hide their faces from the sunne.
In Fynes Moryson's "Itinerary" (1617) is a similar allusion to the habit of carrying umbrellas in hot countries "to auoide the beames of the Sunne". Their employment, says the author, is dangerous, "because they gather the heate into a pyramidall point, and thence cast it down perpendicularly upon the head, except they know how to carry them for auoyding that danger".
In France, the umbrella ("parapluie") began to appear in the 1660s, when the fabric of parasols carried for protection against the sun was coated with wax. The inventory of the French royal court in 1763 mentioned "eleven parasols of taffeta in different colours" as well as "three parasols of waxed "toile", decorated around the edges with lace of gold and silver." They were rare, and the word "parapluie" ("against the rain") did not enter the dictionary of the Académie française until 1718. 
18th and 19th centuries.
Kersey's Dictionary (1708) describes an umbrella as a "screen commonly used by women to keep off rain".
The first lightweight folding umbrella in Europe was introduced in 1710 by a Paris merchant named Jean Marius, whose shop was located near the barrier of Saint-Honoré. It could be opened and closed in the same way as modern umbrellas, and weighed less than one kilogram. Marius received from the King the exclusive right to produce folding umbrellas for five years. A model was purchased by the Princess Palatine in 1712, and she enthused about it to her aristocratic friends, making it an essential fashion item for Parisiennes. In 1759, a French scientist named Navarre presented a new design to the French Academy of Sciences for an umbrella combined with a cane. Pressing a small button on the side of the cane opened the umbrella.
Their use became widespread in Paris. In 1768, a Paris magazine reported: "The common usage for quite some time now is not to go out without an umbrella, and to have the inconvenience of carrying it under your arm for six months in order to use it perhaps six times. Those who do not want to be mistaken for vulgar people much prefer to take the risk of being soaked, rather than to be regarded as someone who goes on foot; an umbrella is a sure sign of someone who doesn't have his own carriage." 
In 1769, the Maison Antoine, a store at the Magasin d'Italie on rue Saint-Denis, was the first to offer umbrellas for rent to those caught in downpours, and it became a common practice. The Lieutenant General of Police of Paris issued regulations for the rental umbrellas; they were made of oiled green silk, and carried a number so they could be found and reclaimed if someone walked off with one. 
By 1808 there were seven shops making and selling umbrellas in Paris; one shop, Sagnier on rue des Vielles-Haudriettes, received the first patent given for an invention in France for a new model of umbrella. By 1813 there were 42 shops; by 1848 there were three hundred seventy-seven small shops making umbrellas in Paris, employing 1400 workers. One of the well-known makers was Boutique Bétaille, which was located at rue Royale 20 from 1880-1939. Another was Revel, based in Lyon. By the end of the century, however, cheaper manufacturers in the Auvergne replaced Paris as the centre of umbrella manufacturing, and the town of Aurillac became the umbrella capital of France. The town still produces about half the umbrellas made in France; the umbrella factories there employ about one hundred workers. 
In Daniel Defoe's Robinson Crusoe, he constructed his own umbrella in imitation of those that he had seen used in Brazil. "I covered it with skins," he says, "the hair outwards, so that it cast off the rain like a pent-house, and kept off the sun so effectually, that I could walk out in the hottest of the weather with greater advantage than I could before in the coolest." From this description the original heavy umbrella came to be called "Robinson" which they retained for many years in England.
Captain James Cook, in one of his voyages in the late 18th century, reported seeing some of the natives of the South Pacific Islands with umbrellas made of palm leaves.
The use of the umbrella or parasol (though not unknown) was uncommon in England during the earlier half of the eighteenth century, as is evident from the comment made by General (then Lieut.-Colonel) James Wolfe, when writing from Paris in 1752; he speaks of the use of umbrellas for protection from the sun and rain, and wonders why a similar practice did not occur in England. About the same time, umbrellas came into general use as people found their value, and got over the shyness natural to its introduction. Jonas Hanway, the founder of the Magdalen Hospital, has the credit of being the first man who ventured to dare public reproach and ridicule by carrying one habitually in London. As he died in 1786, and he is said to have carried an umbrella for thirty years, the date of its first use by him may be set down at about 1750. John Macdonald relates that in 1770, he used to be addressed as, "Frenchman, Frenchman! why don't you call a coach?" whenever he went out with his umbrella. By 1788 however they seem to have been accepted: a London newspaper advertises the sale of 'improved and pocket Umbrellas, on steel frames, with every other kind of common Umbrella.' But full acceptance is not complete even today with some considering umbrellas effete.
Since then, the umbrella has come into general use, in consequence of numerous improvements. In China people learned how to waterproof their paper umbrellas with wax and lacquer. The transition to the present portable form is due, partly, to the substitution of silk and gingham for the heavy and troublesome oiled silk, which admitted of the ribs and frames being made much lighter, and also to many ingenious mechanical improvements in the framework. Victorian era umbrellas had frames of wood or baleen, but these devices were expensive and hard to fold when wet. Samuel Fox invented the steel-ribbed umbrella in 1852; however, the "Encyclopédie Méthodique" mentions metal ribs at the end of the eighteenth century, and they were also on sale in London during the 1780s. Modern designs usually employ a telescoping steel trunk; new materials such as cotton, plastic film and nylon often replace the original silk.
Modern use.
National Umbrella Day is held on February 10 each year around the world.
In 1928, Hans Haupt's pocket umbrellas appeared. In Vienna in 1928, Slawa Horowitz, a student studying sculpture at the Akademie der Bildenden Kunste Wien (Academy of Fine Arts), developed a prototype for an improved compact foldable umbrella for which she received a patent on 19 September 1929. The umbrella was called "Flirt" and manufactured by the Austrian company "Brüder Wüster" and their German associates "Kortenbach & Rauh". In Germany, the small foldable umbrellas were produced by the company "Knirps", which became a synonym in the German language for small foldable umbrellas in general. 
In 1969, Bradford E Phillips, the owner of Totes Incorporated of Loveland, Ohio obtained a patent for his "working folding umbrella".
Umbrellas have also been fashioned into hats as early as 1880 and at least as recently as 1987.
Golf umbrellas, one of the largest sizes in common use, are typically around across, but can range anywhere from .
Umbrellas are now a consumer product with a large global market. As of 2008, most umbrellas worldwide are made in China, mostly in the Guangdong, Fujian and Zhejiang provinces. The city of Shangyu alone had more than a thousand umbrella factories. In the US alone, about 33 million umbrellas, worth $348 million, are sold each year.
Umbrellas continue to be actively developed. In the US, so many umbrella-related patents are being filed that the U.S. Patent Office employs four full-time examiners to assess them. As of 2008, the office registered 3000 active patents on umbrella-related inventions. Nonetheless, Totes, the largest American umbrella producer, has stopped accepting unsolicited proposals. Its director of umbrella development was reported as saying that while umbrellas are so ordinary that everyone thinks about them, "it's difficult to come up with an umbrella idea that hasn’t already been done."
While the predominate canopy shape of an umbrella is round, canopy shapes have been streamlined to improve aerodynamic response to wind. Examples include the stealth-shaped canopy of Rizotti (1996), scoop-shaped canopy of Lisciandro (2004), and teardrop-shaped canopies of Hollinger (2004).
In 2005 Gerwin Hoogendoorn, a Dutch industrial design student of the Delft University of Technology in the Netherlands, invented an aerodynamically streamlined storm umbrella (with a similar shape as a stealth plane) which can withstand wind force 10 (winds of up to 100 km/h or 70 mp/h) and won't turn inside-out like a regular umbrella as well as being equipped with so-called ‘eyesavers’ which protect others from being accidentally wounded by the tips. Hoogendoorn's storm umbrella was nominated for and won several design awards and was featured on Good Morning America. The umbrella is sold in Europe as the Senz umbrella and is sold under license by Totes in the United States.
The "DAVEK" line of upscale umbrellas features a uniquely strong, patented frame system and unconditional lifetime guarantee. Alan Kaufman's "Nubrella" and Greg Brebner's "Blunt" are other contemporary designs.
Other uses.
The umbrella is used in weather forecasting as an icon for rain. Two variations, a plain umbrella (☂, U+2602) and an umbrella with raindrops overhead (☔, U+2614), are encoded in the Miscellaneous Symbols block of Unicode.
In religious ceremony.
As a canopy of state, umbrellas were generally used in southern and eastern Europe, and then passed from the imperial court into church ceremony. They are found in the ceremonies of the Byzantine Church, were borne over the Host in procession, and form part of the Pontifical regalia.
Catholic Church.
The "ombrellino" or umbraculum is addof the papal regalia. Although the popes no longer use it personally, it is displayed on the coat of arms of a sede vacante (the papal arms used between the death of a pope and the election of his successor). This umbraculum is normally made of alternating red and gold fabric, and is usually displayed in a partially unfolded manner. The popes have traditionally bestowed the use of the umbraculum as a mark of honor upon specific persons and places. The use of an umbraculum is one of the honorary symbols of a basilica and may be used in the basilica's coat of arms, and carried in processions by the basilica's canons.
A large umbrella is displayed in each of the Basilicas of Rome, and a cardinal bishop who receives his title from one of those churches has the privilege of having an umbrella carried over his head in solemn processions. It is possible that the galero (wide-brimmed cardinal's hat) may be derived from this umbrella. Beatiano, an Italian herald, says that "a vermilion umbrella in a field argent symbolises dominion."
An umbrella, also known as the "umbraculum" or "ombrellino", is used in Roman Catholic liturgy as well. It is held over the Holy Sacrament of the Eucharist and its carrier by a server in short processions taking place indoors, or until the priest is met at the sanctuary entrance by the bearers of the processional canopy or "baldacchino". It is regularly white or golden (the colours reserved for the Holy Sacrament) and made of silk.
Oriental Orthodox Churches.
In several Oriental Orthodox Churches, such as the Ethiopian Orthodox Tewahedo Church, umbrellas are used liturgically to show honor to a person (such as a bishop) or a holy object. In the ceremonies of Timkat (Epiphany), priests will carry a model of the Tablets of Stone, called a Tabot, on their heads in procession to a body of water, which will then be blessed. Brightly colored embroidered and fringed liturgical parasols are carried above the Tabota during this procession. Such processions also take place on other major feast days.
In photography.
Umbrellas with a reflective inside are used by photographers as a diffusion device when employing artificial lighting, and as a glare shield and shade, most often in portrait situations. Some umbrellas are shoot-through umbrellas, meaning the light goes through the umbrella and is diffused, rather than reflecting off the inside of the umbrella.
For protection against attackers.
In 1838, the Baron Charles Random de Berenger instructed readers of his book "How to Protect Life and Property" in several methods of using an umbrella as an improvised weapon against highwaymen.
In 1897, journalist J. F. Sullivan proposed the umbrella as a misunderstood weapon in a tongue-in-cheek article for the "Ludgate Monthly".
Between 1899 and 1902, both umbrellas and walking sticks as self defence weapons were incorporated into the repertoire of Bartitsu.
In January 1902, an article in "The Daily Mirror" instructed women on how they could defend themselves from ruffians with an umbrella or parasol.
In March 2011 media outlets revealed that French president Nicolas Sarkozy has started using a £10,000 armor-plated umbrella to protect him from attackers. "Para Pactum" is a Kevlar-coated device made by The Real Cherbourg. It will be carried by a member of Sarkozy's security team.
During the 2014 Hong Kong protests, sometimes referred to as the "Umbrella Revolution", protesters used umbrellas as shields against pepper spray and tear gas from riot police.
As a weapon of attack.
John Steed in the television series "the Avengers" used an umbrella which was part yardstick.
In 1978 Bulgarian dissident writer Georgi Markov was killed in London by a dose of ricin injected via a modified umbrella. The KGB is widely believed to have developed a modified umbrella that could deliver a deadly pellet.
In 2005 in a well-known case in South Africa, Brian Hahn, associate professor in mathematics and applied mathematics at the University of Cape Town was beaten to death with an umbrella by ex-doctoral student Maleafisha Steve Tladi.
In the 1992 film "Batman Returns", the Penguin (Danny DeVito) sports a bullet and gas-firing umbrella.
A high-tech bullet-resistant umbrella is used extensively as a weapon in the 2015 film "" by characters Harry Hart (Colin Firth) and Eggsy Unwin (Taron Egerton).
In architecture.
In the 1950s Frei Otto transformed the universally used individual umbrella into an item of lightweight architecture. He developed a new umbrella form, based on the minimum surface principle. The tension loaded membrane of the funnel-shaped umbrella is now stretched under the compression-loaded bars. This construction type made it technically and structurally possible to build very large convertible umbrellas. The first umbrellas of this kind (Federal Garden Exhibition, Kassel, 1955) were fixed, Frei Otto constructed the first convertible large umbrellas for the Federal Garden Exhibition in Cologne 1971. In 1978 he built a group of ten convertible umbrellas for British rock group Pink Floyd´s American tour. The great beauty of these lightweight structures inspired many subsequent projects built all over the world. The largest convertible umbrellas built until now were designed by Mahmoud Bodo Rasch and his team at SL-Rasch to provide shelter from sun and rain for the great mosques in Saudi Arabia.
Later works by the architect Le Corbusier such as Centre Le Corbusier and Villa Shodhan involve a parasol, which served as a roof structure and provided cover from the sun and wind.

</doc>
<doc id="67029" url="https://en.wikipedia.org/wiki?curid=67029" title="Passive solar building design">
Passive solar building design

In passive solar building design, windows, walls, and floors are made to collect, store, and distribute solar energy in the form of heat in the winter and reject solar heat in the summer. This is called passive solar design because, unlike active solar heating systems, it does not involve the use of mechanical and electrical devices.
The key to design a passive solar building is to best take advantage of the local climate performing an accurate site analysis. Elements to be considered include window placement and size, and glazing type, thermal insulation, thermal mass, and shading. Passive solar design techniques can be applied most easily to new buildings, but existing buildings can be adapted or "retrofitted".
Passive energy gain.
"Passive solar" technologies use sunlight without active mechanical systems (as contrasted to active solar). Such technologies convert sunlight into usable heat (in water, air, and thermal mass), cause air-movement for ventilating, or future use, with little use of other energy sources. A common example is a solarium on the equator-side of a building. Passive cooling is the use of the same design principles to reduce summer cooling requirements.
Some passive systems use a small amount of conventional energy to control dampers, shutters, night insulation, and other devices that enhance solar energy collection, storage, and use, and reduce undesirable heat transfer.
Passive solar technologies include direct and indirect solar gain for space heating, solar water heating systems based on the thermosiphon, use of thermal mass and phase-change materials for slowing indoor air temperature swings, solar cookers, the solar chimney for enhancing natural ventilation, and earth sheltering.
More widely, passive solar technologies include the solar furnace and solar forge, but these typically require some external energy for aligning their concentrating mirrors or receivers, and historically have not proven to be practical or cost effective for widespread use. 'Low-grade' energy needs, such as space and water heating, have proven, over time, to be better applications for passive use of solar energy.
As a science.
The scientific basis for passive solar building design has been developed from a combination of climatology, thermodynamics (particularly heat transfer: conduction (heat), convection, and electromagnetic radiation), fluid mechanics/natural convection (passive movement of air and water without the use of electricity, fans or pumps), and human thermal comfort based on heat index, psychrometrics and enthalpy control for buildings to be inhabited by humans or animals, sunrooms, solariums, and greenhouses for raising plants.
Specific attention is divided into: the site, location and solar orientation of the building, local sun path, the prevailing level of insolation (latitude/sunshine/clouds/precipitation), design and construction quality/materials, placement/size/type of windows and walls, and incorporation of solar-energy-storing thermal mass with heat capacity.
While these considerations may be directed toward any building, achieving an ideal optimized cost/performance solution requires careful, holistic, system integration engineering of these scientific principles. Modern refinements through computer modeling (such as the comprehensive U.S. Department of Energy "Energy Plus" building energy simulation software), and application of decades of lessons learned (since the 1970s energy crisis) can achieve significant energy savings and reduction of environmental damage, without sacrificing functionality or aesthetics. In fact, passive-solar design features such as a greenhouse/sunroom/solarium can greatly enhance the livability, daylight, views, and value of a home, at a low cost per unit of space.
Much has been learned about passive solar building design since the 1970s energy crisis. Many unscientific, intuition-based expensive construction experiments have attempted and failed to achieve zero energy - the total elimination of heating-and-cooling energy bills.
Passive solar building construction may not be difficult or expensive (using off-the-shelf existing materials and technology), but the scientific passive solar building design is a non-trivial engineering effort that requires significant study of previous counter-intuitive lessons learned, and time to enter, evaluate, and iteratively refine the simulation input and output.
One of the most useful post-construction evaluation tools has been the use of thermography using digital thermal imaging cameras for a formal quantitative scientific energy audit. Thermal imaging can be used to document areas of poor thermal performance such as the negative thermal impact of roof-angled glass or a skylight on a cold winter night or hot summer day.
The scientific lessons learned over the last three decades have been captured in sophisticated comprehensive building energy simulation computer software systems (like U.S. DOE Energy Plus, "et al.").
Scientific passive solar building design with quantitative cost benefit product optimization is not easy for a novice. The level of complexity has resulted in ongoing bad-architecture, and many intuition-based, unscientific construction experiments that disappoint their designers and waste a significant portion of their construction budget on inappropriate ideas.
The economic motivation for scientific design and engineering is significant. If it had been applied comprehensively to new building construction beginning in 1980 (based on 1970s lessons learned), America could be saving over $250,000,000 per year on expensive energy and related pollution today.
Since 1979, Passive Solar Building Design has been a critical element of achieving zero energy by educational institution experiments, and governments around the world, including the U.S. Department of Energy, and the energy research scientists that they have supported for decades. The cost effective proof of concept was established decades ago, but cultural assimilation into architecture, construction trades, and building-owner decision making has been very slow and difficult to change.
The new terms "Architectural Science" and "Architectural Technology" are being added to some schools of Architecture, with a future goal of teaching the above scientific and energy-engineering principles.
The solar path in passive design.
The ability to achieve these goals simultaneously is fundamentally dependent on the seasonal variations in the sun's path throughout the day.
This occurs as a result of the inclination of the Earth's axis of rotation in relation to its orbit. The sun path is unique for any given latitude.
In Northern Hemisphere non-tropical latitudes farther than 23.5 degrees from the equator:
The converse is observed in the Southern Hemisphere, but the sun rises to the east and sets toward the west regardless of which hemisphere you are in.
In equatorial regions at less than 23.5 degrees, the position of the sun at solar noon will oscillate from north to south and back again during the year.
In regions closer than 23.5 degrees from either north-or-south pole, during summer the sun will trace a complete circle in the sky without setting whilst it will never appear above the horizon six months later, during the height of winter.
The 47-degree difference in the altitude of the sun at solar noon between winter and summer forms the basis of passive solar design. This information is combined with local climatic data (degree day) heating and cooling requirements to determine at what time of the year solar gain will be beneficial for thermal comfort, and when it should be blocked with shading. By strategic placement of items such as glazing and shading devices, the percent of solar gain entering a building can be controlled throughout the year.
One passive solar sun path design problem is that although the sun is in the same relative position six weeks before, and six weeks after, the solstice, due to "thermal lag" from the thermal mass of the Earth, the temperature and solar gain requirements are quite different before and after the summer or winter solstice. Movable shutters, shades, shade screens, or window quilts can accommodate day-to-day and hour-to-hour solar gain and insulation requirements.
Careful arrangement of rooms completes the passive solar design. A common recommendation for residential dwellings is to place living areas facing solar noon and sleeping quarters on the opposite side. A heliodon is a traditional movable light device used by architects and designers to help model sun path effects. In modern times, 3D computer graphics can visually simulate this data, and calculate performance predictions.
Passive solar heat transfer principles.
Personal thermal comfort is a function of personal health factors (medical, psychological, sociological and situational), ambient air temperature, mean radiant temperature, air movement (wind chill, turbulence) and relative humidity (affecting human evaporative cooling). Heat transfer in buildings occurs through convection, conduction, and thermal radiation through roof, walls, floor and windows.
Convective heat transfer.
Convective heat transfer can be beneficial or detrimental. Uncontrolled air infiltration from poor weatherization / weatherstripping / draft-proofing can contribute up to 40% of heat loss during winter; however, strategic placement of operable windows or vents can enhance convection, cross-ventilation, and summer cooling when the outside air is of a comfortable temperature and relative humidity. Filtered energy recovery ventilation systems may be useful to eliminate undesirable humidity, dust, pollen, and microorganisms in unfiltered ventilation air.
Natural convection causing rising warm air and falling cooler air can result in an uneven stratification of heat. This may cause uncomfortable variations in temperature in the upper and lower conditioned space, serve as a method of venting hot air, or be designed in as a natural-convection air-flow loop for passive solar heat distribution and temperature equalization. Natural human cooling by perspiration and evaporation may be facilitated through natural or forced convective air movement by fans, but ceiling fans can disturb the stratified insulating air layers at the top of a room, and accelerate heat transfer from a hot attic, or through nearby windows. In addition, high relative humidity inhibits evaporative cooling by humans.
Radiative heat transfer.
The main source of heat transfer is radiant energy, and the primary source is the sun. Solar radiation occurs predominantly through the roof and windows (but also through walls). Thermal radiation moves from a warmer surface to a cooler one. Roofs receive the majority of the solar radiation delivered to a house. A cool roof, or green roof in addition to a radiant barrier can help prevent your attic from becoming hotter than the peak summer outdoor air temperature (see albedo, absorptivity, emissivity, and reflectivity).
Windows are a ready and predictable site for thermal radiation.
Energy from radiation can move into a window in the day time, and out of the same window at night. Radiation uses photons to transmit electromagnetic waves through a vacuum, or translucent medium. Solar heat gain can be significant even on cold clear days. Solar heat gain through windows can be reduced by insulated glazing, shading, and orientation. Windows are particularly difficult to insulate compared to roof and walls. Convective heat transfer through and around window coverings also degrade its insulation properties. When shading windows, external shading is more effective at reducing heat gain than internal window coverings.
Western and eastern sun can provide warmth and lighting, but are vulnerable to overheating in summer if not shaded. In contrast, the low midday sun readily admits light and warmth during the winter, but can be easily shaded with appropriate length overhangs or angled louvres during summer and leaf bearing summer shade trees which shed their leaves in the fall. The amount of radiant heat received is related to the location latitude, altitude, cloud cover, and seasonal / hourly angle of incidence (see Sun path and Lambert's cosine law).
Another passive solar design principle is that thermal energy can be stored in certain building materials and released again when heat gain eases to stabilize diurnal (day/night) temperature variations. The complex interaction of thermodynamic principles can be counterintuitive for first-time designers. Precise computer modeling can help avoid costly construction experiments.
Design elements for residential buildings in temperate climates.
The precise amount of equator-facing glass and thermal mass should be based on careful consideration of latitude, altitude, climatic conditions, and heating/cooling degree day requirements.
Factors that can degrade thermal performance:
Efficiency and economics of passive solar heating.
Technically, PSH is highly efficient. Direct-gain systems can utilize (i.e. convert into "useful" heat) 65-70% of the energy of solar radiation that strikes the aperture or collector.
Passive solar fraction (PSF) is the percentage of the required heat load met by PSH and hence represents potential reduction in heating costs. RETScreen International has reported a PSF of 20-50%. Within the field of sustainability, energy conservation even of the order of 15% is considered substantial.
Other sources report the following PSFs:
In favorable climates such as the southwest United States, highly optimized systems can exceed 75% PSF.
For more information see Solar Air Heat
Key passive solar building design concepts.
There are six primary passive solar energy configurations:
Direct solar gain.
Direct gain attempts to control the amount of direct solar radiation reaching the living space. This direct solar gain is a critical part of passive solar house designation as it imparts to a direct gain.
The cost effectiveness of these configurations are currently being investigated in great detail and are demonstrating promising results.
Indirect solar gain.
Indirect gain attempts to control solar radiation reaching an area adjacent but not part of the living space. Heat enters the building through windows and is captured and stored in thermal mass (e.g. water tank, masonry wall) and slowly transmitted indirectly to the building through conduction and convection.
Efficiency can suffer from slow response (thermal lag) and heat losses at night. Other issues include the cost of insulated glazing and developing effective systems to redistribute heat throughout the living area.
Isolated solar gain.
Isolated gain involves utilizing solar energy to passively move heat from or to the living space using a fluid, such as water or air by natural convection or forced convection. Heat gain can occur through a sunspace, solarium or solar closet. These areas may also be employed usefully as a greenhouse or drying cabinet. An equator-side sun room may have its exterior windows higher than the windows between the sun room and the interior living space, to allow the low winter sun to penetrate to the cold side of adjacent rooms. Glass placement and overhangs prevent solar gain during the summer. Earth cooling tubes or other passive cooling techniques can keep a solarium cool in the summer.
Measures should be taken to reduce heat loss at night e.g. window coverings or movable window insulation.
Examples:
Heat storage.
The sun doesn't shine all the time. Heat storage, or thermal mass, keeps the building warm when the sun can't heat it.
In diurnal solar houses, the storage is designed for one or a few days. The usual method is a custom-constructed thermal mass. This includes a Trombe wall, a ventilated concrete floor, a cistern, water wall or roof pond. It is also feasible to use the thermal mass of the earth itself, either as-is or by incorporation into the structure by banking or using rammed earth as a structural medium.
In subarctic areas, or areas that have long terms without solar gain (e.g. weeks of freezing fog), purpose-built thermal mass is very expensive. Don Stephens pioneered an experimental technique to use the ground as thermal mass large enough for annualized heat storage. His designs run an isolated thermosiphon 3 m under a house, and insulate the ground with a 6 m waterproof skirt.
Insulation.
Thermal insulation or superinsulation (type, placement and amount) reduces unwanted leakage of heat. Some passive buildings are actually constructed of insulation.
Special glazing systems and window coverings.
The effectiveness of direct solar gain systems is significantly enhanced by insulative (e.g. double glazing), spectrally selective glazing (low-e), or movable window insulation (window quilts, bifold interior insulation shutters, shades, etc.).
Generally, Equator-facing windows should not employ glazing coatings that inhibit solar gain.
There is extensive use of super-insulated windows in the German Passive House standard. Selection of different spectrally selective window coating depends on the ratio of heating versus cooling degree days for the design location.
Glazing selection.
Equator-facing glass.
The requirement for vertical equator-facing glass is different from the other three sides of a building. Reflective window coatings and multiple panes of glass can reduce useful solar gain. However, direct-gain systems are more dependent on double or triple glazing to reduce heat loss. Indirect-gain and isolated-gain configurations may still be able to function effectively with only single-pane glazing. Nevertheless, the optimal cost-effective solution is both location and system dependent.
Roof-angle glass / Skylights.
Skylights admit harsh direct overhead sunlight and glare either horizontally (a flat roof) or pitched at the same angle as the roof slope. In some cases, horizontal skylights are used with reflectors to increase the intensity of solar radiation (and harsh glare), depending on the roof angle of incidence. When the winter sun is low on the horizon, most solar radiation reflects off of roof angled glass ( the angle of incidence is nearly parallel to roof-angled glass morning and afternoon ). When the summer sun is high, it is nearly perpendicular to roof-angled glass, which maximizes solar gain at the wrong time of year, and acts like a solar furnace. Skylights should be covered and well-insulated to reduce natural convection ( warm air rising ) heat loss on cold winter nights, and intense solar heat gain during hot spring/summer/fall days.
The equator-facing side of a building is south in the northern hemisphere, and north in the southern hemisphere. Skylights on roofs that face away from the equator provide mostly indirect illumination, except for summer days when the sun may rise on the non-equator side of the building (at some latitudes). Skylights on east-facing roofs provide maximum direct light and solar heat gain in the summer morning. West-facing skylights provide afternoon sunlight and heat gain during the hottest part of the day.
Some skylights have expensive glazing that partially reduces summer solar heat gain, while still allowing some visible light transmission. However, if visible light can pass through it, so can some radiant heat gain (they are both electromagnetic radiation waves).
You can partially reduce some of the unwanted roof-angled-glazing summer solar heat gain by installing a skylight in the shade of deciduous (leaf-shedding) trees, or by adding a movable insulated opaque window covering on the inside or outside of the skylight. This would eliminate the daylight benefit in the summer. If tree limbs hang over a roof, they will increase problems with leaves in rain gutters, possibly cause roof-damaging ice dams, shorten roof life, and provide an easier path for pests to enter your attic. Leaves and twigs on skylights are unappealing, difficult to clean, and can increase the glazing breakage risk in wind storms.
"Sawtooth roof glazing" with vertical-glass-only can bring some of the passive solar building design benefits into the core of a commercial or industrial building, without the need for any roof-angled glass or skylights.
Skylights provide daylight. The only view they provide is essentially straight up in most applications. Well-insulated light tubes can bring daylight into northern rooms, without using a skylight. A passive-solar greenhouse provides abundant daylight for the equator-side of the building.
Infrared thermography color thermal imaging cameras ( used in formal energy audits ) can quickly document the negative thermal impact of roof-angled glass or a skylight on a cold winter night or hot summer day.
The U.S. Department of Energy states: "vertical glazing is the overall best option for sunspaces." Roof-angled glass and sidewall glass are not recommended for passive solar sunspaces.
The U.S. DOE explains drawbacks to roof-angled glazing: Glass and plastic have little structural strength. When installed vertically, glass (or plastic) bears its own weight because only a small area (the top edge of the glazing) is subject to gravity. As the glass tilts off the vertical axis, however, an increased area (now the sloped cross-section) of the glazing has to bear the force of gravity. Glass is also brittle; it does not flex much before breaking. To counteract this, you usually must increase the thickness of the glazing or increase the number of structural supports to hold the glazing. Both increase overall cost, and the latter will reduce the amount of solar gain into the sunspace.
Another common problem with sloped glazing is its increased exposure to the weather. It is difficult to maintain a good seal on roof-angled glass in intense sunlight. Hail, sleet, snow, and wind may cause material failure. For occupant safety, regulatory agencies usually require sloped glass to be made of safety glass, laminated, or a combination thereof, which reduce solar gain potential. Most of the roof-angled glass on the Crowne Plaza Hotel Orlando Airport sunspace was destroyed in a single windstorm. Roof-angled glass increases construction cost, and can increase insurance premiums. Vertical glass is less susceptible to weather damage than roof-angled glass.
It is difficult to control solar heat gain in a sunspace with sloped glazing during the summer and even during the middle of a mild and sunny winter day. Skylights are the antithesis of zero energy building Passive Solar Cooling in climates with an air conditioning requirement.
Angle of incident radiation.
The amount of solar gain transmitted through glass is also affected by the angle of the incident solar radiation. Sunlight striking glass within 20 degrees of perpendicular is mostly transmitted through the glass, whereas sunlight at more than 35 degrees from perpendicular is mostly reflected
All of these factors can be modeled more precisely with a photographic light meter and a heliodon or optical bench, which can quantify the ratio of reflectivity to transmissivity, based on angle of incidence.
Alternatively, passive solar computer software can determine the impact of sun path, and cooling-and-heating degree days on energy performance. Regional climatic conditions are often available from local weather services.
Operable shading and insulation devices.
A design with too much equator-facing glass can result in excessive winter, spring, or fall day heating, uncomfortably bright living spaces at certain times of the year, and excessive heat transfer on winter nights and summer days.
Although the sun is at the same altitude 6-weeks before and after the solstice, the heating and cooling requirements before and after the solstice are significantly different. Heat storage on the Earth's surface causes "thermal lag." Variable cloud cover influences solar gain potential. This means that latitude-specific fixed window overhangs, while important, are not a complete seasonal solar gain control solution.
Control mechanisms (such as manual-or-motorized interior insulated drapes, shutters, exterior roll-down shade screens, or retractable awnings) can compensate for differences caused by thermal lag or cloud cover, and help control daily / hourly solar gain requirement variations. 
Home automation systems that monitor temperature, sunlight, time of day, and room occupancy can precisely control motorized window-shading-and-insulation devices.
Exterior colors reflecting - absorbing.
Materials and colors can be chosen to reflect or absorb solar thermal energy. Using information on a Color for electromagnetic radiation to determine its thermal radiation properties of reflection or absorption can assist the choices.
Another major issue for many window systems is that they can be potentially vulnerable sites of excessive thermal gain or heat loss. Whilst high mounted clerestory window and traditional skylights can introduce daylight in poorly oriented sections of a building, unwanted heat transfer may be hard to control. Thus, energy that is saved by reducing artificial lighting is often more than offset by the energy required for operating HVAC systems to maintain thermal comfort.
Various methods can be employed to address this including but not limited to window coverings, insulated glazing and novel materials such as aerogel semi-transparent insulation, optical fiber embedded in walls or roof, or hybrid solar lighting at Oak Ridge National Laboratory.
Reflecting elements, from active and passive daylighting collectors, such as light shelves, lighter wall and floor colors, mirrored wall sections, interior walls with upper glass panels, and clear or translucent glassed hinged doors and sliding glass doors take the captured light and passively reflect it further inside. The light can be from passive windows or skylights and solar light tubes or from active daylighting sources. In traditional Japanese architecture the Shōji sliding panel doors, with translucent Washi screens, are an original precedent. International style, Modernist and Mid-century modern architecture were earlier innovators of this passive penetration and reflection in industrial, commercial, and residential applications.
Passive solar water heating.
There are many ways to use solar thermal energy to heat water for domestic use. Different active-and-passive solar hot water technologies have different location-specific economic cost benefit analysis implications.
Fundamental passive solar hot water heating involves no pumps or anything electrical. It is very cost effective in climates that do not have lengthy sub-freezing, or very-cloudy, weather conditions. Other active solar water heating technologies, etc. may be more appropriate for some locations.
It is possible to have active solar hot water which is also capable of being "off grid" and qualifies as sustainable. This is done by the use of a photovoltaic cell which uses energy from the sun to power the pumps.
Comparison to the Passive House standard in Europe.
There is growing momentum in Europe for the approach espoused by the Passive House ("Passivhaus" in German) Institute in Germany. Rather than relying solely on traditional passive solar design techniques, this approach seeks to make use of all passive sources of heat, minimises energy usage, and emphasises the need for high levels of insulation reinforced by meticulous attention to detail in order to address thermal bridging and cold air infiltration. Most of the buildings built to the Passive House standard also incorporate an active heat recovery ventilation unit with or without a small (typically 1 kW) incorporated heating component.
The energy design of Passive House buildings is developed using a spreadsheet-based modeling tool called the Passive House Planning Package (PHPP) which is updated periodically. The current version is PHPP2007, where 2007 is the year of issue. A building may be certified as a "Passive House" when it can be shown that it meets certain criteria, the most important being that the annual specific heat demand for the house should not exceed 15kWh/m2a.
Design tools.
Traditionally a heliodon was used to simulate the altitude and azimuth of the sun shining on a model building at any time of any day of the year. In modern times, computer programs can model this phenomenon and integrate local climate data (including site impacts such as overshadowing and physical obstructions) to predict the solar gain potential for a particular building design over the course of a year. GPS-based smartphone applications can now do this inexpensively on a hand held device. These design tools provide the passive solar designer the ability to evaluate local conditions, design elements and orientation prior to construction. Energy performance optimization normally requires an iterative-refinement design-and-evaluate process. There is no such thing as a "one-size-fits-all" universal passive solar building design that would work well in all locations.
Levels of application.
Many detached suburban houses can achieve reductions in heating expense without obvious changes to their appearance, comfort or usability. This is done using good siting and window positioning, small amounts of thermal mass, with good-but-conventional insulation, weatherization, and an occasional supplementary heat source, such as a central radiator connected to a (solar) water heater. Sunrays may fall on a wall during the daytime and raise the temperature of its thermal mass. This will then radiate heat into the building in the evening. External shading, or a radiant barrier plus air gap, may be used to reduce undesirable summer solar gain.
An extension of the "passive solar" approach to seasonal solar capture and storage of heat and cooling. These designs attempt to capture warm-season solar heat, and convey it to a seasonal thermal store for use months later during the cold season ("annualised passive solar.") Increased storage is achieved by employing large amounts of thermal mass or earth coupling. Anecdotal reports suggest they can be effective but no formal study has been conducted to demonstrate their superiority. The approach also can move cooling into the warm season. Examples:
A "purely passive" solar-heated house would have no mechanical furnace unit, relying instead on energy captured from sunshine, only supplemented by "incidental" heat energy given off by lights, computers, and other task-specific appliances (such as those for cooking, entertainment, etc.), showering, people and pets. The use of natural convection air currents (rather than mechanical devices such as fans) to circulate air is related, though not strictly solar design. Passive solar building design sometimes uses limited electrical and mechanical controls to operate dampers, insulating shutters, shades, awnings, or reflectors. Some systems enlist small fans or solar-heated chimneys to improve convective air-flow. A reasonable way to analyse these systems is by measuring their coefficient of performance. A heat pump might use 1 J for every 4 J it delivers giving a COP of 4. A system that only uses a 30 W fan to more-evenly distribute 10 kW of solar heat through an entire house would have a COP of 300.
Passive solar building design is often a foundational element of a cost-effective zero energy building. Although a ZEB uses multiple passive solar building design concepts, a ZEB is usually not purely passive, having active mechanical renewable energy generation systems such as: wind turbine, photovoltaics, micro hydro, geothermal, and other emerging alternative energy sources.
Passive solar design on skyscrapers.
There has been recent interest in the utilization of the large amounts of surface area on skyscrapers to improve their overall energy efficiency. Because skyscrapers are increasingly ubiquitous in urban environments, yet require large amounts of energy to operate, there is potential for large amounts of energy savings employing passive solar design techniques. One study, which analyzed the proposed 22 Bishopsgate tower in London, found that a 35% energy decrease in demand can theoretically be achieved through indirect solar gains, by rotating the building to achieve optimum ventilation and daylight penetration, usage of high thermal mass flooring material to decrease temperature fluctuation inside the building, and using double or triple glazed low emissivity window glass for direct solar gain. Indirect solar gain techniques included moderating wall heat flow by variations of wall thickness (from 20 to 30 cm), using window glazing on the outdoor space to prevent heat loss, dedicating 15-20% of floor area for thermal storage, and implementing a Trombe wall to absorb heat entering the space. Overhangs are used to block direct sunlight in the summer, and allow it in the winter, and heat reflecting blinds are inserted between the thermal wall and the glazing to limit heat build-up in the summer months.
Another study analyzed double-green skin facade (DGSF) on the outside of high rise buildings in Hong Kong. Such a green facade, or vegetation covering the outer walls, can combat the usage of air conditioning greatly - as much as 80%, as discovered by the researchers.
In more temperate climates, strategies such as glazing, adjustment of window-to-wall ratio, sun shading and roof strategies can offer considerable energy savings, in the 30% to 60% range.

</doc>
<doc id="67030" url="https://en.wikipedia.org/wiki?curid=67030" title="Errol Flynn">
Errol Flynn

Errol Leslie Thomson Flynn (20 June 1909 – 14 October 1959) was an Australian-American actor. He was known for his romantic swashbuckler roles in Hollywood films.
Early life.
Flynn was born in a suburb of Hobart, Tasmania, where his father, Theodore Thomson Flynn, was a lecturer (1909) and later professor (1911) of biology at the University of Tasmania. Flynn was born at the Queen Alexandra Hospital in Battery Point. His mother was born Lily Mary Young, but dropped the first names Lily Mary shortly after she was married and changed her name to Marelle. Flynn described his mother's family as ""seafaring folk"" and this appears to be where his lifelong interest in boats and the sea originated. Despite Flynn's claims, the evidence indicates that he was not descended from any of the Bounty mutineers. Married at St. John's Church of England, Birchgrove, Sydney, on 23 January 1909, both of his parents were native-born Australians of Irish, English and Scottish descent.
After early schooling in Hobart, from 1923 to 1925 Flynn was educated at the South West London College, a private boarding school in Barnes, London, and in 1926 returned to Australia to attend Sydney Church of England Grammar School ("Shore School") where he was the classmate of a future Australian prime minister, John Gorton. He concluded his formal education with being expelled from Shore for theft, and—according to his own account—having been caught in a romantic assignation with the school's laundress. After being dismissed from a job as a junior clerk with a Sydney shipping company for pilfering petty cash, he went to Papua New Guinea at the age of eighteen, seeking and failing to find his fortune in tobacco planting and metals mining. He spent the next five years oscillating between the New Guinea frontier territory and Sydney. In January 1931, he became engaged to Naomi Campbell-Dibbs, the youngest daughter of Mr and Mrs R Campbell-Dibbs of Temora and Bowral NSW, a relationship which ended before 1935.
Early career.
In early 1933, Flynn appeared as an amateur actor in the Australian film "In the Wake of the Bounty", in the lead role of Fletcher Christian. Later that year he returned to Britain to pursue a career in acting, and soon secured a job with the Northampton Repertory Company at the town's Royal Theatre (now part of Royal & Derngate), where he worked and received his training as a professional actor for seven months. Northampton is home to an art-house cinema named after him, the Errol Flynn Filmhouse. He performed at the 1934 Malvern Festival and in Glasgow, and briefly in London's West End.
In 1934 Flynn was dismissed from Northampton Rep. after he threw a female stage manager down a stairwell. He returned to Warner Brothers' Teddington Studios in Middlesex where he had worked as an extra in the film "I Adore You" before going to Northampton. With his new-found acting skills he was cast as the lead in "Murder at Monte Carlo" (currently a lost film). During its filming he was signed by Warner Bros. and emigrated to the U.S. as a contract actor.
Hollywood.
Flynn was an immediate sensation in his first starring Hollywood role, "Captain Blood" (1935). Typecast as a swashbuckler, he helped to re-invent the action-adventure genre with a succession of films over the next six years, most under the direction of Michael Curtiz: "The Charge of the Light Brigade" (1936),"The Prince and the Pauper" (1937), "The Adventures of Robin Hood" (1938; his first Technicolor film), "The Dawn Patrol" (1938), "Dodge City" (1939), "The Private Lives of Elizabeth and Essex" (1939) and "The Sea Hawk" (1940).
In collaboration with Hollywood's best fight arrangers, Flynn became noted for fast-paced sword fights, beginning with "The Adventures of Robin Hood", "The Sea Hawk" and "Captain Blood". He demonstrated an acting range beyond action-adventure roles in light contemporary social comedies ("The Perfect Specimen" (1937) and "Four's a Crowd" (1938)), and melodrama ("The Sisters" (1938)). During this period Flynn published his first book, "Beam Ends" (1937), an autobiographical account of his sailing experiences around Australia as a youth. He also travelled to Spain, in 1937, as a war correspondent during the Spanish Civil War.
Flynn co-starred with Olivia de Havilland a total of eight times, and together they made the most successful on-screen romantic partnership in Hollywood in the late 1930s-early 1940s in "Captain Blood" (1935), "The Charge of the Light Brigade" (1936), "The Adventures of Robin Hood" (1938), "Four's a Crowd" (1938), "Dodge City" (1939), "The Private Lives of Elizabeth and Essex" (1939), "Santa Fe Trail" (1940) and "They Died with Their Boots On" (1941). While Flynn acknowledged his personal attraction to de Havilland, assertions by film historians that they were romantically involved during the filming of "Robin Hood" were denied by de Havilland. "Yes, we did fall in love and I believe that this is evident in the screen chemistry between us", she told an interviewer in 2009. "But his circumstances marriage to actress Lili Damita at the time prevented the relationship going further. I have not talked about it a great deal but the relationship was not consummated. Chemistry was there though. It was there."
Flynn's relationship with Bette Davis, his co-star in "The Private Lives of Elizabeth and Essex" (1939), was quarrelsome; Davis allegedly slapped him across the face far harder than necessary during one scene. Flynn attributed her anger to unrequited romantic interest; but according to others, Davis resented sharing equal billing with a man she considered incapable of playing any role beyond a dashing adventurer. "He himself openly said, 'I don't know really anything about acting'", she told an interviewer, "and I admire his honesty, because he's absolutely right." Years later, however, de Havilland recounted that during a private screening of "Elizabeth and Essex", an astounded Davis exclaimed, "Damn it! The man "could" act!"
In 1940, at the zenith of his career, Flynn was voted the fourth most popular star in the US and the seventh most popular in Britain. He was a member of the Hollywood Cricket Club with David Niven, and a talented tennis player on the California club circuit. His suave, debonair, devil-may-care attitude was characterised as "Errolesque" by author Benjamin S. Johnson in his treatise, "An Errolesque Philosophy on Life".
Second World War.
Flynn became a naturalised American citizen on 14 August 1942. As the United States had by then entered the Second World War, he attempted to enlist in the armed services, but failed the physical exam due to multiple heart problems (including at least one heart attack), recurrent malaria (contracted in New Guinea), chronic back pain (self-medicated with morphine and later, heroin), chronic tuberculosis and numerous venereal diseases. This created an image problem for both Flynn, the supposed paragon of male physical prowess, and for Warner Brothers, which continued to cast him in athletic roles, including such patriotic productions as "Dive Bomber" (1941), "Desperate Journey" (1942) and "Objective, Burma!" (1945).
Despite widespread criticism, Flynn's failure to join other Hollywood stars in military service was never explained by the studio, which had no desire to publicise the health problems of one of its most valuable assets.
Post-war career.
In 1946, Flynn published an adventure novel, "Showdown", and earned a reported $184,000 (). In 1947 he signed a 15-year contract with Warner Bros. for $225,000 per film. His income totaled $214,000 that year, and $200,000 in 1948.
After the Second World War the taste of the American filmgoing audience changed from European-themed material and the English history-based escapist epics in which Flynn excelled to more gritty, urban realism and film noir, reflecting modern American life. Flynn tried unsuccessfully to make the transition in "Uncertain Glory" (1944) and "Cry Wolf" (1947), and then increasingly passe Westerns such as "Silver River" (1948) and "Montana" (1950). 
Flynn's behaviour became increasingly disruptive during filming; he was released from his contract in 1950 by Jack L. Warner as part of a stable-clearing of 1930s glamour-generation stars. His Hollywood career over at the age of 41, Flynn entered a steep financial and physical decline.
Europe.
In the 1950s, after losing his savings from the Hollywood years in a series of financial disasters, including "The Story of William Tell" (1954), he became a parody of himself, sailing aimlessly around the Western Mediterranean aboard his yacht . Heavy alcohol abuse left him prematurely aged and overweight. He staved off financial ruin with roles in forgettable productions such as "King's Rhapsody" (1955) in the UK's failing film industry, "Hello God" (1951) and "Crossed Swords" (1954). He performed in such also-ran Hollywood films as "Mara Maru" (1952) and "Istanbul" (1957), and made occasional television appearances. As early as 1952 he had been seriously ill with hepatitis resulting in liver damage. In 1956 he presented and sometimes performed in the television anthology series "The Errol Flynn Theatre" that was filmed in Britain. He enjoyed a brief revival of popularity with "The Sun Also Rises" (1957); The Big Boodle (1957), filmed in Cuba; "Too Much, Too Soon" (1958); and "The Roots of Heaven" (1958). He met with Stanley Kubrick to discuss a role in "Lolita", but nothing came of it. Flynn went to Cuba in late 1958 to film the self-produced B film "Cuban Rebel Girls", where he met Fidel Castro and was initially an enthusiastic supporter of the Cuban Revolution. He wrote a series of newspaper and magazine articles for the New York Journal American and other publications documenting his time in Cuba with Castro. Many of these pieces were lost until 2009, when they were rediscovered in a collection at the University of Texas at Austin's Center for American History. He narrated a short film titled "Cuban Story: The Truth About Fidel Castro Revolution" (1959), his last known work as an actor.
Personal life.
Lifestyle.
Flynn developed a reputation for womanising, hard drinking and for a time in the 1940s, narcotics abuse. The expression "in like Flynn" is said to have been coined to refer to the supreme ease with which he reputedly seduced women, though there is dispute about its origin. Flynn was reportedly fond of the expression, and later claimed that he wanted to call his memoir "In Like Me". (The publisher insisted on a more tasteful title, "My Wicked, Wicked Ways".)
His lifestyle caught up with him in 1942 when two under-age girls, Betty Hansen and Peggy Satterlee, accused him of statutory rape at the Bel Air home of Flynn's friend Frederick McEvoy and on board Flynn's yacht, respectively. The scandal received immense press attention. Many of Flynn's fans, assuming that his screen persona was a reflection of his actual personality, refused to accept that the charges were true. Some founded organisations to publicly protest the accusation. (One such group, the American Boys' Club for the Defense of Errol Flynn—ABCDEF—accumulated a substantial membership that included William F. Buckley, Jr.)
The trial took place in late January and early February 1943; Flynn's attorney, Jerry Giesler, impugned the women's character and morals, and accused them of numerous indiscretions, including affairs with married men and abortions. Flynn was acquitted, but the trial's widespread coverage and lurid overtones permanently damaged his carefully cultivated screen image as an idealised romantic lead player.
Marriages and family.
Flynn was married three times: to actress Lili Damita from 1935 until 1942 (one son, Sean Flynn, 31 May 1941-disappeared MIA 1971); to Nora Eddington from 1943-49 (two daughters, Deirdre, born 1945 and Rory, born 1947); and to actress Patrice Wymore from 1950 until his death (one daughter, Arnella Roma, 1953–98). In Hollywood, he tended to refer to himself as Irish rather than Australian (his father Theodore Thomson Flynn had been a biologist and a professor at the Queen's University of Belfast in Northern Ireland during the latter part of his career). After quitting Hollywood, Flynn lived with Wymore in Port Antonio, Jamaica in the early 1950s. He was largely responsible for developing tourism to this area and for a while owned the Titchfield Hotel which was decorated by the artist Olga Lehmann. He popularised trips down rivers on bamboo rafts.
Flynn was a longtime friend of the painter Boris Smirnoff, who painted his portrait several times, as well as those of Lili Damita, Patrice Wymore and celebrity friends such as Edward G. Robinson, Jean Harlow, Norma Shearer and Barbara Stanwyck.
The gossips took note of his close friendships with Lupe Vélez, Marlene Dietrich, Dolores del Río and Carole Lombard. Lombard is said to have resisted his advances. She had already met and fallen in love with Clark Gable, but she liked Flynn and invited him to her extravagant soirees.
His only son, Sean (born 31 May 1941), was an actor and war correspondent. He and his colleague Dana Stone disappeared in Cambodia in 1970, during the Vietnam War, while both were working as freelance photojournalists for "Time" magazine. Neither man's remains has ever been found; it is generally assumed that they were killed by Khmer Rouge guerillas. After a decade-long search financed by his mother, Sean was officially declared dead in 1984. In 2010 a British team uncovered the remains of a Western hostage in the Cambodian jungle, but DNA comparisons with samples from the Flynn family were negative. Sean's life is recounted in the book "Inherited Risk: Errol and Sean Flynn in Hollywood and Vietnam".
Flynn's daughter Rory has one son, Sean Rio Flynn, named after her half-brother. He is an actor. Rory Flynn wrote a book about her father, "The Baron of Mulholland: A Daughter Remembers Errol Flynn"
Death.
By 1959, Flynn's financial difficulties had become so serious that he flew to Vancouver, British Columbia on 9 October to negotiate the lease of his yacht "Zaca" to the businessman George Caldough. As Caldough was driving Flynn and the young actress Beverly Aadland, who had accompanied him on the trip, to the airport on 14 October for a Los Angeles-bound flight, Flynn began complaining of severe pain in his back and legs. Caldough transported him to the residence of a doctor, Grant Gould, who noted that Flynn had considerable difficulty negotiating the building's stairway. Gould, assuming that the pain was due to degenerative disc disease and spinal osteoarthritis, administered 50 milligrams of demerol intravenously. As Flynn's discomfort diminished, he "reminisced at great length about his past experiences" to those present. He refused a drink when offered it. Gould then performed a leg massage in the apartment's bedroom and advised Flynn to rest there before resuming his journey. Flynn responded that he felt "ever so much better". After 20 minutes Aadland checked on Flynn and discovered him unresponsive. Despite immediate emergency medical treatment from Gould and a swift transferral by ambulance to Vancouver General Hospital, he did not regain consciousness and was pronounced dead that evening. The coroner's report noted the cause of death as a heart attack, with a significant incidental finding of cirrhosis of the liver.
Both of Flynn's parents survived him, as did his former wives and estranged third wife, Patrice Wymore, and his four children. He is buried at Forest Lawn Memorial Park Cemetery in Glendale, California.
Posthumous controversies.
In 1961, Beverly Aadland's mother, Florence, co-wrote "The Big Love" with Tedd Thomey, alleging that Flynn had been involved in a sexual relationship with her daughter, who was 15 at the time. The book was later made into a play starring Tracey Ullman as Florence Aadland.
In 1980, author Charles Higham published a controversial biography, "Errol Flynn: The Untold Story", in which he alleged that Flynn was a fascist sympathiser who spied for the Nazis before and during the Second World War, and that he was bisexual and had affairs with Tyrone Power, Howard Hughes and Truman Capote. The author accused Flynn of arranging to have "Dive Bomber" filmed on location at the San Diego Naval Base for the benefit of Japanese military planners, who needed information on American warships and defense installations. Higham later admitted to the "New York Times" that he had no documents proving Flynn was a Nazi agent. Flynn's former housemate David Niven criticised Higham for his unfounded accusations; Higham responded that Niven was ignorant of his friend's activities. In 1981 Flynn's daughters, Rory and Deirdre, hired Melvin Belli to sue Higham and his publisher Doubleday for libel; the suit was eventually dismissed on the grounds that one cannot, legally, libel a dead person.
In a 2000 article Higham repeated his Nazi collaboration accusations, and also linked Flynn with Dr. Hermann Erben, an Austrian who served in German military intelligence. Subsequent biographers—notably Tony Thomas ("Errol Flynn: The Spy Who Never Was") and Buster Wiles ("My Days With Errol Flynn: The Autobiography of a Stuntman")—rejected Higham's claims as pure fabrication. They accused him of altering FBI documents in an attempt to validate his allegations. Flynn's political leanings, they said, appear to have been leftist: he supported the Spanish Republic in the Spanish Civil War and, for a time, the Cuban Revolution. In 2000 the British Home Office released MI5 files suggesting that Flynn worked for the Allies during the war. Flynn reportedly volunteered for espionage duties in Ireland, but was turned down because of FDR's fear that he sympathised with the Nazis. In "Errol Flynn: Satan's Angel" (2000), David Bret also alleged that Flynn was bisexual, but denounced the Nazi charges.
In a 1982 interview with "Penthouse" magazine, Ronald DeWolf, son of the author L. Ron Hubbard, said that his father's friendship with Flynn was so strong that Hubbard's family considered Flynn an adoptive father to DeWolf. He claimed that Flynn and his father engaged in illegal activities together, including drug smuggling and sexual acts with under-age girls; but Flynn never joined Hubbard's religious group, Scientology.
Select radio performances.
Flynn appeared in numerous radio performances:
Theatre performances.
Flynn appeared on stage in a number of performances, particularly early in his career:

</doc>
<doc id="67031" url="https://en.wikipedia.org/wiki?curid=67031" title="Sarbanes–Oxley Act">
Sarbanes–Oxley Act

The Sarbanes–Oxley Act of 2002 (), also known as the "Public Company Accounting Reform and Investor Protection Act" (in the Senate) and "Corporate and Auditing Accountability and Responsibility Act" (in the House) and more commonly called Sarbanes–Oxley, Sarbox or SOX, is a United States federal law that set new or expanded requirements for all U.S. public company boards, management and public accounting firms. There are also a number of provisions of the Act that also apply to privately held companies, for example the willful destruction of evidence to impede a Federal investigation.
The bill, which contains eleven sections, was enacted as a reaction to a number of major corporate and accounting scandals, including Enron and Worldcom. The sections of the bill cover responsibilities of a public corporation’s board of directors, adds criminal penalties for certain misconduct, and required the Securities and Exchange Commission to create regulations to define how public corporations are to comply with the law.
Background.
Sarbanes–Oxley was named after sponsors U.S. Senator Paul Sarbanes (D-MD) and U.S. Representative Michael G. Oxley (R-OH). As a result of SOX, top management must individually certify the accuracy of financial information. In addition, penalties for fraudulent financial activity are much more severe. Also, SOX increased the oversight role of boards of directors and the independence of the outside auditors who review the accuracy of corporate financial statements.
The bill, which contains eleven sections, was enacted as a reaction to a number of major corporate and accounting scandals, including those affecting Enron, Tyco International, Adelphia, Peregrine Systems, and WorldCom. These scandals cost investors billions of dollars when the share prices of affected companies collapsed, and shook public confidence in the US securities markets.
The act contains eleven titles, or sections, ranging from additional corporate board responsibilities to criminal penalties, and requires the Securities and Exchange Commission (SEC) to implement rulings on requirements to comply with the law. Harvey Pitt, the 26th chairman of the SEC, led the SEC in the adoption of dozens of rules to implement the Sarbanes–Oxley Act. It created a new, quasi-public agency, the Public Company Accounting Oversight Board, or PCAOB, charged with overseeing, regulating, inspecting, and disciplining accounting firms in their roles as auditors of public companies. The act also covers issues such as auditor independence, corporate governance, internal control assessment, and enhanced financial disclosure. The nonprofit arm of Financial Executives International (FEI), Financial Executives Research Foundation (FERF), completed extensive research studies to help support the foundations of the act.
The act was approved by the House by a vote of 423 in favor, 3 opposed, and 8 abstaining and by the Senate with a vote of 99 in favor and 1 abstaining. President George W. Bush signed it into law, stating it included "the most far-reaching reforms of American business practices since the time of Franklin D. Roosevelt. The era of low standards and false profits is over; no boardroom in America is above or beyond the law.""
In response to the perception that stricter financial governance laws are needed, SOX-type regulations were subsequently enacted in Canada (2002), Germany (2002), South Africa (2002), France (2003), Australia (2004), India (2005), Japan (2006), Italy (2006), Israel, and Turkey.(see also similar laws in other countries below.)
Debate continued as of 2007 over the perceived benefits and costs of SOX. Opponents of the bill have claimed it has reduced America's international competitive edge against foreign financial service providers because it has introduced an overly complex regulatory environment into US financial markets. A study commissioned by NYC Mayor Michael Bloomberg and US Sen. Charles Schumer, (D-NY), cited this as one reason America's financial sector is losing market share to other financial centers worldwide. Proponents of the measure said that SOX has been a "godsend" for improving the confidence of fund managers and other investors with regard to the veracity of corporate financial statements.
The 10th anniversary of SOX coincided with the passing of the Jumpstart Our Business Startups (JOBS) Act, designed to give emerging companies an economic boost, and cutting back on a number of regulatory requirements.
Major elements.
Public Company Accounting Oversight Board
History and context: events contributing to the adoption of Sarbanes–Oxley.
A variety of complex factors created the conditions and culture in which a series of large corporate frauds occurred between 2000–2002. The spectacular, highly publicized frauds at Enron, WorldCom, and Tyco exposed significant problems with conflicts of interest and incentive compensation practices. The analysis of their complex and contentious root causes contributed to the passage of SOX in 2002. In a 2004 interview, Senator Paul Sarbanes stated:
Timeline and passage of Sarbanes–Oxley.
The House passed Rep. Oxley's bill (H.R. 3763) on April 24, 2002, by a vote of 334 to 90. The House then referred the "Corporate and Auditing Accountability, Responsibility, and Transparency Act" or "CAARTA" to the Senate Banking Committee with the support of President George W. Bush and the SEC. At the time, however, the Chairman of that Committee, Senator Paul Sarbanes (D-MD), was preparing his own proposal, Senate Bill 2673.
Senator Sarbanes's bill passed the Senate Banking Committee on June 18, 2002, by a vote of 17 to 4. On June 25, 2002, WorldCom revealed it had overstated its earnings by more than $3.8 billion during the past five quarters (15 months), primarily by improperly accounting for its operating costs. Senator Sarbanes introduced Senate Bill 2673 to the full Senate that same day, and it passed 97–0 less than three weeks later on July 15, 2002.
The House and the Senate formed a Conference Committee to reconcile the differences between Sen. Sarbanes's bill (S. 2673) and Rep. Oxley's bill (H.R. 3763). The conference committee relied heavily on S. 2673 and "most changes made by the conference committee strengthened the prescriptions of S. 2673 or added new prescriptions." (John T. Bostelman, The Sarbanes–Oxley Deskbook § 2–31.)
The Committee approved the final conference bill on July 24, 2002, and gave it the name "the Sarbanes–Oxley Act of 2002." The next day, both houses of Congress voted on it without change, producing an overwhelming margin of victory: 423 to 3 in the House and 99 to 0 in the Senate. On July 30, 2002, President George W. Bush signed it into law, stating it included "the most far-reaching reforms of American business practices since the time of Franklin D. Roosevelt." 
Analyzing the cost-benefits of Sarbanes–Oxley.
A significant body of academic research and opinion exists regarding the costs and benefits of SOX, with significant differences in conclusions. This is due in part to the difficulty of isolating the impact of SOX from other variables affecting the stock market and corporate earnings. Section 404 of the act, which requires management and the external auditor to report on the adequacy of a company's internal control on financial reporting, is often singled out for analysis. Conclusions from several of these studies and related criticism are summarized below:
Effects on exchange listing choice of non-US companies.
Some have asserted that Sarbanes–Oxley legislation has helped displace business from New York to London, where the Financial Conduct Authority regulates the financial sector with a lighter touch. In the UK, the non-statutory Combined Code of Corporate Governance plays a somewhat similar role to SOX. See Howell E. Jackson & Mark J. Roe, "Public Enforcement of Securities Laws: Preliminary Evidence" (Working Paper January 16, 2007). London based Alternative Investment Market claims that its spectacular growth in listings almost entirely coincided with the Sarbanes Oxley legislation. In December 2006 Michael Bloomberg, New York's mayor, and Charles Schumer, a U.S. senator from New York, expressed their concern.
The Sarbanes–Oxley Act's effect on non-U.S. companies cross-listed in the U.S. is different on firms from developed and well regulated countries than on firms from less developed countries according to Kate Litvak. Companies from badly regulated countries see benefits that are higher than the costs from better credit ratings by complying to regulations in a highly regulated country (USA), but companies from developed countries only incur the costs, since transparency is adequate in their home countries as well. On the other hand, the benefit of better credit rating also comes with listing on other stock exchanges such as the London Stock Exchange.
Piotroski and Srinivasan (2008) examine a comprehensive sample of international companies that list onto U.S. and U.K. stock exchanges before and after the enactment of the Act in 2002. Using a sample of all listing events onto U.S. and U.K. exchanges from 1995–2006, they find that the listing preferences of large foreign firms choosing between U.S. exchanges and the LSE's Main Market did not change following SOX. In contrast, they find that the likelihood of a U.S. listing among small foreign firms choosing between the Nasdaq and LSE's Alternative Investment Market decreased following SOX. The negative effect among small firms is consistent with these companies being less able to absorb the incremental costs associated with SOX compliance. The screening of smaller firms with weaker governance attributes from U.S. exchanges is consistent with the heightened governance costs imposed by the Act increasing the bonding-related benefits of a U.S. listing.
Implementation of key provisions.
Sarbanes–Oxley Section 302: Disclosure controls.
Under Sarbanes–Oxley, two separate sections came into effect—one civil and the other criminal. (Section 302) (civil provision); (Section 906) (criminal provision).
Section 302 of the Act mandates a set of internal procedures designed to ensure accurate financial disclosure. The signing officers must certify that they are "responsible for establishing and maintaining internal controls" and "have designed such internal controls to ensure that material information relating to the company and its consolidated subsidiaries is made known to such officers by others within those entities, particularly during the period in which the periodic reports are being prepared." . The officers must "have evaluated the effectiveness of the company's internal controls as of a date within 90 days prior to the report" and "have presented in the report their conclusions about the effectiveness of their internal controls based on their evaluation as of that date." "Id.".
The SEC interpreted the intention of Sec. 302 in Final Rule 33–8124. In it, the SEC defines the new term "disclosure controls and procedures," which are distinct from "internal controls over financial reporting." Under both Section 302 and Section 404, Congress directed the SEC to promulgate regulations enforcing these provisions.
External auditors are required to issue an opinion on whether effective internal control over financial reporting was maintained in all material respects by management. This is in addition to the financial statement opinion regarding the accuracy of the financial statements. The requirement to issue a third opinion regarding management's assessment was removed in 2007.
A Lord & Benoit Report: Bridging the Sarbanes-Oxley Disclosure Control Gap was filed with the SEC Subcommittee n internal controls which reported that those companies with ineffective internal controls, the expected rate of full and accurate disclosure under Section 302 will range between 8 and 15 percent. A full 9 out of every 10 companies with ineffective Section 404 controls self reported effective 
302 controls in the same period end that an adverse Section 404 was reported, 90% in accurate without a Section 404 audit. http://www.section404.org/UserFiles/File/Lord_Benoit_Report_1_Bridging_the_Disclosure_Control_Gap.pdf
Sarbanes–Oxley Section 303: Improper influence on conduct of audits.
a. Rules To Prohibit. It shall be unlawful, in contravention of such rules or regulations as the Commission shall prescribe as necessary and appropriate in the public interest or for the protection of investors, for any officer or director of an issuer, or any other person acting under the direction thereof, to take any action to fraudulently influence, coerce, manipulate, or mislead any independent public or certified accountant engaged in the performance of an audit of the financial statements of that issuer for the purpose of rendering such financial statements materially misleading.
b. Enforcement. In any civil proceeding, the Commission shall have exclusive authority to enforce this section and any rule or regulation issued under this section.
c. No Preemption of Other Law. The provisions of subsection (a) shall be in addition to, and shall not supersede or preempt, any other provision of law or any rule or regulation issued thereunder.
d. Deadline for Rulemaking. The Commission shall—1. propose the rules or regulations required by this section, not later than 90 days after the date of enactment of this Act; and
2. issue final rules or regulations required by this section, not later than 270 days after that date of enactment.[http://taft.law.uc.edu/CCL/SOact/sec303.html]
Sarbanes–Oxley Section 401: Disclosures in periodic reports (Off-balance sheet items).
The bankruptcy of Enron drew attention to off-balance sheet instruments that were used fraudulently. During 2010, the court examiner's review of the Lehman Brothers bankruptcy also brought these instruments back into focus, as Lehman had used an instrument called "Repo 105" to allegedly move assets and debt off-balance sheet to make its financial position look more favorable to investors. Sarbanes-Oxley required the disclosure of all material off-balance sheet items. It also required an SEC study and report to better understand the extent of usage of such instruments and whether accounting principles adequately addressed these instruments; the SEC report was issued June 15, 2005. Interim guidance was issued in May 2006, which was later finalized. Critics argued the SEC did not take adequate steps to regulate and monitor this activity.
Sarbanes–Oxley Section 404: Assessment of internal control.
The most contentious aspect of SOX is Section 404, which requires management and the external auditor to report on the adequacy of the company's internal control on financial reporting (ICFR). This is the most costly aspect of the legislation for companies to implement, as documenting and testing important financial manual and automated controls requires enormous effort.
Under Section 404 of the Act, management is required to produce an "internal control report" as part of each annual Exchange Act report. "See" . The report must affirm "the responsibility of management for establishing and maintaining an adequate internal control structure and procedures for financial reporting." . The report must also "contain an assessment, as of the end of the most recent fiscal year of the Company, of the effectiveness of the internal control structure and procedures of the issuer for financial reporting." To do this, managers are generally adopting an internal control framework such as that described in COSO.
To help alleviate the high costs of compliance, guidance and practice have continued to evolve. The Public Company Accounting Oversight Board (PCAOB) approved Auditing Standard No. 5 for public accounting firms on July 25, 2007. This standard superseded Auditing Standard No. 2, the initial guidance provided in 2004. The SEC also released its interpretive guidance on June 27, 2007. It is generally consistent with the PCAOB's guidance, but intended to provide guidance for management. Both management and the external auditor are responsible for performing their assessment in the context of a top-down risk assessment, which requires management to base both the scope of its assessment and evidence gathered on risk. This gives management wider discretion in its assessment approach. These two standards together require management to:
SOX 404 compliance costs represent a tax on inefficiency, encouraging companies to centralize and automate their financial reporting systems. This is apparent in the comparative costs of companies with decentralized operations and systems, versus those with centralized, more efficient systems. For example, the 2007 Financial Executives International (FEI) survey indicated average compliance costs for decentralized companies were $1.9 million, while centralized company costs were $1.3 million. Costs of evaluating manual control procedures are dramatically reduced through automation.
Sarbanes–Oxley 404 and smaller public companies.
The cost of complying with SOX 404 impacts smaller companies disproportionately, as there is a significant fixed cost involved in completing the assessment. For example, during 2004 U.S. companies with revenues exceeding $5 billion spent 0.06% of revenue on SOX compliance, while companies with less than $100 million in revenue spent 2.55%.
This disparity is a focal point of 2007 SEC and U.S. Senate action. The PCAOB intends to issue further guidance to help companies scale their assessment based on company size and complexity during 2007. The SEC issued their guidance to management in June, 2007.
After the SEC and PCAOB issued their guidance, the SEC required smaller public companies (non-accelerated filers) with fiscal years ending after December 15, 2007 to document a Management Assessment of their Internal Controls over Financial Reporting (ICFR). Outside auditors of non-accelerated filers however opine or test internal controls under PCAOB (Public Company Accounting Oversight Board) Auditing Standards for years ending after December 15, 2008. Another extension was granted by the SEC for the outside auditor assessment until years ending after December 15, 2009. The reason for the timing disparity was to address the House Committee on Small Business concern that the cost of complying with Section 404 of the Sarbanes–Oxley Act of 2002 was still unknown and could therefore be disproportionately high for smaller publicly held companies. On October 2, 2009, the SEC granted another extension for the outside auditor assessment until fiscal years ending after June 15, 2010. The SEC stated in their release that the extension was granted so that the SEC's Office of Economic Analysis could complete a study of whether additional guidance provided to company managers and auditors in 2007 was effective in reducing the costs of compliance. They also stated that there will be no further extensions in the future.
On September 15, 2010 the SEC issued final rule 33-9142 the permanently exempts registrants that are neither accelerated nor large accelerated filers as defined by Rule 12b-2 of the Securities and Exchange Act of 1934 from Section 404(b) internal control audit requirement.
Sarbanes–Oxley Section 802: Criminal penalties for influencing US Agency investigation/proper administration.
Section 802(a) of the SOX, states:
Sarbanes–Oxley Section 906: Criminal Penalties for CEO/CFO financial statement certification.
§ 1350. Section 906 states: Failure of corporate officers to certify financial reports
(a) Certification of Periodic Financial Reports.— Each periodic report containing financial statements filed by an issuer with the Securities Exchange Commission pursuant to section 13(a) or 15(d) of the Securities Exchange Act of 1934 (15 U.S.C. 78m (a) or 78o (d)) shall be accompanied by a written statement by the chief executive officer and chief financial officer (or equivalent thereof) of the issuer.
(b) Content.— The statement required under subsection (a) shall certify that the periodic report containing the financial statements fully complies with the requirements of section 13(a) or 15(d) of the Securities Exchange Act of 1934 (15 U.S.C. 78m or 78o (d)) and that information contained in the periodic report fairly presents, in all material respects, the financial condition and results of operations of the issuer.
(c) Criminal Penalties.— Whoever— 
(1) certifies any statement as set forth in subsections (a) and (b) of this section knowing that the periodic report accompanying the statement does not comport with all the requirements set forth in this section shall be fined not more than $1,000,000 or imprisoned not more than 10 years, or both; or
(2) willfully certifies any statement as set forth in subsections (a) and (b) of this section knowing that the periodic report accompanying the statement does not comport with all the requirements set forth in this section shall be fined not more than $5,000,000, or imprisoned not more than 20 years, or both. [http://www.law.cornell.edu/uscode/18/1350.html]
Sarbanes–Oxley Section 1107: Criminal penalties for retaliation against whistleblowers.
Section 1107 of the SOX states:
Clawbacks of executive compensation for misconduct.
One of the highlights of the law was a provision that allowed the SEC to force a company's CEO or CFO to disgorge any executive compensation (such as bonus pay or proceeds from stock sales) earned within a year of misconduct that results in an earnings restatement. However, according to Gretchen Morgenson of "The New York Times", such clawbacks have actually been rare, due in part to the requirement that the misconduct must be either deliberate or reckless. The SEC did not attempt to claw back any executive compensation until 2007, and as of December 2013 had only brought 31 cases, 13 of which were begun after 2010. However, according to Dan Whalen of the accounting research firm Audit Analytics, the threat of clawbacks, and the time-consuming litigation associated with them, has forced companies to tighten their financial reporting standards.
Criticism.
Congressman Ron Paul and others such as former Arkansas governor Mike Huckabee have contended that SOX was an unnecessary and costly government intrusion into corporate management that places U.S. corporations at a competitive disadvantage with foreign firms, driving businesses out of the United States. In an April 14, 2005 speech before the U.S. House of Representatives, Paul stated, "These regulations are damaging American capital markets by providing an incentive for small US firms and foreign firms to deregister from US stock exchanges. According to a study by a researcher at the Wharton Business School, the number of American companies deregistering from public stock exchanges nearly tripled during the year after Sarbanes–Oxley became law, while the New York Stock Exchange had only 10 new foreign listings in all of 2004. The reluctance of small businesses and foreign firms to register on American stock exchanges is easily understood when one considers the costs Sarbanes–Oxley imposes on businesses. According to a survey by Korn/Ferry International, Sarbanes–Oxley cost Fortune 500 companies an average of $5.1 million in compliance expenses in 2004, while a study by the law firm of Foley and Lardner found the Act increased costs associated with being a publicly held company by 130 percent." 
A research study published by Joseph Piotroski of Stanford University and Suraj Srinivasan of Harvard Business School titled "Regulation and Bonding: Sarbanes Oxley Act and the Flow of International Listings" in the Journal of Accounting Research in 2008 found that following the act's passage, smaller international companies were more likely to list in stock exchanges in the U.K. rather than U.S. stock exchanges.
During the financial crisis of 2007–2010, critics blamed Sarbanes–Oxley for the low number of Initial Public Offerings (IPOs) on American stock exchanges during 2008. In November 2008, Newt Gingrich and co-author David W. Kralik called on Congress to repeal Sarbanes–Oxley.
According to the National Venture Capital Association, in all of 2008 there have been just six companies that have gone public. Compare that with 269 IPOs in 1999, 272 in 1996, and 365 in 1986." (According to Hoover's IPO Scorecard, however, 31, not six companies went public on the major U.S. stock exchanges in 2008, a year when the economy was much worse than 2007 (when 209 companies went public) or 2006 (205 IPOs).)
However, the number of IPOs had declined to 87 in 2001, well down from the highs, but before Sarbanes–Oxley was passed. In 2004, IPOs were up 195% from the previous year to 233. There were 196 IPOs in 2005, 205 in 2006 (with a sevenfold increase in deals over $1 billion) and 209 in 2007.
A 2012 Wall St. Journal editorial stated, "One reason the U.S. economy isn't creating enough jobs is that it's not creating enough employers... For the third year in a row the world's leading exchange for new stock offerings was located not in New York, but in Hong Kong... Given that the U.S. is still home to the world's largest economy, there's no reason it shouldn't have the most vibrant equity markets—unless regulation is holding back the creation of new public companies. On that score it's getting harder for backers of the Sarbanes-Oxley accounting law to explain away each disappointing year since its 2002 enactment as some kind of temporary or unrelated setback."
Praise.
Former Federal Reserve Chairman Alan Greenspan praised the Sarbanes–Oxley Act in 2005: "I am surprised that the Sarbanes–Oxley Act, so rapidly developed and enacted, has functioned as well as it has...the act importantly reinforced the principle that shareholders own our corporations and that corporate managers should be working on behalf of shareholders to allocate business resources to their optimum use."
SOX has been praised by a cross-section of financial industry experts, citing improved investor confidence and more accurate, reliable financial statements. The CEO and CFO are now required to unequivocally take ownership for their financial statements under Section 302, which was not the case prior to SOX. Further, auditor conflicts of interest have been addressed, by prohibiting auditors from also having lucrative consulting agreements with the firms they audit under Section 201. SEC Chairman Christopher Cox stated in 2007: "Sarbanes–Oxley helped restore trust in U.S. markets by increasing accountability, speeding up reporting, and making audits more independent."
The Financial Executives International (FEI) 2007 study and research by the Institute of Internal Auditors (IIA) also indicate SOX has improved investor confidence in financial reporting, a primary objective of the legislation. The IIA study also indicated improvements in board, audit committee, and senior management engagement in financial reporting and improvements in financial controls.
Financial restatements increased significantly in the wake of the SOX legislation, as companies "cleaned up" their books. Glass, Lewis & Co. LLC is a San Francisco-based firm that tracks the volume of do-overs by public companies. Its March 2006 report, "Getting It Wrong the First Time," shows 1,295 restatements of financial earnings in 2005 for companies listed on U.S. securities markets, almost twice the number for 2004. "That's about one restatement for every 12 public companies—up from one for every 23 in 2004," says the report.
One fraud uncovered by the Securities and Exchange Commission (SEC) in November 2009 may be directly credited to Sarbanes-Oxley. The fraud, which spanned nearly 20 years and involved over $24 million, was committed by Value Line () against its mutual fund shareholders. The fraud was first reported to the SEC in 2004 by the then Value Line Fund () portfolio manager and Chief Quantitative Strategist, Mr. John (Jack) R. Dempsey of Easton, Connecticut, who was required to sign a Code of Business Ethics as part of SOX. Restitution totaling $34 million was placed in a fair fund and returned to the affected Value Line mutual fund investors. The Commission ordered Value Line to pay a total of $43,705,765 in disgorgement, prejudgment interest and civil penalty, and ordered Buttner, CEO and Henigson, COO to pay civil penalties of $1,000,000 and $250,000, respectively. The Commission further imposed officer and director bars and broker-dealer, investment adviser, and investment company associational bars (“Associational Bars”) against Buttner and Henigson. No criminal charges were filed.
The Sarbanes–Oxley Act has been praised for nurturing an ethical culture as it forces top management to be transparent and employees to be responsible for their acts whilst protecting whistleblowers.
Legal challenges.
A lawsuit ("Free Enterprise Fund v. Public Company Accounting Oversight Board") was filed in 2006 challenging the constitutionality of the PCAOB. The complaint argues that because the PCAOB has regulatory powers over the accounting industry, its officers should be appointed by the President, rather than the SEC. Further, because the law lacks a "severability clause," if part of the law is judged unconstitutional, so is the remainder. If the plaintiff prevails, the U.S. Congress may have to devise a different method of officer appointment. Further, the other parts of the law may be open to revision.
The lawsuit was dismissed from a District Court; the decision was upheld by the Court of Appeals on August 22, 2008. Judge Kavanaugh, in his dissent, argued strongly against the constitutionality of the law. On May 18, 2009, the United States Supreme Court agreed to hear this case. On December 7, 2009, it heard the oral arguments. On June 28, 2010, the United States Supreme Court unanimously turned away a broad challenge to the law, but ruled 5–4 that a section related to appointments violates the Constitution's separation of powers mandate. The act remains "fully operative as a law" pending a process correction.
In its March 4, 2014 "Lawson v. FMR LLC" decision the United States Supreme Court rejected a narrow reading of the SOX whistleblower protection and instead held that the anti-retaliation protection that the Sarbanes–Oxley Act of 2002 provides to whistleblowers applies also to employees of a public company's private contractors and subcontractors.
In its February 25, 2015 "Yates v. United States (2015)" decision the US Supreme Court sided with Yates by reversing the previous judgement, with a plurality of the justices reading the Act to cover "only objects one can use to record or preserve information, not all objects in the physical world." Justice Samuel Alito concurred in the judgment and noted that the statute's nouns and verbs only applies to filekeeping and not fish.

</doc>
<doc id="67032" url="https://en.wikipedia.org/wiki?curid=67032" title="USS Barbero (SS-317)">
USS Barbero (SS-317)

USS "Barbero" (SS/SSA/SSG-317) was a "Balao"-class submarine of the United States Navy, named for a family of fishes commonly called surgeon fish.
"Barbero" was laid by the Electric Boat Company of Groton, Connecticut on 25 March 1943. She was launched on 12 December 1943, sponsored by Mrs. Katherine R. Keating, and commissioned on 29 April 1944, Lieutenant Commander Irvin S. Hartman in command.
Service history.
World War II.
"Barbero"s war operations span the period from 9 August 1944– 2 January 1945, during which she completed two war patrols. She is credited with sinking three Japanese merchant ships totaling 9,126 tons while patrolling in the Java and South China Seas.
"Barbero" embarked on her first war patrol from Pearl Harbor on 9 August 1944. She reached her patrol area, located east of the central Philippines off San Bernardino Strait, on 24 August. Though she had been stationed there to report and to interdict any attempt by the Japanese to use the strait to contest the Allied invasion of the Palau Islands, they tried no such move. Moreover, during the 31 days that she spent in the area, she encountered no large target. The high point of the patrol came when "Barbero" lobbed 25 rounds at a radar station on Batag Island with her 5"/25 caliber gun deck gun. Though she obtained no definite results, her failure to detect radar activity the following night prompted her claim to have neutralized the station. The submarine cleared the area on 24 September 1944 and headed for western Australia, concluding a disappointing 56-day patrol at Fremantle on 4 October.
After refit and a brief training period, "Barbero" departed Fremantle on her second war patrol on 26 October. On this patrol, Comdr. Hartman also assumed command of a coordinated attack group (nicknamed a wolfpack) consisting of "Barbero", , and . The first phase of this two-part patrol lasted from 26 October until 15 November. It was conducted in the Makassar Strait and in the area west of Mindoro. On 2 November, after a lively passage of Lombok Strait during which picket boats and shore batteries fired on her, "Barbero" sank her first ship, a 2,700-ton transport loaded with troops. On 8 November, she sent a 7,500-ton tanker to the bottom during a coordinated attack on a small convoy carried out in cooperation with "Redfin". "Redfin" received credit for sinking the other tanker in the convoy.
Following a three-day stop at Mios Woendi to rearm with torpedoes and perform minor upkeep, "Barbero" embarked on the second portion of the patrol. This longer phase lasted from 18 November 1944 to 2 January 1945 and was conducted in the South China Sea northeast of Borneo. On 24 December in an attack on an escorted four-ship convoy, a 7,500-ton gasoline tanker was sunk and a second tanker grossing about 5,000 tons was damaged. On Christmas Day, the submarine claimed her gift from the Japanese in the form of a 4,000-ton cargo ship that she sank. On 27 December, while she attempted the perilous repassage of Lombok Strait, fragments from an aerial bomb that narrowly missed "Barbero" close aboard aft damaged her port reduction gear. The damage forced her to cover the distance remaining to Fremantle on a single screw, and put her out of action for the remainder of the war. In 68 days away from Fremantle, Barbero returned having sunk four ships totalling 21,700 tons and claiming damage to a fifth of 5,000 tons.
Post-war cargo and Regulus missile conversions.
In September 1945, she was ordered to Mare Island Naval Shipyard – where she underwent pre-inactivation overhaul – and was placed in commission in reserve on 25 April 1946.
Following conversion to a cargo submarine at Mare Island, "Barbero" was recommissioned, redesignated SSA-317, and assigned to the Pacific Fleet on 31 March 1948. From October 1948 – March 1950, she took part in an experimental program to evaluate her capabilities as a cargo carrier. Experimentation ended in early 1950, and she was decommissioned into the reserve on 30 June 1950.
On 1 February 1955, "Barbero" entered Mare Island Naval Shipyard for her second conversion, equipping her to launch the Regulus I nuclear cruise missile. She was thus redesignated SSG-317 and recommissioned on 28 October 1955 under Lt. Cmdr. Samuel T. Bussey as the second submarine equipped with the Regulus missile, having been converted in 1953. In this role, "Barbero" was equipped with a hangar housing two missiles and a launcher on the after deck. One of the limitations of Regulus was that the firing submarine had to surface, the missile then being rolled out onto the launcher and fired. Regulus I also required guidance from submarines or other platforms after firing. She operated off the coast of California, firing her first test missile near San Clemente Island on 14 March 1956, until April 1956, when she transited the Panama Canal and joined the Atlantic Fleet, based at Norfolk, Virginia. From there, she served as an integral part in the nation's defense against nuclear war by carrying out early versions of the deterrent patrols that later became a regular feature of the submarine force after the introduction of the fleet ballistic missile submarine. She conducted patrols in the Atlantic for the next three years, under a policy of having one Regulus boat in each ocean.
On 1 July 1959, Barbero was reassigned to the Pacific Fleet's Submarine Squadron 1, based at Pearl Harbor to conduct deterrent patrols in the Pacific due to a shift in policy placing all deployed Regulus assets there. She usually patrolled with "Tunny" to provide four missiles on station. In January 1962, "Barbero" entered the Pearl Harbor Naval Shipyard for an extensive five-month overhaul. During that time, members of her crew pursued various fields of training and education at service schools in Hawaii and on the United States West Coast. After concluding the overhaul in June 1962, "Barbero" carried out local operations and conducted refresher training. She then completed a single deterrent patrol in the latter half of 1962. "Barbero" conducted another two deterrent patrols in 1963, making liberty calls in East Asian ports at the conclusion of each.
Missile Mail.
In 1959 "Barbero" assisted the United States Post Office Department, predecessor of what in 1971 became today's United States Postal Service (USPS), in its search for faster, more efficient forms of mail transportation. The Post Office tried its first and only delivery of "Missile Mail", though the idea of delivering mail by rocket was not new. Shortly before noon on 8 June 1959, off the northern Florida coast under command of her new skipper Robert H. Blount, "Barbero" fired a Regulus cruise missile towards the Naval Auxiliary Air Station, Mayport, Florida. Twenty-two minutes later the training type missile landed at its target; its training-type warhead having been configured to contain two official USPS mail containers.
The Post Office had officially established a branch post office on "Barbero" and delivered some 3,000 pieces of mail to it before "Barbero" left Norfolk, Virginia. The mail consisted entirely of commemorative postal covers addressed to President of the United States Dwight Eisenhower, other government officials, the Postmasters General of all members of the Universal Postal Union, and so on. They contained letters from United States Postmaster General Arthur E. Summerfield. Their postage (four cents domestic, eight cents international) had been cancelled "USS Barbero 8 June 9.30 am 1959" before the boat put to sea. In Mayport, the Regulus was opened and the mail forwarded to the Jacksonville, Florida Post Office for further sorting and routing.
Upon witnessing the missile's landing, Summerfield stated, "This peacetime employment of a guided missile for the important and practical purpose of carrying mail, is the first known official use of missiles by any Post Office Department of any nation." Summerfield proclaimed the event to be "of historic significance to the peoples of the entire world," and predicted that "before man reaches the moon, mail will be delivered within hours from New York to California, to Britain, to India or Australia by guided missiles. We stand on the threshold of rocket mail."
Decommissioning and disposal.
Regulus was superseded by the Polaris missile in 1964. "Barbero" ended her nuclear strategic deterrent patrols and was decommissioned on 30 June 1964. She was struck from the Naval Vessel Registry on 1 July 1964, prior to being used as a target and sunk by the submarine off Pearl Harbor, Hawaii, on 7 October 1964.
Awards.
"Barbero" received two battle stars for her World War II service.

</doc>
<doc id="67035" url="https://en.wikipedia.org/wiki?curid=67035" title="Hemisphere">
Hemisphere

Hemisphere may refer to:
As half of the Earth:
As half of the brain:

</doc>
<doc id="67039" url="https://en.wikipedia.org/wiki?curid=67039" title="Active solar">
Active solar

Solar hot water systems use pumps or fans to circulate fluid (often a mixture of water and glycol to prevent freezing during winter periods) or air, through solar collectors, and are therefore classified under active solar technology.
Some of the basic benefits of active systems is that controls (usually electrical) can be used to maximize their effectiveness. For example, a passive solar thermal array which does not rely on pumps and sensors will only start circulating when a certain amount of internal energy has built up in the system. Using sensors and pumps, a relatively small amount of energy (i.e. that used to power a pump and controller) can harvest a far larger amount of available thermal energy by switching on as soon as a useful temperature differential becomes present. Controls also allow a greater variety of choices for utilizing the energy that becomes available. For example, a solar thermal array could heat a swimming pool on a relatively cool morning where heating a domestic hot water cylinder was impractical due to the different stored water temperatures. Later in the day as the temperature rises the controls could be used to switch the solar heated water over to the cylinder instead.
The downside to Active Solar systems is that the external power sources can fail (probably rendering them useless), and the controls need maintenance.
Most solar collectors are fixed in their array position mounting, but can have a higher performance if they track the path of the sun through the sky (however it is unusual for thermal collectors to be mounted in this way). Solar trackers, used to orient solar arrays may be driven by either passive or active technology, and can have a significant gain in energy yield over the course of a year when compared to a fixed array. Again passive solar tracking would rely on the inherent thermo-dynamic properties of the materials used in the system rather than an external power source to generate its tracking movement. Active Solar Tracking would utilise sensors and motors track the path of the sun across the sky. This action can be caused by geographical and time data being programmed into the controls. However, some systems actually track the brightest point in the sky using light sensors, and manufacturers claim this can add a significant extra yield over and above geographical tracking.
See also.
Passive examples:
-a tree in front of windows (a deciduous tree) 
-windows (large facing south, smaller facing north)
Active examples:
-collectors (solar panels)
-storage (water heaters)
-dispurts(pumps)

</doc>
<doc id="67042" url="https://en.wikipedia.org/wiki?curid=67042" title="Weatherization">
Weatherization

Weatherization (American English) or weatherproofing (British English) is the practice of protecting a building and its interior from the elements, particularly from sunlight, precipitation, and wind, and of modifying a building to reduce energy consumption and optimize energy efficiency.
Weatherization is distinct from building insulation, although building insulation requires weatherization for proper functioning. Many types of insulation can be thought of as weatherization, because they block drafts or protect from cold winds. Whereas insulation primarily reduces "conductive" heat flow, weatherization primarily reduces "convective" heat flow.
In the United States, buildings use one third of all energy consumed and two thirds of all electricity. Due to the high energy usage, they are a major source of the pollution that causes urban air quality problems and pollutants that contribute to climate change. Building energy usage accounts for 49 percent of sulfur dioxide emissions, 25 percent of nitrous oxide emissions, and 10 percent of particulate emissions.
Weatherization procedures.
Typical weatherization procedures include:
The phrase "whole-house weatherization" extends the traditional definition of weatherization to include installation of modern, energy-saving heating and cooling equipment, or repair of old, inefficient equipment (furnaces, boilers, water heaters, programmable thermostats, air conditioners, and so on). The "Whole-House" approach also looks at how the house performs as a system.
Air Quality.
Weatherization generally does not cause indoor air problems by adding new pollutants to the air. (There are a few exceptions, such as caulking, that can sometimes emit pollutants.) However, measures such as installing storm windows, weather stripping, caulking, and blown-in wall insulation can reduce the amount of outdoor air infiltrating into a home. Consequently, after weatherization, concentrations of indoor air pollutants from sources inside the home can increase.
Weatherization can have a negative impact on indoor air quality, especially among occupants with respiratory illnesses. This occurs because of a decrease in air exchange in the home, and resulting increase in moisture. This leads to higher concentrations of pollutants in the air.
US Weatherization Assistance Program.
Weatherization has become increasingly high-profile as the cost of home heating has risen. The US Weatherization Assistance Program (WAP) was created within the US Department of Energy in 1976 to help low-income families reduce energy consumption and costs across all fifty states, the District of Columbia, including Native American tribes.
The US Department of Energy estimates that over 6.2 million homes have been weatherized, saving 30.5 MBtu of energy per household each year. It estimates weatherization returns $2.69 for each dollar spent on the program, realized in energy and non-energy benefits. Families whose homes are weatherized are expected to save $358 on their first year's utility bills.
Many state Low Income Home Energy Assistance Programs work side by side with WAP to provide both immediate and long term solutions to energy poverty.

</doc>
<doc id="67043" url="https://en.wikipedia.org/wiki?curid=67043" title="Thermal insulation">
Thermal insulation

Thermal insulation is the reduction of heat transfer (the transfer of thermal energy between objects of differing temperature) between objects in thermal contact or in range of radiative influence. Thermal insulation can be achieved with specially engineered methods or processes, as well as with suitable object shapes and materials.
Heat flow is an inevitable consequence of contact between objects of differing temperature. Thermal insulation provides a region of insulation in which thermal conduction is reduced or thermal radiation is reflected rather than absorbed by the lower-temperature body.
The insulating capability of a material is measured with thermal conductivity (k). Low thermal conductivity is equivalent to high insulating capability (R-value). In thermal engineering, other important properties of insulating materials are product density (ρ) and specific heat capacity (c).
Definition.
Insulation.
Low thermal conductivity ("k") materials reduce heat fluxes. The smaller the "k" value, the larger the corresponding thermal resistance ("R") value. Thermal conductivity is measured in watts-per-meter per kelvin (W·m−1·K−1), represented as "k". As the thickness of insulating material increases, the thermal resistance—or R-value—also increases.
For a cylinder, the convective thermal resistance is inversely proportional to the surface area and therefore the radius of the cylinder, while the thermal resistance of a cylindrical shell (the insulation layer) depends on the ratio between outside and inside radius, not on the radius itself. Suppose for example that we double the outside radius of a cylinder by applying insulation. We have added a fixed amount of conductive resistance (equal to ln(2)/(2πkL)) but at the same time we have halved the value of the convective resistance. Because convective resistance tends to infinity when the radius approaches zero, at small enough radiuses the decrease in convective resistance will be larger than the added conductive resistance, resulting in lower total resistance. 
This implies that adding insulation actually increases the heat transfer, until a critical radius is reached, at which point the heat transfer is at maximum. Above this critical radius, added insulation decreases the heat transfer. For insulated cylinders, the critical radius is given by the equation 
This equation shows that the critical radius depends only on the heat transfer coefficient and the thermal conductivity of the insulation. If the radius of the uninsulated cylinder is larger than the critical radius for insulation, the addition of any amount of insulation will decrease the heat transfer.
Applications.
Clothing and natural animal insulation in birds and mammals.
Gases possess poor thermal conduction properties compared to liquids and solids, and thus makes a good insulation material if they can be trapped. In order to further augment the effectiveness of a gas (such as air) it may be disrupted into small cells which cannot effectively transfer heat by natural convection. Convection involves a larger bulk flow of gas driven by buoyancy and temperature differences, and it does not work well in small cells where there is little density difference to drive it. 
In order to accomplish gas cell formation in man-made thermal insulation, glass and polymer materials can be used to trap air in a foam-like structure. This principle is used industrially in building and piping insulation such as (glass wool), cellulose, rock wool, polystyrene foam (styrofoam), urethane foam, vermiculite, perlite, and cork. Trapping air is also the principle in all highly insulating clothing materials such as wool, down feathers and fleece. 
The air-trapping property is also the insulation principle employed by homeothermic animals to stay warm, for example down feathers, and insulating hair such as natural sheep's wool. In both cases the primary insulating material is air, and the polymer used for trapping the air is natural keratin protein.
Buildings.
Maintaining acceptable temperatures in buildings (by heating and cooling) uses a large proportion of global energy consumption. Building insulations also commonly use the principle of small trapped air-cells as explained above, e.g. fiberglass (specifically glass wool), cellulose, rock wool, polystyrene foam, urethane foam, vermiculite, perlite, cork, etc. 
When well insulated, a building:
Many forms of thermal insulation also reduce noise and vibration, both coming from the outside and from other rooms inside a building, thus producing a more comfortable environment.
Window insulation film can be applied in weatherization applications to reduce incoming thermal radiation in summer and loss in winter.
In industry, energy has to be expended to raise, lower, or maintain the temperature of objects or process fluids. If these are not insulated, this increases the energy requirements of a process, and therefore the cost and environmental impact.
Mechanical systems.
Space heating and cooling systems distribute heat throughout buildings by means of pipe or ductwork. Insulating these pipes using pipe insulation reduces energy into unoccupied rooms and prevents condensation from occurring on cold and chilled pipework.
Pipe insulation is also used on water supply pipework to help delay pipe freezing for an acceptable length of time.
Spacecraft.
Launch and re-entry place severe mechanical stresses on spacecraft, so the strength of an insulator is critically important (as seen by the failure of insulating foam on the Space Shuttle Columbia). Re-entry through the atmosphere generates very high temperatures due to compression of the air at high speeds. Insulators must meet demanding physical properties beyond their thermal transfer retardant properties. E.g. reinforced carbon-carbon composite nose cone and silica fiber tiles of the Space Shuttle. See also Insulative paint.
Automotive.
Internal combustion engines produce a lot of heat during their combustion cycle. This can have a negative effect when it reaches various heat-sensitive components such as sensors, batteries and starter motors. As a result, thermal insulation is necessary to prevent the heat from the exhaust reaching these components.
High performance cars often use thermal insulation as a means to increase engine performance.
Factors influencing performance.
Insulation performance is influenced by many factors the most prominent of which include:
It is important to note that the factors influencing performance may vary over time as material ages or environmental conditions change.
Calculating requirements.
Industry standards are often rules of thumb, developed over many years, that offset many conflicting goals: what people will pay for, manufacturing cost, local climate, traditional building practices, and varying standards of comfort. Both heat transfer and layer analysis may be performed in large industrial applications, but in household situations (appliances and building insulation), air tightness is the key in reducing heat transfer due to air leakage (forced or natural convection). Once air tightness is achieved, it has often been sufficient to choose the thickness of the insulating layer based on rules of thumb. Diminishing returns are achieved with each successive doubling of the insulating layer.
It can be shown that for some systems, there is a minimum insulation thickness required for an improvement to be realized.

</doc>
<doc id="67046" url="https://en.wikipedia.org/wiki?curid=67046" title="Thermal mass">
Thermal mass

In building design, thermal mass is a property of the mass of a building which enables it to store heat, providing "inertia" against temperature fluctuations. It is sometimes known as the thermal flywheel effect. For example, when outside temperatures are fluctuating throughout the day, a large thermal mass within the insulated portion of a house can serve to "flatten out" the daily temperature fluctuations, since the thermal mass will absorb thermal energy when the surroundings are higher in temperature than the mass, and give thermal energy back when the surroundings are cooler, without reaching thermal equilibrium. This is distinct from a material's insulative value, which reduces a building's thermal conductivity, allowing it to be heated or cooled relatively separate from the outside, or even just retain the occupants' thermal energy longer.
Scientifically, thermal mass is equivalent to thermal capacitance or heat capacity, the ability of a body to store thermal energy. It is typically referred to by the symbol "Cth" and measured in units of J/°C or J/K (which are equivalent). Thermal mass may also be used for bodies of water, machines or machine parts, living things, or any other structure or body in engineering or biology. In those contexts, the term "heat capacity" is typically used instead.
Background.
The equation relating thermal energy to thermal mass is:
where "Q" is the thermal energy transferred, "Cth" is the thermal mass of the body, and Δ"T" is the change in temperature.
For example, if 250 J of heat energy is added to a copper gear with a thermal mass of 38.46 J/°C, its temperature will rise by 6.50 °C.
If the body consists of a homogeneous material with sufficiently known physical properties, the thermal mass is simply the mass of material present times the specific heat capacity of that material. For bodies made of many materials, the sum of heat capacities for their pure components may be used in the calculation, or in some cases (as for a whole animal, for example) the number may simply be measured for the entire body in question, directly.
As an extensive property, heat capacity is characteristic of an object; its corresponding intensive property is specific heat capacity, expressed in terms of a measure of the amount of material such as mass or number of moles, which must be multiplied by similar units to give the heat capacity of the entire body of material. Thus the heat capacity can be equivalently calculated as the product of the mass "m" of the body and the specific heat capacity "c" for the material, or the product of the number of moles of molecules present "n" and the molar specific heat capacity formula_2. For discussion of "why" the thermal energy storage abilities of pure substances vary, see factors that affect specific heat capacity.
For a body of uniform composition, formula_3 can be approximated by
where formula_5 is the mass of the body and formula_6 is the isobaric specific heat capacity of the material averaged over temperature range in question. For bodies composed of numerous different materials, the thermal masses for the different components can just be added together.
Thermal mass in buildings.
Thermal mass is effective in improving building comfort in any place that experiences these types of daily temperature fluctuations—both in winter as well as in summer.
When used well and combined with passive solar design, thermal mass can play an important role in major reductions to energy use in active heating and cooling systems.
The terms "heavy-weight" and "light-weight" are often used to describe buildings with different thermal mass strategies, and affects the choice of numerical factors used in subsequent calculations to describe their thermal response to heating and cooling.
In building services engineering, the use of dynamic simulation computational modelling software has allowed for the accurate calculation of the environmental performance within buildings with different constructions and for different annual climate data sets. This allows the architect or engineer to explore in detail the relationship between heavy-weight and light-weight constructions, as well as insulation levels, in reducing energy consumption for mechanical heating or cooling systems, or even removing the need for such systems altogether.
Properties required for good thermal mass.
Ideal materials for thermal mass are those materials that have:
Any solid, liquid, or gas that has mass will have some thermal mass. A common misconception is that only concrete or earth soil has thermal mass; even air has thermal mass (although very little).
A table of volumetric heat capacity for building materials is available here, but note that their definition of thermal mass is slightly different.
Use of thermal mass in different climates.
The correct use and application of thermal mass is dependent on the prevailing climate in a district.
Temperate and cold temperate climates.
Solar-exposed thermal mass.
Thermal mass is ideally placed within the building and situated where it still can be exposed to low-angle winter sunlight (via windows) but insulated from heat loss. In summer the same thermal mass should be obscured from higher-angle summer sunlight in order to prevent overheating of the structure.
The thermal mass is warmed passively by the sun or additionally by internal heating systems during the day. Thermal energy stored in the mass is then released back into the interior during the night. It is essential that it be used in conjunction with the standard principles of passive solar design.
Any form of thermal mass can be used. A concrete slab foundation either left exposed or covered with conductive materials, e.g. tiles, is one easy solution. Another novel method is to place the masonry facade of a timber-framed house on the inside ('reverse-brick veneer'). Thermal mass in this situation is best applied over a large area rather than in large volumes or thicknesses. 7.5–10 cm (3-4") is often adequate.
Since the most important source of thermal energy is the Sun, the ratio of glazing to thermal mass is an important factor to consider. Various formulas have been devised to determine this. As a general rule, additional solar-exposed thermal mass needs to be applied in a ratio from 6:1 to 8:1 for any area of sun-facing (north-facing in Southern Hemisphere or south-facing in Northern Hemisphere) glazing above 7% of the total floor area. For example, a 200 m2 house with 20 m2 of sun-facing glazing has 10% of glazing by total floor area; 6 m2 of that glazing will require additional thermal mass. Therefore, using the 6:1 to 8:1 ratio above, an additional 36–48 m2 of solar-exposed thermal mass is required. The exact requirements vary from climate to climate.
Thermal mass for limiting summertime overheating.
Thermal mass is ideally placed within a building where it is shielded from direct solar gain but exposed to the building occupants. It is therefore most commonly associated with solid concrete floor slabs in naturally ventilated or low-energy mechanically ventilated buildings where the concrete soffit is left exposed to the occupied space.
During the day heat is gained from the sun, the occupants of the building, and any electrical lighting and equipment, causing the air temperatures within the space to increase, but this heat is absorbed by the exposed concrete slab above, thus limiting the temperature rise within the space to be within acceptable levels for human thermal comfort. In addition the lower surface temperature of the concrete slab also absorbs radiant heat directly from the occupants, also benefiting their thermal comfort.
By the end of the day the slab has in turn warmed up, and now, as external temperatures decrease, the heat can be released and the slab cooled down, ready for the start of the next day. However this "regeneration" process is only effective if the building ventilation system is operated at night to carry away the heat from the slab. In naturally ventilated buildings it is normal to provide automated window openings to facilitate this process automatically.
Hot, arid climates (e.g. desert).
This is a classical use of thermal mass. Examples include adobe or rammed earth houses. Its function is highly dependent on marked diurnal temperature variations. The wall predominantly acts to retard heat transfer from the exterior to the interior during the day. The high volumetric heat capacity and thickness prevents thermal energy from reaching the inner surface. When temperatures fall at night, the walls re-radiate the thermal energy back into the night sky. In this application it is important for such walls to be massive to prevent heat transfer into the interior.
Hot humid climates (e.g. sub-tropical and tropical).
The use of thermal mass is the most challenging in this environment where night temperatures remain elevated. Its use is primarily as a temporary heat sink. However, it needs to be strategically located to prevent overheating. It should be placed in an area that is not directly exposed to solar gain and also allows adequate ventilation at night to carry away stored energy without increasing internal temperatures any further. If to be used at all it should be used in judicious amounts and again not in large thicknesses.
Cold incoming tap water may be piped through radiators to draw summer thermal energy from the air. In most areas, its initial temperature is degrees. Since the existing plumbing is deep underground, it's well insulated from the heat of the day.
Seasonal energy storage.
If enough mass is used it can create a seasonal advantage. That is, it can heat in the winter and cool in the summer. This is sometimes called passive annual heat storage or PAHS. The PAHS system has been successfully used at 7000 ft. in Colorado and in a number of homes in Montana. The Earthships of New Mexico utilize passive heating and cooling as well as using recycled tires for foundation wall yielding a maximum PAHS/STES. It has also been used successfully in the UK at Hockerton Housing Project.

</doc>
<doc id="67047" url="https://en.wikipedia.org/wiki?curid=67047" title="Soichiro Honda">
Soichiro Honda

Early years.
Honda was born in Tenryū, Shizuoka, a small village under Mount Fuji near Hamamatsu on November 17, 1906. He spent his early childhood helping his father, Gihei, a blacksmith, with his bicycle repair business. At the time his mother, Mika, was a weaver. Honda was not interested in traditional education. His school handed grade reports to the children, but required that they be returned stamped with the family seal, to make sure that a parent had seen it. Soichiro created a stamp to forge his family seal out of a used rubber bicycle pedal cover. The fraud was soon discovered when Honda started to make forged stamps for other children. Honda was unaware that the stamp was supposed to be mirror-imaged. His family name 本田 was symmetrical when written vertically, so it did not cause a problem, but some of other children's family names were not.
Even as a toddler, Honda had been thrilled by the first car that was ever seen in his village, and often used to say in later life that he could never forget the smell of oil it gave off. 
Soichiro once borrowed one of his father's bicycles to see a demonstration of an airplane made by pilot Art Smith, which cemented his love for machinery and invention. 
At 15, without any formal education, Honda left home and headed to Tokyo to look for work. He obtained an apprenticeship at a garage in 1922, and after some hesitation over his employment, he stayed for six years, working as a car mechanic before returning home to start his own auto repair business in 1928 at the age of 22.
Honda raced a turbocharged Ford in the "1st Japan Automobile Race" at Tamagawa Speedway in 1936. He crashed and seriously injured his left eye. His brother was also injured. After that, he quit racing.
Development of Honda Motor Co., Ltd..
In 1937, Honda founded Tōkai Seiki to produce piston rings for Toyota. During World War II, a US B-29 bomber attack destroyed Tōkai Seiki's Yamashita plant in 1944 and the Itawa plant collapsed in the 1945 Mikawa earthquake. After the war, Honda sold the salvageable remains of the company to Toyota for ¥450,000 and used the proceeds to found the Honda Technical Research Institute in October 1946. In 1948 he started producing a complete motorized bicycle, the Type A, which was driven by the first mass-produced engine designed by Honda, and was sold until 1951. The Type D in 1949 was a true motorcycle with a pressed-steel frame designed and produced by Honda and with a 2-stroke, engine, and became the very first model in the Dream series of motorcycles. The Society of Automotive Engineers of Japan lists both the Type A and the Type D models as two of their "240 Landmarks of Japanese Automotive Technology".
As president of the Honda Motor Company, Soichiro Honda turned the company into a billion-dollar multinational that produced the best-selling motorcycles in the world. Honda's engineering and marketing skills resulted in Honda motorcycles outselling Triumph and Harley-Davidson in their respective home markets. The next year, Honda was reacquainted with Takeo Fujisawa, whom he knew during his days as a supplier of piston rings to Nakajima Aircraft Company. Honda hired Fujisawa, who oversaw the financial side of the company and helped the firm expand. In 1959, Honda Motorcycles opened its first dealership in the United States.
Honda remained president until his retirement in 1973, where he stayed on as director and was appointed "supreme advisor" in 1983. His status was such that "People" magazine placed him on their "25 Most Intriguing People of the Year" list for 1980, dubbing him "the Japanese Henry Ford." In retirement, Honda busied himself with work connected with the Honda Foundation.
Last years.
Even at his advanced age, Soichiro and his wife Sachi both held private pilot's licenses. He also enjoyed skiing, hang-gliding and ballooning at 77, and he was a highly accomplished artist. He and Takeo Fujisawa made a pact never to force their own sons to join the company. His son, Hirotoshi Honda, was the founder and former CEO of Mugen Motorsports, a tuner for Honda vehicles who also created original racing vehicles. 
ASME established the Soichiro Honda Medal in recognition of Mr. Honda's achievements in 1982; this medal recognizes outstanding achievement or significant engineering contributions in the field of personal transportation. In 1989, he was inducted into the Automotive Hall of Fame near Detroit. Soichiro Honda died on August 5, 1991 of liver failure. He was posthumously appointed to the senior third rank in the order of precedence and appointed a Grand Cordon of the Order of the Rising Sun.

</doc>
<doc id="67049" url="https://en.wikipedia.org/wiki?curid=67049" title="Selaginella">
Selaginella

Selaginella is the sole genus of vascular plants in the family Selaginellaceae, the spikemosses or lesser clubmosses. This family is placed in the class Isoetopsida, distinguished from the sister group Lycopodiopsida by having scale-leaves bearing a ligule and by having spores of two types. They are sometimes included in an informal paraphyletic group called the "fern allies". "S. moellendorffii" is an important model organism. Its genome has been sequenced by the United States Department of Energy's Joint Genome Institute.
Description.
"Selaginella" species are creeping or ascendant plants with simple, scale-like leaves (microphylls) on branching stems from which roots also arise. Unusually for the lycopods, each microphyll contains a branching vascular trace. Each microphyll and sporophyll has a small scale-like outgrowth called a ligule at the base of the upper surface. The plants are heterosporous (megaspores and microspores). Under dry conditions, some species of "Selaginella" roll into brown balls (a phenomenon known as poikilohydry). In this state, they may be uprooted. Under moist conditions the brown balls become green, because of which these are also known as resurrection plants (as in "Selaginella bryopteris").
Generic division.
Many scientists still place the Selaginellales in the class Lycopodiopsida (often misconstructed as "Lycopsida"). Some modern authors recognize three generic divisions of "Selaginella": "Selaginella", "Bryodesma" Sojak 1992, and "Lycopodioides" Boehm 1760. "Lycopodioides" would include the North American species "S. apoda" and "S. eclipes", while "Bryodesma" would include "S. rupestris" (as "Bryodesma rupestre"). "Stachygynandrum" is also sometimes used to include the bulk of species.
The first major attempt to define and subdivide the group was by Palisot de Beauvois in 1803-1805. He established the genus "Selaginella" as a monotypic genus, and placed the bulk of species in "Stachygynandrum". "Gymnogynum" was another monotypic genus, but that name is superseded by his own earlier name of "Didiclis". This turns out, today, to be a group of around 45-50 species also known as the "Articulatae", since his "Didiclis/Gymnogynum" genus was based on "Selaginella plumosa". He also described the genus "Diplostachyum" to include a group of species similar to "Selaginella apoda". Spring inflated the genus "Selaginella" to hold all selaginelloid species four decades later.
Phylogenetic studies by Korall & Kenrick determined that the "Euselaginella" group, comprising solely the type species, "Selaginella selaginoides" and a closely related Hawaiian species, "Selaginella deflexa", is a basal and anciently diverging sister to all other "Selaginella" species. Beyond this, their study split the remainder of species into two broad groups, one including the "Bryodesma" species, the "Articulatae", section "Ericetorum" Jermy and others, and the other centered on the broad "Stachygynandrum" group.
In the "Manual of Pteridology", the following classification was used by Walton & Alston:
genus: "Selaginella"
Species.
There are about 700 species of "Selaginella", showing a wide range of characters; the genus is overdue for a revision which might include subdivision into several genera. Better-known spikemosses include:
A few species of "Selaginella" are desert plants known as "resurrection plants", because they curl up in a tight, brown or reddish ball during dry times, and uncurl and turn green in the presence of moisture. Other species are tropical forest plants that appear at first glance to be ferns.
Cultivation.
A number of "Selaginella" species are popular plants for cultivation, mostly tropical species. Some of the species popularly cultivated and actively available commercially include:

</doc>
<doc id="67050" url="https://en.wikipedia.org/wiki?curid=67050" title="Tutsi">
Tutsi

The Tutsi (; ), or Abatutsi, are a population inhabiting the African Great Lakes region. Historically, they were often referred to as the Watutsi, Watusi, Wahuma or the Wahima. The Tutsi form a subgroup of the Banyarwanda and the Barundi peoples, who reside primarily in Rwanda and Burundi, but with significant populations also found in Uganda, the Democratic Republic of the Congo and Tanzania. They speak Rwanda-Rundi, a group of Bantu languages.
The Tutsi are the second largest population division among the three largest groups in Rwanda and Burundi; the other two being the Hutu (largest) and the Twa (smallest). Small numbers of Hema, Kiga and Furiiru people also live near the Tutsi in Rwanda. The Northern Tutsi who reside in Rwanda are called Ruguru (Banyaruguru), while southern Tutsi that live in Burundi are known as Hima, and the Tutsi that inhabit the Kivu plateau in the Congo go by Banyamulenge.
Origins and classification.
The definitions of "Hutu" and "Tutsi" people may have changed through time and location. Social structures were not stable throughout Rwanda, even during colonial times under the Belgian rule. The Tutsi aristocracy or elite was distinguished from Tutsi commoners, and wealthy Hutu were often indistinguishable from upper-class Tutsi.
When the European colonists conducted censuses, they wanted to identify the people throughout Rwanda-Burundi according to a simple classification scheme. They defined "Tutsi" as anyone owning more than ten cows (a sign of wealth) or with the physical feature of a longer nose, or longer neck, commonly associated with the Tutsi.
The Europeans believed that some Tutsis had facial characteristics that were generally atypical of other Bantus. They sought to explain these purported divergent physical traits by postulating admixture with or partial descent from migrants of Caucasoid stock, who usually were said to have arrived in the Great Lakes region from the Horn of Africa and/or North Africa.
By contrast, the Europeans considered the majority Hutu to be characteristic Bantu people of Central African origin.
The Tutsi have lived in the areas where they are for 400-500 years, leading to considerable intermarriage with the Hutu, a Bantu people in the area. To note the names Hutu and Bantu are not the same as they imply different things. Due to the history of intermingling and intermarrying of Hutus and Tutsis, ethnographers and historians have lately come to agree that Hutu and Tutsis cannot be properly called distinct ethnic groups.
Genetics.
Y-DNA (paternal lineages).
Modern-day genetic studies of the Y-chromosome generally indicate that the Tutsi, like the Hutu, are largely of Bantu extraction (60%E1b1a, 20% B, 4% E3). Paternal genetic influences associated with the Horn of Africa and North Africa are few (16% E1b1b), and are ascribed to much earlier inhabitants who were assimilated. However, the Tutsi have considerably more Nilo-Saharan paternal lineages (14.9% B) than the Hutu (4.3% B).
Trombetta et al. (2015) found 22.2% of E1b1b in a small sample of Tutsis from Burundi, but no bearers of the haplogroup among the local Hutu and Twa populations. The subclade was of the M293 variety, which suggests that the ancestors of Tutsis in this area may have assimilated some South Cushitic pastoralists.
Autosomal DNA (overall ancestry).
In general, the Tutsi appear to share a close genetic kinship with neighboring Bantu populations, particularly the Hutus. However, it is unclear whether this similarity is primarily due to extensive genetic exchanges between these communities through intermarriage or whether it ultimately stems from common origins:
[...]generations of gene flow obliterated whatever clear-cut physical distinctions may have once existed between these two Bantu peoples – renowned to be height, body build, and facial features. With a spectrum of physical variation in the peoples, Belgian authorities legally mandated ethnic affiliation in the 1920s, based on economic criteria. Formal and discrete social divisions were consequently imposed upon ambiguous biological distinctions. To some extent, the permeability of these categories in the intervening decades helped to reify the biological distinctions, generating a taller elite and a shorter underclass, but with little relation to the gene pools that had existed a few centuries ago. The social categories are thus real, but there is little if any detectable genetic differentiation between Hutu and Tutsi.
Tishkoff et al. (2009) found their mixed Hutu and Tutsi samples from Rwanda to be predominately of Bantu origin, with minor gene flow from Afro-Asiatic communities (17.7% Afro-Asiatic genes found in the mixed Hutu/Tutsi population).
History.
Prior to the arrival of colonists, Rwanda had been ruled by a Tutsi-dominated monarchy after mid-1600. Beginning in about 1880, Roman Catholic missionaries arrived in the Great Lakes region. Later, when German forces occupied the area during World War I, the conflict and efforts for Catholic conversion became more pronounced. As the Tutsi resisted conversion, missionaries found success only among the Hutu. In an effort to reward conversion, the colonial government confiscated traditionally Tutsi land and reassigned it to Hutu tribes.
The area was ruled as a colony by Germany (prior to World War I) and Belgium. Because Tutsis had been the traditional governing elite, both colonial powers kept this system and allowed only the Tutsi to be educated and to participate in the colonial government. Such discriminatory policies engendered resentment.
When the Belgians took over, they believed it could be better governed if they continued to identify the different populations. In the 1920s, they required people to identify with a particular ethnic group and classified them accordingly in censuses. European colonists viewed Africans in general as children who needed to be guided, but noted the Tutsi to be the ruling culture in Rwanda-Burundi.
In 1959, Belgium reversed its stance and allowed the majority Hutu to assume control of the government through universal elections after independence. This partly reflected internal Belgian domestic politics, in which the discrimination against the Hutu majority came to be regarded as similar to oppression within Belgium stemming from the Flemish-Walloon conflict, and the democratization and empowerment of the Hutu was seen as a just response to the Tutsi domination. Belgian policies wavered and flip-flopped considerably during this period leading up to independence of Burundi and Rwanda.
Independence of Rwanda and Burundi (1962).
The Hutu majority in Rwanda had revolted against the Tutsi and was able to take power. Tutsis fled and created exile communities outside Rwanda in Uganda and Tanzania. Since Burundi's independence, more extremist Tutsi came to power and oppressed the Hutus, especially those who were educated. Their actions led to the deaths of up to 200,000 Hutus. Overt discrimination from the colonial period was continued by different Rwandan and Burundian governments, including identity cards that distinguished Tutsi and Hutu.
Burundi genocide (1994).
In 1993, Burundi's first democratically elected president, Melchior Ndadaye, a Hutu, was assassinated by Tutsi officers, as was the person entitled to succeed him under the constitution. This sparked a genocide in Burundi between Hutu political structures and the Tutsi military, in which "possibly as many as 25,000 Tutsi" were murdered by the former and "at least as many" were killed by the latter. Since the 2000 Arusha Peace Process, today in Burundi the Tutsi minority shares power in a more or less equitable manner with the Hutu majority. Traditionally, the Tutsi had held more economic power and controlled the military.
Rwanda genocide (1994).
A similar pattern of events took place in Rwanda, but there the Hutu came to power in 1962. They in turn often oppressed the Tutsi, who fled the country. After the anti-Tutsi violence around 1959-1961, Tutsis fled in large numbers.
These exile Tutsi communities gave rise to Tutsi rebel movements. Exiled Tutsis attacked Rwanda in 1990 with the intention of liberating Rwanda. The fighting culminated in the Hutu mass killings of Tutsi and Hutu in the Rwandan Genocide of 1994, in which the Hutu then in power killed an estimated 500,000–1,000,000 people, largely of Tutsi origin.
At the same time in 1994, the Rwandan Patriotic Front (RPF), mostly made up of diasporic Tutsi in Uganda, advanced to Rwanda. It had experience in organized irregular warfare from the Ugandan Bush War, and got much support from the government of Uganda. The initial RPF advance was halted by the lift of French arms to the Rwandan government. Attempts at peace culminated in the Arusha Accords. The agreement broke down after the assassination of the Rwandan and Burundian Presidents. Victorious in the aftermath of the genocide, the RPF came to power in July 1994.
Language.
Tutsis speak Rwanda-Rundi as their native language. It is a member of the Bantu subgroup of the Niger–Congo family. Rwanda-Rundi is subdivided into the Kinyarwanda and Kirundi dialects, which have been standardized as official languages of Burundi and Rwanda. It is also spoken as a mother tongue by the Hutu and Twa. Additionally, many Tutsis speak French, the third official language of Rwanda and Burundi, as a lingua franca. The Hima speak the same language, but call their language Hima.
Culture.
In the Rwanda territory, from the 15th century until 1961, the Tutsi were ruled by a king (the "mwami"). Belgium abolished the monarchy in response to Hutu activism, following the national referendum that led to independence. By contrast, in the northwestern part of the country (predominantly Hutu), large regional landholders shared power, similar to Bugandan society (in what is now Uganda).
Under their holy king, Tutsi culture traditionally revolved around administering justice and government. They were the only proprietors of cattle, and sustained themselves on their own products. Additionally, their lifestyle afforded them a lot of leisure time, which they spent cultivating the high arts of poetry, weaving and music. Due to the Tutsi's status as a dominant minority vis-a-vis the Hutu farmers and the other local inhabitants, this relationship has been likened to that between lords and serfs in feudal Europe.
According to Fage (2013), the Tutsi are serologically related to Bantu and Nilotic populations. This in turn rules out a possible Cushitic origin for the founding Tutsi-Hima ruling class in the lacustrine kingdoms. However, the royal burial customs of the latter kingdoms are quite similar to those practiced by the former Cushitic Sidama states in the southern Gibe region of Ethiopia. By contrast, Bantu populations to the north of the Tutsi-Hima in Kenya were until modern times essentially without a king, while there were a number of Bantu kingdoms to the south of the Tutsi-Hima in Tanzania, all of which shared the Tutsi-Hima's chieftancy pattern. Since the Cushitic Sidama kingdoms interacted with Nilotic groups, Fage thus proposes that the Tutsi may have descended from one such migrating Nilotic population. The Tutsis' Nilotic ancestors would thereby in earlier times have served as cultural intermediaries, adopting some monarchical traditions from adjacent Cushitic kingdoms and subsequently taking those borrowed customs south with them when they first settled amongst Bantu autochthones in the Great Lakes area.
However, little difference can be ascertained between the cultures today of the Tutsi and Hutu; both groups speak the same Bantu language. The rate of intermarriage between the two groups were traditionally very high, and relations were amicable until the 20th century. Many scholars have concluded that the determination of Tutsi was and is mainly an expression of class or caste, rather than ethnicity.
As noted above, DNA studies show clearly that the peoples are more closely related to each other than to any other. Differences arose due to social constructs, which create greater differences between the groups. During the 1980s, school principals reported that, although secondary school admissions were proportional to the groups within the country and were made by competition within ethnic groups (in accordance with quotas mandated by the Habyarimana government), the students of Tutsi origin (14% of intake) comprised nearly 50% of graduates, on average. This report provoked accusations of tribal favoritism.
Congolese Tutsi.
The "Banyamulenge" are an ethnic group of the Tutsi from the Democratic Republic of the Congo. The term "Banyamulenge", which means people of Mulenge in Kinyarwanda, is rather a collective denomination of descendants of Tutsi migrants from Rwanda most of whom are concentrated on the Itombwe Plateau of South Kivu, close to the Burundi-Congo-Rwanda border and were there for over 500 years.In 1924,more groups of Tutsi migrants added into the highlands of South Kivu, where they were later joined, from 1959 to 1962 by successive waves of Tutsi refugees fleeing persecution. Its use has been controversial, but since the late 1990s, following the Rwanda Genocide, it has been used by Congolese Tutsi, formally known as "Banyarwanda" (people of Rwanda) to avoid being seen as foreigners.
The Banyamulenge have an ambiguous political and social position in Congo, which has been an issue of contention with other ethnic groups. They played a key role in the run-up to the First Congo War in 1996-7 and Second Congo War of 1998-2003.

</doc>
<doc id="67052" url="https://en.wikipedia.org/wiki?curid=67052" title="Nilotic peoples">
Nilotic peoples

Nilotic peoples are peoples indigenous to the Nile Valley that speak Nilotic languages, which comprise a large sub-group of the Nilo-Saharan languages spoken in South Sudan, Uganda, Kenya, and northern Tanzania. In a more general sense, the Nilotic peoples include all descendants of the original Nilo-Saharan speakers. Among these are the Luo, Sara, Masai, Kalenjin, Dinka, Nuer, Shilluk, Ateker, and the Maa-speaking peoples, each of which is a cluster of several ethnic groups.
Nilotes form the majority of the population in South Sudan, an area that is believed to be their original point of dispersal. They also constitute the second-largest group of peoples inhabiting the African Great Lakes region (after the Bantu peoples), with a notable presence in southwestern Ethiopia as well.
Nilotes primarily adhere to Christianity and traditional faiths, including the Dinka religion.
Name.
The terms "Nilotic" and "Nilote" were previously used as racial sub-classifications, based on anthropological observations of the distinct body morphology of many Nilotic speakers. These perceptions were later widely discarded by 20th century social-scientists, but today they again find support in population genetics.
These terms are now mainly used to distinguish "Nilotic people" based on ethnic/linguistic affiliation. Etymologically, the terms Nilotic and Nilote (singular nilot) derive from the Nile Valley; specifically, the Upper Nile and its tributaries, where most Sudanese Nilo-Saharan-speaking people live.
Origins and history.
A Proto-Nilotic unity, separate from an earlier undifferentiated Eastern Sudanic unity, is assumed to have emerged by the 3rd millennium BC. The emergence of the Proto-Nilotes may have been connected with the domestication of livestock. The Eastern Sudanic unity must have been considerably earlier still, perhaps around the 5th millennium BC (while the proposed Nilo-Saharan unity would even date to the Upper Paleolithic about 15kya). The original locus of the early Nilotic speakers was presumably east of the Nile in what is now South Sudan. The Proto-Nilotes of the 3rd millennium BC were pastoralists, while their neighbors, the Proto-Central Sudanic peoples were mostly agriculturalists.
By about the 12th century, Luo peoples occupied the area that now lies in eastern Bahr el Ghazal. A "Nilotic expansion" southward, accompanied by ethnic and linguistic diversification, took place from about the 14th century. The reason was presumably the Islamization of Sudan and the eventual collapse of the Nubian Christian kingdom of Alodia.
Most of the Luo moved to nearly all the countries neighbouring Sudan, resulting in many separate groups with variation in language and tradition as each group moved further away from their kin. A branch of the Luo, the Shilluk (or Chollo) nation, comprising more than one hundred clans and sub-tribes, was founded by a chief named Nyikango sometime in the middle of the 15th century. They evolved a nation with a feudal-style system. Nyikango and his nation moved northward along the Nile, towards Kush. The rest of the Luo groups rejected Nyikango's idea and kept a south and westwards migration.
The further spread of the Nilotes to their modern territories was a process spanning about the 15th to 18th centuries. Thus, the Maasai, according to their own oral history, migrated south from the lower Nile valley north of Lake Turkana in about the 15th century, arriving in a long trunk of land stretching from what is now northern Kenya to what is now central Tanzania during the 17th and 18th centuries.
Ethnic/linguistic divisions.
Languages.
Linguistically, Nilotic people are divided into three sub-groups: 
Ethnic groups.
Nilotic people constitute the bulk of the population of South Sudan. The largest of the Sudanese Nilotic peoples is the Dinka, which includes as many as twenty-five ethnic subdivisions. The next largest group is the Nuer, followed by the Shilluk.
The Nilotic people in Uganda include the Luo (Acholi, Alur and Adhola), Ateker (Teso and Karamojong), Lango and Kumam.
In East Africa, the Nilotes are often subdivided into three general groups:
Culture and religion.
Most Nilotes practice pastoralism. Some tribes are also known for a tradition of cattle raiding.
Nilotes in East Africa have through interaction adopted many customs and practices from neighboring Southern Cushitic groups. The latter include the age set system of social organization, circumcision, and vocabulary terms.
In terms of religious beliefs, Nilotes primarily adhere to traditional faiths and Christianity. There is a pantheon of deities in the Dinka religion. The Supreme, Creator God is Nhialic, who is the God of the sky and rain, and the ruler of all the spirits. He is believed to be present in all of creation, and to control the destiny of every human, plant and animal on Earth. Nhialic is also known as Jaak, Juong or Dyokin by other Nilotic groups, such as the Nuer and Shilluk. Dengdit or Deng, is the sky God of rain and fertility, empowered by Nhialic. Deng's mother is Abuk, the patron Goddess of gardening and all women, represented by a snake. Garang, another deity, is believed or assumed by some Dinka to be a God suppressed by Deng whose spirits can cause most Dinka women, and some men, to scream. The term "Jok" refers to a group of ancestral spirits.
In the Lotuko mythology, the chief God is called Ajok. He is generally seen as kind and benevolent, but can be angered. He once reportedly answered a woman's prayer for the resurrection of her son. Her husband, however, was angry and re-killed the child. According to the Lotuko religion, Ajok was annoyed by his actions and swore to never resurrect any Lotuko again, and in this manner, death was said to have become permanent.
Genetics.
Y DNA.
A Y-chromosome study by Wood et al. (2005) tested various populations in Africa for paternal lineages, including 26 Maasai and 9 Luo from Kenya and 9 Alur from the Democratic Republic of Congo. The signature Nilotic paternal marker Haplogroup A3b2 was observed in 27% of the Maasai, 22% of the Alur, and 11% of the Luo. Haplogroup B, another characteristically Nilotic paternal marker according to Gomes et al. (2010), was found in 22% of Wood et al.'s Luo samples and 8% of the studied Maasai. The E1b1b haplogroup was also observed in 50% of the Maasai, which is indicative of substantial gene flow into this population from Cushitic males. In addition, 67% of the Alur samples possessed the Sub-Saharan E2 haplogroup.
Another study by Hassan et al. (2008) analysed the Y-DNA of populations in the Sudan region, with various local Nilotic groups included for comparison. The researchers found the signature Nilotic A and B clades to be the most common paternal lineages amongst the Nilo-Saharan speakers, except those inhabiting western Sudan, where an appreciable North African influence was noted. Haplogroup A was observed amongst 62% of Dinka, 53.3% of Shilluk, 46.4% of Nuba, 33.3% of Nuer, 31.3% of Fur and 18.8% of Masalit. Haplogroup B was found in 50% of Nuer, 26.7% of Shilluk, 23% of Dinka, 14.3% of Nuba, 3.1% of Fur and 3.1% of Masalit. The E1b1b clade was also observed in 71.9% of the Masalit, 59.4% of the Fur, 39.3% of the Nuba, 20% of the Shilluk, 16.7% of the Nuer, and 15% of the Dinka. Hassan et al. attributed the atypically high frequencies of the haplogroup in the Masalit to either a recent population bottleneck that likely altered the community's original haplogroup diversity or to geographical proximity to E1b1b's place of origin in North Africa, where the researchers suggest that the clade "might have been brought to Sudan from[...] after the progressive desertification of the Sahara around 6,000–8,000 years ago". Henn et al. (2008) similarly observed Afro-Asiatic influence in the Nilotic Datog of northern Tanzania, 43% of whom carried the M293 sub-clade of E1b1b.
mtDNA.
Unlike their paternal DNA, the maternal lineages of Nilotes in general show low-to-negligible amounts of Afro-Asiatic and other extraneous influences. An mtDNA study by Castri et al. (2008) examined the maternal ancestry of various Nilotic populations in Kenya, with Turkana, Samburu, Maasai and Luo individuals sampled. Almost all of the tested Nilotes belonged to various Sub-Saharan macro-haplogroup L sub-clades, including L0, L2, L3, L4 and L5. Low levels of maternal gene flow from North Africa and the Horn of Africa were also observed in a few groups, mainly via the presence of mtDNA haplogroup M and haplogroup I lineages in about 12.5% of the Maasai and 7% of the Samburu samples, respectively.
Autosomal DNA.
The autosomal DNA of Nilotic peoples has been examined in a comprehensive study by Tishkoff et al. (2009) on the genetic affiliations of various populations in Africa. According to the researchers, Nilotes generally form their own Sub-Saharan genetic cluster. The authors also found that certain Nilotic populations in the eastern Great Lakes region, such as the Maasai, showed some additional Afro-Asiatic affinities due to repeated assimilation of Cushitic-speaking peoples over the past 5000 or so years.
Physiology.
Physically, Nilotes are noted for their typically very dark skin color and slender, tall bodies. They often possess exceptionally long limbs, particularly vis-a-vis the distal segments (forearms, calves). This characteristic is thought to be a climatic adaptation to allow their bodies to shed heat more efficiently.
Sudanese Nilotes are regarded as one of the tallest peoples in the world. Roberts and Bainbridge (1963) reported average values of 182.6 cm (71.9") for height and 58.8 kg (129.6 lbs) for weight in a sample of Sudanese Shilluk. Another sample of Sudanese Dinka had a stature/weight ratio of 181.9 cm/58.0 kg (71.6"/127.9 lbs), with an extremely ectomorphic somatotype of 1.6-3.5-6.2.
In terms of facial features, Hiernaux (1975) observed that the nasal profile most common amongst Nilotic populations is broad, with characteristically high index values ranging from 86.9 to 92.0. He also reported that lower nasal indices are often found amongst Nilotes who inhabit the more southerly Great Lakes region, such as the Maasai, a fact which he attributed to genetic differences.
Additionally, it has been remarked that the Nilotic groups presently inhabiting the African Great Lakes region are sometimes also smaller in stature than those residing in the Sudan region. Campbell et al. (2006) recorded measurements of 172.0 cm/53.6 kg (67.7"/118.2 lbs) in a sample of agricultural Turkana in northern Kenya, and of 174.9 cm/53.0 kg (68.8"/116.8 lbs) in pastoral Turkana. Hiernaux similarly listed a height of 172.7 cm (68") for Maasai in southern Kenya, with an extreme trunk/leg length ratio of 47.7.
Many Nilotic groups also excel in long and middle distance running. It has been argued that this sporting prowess stems from their exceptional running economy, which in turn is a function of slim body morphology and slender legs. A study by Pitsiladis et al. (2006) questioning 404 elite Kenyan distance runners found that 76% of the international-class respondents hailed from the Kalenjin ethnic group and that 79% spoke a Nilotic language.

</doc>
<doc id="67054" url="https://en.wikipedia.org/wiki?curid=67054" title="Diaphragm">
Diaphragm

Diaphragm () may refer to any of the following:

</doc>
<doc id="67055" url="https://en.wikipedia.org/wiki?curid=67055" title="Iron Man">
Iron Man

Iron Man (Tony Stark) is a fictional superhero appearing in American comic books published by Marvel Comics, as well as its associated media. The character was created by writer and editor Stan Lee, developed by scripter Larry Lieber, and designed by artists Don Heck and Jack Kirby. He made his first appearance in "Tales of Suspense" #39 (cover dated March 1963).
An American billionaire playboy, business magnate, and ingenious engineer, Tony Stark suffers a severe chest injury during a kidnapping in which his captors attempt to force him to build a weapon of mass destruction. He instead creates a powered suit of armor to save his life and escape captivity. Later, Stark augments his suit with weapons and other technological devices he designed through his company, Stark Industries. He uses the suit and successive versions to protect the world as Iron Man, while at first concealing his true identity. Initially, Iron Man was a vehicle for Stan Lee to explore Cold War themes, particularly the role of American technology and business in the fight against communism. Subsequent re-imaginings of Iron Man have transitioned from Cold War themes to contemporary concerns, such as corporate crime and terrorism.
Throughout most of the character's publication history, Iron Man has been a founding member of the superhero team the Avengers and has been featured in several incarnations of his own various comic book series. Iron Man has been adapted for several animated TV shows and films. The character is portrayed by Robert Downey Jr. in the live action film "Iron Man" (2008), which was a critical and box office success. Downey, who received much acclaim for his performance, reprised the role in a cameo in "The Incredible Hulk" (2008), two "Iron Man" sequels "Iron Man 2" (2010) and "Iron Man 3" (2013), "The Avengers" (2012) and ' (2015), and will do so again in ' (2016), ' (2017) and both parts of ' (2018/2019) in the Marvel Cinematic Universe.
Iron Man was ranked 12th on IGN's "Top 100 Comic Book Heroes" in 2011, and third in their list of "The Top 50 Avengers" in 2012.
Publication history.
Premiere.
Iron Man's Marvel Comics premiere in "Tales of Suspense" #39 (cover dated March 1963) was a collaboration among editor and story-plotter Stan Lee, scripter Larry Lieber, story-artist Don Heck, and cover-artist and character-designer Jack Kirby. In 1963, Lee had been toying with the idea of a businessman superhero. He wanted to create the "quintessential capitalist", a character that would go against the spirit of the times and Marvel's readership. Lee said,
He set out to make the new character a wealthy, glamorous ladies' man, but one with a secret that would plague and torment him as well. Writer Gerry Conway said, "Here you have this character, who on the outside is invulnerable, I mean, just can't be touched, but inside is a wounded figure. Stan made it very much an in-your-face wound, you know, his heart was broken, you know, literally broken. But there's a metaphor going on there. And that's, I think, what made that character interesting." Lee based this playboy's looks and personality on Howard Hughes, explaining, "Howard Hughes was one of the most colorful men of our time. He was an inventor, an adventurer, a multi-billionaire, a ladies' man and finally a nutcase." "Without being crazy, he was Howard Hughes," Lee said.
While Lee intended to write the story himself, a minor deadline emergency eventually forced him to hand over the premiere issue to Lieber, who fleshed out the story. The art was split between Kirby and Heck. "He designed the costume," Heck said of Kirby, "because he was doing the cover. The covers were always done first. But I created the look of the characters, like Tony Stark and his secretary Pepper Potts." In a 1990 interview, when asked if he had "a specific model for Tony Stark and the other characters?", Heck replied "No, I would be thinking more along the lines of some characters I like, which would be the same kind of characters that Alex Toth liked, which was an Errol Flynn type." Iron Man first appeared in 13- to 18-page stories in "Tales of Suspense," which featured anthology science fiction and supernatural stories. The character's original costume was a bulky gray armored suit, replaced by a golden version in the second story (issue #40, April 1963). It was redesigned as sleeker, red-and-golden armor in issue #48 (Dec. 1963) by that issue's interior artist, Steve Ditko, although Kirby drew it on the cover. As Heck recalled in 1985, "he second costume, the red and yellow one, was designed by Steve Ditko. I found it easier than drawing that bulky old thing. The earlier design, the robot-looking one, was more Kirbyish."
In his premiere, Iron Man was an anti-communist hero, defeating various Vietnamese agents. Lee later regretted this early focus. Throughout the character’s comic book series, technological advancement and national defense were constant themes for Iron Man, but later issues developed Stark into a more complex and vulnerable character as they depicted his battle with alcoholism (as in the "Demon in a Bottle" storyline) and other personal difficulties.
From issue #59 (Nov. 1964) to its final issue #99 (March 1968), the anthological science-fiction backup stories in "Tales of Suspense" were replaced by a feature starring the superhero Captain America. Lee and Heck introduced several adversaries for the character including the Mandarin in issue #50 (Feb. 1964), the Black Widow in #52 (April 1964) and Hawkeye five issues later.
Lee said that "of all the comic books we published at Marvel, we got more fan mail for Iron Man from women, from females, than any other title...We didn't get much fan mail from girls, but whenever we did, the letter was usually addressed to Iron Man."
Lee and Kirby included Iron Man in "The Avengers" #1 (Sept. 1963) as a founding member of the superhero team. The character has since appeared in every subsequent volume of the series.
Writers have updated the war and locale in which Stark is injured. In the original 1963 story, it was the Vietnam War. In the 1990s, it was updated to be the first Gulf War, and later updated again to be the war in Afghanistan. Stark's time with the Asian Nobel Prize-winning scientist Ho Yinsen is consistent through nearly all incarnations of the Iron Man origin, depicting Stark and Yinsen building the original armor together. One exception is the direct-to-DVD animated feature film "The Invincible Iron Man", in which the armor Stark uses to escape his captors is not the first Iron Man suit.
Themes.
The original "Iron Man" title explored Cold War themes, as did other Stan Lee projects in the early years of Marvel Comics. Where "The Fantastic Four" and "The Incredible Hulk" respectively focused on American domestic and government responses to the Communist threat, "Iron Man" explored industry's role in the struggle. Tony Stark's real-life model, Howard Hughes, was a significant defense contractor who developed new weapons technologies. Hughes was an icon both of American individualism and of the burdens of fame.
Historian Robert Genter, in "The Journal of Popular Culture", writes that Tony Stark specifically presents an idealized portrait of the American inventor. Where earlier decades had seen important technological innovations come from famous "individuals" (e.g., Nikola Tesla, Thomas Edison, Alexander Graham Bell, the Wright brothers), the 1960s saw new technologies (including weapons) being developed mainly by the research teams of "corporations". As a result, little room remained for the inventor who wanted credit for, and creative and economic control over, his/her own creations.
Issues of entrepreneurial autonomy, government supervision of research, and ultimate loyalty figured prominently in early "Iron Man" stories — the same issues affecting American scientists and engineers of that era. Tony Stark, writes Genter, is an inventor who finds motive in his emasculation as an autonomous creative individual. This blow is symbolized by his chest wound, inflicted at the moment he is forced to invent things for the purposes of others, instead of just himself. To Genter, Stark's transformation into Iron Man represents Stark's effort to reclaim his autonomy, and thus his manhood. The character's pursuit of women in bed or in battle, writes Genter, represents another aspect of this effort. The pattern finds parallels in other works of 1960s popular fiction by authors such as "Ian Fleming (creator of James Bond), Mickey Spillane (Mike Hammer), and Norman Mailer, who made unregulated sexuality a form of authenticity."
First series.
After issue #99 (March 1968), the "Tales of Suspense" series was renamed "Captain America". An Iron Man story appeared in the one-shot comic "Iron Man and Sub-Mariner" (April 1968), before the "Golden Avenger" made his solo debut with "The Invincible Iron Man" #1 (May 1968). The series' indicia gives its copyright title "Iron Man," while the trademarked cover logo of most issues is "The Invincible Iron Man." Artist George Tuska began a decade long association with the character with "Iron Man" #5 (Sept. 1968). Writer Mike Friedrich and artist Jim Starlin's brief collaboration on the "Iron Man" series introduced Mentor, Starfox, and Thanos in issue #55 (Feb. 1973). Friedrich scripted a metafictional story in which Iron Man visited the San Diego Comic Convention and met several Marvel Comics writers and artists. He then wrote the multi-issue "War of the Super-Villains" storyline which ran through 1975.
Writer David Michelinie, co-plotter/inker Bob Layton, and penciler John Romita Jr. became the creative team on the series with "Iron Man" #116 (Nov. 1978). Micheline and Layton established Tony Stark's alcoholism with the story "Demon in a Bottle", and introduced several supporting characters, including Stark's bodyguard girlfriend Bethany Cabe; Stark's personal pilot and confidant James Rhodes, who later became the superhero War Machine; and rival industrialist Justin Hammer, who was revealed to be the employer of numerous high-tech armed enemies Iron Man fought over the years. The duo also introduced the concept of Stark's specialized armors as he acquired a dangerous vendetta with Doctor Doom. The team worked together through #154 (Jan. 1982), with Michelinie writing three issues without Layton.
Following Michelinie and Layton's departures, Dennis O'Neil became the new writer of the series and had Stark relapse into alcoholism. Much of O'Neil's work on this plot thread was based on experiences with alcoholics he knew personally. Jim Rhodes replaced Stark as Iron Man in issue #169 (April 1983) and wore the armor for the next two years of stories. O'Neil returned Tony Stark to the Iron Man role in issue #200 (Nov. 1985). Michelinie and Layton became the creative team once again in issue #215 (Feb. 1987). They crafted the "Armor Wars" storyline beginning in #225 (Dec. 1987) through #231 (June 1988). John Byrne and John Romita Jr. produced a sequel titled "Armor Wars II" in issues #258 (July 1990) to #266 (March 1991). The series had a crossover with the other "Avengers" related titles as part of the "" storyline.
Later volumes.
This initial series ended with issue #332 (Sept. 1996). Jim Lee, Scott Lobdell, and Jeph Loeb authored a second volume of the series which was drawn primarily by Whilce Portacio and Ryan Benjamin. This volume took place in a parallel universe and ran 13 issues (Nov. 1996 - Nov. 1997). Volume 3, whose first 25 issues were written by Kurt Busiek and then by Busiek and Roger Stern, ran 89 issues (Feb. 1998 - Dec. 2004). Later writers included Joe Quesada, Frank Tieri, Mike Grell, and John Jackson Miller. Issue #41 (June 2001) was additionally numbered #386, reflecting the start of dual numbering starting from the premiere issue of volume one in 1968. The final issue was dual-numbered as #434. The next Iron Man series, "The Invincible Iron Man" vol. 4, debuted in early 2005 with the Warren Ellis-written storyline "Extremis", with artist Adi Granov. It ran 35 issues (Jan. 2005 - Jan. 2009), with the cover logo simply "Iron Man" beginning with issue #13, and "Iron Man: Director of S.H.I.E.L.D.", beginning issue #15. On the final three issues, the cover logo was overwritten by "War Machine, Weapon of S.H.I.E.L.D.", which led to the launch of a "War Machine" ongoing series.
"The Invincible Iron Man" vol. 5, by writer Matt Fraction and artist Salvador Larroca, began with a premiere issue cover-dated July 2008. For a seven-month overlap, Marvel published both volume four and volume five simultaneously. Volume five jumped its numbering of issues from #33 to #500, cover dated March 2011, to reflect the start from the premiere issue of volume one in 1968.
After the conclusion of "The Invincible Iron Man" a new Iron Man series was started as a part of Marvel Now!. Written by Kieron Gillen and illustrated by Greg Land, it began with issue #1 in November 2012. The series revealed Tony was adopted, and that he had a disabled half-brother named Arno.
Many Iron Man annuals, miniseries, and one-shot titles have been published through the years, such as "Age of Innocence: The Rebirth of Iron Man" (Feb. 1996), "Iron Man: The Iron Age" #1-2 (Aug.-Sept. 1998), "Iron Man: Bad Blood" #1-4 (Sept.-Dec. 2000), "Iron Man House of M" #1-3 (Sept.-Nov. 2005), "Fantastic Four / Iron Man: Big in Japan" #1-4 (Dec. 2005 - March 2006), "Iron Man: The Inevitable" #1-6 (Feb.-July 2006), "Iron Man / Captain America: Casualties of War" (Feb. 2007), "Iron Man: Hypervelocity" #1-6 (March-Aug. 2007), "Iron Man: Enter the Mandarin" #1-6 (Nov. 2007 - April 2008), and "Iron Man: Legacy of Doom" (June-Sept. 2008). Publications have included such spin-offs as the one-shot "Iron Man 2020" (June 1994), featuring a different Iron Man in the future, and the animated TV series adaptations "Marvel Action Hour, Featuring Iron Man" #1-8 (Nov. 1994 - June 1995) and "Marvel Adventures Iron Man" #1-12 (July 2007 - June 2008).
Fictional character biography.
Origins.
Anthony Edward Stark, the adopted son of wealthy industrialist and head of Stark Industries, Howard Stark, and Maria Stark, was born on Long Island. A boy genius, he enters MIT at the age of 15 to study electrical engineering and later receives master's degrees in electrical engineering and physics. After his parents are killed in a car accident, he inherits his father's company.
Tony Stark is injured by a booby trap and captured by enemy forces led by Wong-Chu. Wong-Chu orders Stark to build weapons, but Stark's injuries are dire and shrapnel is moving towards his heart. His fellow prisoner, Ho Yinsen, a Nobel Prize-winning physicist whose work Stark had greatly admired during college, constructs a magnetic chest plate to keep the shrapnel from reaching Stark's heart, keeping him alive. In secret, Stark and Yinsen use the workshop to design and construct a suit of powered armor, which Stark uses to escape. But during the escape attempt, Yinsen sacrifices his life to save Stark's by distracting the enemy as Stark recharges. Stark takes revenge on his kidnappers and heads back to rejoin the American forces, on his way meeting a wounded American Marine fighter pilot, James "Rhodey" Rhodes.
Back home, Stark discovers that the shrapnel fragment lodged in his chest cannot be removed without killing him, and he is forced to wear the armor's chestplate beneath his clothes to act as a regulator for his heart. He must recharge the chestplate every day or else risk the shrapnel killing him. The cover story that Stark tells the news media and general public is that Iron Man is his (presumably robotic) personal bodyguard, and corporate mascot. To that end, Iron Man fights threats to his company (e.g., Communist opponents Black Widow, the Crimson Dynamo, and the Titanium Man), as well as independent villains like the Mandarin (who eventually becomes his greatest enemy). No one suspects Stark of being Iron Man, as he cultivates a strong public image of being merely a rich playboy and industrialist. Two notable members of the series' supporting cast, at this point, are his personal chauffeur Harold "Happy" Hogan, and secretary Virginia "Pepper" Potts—to both of whom he eventually reveals his dual identity. Meanwhile, James Rhodes finds his own niche as Stark's personal pilot, ultimately revealing himself to be a man of extraordinary skill and daring, in his own right.
The series took an anti-Communist stance in its early years, which was softened as public (and therefore, presumably, reader) opposition rose to the Vietnam War. This change evolved in a series of storylines featuring Stark profoundly reconsidering his political opinions, and the morality of manufacturing weapons for the U.S. military. Stark shows himself to be occasionally arrogant, and willing to act unethically in order to 'let the ends justify the means'. This leads to personal conflicts with the people around him, both in his civilian and superhero identities. Stark uses his vast personal fortune not only to outfit his own armor, but also to develop weapons for S.H.I.E.L.D.; other technologies (e.g., Quinjets used by the Avengers); and, the image inducers used by the X-Men. Eventually, Stark's heart condition is discovered by the public and resolved with an artificial heart transplant.
1970s and early 1980s.
Later on, Stark expands on his armor designs and begins to build his arsenal of specialized armors for particular situations such as for space travel and stealth. Stark develops a serious dependency on alcohol in the "Demon in a Bottle" storyline. The first time it becomes a problem is when Stark discovers that the national security agency S.H.I.E.L.D. has been buying a controlling interest in his company in order to ensure Stark's continued weapons development for them. At the same time, it was revealed that several minor supervillains armed with advanced weapons who had bedeviled Stark throughout his superhero career were in fact in the employ of Stark's business rival, Justin Hammer who began to plague Stark more directly. At one point in Hammer's manipulations, the Iron Man armor is even taken over and used to murder a diplomat. Although Iron Man is not immediately under suspicion, Stark is forced to hand the armor over to the authorities. Eventually Stark and Rhodes, who is now his personal pilot and confidant, track down and defeat those responsible, although Hammer would return to bedevil Stark again. With the support of his then-girlfriend, Bethany Cabe, his friends and his employees, Stark pulls through these crises and overcomes his dependency on alcohol. Even as he recovers from this harrowing personal trial, Stark's life is further complicated when he has a confrontation with Doctor Doom that is interrupted by an opportunistic enemy sending them back in time to the time of King Arthur. Once there, Iron Man thwarts Doom's attempt to solicit the aid of Morgan Le Fay, and the Latverian ruler swears deadly vengeance—to be indulged sometime after the two return to their own time. This incident was collected and published as "Doomquest."
Some time later, a ruthless rival, Obadiah Stane, manipulates Stark emotionally into a serious relapse. As a result, Stark loses control of Stark International to Stane, becomes a homeless alcohol-abusing vagrant and gives up his armored identity to Rhodes, who becomes the new Iron Man for a lengthy period of time. Eventually, Stark recovers and joins a new startup, Circuits Maximus. Stark concentrates on new technological designs, including building a new set of armor as part of his recuperative therapy. Rhodes continues to act as Iron Man but steadily grows more aggressive and paranoid, due to the armor not having been calibrated properly for his use. Eventually Rhodes goes on a rampage, and Stark has to don a replica of his original armor to stop him. Fully recovered, Stark confronts Stane who has himself designed a version of armor based on designs seized along with Stark International, dubbing himself the 'Iron Monger'. Defeated in battle, Stane, rather than give Stark the satisfaction of taking him to trial, commits suicide. Shortly thereafter, Stark regains his personal fortune, but decides against repurchasing Stark International until much later; he instead creates Stark Enterprises, headquartered in Los Angeles.
Late 1980s and 1990s.
In an attempt to stop other people from misusing his designs, Stark goes about disabling other armored heroes and villains who are using suits based on the Iron Man technology, the designs of which were stolen by his enemy Spymaster. His quest to destroy all instances of the stolen technology severely hurts his reputation as Iron Man. After attacking and disabling a series of minor villains such as Stilt-Man, he attacks and defeats the government operative known as Stingray. The situation worsens when Stark realizes that Stingray's armor does not incorporate any of his designs. He publicly "fires" Iron Man while covertly pursuing his agenda. He uses the cover story of wanting to help disable the rogue Iron Man to infiltrate and disable the armor of the S.H.I.E.L.D. operatives known as the Mandroids, and disabling the armor of the Guardsmen, in the process allowing some of the villains that they guard to escape. This leads the United States government to declare Iron Man a danger and an outlaw. Iron Man then travels to Russia where he inadvertently causes the death of the Soviet Titanium Man during a fight. Returning to the U.S., he faces an enemy commissioned by the government named Firepower. Unable to defeat him head on, Stark fakes Iron Man's demise, intending to retire the suit permanently. When Firepower goes rogue, Stark creates a new suit, claiming that a new person is in the armor.
Stark's health continues to deteriorate, and he discovers the armor's cybernetic interface is causing irreversible damage to his nervous system. His condition is aggravated by a failed attempt on his life by Kathy Dare, a mentally unbalanced former lover, which injures his spine, paralyzing him. Stark has a nerve chip implanted into his spine to regain his mobility. Still, Stark's nervous system continues its slide towards failure, and he constructs a "skin" made up of artificial nerve circuitry to assist it. Stark begins to pilot a remote-controlled Iron Man armor, but when faced with the Masters of Silence, the telepresence suit proves inadequate. Stark then designs a more heavily armed version of the suit to wear, the "Variable Threat Response Battle Suit", which becomes known as the War Machine armor. Ultimately, the damage to his nervous system becomes too extensive. Faking his death, Stark places himself in suspended animation to heal as Rhodes takes over both the running of Stark Enterprises and the mantle of Iron Man, although he utilizes the War Machine armor. Stark ultimately makes a full recovery by using a chip to reprogram himself and resumes the Iron Man identity. When Rhodes learns that Stark has manipulated his friends by faking his own death, he becomes enraged and the two friends part ways, Rhodes continuing as War Machine in a solo career.
The story arc "The Crossing" reveals Iron Man as a traitor among the Avengers' ranks, due to years of manipulation by the time-traveling dictator Kang the Conqueror. Stark, as a sleeper agent in Kang's thrall, kills Marilla, the nanny of Crystal and Quicksilver's daughter Luna, as well as Rita DeMara, the female Yellowjacket, then Amanda Chaney, an ally of the Avengers. The "Avengers Forever" limited series later retcons these events as the work of a disguised Immortus, not Kang, and that the mental control had gone back only a few months.
Needing help to defeat both Stark and the ostensible Kang, the team travels back in time to recruit a teenaged Anthony Stark from an alternate timeline to assist them. The young Stark steals an Iron Man suit in order to aid the Avengers against his older self. The sight of his younger self shocks the older Stark enough for him to regain momentary control of his actions, and he sacrifices his life to stop Kang. The young Stark later builds his own suit to become the new Iron Man, and, remaining in the present day, gains legal control of "his" company.
During the battle with the creature called Onslaught, the teenaged Stark dies, along with many other superheroes. Franklin Richards preserves these "dead" heroes in the "Heroes Reborn" pocket universe, in which Anthony Stark is once again an adult hero; Franklin recreates the heroes in the pocket universe in the forms he is most familiar with rather than what they are at the present. The reborn adult Stark, upon returning to the normal Marvel Universe, merges with the original Stark, who had died during "The Crossing", but was resurrected by Franklin Richards. This new Anthony Stark possesses the memories of both the original and teenage Anthony Stark, and thus considers himself to be essentially both of them. With the aid of the law firm Nelson & Murdock, he successfully regains his fortune and, with Stark Enterprises having been sold to the Fujikawa Corporation following Stark's death, sets up a new company, Stark Solutions. He returns from the pocket universe with a restored and healthy heart. After the Avengers reform, Stark demands a hearing be convened to look into his actions just prior to the Onslaught incident. Cleared of wrongdoing, he rejoins the Avengers.
2000s.
At one point, Stark's armor becomes sentient despite fail-safes to prevent its increasingly sophisticated computer systems from doing so. Initially, Stark welcomes this "living" armor for its improved tactical abilities. The armor begins to grow more aggressive, killing indiscriminately and eventually desiring to replace Stark altogether. In the final confrontation on a desert island, Stark suffers another heart attack. The armor sacrifices its own existence to save its creator's life, giving up essential components to give Stark a new, artificial heart. This new heart solves Stark's health problems, but it does not have an internal power supply, so Stark becomes once again dependent on periodic recharging. The sentient armor incident so disturbs Stark that he temporarily returns to using an unsophisticated early model version of his armor to avoid a repeat incident. He dabbles with using liquid metal circuitry known as S.K.I.N. that forms into a protective shell around his body, but eventually returns to more conventional hard metal armors.
During this time, Stark engages in a romance with Rumiko Fujikawa (first appearance in "Iron Man" (vol. 3) #4), a wealthy heiress and daughter of the man who had taken over his company during the "Heroes Reborn" period. Her relationship with Stark endures many highs and lows, including an infidelity with Stark's rival, Tiberius Stone, in part because the fun-loving Rumiko believes that Stark is too serious and dull. Their relationship ends with Rumiko's death at the hands of an Iron Man impostor in "Iron Man" (vol. 3) #87.
In "Iron Man" (vol. 3) #55 (July 2002), Stark publicly reveals his dual identity as Iron Man, not realizing that by doing so, he has invalidated the agreements protecting his armor from government duplication, since those contracts state that the Iron Man armor would be used by an "employee" of Tony Stark, not by Stark himself. When he discovers that the United States military is again using his technology, and its defective nature nearly causes a disaster to Washington, D.C. which Iron Man barely manages to avert, Stark accepts a Presidential appointment as Secretary of Defense. In this way, he hopes to monitor and direct how his designs are used.</ref>
In the "Avengers Disassembled" storyline, Stark is forced to resign after launching into a tirade against the Latverian ambassador at the United Nations, being manipulated by the mentally imbalanced Scarlet Witch, who destroys the Avengers Mansion and kills several members. Stark publicly stands down as Iron Man, but actually continues using the costume. He joins the Avengers in stopping the breakout in progress from the Raft and even saves Captain America from falling. Tony changes the Avengers base to Stark Tower. The Ghost, the Living Laser and Spymaster reappear and shift Iron Man from standard superhero stories to dealing with politics and industrialism.
"New Avengers: Illuminati" #1 (June 2006) reveals that years before, Stark had started participating with a group of leaders including the Black Panther, Professor X, Mister Fantastic, Black Bolt, Doctor Strange, and Namor. The goal of the group (dubbed the Illuminati by Marvel) was to strategize overarching menaces, in which the Black Panther rejects a membership offer. Stark's goal is to create a governing body for all superheroes in the world, but the beliefs of its members instead force them all to share vital information.
"Civil War".
In the "Civil War" storyline, after the actions of inexperienced superheroes The New Warriors result in the destruction of several city blocks, including the elementary school, in Stamford, Connecticut, there is an outcry across America against super-humans. Learning of the Government's proposed plans, Tony Stark suggests a new plan to instigate a Superhuman Registration Act. The Act would force every super-powered individual in the U.S. to register their identity with the government and act as licensed agents. The Act would force inexperienced super-humans to receive training in how to use and control their abilities, something in which Tony strongly believes. Since his struggle with alcoholism, Stark has carried a tremendous burden of guilt after nearly killing an innocent bystander while piloting the armor drunk. Reed Richards of the Fantastic Four and Dr. Henry "Hank" Pym both agree with Stark's proposal; unfortunately, not everyone does. After Captain America is ordered to bring in anyone who refuses to register, he and other anti-registration superheroes go rogue, coming into conflict with the pro-registration heroes, led by Iron Man. The war ends when Captain America surrenders to prevent further collateral damage and civilian casualties, although he had defeated Stark by defusing his armor. Stark is appointed the new director of S.H.I.E.L.D., and organizes a new government-sanctioned group of Avengers. Shortly afterwards, Captain America is assassinated while in custody. This leaves Stark with a great amount of guilt and misgivings about the cost of his victory and he tearfully states that "it wasn't worth it". He serves as one of the pallbearers at the memorial service for Captain America, along with Ben Grimm, Ms. Marvel, Rick Jones, T'Challa and Sam Wilson.
"Secret Invasion".
To tie into the 2008 "Iron Man" feature film, Marvel launched a new "Iron Man" ongoing series, "The Invincible Iron Man", with writer Matt Fraction and artist Salvador Larocca. The series inaugural six-part storyline was "The Five Nightmares", which saw Stark targeted by Ezekiel Stane, the son of Stark's former nemesis, Obadiah Stane.
In the "Secret Invasion" storyline, after Tony Stark survives an encounter with Ultron taking over his body, he is confronted in the hospital by Spider-Woman, holding the corpse of a Skrull posing as Elektra. Becoming keenly aware of the upcoming invasion of the Skrulls, Tony gathers the Illuminati and reveals the corpse to them, declaring that they are at war. After Black Bolt reveals himself as a Skrull and is killed by Namor, a squadron of Skrulls attack, forcing Tony to evacuate the other Illuminati members and destroy the area, killing all the Skrulls. Realizing that they are incapable of trusting each other, the members all separate to form individual plans for the oncoming invasion.
Stark is discredited and publicly vilified after his inability to anticipate or prevent a secret infiltration and invasion of Earth by the shape-shifting alien Skrull race, and by the Skrull disabling of his StarkTech technology, which had a virtual monopoly on worldwide defense. After the invasion, the U.S. government removes him as head of S.H.I.E.L.D. and disbands the Avengers, handing control of the Initiative over to Norman Osborn.
"Dark Reign".
With his Extremis powers failing, Stark uploads a virus to destroy all records of the Registration Act, thus preventing Osborn from learning the identities of his fellow heroes and anything that Osborn could possibly exploit, including repulsor generators. The only copy of that database remaining is in Stark's brain, which he is trying to delete bit by bit while on the run in one of his extra armors. As Norman Osborn has him hunted as a fugitive, Stark travels worldwide on his quest to wipe out his mental database, going so far as to inflict brain damage on himself in order to ensure that the relevant information is wiped as a suicide attempt could damage the wrong parts of his brain while leaving Osborn with enough material to salvage the right information. When Osborn personally catches up to the debilitated Stark and beats him savagely, Pepper Potts broadcasts the beatings worldwide, costing Osborn credibility and giving Stark public sympathy. Stark goes into a vegetative state, having previously granted Donald Blake (alter ego of the Norse-god superhero Thor) power of attorney. A holographic message stored in Pepper's armor reveals that Stark had developed a means of 'rebooting' his mind from his current state prior to his destruction of the database, with Blake and Bucky resolving to use it to restore him to normal despite Stark's offer in the message to stay in his current state if it would make things easier and Pepper's own uncertainty about the fact that Tony can come back when so many others cannot. Meanwhile, in Stark's subconscious, he is trapped in a scenario where figments of his own mind are preventing him from moving on and returning to the waking world. When the procedure fails to work, Bucky calls in Doctor Strange, who attempts to and succeeds in restoring Stark back to consciousness. It turns out the backup Stark created was made prior to the Civil War, and as such he does not remember anything that took place during the event, although he still concludes after reviewing his past actions that he would not have done anything differently. His brain damage means that he is now dependent on an arc reactor to sustain his body's autonomous functions such as breathing, blinking and a heartbeat due to the brain damage he sustained.
2010s.
"Siege".
In the "Siege" storyline, Tony Stark is seen under the care of Dr. Donald Blake and Maria Hill. When the two spot the attack on Asgard, Blake tells Maria to run away with Stark. Hill leaves Stark to assist Blake, now as Thor, after his ambush by Osborn and his attack dog the Sentry. Hill rescues Thor and brings him back to Broxton to recuperate. When Osborn declares martial law and unleashes Daken and the Sentry on Broxton to root out Thor and Hill, Thor reveals himself to defend the town. Hill returns to Tony Stark's hiding place to move him to a safer location and are joined soon after by Speed of the Young Avengers, who holds a certain indestructible suitcase that Edwin Jarvis had given Captain America earlier. Hill orders Speed to surrender when Stark stops her and asks Speed to give him the case. While Osborn is battling the New Avengers, Stark appears in a variant of his MK III armor and proceeds to disable Osborn's Iron Patriot armor. Osborn orders the Sentry to annihilate Asgard, rather than allow the Avengers to have it, which the Sentry does, practically leveling the city before the horrified eyes of Thor. After Asgard falls, literally, Stark stands alongside his fellow heroes, as the now armor-free Osborn exclaims they are all doomed and he 'was saving them from him' pointing up towards a Void-possessed Sentry hovering over them. As the Void tears apart the teams, Loki gives them the power to fight back through the Norn Stones. When the Void kills Loki, Thor's rage-fueled blows rattle the creature. Tony then tells Thor to get the Void away from Asgard, which he does. Tony then drops the commandeered H.A.M.M.E.R. Helicarrier 'as a bullet', subduing the Void. When Robert Reynolds begs to be killed, Thor denies the request, but is forced to when the Void resurfaces. Sometime later, the Super-Human Registration Act is repealed and Tony is given back his company and armory. As a symbol for their heroics and their new unity, Thor places a remaining Asgardian tower on Stark Tower where the Watchtower once stood.
"Heroic Age".
In the 2010-2011 "Stark: Resilient" storyline, Tony builds a new armor, the Bleeding Edge, with the help of Mister Fantastic. This new armor fully utilizes the repulsor tech battery embedded in his chest to power Tony's entire body and mind thus allowing him access to Extremis once more. Furthermore, the battery operates as his "heart" and is predominantly the only thing keeping him alive. Later, Tony announces that he will form a new company, Stark Resilient. He states that he will stop developing weapons, instead, he plans to use his repulsor technology to give free energy to the world. Justine and Sasha Hammer create their own armored hero, Detroit Steel, to take Stark's place as the Army's leading weapons-builder. Stark's plan consists of building two repulsor-powered cars. The Hammers try to foil his efforts. The first car is destroyed by sabotage, while Detroit Steel attacks Stark Resilient's facilities while Tony tests the second car. Through a legal maneuver, Tony is able to get the Hammers to stop their attacks and releases a successful commercial about his new car.
"Fear Itself".
In the 2011 "Fear Itself" storyline, Earth is attacked by the Serpent, the God of Fear and the long-forgotten brother of Odin. In Paris, Iron Man fights Grey Gargoyle, who has become Mokk, Breaker of Faith and one of the Serpent's Worthy. Mokk leaves Iron Man unconscious and transforms Detroit Steel into stone. When Iron Man awakens, he sees that Mokk has turned all the people in Paris to stone and left. To defeat the Serpent's army, Tony drinks a bottle of wine—thus 'sacrificing' his sobriety—to gain an audience with Odin, who allows Tony to enter the realm of Svartalfheim. There, Tony and the dwarves of Svartalfheim work to build weapons the Avengers can use against the Worthy. Tony upgrades his armor with uru-infused enchantments and delivers the finished weapons to the Avengers, who use them for the final battle against the Serpent's forces. Iron Man watches as Thor kills the Serpent, but dies in the process. After the battle is over, Tony melts down the weapons he created and repairs Captain America's shield, which had been broken by Serpent, and gives it back to Captain America, telling him that the shield is now stronger. During a subsequent argument with Odin about the gods' lack of involvement in the recent crisis, Odin gives Tony a brief opportunity to see the vastness of the universe the way he sees it, before, as thanks for Tony's role in the recent crisis, he restores all the people that the Grey Gargoyle killed during his rampage.
Return of the Mandarin and Marvel NOW!
In the storylines "Demon" and "The Long Way Down", Stark is subpoenaed by the U.S. government after evidence surfaces of him using the Iron Man armor while under the influence of intoxicants. Mandarin and Zeke Stane upgrade some of Iron Man's old enemies and send them to commit acts of terrorism across the world, intending to discredit Iron Man. General Bruce Babbage forces Stark to wear a tech governor, a device that allows Babbage to deactivate Stark's armor whenever he wants. To fight back, Tony undergoes a surgical procedure that expels the Bleeding Edge technology out of his body and replaces his repulsor node with a new model, forcing Babbage to remove the tech governor off his chest. He announces his retirement as Iron Man, faking Rhodes' death and giving him a new armor so that he becomes the new Iron Man. This leads into the next storyline, "The Future", in which the Mandarin takes control of Stark's mind and uses him to create new armored bodies for the alien spirits inhabiting his rings, but Stark allies himself with some of his old enemies, who have also been imprisoned by Mandarin, and manages to defeat and escape him. The final issue of this storyline concluded Matt Fraction's series.
In the ongoing series that premiered in 2012 as part of the Marvel NOW! relaunch, Tony Stark has hit a technological ceiling. After the death of Dr. Maya Hansen and the destruction of all of the Extremis Ver. 2 kits that were being sold to the black market, Tony decides that the Earth is not safe without him learning more from what's in the final frontier. He takes his new suit, enhanced with an artificial intelligence named P.E.P.P.E.R. and joins Peter Quill and The Guardians of the Galaxy after helping them thwart a Badoon attack on Earth.
Superior Iron Man.
Tony Stark's personality was inverted because of the events of "AXIS", bringing out more dark aspects of himself like irresponsibility, egotism and alcoholism; where other heroes and villains were returned to normal at the conclusion of the crisis, Stark protected himself from the change via an energy shield, which also shielded Alex Summers and Sabretooth from the blast. After relocating to San Francisco, he built himself a new, all-white armor and handed the citizens the Extremis 3.0 app, a version of the techno-virus that could offer beauty, health or even immortallity, for free. When every person in the city viewed Iron Man like a messiah for making their dreams come true, he ended the free trial mode and started charging the app an exorbitant daily fee of $99.99, making many of them desperate for more to the point of resorting to crime. The Extremis 3.0 fever caught the attention of Daredevil, who confronted Stark at his new Alcatraz Island penthouse, but was easily brushed off. Iron Man later used Extremis 3.0 to temporarily restore Daredevil's sight, if only to prove his point. Although Daredevil deduced that Stark had actually added Extremis to the water supply and the phones simply transmitted an activation signal, Stark retaliated by subjecting Murdock to minor brain damage to prevent him from sharing this revelation with others while he reprogrammed Extremis to activate via a signal Murdock could not detect.
When Tony Stark sought a new chief of security, he considers Prodigy, Victor Mancha and the third Beetle, before giving the position to Scott Lang. However, Lang declines the job, moving instead to Miami to stay near his daughter Cassie.
After discovering that new villain Teen Abomination was the son of Happy Hogan, Stark decided to help him, but this minor act of redemption was too little too late in the eyes of Pepper Potts, who attacks Stark with the aid of an A.I. based on Stark's mind, created after a similar instance where Stark's mind was altered by external forces in the event of Stark going too far. This culminated in a confrontation between the two Starks, as Stark calls on the unwitting aid of all 'infected' with the Extremis upgrade while the A.I. uses Stark's various old armors to attack him. Although Stark technically wins the battle as he destroys his other armors and deletes the A.I. backup, Pepper states that she has used her own business acumen to buy up various major news outlets and reveal the truth about his goals with Extremis, bluntly informing him that even if he continues his Extremis upgrade project, he will have to do it alone, accepting his fate of being regarded as a monster by all who know him.
Time Runs Out.
During the "Time Runs Out" storyline, an attempt at reclaiming Wakanda from the Cabal that Namor had created to destroy incursive Earths resulted in Tony being held captive in the Necropolis. After the Cabal had been apparently killed following a truce made by S.H.I.E.L.D. and the Illuminati, the Illuminati returned to Necropolis and freed Tony, who was forced to flee due to the Illuminati's unwillingness to let Stark be there with them when they met Rogers and the Avengers to prevent old fires from being stoked. When the Shi'ar and their allies arrived to Earth in order to destroy it as it was the focal point of the Multiverse's decay, the Avengers and the Illuminati tried to retaliate against the enemy fleet. However, they failed. As they ran out of options, Iron Man flew to Sol's Hammer in order to use it. Iron Man successfully destroyed the Shi'ar fleet saving the Earth, but the final incursion was on the horizon. With only a few minutes before the event, Steve Rogers confronted Iron Man to settle up. The ensuing fight between the two old friends led Steve Rogers to force Iron Man to admit that he had lied to him and all of their allies, when he had known about the incursions all along, but Iron Man also confessed that he wouldn't change a thing. The final incursion started and Earth-1610 started approaching Earth-616 while Iron Man and Steve Rogers kept fighting. Earth-1610's S.H.I.E.L.D. launched a full invasion to destroy Earth-616, where Tony Stark and Steve Rogers were crushed by a Helicarrier.
All-New, All-Different Marvel.
After the events of the Secret Wars crossover event, Tony Stark has returned to his normal self with no signs of his inverted personality. Eight months following the return of the universe as seen in the "All-New, All-Different Marvel" event, Tony had been working on his laboratory non-stop after his position as an innovator had been put in doubt. Because an M.I.T. student reverse-engineered some of his technology, Stark developed a new armor which can change shape, according to the situation he would find himself in. When Stark's new A.I. Friday informed him that Madame Masque had broken into the ruins of Castle Doom, he travelled to Latveria to investigate and ran into some revolutionaries who are then defeated by a man in a suit. To his amazement, Iron Man's armor computer identifies him as Victor von Doom where his face was restored. Victor claims to Iron Man that he wanted to help. 
After learning from Doctor Doom that Madame Masque has taken a decoy of the Wand of Watoomb, Tony Stark confronts Madame Masque in her Montreal hotel room where he learned that the woman she killed was an ex-HYDRA Agent and the man with her was a male prostitute. Upon learning that Madame Masque is not on cahoots with Doctor Doom, Tony is attacked by Madame Masque. Tony Stark had his armor on stealth mode in case of this. When Iron Man tries to get her to be civil, Madame Masque somehow creates a burst of energy which damages Iron Man. After Iron Man's suit has been damaged, Friday controls the suit and takes Iron Man to a safe location. When meeting with Doctor Strange, Iron Man asked him about Doctor Doom's new makeover. Iron Man later tracks Madame Masque to Marina del Rey After finding a tape recorder with Madame Masque's messages, Tony is attacked by several black silhouettes with swords. 
Iron Man barely escapes the ninjas that are attacking him and manages to defeat most of them. Before Iron Man can interrogate one of the ninjas that attacked him, a failsafe in the ninja armor activates where the ninjas are electrocuted. When visiting the St. Jude Children's Research Hospital, Tony Stark is greeted by the children while Friday keeps him posted on any signs of Madame Masque and/or the ninjas. When Victor von Doom was sighted, Tony Stark becomes Iron Man and wants Victor to head to the roof to be questioned. Iron Man and Doctor Doom later arrive at Mary Jane Watson's newest Chicago night club Jackpot when Madame Masque starts attacking it. Iron Man and Doctor Doom fight Madame Masque with the resulting battle causing damages to the Jackpot nightclub. When Mary Jane Watson distracted Madame Masque by using a microphone to knock off her mask, Iron Man and Doctor Doom discover that Madame Masque has a demon possession. As Iron Man holds down Madame Masque's body, Doctor Doom performs an exorcism on Madame Masque which works.
Upon Iron Man regaining consciousness, Doctor Strange arrives upon being contacted by Friday. Iron Man is informed by Doctor Strange that he will take Madame Masque with him to fix her metaphysically and then hand her over at S.H.I.E.L.D. Iron Man also informs him of Doctor Doom's help who had left the scene some time ago. When Tony thanks Stephen for his help, he tells him that he had to as they were "Awesome Facial Hair Bros" after all. Three days later, Iron Man approaches Mary Jane to offer her a job in order to make up for what happened to her nightclub. After speaking with War Machine, Tony Stark meets up at a diner with Amanda Pepara when they are unexpectedly joined by Victor von Doom who wanted to make sure that the demonic possession that affected Madame Masque has not affected Stark or Amara. Stark shows Mary Jane the demonstration on the people that he will be working with. They are interrupted by Friday who tells Tony that War Machine is missing.
Before becoming Iron Man and heading to Tokyo, Tony receives from Mary Jane the emergency number for Peter Parker. In Tokyo, Iron Man is contacted by Spider-Man at War Machine's last known location as he is being observed by ninjas.
Powers and abilities.
Armor.
Iron Man possesses powered armor that gives him superhuman strength and durability, flight, and an array of weapons. The armor is invented and worn by Stark (with occasional short-term exceptions). Other people who have assumed the Iron Man identity include Stark's long-time partner and best friend James Rhodes; close associates Harold "Happy" Hogan; Eddie March; and (briefly) Michael O'Brien.
The weapons systems of the suit have changed over the years, but Iron Man's standard offensive weapons have always been the repulsor rays that are fired from the palms of his gauntlets. Other weapons built into various incarnations of the armor include: the uni-beam projector in its chest; pulse bolts (that pick up kinetic energy along the way; so the farther they travel, the harder they hit); an electromagnetic pulse generator; and a defensive energy shield that can be extended up to 360 degrees. Other capabilities include: generating ultra-freon (i.e., a freeze-beam); creating and manipulating magnetic fields; emitting sonic blasts; and projecting 3-dimensional holograms (to create decoys).
In addition to the general-purpose model he wears, Stark has developed several specialized suits for space travel, deep-sea diving, stealth, and other special purposes. Stark has modified suits, like the Hulkbuster heavy armor. The Hulkbuster armor is composed of add-ons to his so-called modular armor, designed to enhance its strength and durability enough to engage the Incredible Hulk in a fight. A later model, designed for use against Thor, is modeled on the Destroyer and uses a mystical power source. Stark develops an electronics pack during the Armor Wars that, when attached to armors that use Stark technologies, will burn out those components, rendering the suit useless. This pack is ineffective on later models. While it is typically associated with James Rhodes, the War Machine armor began as one of Stark's specialty armors.
The most recent models of Stark's armor, beginning with the Extremis Armor, are now stored in the hollow portions of Stark's bones, and the personal area networking implement used to control it is implanted in his forearm, and connected directly to his central nervous system.
The Extremis has since been removed, and he now uses more conventional armors. Some armors still take a liquid form, but are not stored within his body. His Endo-Sym Armor incorporates a combination of the liquid smart-metal with the alien Venom symbiote, psionically controlled by Stark.
Post-Secret Wars, Stark uses a more streamlined suit of armor that can practically 'morph' into other armors or weapons.
Powers.
After being critically injured during a battle with the Extremis-enhanced Mallen, Stark injects his nervous system with modified techno-organic virus-like body restructuring machines (the Extremis process). By rewriting his own biology, Stark is able to save his life, gain an enhanced healing factor, and partially merge with the Iron Man armor, superseding the need for bulky, AI-controlled armors in favor of lighter designs, technopathically controlled by his own brain. His enhanced technopathy extends to every piece of technology, limitless and effortlessly due to his ability to interface with communication satellites and wireless connections to increase his "range". Some components of the armor-sheath are now stored in Tony's body, able to be recalled, and extruded from his own skin, at will.
During the "Secret Invasion" storyline the Extremis package is catastrophically shutdown by a virus, forcing him again to rely on the previous iteration of his armor, and restoring his previous limitations. Furthermore, Osborn's takeover of most of the few remaining Starktech factories, with Ezekiel Stane systematically crippling the others, limits Tony to the use of lesser, older and weaker armors.
After being forced to "wipe out" his brain to prevent Norman Osborn from gaining his information, Tony Stark is forced to have a new arc reactor, of Rand design installed in his chest. The process greatly improves his strength, stamina and intellect. The procedure left him with virtually no autonomic functions: as his brain was stripped of every biological function, Tony is forced to rely on a digital backup of his memories (leaving him with severe gaps and lapses in his long-term memory) and on software routine in the arc reactor for basic stimuli reaction, such as blinking and breathing. The "Bleeding Edge" package of armor and physical enhancement is now equal in power, if not a more advanced, version of the old Extremis tech.
Skills.
Tony Stark is an inventive genius whose expertise in the fields of mathematics, physics, chemistry, and computer science rivals that of Reed Richards, Hank Pym, and Bruce Banner, and his expertise in electrical engineering and mechanical engineering surpasses even theirs. He is regarded as one of the most intelligent characters in the Marvel Universe. He graduated with advanced degrees in physics and engineering at the age of 17 from Massachusetts Institute of Technology (MIT) and further developed his knowledge ranging from artificial intelligence to quantum mechanics as time progressed. His expertise extends to his ingenuity in dealing with difficult situations, such as difficult foes and deathtraps, in which he is capable of using available tools, including his suit, in unorthodox but effective ways. He is well respected in the business world, able to command people's attention when he speaks on economic matters, having over the years built up several multimillion-dollar companies from virtually nothing. He is noted for the loyalty he commands from and returns to those who work for him, as well as for his business ethics. Thus he immediately fired an employee who made profitable, but illegal, sales to Doctor Doom. He strives to be environmentally responsible in his businesses.
At a time when Stark was unable to use his armor for a period, he received some combat training from Captain America and has become physically formidable on his own when the situation demands it. In addition, Stark possesses great business and political acumen. On multiple occasions he reacquired control of his companies after losing them amid corporate takeovers.
Due to his membership in the Illuminati, Iron Man was given the Space Infinity Gem to safeguard. It allows the user to exist in any location (or all locations), move any object anywhere throughout the universe and warp or rearrange space.
In other media.
In 1966, Iron Man was featured in a series of cartoons. In 1981, Iron Man guest appeared in "Spider-Man and His Amazing Friends", but only as Tony Stark. He went on to feature again in his own series in the 1990s as part of the "Marvel Action Hour" with the Fantastic Four; Robert Hays provided his voice in these animated cartoons. Iron Man makes an appearance in the episode "Shell Games" of "."
Apart from comic books, Iron Man appears in Capcom's ""Vs."" video games, including "Marvel Super Heroes", ' as either a Gold War Machine or Hyper Armor War Machine, ', ', and "Ultimate Marvel vs. Capcom 3". Iron Man is a playable character in "Iron Man", the 1992 arcade game "Captain America and the Avengers", ' and , and ', as well as being featured as an unlockable character in ' and "Tony Hawk's Underground". In the 2009 animated series, "", most of the characters, including Tony Stark, are teenagers. An anime adaptation began airing in Japan in October 2010 as part of a collaboration between Marvel Animation and Madhouse, in which Stark, voiced by Keiji Fujiwara, travels to Japan where he ends up facing off against the Zodiac.
In 2008, a film adaptation titled "Iron Man" was released, starring Robert Downey Jr. as Tony Stark and directed by Jon Favreau. "Iron Man" received very positive reviews from film critics, grossing $318 million domestically and $585 million worldwide. The character of Tony Stark, again played by Robert Downey Jr., appeared at the end of the 2008 film "The Incredible Hulk". Downey reprised his role in "Iron Man 2" (2010), "Marvel's The Avengers" (2012), "Iron Man 3" (2013), ' (2015), and ' (2016), and will appear in "" (2017) and a two part third Avengers film titled "Avengers: Infinity War" (2018/2019).
In 2015, it was announced that Eoin Colfer would be writing an "Iron Man" novel, set for release in 2016.

</doc>
<doc id="67056" url="https://en.wikipedia.org/wiki?curid=67056" title="Thoracic cavity">
Thoracic cavity

The thoracic cavity (or chest cavity) is the chamber of the body of vertebrates that is protected by the thoracic wall (rib cage and associated skin, muscle, and fascia). The central compartment of the thoracic cavity is the mediastinum. There are two openings of the thoracic cavity, a superior thoracic aperture known as the thoracic inlet and a lower inferior thoracic aperture known as the thoracic outlet.
The thoracic cavity includes the tendons as well as the cardiovascular system which could be damaged from injury to the back, spine or the neck.
Structure.
Structures within the thoracic cavity include:
It contains three potential spaces lined with mesothelium: the paired pleural cavities and the pericardial cavity. The mediastinum comprises those organs which lie in the centre of the chest between the lungs. The cavity also contains two openings one at the top, the superior thoracic aperture also called the thoracic inlet, and a lower inferior thoracic aperture which is much larger than the inlet.
Clinical significance.
If the pleural cavity is breached from the outside, as by a bullet wound or knife wound, a pneumothorax, or air in the cavity, may result. If the volume of air is significant, one or both lungs may collapse, which requires immediate medical attention.

</doc>
<doc id="67058" url="https://en.wikipedia.org/wiki?curid=67058" title="Abdominal cavity">
Abdominal cavity

The abdominal cavity is a large body cavity in humans and many other animals that contains many organs. It is a part of the abdominopelvic cavity. It is located below the thoracic cavity, and above the pelvic cavity. Its dome-shaped roof is the thoracic diaphragm, a thin sheet of muscle under the lungs, and its floor is the pelvic inlet , opening into the pelvis.
Structure.
Organs.
Organs of the abdominal cavity include the stomach, liver, gallbladder, spleen, pancreas, small intestine, kidneys, large intestine, and adrenal glands.
Peritoneum.
The abdominal cavity is lined with a protective membrane termed the peritoneum. The inside wall is covered by the parietal peritoneum. The kidneys are located in the abdominal cavity behind the peritoneum, in the retroperitoneum. The viscera are also covered by visceral peritoneum.
Between the visceral and parietal peritoneum is the peritoneal cavity, which is a potential space. It contains serous fluid that allows motion. This motion is apparent of the gastrointestinal tract. The peritoneum, by virtue of its connection to the two (parietal and visceral) portions, gives support to the abdominal organs.
The peritoneum divides the cavity into numerous compartments. One of these the lesser sac is located behind the stomach and joins into the greater sac via the foramen of Winslow. Some of the organs are attached to the walls of the abdomen via folds of peritoneum and ligaments, such as the liver and others use broad areas of the peritoneum, such as the pancreas. The peritoneal ligaments are actually dense folds of the peritoneum that are used to connect viscera to viscera or viscera to the walls of the abdomen. They are named in such a way as to show what they connect typically. For example, the gastrocolic ligament connects the stomach and colon and the splenocolic ligament connects the spleen and the colon, or sometimes by their shape as the round ligament or triangular ligament.
Mesentery.
Mesenteries are folds of peritoneum that are attached to the walls of the abdomen and enclose viscera completely. They are supplied with plentiful amounts of blood. The three most important mesenteries are mesentery for the small intestine, the transverse mesocolon, which attaches the back portion of the colon to the abdominal wall, and the mesosigmoid which enfolds the sigmoid portion of the colon.
Omenta.
The omentum are specialized folds of peritoneum that enclose nerves, blood vessels, lymph channels, fatty tissue, and connective tissue. There are two omenta. First, is the greater omentum that hangs off of the small intestine and greater curvature of the stomach. The other is the lesser omentum that extends between the stomach and the liver.
Clinical significance.
Ascites.
When fluid collects in the abdominal cavity it is called ascites. This is usually not noticeable until enough has collected to distend the abdomen. The collection of fluid will cause pressure on the viscera, veins, and the thoracic cavity. Treatment is directed at the cause of the fluid accumulation. One method is to decrease the portal vein pressure, especially useful in treating cirrhosis. Chylous ascites heals best if the lymphatic vessel involved is closed. Heart failure can cause recurring ascites.
Inflammation.
Another disorder is called peritonitis which usually accompanies inflammatory processes elsewhere. It can be caused by damage to an organ, or from a contusion to the abdominal wall from the outside or by surgery. It may be brought in by the bloodstream or the lymphatic system. The most common origin is the gastrointestinal tract. Peritonitis can be acute or chronic, generalized, or localized, and may be have one origin or multiple origins. The omenta can help control the spread of infection; however without treatment, the infection will spread throughout the cavity. An abscess may form as a secondary reaction to an infection. Antibiotics have become an important tool in fighting abscesses; however external drainage is usually required also.

</doc>
<doc id="67060" url="https://en.wikipedia.org/wiki?curid=67060" title="Hot spring">
Hot spring

A hot spring is a spring produced by the emergence of geothermally heated groundwater that rises from the Earth's crust. There are geothermal hot springs in many locations all over the crust of the earth. While some of these springs contain water that is the correct temperature for bathing, others are so hot that immersion can result in injury or death.
Definitions.
There is no universally accepted definition of a hot spring. For example, one can find the phrase "hot spring" defined as any geothermal spring
The related term "warm spring" is defined as a spring with water temperature less than a hot spring by many sources, although Pentecost et al. (2003) suggest that the phrase "warm spring" is not useful and should be avoided. The US NOAA Geophysical Data Center defines a "warm spring" as a spring with water between 
Sources of heat.
Water issuing from a hot spring is heated geothermally, that is, with heat produced from the Earth's mantle. In general, the temperature of rocks within the earth increases with depth. The rate of temperature increase with depth is known as the geothermal gradient. If water percolates deeply enough into the crust, it will be heated as it comes into contact with hot rocks. The water from hot springs in non-volcanic areas is heated in this manner.
In active volcanic zones such as Yellowstone National Park, water may be heated by coming into contact with magma (molten rock). The high temperature gradient near magma may cause water to be heated enough that it boils or becomes superheated. If the water becomes so hot that it builds steam pressure and erupts in a jet above the surface of the Earth, it is called a geyser. If the water only reaches the surface in the form of steam, it is called a fumarole. If the water is mixed with mud and clay, it is called a mud pot.
Note that hot springs in volcanic areas are often at or near the boiling point. People have been seriously scalded and even killed by accidentally or intentionally entering these springs.
Warm springs are sometimes the result of hot and cold springs mixing. They may occur within a volcanic area or outside of one. One example of a non-volcanic warm spring is Warm Springs, Georgia (frequented for its therapeutic effects by paraplegic U.S. President Franklin D. Roosevelt, who built the Little White House there).
Flow rates.
Hot springs range in flow rate from the tiniest "seeps" to veritable rivers of hot water. Sometimes there is enough pressure that the water shoots upward in a geyser, or fountain.
High flow hot springs.
There are many claims in the literature about the flow rates of hot springs. It should be noted that there are many more high flow non-thermal springs than geothermal springs. For example, there are 33 recognized "magnitude one springs" (having a flow in excess of in Florida alone. Silver Springs, Florida has a flow of more than . Springs with high flow rates include:
Therapeutic uses.
Because heated water can hold more dissolved solids than cold water, warm and especially hot springs often have very high mineral content, containing everything from simple calcium to lithium, and even radium. Because of both the folklore and the claimed medical value some of these springs have, they are often popular tourist destinations, and locations for rehabilitation clinics for those with disabilities.
Biota in hot springs.
A thermophile is an organism — a type of extremophile — that thrives at relatively high temperatures, between . Thermophiles are found in hot springs, as well as deep sea hydrothermal vents and decaying plant matter such as peat bogs and compost.
Some hot springs biota are infectious to humans. For example:
Notable hot springs.
There are hot springs in many countries and on all continents of the world. Countries that are renowned for their hot springs include China, Costa Rica, Iceland, Iran, Japan, New Zealand, Peru, Taiwan, Turkey, and United States, but there are hot springs in many other places as well:
Etiquette.
The customs and practices observed differ depending on the hot spring. It is common practice that bathers should wash before entering the water so as not to contaminate the water (with/without soap). In many countries, like Japan, it is required to enter the hot spring with no clothes on, including swimwear. Typically in these circumstances, there are different facilities or times for men and women. In some countries, if it is a public hot spring, swimwear is required.

</doc>
<doc id="67062" url="https://en.wikipedia.org/wiki?curid=67062" title="Pastry">
Pastry

Pastry is a dough of flour and water and shortening that may be savoury or sweetened. Sweetened pastries are often described as "bakers' confectionery". The word "Pastries" suggests many kinds of baked products made from ingredients such as flour, sugar, milk, butter, shortening, baking powder, and eggs. Small tarts and other sweet baked products are called pastries. Common pastry dishes include pies, tarts, quiches and pasties.
Pastry can also refer to the pastry dough, from which such baked products are made. Pastry dough is rolled out thinly and used as a base for baked products.
Pastry is differentiated from bread by having a higher fat content, which contributes to a flaky or crumbly texture. A good pastry is light and airy and fatty, but firm enough to support the weight of the filling. When making a shortcrust pastry, care must be taken to blend the fat and flour thoroughly before adding any liquid. This ensures that the flour granules are adequately coated with fat and less likely to develop gluten. On the other hand, overmixing results in long gluten strands that toughen the pastry. In other types of pastry such as Danish pastry and croissants, the characteristic flaky texture is achieved by repeatedly rolling out a dough similar to that for yeast bread, spreading it with butter, and folding it to produce many thin layers.
Chemistry.
Different kinds of pastries are made by utilizing the natural characteristics of wheat flour and certain fats. When wheat flour is mixed with water and kneaded into plain dough, it develops strands of gluten, which are what make bread tough and elastic. In a typical pastry, however, this toughness is unwanted, so fat or oil is added to slow down the development of gluten. Lard or suet work well because they have a coarse, crystalline structure that is very effective. Using unclarified butter does not work well because of its water content; clarified butter, which is virtually water-free, is better, but shortcrust pastry using only butter may develop an inferior texture. If the fat is melted with hot water or if liquid oil is used, the thin oily layer between the grains offers less of an obstacle to gluten formation and the resulting pastry is tougher.
History.
The European tradition of pastry-making is often traced back to the shortcrust era of flaky doughs that were in use throughout the Mediterranean in ancient times.
In the ancient Mediterranean, the Romans, Greeks and Phoenicians all had filo-style pastries in their culinary traditions. There is also strong evidence that Egyptians produced pastry-like confections. They had professional bakers that surely had the skills to do so, and they also had needed materials like flour, oil, and honey.
In the plays of Aristophanes, written in the 5th century BC, there is mention of sweetmeats, including small pastries filled with fruit. The Roman cuisine used flour, oil and water to make pastries that were used to cover meats and fowls during baking in order to keep in the juices, but the pastry was not meant to be eaten. A pastry that was meant to be eaten was a richer pastry that was made into small pastries containing eggs or little birds and that were often served at banquets. Greeks and Roman both struggled in making a good pastry because they used oil in the cooking process, and oil causes the pastry to lose its stiffness.
In the medieval cuisine of Northern Europe, pastry chefs were able to produce nice, stiff pastries because they cooked with shortening and butter. Some incomplete lists of ingredients have been found in medieval cookbooks, but no full, detailed versions. There were stiff, empty pastries called coffins or 'huff paste', that were eaten by servants only and included an egg yolk glaze to help make them more enjoyable to consume. Medieval pastries also included small tarts to add richness.
It was not until about the mid-16th century that actual pastry recipes began appearing.
These recipes were adopted and adapted over time in various European countries, resulting in the myriad pastry traditions known to the region, from Portuguese "pastéis de nata" in the west to Russian "pirozhki" in the east. The use of chocolate in pastry-making in the west, so commonplace today, arose only after Spanish and Portuguese traders brought chocolate to Europe from the New World starting in the 16th century. Many culinary historians consider French pastry chef Antonin Carême (1784–1833) to have been the first great master of pastry making in modern times.
Pastry-making also has a strong tradition in many parts of Asia. Chinese pastry is made from rice, or different types of flour, with fruit, sweet bean paste or gail-based fillings. The mooncakes are part of Chinese Mid Autumn Festival traditions, while cha siu bao, steamed or baked pork buns, are a regular savory dim sum menu item. In the 19th century, the British brought western-style pastry to the far east, though it would be the French-influenced Maxim in the 1950s that made western pastry popular in Chinese-speaking regions starting with Hong Kong. Still, the term "western cake" (西餅) is used to differentiate between the automatically assumed Chinese pastry Other Asian countries such as Korea prepare traditional pastry-confections such as tteok, hangwa, and yaksik with flour, rice, fruits, and regional specific ingredients to make unique desserts. Japan also has specialized pastry-confections better known as mochi and manjū. Pastry-confections that originate in Asia are clearly distinct from those that originate in the west, which are generally much sweeter.
Pastry chefs.
Pastry chefs use a combination of culinary ability and creativity in baking, decoration, and flavoring with ingredients. Many baked goods require a lot of time and focus. Presentation is an important aspect of pastry and dessert preparation. The job is often physically demanding, requiring attention to detail and long hours. Pastry chefs are also responsible for creating new recipes to put on the menu, and they work in restaurants, bistros, large hotels, casinos and bakeries. Pastry baking is usually done in an area slightly separate from the main kitchen. This section of the kitchen is in charge of making pastries, desserts, and other baked goods.

</doc>
<doc id="67064" url="https://en.wikipedia.org/wiki?curid=67064" title="Rose water">
Rose water

Rose water is a flavoured water made by steeping rose petals in water. It is the hydrosol portion of the distillate of rose petals, a by-product of the production of rose oil for use in perfume. It is used to flavour food, as a component in some cosmetic and medical preparations, and for religious purposes throughout Europe and Asia. Rose syrup (not to be confused with rose-hip syrup) is made from rose water, with sugar added.
Origin.
The cultivation of various fragrant flowers for obtaining perfumes including rose water may date back to Sassanid Persia. Locally it was known as "golāb" in Middle Persian, and as "zoulápin" in Byzantine Greek.
The modern mass production of rose water through steam distillation was refined by Persian chemist Avicenna in the medieval Islamic world which led to more efficient and economic uses for perfumery industries. This allowed for more efficient and lucrative trade.
Since ancient times, roses have been used medicinally, nutritionally, and as a source of perfume. The ancient Greeks, Romans and Phoenicians considered large public rose gardens to be as important as croplands such as orchards and wheat fields.
Rose perfumes are made from rose oil, also called "attar of roses", which is a mixture of volatile essential oils obtained by steam-distilling the crushed petals of roses, a process first developed in Iran (Persia). Rose water is a by-product of this process.
Uses.
Edible.
Rose water has a very distinctive flavour and is used heavily in Persian and Middle Eastern cuisine—especially in sweets such as nougat, gumdrops, and baklava. For example, rose water is used to give some types of Turkish delight (Rahat lokum) their distinctive flavors.
The Cypriot version of mahalebi uses rosewater. In Iran, it is also added to tea, ice cream, cookies and other sweets in small quantities, and in the Arab world, Pakistan and India it is used to flavor milk and dairy-based dishes such as rice pudding. It is also a key ingredient in sweet lassi, a drink made from yogurt, sugar and various fruit juices, and is also used to make jallab. In Malaysia and Singapore, sweet red-tinted rose water is mixed with milk, which then turns pink to make a sweet drink called bandung. Rose water is frequently used as a halal substitute for red wine and other alcohols in cooking; the Premier League offer a rose water-based beverage as an alternative for champagne when rewarding Muslim players.
Marzipan has long been flavoured with rose water. Marzipan originated in the Middle East and arrived in Western Europe by the Middle Ages; it continues to be served as a postprandial snack. Rose water was also used to make Waverly Jumbles. American and European bakers enjoyed the floral flavouring of rose water in their baking until the 19th century when vanilla flavouring became popular.
In parts of the Middle East, rose water is commonly added to lemonade or milk.
Cosmetic and medicinal use.
Rose water is a usual component of perfume. A rose water ointment is occasionally used as an emollient, and rose water is sometimes used in cosmetics such as cold creams.
Medicinal use-Ayurveda: In India, rose water is used as eye drops to clear them. Some people in India also use rose water as spray applied directly to the face for natural fragrance and moisturizer, especially during winters. It is also used in Indian sweets and other food preparations (particularly gulab jamun). Rose water is often sprinkled in Indian weddings to welcome guests.
Religious uses.
Rose water is used as a perfume in religious ceremonies (Muslim, Hindu and Zoroastrian). Water used to clean the Kaaba, the Qibla for Muslims located in Mecca, combines water from the Zamzam Well with rose water as an additive. In the Indian subcontinent during Muslim burials, rose water is often sprinkled in the dug grave before placing the body inside. Rose water is used in some Hindu rituals as well. Rose water also figures in Christianity, particularly in the Eastern Orthodox Church.
Composition.
Depending on the origin and type of manufacturing method of rosewater obtained from the sepals and petals of "Rosa × damascena" from Central Iran through steam distillation, the following monoterpenoid and alkane components could be identified with GC-MS: mostly citronellol, nonadecane, geraniol and phenyl ethyl alcohol, and also henicosane, 9-nonadecen, eicosane, linalool, citronellyl acetate, methyleugenol, heptadecane, pentadecane, docosane, nerol, disiloxane, octadecane, and pentacosane.
Usually, phenylethyl alcohol is responsible for the typical odour of rose water but not always present in rosewater products.

</doc>
<doc id="67065" url="https://en.wikipedia.org/wiki?curid=67065" title="Word-sense disambiguation">
Word-sense disambiguation

In computational linguistics, word-sense disambiguation (WSD) is an open problem of natural language processing and ontology. WSD is identifying which sense of a word (i.e. meaning) is used in a sentence, when the word has multiple meanings. The solution to this problem impacts other computer-related writing, such as discourse, improving relevance of search engines, anaphora resolution, coherence, inference "et cetera".
The human brain is quite proficient at word-sense disambiguation. The fact that natural language is formed in a way that requires so much of it is a reflection of that neurologic reality. In other words, human language developed in a way that reflects (and also has helped to shape) the innate ability provided by the brain's neural networks. In computer science and the information technology that it enables, it has been a long-term challenge to develop the ability in computers to do natural language processing and machine learning.
To date, a rich variety of techniques have been researched, from dictionary-based methods that use the knowledge encoded in lexical resources, to supervised machine learning methods in which a classifier is trained for each distinct word on a corpus of manually sense-annotated examples, to completely unsupervised methods that cluster occurrences of words, thereby inducing word senses. Among these, supervised learning approaches have been the most successful algorithms to date.
Current accuracy is difficult to state without a host of caveats. In English, accuracy at the coarse-grained (homograph) level is routinely above 90%, with some methods on particular homographs achieving over 96%. On finer-grained sense distinctions, top accuracies from 59.1% to 69.0% have been reported in recent evaluation exercises (SemEval-2007, Senseval-2), where the baseline accuracy of the simplest possible algorithm of always choosing the most frequent sense was 51.4% and 57%, respectively.
About.
Disambiguation requires two strict inputs: a dictionary to specify the senses which are to be disambiguated and a corpus of language data to be disambiguated (in some methods, a training corpus of language examples is also required). WSD task has two variants: "lexical sample" and "all words" task. The former comprises disambiguating the occurrences of a small sample of target words which were previously selected, while in the latter all the words in a piece of running text need to be disambiguated. The latter is deemed a more realistic form of evaluation, but the corpus is more expensive to produce because human annotators have to read the definitions for each word in the sequence every time they need to make a tagging judgement, rather than once for a block of instances for the same target word.
To give a hint how all this works, consider two examples of the distinct senses that exist for the (written) word ""bass"":
and the sentences:
To a human, it is obvious that the first sentence is using the word ""bass (fish)"", as in the former sense above and in the second sentence, the word ""bass (instrument)"" is being used as in the latter sense below. Developing algorithms to replicate this human ability can often be a difficult task, as is further exemplified by the implicit equivocation between ""bass (sound)"" and ""bass" (musical instrument)".
History.
WSD was first formulated into as a distinct computational task during the early days of machine translation in the 1940s, making it one of the oldest problems in computational linguistics. Warren Weaver, in his famous 1949 memorandum on translation, first introduced the problem in a computational context. Early researchers understood the significance and difficulty of WSD well. In fact, Bar-Hillel (1960) used the above example to argue that WSD could not be solved by "electronic computer" because of the need in general to model all world knowledge.
In the 1970s, WSD was a subtask of semantic interpretation systems developed within the field of artificial intelligence, starting with Wilks' preference semantics. However, since WSD systems were at the time largely rule-based and hand-coded they were prone to a knowledge acquisition bottleneck.
By the 1980s large-scale lexical resources, such as the Oxford Advanced Learner's Dictionary of Current English (OALD), became available: hand-coding was replaced with knowledge automatically extracted from these resources, but disambiguation was still knowledge-based or dictionary-based.
In the 1990s, the statistical revolution swept through computational linguistics, and WSD became a paradigm problem on which to apply supervised machine learning techniques.
The 2000s saw supervised techniques reach a plateau in accuracy, and so attention has shifted to coarser-grained senses, domain adaptation, semi-supervised and unsupervised corpus-based systems, combinations of different methods, and the return of knowledge-based systems via graph-based methods. Still, supervised systems continue to perform best.
Difficulties.
Differences between dictionaries.
One problem with word sense disambiguation is deciding what the senses are. In cases like the word "bass" above, at least some senses are obviously different. In other cases, however, the different senses can be closely related (one meaning being a metaphorical or metonymic extension of another), and in such cases division of words into senses becomes much more difficult. Different dictionaries and thesauruses will provide different divisions of words into senses. One solution some researchers have used is to choose a particular dictionary, and just use its set of senses. Generally, however, research results using broad distinctions in senses have been much better than those using narrow ones. However, given the lack of a full-fledged coarse-grained sense inventory, most researchers continue to work on fine-grained WSD.
Most research in the field of WSD is performed by using WordNet as a reference sense inventory for English. WordNet is a computational lexicon that encodes concepts as synonym sets (e.g. the concept of car is encoded as { car, auto, automobile, machine, motorcar }). Other resources used for disambiguation purposes include Roget's Thesaurus and Wikipedia. More recently, BabelNet, a multilingual encyclopedic dictionary, has been used for multilingual WSD.
Part-of-speech tagging.
In any real test, part-of-speech tagging and sense tagging are very closely related with each potentially making constraints to the other. And the question whether these tasks should be kept together or decoupled is still not unanimously resolved, but recently scientists incline to test these things separately (e.g. in the Senseval/SemEval competitions parts of speech are provided as input for the text to disambiguate).
It is instructive to compare the word sense disambiguation problem with the problem of part-of-speech tagging. Both involve disambiguating or tagging with words, be it with senses or parts of speech. However, algorithms used for one do not tend to work well for the other, mainly because the part of speech of a word is primarily determined by the immediately adjacent one to three words, whereas the sense of a word may be determined by words further away. The success rate for part-of-speech tagging algorithms is at present much higher than that for WSD, state-of-the art being around 95% accuracy or better, as compared to less than 75% accuracy in word sense disambiguation with supervised learning. These figures are typical for English, and may be very different from those for other languages.
Inter-judge variance.
Another problem is inter-judge variance. WSD systems are normally tested by having their results on a task compared against those of a human. However, while it is relatively easy to assign parts of speech to text, training people to tag senses is far more difficult. While users can memorize all of the possible parts of speech a word can take, it is often impossible for individuals to memorize all of the senses a word can take. Moreover, humans do not agree on the task at hand – give a list of senses and sentences, and humans will not always agree on which word belongs in which sense.
Thus, a computer cannot be expected to give better performance on such a task than a human (indeed, since the human serves as the standard, the computer being better than the human is incoherent), so the human performance serves as an upper bound. Human performance, however, is much better on coarse-grained than fine-grained distinctions, so this again is why research on coarse-grained distinctions has been put to test in recent WSD evaluation exercises.
Common sense.
Some AI researchers like Douglas Lenat argue that one cannot parse meanings from words without some form of common sense ontology.
For example, comparing these two sentences:
To properly identify senses of words one must know common sense facts. Moreover, sometimes the common sense is needed to disambiguate such words like pronouns in case of having anaphoras or cataphoras in the text.
Sense inventory and algorithms' task-dependency.
A task-independent sense inventory is not a coherent concept: each task requires its own division of word meaning into senses relevant to the task. For example, the ambiguity of 'mouse' (animal or device) is not relevant in English-French machine translation, but is relevant in information retrieval. The opposite is true of 'river', which requires a choice in French (fleuve 'flows into the sea', or rivière 'flows into a river').
Also, completely different algorithms might be required by different applications. In machine translation, the problem takes the form of target word selection. Here, the "senses" are words in the target language, which often correspond to significant meaning distinctions in the source language ("bank" could translate to the French "banque"—that is, 'financial bank' or "rive"—that is, 'edge of river'). In information retrieval, a sense inventory is not necessarily required, because it is enough to know that a word is used in the same sense in the query and a retrieved document; what sense that is, is unimportant.
Discreteness of senses.
Finally, the very notion of "word sense" is slippery and controversial. Most people can agree in distinctions at the coarse-grained homograph level (e.g., pen as writing instrument or enclosure), but go down one level to fine-grained polysemy, and disagreements arise. For example, in Senseval-2, which used fine-grained sense distinctions, human annotators agreed in only 85% of word occurrences. Word meaning is in principle infinitely variable and context sensitive. It does not divide up easily into distinct or discrete sub-meanings. Lexicographers frequently discover in corpora loose and overlapping word meanings, and standard or conventional meanings extended, modulated, and exploited in a bewildering variety of ways. The art of lexicography is to generalize from the corpus to definitions that evoke and explain the full range of meaning of a word, making it seem like words are well-behaved semantically. However, it is not at all clear if these same meaning distinctions are applicable in computational applications, as the decisions of lexicographers are usually driven by other considerations. Recently, a task – named lexical substitution – has been proposed as a possible solution to the sense discreteness problem. The task consists of providing a substitute for a word in context that preserves the meaning of the original word (potentially, substitutes can be chosen from the full lexicon of the target language, thus overcoming discreteness).
Approaches and methods.
As in all natural language processing, there are two main approaches to WSD – deep approaches and shallow approaches.
Deep approaches presume access to a comprehensive body of world knowledge. Knowledge, such as "you can go fishing for a type of fish, but not for low frequency sounds" and "songs have low frequency sounds as parts, but not types of fish", is then used to determine in which sense the word "bass" is used. These approaches are not very successful in practice, mainly because such a body of knowledge does not exist in a computer-readable format, outside very limited domains. However, if such knowledge did exist, then deep approaches would be much more accurate than the shallow approaches. Also, there is a long tradition in computational linguistics, of trying such approaches in terms of coded knowledge and in some cases, it is hard to say clearly whether the knowledge involved is linguistic or world knowledge. The first attempt was that by Margaret Masterman and her colleagues, at the Cambridge Language Research Unit in England, in the 1950s. This attempt used as data a punched-card version of Roget's Thesaurus and its numbered "heads", as an indicator of topics and looked for repetitions in text, using a set intersection algorithm. It was not very successful, but had strong relationships to later work, especially Yarowsky's machine learning optimisation of a thesaurus method in the 1990s.
Shallow approaches don't try to understand the text. They just consider the surrounding words, using information such as "if "bass" has words "sea" or "fishing" nearby, it probably is in the fish sense; if "bass" has the words "music" or "song" nearby, it is probably in the music sense." These rules can be automatically derived by the computer, using a training corpus of words tagged with their word senses. This approach, while theoretically not as powerful as deep approaches, gives superior results in practice, due to the computer's limited world knowledge. However, it can be confused by sentences like "The dogs bark at the tree" which contains the word "bark" near both "tree" and "dogs".
There are four conventional approaches to WSD:
Almost all these approaches normally work by defining a window of "n" content words around each word to be disambiguated in the corpus, and statistically analyzing those "n" surrounding words. Two shallow approaches used to train and then disambiguate are Naïve Bayes classifiers and decision trees. In recent research, kernel-based methods such as support vector machines have shown superior performance in supervised learning. Graph-based approaches have also gained much attention from the research community, and currently achieve performance close to the state of the art.
Dictionary- and knowledge-based methods.
The Lesk algorithm is the seminal dictionary-based method. It is based on the hypothesis that words used together in text are related to each other and that the relation can be observed in the definitions of the words and their senses. Two (or more) words are disambiguated by finding the pair of dictionary senses with the greatest word overlap in their dictionary definitions. For example, when disambiguating the words in "pine cone", the definitions of the appropriate senses both include the words evergreen and tree (at least in one dictionary).
An alternative to the use of the definitions is to consider general word-sense relatedness and to compute the semantic similarity of each pair of word senses based on a given lexical knowledge base such as WordNet. Graph-based methods reminiscent of spreading activation research of the early days of AI research have been applied with some success. More complex graph-based approaches have been shown to perform almost as well as supervised methods or even outperforming them on specific domains. Recently, it has been reported that simple graph connectivity measures, such as degree, perform state-of-the-art WSD in the presence of a sufficiently rich lexical knowledge base. Also, automatically transferring knowledge in the form of semantic relations from Wikipedia to WordNet has been shown to boost simple knowledge-based methods, enabling them to rival the best supervised systems and even outperform them in a domain-specific setting.
The use of selectional preferences (or selectional restrictions) is also useful, for example, knowing that one typically cooks food, one can disambiguate the word bass in "I am cooking basses" (i.e., it's not a musical instrument).
Supervised methods.
Supervised methods are based on the assumption that the context can provide enough evidence on its own to disambiguate words (hence, common sense and reasoning are deemed unnecessary). Probably every machine learning algorithm going has been applied to WSD, including associated techniques such as feature selection, parameter optimization, and ensemble learning. Support Vector Machines and memory-based learning have been shown to be the most successful approaches, to date, probably because they can cope with the high-dimensionality of the feature space. However, these supervised methods are subject to a new knowledge acquisition bottleneck since they rely on substantial amounts of manually sense-tagged corpora for training, which are laborious and expensive to create.
Semi-supervised methods.
Because of the lack of training data, many word sense disambiguation algorithms use semi-supervised learning, which allows both labeled and unlabeled data. The Yarowsky algorithm was an early example of such an algorithm. It uses the ‘One sense per collocation’ and the ‘One sense per discourse’ properties of human languages for word sense disambiguation. From observation, words tend to exhibit only one sense in most given discourse and in a given collocation.
The bootstrapping approach starts from a small amount of seed data for each word: either manually tagged training examples or a small number of surefire decision rules (e.g., 'play' in the context of 'bass' almost always indicates the musical instrument). The seeds are used to train an initial classifier, using any supervised method. This classifier is then used on the untagged portion of the corpus to extract a larger training set, in which only the most confident classifications are included. The process repeats, each new classifier being trained on a successively larger training corpus, until the whole corpus is consumed, or until a given maximum number of iterations is reached.
Other semi-supervised techniques use large quantities of untagged corpora to provide co-occurrence information that supplements the tagged corpora. These techniques have the potential to help in the adaptation of supervised models to different domains.
Also, an ambiguous word in one language is often translated into different words in a second language depending on the sense of the word. Word-aligned bilingual corpora have been used to infer cross-lingual sense distinctions, a kind of semi-supervised system.
Unsupervised methods.
Unsupervised learning is the greatest challenge for WSD researchers. The underlying assumption is that similar senses occur in similar contexts, and thus senses can be induced from text by clustering word occurrences using some measure of similarity of context, a task referred to as word sense induction or discrimination. Then, new occurrences of the word can be classified into the closest induced clusters/senses. Performance has been lower than for the other methods described above, but comparisons are difficult since senses induced must be mapped to a known dictionary of word senses. If a mapping to a set of dictionary senses is not desired, cluster-based evaluations (including measures of entropy and purity) can be performed. Alternatively, word sense induction methods can be tested and compared within an application. For instance, it has been shown that word sense induction improves Web search result clustering by increasing the quality of result clusters and the degree diversification of result lists. It is hoped that unsupervised learning will overcome the knowledge acquisition bottleneck because they are not dependent on manual effort.
Other approaches.
Other approaches may vary differently in their methods:
Local impediments and summary.
The knowledge acquisition bottleneck is perhaps the major impediment to solving the WSD problem. Unsupervised methods rely on knowledge about word senses, which is barely formulated in dictionaries and lexical databases. Supervised methods depend crucially on the existence of manually annotated examples for every word sense, a requisite that can so far be met only for a handful of words for testing purposes, as it is done in the Senseval exercises.
Therefore, one of the most promising trends in WSD research is using the largest corpus ever accessible, the World Wide Web, to acquire lexical information automatically. WSD has been traditionally understood as an intermediate language engineering technology which could improve applications such as information retrieval (IR). In this case, however, the reverse is also true: Web search engines implement simple and robust IR techniques that can be successfully used when mining the Web for information to be employed in WSD. Therefore, the lack of training data provoked appearing some new algorithms and techniques described here:
External knowledge sources.
Knowledge is a fundamental component of WSD. Knowledge sources provide data which are essential to associate senses with words. They can vary from corpora of texts, either unlabeled or annotated with word senses, to machine-readable dictionaries, thesauri, glossaries, ontologies, etc. They can be classified as follows:
Structured:
Unstructured:
Evaluation.
Comparing and evaluating different WSD systems is extremely difﬁcult, because of the different test sets, sense inventories, and knowledge resources adopted. Before the organization of speciﬁc evaluation campaigns most systems were assessed on in-house, often small-scale, data sets. In order to test one's algorithm, developers should spend their time to annotate all word occurrences. And comparing methods even on the same corpus is not eligible if there is different sense inventories.
In order to define common evaluation datasets and procedures, public evaluation campaigns have been organized. Senseval (now renamed SemEval) is an international word sense disambiguation competition, held every three years since 1998: Senseval-1 (1998), Senseval-2 (2001), Senseval-3 (2004), and its successor, SemEval (2007). The objective of the competition is to organize different lectures, preparing and hand-annotating corpus for testing systems, perform a comparative evaluation of WSD systems in several kinds of tasks, including all-words and lexical sample WSD for different languages, and, more recently, new tasks such as semantic role labeling, gloss WSD, lexical substitution, etc. The systems submitted for evaluation to these competitions usually integrate different techniques and often combine supervised and knowledge-based methods (especially for avoiding bad performance in lack of training examples).
In recent years 2007-2012, the WSD evaluation task choices had grown and the criterion for evaluating WSD has changed drastically depending on the variant of the WSD evaluation task. Below enumerates the variety of WSD tasks:
Task design choices.
As technology evolves, the Word Sense Disambiguation (WSD) tasks grows in different flavors towards various research directions and for more languages:

</doc>
<doc id="67066" url="https://en.wikipedia.org/wiki?curid=67066" title="Satellite radio">
Satellite radio

Satellite radio is – according to "article 1.39" of the International Telecommunication Union´s (ITU) ITU Radio Regulations (RR) – a "Broadcasting-satellite service".
The satellite's signals are broadcast nationwide, across a much wider geographical area than terrestrial radio stations, and the service is primarily intended for the occupants of motor vehicles. It is available by subscription, mostly commercial free, and offers subscribers more stations and a wider variety of programming options than terrestrial radio.
Satellite radio technology was inducted into the Space Foundation Space Technology Hall of Fame in 2002. Satellite radio uses the 2.3 GHz S band in North America for nationwide digital audio broadcasting (DAB). In other parts of the world, satellite radio uses the 1.4 GHz L band allocated for DAB.
Satellite radio subscribers purchase a receiver and pay a monthly subscription fee to listen to programming. They can listen through built-in or portable receivers in automobiles; in the home and office with a portable or tabletop receiver equipped to connect the receiver to a stereo system; or on the Internet.
Ground stations transmit signals to the satellites, which are orbiting over 22,000 miles above the surface of the Earth. The satellites send the signals back down to radio receivers in cars and homes. This signal contains scrambled broadcasts, along with meta data about each specific broadcast. The signals are unscrambled by the radio receiver modules, which display the broadcast information. In urban areas, ground repeaters enable signals to be available even if the satellite signal is blocked. The technology allows for nationwide broadcasting, so that, for instance US listeners can hear the same stations anywhere in the country.
History.
United States.
Sirius Satellite Radio was founded by Martine Rothblatt, David Margolese and Robert Briskman. In June 1990, Rothblatt's shell company, Satellite CD Radio, Inc., petitioned the Federal Communications Commission (FCC) to assign new frequencies for satellites to broadcast digital sound to homes and cars. The company identified and argued in favor of the use of the S-band frequencies that the FCC subsequently decided to allocate to digital audio broadcasting. The National Association of Broadcasters contended that satellite radio would harm local radio stations.
In April 1992, Rothblatt resigned as CEO of Satellite CD Radio and former NASA engineer Robert Briskman, who designed the company's satellite technology, was then appointed chairman and CEO. Six months later, Rogers Wireless co-founder David Margolese, who had provided financial backing for the venture, acquired control of the company and succeeded Briskman. Margolese renamed the company CD Radio, and spent the next five years lobbying the FCC to allow satellite radio to be deployed, and the following five years raising $1.6 billion, which was used to build and launch three satellites into elliptical orbit from Kazakhstan in July 2000. In 1997, after Margolese had obtained regulatory clearance and "effectively created the industry," the FCC also sold a license to the American Mobile Radio Corporation, which changed its name to XM Satellite Radio in October 1998. XM was founded by Lon Levin and Gary Parsons, who served as chairman until November 2009.
CD Radio purchased their license for $83.3 million, and American Mobile Radio Corporation bought theirs for $89.9 million. Digital Satellite Broadcasting Corporation and Primosphere were unsuccessful in their bids for licenses. Sky Highway Radio Corporation had also expressed interest in creating a satellite radio network, before being bought out by CD Radio in 1993 for $2 million. In November 1999, Margolese changed the name of CD Radio to Sirius Satellite Radio. In November 2001, Margolese stepped down as CEO, remaining as chairman until November 2003, with Sirius issuing a statement thanking him "for his great vision, leadership and dedication in creating both Sirius and the satellite radio industry."
XM’s first satellite was launched on March 18, 2001 and its second on May 8, 2001. Its first broadcast occurred on September 25, 2001, nearly four months before Sirius. Sirius launched the initial phase of its service in four cities on February 14, 2002, expanding to the rest of the contiguous United States on July 1, 2002. The two companies spent over $3 billion combined to develop satellite radio technology, build and launch the satellites, and for various other business expenses. Stating that it was the only way satellite radio could survive, Sirius and XM announced their merger on February 19, 2007, becoming Sirius XM Radio. The FCC approved the merger on July 25, 2008, concluding that it was not a monopoly, primarily due to Internet audio-streaming competition.
Canada.
XM satellite radio was launched in Canada on November 29, 2005. Sirius followed two days later on December 1, 2005. Sirius Canada and XM Radio Canada announced their merger into Sirius XM Canada on November 24, 2010. It was approved by the Canadian Radio-television and Telecommunications Commission on April 12, 2011.
Africa and Eurasia.
WorldSpace was founded by Ethiopia-born lawyer Noah Samara in Washington, D.C., in 1990, with the goal of making satellite radio programming available to the developing world. On June 22, 1991, the FCC gave WorldSpace permission to launch a satellite to provide digital programming to Africa and the Middle East. WorldSpace first began broadcasting satellite radio on October 1, 1999, in Africa. India would ultimately account for over 90% of WorldSpace’s subscriber base. In 2008, WorldSpace announced plans to enter Europe, but those plans were set aside when the company filed for Chapter 11 bankruptcy in November 2008. In March 2010, the company announced it would be de-commissioning its two satellites (one served Asia, the other served Africa). Liberty Media, which owns 50% of Sirius XM Radio, had considered purchasing WorldSpace’s assets, but talks between the companies collapsed.
Japan.
MobaHo! was a mobile satellite digital audio/video broadcasting service in Japan, whose services began on October 20, 2004, and ended on March 31, 2009.
System design.
Satellite radio uses the 2.3 GHz S band in North America for nationwide digital audio broadcasting (DAB). MobaHO! operated at 2.6 GHz. In other parts of the world, satellite radio uses the 1.4 GHz L band allocated for DAB.
Satellite radio subscribers purchase a receiver and pay a monthly subscription fee to listen to programming. They can listen through built-in or portable receivers in automobiles; in the home and office with a portable or tabletop receiver equipped to connect the receiver to a stereo system; or on the Internet.
Ground stations transmit signals to the satellites, which are orbiting over 22,000 miles above the surface of the Earth. The satellites send the signals back down to radio receivers in cars and homes. This signal contains scrambled broadcasts, along with meta data about each specific broadcast. The signals are unscrambled by the radio receiver modules, which display the broadcast information. In urban areas, ground repeaters enable signals to be available even if the satellite signal is blocked. The technology allows for nationwide broadcasting, so that, for instance US listeners can hear the same stations anywhere in the country.
Content, availability and market penetration.
Satellite radio in the US offers commercial-free music stations, as well as talk, news and sports, some of which include commercials. In 2004, satellite radio companies in the United States began providing background music to hotels, retail chains, restaurants, airlines and other businesses. Sirius XM Music for Business is a cheaper alternative to the costlier Muzak, and it doesn’t come with a long-term commitment. On April 30, 2013, Sirius XM CEO Jim Meyer stated that the company would be pursuing opportunities over the next few years to provide in-car services through their existing satellites, including telematics (automated security and safety, such as stolen vehicle tracking and roadside assistance) and entertainment (such as weather and gas prices).
As of April 2013, Sirius XM had 24.4 million subscribers. This was primarily due to the company’s partnerships with automakers and car dealers. Roughly 60% of new cars sold come equipped with Sirius XM, and just under half of those units gain paid subscriptions. The company has long-term deals with General Motors, Ford, Toyota, Kia, Bentley, BMW, Volkswagen, Nissan, Hyundai and Mitsubishi. The presence of Howard Stern, whose show attracts over 12 million listeners per week, has also been a factor in the company’s steady growth. As of 2013, the main competition to satellite radio is streaming Internet services, such as Pandora and Spotify, as well as FM and AM Radio.
Satellite radio vs. other formats.
Satellite radio differs from AM or FM radio and digital television radio (or DTR) in the following ways. The table applies primarily to the United States.
² The sound quality with both satellite radio providers and DTR providers varies with each channel. Some channels have near CD-quality audio, and others use low-bandwidth audio suitable only for speech. Since only a certain amount of bandwidth is available within the licenses available, adding more channels means that the quality on some channels must be reduced. Both the frequency response and the dynamic range of satellite channels can be superior to most, but not all AM or FM radio stations, as most AM and FM stations clip the audio peaks to sound louder; even the worst channels are still superior to most AM radios, but a very few AM tuners are equal to or better than the best FM or satellite broadcasts when tuned to a local station, even if not capable of stereo. The use of HD Radio technology can allow AM and FM broadcasts to exceed the quality of satellite. AM does not suffer from multipath distortion or flutter in a moving vehicle like FM, nor does it become silent as you go behind a big hill like satellite radio.
³ Some satellite radio services and DTR services act as "in situ" repeaters for local AM/FM stations and thus feature a high frequency of interruption.
4 Nonprofit stations and public radio networks such as CBC/Radio-Canada, NPR, and PRI-affiliated stations and the BBC are commercial-free. In the US, all stations are required to have periodic station identifications and public service announcements.
5 In the United States, the FCC regulates technical broadcast spectrum only. Program content is unregulated. However, the FCC has tried in the past to expand its reach to regulate content to satellite radio and cable television, and its options are still open to attempt such in the future. The FCC does issue licenses to both satellite radio providers (XM and Sirius) and controls who holds these licenses to broadcast.
6 Degree of content regulation varies by country; however, the majority of industrialized nations have regulations regarding obscene and/or objectionable content.

</doc>
<doc id="67067" url="https://en.wikipedia.org/wiki?curid=67067" title="Staggers Rail Act">
Staggers Rail Act

The Staggers Rail Act of 1980 is a United States federal law that deregulated the American railroad industry to a significant extent, and replaced the regulatory structure that existed since the 1887 Interstate Commerce Act.
Background.
In the aftermath of the Great Depression and World War II, many railroads were driven out of business due to competition from the new interstate highway system and airlines. The rise of the automobile led to the end of passenger train service on most railroads. Trucking businesses had become major competitors by the 1930s with the advent of improved paved roads, and after the war they expanded their operations as the highway network grew, and acquired increased market share of freight business. During this time, however, the railroads continued to be regulated by the Interstate Commerce Commission (ICC) and a complex system for setting shipping rates.
The Staggers Act followed the Railroad Revitalization and Regulatory Reform Act of 1976 (often called the "4R Act"), which reduced federal regulation of railroads and authorized implementation details for Conrail, the newly created northeastern railroad system. The 4R reforms included allowance of a greater range for railroad pricing without close regulatory restraint, greater independence from collective rate making procedures in rail pricing and service offers, contract rates, and, to a lesser extent, greater freedom for entry into and exit from rail markets.
Although the 4R Act established these guidelines, the ICC at first did not give much effect to its legislative mandates. However, as regulatory change began to appear in the 1976-79 period, including the phasing in of the loss of collective ratemaking authority, most of the major railroads shifted away from their effort to maintain the historic regulatory system, and came to support greater freedom for rail pricing, both as to higher and lower rail rates. Major railroad shippers also continued to believe that they would be better served by more flexibility to arrive at tailored arrangements that were mutually beneficial to a particular shipper, and to the carrier serving a particular shipper. These judgments supported a second round of legislation.
Provisions of the act.
The major regulatory changes of the Staggers Act were as follows:
The Act also had provisions allowing the Commission to require access by one railroad to another railroad's facilities where one railroad had in effect "bottleneck" control of traffic. These provisions dealt with "reciprocal switching" (handling of railroad cars between long-haul rail carriers and local customers) and trackage rights. However, these provisions did not have as much effect as those described above.
Sponsor.
The act was named for Congressman Harley Staggers (D-WV), who chaired the House Interstate and Foreign Commerce Committee. Although it is traditional for laws to be known by the names of their sponsors, this is believed to be the first (but not last) case in which the sponsor's name was officially incorporated into the text of a Federal statute.
Impact of the law.
Studies of the rail industry showed dramatic benefits for both railroads and their users from this alteration in the regulatory system. According to the Department of Transportation's Freight Management and Operations section's studies, railroad industry costs and prices were halved over a ten-year period, the railroads reversed their historic loss of traffic (as measured by ton-miles) to the trucking industry, and railroad industry profits began to recover after decades of low profits and widespread railroad insolvencies. In 2007 the Government Accountability Office reported to Congress that "The railroad industry is increasingly healthy and rail rates have generally declined since 1985, despite recent rate increases... There is widespread consensus that the freight rail industry has benefited from the Staggers Rail Act."
The Association of American Railroads, the principal railroad industry trade association, stated that the Staggers Act has led to a 51 percent reduction in average shipping rates, and that $480 billion has been reinvested by the industry into their rail systems.
Related deregulatory legislation.
The Staggers Act was one of three major deregulation laws passed by Congress in a two-year period, as the cumulative result of efforts to reform transport regulation begun in 1971, during the Richard Nixon Administration. The other two laws were the Airline Deregulation Act of 1978 and the Motor Carrier Act of 1980. This legislation in effect superseded almost a century of detailed regulation begun with the establishment of the ICC in 1887. The Interstate Commerce Commission Termination Act of 1995 abolished the ICC, and created its successor agency, the Surface Transportation Board, an administrative affiliate of the United States Department of Transportation.

</doc>
<doc id="67068" url="https://en.wikipedia.org/wiki?curid=67068" title="Railroad Revitalization and Regulatory Reform Act">
Railroad Revitalization and Regulatory Reform Act

The Railroad Revitalization and Regulatory Reform Act of 1976, often called the "4R Act," is a United States federal law that established the basic outlines of regulatory reform in the railroad industry and provided transitional operating funds following the 1970 bankruptcy of Penn Central Transportation Company. The law approved the "Final System Plan" for the newly created Conrail and authorized acquisition of Northeast Corridor tracks and facilities by Amtrak.
The Act was the first in a series of laws which collectively are described as the deregulation of transportation in the United States. It was followed by the Airline Deregulation Act (1978), Staggers Rail Act (1980), and the Motor Carrier Act of 1980.
Background.
Following the massive bankruptcy of the Penn Central in 1970, Congress created Amtrak to take over the failed company's intercity passenger train service, under the Rail Passenger Service Act. Congress passed the Regional Rail Reorganization Act of 1973 (the "3R Act") to salvage viable freight operations from Penn Central and other failing rail lines in the northeast, mid-Atlantic and midwestern regions, through the creation of Conrail.
Overview of law.
The 4R Act:
The "Declaration of policy" in the Act (Section 101), was as follows:
The financial assistance provisions of the Act were largely palliative and transitional. They were extended on the condition that changes in the regulatory system governing railroads be enacted, with the hope that a regulatory system which gave railroads more freedom in pricing and service arrangements, subject to greater competitive constraints, would yield a more viable industry and better service for its users. Studies of the legislative history of the Act indicate that the Gerald Ford administration secured the regulatory provisions only by threatening a veto of any act containing financial assistance for railroads but no reform of the regulatory system.
The changes in regulation provided for were as follows:
In a signing statement, President Ford stated,
Initial reaction to the act.
Many of the members of the Interstate Commerce Commission (ICC) at the time of law's enactment were highly unsympathetic to the aims and provisions of the 4R Act. The regulatory provisions had been enacted over several commissioners' objections, and the Commission's implementation of the Act initially had little impact on the way the rail industry functioned.
When President Jimmy Carter nominated A. Daniel O’Neal (originally appointed by President Richard Nixon) to chair the ICC, O’Neal began to develop the possibilities for opening up the rail market to competition and innovation. Also, in 1978 a group of major railroads formed an organization called TRAIN (Transportation by Rail for Agricultural and Industrial Needs) to support further deregulation of the industry. These carriers’ perception was that with collective rate making limited, and a Commission apparently more interested in letting their rates go down than go up, the regulatory system, as a whole, in the net, no longer favored them.
Large shippers of goods by rail also wished to have more flexibility in the rail market. The net result of compromise between the carriers and the shippers, and the Jimmy Carter administration’s push for a more competitive transport was the Staggers Act of 1980. The Staggers Act worked from the 4R Act template, but extended its provisions. One of the key changes from the 1976 Act was allowance of secret contracts between carriers and shippers, not limited to large-investment situations and not effectively subject to regulatory review. According to former Congressional Budget Office analyst Christopher Barnekov, such contracts allowed the rail carriers and their shippers much more opportunity readily to develop more efficient transport arrangements which lowered costs for the carriers, yielding better returns for the carriers and lower rates for the shippers.
Thus railroad "deregulation" was a two step process, starting with the 4R Act and concluding with the Staggers Act. In substance, the railroad regulatory reform legislation, in the 1970-1980 period, turned toward greater use of market systems to deal with the problems of the rail industry in the United States, rather than resorting to nationalization, which had been considered from time to time.

</doc>
<doc id="67069" url="https://en.wikipedia.org/wiki?curid=67069" title="Wesley Crusher">
Wesley Crusher

Wesley Crusher is a fictional character in the television series "", appearing regularly in the first four seasons and sporadically afterwards. He is the son of Beverly Crusher and is portrayed by actor Wil Wheaton.
Overview.
Television series and films.
In the television series "Star Trek: The Next Generation", Wesley Crusher first arrives on the "Enterprise"-D with his mother, soon after Captain Jean-Luc Picard assumes command. Crusher's father was killed while under Picard's command, with Picard delivering the message to Wesley and to his mother, Beverly. Picard initially found Wesley irritating, as he is often uncomfortable around all children, a fact which he discloses to his first officer, Commander William Riker, in the pilot episode "Encounter at Farpoint". In early episodes of the series, Picard does not allow Wesley on the bridge of the ship. However, during the first season, Picard comes to realize that he understands many things beyond his age and has inherited his mother's high level of intelligence, and grants him more opportunities on board the ship. An alien known as The Traveler tells Captain Picard that Wesley possesses a unique intelligence and great potential when provided encouragement and opportunity, comparing him to a child prodigy like Mozart. Picard soon appoints Crusher as an acting ensign.
Crusher eventually takes the entrance exam for Starfleet Academy. His test score ranks lower than required, and he is not accepted into the Academy in his first attempt, as shown in the episode "". Later, he misses his second chance to take the Academy entrance exam during the episode "Ménage à Troi" in which he assists the "Enterprise"-D crew in rescuing Riker, Deanna Troi, and from hostile Ferengi, for which Picard grants him a field promotion to full ensign.
In the third season episode "The Bonding", Wesley reveals that following his father's death, he harbored animosity towards Picard, because Picard was in command of the "Stargazer" during the mission in which Wesley's father was killed. By the end of the episode, he no longer harbors these feelings.
Crusher is then invited to reapply the following year, taking the exam and being accepted into the Academy where he joins an elite group of cadets known as Nova Squadron. His involvement with this group leads to his losing academic credits when a squadron-mate is killed attempting a dangerous and prohibited flight maneuver and, under pressure from the team leader Nick Locarno, Crusher abets the squadron's efforts to cover up the truth. Although the crew's intervention and Crusher's own testimony saves him from expulsion, all of Cadet Crusher's academic credits for the year are canceled and he is required to repeat the year and graduate after most of the rest of his class. He remains in the Academy thereafter until the Traveler re-contacts him, whereupon he resigns his commission and goes with the Traveler to explore other planes of reality.
He is next seen sitting next to his mother in the background of the wedding scenes in the feature film '. In a scene deleted from the movie, Captain Picard asks Crusher if he's excited to serve on board the USS "Titan" (Captain Riker's ship), and Crusher tells him that he will be running the night shift in Engineering, indicating that Wesley returned to Starfleet at some point prior to the events of the film.
Spin-off works.
The subsequently released "" novel miniseries and the 2007 novel relaunch of "Star Trek: The Next Generation" revealed that Crusher had become a full-fledged Traveler. He was wearing a Starfleet uniform in "Star Trek: Nemesis" because he had mistakenly arrived to Riker and Troi's wedding naked, expecting a Betazoid wedding. Several years later, Crusher enlisted the aid of the "Enterprise"-E crew in stopping a powerful machine from destroying all organic life in the galaxy.
Reception.
The Wesley Crusher character was widely unpopular among Star Trek fans. Many fans considered the character to be a Gary Stu and a stand-in for Gene Roddenberry, whose middle name was "Wesley". The character's role in the show was greatly downplayed after the first season when Roddenberry's involvement in the show's production became more peripheral, and he was written out altogether in the season four episode Final Mission following the end of Roddenberry's hands-on involvement.
Some fans disliked the idea of a young boy who seems to constantly save the whole ship as a deus ex machina plot device. Commentators have observed at least seven times in which Wesley, "who has trouble getting into the Starfleet Academy" and is on a ship "filled with Starfleet's best and brightest crew members", has come up "with the needed solution". The dislike of Star Trek fans for Wesley Crusher has become somewhat of a pop-culture meme, reflected in other TV shows such as "The Big Bang Theory" and in a 2009 "Family Guy" episode, "Not All Dogs Go to Heaven", which included the main "The Next Generation" cast and featured Wil Wheaton in character as Wesley being bullied by Patrick Stewart.

</doc>
<doc id="67072" url="https://en.wikipedia.org/wiki?curid=67072" title="Tasha Yar">
Tasha Yar

Natasha "Tasha" Yar is a fictional character that mainly appeared in the of the American science fiction television series "". Portrayed by Denise Crosby, she is chief of security aboard the Starfleet starship USS "Enterprise"-D and carries the rank of lieutenant. The character's concept was originally based upon the character of Vasquez from the 1986 film "Aliens". Following further development she became known first as Tanya, and then Tasha. Crosby had originally auditioned for the role of Deanna Troi, while Rosalind Chao became a favorite for Tasha. After Marina Sirtis auditioned for the role, the show's creator Gene Roddenberry decided to switch the roles for the actresses, with Sirtis becoming Troi and Crosby becoming Yar (Chao would later appear on the series in a recurring role as Keiko O'Brien).
The character first appeared in the series' pilot episode, "Encounter at Farpoint". After Crosby decided to leave the show, Yar was killed by the creature Armus in "Skin of Evil", the 23rd episode of the season. She was written back into the show for a guest appearance in the episode "Yesterday's Enterprise", in which the timeline was altered so that she did not die, and again in the final episode of the series "", in events set prior to the pilot.
She was described as a forerunner to other strong women in science fiction, such as Kara Thrace from the 2004 version of "Battlestar Galactica", while providing a step between the appearances of female characters in ' to the command positions they have in ' and "". Questions were raised over the sexuality of the character, and it was thought that the events in the episode "The Naked Now" were designed to establish her heterosexuality. The manner of her first death was received with mostly negative reviews. One critic called it typical of the death of a "Star Trek" security officer, and the scene was also included in a list of "naff" sci-fi deaths.
Concept and development.
Inspired by Vasquez in "Aliens", the character was initially named "Macha Hernandez" and was the tactical officer of the "Enterprise". This had been changed by the first casting call—issued on December 10, 1986—when she was given the position of security chief. The producers considered Jenette Goldstein, who had played Vasquez, for the role, but writer Dorothy Fontana pointed out that the actress "is not Latina. She is petite, blue-eyed, freckle-faced". The character was subsequently renamed "Tanya" around March 13.
By the time that the writers' and directors' guide for the series was published, dated March 23, 1987, the character was named Natasha "Tasha" Yar. Her surname was suggested by Robert Lewin, drawing inspiration from the Babi Yar atrocities in Ukraine during the Second World War. Her biography stated that she was 28 years old, and confirmed her Ukrainian descent. She was planned to have a friendship with teenager Wesley Crusher, and was described in the guide as "treat this boy like the most wonderful person imaginable. Wes is the childhood friend that Tasha never had." In April 1987, Lianne Langland, Julia Nickson, Rosalind Chao, Leah Ayres and Bunty Bailey were each listed as being in contention for the role. Chao was a favorite candidate, while Denise Crosby was described as "the only possibility" for the character of Troi. The production staff were not keen on having two actresses in the bridge crew roles with similar physical types and hair colors, and so the team took account of the casting of the two parts together. The writers and directors guide described Yar as having a muscular but very feminine body type, and being sufficiently athletic to defeat most other crew members in martial arts. After Crosby and Marina Sirtis had each auditioned for Troi and Yar respectively, Gene Roddenberry decided to switch the actresses and cast Crosby as Tasha Yar. He felt that Sirtis' appearance was better suited to the "exotic" Troi.
Before the end of the first season, Crosby asked to be released from her contract as she was unhappy that her character was not being developed. She later said "I was miserable. I couldn't wait to get off that show. I was dying." Roddenberry agreed to her request, and she left on good terms. The final episode she filmed was "", which was completed after Yar's death in "Skin of Evil". Her last scene was during the final act of the episode, in which she can be seen waving goodbye to the camera as the cargo bay doors close. After her departure, archive footage of Crosby as Yar was used in the episodes "" and "".
Crosby was happy to return in "Yesterday's Enterprise" due to the strength of the script, saying that "I had more to do in that episode than I'd ever had to do before." Prior to the episode being aired, the media had to be reassured that Yar was not returning in a dream sequence. Following her appearance in that episode, Crosby pitched the idea of Yar's daughter, Sela, to the producers. She made her first appearance in this role in the two-part "", and appeared once more in another two-part episode, "".
Denise Crosby returned twice more in the non-canon "Star Trek" universe. In 2007, she appeared as an ancestor of Tasha Yar, Jenna Yar, in "", an episode of the fan-produced series "". Tasha Yar was written into "Star Trek Online" as part of the three-year anniversary celebration in 2013. Denise Crosby recorded audio for the game, in scenes set after those in "Yesterday's Enterprise".
Appearances.
Natasha Yar's origins are explained in the season four episode "". She was born on the planet Turkana IV in 2337. She had a younger sister named Ishara (Beth Toussaint), who was born five years after her. Shortly after Ishara's birth, the girls' parents were killed and they were taken in by other people. However, they were subsequently abandoned and Tasha was required to look after her sister on her own. The government on the planet had collapsed, and the sisters were forced to scavenge for food while avoiding rape gangs. In 2352, aged 15, Tasha managed to leave Turkana IV. She never saw Ishara again; the latter joined the "Coalition", one of the factions on the planet before Tasha left. Tasha refused to join the cadres on the planet, blaming them for her parent's deaths.
Yar appeared for the first time in the pilot episode of "" as the Security and Tactical Officer on board the USS "Enterprise"-D. When Captain Picard (Patrick Stewart) orders an emergency saucer separation, Yar is one of the bridge crew to accompany him to the battle bridge. She is amongst the crew abducted by Q (John de Lancie), and later serves on the away team to Farpoint Station. In "The Naked Now", while the crew are under the influence of an alien ailment, she initiates a sexual encounter with the android Data (Brent Spiner). In "" Yar is abducted by Lutan (Jessie Lawrence Ferguson), the leader of the planet Ligon II, after she demonstrates her combat skills on the holodeck. She defeats Lutan's wife Yareena in ritual combat. Yareena is revived on the "Enterprise" by Doctor Crusher (Gates McFadden). During the events of "Where No One Has Gone Before", Yar begins to hallucinate that she is back on Turkana IV and running for her life. In "The Arsenal of Freedom", Yar and Data are trapped together on the surface of the planet Minos and are attacked by a series of sentry probes which adapt to Data and Yar's phasers. The situation is resolved by Captain Picard, who is trapped elsewhere on the planet's surface with Dr. Crusher.
Yar forms part of the away team which beams down to Vagra II to rescue Deanna Troi (Marina Sirtis) from a crashed shuttlecraft in "Skin of Evil". She is killed by the creature Armus (Mart McChesney and Ron Gans) in a display of his power. The crew hold a memorial service for her on the holodeck, and Worf (Michael Dorn) replaces her as chief tactical and security officer. After her death it is revealed that Data keeps a small hologram of her in his quarters. Despite Data's lack of emotions, he is described by reviewers as being sentimentally attached to her image. During the court hearing on Data's stature as a sentient being in "", he explains that he and Yar were intimate and that she was special to him.
After the USS "Enterprise"-C emerges from a rift in space-time in "Yesterday's Enterprise", the timeline is changed and Yar is once again alive and in her former position on the "Enterprise"-D. She works with the older Enterprise's helmsman, Richard Castillo (Christopher McDonald), and the two become close. Guinan (Whoopi Goldberg), who has some awareness of the timeline that would be restored by the "Enterprise"-C returning into the rift, confides in Yar that she believes that Yar died senselessly in that timeline. Based on that advice, Yar transfers to the "Enterprise"-C and returns with it to two decades into the past, and its expected destruction at the hands of the Romulans while defending the Klingon outpost Narendra III. The alternative universe version of Yar travelled back in time on board the "Enterprise"-C, and into the main timeline. This process was later described as "world jumping" rather than a typical timeline travel story by critics.
Yar's half-Romulan daughter Sela explains in "Redemption" that several members of the "Enterprise"-C crew were captured by the Romulans when it returned through the rift, including Yar. A Romulan general offered to spare the crew's lives if she became his consort. After a year, Yar gave birth to Sela. When Sela was four, Yar attempted to escape but Sela screamed to prevent her from being taken away from her father. After she was caught, Yar was executed.
The series finale "" includes Yar's final appearance, in scenes which take place prior to and in the early parts of "Encounter at Farpoint". As most of the bridge crew are yet to join the "Enterprise"-D in the scenes, Yar is one of the senior members of the crew under Captain Picard in the earliest of the three timeframes in the episode. She needs to be convinced by Picard to put the ship in danger in order to destroy the temporal anomaly that threatens to prevent life from evolving on Earth.
Reception and commentary.
Science fiction writer Keith DeCandido considered Yar the most interesting role to appear in the "writer's bible", while Hal Boedeker characterized her as "forceful" in an article on women in "Star Trek" for "Knight Ridder". A Den of Geek article also about women in "Star Trek" described the character as a predecessor to Kara "Starbuck" Thrace in the 2004 re-imagining of "Battlestar Galactica". A "Post-Tribune" review of the series following the pilot described Yar as a "tough cookie" and the reviewer's favorite crew member. Frank Oglesbee, in his article on Kira Nerys, outlined the progression of female roles in "gender assumptions" from ' where women were on the bridge, through Tasha Yar in "The Next Generation" where they were in command positions, to "Deep Space Nine" and ' where women were in lead parts. He noted specifically that women appeared in command positions more regularly as main and supporting characters, and were portrayed as more assertive and combative, with leading roles in action sequences.
In Sarah Projansky's contribution on rape in "Star Trek" to the book "Enterprise Zones: Critical Positions on Star Trek", she extrapolates that Yar's introduction to Starfleet was similar to the actions of United States Army soldiers issuing supplies to the survivors of the atomic bombings of Hiroshima and Nagasaki and the liberation of Nazi concentration camps during the Second World War. The idea of an American savior of colonial rape victims stems from U.S. propaganda during the war, stating that "In "TNG", the Federation citizen represents a new and improved version of this U.S. savior citizen; the Federation citizen is a post-nationalist, post-sexist, and post-racist soldier—feminist".
Reviewers have questioned the character's sexuality since the end of the series. "Curve" magazine speculated that Yar was a "closeted" lesbian. In the book "Science Fiction Audiences: Watching Doctor Who and Star Trek", the authors describe her as "an obvious bisexual", but that "she should be a lesbian". Referring to the events in "The Naked Now", the authors explain "when they decided to straighten her, they used an android. So we ended up heterosexualizing two perfectly wonderful characters". The authors of the book "Deep Space and Sacred Time: Star Trek in the American Mythos" also thought that having Data and Yar consummate sexually was a means to state early on in the series the heterosexuality of the two most androgynous characters in the show.
Fans responded negatively to the departure of Yar as they felt that the character had potential for future expansion. Reviewers were also critical of the manner of Yar's death. Keith DeCandido called it "pointless", but thought that it was no worse than the deaths of other security officer "redshirts" throughout the history of "Star Trek". He said that he preferred her death in "Skin of Evil" to the "clichéd-up-the-wazoo" death she experienced in "Yesterday's Enterprise". Gary Westfahl, in his book "Space and Beyond: The Frontier Theme in Science Fiction", described Yar's death as one of the most notable ones in "Star Trek", alongside that of Spock in "" and James T. Kirk in "Star Trek Generations". "SFX" magazine included her first death in a 2012 list of the top 21 "Naff Sci-Fi Deaths", while the "Chicago Sun-Times" described her death in "Yesterday's Enterprise" as a "hero's death".

</doc>
<doc id="67073" url="https://en.wikipedia.org/wiki?curid=67073" title="Ro Laren">
Ro Laren

Ro Laren is a recurring character on "", played by Michelle Forbes.
Ro Laren was also planned to be a main character in the next Star Trek television series, "". The creators of "Deep Space Nine" wanted a strong-willed Bajoran character who would engender interpersonal conflict with the station's commander, Benjamin Sisko. However, Michelle Forbes turned down the role, so another Bajoran character named Kira Nerys was created to be Sisko's second-in-command. 
Appearances.
Ro Laren appears as a recurring character in six episodes of "The Next Generation"'s fifth season, and also appears in one episode of season six and one episode in season seven:
Fictional character biography.
Ro Laren's personnel file, seen in the episode "", indicates she was born on January 17, 2340 on Bajor to Ro Talia and Gale, and that she attended Starfleet Academy from 2358 to 2362.
In accordance with Bajoran tradition, her family name precedes her personal name. When she first joins the crew of the "Enterprise"-D, Picard addresses her as "Ensign Laren". She corrects him, saying that Laren is her given name and Ro is her family name. She is then referred to as "Ensign Ro".
Ro joins the crew as a troubled young Bajoran officer who was court-martialled while serving on the USS "Wellington" for disobeying orders on an away mission, resulting in eight deaths. She was imprisoned on Jaros II, but Admiral Kennelly gives her a second chance by assigning her to a mission alongside the "Enterprise" crew. 
Once past her initial hostility, Ro forms friendships with several "Enterprise" crew members, including Ten Forward bar manager , Lieutenant Commander Geordi La Forge, and Commander William Riker. As a Bajoran, she has a great dislike for Cardassians, as she was forced to watch them torture her father to death when she was a child. In the first episode in which she appears, "", she is given a full pardon in exchange for going on a secret mission to help stop dissident raids in order to protect a treaty with Cardassia.
In "The Next Phase", Ro and La Forge are presumed dead after a transporter malfunction while returning from rescue operations on a disabled Romulan ship. Upon regaining consciousness, they discover they have become invisible and intangible to everyone but each other. At first, Ro believes they are dead, converted to a ghostlike existence, which awakens Bajoran spiritual beliefs she thought she had abandoned decades earlier. By invisibly observing the crew of the Romulan vessel, Ro and La Forge discover that the accident, as well as the damage to the Romulan ship, is the result of a Romulan weapons test and that the Romulans are performing sabotage that will destroy the "Enterprise"-D the moment it accelerates to warp drive. After defeating a similarly cloaked Romulan operative, Ro and La Forge alert the crew and return to normal, warning Picard and averting disaster.
In the episode "", Ro, Picard, Guinan and Keiko O'Brien are transformed into adolescent versions of themselves while retaining their adult intelligence. As children, they foil a Ferengi plot to steal the "Enterprise" and sell the crew as slave labor. During this episode, some of the unpleasant backstory of Ro's original childhood is revealed. The adolescent Ro is portrayed by Megan Parlen.
As Executive Officer, Commander Riker is a strict taskmaster toward the former convict while she struggles to earn his trust and respect. However, their mutual animosity is forgotten when they and the rest of the crew suffer forced amnesia in "". Ro concludes that her feelings toward Riker are those of passion and sexual familiarity, and she invites herself to his quarters, seducing him. When their memories are restored, Counselor Troi explains that the pair likely always had sexual attraction toward each other.
In her final episode, "", the newly promoted Lieutenant Ro is asked by Starfleet to infiltrate the rebel Maquis, who are undermining the Federation treaty with Cardassia. During the mission, she finds her loyalties in conflict with her duties as she becomes increasingly sympathetic to the plight of the Maquis. Despite Picard's effort to force her to do her duty, she betrays Starfleet and joins the rebel group. Riker says she seemed pretty sure she was doing the right thing, though she regretted disappointing Picard.
Ro's life after leaving Starfleet for the Maquis is a subject for some novels (which the copyright holder Paramount has treated as non-canonical compared to the series and films although the novels have not been openly contradicted by canon). Within "Star Trek" canon, the Dominion helps Cardassia exterminate the majority of the Maquis, but Ro's specific fate is unknown.
In many of the "Star Trek" novels, Ro Laren becomes the chief of security aboard Deep Space Nine (DS9), fulfilling the writers' aforementioned original intention for the character to be involved in the spin-off series. According to the most recent stories, Ro Laren has been promoted to Captain and is now commanding officer of Deep Space Nine. However, the novels are not considered canonical by Paramount, owners of the "Star Trek" television and film franchise.
An unproduced script for Star Trek Voyager was to have included a flashback to Tom Paris' years at Starfleet Academy, and shown that Ro was a classmate of his.
Novel appearances.
The character of Ro Laren is further developed in the novels. They agree that Ro continued to fight for the Maquis until its destruction. According to "Abyss", during this time, she crashed on Sindorin and was rescued by Ingavi colonists.
After the demise of the Maquis, the novels diverge. In "Behind Enemy Lines", Ro comes to Picard as a refugee on the Bajoran vessel "Orb of Peace". However this account, and her relationship with Picard in general, is contradicted in the "relaunch" series. According to "Avatar", after the Dominion's destruction of the Maquis, Ro continued to fight in resistance cells until the Dominion's retreat to Cardassia, and had no contact with Picard. The "relaunch" has been the basis for DS9's publishing line as of "Avatar" in 2001.
After Sisko's disappearance and the end of the Dominion War, Ro returns to Bajor where she is given a commission in the Bajoran Militia and a position in their planetside headquarters. Not finding a place there, she is reassigned to DS9 as station's chief of security. When Bajor finally joins the Federation, Ro intends to leave her post to escape punishment for crimes she committed with the Maquis, but is convinced by Picard to accept a pardon and rejoin Starfleet. She is granted the rank of Lieutenant and remains on DS9 as station's chief of security. She and Quark develop mild romantic feelings for each other.
As of the "Typhon Pact" series of novels, Ro has been promoted to Captain and serves as the commanding officer of DS9 and its successor station following the first DS9's destruction by the Typhon Pact. 

</doc>
