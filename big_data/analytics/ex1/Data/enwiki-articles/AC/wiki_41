<doc id="14973" url="https://en.wikipedia.org/wiki?curid=14973" title="Ithaca, New York">
Ithaca, New York

The city of Ithaca is in New York's Southern Tier region and is the county seat of Tompkins County, as well as the largest community in the Ithaca-Tompkins County metropolitan area. This contains the municipalities of the Town of Ithaca, the village of Cayuga Heights, and other towns and villages in Tompkins County. The city of Ithaca is located on the southern shore of Cayuga Lake, in Central New York. It is named for the Greek island of Ithaca.
Ithaca is home to Cornell University, an Ivy League school of over 20,000 students, most of whom study at its local campus. Ithaca College is located just south of the city in the Town of Ithaca, adding to the area's "college town" atmosphere. Nearby is Tompkins Cortland Community College (TC3). These three colleges bring tens of thousands of students who increase Ithaca's seasonal population during the school year. Some students settle in the area after graduation. The city's voters are notably more liberal than those in the remainder of Tompkins County or in upstate New York, generally voting for Democratic Party candidates.
In 2010, the city's population was 30,014. In 2004, Gayraud Townsend, a 20-year-old senior in Cornell's School of Industrial and Labor Relations, was sworn in as alderman of the city council, the first black male to be elected to the council and the youngest African American to be elected to office in the United States. He served his full term and has mentored other student politicians. In 2011 Cornell Class of 2009 graduate Svante Myrick was elected as the youngest mayor of the city of Ithaca.
Namgyal Monastery in Ithaca is the North American seat of Tenzin Gyatso, the 14th Dalai Lama.
History.
Early history.
Indigenous people occupied this area for thousands of years. At the time of European contact, this area was controlled by the Cayuga Indians, one of the powerful Five Nations of the "Haudenosaunee" or Iroquois League. Jesuit missionaries from New France (Quebec) are said to have had a mission to the Cayuga as early as 1657.
Saponi and Tutelo Indians, Algonquian-speaking tribes, later occupied lands at the south end of Cayuga Lake. Dependent tributaries of the Cayuga, they had been permitted to settle on the tribe's hunting lands at the south end of Cayuga Lake, as well as in Pony (originally Sapony) Hollow of what is known as present-day Newfield, New York. Remnants of these tribes had been forced from North Carolina by tribal conflicts and European colonial encroachment. Similarly, the Tuscarora people, an Iroquoian-speaking tribe from the Carolinas, migrated after defeat in the Yamasee War; they settled with the Oneida people and became the sixth nation of the Haudenosaunee, with chiefs stating the migration was complete in 1722.
During the Revolutionary War, four of the then six Iroquois nations were allied with the British, although bands made decisions on fighting in a highly decentralized way. Conflict with the rebel colonists was fierce throughout the Mohawk Valley and western New York. In retaliation for conflicts to the east, the 1779 Sullivan Expedition was conducted against the Iroquois peoples in the west of the state, destroying more than 40 villages and stored winter crops. It destroyed the Tutelo village of Coregonal, located near what is now the junction of state routes 13 and 13A just south of the Ithaca city limits. Most Iroquois were forced from the state after the Revolutionary War, but some remnants remained. The state sold off the former Iroquois lands to stimulate development and settlement by European Americans; lands were also granted as payment to veterans of the war.
Within the current boundaries of the City of Ithaca, Native Americans maintained only a temporary hunting camp at the base of Cascadilla Gorge. In 1788 eleven men from Kingston, New York came to the area with two Delaware people (Lenape) guides, to explore what they considered wilderness. The following year Jacob Yaple, Isaac Dumond, and Peter Hinepaw returned with their families and constructed log cabins. That same year Abraham Bloodgood of Albany obtained a patent from the state for 1400 acres, which included all of the present downtown west of Tioga Street. In 1790, the federal government and state began an official program to grant land in the area, known as the Central New York Military Tract, as payment for service to the American soldiers of the Revolutionary War, as the government was cash poor. Most local land titles trace back to these Revolutionary war grants.
Partition of the Military Tract.
As part of this process, the Central New York Military Tract, which included northern Tompkins County, was surveyed by Simeon DeWitt, Bloodgood's son-in-law. DeWitt was also the nephew of Governor George Clinton. The Commissioners of Lands of New York State (chairman Gov. George Clinton) met in 1790. The Military Tract township in which proto-Ithaca was located was named the Town of Ulysses. A few years later DeWitt moved to Ithaca, then called variously "The Flats," "The City," or "Sodom"; he renamed it for the Greek island home of Ulysses in the spirit of the multitude of settlement names in the region derived from classical literature, such as Aurelius, Ovid, and especially of Ulysses, New York, the town that contained Ithaca at the time.
Around 1791 DeWitt surveyed was is now the present downtown area into lots and sold them at modest prices. That same year John Yaple built a grist mill on Cascadilla Creek. The first frame house was erected in 1800 by Abram Markle. In 1804 the village had a postmaster, and in 1805 a tavern.
Growth.
Ithaca became a transshipping point for salt from curing beds near Salina, New York to buyers south and east. This prompted construction in 1810 of the Owego Turnpike. When the War of 1812 cut off access to Nova Scotia gypsum, used for fertilizer, Ithaca became the center of trade in Cayuga gypsum. The Cayuga Steamboat Company was organized in 1819 and in 1820 launched the first steamboat on Cayuga Lake, the "Enterprise." In 1821, the village was incorporated at the same time the Town of Ithaca was organized and separated from the parent Town of Ulysses. In 1834, the Ithaca and Owego Railroad's first horse-drawn train began service, connecting traffic on the east-west Erie Canal (completed in 1825) with the Susquehanna River to the south to expand the trade network.
With the depression of 1837, the railroad was re-organized as the Cayuga & Susquehanna. It was re-engineered with switchbacks in the late 1840s; much of this route in the late 20th century was converted to trails under the Rails to Trails program; it is used by the South Hill Recreation Way.
However, easier railroad routes were constructed, such as that of the Syracuse, Binghamton & New York (1854). In the decade following the Civil War, railroads were built from Ithaca to surrounding points (Geneva; Cayuga; Cortland; and Elmira, New York; and Athens, Pennsylvania), mainly with financing from Ezra Cornell. The geography of the city, on a steep hill by the lake, has prevented it from being directly connected to a major transportation artery. When the Lehigh Valley Railroad built its main line from Pennsylvania to Buffalo, New York in 1890, it bypassed Ithaca (running via eastern Schuyler County on easier grades), as the Delaware, Lackawanna and Western Railroad had done in the 1850s.
In the late 19th century, more industry developed in Ithaca. In 1883 William Henry Baker and his partners started the Ithaca Gun Company, making shotguns. The original factory was located in the Fall Creek neighborhood of the city, on a slope later known as Gun Hill, where the nearby waterfall supplied the main source of energy for the plant. The company became an icon in the hunting and shooting world, its shotguns famous for their fine decorative work. Wooden gunstocks with knots or other imperfections were donated to the high school woodworking shop to be made into lamps. John Philip Sousa and trick-shooter Annie Oakley favored Ithaca guns. In 1937 the company began producing the Ithaca 37, based on a 1915 patent by noted firearms designer John Browning. Its 12-gauge shotguns were the standard used for decades by the New York Police Department and Los Angeles Police Department.
In 1885, Ithaca Children's Home was established on West State Street. The orphanage had two programs at the time: a residential home for both orphaned and destitute children, and a day nursery. The village established its first trolley in 1887. Ithaca developed as a small manufacturing and retail center and was incorporated as a city in 1888. The largest industrial company in the area was Morse Chain, elements of which were absorbed into Emerson Power Transmission on South Hill and Borg Warner Automotive in Lansing, New York.
Ithaca claims to be the birthplace of the ice cream sundae, created in 1892 when fountain shop owner Chester Platt "served his local priest vanilla ice cream covered in cherry syrup with a dark candied cherry on top. The priest suggested the dessert be named after the day, Sunday — although the spelling was later changed out of fear some would find it offensive." The local Unitarian church, where the priest, Rev. John Scott, preached, has an annual "Sundae Sunday" every September in commemoration. Ithaca's claim has long been disputed by Two Rivers, Wisconsin.
In 1903 a typhoid epidemic, resulting from poor sanitation infrastructure, devastated the city. One out of 10 citizens fell ill or died.
In 1900 Cornell anatomy professor G.S. Moler made an early movie using frame-by-frame technology. For "The Skeleton Dance," he took single-frame photos of a human skeleton in varying positions, giving the illusion of a dancing skeleton. During the early 20th century, Ithaca was an important center in the silent film industry. These films often featured the local natural scenery. Many of these films were the work of Leopold Wharton and his brother Theodore; their studio was on the site of what is now Stewart Park.
The Star Theatre on East Seneca Street was built in 1911 and became the most popular vaudeville venue in the region. Wharton movies were also filmed and shown there. After the film industry centralized in Hollywood, production in Ithaca effectively ceased. Few of the silent films made in Ithaca have been preserved.
After World War II, the Langmuir Research Labs of General Electric developed as a major employer; the defense industry continued to expand. GE's headquarters were based in Schenectady, New York, to the east in the Mohawk Valley.
Recent history.
For decades, the Ithaca Gun Company tested their shotguns behind the plant on Lake Street; the shot fell into Fall Creek (a tributary of Cayuga Lake) at the base of Ithaca Falls. Lead contaminated the water supply, air and land. A major lead clean-up effort sponsored by the United States Superfund took place from 2002 to 2004, managed through the Environmental Protection Agency. The old Ithaca Gun building has been dismantled. It is scheduled to be replaced by development of an apartment complex on the cleaned land.
The former Morse Chain company factory on South Hill, now owned by Emerson Power Transmission, was the site of extensive groundwater and soil contamination from its industrial operations. Emerson Power Transmission has been working with the state and South Hill residents to determine the extent and danger of the contamination and aid in cleanup.
Geography and climate.
The valley in which Cayuga Lake is located is long and narrow with a north-south orientation. Ithaca is at the southern end (the "head") of the lake, but the valley continues to the southwest behind the city. Originally a river valley, it was deepened and widened by the action of Pleistocene ice sheets over the last several hundred thousand years. The lake, which drains to the north, formed behind a dam of glacial moraine. The rock is predominantly Devonian and, north of Ithaca, is relatively fossil rich. Glacial erratics can be found in the area. The world-renowned fossils found in this area can be examined at the Museum of the Earth.
Ithaca was founded on flat land just south of the lake — land that formed in fairly recent geological times when silt filled the southern end of the lake. The city ultimately spread to the adjacent hillsides, which rise several hundred feet above the central flats: East Hill, West Hill, and South Hill. Its sides are fairly steep, and a number of the streams that flow into the valley from east or west have cut deep canyons, usually with several waterfalls.
Ithaca experiences a moderate continental climate. Winters are long, cold, and snowy, with temperatures reaching or lower on an average 9.9 nights annually and an average of of snow per season. The largest snowfall in one day was on February 14, 1914. Summers are warm and humid, with usually comfortable temperatures. Readings of or higher occur on an average of just 5.2 days per year, and + temperatures have only occurred ten times since record-keeping began in 1893. The average date of the first freeze is October 5, and the average date of the last freeze is May 15, giving Ithaca a growing season of 141 days. The average date of the first and last snowfalls are November 12 and April 7, respectively. Extreme temperatures range from as recently as February 2, 1961 up to on July 9, 1936.
The valley flatland has slightly milder weather in winter, and occasionally Ithacans experience simultaneous snow on the hills and rain in the valley. The phenomenon of mixed precipitation (rain, wind, and snow), common in the late fall and early spring, is known tongue-in-cheek as "ithacation" to many of the local residents.
The natural vegetation of the Ithaca area, seen in areas unbuilt and unfarmed, is northern temperate broadleaf forest, dominated by deciduous trees.
Due to the microclimates created by the impact of the lakes, the region surrounding Ithaca (Finger Lakes American Viticultural Area) experiences a short but adequate growing season for winemaking similar to the Rhine Valley wine district of Germany. As such, the region is home to many wineries.
Education.
The two major postsecondary educational institutions located in Ithaca were each founded in the late nineteenth century. In 1865, Ezra Cornell founded Cornell University, which overlooks the town from East Hill. It was opened as a coeducational institution. Women first enrolled in 1870. Ezra Cornell also established a public library for the city. Ithaca College was founded as the Ithaca Conservatory of Music in 1892. Ithaca College was originally located in the downtown area, but relocated to South Hill in the 1960s.
Ithaca is a major educational center in Central New York. In 2011 there were about 21,000 students enrolled at Cornell and about 6,400 at Ithaca College. Tompkins Cortland Community College is located in the neighboring Town of Dryden, and has an extension center in downtown Ithaca. Empire State College offers non-traditional college courses to adults in downtown Ithaca.
The public school system is based in Ithaca. The Ithaca City School District, which encompasses Ithaca and the surrounding area, enrolls about 5,500 K-12 students in eight elementary schools, two middle schools, Ithaca High School, and the Lehman Alternative Community School. Several private elementary and secondary schools are located in the Ithaca area, including the Roman Catholic Immaculate Conception School, the Cascadilla School, the New Roots Charter School, the Elizabeth Ann Clune Montessori School, and the Ithaca Waldorf School. Ithaca has two networks for supporting its home-schooling families: Loving Education At Home (LEAH) and the Northern Light Learning Center (NLLC).
Economy.
The economy of Ithaca is based on education, with technology and tourism in supporting roles. As of 2006, Ithaca has continued to have one of the few expanding economies in New York State outside New York City. It draws commuters for work from the neighboring rural counties of Cortland, Tioga, and Schuyler, as well as from the more urbanized Chemung County.
Ithaca has tried to maintain its traditional downtown shopping area with its pedestrian orientation; this includes the Ithaca Commons pedestrian mall and Center Ithaca, a small mixed-use complex built at the end of the urban renewal era. Another commercial center, Collegetown, is located next to the Cornell campus. It features a number of restaurants, shops, and bars, and an increasing number of high-rise apartments. It is primarily frequented by Cornell University students.
Ithaca has many of the businesses characteristic of small American university towns: bookstores, art house cinemas, craft stores, and vegetarian-friendly restaurants. The collective Moosewood Restaurant, founded in 1973, published a number of vegetarian cookbooks. "Bon Appetit" magazine ranked it among the thirteen most influential restaurants of the 20th century. Ithaca has many local restaurants and chains both in the city and town with a range of ethnic foods. It has become a destination and residence for retirees, and Cornell has programs to appeal to them.
The Ithaca Farmers Market, a cooperative with 150 vendors who live within 30 miles of Ithaca, first opened for business on Saturdays in 1973. It is located at Steamboat Landing, where steamboats from Cayuga Lake used to dock.
The South Hills Business Campus originally opened in 1957 as the regional headquarters of the National Cash Register Company. Running three full factory shifts, NCR was a major employer. Although it was sold in 1991 to American Telephone and Telegraph and later acquired by Cognitive TPG, TPG remains a major tenant of the South Hill Business Campus, which is now owned by a group of private investors.
Culture.
Founded in 1983, the Sciencenter, is a non-profit hands-on science museum, accredited by the American Alliance of Museums (AAM) and is a member of the Association of Science-Technology Centers (ASTC) and Association of Children’s Museums (ACM).
The Museum of the Earth is a natural history museum created in 2003 by the Paleontological Research Institution (PRI). The PRI was founded in Ithaca in 1932 and is the publisher of the oldest journal of paleontology in the western hemisphere. Exhibits cover the 4.5 billion year history of the earth in an accessible manner, including interactive displays. As of 2004, the PRI is now formally affiliated with Cornell.
The Cayuga Nature Center occupies the site of the 1914 Cayuga Preventorium, a facility for children with tuberculosis; treatment of what was then considered an incurable disease was based on rest and good nutrition. In 1981, the Cayuga Nature Center was incorporated as an independent, private, non-profit educational organization, offering environmental education to local school districts. In 2011, the PRI merged with the Cayuga Nature Center, making it a sister organization to the Museum of the Earth.
The Cornell Lab of Ornithology is located in the Imogene Powers Johnson Center for Birds and Biodiversity. The Lab's Visitors' Center and observation areas are open to the public. Displays include a surround sound theater, object-theater presentation, sound studio, and informational kiosks featuring bird sounds and information.
The Herbert F. Johnson Museum of Art houses one of the finest collections of art in upstate New York. Special exhibitions are mounted each year, plus selections from a global permanent collection, which is displayed on six public floors. The collection includes art from throughout Asia, Africa, Europe, the Americas, graphic arts, medallic art, and Tiffany glass, ranging from the ancient to the contemporary.
The Center for the Arts at Ithaca, Inc., operates the "Hangar Theatre". Opened in 1975 in a renovated municipal airport hangar, the Hangar hosts a summer season and brings a range of theatre to regional audiences including students, producing a school tour and Artists-in-the-Schools programs.
Ithaca is also the home to Kitchen Theatre Company, a non-profit professional company with a theatre on West State Street; and Civic Ensemble, a creative collaborative ensemble staging emerging playwrights' work and community-based original productions.
Ithaca is noted for its annual community celebration, The Ithaca Festival. The Constance Saltonstall Foundation for the Arts provides grants and summer fellowships at the Saltonstall Arts Colony for New York State artists and writers. Ithaca also hosts one of the largest used-book sales in the United States.
The city and town also sponsor The Apple Festival in the fall, the Chili Fest in February, the Finger Lakes International Dragon Boat Festival in July; Porchfest in late September, and the Ithaca Brew Fest in Stewart Park in September.
Ithaca has also pioneered the Ithaca Health Fund, a popular cooperative health insurance. Ithaca is home to one of the United States' first local currency systems, Ithaca Hours, developed by Paul Glover.
Music.
Ithaca is the home of the Cayuga Chamber Orchestra.
The School of Music at Ithaca College was originally founded in 1892 by William Egbert as a music conservatory on Buffalo Street. Among the degree programs offered are those in Performance, Theory, and Composition. Since 1941, the School of Music has been accredited by the National Association of Schools of Music.
Ithaca's Suzuki school, Ithaca Talent Education, provides musical training for children of all ages and also teacher training for undergraduate and graduate-level students. The Community School of Music and Art uses an extensive scholarship system to offer classes and lessons to any student, regardless of age, background, economic status, or artistic ability.
A number of musicians call Ithaca home, most notably Samite of Uganda, The Burns Sisters, The Horse Flies, Johnny Dowd, Mary Lorson, cellist Hank Roberts, reggae band John Brown's Body and X Ambassadors. Old-time music is a staple and folk music is featured weekly on WVBR-FM's "Bound for Glory", North America's longest-running live folk concert broadcast. The Finger Lakes GrassRoots Festival of Music and Dance, hosted by local band Donna the Buffalo, is held annually during the third week in July in the nearby village of Trumansburg, with more than 60 local, national and international acts.
Media.
The "Ithaca Journal", founded in 1815, is a morning daily newspaper owned by Gannett. The alternative weekly newspaper, "Ithaca Times," is distributed free of charge. Other area publications include "Tompkins Weekly", "14850 Magazine", the "Cornell Daily Sun", the "Ithacan" at Ithaca College, and the Ithaca High School "Tattler", one of the oldest student newspapers in the United States.
Ithaca is home to several radio stations.
Public radio:
Other FM stations include: Saga's "98.7 The Vine", a low-powered translator station; WFIZ "Z95.5", airing a top-40 (CHR) format; contemporary Christian music station WCII 88.9; and classic rock "The Wall" WLLW 99.3 and 96.3, based in Seneca Falls with a transmitter in Ithaca.
Local government.
There are two governmental entities in the area: the Town of Ithaca and the City of Ithaca. The Town of Ithaca is one of the nine towns comprising Tompkins County. The City of Ithaca is surrounded by, but legally independent of, the Town.
The City of Ithaca has a mayor-council government. The charter of the City of Ithaca provides for a full-time mayor and city judge, each independent and elected at-large. Since 1995, the mayor has been elected to a four-year term, and since 1989, the city judge has been elected to a six-year term.
Since 1983, the city has been divided into five wards. Each elects two representatives to the city council, known as the Common Council, for staggered four-year terms. In March 2015, the Common Council unanimously adopted a resolution recognizing freedom from domestic violence as a fundamental human right.
Since students won the right to vote where they attend colleges, some have become more active in local politics. In 2004, Gayraud Townsend, a 20-year-old senior in Cornell's School of Industrial and Labor Relations, was sworn in as alderman of the city council, representing the 4th Ward. He is the first black male to be elected to the council and was then the youngest African American to be elected to office in the United States. He served his full term and has mentored other young student politicians. In 2011 Cornell undergraduate Svante Myrick was elected as the youngest mayor of the city of Ithaca.
In December 2005, the City and Town governments began discussing opportunities for increased government consolidation, including the possibility of joining the two into a single entity. This topic had been previously discussed in 1963 and 1969. Cayuga Heights, a village adjacent to the city on its northeast, voted against annexation into the city of Ithaca in 1954.
Politics.
Politically, the majority of city's voters (many of them students) have supported liberalism and the Democratic Party. A November 2004 study by ePodunk lists it as New York's most liberal city. This contrasts with the more conservative leanings of the generally rural Upstate New York region; the city's voters are also more liberal than those in the rest of Tompkins County. In 2008, Barack Obama, running against New York State's US Senator Hillary Clinton, won Tompkins County in the Democratic Presidential Primary, the only county that he won in New York State, likely due to support from younger voters. Obama won Tompkins County (including Ithaca) by a wide margin of 41% over his opponent John McCain in the November 2008 election.
Greater Ithaca.
The term "Greater Ithaca" encompasses both the City and Town of Ithaca, as well as several smaller settled places within or adjacent to the Town:
Municipalities
Census-designated places
Demographics.
Ithaca is the larger principal city of the Ithaca-Cortland CSA, a Combined Statistical Area that includes the Ithaca metropolitan area (Tompkins County) and the Cortland micropolitan area (Cortland County), which had a combined population of 145,100 at the 2000 census.
As of the census of 2000, there were 29,287 people, 10,287 households, and 2,962 families residing in the city. The population density was 5,360.9 people per square mile (2,071.0/km²). There were 10,736 housing units at an average density of 1,965.2 per square mile (759.2/km²). The racial makeup of the city was 73.97% White, 13.65% Asian, 6.71% Black or African American, 0.39% Native American, 0.05% Pacific Islander, 1.86% from other races, and 3.36% from two or more races. Hispanic or Latino of any race were 5.31% of the population.
There were 10,287 households out of which 14.2% had children under the age of 18 living with them, 19.0% were married couples living together, 7.8% had a female householder with no husband present, and 71.2% were non-families. 43.3% of all households were made up of individuals and 7.4% had someone living alone who was 65 years of age or older. The average household size was 2.13 and the average family size was 2.81.
In the city the population was spread out with 9.2% under the age of 18, 53.8% from 18 to 24, 20.1% from 25 to 44, 10.6% from 45 to 64, and 6.3% who were 65 years of age or older. The median age was 22 years. For every 100 females there were 102.6 males. For every 100 females age 18 and over, there were 102.2 males.
The median income for a household in the city was $21,441, and the median income for a family was $42,304. Males had a median income of $29,562 versus $27,828 for females. The per capita income for the city was $13,408. About 13.2% of individuals and 4.2% of families were below the poverty line.
Transportation.
Roads.
Ithaca is in the rural Finger Lakes region about northwest of New York City; the nearest larger cities, Binghamton and Syracuse, are an hour's drive away by car, Rochester and Scranton are two hours, Buffalo and Albany are three. New York City, Philadelphia, and Ottawa are about four hours away. Cleveland, Boston, Washington DC, and Montreal are about five hours away.
Ithaca lies at over a half hour's drive from any interstate highway, and all car trips to Ithaca involve some driving on two-lane state rural highways. The city is at the convergence of many regional two-lane state highways: Routes 13, 13A, 34, 79, 89, 96, 96B, and 366. These are usually not congested except in Ithaca proper. However, Route 79 between the I-81 access at Whitney Point and Ithaca receives a significant amount of Ithaca-bound congestion right before Ithaca's colleges reopen after breaks.
There is frequent intercity bus service by Greyhound Lines, New York Trailways, and Shortline (Coach USA), particularly to Binghamton and New York City, with limited service to Rochester, Buffalo and Syracuse, and (via connections in Binghamton) to Utica and Albany. The bus station serving all these companies is the former Delaware, Lackawanna & Western railway station on Meadow St. between W State and W Seneca streets, a little over half a mile west of downtown Ithaca. Cornell University runs a premium Campus to Campus bus between its Ithaca campus and its medical school in New York City which is open to the public.
Ithaca is the center of an extensive bus public transportation network. TCAT, Inc (Tompkins Consolidated Area Transit, Inc.) is a not-for-profit corporation that provides public transportation for Tompkins County New York. TCAT was reorganized as a non-profit corporation in 2004 and is primarily supported locally by Cornell University, the City of Ithaca and Tompkins County. TCAT's ridership increased from 2.7 million in 2004 to 4.4 million in 2013. http://www.tcatbus.com/files/all/tcat_2013_yearbook_-_final.pdf TCAT operates 33 routes, many running seven days a week. It has frequent service to downtown, Cornell, Ithaca College, and the Shops at Ithaca Mall in the neighboring Town of Lansing, but less frequent service to many residential and rural areas, including Trumansburg and Newfield. Chemung County Transit (C-TRAN) runs weekday commuter service from Chemung County to Ithaca. Cortland Transit runs commuter service to Cornell University. Tioga County Public Transit operates three routes to Ithaca and Cornell, but will cease operating on November 30, 2014.
GADABOUT Transportation Services, Inc. provides demand-response paratransit service for seniors over 60 and people with disabilities. Ithaca Dispatch provides local and regional taxi service. In addition, Ithaca Airline Limousine and IthaCar Service connect to the local airports.
In July 2008, a non-profit called Ithaca Carshare began a carsharing service in Ithaca. Ithaca Carshare has a fleet of vehicles shared by over 1500 members as of July 2015 and has become a popular service among both city residents and the college communities. Vehicles are located throughout Ithaca downtown and the two major institutions. With Ithaca Carshare as the first locally run carsharing organization in New York State, others have since launched in Buffalo, Albany, NY, and Syracuse. Independent studies have shown that for each Ithaca Carshare vehicle in the fleet, 15 fewer personally owner cars are owned.
Rideshare services to promote carpooling and vanpooling are operated by ZIMRIDE and VRIDE. A community mobility education program, Way2Go is operated by Cornell Cooperative Extension of Tompkins County. Way2Go's website provides consumer information and videos. Way2Go works collaboratively to help people save money, stress less, go green and improve mobility options. The 2-1-1 Tompkins/Cortland Help line connects people with services, including transportation, in the community, by telephone and web on a 24/7 basis. The information and referral service is operated by the Human Services Coalition of Tompkins County, Inc. Together, 2-1-1 Information and Referral and Way2Go are a one-call, one-click resource designed to mobility services information for Ithaca and throughout Tompkins County.
As a growing urban area, Ithaca is facing steady increases in levels of vehicular traffic on the city grid and on the state highways. Outlying areas have limited bus service, and many people consider a car essential. However, many consider Ithaca a walkable and bikeable community. One positive trend for the health of downtown Ithaca is the new wave of increasing urban density in and around the Ithaca Commons. Because the downtown area is the region's central business district, dense mixed-use development that includes housing may increase the proportion of people who can walk to work and recreation, and mitigate the likely increased pressure on already busy roads as Ithaca grows. The downtown area is also the area best served by frequent public transportation. Still, traffic congestion around the Commons is likely to progressively increase.
Airports.
Ithaca is served by Ithaca Tompkins Regional Airport, located about three miles to the northeast of the city center. American Airlines offers flights to its hub at Philadelphia, operated by Piedmont Airlines using de Havilland Canada Dash 8 turboprop airliners. Delta Connection provides service to its hub at Detroit Metro airport, operated by Endeavor Air using Bombardier CRJ-200 airliners. United Express offers three daily flights to Newark Liberty International Airport, operated by CommutAir using Bombardier Dash 8 turboprops. Some residents choose to travel to Syracuse Hancock International Airport, Greater Binghamton Airport, Elmira-Corning Regional Airport or Greater Rochester International Airport for more airline service options.
Railways.
Norfolk Southern freight trains reach Ithaca from Sayre, Pennsylvania, mainly to deliver coal to AES Cayuga, a coal power plant (known as Milliken Station during NYSEG ownership) and haul out salt from the Cargill salt mine, both on the east shore of Cayuga Lake. There is no passenger rail service, although from the 1870s through the 1950s there were trains to Buffalo via Geneva, New York; to New York City via Wilkes-Barre, Pennsylvania (Lehigh Valley Railroad) and Scranton, Pennsylvania (DL&W); to Auburn, New York; and to the US northeast via Cortland, New York; service to Buffalo and New York City lasted until 1961. The Lehigh Valley's top New York City-Ithaca-Buffalo passenger train, "The Black Diamond", was optimistically publicized as 'The Handsomest Train in the World', perhaps to compensate for its roundabout route to Buffalo. It was named after the railroad's largest commodity, anthracite coal.
Ithaca was the fourth community in New York state with a street railway; streetcars ran from 1887 to summer 1935.
Reputation.
In addition to its liberal politics, Ithaca is commonly listed among the most culturally liberal of American small cities. The "Utne Reader" named Ithaca "America's most enlightened town" in 1997. According to ePodunk's Gay Index, Ithaca has a score of 231, versus a national average score of 100.
Like many small college towns, Ithaca has also received accolades for having a high overall quality of life. In 2004, "Cities Ranked and Rated" named Ithaca the best "emerging city" to live in the United States. In 2006, the Internet realty website "Relocate America" named Ithaca the fourth best city in the country to relocate to. In July 2006, Ithaca was listed as one of the "12 Hippest Hometowns for Vegetarians" by "VegNews Magazine" and chosen by "Mother Earth News" as one of the "12 Great Places You've Never Heard Of."
In 2012, the city was listed among the 10 best places to retire in the U.S. by U.S. News.
Ithaca was also ranked 13th among America's Best College Towns by "Travel + Leisure" in 2013 and ranked as the #1 Best College Town in America in the American Institute for Economic Research's 2013-2014 College Destination Index.
In its earliest years during frontier days, what is now Ithaca was briefly known by the names "The Flats" and "Sodom," the name of the Biblical city of sin, due to its reputation as a town of "notorious immorality", a place of horse racing, gambling, profanity, Sabbath breaking, and readily available liquor. These names did not last long; Simeon DeWitt renamed the town Ithaca in the early 19th century, though nearby Robert H. Treman State Park still contains Lucifer Falls.
In popular culture.
Movies/TV show.
See also The Whartons Studio for films shot in Ithaca prior to 1920.

</doc>
<doc id="14975" url="https://en.wikipedia.org/wiki?curid=14975" title="Ivy League">
Ivy League

The Ivy League is a collegiate athletic conference comprising sports teams from eight private institutions of higher education in the Northeastern United States. The conference name is also commonly used to refer to those eight schools as a group. The eight institutions are Brown University, Columbia University, Cornell University, Dartmouth College, Harvard University, the University of Pennsylvania, Princeton University, and Yale University. The term "Ivy League" has connotations of academic excellence, selectivity in admissions, and social elitism.
While the term was in use as early as 1933, it only became official after the formation of the NCAA Division I athletic conference in 1954. Seven of the eight schools were founded during the United States colonial period; the exception is Cornell, which was founded in 1865. Ivy League institutions account for seven of the nine Colonial Colleges chartered before the American Revolution, the other two being Rutgers University and College of William & Mary.
Ivy League schools are generally viewed as some of the most prestigious, and are ranked among the best universities worldwide. All eight universities place in the top fifteen of the "U.S. News & World Report" 2016 university rankings, including the top four schools and five of the top nine. U.S. News has named a member of the Ivy League as the best national university in each of the past sixteen years ending with the 2016 rankings: Princeton nine times, Harvard twice and the two schools tied for first five times.
Undergraduate enrollments range from about 4,000 to 14,000, making them larger than those of a typical private liberal arts college and smaller than a typical public state university. Total enrollments, including graduate students, range from approximately 6,100 at Dartmouth to over 20,000 at Columbia, Cornell, Harvard, and Penn. Ivy League financial endowments range from Brown's $3 billion to Harvard's $36.4 billion, the largest financial endowment of any academic institution in the world.
Members.
Ivy League universities have some of the largest university financial endowments in the world, which allows the universities to provide many resources for their academic programs and research endeavors. As of 2014, Harvard University has an endowment of $36.4 billion. Additionally, each university receives millions of dollars in research grants and other subsidies from federal and state governments.
History.
Origin of the name.
Students have long revered the ivied walls of older colleges. "Planting the ivy" was a customary class day ceremony at many colleges in the 1800s. In 1893 an alumnus told "The Harvard Crimson", "In 1850, class day was placed upon the University Calendar... the custom of planting the ivy, while the ivy oration was delivered, arose about this time." At Penn, graduating seniors started the custom of planting ivy at a university building each spring in 1873 and that practice was formally designated as "Ivy Day" in 1874. Ivy planting ceremonies are reported for Yale, Simmons, Bryn Mawr and many others. Princeton's "Ivy Club" was founded in 1879.
The first usage of "Ivy" in reference to a group of colleges is from sportswriter Stanley Woodward (1895–1965).
The first known instance of the term "Ivy League" being used appeared in "The Christian Science Monitor" on February 7, 1935. Several sportswriters and other journalists used the term shortly later to refer to the older colleges, those along the northeastern seaboard of the United States, chiefly the nine institutions with origins dating from the colonial era, together with the United States Military Academy (West Point), the United States Naval Academy, and a few others. These schools were known for their long-standing traditions in intercollegiate athletics, often being the first schools to participate in such activities. However, at this time, none of these institutions made efforts to form an athletic league.
A common folk etymology attributes the name to the Roman numeral for four (IV), asserting that there was such a sports league originally with four members. The "Morris Dictionary of Word and Phrase Origins" helped to perpetuate this belief. The supposed "IV League" was formed over a century ago and consisted of Harvard, Yale, Princeton, and a fourth school that varies depending on who is telling the story. However, it is clear that Harvard, Princeton, Yale and Columbia met on November 23, 1876 at the so-called Massasoit Convention to decide on uniform rules for the emerging game of American football, which rapidly spread.
Pre–Ivy League.
Seven out of the eight Ivy League schools were founded before the American Revolution; Cornell was founded just after the American Civil War. These seven were the primary colleges in the Northern and Middle Colonies, and their early faculties and founding boards were largely, therefore, drawn from other Ivy League institutions. There were also some British graduates from the University of Cambridge, the University of Oxford, the University of St. Andrews, the University of Edinburgh, and elsewhere on their boards. Similarly, the founder of The College of William & Mary, in 1693, was a British graduate of the University of Edinburgh. Cornell provided Stanford University with its first president.
The influence of these institutions on the founding of other colleges and universities is notable. This included the Southern public college movement which blossomed in the decades surrounding the turn of the 19th century when Georgia, South Carolina, North Carolina and Virginia established what became the flagship universities for each of these states. In 1801, a majority of the first board of trustees for what became the University of South Carolina were Princeton alumni. They appointed Jonathan Maxcy, a Brown graduate, as the university's first president. Thomas Cooper, an Oxford alumnus and University of Pennsylvania faculty member, became the second president of the South Carolina college. The founders of the University of California came from Yale, hence the school colors of University of California are Yale Blue and California Gold.
Some of the Ivy League schools have identifiable Protestant roots, while others were founded as non-sectarian schools. Church of England "King's College" broke up during the Revolution and was reformed as public nonsectarian Columbia College. In the early nineteenth century, the specific purpose of training Calvinist ministers was handed off to theological seminaries, but a denominational tone and such relics as compulsory chapel often lasted well into the twentieth century. Penn and Brown were officially founded as nonsectarian schools. Brown's charter promised no religious tests and "full liberty of conscience", but placed control in the hands of a board of twenty-two Baptists, five Quakers, four Congregationalists, and five Episcopalians. Cornell has been strongly nonsectarian from its founding.
"Ivy League" is sometimes used as a way of referring to an elite class, even though institutions such as Cornell University were among the first in the United States to reject racial and gender discrimination in their admissions policies. This dates back to at least 1935. Novels and memoirs attest this sense, as a social elite; to some degree independent of the actual schools.
After the Second World War, the present Ivy League institutions slowly widened their selection of their students. They had always had distinguished faculties; some of the first Americans with doctorates had taught for them; but they now decided that they could not both be world-class research institutions and be competitive in the highest ranks of American college sport; in addition, the schools experienced the scandals of any other big-time football programs, although more quietly.
History of the athletic league.
19th and early 20th centuries.
The first formal athletic league involving eventual Ivy League schools (or any US colleges, for that matter) was created in 1870 with the formation of the Rowing Association of American Colleges. The RAAC hosted a de facto national championship in rowing during the period 1870–1894. In 1895, Cornell, Columbia, and Penn founded the Intercollegiate Rowing Association, which remains the oldest collegiate athletic organizing body in the US. To this day, the IRA Championship Regatta determines the national champion in rowing and all of the Ivies are regularly invited to compete.
A basketball league was later created in 1902, when Columbia, Cornell, Harvard, Yale and Princeton formed the Eastern Intercollegiate Basketball League; they were later joined by Penn and Dartmouth.
In 1906, the organization that eventually became the National Collegiate Athletic Association was formed, primarily to formalize rules for the emerging sport of football. But of the 39 original member colleges in the NCAA, only two of them (Dartmouth and Penn) later became Ivies.
In February 1903, intercollegiate wrestling began when Yale accepted a challenge from Columbia, published in the Yale News. The dual meet took place prior to a basketball game hosted by Columbia and resulted in a tie. Two years later, Penn and Princeton also added wrestling teams, leading to the formation of the student-run Intercollegiate Wrestling Association, now the Eastern Intercollegiate Wrestling Association (EIWA), the first and oldest collegiate wrestling league in the US.
In 1930, Columbia, Cornell, Dartmouth, Penn, Princeton and Yale formed the Eastern Intercollegiate Baseball League; they were later joined by Harvard, Brown, Army and Navy.
Before the formal establishment of the Ivy League, there was an "unwritten and unspoken agreement among certain Eastern colleges on athletic relations". The earliest reference to the "Ivy colleges" came in 1933, when Stanley Woodward of the New York Herald Tribune used it to refer to the eight current members plus Army. In 1935, the Associated Press reported on an example of collaboration between the schools:
Despite such collaboration, the universities did not seem to consider the formation of the league as imminent. Romeyn Berry, Cornell's manager of athletics, reported the situation in January 1936 as follows:
Within a year of this statement and having held month-long discussions about the proposal, on December 3, 1936, the idea of "the formation of an Ivy League" gained enough traction among the undergraduate bodies of the universities that the "Columbia Daily Spectator", "The Cornell Daily Sun", "The Dartmouth", "The Harvard Crimson", "The Daily Pennsylvanian", "The Daily Princetonian" and the "Yale Daily News" would simultaneously run an editorial entitled "Now Is the Time", encouraging the seven universities to form the league in an effort to preserve the ideals of athletics. Part of the editorial read as follows:
The Ivies have been competing in sports as long as intercollegiate sports have existed in the United States. Rowing teams from Harvard and Yale met in the first sporting event held between students of two U.S. colleges on Lake Winnipesaukee, New Hampshire, on August 3, 1852. Harvard's team, "The Oneida", won the race and was presented with trophy black walnut oars from then presidential nominee General Franklin Pierce.
The proposal did not succeed—on January 11, 1937, the athletic authorities at the schools rejected the "possibility of a heptagonal league in football such as these institutions maintain in basketball, baseball and track." However, they noted that the league "has such promising possibilities that it may not be dismissed and must be the subject of further consideration."
Post-World War II.
In 1945 the presidents of the eight schools signed the first "Ivy Group Agreement", which set academic, financial, and athletic standards for the football teams. The principles established reiterated those put forward in the Harvard-Yale-Princeton Presidents' Agreement of 1916. The Ivy Group Agreement established the core tenet that an applicant's ability to play on a team would not influence admissions decisions:
In 1954, the presidents extended the Ivy Group Agreement to all intercollegiate sports, effective with the 1955-56 basketball season. This is generally reckoned as the formal formation of the Ivy League. As part of the transition, Brown, the only Ivy that hadn't joined the EIBL, did so for the 1954-55 season. A year later, the Ivy League absorbed the EIBL. The Ivy League claims the EIBL's history as its own. Through the EIBL, it is the oldest basketball conference in Division I.
As late as the 1960s many of the Ivy League universities' undergraduate programs remained open only to men, with Cornell the only one to have been coeducational from its founding (1865) and Columbia being the last (1983) to become coeducational. Before they became coeducational, many of the Ivy schools maintained extensive social ties with nearby Seven Sisters women's colleges, including weekend visits, dances and parties inviting Ivy and Seven Sisters students to mingle. This was the case not only at Barnard College and Radcliffe College, which are adjacent to Columbia and Harvard, but at more distant institutions as well. The movie "Animal House" includes a satiric version of the formerly common visits by Dartmouth men to Massachusetts to meet Smith and Mount Holyoke women, a drive of more than two hours. As noted by Irene Harwarth, Mindi Maline, and Elizabeth DeBra, "The 'Seven Sisters' was the name given to Barnard, Smith, Mount Holyoke, Vassar, Bryn Mawr, Wellesley, and Radcliffe, because of their parallel to the Ivy League men’s colleges."
In 1982 the Ivy League considered adding two members, with Army, Navy, and Northwestern as the most likely candidates; if it had done so, the league could probably have avoided being moved into the recently created Division I-AA (now Division I FCS) for football. In 1983, following the admission of women to Columbia College, Columbia University and Barnard College entered into an athletic consortium agreement by which students from both schools compete together on Columbia University women's athletic teams, which replaced the women's teams previously sponsored by Barnard.
When Army and Navy departed the Eastern Intercollegiate Baseball League in 1992, all intercollegiate competition involving the eight schools became united under the Ivy League banner.
Academics.
Admissions.
The Ivy League schools are highly selective, with acceptance rates since 2000 ranging from 6 to 16 percent at each of the universities. Admitted students come from around the world, although students from New England and the Northeastern United States make up a significant proportion of students.
Prestige.
Members of the League have been highly ranked by various university rankings.
Further, Ivy League members have produced many Nobel laureates, winners of the Nobel Prize and the Nobel Memorial Prize in Economic Sciences. Listed from in order from greatest number of Nobel laureates are: Harvard with 153 Nobel winners, the most out of any university in the world. This is followed by Columbia with 101 winners, Yale with 52, Cornell with 45, Princeton with 37, and Penn with 29 Nobel laureates. These figures are self-reported by the universities themselves, who use widely varying definitions for which Nobel winners are claimed as affiliates, for example, only degree-holding alumni or active faculty or former faculty, visiting faculty, adjunct faculty, etc. Many universities are notorious for claiming laureates with only tenuous informal connections in order to inflate their count of winners.
Collaboration.
Collaboration between the member schools is illustrated by the student-led Ivy Council that meets in the fall and spring of each year, with representatives from every Ivy League school. The governing body of the Ivy League is the Council of Ivy Group Presidents, composed of each university president. During meetings, the presidents often discuss common procedures and initiatives for the universities.
Culture.
Fashion and lifestyle.
Different fashion trends and styles have emerged from Ivy League campuses over time, and fashion trends such as Ivy League and Preppy are styles often associated with the Ivy League and its culture.
Ivy League style is a style of men's dress, popular during the late 1950s, believed to have originated on Ivy League campuses. The clothing stores J. Press and Brooks Brothers represent perhaps the quintessential Ivy League dress manner. The Ivy League style is said to be the predecessor to the preppy style of dress.
Preppy fashion started around 1912 to the late 1940s and 1950s as the Ivy League style of dress. J. Press represents the quintessential preppy clothing brand, stemming from the collegiate traditions that shaped the preppy subculture. In the mid-twentieth century J. Press and Brooks Brothers, both being pioneers in preppy fashion, had stores on Ivy League school campuses, including Harvard, Princeton, and Yale.
Some typical preppy styles also reflect traditional upper class New England leisure activities, such as equestrian, sailing or yachting, hunting, fencing, rowing, lacrosse, tennis, golf, and rugby. Longtime New England outdoor outfitters, such as L.L. Bean, became part of conventional preppy style. This can be seen in sport stripes and colours, equestrian clothing, plaid shirts, field jackets and nautical-themed accessories. Vacationing in Palm Beach, Florida, long popular with the East Coast upper class, led to the emergence of bright colour combinations in leisure wear seen in some brands such as Lilly Pulitzer. By the 1980s, other brands such as Lacoste, Izod and Dooney & Bourke became associated with preppy style.'
Today, these styles continue to be popular on Ivy League campuses, throughout the U.S., and abroad, and are oftentimes labeled as "Classic American style" or "Traditional American style".
Social elitism.
The Ivy League is often associated with the upper class White Anglo-Saxon Protestant community of the Northeast, Old Money, or more generally, the American upper middle and upper classes. Although most Ivy League students come from upper middle and upper class families, the student body has become increasingly more economically and ethnically diverse. The universities provide significant financial aid to help increase the enrollment of lower income and middle class students. Several reports suggest, however, that the proportion of students from less-affluent families remains low.
Phrases such as "Ivy League snobbery" are ubiquitous in nonfiction and fiction writing of the early and mid-twentieth century. A Louis Auchincloss character dreads "the aridity of snobbery which he knew infected the Ivy League colleges". A business writer, warning in 2001 against discriminatory hiring, presented a cautionary example of an attitude to avoid (the bracketed phrase is his):
The phrase "Ivy League" historically has been perceived as connected not only with academic excellence, but also with social elitism. In 1936, sportswriter John Kieran noted that student editors at Harvard, Yale, Princeton, Cornell, Columbia, Dartmouth, and Penn were advocating the formation of an athletic association. In urging them to consider "Army and Navy and Georgetown and Fordham and Syracuse and Brown and Pitt" as candidates for membership, he exhorted:
Aspects of Ivy stereotyping were illustrated during the 1988 presidential election, when George H. W. Bush (Yale '48) derided Michael Dukakis (graduate of Harvard Law School) for having "foreign-policy views born in Harvard Yard's boutique." "New York Times" columnist Maureen Dowd asked "Wasn't this a case of the pot calling the kettle elite?" Bush explained however that, unlike Harvard, Yale's reputation was "so diffuse, there isn't a symbol, I don't think, in the Yale situation, any symbolism in it... Harvard boutique to me has the connotation of liberalism and elitism" and said "Harvard" in his remark was intended to represent "a philosophical enclave" and not a statement about class. Columnist Russell Baker opined that "Voters inclined to loathe and fear elite Ivy League schools rarely make fine distinctions between Yale and Harvard. All they know is that both are full of rich, fancy, stuck-up and possibly dangerous intellectuals who never sit down to supper in their undershirt no matter how hot the weather gets." Still, the last four presidents have all attended Ivy League schools for at least part of their education— George H. W. Bush (Yale undergrad), Bill Clinton (Yale Law School), George W. Bush (Yale undergrad, Harvard Business School), and Barack Obama (Columbia undergrad, Harvard Law School).
U.S. presidents in the Ivy League.
Of the forty-three men who have served as President of the United States, fourteen have graduated from an Ivy League university. Of them, eight have degrees from Harvard, five from Yale, three (two were honorary) from Columbia and two from Princeton. Eleven presidents have earned Ivy undergraduate degrees. Two of these were transfer students: Barack Obama transferred from Occidental College and John F. Kennedy transferred from another Ivy, Princeton, where he had been class of 1939. John Adams was the first president to graduate from college, graduating from Harvard in 1755.
Student demographics.
Geographic distribution.
Students of the Ivy League largely hail from the Northeast, largely from the New York City, Boston, and Philadelphia areas. As all eight Ivy League universities are within the Northeast, it is no surprise that most graduates end up working and residing in the Northeast after graduation. An unscientific survey of Harvard seniors from the Class of 2013 found that 42% hailed from the Northeast and 55% overall were planning on working and residing in the Northeast. Boston and New York City are traditionally where many Ivy League graduates end up living.
Socioeconomics and social class.
Students of the Ivy League, both graduate and undergraduate, come primarily from upper middle and upper class families. In recent years, however, the universities have looked towards increasing socioeconomic and class diversity, by providing greater financial aid packages to applicants from lower, working, and middle class American families.
In 2013, 46% of Harvard College students came from families in the top 3.8% of all American households (over $200,000 per annum). In 2012, the bottom 25% of the American income distribution accounted for only 3-4% of students at Brown, a figure that had remained unchanged since 1992. In 2014, 69% of incoming freshmen students at Yale College came from families with annual incomes of over $120,000, putting most Yale College students in the upper middle and/or upper class. (The median household income in the U.S. in 2013 was $52,700.)
In the 2011-2012 academic year, students qualifying for Pell Grants (federally funded scholarships on the basis of need) comprised 20% at Harvard, 18% at Cornell, 17% at Penn, 16% at Columbia, 15% at Dartmouth and Brown, 14% at Yale, and 12% at Princeton. Nationally, 35% of American university students qualify for a Pell Grant.
Competition and athletics.
Ivy champions are recognized in sixteen men's and sixteen women's sports. In some sports, Ivy teams actually compete as members of another league, the Ivy championship being decided by isolating the members' records in play against each other; for example, the six league members who participate in ice hockey do so as members of ECAC Hockey, but an Ivy champion is extrapolated each year. The Ivy League was the last Division I basketball conference to institute a conference postseason tournament; the first tournaments for men and women will be held at the end of the 2016–17 season. The tournaments will only award the Ivy League automatic bids for the NCAA Division I Men's and Women's Basketball Tournaments; the official conference championships will continue to be awarded based solely on regular-season results. Before the 2016–17 season, the automatic bids were based solely on regular-season record, with a one-game playoff (or series of one-game playoffs if more than two teams were tied) held to determine the automatic bid. The Ivy League is one of only two Division I conferences which award their official basketball championships solely on regular-season results; the other is the Southeastern Conference. Since its inception, an Ivy League school has yet to win either the men's or women's Division I NCAA Basketball Tournament.
On average, each Ivy school has more than 35 varsity teams. All eight are in the top 20 for number of sports offered for both men and women among Division I schools.
Unlike most Division I athletic conferences, the Ivy League prohibits the granting of athletic scholarships; all scholarships awarded are need-based (financial aid). Ivy League teams' non-league games are often against the members of the Patriot League, which have similar academic standards and athletic scholarship policies.
In the time before recruiting for college sports became dominated by those offering athletic scholarships and lowered academic standards for athletes, the Ivy League was successful in many sports relative to other universities in the country. In particular, Princeton won 26 recognized national championships in college football (last in 1935), and Yale won 18 (last in 1927). Both of these totals are considerably higher than those of other historically strong programs such as Alabama, which has won 15, Notre Dame, which claims 11 but is credited by many sources with 13, and USC, which has won 11. Yale, whose coach Walter Camp was the "Father of American Football," held on to its place as the all-time wins leader in college football throughout the entire 20th century, but was finally passed by Michigan on November 10, 2001. Harvard, Yale, Princeton and Penn each have over a dozen former scholar-athletes enshrined in the College Football Hall of Fame. Currently Dartmouth holds the record for most Ivy League football titles, with 18, followed closely by Harvard and Penn, each with 17 titles. In addition, the Ivy League has produced Super Bowl winners Kevin Boothe (Cornell), two-time Pro Bowler Zak DeOssie (Brown), Sean Morey (Brown), All-Pro selection Matt Birk (Harvard), Calvin Hill (Yale), Derrick Harmon (Cornell) and 1999 "Mr. Irrelevant" Jim Finn (Penn).
Beginning with the 1982 football season, the Ivy League has competed in Division I-AA (renamed FCS in 2006). The Ivy League teams are eligible for the FCS tournament held to determine the national champion, and the league champion is eligible for an automatic bid (and any other team may qualify for an at-large selection) from the NCAA. However, the Ivy League has not played any postseason games at all since 1956 due to the league's concerns about the extended December schedule's effects on academics. For this reason, any Ivy League team invited to the FCS playoffs turns down the bid. The Ivy League plays a strict 10-game schedule, compared to other FCS members' schedules of 11 (or, in some seasons, 12) regular season games, plus post-season, which was most recently expanded in 2013 to five rounds with 24 teams, with a bye week for the top eight teams. Football is the only sport in which the Ivy League declines to compete for a national title.
In addition to varsity football, Penn, Princeton and Cornell also field teams in the eight-team Collegiate Sprint Football League, in which all players must weigh 172 pounds or less. Penn and Princeton are the last remaining founding members of the league from its 1934 debut, and Cornell is the next-oldest, joining in 1937. Yale and Columbia previously fielded teams in the league but no longer do so.
The Ivy League is home to some of the oldest college rugby teams. Although these teams are not "varsity" sports, they compete annually in the Ivy Rugby Conference.
Historical results.
The table above includes the number of team championships won from the beginning of official Ivy League competition (1956–57 academic year) through 2011–12. Princeton and Harvard have on occasion won ten or more Ivy League titles in a year, an achievement accomplished six times by Harvard and 21 times by Princeton, including a conference-record 15 championships in 2010-11. Only once has one of the other six schools earned more than eight titles in a single academic year (Cornell with nine in 2005-06). In the 33 academic years beginning 1979-80, Princeton has averaged 11 championships per year, one-third of the conference total of 33 sponsored sports.
In the seven academic years beginning 2005-06, Harvard has won Ivy titles in 22 different sports, two-thirds of the league total, and Princeton has won championships in 31 different sports, all except wrestling and men's tennis.
Rivalries.
Rivalries run deep in the Ivy League. For instance, Princeton and Penn are longstanding men's basketball rivals; "Puck Frinceton", and "Pennetrate the Puss" t-shirts are worn by Quaker fans at games. In only 11 instances in the history of Ivy League basketball, and in only seven seasons since Yale's 1962 title, has neither Penn nor Princeton won at least a share of the Ivy League title in basketball, with Princeton champion or co-champion 26 times and Penn 25 times. Penn has won 21 outright, Princeton 19 outright. Princeton has been a co-champion 7 times, sharing 4 of those titles with Penn (these 4 seasons represent the only times Penn has been co-champion). Harvard won its first title of either variety in 2011, losing a dramatic play-off game to Princeton for the NCAA tournament bid, then rebounded to win outright championships in 2012, 2013, and 2014.
Rivalries exist between other Ivy league teams in other sports, including Cornell and Harvard in hockey, Harvard and Princeton in swimming, and Harvard and Penn in football (Penn and Harvard have each had two unbeaten seasons since 2001). In men's lacrosse, Cornell and Princeton are perennial rivals, and they are the only two Ivy League teams to have won the NCAA tournament. In 2009, the Big Red and Tigers met for their 70th game in the NCAA tournament. No team other than Harvard or Princeton has won the men's swimming conference title outright since 1972, although Yale, Columbia, and Cornell have shared the title with Harvard and Princeton during this time. Similarly, no program other than Princeton and Harvard has won the women's swimming championship since Brown's 1999 title. Princeton or Cornell has won every indoor and outdoor track and field championship, both men's and women's, every year since 2002-03, with one exception (Columbia women won indoor championship in 2012). Harvard and Yale are football and crew rivals although the competition has become unbalanced; Harvard has won all but one of the last 11 football games and all but one of the last 13 crew races.
Intra-conference football rivalries.
The Yale-Princeton series is the nation's second longest, exceeded only by "The Rivalry" between Lehigh and Lafayette, which began later in 1884 but included two or three games in each of 17 early seasons. For the first three decades of the Yale-Princeton rivalry, the two played their season-ending game at a neutral site, usually New York City, and with one exception (1890: Harvard), the winner of the game also won at least a share of the national championship that year, covering the period 1869 through 1903. This phenomenon of a finale contest at a neutral site for the national title created a social occasion for the society elite of the metropolitan area akin to a Super Bowl in the era prior to the establishment of the NFL in 1920. These football games were also financially profitable for the two universities, so much that they began to play baseball games in New York City as well, drawing record crowds for that sport also, largely from the same social demographic. In a period when the only professional sports were fledgling baseball leagues, these high profile early contests between Princeton and Yale played a role in popularizing spectator sports, demonstrating their financial potential and raising public awareness of Ivy universities at a time when few people attended college.
National team championships.
This list, which is current through July 1, 2015, includes NCAA championships and women's AIAW championships (one each for Yale and Dartmouth). Excluded from this list are all other national championships earned outside the scope of NCAA competition, including football titles and retroactive Helms Foundation titles.

</doc>
<doc id="14976" url="https://en.wikipedia.org/wiki?curid=14976" title="Ithaca Hours">
Ithaca Hours

The Ithaca HOUR is a local currency used in Ithaca, New York and is the oldest and largest local currency system in the United States that is still operating. It has inspired other similar systems in Madison, Wisconsin; Corvallis, Oregon; and a proposed system in the Lehigh Valley, Pennsylvania. One Ithaca HOUR is valued at US$10 and is generally recommended to be used as payment for one hour's work, although the rate is negotiable.
The currency.
Ithaca HOURS are not backed by national currency and cannot be freely converted to national currency, although some businesses may agree to buy them.
HOURS are printed on high-quality paper and use faint graphics that would be difficult to reproduce, and each bill is stamped with a serial number, in order to discourage counterfeiting.
In 2002, a one-tenth hour bill was introduced, partly due to the encouragement and funding from Alternatives Federal Credit Union and feedback from retailers who complained about the awkwardness of only having larger denominations to work with; the bills bear the signatures of both HOURS president Steve Burke and the president of AFCU.
While the Ithaca Hour continues to exist, in recent years it has fallen into disuse. Media accounts from the year 2011 indicate that the number of businesses accepting Hours has declined. Several reasons are attributed to this. First has been the founder, Paul Glover, moving out of town. While in Ithaca, Glover had acted as an evangelist and networker for Hours, helping to spread their use and helping businesses find ways to spend Hours they had received. Secondly, a general shift away from cash transactions towards electronic transfers with debit or credit cards. Glover has emphasized that every local currency needs at least one full-time networker to "promote, facilitate and troubleshoot" currency circulation.
Origin.
Ithaca Hours were started by Paul Glover in November 1991. The system has historical roots in scrip and alternative and local currencies that proliferated in America during the Great Depression.
While doing research into local economics during 1989, Glover had seen an "Hour" note 19th century British industrialist Robert Owen issued to his workers for spending at his company store. After Ithaca Hours began, he discovered that Owen's Hours were based on Josiah Warren's "Time Store" notes of 1827.
In May 1991, local student Patrice Jennings interviewed Glover about the Ithaca LETS enterprise. This conversation strongly reinforced his interest in trade systems. Jennings's research on the Ithaca LETS and its failure was integral to the development of the HOUR currency; conversations between Jennings and Glover helped to ensure that HOURS used knowledge of what had not worked with the LETS system.
Within a few days, he had designs for the HOUR and Half HOUR notes. He established that each HOUR would be worth the equivalent of $10, which was about the average hourly amount that workers earned in surrounding Tompkins County, although the exact rate of exchange for any given transaction was to be decided by the parties themselves. At GreenStar Cooperative Market, a local food co-op, Glover approached Gary Fine, a local massage therapist, with photocopied samples. Fine became the first person to sign a list formally agreeing to accept HOURS in exchange for services. Soon after, Jim Rohrrsen, the proprietor of a local toy store, became the first retailer to sign-up to accept Ithaca HOURS in exchange for merchandise.
When the system was first started, 90 people agreed to accept HOURS as pay for their services. They all agreed to accept HOURS despite the lack of a business plan or guarantee. Glover then began to ask for small donations to help pay for printing HOURS.
Fine Line Printing completed the first run of 1,500 HOURS and 1,500 Half HOURS in October 1991. These notes, the first modern local currency, were nearly twice as large as the current Ithaca HOURS. Because they didn't fit well in people's wallets, almost all of the original notes have been removed from circulation.
The first issue of Ithaca Money was printed at Our Press, a printing shop in Chenango Bridge, New York, on October 16, 1991. The next day Glover issued 10 HOURS to Ithaca Hours, the organization he founded to run the system, as the first of four reimbursements for the cost of printing HOURS. The day after that, October 18, 1991, 382 HOURS were disbursed and prepared for mailing to the first 93 pioneers.
On October 19, 1991, Glover bought a samosa from Catherine Martinez at the Farmers' Market with Half HOUR #751—the first use of an HOUR. Several other Market vendors enrolled that day.
Stacks of the Ithaca Money newspaper were distributed all over town with an invitation to "join the fun."
A Barter Potluck was held at GIAC on November 12, 1991, the first of many monthly gatherings where food and skills were exchanged, acquaintances made, and friendships renewed.
Management & philosophy.
In 1996, Glover was running the Ithaca Hours system from his home, and the system had an advisory board and a governing board called the "Barter Potluck". The board and Glover put forth the idea that economic interactions should be based on harmony rather than on more Hobbsian forms of competition. In one interview, Glover stated that "There's a growing movement called "ecological economics" and Ithaca HOURS is part of that cosmos. Last year I wrote an article which discusses moving us toward the provision of food, fuel, clothing, housing, transportation, other necessities in ways which are healing of nature, or which are less depleting at least and which bring people together on the basis of their shared pride, not arrogance." Thus one underlying principle of the local currency movement is to create "fair trade" with a minimum of conflict or exploitation of either people or natural resources.
The Advisory Board incorporated the Ithaca HOUR system as Ithaca Hours, Inc. in October 1998, and hosted the first elections for Board of Directors in March 1999. The first Board of Directors included Monica Hargraves, Dan Cogan, Margaret McCasland, Erica Van Etten, Greg Spence Wolf, Bob LeRoy, LeGrace Benson, Wally Woods, Jennifer Elges, and Donald Stephenson. In May 1999 Glover turned the administration of Ithaca HOURS over to the newly elected Board of Directors. Glover has continued to support Ithaca Hours through community outreach to present, most notably through the Ithaca Health Fund (now incorporated as part
of the Ithaca Health Alliance) and Ithaca Community News.
The current Board of Directors, 2014-2015, includes Erik Lehmann (Chair), Danielle Klock, and Bob LeRoy.
Economic development.
Several million dollars value of HOURS have been traded since 1991 among thousands of residents and over 500 area businesses, including the Cayuga Medical Center, Alternatives Federal Credit Union, the public library, many local farmers, movie theatres, restaurants, healers, plumbers, carpenters, electricians, and landlords.
One of the primary functions of the Ithaca Hours system is to promote local economic development. Businesses who receive Hours must spend them on local goods and services, thus building a network of inter-supporting local businesses. While non-local businesses are welcome to accept Hours, those businesses need to spend them on local goods and services to be economically sustainable.
In their mission to promote local economic development, the Board of Directors also makes interest-free loans of Ithaca HOURS to local businesses and grants to local non-profit organizations.

</doc>
<doc id="14979" url="https://en.wikipedia.org/wiki?curid=14979" title="Interstellar cloud">
Interstellar cloud

An interstellar cloud is the generic name given to an accumulation of gas, plasma, and dust in our and other galaxies. Put differently, an interstellar cloud is a denser-than-average region of the interstellar medium. Depending on the density, size, and temperature of a given cloud, the hydrogen in it can be neutral (H I regions), ionized (H II regions) (i.e. a plasma), or molecular (molecular clouds). Neutral and ionized clouds are sometimes also called diffuse clouds, while molecular clouds are sometimes also referred to as dense clouds.
Chemical compositions.
Analyzing the composition of interstellar clouds is achieved by studying electromagnetic radiation that we receive from them. Large radio telescopes scan the intensity in the sky of particular frequencies of electromagnetic radiation which are characteristic of certain molecules' spectra. Some interstellar clouds are cold and tend to give out EM radiation of large wavelengths. A map of the abundance of these molecules can be made, enabling an understanding of the varying composition of the clouds. In hot clouds, there are often ions of many elements, whose spectra can be seen in visible and ultraviolet light.
Radio telescopes can also scan over the frequencies from one point in the map, recording the intensities of each type of molecule. Peaks of frequencies mean that an abundance of that molecule or atom is present in the cloud. The height of the peak is proportional to the relative percentage that it makes up.
Unexpected chemicals detected in interstellar clouds.
Until recently the rates of reactions in interstellar clouds were expected to be very slow, with minimal products being produced due to the low temperature and density of the clouds. However, organic molecules were observed in the spectra that scientists would not have expected to find under these conditions, such as formaldehyde, methanol, and vinyl alcohol. The reactions needed to create such substances are familiar to scientists only at the much higher temperatures and pressures of earth and earth-based laboratories. The fact that they were found indicates that these chemical reactions in interstellar clouds take place faster than suspected, likely in gas-phase reactions unfamiliar to organic chemistry as observed on earth. These reactions are studied in the CRESU experiment.
Interstellar clouds also provide a medium to study the presence and proportions of metals in space. The presence and ratios of these elements may help develop theories on the means of their production, especially when their proportions are inconsistent with those expected to arise from stars as a result of fusion and thereby suggest alternate means, such as cosmic ray spallation.
High-velocity cloud.
These interstellar clouds possess a velocity higher than can be explained by the rotation of the Milky Way. By definition, these clouds must have a vlsr greater than 90 km s−1, where vlsr is the local standard rest velocity. They are detected primarily in the 21 cm line of neutral hydrogen, and typically have a lower portion of heavy elements than is normal for interstellar clouds in the Milky Way.
Theories intended to explain these unusual clouds include materials left over from the formation of our galaxy, or tidally-displaced matter drawn away from other galaxies or members of the Local Group. An example of the latter is the Magellanic Stream. To narrow down the origin of these clouds, a better understanding of their distances and metallicity is needed.
High-velocity clouds are identified with an HVC prefix, as with HVC 127-41-330.

</doc>
<doc id="14980" url="https://en.wikipedia.org/wiki?curid=14980" title="Imhotep">
Imhotep

Imhotep (; also spelled Immutef, Im-hotep, or Ii-em-Hotep; called "Imuthes" (Ἰμούθης) by the Greeks; fl. 27th century BC (c. 2650–2600 BC); Egyptian: "ỉỉ-m-ḥtp" "*jā-im-ḥātap" meaning "the one who comes in peace, is with peace") was an Egyptian polymath who served under the Third Dynasty king Djoser as chancellor to the pharaoh and high priest of the sun god Ra at Heliopolis. He is considered by some to be the earliest known architect and engineer and physician in history, though two other physicians, Hesy-Ra and Merit-Ptah, lived around the same time. The full list of his titles is:
He was one of only a few commoners ever to be accorded divine status after death. The center of his cult was Memphis. From the First Intermediate Period onward Imhotep was also revered as a poet and philosopher. His sayings were famously referenced in poems: "I have heard the words of Imhotep and Hordedef with whose discourses men speak so much."
The location of Imhotep's self-constructed tomb was well hidden from the beginning and it remains unknown, despite efforts to find it. The consensus is that it is hidden somewhere at Saqqara. Imhotep's historicity is confirmed by two contemporary inscriptions made during his lifetime on the base or pedestal of one of Djoser's statues (Cairo JE 49889) and also by a graffito on the enclosure wall surrounding Sekhemkhet's unfinished step-pyramid. The latter inscription suggests that Imhotep outlived Djoser by a few years and went on to serve in the construction of King Sekhemkhet's pyramid, which was abandoned due to this ruler's brief reign.
Attribution of achievements and inventions.
Architecture and engineering.
Imhotep was one of the chief officials of the Pharaoh Djoser. Egyptologists ascribe to him the design of the Pyramid of Djoser (the Step Pyramid) at Saqqara in Egypt in 2630 – 2611 BC. He may have been responsible for the first known use of stone columns to support a building.
As an instigator of Egyptian culture, Imhotep's idealized image lasted well into the Ptolemaic period. The Egyptian historian Manetho credited him with inventing the method of a stone-dressed building during Djoser's reign, though he was not the first to actually build with stone. Stone walling, flooring, lintels, and jambs had appeared sporadically during the Archaic Period, though it is true that a building of the Step Pyramid's size and made entirely out of stone had never before been constructed. Prior to Djoser, pharaohs were buried in mastaba tombs.
Medicine.
Imhotep was a major figure in Ancient Egyptian medicine. He was the author of a medical treatise remarkable for being devoid of magical thinking: the so-called Edwin Smith papyrus containing anatomical observations, ailments, and cures. The surviving copy of the papyrus was probably written around 1700 BC but may be a copy of texts written a thousand years earlier. However, this attribution of authorship is speculative. Today the Papyrus is on display at the Brooklyn Children's Museum, New York City. The 48 medical cases described within the Edwin Smith Surgical Papyrus concern:
Deification.
Two thousand years after his death, Imhotep's status was raised to that of a deity of medicine and healing. He was identified or confused with Thoth, the god of architecture, mathematics, medicine and patron of the scribes, having Imhotep's cult merging with that of his former tutelary god. He was also associated with Amenhotep son of Hapu, who was another deified architect, in the region of Thebes where they were worshiped as "brothers" in temples dedicated to Thoth and later in Hermopolis following the syncretist concept of Hermes-Thot, a concept that led to another syncretic belief, that of Hermes Trismegistus and hermeticism. Imhotep was also linked to Asklepios by the Greeks.
Birth myths.
According to myth, Imhotep's mother was a mortal named Kheredu-ankh, elevated later to semi-divine status by claims that she was the daughter of Banebdjedet. Alternatively, since Imhotep was known as the "Son of Ptah," his mother was sometimes claimed to be Sekhmet, the patron of Upper Egypt whose consort was Ptah. According to another tale, his father may have been an architect named Kanofer.
Legacy.
According to the "Encyclopædia Britannica", "The evidence afforded by Egyptian and Greek texts support the view that Imhotep's reputation was very respected in early times ... His prestige increased with the lapse of centuries and his temples in Greek times were the centers of medical teachings."
It is Imhotep, says Sir William Osler, who was the real "Father of Medicine", "the first figure of a physician to stand out clearly from the mists of antiquity."
Descriptions of Imhotep by James Henry Breasted "et al." :
Imhotep's dreams.
The Upper Egyptian Famine Stela, which dates from the Ptolemaic period, bears an inscription containing a legend about a famine of seven years duration during the reign of Djoser. Imhotep is credited with having been instrumental in ending it. One of his priests explained the connection between the god Khnum and the rise of the Nile to the king, who then had a dream in which the Nile god spoke to him, promising to end the drought.
Biographical papyrus.
A demotic papyrus from the ancient Egyptian temple of Tebtunis, dating to the 2nd century AD, preserves a long story about Imhotep. King Djoser plays a prominent role in the story, which also mentions Imhotep's family; his father the god Ptah, his mother Khereduankh, and his little-sister Renpetneferet. At one point Djoser desires the young Renpetneferet, and Imhotep disguises himself and tries to rescue her. The text also refers to the royal tomb of Djoser, by which the Step Pyramid must be meant. An anachronistic detail is a battle between the Egyptian and Assyrian armies where Imhotep fights an Assyrian sorceress in a duel of magic.

</doc>
<doc id="14981" url="https://en.wikipedia.org/wiki?curid=14981" title="Ictinus">
Ictinus

Ictinus (; , "Iktinos") was an architect active in the mid 5th century BC. Ancient sources identify Ictinus and Callicrates as co-architects of the Parthenon.
Pausanias identifies Ictinus as architect of the Temple of Apollo at Bassae. That temple was Doric on the exterior, Ionic on the interior, and incorporated a Corinthian column, the earliest known, at the center rear of the cella. Sources also identify Ictinus as architect of the Telesterion at Eleusis, a gigantic hall used in the Eleusinian Mysteries.
The artist Jean Auguste Dominique Ingres painted a scene showing Ictinus together with the lyric poet Pindar. The painting is known as "Pindar and Ictinus" and is exhibited at the National Gallery, London.

</doc>
<doc id="14982" url="https://en.wikipedia.org/wiki?curid=14982" title="Isidore of Miletus">
Isidore of Miletus

Isidore of Miletus () was one of the two main Byzantine Greek architects (Anthemius of Tralles was the other) that Emperor Justinian I commissioned to design the church of Hagia Sophia in Constantinople from 532-537. He also created the first comprehensive compilation of Archimedes' works.
Summary.
Isidore of Miletus was a renowned scientist and mathematician before Emperor Justinian I hired him, “Isidorus taught stereometry and physics at the universities, first of Alexandria then of Constantinople, and wrote a commentary on an older treatise on vaulting.” Isidore is also renowned for producing the first comprehensive compilation of Archimedes' work.
Emperor Justinian I appointed his architects to rebuild the Hagia Sophia following his victory over protesters within the capital city of his Roman Empire, Constantinople. The first basilica was completed in 360 and remodelled from 404 to 415, but had been damaged in 532 in the course of the Nika Riot, “The temple of Sophia, the baths of Zeuxippus, and the imperial courtyard from the Propylaia all the way to the so-called House of Ares were burned up and destroyed, as were both of the great porticoes that lead to the forum that is named after Constantine, houses of prosperous people, and a great deal of other properties.”
The warring factions of Byzantine society, the Blues and the Greens, opposed each other in the chariot races at the Hippodrome and often resorted to violence. During the Nika Riot, more than thirty thousand people died. Emperor Justinian I ensured that his new structure would not be burned down, like its predecessors, by commissioning architects that would build the church mainly out of stone, rather than wood, “He compacted it of baked brick and mortar, and in many places bound it together with iron, but made no use of wood, so that the church should no longer prove combustible.”
Isidore of Miletus and Anthemius of Tralles originally planned on a main hall of the Hagia Sophia that measured 230 feet by 250 feet, making it the largest church in Constantinople, but the original dome was nearly 20 feet lower than it was constructed, “Justinian suppressed these riots and took the opportunity of marking his victory by erecting in 532-7 the new Hagia Sophia, one of the largest, most lavish, and most expensive buildings of all time.”
Although Isidore of Miletus and Anthemius of Tralles were not formally educated in architecture, they were scientists that could organize the logistics of drawing thousands of labourers and unprecedented loads of rare raw materials from around the Roman Empire to create the Hagia Sophia for Emperor Justinian I. The finished product was built in admirable form for the Roman Emperor, “All of these elements marvellously fitted together in mid-air, suspended from one another and reposing only on the parts adjacent to them, produce a unified and most remarkable harmony in the work, and yet do not allow the spectators to rest their gaze upon any one of them for a length of time.”
Conclusion.
The Hagia Sophia architects innovatively combined the longitudinal structure of a Roman basilica and the central plan of a drum-supported dome, in order to withstand the high magnitude earthquakes of the Marmara Region, “However, in May 558, little more than 20 years after the Church’s dedication, following the earthquakes of August 553 and December 557, parts of the central dome and its supporting structure system collapsed.” The Hagia Sophia was repeatedly cracked by earthquakes and was quickly repaired. Isidore of Miletus’ nephew, Isidore the Younger, introduced the new dome design that can be viewed in the Hagia Sophia in present day Istanbul, Turkey.
After a great earthquake in 989 ruined the dome of Hagia Sophia, the Byzantine officials summoned Trdat the Architect to Byzantium to organize repairs. The restored dome was completed by 994.

</doc>
<doc id="14984" url="https://en.wikipedia.org/wiki?curid=14984" title="International Atomic Energy Agency">
International Atomic Energy Agency

The International Atomic Energy Agency (IAEA) is an international organization that seeks to promote the peaceful use of nuclear energy, and to inhibit its use for any military purpose, including nuclear weapons. The IAEA was established as an autonomous organization on 29 July 1957. Though established independently of the United Nations through its own international treaty, the IAEA Statute, the IAEA reports to both the United Nations General Assembly and Security Council.
The IAEA has its headquarters in Vienna. The IAEA has two "Regional Safeguards Offices" which are located in Toronto, Canada, and in Tokyo, Japan. The IAEA also has two liaison offices which are located in New York City, United States, and in Geneva, Switzerland. In addition, the IAEA has three laboratories located in Vienna and Seibersdorf, Austria, and in Monaco.
The IAEA serves as an intergovernmental forum for scientific and technical cooperation in the peaceful use of nuclear technology and nuclear power worldwide. The programs of the IAEA encourage the development of the peaceful applications of nuclear technology, provide international safeguards against misuse of nuclear technology and nuclear materials, and promote nuclear safety (including radiation protection) and nuclear security standards and their implementation.
The IAEA and its former Director General, Mohamed ElBaradei, were jointly awarded the Nobel Peace Prize on 7 October 2005. The IAEA's current Director General is Yukiya Amano.
History.
In 1953, the President of the United States, Dwight D. Eisenhower, proposed the creation of an international body to both regulate and promote the peaceful use of atomic power (nuclear power), in his Atoms for Peace address to the UN General Assembly. In September 1954, the United States proposed to the General Assembly the creation of an international agency to take control of fissile material, which could be used either for nuclear power or for nuclear weapons. This agency would establish a kind of "nuclear bank."
The United States also called for an international scientific conference on all of the peaceful aspects of nuclear power. By November 1954, it had become clear that the Soviet Union would reject any international custody of fissile material, but that a "clearing house" for nuclear transactions might be possible. From 8 to 20 August 1955, the United Nations held the International Conference on the Peaceful Uses of Atomic Energy in Geneva, Switzerland. In October 1956, a Conference on the IAEA Statute was held at the Headquarters of the United Nations to approve the founding document for the IAEA, which was negotiated in 1955-1956 by a group of twelve countries. The Statute of the IAEA was approved on 23 October 1956 and came into force on 29 July 1957.
Former U.S. Congressman W. Sterling Cole served as the IAEA's first Director General from 1957 to 1961. Cole served only one term, after which the IAEA was headed by two Swedes for nearly four decades: the scientist Sigvard Eklund held the job from 1961 to 1981, followed by former Swedish Foreign Minister Hans Blix, who served from 1981 to 1997. Blix was succeeded as Director General by Mohamed ElBaradei of Egypt, who served until November 2009.
Beginning in 1986, in response to the nuclear reactor explosion and disaster near Chernobyl, Ukraine, the IAEA redoubled its efforts in the field of nuclear safety. The same happened after the 2011 Fukushima disaster in Fukushima, Japan.
Both the IAEA and its then Director General, ElBaradei, were awarded the Nobel Peace Prize in 2005. In ElBaradei's acceptance speech in Oslo, he stated that only one percent of the money spent on developing new weapons would be enough to feed the entire world, and that, if we hope to escape self-destruction, then nuclear weapons should have no place in our collective conscience, and no role in our security.
On 2 July 2009, Yukiya Amano of Japan was elected as the Director General for the IAEA, defeating Abdul Samad Minty of South Africa and Luis E. Echávarri of Spain. On 3 July 2009, the Board of Governors voted to appoint Yukiya Amano "by acclamation," and IAEA General Conference in September 2009 approved. He took office on 1 December 2009.
Structure and function.
General.
The IAEA's mission is guided by the interests and needs of Member States, strategic plans and the vision embodied in the IAEA Statute (see below). Three main pillars – or areas of work – underpin the IAEA's mission: Safety and Security; Science and Technology; and Safeguards and Verification
The IAEA as an autonomous organization is not under direct control of the UN, but the IAEA does report to both the UN General Assembly and Security Council. Unlike most other specialized international agencies, the IAEA does much of its work with the Security Council, and not with the United Nations Economic and Social Council. The structure and functions of the IAEA are defined by its founding document, the IAEA Statute (see below). The IAEA has three main bodies: the Board of Governors, the General Conference, and the Secretariat.
The IAEA exists to pursue the "safe, secure and peaceful uses of nuclear sciences and technology" (Pillars 2005). The IAEA executes this mission with three main functions: the inspection of existing nuclear facilities to ensure their peaceful use, providing information and developing standards to ensure the safety and security of nuclear facilities, and as a hub for the various fields of science involved in the peaceful applications of nuclear technology.
The IAEA recognizes knowledge as the nuclear energy industry’s most valuable asset and resource, without which the industry cannot operate safely and economically. Following the IAEA General Conference since 2002 resolutions the Nuclear Knowledge Management, a formal programme was established to address Member States' priorities in the 21st century.
In 2004, the IAEA developed a Programme of Action for Cancer Therapy (PACT). PACT responds to the needs of developing countries to establish, to improve, or to expand radiotherapy treatment programs. The IAEA is raising money to help efforts by its Member States to save lives and to reduce suffering of cancer victims.
The IAEA has established programs to help developing countries in planning to build systematically the capability to manage a nuclear power program, including the Integrated Nuclear Infrastructure Group, which has carried out Integrated Nuclear Infrastructure Review missions in Indonesia, Jordan, Thailand and Vietnam. The IAEA reports that roughly 60 countries are considering how to include nuclear power in their energy plans.
To enhance the sharing of information and experience among IAEA Member States concerning the seismic safety of nuclear facilities, in 2008 the IAEA established the International Seismic Safety Centre. This centre is establishing safety standards and providing for their application in relation to site selection, site evaluation and seismic design.
Board of Governors.
The Board of Governors is one of two policy making bodies of the IAEA. The Board consists of 22 member states elected by the General Conference, and at least 10 member states nominated by the outgoing Board. The outgoing Board designates the ten members who are the most advanced in atomic energy technology, plus the most advanced members from any of the following areas that are not represented by the first ten: North America, Latin America, Western Europe, Eastern Europe, Africa, Middle East and South Asia, South East Asia, the Pacific, and the Far East. These members are designated for one year terms. The General Conference elects 22 members from the remaining nations to two-year terms. Eleven are elected each year. The 22 elected members must also represent a stipulated geographic diversity. The 35 Board members for the period 2012–2013 are: Algeria, Argentina, Australia, Belgium, Brazil, Bulgaria, Canada, China, Costa Rica, Cuba, Egypt, France, Germany, Greece, Hungary, India, Indonesia, Italy, Japan, the Republic of Korea, Libya, Mexico, Nigeria, Norway, Pakistan, Poland, the Russian Federation, Saudi Arabia, South Africa, Sweden, Thailand, the United Kingdom, Tanzania, the United States of America and Uruguay.
The Board, in its five yearly meetings, is responsible for making most of the policy of the IAEA. The Board makes recommendations to the General Conference on IAEA activities and budget, is responsible for publishing IAEA standards and appoints the Director General subject to General Conference approval. Board members each receive one vote. Budget matters require a two-thirds majority. All other matters require only a simple majority. The simple majority also has the power to stipulate issues that will thereafter require a two-thirds majority. Two-thirds of all Board members must be present to call a vote. The Board elects its own chairman.
General Conference.
The General Conference is made up of all 168 member states. It meets once a year, typically in September, to approve the actions and budgets passed on from the Board of Governors. The General Conference also approves the nominee for Director General and requests reports from the Board on issues in question (Statute). Each member receives one vote. Issues of budget, Statute amendment and suspension of a member's privileges require a two- thirds majority and all other issues require a simple majority. Similar to the Board, the General Conference can, by simple majority, designate issues to require a two- thirds majority. The General Conference elects a President at each annual meeting to facilitate an effective meeting. The President only serves for the duration of the session (Statute).
The main function of the General Conference is to serve as a forum for debate on current issues and policies. Any of the other IAEA organs, the Director General, the Board and member states can table issues to be discussed by the General Conference (IAEA Primer). This function of the General Conference is almost identical to the General Assembly of the United Nations.
Secretariat.
The Secretariat is the professional and general service staff of the IAEA. The Secretariat is headed by the Director General. The Director General is responsible for enforcement of the actions passed by the Board of Governors and the General Conference. The Director General is selected by the Board and approved by the General Conference for renewable four-year terms. The Director General oversees six departments that do the actual work in carrying out the policies of the IAEA: Nuclear Energy, Nuclear Safety and Security, Nuclear Sciences and Applications, Safeguards, Technical Cooperation, and Management.
The IAEA budget is in two parts. The regular budget funds most activities of the IAEA and is assessed to each member nation (€344 million in 2014). The Technical Cooperation Fund is funded by voluntary contributions with a general target in the US$90 million range.
Missions.
The IAEA is generally described as having three main missions:
Peaceful uses.
According to Article II of the IAEA Statute, the objective of the IAEA is "to accelerate and enlarge the contribution of atomic energy to peace, health and prosperity throughout the world." Its primary functions in this area, according to Article III, are to encourage research and development, to secure or provide materials, services, equipment and facilities for Member States, to foster exchange of scientific and technical information and training.
Three of the IAEA's six Departments are principally charged with promoting the peaceful uses of nuclear energy. The Department of Nuclear Energy focuses on providing advice and services to Member States on nuclear power and the nuclear fuel cycle. The Department of Nuclear Sciences and Applications focuses on the use of non-power nuclear and isotope techniques to help IAEA Member States in the areas of water, energy, health, biodiversity, and agriculture. The Department of Technical Cooperation provides direct assistance to IAEA Member States, through national, regional, and inter-regional projects through training, expert missions, scientific exchanges, and provision of equipment.
Safeguards.
Article II of the IAEA Statute defines the Agency's twin objectives as promoting peaceful uses of atomic energy and "ensur, so far as it is able, that assistance provided by it or at its request or under its supervision or control is not used in such a way as to further any military purpose." To do this, the IAEA is authorized in Article III.A.5 of the Statute "to establish and administer safeguards designed to ensure that special fissionable and other materials, services, equipment, facilities, and information made available by the Agency or at its request or under its supervision or control are not used in such a way as to further any military purpose; and to apply safeguards, at the request of the parties, to any bilateral or multilateral arrangement, or at the request of a State, to any of that State's activities in the field of atomic energy."
The Department of Safeguards is responsible for carrying out this mission, through technical measures designed to verify the correctness and completeness of states' nuclear declarations.
Nuclear safety.
The IAEA classifies safety as one of its top three priorities. It spends 8.9 percent of its 352 million-euro ($469 million) regular budget in 2011 on making plants secure from accidents. Its resources are used on the other two priorities: technical cooperation and preventing nuclear weapons proliferation.
The IAEA itself says that, beginning in 1986, in response to the nuclear reactor explosion and disaster near Chernobyl, Ukraine, the IAEA redoubled its efforts in the field of nuclear safety. The IAEA says that the same happened after the Fukushima disaster in Fukushima, Japan.
In June 2011, the IAEA chief said he had "broad support for his plan to strengthen international safety checks on nuclear power plants to help avoid any repeat of Japan's Fukushima crisis". Peer-reviewed safety checks on reactors worldwide, organized by the IAEA, have been proposed.
Criticism.
Russian nuclear accident specialist Iouli Andreev is critical of the response to Fukushima, and says that the IAEA did not learn from the 1986 Chernobyl disaster. He has accused the IAEA and corporations of "wilfully ignoring lessons from the world's worst nuclear accident 25 years ago to protect the industry's expansion". The IAEA's role "as an advocate for nuclear power has made it a target for protests".
The journal "Nature" has reported that the IAEA response to the Fukushima I nuclear accidents in Japan was "sluggish and sometimes confusing", drawing calls for the agency to "take a more proactive role in nuclear safety". But nuclear experts say that the agency's complicated mandate and the constraints imposed by its member states mean that reforms will not happen quickly or easily, although its INES "emergency scale is very likely to be revisited" given the confusing way in which it was used in Japan.
Some scientists say that the 2011 Japanese nuclear accidents have revealed that the nuclear industry lacks sufficient oversight, leading to renewed calls to redefine the mandate of the IAEA so that it can better police nuclear power plants worldwide. There are several problems with the IAEA says Najmedin Meshkati of University of Southern California:
It recommends safety standards, but member states are not required to comply; it promotes nuclear energy, but it also monitors nuclear use; it is the sole global organization overseeing the nuclear energy industry, yet it is also weighed down by checking compliance with the Nuclear Non-Proliferation Treaty (NPT).
The journal "Nature" has reported that "the world must strengthen the ability of the International Atomic Energy Agency to make independent assessments of nuclear safety" and that "the public would be better served by an IAEA more able to deliver frank and independent assessments of nuclear crises as they unfold".
Membership.
The process of joining the IAEA is fairly simple. Normally, a State would notify the Director General of its desire to join, and the Director would submit the application to the Board for consideration. If the Board recommends approval, and the General Conference approves the application for membership, the State must then submit its instrument of acceptance of the IAEA Statute to the United States, which functions as the depositary Government for the IAEA Statute. The State is considered a member when its acceptance letter is deposited. The United States then informs the IAEA, which notifies other IAEA Member States. Signature and ratification of the Nuclear Non-Proliferation Treaty (NPT) are not preconditions for membership in the IAEA.
The IAEA has 168 member states. Most UN members and the Holy See are Member States of the IAEA. Non-member states Cape Verde (2007), Tonga (2011) and Comoros (2014) have been approved for membership and will become a Member State if they deposit the necessary legal instruments.
Four states have withdrawn from the IAEA. North Korea was a Member State from 1974–1994, but withdrew after the Board of Governors found it in non-compliance with its safeguards agreement and suspended most technical cooperation. Nicaragua became a member in 1957, withdrew its membership in 1970, and rejoined in 1977, Honduras joined in 1957, withdrew in 1967, and rejoined in 2003, while Cambodia joined in 1958, withdrew in 2003, and rejoined in 2009.

</doc>
<doc id="14985" url="https://en.wikipedia.org/wiki?curid=14985" title="International Civil Aviation Organization">
International Civil Aviation Organization

The International Civil Aviation Organization (ICAO, pronounced ; , OACI), is a specialized agency of the United Nations. It codifies the principles and techniques of international air navigation and fosters the planning and development of international air transport to ensure safe and orderly growth. Its headquarters are located in the "Quartier International" of Montreal, Canada.
The ICAO Council adopts standards and recommended practices concerning air navigation, its infrastructure, flight inspection, prevention of unlawful interference, and facilitation of border-crossing procedures for international civil aviation. ICAO defines the protocols for air accident investigation followed by transport safety authorities in countries signatory to the Convention on International Civil Aviation ("Chicago Convention").
The Air Navigation Commission (ANC) is the technical body within ICAO. The Commission is composed of 19 Commissioners, nominated by the ICAO's contracting states, and appointed by the ICAO Council. Commissioners serve as independent experts, who although nominated by their states, do not serve as state or political representatives. The development of Aviation Standards and Recommended Practices is done under the direction of the ANC through the formal process of ICAO Panels. Once approved by the Commission, standards are sent to the Council, the political body of ICAO, for consultation and coordination with the Member States before final adoption.
ICAO is distinct from the International Air Transport Association (IATA), a trade association representing 262 of the world’s airlines, also headquartered in Montreal, or with the Civil Air Navigation Services Organisation (CANSO), an organization for Air Navigation Service Providers (ANSPs) with its headquarters at Amsterdam Airport Schiphol in the Netherlands. These are trade associations representing specific aviation interests, whereas ICAO is a body of the United Nations.
History.
The forerunner to ICAO was the International Commission for Air Navigation (ICAN). It held its first convention in 1903 in Berlin, Germany but no agreements were reached among the eight countries that attended. At the second convention in 1906, also held in Berlin, 27 countries attended. The third convention, held in London in 1912 allocated the first radio callsigns for use by aircraft. ICAN continued to operate until 1945.
Fifty-two countries signed the Convention on International Civil Aviation, also known as the Chicago Convention, in Chicago, Illinois, on 7 December 1944. Under its terms, a Provisional International Civil Aviation Organization (PICAO) was to be established, to be replaced in turn by a permanent organization when 26 countries ratified the convention. Accordingly, PICAO began operating on 6 June 1945, replacing ICAN. The 26th country ratified the Convention on 5 March 1947 and, consequently PICAO was disestablished on 4 April 1947 and replaced by ICAO, which began operations the same day. In October 1947, ICAO became an agency of the United Nations linked to the United Nations Economic and Social Council (ECOSOC).
In April 2013, Qatar offered to serve as the new permanent seat of the Organization. Qatar promised to construct a massive new headquarters for ICAO and cover all moving expenses, stating that Montreal "was too far from Europe and Asia", "had cold winters," was hard to attend due to the refusal of the Canadian government to provide visas in a timely manner, and that the taxes imposed on ICAO by Canada were too high. According to the "Globe and Mail", Qatar's move was at least partly motivated by the pro-Israel foreign policy of Canadian Prime Minister Stephen Harper. Approximately one month later, Qatar withdrew its bid after a separate proposal to the ICAO's governing council to move the ICAO triennial conference to Doha was defeated by a vote of 22–14.
Statute.
The 9th edition of the Convention on International Civil Aviation includes modifications from 1948 up to year 2006. ICAO refers to its current edition of the Convention as the "Statute", and designates it as ICAO Doc 7300/9. The Convention has 19 Annexes that are listed by title in the article Convention on International Civil Aviation.
Membership.
, there are 191 ICAO members, consisting of 190 of the 193 UN members (all but Dominica, Liechtenstein, and Tuvalu), plus the Cook Islands.
Liechtenstein has delegated Switzerland to implement the treaty to make it applicable in the territory of Liechtenstein.
Council.
The Council of ICAO is elected by the Assembly every 3 years and consists of 36 members elected in 3 categories. The present Council was elected on 1 October 2013 at the 38th Assembly of ICAO at Montreal. The Structure of present Council is as follows:
PART I – (States of chief importance in air transport) – Australia, Brazil, Canada, China, France, Germany, Italy, Japan, Russian Federation, United Kingdom and the United States. All of them have been re-elected.
PART II – (States which make the largest contribution to the provision of facilities for international civil air navigation) – Argentina, Egypt, India, Mexico, Nigeria, Norway, Portugal, Saudi Arabia, Singapore, South Africa, Spain and Venezuela. Except Norway, Portugal and Venezuela, all others have been re-elected.
PART III– (States ensuring geographic representation)- Bolivia, Burkina Faso, Cameroon, Chile, Dominican Republic, Kenya, Libya, Malaysia, Nicaragua, Poland, Republic of Korea, United Arab Emirates and United Republic of Tanzania. Poland has been elected for the first time.
Standards.
ICAO also standardizes certain functions for use in the airline industry, such as the Aeronautical Message Handling System (AMHS). This makes it a standards organization.
Each country should have an accessible Aeronautical Information Publication (AIP), based on standards defined by ICAO, containing information essential to air navigation. Countries are required to update their AIP manuals every 28 days and so provide definitive regulations, procedures and information for each country about airspace and airports. ICAO's standards also dictate that temporary hazards to aircraft are regularly published using NOTAMs.
ICAO defines an International Standard Atmosphere (also known as ICAO Standard Atmosphere), a model of the standard variation of pressure, temperature, density, and viscosity with altitude in the Earth's atmosphere. This is useful in calibrating instruments and designing aircraft.
ICAO standardizes machine-readable passports worldwide. Such passports have an area where some of the information otherwise written in textual form is written as strings of alphanumeric characters, printed in a manner suitable for optical character recognition. This enables border controllers and other law enforcement agents to process such passports quickly, without having to input the information manually into a computer. ICAO publishes Doc 9303 "Machine Readable Travel Documents", the technical standard for machine-readable passports. A more recent standard is for biometric passports. These contain biometrics to authenticate the identity of travellers. The passport's critical information is stored on a tiny RFID computer chip, much like information stored on smartcards. Like some smartcards, the passport book design calls for an embedded contactless chip that is able to hold digital signature data to ensure the integrity of the passport and the biometric data.
ICAO is active in infrastructure management, including Communication, Navigation, Surveillance / Air Traffic Management (CNS/ATM) systems, which employ digital technologies (like satellite systems with various levels of automation) in order to maintain a seamless global air traffic management system.
Registered codes.
Both ICAO and IATA have their own airport and airline code systems.
ICAO uses 4-letter airport codes (vs. IATA's 3-letter codes). The ICAO code is based on the region and country of the airport—for example, Charles de Gaulle Airport has an ICAO code of LFPG, where L indicates Southern Europe, F, France, PG, Paris de Gaulle, while Orly Airport has the code LFPO (the 3rd letter sometimes refers to the particular flight information region (FIR) or the last two may be arbitrary). In most of the world, ICAO and IATA codes are unrelated; for example, Charles de Gaulle Airport has an IATA code of CDG and Orly, ORY. However, the location prefix for continental United States is K and ICAO codes are usually the IATA code with this prefix. For example, the ICAO code for Los Angeles International Airport is KLAX. Canada follows a similar pattern, where a prefix of C is usually added to an IATA code to create the ICAO code. For example, Edmonton International Airport is YEG or CYEG. (In contrast, airports in Hawaii are in the Pacific region and so have ICAO codes that start with PH; Kona International Airport's code is PHKO. Similarly, airports in Alaska have ICAO codes that start with PA. Merrill Field, for instance is PAMR.) Note that not all airports are assigned codes in both systems; for example, airports that do not have airline service do not need an IATA code.
ICAO also assigns 3-letter airline codes (versus the more-familiar 2-letter IATA codes—for example, UAL vs. UA for United Airlines). ICAO also provides telephony designators to aircraft operators worldwide, a one- or two-word designator used on the radio, usually, but not always, similar to the aircraft operator name. For example, the identifier for Japan Airlines International is JAL and the designator is Japan Air, but Aer Lingus is EIN and Shamrock. Thus, a Japan Airlines flight numbered 111 would be written as "JAL111" and pronounced "Japan Air One One One" on the radio, while a similarly numbered Aer Lingus would be written as "EIN111" and pronounced "Shamrock One One One". In the US, FAA practices require the digits of the flight number to be spoken in group format ("Japan Air One Eleven" in the above example) while individual digits are used for the aircraft tail number used for unscheduled civil flights.
ICAO maintains the standards for aircraft registration ("tail numbers"), including the alphanumeric codes that identify the country of registration. For example, airplanes registered in the United States have tail numbers starting with N.
ICAO is also responsible for issuing alphanumeric aircraft type codes containing two to four characters. These codes provide the identification that is typically used in flight plans. The Boeing 747 would use B741, B742, B743, etc., depending on the particular variant.
Regions and regional offices.
ICAO has a headquarters, seven regional offices, and one regional sub-office :
ICAO and climate change.
Emissions from international aviation are specifically excluded from the targets agreed under the Kyoto Protocol. Instead, the Protocol invites developed countries to pursue the limitation or reduction of emissions through the International Civil Aviation Organization. ICAO's environmental committee continues to consider the potential for using market-based measures such as trading and charging, but this work is unlikely to lead to global action. It is currently developing guidance for states who wish to include aviation in an emissions trading scheme (ETS) to meet their Kyoto commitments, and for airlines who wish to participate voluntarily in a trading scheme.
Emissions from domestic aviation are included within the Kyoto targets agreed by countries. This has led to some national policies such as fuel and emission taxes for domestic air travel in the Netherlands and Norway, respectively. Although some countries tax the fuel used by domestic aviation, there is no duty on kerosene used on international flights.
ICAO is currently opposed to the inclusion of aviation in the European Union Emission Trading Scheme (EU ETS). The EU, however, is pressing ahead with its plans to include aviation.
Investigations of air disasters.
Most air accident investigations are carried out by an agency of a country that is associated in some way with the accident. For example, the Air Accidents Investigation Branch conducts accident investigations on behalf of the British Government. ICAO has conducted three investigations involving air disasters, of which two were passenger airliners shot down while in international flight over hostile territory.

</doc>
<doc id="14986" url="https://en.wikipedia.org/wiki?curid=14986" title="International Maritime Organization">
International Maritime Organization

The International Maritime Organization (IMO), known as the Inter-Governmental Maritime Consultative Organization (IMCO) until 1982, is a specialised agency of the United Nations responsible for regulating shipping. The IMO was established in Geneva in 1948 and came into force ten years later, meeting for the first time in 1959. Headquartered in London, United Kingdom, the IMO has 171 Member States and three Associate Members.
The IMO's primary purpose is to develop and maintain a comprehensive regulatory framework for shipping and its remit today includes safety, environmental concerns, legal matters, technical co-operation, maritime security and the efficiency of shipping. IMO is governed by an assembly of members and is financially administered by a council of members elected from the assembly. The work of IMO is conducted through five committees and these are supported by technical subcommittees. Member organizations of the UN organizational family may observe the proceedings of the IMO. Observer status is granted to qualified non-governmental organizations.
IMO is supported by a permanent secretariat of employees who are representative of the organization's members. The secretariat is composed of a Secretary-General who is periodically elected by the assembly, and various divisions such as those for marine safety, environmental protection and a conference section.
History.
SOLAS.
Inter-Governmental Maritime Consultative Organization (IMCO) was formed to fulfill a desire to bring the regulation of the safety of shipping into an international framework, for which the creation of the United Nations provided an opportunity. Hitherto such international conventions had been initiated piecemeal, notably the Safety of Life at Sea Convention (SOLAS), first adopted in 1914 following the "Titanic" disaster. IMCO's first task was to update that Convention; the resulting 1960 Convention was subsequently recast and updated in 1974 and it is that Convention that has been subsequently modified and updated to adapt to changes in safety requirements and technology.
When IMCO began its operations in 1958 certain other pre-existing instruments were brought under its aegis, most notable the International Convention for the Prevention of Pollution of the Sea by Oil (OILPOL) 1954. Throughout its existence IMCO, renamed the IMO in 1982, has continued to produce new and updated instruments across a wide range of maritime issues covering not only safety of life and marine pollution but also encompassing safe navigation, search and rescue, wreck removal, tonnage measurement, liability and compensation, ship recycling, the training and certification of seafarers, and piracy. More recently SOLAS has been amended to bring an increased focus on maritime security through the International Ship and Port Facility Security (ISPS) Code. The IMO has also increased its focus on air emissions from ships.
In January 1959, IMO began to maintain and promote the 1954 OILPOL Convention. Under the guidance of IMO, the convention was amended in 1962, 1969, and 1971.
Torrey Canyon.
As oil trade and industry developed, many people in the industry began to recognize a need for further improvements in regards to oil pollution prevention at sea. This became increasingly apparent in 1967, when the tanker "Torrey Canyon" spilled 120,000 tons of crude oil when it ran aground entering the English Channel
The "Torrey Canyon" grounding was the largest oil pollution incident recorded up to that time. Among other things, the accident forced the maritime industry and public to question the efficacy of standing regulations and preventative measures pertaining to oil pollution at sea. This incident prompted a series of new conventions.
MARPOL 73/78.
IMO held an emergency session of its Council to deal with the need to readdress regulations pertaining to maritime pollution. In 1969, the IMO Assembly decided to host an international gathering in 1973 dedicated to this issue. The goal at hand was to develop an international agreement for controlling general environmental contamination by ships when out at sea.
During the next few years IMO brought to the forefront a series of measures designed to prevent large ship accidents and to minimize their effects. It also detailed how to deal with the environmental threat caused by routine ship duties such as the cleaning of oil cargo tanks or the disposal of engine room wastes. Interestingly enough in terms of tonnage the afore-mentioned was a bigger problem than accidental pollution.
The most significant thing to come out of this conference was the International Convention for the Prevention of Pollution from Ships, 1973. It covers not only accidental and operational oil pollution but also different types of pollution by chemicals, goods in packaged form, sewage, garbage and air pollution.
The original MARPOL was signed on 17 February 1973, but did not come into force due to lack of ratifications. The current convention is a combination of 1973 Convention and the 1978 Protocol. It entered into force on 2 October 1983. As of May 2013, 152 states, representing 99.2 per cent of the world's shipping tonnage, are involved in the convention.
In 1983 the IMO established the World Maritime University in Malmö, Sweden.
Membership.
To become a member of the IMO, a state ratifies a multilateral treaty known as the Convention on the International Maritime Organization. As of 2015, there are 171 member states of the IMO, which includes 170 of the UN members and the Cook Islands. The first state to ratify the convention was the United Kingdom in 1949. The most recent member to join was Zambia, which became an IMO member in 2014.
The three associate members of the IMO are the Faroe Islands, Hong Kong and Macao.
Most UN member states that are not members of IMO are landlocked countries. These include Afghanistan, Andorra, Armenia, Belarus, Bhutan, Botswana, Burkina Faso, Burundi, Central African Republic, Chad, Kyrgyzstan, Laos, Lesotho, Liechtenstein, Mali, Niger, Rwanda, South Sudan, Swaziland, Tajikistan and Uzbekistan. However, the Federated States of Micronesia and Nauru, which are island nations in the Pacific Ocean, are non-members.
Structure.
The Organization consists of an Assembly, a Council and five main Committees: the Maritime Safety Committee; the Marine Environment Protection Committee; the Legal Committee; the Technical Co-operation Committee and the Facilitation Committee. A number of Sub-Committees support the work of the main technical committees.
Legal instruments.
IMO is the source of approximately 60 legal instruments that guide the regulatory development of its member states to improve safety at sea, facilitate trade among seafaring states and protect the maritime environment. The most well known is the International Convention for the Safety of Life at Sea (SOLAS), as well as International Convention on Oil Pollution Preparedness, Response and Co-operation (OPRC). Others include the International Oil Pollution Compensation Funds. It also functions as a depository of yet to be ratified treaties, such as the International Convention on Liability and Compensation for Damage in Connection with the Carriage of Hazardous and Noxious Substances by Sea, 1996 (HNS Convention) and Nairobi International Convention of Removal of Wrecks (2007).
IMO regularly enacts regulations, which are broadly enforced by national and local maritime authorities in member countries, such as the International Regulations for Preventing Collisions at Sea (COLREG). The IMO has also enacted a Port State Control (PSC) authority, allowing domestic maritime authorities such as coast guards to inspect foreign-flag ships calling at ports of the many port states. Memoranda of Understanding (protocols) were signed by some countries unifying Port State Control procedures among the signatories.
Current issues.
Recent initiatives at the IMO have included amendments to SOLAS, which upgraded fire protection standards on passenger ships, the International Convention on Standards of Training, Certification and Watchkeeping for Seafarers (STCW) which establishes basic requirements on training, certification and watchkeeping for seafarers and to the Convention on the Prevention of Maritime Pollution (MARPOL 73/78), which required double hulls on all tankers.
In December 2002, new amendments to the 1974 SOLAS Convention were enacted. These amendments gave rise to the International Ship and Port Facility Security (ISPS) Code, which went into effect on 1 July 2004. The concept of the code is to provide layered and redundant defences against smuggling, terrorism, piracy, stowaways, etc. The ISPS Code required most ships and port facilities engaged in international trade to establish and maintain strict security procedures as specified in ship and port specific Ship Security Plans and Port Facility Security Plans.
The IMO is also responsible for publishing the International Code of Signals for use between merchant and naval vessels.
The First Intersessional Meeting of IMO's Working Group on Greenhouse Gas Emissions from Ships took place in Oslo, Norway (23–27 June 2008), tasked with developing the technical basis for the reduction mechanisms that may form part of a future IMO regime to control greenhouse gas emissions from international shipping, and a draft of the actual reduction mechanisms themselves, for further consideration by IMO's Marine Environment Protection Committee (MEPC).
IMO has harmonized information available to seafarers and shore-side traffic services called e-Navigation. An e-Navigation strategy was ratified in 2005, and an implementation plan was developed through three IMO sub-committees. The plan was completed by 2014 and implemented in November of that year.
IMO has also served as a key partner and enabler of US international and interagency efforts to establish Maritime Domain Awareness.
Governance of IMO.
The governing body of the International Maritime Organization is the Assembly which meets every two years. In between Assembly sessions a Council, consisting of 40 Member States elected by the Assembly, acts as the governing body. The technical work of the International Maritime Organization is carried out by a series of Committees. The Secretariat consists of some 300 international civil servants headed by a Secretary-General.
Secretary-General.
The current Secretary-General is Ki Tack Lim (South Korea), elected for a four-year term at the 106th session of the IMO Council in June 2015 and at the 27th session of the IMO's Assembly in November 2015. His mandate started on 1 January 2016.
Previous Secretaries-General:
Technical committees.
The technical work of the International Maritime Organisation is carried out by a series of Committees. This includes:
Maritime Safety Committee.
It is regulated in the Article 28(a) of the Convention on the IMO:
The Maritime Safety Committee is the most senior of these and is the main Technical Committee; it oversees the work of its nine sub-committees and initiates new topics. One broad topic it deals with is the effect of the human element on casualties; this work has been put to all of the sub-committees, but meanwhile, the Maritime Safety Committee has developed a code for the management of ships which will ensure that agreed operational procedures are in place and followed by the ship and shore-side staff.
Sub-Committees.
The MSC and MEPC are assisted in their work by a number of sub-committees which are open to all Member States:
Until 2013 there were nine Sub-Committees as follows:
Resolutions.
Resolution MSC.255(84), of 16 May 2008, adopts the "Code of the International Standards and Recommended Practices for a Safety Investigation into a Marine Casualty or Marine Incident". It is also known as the Casualty Investigation Code.

</doc>
<doc id="14987" url="https://en.wikipedia.org/wiki?curid=14987" title="International Labour Organization">
International Labour Organization

The International Labour Organization (ILO) is a United Nations agency dealing with labour issues, particularly international labour standards, social protection, and work opportunities for all. The ILO has 187 member states: 186 of the 193 UN member states plus the Cook Islands are members of the ILO.
In 1969, the organization received the Nobel Peace Prize for improving peace among classes, pursuing decent work and justice for workers, and providing technical assistance to other developing nations.
The ILO registers complaints against entities that are violating international rules; however, it does not impose sanctions on governments.
Governance, organization, and membership.
Unlike other United Nations specialized agencies, the International Labour Organization has a tripartite governing structure – representing governments, employers, and workers (usually with a ratio of 2:1:1). The rationale behind the tripartite structure is the creation of free and open debate among governments and social partners.
The ILO secretariat (staff) is referred to as the International Labour Office.
Governing Body.
The Governing Body decides the agenda of the International Labour Conference, adopts the draft programme and budget of the organization for submission to the conference, elects the director-general, requests information from member states concerning labour matters, appoints commissions of inquiry and supervises the work of the International Labour Office.
Juan Somavía was the ILO's director-general from 1999 until October 2012, when Guy Ryder was elected as his replacement.
This governing body is composed of 28 government representatives, 14 workers' representatives, and 14 employers' representatives.
Ten of the government seats are held by member states that are nations of "chief industrial importance," as first considered by an "impartial committee." The nations are Brazil, China, France, Germany, India, Italy, Japan, the Russian Federation, the United Kingdom and the United States. The terms of office are three years.
International Labour Conference.
The ILO organizes the International Labour Conference in Geneva every year in June, where conventions and recommendations are crafted and adopted. Also known as the parliament of Labour, the conference also makes decisions about the ILO's general policy, work programme and budget.
Each member state has four representatives at the conference: two government delegates, an employer delegate and a worker delegate. All of them have individual voting rights, and all votes are equal, regardless of the population of the delegate's member state. The employer and worker delegates are normally chosen in agreement with the "most representative" national organizations of employers and workers. Usually, the workers' delegates coordinate their voting, as do the employers' delegates. All delegate have the same rights, and are not required to vote in blocs.
Conventions.
Through July 2011, the ILO has adopted 189 conventions. If these conventions are ratified by enough governments, they become in force. However, ILO conventions are considered international labour standards regardless of ratifications. When a convention comes into force, it creates a legal obligation for ratifying nations to apply its provisions.
Every year the International Labour Conference's Committee on the Application of Standards examines a number of alleged breaches of international labour standards. Governments are required to submit reports detailing their compliance with the obligations of the conventions they have ratified. Conventions that have not been ratified by member states have the same legal force as do recommendations.
In 1998, the 86th International Labour Conference adopted the "Declaration on Fundamental Principles and Rights at Work". This declaration contains four fundamental policies:
The ILO asserts that its members have an obligation to work towards fully respecting these principles, embodied in relevant ILO Conventions. The ILO Conventions which embody the fundamental principles have now been ratified by most member states.
Recommendations.
Recommendations do not have the binding force of conventions and are not subject to ratification. Recommendations may be adopted at the same time as conventions to supplement the latter with additional or more detailed provisions. In other cases recommendations may be adopted separately and may address issues separate from particular conventions.
Membership.
As of April 2016, the ILO has 187 state members. 186 of the 193 member states of the United Nations plus the Cook Islands are members of the ILO. The UN member states which are not members of the ILO are Andorra, Bhutan, Liechtenstein, Micronesia, Monaco, Nauru, and North Korea.
The ILO constitution permits any member of the UN to become a member of the ILO. To gain membership, a nation must inform the Director-General that it accepts all the obligations of the ILO constitution. Other states can be admitted by a two-thirds vote of all delegates, including a two-thirds vote of government delegates, at any ILO General Conference. The Cook Islands, a non-UN state, joined in June 2015.
Members of the ILO under the League of Nations automatically became members when the organization's new constitution came into effect after World War II.
Position within the UN.
The International Labour Organization (ILO) is a specialized agency of the United Nations (UN). As with other UN specialized agencies (or programmes) working on international development, the ILO is also a member of the United Nations Development Group.
History.
Origins.
While the ILO was established as an agency of the League of Nations following World War I, its founders had made great strides in social thought and action before 1919. The core members all knew one another from earlier private professional and ideological networks, in which they exchanged knowledge, experiences, and ideas on social policy. Prewar "epistemic communities", such as the International Association for Labour Legislation (IALL), founded in 1900, and political networks, such as the Socialist Second International, were a decisive factor in the institutionalization of international labour politics.
In the post–World War I euphoria, the idea of a "makeable society" was an important catalyst behind the social engineering of the ILO architects. As a new discipline, international labour law became a useful instrument for putting social reforms into practice. The utopian ideals of the founding members—social justice and the right to decent work—were changed by diplomatic and political compromises made at the Paris Peace Conference of 1919, showing the ILO's balance between idealism and pragmatism.
Over the course of the First World War, the international labour movement proposed a comprehensive programme of protection for the working classes, conceived as compensation for labour's support during the war. Post-war reconstruction and the protection of labour unions occupied the attention of many nations during and immediately after World War I. In Great Britain, the Whitley Commission, a subcommittee of the Reconstruction Commission, recommended in its July 1918 Final Report that "industrial councils" be established throughout the world. The British Labour Party had issued its own reconstruction programme in the document titled "Labour and the New Social Order". In February 1918, the third Inter-Allied Labour and Socialist Conference (representing delegates from Great Britain, France, Belgium and Italy) issued its report, advocating an international labour rights body, an end to secret diplomacy, and other goals. And in December 1918, the American Federation of Labor (AFL) issued its own distinctively apolitical report, which called for the achievement of numerous incremental improvements via the collective bargaining process.
IFTU Bern Conference.
As the war drew to a close, two competing visions for the post-war world emerged. The first was offered by the International Federation of Trade Unions (IFTU), which called for a meeting in Bern, Switzerland, in July 1919. The Bern meeting would consider both the future of the IFTU and the various proposals which had been made in the previous few years. The IFTU also proposed including delegates from the Central Powers as equals. Samuel Gompers, president of the AFL, boycotted the meeting, wanting the Central Powers delegates in a subservient role as an admission of guilt for their countries' role in the bringing about war. Instead, Gompers favoured a meeting in Paris which would only consider President Woodrow Wilson's Fourteen Points as a platform. Despite the American boycott, the Bern meeting went ahead as scheduled. In its final report, the Bern Conference demanded an end to wage labour and the establishment of socialism. If these ends could not be immediately achieved, then an international body attached to the League of Nations should enact and enforce legislation to protect workers and trade unions.
Commission on International Labour Legislation.
Meanwhile, the Paris Peace Conference sought to dampen public support for communism. Subsequently, the Allied Powers agreed that clauses should be inserted into the emerging peace treaty protecting labour unions and workers' rights, and that an international labour body be established to help guide international labour relations in the future. The advisory Commission on International Labour Legislation was established by the Peace Conference to draft these proposals. The Commission met for the first time on 1 February 1919, and Gompers was elected chairman.
Two competing proposals for an international body emerged during the Commission's meetings. The British proposed establishing an international parliament to enact labour laws which each member of the League would be required to implement. Each nation would have two delegates to the parliament, one each from labour and management. An international labour office would collect statistics on labour issues and enforce the new international laws. Philosophically opposed to the concept of an international parliament and convinced that international standards would lower the few protections achieved in the United States, Gompers proposed that the international labour body be authorized only to make recommendations, and that enforcement be left up to the League of Nations. Despite vigorous opposition from the British, the American proposal was adopted.
Gompers also set the agenda for the draft charter protecting workers' rights. The Americans made 10 proposals. Three were adopted without change: That labour should not be treated as a commodity; that all workers had the right to a wage sufficient to live on; and that women should receive equal pay for equal work. A proposal protecting the freedom of speech, press, assembly, and association was amended to include only freedom of association. A proposed ban on the international shipment of goods made by children under the age of 16 was amended to ban goods made by children under the age of 14. A proposal to require an eight-hour work day was amended to require the eight-hour work day "or" the 40-hour work week (an exception was made for countries where productivity was low). Four other American proposals were rejected. Meanwhile, international delegates proposed three additional clauses, which were adopted: One or more days for weekly rest; equality of laws for foreign workers; and regular and frequent inspection of factory conditions.
The Commission issued its final report on 4 March 1919, and the Peace Conference adopted it without amendment on 11 April. The report became Part XIII of the Treaty of Versailles.
Interwar period.
The first annual conference (referred to as the International Labour Conference, or ILC) began on 29 October 1919 at the Pan American Union (building) in Washington, D.C. and adopted the first six International Labour Conventions, which dealt with hours of work in industry, unemployment, maternity protection, night work for women, minimum age,night work for young persons in industry. The prominent French socialist Albert Thomas became its first Director General.
Despite open disappointment and sharp critique, the revived International Federation of Trade Unions (IFTU) quickly adapted itself to this mechanism. The IFTU increasingly oriented its international activities around the lobby work of the ILO.
At the time of establishment, the U.S. government was not a member of ILO, as the US Senate rejected the Covenant of the League of Nations, and the United States could not join any of its agencies. Following the election of Franklin Delano Roosevelt to the U.S. presidency, the new administration made renewed efforts to join the ILO even without League membership. On 19 June 1934, the U.S. Congress passed a joint resolution authorizing the President to join ILO without joining the League of Nations as a whole. On 22 June 1934, the ILO adopted a resolution inviting the U.S. government to join the organization. On 20 August 1934, the U.S. government responded positively and took its seat at the ILO.
Wartime and the United Nations.
During the Second World War, when Switzerland was surrounded by German troops, ILO Director John G. Winant made the decision to leave Geneva. In August 1940, the Government of Canada officially invited the ILO to be housed at McGill University in Montreal. Forty staff members were transferred to the temporary offices and continued to work from McGill until 1948.
The ILO became the first specialized agency of the United Nations system after the demise of the League in 1946. Its constitution, as amended, includes the Declaration of Philadelphia (1944) on the aims and purposes of the organization.
Cold War era.
In July 1970, the United States withdrew 50% of its financial support to the ILO following the appointment of an Assistant-Director General from the Soviet Union. This appointment (by the ILO's British Director-General, C. Wilfred Jenks) drew particular criticism from AFL-CIO president George Meany and from Congressman John E. Rooney. However, the funds were eventually paid.
On 12 June 1975, the ILO voted to grant the Palestinian Liberation Organization observer status at its meetings. Representatives of the United States and Israel walked out of the meeting. The U.S. House of Representatives subsequently decided to withhold funds. The United States gave notice of full withdrawal on 6 November 1975, stating that the organization had become politicized. The United States also suggested that representation from communist countries was not truly "tripartite"—including government, workers, and employers—because of the structure of these economies. The withdrawal became effective on 1 November 1977.
The United States returned to the organization in 1980 after extracting some concessions from the organization. It was partly responsible for the ILO's shift away from a human rights approach and towards support for the Washington Consensus. Economist Guy Standing wrote "the ILO quietly ceased to be an international body attempting to redress structural inequality and became one promoting employment equity".
Programs.
Labour statistics.
The ILO is a major provider of labour statistics. Labour statistics are an important tool for its member states to monitor their progress toward improving labour standards. As part of their statistical work, ILO maintains several databases. This database covers 11 major data series for over 200 countries. In addition, ILO publishes a number of compilations of labour statistics, such as the Key Indicators of Labour Markets (KILM). KILM covers 20 main indicators on labour participation rates, employment, unemployment, educational attainment, labour cost, and economic performance. Many of these indicators have been prepared by other organizations. For example, the Division of International Labour Comparisons of the U.S. Bureau of Labor Statistics prepares the hourly compensation in manufacturing indicator.
The U.S. Department of Labor also publishes a yearly report containing a "List of Goods Produced by Child Labor or Forced Labor" issued by the Bureau of International Labor Affairs. The December 2014 updated edition of the report listed a total of 74 countries and 136 goods.
Training and teaching units.
The International Training Centre of the International Labour Organization (ITCILO) is based in Turin, Italy. Together with the University of Turin, Faculty of Law, the ITC offers training for ILO officers and secretariat members, as well as offering educational programmes. For instance, the ITCILO offers a Master of Laws (LL.M.) programme in Management of Development, which aims specialize professionals in the field of cooperation and development.
Child labour.
The term child labour is often defined as work that deprives children of their childhood, potential, dignity, and is harmful to their physical and mental development.
Child labour refers to work that:
In its most extreme forms, child labour involves children being enslaved, separated from their families, exposed to serious hazards and illnesses and/or left to fend for themselves on the streets of large cities – often at a very early age. Whether or not particular forms of "work" can be called child labour depends on the child's age, the type and hours of work performed, the conditions under which it is performed and the objectives pursued by individual countries. The answer varies from country to country, as well as among sectors within countries. 
Not all work done by children falls under the classification of child labour and therefore should not be so readily targeted for elimination. Children's or adolescents' participation in work that does not negatively affect their health and personal development or interfere with their schooling, is generally regarded as being something positive. This includes activities such as helping their parents around the home, assisting in a family business or earning pocket money outside school hours and during school holidays. These kinds of activities contribute to children's development and to the welfare of their families; they provide them with skills and experience, and help to prepare them to be productive members of society during their adult life.
ILO's response to child labour.
The ILO's International Programme on the Elimination of Child Labour (IPEC) was created in 1992 with the overall goal of the progressive elimination of child labour, which was to be achieved through strengthening the capacity of countries to deal with the problem and promoting a worldwide movement to combat child labour. IPEC currently has operations in 88 countries, with an annual expenditure on technical cooperation projects that reached over US$74 million, €50 million in 2006. It is the largest programme of its kind globally and the biggest single operational programme of the ILO.
The number and range of IPEC's partners have expanded over the years and now include employers' and workers' organizations, other international and government agencies, private businesses, community-based organizations, NGOs, the media, parliamentarians, the judiciary, universities, religious groups and, of course, children and their families.
IPEC's work to eliminate child labour is an important facet of the ILO's Decent Work Agenda. Child labour not only prevents children from acquiring the skills and education they need for a better future, it also perpetuates poverty and affects national economies through losses in competitiveness, productivity and potential income.
ILO's Exceptions in Indigenous Communities.
Because of different cultural views involving labour, the International Labour Organization (ILO) developed a series of culturally sensitive mandates including Conventions No. 169, 107, 138, and 182 to protect indigenous culture, traditions, and identities. Conventions No. 138 and 182 lead in the fight against child labour, while No. 107 and 169 promote the right of indigenous and tribal peoples and protect their right to define their own developmental priorities. The ILO recognizes these changes are necessary to respect the culture and traditions of other communities while also looking after the welfare of children.
In many indigenous communities, parents believe children learn important life lessons through the act of work and through the participation in daily life. Working is seen as a learning process preparing children of the future tasks they will eventually have to do as an adult. It is a belief that the family's and child well-being and survival is a shared responsibility between members of the whole family. They also see work as an intrinsic part of their child's developmental process. While these attitudes toward child work remain, many children and parents from indigenous communities still highly value education. ILO wants to include these communities in the fight against exploitative child labour while being sensitive to their traditions and values.
Issues.
Forced labour.
The ILO has considered the fight against forced labour to be one of its main priorities. During the interwar years, the issue was mainly considered a colonial phenomenon, and the ILO's concern was to establish minimum standards protecting the inhabitants of colonies from the worst abuses committed by economic interests. After 1945, the goal became to set a uniform and universal standard, determined by the higher awareness gained during World War II of politically and economically motivated systems of forced labour, but debates were hampered by the Cold War and by exemptions claimed by colonial powers. Since the 1960s, declarations of labour standards as a component of human rights have been weakened by government of postcolonial countries claiming a need to exercise extraordinary powers over labour in their role as emergency regimes promoting rapid economic development.
In June 1998 the International Labour Conference adopted a Declaration on Fundamental Principles and Rights at Work and its Follow-up that obligates member States to respect, promote and realize freedom of association and the right to collective bargaining, the elimination of all forms of forced or compulsory labour, the effective abolition of child labour, and the elimination of discrimination in respect of employment and occupation.
With the adoption of the Declaration, the International Labour Organization (ILO) created the InFocus Programme on Promoting the Declaration which is responsible for the reporting processes and technical cooperation activities associated with the Declaration; and it carries out awareness raising, advocacy and knowledge functions.
In November 2001, following the publication of the in Focus Programme's first Global Report on forced labour, the ILO Governing Body created a Special Action Programme to Combat Forced Labour (SAP-FL), as part of broader efforts to promote the 1998 Declaration on Fundamental Principles and Rights at Work and its Follow-up.
Since its inception, SAP-FL has focused on raising global awareness of forced labour in its different forms, and mobilising action against its manifestation. Several thematic and country-specific studies and surveys have since been undertaken, on such diverse aspects of forced labour as bonded labour, human trafficking, forced domestic work, rural servitude, and forced prison labour.
The Special Action Programme to combat Forced Labour (SAP-FL) has spearheaded the ILO's work in this field since early 2002. The programme is designed to:
Minimum wage law.
To protect the right of labours for fixing minimum wage, ILO has created Minimum Wage-Fixing Machinery Convention, 1928, Minimum Wage Fixing Machinery (Agriculture) Convention, 1951 and Minimum Wage Fixing Convention, 1970 as minimum wage law.
HIV/AIDS.
The International Labour Organization (ILO) is the lead UN-agency on HIV workplace policies and programmes and private sector mobilization. The ILO recognizes that HIV has a potentially devastating impact on labour and productivity and represents an enormous burden for working people, their families and communities. ILOAIDS is the branch of the ILO dedicated to this issue.
The ILO has been involved with the HIV response since 1998. In June 2001, the ILO's Governing Body adopted a pioneering Code of Practice on HIV/AIDS and the World of Work, which was launched during a special session of the UN General Assembly.
The same year, ILO became a cosponsor of the Joint United Nations Programme on HIV/AIDS (UNAIDS).
In 2010, the 99th International Labour Conference adopted the ILO's Recommendation concerning HIV and AIDS and the World of Work, 2010 (No. 200), the first international labour standard on HIV and AIDS. The Recommendation lays out a comprehensive set of principles to protect the rights of HIV-positive workers and their families, while scaling up prevention in the workplace. Working under the theme of 'Preventing HIV, Protecting Human Rights at Work,' ILOAIDS undertakes a range of policy advisory, research and technical support functions in the area of HIV and AIDS and the world of work. ILO also works on promoting social protection as a means of reducing vulnerability to HIV and mitigating its impact on those living with or affected by HIV.
ILOAIDS is currently engaged in the "Getting to Zero" campaign to arrive at zero new infections, zero AIDS-related deaths and zero-discrimination by 2015. Building on this campaign, ILOAIDS is executing a programme of voluntary and confidential counselling and testing at work, known as VCT@WORK.
Migrant workers.
As the word "migrant" suggests, migrant workers refer to those who moves from place to place to do their job. For the rights of migrant workers, ILO has adopted conventions, including Migrant Workers (Supplementary Provisions) Convention, 1975 and United Nations Convention on the Protection of the Rights of All Migrant Workers and Members of Their Families in 1990.
Domestic workers.
Domestic workers are those who perform a variety of tasks for and in other peoples' homes. For example, they may cook / clean the house and look after children. Yet they are often the ones with the least consideration, excluded from labour and social protection. This is mainly due to the fact that women have traditionally carried out the tasks without pay. For the rights and decent work of domestic workers including migrant domestic workers, ILO has adopted Convention on domestic workers on 16 June 2011.
ILO and globalization.
Seeking a process of globalization that is inclusive, democratically governed and provides opportunities and tangible benefits for all countries and people. The World Commission on the Social Dimension of Globalization was established by the ILO's Governing Body in February 2002 at the initiative of the Director-General in response to the fact that there did not appear to be a space within the multilateral system that would cover adequately and comprehensively the social dimension of the various aspects of globalization. The World Commission Report, A Fair Globalization: Creating Opportunities for All, is the first attempt at structured dialogue among representatives of constituencies with different interests and opinions on the social dimension of globalization, aimed at finding common ground on one of the most controversial and divisive subjects of our time.
Further reading.
Infrastructure of Peace,” /laureates/1969/labour-lecture.html online

</doc>
<doc id="14990" url="https://en.wikipedia.org/wiki?curid=14990" title="IMO">
IMO

IMO or Imo may refer to:

</doc>
<doc id="14996" url="https://en.wikipedia.org/wiki?curid=14996" title="International English">
International English

International English is the concept of the English language as a global means of communication in numerous dialects, and also the movement towards an international standard for the language.It is also referred to as Global English, World English, Common English, Continental English, General English, Engas (English as associate language), or Globish. Sometimes, these terms refer simply to the array of varieties of English spoken throughout the world.
Sometimes, "international English" and the related terms above refer to a desired standardisation, i.e. Standard English; however, there is no consensus on the path to this goal. There have been many proposals for making International English more accessible to people from different nationalities. Basic English is an example, but it failed to make progress. More recently, there have been proposals for English as a lingua franca (ELF). It has also been argued that International English is held back by its traditional spelling. There has been slow progress in adopting alternate spellings.
Historical context.
The modern concept of International English does not exist in isolation, but is the product of centuries of development of the English language.
The English language evolved in England, from a set of West Germanic dialects spoken by the Angles and Saxons, who arrived from continental Europe in the 5th century. Those dialects came to be known as "Englisc" (literally "Anglish"), the language today referred to as Anglo-Saxon or Old English (the language of the poem "Beowulf"). English is thus more closely related to West Frisian than to any other modern language, although less than a quarter of the vocabulary of Modern English is shared with West Frisian or other West Germanic languages because of extensive borrowings from Norse, Norman, Latin, and other languages. It was during the Viking invasions of the Anglo-Saxon period that Old English was influenced by contact with Norse, a group of North Germanic dialects spoken by the Vikings, who came to control a large region in the North of England known as the Danelaw. Vocabulary items entering English from Norse (including the pronouns "they", and "them") are thus attributable to the on-again-off-again Viking occupation of Northern England during the centuries prior to the Norman Conquest (see, e.g., Canute the Great). Soon after the Norman Conquest of 1066, the "Englisc" language ceased being a literary language (see, e.g., Ormulum) and was replaced by Anglo-Norman as the written language of England. During the Norman Period, English absorbed a significant component of French vocabulary (approximately one-third of the vocabulary of Modern English). With this new vocabulary, additional vocabulary borrowed from Latin (with Greek, another approximately one-third of Modern English vocabulary, though some borrowings from Latin and Greek date from later periods), a simplified grammar, and use of the orthographic conventions of French instead of Old English orthography, the language became Middle English (the language of Chaucer). The "difficulty" of English as a written language thus began in the High Middle Ages, when French orthographic conventions were used to spell a language whose original, more suitable orthography had been forgotten after centuries of nonuse. During the late medieval period, King Henry V of England (lived 1387–1422) ordered the use of the English of his day in proceedings before him and before the government bureaucracies. That led to the development of Chancery English, a standardised form used in the government bureaucracy. (The use of so-called Law French in English courts continued through the Renaissance, however.)
The emergence of English as a language of Wales results from the incorporation of Wales into England and also dates from approximately this time period. Soon afterward, the development of printing by Caxton and others accelerated the development of a standardised form of English. Following a change in vowel pronunciation that marks the transition of English from the medieval to the Renaissance period, the language of the Chancery and Caxton became Early Modern English (the language of Shakespeare's day) and with relatively moderate changes eventually developed into the English language of today. Scots, as spoken in the lowlands and along the east coast of Scotland, developed largely independent of Modern English, and is based on the Northern dialects of Anglo-Saxon, particularly Northumbrian, which also serve as the basis of Northern English dialects such as those of Yorkshire and Newcastle upon Tyne. Northumbria was within the Danelaw and therefore experienced greater influence from Norse than did the Southern dialects. As the political influence of London grew, the Chancery version of the language developed into a written standard across Great Britain, further progressing in the modern period as Scotland became united with England as a result of the Acts of Union of 1707.
There have been two introductions of English to Ireland, a medieval introduction that led to the development of the now-extinct Yola dialect and a modern introduction in which Hibernian English largely replaced Irish as the most widely spoken language during the 19th century, following the Act of Union of 1800. Received Pronunciation (RP) is generally viewed as a 19th-century development and is not reflected in North American English dialects, which are based on 18th-century English.
The establishment of the first permanent English-speaking colony in North America in 1607 was a major step towards the globalisation of the language. British English was only partially standardised when the American colonies were established. Isolated from each other by the Atlantic Ocean, the dialects in England and the colonies began evolving independently.
The British colonisation of Australia in 1788 brought the English language to Oceania. By the 19th century, the standardisation of British English was more settled than it had been in the previous century, and this relatively well-established English was brought to Africa, Asia and New Zealand. It developed both as the language of English-speaking settlers from Britain and Ireland, and as the administrative language imposed on speakers of other languages in the various parts of the British Empire. The first form can be seen in New Zealand English, and the latter in Indian English. In Europe, English received a more central role particularly since 1919, when the Treaty of Versailles was composed not only in French, the common language of diplomacy at the time, but, under special request from American president Woodrow Wilson, also in English - a major milestone in the globalisation of English.
The English-speaking regions of Canada and the Caribbean are caught between historical connections with the UK and the Commonwealth and geographical and economic connections with the U.S. In some things they tend to follow British standards, whereas in others, especially commercial, they follow the U.S. standard.
English as a global language.
Braj Kachru divides the use of English into three concentric circles.
The "inner circle" is the traditional base of English and includes countries such as the United Kingdom and Ireland and the anglophone populations of the former British colonies of the United States, Australia, New Zealand, South Africa, Canada, and various islands of the Caribbean, Indian Ocean, and Pacific Ocean.
In the "outer circle" are those countries where English has official or historical importance ("special significance"). This includes most of the countries of the Commonwealth of Nations (the former British Empire), including populous countries such as India, Pakistan, and Nigeria; and others, such as the Philippines, under the sphere of influence of English-speaking countries. Here English may serve as a useful lingua franca between ethnic and language groups. Higher education, the legislature and judiciary, national commerce, and so on, may all be carried out predominantly in English.
The "expanding circle" refers to those countries where English has no official role, but is nonetheless important for certain functions, e.g. international business and tourism. By the twenty-first century, the number of non-native English speakers has come to significantly outnumber the number of native speakers by a factor of three, according to the British Council. Darius Degher, a professor at Malmö University in Sweden, uses the term "decentered English" to describe this shift, along with attendant changes in what is considered to be important to English users and learners.
Research on English as a lingua franca in the sense of "English in the Expanding Circle" is comparatively recent. Linguists who have been active in this field are Jennifer Jenkins, Barbara Seidlhofer, Christiane Meierkord and Joachim Grzega.
English as a lingua franca in foreign language teaching.
English as an additional language (EAL) is usually based on the standards of either American English or British English as well as incorporating foreign terms. English as an international language (EIL) is EAL with emphasis on learning different major dialect forms; in particular, it aims to equip students with the linguistic tools to communicate internationally. Roger Nunn considers different types of competence in relation to the teaching of English as an International Language, arguing that linguistic competence has yet to be adequately addressed in recent considerations of EIL.
Several models of "simplified English" have been suggested for teaching English as a foreign language:
Furthermore, Randolph Quirk and Gabriele Stein thought about a Nuclear English, which, however, has never been fully developed.
With reference to the term "Globish", Robert McCrum has used this to mean "English as global language". Jean-Paul Nerriere uses it for a constructed language.
Basic Global English.
Basic Global English, or BGE, is a concept of global English initiated by German linguist Joachim Grzega. It evolved from the idea of creating a type of English that can be learned more easily than regular British or American English and that serves as a tool for successful global communication. BGE is guided by creating "empathy and tolerance" between speakers in a global context. This applies to the context of global communication, where different speakers with different mother tongues come together. BGE aims to develop this competence as quickly as possible.
English language teaching is almost always related to a corresponding culture, e. g. learners will either deal with American English and therefore with American culture or British English and therefore with British culture. Basic Global English is supposed to solve this problem by creating one collective version of English. Additionally, it is a system that is suited for self-teaching as well as regular teaching.
BGE is based on 20 elementary grammar rules that provide a certain degree of variation. For example, regular as well as irregular formed verbs are accepted. Pronunciation rules are not as strict as in British or American English, so there is a certain degree of variation for the learners. Exceptions that cannot be used are pronunciations that would be harmful to mutual understanding and therefore minimize the success of communication.
Basic Global English is based on a 750-word vocabulary. Additionally, every learner has to acquire the knowledge of 250 additional words. These words can be chosen freely, according to the specific needs and interests of the learner.
BGE provides not only basic language skills, but also so called "Basic Politeness Strategies". These include creating a positive atmosphere, accepting an offer with "Yes, please" or refusing with "No, thank you", and small talk topics to choose and to avoid.
Basic Global English has been tested in two elementary schools in Germany. For the practical test of BGE, 12 lessons were prepared in order to cover half of a school year. After the BGE teaching, students were able to answer questions about themselves, their family, their hobbies etc. Additionally they were able to form questions themselves about the same topics. Besides that, they also learned the numbers from 1 to 31 and vocabulary including things in their school bag and in their classroom. The students as well as the parents had a positive impression of the project.
Varying concepts.
Universality and flexibility.
International English sometimes refers to English as it is actually being used and developed in the world; as a language owned not just by native speakers, but by all those who come to use it.
Basically, it covers the English language at large, often (but not always or necessarily) implicitly seen as standard. It is certainly also commonly used in connection with the acquisition, use, and study of English as the world's lingua franca ('TEIL: Teaching English as an International Language'), and especially when the language is considered as a whole in contrast with "British English", "American English", "South African English", and the like. — McArthur (2002, p. 444–445)
It especially means English words and phrases generally understood throughout the English-speaking world as opposed to localisms. The importance of non-native English language skills can be recognised behind the long-standing joke that the international language of science and technology is broken English.
Neutrality.
International English reaches towards cultural neutrality. This has a practical use:
What could be better than a type of English that saves you from having to re-edit publications for individual regional markets! Teachers and learners of English as a second language also find it an attractive idea — both often concerned that their English should be neutral, without American or British or Canadian or Australian coloring. Any regional variety of English has a set of political, social and cultural connotations attached to it, even the so-called 'standard' forms.
According to this viewpoint, International English is a concept of English that minimises the aspects defined by either the colonial imperialism of Victorian Britain or the cultural imperialism of the 20th century United States. While British colonialism laid the foundation for English over much of the world, International English is a product of an emerging world culture, very much attributable to the influence of the United States as well, but conceptually based on a far greater degree of cross-talk and linguistic transculturation, which tends to mitigate both U.S. influence and British colonial influence.
The development of International English often centres on academic and scientific communities, where formal English usage is prevalent, and creative use of the language is at a minimum. This formal International English allows entry into Western culture as a whole and Western cultural values in general.
Opposition.
The continued growth of the English language itself is seen by authors such as Alistair Pennycook as a kind of cultural imperialism, whether it is English in one form or English in two slightly different forms.
Robert Phillipson argues against the possibility of such neutrality in his "Linguistic Imperialism" (1992). Learners who wish to use purportedly correct English are in fact faced with the dual standard of American English and British English, and other less known standard Englishes (including Australian, Scottish and Canadian).
Edward Trimnell, author of "Why You Need a Foreign Language & How to Learn One" (2005) argues that the international version of English is only adequate for communicating basic ideas. For complex discussions and business/technical situations, English is not an adequate communication tool for non-native speakers of the language. Trimnell also asserts that native English-speakers have become "dependent on the language skills of others" by placing their faith in international English.
Appropriation theory.
There are also some who reject both linguistic imperialism and David Crystal's theory of the neutrality of English. They argue that the phenomenon of the global spread of English is better understood in the framework of appropriation (e.g. Spichtinger 2000), that is, English used for local purposes around the world. Demonstrators in non-English speaking countries often use signs in English to convey their demands to TV-audiences around the globe, for instance.
In English-language teaching Bobda shows how Cameroon has moved away from a mono-cultural, Anglo-centered way of teaching English and has gradually appropriated teaching material to a Cameroonian context. Non-Western topics treated are, for instance, the rule of Emirs, traditional medicine or polygamy (1997:225). Kramsch and Sullivan (1996) describe how Western methodology and textbooks have been appropriated to suit local Vietnamese culture. The Pakistani textbook "Primary Stage English" includes lessons such as "Pakistan My Country", "Our Flag", or "Our Great Leader" (Malik 1993: 5,6,7) which might well sound jingoistic to Western ears. Within the native culture, however, establishing a connection between English Language Teaching (ELT), patriotism and Muslim faith is seen as one of the aims of ELT, as the chairman of the Punjab Textbook Board openly states: "The board ... takes care, through these books to inoculate in the students a love of the Islamic values and awareness to guard the ideological frontiers of your students home lands" (Punjab Text Book Board 1997).
Many Englishes.
There are many difficult choices that have to be made if there is to be further standardisation of English in the future. These include the choice over whether to adopt a current standard, or move towards a more neutral, but artificial one. A true International English might supplant both current American and British English as a variety of English for international communication, leaving these as local dialects, or would rise from a merger of General American and standard British English with admixture of other varieties of English and would generally replace all these varieties of English.
We may, in due course, all need to be in control of two standard Englishes—the one which gives us our national and local identity, and the other which puts us in touch with the rest of the human race. In effect, we may all need to become bilingual in our own language. — David Crystal (1988: p. 265)
This is the situation long faced by many users of English who possess a "non-standard" dialect of English as their birth tongue but have also learned to write (and perhaps also speak) a more standard dialect. Many academics often publish material in journals requiring different varieties of English and change style and spellings as necessary without great difficulty.
As far as spelling is concerned, the differences between American and British usage became noticeable due to the first influential lexicographers (dictionary writers) on each side of the Atlantic. Samuel Johnson's dictionary of 1755 greatly favoured Norman-influenced spellings such as "centre" and "colour"; on the other hand, Noah Webster's first guide to American spelling, published in 1783, preferred spellings like "center" and the Latinate "color". The difference in strategy and philosophy of Johnson and Webster are largely responsible for the main division in English spelling that exists today. However, these differences are extremely minor. Spelling is but a small part of the differences between dialects of English, and may not even reflect dialect differences at all (except in phonetically spelled dialogue). International English refers to much more than an agreed spelling pattern.
Dual standard.
Two approaches to International English are the individualistic and inclusive approach and the new dialect approach.
The individualistic approach gives control to individual authors to write and spell as they wish (within purported standard conventions) and to accept the validity of differences. The "Longman Grammar of Spoken and Written English", published in 1999, is a descriptive study of both American and British English in which each chapter follows individual spelling conventions according to the preference of the main editor of that chapter.
The new dialect approach appears in "The Cambridge Guide to English Usage" (Peters, 2004) which attempts to avoid any language bias and accordingly uses an idiosyncratic international spelling system of mixed American and British forms (but tending to prefer the American English spellings).

</doc>
<doc id="14997" url="https://en.wikipedia.org/wiki?curid=14997" title="International African Institute">
International African Institute

The International African Institute (IAI) was founded (as the International Institute of African Languages and Cultures ) in 1926 in London for the study of African languages. Frederick Lugard was the first chairman (1926 to his death in 1945); Diedrich Hermann Westermann (1926 to 1939) and Maurice Delafosse (1926) were the initial co-directors.
Since 1928, the IAI has published a quarterly journal, "Africa".
The IAI's mission is "to promote the education of the public in the study of Africa and its languages and cultures". Its operations includes seminars, journals, monographs, edited volumes and stimulating scholarship within Africa.
Archives.
The archives of the International African Institute are held at the Archives Division of the Library of the London School of Economics. An online catalogue of these papers is available.
History.
Africa alphabet.
In 1928, the IAI (then IIALC) published an "Africa Alphabet" to facilitate standardization of Latin-based writing systems for African languages.
Prize for African language literature, 1929-50.
From April 1929 to 1950, the IAI offered prizes for works of literature in African languages.

</doc>
<doc id="14998" url="https://en.wikipedia.org/wiki?curid=14998" title="IAI">
IAI

IAI is an acronym for:

</doc>
<doc id="15000" url="https://en.wikipedia.org/wiki?curid=15000" title="Insulin-like growth factor">
Insulin-like growth factor

The insulin-like growth factors (IGFs) are proteins with high sequence similarity to insulin. IGFs are part of a complex system that cells use to communicate with their physiologic environment. This complex system (often referred to as the IGF "axis") consists of two cell-surface receptors (IGF1R and IGF2R), two ligands (Insulin-like growth factor 1 (IGF-I) and Insulin-like growth factor 2 (IGF-2)), a family of six high-affinity IGF-binding proteins (IGFBP-1 to IGFBP-6), as well as associated IGFBP degrading enzymes, referred to collectively as proteases.
IGF1/GH Axis.
The IGF "axis" is also commonly referred to as the Growth Hormone/IGF-I Axis. Insulin-like growth factor 1 (IGF-1) is mainly secreted by the liver as a result of stimulation by growth hormone (GH). IGF-I is important for both the regulation of normal physiology, as well as a number of pathological states, including cancer. The IGF axis has been shown to play roles in the promotion of cell proliferation and the inhibition of cell death (apoptosis). Insulin-like growth factor 2 (IGF-2) is thought to be a primary growth factor required for early development while IGF-1 expression is required for achieving maximal growth. Gene knockout studies in mice have confirmed this, though other animals are likely to regulate the expression of these genes in distinct ways. While IGF-2 may be primarily fetal in action it is also essential for development and function of organs such as the brain, liver, and kidney.
Factors that are thought to cause variation in the levels of GH and IGF-1 in the circulation include an individual's genetic make-up, the time of day, age, sex, exercise status, stress levels, nutrition level, body mass index (BMI), disease state, race, estrogen status, and xenobiotic intake.
IGF-I has an involvement in regulating neural development including neurogenesis, myelination, synaptogenesis, and dendritic branching and neuroprotection after neuronal damage. Increased serum levels of IGF-I in children have been associated with higher IQ.
IGF-I shapes the development of the cochlea through controlling apoptosis. Its deficit can cause hearing loss. Serum level of it also underlies a correlation between short height and reduced hearing abilities particularly around 3–5 years of age, and at age 18 (late puberty).
IGF receptors.
The IGFs are known to bind the IGF-1 receptor, the insulin receptor, the IGF-2 receptor, the insulin-related receptor and possibly other receptors. The IGF-1 receptor is the "physiological" receptor—IGF-I binds to it at significantly higher affinity than it binds the insulin receptor. Like the insulin receptor, the IGF-I receptor is a receptor tyrosine kinase—meaning the receptor signals by causing the addition of a phosphate molecule on particular tyrosines. The IGF-2 receptor only binds IGF-2 and acts as a "clearance receptor"—it activates no intracellular signaling pathways, functioning only as an IGF-2 sequestering agent and preventing IGF-2 signaling.
Organs and tissues affected by IGF-I.
Since many distinct tissue types express the IGF-1 receptor, IGF-1's effects are diverse. It acts as a neurotrophic factor, inducing the survival of neurons. It may catalyse skeletal muscle hypertrophy, by inducing protein synthesis, and by blocking muscle atrophy. It is protective for cartilage cells, and is associated with activation of osteocytes, and thus may be an anabolic factor for bone. Since at high concentrations it is capable of activating the insulin receptor, it can also complement for the effects of insulin. Receptors for IGF-I are found in vascular smooth muscle, while typical receptors for insulin are not found in vascular smooth muscle.
IGF-Binding Proteins.
IGF-1 and IGF-2 are regulated by a family of proteins known as the IGF-Binding Proteins. These proteins help to modulate IGF action in complex ways that involve both inhibiting IGF action by preventing binding to the IGF-1 receptor as well as promoting IGF action possibly through aiding in delivery to the receptor and increasing IGF half-life. Currently, there are six characterized IGF Binding Proteins (IGFBP-1 to IGFBP-6). There is currently significant data suggesting that IGFBPs play important roles in addition to their ability to regulate IGFs. 
IGF-I and IGFBP-3 are GH dependent, whereas IGFBP-1 is insulin regulated.
IGFBP-1 production from the liver is significantly elevated during insulinopenia while serum levels of bioactive IGF-I is increased by insulin.
Diseases affected by IGF.
Studies of recent interest show that the Insulin/IGF axis play an important role in aging. Nematodes, fruit-flies, and other organisms have an increased life span when the gene equivalent to the mammalian insulin is knocked out. It is somewhat difficult to relate this finding to the mammals, however, because in the smaller organism there are many genes (at least 37 in the nematode "Caenorhabditis elegans") that are "insulin-like" or "IGF-1-like", whereas in the mammals insulin-like proteins comprise only seven members (insulin, IGFs, relaxins, EPIL, and relaxin-like factor). The human insulin-like genes have apparently distinct roles with some but less crosstalk presumably because there are multiple insulin-receptor-like proteins in humans. Simpler organisms typically have fewer receptors; for example, only one insulin-like receptor exists in the nematode "C. elegans". Additionally, "C. elegans" do not have specialized organs such as the (Islets of Langerhans), which sense insulin in response to glucose homeostasis. Moreover, IGF1 affects lifespan in nematodes by causing dauer formation, a developmental stage of C. elegans larva. There is no mammalian correlate. Therefore, it is an open question as to whether either IGF-1 or insulin in the mammal may perturb aging, although there is the suggestion that dietary restriction phenomena may be related.
Other studies are beginning to uncover the important role the IGFs play in diseases such as cancer and diabetes, showing for instance that IGF-1 stimulates growth of both prostate and breast cancer cells. Researchers are not in complete agreement about the degree of cancer risk that IGF-1 poses.

</doc>
<doc id="15001" url="https://en.wikipedia.org/wiki?curid=15001" title="IGF">
IGF

IGF may stand for:

</doc>
<doc id="15004" url="https://en.wikipedia.org/wiki?curid=15004" title="Idiot">
Idiot

An idiot, dolt, dullard or (archaically) mome is a person perceived to be lacking intelligence, or someone who acts in a self-defeating or significantly counterproductive way. Along with the similar terms moron, imbecile, and cretin, the word archaically referred to the intellectually disabled, but have all since gained specialized meanings in modern times. An idiot is said to be idiotic, and to suffer from idiocy. A dunce is an idiot who is specifically incapable of learning. An idiot differs from a fool (who is unwise) and an ignoramus (who is uneducated/an ignorant), neither of which refers to someone with low intelligence. In modern English usage, the terms "idiot" and "idiocy" describe an extreme folly or stupidity, and its symptoms (foolish or stupid utterance or deed). In psychology, it is a historical term for the state or condition now called profound intellectual disability.
Etymology.
Idiot is a word derived from the Greek , "idiōtēs" ("person lacking professional skill", "a private citizen", "individual"), from , "idios" ("private", "one's own"). In Latin the word "idiota" ("ordinary person, layman") preceded the Late Latin meaning "uneducated or ignorant person". Its modern meaning and form dates back to Middle English around the year 1300, from the Old French "idiote" ("uneducated or ignorant person"). The related word "idiocy" dates to 1487 and may have been analogously modeled on the words prophet and prophecy. The word has cognates in many other languages.
An idiot in Athenian democracy was someone who was characterized by "self-centeredness" and concerned almost exclusively with "private"—as opposed to "public"—affairs. Idiocy was the natural state of ignorance into which all persons were born and its opposite, citizenship, was effected through formalized education. In Athenian democracy, idiots were "born" and citizens were "made" through education (although citizenship was also largely hereditary). "Idiot" originally referred to "layman, person lacking professional skill", "person so mentally deficient as to be incapable of ordinary reasoning". Declining to take part in public life, such as democratic government of the polis (city state), was considered dishonorable. "Idiots" were seen as having bad judgment in public and political matters. Over time, the term "idiot" shifted away from its original connotation of selfishness and came to refer to individuals with overall bad judgment–individuals who are "stupid". According to the Bauer-Danker Lexicon, the noun ίδιωτής in ancient Greek meant "civilian" (ref Josephus Bell 2 178), "private citizen" (ref sb 3924 9 25), "private soldier as opposed to officer," (Polybius 1.69), "relatively unskilled, not clever," (Herodotus 2,81 and 7 199). The military connotation in Bauer's definition stems from the fact that ancient Greek armies in the time of total war mobilized all male citizens (to the age of 50) to fight, and many of these citizens tended to fight poorly and ignorantly.
Disability.
In 19th and early 20th century medicine and psychology, an "idiot" was a person with a very severe intellectual disability. In the early 1900s, Dr. Henry H. Goddard proposed a classification system for intellectual disability based on the Binet-Simon concept of mental age. Individuals with the lowest mental age level (less than three years) were identified as "idiots"; "imbeciles" had a mental age of three to seven years, and "morons" had a mental age of seven to ten years. The term "idiot" was used to refer to people having an IQ below 30. IQ, or intelligence quotient, was originally determined by dividing a person's mental age, as determined by standardized tests, by their actual age. The concept of mental age has fallen into disfavor, though, and IQ is now determined on the basis of statistical distributions.
In current American medical classification, these people are now said to have "profound intellectual disability" but this term is not in use in the United Kingdom.
United States law.
Until 2007, the California Penal Code Section 26 stated that "Idiots" were one of six types of people who are not capable of committing crimes. In 2007 the code was amended to read "persons who are mentally incapacitated." In 2008, Iowa voters passed a measure replacing "idiot, or insane person" in the State's constitution with "person adjudged mentally incompetent."
In several U.S. states, "idiots" do not have the right to vote:
The constitution of the state of Arkansas was amended in the general election of 2008 to, among other things, repeal a provision (Article 3, Section 5) which had until its repeal prohibited "idiots or insane persons" from voting.
"Idiots" are also barred from voting in British parliamentary elections.
In literature.
A few authors have used "idiot" characters in novels, plays and poetry. Often these characters are used to highlight or indicate something else (allegory). Examples of such usage are William Faulkner's "The Sound and the Fury" and William Wordsworth's "The Idiot Boy". Idiot characters in literature are often confused with or subsumed within mad or lunatic characters. The most common imbrication between these two categories of mental impairment occurs in the polemic surrounding Edmund from William Shakespeare's "King Lear". In Fyodor Dostoevsky's novel "The Idiot", the idiocy of the main character, Prince Lev Nikolaievich Myshkin, is attributed more to his honesty, trustfulness, kindness, and humility, than to a lack of intellectual ability. Nietzsche claimed, in his "The Antichrist", that Jesus was an idiot. This resulted from his description of Jesus as having an aversion toward the material world.

</doc>
<doc id="15012" url="https://en.wikipedia.org/wiki?curid=15012" title="Islamism">
Islamism

Islamism, also known as Political Islam ( "islām siyāsī"), is an Islamic revival movement often characterized by moral conservatism, literalism, and the attempt "to implement Islamic values in all spheres of life." Islamism favors the reordering of government and society in accordance with the Shari'a. The different Islamist movements have been described as "oscillating between two poles": at one end is a strategy of Islamization of society through state power seized by revolution or invasion; at the other "reformist" pole Islamists work to Islamize society gradually "from the bottom up". The movements have "arguably altered the Middle East more than any trend since the modern states gained independence", redefining "politics and even borders" according to one journalist (Robin Wright).
Islamists may emphasize the implementation of Sharia (Islamic law); of pan-Islamic political unity, including an Islamic state; and of the selective removal of non-Muslim, particularly Western military, economic, political, social, or cultural influences in the Muslim world that they believe to be incompatible with Islam.
Some observers (such as Graham Fuller) suggest Islamism's tenets are less strict, and can be defined as a form of identity politics or "support for identity, authenticity, broader regionalism, revivalism, [and revitalization of the community." Following the Arab Spring, political Islam became heavily involved with political democracy, but also spawned "the most aggressive and ambitious Islamist militia" to date, ISIS.
Islamists generally oppose the use of the term, claiming that their political beliefs and goals are simply an expression of Islamic religious belief. Similarly, some experts (Bernard Lewis) favor the term "activist Islam", and some (Robin Wright) have equated the term "militant Islam" with Islamism.
Central and prominent figures of modern Islamism include Hasan al-Banna, Sayyid Qutb, Abul Ala Maududi,
and Ruhollah Khomeini. Some of these proponents emphasise peaceful political processes, whereas Sayyid Qutb in particular called for violence, and those followers are generally considered Islamic extremists.
Definitions.
Islamism has been defined as:
Islamism takes different forms and spans a wide range of strategies and tactics towards the powers in place -- "destruction, opposition, collaboration, indifference" that have varied as "circumstances have changed" —and thus is not a united movement.
Moderate and reformist Islamists who accept and work within the democratic process include parties like the Tunisian Ennahda Movement. Jamaat-e-Islami of Pakistan is basically a socio-political and democratic Vanguard party but has also gained political influence through military coup d'état in past. The Islamist groups like Hezbollah in Lebanon and Hamas in Palestine participate in democratic and political process as well as armed attacks, seeking to abolish the state of Israel. Radical Islamist organizations like al-Qaeda and the Egyptian Islamic Jihad, and groups such as the Taliban, entirely reject democracy, often declaring as "kuffar" those Muslims who support it (see "takfirism"), as well as calling for violent/offensive jihad or urging and conducting attacks on a religious basis.
Another major division within Islamism is between what Graham E. Fuller has described as the fundamentalist "guardians of the tradition" (Salafis, such as those in the Wahhabi movement) and the "vanguard of change and Islamic reform" centered around the Muslim Brotherhood. Olivier Roy argues that "Sunni pan-Islamism underwent a remarkable shift in the second half of the 20th century" when the Muslim Brotherhood movement and its focus on Islamisation of pan-Arabism was eclipsed by the Salafi movement with its emphasis on "sharia rather than the building of Islamic institutions," and rejection of Shia Islam. Following the Arab Spring, Roy has described Islamism as "increasingly interdependent" with democracy in much of the Arab Muslim world, such that "neither can now survive without the other." While Islamist political culture itself may not be democratic, Islamists need democratic elections to maintain their legitimacy. At the same time, their popularity is such that no government can call itself democratic that excludes mainstream Islamist groups.
History of the term.
The term, which originally denoted the religion of Islam, first appeared in English as "Islamismus" in 1696, and as "Islamism" in 1712. By the turn of the twentieth century it had begun to be displaced by the shorter and purely Arabic term "Islam" and by 1938, when Orientalist scholars completed "The Encyclopaedia of Islam", seems to have virtually disappeared from English usage.
The term "Islamism" acquired its contemporary connotations in French academia in the late 1970s and early 1980s. From French, it began to migrate to the English language in the mid-1980s, and in recent years has largely displaced the term Islamic fundamentalism in academic circles.
The use of the term Islamism was at first "a marker for scholars more likely to sympathize" with new Islamic movements; however, as the term gained popularity it became more specifically associated with political groups such as the Taliban or the Algerian Armed Islamic Group, as well as with highly publicized acts of violence.
"Islamists" who have spoken out against the use of the term insisting they are merely "Muslims", include Ayatollah Mohammad Hussein Fadlallah, the spiritual mentor of Hizbullah, and Abbassi Madani, leader of the Algerian Islamic Salvation Front.
A 2003 article in "Middle East Quarterly" states:
In summation, the term Islamism enjoyed its first run, lasting from Voltaire to the First World War, as a synonym for Islam. Enlightened scholars and writers generally preferred it to Mohammedanism. Eventually both terms yielded to Islam, the Arabic name of the faith, and a word free of either pejorative or comparative associations. There was no need for any other term, until the rise of an ideological and political interpretation of Islam challenged scholars and commentators to come up with an alternative, to distinguish Islam as modern ideology from Islam as a faith... To all intents and purposes, Islamic fundamentalism and Islamism have become synonyms in contemporary American usage.
The Council on American–Islamic Relations complained in 2013 that the Associated Press's definition of "Islamist"—a "supporter of government in accord with the laws of Islam who view the Quran as a political model"—had become a pejorative shorthand for "Muslims we don't like."
The AP Stylebook entry for Islamist now reads as follows: "An advocate or supporter of a political movement that favors reordering government and society in accordance with laws prescribed by Islam. Do not use as a synonym for Islamic fighters, militants, extremists or radicals, who may or may not be Islamists. Where possible, be specific and use the name of militant affiliations: al-Qaida-linked, Hezbollah, Taliban, etc. Those who view the Quran as a political model encompass a wide range of Muslims, from mainstream politicians to militants known as jihadi."
Relation to Islam.
Islamism is a controversial concept not just because it posits a political role for Islam but also because its supporters believe their views merely reflect Islam, while the contrary idea that Islam is, or can be, apolitical is an error. Scholars and observers who do not believe that Islam is merely a political ideology include Fred Halliday, John Esposito and Muslim intellectuals like Javed Ahmad Ghamidi. Hayri Abaza argues the failure to distinguish between Islam and Islamism leads many in the West to support illiberal Islamic regimes, to the detriment of progressive moderates who seek to separate religion from politics.
Islamists have asked the question, "If Islam is a way of life, how can we say that those who want to live by its principles in legal, social, political, economic, and political spheres of life are not Muslims, but Islamists and believe in Islamism, not Islam?" Similarly, a writer for the International Crisis Group maintains that "the conception of 'political Islam'" is a creation of Americans to explain the Iranian Islamic Revolution and apolitical Islam was a historical fluke of the "short-lived era of the heyday of secular Arab nationalism between 1945 and 1970", and it is quietist/non-political Islam, not Islamism, that requires explanation.
On the other hand, Muslim-owned and run media (not just Western media) have used the terms "Islamist" and "Islamism" — as distinguished from Muslim and Islam — to distinguish groups such as the Islamic Salvation Front in Algeria or Jamaa Islamiya in Egypt, which actively seek to implement Islamic law, from mainstream Muslim groups.
Another source distinguishes Islamist from Islamic "by the fact that the latter refers to a religion and culture in existence over a millennium, whereas the first is a political/religious phenomenon linked to the great events of the 20th century". Islamists have, at least at times, defined themselves as "Islamiyyoun/Islamists" to differentiate themselves from "Muslimun/Muslims".
According to historian Bernard Lewis, Islamism, (or as he terms it "activist" Islam), along with "quietism," form two "particular ... political traditions" in Islam.
Daniel Pipes describes Islamism as a modern ideology that owes more to European utopian political ideologies and "isms" than to the traditional Islamic religion.
Influence.
Few observers contest the influence of Islamism in the Muslim world. Following the collapse of the Soviet Union, political movements based on the liberal ideology of free expression and democratic rule have led the opposition in other parts of the world such as Latin America, Eastern Europe and many parts of Asia; however "the simple fact is that political Islam currently reigns as the most powerful ideological force across the Muslim world today". 
Even some of those (such as Olivier Roy) who see Islamism as fraught with contradictions believe "the socioeconomic realities that sustained the Islamist wave are still here and are not going to change: poverty, uprootedness, crises in values and identities, the decay of the educational systems, the North-South opposition, and the problem of immigrant integration into the host societies".
The strength of Islamism draws from the strength of religiosity in general in the Muslim world. Compared to Western societies, "hat is striking about the Islamic world is that ... it seems to have been the least penetrated by irreligion".
Where other peoples may look to the physical or social sciences for answers in areas which their ancestors regarded as best left to scripture, in the Muslim world, religion has become more encompassing, not less, as "in the last few decades, it has been the fundamentalists who have increasingly represented the cutting edge" of Muslim culture.
In Egypt and the rest of the Muslim world "the word secular, a label proudly worn 30 years ago, is shunned" and "used to besmirch" political foes.
The small secular opposition parties "cannot compare" with Islamists in terms of "doggedness, courage," "risk-taking" or "organizational skills".
In the Middle East and Pakistan, religious discourse dominates societies, the airwaves, and thinking about the world. Radical mosques have proliferated throughout Egypt. Book stores are dominated by works with religious themes ... The demand for sharia, the belief that their governments are unfaithful to Islam and that Islam is the answer to all problems, and the certainty that the West has declared war on Islam; these are the themes that dominate public discussion. Islamists may not control parliaments or government palaces, but they have occupied the popular imagination.
Moderate strains of Islamism have been described as "competing in the democratic public square in places like Turkey, Tunisia, Malaysia and Indonesia. In Morocco, the Islamist Justice and Development Party (PJD) supported King Muhammad VI's "Mudawana", a "startlingly progressive family law" which grants women the right to a divorce, raises the minimum age for marriage to 18, and, in the event of separation, stipulates equal distribution of property.
Even before the Arab Spring, Islamists in Egypt and other Muslim countries had been described as "extremely influential. ... They determine how one dresses, what one eats. In these areas, they are incredibly successful. ... Even if the Islamists never come to power, they have transformed their countries." Democratic, peaceful and political Islamists are now dominating the spectrum of Islamist ideology as well as the political system of the Muslim world.
Sources of strength.
Amongst the various reasons for the global strength of Islamism are:
Western alienation.
Muslim alienation from Western ways, including its political ways.
Western patronage.
During the 1970s and sometimes later, Western and pro-Western governments often supported sometimes fledgling Islamists and Islamist groups that later came to be seen as dangerous enemies. Islamists were considered by Western governments bulwarks against—what were thought to be at the time—more dangerous leftist/communist/nationalist insurgents/opposition, which Islamists were correctly seen as opposing. The US spent billions of dollars to aid the mujahideen Muslim Afghanistan enemies of the Soviet Union, and non-Afghan veterans of the war returned home with their prestige, "experience, ideology, and weapons", and had considerable impact.
Although it is a strong opponent of Israel's existence, Hamas, officially created in 1987, traces back its origins to institutions and clerics supported by Israel in the 1970s and 1980s. Israel tolerated and supported Islamist movements in Gaza, with figures like Ahmed Yassin, as Israel perceived them preferable to the secular and then more powerful al-Fatah with the PLO.
Egyptian President Anwar Sadatwhose policies included opening Egypt to Western investment ("infitah"); transferring Egypt's allegiance from the Soviet Union to the United States; and making peace with Israelreleased Islamists from prison and welcomed home exiles in tacit exchange for political support in his struggle against leftists. His "encouraging of the emergence of the Islamist movement" was said to have been "imitated by many other Muslim leaders in the years that followed." This "gentlemen's agreement" between Sadat and Islamists broke down in 1975 but not before Islamists came to completely dominate university student unions. Sadat was later assassinated and a formidable insurgency was formed in Egypt in the 1990s. The French government has also been reported to have promoted Islamist preachers "in the hope of channeling Muslim energies into zones of piety and charity."
Resurgence of Islam.
The resurgence of Islamic devotion and the attraction to things Islamic can be traced to several events.
Saudi Arabian funding.
Starting in the mid-1970s the Islamic resurgence was funded by an abundance of money from Saudi Arabian oil exports. The tens of billions of dollars in "petro-Islam" largesse obtained from the recently heightened price of oil funded an estimated "90% of the expenses of the entire faith."
Throughout the Muslim world, religious institutions for people both young and old, from children's maddrassas to high-level scholarships received Saudi funding,
"books, scholarships, fellowships, and mosques" (for example, "more than 1500 mosques were built and paid for with money obtained from public Saudi funds over the last 50 years"), along with training in the Kingdom for the preachers and teachers who went on to teach and work at these universities, schools, mosques, etc.
The funding was also used to reward journalists and academics who followed the Saudis' strict interpretation of Islam; and satellite campuses were built around Egypt for Al Azhar, the world's oldest and most influential Islamic university.
The interpretation of Islam promoted by this funding was the strict, conservative Saudi-based Wahhabism or Salafism. In its harshest form it preached that Muslims should not only "always oppose" infidels "in every way," but "hate them for their religion ... for Allah's sake," that democracy "is responsible for all the horrible wars of the 20th century," that Shia and other non-Wahhabi Muslims were infidels, etc. While this effort has by no means converted all, or even most Muslims to the Wahhabist interpretation of Islam, it has done much to overwhelm more moderate local interpretations, and has set the Saudi-interpretation of Islam as the "gold standard" of religion in minds of some or many Muslims.
Grand Mosque seizure.
The strength of the Islamist movement was manifest in an event which might have seemed sure to turn Muslim public opinion against fundamentalism, but did just the opposite. In 1979 the Grand Mosque in Mecca Saudi Arabia was seized by an armed fundamentalist group and held for over a week. Scores were killed, including many pilgrim bystanders in a gross violation of one of the most holy sites in Islam (and one where arms and violence are strictly forbidden).
Instead of prompting a backlash against the movement from which the attackers originated, however, Saudi Arabia, already very conservative, responded by shoring up its fundamentalist credentials with even more Islamic restrictions. Crackdowns followed on everything from shopkeepers who did not close for prayer and newspapers that published pictures of women, to the selling of dolls, teddy bears (images of animate objects are considered haraam), and dog food (dogs are considered unclean).
In other Muslim countries, blame for and wrath against the seizure was directed not against fundamentalists, but against Islamic fundamentalism's foremost geopolitical enemy – the United States. Ayatollah Khomeini sparked attacks on American embassies when he announced:
It is not beyond guessing that this is the work of criminal American imperialism and international Zionism despite the fact that the object of the fundamentalists' revolt was the Kingdom of Saudi Arabia, America's major ally in the region. Anti-American demonstrations followed in the Philippines, Turkey, Bangladesh, India, the UAE, Pakistan, and Kuwait. The US Embassy in Libya was burned by protesters chanting pro-Khomeini slogans and the embassy in Islamabad, Pakistan was burned to the ground.
Charitable work.
Islamist movements such as the Muslim Brotherhood, "are well known for providing shelters, educational assistance, free or low cost medical clinics, housing assistance to students from out of town, student advisory groups, facilitation of inexpensive mass marriage ceremonies to avoid prohibitively costly dowry demands, legal assistance, sports facilities, and women's groups." All this compares very favourably against incompetent, inefficient, or neglectful governments whose commitment to social justice is limited to rhetoric.
Power of identity politics.
Islamism can also be described as part of identity politics, specifically the religiously-oriented nationalism that emerged in the Third World in the 1970s: "resurgent Hinduism in India, Religious Zionism in Israel, militant Buddhism in Sri Lanka, resurgent Sikh nationalism in the Punjab, 'Liberation Theology' of Catholicism in Latin America, and of course, Islamism in the Muslim world." These all challenged Westernized ruling elites on behalf of 'authenticity' and tradition.
Criticism.
Islamism, or elements of Islamism, have been criticized for: repression of free expression and individual rights, rigidity, hypocrisy, lack of true understanding of Islam, misinterpreting the Quran and Sunnah, antisemitism, and for innovations to Islam (bid‘ah), notwithstanding Islamists' proclaimed opposition to any such innovation.
History.
Extremism within Islam goes back to the 7th century to the Kharijites. From their essentially political position, they developed extreme doctrines that set them apart from both mainstream Sunni and Shiʿa Muslims. The Kharijites were particularly noted for adopting a radical approach to Takfir, whereby they declared other Muslims to be unbelievers and therefore deemed them worthy of death.
Predecessor movements.
Some Islamic revivalist movements and leaders pre-dating Islamism include:
Early history.
The end of the 19th century saw the dismemberment of most of the Muslim Ottoman Empire by non-Muslim European colonial powers. The empire spent massive sums on Western civilian and military technology to try to modernize and compete with the encroaching European powers, and in the process went deep into debt to these powers.
In this context, the publications of Jamal ad-din al-Afghani (1837–97), Muhammad Abduh (1849–1905) and Rashid Rida (1865–1935) preached Islamic alternatives to the political, economic, and cultural decline of the empire. Muhammad Abduh and Rashid Rida formed the beginning of the Islamist movement, as well as the reformist Islamist movement.
Their ideas included the creation of a truly Islamic society under sharia law, and the rejection of taqlid, the blind imitation of earlier authorities, which they believed deviated from the true messages of Islam. Unlike some later Islamists, Early Salafiyya strongly emphasized the restoration of the Caliphate.
Muhammad Iqbal.
Muhammad Iqbal was a philosopher, poet and politician in British India who is widely regarded as having inspired the Islamic Nationalism and Pakistan Movement in British India. Iqbal is admired as a prominent classical poet by Pakistani, Iranian, Indian and other international scholars of literature. Though Iqbal is best known as an eminent poet, he is also a highly acclaimed "Islamic philosophical thinker of modern times".
While studying law and philosophy in England and Germany, Iqbal became a member of the London branch of the All India Muslim League. He came back to Lahore in 1908. While dividing his time between law practice and philosophical poetry, Iqbal had remained active in the Muslim League. He did not support Indian involvement in World War I and remained in close touch with Muslim political leaders such as Muhammad Ali Johar and Muhammad Ali Jinnah. He was a critic of the mainstream Indian nationalist and secularist Indian National Congress. Iqbal's seven English lectures were published by Oxford University press in 1934 in a book titled The Reconstruction of Religious Thought in Islam. These lectures dwell on the role of Islam as a religion as well as a political and legal philosophy in the modern age.
Iqbal expressed fears that not only would secularism and secular nationalism weaken the spiritual foundations of Islam and Muslim society, but that India's Hindu-majority population would crowd out Muslim heritage, culture and political influence. In his travels to Egypt, Afghanistan, Palestine and Syria, he promoted ideas of greater Islamic political co-operation and unity, calling for the shedding of nationalist differences. Sir Muhammad Iqbal was elected president of the Muslim League in 1930 at its session in Allahabad as well as for the session in Lahore in 1932. In his Allahabad Address on 29 December 1930, Iqbal outlined a vision of an independent state for Muslim-majority provinces in northwestern India. This address later inspired the Pakistan movement.
The thoughts and vision of Iqbal later influenced many reformist Islamists, e.g., Muhammad Asad, Sayyid Abul Ala Maududi and Ali Shariati.
Sayyid Abul Ala Maududi.
Sayyid Abul Ala Maududi was an important early twentieth-century figure in the Islamic revival in India, and then after independence from Britain, in Pakistan. Trained as a lawyer he chose the profession of journalism, and wrote about contemporary issues and most importantly about Islam and Islamic law. Maududi founded the Jamaat-e-Islami party in 1941 and remained its leader until 1972. However, Maududi had much more impact through his writing than through his political organising. His extremely influential books (translated into many languages) placed Islam in a modern context, and influenced not only conservative ulema but liberal modernizer Islamists such as al-Faruqi, whose "Islamization of Knowledge" carried forward some of Maududi's key principles.
Maududi believed that Islam was all-encompassing: "Everything in the universe is 'Muslim' for it obeys God by submission to His laws... The man who denies God is called Kafir (concealer) because he conceals by his disbelief what is inherent in his nature and embalmed in his own soul."
Maududi also believed that Muslim society could not be Islamic without Sharia, and Islam required the establishment of an Islamic state. This state should be a "theo-democracy," based on the principles of: "tawhid" (unity of God), "risala" (prophethood) and "khilafa" (caliphate).
Although Maududi talked about Islamic revolution, by "revolution" he meant not the violence or populist policies of the Iranian Revolution, but the gradual changing the hearts and minds of individuals from the top of society downward through an educational process or "da'wah".
Muslim Brotherhood.
Roughly contemporaneous with Maududi was the founding of the Muslim Brotherhood in Ismailiyah, Egypt in 1928 by Hassan al Banna. His was arguably the first, largest and most influential modern Islamic political/religious organization. Under the motto "the Qur'an is our constitution,"
it sought Islamic revival through preaching and also by providing basic community services including schools, mosques, and workshops. Like Maududi, Al Banna believed in the necessity of government rule based on Shariah law implemented gradually and by persuasion, and of eliminating all imperialist influence in the Muslim world.
Some elements of the Brotherhood, though perhaps against orders, did engage in violence against the government, and its founder Al-Banna was assassinated in 1949 in retaliation for the assassination of Egypt's premier Mahmud Fami Naqrashi three months earlier. The Brotherhood has suffered periodic repression in Egypt and has been banned several times, in 1948 and several years later following confrontations with Egyptian president Gamal Abdul Nasser, who jailed thousands of members for several years.
Despite periodic repression, the Brotherhood has become one of the most influential movements in the Islamic world, particularly in the Arab world. For many years it was
described as "semi-legal" and was the only opposition group in Egypt able to field candidates during elections. In the Egyptian parliamentary election, 2011–2012, the political parties identified as "Islamist" (the Brotherhood's Freedom and Justice Party, Salafi Al-Nour Party and liberal Islamist Al-Wasat Party) won 75% of the total seats. Mohamed Morsi, an Islamist democrat of Muslim Brotherhood, was the first democratically elected president of Egypt. He was deposed during the 2013 Egyptian coup d'état.
Sayyid Qutb.
Maududi's political ideas influenced Sayyid Qutb, a leading member of the Muslim Brotherhood movement, and one of the key philosophers of Islamism and highly influential thinkers of Islamic universalism. Qutb believed things had reached such a state that the Muslim community had literally ceased to exist. It "has been extinct for a few centuries," having reverted to Godless ignorance (Jahiliyya).
To eliminate jahiliyya, Qutb argued Sharia, or Islamic law, must be established. Sharia law was not only accessible to humans and essential to the existence of Islam, but also all-encompassing, precluding "evil and corrupt" non-Islamic ideologies like communism, nationalism, or secular democracy.
Qutb preached that Muslims must engage in a two-pronged attack of converting individuals through preaching Islam peacefully and also waging what he called militant jihad so as to forcibly eliminate the "power structures" of Jahiliyya – not only from the Islamic homeland but from the face of the earth.
Qutb was both a member of the brotherhood and enormously influential in the Muslim world at large. Qutb is considered by some (Fawaz A. Gerges) to be "the founding father and leading theoretician" of modern jihadists, such as Osama bin Laden. However, the Muslim Brotherhood in Egypt and in Europe has not embraced his vision of undemocratic Islamic state and armed jihad, something for which they have been denounced by radical Islamists.
Six-Day War (1967).
The quick and decisive defeat of the Arab troops during the Six-Day War by Israeli troops constituted a pivotal event in the Arab Muslim world. The defeat along with economic stagnation in the defeated countries, was blamed on the secular Arab nationalism of the ruling regimes. A steep and steady decline in the popularity and credibility of secular, socialist and nationalist politics ensued. Ba'athism, Arab socialism, and Arab nationalism suffered, and different democratic and anti-democratic Islamist movements inspired by Maududi and Sayyid Qutb gained ground.
Islamic Republic in Iran.
The first modern "Islamist state" (with the possible exception of Zia's Pakistan) was established among the Shia of Iran. In a major shock to the rest of the world, Ayatollah Ruhollah Khomeini led the Iranian Revolution of 1979 to overthrow the oil-rich, well-armed, Westernized and pro-American secular monarchy ruled by Shah Muhammad Reza Pahlavi.
The views of Ali Shariati, ideologue of the Iranian Revolution, had resemblance with Mohammad Iqbal, ideological father of the State of Pakistan, but Khomeini's beliefs is perceived to be placed somewhere between beliefs of Sunni Islamic thinkers like Mawdudi and Qutb. He believed that complete imitation of the Prophet Mohammad and his successors such as Ali for restoration of Sharia law was essential to Islam, that many secular, Westernizing Muslims were actually agents of the West serving Western interests, and that the acts such as "plundering" of Muslim lands was part of a long-term conspiracy against Islam by the Western governments.
His views differed with Sunni scholars in:
The revolution was influenced by Marxism through Islamist thought and also by writings that sought either to counter Marxism (Muhammad Baqir al-Sadr's work) or to integrate socialism and Islamism (Ali Shariati's work). A strong wing of the revolutionary leadership was made up of leftists or "radical populists", such as Ali Akbar Mohtashami-Pur.
While initial enthusiasm for the Iranian revolution in the Muslim world was intense, it has waned as critics hold and campaign that "purges, executions, and atrocities tarnished its image".
The Islamic Republic has also maintained its hold on power in Iran in spite of US economic sanctions, and has created or assisted like-minded Shia terrorist groups in Iraq, Egypt, Syria, Jordan (SCIRI) and Lebanon (Hezbollah) (two Muslim countries that also have large Shiite populations).
During the 2006 Israel-Lebanon conflict, the Iranian government enjoyed something of a resurgence in popularity amongst the predominantly Sunni "Arab street," due to its support for Hezbollah and to President Mahmoud Ahmadinejad's vehement opposition to the United States and his call that Israel shall vanish.
Afghanistan.
In 1979, the Soviet Union deployed its 40th Army into Afghanistan, attempting to suppress an Islamic rebellion against an allied Marxist regime in the Afghan Civil War. The conflict, pitting indigenous impoverished Muslims (mujahideen) against an anti-religious superpower, galvanized thousands of Muslims around the world to send aid and sometimes to go themselves to fight for their faith. Leading this pan-Islamic effort was Palestinian sheikh Abdullah Yusuf Azzam. While the military effectiveness of these "Afghan Arabs" was marginal, an estimated 16,000 to 35,000 Muslim volunteers came from around the world came to fight in Afghanistan.
When the Soviet Union abandoned the Marxist Najibullah regime and withdrew from Afghanistan in 1989 (the regime finally fell in 1992), the victory was seen by many Muslims as the triumph of Islamic faith over superior military power and technology that could be duplicated elsewhere.
The jihadists gained legitimacy and prestige from their triumph both within the militant community and among ordinary Muslims, as well as the confidence to carry their jihad to other countries where they believed Muslims required assistance.|
The "veterans of the guerrilla campaign" returning home to Algeria, Egypt, and other countries "with their experience, ideology, and weapons," were often eager to continue armed jihad.
The collapse of the Soviet Union itself, in 1991, was seen by many Islamists, including Bin Laden, as the defeat of a superpower at the hands of Islam. Concerning the $6 billion in aid given by the US and Pakistan's military training and intelligence support to the mujahideen, bin Laden wrote: "he US has no mentionable role" in "the collapse of the Soviet Union ... rather the credit goes to God and the mujahidin" of Afghanistan.
Persian Gulf War.
Another factor in the early 1990s that worked to radicalize the Islamist movement was the Gulf War, which brought several hundred thousand US and allied non-Muslim military personnel to Saudi Arabian soil to put an end to Saddam Hussein's occupation of Kuwait. Prior to 1990 Saudi Arabia played an important role in restraining the many Islamist groups that received its aid. But when Saddam, secularist and Ba'athist dictator of neighboring Iraq, attacked Saudi Arabia (his enemy in the war), western troops came to protect the Saudi monarchy. Islamists accused the Saudi regime of being a puppet of the west.
These attacks resonated with conservative Muslims and the problem did not go away with Saddam's defeat either, since American troops remained stationed in the kingdom, and a de facto cooperation with the Palestinian-Israeli peace process developed. Saudi Arabia attempted to compensate for its loss of prestige among these groups by repressing those domestic Islamists who attacked it (bin Laden being a prime example), and increasing aid to Islamic groups (Islamist madrassas around the world and even aiding some violent Islamist groups) that did not, but its pre-war influence on behalf of moderation was greatly reduced. One result of this was a campaign of attacks on government officials and tourists in Egypt, a bloody civil war in Algeria and Osama bin Laden's terror attacks climaxing in the 9/11 attack.
Jihad movements of Egypt.
While Qutb's ideas became increasingly radical during his imprisonment prior to his execution in 1966, the leadership of the Brotherhood, led by Hasan al-Hudaybi, remained moderate and interested in political negotiation and activism. Fringe or splinter movements inspired by the final writings of Qutb in the mid-1960s (particularly the manifesto "Milestones", a.k.a. "Ma'alim fi-l-Tariq") did, however, develop and they pursued a more radical direction. By the 1970s, the Brotherhood had renounced violence as a means of achieving its goals.
The path of violence and military struggle was then taken up by the Egyptian Islamic Jihad organization responsible for the assassination of Anwar Sadat in 1981. Unlike earlier anti-colonial movements the extremist group directed its attacks against what it believed were "apostate" leaders of Muslim states, leaders who held secular leanings or who had introduced or promoted Western/foreign ideas and practices into Islamic societies. Its views were outlined in a pamphlet written by Muhammad Abd al-Salaam Farag, in which he states:
...there is no doubt that the first battlefield for jihad is the extermination of these infidel leaders and to replace them by a complete Islamic Order...
Another of the Egyptian groups which employed violence in their struggle for Islamic order was al-Gama'a al-Islamiyya (Islamic Group). Victims of their campaign against the Egyptian state in the 1990s included the head of the counter-terrorism police (Major General Raouf Khayrat), a parliamentary speaker (Rifaat al-Mahgoub), dozens of European tourists and Egyptian bystanders, and over 100 Egyptian police. Ultimately the campaign to overthrow the government was unsuccessful, and the major jihadi group, Jamaa Islamiya (or al-Gama'a al-Islamiyya), renounced violence in 2003. Other lesser known groups include the Islamic Liberation Party, Salvation from Hell and Takfir wal-Hijra, and these groups have variously been involved in activities such as attempted assassinations of political figures, arson of video shops and attempted takeovers of government buildings.
Hamas of Gaza.
Hamas is a Palestinian Sunni Islamist organization that governs the Gaza Strip where it has moved to establish sharia law in matters such as separation of the genders, using the lash for punishment, and Islamic dress code. 
Hamas also has a military resistance wing, the Izz ad-Din al-Qassam Brigades.
For some decades prior to the First Palestine Intifada in 1987, the Muslim Brotherhood in Palestine took a "quiescent" stance towards Israel, focusing on preaching, education and social services, and benefiting from Israel's "indulgence" to build up a network of mosques and charitable organizations. As the First Intifada gathered momentum and Palestinian shopkeepers closed their shops in support of the uprising, the Brotherhood announced the formation of HAMAS ("zeal"), devoted to Jihad against Israel. Rather than being more moderate than the PLO, the 1988 Hamas charter took a more uncompromising stand, calling for the destruction of Israel and the establishment of an Islamic state in Palestine. It was soon competing with and then overtaking the PLO for control of the intifada. The Brotherhood's base of devout middle class found common cause with the impoverished youth of the intifada in their cultural conservatism and antipathy for activities of the secular middle class such as drinking alcohol and going about without hijab.
Hamas has continued to be a major player in Palestine. From 2000 to 2007 it killed 542 people in 140 suicide bombing or "martyrdom operations". In the January 2006 legislative election—its first foray into the political process—it won the majority of the seats, and in 2007 it drove the PLO out of Gaza. Hamas has been praised by Muslims for driving Israel out of the Gaza Strip, but criticized for failure to achieve its demands in the 2008-9 and 2014 Gaza Wars despite heavy destruction and significant loss of life.
Sudan and Turabi.
For many years, Sudan had an Islamist regime under the leadership of Hassan al-Turabi. His National Islamic Front first gained influence when strongman General Gaafar al-Nimeiry invited members to serve in his government in 1979. Turabi built a powerful economic base with money from foreign Islamist banking systems, especially those linked with Saudi Arabia. He also recruited and built a cadre of influential loyalists by placing sympathetic students in the university and military academy while serving as minister of education.
After al-Nimeiry was overthrown in 1985 the party did poorly in national elections, but in 1989 it was able to overthrow the elected post-al-Nimeiry government with the help of the military. Turabi was noted for proclaiming his support for the democratic process and a liberal government before coming to power, but strict application of sharia law, torture and mass imprisonment of the opposition, and an intensification of the long-running war in southern Sudan, once in power. The NIF regime also harbored Osama bin Laden for a time (before 9/11), and worked to unify Islamist opposition to the American attack on Iraq in the 1991 Gulf War.
After Sudanese intelligence services were implicated in an assassination attempt on the President of Egypt, UN economic sanctions were imposed on Sudan, a poor country, and Turabi fell from favor. He was imprisoned for a time in 2004-5. Some of the NIF policies, such as the war with the non-Muslim south, have been reversed, though the National Islamic Front still holds considerable power in the government of Omar al-Bashir and National Congress Party, another Islamist party in country.
Algeria.
An Islamist movement influenced by Salafism and the jihad in Afghanistan, as well as the Muslim Brotherhood, was the FIS or Front Islamique de Salut (the Islamic Salvation Front) in Algeria. Founded as a broad Islamist coalition in 1989 it was led by Abbassi Madani, and a charismatic Islamist young preacher, Ali Belhadj. Taking advantage of economic failure and unpopular social liberalization and secularization by the ruling leftist-nationalist FLN government, it used its preaching to advocate the establishment of a legal system following Sharia law, economic liberalization and development program, education in Arabic rather than French, and gender segregation, with women staying home to alleviate the high rate of unemployment among young Algerian men. The FIS won sweeping victories in local elections and it was going to win national elections in 1991 when voting was canceled by a military coup d'état.
As Islamists took up arms to overthrow the government, the FIS's leaders were arrested and it became overshadowed by Islamist guerrilla groups, particularly the Islamic Salvation Army, MIA and Armed Islamic Group (or GIA). A bloody and devastating civil war ensued in which between 150,000 and 200,000 people were killed over the next decade.
The civil war was not a victory for Islamists. By 2002 the main guerrilla groups had either been destroyed or had surrendered. The popularity of Islamist parties has declined to the point that "the Islamist candidate, Abdallah Jaballah, came a distant third with 5% of the vote" in the 2004 presidential election.
Taliban in Afghanistan.
In Afghanistan, the mujahideen's victory against the Soviet Union in the 1980s did not lead to justice and prosperity, due to a vicious and destructive civil war between political and tribal warlords, making Afghanistan one of the poorest countries on earth. In 1992, the Democratic Republic of Afghanistan ruled by communist forces collapsed, and democratic Islamist elements of mujahdeen founded the Islamic State of Afghanistan. In 1996, a more conservative and anti-democratic Islamist movement known as the Taliban rose to power, defeated most of the warlords and took over roughly 80% of Afghanistan.
The Taliban were spawned by the thousands of madrasahs the Deobandi movement established for impoverished Afghan refugees and supported by governmental and religious groups in neighboring Pakistan. The Taliban differed from other Islamist movements to the point where they might be more properly described as Islamic fundamentalist or neofundamentalist, interested in spreading "an idealized and systematized version of conservative tribal village customs" under the label of Sharia to an entire country. Their ideology was also described as being influenced by Wahhabism, and the extremist jihadism of their guest Osama bin Laden.
The Taliban considered "politics" to be against Sharia and thus did not hold elections. They were led by Mullah Mohammed Omar who was given the title "Amir al-Mu'minin" or Commander of the Faithful, and a pledge of loyalty by several hundred Taliban-selected Pashtun clergy in April 1996. Taliban were overwhelmingly Pashtun and were accused of not sharing power with the approximately 60% of Afghans who belonged to other ethnic groups. (see: Taliban#Ideology)
The Taliban's hosting of Osama bin Laden led to an American-organized attack which drove them from power following the 9/11 attacks.
Taliban are still very much alive and fighting a vigorous insurgency with suicide bombings and armed attacks being launched against NATO and Afghan government targets.
Bangladesh.
Currently, the Bangladesh Nationalist Party is the second largest party in the Parliament of Bangladesh and the main opposition party, followed by Jamaat-e-Islami Bangladesh. The BNP promotes a center-right policy combining elements of conservatism, Islamism, nationalism and anti-communism. Since 2000, it has been allied with the Islamic parties Jamaat-e-Islami Bangladesh, and, Islami Oikya Jote.
Pakistan.
Early in the history of the state of Pakistan (12 March 1949), a parliamentary resolution (the Objectives Resolution) was adopted in accordance with the vision of founding fathers of Pakistan (Muhammad Iqbal, Muhammad Ali Jinnah, Liaquat Ali Khan). proclaiming:
This resolution later became a key source of inspiration for writers of the Constitution of Pakistan, and is included in the constitution as preamble.
In July 1977, General Zia-ul-Haq overthrew Prime Minister Zulfiqar Ali Bhutto's regime in Pakistan. Ali Bhutto, a leftist in democratic competition with Islamists, had announced banning alcohol and nightclubs within six months, shortly before he was overthrown. Zia-ul-Haq was much more committed to Islamism, and "Islamization" or implementation of Islamic law, became a cornerstone of his eleven-year military dictatorship and Islamism became his "official state ideology". Zia ul Haq was an admirer of Mawdudi and Mawdudi's party Jamaat-e-Islami became the "regime's ideological and political arm". In Pakistan this Islamization from above was "probably" more complete "than under any other regime except those in Iran and Sudan," but Zia-ul-Haq was also criticized by many Islamists for imposing "symbols" rather than substance, and using Islamization to legitimize his means of seizing power. Unlike neighboring Iran, Zia-ul-Haq's policies were intended to "avoid revolutionary excess", and not to strain relations with his American and Persian Gulf state allies. Zia-ul-Haq was killed in 1988 but Islamization remains an important element in Pakistani society.
Turkey.
Turkey had a number of Islamist parties, often changing names as they were banned by the constitutional court for anti-secular activities. Necmettin Erbakan (1926-2011) was the leader of several of the parties, the National Order Party ("Milli Nizam Partisi", 1970-1971), the National Salvation Party ("Milli Selamet Partisi", 1972-1981), and the Welfare Party ("Refah Partisi", 1983-1998); he also became a member of the Felicity Party ("Saadet Partisi", 2003-2011).
The Justice and Development Party (AKP), which has dominated Turkish politics from 2002 to 2015, is sometimes described as Islamist, but rejects such labelling.
Ismet Özel, a prominent Islamist intellectual, argued that Mustafa Kemal Atatürk's secular authoritarian policy, ironically, Islamicized the Turkish nation by forcing people to internalize and value their religious identity and not simply to take it for granted as in the past.
Islamic State of Iraq and the Levant.
"The Islamic State", formerly known as the "Islamic State of Iraq and the Levant" and before that as the "Islamic State of Iraq", (and called the acronym "Daesh" by its many detractors), is a Wahhabi/Salafi jihadist extremist militant group which is led by and mainly composed of Sunni Arabs from Iraq and Syria. In 2014, the group proclaimed itself a caliphate, with religious, political and military authority over all Muslims worldwide. 
, it had control over territory occupied by ten million people in Iraq and Syria, and has nominal control over small areas of Libya, Nigeria and Afghanistan. (While a self-described state, it lacks international recognition.) The group also operates or has affiliates in other parts of the world, including North Africa and South Asia.
Originating as the "Jama'at al-Tawhid wal-Jihad" in 1999, it pledged allegiance to al-Qaeda in 2004, participated in the Iraqi insurgency that followed the March 2003 invasion of Iraq by Western forces, joined the fight in the Syrian Civil War beginning in March 2011, and was expelled from al-Qaeda in early 2014, (which complained of its failure to consult and "notorious intransigence"). The group gained prominence after it drove Iraqi government forces out of key cities in western Iraq in a 2014 offensive. The group is adept at social media, posting Internet videos of beheadings of soldiers, civilians, journalists and aid workers, and is known for its destruction of cultural heritage sites.
The United Nations has held ISIL responsible for human rights abuses and war crimes, and Amnesty International has reported ethnic cleansing by the group on a "historic scale". The group has been designated a terrorist organisation by the United Nations, the European Union and member states, the United States, India, Indonesia, Turkey, Saudi Arabia, Syria and other countries.
Hizb ut-Tahrir.
Hizb ut-Tahrir is an influential international Islamist movement, founded in 1953 by an Islamic Qadi "(judge)" Taqiuddin al-Nabhani. HT is unique from most other Islamist movements in that the party focuses not on implementation of Sharia on local level or on providing social services, but on unifying the Muslim world under its vision of a new Islamic caliphate spanning from North Africa and the Middle East to much of central and South Asia.
To this end it has drawn up and published a 187-article constitution for its proposed caliphate state specifying specific policies such as sharia law, a "unitary ruling system" headed by a caliph elected by Muslims, an economy based on the gold standard, public ownership of utilities, public transport, and energy resources, and Arabic as the "sole language of the State."
In its focus on the Caliphate, the party takes a different view of Muslim history than some other Islamists such as Muhammad Qutb. HT sees Islam's pivotal turning point as occurring not with the death of Ali, or one of the other four rightly guided Caliphs in the 7th century, but with the abolition of the Ottoman Caliphate in 1924. This is believed to have ended the true Islamic system, something for which it blames "the disbelieving (Kafir) colonial powers" working through Turkish modernist Mustafa Kemal Atatürk.
HT does not engage in armed jihad or work for a democratic system, but works to take power through "ideological struggle" to change Muslim public opinion, and in particular through elites who will "facilitate" a "change of the government," i.e., launch a "bloodless" coup. It allegedly attempted and failed such coups in 1968 and 1969 in Jordan, and in 1974 in Egypt, and is now banned in both countries. But many HT members have gone on to join terrorist groups and many jihadi terrorists have cited HT as their key influence.
The party is sometimes described as "Leninist" and "rigidly controlled by its central leadership," with its estimated one million members required to spend "at least two years studying party literature under the guidance of mentors "(Murshid)"" before taking "the party oath." HT is particularly active in the ex-soviet republics of Central Asia and in Europe.
In the UK its rallies have drawn thousands of Muslims, and the party has been described by two observers (Robert S. Leiken and Steven Brooke) to have outpaced the Muslim Brotherhood in both membership and radicalism.
London.
Greater London has over 900,000 Muslims, (most of South Asian origins and concentrated in the East London boroughs of Newham, Tower Hamlets and Waltham Forest), and among them are some with a strong Islamist outlook. Their presence, combined with a perceived British policy of allowing them free rein, heightened by exposés such as the 2007 Channel 4 documentary programme "Undercover Mosque", has given rise to the term Londonistan. Following the 9/11 attacks, however, Abu Hamza al-Masri, the imam of the Finsbury Park Mosque, was arrested and charged with incitement to terrorism which has caused many Islamists to leave the UK to avoid internment.
Post-Arab Spring.
One observer (Quinn Mecham) notes four trends in Islamism rising from the Arab Spring of 2010-11: 
Counter-response.
The U.S. government has engaged in efforts to counter Islamism, or violent Islamism, since 2001. These efforts were centred in the U.S. around public diplomacy programmes conducted by the State Department. There have been calls to create an independent agency in the U.S. with a specific mission of undermining Islamism and jihadism. Christian Whiton, an official in the George W. Bush administration, called for a new agency focused on the nonviolent practice of "political warfare" aimed at undermining the ideology. U.S. Defense Secretary Robert Gates called for establishing something similar to the defunct U.S. Information Agency, which was charged with undermining the communist ideology during the Cold War.
Parties and organizations.
This is a list of parties and organizations which aim for the implementation of Sharia or an Islamic State, or subscribe to Muslim identity politics, or in some other way fulfil the definitions of political Islam, activist Islam, or Islamism laid out in this article; or have been widely described as such by others.

</doc>
<doc id="15014" url="https://en.wikipedia.org/wiki?curid=15014" title="Instructional theory">
Instructional theory

An instructional theory is "a theory that offers explicit guidance on how to better help people learn and develop." Instructional theories focus on how to structure material for promoting the education of human beings, particularly youth. Originating in the United States in the late 1970s, "instructional theory" is typically influenced by three general influences in educational thought: the behaviorist, the cognitive, and the constructivist schools of thought. Instructional theory is heavily influenced by the 1956 work of Benjamin Bloom, a University of Chicago professor, and the results of his Taxonomy of Education Objectives—one of the first modern codifications of the learning process. 
One of the first instructional theorists was Robert M. Gagne, who in 1965 published "Conditions of Learning" for the Florida State University's Department of Educational Research.
Definition.
Instructional theory is different than learning theory. A learning theory "describes" how learning takes place, and an instructional theory "prescribes" how to better help people learn. Learning theories often inform instructional theory, and three general theoretical stances take part in this influence: behaviorism (learning as response acquisition), cognitivism (learning as knowledge acquisition), and constructivism (learning as knowledge construction).
Instructional theory helps us create conditions that increases the probability of learning.
Overview.
Instructional theories identify what instruction or teaching should be like. It outlines strategies that an educator may adopt to achieve the learning objectives. Instructional theories are adapted based on the concept being taught and more importantly, as per the learning style of the students. They are used as an educational tool using which teachers and trainers can help a learner to learn. Instructional theories encompass different instructional models, instructional strategies and instructional methods.
Instructional theory should carry out 4 tasks – knowledge selection, knowledge sequence, interaction management and setting of interaction environment.
Instructions can be of several types, which are generally presented as dichotomies – 
Critiques.
Paulo Freire's work appears to critique instructional approaches that adhere to the knowledge acquisition stance, and his work "Pedagogy of the Oppressed" has had a broad influence over a generation of American educators with his critique of various "banking" models of education and analysis of the teacher-student relationship.
Freire explains, "Narration (with the teacher as narrator) leads the students to memorize mechanically the narrated content. Worse yet, it turns them into "containers", into "receptacles" to be "filled" by the teacher. The more completely she fills the receptacles, the better a teacher she is. The more meekly the receptacles permit themselves to be filled, the better students they are." In this way he explains educator creates an act of depositing knowledge in a student. The student thus becomes a repository of knowledge. Freire explains that this system that diminishes creativity and knowledge suffers. Knowledge, according to Freire, comes about only through the learner by inquiry and pursuing the subjects in the world and through interpersonal interaction.
Freire further states, "In the banking concept of education, knowledge is a gift bestowed by those who consider themselves knowledgeable upon those whom they consider to know nothing. Projecting an absolute ignorance onto others, a characteristic of the ideology of oppression, negates education and knowledge as processes of inquiry. The teacher presents himself to his students as their necessary opposite; by considering their ignorance absolute, he justifies his own existence. The students, alienated like the slave in the Hegelian dialectic, accept their ignorance as justifying the teacher's existence—but, unlike the slave, they never discover that they educate the teacher."
Freire then offered an alternative stance and wrote, "The raison d'etre of libertarian education, on the other hand, lies in its drive towards reconciliation. Education must begin with the solution of the teacher-student contradiction, by reconciling the poles of the contradiction so that both are simultaneously teachers and students."

</doc>
<doc id="15018" url="https://en.wikipedia.org/wiki?curid=15018" title="Infusoria">
Infusoria

Infusoria is a collective term for minute aquatic creatures such as ciliates, euglenoids, protozoa, unicellular algae and small invertebrates that exist in freshwater ponds. Some authors (e.g., Bütschli) used the term as a synonym for Ciliophora. In modern formal classifications, the term is considered obsolete; the microorganisms previously included in the Infusoria are mostly assigned to the kingdom Protista which itself is a polyphyletic assemblage of groups.
Aquarium use.
Infusoria are used by owners of aquariums to feed fish fry; newly hatched fry of many common aquarium species can be successfully raised on this food during early development due to its size and nutritional content. Many home aquaria are unable to naturally supply sufficient infusoria for fish-rearing, so hobbyists may create and maintain their own supply cultures or use one of the many commercial cultures available. Infusoria can be cultured by soaking any decomposing vegetative matter, such as papaya skin, in a jar of aged water. The culture will be grown in two to three days, depending on temperature and light received. The water will first turn cloudy, but it will clear up once the infusoria eat the bacteria which caused the cloudiness. At this point, the infusoria will be ready, and will usually be visible to the naked eye as small, white specks swimming in the container.

</doc>
<doc id="15019" url="https://en.wikipedia.org/wiki?curid=15019" title="ISO/IEC 8859-1">
ISO/IEC 8859-1

ISO/IEC 8859-1:1998, "Information technology — 8-bit single-byte coded graphic character sets — Part 1: Latin alphabet No. 1", is part of the ISO/IEC 8859 series of ASCII-based standard character encodings, first edition published in 1987. It is generally intended for Western European languages (see below for a list). It is the basis for most popular 8-bit character sets, including Windows-1252 and the first block of characters in Unicode.
ISO-8859-1 is the IANA preferred name for this standard when supplemented with the C0 and C1 control codes from ISO/IEC 6429 (see below for HTML5 exception). The following other aliases are registered for ISO-8859-1: iso-ir-100, csISOLatin1, latin1, l1, IBM819, CP819.
The Windows-1252 codepage coincides with ISO-8859-1 for all codes except the range 128 to 159 (hex 80 to 9F), where the little-used C1 controls are replaced with additional characters including all the missing characters provided by ISO-8859-15. Code page 28591 a.k.a. Windows-28591 is the actual ISO-8859-1 codepage.
Coverage.
ISO 8859-1 encodes what it refers to as "Latin alphabet no. 1," consisting of 191 characters from the Latin script. This character-encoding scheme is used throughout the Americas, Western Europe, Oceania, and much of Africa. It is also commonly used in most standard romanizations of East-Asian languages. In April 2016, 6.5% of all web sites use ISO 8859-1, that by HTML5 standards should be the same encoding as Windows-1252, and given that it's the case, then its 0.9% share can be added to it giving 7.4% of web sites effectively using "ISO 8859-1" in the form of Windows-1252.
Each character is encoded as a single eight-bit code value. These code values can be used in almost any data interchange system to communicate in the following European languages (with a few exceptions due to missing characters, as noted):
Quotation marks.
For some languages listed above, the correct typographical quotation marks are missing, as only « », " ", and ' ' are included. Also, this scheme does not provide for oriented (6- or 9-shaped) single or double quotation marks. Some fonts will display the spacing grave accent (0x60) and the apostrophe (0x27) as a matching pair of oriented single quotation marks, but this is not considered part of the modern standard.
History.
ISO 8859-1 was based on the Multinational Character Set used by Digital Equipment Corporation in the popular VT220 terminal. It was developed within ECMA, the
European Computer Manufacturers Association, and published in March 1985 as ECMA-94, by which name it is still sometimes known. The second edition of ECMA-94 (June 1986) also included ISO 8859-2, ISO 8859-3, and ISO 8859-4 as part of the specification.
In 1985, Commodore adopted ISO 8859-1 for its new AmigaOS operating system. The Seikosha MP-1300AI impact dot-matrix printer, used with the Amiga 1000, included this encoding. 
In 1992, the IANA registered the character map ISO_8859-1:1987, more commonly known by its preferred MIME name of ISO-8859-1 (note the extra hyphen over ISO 8859-1), a superset of ISO 8859-1, for use on the Internet. This map assigns the C0 and C1 control characters to the unassigned code values thus provides for 256 characters via every possible 8-bit value.
ISO-8859-1 is (according to the standards at least) the default encoding of documents delivered via HTTP with a MIME type beginning with "text/" (however the HTML5 specification requires that documents advertised as ISO-8859-1 actually be parsed with the Windows-1252 encoding.) It is the default encoding of the values of certain descriptive HTTP headers, and defines the repertoire of characters allowed in HTML 3.2 documents (HTML 4.0, however, is based on Unicode). It and Windows-1252 are often assumed to be the encoding of text on Unix and Microsoft Windows in the absence of locale or other information, this is only gradually being replaced with Unicode encoding such as UTF-8 or UTF-16.
Similar character sets.
ISO-8859-1 was incorporated as the first 256 code points of ISO/IEC 10646 and Unicode.
The lower range 32 to 126 (hex 20 to 7E, the G0 subset) maps exactly to the same coded G0 subset of the ISO 646 US variant (commonly known as ASCII), whose ISO 2022 standard switch sequence is "ESC ( B". The higher range 160 to 255 (hex A0 to FF, the G1 subset) maps exactly to the same subset initiated by the ISO 2022 standard switch sequence "ESC . A".
ISO/IEC 8859-1 is missing some characters for French and Finnish text and the euro sign. In order to provide some of these characters, ISO/IEC 8859-15 was developed as an update of ISO/IEC 8859-1. This required, however, the removal of some infrequently used characters from ISO/IEC 8859-1, including fraction symbols and letter-free diacritics: ¤, ¦, ¨, ´, ¸, ¼, ½, and ¾.
The popular Windows-1252 character set adds all the missing characters provided by ISO/IEC 8859-15, plus a number of typographic symbols, by replacing the rarely used C1 controls in the range 128 to 159 (hex 80 to 9F). It is very common to mislabel text data with the charset label ISO-8859-1, even though the data is really Windows-1252 encoded. Many web browsers and e-mail clients will interpret ISO-8859-1 control codes as Windows-1252 characters, and that behavior was later standardized in HTML5, in order to accommodate such mislabeling and care should be taken to avoid generating these characters in ISO-8859-1 labeled content.
The Apple Macintosh computer introduced a character encoding called Mac Roman, or Mac-Roman, in 1984. It was meant to be suitable for Western European desktop publishing. It is a superset of ASCII, like ISO-8859-1, and has most of the characters that are in ISO-8859-1 but in a totally different arrangement. A later version, registered with IANA as "Macintosh", replaced the generic currency sign ¤ with the euro sign €. The few printable characters that are in ISO 8859-1 but not in this set are often a source of trouble when editing text on websites using older Macintosh browsers (including the last version of Internet Explorer for Mac). However the extra characters that Windows-1252 has in the C1 codepoint range are all supported in MacRoman.
DOS had code page 850, which had all printable characters that ISO-8859-1 had (albeit in a totally different arrangement) plus the most widely used graphic characters from code page 437.

</doc>
<doc id="15020" url="https://en.wikipedia.org/wiki?curid=15020" title="ISO/IEC 8859">
ISO/IEC 8859

ISO/IEC 8859 is a joint ISO and IEC series of standards for 8-bit character encodings. The series of standards consists of numbered parts, such as ISO/IEC 8859-1, ISO/IEC 8859-2, etc. There are 15 parts, excluding the abandoned ISO/IEC 8859-12. The ISO working group maintaining this series of standards has been disbanded.
ISO/IEC 8859 parts 1, 2, 3, and 4 were originally Ecma International standard ECMA-94.
Introduction.
While the bit patterns of the 95 printable ASCII characters are sufficient to exchange information in modern English, most other languages that use Latin alphabets need additional symbols not covered by ASCII. ISO/IEC 8859 sought to remedy this problem by utilizing the eighth bit in an 8-bit byte to allow positions for another 96 printable characters. Early encodings were limited to 7 bits because of restrictions of some data transmission protocols, and partially for historical reasons. However, more characters were needed than could fit in a single 8-bit character encoding, so several mappings were developed, including at least ten suitable for various Latin alphabets.
The ISO/IEC 8859-"n" encodings only contain printable characters, and were designed to be used in conjunction with control characters mapped to the unassigned bytes. To this end a series of encodings registered with the IANA add the C0 control set (control characters mapped to bytes 0 to 31) from ISO 646 and the C1 control set (control characters mapped to bytes 128 to 159) from ISO 6429, resulting in full 8-bit character maps with most, if not all, bytes assigned. These sets have ISO-8859-"n" as their preferred MIME name or, in cases where a preferred MIME name isn't specified, their canonical name. Many people use the terms ISO/IEC 8859-"n" and ISO-8859-"n" interchangeably. ISO/IEC 8859-11 did not get such a charset assigned, presumably because it was almost identical to TIS 620.
Characters.
The ISO/IEC 8859 standard is designed for reliable information exchange, not typography; the standard omits symbols needed for high-quality typography, such as optional ligatures, curly quotation marks, dashes, etc. As a result, high-quality typesetting systems often use proprietary or idiosyncratic extensions on top of the ASCII and ISO/IEC 8859 standards, or use Unicode instead.
As a rule of thumb, if a character or symbol was not already part of a widely used data-processing character set and was also not usually provided on typewriter keyboards for a national language, it didn't get in. Hence the directional double quotation marks "«" and "»" used for some European languages were included, but not the directional double quotation marks "“" and "”" used for English and some other languages. French didn't get its "œ" and "Œ" ligatures because they could be typed as 'oe'. Ÿ, needed for all-caps text, was left out as well. These characters were, however, included later with ISO/IEC 8859-15, which also introduced the new euro sign character €. Likewise Dutch did not get the 'ĳ' and 'Ĳ' letters, because Dutch speakers had become used to typing these as two letters instead. Romanian did not initially get its ‹Ș›/‹ș› and ‹Ț›/‹ț› (with comma) letters, because these letters were initially unified with ‹Ş›/‹ş› and ‹Ţ›/‹ţ› (with cedilla) by the Unicode Consortium, considering the shapes with comma beneath to be glyph variants of the shapes with cedilla. However, the letters with explicit comma below were later added to the Unicode standard and are also in ISO/IEC 8859-16.
Most of the ISO/IEC 8859 encodings provide diacritic marks required for various European languages using the Latin script. Others provide non-Latin alphabets: Greek, Cyrillic, Hebrew, Arabic and Thai. Most of the encodings contain only spacing characters although the Thai, Hebrew, and Arabic ones do also contain combining characters. However, the standard makes no provision for the scripts of East Asian languages ("CJK"), as their ideographic writing systems require many thousands of code points. Although it uses Latin based characters, Vietnamese does not fit into 96 positions (without using combining diacritics) either. Each Japanese syllabic alphabet (hiragana or katakana, see Kana) would fit, but like several other alphabets of the world they aren't encoded in the ISO/IEC 8859 system.
The Parts of ISO/IEC 8859.
ISO/IEC 8859 is divided into the following parts:
Each part of ISO 8859 is designed to support languages that often borrow from each other, so the characters needed by each language are usually accommodated by a single part. However, there are some characters and language combinations that are not accommodated without transcriptions. Efforts were made to make conversions as smooth as possible. For example, German has all of its seven special characters at the same positions in all Latin variants (1–4, 9, 10, 13–16), and in many positions the characters only differ in the diacritics between the sets. In particular, variants 1–4 were designed jointly, and have the property that every encoded character appears either at a given position or not at all.
Table.
At position 0xA0 there's always the non breaking space and 0xAD is mostly the soft hyphen, which only shows at line breaks. Other empty fields are either unassigned or the system used isn't able to display them.
There are new additions as ISO/IEC 8859-7:2003 and ISO/IEC 8859-8:1999 versions. LRM stands for left-to-right mark (U+200E) and RLM stands for right-to-left mark (U+200F).
Relationship to Unicode and the UCS.
Since 1991, the Unicode Consortium has been working with ISO and IEC to develop the Unicode Standard and ISO/IEC 10646: the Universal Character Set (UCS) in tandem. Newer editions of ISO/IEC 8859 express characters in terms of their Unicode/UCS names and the "U+nnnn" notation, effectively causing each part of ISO/IEC 8859 to be a Unicode/UCS character encoding scheme that maps a very small subset of the UCS to single 8-bit bytes. The first 256 characters in Unicode and the UCS are identical to those in ISO/IEC-8859-1 (Latin-1).
Single-byte character sets including the parts of ISO/IEC 8859 and derivatives of them were favoured throughout the 1990s, having the advantages of being well-established and more easily implemented in software: the equation of one byte to one character is simple and adequate for most single-language applications, and there are no combining characters or variant forms. As Unicode-enabled operating systems became more widespread, ISO/IEC 8859 and other legacy encodings became less popular. While remnants of ISO 8859 and single-byte character models remain entrenched in many operating systems, programming languages, data storage systems, networking applications, display hardware, and end-user application software, most modern computing applications use Unicode internally, and rely on conversion tables to map to and from other encodings, when necessary.
Development status.
The ISO/IEC 8859 standard was maintained by ISO/IEC Joint Technical Committee 1, Subcommittee 2, Working Group 3 (ISO/IEC JTC 1/SC 2/WG 3). In June 2004, WG 3 disbanded, and maintenance duties were transferred to SC 2. The standard is not currently being updated, as the Subcommittee's only remaining working group, WG 2, is concentrating on development of Unicode's Universal Coded Character Set.

</doc>
<doc id="15022" url="https://en.wikipedia.org/wiki?curid=15022" title="Infrared">
Infrared

Infrared (IR) is invisible radiant energy, electromagnetic radiation with longer wavelengths than those of visible light, extending from the nominal red edge of the visible spectrum at 700 nanometers (frequency 430 THz) to 1 mm (300 GHz) (although people can see infrared up to at least 1050 nm in experiments). Most of the thermal radiation emitted by objects near room temperature is infrared.
Infrared radiation was discovered in 1800 by astronomer Sir William Herschel, who discovered a type of invisible radiation in the spectrum lower in energy than red light, by means of its effect upon a thermometer. Slightly more than half of the total energy from the Sun was eventually found to arrive on Earth in the form of infrared. The balance between absorbed and emitted infrared radiation has a critical effect on Earth's climate.
Infrared energy is emitted or absorbed by molecules when they change their rotational-vibrational movements. Infrared energy excites vibrational modes in a molecule through a change in the dipole moment, making it a useful frequency range for study of these energy states for molecules of the proper symmetry. Infrared spectroscopy examines absorption and transmission of photons in the infrared energy range.
Infrared radiation is used in industrial, scientific, and medical applications. Night-vision devices using active near-infrared illumination allow people or animals to be observed without the observer being detected. Infrared astronomy uses sensor-equipped telescopes to penetrate dusty regions of space, such as molecular clouds; detect objects such as planets, and to view highly red-shifted objects from the early days of the universe. Infrared thermal-imaging cameras are used to detect heat loss in insulated systems, to observe changing blood flow in the skin, and to detect overheating of electrical apparatus.
Thermal-infrared imaging is used extensively for military and civilian purposes. Military applications include target acquisition, surveillance, night vision, homing and tracking. Humans at normal body temperature radiate chiefly at wavelengths around 10 μm (micrometers). Non-military uses include thermal efficiency analysis, environmental monitoring, industrial facility inspections, remote temperature sensing, short-ranged wireless communication, spectroscopy, and weather forecasting.
Definition and relationship to the electromagnetic spectrum.
Infrared radiation extends from the nominal red edge of the visible spectrum at 700 nanometers (nm) to 1 mm. This range of wavelengths corresponds to a frequency range of approximately 430 THz down to 300 GHz. Below infrared is the microwave portion of the electromagnetic spectrum.
Natural infrared.
Sunlight, at an effective temperature of 5,780 kelvins, is composed of nearly thermal-spectrum radiation that is slightly more than half infrared. At zenith, sunlight provides an irradiance of just over 1 kilowatt per square meter at sea level. Of this energy, 527 watts is infrared radiation, 445 watts is visible light, and 32 watts is ultraviolet radiation. Nearly all the infrared radiation in sunlight is near infrared, shorter than 4 micrometers.
On the surface of Earth, at far lower temperatures than the surface of the Sun, almost all thermal radiation consists of infrared in mid-infrared region, much longer than in sunlight. Of these natural thermal radiation processes only lightning and natural fires are hot enough to produce much visible energy, and fires produce far more infrared than visible-light energy.
Regions within the infrared.
In general, objects emit infrared radiation across a spectrum of wavelengths, but sometimes only a limited region of the spectrum is of interest because sensors usually collect radiation only within a specific bandwidth. Thermal infrared radiation also has a maximum emission wavelength, which is inversely proportional to the absolute temperature of object, in accordance with Wien's displacement law.
Therefore, the infrared band is often subdivided into smaller sections.
Commonly used sub-division scheme.
A commonly used sub-division scheme is:
NIR and SWIR is sometimes called "reflected infrared", whereas MWIR and LWIR is sometimes referred to as "thermal infrared". Due to the nature of the blackbody radiation curves, typical "hot" objects, such as exhaust pipes, often appear brighter in the MW compared to the same object viewed in the LW.
CIE division scheme.
The International Commission on Illumination (CIE) recommended the division of infrared radiation into the following three bands:
ISO 20473 scheme.
ISO 20473 specifies the following scheme:
Astronomy division scheme.
Astronomers typically divide the infrared spectrum as follows:
These divisions are not precise and can vary depending on the publication. The three regions are used for observation of different temperature ranges, and hence different environments in space.
A photometric system used in astronomy allocates letters to different spectral regions according to filters used; JHK is a frequent set for the near-infrared, and this set of letters appears in the titles of many papers.
Sensor response division scheme.
A third scheme divides up the band based on the response of various detectors:
Near-infrared is the region closest in wavelength to the radiation detectable by the human eye, mid- and far-infrared are progressively further from the visible spectrum. Other definitions follow different physical mechanisms (emission peaks, vs. bands, water absorption) and the newest follow technical reasons (the common silicon detectors are sensitive to about 1,050 nm, while InGaAs's sensitivity starts around 950 nm and ends between 1,700 and 2,600 nm, depending on the specific configuration). Unfortunately, international standards for these specifications are not currently available.
The onset of infrared is defined (according to different standards) at various values typically between 700 nm and 800 nm, but the boundary between visible and infrared light is not precisely defined. The human eye is markedly less sensitive to light above 700 nm wavelength, so longer wavelengths make insignificant contributions to scenes illuminated by common light sources. However, particularly intense near-IR light (e.g., from IR lasers, IR LED sources, or from bright daylight with the visible light removed by colored gels) can be detected up to approximately 780 nm, and will be perceived as red light. Sources providing wavelengths as long as 1050 nm can be seen as a dull red glow in intense sources, causing some difficulty in near-IR illumination of scenes in the dark (usually this practical problem is solved by indirect illumination). Leaves are particularly bright in the near IR, and if all visible light leaks from around an IR-filter are blocked, and the eye is given a moment to adjust to the extremely dim image coming through a visually opaque IR-passing photographic filter, it is possible to see the Wood effect that consists of IR-glowing foliage.
Telecommunication bands in the infrared.
In optical communications, the part of the infrared spectrum that is used is divided into seven bands based on availability of light sources transmitting/absorbing materials (fibers) and detectors:
The C-band is the dominant band for long-distance telecommunication networks. The S and L bands are based on less well established technology, and are not as widely deployed.
Heat.
Infrared radiation is popularly known as "heat radiation", but light and electromagnetic waves of any frequency will heat surfaces that absorb them. Infrared light from the Sun accounts for 49% of the heating of Earth, with the rest being caused by visible light that is absorbed then re-radiated at longer wavelengths. Visible light or ultraviolet-emitting lasers can char paper and incandescently hot objects emit visible radiation. Objects at room temperature will emit radiation concentrated mostly in the 8 to 25 µm band, but this is not distinct from the emission of visible light by incandescent objects and ultraviolet by even hotter objects (see black body and Wien's displacement law).
Heat is energy in transit that flows due to temperature difference. Unlike heat transmitted by thermal conduction or thermal convection, thermal radiation can propagate through a vacuum. Thermal radiation is characterized by a particular spectrum of many wavelengths that is associated with emission from an object, due to the vibration of its molecules at a given temperature. Thermal radiation can be emitted from objects at any wavelength, and at very high temperatures such radiations are associated with spectra far above the infrared, extending into visible, ultraviolet, and even X-ray regions (i.e., the solar corona). Thus, the popular association of infrared radiation with thermal radiation is only a coincidence based on typical (comparatively low) temperatures often found near the surface of planet Earth.
The concept of emissivity is important in understanding the infrared emissions of objects. This is a property of a surface that describes how its thermal emissions deviate from the ideal of a black body. To further explain, two objects at the same physical temperature will not show the same infrared image if they have differing emissivity. For example, for any pre-set emissivity value, objects with higher emissivity will appear hotter, and those with a lower emissivity will appear cooler. For that reason, incorrect selection of emissivity will give inaccurate results when using infrared cameras and pyrometers.
Applications.
Night vision.
The use of infrared light and night vision devices should not be confused with thermal imaging, which creates images based on differences in surface temperature by detecting infrared radiation (heat) that emanates from objects and their surrounding environment.
Thermography.
Infrared radiation can be used to remotely determine the temperature of objects (if the emissivity is known). This is termed thermography, or in the case of very hot objects in the NIR or visible it is termed pyrometry. Thermography (thermal imaging) is mainly used in military and industrial applications but the technology is reaching the public market in the form of infrared cameras on cars due to the massively reduced production costs.
Thermographic cameras detect radiation in the infrared range of the electromagnetic spectrum (roughly 900–14,000 nanometers or 0.9–14 μm) and produce images of that radiation. Since infrared radiation is emitted by all objects based on their temperatures, according to the black body radiation law, thermography makes it possible to "see" one's environment with or without visible illumination. The amount of radiation emitted by an object increases with temperature, therefore thermography allows one to see variations in temperature (hence the name).
Hyperspectral imaging.
A hyperspectral image, a basis for chemical imaging, is a "picture" containing continuous spectrum through a wide spectral range. Hyperspectral imaging is gaining importance in the applied spectroscopy particularly in the fields of NIR, SWIR, MWIR, and LWIR spectral regions. Typical applications include biological, mineralogical, defence, and industrial measurements.
Thermal Infrared Hyperspectral Camera can be applied similarly to a Thermographic camera, with the fundamental difference that each pixel contains a full LWIR spectrum. Consequently, chemical identification of the object can be performed without a need for an external light source such as the Sun or the Moon. Such cameras are typically applied for geological measurements, outdoor surveillance and UAV applications.
Other imaging.
In infrared photography, infrared filters are used to capture the near-infrared spectrum. Digital cameras often use infrared blockers. Cheaper digital cameras and camera phones have less effective filters and can "see" intense near-infrared, appearing as a bright purple-white color. This is especially pronounced when taking pictures of subjects near IR-bright areas (such as near a lamp), where the resulting infrared interference can wash out the image. There is also a technique called 'T-ray' imaging, which is imaging using far-infrared or terahertz radiation. Lack of bright sources can make terahertz photography more challenging than most other infrared imaging techniques. Recently T-ray imaging has been of considerable interest due to a number of new developments such as terahertz time-domain spectroscopy.
Tracking.
Infrared tracking, also known as infrared homing, refers to a passive missile guidance system, which uses the emission from a target of electromagnetic radiation in the infrared part of the spectrum to track it. Missiles that use infrared seeking are often referred to as "heat-seekers", since infrared (IR) is just below the visible spectrum of light in frequency and is radiated strongly by hot bodies. Many objects such as people, vehicle engines, and aircraft generate and retain heat, and as such, are especially visible in the infrared wavelengths of light compared to objects in the background.
Heating.
Infrared radiation can be used as a deliberate heating source. For example, it is used in infrared saunas to heat the occupants. It may also be used in other heating applications, such as to remove ice from the wings of aircraft (de-icing). Infrared can be used in cooking and heating food as it predominantly heats the opaque, absorbent objects, rather than the air around them.
Infrared heating is also becoming more popular in industrial manufacturing processes, e.g. curing of coatings, forming of plastics, annealing, plastic welding, and print drying. In these applications, infrared heaters replace convection ovens and contact heating.
Efficiency is achieved by matching the wavelength of the infrared heater to the absorption characteristics of the material.
Communications.
IR data transmission is also employed in short-range communication among computer peripherals and personal digital assistants. These devices usually conform to standards published by IrDA, the Infrared Data Association. Remote controls and IrDA devices use infrared light-emitting diodes (LEDs) to emit infrared radiation that is focused by a plastic lens into a narrow beam. The beam is modulated, i.e. switched on and off, to encode the data. The receiver uses a silicon photodiode to convert the infrared radiation to an electric current. It responds only to the rapidly pulsing signal created by the transmitter, and filters out slowly changing infrared radiation from ambient light. Infrared communications are useful for indoor use in areas of high population density. IR does not penetrate walls and so does not interfere with other devices in adjoining rooms. Infrared is the most common way for remote controls to command appliances.
Infrared remote control protocols like RC-5, SIRC, are used to communicate with infrared.
Free space optical communication using infrared lasers can be a relatively inexpensive way to install a communications link in an urban area operating at up to 4 gigabit/s, compared to the cost of burying fiber optic cable.
Infrared lasers are used to provide the light for optical fiber communications systems. Infrared light with a wavelength around 1,330 nm (least dispersion) or 1,550 nm (best transmission) are the best choices for standard silica fibers.
IR data transmission of encoded audio versions of printed signs is being researched as an aid for visually impaired people through the RIAS (Remote Infrared Audible Signage) project.
Spectroscopy.
Infrared vibrational spectroscopy (see also near-infrared spectroscopy) is a technique that can be used to identify molecules by analysis of their constituent bonds. Each chemical bond in a molecule vibrates at a frequency characteristic of that bond. A group of atoms in a molecule (e.g., CH2) may have multiple modes of oscillation caused by the stretching and bending motions of the group as a whole. If an oscillation leads to a change in dipole in the molecule then it will absorb a photon that has the same frequency. The vibrational frequencies of most molecules correspond to the frequencies of infrared light. Typically, the technique is used to study organic compounds using light radiation from 4000–400 cm−1, the mid-infrared. A spectrum of all the frequencies of absorption in a sample is recorded. This can be used to gain information about the sample composition in terms of chemical groups present and also its purity (for example, a wet sample will show a broad O-H absorption around 3200 cm−1).
Thin film metrology.
In the semiconductor industry, infrared light can be used to characterize materials such as thin films and periodic trench structures. By measuring the reflectance of light from the surface of a semiconductor wafer, the index of refraction (n) and the extinction Coefficient (k) can be determined via the Forouhi-Bloomer dispersion equations. The reflectance from the infrared light can also be used to determine the critical dimension, depth, and sidewall angle of high aspect ratio trench structures.
Meteorology.
Weather satellites equipped with scanning radiometers produce thermal or infrared images, which can then enable a trained analyst to determine cloud heights and types, to calculate land and surface water temperatures, and to locate ocean surface features. The scanning is typically in the range 10.3–12.5 µm (IR4 and IR5 channels).
High, cold ice clouds such as Cirrus or Cumulonimbus show up bright white, lower warmer clouds such as Stratus or Stratocumulus show up as grey with intermediate clouds shaded accordingly. Hot land surfaces will show up as dark-grey or black. One disadvantage of infrared imagery is that low cloud such as stratus or fog can be a similar temperature to the surrounding land or sea surface and does not show up. However, using the difference in brightness of the IR4 channel (10.3–11.5 µm) and the near-infrared channel (1.58–1.64 µm), low cloud can be distinguished, producing a "fog" satellite picture. The main advantage of infrared is that images can be produced at night, allowing a continuous sequence of weather to be studied.
These infrared pictures can depict ocean eddies or vortices and map currents such as the Gulf Stream, which are valuable to the shipping industry. Fishermen and farmers are interested in knowing land and water temperatures to protect their crops against frost or increase their catch from the sea. Even El Niño phenomena can be spotted. Using color-digitized techniques, the gray-shaded thermal images can be converted to color for easier identification of desired information.
The main water vapour channel at 6.40 to 7.08 µm can be imaged by some weather satellites and shows the amount of moisture in the atmosphere.
Climatology.
In the field of climatology, atmospheric infrared radiation is monitored to detect trends in the energy exchange between the earth and the atmosphere. These trends provide information on long-term changes in Earth's climate. It is one of the primary parameters studied in research into global warming, together with solar radiation.
A pyrgeometer is utilized in this field of research to perform continuous outdoor measurements. This is a broadband infrared radiometer with sensitivity for infrared radiation between approximately 4.5 µm and 50 µm.
Astronomy.
Astronomers observe objects in the infrared portion of the electromagnetic spectrum using optical components, including mirrors, lenses and solid state digital detectors. For this reason it is classified as part of optical astronomy. To form an image, the components of an infrared telescope need to be carefully shielded from heat sources, and the detectors are chilled using liquid helium.
The sensitivity of Earth-based infrared telescopes is significantly limited by water vapor in the atmosphere, which absorbs a portion of the infrared radiation arriving from space outside of selected atmospheric windows. This limitation can be partially alleviated by placing the telescope observatory at a high altitude, or by carrying the telescope aloft with a balloon or an aircraft. Space telescopes do not suffer from this handicap, and so outer space is considered the ideal location for infrared astronomy.
The infrared portion of the spectrum has several useful benefits for astronomers. Cold, dark molecular clouds of gas and dust in our galaxy will glow with radiated heat as they are irradiated by imbedded stars. Infrared can also be used to detect protostars before they begin to emit visible light. Stars emit a smaller portion of their energy in the infrared spectrum, so nearby cool objects such as planets can be more readily detected. (In the visible light spectrum, the glare from the star will drown out the reflected light from a planet.)
Infrared light is also useful for observing the cores of active galaxies, which are often cloaked in gas and dust. Distant galaxies with a high redshift will have the peak portion of their spectrum shifted toward longer wavelengths, so they are more readily observed in the infrared.
Infrared cleaning.
Infrared cleaning is a technique used by some Motion picture film scanner, film scanners and flatbed scanners to reduce or remove the effect of dust and scratches upon the finished scan. It works by collecting an additional infrared channel from the scan at the same position and resolution as the three visible color channels (red, green, and blue). The infrared channel, in combination with the other channels, is used to detect the location of scratches and dust. Once located, those defects can be corrected by scaling or replaced by inpainting.
Art conservation and analysis.
, as called by art conservators, can be applied to paintings to reveal underlying layers in a completely non-destructive manner, in particular the underdrawing or outline drawn by the artist as a guide. This often reveals the artist's use of carbon black, which shows up well in reflectograms, as long as it has not also been used in the ground underlying the whole painting. Art conservators are looking to see whether the visible layers of paint differ from the underdrawing or layers in between – such alterations are called pentimenti when made by the original artist. This is very useful information in deciding whether a painting is the prime version by the original artist or a copy, and whether it has been altered by over-enthusiastic restoration work. In general, the more pentimenti the more likely a painting is to be the prime version. It also gives useful insights into working practices.
Among many other changes in the Arnolfini Portrait of 1434 (left), the man's face was originally higher by about the height of his eye; the woman's was higher, and her eyes looked more to the front. Each of his feet was underdrawn in one position, painted in another, and then overpainted in a third. These alterations are seen in infrared reflectograms.
Recent progress in the design of infrared sensitive cameras made it possible to discover and depict not only underpaintings and pentimenti but entire paintings which were later overpainted by the artist. Notable examples are Picasso's "Woman ironing" and "Blue room", where in both cases, a portrait of a man has been made visible under the painting as it is known today.
Similar uses of infrared are made by conservators and scientists on various types of objects, especially very old written documents such as the Dead Sea Scrolls, the Roman works in the Villa of the Papyri, and the Silk Road texts found in the Dunhuang Caves. Carbon black used in ink can show up extremely well.
Biological systems.
The pit viper has a pair of infrared sensory pits on its head. There is uncertainty regarding the exact thermal sensitivity of this biological infrared detection system.
Other organisms that have thermoreceptive organs are pythons (family Pythonidae), some boas (family Boidae), the Common Vampire Bat ("Desmodus rotundus"), a variety of jewel beetles ("Melanophila acuminata"), darkly pigmented butterflies ("Pachliopta aristolochiae" and "Troides rhadamantus plateni"), and possibly blood-sucking bugs ("Triatoma infestans").
Although near-infrared vision (780–1000 nm) has long been deemed impossible due to noise in visual pigments, sensation of near-infrared light was reported in the common carp and in three cichlid species. Fish use NIR to capture prey and for phototactic swimming orientation. NIR sensation in fish may be relevant under poor lighting conditions during twilight and in turbid surface waters.
Photobiomodulation.
Near-infrared light, or photobiomodulation, is used for treatment of chemotherapy-induced oral ulceration as well as wound healing. There is some work relating to anti-herpes virus treatment. Research projects include work on central nervous system healing effects via cytochrome c oxidase upregulation and other possible mechanisms.
Health hazard.
Strong infrared radiation in certain industry high-heat settings may be hazardous to the eyes, resulting in damage or blindness to the user. Since the radiation is invisible, special IR-proof goggles must be worn in such places.
Earth as an infrared emitter.
Earth's surface and the clouds absorb visible and invisible radiation from the sun and re-emit much of the energy as infrared back to atmosphere. Certain substances in the atmosphere, chiefly cloud droplets and water vapor, but also carbon dioxide, methane, nitrous oxide, sulfur hexafluoride, and chlorofluorocarbons, absorb this infrared, and re-radiate it in all directions including back to Earth. Thus, the greenhouse effect keeps the atmosphere and surface much warmer than if the infrared absorbers were absent from the atmosphere.
History of infrared science.
The discovery of infrared radiation is ascribed to William Herschel, the astronomer, in the early 19th century. Herschel published his results in 1800 before the Royal Society of London. Herschel used a prism to refract light from the sun and detected the infrared, beyond the red part of the spectrum, through an increase in the temperature recorded on a thermometer. He was surprised at the result and called them "Calorific Rays". The term 'Infrared' did not appear until late in the 19th century.
Other important dates include:

</doc>
<doc id="15023" url="https://en.wikipedia.org/wiki?curid=15023" title="Icosidodecahedron">
Icosidodecahedron

In geometry, an icosidodecahedron is a polyhedron with twenty triangular faces and twelve pentagonal faces. An icosidodecahedron has 30 identical vertices, with two triangles and two pentagons meeting at each, and 60 identical edges, each separating a triangle from a pentagon. As such it is one of the Archimedean solids and more particularly, a quasiregular polyhedron.
An icosidodecahedron has icosahedral symmetry, and its first stellation is the compound of a dodecahedron and its dual icosahedron, with the vertices of the icosahedron located at the midpoints of the edges of either.
Its dual polyhedron is the rhombic triacontahedron. An icosidodecahedron can be split along any of six planes to form a pair of pentagonal rotundae, which belong among the Johnson solids.
The icosidodecahedron can be considered a "pentagonal gyrobirotunda", as a combination of two rotundae (compare pentagonal orthobirotunda, one of the Johnson solids). In this form its symmetry is D5d, [10,2+], (2*5), order 20.
The wire-frame figure of the icosidodecahedron consists of six flat regular decagons, meeting in pairs at each of the 30 vertices.
Cartesian coordinates.
Convenient Cartesian coordinates for the vertices of an icosidodecahedron with unit edges are given by the even permutations of:
where ϕ is the golden ratio, (1+√5)/2.
Orthogonal projections.
The icosidodecahedron has four special orthogonal projections, centered on a vertex, an edge, a triangular face, and a pentagonal face. The last two correspond to the A2 and H2 Coxeter planes.
Surface area and volume.
The surface area "A" and the volume "V" of the icosidodecahedron of edge length "a" are:
Spherical tiling.
The icosidodecahedron can also be represented as a spherical tiling, and projected onto the plane via a stereographic projection. This projection is conformal, preserving angles but not areas or lengths. Straight lines on the sphere are projected as circular arcs on the plane.
Related polyhedra.
The icosidodecahedron is a rectified dodecahedron and also a rectified icosahedron, existing as the full-edge truncation between these regular solids.
The icosidodecahedron contains 12 pentagons of the dodecahedron and 20 triangles of the icosahedron:
The icosidodecahedron exists in a sequence of symmetries of quasiregular polyhedra and tilings with vertex configurations (3."n")2, progressing from tilings of the sphere to the Euclidean plane and into the hyperbolic plane. With orbifold notation symmetry of *"n"32 all of these tilings are wythoff construction within a fundamental domain of symmetry, with generator points at the right angle corner of the domain.
Dissection.
The icosidodecahedron is related to the Johnson solid called a pentagonal orthobirotunda created by two pentagonal rotunda connected as mirror images. The "icosidodecahedron" can therefore be called a "pentagonal gyrobirotunda" with the gyration between top and bottom halves. 
Related polyhedra.
Eight uniform star polyhedra share the same vertex arrangement. Of these, two also share the same edge arrangement: the small icosihemidodecahedron (having the triangular faces in common), and the small dodecahemidodecahedron (having the pentagonal faces in common). The vertex arrangement is also shared with the compounds of five octahedra and of five tetrahemihexahedra.
Related polytopes.
In four-dimensional geometry the icosidodecahedron appears in the regular 600-cell as the equatorial slice that belongs to the vertex-first passage of the 600-cell through 3D space. In other words: the 30 vertices of the 600-cell which lie at arc distances of 90 degrees on its circumscribed hypersphere from a pair of opposite vertices, are the vertices of an icosidodecahedron. The wire frame figure of the 600-cell consists of 72 flat regular decagons. Six of these are the equatorial decagons to a pair of opposite vertices. They are precisely the six decagons which form the wire frame figure of the icosidodecahedron.
Icosidodecahedral graph.
In the mathematical field of graph theory, a icosidodecahedral graph is the graph of vertices and edges of the icosidodecahedron, one of the Archimedean solids. It has 30 vertices and 60 edges, and is a quartic graph Archimedean graph.

</doc>
<doc id="15024" url="https://en.wikipedia.org/wiki?curid=15024" title="ISO 8601">
ISO 8601

ISO 8601 "Data elements and interchange formats – Information interchange – Representation of dates and times" is an international standard covering the exchange of date and time-related data. It was issued by the International Organization for Standardization (ISO) and was first published in 1988. The purpose of this standard is to provide an unambiguous and well-defined method of representing dates and times, so as to avoid misinterpretation of numeric representations of dates and times, particularly when data are transferred between countries with different conventions for writing numeric dates and times.
In general, ISO 8601 applies to representations and formats of dates in the Gregorian (and potentially proleptic Gregorian) calendar, times based on the 24-hour timekeeping system (including optional time zone information), and combinations thereof. The standard does not assign any specific meaning to elements of the date/time to be represented; the meaning will depend on the context of its use. In addition, dates and times to be represented cannot include words with no specified numerical meaning in the standard (e.g., names of years in the Chinese calendar) or that do not use characters (e.g., images, sounds).
In representations for interchange, dates and times are arranged so the largest temporal term (the year) is placed to the left and each successively smaller term is placed to the right of the previous term. Representations must be written in a combination of Arabic numerals and certain characters (such as "-", ":", "W", and "Z") that are given specific meanings within the standard; the implication is that some commonplace ways of writing parts of dates, such as "January" or "Thursday", are not allowed in interchange representations.
History.
The first edition of the ISO 8601 standard was published as "ISO 8601:1988" in 1988. It unified and replaced a number of older ISO standards on various aspects of date and time notation: ISO 2014, ISO 2015, ISO 2711, ISO 3307, and ISO 4031. It has been superseded by a second edition "ISO 8601:2000" in 2000 and by the current third edition "ISO 8601:2004" published on 1 December 2004. ISO 8601 was prepared by, and is under the direct responsibility of, ISO Technical Committee TC 154.
ISO 2014, though superseded, is the standard that originally introduced the all-numeric date notation in most-to-least-significant order . The ISO week numbering system was introduced in ISO 2015, and the identification of days by ordinal dates was originally defined in ISO 2711.
Dates.
The standard uses the Gregorian calendar, which serves as an international standard for civil use.
ISO 8601 fixes a reference calendar date to the Gregorian calendar of 20 May 1875 as the date the "Convention du Mètre" (Metre Convention) was signed in Paris. However, ISO calendar dates before the Convention are still compatible with the Gregorian calendar all the way back to the official introduction of the Gregorian calendar on . Earlier dates, in the proleptic Gregorian calendar, may be used by mutual agreement of the partners exchanging information. The standard states that every date must be consecutive, so usage of the Julian calendar would be contrary to the standard (because at the switchover date, the dates would not be consecutive).
Years.
ISO 8601 prescribes, as a minimum, a four-digit year to avoid the year 2000 problem. It therefore represents years from 0000 to 9999, year 0000 being equal to 1 BC and all others AD. However, years prior to 1583 are not automatically allowed by the standard. Instead "values in the range [0000 through shall only be used by mutual agreement of the partners in information interchange."
To represent years before 0000 or after 9999, the standard also permits the expansion of the year representation but only by prior agreement between the sender and the receiver. An expanded year representation [±YYYYY] must have an agreed-upon number of extra year digits beyond the four-digit minimum, and it must be prefixed with a + or − sign instead of the more common AD/BC (or BCE/CE) notation; by convention 1 BC is labelled +0000, 2 BC is labeled -0001, and so on.
Calendar dates.
Calendar date representations are in the form shown in the box to the right. indicates a four-digit year, 0000 through 9999. [MM indicates a two-digit month of the year, 01 through 12. indicates a two-digit day of that month, 01 through 31. For example, "5 April 1981" may be represented as either in the "extended format" or "19810405" in the "basic format".
The standard also allows for calendar dates to be written with reduced accuracy. For example, one may write to mean "1981 April", and one may simply write "1981" to refer to that year or "19" to refer to the century from 1900 to 1999 inclusive. Although the standard allows both the YYYY-MM-DD and YYYYMMDD formats for complete calendar date representations, if the day is omitted then only the format is allowed. By disallowing dates of the form YYYYMM, the standard avoids confusion with the truncated representation YYMMDD (still often used).
Week dates.
Week date representations are in the format as shown in the box to the right. indicates the "ISO week-numbering year" which is slightly different from the traditional Gregorian calendar year (see below). [Www is the "week number" prefixed by the letter "W", from W01 through W53. is the "weekday number", from 1 through 7, beginning with Monday and ending with Sunday.
There are several mutually equivalent and compatible descriptions of week 01:
As a consequence, if 1 January is on a Monday, Tuesday, Wednesday or Thursday, it is in week 01. If 1 January is on a Friday, Saturday or Sunday, it is in week 52 or 53 of the previous year (there is no week 00). 28 December is always in the last week of its year.
The week number can be described by counting the Thursdays: week 12 contains the 12th Thursday of the year.
The "ISO week-numbering year" starts at the first day (Monday) of week 01 and ends at the Sunday before the new ISO year (hence without overlap or gap). It consists of 52 or 53 full weeks. The ISO week-numbering year number deviates from the number of the traditional Gregorian calendar year on a Friday, Saturday, and Sunday, or a Saturday and Sunday, or just a Sunday, at the start of the traditional Gregorian calendar year (which are at the end of the previous ISO week-numbering year) and a Monday, Tuesday and Wednesday, or a Monday and Tuesday, or just a Monday, at the end of the traditional Gregorian calendar year (which are in week 01 of the next ISO week-numbering year). For Thursdays, the ISO week-numbering year number is always equal to the traditional Gregorian calendar year number.
Examples:
Ordinal dates.
An ordinal date is a simple form for occasions when the arbitrary nature of week and month definitions are more of an impediment than an aid, for instance, when comparing dates from different calendars. As represented above, indicates a year. [DDD is the day of that year, from 001 through 365 (366 in leap years). For example, is also .
This format is used with simple hardware systems that have a need for a date system, but where including full calendar calculation software may be a significant nuisance. This system is sometimes referred to as "Julian Date", but this can cause confusion with the astronomical Julian day, a sequential count of the number of days since day 0 beginning Greenwich noon, Julian proleptic calendar (or noon on ISO date which uses the Gregorian proleptic calendar with a year ).
Times.
ISO 8601 uses the 24-hour clock system. The "basic format" is and the "extended format" is [hh::.
So a time might appear as either "134730" in the "basic format" or "13:47:30" in the "extended format".
Either the seconds, or the minutes and seconds, may be omitted from the basic or extended time formats for greater brevity but decreased accuracy: [hhand [hh are the resulting reduced accuracy time formats.
"Midnight" is a special case and may be referred to as either "00:00" or "24:00". The notation "00:00" is used at the beginning of a calendar day and is the more frequently used. At the end of a day use "24:00". "2007-04-05T24:00" is the same instant as "2007-04-06T00:00" (see "Combined date and time representations" below).
Decimal fractions may be added to any of the three time elements. However, a fraction may only be added to the lowest order time element in the representation. A decimal mark, either a comma or a dot (without any preference as stated in resolution 10 of the 22nd General Conference CGPM in 2003, but with a preference for a comma according to ISO 8601:2004) is used as a separator between the time element and its fraction. To denote "14 hours, 30 and one half minutes", do not include a seconds figure. Represent it as "14:30,5", "1430,5", "14:30.5", or "1430.5". There is no limit on the number of decimal places for the decimal fraction. However, the number of decimal places needs to be agreed to by the communicating parties.
Time zone designators.
Time zones in ISO 8601 are represented as local time (with the location unspecified), as UTC, or as an offset from UTC.
If no UTC relation information is given with a time representation, the time is assumed to be in local time. While it "may" be safe to assume local time when communicating in the same time zone, it is ambiguous when used in communicating across different time zones. Even within a single geographic time zone, some local times will be ambiguous if the region observes daylight saving time. It is usually preferable to indicate a time zone (zone designator) using the standard's notation.
UTC.
If the time is in UTC, add a "Z" directly after the time without a space. "Z" is the zone designator for the zero UTC offset. "09:30 UTC" is therefore represented as "09:30Z" or "0930Z". "14:45:15 UTC" would be "14:45:15Z" or "144515Z".
UTC time is also known as "Zulu time", since "Zulu" is the NATO phonetic alphabet word for "Z".
Time offsets from UTC.
The offset from UTC is appended to the time in the same way that 'Z' was above, in the form ±±[hhor ±[hh. So if the time being described is one hour ahead of UTC (such as the time in Berlin during the winter), the zone designator would be "+01:00", "+0100", or simply "+01". To represent a time behind UTC the offset is negative. For example, the time in New York in winter is . For other time offsets see List of UTC time offsets. To represent a negative offset, ISO 8601 specifies using either a hyphen–minus or a minus sign character. If the interchange character set is limited and does not have a minus sign character, then the hyphen–minus should be used. ASCII does not have a minus sign, so its hyphen-minus character (code is 45 decimal or 2D hexadecimal) would be used. If the character set has a minus sign, then that character should be used. Unicode has a minus sign, and its character code is U+2212 (2212 hexadecimal); the HTML character entity invocation is codice_1. Using the minus sign conflicts with similar standards, so ISO 8601 implementations will probably accept both hyphen–minus and minus sign characters.
The following times all refer to the same moment: "18:30Z", "22:30+04", "1130−0700", and "15:00−03:30". Nautical time zone letters are not used with the exception of Z. To calculate UTC time one has to subtract the offset from the local time, e.g. for "15:00−03:30" do 15:00 − (−03:30) to get 18:30 UTC.
An offset of zero, in addition to having the special representation "Z", can also be stated numerically as "+00:00", "+0000", or "+00". However, it is not permitted to state it numerically with a negative sign, as "−00:00", "−0000", or "−00". The section dictating sign usage (section 3.4.2 in the 2004 edition of the standard) states that a plus sign must be used for a positive or zero value, and a minus sign for a negative value. Contrary to this rule, RFC 3339, which is otherwise a profile of ISO 8601, permits the use of "-00", with the same denotation as "+00" but a differing connotation.
ISO 8601 permits the hyphen (-) to be used as the minus (−) character when the character set is limited. In contrast, RFC 3339 explicitly requires the hyphen (-) symbol to represent negative offsets and does not allow for use of the minus (−) symbol.
Combined date and time representations.
A single point in time can be represented by concatenating a complete date expression, the letter "T" as a delimiter, and a valid time expression. For example, .
If a time zone designator is required, it follows the combined date and time. For example, "2007-04-05T14:30Z" or "2007-04-05T12:30-02:00".
Either basic or extended formats may be used, but both date and time must use the same format. The date expression may be calendar, week, or ordinal, and must use a complete representation. The time may be represented using a specified reduced accuracy format. It is permitted to omit the 'T' character by mutual agreement.
Durations.
Durations are a component of time intervals and define the amount of intervening time in a time interval. They should only be used as part of a time interval as prescribed by the standard. Time intervals are discussed in the next section.
Durations are represented by the format Por P[nW as shown to the right. In these representations, the is replaced by the value for each of the date and time elements that follow the [n. Leading zeros are not required, but the maximum number of digits for each element should be agreed to by the communicating parties. The capital letters "P", "Y", "M", "W", "D", "T", "H", "M", and "S" are designators for each of the date and time elements and are not replaced.
For example, "P3Y6M4DT12H30M5S" represents a duration of "three years, six months, four days, twelve hours, thirty minutes, and five seconds".
Date and time elements including their designator may be omitted if their value is zero, and lower order elements may also be omitted for reduced precision. For example, "P23DT23H" and "P4Y" are both acceptable duration representations.
To resolve ambiguity, "P1M" is a one-month duration and "PT1M" is a one-minute duration (note the time designator, T, that precedes the time value). The smallest value used may also have a decimal fraction, as in "P0.5Y" to indicate half a year. This decimal fraction may be specified with either a comma or a full stop, as in "P0,5Y" or "P0.5Y". The standard does not prohibit date and time values in a duration representation from exceeding their "carry over points" except as noted below. Thus, "PT36H" could be used as well as "P1DT12H" for representing the same duration. But keep in mind that "PT36H" is not the same as "P1DT12H" when switching from or to Daylight saving time.
Alternatively, a format for duration based on combined date and time representations may be used by agreement between the communicating parties either in the basic format PYYYYMMDDThhmmss or in the extended format . For example, the first duration shown above would be . However, individual date and time values cannot exceed their moduli (e.g. a value of 13 for the month or 25 for the hour would not be permissible).
Time intervals.
A time interval is the intervening time between two time points. The amount of intervening time is expressed by a duration (as described in the previous section). The two time points (start and end) are expressed by either a combined date and time representation or just a date representation.
There are four ways to express a time interval:
Of these, the first three require two values separated by an "interval designator" which is usually a solidus (more commonly referred to as a forward slash "/"). Section 4.4.2 of the standard notes that: "In certain application areas a double hyphen is used as a separator instead of a solidus." The standard does not define the term "double hyphen", but previous versions used notations like "2000--2002". Use of a double hyphen instead of a solidus allows inclusion in computer filenames. A solidus is a reserved character and not allowed in a filename in common operating systems.
For <start>/<end> expressions, if any elements are missing from the end value, they are assumed to be the same as for the start value including the time zone. This feature of the standard allows for concise representations of time intervals. For example, the date of a two-hour meeting including the start and finish times could be simply shown as "2007-12-14T13:30/15:30", where "/15:30" implies "/2007-12-14T15:30" (the same date as the start), or the beginning and end dates of a monthly billing period as "2008-02-15/03-14", where "/03-14" implies "/2008-03-14" (the same year as the start).
If greater precision is desirable to represent the time interval, then more time elements can be added to the representation. An interval denoted can start at any time on and end at any time on , whereas includes the start and end times.
To explicitly include all of the start and end dates, the interval would be represented as .
Repeating intervals.
Repeating intervals are specified in clause "4.5 Recurring time interval". They are formed by adding "Rto the beginning of an interval expression, where "R" is used as the letter itself and [n is replaced by the number of repetitions. Leaving out the value for means an unbounded number of repetitions. If the interval specifies the start (forms 1 and 2 above), then this is the start of the repeating interval. If the interval specifies the end but not the start (form 3 above), then this is the end of the repeating interval. For example, to repeat the interval of "P1Y2M10DT2H30M" five times starting at , use .
Truncated representations.
ISO 8601:2000 allowed truncation (by agreement), where leading components of a date or time are omitted. Notably, this allowed two-digit years to be used and the ambiguous formats YY-MM-DD and YYMMDD. This provision was removed in ISO 8601:2004.
Usage.
On the Internet, the World Wide Web Consortium (W3C) uses ISO 8601 in defining a profile of the standard that restricts the supported date and time formats to reduce the chance of error and the complexity of software.
RFC 3339 defines a profile of ISO 8601 for use in Internet protocols and standards. It explicitly excludes durations and dates before the common era. The more complex formats such as week numbers and ordinal days are not permitted.
RFC 3339 deviates from ISO 8601 in allowing a zero timezone offset to be specified as "-00:00", which ISO 8601 forbids. RFC 3339 intends "-00:00" to carry the connotation that it is not stating a preferred timezone, whereas the conforming "+00:00" or any non-zero offset connotes that the offset being used is preferred. This convention regarding "-00:00" is derived from earlier RFCs, such as RFC 2822 which uses it for timestamps in email headers. RFC 2822 made no claim that any part of its timestamp format conforms to ISO 8601, and so was free to use this convention without conflict.
ISO 8601 is referenced by several specifications, but the full range of options of ISO 8601 is not always used. For example, the various electronic program guide standards for TV, digital radio, etc. use several forms to describe points in time and durations. The ID3 audio meta-data specification also makes use of a subset of ISO 8601.
The GeneralizedTime makes use of another subset of ISO 8601.
The ISO 8601 week date, as of 2006, appeared in its basic form on major brand commercial packaging in the United States. Its appearance depended on the particular packaging, canning, or bottling plant more than any particular brand. The format is particularly useful for quality assurance, so that production errors can be readily traced to work weeks, and products can be correctly targeted for recall.
External links.
Implementation overview

</doc>
<doc id="15027" url="https://en.wikipedia.org/wiki?curid=15027" title="Isa">
Isa

Isa or ISA may refer to:

</doc>
<doc id="15028" url="https://en.wikipedia.org/wiki?curid=15028" title="International Seabed Authority">
International Seabed Authority

The International Seabed Authority (ISA) (, ) is an intergovernmental body based in Kingston, Jamaica, that was established to organize, regulate and control all mineral-related activities in the international seabed area beyond the limits of national jurisdiction, an area underlying most of the world’s oceans. It is an organization established by the Law of the Sea Convention.
Origin.
Following at least ten preparatory meetings over the years, the Authority held its first inaugural meeting in its host country, Jamaica, on 16 November 1994, the day the Convention came into force. The articles governing the Authority have been made "noting the political and economic changes, including market-oriented approaches, affecting the implementation" of the Convention. The Authority obtained its observer status to the United Nations in October 1996.
Currently, the Authority has 167 members and the European Union, composed of all parties to the Law of the Sea Convention.
Two principal organs establish the policies and govern the work of the Authority: the Assembly, in which all members are represented, and a 36-member Council elected by the Assembly. Council members are chosen according to a formula designed to ensure equitable representation of countries from various groups, including those engaged in seabed mineral exploration and the land-based producers of minerals found on the seabed. The Authority holds one annual session, usually of two weeks' duration.
The Authority operates by contracting with private and public corporations and other entities authorizing them to explore, and eventually exploit, specified areas on the deep seabed for mineral resources essential for building most technological products. The Convention also established a body called the Enterprise which is to serve as the Authority’s own mining operator, but no concrete steps have been taken to bring this into being.
Activities.
The Authority has a budget of $5.8 million a year (rising to an authorized $6.3 million for each of the years 2009-2010) and a staff of some 35 people. In June 2008, the Assembly of the Authority elected by acclamation Nii Allotey Odunton of Ghana, Deputy to the Secretary-General since 1996, for a four-year term as Secretary-General beginning 1 January 2009. He succeeded Satya Nandan of Fiji, the first Secretary-General of the Authority, who left after three consecutive four-year terms since 1996.
The exploitation system envisaged in the Law of the Sea Convention, overseen by the Authority, came to life with the signature in 2001/02 of 15-year contracts with seven organizations that had applied for specific seabed areas in which they were authorized to explore for polymetallic nodules. In 2006, a German entity was added to the list.
The eight current contractors are: Yuzhmorgeologya (Russian Federation); Interoceanmetal Joint Organization (IOM) (Bulgaria, Cuba, Slovakia, Czech Republic, Poland and Russian Federation); the Government of the Republic of Korea; China Ocean Minerals Research and Development Association (COMRA) (China); Deep Ocean Resources Development Company (DORD) (Japan); Institut français de recherche pour l’exploitation de la mer (IFREMER) (France); the Government of India, and the Federal Institute for Geosciences and Natural Resources of Germany.
All but one of the current areas of exploration are in the Clarion-Clipperton Zone, in the Equatorial North Pacific Ocean south and southeast of Hawaii. The remaining area, being explored by India, is in the Central Indian Basin of the Indian Ocean.
Each area is limited to , of which half is to be relinquished to the Authority after eight years. Each contractor is required to report once a year on its activities in its assigned area. So far, none of them has indicated any serious move to begin commercial exploitation.
In 2008, the Authority received two new applications for authorization to explore for polymetallic nodules, coming for the first time from private firms in developing island nations of the Pacific. Sponsored by their respective governments, they were submitted by Nauru Ocean Resources Inc. and Tonga Offshore Mining Limited. A 15-year exploration contract was granted by the Authority to Nauru Ocean Resources Inc. on 22 July 2011 and to Tonga Offshore Mining Limited on 12 January 2012.
The Authority's main legislative accomplishment to date has been the adoption, in the year 2000, of regulations governing exploration for polymetallic nodules. These resources, also called manganese nodules, contain varying amounts of manganese, cobalt, copper and nickel. They occur as potato-sized lumps scattered about on the surface of the ocean floor, mainly in the central Pacific Ocean but with some deposits in the Indian Ocean.
The Council of the Authority began work, in August 2002, on another set of regulations, covering polymetallic sulfides and cobalt-rich ferromanganese crusts, which are rich sources of such minerals as copper, iron, zinc, silver and gold, as well as cobalt. The sulphides are found around volcanic hot springs, especially in the western Pacific Ocean, while the crusts occur on oceanic ridges and elsewhere at several locations around the world. The Council decided in 2006 to prepare separate sets of regulations for sulphides and for crusts, with priority given to sulphides. It devoted most of its sessions in 2007 and 2008 to this task, but several issues remained unresolved. Chief among these were the definition and configuration of the area to be allocated to contractors for exploration, the fees to be paid to the Authority and the question of how to deal with any overlapping claims that might arise. Meanwhile, the Legal and Technical Commission reported progress on ferromanganese crusts.
In addition to its legislative work, the Authority organizes annual workshops on various aspects of seabed exploration, with emphasis on measures to protect the marine environment from any harmful consequences. It disseminates the results of these meetings through publications. Studies over several years covering the key mineral area of the Central Pacific resulted in a technical study on biodiversity, species ranges and gene flow in the abyssal Pacific nodule province, with emphasis on predicting and managing the impacts of deep seabed mining A workshop at Manoa, Hawaii, in October 2007 produced a rationale and recommendations for the establishment of "preservation reference areas" in the Clarion-Clipperton Zone, where nodule mining would be prohibited in order to leave the natural environment intact. The most recent workshop, held at Chennai, India, in February 2008, concerned polymetallic nodule mining technology, with special reference to its current status and challenges ahead 
Contrary to early hopes that seabed mining would generate extensive revenues for both the exploiting countries and the Authority, no technology has yet been developed for gathering deep-sea minerals at costs that can compete with land-based mines. Until recently, the consensus has been that economic mining of the ocean depths might be decades away. Moreover, the United States, with some of the most advanced ocean technology in the world, has not yet ratified the Law of the Sea Convention and is thus not a member of the Authority.
In recent years, however, interest in deep-sea mining, especially with regard to ferromanganese crusts and polymetallic sulphides, has picked up among several firms now operating in waters within the national zones of Papua New Guinea, Fiji and Tonga. Papua New Guinea was the first country in the world to grant commercial exploration licenses for seafloor massive sulphide deposits when it granted the initial license to Nautilus Minerals in 1997. Japan’s new ocean policy emphasizes the need to develop methane hydrate and hydrothermal deposits within Japan’s exclusive economic zone and calls for the commercialization of these resources within the next 10 years. Reporting on these developments in his annual report to the Authority in April 2008, Secretary-General Nandan referred also to the upward trend in demand and prices for cobalt, copper, nickel and manganese, the main metals that would be derived from seabed mining, and he noted that technologies being developed for offshore extraction could be adapted for deep sea mining.
In its preamble, UNCLOS defines the international seabed area—the part under ISA jurisdiction—as “the seabed and ocean floor and the subsoil thereof, beyond the limits of national jurisdiction”. There are no maps annexed to the Convention to delineate this area. Rather, UNCLOS outlines the areas of national jurisdiction, leaving the rest for the international portion. National jurisdiction over the seabed normally leaves off at seaward from baselines running along the shore, unless a nation can demonstrate that its continental shelf is naturally prolonged beyond that limit, in which case it may claim up to . ISA has no role in determining this boundary. Rather, this task is left to another body established by UNCLOS, the Commission on the Limits of the Continental Shelf, which examines scientific data submitted by coastal states that claim a broader reach. Maritime boundaries between states are generally decided by bilateral negotiation (sometimes with the aid of judicial bodies), not by ISA.
Recently, there has been much interest in the possibility of exploiting seabed resources in the Arctic Ocean, bordered by Canada, Denmark, Iceland, Norway, Russia and the United States (see Territorial claims in the Arctic). Mineral exploration and exploitation activities in any seabed area not belonging to these states would fall under ISA jurisdiction.
Endowment fund.
In 2006 the Authority established an Endowment Fund to Support Collaborative Marine Scientific Research on the International Seabed Area. The Fund will aid experienced scientists and technicians from developing countries to participate in deep-sea research organized by international and national institutions. A campaign was launched in February 2008 to identify participants, establish a network of cooperating bodies and seek outside funds to augment the initial $3 million endowment from the Authority.
The International Seabed Authority Endowment Fund promotes and encourages the conduct of collaborative marine scientific research in the international seabed area through two main activities:
The Secretariat of the International Seabed Authority is facilitating these activities by creating and maintaining an ongoing list of opportunities for scientific collaboration, including research cruises, deep-sea sample analysis, and training and internship programmes. This entails building a network of co-operating groups interested in (or presently undertaking) these types of activities and programmes, such as universities, institutions, contractors with the Authority and other entities.
The Secretariat is also actively seeking applications from scientists and other technical personnel from developing nations to be considered for assistance under the Fund. Application guidelines have been prepared for potential recipients to participate in marine scientific research programmes or other scientific co-operation activity, to enroll in training programmes, and to qualify for technical assistance. An advisory panel will evaluate all incoming applications and make recommendations to the Secretary-General of the International Seabed Authority so successful applicants may be awarded with Fund assistance.
To maximize opportunities for and participation in the Fund, the Secretariat is also seeking donations and in-kind contributions to build on the initial investment of US$3 million. This entails raising awareness of the Fund, reporting on its successes and encouraging new activities and participants.
Controversy.
The exact nature of the ISA's mission and authority has been questioned by opponents of the Law of the Sea Treaty who are generally skeptical of multilateral engagement by the United States. The United States is the only major maritime power that has not ratified the Convention (see United States non-ratification of the UNCLOS), with one of the main anti-ratification arguments being a charge that the ISA is flawed or unnecessary. In its original form, the Convention included certain provisions that some found objectionable, such as:
Because of these concerns, the United States pushed for modification of the Convention, obtaining a 1994 Agreement on Implementation that somewhat mitigates them and thus modifies the ISA's authority. Despite this change the United States has not ratified the Convention and so is not a member of ISA, although it sends sizable delegations to participate in meetings as an observer. On 31 October 2007 the Foreign Relations Committee of the United States Senate, by a vote of 17 to 4, recommended ratification, and President George W. Bush publicly supported U.S. accession to the Convention; no date has yet been set for action by the full Senate.

</doc>
<doc id="15029" url="https://en.wikipedia.org/wiki?curid=15029" title="Industry Standard Architecture">
Industry Standard Architecture

Industry Standard Architecture (ISA) is a retronym term for the 16-bit internal bus of IBM PC/AT and similar computers based on the Intel 80286 and its immediate successors during the 1980s. The bus was (largely) backward compatible with the 8-bit bus of the 8088-based IBM PC, including the IBM PC/XT as well as IBM PC compatibles. 
Originally referred to as the PC/AT-bus it was also termed "I/O Channel" by IBM. The ISA concept was coined by competing PC-clone manufacturers in the late 1980s or early 1990s as a reaction to IBM attempts to replace the AT-bus with its new and incompatible Micro Channel architecture.
The 16-bit ISA bus was also used with 32-bit processors for several years. An attempt to extend it to 32 bits, called Extended Industry Standard Architecture (EISA), was not very successful, however. Later buses such as VESA Local Bus and PCI were used instead, often along with ISA slots on the same mainboard. Derivatives of the AT bus structure were and still are used in ATA/IDE, the PCMCIA standard, Compact Flash, the PC/104 bus, and internally within Super I/O chips.
History.
Compaq created the term "Industry Standard Architecture" (ISA) to replace "PC compatible". The ISA bus was developed by a team led by Mark Dean at IBM as part of the IBM PC project in 1981. It originated as an 8-bit system. The newer 16-bit standard, the IBM AT bus, was introduced in 1984. In 1988, the "Gang of Nine" PC compatible manufacturers, including Compaq, proposed the 32-bit Extended Industry Standard Architecture (EISA) standard and in the process retroactively renamed the AT bus to "ISA" to avoid infringing IBM's trademark on its PC/AT computer. IBM designed the 8 bit version as a buffered interface to the external bus of the Intel 8088 (16/8 bit) CPU used in the original IBM PC and PC/XT, and the 16-bit version as an upgrade for the external bus of the Intel 80286 CPU used in the IBM AT. Therefore, the ISA bus was synchronous with the CPU clock, until sophisticated buffering methods were developed and implemented by chipsets to interface ISA to much faster CPUs.
Designed to connect peripheral cards to the motherboard, ISA allows for bus mastering although only the first 16 MB of main memory are available for direct access. The 8-bit bus ran at 4.77 MHz (the clock speed of the IBM PC and IBM PC/XT's 8088 CPU), while the 16-bit bus operated at 6 or 8 MHz (because the 80286 CPUs in IBM PC/AT computers ran at 6 MHz in early models and 8 MHz in later models.) IBM RT/PC also used the 16-bit bus. It was also available on some non-IBM compatible machines such as Motorola 68k-based Apollo (68020) and Amiga 3000 (68030) workstations, the short-lived AT&T Hobbit and later PowerPC based BeBox.
Companies like Dell improved the AT bus's performance, but in 1987, IBM moved to replace the AT bus with their proprietary Micro Channel Architecture (MCA) in an effort to regain control of the PC architecture and the PC market. (Note the relationship between the IBM term "I/O Channel" for the AT-bus and the name "Micro Channel" for IBM's intended replacement.) MCA had many features that would later appear in PCI, the successor of ISA, but MCA was a closed standard, unlike ISA (PC-bus and AT-bus) for which IBM had released full specifications and even circuit schematics. The system was far more advanced than the AT bus, and computer manufacturers responded with the Extended Industry Standard Architecture (EISA) and later, the VESA Local Bus (VLB). In fact, VLB used some electronic parts originally intended for MCA because component manufacturers already were equipped to manufacture them. Both EISA and VLB were backwards-compatible expansions of the AT (ISA) bus.
Users of ISA-based machines had to know special information about the hardware they were adding to the system. While a handful of devices were essentially "plug-n-play", this was rare. Users frequently had to configure several parameters when adding a new device, such as the IRQ line, I/O address, or DMA channel. MCA had done away with this complication, and PCI actually incorporated many of the ideas first explored with MCA (though it was more directly descended from EISA).
This trouble with configuration eventually led to the creation of ISA PnP, a plug-n-play system that used a combination of modifications to hardware, the system BIOS, and operating system software to automatically manage resource allocations. In reality, ISA PnP can be troublesome, and did not become well-supported until the architecture was in its final days.
PCI slots were the first physically incompatible expansion ports to directly squeeze ISA off the motherboard. At first, motherboards were largely ISA, including a few PCI slots. By the mid-1990s, the two slot types were roughly balanced, and ISA slots soon were in the minority of consumer systems. Microsoft's PC 99 specification recommended that ISA slots be removed entirely, though the system architecture still required ISA to be present in some vestigial way internally to handle the floppy drive, serial ports, etc., which was why the software compatible LPC bus was created. ISA slots remained for a few more years, and towards the turn of the century it was common to see systems with an Accelerated Graphics Port (AGP) sitting near the central processing unit, an array of PCI slots, and one or two ISA slots near the end. In late 2008, even floppy disk drives and serial ports were disappearing, and the extinction of vestigial ISA (by then the LPC bus) from chipsets was on the horizon.
It is also notable that PCI slots are "rotated" compared to their ISA counterparts—PCI cards were essentially inserted "upside-down," allowing ISA and PCI connectors to squeeze together on the motherboard. Only one of the two connectors can be used in each slot at a time, but this allowed for greater flexibility.
The AT Attachment (ATA) hard disk interface is directly descended from ISA (the AT bus). ATA has its origins in hardcards that integrated a hard disk controller (HDC) — usually with an ST-506/ST-412 interface — and a hard disk drive on the same ISA adapter. This was at best awkward from a mechanical structural standpoint, as ISA slots were not designed to support such heavy devices as hard disks (and the 3.5" form-factor hard disks of the time were about twice as tall and heavy as modern drives), so the next generation of Integrated Drive Electronics drives moved both the drive and controller to a drive bay and used a ribbon cable and a very simple interface board to connect it to an ISA slot. ATA, at its essence, is basically a standardization of this arrangement, combined with a uniform command structure for software to interface with the controller on a drive. ATA has since been separated from the ISA bus, and connected directly to the local bus (usually by integration into the chipset), to be clocked much much faster than ISA could support and with much higher throughput. (Notably when ISA was introduced as the AT bus, there "was" no distinction between a "local" and "extension" bus, and there were no chipsets.) Still, ATA retains details which reveal its relationship to ISA. The 16-bit transfer size is the most obvious example; the signal timing, particularly in the PIO modes, is also highly correlated, and the interrupt and DMA mechanisms are clearly from ISA. (The article about ATA has more detail about this history.)
ISA bus architecture.
The PC/XT-bus is an eight-bit ISA bus used by Intel 8086 and Intel 8088 systems in the IBM PC and IBM PC XT in the 1980s. Among its 62 pins were demultiplexed and electrically buffered versions of the 8 data and 20 address lines of the 8088 processor, along with power lines, clocks, read/write strobes, interrupt lines, etc. Power lines included −5 V and ±12 V in order to directly support pMOS and enhancement mode nMOS circuits such as dynamic RAMs among other things. The XT bus architecture uses a single Intel 8259 PIC, giving eight vectorized and prioritized interrupt lines. It has four DMA channels originally provided by the Intel 8237, 3 of the DMA channels are brought out to the XT bus expansion slots; of these, 2 are normally already allocated to machine functions (diskette drive and hard disk controller): 
The PC/AT-bus, a 16-bit (or 80286-) version of the PC/XT bus, was introduced with the IBM PC/AT. This bus was officially termed "I/O Channel" by IBM. It extends the XT-bus by adding a second shorter edge connector in-line with the eight-bit XT-bus connector, which is unchanged, retaining compatibility with most 8-bit cards. The second connector adds four additional address lines for a total of 24, and 8 additional data lines for a total of 16. It also adds new interrupt lines connected to a second 8259 PIC (connected to one of the lines of the first) and 4 × 16-bit DMA channels, as well as control lines to select 8- or 16-bit transfers.
The 16-bit AT bus slot originally used two standard edge connector sockets in early IBM PC/AT machines. However, with the popularity of the AT-architecture and the 16-bit ISA bus, manufacturers introduced specialized 98-pin connectors that integrated the two sockets into one unit. These can be found in almost every AT-class PC manufactured after the mid-1980s. The ISA slot connector is typically black (distinguishing it from the brown EISA connectors and white PCI connectors).
Number of devices.
Motherboard devices have dedicated IRQs (not present in the slots). 16-bit devices can use either PC-bus or PC/AT-bus IRQs. It is therefore possible to connect up to 6 devices that use one 8-bit IRQ each, or up to 5 devices that use one 16-bit IRQ each. At the same time, up to 4 devices may use one 8-bit DMA channel each, while up to 3 devices can use one 16-bit DMA channel each.
Varying bus speeds.
Originally, the bus clock was synchronous with the CPU clock, resulting in varying bus clock frequencies among the many different IBM "clones" on the market (sometimes as high as 16 or 20 MHz), leading to software or electrical timing problems for certain ISA cards at bus speeds they were not designed for. Later motherboards or integrated chipsets used a separate clock generator, or a clock divider which either fixed the ISA bus frequency at 4, 6, or 8 MHz or allowed the user to adjust the frequency via the BIOS setup. When used at a higher bus frequency, some ISA cards (certain Hercules-compatible video cards, for instance), could show significant performance improvements.
8/16-bit incompatibilities.
Memory address decoding for the selection of 8 or 16-bit transfer mode was limited to 128 kB sections: A0000 – BFFFF, C0000 – DFFFF, E0000 – FFFFF leading to problems when mixing 8- and 16-bit cards, as they could not co-exist in the same 128 kiB area.
Past and current use.
ISA is still used today for specialized industrial purposes. In 2008 IEI Technologies released a modern motherboard for Intel Core 2 Duo processors which, in addition to other special I/O features, is equipped with two ISA slots. It is marketed to industrial and military users who have invested in expensive specialized ISA bus adaptors, which are not available in PCI bus versions.
Similarly, ADEK Industrial Computers is releasing a motherboard in early 2013 for Intel Core i3/i5/i7 processors, which contains one (non-DMA) ISA slot.
The PC/104 bus, used in industrial and embedded applications, is a derivative of the ISA bus, utilizing the same signal lines with different connectors. The LPC bus has replaced the ISA bus as the connection to the legacy I/O devices on recent motherboards; while physically quite different, LPC looks just like ISA to software, so that the peculiarities of ISA such as the 16 MiB DMA limit (which corresponds to the full address space of the Intel 80286 CPU used in the original IBM AT) are likely to stick around for a while.
ATA.
As explained in the "History" section, ISA was the basis for development of the ATA interface, used for ATA (a.k.a. IDE) and more recently Serial ATA (SATA) hard disks. Physically, ATA is essentially a simple subset of ISA, with 16 data bits, support for exactly one IRQ and one DMA channel, and 3 address bits plus two IDE address select ("chip select") lines, plus a few unique signal lines specific to ATA/IDE hard disks (such as the Cable Select/Spindle Sync. line.) ATA goes beyond and far outside the scope of ISA by also specifying a set of physical device registers to be implemented on every ATA (IDE) drive and accessed using the address bits and address select signals in the ATA physical interface channel; ATA also specifies a full set of protocols and device commands for controlling fixed disk drives using these registers, through which all operations of ATA hard disks are performed. A further deviation between ISA and ATA is that while the ISA bus remained locked into a single standard clock rate (for backward compatibility), the ATA interface offered many different speed modes, could select among them to match the maximum speed supported by the attached drives, and kept adding faster speeds with later versions of the ATA standard (up to 133 MB/s for ATA-6, the latest.) In most forms, ATA ran much faster than ISA.
XT-IDE.
Before the 16-bit ATA/IDE interface, there was an 8-bit XT-IDE (also known as XTA) interface for hard disks. It was not nearly as popular as ATA has become, and XT-IDE hardware is now fairly hard to find. Some XT-IDE adapters were available as 8-bit ISA cards, and XTA sockets were also present on the motherboards of Amstrad's later XT clones as well as a short-lived line of Philips units. The XTA pinout was very similar to ATA, but only eight data lines and two address lines were used, and the physical device registers had completely different meanings. A few hard drives (such as the Seagate ST351A/X) could support either type of interface, selected with a jumper.
Many later AT (and AT successor) motherboards had no integrated hard drive interface but relied on a separate hard drive interface plugged into an ISA/EISA/VLB slot. There were even a few 80486 based units shipped with MFM/RLL interfaces and drives instead of the increasingly common AT-IDE.
Commodore built the XT-IDE based peripheral hard drive / memory expansion unit A590 for their Amiga 500 and 500+ computers that also supported a SCSI drive. AT-IDE type interfaces only entered the keyboard-cased Amiga line upon introduction of the A600 and A1200 which have an integrated interface and 44 pin connector. Many owners removed the 2,5 inch bracket and installed a 3,5 inch drive with an adapter cable.
PCMCIA.
The PCMCIA specification can be seen as a superset of ATA. The standard for PCMCIA hard disk interfaces, which included PCMCIA flash drives, allows for the mutual configuration of the port and the drive in an ATA mode. As a de facto extension, most PCMCIA flash drives additionally allow for a simple ATA mode that is enabled by pulling a single pin low, so that PCMCIA hardware and firmware are unnecessary to use them as an ATA drive connected to an ATA port. PCMCIA flash drive to ATA adapters are thus simple and inexpensive, but are not guaranteed to work with any and every standard PCMCIA flash drive. Further, such adapters cannot be used as generic PCMCIA ports, as the PCMCIA interface is much more complex than ATA.
Emulation by embedded chips.
Although most modern computers do not have physical ISA buses, all IBM compatible computers — x86, and x86-64 (most non-mainframe, non-embedded) — have ISA buses allocated in virtual address space. Embedded controller chips (southbridge) and CPUs themselves provide services such as temperature monitoring and voltage readings through these buses as ISA devices.
Standardization.
IEEE started a standardization of the ISA bus in 1985, called the P996 specification. However, despite there even having been books published on the P996 specification, it never officially progressed past draft status.

</doc>
<doc id="15030" url="https://en.wikipedia.org/wiki?curid=15030" title="Intergovernmental Panel on Climate Change">
Intergovernmental Panel on Climate Change

The Intergovernmental Panel on Climate Change (IPCC) is a scientific intergovernmental body under the auspices of the United Nations, set up at the request of member governments. It was first established in 1988 by two United Nations organizations, the World Meteorological Organization (WMO) and the United Nations Environment Programme (UNEP), and later endorsed by the United Nations General Assembly through Resolution 43/53. Membership of the IPCC is open to all members of the WMO and UNEP.
The IPCC produces reports that support the United Nations Framework Convention on Climate Change (UNFCCC), which is the main international treaty on climate change. The ultimate objective of the UNFCCC is to "stabilize greenhouse gas concentrations in the atmosphere at a level that would prevent dangerous anthropogenic human-induced interference with the climate system". IPCC reports cover "the scientific, technical and socio-economic information relevant to understanding the scientific basis of risk of human-induced climate change, its potential impacts and options for adaptation and mitigation."
The IPCC does not carry out its own original research, nor does it do the work of monitoring climate or related phenomena itself. The IPCC bases its assessment on the published literature, which includes peer-reviewed and non-peer-reviewed sources.
Thousands of scientists and other experts contribute (on a voluntary basis, without payment from the IPCC) to writing and reviewing reports, which are then reviewed by governments. IPCC reports contain a "Summary for Policymakers", which is subject to line-by-line approval by delegates from all participating governments. Typically this involves the governments of more than 120 countries.
The IPCC provides an internationally accepted authority on climate change, producing reports which have the agreement of leading climate scientists and the consensus of participating governments. The 2007 Nobel Peace Prize was shared, in two equal parts, between the IPCC and Al Gore.
Aims.
The principles that the IPCC operates under are set out in the relevant WMO Executive Council and UNEP Governing Council resolutions and decisions, as well as on actions in support of the UNFCCC process.
The aims of the IPCC are to assess scientific information relevant to:
Organization.
Korean economist Hoesung Lee is the chair of the IPCC since October 8, 2015, following the election of the new IPCC Bureau. 
Before this election, the IPCC was led by his vice-Chair Ismail El Gizouli, who was designated acting Chair after the resignation of Rajendra K. Pachauri in February 2015. The previous chairs were Rajendra K. Pachauri, elected in May 2002; Robert Watson in 1997; and Bert Bolin in 1988. The chair is assisted by an elected bureau including vice-chairs, working group co-chairs, and a secretariat.
The IPCC Panel is composed of representatives appointed by governments and organizations. Participation of delegates with appropriate expertise is encouraged. Plenary sessions of the IPCC and IPCC Working groups are held at the level of government representatives. Non Governmental and Intergovernmental Organizations may be allowed to attend as observers. Sessions of the IPCC Bureau, workshops, expert and lead authors meetings are by invitation only. Attendance at the 2003 meeting included 350 government officials and climate change experts. After the opening ceremonies, closed plenary sessions were held. The meeting report states there were 322 persons in attendance at Sessions with about seven-eighths of participants being from governmental organizations.
There are several major groups:
The IPCC receives funding through the IPCC Trust Fund, established in 1989 by the United Nations Environment Programme (UNEP) and the World Meteorological Organization (WMO), Costs of the Secretary and of housing the secretariat are provided by the WMO, while UNEP meets the cost of the Depute Secretary. Annual cash contributions to the Trust Fund are made by the WMO, by UNEP, and by IPCC Members; the scale of payments is determined by the IPCC Panel, which is also responsible for considering and adopting by consensus the annual budget. The organisation is required to comply with the Financial Regulations and Rules of the WMO.
Assessment reports.
The IPCC has published five comprehensive assessment reports reviewing the latest climate science, as well as a number of special reports on particular topics. These reports are prepared by teams of relevant researchers selected by the Bureau from government nominations. Drafts of these reports are made available for comment in open review processes to which anyone may contribute.
The IPCC published its first assessment report in 1990, a supplementary report in 1992, a second assessment report (SAR) in 1995, a third assessment report (TAR) in 2001, a fourth assessment report (AR4) in 2007 and a fifth assessment report (AR5) in 2014.
Each assessment report is in three volumes, corresponding to Working Groups I, II, and III. Unqualified, "the IPCC report" is often used to mean the Working Group I report, which covers the basic science of climate change.
Scope and preparation of the reports.
The IPCC does not carry out research nor does it monitor climate related data. Lead authors of IPCC reports assess the available information about climate change based on published sources. According to IPCC guidelines, authors should give priority to peer-reviewed sources. Authors may refer to non-peer-reviewed sources (the "grey literature"), provided that they are of sufficient quality. Examples of non-peer-reviewed sources include model results, reports from government agencies and non-governmental organizations, and industry journals. Each subsequent IPCC report notes areas where the science has improved since the previous report and also notes areas where further research is required.
There are generally three stages in the review process:
Review comments are in an open archive for at least five years.
There are several types of endorsement which documents receive:
The Panel is responsible for the IPCC and its endorsement of Reports allows it to ensure they meet IPCC standards.
There have been a range of commentaries on the IPCC's procedures, examples of which are discussed later in the article (see also IPCC Summary for Policymakers). Some of these comments have been supportive, while others have been critical. Some commentators have suggested changes to the IPCC's procedures.
Authors.
Each chapter has a number of authors who are responsible for writing and editing the material. A chapter typically has two "coordinating lead authors", ten to fifteen "lead authors", and a somewhat larger number of "contributing authors". The coordinating lead authors are responsible for assembling the contributions of the other authors, ensuring that they meet stylistic and formatting requirements, and reporting to the Working Group chairs. Lead authors are responsible for writing sections of chapters. Contributing authors prepare text, graphs or data for inclusion by the lead authors.
Authors for the IPCC reports are chosen from a list of researchers prepared by governments and participating organisations, and by the Working Group/Task Force Bureaux, as well as other experts known through their published work. The choice of authors aims for a range of views, expertise and geographical representation, ensuring representation of experts from developing and developed countries and countries with economies in transition.
First assessment report.
The IPCC first assessment report was completed in 1990, and served as the basis of the UNFCCC.
The executive summary of the WG I Summary for Policymakers report says they are certain that emissions resulting from human activities are substantially increasing the atmospheric concentrations of the greenhouse gases, resulting on average in an additional warming of the Earth's surface. They calculate with confidence that CO2 has been responsible for over half the enhanced greenhouse effect. They predict that under a "business as usual" (BAU) scenario, global mean temperature will increase by about 0.3 °C per decade during the century. They judge that global mean surface air temperature has increased by 0.3 to 0.6 °C over the last 100 years, broadly consistent with prediction of climate models, but also of the same magnitude as natural climate variability. The unequivocal detection of the enhanced greenhouse effect is not likely for a decade or more.
Supplementary report of 1992.
The 1992 supplementary report was an update, requested in the context of the negotiations on the UNFCCC at the Earth Summit (United Nations Conference on Environment and Development) in Rio de Janeiro in 1992.
The major conclusion was that research since 1990 did "not affect our fundamental understanding of the science of the greenhouse effect and either confirm or do not justify alteration of the major conclusions of the first IPCC scientific assessment". It noted that transient (time-dependent) simulations, which had been very preliminary in the FAR, were now improved, but did not include aerosol or ozone changes.
Second assessment report.
"Climate Change 1995", the IPCC Second Assessment Report (SAR), was finished in 1996. It is split into four parts:
Each of the last three parts was completed by a separate working group, and each has a Summary for Policymakers (SPM) that represents a consensus of national representatives. The SPM of the WG I report contains headings:
Third assessment report.
The Third Assessment Report (TAR) was completed in 2001 and consists of four reports, three of them from its working groups:
A number of the TAR's conclusions are given quantitative estimates of how probable it is that they are correct, e.g., greater than 66% probability of being correct. These are "Bayesian" probabilities, which are based on an expert assessment of all the available evidence.
"Robust findings" of the TAR Synthesis Report include:
Comments on the TAR.
In 2001, 16 national science academies issued a joint statement on climate change.
The joint statement was made by the Australian Academy of Science, the Royal Flemish Academy of Belgium for Science and the Arts, the Brazilian Academy of Sciences, the Royal Society of Canada, the Caribbean Academy of Sciences, the Chinese Academy of Sciences, the French Academy of Sciences, the German Academy of Natural Scientists Leopoldina, the Indian National Science Academy, the Indonesian Academy of Sciences, the Royal Irish Academy, Accademia Nazionale dei Lincei (Italy), the Academy of Sciences Malaysia, the Academy Council of the Royal Society of New Zealand, the Royal Swedish Academy of Sciences, and the Royal Society (UK).
The statement, also published as an editorial in the journal Science, stated "we support the [TAR's] conclusion that it is at least 90% certain that temperatures will continue to rise, with average global surface temperature projected to increase by between 1.4 and 5.8 °C above 1990 levels by 2100".
The TAR has also been endorsed by the Canadian Foundation for Climate and Atmospheric Sciences, Canadian Meteorological and Oceanographic Society, and European Geosciences Union (refer to "Endorsements of the IPCC").
In 2001, the US National Research Council (US NRC) produced a report that assessed Working Group I's (WGI) contribution to the TAR. US NRC (2001) "generally agrees" with the WGI assessment, and describes the full WGI report as an "admirable summary of research activities in climate science".
IPCC author Richard Lindzen has made a number of criticisms of the TAR. Among his criticisms, Lindzen has stated that the WGI Summary for Policymakers (SPM) does not faithfully summarize the full WGI report. For example, Lindzen states that the SPM understates the uncertainty associated with climate models. John Houghton, who was a co-chair of TAR WGI, has responded to Lindzen's criticisms of the SPM. Houghton has stressed that the SPM is agreed upon by delegates from many of the world's governments, and that any changes to the SPM must be supported by scientific evidence.
IPCC author Kevin Trenberth has also commented on the WGI SPM. Trenberth has stated that during the drafting of the WGI SPM, some government delegations attempted to "blunt, and perhaps obfuscate, the messages in the report". However, Trenberth concludes that the SPM is a "reasonably balanced summary".
US NRC (2001) concluded that the WGI SPM and Technical Summary are "consistent" with the full WGI report. US NRC (2001) stated:
[...] the full report is adequately summarized in the Technical Summary. The full WGI report and its Technical Summary are not specifically directed at policy. The Summary for Policymakers reflects less emphasis on communicating the basis for uncertainty and a stronger emphasis on areas of major concern associated with human-induced climate change. This change in emphasis appears to be the result of a summary process in which scientists work with policy makers on the document. Written responses from U.S. coordinating and lead scientific authors to the committee indicate, however, that (a) no changes were made without the consent of the convening lead authors (this group represents a fraction of the lead and contributing authors) and (b) most changes that did occur lacked significant impact.
Fourth assessment report.
The Fourth Assessment Report (AR4) was published in 2007. Like previous assessment reports, it consists of four reports:
People from over 130 countries contributed to the IPCC Fourth Assessment Report, which took 6 years to produce. Contributors to AR4 included more than 2500 scientific expert reviewers, more than 800 contributing authors, and more than 450 lead authors.
"Robust findings" of the Synthesis report include:
Global warming projections from AR4 are shown below. The projections apply to the end of the 21st century (2090–99), relative to temperatures at the end of the 20th century (1980–99). Add 0.7 °C to projections to make them relative to pre-industrial levels instead of 1980–99. Descriptions of the greenhouse gas emissions scenarios can be found in Special Report on Emissions Scenarios.
"Likely" means greater than 66% probability of being correct, based on expert judgement.
Response to AR4.
Several science academies have referred to and/or reiterated some of the conclusions of AR4. These include:
The Netherlands Environmental Assessment Agency (PBL, "et al.", 2009; 2010) has carried out two reviews of AR4. These reviews are generally supportive of AR4's conclusions. PBL (2010) make some recommendations to improve the IPCC process. A literature assessment by the US National Research Council (US NRC, 2010) concludes:Climate change is occurring, is caused largely by human activities, and poses significant risks for—and in many cases is already affecting—a broad range of human and natural systems ["emphasis in original text"]. [...] This conclusion is based on a substantial array of scientific evidence, including recent work, and is consistent with the conclusions of recent assessments by the U.S. Global Change Research Program [...], the Intergovernmental Panel on Climate Change’s Fourth Assessment Report [...], and other assessments of the state of scientific knowledge on climate change.
Some errors have been found in the IPCC AR4 Working Group II report. Two errors include the melting of Himalayan glaciers (see later section), and Dutch land area that is below sea level.
Fifth assessment report.
The IPCC's Fifth Assessment Report (AR5) was completed in 2014. AR5 followed the same general format as of AR4, with three Working Group reports and a Synthesis report. The Working Group I report (WG1) was published in September 2013.
Conclusions of AR5 are summarized below:
Representative Concentration Pathways.
Projections in AR5 are based on "Representative Concentration Pathways" (RCPs). The RCPs are consistent with a wide range of possible changes in future anthropogenic greenhouse gas emissions. Projected changes in global mean surface temperature and sea level are given in the main RCP article.
Special reports.
In addition to climate assessment reports, the IPCC is publishing Special Reports on specific topics. The preparation and approval process for all IPCC Special Reports follows the same procedures as for IPCC Assessment Reports. In the year 2011 two IPCC Special Report were finalized, the Special Report on Renewable Energy Sources and Climate Change Mitigation (SRREN) and the Special Report on Managing Risks of Extreme Events and Disasters to Advance Climate Change Adaptation (SREX). Both Special Reports were requested by governments.
Special Report on Emissions Scenarios (SRES).
The Special Report on Emissions Scenarios (SRES) is a report by the IPCC which was published in 2000. The SRES contains "scenarios" of future changes in emissions of greenhouse gases and sulfur dioxide. One of the uses of the SRES scenarios is to project future changes in climate, e.g., changes in global mean temperature. The SRES scenarios were used in the IPCC's Third and Fourth Assessment Reports.
The SRES scenarios are "baseline" (or "reference") scenarios, which means that they do not take into account any current or future measures to limit greenhouse gas (GHG) emissions (e.g., the Kyoto Protocol to the United Nations Framework Convention on Climate Change). SRES emissions projections are broadly comparable in range to the baseline projections that have been developed by the scientific community.
Comments on the SRES.
There have been a number of comments on the SRES. Parson "et al." (2007) stated that the SRES represented "a substantial advance from prior scenarios". At the same time, there have been criticisms of the SRES.
The most prominently publicized criticism of SRES focused on the fact that all but one of the participating models compared gross domestic product (GDP) across regions using market exchange rates (MER), instead of the more correct purchasing-power parity (PPP) approach. This criticism is discussed in the main SRES article.
Special report on renewable energy sources and climate change mitigation (SRREN).
This report assesses existing literature on renewable energy commercialisation for the mitigation of climate change. It covers the six most important renewable energy technologies, as well as their integration into present and future energy systems. It also takes into consideration the environmental and social consequences associated with these technologies, the cost and strategies to overcome technical as well as non-technical obstacles to their application and diffusion.
More than 130 authors from all over the world contributed to the preparation of IPCC Special Report on Renewable Energy Sources and Climate Change Mitigation (SRREN) on a voluntary basis – not to mention more than 100 scientists, who served as contributing authors.
Special Report on managing the risks of extreme events and disasters to advance climate change adaptation (SREX).
The report assesses the effect that climate change has on the threat of natural disasters and how nations can better manage an expected change in the frequency of occurrence and intensity of severe weather patterns. It aims to become a resource for decision-makers to prepare more effectively for managing the risks of these events. A potentially important area for consideration is also the detection of trends in extreme events and the attribution of these trends to human influence.
More than 80 authors, 19 review editors, and more than 100 contributing authors from all over the world contributed to the preparation of SREX.
Methodology reports.
Within IPCC the National Greenhouse Gas Inventory Program develops methodologies to estimate emissions of greenhouse gases. This has been undertaken since 1991 by the IPCC WGI in close collaboration with the Organisation for Economic Co-operation and Development and the International Energy Agency.
The objectives of the National Greenhouse Gas Inventory Program are:
Revised 1996 IPCC Guidelines for National Greenhouse Gas Inventories.
The 1996 Guidelines for National Greenhouse Gas Investories provide the methodological basis for the estimation of national greenhouse gas emissions inventories. Over time these guidelines have been completed with good practice reports: "Good Practice Guidance and Uncertainty Management in National Greenhouse Gas Inventories" and "Good Practice Guidance for Land Use, Land-Use Change and Forestry".
The 1996 guidelines and the two good practice reports are to be used by parties to the UNFCCC and to the Kyoto Protocol in their annual submissions of national greenhouse gas inventories.
2006 IPCC Guidelines for National Greenhouse Gas Inventories.
The 2006 "IPCC Guidelines for National Greenhouse Gas Inventories" is the latest version of these emission estimation methodologies, including a large number of default emission factors. Although the IPCC prepared this new version of the guidelines on request of the parties to the UNFCCC, the methods have not yet been officially accepted for use in national greenhouse gas emissions reporting under the UNFCCC and the Kyoto Protocol.
Activities.
The IPCC concentrates its activities on the tasks allotted to it by the relevant WMO Executive Council and UNEP Governing Council resolutions and decisions as well as on actions in support of the UNFCCC process. While the preparation of the assessment reports is a major IPCC function, it also supports other activities, such as the Data Distribution Centre and the National Greenhouse Gas Inventories Programme, required under the UNFCCC. This involves publishing default emission factors, which are factors used to derive emissions estimates based on the levels of fuel consumption, industrial production and so on.
The IPCC also often answers inquiries from the UNFCCC Subsidiary Body for Scientific and Technological Advice (SBSTA).
Nobel Peace Prize.
In December 2007, the IPCC was awarded the Nobel Peace Prize "for their efforts to build up and disseminate greater knowledge about man-made climate change, and to lay the foundations for the measures that are needed to counteract such change". The award is shared with Former U.S. Vice-President Al Gore for his work on climate change and the documentary "An Inconvenient Truth".
Responses.
There is widespread support for the IPCC in the scientific community, which is reflected in publications by other scientific bodies and experts. However, criticisms of the IPCC have been made.
Since 2010 the IPCC has come under yet unparalleled public and political scrutiny. The global IPCC consensus approach has been challenged internally and externally with the 2009 Climatic Research Unit email controversy ("Climategate") an important (but not sole) threshold. It has been deemed an information monopoly with results for both the quality and the impact of the IPCC work as such.
Projected date of melting of Himalayan glaciers.
A paragraph in the 2007 Working Group II report ("Impacts, Adaptation and Vulnerability"), chapter 10 included a projection that Himalayan glaciers could disappear by 2035
This projection was not included in the final summary for policymakers. The IPCC has since acknowledged that the date is incorrect, while reaffirming that the conclusion in the final summary was robust. They expressed regret for "the poor application of well-established IPCC procedures in this instance". The date of 2035 has been correctly quoted by the IPCC from the WWF report, which has misquoted its own source, an ICSI report "Variations of Snow and Ice in the past and at present on a Global and Regional Scale".
Rajendra K. Pachauri responded in an interview with "Science".
Watson criticism.
Former IPCC chairman Robert Watson has said "The mistakes all appear to have gone in the direction of making it seem like climate change is more serious by overstating the impact. That is worrying. The IPCC needs to look at this trend in the errors and ask why it happened". Martin Parry, a climate expert who had been co-chair of the IPCC working group II, said that "What began with a single unfortunate error over Himalayan glaciers has become a clamour without substance" and the IPCC had investigated the other alleged mistakes, which were "generally unfounded and also marginal to the assessment".
Emphasis of the "hockey stick" graph.
The third assessment report (TAR) prominently featured a graph labeled "Millennial Northern Hemisphere temperature reconstruction" based on a 1999 paper by Michael E. Mann, Raymond S. Bradley and Malcolm K. Hughes (MBH99), which has been referred to as the "hockey stick graph". This graph extended the similar graph in Figure 3.20 from the IPCC Second Assessment Report of 1995, and differed from a schematic in the first assessment report that lacked temperature units, but appeared to depict larger global temperature variations over the past 1000 years, and higher temperatures during the Medieval Warm Period than the mid 20th century. The schematic was not an actual plot of data, and was based on a diagram of temperatures in central England, with temperatures increased on the basis of documentary evidence of Medieval vineyards in England. Even with this increase, the maximum it showed for the Medieval Warm Period did not reach temperatures recorded in central England in 2007. The MBH99 finding was supported by cited reconstructions by , , and , using differing data and methods. The Jones et al. and Briffa reconstructions were overlaid with the MBH99 reconstruction in Figure 2.21 of the IPCC report.
These studies were widely presented as demonstrating that the current warming period is exceptional in comparison to temperatures between 1000 and 1900, and the MBH99 based graph featured in publicity. Even at the draft stage, this finding was disputed by contrarians: in May 2000 Fred Singer's Science and Environmental Policy Project held a press event on Capitol Hill, Washington, D.C., featuring comments on the graph Wibjörn Karlén and Singer argued against the graph at a United States Senate Committee on Commerce, Science and Transportation hearing on 18 July 2000. Contrarian John Lawrence Daly featured a modified version of the IPCC 1990 schematic, which he mis-identified as appearing in the IPCC 1995 report, and argued that "Overturning its own previous view in the 1995 report, the IPCC presented the 'Hockey Stick' as the new orthodoxy with hardly an apology or explanation for the abrupt U-turn since its 1995 report". Criticism of the MBH99 reconstruction in a review paper, which was quickly discredited in the Soon and Baliunas controversy, was picked up by the Bush administration, and a Senate speech by US Republican senator James Inhofe alleged that "manmade global warming is the greatest hoax ever perpetrated on the American people". The data and methodology used to produce the "hockey stick graph" was criticized in papers by Stephen McIntyre and Ross McKitrick, and in turn the criticisms in these papers were examined by other studies and comprehensively refuted by , which showed errors in the methods used by McIntyre and McKitrick.
On 23 June 2005, Rep. Joe Barton, chairman of the House Committee on Energy and Commerce wrote joint letters with Ed Whitfield, Chairman of the Subcommittee on Oversight and Investigations demanding full records on climate research, as well as personal information about their finances and careers, from Mann, Bradley and Hughes. Sherwood Boehlert, chairman of the House Science Committee, said this was a "misguided and illegitimate investigation" apparently aimed at intimidating scientists, and at his request the U.S. National Academy of Sciences arranged for its National Research Council to set up a special investigation. The National Research Council's report agreed that there were some statistical failings, but these had little effect on the graph, which was generally correct. In a 2006 letter to "Nature", Mann, Bradley, and Hughes pointed out that their original article had said that "more widespread high-resolution data are needed before more confident conclusions can be reached" and that the uncertainties were "the point of the article".
The IPCC Fourth Assessment Report (AR4) published in 2007 featured a graph showing 12 proxy based temperature reconstructions, including the three highlighted in the 2001 Third Assessment Report (TAR); as before, and had both been calibrated by newer studies. In addition, analysis of the Medieval Warm Period cited reconstructions by (as cited in the TAR) and . Ten of these 14 reconstructions covered 1,000 years or longer. Most reconstructions shared some data series, particularly tree ring data, but newer reconstructions used additional data and covered a wider area, using a variety of statistical methods. The section discussed the divergence problem affecting certain tree ring data.
Conservative nature of IPCC reports.
Some critics have contended that the IPCC reports tend to underestimate dangers, understate risks, and report only the "lowest common denominator" findings.
On 1 February 2007, the eve of the publication of IPCC's major report on climate, a study was published suggesting that temperatures and sea levels have been rising at or above the maximum rates proposed during the last IPCC report in 2001. The study compared IPCC 2001 projections on temperature and sea level change with observations. Over the six years studied, the actual temperature rise was near the top end of the range given by IPCC's 2001 projection, and the actual sea level rise was above the top of the range of the IPCC projection.
Another example of scientific research which suggests that previous estimates by the IPCC, far from overstating dangers and risks, have actually understated them is a study on projected rises in sea levels. When the researchers' analysis was "applied to the possible scenarios outlined by the Intergovernmental Panel on Climate Change (IPCC), the researchers found that in 2100 sea levels would be 0.5–1.4 m cm above 1990 levels. These values are much greater than the 9–88 cm as projected by the IPCC itself in its Third Assessment Report, published in 2001". This may have been due, in part, to the expanding human understanding of climate.
In reporting criticism by some scientists that IPCC's then-impending January 2007 report understates certain risks, particularly sea level rises, an AP story quoted Stefan Rahmstorf, professor of physics and oceanography at Potsdam University as saying "In a way, it is one of the strengths of the IPCC to be very conservative and cautious and not overstate any climate change risk".
In his December 2006 book, "", and in an interview on Fox News on 31 January 2007, energy expert Joseph Romm noted that the IPCC Fourth Assessment Report is already out of date and omits recent observations and factors contributing to global warming, such as the release of greenhouse gases from thawing tundra.
Political influence on the IPCC has been documented by the release of a memo by ExxonMobil to the Bush administration, and its effects on the IPCC's leadership. The memo led to strong Bush administration lobbying, evidently at the behest of ExxonMobil, to oust Robert Watson, a climate scientist, from the IPCC chairmanship, and to have him replaced by Pachauri, who was seen at the time as more mild-mannered and industry-friendly.
IPCC processes.
Michael Oppenheimer, a long-time participant in the IPCC and coordinating lead author of the Fifth Assessment Report conceded in Science Magazine's State of the Planet 2008-2009 some limitations of the IPCC consensus approach and asks for concurring, smaller assessments of special problems instead of the large scale approach as in the previous IPCC assessment reports. It has become more important to provide a broader exploration of uncertainties. Others see as well mixed blessings of the drive for consensus within the IPCC process and ask to include dissenting or minority positions or to improve statements about uncertainties.
The IPCC process on climate change and its efficiency and success has been compared with dealings with other environmental challenges (compare Ozone depletion and global warming). In case of the Ozone depletion global regulation based on the Montreal Protocol has been successful, in case of Climate Change, the Kyoto Protocol failed. The Ozone case was used to assess the efficiency of the IPCC process.
The lockstep situation of the IPCC is having built a broad science consensus while states and governments still follow different, if not opposing goals. The underlying linear model of policy-making of "more knowledge we have, the better the political response will be" is being doubted.
According to Sheldon Ungar's comparison with global warming, the actors in the ozone depletion case had a better understanding of scientific ignorance and uncertainties. The ozone case communicated to lay persons "with easy-to-understand bridging metaphors derived from the popular culture" and related to "immediate risks with everyday relevance", while the public opinion on climate change sees no imminent danger. The stepwise mitigation of the ozone layer challenge was based as well on successfully reducing regional burden sharing conflicts. In case of the IPCC conclusions and the failure of the Kyoto Protocol, varying regional cost-benefit analysis and burden-sharing conflicts with regard to the distribution of emission reductions remain an unsolved problem. In the UK, a report for a House of Lords committee asked to urge the IPCC to involve better assessments of costs and benefits of climate change but the Stern Review ordered by the UK government made a stronger argument in favor to combat human-made climate change.
Outdatedness of reports.
Since the IPCC does not carry out its own research, it operates on the basis of scientific papers and independently documented results from other scientific bodies, and its schedule for producing reports requires a deadline for submissions prior to the report's final release. In principle, this means that any significant new evidence or events that change our understanding of climate science between this deadline and publication of an IPCC report cannot be included. In an area of science where our scientific understanding is rapidly changing, this has been raised as a serious shortcoming in a body which is widely regarded as the ultimate authority on the science. However, there has generally been a steady evolution of key findings and levels of scientific confidence from one assessment report to the next.
The submission deadlines for the Fourth Assessment Report (AR4) differed for the reports of each Working Group. Deadlines for the Working Group I report were adjusted during the drafting and review process in order to ensure that reviewers had access to unpublished material being cited by the authors. The final deadline for cited publications was 24 July 2006. The final WG I report was released on 30 April 2007 and the final AR4 Synthesis Report was released on 17 November 2007.
Rajendra Pachauri, the IPCC chair, admitted at the launch of this report that since the IPCC began work on it, scientists have recorded "much stronger trends in climate change", like the unforeseen dramatic melting of polar ice in the summer of 2007, and added, "that means you better start with intervention much earlier".
Burden on participating scientists.
Scientists who participate in the IPCC assessment process do so without any compensation other than the normal salaries they receive from their home institutions. The process is labor-intensive, diverting time and resources from participating scientists' research programs. Concerns have been raised that the large uncompensated time commitment and disruption to their own research may discourage qualified scientists from participating.
In May 2010, Pachauri noted that the IPCC currently had no process for responding to errors or flaws once it issued a report. The problem, according to Pachauri, was that once a report was issued the panels of scientists producing the reports were disbanded.
Proposed organizational overhaul.
In February 2010, in response to controversies regarding claims in the Fourth Assessment Report, five climate scientists – all contributing or lead IPCC report authors – wrote in the journal "Nature" calling for changes to the IPCC. They suggested a range of new organizational options, from tightening the selection of lead authors and contributors, to dumping it in favor of a small permanent body, or even turning the whole climate science assessment process into a moderated "living" Wikipedia-IPCC. Other recommendations included that the panel employ a full-time staff and remove government oversight from its processes to avoid political interference.
InterAcademy Council review.
In March 2010, at the invitation of the United Nations secretary-general and the chair of the IPCC, the InterAcademy Council (IAC) was asked to review the IPCC's processes for developing its reports. The IAC panel, chaired by Harold Tafler Shapiro, convened on 14 May 2010 and released its report on 1 September 2010.
The IAC found that, "The IPCC assessment process has been successful overall". The panel, however, made seven formal recommendations for improving the IPCC's assessment process, including:
The panel also advised that the IPCC avoid appearing to advocate specific policies in response to its scientific conclusions. Commenting on the IAC report, "Nature News" noted that "The proposals were met with a largely favourable response from climate researchers who are eager to move on after the media scandals and credibility challenges that have rocked the United Nations body during the past nine months".
Endorsements of the IPCC.
Various scientific bodies have issued official statements endorsing and concurring with the findings of the IPCC.

</doc>
<doc id="15031" url="https://en.wikipedia.org/wiki?curid=15031" title="IPCC (disambiguation)">
IPCC (disambiguation)

IPCC may refer to:

</doc>
<doc id="15032" url="https://en.wikipedia.org/wiki?curid=15032" title="IBM Personal Computer">
IBM Personal Computer

The IBM Personal Computer, commonly known as the IBM PC, is the original version and progenitor of the IBM PC compatible hardware platform. It is IBM model number 5150, and was introduced on August 12, 1981. It was created by a team of engineers and designers under the direction of Don Estridge of the IBM Entry Systems Division in Boca Raton, Florida.
The generic term "personal computer" was in use before 1981, applied as early as 1972 to the Xerox PARC's Alto, but because of the success of the IBM Personal Computer, the term "PC" came to mean more specifically a desktop microcomputer compatible with IBM's PC products. Within a short time of the introduction, third-party suppliers of peripheral devices, expansion cards, and software proliferated; the influence of the IBM PC on the personal computer market was substantial in standardizing a platform for personal computers. ""IBM compatible"" became an important criterion for sales growth; only the Apple Macintosh family kept significant market share without compatibility with the IBM personal computer.
History.
Rumors.
International Business Machines (IBM), one of the world's largest companies, had a 62% share of the mainframe computer market in 1981. Its share of the overall computer market, however, had declined from 60% in 1970 to 32% in 1980. Perhaps distracted by , the "Colossus of Armonk" completely missed the fast-growing minicomputer market during the 1970s, and was behind rivals such as Wang, Hewlett-Packard (HP), and Control Data in other areas.
In 1979 "BusinessWeek" asked "Is IBM just another stodgy, mature company?" By 1981 its stock price had declined by 22%. IBM's earnings for the first half the year grew by 5.3%—one third of the inflation rate—while those of minicomputer maker Digital Equipment Corporation (DEC) grew by more than 35%. The company began selling minicomputers, but in January 1982 the United States Department of Justice ended the antitrust suit because, "The New York Times" reported, the government "recognized what computer experts and securities analysts had long since concluded: I.B.M. no longer dominates the computer business".
IBM wished to avoid the same outcome with the new personal computer industry, dominated by the Commodore PET, Atari 8-bit family, Apple II, Tandy Corporation's TRS-80, and various CP/M machines. With $150 million in sales by 1979 and projected annual growth of more than 40% in the early 1980s, the microcomputer market was large enough for IBM's attention. Other large technology companies such as HP, Texas Instruments, and Data General had entered it, and some large IBM customers were buying Apples, so the company saw introducing its own personal computer as both an experiment in a new market and a defense against rivals, large and small.
In 1980 and 1981 rumors spread of an IBM personal computer, perhaps a miniaturized version of the IBM System/370, while Matsushita acknowledged that it had discussed with IBM the possibility of manufacturing a personal computer for the American company. The Japanese project, codenamed "Go", ended before the 1981 release of the American-designed IBM PC codenamed "Chess", but two simultaneous projects further confused rumors about the forthcoming product.
Too late?
Data General and Texas Instruments' small computers were not very successful, but observers expected AT&T to soon enter the computer industry, and other large companies such as Exxon, Montgomery Ward, Pentel, and Sony were designing their own microcomputers. Whether IBM had waited too long to enter an industry Apple and others were already successful in was unclear.
An observer stated that "IBM bringing out a personal computer would be like teaching an elephant to tap dance." Successful microcomputer company Vector Graphic's fiscal 1980 revenue was $12 million. A single IBM computer in the early 1960s cost as much as $9 million, occupied one quarter acre of air-conditioned space, and had a staff of 60 people; in 1980 its least-expensive computer, the 5120, still cost about $13,500. The company only sold through its internal sales force, had no experience with resellers or retail stores, and did not introduce the first product designed to work with non-IBM equipment until 1980.
Another observer claimed that IBM made decisions so slowly that, when tested, "what they found is that it would take at least nine months to ship an empty box". As with other large computer companies, its new products typically required about four to five years for development. IBM had to learn how to quickly develop, mass-produce, and market new computers. While the company traditionally let others pioneer a new market—IBM released its first commercial computer a year after Remington Rand's UNIVAC in 1951, but within five years had 85% of the market—the personal-computer development and pricing cycles were much faster than for mainframes, with products designed in a few months and obsolete quickly.
Many in the microcomputer industry resented IBM's power and wealth, and disliked the perception that an industry founded by startups needed a latecomer so staid that it had a strict dress code and employee songbook. The potential importance to microcomputers of a company so prestigious, that a popular saying in American companies stated "No one ever got fired for buying IBM", was nonetheless clear. "InfoWorld", which described itself as "The Newsweekly for Microcomputer Users", stated that "for my grandmother, and for millions of people like her, "IBM " and "computer" are synonymous". "BYTE" ("The Small Systems Journal") stated in an editorial just before the announcement of the IBM PC:
The editorial acknowledged that "some factions in our industry have looked upon IBM as the 'enemy'", but concluded with optimism: "I want to see personal computing take a giant step."
Predecessors.
Desktop sized programmable calculators by Hewlett Packard had evolved into the HP 9830 BASIC language computer by 1972. In 1972–1973 a team led by Dr. Paul Friedl at the IBM Los Gatos Scientific Center developed a portable computer prototype called SCAMP (Special Computer APL Machine Portable) based on the IBM PALM processor with a Philips compact cassette drive, small CRT, and full-function keyboard. SCAMP emulated an IBM 1130 minicomputer to run APL\1130. In 1973 APL was generally available only on mainframe computers, and most desktop sized microcomputers such as the Wang 2200 or HP 9800 offered only BASIC. Because it was the first to emulate APL\1130 performance on a portable, single-user computer, "PC Magazine" in 1983 designated SCAMP a "revolutionary concept" and "the world's first personal computer". The prototype is in the Smithsonian Institution. A non-working industrial design model was also created in 1973 illustrating how the SCAMP engineering prototype could be transformed into a usable product design for the marketplace. IBM executive Bill Lowe used the engineering prototype and design model in his early efforts to demonstrate the viability of creating a single-user computer.
Successful demonstrations of the 1973 SCAMP prototype led to the IBM 5100 portable microcomputer in 1975. In the late 1960s such a machine would have been nearly as large as two desks and would have weighed about half a ton. The 5100 was a complete computer system programmable in BASIC or APL, with a small built-in CRT monitor, keyboard, and tape drive for data storage. It was also very expensive, up to ; the computer was designed for professional and scientific customers, not business users or hobbyists. "BYTE" in 1975 announced the 5100 with the headline "Welcome, IBM, to personal computing", but "PC Magazine" in 1984 described 5100s as "little mainframes" and stated that "as personal computers, these machines were dismal failures ... the antithesis of user-friendly", with no IBM support for third-party software. Despite news reports that it was the first IBM product without a model number, when the PC was introduced in 1981 it was designated as the IBM 5150, putting it in the "5100" series though its architecture was not directly descended from the IBM 5100. Later models followed in the trend: For example, the PC/XT, IBM Portable Personal Computer, and PC AT are IBM machine types 5160, 5155, and 5170, respectively.
Following SCAMP, the IBM Boca Raton Laboratory created several single-user computer design concepts to support Lowe's ongoing effort to convince IBM there was a strategic opportunity in the personal computer business. A selection of these early IBM design concepts created in the infancy of personal computing is highlighted in the book ‘’DELETE: A Design History of Computer Vapourware.‘’ One such concept in 1977, code-named Aquarius, was a working prototype utilizing advanced bubble memory cartridges. While this design was more powerful and smaller than Apple II launched the same year, the advanced bubble technology was deemed unstable and not ready for mass production.
Project Chess.
Some employees opposed IBM entering the market. One said, "Why on earth would you care about the personal computer? It has nothing at all to do with office automation." "Besides", he added, "all it can do is cause embarrassment for IBM". The company had determined from studying the market for years, and building the prototypes during the 1970s, that IBM was unable to internally build a personal computer profitably.
IBM President John Opel was not among those skeptical of personal computers. He and CEO Frank Cary had created more than one dozen semi-autonomous "Independent Business Units" (IBU) to encourage innovation; "Fortune" called them "How to start your own company without leaving IBM". After Lowe became the first head of the Entry Level Systems IBU in Boca Raton his team researched the market. Computer dealers were very interested in selling an IBM product, but told Lowe that the company could not design, sell, or service it as IBM had previously done. An IBM microcomputer, they said, must be composed of standard parts that store employees could repair. While dealers disliked Apple's business practices, including a shortage of the Apple II while the company focused on the more sophisticated Apple III, they saw no alternative because they doubted that IBM's traditional sales methods and bureaucracy would change.
Atari in 1980 proposed that it act as original equipment manufacturer for an IBM microcomputer. Aware that the company needed to enter the market quickly—even the schools in Broward County, near Boca Raton, purchased Apples—in July 1980 Lowe met with Opel, Cary, and others on the important Corporate Management Committee. Lowe demonstrated the proposal with an industrial design model based on the Atari 800 platform, and suggested acquiring Atari "because we can't do this within the culture of IBM".
Cary agreed about the culture, observing that IBM would need "four years and three hundred people" to develop its own personal computer; Lowe, however, promised one in a year if done without traditional IBM methods. Instead of acquiring Atari, the committee allowed him to form an independent group of employees—"the Dirty Dozen", led by engineer Bill Sydnes—which, Lowe promised, could design a prototype in 30 days. The crude prototype barely worked when he demonstrated it in August, but Lowe presented a detailed business plan that proposed that the new computer have an open architecture, use non-proprietary components and software, and be sold through retail stores, all contrary to IBM practice.
The committee agreed that Lowe's approach was the most likely to succeed. With Opel's strong support, in October it approved turning the group into another IBU codenamed "Project Chess" to develop "Acorn", with unusually large funding to help achieve the goal of introducing the product within one year of the August demonstration. After Lowe's promotion Don Estridge became the head of Chess, and by January 1981 the team made its first demonstration of the computer within IBM. Other key members included Sydnes, Lewis Eggebrecht, David Bradley, Mark Dean, and David O'Connor. Many were already hobbyists who owned their own computers including Estridge, who had an Apple II. After the team received permission to expand to 150 by the end of 1980, it received more than 500 calls in one day from IBM employees interested in joining the IBU.
Open standards.
IBM normally was vertically integrated, internally developing all hardware and software and discouraging customers from purchasing third-party products compatible with IBM products. For the PC the company avoided doing so as much as possible; choosing, for example, to license Microsoft BASIC despite having a BASIC of its own for mainframes. Although the company denied doing so, many observers concluded that IBM intentionally emulated Apple when designing the PC. The many Apple II owners on the team influenced its decision to design the computer with an open architecture and publish technical information so others could create software and expansion slot peripherals.
Although the company knew that it could not avoid competition from third-party software on proprietary hardware—Digital Research released CP/M-86 for the IBM Displaywriter, for example—it considered using the IBM 801 RISC processor and its operating system, developed at the Thomas J. Watson Research Center in Yorktown Heights, New York. The 801 processor was more than an order of magnitude more powerful than the Intel 8088, and the operating system more advanced than the PC DOS 1.0 operating system from Microsoft. Ruling out an in-house solution made the team’s job much easier and may have avoided a delay in the schedule, but the ultimate consequences of this decision for IBM were far-reaching.
IBM had recently developed the Datamaster business microcomputer, which used a processor and other chips from Intel; familiarity with them and the immediate availability of the 8088 was a reason for choosing it for the PC. The 62-pin expansion bus slots were designed to be similar to the Datamaster slots. Differences from the Datamaster included avoiding an all-in-one design while limiting the computer's size so that it would still fit on a standard desktop with the keyboard (also similar to the Datamaster's), and 5.25" disk drives instead of 8". Delays due to in-house development of the Datamaster software was a reason why IBM chose Microsoft BASIC—already available for the 8088—and published available technical information to encourage third-party developers. IBM chose the 8088 over the similar but superior 8086 because Intel offered a better price on the former and could provide more units, and the 8088's 8-bit bus reduced the cost of the rest of the computer.
The design for the computer was essentially complete by April 1981, when the manufacturing team took over the project. IBM could not only use its own hardware and make a profit with "Acorn". To save time and money, the IBU built the machine with commercial off-the-shelf parts from original equipment manufacturers whenever possible, with assembly occurring in Boca Raton. The IBU would decide whether it would be more economical to "Make or Buy" each manufacturing step. Various IBM divisions for the first time competed with outsiders to build parts of the new computer; a North Carolina IBM factory built the keyboard, the Endicott, New York factory had to lower its bid for printed circuit boards, and a Taiwanese company built the monitor. The IBU chose an existing monitor from IBM Japan and an Epson printer. Because of the off-the-shelf parts only the system unit and keyboard has unique IBM industrial design elements, the IBM copyright appears in only the ROM BIOS and on the company logo, and the company reportedly received no patents on the PC.
Because the product would carry the IBM logo, the only corporate division the IBU could not bypass was the Quality Assurance Unit. Another aspect of IBM that did not change was its emphasis on secrecy. Those working on the project were under strict confidentiality agreements. When an individual mentioned in public on a Saturday that his company was working on software for a new IBM computer, IBM security appeared at the company on Monday to investigate the leak. Developers received prototype computers in boxes lined with lead to block X-rays and sealed with solder, and had to keep them in locked, windowless rooms; to develop software Microsoft emulated the PC on a DEC minicomputer and used the prototype for debugging. After the PC's debut, IBM Boca Raton employees continued to decline to discuss their jobs in public. One writer compared the "silence" after asking one about his role at the company to " at the Boston Marathon: the conversation is over".
Debut.
After developing it in 12 months—faster than any other hardware product in company history—IBM announced the Personal Computer on 12 August 1981. Pricing started at for a configuration with 16K RAM, Color Graphics Adapter, and no disk drives. The company intentionally set prices for it and other configurations that were comparable to those of Apple and other rivals; one analyst stated that IBM "has taken the gloves off", while the company said "we suggest PC's price invites comparison". Microsoft, Personal Software, and Peachtree Software were among the developers of nine launch titles, including EasyWriter and VisiCalc. In addition to the existing corporate sales force IBM opened its own Product Center retail stores. After studying Apple's successful distribution network, the company for the first time sold through others, ComputerLand and Sears Roebuck. Because retail stores receive revenue from repairing computers and providing warranty service, IBM broke a 70-year tradition by permitting and training non-IBM service personnel to fix the PC.
"BYTE" described IBM as having "the strongest marketing organization in the world", but the PC's marketing also differed from that of previous products. The company was aware of its corporate reputation among potential customers; an early advertisement began "Presenting the IBM of Personal Computers". The advertisements emphasized the novelty of an individual owning an IBM computer, describing "a product "you" may have a personal interest in" and asking readers to think of "'My own IBM computer. Imagine that' ... it's yours. For your business, your project, your department, your class, your family and, indeed, for yourself."
The Little Tramp.
After considering Alan Alda, Beverly Sills, Kermit the Frog, and Billy Martin as celebrity endorsers IBM chose Charlie Chaplin's The Little Tramp character—played by Billy Scudder—for a series of advertisements based on Chaplin's films. The very popular and award-winning $36-million marketing campaign made the star of "Modern Times"—a film that expresses Chaplin's opposition to big business, mechanization, and technological efficiency—the (as "Creative Computing" described him) "warm cuddly" mascot of one of the world's largest companies.
Chaplin and his character became so widely associated with IBM—"Time" stated that "The Tramp ... has given a human face"—that others used his bowler hat and cane to represent or satirize the company. Although the Chaplin estate sued those like Otrona who used the trademark without permission, "PC Magazine"s April 1983 issue had 12 advertisements that referred to the Little Tramp.
Third-party products.
"We encourage third-part suppliers the PC ... we are delighted to have them", IBM stated. It did not sell internally developed PC software until April 1984, instead relying on already established software companies. The company contacted Microsoft even before the official approval of Chess, and it and others received cooperation that was, one writer said, ""unheard of"" for IBM. Such openness surprised observers; "BYTE" called it "striking" and "startling", and one developer reported that "it's a very different IBM." Another said "They were very open and helpful about giving us all the technical information we needed. The feeling was so radically different—it's like stepping out into a warm breeze." He concluded, "After years of hassling—fighting the Not-Invented-Here attitude—we're the gods."
Most other personal-computer companies that did not disclose technical details; Texas Instruments, for example, intentionally made developing third-party TI 99/4A software difficult, even requiring a lockout chip in cartridges. IBM itself kept its mainframe technology so secret that rivals were indicted for industrial espionage. For the PC, however, IBM immediately released detailed information. The US$36 "IBM PC Technical Reference Manual" included complete circuit schematics, commented ROM BIOS source code, and other engineering and programming information for all of IBM's PC-related hardware, plus instructions on designing third-party peripherals. It was so comprehensive that one reviewer suggested that the manual could serve as a university textbook, and so clear that a developer claimed that he could design an expansion card without seeing the physical computer.
IBM marketed the technical manual in full-page color print advertisements, stating that "our software story is still being written. Maybe by you". Sydnes stated that "The definition of a personal computer "is" third-party hardware and software". Estridge said that IBM did not keep software development proprietary because it could not "out-BASIC Microsoft BASIC. We would have to ... out-VisiCalc VisiCorp and out-Peachtree Peachtree—and you just can't do that", and unlike IBM's own BASIC "Microsoft BASIC had hundreds of thousands of users around the world. How are you going to argue with that?"
Another advertisement told developers that the company would consider publishing software for "Education. Entertainment. Personal finance. Data management. Self-improvement. Games. Communications. And yes, business." Estridge explicitly invited small, "cottage" amateur and professional developers to create products "with", he said, "our logo and our support". IBM sold the PC at a large discount to employees, encouraged them to write software, and distributed a catalog of inexpensive software written by individuals that might not otherwise appear in public.
Reaction.
"BYTE" was correct in predicting that an IBM personal computer would receive much public attention. Its rapid development amazed observers, as did the willingness of the Colossus of Armonk to sell as a launch title Microsoft "Adventure" (a video game that, its press release stated, brought "players into a fantasy world of caves and treasures"); the company even offered an optional joystick port. Future Computing estimated that "IBM's Billion Dollar Baby" would have $2.3 billion in hardware sales by 1986. David Bunnell, an editor at Osborne/McGraw-Hill, recalled that
Within seven weeks Bunnell helped found "PC Magazine", the first periodical for the new computer.
Competitors were more skeptical. Adam Osborne said "when you buy a computer from IBM, you buy a la carte. By the time you have a computer that does anything, it will cost more than an Apple. I don't think Apple has anything to worry about." Apple's Mike Markkula agreed that IBM's product was more expensive than the Apple II, and claimed that the Apple III "offers better performance". He denied that the IBM PC offered more memory, stating that his company could offer more than 128K "but frankly we don't know what anyone would do with that memory". Jon Shirley of Tandy admitted that IBM had a "legendary service reputation" but claimed that its thousands of Radio Shack stores "can provide better service", while predicting the IBM PC's "major market will be IBM addicts"; another executive claimed that Tandy could undersell a $3,000 IBM computer by $1,000. Many criticized the PC's design as not innovative and outdated, and believed that its alleged weaknesses, such as the use of single-sided, single-density disks with less storage than the computer's RAM, existed because the company was uncertain about the market and was experimenting before releasing a better computer. (Estridge later boasted, "Many ... said that there was nothing technologically new in this machine. That was the best news we could have had; we actually had done what we had set out to do.")
Rivals such as Apple, Tandy, and Commodore—together with more than 50% of the personal-computer market—had many advantages. While IBM began with one microcomputer, little available hardware or software, and a couple of hundred dealers, Radio Shack had 14 million customers and 8,000 stores—more than McDonald's—that only sold its broad range of computers and accessories. Apple had five times as many dealers in the US as IBM, an established international distribution network, and an installed base of more than 250,000 customers. Hundreds of independent developers produced software and peripherals for both companies' computers; at least ten Apple databases and ten word processors were available, while the PC had no databases and one word processor.
Steve Jobs at Apple ordered a team to examine an IBM PC. After finding it unimpressive—Chris Espinosa called the computer "a half-assed, hackneyed attempt"—the company confidently purchased a full-page advertisement in "The Wall Street Journal" with the headline "Welcome, IBM. Seriously". Microsoft head Bill Gates was at Apple headquarters the day of IBM's announcement and later said "They didn't seem to care. It took them a full year to realize what had happened".
Success.
The IBM PC was immediately successful. "BYTE" reported a rumor that more than 40,000 were ordered on the day of the announcement; John Dvorak recalled that one dealer that day praised the computer as an "incredible winner, and IBM knows how to treat us — none of the Apple arrogance". One dealer received 22 $1,000 deposits from customers although he could not promise a delivery date. The company could have sold its entire projected first-year production to employees, and IBM customers that were reluctant to purchase Apples were glad to buy microcomputers from its traditional supplier. By October some referred to the computer simply as the "PC".
"BYTE" estimated that 90% of the 40,000 first-day orders were from software developers. By COMDEX in November Tecmar developed 20 products including memory expansion and expansion chassis, surprising even IBM. Jerry Pournelle reported after attending the West Coast Computer Faire in early 1982 that because IBM "encourages amateurs" with "documents that tell all", "an explosion of [third-party] hardware and software" was visible at the convention. Many manufacturers of professional business application software, who had been planning/developing versions for the Apple II, promptly switched their efforts over to the IBM PC when it was announced. Often, these products needed the capacity and speed of a hard-disk. Although IBM did not offer a hard-disk option for almost two years following introduction of its PC, business sales were nonetheless catalyzed by the simultaneous availability of hard-disk subsystems, like those of Tallgrass Technologies which sold in Computerland stores alongside the IBM 5150 at the introduction in 1981.
Although IBM sold fewer than 100,000 PCs in its first year, "PC World" counted 753 software packages for the PC—more than four times the number available for the Apple Macintosh one year after its 1984 release—including 422 applications and almost 200 utilities and languages. "InfoWorld" reported that "most of the major software houses have been frantically adapting their programs to run on the PC", with new PC-specific developers composing "an entire subindustry that has formed around the PC's open system", which Dvorak described as a "de facto standard microcomputer". The magazine estimated that "hundreds of tiny garage-shop operations" were in "bloodthirsty" competition to sell peripherals, with 30 to 40 companies in a price war for memory-expansion cards, for example. "PC Magazine" renamed its planned "1001 Products to Use with Your IBM PC" special issue after the number of product listings it received exceeded the figure. Tecmar and other companies that benefited from IBM's openness rapidly grew in size and importance, as did "PC Magazine"; within two years it expanded from 96 bimonthly to 800 monthly pages, including almost 500 pages of advertisements.
By the end of 1982 IBM was selling one PC every minute of the business day. It estimated that 50 to 70% of PCs sold in retail stores went to the home, and the publicity from selling a popular product to consumers caused IBM to, a spokesman said, "enter the world" by familiarizing them with the Colossus of Armonk. Although the PC only provided two to three percent of sales the company found that it had underestimated demand by as much as 800%. Because its prices were based on forecasts of much lower volume—250,000 over five years, which would have made the PC a very successful IBM product—the PC became very profitable; at times the company sold almost that many computers per month. Estridge claimed in 1983 that from October 1982 to March 1983 customer demand quadrupled. He stated that the company had increased production three times in one year, and warned of a component shortage if demand continued to increase.
By mid-1983 Yankee Group estimated that ten new IBM PC-related products appeared every day. In August 1983 the Chess IBU, with 4,000 employees, became the Entry Systems Division, which observers believed indicated that the PC was significantly important to IBM overall, and no longer an experiment. The PC surpassed the Apple II as the best-selling personal computer with more than 750,000 sold by the end of the year, while DEC only sold 69,000 microcomputers in the first nine months of the year despite offering three models for different markets. Retailers also benefited, with 65% of BusinessLand's revenue coming from the PC. Demand still so exceeded supply two years after its debut that, despite IBM shipping 40,000 PCs a month, dealers reportedly received 60% or less of their desired quantity. Pournelle received the PC he paid for in early July 1983 on 1 November, and IBM Boca Raton employees and neighbors had to wait five weeks to buy the computers assembled there.
Domination.
Yankee Group also stated that the PC had by 1983 "destroyed the market for some older machines" from companies like Vector Graphic, North Star, and Cromemco. "inCider" wrote "This may be an Apple magazine, but let's not kid ourselves, IBM has devoured competitors like a cloud of locusts". By February 1984 "BYTE" reported on "the phenomenal market acceptance of the IBM PC", and by fall concluded that the company "has given the field its third major standard, after the Apple II and CP/M".
By then Apple was less welcoming of the rival that "inCider" stated had a "godlike" reputation. Its focus on the III had delayed improvements to the II, and the sophisticated Lisa was unsuccessful in part because, unlike the II and the PC, Apple discouraged third-party developers. The head of a retail chain said "It appears that IBM had a better understanding of why the Apple II was successful than had Apple." Jobs, after trying to recruit Estridge to become Apple's president, admitted that in two years IBM had joined Apple as "the industry's two strongest competitors". He warned in a speech before previewing the forthcoming "1984" Super Bowl commercial: "It appears IBM wants it "all" ... Will Big Blue dominate the entire computer industry? The entire information age? Was George Orwell right about 1984?"
IBM had $4 billion in annual PC revenue by 1984, more than twice that of Apple and as much as the sales of Apple, Commodore, HP, and Sperry combined, and 6% of total revenue. A "Fortune" survey found that 56% of American companies with personal computers used IBM PCs, compared to Apple's 16%. A 1983 study of corporate customers similarly found that two thirds of large customers standardizing on one computer chose the PC, compared to 9% for Apple. IBM's own documentation described the PC as inferior to competitors' less-expensive products, but the company generally did not compete on price; rather, the study found that they preferred "IBM's hegemony" because of its support. Most companies with mainframes used their PCs with the larger computers, which likely benefited IBM's mainframe sales and discouraged their purchasing non-IBM hardware.
In 1984 IBM introduced the PC/AT, unlike its predecessor the most sophisticated personal computer from any major company. By 1985 the PC family had more than doubled Future Computing's 1986 revenue estimate, with more than 12,000 applications and 4,500 dealers and distributors worldwide. In his obituary that year, "The New York Times" wrote that Estridge had led the "extraordinarily successful entry of the International Business Machines Corporation into the personal computer field". The Entry Systems Division had 10,000 employees and by itself would have been the world's third-largest computer company behind IBM and DEC, with more revenue than IBM's minicomputer business despite its much later start. IBM was the only major company with significant minicomputer and microcomputer businesses, in part because rivals like DEC and Wang did not adjust to the retail market.
Rumors of "lookalike", compatible computers, created without IBM's approval, began almost immediately after the IBM PC's release. Other manufacturers soon reverse engineered the BIOS to produce their own non-infringing functional copies. Columbia Data Products introduced the first IBM-PC compatible computer in June 1982. In November 1982, Compaq Computer Corporation announced the "Compaq Portable", the first portable IBM PC compatible. The first models were shipped in January 1983.
IBM PC as standard.
The success of the IBM computer led other companies to develop "IBM Compatibles", which in turn led to branding like diskettes being advertised as "IBM format". An IBM PC clone could be built with off-the-shelf parts, but the BIOS required some reverse engineering. Companies like Compaq, Phoenix Software Associates, American Megatrends, Award, and others achieved fully functional versions of the BIOS, allowing companies like DELL, Gateway and HP to manufacture PCs that worked like IBM's product. The IBM PC became the industry standard.
Third-party distribution.
Because IBM had no retail experience, the retail chains ComputerLand and Sears Roebuck provided important knowledge of the marketplace. They became the main outlets for the new product. More than 190 Computerland stores already existed, while Sears was in the process of creating a handful of in-store computer centers for sale of the new product. This guaranteed IBM widespread distribution across the U.S.
Targeting the new PC at the home market, Sears Roebuck sales failed to live up to expectations. This unfavorable outcome revealed that the strategy of targeting the office market was the key to higher sales.
Models.
All IBM personal computers are software backwards-compatible with each other in general, but not every program will work in every machine. Some programs are time sensitive to a particular speed class. Older programs will not take advantage of newer higher-resolution and higher-color display standards, while some newer programs require newer display adapters. (Note that as the display adapter was an adapter card in all of these IBM models, newer display hardware could easily be, and often was, retrofitted to older models.) A few programs, typically very early ones, are written for and require a specific version of the IBM PC BIOS ROM. Most notably, BASICA which was dependent on the BIOS ROM had a sister program called GW-BASIC which supported more functions, was 100% backwards compatible and could run independently from the BIOS ROM.
Original PC.
The CGA video card, with a suitable modulator, could use an NTSC television set or an RGBi monitor for display; IBM's RGBi monitor was their display model 5153. The other option that was offered by IBM was an MDA and their monochrome display model 5151. It was possible to install both an MDA and a CGA card and use both monitors concurrently if supported by the application program. For example, AutoCAD, Lotus 1-2-3 and others allowed use of a CGA Monitor for graphics and a separate monochrome monitor for text menus. Some model 5150 PCs with CGA monitors and a printer port also included the MDA adapter by default, because IBM provided the MDA port and printer port on the same adapter card; it was in fact an MDA/printer port combo card.
Although cassette tape was originally envisioned by IBM as a low-budget storage alternative, the most commonly used medium was the floppy disk. The 5150 was available with one or two 5-1/4" floppy drives - with two drives the program disc(s) would be in drive A, while drive B would hold the disc(s) for working files; with one drive the user had to swap program and file discs into the single drive. For models without any drives or storage medium, IBM intended users to connect their own cassette recorder via the 5150's cassette socket. The cassette tape socket was physically the same DIN plug as the keyboard socket and next to it, but electrically completely different.
A hard disk could not be installed into the 5150's system unit without changing to a higher-rated power supply (although later drives with lower power consumption have been known to work with the standard 63.5 Watt unit). The "IBM 5161 Expansion Chassis" came with its own power supply and one 10 MB hard disk and allowed the installation of a second hard disk. The system unit had five expansion slots, and the expansion unit had eight; however, one of the system unit's slots and one of the expansion unit's slots had to be occupied by the Extender Card and Receiver Card, respectively, which were needed to connect the expansion unit to the system unit and make the expansion unit's other slots available, for a total of 11 slots. A working configuration required that some of the slots be occupied by display, disk, and I/O adapters, as none of these were built into the 5150's motherboard; the only motherboard external connectors were the keyboard and cassette ports.
The simple PC speaker sound hardware was also on board.
The original PC's maximum memory using IBM parts was 256 kB, achievable through the installation of 64 kB on the motherboard and three 64 kB expansion cards. The processor was an Intel 8088 running at 4.77 MHz, 4/3 the standard NTSC color burst frequency of 315/88 = 3.579 MHz. (In early units, the Intel 8088 used was a 1978 version, later were 1978/81/2 versions of the Intel chip; second-sourced AMDs were used after 1983). Some owners replaced the 8088 with an NEC V20 for a slight increase in processing speed and support for real mode 80186 instructions. The V20 gained its speed increase through the use of a hardware multiplier which the 8088 lacked. An Intel 8087 co-processor could also be added for hardware floating-point arithmetic.
IBM sold the first IBM PCs in configurations with 16 or 64 kB of RAM preinstalled using either nine or thirty-six 16-kilobit DRAM chips. (The ninth bit was used for parity checking of memory.) After the IBM XT shipped, the IBM PC motherboard was configured more like the XTs motherboard with 8 narrower slots, as well as the same RAM configuration as the IBM XT. (64 kB in one bank, expandable to 256kB by populating the other 3 banks.)
Although the TV-compatible video board, cassette port and Federal Communications Commission Class B certification were all aimed at making it a home computer, the original PC proved too expensive for the home market. At introduction, a PC with 64 kB of RAM and a single 5.25-inch floppy drive and monitor sold for (), while the cheapest configuration () that had no floppy drives, only 16 kB RAM, and no monitor (again, under the expectation that users would connect their existing TV sets and cassette recorders) proved too unattractive and low-spec, even for its time (cf. footnotes to the above IBM PC range table). While the 5150 did not become a top selling home computer, its floppy-based configuration became an unexpectedly large success with businesses.
XT.
The "IBM Personal Computer XT", IBM model 5160, was introduced two years after the PC and featured a 10 megabyte hard drive. It had eight expansion slots but the same processor and clock speed as the PC. The XT had no cassette jack, but still had the Cassette Basic interpreter in ROMs.
The XT could take 256 kB of memory on the main board (using 64 kbit DRAM); later models were expandable to 640 kB. The remaining 384 kilobytes of the 8088 address space were used for the BIOS ROM, adapter ROM and RAM space, including video RAM space. It was usually sold with a Monochrome Display Adapter (MDA) video card or a CGA video card.
The eight expansion slots were the same as the model 5150 but were spaced closer together. Although rare, a card designed for the 5150 would be wide enough to obstruct the adjacent slot in an XT. Because of the spacing, an XT motherboard would not fit into a case designed for the PC motherboard, but the slots and peripheral cards were compatible. The XT expansion bus (later called "8 bit Industry Standard Architecture" (ISA) by competitors) was retained in the IBM AT, which added connectors for some slots to allow 16-bit transfers; 8 bit cards could be used in an AT.
XT/370.
The "IBM Personal Computer XT/370" was an XT with three custom 8-bit cards: the processor card (370PC-P) contained a modified Motorola 68000 chip, microcoded to execute System/370 instructions, a second 68000 to handle bus arbitration and memory transfers, and a modified 8087 to emulate the S/370 floating point instructions. The second card (370PC-M) connected to the first and contained 512 kB of memory. The third card (PC3277-EM), was a 3270 terminal emulator necessary to install the system software for the VM/PC software to run the processors.
The computer booted into DOS, then ran the VM/PC Control Program.
PCjr.
The "IBM PCjr" was IBM's first attempt to enter the market for relatively inexpensive educational and home-use personal computers. The PCjr, IBM model number 4860, retained the IBM PC's 8088 CPU and BIOS interface for compatibility, but its cost and differences in the PCjr's architecture, as well as other design and implementation decisions, eventually led to the PCjr, and the related IBM JX, being commercial failures.
Portable.
The "IBM Portable Personal Computer" 5155 model 68 was an early portable computer developed by IBM after the success of Compaq's suitcase-size portable machine (the Compaq Portable). It was released in February, 1984, and was eventually replaced by the IBM Convertible.
The Portable was an XT motherboard, transplanted into a Compaq-style luggable case. The system featured 256 kilobytes of memory (expandable to 512 kB), an added CGA card connected to an internal monochrome (amber) composite monitor, and one or two half-height 5.25" 360K floppy disk drives. Unlike the Compaq Portable, which used a dual-mode monitor and special display card, IBM used a stock CGA board and a composite monitor, which had lower resolution. It could however, display color if connected to an external monitor or television.
AT.
The "IBM Personal Computer/AT" (model 5170), announced August 15, 1984, used an Intel 80286 processor, originally running at 6 MHz. It had a 16-bit ISA bus and 20 MB hard drive. A faster model, running at 8 MHz and sporting a 30-megabyte hard disk was introduced in 1986.
The AT was designed to support multitasking; the new SysRq (System request key), little noted and often overlooked, is part of this design, as is the 80286 itself, the first Intel 16-bit processor with multitasking features (i.e. the 80286 protected mode). IBM made some attempt at marketing the AT as a multi-user machine, but it sold mainly as a faster PC for power users. For the most part, IBM PC/ATs were used as more powerful DOS (single-tasking) personal computers, in the literal sense of the PC name.
Early PC/ATs were plagued with reliability problems, in part because of some software and hardware incompatibilities, but mostly related to the internal 20 MB hard disk, and High Density Floppy Disk Drive.
While some people blamed IBM's hard disk controller card and others blamed the hard disk manufacturer Computer Memories Inc. (CMI), the IBM controller card worked fine with other drives, including CMI's 33-MB model. The problems introduced doubt about the computer and, for a while, even about the 286 architecture in general, but after IBM replaced the 20 MB CMI drives, the PC/AT proved reliable and became a lasting industry standard.
AT/370.
The "IBM Personal Computer AT/370" was an AT with two custom 16-bit cards, running almost exactly the same setup as the XT/370.
Convertible.
The IBM PC Convertible, released April 3, 1986, was IBM's first laptop computer and was also the first IBM computer to utilize the 3.5" floppy disk which went on to become the standard. Like modern laptops, it featured power management and the ability to run from batteries. It was the follow-up to the IBM Portable and was model number 5140. The concept and the design of the body was made by the German industrial designer Richard Sapper.
It utilized an Intel 80c88 CPU (a CMOS version of the Intel 8088) running at 4.77 MHz, 256 kB of RAM (expandable to 640 kB), dual 720 kB 3.5" floppy drives, and a monochrome CGA-compatible LCD screen at a price of $2,000. It weighed 13 pounds (5.8 kg) and featured a built-in carrying handle.
The PC Convertible had expansion capabilities through a proprietary ISA bus-based port on the rear of the machine. Extension modules, including a small printer and a video output module, could be snapped into place. The machine could also take an internal modem, but there was no room for an internal hard disk.
Next-generation IBM PS/2.
The IBM PS/2 line was introduced in 1987. The Model 30 at the bottom end of the lineup was very similar to earlier models; it used an 8086 processor and an ISA bus. The Model 30 was not "IBM compatible" in that it did not have standard 5.25-inch drive bays; it came with a 3.5-inch floppy drive and optionally a 3.5-inch-sized hard disk. Most models in the PS/2 line further departed from "IBM compatible" by replacing the ISA bus completely with Micro Channel Architecture.
Technology.
Electronics.
The main circuit board in an PC is called the motherboard (IBM terminology calls it a "planar"). This mainly carries the CPU and RAM, and it has a bus with slots for expansion cards. On the motherboard are also the ROM subsystem, DMA and IRQ controllers, coprocessor socket, sound (PC speaker, tone generation) circuitry, and keyboard interface. The original PC also has a cassette interface.
The bus used in the original PC became very popular, and it was subsequently named ISA. While it was popular, it was more commonly known as the PC-bus or XT-bus; the term "ISA" arose later when industry leaders chose to continue manufacturing machines based on the IBM PC AT architecture rather than license the PS/2 architecture and its MCA bus from IBM. The XT-bus was then retroactively named "8-bit ISA" or "XT ISA", while the unqualified term "ISA" usually refers to the 16-bit AT-bus (as better defined in the ISA specifications.) The AT-bus is an extension of the PC-/XT-bus and is in use to this day in computers for industrial use, where its relatively low speed, 5 volt signals, and relatively simple, straightforward design (all by year 2011 standards) give it technical advantages (e.g. noise immunity for reliability).
A monitor and any floppy or hard disk drives are connected to the motherboard through cables connected to graphics adapter and disk controller cards, respectively, installed in expansion slots. Each expansion slot on the motherboard has a corresponding opening in the back of the computer case through which the card can expose connectors; a blank metal cover plate covers this case opening (to prevent dust and debris intrusion and control airflow) when no expansion card is installed. Memory expansion beyond the amount installable on the motherboard was also done with boards installed in expansion slots, and I/O devices such as parallel, serial, or network ports were likewise installed as individual expansion boards. For this reason, it was easy to fill the five expansion slots of the PC, or even the eight slots of the XT, even without installing any special hardware. Companies like Quadram and AST addressed this with their popular multi-I/O cards which combine several peripherals on one adapter card that uses only one slot; Quadram offered the QuadBoard and AST the SixPak.
Intel 8086 and 8088-based PCs require expanded memory (EMS) boards to work with more than 640 kB of memory. (Though the 8088 can address one megabyte of memory, the last 384 kB of that is used or reserved for the BIOS ROM, BASIC ROM, extension ROMs installed on adapter cards, and memory address space used by devices including display adapter RAM and even the 64 kB EMS page frame itself.) The original IBM PC AT used an Intel 80286 processor which can access up to 16 MB of memory (though standard DOS applications cannot use more than one megabyte without using additional APIs.) Intel 80286-based computers running under OS/2 can work with the maximum memory.
Peripheral integrated circuits.
The set of peripheral chips selected for the original IBM PC defined the functionality of an IBM compatible. These became the de facto base for later application specific integrated circuits (ASIC)s used in compatible products.
The original system chips were one Intel 8259 programmable interrupt controller (PIC) (at I/O address ), one Intel 8237 direct memory access (DMA) controller (at I/O address ),and an Intel 8253 programmable interval timer (PIT) (at I/O address ). The PIT provides the clock ticks, dynamic memory refresh timing, and can be used for speaker output; one DMA channel is used to perform the memory refresh.
The mathematics coprocessor was the Intel 8087 using I/O address 0xF0. This was an option for users who needed extensive floating-point arithmetic, such as users of computer-aided drafting.
The IBM PC AT added a second, slave 8259 PIC (at I/O address ), a second 8237 DMA controller for 16-bit DMA (at I/O address ), a DMA address register (implemented with a 74LS612 IC) (at I/O address ), and a Motorola MC146818 real-time clock (RTC) with nonvolatile memory (NVRAM) used for system configuration (replacing the DIP switches and jumpers used for this purpose in PC and PC/XT models (at I/O address ). On expansion cards, the Intel 8255 programmable peripheral interface (PPI) (at I/O addresses is used for parallel I/O controls the printer, and the 8250 universal asynchronous receiver/transmitter (UART) (at I/O address or ) controls the serial communication at the (pseudo-) RS-232 port.
Joystick port.
IBM offered a Game Control Adapter for the PC, which supported analog joysticks similar to those on the Apple II. Although analog controls proved inferior for arcade-style games, they were an asset in certain other genres such as flight simulators. The joystick port on the IBM PC supported two controllers, but required a Y-splitter cable to connect both at once. It remained the standard joystick interface on IBM compatibles until being replaced by USB during the 2000s.
Keyboard.
The keyboard that came with the IBM 5150 was an extremely reliable and high-quality electronic keyboard originally developed in North Carolina for the Datamaster. Each key was rated to be reliable to over 100 million keystrokes. For the IBM PC, a separate keyboard housing was designed with a novel usability feature that allowed users to adjust the keyboard angle for personal comfort. Compared with the keyboards of other small computers at the time, the IBM PC keyboard was far superior and played a significant role in establishing a high-quality impression. For example, the industrial design of the adjustable keyboard, together with the system unit, was recognized with a major design award. "Byte" magazine in the fall of 1981 went so far as to state that the keyboard was 50% of the reason to buy an IBM PC. The importance of the keyboard was definitely established when the 1983 IBM PCjr flopped, in very large part for having a much different and mediocre Chiclet keyboard that made a poor impression on customers. Oddly enough, the same thing almost happened to the original IBM PC when in early 1981 management seriously considered substituting a cheaper and lower quality keyboard. This mistake was narrowly avoided on the advice of one of the original development engineers.
However, the original 1981 IBM PC 84-key keyboard was criticized by typists for its non-standard placement of the and left keys, and because it did not have separate cursor and numeric pads that were popular on the pre-PC DEC VT100 series video terminals. In 1982, Key Tronic introduced the now standard 101-key PC keyboard. In 1984, IBM corrected the and left keys on its AT keyboard, but shortened the key, making it harder to reach. In 1986, IBM changed to the 101 key enhanced keyboard, which added the separate cursor and numeric key pads, relocated all the function keys and the keys, and the key was also relocated to the opposite side of the keyboard.
Another feature of the original keyboard is the relatively loud "click" sound each key made when pressed. Since typewriter users were accustomed to keeping their eyes on the hardcopy they were typing from and had come to rely on the mechanical sound that was made as each character was typed onto the paper to ensure that they had pressed the key hard enough (and only once), the PC keyboard used a keyswitch that produced a click and tactile bump intended to provide that same reassurance.
The IBM PC keyboard is very robust and flexible. The low-level interface for each key is the same: each key sends a signal when it is pressed and another signal when it is released. An integrated microcontroller in the keyboard scans the keyboard and encodes a "scan code" and "release code" for each key as it is pressed and released separately. Any key can be used as a shift key, and a large number of keys can be held down simultaneously and separately sensed. The controller in the keyboard handles typematic operation, issuing periodic repeat scan codes for a depressed key and then a single release code when the key is finally released.
An "IBM PC compatible" may have a keyboard that does not recognize every key combination a true IBM PC does, such as shifted cursor keys. In addition, the "compatible" vendors sometimes used proprietary keyboard interfaces, preventing the keyboard from being replaced.
Although the PC/XT and AT used the same style of keyboard connector, the low-level protocol for reading the keyboard was different between these two series. The AT keyboard uses a bidirectional interface which allows the computer to send commands to the keyboard. An AT keyboard could not be used in an XT, nor the reverse. Third-party keyboard manufacturers provided a switch on some of their keyboards to select either the AT-style or XT-style protocol for the keyboard.
Character set.
The original IBM PC used the 7-bit ASCII alphabet as its basis, but extended it to 8 bits with nonstandard character codes. This character set was not suitable for some international applications, and soon a veritable cottage industry emerged providing variants of the original character set in various national variants. In IBM tradition, these variants were called code pages. These codings are now obsolete, having been replaced by more systematic and standardized forms of character coding, such as ISO 8859-1, Windows-1251 and Unicode. The original character set is known as code page 437.
Storage media.
Cassette tape.
IBM equipped the model 5150 with a cassette port for connecting a cassette drive and assumed that home users would purchase the low-end model and save files to cassette tapes as was typical of home computers of the time. However, adoption of the floppy- and monitor-less configuration was low; few (if any) IBM PCs left the factory without a floppy disk drive installed. Also, DOS was not available on cassette tape, only on floppy disks (hence "Disk Operating System"). 5150s with just external cassette recorders for storage could only use the built-in ROM BASIC as their operating system. As DOS saw increasing adoption, the incompatibility of DOS programs with PCs that used only cassettes for storage made this configuration even less attractive. The ROM BIOS supported cassette operations.
The IBM PC cassette interface encodes data using frequency modulation with a variable data rate. Either a one or a zero is represented by a single cycle of a square wave, but the square wave frequencies differ by a factor of two, with ones having the lower frequency. Therefore, the bit periods for zeros and ones also differ by a factor of two, with the unusual effect that a data stream with more zeros than ones will use less tape (and time) than an equal-length (in bits) data stream containing more ones than zeros, or equal numbers of each.
IBM also had an exclusive license agreement with Microsoft to include BASIC in the ROM of the PC; clone manufacturers could not have ROM BASIC on their machines, but it also became a problem as the XT, AT, and PS/2 eliminated the cassette port and IBM was still required to install the (now useless) BASIC with them. The agreement finally expired in 1991 when Microsoft replaced BASICA/GW-BASIC with QBASIC. The main core BASIC resided in ROM and "linked" up with the RAM-resident BASIC.COM/BASICA.COM included with PC-DOS (they provided disk support and other extended features not present in ROM BASIC). Because BASIC was over 50k in size, this served a useful function during the first three years of the PC when machines only had 64k-128k of memory, but became less important by 1985. For comparison, clone makers such as Compaq were forced to include a version of BASIC that resided entirely in RAM.
Floppy diskettes.
Most or all 5150 PCs had one or two 5.25-inch floppy disk drives. These were either single-sided double-density (SSDD) or double-sided double-density (DSDD) drives. The IBM PC never used single density floppy drives. The drives and disks were commonly referred to by capacity, such as "160KB floppy disk" or "360KB floppy drive". DSDD drives were backwards compatible; they could read and write SSDD floppies. The same type of physical diskette media could be used for both drives, but a disk formatted for double-sided use could not be read on a single-sided drive.
The disks were Modified Frequency Modulation (MFM) coded in 512-byte sectors, and were soft-sectored. They contained 40 tracks per side at the 48 track per inch (TPI) density, and initially were formatted to contain eight sectors per track. This meant that SSDD disks initially had a formatted capacity of 160 kB, operating system was later updated to allow formatting the disks with nine sectors per track. This yielded a formatted capacity of 180 kB with SSDD disks while DSDD disks had a capacity of 320 kB. However, the DOS /drives, and 360 kB with DSDD disks/drives. The "unformatted" capacity of the floppy disks was advertised as "250KB" for SSDD and "500KB" for DSDD ("KB" ambiguously referring to either 1000 or 1024 bytes; essentially the same for rounded-off values), however these "raw" 250/500 kB were not the same thing as the usable formatted capacity; under DOS, the maximum capacity for SSDD and DSDD disks was 180 kB and 360 kB, respectively. Regardless of type, the file system of all floppy disks (under DOS) was FAT12.
The earliest IBM PCs had only single-sided floppy drives until double-sided drives became available in the spring of 1982. After the upgraded 64k-256k motherboard PCs arrived in early 1983, single-sided drives and the cassette model were discontinued.
IBM's original floppy disk controller card also included an external 37-pin D-shell connector. This allowed users to connect additional external floppy drives by third party vendors, but IBM did not offer their own external floppies until 1986.
The industry-standard way of setting floppy drive numbers was via setting jumper switches on the drive unit, however IBM chose to instead use a method known as the "cable twist" which had a floppy data cable with a bend in the middle of it that served as a switch for the drive motor control. This eliminated the need for users to adjust jumpers while installing a floppy drive.
Fixed disks.
The 5150 could not itself power hard drives without retrofitting a stronger power supply, but IBM later offered the 5161 Expansion Unit, which not only provided more expansion slots, but also included a 10 MB (later 20 MB) hard drive powered by the 5161's own separate 130-watt power supply. The IBM 5161 Expansion Unit was released in early 1983.
During the first year of the IBM PC, it was commonplace for users to install third-party Winchester hard disks which generally connected to the floppy controller and required a patched version of PC-DOS which treated them as a giant floppy disk (there was no subdirectory support).
IBM began offering hard disks with the XT, however the original PC was never sold with them. Nonetheless, many users installed hard disks and upgraded power supplies in them.
After floppy disks became obsolete in the early 2000s, the letters A and B became unused. But for 25 years, virtually all DOS-based PC software assumed the program installation drive was C, so the primary HDD continues to be "the C drive" even today.
Other operating system families (e.g. Unix) are not bound to these designations.
OS support.
Which operating system IBM customers would choose was at first unclear. Although the company expected that most would use PC DOS IBM supported using CP/M-86—which became available six months after DOS—or UCSD p-System as operating systems. IBM promised that it would not favor one operating system over the others; the CP/M-86 support surprised Gates, who claimed that IBM was "blackmailed into it". IBM was correct, nonetheless, in its expectation; one survey found that 96.3% of PCs were ordered with the $40 DOS compared to 3.4% for the $240 CP/M-86.
The IBM PC's ROM BASIC and BIOS supported cassette tape storage. PC DOS itself did not support cassette tape storage. PC DOS version 1.00 supported only 160 kB SSDD floppies, but version 1.1, which was released nine months after the PC's introduction, supported 160 kB SSDD and 320 kB DSDD floppies. Support for the slightly larger nine sector per track 180 kB and 360 kB formats arrived 10 months later in March 1983.
BIOS.
The BIOS (Basic Input/Output System) provided the core ROM code for the PC. It contained a library of functions that software could call for basic tasks such as video output, keyboard input, and disk access in addition to interrupt handling, loading the operating system on boot-up, and testing memory and other system components. Thanks to the vectored interrupts of the x86 CPUs, clone makers could easily reverse-engineer the IBM PC BIOS without stealing any copyrighted code.
The original IBM PC BIOS was 8k in size and occupied four 2k ROM chips on the motherboard, with a fifth and sixth empty slot left for any extra ROMs the user wished to install. IBM offered three different BIOS revisions during the PC's lifespan. The initial BIOS was dated April 1981 and came on the earliest models with single-sided floppy drives and PC DOS 1.00. The second version was dated October 1981 and arrived on the "Revision B" models sold with double-sided drives and PC DOS 1.10. It corrected some bugs, but was otherwise unchanged. Finally, the third BIOS version was dated October 1982 and found on all IBM PCs with the newer 64k-256k motherboard. This revision was more-or-less identical to the XT's BIOS. It added support for detecting ROMs on expansion cards as well as the ability to use 640k of memory (the earlier BIOS revisions had a limit of 544k). Unlike the XT, the original PC remained functionally unchanged from 1983 until its discontinuation in early 1987 and did not get support for 101-key keyboards or 3.5" floppy drives, nor was it ever offered with half-height floppies.
Video output.
IBM initially offered two video adapters for the PC, the Color/Graphics Adapter and the Monochrome Display and Printer Adapter. CGA was intended to be a typical home computer display; it had NTSC output and could be connected to a composite monitor or a TV set with an RF modulator in addition to RGB for digital RGBI-type monitors, although IBM did not offer their own RGB monitor until 1983. Supported graphics modes were 40 or 80x25 color text with 8x8 character resolution, 320x200 bitmap graphics with two fixed 4-color palettes, or 640x200 monochrome graphics.
The MDA card and its companion 5151 monitor supported only 80x25 text with a 9x14 character resolution (total pixel resolution was 720x350). It was mainly intended for the business market and so also included a printer port.
During 1982, the first third-party video card for the PC appeared when Hercules Computer Technologies released a clone of the MDA that could use bitmap graphics. Although not supported by the BIOS, the Hercules Graphics Adapter became extremely popular for business use due to allowing sharp, high resolution graphics plus text and itself was widely cloned by other manufacturers.
In 1985, after the launch of the IBM AT, the new Enhanced Graphics Adapter became available which could support 320x200 or 640x200 in 16 colors in addition to high-resolution 640x350 16 color graphics.
IBM also offered a video board for the PC, XT, and AT known as the Professional Graphics Adapter during 1984-86, mainly intended for CAD design. It was extremely expensive, required a special monitor, and was rarely ordered by customers.
VGA graphics cards could also be installed in IBM PCs and XTs, although they were introduced after the computer's discontinuation.
Serial port addresses and interrupts.
The serial port is an 8250 or a derivative (such as the 16450 or 16550), mapped to eight consecutive IO addresses and one interrupt request line.
Only COM1: and COM2: addresses were defined by the original PC. Attempts to share IRQ3 and IRQ4 to use additional ports require special measures in hardware and software, since shared IRQs were not defined in the original PC design. The most typical devices plugged into the serial port were modems and mice. Plotters and serial printers were also among the more commonly-used serial peripherals, and there were numerous other more unusual uses such as operating cash registers, factory equipment, and connecting terminals.
Printer port.
IBM made a deal with Japan-based Epson to producer printers for the PC and all IBM-branded printers were manufactured by that company (Epson of course also sold printers with their own name). There was a considerable amount of controversy when IBM included a printer port on the PC that did not follow the industry-standard Centronics design, and it was rumored that this had been done to prevent customers from using non-Epson/IBM printers with their machines (plugging a Centronics printer into an IBM PC could damage the printer, the parallel port, or both). Although third-party cards were available with Centronics ports on them, PC clones quickly copied the IBM printer port and by the late 80s, it had largely displaced the Centronics standard.
Reception.
"BYTE" wrote in October 1981 that the IBM PC's "hardware is impressive, but even more striking are two decisions made by IBM: to use outside suppliers already established in the microcomputer industry, and to provide information and assistance to independent, small-scale software writers and manufacturers of peripheral devices". It praised the "smart" hardware design and stated that its price was not much higher than the 8-bit machines from Apple and others. The reviewer admitted that the computer "came as a shock. I expected that the giant would stumble by overestimating or underestimating the capabilities the public wants and stubbornly insisting on incompatibility with the rest of the microcomputer world. But IBM didn't stumble at all; instead, the giant jumped leagues in front of the competition ... the only disappointment about the IBM Personal Computer is its dull name".
In a more detailed review in January 1982, "BYTE" called the IBM PC "a synthesis of the best the microcomputer industry has offered to date ... as well designed on the inside as it is on the outside". The magazine praised the keyboard as "bar none, the best ... on any microcomputer", describing the unusual Shift key locations as "minor compared to some of the gigantic mistakes made on almost every other microcomputer keyboard". The review also complimented IBM's manuals, which it predicted "will set the standard for all microcomputer documentation in the future. Not only are they well packaged, well organized, and easy to understand, but they are also "complete"". Observing that detailed technical information was available "much earlier ... than it has been for other machines", the magazine predicted that "given a reasonable period of time, plenty of hardware and software will probably be developed for" the computer. The review stated that although the IBM PC cost more than comparably configured Apple II and TRS-80 computers, and the insufficient number of slots for all desirable expansion cards was its most serious weakness, "you get a "lot" more for your money" and concluded, "In two years or so, I think [it will be one of the most popular and best-supported ... IBM should be proud of the people who designed it".
In a special 1984 issue dedicated to the IBM PC, "BYTE" concluded that the PC had succeeded both because of its features like an 80-column screen, open architecture, and high-quality keyboard, and "the failure of other major companies to provide these same fundamental features earlier. In retrospect, it seems IBM stepped into a void that remained, paradoxically, at the center of a crowded market".
Longevity.
Many IBM PCs have remained in service long after their technology became largely obsolete. In June 2006, IBM PC and XT models were still in use at the majority of U.S. National Weather Service upper-air observing sites, used to process data as it is returned from the ascending radiosonde, attached to a weather balloon, although they have been slowly phased out. Factors that have contributed to the 5150 PC's longevity are its flexible modular design, its open technical standard (making information needed to adapt, modify, and repair it readily available), use of few special nonstandard parts, and rugged high-standard IBM manufacturing, which provided for exceptional long-term reliability and durability. Many newer PCs, by contrast, use proprietary parts and PCs themselves become obsolete quickly. According to Moore's Law the power of a microprocessor doubles every 18 months and it becomes easier to simply dispose of the PC than to upgrade or repair it.
The slot specifications are still used in current PCs as well as the limitation of having four active partitions on a hard disk. Many systems still come with PS/2 style keyboard and mouse connectors, and power supply connectors are based on later standards.
Collectability.
The IBM model 5150 Personal Computer has become a collectable among vintage computer collectors, due to the system being the first true “PC” as we know them today. Today these systems can fetch anywhere from $100 to $4500, depending on cosmetic and operational condition. The IBM model 5150 has proven to be reliable; despite their age of 30 years or more, some still function as they did when new.

</doc>
<doc id="15033" url="https://en.wikipedia.org/wiki?curid=15033" title="Counties of Ireland">
Counties of Ireland

The counties of Ireland (; Ulster-Scots: "coonties o Airlann") are sub-national divisions that have been, and in some cases continue to be, used to geographically demarcate areas of local government. These land divisions were formed following the Norman invasion of Ireland in imitation of the counties then in use as units of local government in the Kingdom of England. The older term "shire" was historically equivalent to "county". The principal function of the county was to impose royal control in the areas of taxation, security and the administration of justice at the local level. Cambro-Norman control was initially limited to the southeastern parts of Ireland; a further four centuries elapsed before the entire island was shired. At the same time, the now obsolete concept of county corporate elevated a small number of towns and cities to a status which was deemed to be no less important than the existing counties in which they lay. This double control mechanism of 32 counties plus 10 counties corporate remained unchanged for a little over two centuries until the early 19th century, since then counties have been adapted and in some cases divided by legislation to meet new administrative and political requirements.
The powers exercised by the Cambro-Norman barons and the Old English nobility waned over time. New offices of political control came to be established at a county level. In the Republic of Ireland, some counties have been split resulting in the creation of new counties. Along with certain defined cities, counties still form the basis for the demarcation of areas of local government in the Republic of Ireland. Currently, there are 26 county level, 3 city level and 2 city and county entities – the modern equivalent of counties corporate – that are used to demarcate areas of local government in the Republic.
In Northern Ireland, counties are no longer used for local government; districts are instead used. Upon the partition of Ireland in 1921, the county became one of the basic land divisions employed, along with county boroughs.
Terminology.
The word "county" has come to be used in different senses for different purposes. In common usage, many people have in mind the 32 counties that existed prior to 1838 – the so-called traditional counties. However, in official usage in the Republic of Ireland, the term often refers to the 28 modern counties. The term is also conflated with the 31 areas currently used to demarcate areas of local government in the Republic of Ireland at the level of LAU 1.
In Ireland, usage of the word "county" nearly always comes before rather than after the county name; thus ""County" Clare" in Ireland as opposed to "Clare "County"" in Michigan, United States. The former "King's County" and "Queen's County" were exceptions; these are now County Offaly and County Laois, respectively. The abbreviation Co. is used, as in "Co. Clare". A further exception occurs in the case of those counties created after 1994 which often drop the word "county" entirely, or use it after the name; thus for example internet search engines show many more uses (on Irish sites) of "Fingal" than of either "County Fingal" or "Fingal County". There appears to be no official guidance in the matter, as even the local council uses all three forms. In informal use, the word "county" is often dropped except where necessary to distinguish between county and town or city; thus "Offaly" rather than "County Offaly", but "County Antrim" to distinguish it from Antrim town. The synonym "shire" is not used for Irish counties, although the Marquessate of Downshire was named in 1789 after County Down.
Parts of some towns and cities were exempt from the jurisdiction of the counties that surrounded them. These towns and cities had the status of a County corporate, many granted by Royal Charter, which had all the judicial, administrative and revenue raising powers of the regular counties.
History.
Pre-Norman divisions of Ireland.
The political geography of Ireland can be traced with some accuracy from the 6th century. At that time Ireland was divided into a patchwork of petty kingdoms with a fluid political hierarchy which, in general, had three traditional grades of king. The lowest level of political control existed at the level of the "tuath" (pl. "tuatha"). A "tuath" was an autonomous group of people of independent political jurisdiction under a rí túaithe, that is, a local petty king. About 150 such units of government existed. Each "rí tuaithe" was in turn subject to a regional or "over-king" . There may have been as many as 20 genuine ruiri in Ireland at any time.
A "king of over-kings" was often a provincial () or semi-provincial king to whom several ruiri were subordinate. No more than six genuine rí ruirech were ever contemporary. Usually, only five such "king of over-kings" existed contemporaneously and so are described in the Irish annals as "fifths" ("cúigí" in Irish). The areas under the control of these kings were : Ulster (), Leinster (), Connacht (), Munster () and Mide (). Later record-makers dubbed them "provinces", in imitation of Roman provinces. In the Norman period, the historic fifths of Leinster and Meath gradually merged, mainly due to the impact of the Pale, which straddled both, thereby forming the present-day province of Leinster.
The use of provinces as divisions of political power was supplanted by the system of counties after the Norman invasion. In modern times clusters of counties have been attributed to certain provinces but these clusters have no legal status. They are today seen mainly in a sporting context, as Ireland's four professional rugby teams play under the names of the provinces, and the Gaelic Athletic Association has separate Provincial councils and Provincial championships.
Norman areas of control.
With the arrival of Cambro-Norman knights in 1169, the Norman invasion of Ireland commenced. This was followed in 1172 by the invasion of King Henry II of England, commencing English royal involvement. The English governed Ireland using a structure similar to that used in England, by dividing the country into shires or counties in the late 12th and early 13th centuries.
In general, counties were made by amalgamating various smaller Irish territories which suited the colonial administration at the time and had little basis in older tribal boundaries. In many cases, this involved dividing an Irish territory in two. For example, the kingdom of Uí Maine was split to form south County Roscommon and most of east County Galway. Many of the counties of Ulster roughly correspond to the territories controlled by the principal clan in that particular area such as the O Donnells of Tír Chonaill whose political power was concentrated in what would become the County of Donegal.
The counties evolved over time, with the earliest defined being set out by King John, including a then much larger County Dublin. Sir John Davies wrote ‘True it is that King John made twelve shires in Leinster and Munster—namely, Dublin, Kildare, Meath, Urial or Louth, Catherlogh, Kilkenny, Wexford, Waterford, Cork, Limerick, Kerry, and Tipperary. Yet these counties did stretch no further than the lands of the English colonies did extend.’ In the Anglo-Saxon controlled territories, a royal official known as a "shire reeve" or sheriff governed the shire. Large parts in the south east of the island had been shired by the Cambro-Norman overlords by the end of the 13th century. Following the resurgence in the 14th and 15th centuries of the Gaelic nobility, the project was suspended, not to be resumed until the mid 16th century by the House of Tudor. There were two phases in this period under the newly created Kingdom of Ireland: the plantation of the midlands under Mary I and the plantation of Munster under her half-sister, Elizabeth I. By the reign of King James I & VI, the process was completed with the plantation of the last remaining princely domains in the province of Ulster. With the late addition of County Wicklow, the number of counties ( or ). stood at thirty two. This number remained fixed for a little over 200 years until the division of some counties into smaller entities for judicial purposes (1836), initially, followed by complete division in the 20th and 21st centuries.
By 1200 there were also shires of Connacht, Cork, Kerry, Limerick, Louth and Tipperary. Waterford, Kilkenny and Wexford apparently date from this time, too, as remnants of Strongbow's palatine county in Leinster. The process continued over time, and in 1206, for example, a special commission was used to determine borders in the Munster area.
The County of Roscommon was separated from Connacht before 1292, and the first session of the Parliament of Ireland in 1297 created the new shires of County Kildare, Meath and Ulster. Carlow, then larger than today, and extending to and including coastal Arklow, probably dates from around 1306.
In the 14th century, "Counties of the Cross" emerged, consisting of scattered areas of church land within the existing divisions. Unlike the secular counties (or liberties), the counties of the cross were administered by royally-appointed sheriffs.
Tudor areas of control.
The Tudor administrations finalised the division of Ireland into counties. Westmeath was separated from Meath (often "East Meath") in 1543. Queen Mary I of England introduced a new policy to pacify the midland areas – plantation. In 1556 King's County and Queen's County were created and the Kingdom of Connacht was broken up into the counties of Galway, Mayo and Sligo, while Leitrim was separated from Roscommon in 1565. At the same time County Clare was created and moved from Munster to Connacht, being returned to Munster in 1602.
In 1583 County Longford was formed from part of Westmeath and transferred to the Province of Connacht.
The Province of Ulster was the last to be shired. The counties of Antrim and Down originated early in the 16th century. These were joined in 1584/5 by the counties of Armagh, Coleraine, Donegal, Fermanagh, Monaghan and Tyrone. County Cavan was also formed in 1584 and transferred from Connacht to Ulster. County Londonderry was incorporated in 1613 from the merger of County Coleraine with the barony of Loughinsholin (in County Tyrone), the North West Liberties of Londonderry (in County Donegal), and the North East Liberties of Coleraine (in County Antrim).
The last county to be formed was County Wicklow in 1606–07, taking in the southern part of Dublin (with the exception of three "islands," exclaves of (mainly) church property), and the northern part of "Catherlough" or Carlow, including Arklow.
Areas that were shired by 1607 and continued in existence down to the local government reforms of 1836, 1898 and 2001 are sometimes referred to as "traditional" or "historic" counties. These areas were separate from the county corporates that existed in some of the larger towns and cities although linked to the county at large for other purposes. From 1898 to 2001, areas with county councils were known as administrative counties while the counties corporate were designated as county boroughs. In other cases, the "traditional" county was divided to form two administrative counties. From 2001, certain administrative counties, which were originally "traditional" counties, underwent further splitting.
Former counties.
Former counties include: County Coleraine, which formed the basis of County Londonderry, the counties of Nether and Upper Tyrone, and Desmond which was, in 1606, split between counties Cork and Kerry. Other names seen on old maps include Caterlaugh or Caterlagh, archaic designations of County Carlow, in the days before much of the north of that county was taken into Wicklow in the early 17th century.
In 1333, the Earldom of Ulster consisted of seven counties: Antrim, Blathewyc, Cragferus, Coulrath, del Art, Dun (also known as Ladcathel), and Twescard.
Carrickfergus would lose its county town status in 1777. Twescard is preserved as the name of a school house at Campbell College, Belfast.
Sub-divisions of counties.
To correspond with the subdivisions of the English shires into honours or baronies, Irish counties were granted out to the Anglo-Norman noblemen in cantreds, later known as baronies, which in turn were subdivided, as in England, into parishes. Parishes were composed of townlands. However, in many cases, these divisions correspond to earlier, pre-Norman, divisions. While there are 331 baronies in Ireland, and more than a thousand civil parishes, there are around sixty thousand townlands that range in size from one to several thousand hectares. Townlands were often traditionally divided into smaller units called "quarters", but these subdivisions are not legally defined.
Counties corporate.
The following towns/cities had charters specifically granting them the status of a county corporate:
The only entirely new counties created in 1898 were the county boroughs of Londonderry and Belfast. Carrickfergus, Drogheda and Kilkenny were abolished; Galway was also abolished, but recreated in 1986.
Exceptions to the county system of control.
Regional presidencies of Connacht and Munster remained in existence until 1672, with special powers over their subsidiary counties. Tipperary remained a county palatine until the passing of the County Palatine of Tipperary Act 1715, with different officials and procedures from other counties. At the same time, Dublin, until the 19th century, had ecclesiastical liberties with rules outside those applying to the rest of Dublin city and county. Exclaves of the county of Dublin existed in counties Kildare and Wicklow. At least eight other enclaves of one county inside another, or between two others, existed. The various enclaves and exclaves were merged into neighbouring and surrounding counties, primarily in the mid-19th century under a series of Orders in Council.
Evolution of functions.
The Church of Ireland exercised functions at the level of civil parish that would later be exercised by county authorities. Vestigial feudal power structures of major old estates remained well into the 18th century. Urban corporations operated individual royal charters. Management of counties came to be exercised by grand juries. Members of grand juries were the local payers of rates who historically held judicial functions, taking maintenance roles in regard to roads and bridges, and the collection of "county cess" taxes. They were usually composed of wealthy "country gentlemen" (i.e. landowners, farmers and merchants):A country gentleman as a member of a Grand Jury...levied the local taxes, appointed the nephews of his old friends to collect them, and spent them when they were gathered in. He controlled the boards of guardians and appointed the dispensary doctors, regulated the diet of paupers, inflicted fines and administered the law at petty sessions. The counties were initially used for judicial purposes, but began to take on some governmental functions in the 17th century, notably with grand juries.
19th and 20th centuries.
In 1836, the use of counties as local government units was further developed, with grand-jury powers extended under the Grand Jury (Ireland) Act 1836. The traditional county of Tipperary was split into two judicial counties (or ridings) following the establishment of assize courts in 1838. Also in that year, local poor law boards, with a mix of magistrates and elected "guardians" took over the health and social welfare functions of the grand juries.
Sixty years later, a more radical reorganisation of local government took place with the passage of the Local Government (Ireland) Act 1898. This Act established a county council for each of the thirty-three Irish administrative counties. Elected county councils took over the powers of the grand juries.The boundaries of the traditional counties changed on a number of occasions. The 1898 Act changed the boundaries of Counties Galway, Clare, Mayo, Roscommon, Sligo, Waterford, Kilkenny, Meath and Louth, and others. County Tipperary was divided into two regions: North Riding and South Riding. Areas of the cities of Belfast, Cork, Dublin, Limerick, Derry and Waterford were carved from their surrounding counties to become county boroughs in their own right and given powers equivalent to those of administrative counties.
Under the Government of Ireland Act 1920, the island was partitioned between Southern Ireland and Northern Ireland. For the purposes of the Act, ... Northern Ireland shall consist of the parliamentary counties of Antrim, Armagh, Down, Fermanagh, Londonderry and Tyrone, and the parliamentary boroughs of Belfast and Londonderry, and Southern Ireland shall consist of so much of Ireland as is not comprised within the said parliamentary counties and boroughs.
The county and county borough borders were thus used to determine the line of partition. Southern Ireland shortly afterwards became the Irish Free State. This partition was entrenched in the Anglo-Irish Treaty, which was ratified in 1922, by which the Irish Free State left the United Kingdom with Northern Ireland making the decision to not separate two days later.
Under the Local Government Provisional Order Confirmation Act 1976, part of the urban area of Drogheda, which lay in County Meath, was transferred to County Louth on 1 January 1977. This resulted in the land area of County Louth increasing slightly at the expense of County Meath. The possibility of a similar action with regard to Waterford City has been raised in recent years, though opposition from Kilkenny has been strong.
Current usage.
In the Republic of Ireland.
In the Republic of Ireland the traditional counties are, in general, the basis for local government, planning and community development purposes, are governed by county councils and are still generally respected for other purposes. Administrative borders have been altered to allocate various towns (e.g. Bray) exclusively into one county having been originally split between two counties.
There are now 26 county councils, three city councils and two city and county councils – a total of 31 local government entities.
County Tipperary was split into North and South Ridings in 1838. These Ridings were established as separate administrative counties under the Local Government (Ireland) Act, 1898.
County Dublin was abolished as an administrative county in 1994, while also remaining a point of reference for purposes other than local government. Its territory was divided into three administrative counties: Dún Laoghaire–Rathdown, Fingal, and South Dublin. The county borough of Dublin, together with the county boroughs of Cork, Galway, Limerick and Waterford, were re-styled as city councils under the Local Government Act 2001, with the same status in law as county councils.
The city councils of Limerick and Waterford were merged with their respective county councils by the Local Government Reform Act 2014, to form new city and county councils. Anomalously, the city of Kilkenny does not have a "city council" as it was a borough but not a county borough. It is now administered by its eponymous county council but is, exceptionally, permitted to retain the style of "city" for ornament only. Also, the 2014 Act abolished North Tipperary and South Tipperary, and re-established County Tipperary.
These 31 "county-level" entities correspond to the first level of local administrative unit for EU and Eurostat purposes. The second level of local administrative unit (LAU) is the District electoral division. Of the administrative structures established under the 1898 Local Government Act, the only type to have been completely abolished was the Rural District, which was rendered void in the early years of the Irish Free State amidst widespread allegations of corruption. At a level above that of LAU is the Region which clusters counties together for NUTS purposes. The Regions are administered by Regional Authorities which were established by the Local Government Act 1991 and came into existence in 1994.
Education.
In 2013 Education and Training Boards (ETBs) were formed throughout the Republic of Ireland, replacing the system of Vocational Education Committees (VECs) created in 1930. Originally, VECs were formed for each administrative county and county borough, and also in a number of larger towns. In 1997 the majority of town VECs were absorbed by the surrounding county. The 33 VEC areas were reduced to 16 ETB areas, with each consisting of one or more local government county or city.
The Institute of technology system was organised on the committee areas or "functional areas", these still remain legal but are not as important as originally envisioned as the institutes are now more national in character and are only really applied today when selecting governing councils, similarly Dublin Institute of Technology was originally a group of several colleges of the City of Dublin committee.
Elections.
Where possible, parliamentary constituencies in the Republic of Ireland follow county boundaries. Under the Electoral Act 1997 a Constituency Commission is established following the publication of census figures every five years. The Commission is charged with defining constituency boundaries, and the 1997 Act provides that "the breaching of county boundaries shall be avoided as far as practicable". This provision does not apply to the boundaries between cities and counties, or between the three counties in the Dublin area.
This system usually results in more populated counties having several constituencies: Dublin, including Dublin city, is subdivided into twelve constituencies, Cork into five. On the other hand, smaller counties such as Carlow and Kilkenny or Laois and Offaly may be paired to form constituencies. An extreme case is the splitting of Ireland's least populated county of Leitrim between the constituencies of Sligo-North Leitrim and Roscommon-South Leitrim.
Each county or city is divided into Local electoral areas for the election of councillors. The boundaries of the areas and the number of councillors assigned are fixed from time to time by order of the Minister for the Environment, Community and Local Government, following a report by the Local Government Commission, and based on population changes recorded in the census.
In Northern Ireland.
In Northern Ireland, a major reorganisation of local government in 1973 replaced the six traditional counties and two county boroughs (Belfast and Derry) with 26 single-tier districts for local government purposes. In 2015, as a result of a reform process that started in 2005, these districts were merged to form 11 new single-tier "super districts".
The six traditional counties remain in use for some purposes, including the three-letter coding of vehicle number plates, the Royal Mail Postcode Address File (which records counties in all addresses although they are no longer required for postcoded mail) and Lord Lieutenancies (for which the former county boroughs are also used). There are no longer official 'county towns'. However the counties are still very widely acknowledged, for example as administrative divisions for sporting and cultural organisations.
Other uses.
The administrative division of the island along the lines of the traditional 32 counties was also adopted by non-governmental and cultural organisations. In particular the Gaelic Athletic Association continues to organise its activities on the basis of GAA counties that, throughout the island, correspond almost exactly to the 32 traditional counties in use at the time of the foundation of that organisation in 1884. The GAA also uses the term "county" for some of its organisational units in Britain and further afield.
List of counties.
The first 32 divisions listed below are the "traditional" counties, 21 of which still have administrative functions as local government divisions in the Republic of Ireland (in some cases with slightly redrawn boundaries). The newer administrative counties established in the Republic are listed at the foot of the table. The Irish-language names of counties in the Republic of Ireland are prescribed by ministerial order, which in the case of three newer counties, omits the word "contae" (county). Irish names form the basis for all English-language county names except Dublin, Waterford, Wexford, and Wicklow, which are of Norse origin. The Ulster-Scot names are principally derived from the North/South Ministerial Council.
In the "Region" column of the table below, except for the six Northern Ireland counties the reference is to NUTS 3 statistical regions of the Republic of Ireland. "County town" is the current or former administrative capital of the county.
Cities which, in the Republic, are currently administered outside the county system, but with the same legal status as administrative counties, are not shown separately: these are Cork, Dublin and, Galway. Also not shown are the former county boroughs of Londonderry and Belfast which in Northern Ireland had the same legal status as the six counties until the reorganisation of local government in 1973.

</doc>
<doc id="15034" url="https://en.wikipedia.org/wiki?curid=15034" title="Information Sciences Institute">
Information Sciences Institute

The USC Information Sciences Institute (ISI), a unit of the University of Southern California (USC) Viterbi School of Engineering, is located in Marina del Rey, California. It researches and develops information processing, computing and communications technologies.
ISI actively participated in the information revolution, including helping to develop and manage the Internet. The Institute conducts basic and applied research supported by more than 20 U.S. government agencies involved in defense, science, health, homeland security, energy and other areas. Annual funding is about $60 million.
The USC Andrew and Erna Viterbi School of Engineering was ranked among the nation’s top 10 engineering graduate schools by "US News & World Report" in 2015. Including ISI, USC is ranked first nationally in federal computer science research and development expenditures.
ISI employs about 350 research scientists, research programmers, graduate students and administrative staff at its Marina del Rey, California headquarters and in Arlington, Virginia. About half of the research staff hold PhDs, and about 40 are research faculty who teach at USC and/or advise graduate students. Several senior researchers are tenured USC faculty in the Viterbi School.
ISI Executive Director Prem Natarajan, previously an executive vice president and principal scientist at Raytheon BBN Technologies, is a natural language expert whose own research focuses on optical character recognition, speech processing and multimedia analysis. Natarajan joined ISI in 2013, succeeding USC Viterbi School vice dean John O'Brien, who served as interim executive director in 2012 and 2013. From 1988 to 2012, ISI was led by former IBM executive Herbert Schorr.
Research and sponsors.
ISI research spans artificial intelligence (AI), cybersecurity, grid computing, quantum computing, microelectronics, supercomputing, nano-satellites and many other areas. AI expertise includes natural language processing, in which ISI has an international reputation, reconfigurable robotics, information integration, motion analysis and social media analysis. Hardware/software expertise includes cyber-physical system security, data mining, reconfigurable computing and cloud computing. In networking, ISI explores Internet resilience, Internet traffic analysis and photonics, among other areas. Researchers also work in scientific data management, wireless technologies, biomimetics and electrical smart grid, in which ISI is advising the Los Angeles Department of Water and Power on a major demonstration project. Another current initiative involves big-data brain imaging jointly with the Keck School of Medicine of USC.
Federal agency sponsors include the Air Force Office of Scientific Research, Department of Defense Advanced Research Projects Agency, Department of Education, Department of Energy, Department of Homeland Security, National Institutes of Health, National Science Foundation, and other scientific, technical and defense-related agencies.
Corporate partners include Chevron Corp. in the Center for Interactive Smart Oilfield Technologies (CiSoft), Lockheed Martin Company in the USC-Lockheed Martin Quantum Computation Center, and Parsons Corp. subsidiary Sparta Inc. in the DETER Project, a cybersecurity research initiative and international testbed. ISI also has partnered with businesses including IBM Corporation, Samsung Electronics Company, the Raytheon Company, GlobalFoundries Inc., Northrop Grumman Corporation and Carl Zeiss AG, and currently is working with Micron Technology, Inc., Altera Corporation and Fujitsu Ltd.
ISI also operates the Metal Oxide Semiconductor Implementation Service (MOSIS), a multi-project electronic circuit wafer service that has prototyped more than 60,000 chips since 1981. MOSIS provides design tools and pools circuit designs to produce specialty and low-volume chips for corporations, universities and other research entities worldwide. The Institute also has given rise to several startup and spinoff companies in grid software, geospatial information fusion, machine translation, data integration and other technologies.
History.
ISI was founded by Keith Uncapher, who headed the computer research group at RAND Corporation in the 1960s and early 1970s. Uncapher decided to leave RAND after his group’s funding was cut in 1971. He approached the University of California at Los Angeles about creating an off-campus technology institute, but was told that a decision would take 15 months. He then presented the concept to USC, which approved the proposal in five days. ISI was launched with three employees in 1972. Its first proposal was funded by the Defense Advanced Research Projects Agency (DARPA) in 30 days for $6 million.
ISI became one of the earliest nodes on ARPANET, the predecessor to the Internet, and in 1977 figured prominently in a demonstration of its international viability. ISI also helped refine the TCP/IP communications protocols fundamental to Net operations, and researcher Paul Mockapetris developed the now-familiar Domain Name System characterized by .com, .org, .net, .gov, and .edu on which the Net still operates. (The names .com, .org et al. were invented at SRI International, an ongoing collaborator.) Steve Crocker originated the Request for Comments (RFC) series, the written record of the network's technical structure and operation that both documented and shaped the emerging Internet. Another ISI researcher, Danny Cohen, became first to implement packet voice and packet video over ARPANET, demonstrating the viability of packet switching for real-time applications.
Jonathan Postel collaborated in development of TCP/IP, DNS and the SMTP protocol that supports email. He also edited the RFC for nearly three decades until his sudden death in 1998, when ISI colleagues assumed responsibility. The Institute retained that role until 2009. Postel simultaneously directed the Internet Assigned Number Authority (IANA) and its predecessor, which assign Internet addresses. IANA was administered from ISI until a nonprofit organization, ICANN, was created for that purpose in 1998.
Other achievements.
Cohen was the first entity to implement, Voice Over Internet Protocol (Voice over IP or VOIP). Some of the first Net security applications, and one of the world's first portable computers, also originated at ISI.
ISI researchers also created or co-created the:
In 2011, several ISI natural language experts advised the IBM team that created Watson, the computer that became the first machine to win against human competitors on the "Jeopardy!" TV show. In 2012, ISI’s Kevin Knight spearheaded a successful drive to crack the Copiale cipher, a lengthy encrypted manuscript that had remained unreadable for 250 years. Also in 2012, the USC-Lockheed Martin Quantum Computation Center (QCC) became the first organization to operate a quantum annealing system outside of its manufacturer, D-Wave Systems, Inc. USC, ISI and Lockheed Martin now are performing basic and applied research into quantum computing. A second quantum annealing system is located at NASA Ames Research Center, and is operated jointly by NASA and Google.
Organizational structure.
ISI is organized into six divisions focused on differing areas of research expertise:
Smaller, specialized research groups operate within almost all divisions.

</doc>
