<doc id="13772" url="https://en.wikipedia.org/wiki?curid=13772" title="History of Poland">
History of Poland

The history of Poland results from the migrations of Slavs who established permanent settlements on the Polish lands during the Early Middle Ages. In 966 AD, Duke Mieszko I of the Piast dynasty adopted Western Christianity; in 1025 Mieszko's son Bolesław I Chrobry formally established a medieval kingdom. The period of the Jagiellonian dynasty in the 14th-16th centuries brought close ties with the Grand Duchy of Lithuania, a cultural Renaissance in Poland and territorial expansion that culminated in the establishment of the Polish–Lithuanian Commonwealth in 1569.
The Commonwealth in its early phase represented a continuation of Jagiellonian prosperity, with its remarkable development of a sophisticated noble democracy. From the mid-17th century the huge state entered a period of decline caused by devastating wars and by the deterioration of the country's political system. Significant internal reforms were introduced during the later part of the 18th century, especially in the Constitution of May 3, 1791, but neighboring powers did not allow the reform process to continue. The independent existence of the Commonwealth ended in 1795 after a series of invasions and partitions of Polish territory carried out by the Russian Empire, the Kingdom of Prussia, and the Austrian Habsburg Monarchy.
From 1795 until 1918 no truly independent Polish state existed, although strong Polish resistance movements operated. After the failure of the last military uprising against the Russian Empire, the January Uprising of 1863, the nation preserved its identity through educational initiatives and through the program of "organic work" intended to modernize the economy and society. The opportunity to regain independence only materialized after World War I, when the three partitioning imperial powers suffered decline in the wake of war and revolution.
The Second Polish Republic, established in 1918, existed as an independent state until 1939, when Nazi Germany and the Soviet Union destroyed it in their invasion of Poland at the beginning of World War II. Millions of Polish citizens perished in the course of the Nazi occupation of Poland between 1939 and 1945 as Germany classified ethnic Poles and other Slavs, Jews and Romani (Gypsies) as subhuman. Nazi authorities targeted the last two groups for extermination in the short term, deferring the extermination and/or enslavement of the Slavs as part of the "Generalplan Ost" ("General Plan for the East") conceived by the Nazi régime. A Polish government-in-exile nonetheless functioned throughout the war and the Poles contributed to the Allied victory through participation in military campaigns, including the final anti-German offensives, on both the eastern and western fronts. The westward advances of the Soviet Red Army in 1944 and 1945 compelled Nazi Germany's forces to retreat from Poland, which led to the establishment of a communist satellite state of the Soviet Union, known from 1952 as the Polish People's Republic.
As a result of territorial adjustments mandated by the victorious Allies at the end of World War II in 1945, Poland's geographic centre of gravity shifted towards the west and the re-defined Polish lands largely lost their traditional multi-ethnic character through the extermination, expulsion and migration of the various nationalities during and after the war.
By the late 1980s the Polish reform movement Solidarity became crucial in bringing about a peaceful transition from a communist state to the capitalist economic system and to liberal parliamentary democracy. This process resulted in the creation of the modern Polish state—the Polish Third Republic.
Prehistory and protohistory.
Members of the "Homo" genus have lived in north Central Europe for thousands of years since its environment was altered by prehistoric glaciation. In prehistoric and protohistoric times, over a period of at least 500,000 years, the area of present-day Poland went through the Stone Age, Bronze Age and Iron Age stages of development, along with the nearby regions. The Neolithic period ushered in the Linear Pottery culture, whose founders migrated from the Danube River area beginning about 5,500 BC. This culture was distinguished by the establishment of the first settled agricultural communities in modern Polish territory. Later, between about 4,400 and 2,000 BC, the native post-Mesolithic populations would also adopt and further develop the agricultural way of life.
Poland's Early Bronze Age began around 2300–2400 BC, whereas its Iron Age commenced c. 700–750 BC. One of the many cultures that have been uncovered, the Lusatian culture, spanned the Bronze and Iron Ages and left notable settlement sites. Around 400 BC, Poland was settled by La Tène culture Celtic arrivals. They were soon followed by emerging cultures with a strong Germanic component, influenced first by the Celts and then by the Roman Empire. The Germanic peoples migrated out of the area by about 500 AD during the great Migration Period of the European Dark Ages. Wooded regions to the north and east were settled by Balts.
According to mainstream archaeological research, Slavs have resided in modern Polish territories for over 1500 years. Recent genetic studies, however, determined that people who live in the current territory of Poland include the descendants of people who inhabited the area for thousands of years, beginning in the early Neolithic period.
Slavs on the territory of Poland were organized into tribal units, of which the larger ones were later known as the Polish tribes; the names of many tribes are found on the list compiled by the anonymous Bavarian Geographer in the 9th century. In the 9th and 10th centuries, these tribes gave rise to developed regions along the upper Vistula, the coast of the Baltic Sea and in Greater Poland. This latest tribal undertaking resulted in the formation of a lasting political structure in the 10th century that became the state of Poland, one of the West Slavic nations.
Piast period (10th century–1385).
Mieszko I.
Poland was established as a nation state under the Piast dynasty, which ruled the country between the 10th and 14th centuries. Historical records of an official Polish state begin with Duke Mieszko I in the second half of the 10th century. Mieszko, who began his rule sometime before 963 and continued as the Polish monarch until his death in 992, chose to be baptized in the Western Latin Rite, probably on 14 April 966, following his marriage to Princess Dobrawa of Bohemia. This event became known as the baptism of Poland, and its date is often used to mark the beginning of Polish statehood symbolically. Mieszko completed a unification of the West Slavic tribal lands that was fundamental to the new country's existence. The "Dagome iudex", a document from the year 991 AD, places Mieszko's country under the protection of the pope. Following its emergence, the Polish nation was led by a series of rulers who converted the population to Christianity, created a strong Kingdom of Poland and fostered a distinctive Polish culture that was integrated into broader European culture.
Bolesław I Chrobry.
Mieszko's son, Duke Bolesław I Chrobry (r. 992–1025), established a Polish Church structure, pursued territorial conquests and was officially crowned the first king of Poland in 1025, near the end of his life. Bolesław also sought to spread Christianity to parts of eastern Europe that remained pagan, but suffered a setback when his greatest missionary, Adalbert of Prague, was killed in Prussia in 997. During the Congress of Gniezno in the year 1000, Holy Roman Emperor Otto III recognized the Archbishopric of Gniezno, an institution crucial for the continuing existence of the sovereign Polish state. During the reign of Otto's successor, Holy Roman Emperor Henry II, Bolesław fought prolonged wars with the Kingdom of Germany between 1002 and 1018.
Piast monarchy under Casimir I, Bolesław II and Bolesław III.
Bolesław's expansive rule overstretched the military resources of the early Polish state, and it was followed by a collapse of the monarchy. Restoration took place under Casimir I (r. 1039–58). Casimir's son Bolesław II the Bold (r. 1058–79) became involved in a conflict with Bishop Stanislaus of Szczepanów that seriously marred his reign. Bolesław had the bishop murdered in 1079 after being excommunicated by the Polish church on charges of adultery. This act sparked a revolt of Polish nobles that led to Bolesław's deposition and expulsion from the country. Around 1116, Gallus Anonymous wrote a seminal chronicle, the "Gesta principum Polonorum", intended as a glorification of his patron Bolesław III Wrymouth (r. 1107–38), a ruler who revived the tradition of military prowess of Bolesław I's time. Gallus' work became important as a key source for the early history of Poland.
Fragmentation.
After Bolesław III divided Poland among his sons in his Testament of 1138, internal fragmentation eroded the Piast monarchical structures in the 12th and 13th centuries. In 1180, Casimir II, who sought papal confirmation of his status as a senior duke, granted immunities and additional privileges to the Polish Church at the Congress of Łęczyca. Around 1220, Wincenty Kadłubek wrote his "Chronica seu originale regum et principum Poloniae", another major source for early Polish history. In 1226, one of the regional Piast dukes, Konrad I of Masovia, invited the Teutonic Knights to help him fight the Baltic Prussian pagans. Konrad's move caused centuries of warfare between Poland and the Teutonic Knights, and later between Poland and the German Prussian state. The first Mongol invasion of Poland began in 1240; it culminated in the defeat of Polish and allied Christian forces and the death of the Silesian Piast Duke Henry II at the Battle of Legnica in 1241. In 1242, Wrocław became the first Polish municipality to be incorporated, as the period of fragmentation brought economic development and growth of towns. In 1264, Bolesław the Pious granted Jewish liberties in the Statute of Kalisz.
Late Piast monarchy under Władysław I and Casimir III.
Attempts to reunite the Polish lands gained momentum in the 13th century, and in 1295, Duke Przemysł II of Greater Poland managed to become the first ruler since Bolesław II to be crowned king of Poland. He ruled over a limited territory and was soon killed. In 1300–05 the Czech ruler Václav II also reigned as king of Poland. The Piast Kingdom was effectively restored under Władysław I the Elbow-high (r. 1306–33), who was crowned king in 1320. In 1308, the Teutonic Knights seized Gdańsk and the surrounding region (Pomerelia).
King Casimir III the Great (r. 1333–70), Władysław's son and the last of the Piast rulers, strengthened and expanded the restored Kingdom of Poland, but the western provinces of Silesia (formally ceded by Casimir in 1339) and most of Pomerania were lost to the Polish state for centuries to come. Progress was made in the recovery of the central province of Mazovia, however, and in 1340, the conquest of Red Ruthenia began, marking Poland's expansion to the east. The Congress of Kraków, a vast convocation of central, eastern, and northern European rulers probably assembled to plan an anti-Turkish crusade, took place in 1364, the same year that the future Jagiellonian University, one of the oldest European universities, was founded.
Angevin transition.
After the Polish royal line and Piast junior branch died out in 1370, Poland came under the rule of Louis I of Hungary of the Angevin dynasty, who presided over a union of Hungary and Poland that lasted until 1382. In 1374, Louis granted the Polish nobility the Privilege of Koszyce to assure the succession of one of his daughters in Poland. His youngest daughter Jadwiga (died 1399) assumed the Polish throne in 1384.
Jagiellonian dynasty (1385–1572).
Dynastic union with Lithuania, Władysław II Jagiełło.
In 1386, Grand Duke Jogaila of Lithuania became also a king of Poland, to rule as Władysław II Jagiełło until 1434. The act established a Polish–Lithuanian union ruled by the Jagiellonian dynasty. The first in a series of formal "unions" was the Union of Krewo of 1385, whereby arrangements were made for the marriage of Jogaila and Queen Jadwiga. The Polish-Lithuanian partnership brought vast areas of Ruthenia controlled by the Grand Duchy of Lithuania into Poland's sphere of influence and proved beneficial for the nationals of both countries, who coexisted and cooperated in one of the largest political entities in Europe for the next four centuries (also after the extinction of the Jagellonian dynasty in 1572). When Queen Jadwiga died in 1399, the Kingdom of Poland fell to her husband's sole possession; her gifts helped to renew the activities of the University in 1400.
In the Baltic Sea region, Poland's struggle with the Teutonic Knights continued and culminated in the Battle of Grunwald (1410), a great victory that the Poles and Lithuanians were unable to follow up with a decisive strike against the main seat of the Order at Malbork Castle. The Union of Horodło of 1413 further defined the evolving relationship between the Crown of Poland and the Grand Duchy of Lithuania and their elites.
The privileges of the "szlachta" (nobility) kept growing and in 1425 the rule of "Neminem captivabimus", which protected the noblemen from arbitrary royal arrests, was formulated.
Władysław III and Casimir IV Jagiellon.
The reign of the young Władysław III (1434–44), a king of Poland and Hungary, was cut short by his death at the Battle of Varna fought against the forces of the Ottoman Empire.
Critical developments of the Jagiellonian period were concentrated in the long reign of Casimir IV Jagiellon (1447–92). In 1454, Royal Prussia was incorporated by Poland and the Thirteen Years' War of 1454–66 with the Teutonic state ensued. In 1466, the milestone Peace of Thorn was concluded. This treaty divided Prussia to create East Prussia, the future Duchy of Prussia, a separate entity that functioned as a fief of Poland under the administration of the Teutonic Knights. Poland also confronted the Ottoman Empire and the Crimean Tatars in the south, and in the east helped Lithuania fight the Grand Duchy of Moscow. The country was developing as a feudal state, with a predominantly agricultural economy and an increasingly dominant landed nobility. Kraków, the royal capital, was turning into a major academic and cultural center, and in 1473 the first printing press began operating there. With the growing importance of the "szlachta", the king's council evolved to become by 1493 a bicameral general sejm (parliament) that no longer represented only the top dignitaries of the realm.
The "Nihil novi" act, adopted in 1505 by the Sejm, transferred most of the legislative power from the monarch to the Sejm. This event marked the beginning of the period known as "Golden Liberty", when the state was ruled in principle by the "free and equal" Polish nobility. In the 16th century, the massive development of folwark agribusinesses operated by the nobility led to increasingly abusive conditions for the peasant serfs who worked them. The political monopoly of the nobles also stifled the development of cities, some of which were thriving during the late Jagiellonian era, and limited the rights of townspeople, effectively holding back the emergence of a middle class.
Early modern Poland under Sigismund I and Sigismund II.
Protestant Reformation movements made deep inroads into Polish Christianity and the resulting Reformation in Poland phenomenon involved a number of different denominations. The policies of religious tolerance that developed were nearly unique in Europe at that time and many who fled regions torn by religious strife found refuge in Poland. The reigns of King Sigismund I and King Sigismund II Augustus witnessed an intense cultivation of culture and science (a Golden Age of the Renaissance in Poland), of which the astronomer Nicolaus Copernicus (died 1543) is the best known representative. In 1525, during the reign of Sigismud I (1506–48), the Teutonic Order was secularized and Duke Albrecht von Hohenzollern performed an act of homage before the Polish king (the Prussian Homage) for his fief, the Duchy of Prussia. Mazovia was finally fully incorporated into the Polish Crown in 1529.
The reign of Sigismund II (1548–72) ended the Jagiellonian period, but gave rise to the Union of Lublin (1569), the ultimate fulfillment of the union with Lithuania. This agreement transferred Ukraine from the Grand Duchy of Lithuania to Poland and transformed the Polish-Lithuanian polity into a real union, preserving it beyond the death of the childless Sigismund II, whose active involvement made the completion of this process possible.
Livonia in the far northeast was incorporated by Poland in 1561 and Poland entered the Livonian War against Russia. The executionist movement (an attempt to prevent domination by the magnate families of Poland and Lithuania) peaked at the sejm in Piotrków in 1562–63. On the religious front, the Polish Brethren split from the Calvinists, and the Protestant Brest Bible was published in 1563. The Jesuits, who arrived in 1564, were destined to make a major impact on Poland's history.
Polish–Lithuanian Commonwealth.
Establishment (1569–1648).
Union of Lublin.
The Union of Lublin of 1569 established the Polish–Lithuanian Commonwealth, a more closely unified federal state than the earlier political arrangement between Poland and Lithuania. The Union was largely run by the nobility through the system of a central parliament and local assemblies, but was led by elected kings. The formal rule of the nobility, who were proportionally more numerous than in other European countries, constituted an early democratic system ("a sophisticated noble democracy"), in contrast to the absolute monarchies prevalent at that time in the rest of Europe. The beginning of the Commonwealth coincided with a period in Polish history of great political power, advancements in civilization and prosperity. The Polish–Lithuanian Union became an influential player in Europe and a vital cultural entity that spread Western culture (with Polish characteristics) eastward. In the second half of the 16th century and the first half of the 17th century, the Commonwealth was one of the largest and most populous states in contemporary Europe, with an area approaching one million square kilometres and a population of about ten million. Its economy was dominated by export-focused agriculture. Nationwide religious toleration was guaranteed at the Warsaw Confederation in 1573.
First elective kings.
After the rule of the Jagiellonian dynasty had ended, Henry of Valois (later King Henry III of France) was the winner of the first "free election" by the Polish nobility in 1573. He had to agree to the restrictive "pacta conventa" obligations, but soon fled Poland when news arrived of the vacancy of the French throne, to which he was the heir presumptive. From the start, the royal elections increased foreign influence in the Commonwealth as foreign powers sought to manipulate the Polish nobility to place candidates amicable to their interests.
The reign of Stephen Báthory of Hungary (1576–86) followed; he was militarily and domestically assertive. The establishment of the legal Crown Tribunal in 1578 meant a transfer of many appellate cases from the royal to noble jurisdiction. Jan Kochanowski, a poet and the premier artistic personality of the Polish Renaissance, died in 1584.
Vasa dynasty kings.
The Commonwealth suffered from dynastic distractions (the Vasa kings unsuccessfully attempted to obtain the Swedish crown and prioritized this activity) during the reigns of the Swedish House of Vasa kings Sigismund III (1587–1632) and Władysław IV (1632–48). The Catholic Church embarked on an ideological counter-offensive and the Counter-Reformation claimed many converts from Polish and Lithuanian Protestant circles. In 1596, the Union of Brest split the Eastern Christians of the Commonwealth to create the Uniate Church of the Eastern Rite, but subject to the authority of the pope. The Zebrzydowski Rebellion against Sigismund III unfolded in 1606–8.
The Commonwealth fought wars between 1605 and 1618 with Russia for supremacy in Eastern Europe in the wake of Russia's Time of Troubles, a period referred to as the Polish–Muscovite War (or the "Dymitriads"). The efforts resulted in expansion of the eastern territories of the Polish–Lithuanian Commonwealth, but the goal of taking over the Russian throne for the Polish ruling dynasty was not achieved. Sweden sought supremacy in the Baltic during the Polish–Swedish wars of 1617–29, and the Ottoman Empire pressed from the south in the Battles at Cecora in 1620 and Khotyn in 1621. The agricultural expansion and serfdom policies in Polish Ukraine resulted in a series of Cossack uprisings. Allied with the Habsburg Monarchy, the Commonwealth did not directly participate in the Thirty Years' War. Władysław's IV reign was mostly peaceful, with a Russian invasion in the form of the Smolensk War of 1632–34 successfully repelled. The Orthodox Church hierarchy, banned in Poland after the Union of Brest, was re-established in 1635.
Decline (1648–1764).
Deluge of wars.
During the reign of John II Casimir Vasa (1648–68), the nobles' democracy fell into decline as a result of foreign invasions and domestic disorder. These calamities multiplied rather suddenly and marked the end of the Polish Golden Age. Their effect was to render the once powerful Commonwealth increasingly vulnerable to foreign intervention.
The Cossack Khmelnytsky Uprising of 1648–57 engulfed the south-eastern regions of the Polish crown; its long-term effects were disastrous for the Commonwealth. The first "liberum veto" (a parliamentary device that allowed any member of the Sejm to dissolve a current session immediately) was exercised by a deputy in 1652. This practice would eventually weaken Poland's central government critically. In the Treaty of Pereyaslav (1654), the Ukrainian rebels declared themselves subjects of the Tsar of Russia. The Second Northern War raged through the core Polish lands in 1655–60, including an invasion of Poland so brutal and devastating that it is referred to as the Swedish Deluge. The war ended in 1660 with the Treaty of Oliva, which resulted in the loss of some of Poland's northern possessions. In 1657 the Treaty of Wehlau-Bromberg established the independence of the Duchy of Prussia. The Commonwealth forces did well in the Russo-Polish War of 1654–67, but the end result was the permanent division of Ukraine between Poland and Russia, as agreed to in the Truce of Andrusovo (1667). Towards the end of the war, the Rokosz of Lubomirski, a major magnate rebellion against the king, destabilized and weakened the country. The large-scale slave raids of the Crimean Tatars also had highly deleterious effects on the Polish economy. "Merkuriusz Polski", the first Polish newspaper, was published in 1661.
John III Sobieski and last military victories.
The Second Polish–Ottoman War (1672–76) broke out during the reign of King Michał Korybut Wiśniowiecki (1669–73) and continued under his successor, John III Sobieski (1674–96). Sobieski intended to pursue Baltic area expansion (and to this end he signed the secret Treaty of Jaworów with France in 1675), but was forced instead to fight protracted wars with the Ottoman Empire. By doing so the hetman who became king briefly revived the Commonwealth's military might. He defeated the expanding Muslims at the Battle of Khotyn in 1673 and decisively helped deliver Vienna from a Turkish onslaught at the Battle of Vienna in 1683. Sobieski's reign marked the last high point in the history of the Commonwealth: in the first half of the 18th century Poland ceased to be an active player in international politics. The Eternal Peace Treaty with Russia of 1686 was the final border settlement between the two countries before the First Partition of Poland in 1772.
The Commonwealth, subjected to almost constant warfare until 1720, suffered enormous population losses and massive damage to its economy and social structure. The government became ineffective in the wake of large-scale internal conflicts, corrupted legislative processes and manipulation by foreign interests. The nobility fell under the control of a handful of feuding magnate families with established territorial domains. The urban population and infrastructure fell into ruin, together with most peasant farms, whose inhabitants were subjected to increasingly extreme forms of serfdom. The development of science, culture and education came to a halt or regressed.
Saxon kings.
The royal election of 1697 brought a ruler of the Saxon House of Wettin to the Polish throne: Augustus II, "the Strong" (r. 1697–1733), who was able to assume the throne only by agreeing to convert to Roman Catholicism. He was succeeded eventually by his son Augustus III (r. 1734–63). The reigns of the Saxon kings (who were both simultaneously prince-electors of Saxony) were disrupted by competing candidates for the throne and witnessed further disintegration of the Commonwealth. The Great Northern War (1700–21), a period seen by the contemporaries as a temporary eclipse, may have been the fatal blow that brought down the Polish political system. Stanisław Leszczyński was installed as king in 1704 under Swedish protection, but lasted only a few years. The Silent Sejm of 1717 marked the beginning of the Commonwealth's existence as a Russian protectorate: the Tsardom would guarantee the reform-impeding Golden Liberty of the nobility from that time on in order to cement the Commonwealth's weak central authority and a state of perpetual political impotence. In a resounding break with traditions of religious tolerance, Protestants were executed during the Tumult of Thorn in 1724. In 1732 Russia, Austria and Prussia, Poland's three increasingly powerful and scheming neighbors, entered into the secret Treaty of the Three Black Eagles with the intention of controlling the future royal succession in the Commonwealth. The War of the Polish Succession was fought in 1733–35 to assist Leszczyński in assuming the throne of Poland for a second time. Amidst considerable foreign involvement, his efforts were unsuccessful. The Kingdom of Prussia became a strong regional power and succeeded in wresting the historically Polish province of Silesia from the Habsburg Monarchy in the Silesian Wars; it thus became an ever greater threat to Poland's security. The personal union between the Commonwealth and the Electorate of Saxony did give rise to the emergence of a reform movement in the Commonwealth and the beginnings of the Polish Enlightenment culture, the major positive developments of this era. The first Polish public library was the Załuski Library in Warsaw, opened to the public in 1747.
Reforms and loss of statehood (1764–95).
Czartoryski reforms and Stanisław August Poniatowski.
During the later part of the 18th century, fundamental internal reforms were attempted in the Polish–Lithuanian Commonwealth as it slid into extinction. The reform activity, initially promoted by the magnate Czartoryski family faction known as the "Familia", provoked a hostile reaction and eventually a military response on the part of neighboring powers—yet it created conditions that fostered economic improvement. The most populous urban center, the capital city of Warsaw, replaced Danzig (Gdańsk) as the leading trade center, and the importance of the more prosperous urban strata increased. The last decades of the independent Commonwealth's existence were characterized by intense reform movements and far-reaching progress in the areas of education, intellectual life, art, and, especially toward the end of the period, in the evolution of the social and political system.
The royal election of 1764 resulted in the elevation of Stanisław August Poniatowski, a refined and worldly aristocrat connected to the Czartoryski family, but hand-picked and imposed by Empress Catherine the Great of Russia, who expected him to be her obedient follower. Stanisław August ruled the Polish–Lithuanian state until its dissolution in 1795. The king spent his reign torn between his desire to implement reforms necessary to save the failing state and the perceived necessity of remaining in a subordinate relationship to his Russian sponsors.
The Bar Confederation (1768–72) was a noble rebellion directed against Russia's influence in general and Stanisław August, who was seen as its representative, in particular. It was fought to preserve Poland's independence and the nobility's traditional interests. After several years, it was brought under control by forces loyal to the king and those of the Russian Empire.
Following the suppression of the Bar Confederation, at the instigation of Frederick the Great of Prussia, Poland was divided up among Prussia, Austria and Russia in 1772, with only a rump state remaining. In what became known as the First Partition of Poland, the outer provinces of the Commonwealth were seized by agreement among the country's three powerful neighbors. In 1773, the Partition Sejm "ratified" under duress the partition as a "fait accompli". However, it also established the Commission of National Education, a pioneering in Europe education authority often called the world's first ministry of education.
Great Sejm and May 3 Constitution.
The long-lasting Sejm convened by Stanisław August in 1788 is known as the Great Sejm, or "Four-Year" Sejm. Its landmark achievement was the passing of the Constitution of May 3, 1791, the first singular pronouncement of a supreme law of the state in modern Europe, also characterized as the world's third oldest constitution. A reformist but moderate document condemned by detractors as being of French revolutionary sympathies, it soon generated strong opposition from the conservative circles of the Commonwealth's upper nobility and the Russian Empress Catherine, who was determined to prevent the rebirth of a strong Commonwealth. The nobility's Targowica Confederation, formed in Russian imperial capital of Saint Petersburg, appealed to Catherine for help, and in May 1792, the Russian army entered the Commonwealth's territory. The Polish–Russian War of 1792, a defensive war fought by the forces of the Commonwealth against Russian invaders, ended when the Polish king, convinced of the futility of resistance, capitulated by joining the Targowica Confederation. The Confederation took over the government, but Russia and Prussia in 1793 arranged for the Second Partition of Poland, which left the country with a critically reduced territory that rendered it essentially incapable of an independent existence. The Commonwealth's Grodno Sejm of 1793, the last Sejm of its existence, was compelled to confirm the new partition.
Kościuszko Uprising and loss of independence.
Radicalized by recent events, Polish reformers (whether in exile or still resident in the reduced area remaining to the Commonwealth) were soon working on preparations for a national insurrection. Tadeusz Kościuszko, a popular general and a veteran of the American Revolution, was chosen as its leader. He returned from abroad and issued Kościuszko's proclamation in Kraków on March 24, 1794. It called for a national uprising under his supreme command. Kościuszko emancipated many peasants in order to enroll them as "kosynierzy" in his army, but the hard-fought insurrection, despite widespread national support, proved incapable of generating the foreign assistance necessary for its success. In the end it was suppressed by the combined forces of Russia and Prussia, with Warsaw captured in November 1794 at the Battle of Praga. In 1795, a Third Partition of Poland was undertaken by all three of the partitioning powers (Russia, Prussia and Austria) as a final division of territory that resulted in the effective dissolution of the Polish–Lithuanian Commonwealth. The Polish king was escorted to Grodno, forced to abdicate, and retired to Saint Petersburg. Kościuszko, initially imprisoned, was allowed to emigrate to the United States in 1796.
The response of the Polish leadership to the last partition is a matter of historical debate. Literary scholars found that the dominant emotion of the first decade was despair that produced a moral desert ruled by violence and treason. On the other hand, historians have looked for signs of resistance to foreign rule. Apart from those who went into exile, the nobility took oaths of loyalty to their new rulers and served as officers in their armies.
Partitioned Poland.
Armed resistance (1795–1864).
Napoleonic wars.
Although no sovereign Polish state existed between 1795 and 1918, the idea of Polish independence was kept alive throughout the 19th century. There were a number of uprisings and other military conflicts against the partitioning powers. Military efforts after the partitions were first based on the alliances of Polish émigrés with post-revolutionary France. Jan Henryk Dąbrowski's Polish Legions fought in French campaigns outside of Poland between 1797 and 1802 in hopes that their involvement and contribution would be rewarded with the liberation of their Polish homeland. The Polish national anthem, "Poland Is Not Yet Lost", or "Dąbrowski's Mazurka", was written in praise of his actions by Józef Wybicki in 1797.
The Duchy of Warsaw, a small, semi-independent Polish state, was created in 1807 by Napoleon Bonaparte in the wake of his defeat of Prussia and the signing of the Peace of Tilsit with Emperor Alexander I of Russia. The Army of the Duchy of Warsaw, led by Józef Poniatowski, participated in numerous campaigns in alliance with France, including the successful Polish–Austrian War of 1809, which, combined with the outcomes of other theaters of the War of the Fifth Coalition, resulted in an enlargement of the Duchy's territory. The French invasion of Russia in 1812 and the German campaign of 1813 saw the Duchy's last military engagements. The Constitution of the Duchy of Warsaw abolished serfdom as a reflection of the ideals of the French Revolution, but it did not promote land reform.
Congress of Vienna.
After Napoleon's defeat, a new European order was established at the Congress of Vienna. Adam Czartoryski, a former close associate of Alexander I, became the leading advocate for the Polish national cause. The Congress implemented a new partition scheme, which took into account some of the gains realized by the Poles during the Napoleonic period. The Duchy of Warsaw was replaced in 1815 with a new Kingdom of Poland, unofficially known as Congress Poland. The residual Polish kingdom was joined to the Russian Empire in a personal union under the Russian tsar, and it was allowed its own constitution and military. East of the Kingdom, large areas of the former Polish–Lithuanian Commonwealth remained directly incorporated into the Russian Empire as the Western Krai; both these territories are generally referred to as the Russian Partition. The Russian, Prussian, and Austrian "partitions" were the lands of the former Commonwealth, not actual units of its administrative division. The Prussian Partition was formed from territories acquired from Poland and included a portion separated as the Grand Duchy of Posen. Peasants under the Prussian administration were gradually enfranchised under the reforms of 1811 and 1823. The limited legal reforms in the Austrian Partition were overshadowed by its rural poverty. The Free City of Kraków was a tiny republic newly created by the Congress of Vienna in 1815 under the joint supervision of the three partitioning powers. As bleak as the new political divisions of the former Commonwealth were to Polish patriots, economic progress was made because the period after the Congress of Vienna witnessed a significant development in the building of early industry in the lands taken over by foreign powers.
Uprising of November 1830.
The increasingly repressive policies of the partitioning powers led to resistance movements in partitioned Poland, and in 1830 Polish patriots staged the November Uprising. This revolt developed into a full-scale war with Russia, but the leadership was taken over by Polish conservatives who were reluctant to challenge the Empire and hostile to broadening the independence movement's social base through measures such as land reform. Despite the significant resources mobilized, a series of mistakes by several successive chief commanders appointed by the Polish government who were either reluctant to serve or performed incompetently in battle led to the defeat of the insurgents by the Russian army in 1831. Congress Poland lost its constitution and military, but formally remained a separate administrative unit within the Russian Empire.
After the defeat of the November Uprising, thousands of former Polish combatants and other activists emigrated to Western Europe. This phenomenon, known as the Great Emigration, soon dominated Polish political and intellectual life. Together with the leaders of the independence movement, the Polish community abroad included the greatest Polish literary and artistic minds, including the Romantic poets Adam Mickiewicz (traditionally considered Poland's greatest poet, who died as an émigré in 1855), Juliusz Słowacki, Cyprian Norwid, and the composer Frédéric Chopin. In occupied and repressed Poland, some sought progress through nonviolent activism focused on education and economy, known as organic work; others, in cooperation with emigrant circles, organized conspiracies and prepared for the next armed insurrection.
Spring of Nations era revolts.
After the authorities in the partitions had found out about secret preparations, the planned national uprising failed to materialize. The Greater Poland Uprising ended in a fiasco in early 1846. In the Kraków Uprising of February 1846, patriotic action was combined with revolutionary demands, but the result was the incorporation of the Republic of Kraków into the Austrian Partition. The Austrian officials took advantage of peasant discontent and incited villagers against the noble-dominated insurgent units. This resulted in the Galician slaughter (1846), a large-scale rebellion of serfs seeking relief from their post-feudal "folwark" condition of slavery. The uprising freed many from bondage and hastened decisions that led to peasant enfranchisement in the Austrian Empire (1848). A new wave of Polish military and other involvement, in the partitions and in other parts of Europe (e.g. Józef Bem in Austria and Hungary), soon took place in the context of the 1848 Spring of Nations revolutions. In particular, the events in Berlin precipitated the Greater Poland Uprising, where peasants in the Prussian Partition, who were by then largely enfranchised, played a prominent role.
Uprising of January 1863.
Despite the limited liberalization measures allowed in the Kingdom of Poland under the rule of Tsar Alexander II of Russia, a renewal of popular liberation activities took place in 1860–61. During large-scale demonstrations in Warsaw, Russian forces inflicted numerous casualties on the civilian participants. The "Red", or left-wing faction, which promoted peasant enfranchisement and cooperated with the Russian revolutionaries, became involved in immediate preparations for a national uprising. The "White", or right-wing faction, was inclined to cooperate with the Russian authorities and countered with partial reform proposals. In order to cripple the manpower potential of the Reds, Aleksander Wielopolski, the conservative leader of the Kingdom's government, arranged for a partial selective conscription of young Poles for the Russian army in the years 1862 and 1863. This action hastened the outbreak of hostilities. The January Uprising, joined and led after the initial period by the Whites, was fought by partisan units against an overwhelmingly advantaged enemy. The war lasted from January 1863 to the spring of 1864, when Romuald Traugutt, the last supreme commander of the insurgency, was captured by the tsarist police.
On 2 March 1864, the Russian authority, compelled by the Uprising to compete for the loyalty of Polish peasants, officially published an enfranchisement decree in Congress Poland along the lines of an earlier land reform proclamation of the insurgents. The act created the conditions necessary for the development of the capitalist system on central Polish lands. At the time when the futility of armed resistance without external support was realized by most Poles, the various sections of Polish society were undergoing deep and far-reaching social, economic and cultural changes.
Formation of modern Polish society under foreign rule (1864–1914).
Repression and organic work.
The failure of the January Uprising in Poland caused a major psychological trauma and became a historic watershed; indeed, it sparked the development of modern Polish nationalism. The Poles, subjected within the territories under the Russian and Prussian administrations to still stricter controls and increased persecution, preserved their identity in non-violent ways. After the Uprising, Congress Poland was downgraded in official usage from the "Kingdom of Poland" to the "Vistula Land" and was more fully integrated into Russia proper, but not entirely obliterated. The Russian and German languages were imposed in all public communication, and the Catholic Church was not spared from severe repression; public education was increasingly subjected to Russification and Germanization measures. Illiteracy was reduced, most effectively in the Prussian partition, but education in Polish was preserved mostly through unofficial efforts. The Prussian government pursued German colonization, including the purchase of Polish-owned land. On the other hand, the region of Galicia in western Ukraine and southern Poland, experienced a gradual relaxation of authoritarian policies and even a Polish cultural revival. Economically and socially backward, it was under the milder rule of the Austro-Hungarian Monarchy and from 1867 allowed increasingly limited autonomy. "Stańczycy", a conservative Polish pro-Austrian faction led by great land owners, dominated the Galician government. The Polish Academy of Arts and Sciences was founded in Kraków in 1872. Positivism replaced Romanticism as the leading intellectual, social and literary trend.
Social activities termed "organic work" consisted of self-help organizations that promoted economic advancement and worked on improving the competitiveness of Polish-owned businesses: industrial, agricultural or other. New commercial methods and ways of generating higher productivity were discussed and implemented through trade associations and special interest groups, while Polish banking and cooperative financial institutions made the necessary business loans available. The other major area of effort in organic work was the educational and intellectual development of the common people. Many libraries and reading rooms were established in small towns and villages, and numerous printed periodicals reflected the growing interest in popular education. Scientific and educational societies were active in a number of cities. Such activities were most pronounced in the Prussian Partition.
Economic development and social change.
Under the partitioning powers, large-scale industrialization, economic diversification and progress were introduced in the traditionally agrarian Polish lands, but this development turned out to be very uneven. In the Prussian Partition, advanced agriculture was practiced, except for Upper Silesia, where the coal-mining industry created a large labor force. The densest network of railroads was built in German-ruled western Poland. In Russian Congress Poland, a striking growth of industry, railways and towns was taking place, all against the background of an extensive, but less productive agriculture. Warsaw (a metallurgical center) and Łódź (a textiles center) grew rapidly, as did the total proportion of the urban population, making the region the most advanced in the Russian Empire (industrial production exceeded agricultural production by 1909). The coming of the railways spurred some industrial growth even in the vast Russian Partition territories outside Congress Poland. The Austrian Partition was rural and poor, except for the industrialized Cieszyn Silesia area. Galician economic expansion after 1890 included oil extraction and resulted in the growth of Lemberg (Lwów, Lviv) and Kraków.
Economic and social changes involving land reform and industrialization, combined with the effects of foreign domination, altered the centuries-old social structure of Polish society. Among the newly emergent strata were wealthy industrialists and financiers, distinct from the traditional, but still critically important landed aristocracy. The intelligentsia, an educated, professional or business middle class, often originated from lower gentry, landless or alienated from their rural possessions, and from urban people. Many smaller agricultural enterprises based on serfdom did not survive the land reforms. The industrial proletariat, a new underprivileged class, was composed mainly of poor peasants or townspeople forced by deteriorating conditions to migrate and search for work in urban centers in their countries of origin or abroad. Millions of residents of the former Commonwealth of various ethnic groups worked or settled in Europe and in North and South America.
Social and economic changes were partial and gradual, and the degree of (fast-paced in some areas) industrialization generally lagged behind the advanced regions of western Europe. The three partitions developed different economies and were more economically integrated with their mother states than with each other (for example the Prussian Partition's agricultural production depended heavily on the German market, whereas the industrial sector of Congress Poland relied more on the Russian market).
Nationalism, socialism and other movements.
In the 1870s–90s, large-scale socialist, nationalist, agrarian and other political movements of great ideological fervor became established in partitioned Poland and Lithuania, along with corresponding political parties to promote them. Of the major parties, the socialist First Proletariat was founded in 1882, the Polish League (precursor of National Democracy) in 1887, the Polish Social Democratic Party of Galicia and Silesia in 1890, the Polish Socialist Party in 1892, the Marxist SDKPiL in 1893, the agrarian People's Party of Galicia in 1895 and the Jewish socialist Bund in 1897. Christian democracy regional associations allied with the Catholic Church were also active; they united into the Polish Christian Democratic Party in 1919. The main minority ethnic groups of the former Commonwealth, including Ukrainians, Lithuanians, Belarusians and Jews, were getting involved in their own national movements and plans, which met with disapproval on the part of those Polish independence activists who counted on an eventual rebirth of the Commonwealth or the rise of a Commonwealth-inspired federal structure (a political movement referred to as Prometheism).
Around the start of the 20th century, the Young Poland cultural movement, centered in Galicia, took advantage of a milieu conducive to liberal expression in that region and was the source of Poland's finest artistic and literary productions. In this same era, Marie Skłodowska-Curie, a pioneer radiation scientist, performed her groundbreaking research in Paris.
Revolution of 1905.
The Revolution of 1905–07 in Russian Poland, the result of many years of pent-up political frustrations and stifled national ambitions, was marked by political maneuvering, strikes and rebellion. The revolt was part of much broader disturbances throughout the Russian Empire associated with the general Revolution of 1905. In Poland, the principal revolutionary figures were Roman Dmowski and Józef Piłsudski. Dmowski was associated with the right-wing nationalist movement National Democracy, whereas Piłsudski was associated with the Polish Socialist Party. As the authorities re-established control within the Russian Empire, the revolt in Congress Poland, placed under martial law, withered as well, partially as a result of tsarist concessions in the areas of national and workers' rights, including Polish representation in the newly created Russian Duma. Some of the acquired gains were, however, rolled back, which coupled with intensified Germanization in the Prussian Partition, left Austrian Galicia as the territory most amenable to patriotic action.
In the Austrian Partition, Polish culture was openly cultivated, and in the Prussian Partition, higher standards of developed civilization were achieved, but the Russian Partition remained of primary importance for the Polish nation and its aspirations. About 15.5 million Polish-speakers lived in core central and western Poland, over a relatively small and compact territory. Much fewer were spread in the east: 1.3 million in Austrian Eastern Galicia and about 2 million along Russia's western districts, with the heaviest concentration in the Vilnius Region.
Polish paramilitary organizations oriented toward independence, such as the Union of Active Struggle, were being formed in 1908–14, mainly in Galicia. The Poles were divided, and their political parties fragmented on the eve of World War I, with Dmowski's National Democracy (pro-Entente) and Piłsudski's faction assuming opposing positions.
World War I and Poland's independence issue.
The outbreak of World War I in the Polish lands offered Poles unexpected hopes for achieving independence as a result of the turbulence that engulfed the empires of the partitioning powers. All three of the monarchies that had benefited from the partition of Polish territories (Germany, Austria and Russia) were dissolved by the end of the war, and many of their territories were dispersed into new political units. At the start of the war, the Poles found themselves conscripted into the armies of the partitioning powers in a war that was not theirs. Furthermore, they were frequently forced to fight each other, since the armies of Germany and Austria were allied against Russia. Piłsudski's paramilitary units stationed in Galicia were turned into the Polish Legions in 1914, and as a part of the Austro-Hungarian Army, they fought on the Russian front until 1917, when the formation was disbanded. Piłsudski, who refused the demands that his men fight under German command, was arrested and imprisoned by the Germans and became a heroic symbol of Polish nationalism.
Due to a series of German victories on the Eastern Front, the area of Congress Poland became occupied by the Central Powers of Germany and Austria; Warsaw was captured by the Germans on 5 August 1915. In the Act of 5th November 1916, a fresh incarnation of the Kingdom of Poland ("Królestwo Regencyjne") was created by Germany and Austria on formerly Russian-controlled territories within the German Mitteleuropa scheme. The sponsor states were never able to agree on a candidate to assume the throne, however; rather, it was governed in turn by German and Austrian Governor-Generals, a Provisional Council of State, and a Regency Council. This puppet, but increasingly autonomous state existed until November 1918, when it was replaced by the newly established Republic of Poland. The existence of this "kingdom" and its planned Polish army had a positive effect on the Polish national efforts on the Allied side. But the Treaty of Brest-Litovsk (March 1918) between Germany and defeated Russia ignored Polish interests.
The independence of Poland had been campaigned for in Russia and in the West by Dmowski and in the West by Ignacy Paderewski. Tsar Nicholas II of Russia, and then the leaders of the February Revolution and the October Revolution of 1917, installed governments who declared in turn their support for Polish independence. In 1917, France formed the Blue Army (placed under Józef Haller) that comprised about 70,000 Poles by the end of the war, including men captured from German and Austrian units and 20,000 volunteers from the U.S. There was also a 30,000-men strong Polish anti-German army in Russia. Dmowski, operating from Paris as head of the Polish National Committee (KNP), became the spokesman for Polish nationalism in the Allied camp. On the initiative of Woodrow Wilson's Fourteen Points, Polish independence was officially endorsed by the Allies in June 1918.
In all, about two million Poles served in the war, counting both sides, and about 400–450,000 died. Much of the fighting on the Eastern Front took place in Poland, and civilian casualties and devastation were high. Total World War I casualties from 1914 to 1918, military and civilian, within the 1919–39 borders of Poland, were estimated at 1,128,000.
The final upsurge of the push for independence of Poland took place on the ground in October–November 1918. With the end of the war, Austro-Hungarian and German units were being disarmed, and the Austrian army's collapse freed Cieszyn and Kraków at the end of October. Lviv was then contested in the Polish–Ukrainian War of 1918–19. Ignacy Daszyński headed the first short-lived independent Polish government in Lublin from November 7, the leftist Provisional People's Government of the Republic of Poland, which was proclaimed as a democracy. Germany, now defeated, was forced by the Allies to stand down its large military forces in Poland. Overtaken by the German Revolution of 1918–19 at home, the Germans released Piłsudski from prison. He arrived in Warsaw on November 10 and was granted extensive authority by the Kingdom's Regency Council, which was also recognized by the Lublin government. On November 22 Piłsudski became the temporary head of state. He was held by many in high regard, but was resented by the right-wing National Democrats. The emerging Polish state was internally divided, heavily war-damaged and economically dysfunctional.
Second Polish Republic (1918–39).
Securing national borders, war with Soviet Russia.
After more than a century of foreign rule, Poland regained its independence at the end of World War I as one of the outcomes of the negotiations that took place at the Paris Peace Conference of 1919. The Treaty of Versailles that emerged from the conference set up an independent nation with an outlet to the sea, but left some of its boundaries to be decided by plebiscites. The largely German Free City of Danzig was granted a separate status that guaranteed its use as a port by Poland. In the end, the settlement of the German-Polish border turned out to be a prolonged and convoluted process. It helped engender the Greater Poland Uprising of 1918–19, the three Silesian Uprisings of 1919–21, the East Prussian plebiscite of 1920, the Upper Silesia plebiscite of 1921 and the 1922 Silesian Convention in Geneva.
Other boundaries were settled by war and subsequent treaties. A total of six border wars were fought in 1918–21, including the Polish–Czechoslovak border conflicts over Cieszyn Silesia in January 1919.
As distressing as these border conflicts were, the Polish–Soviet War of 1919–21 was the most important conflict of the era. Piłsudski had entertained far-reaching anti-Russian cooperative designs in Eastern Europe, and in 1919 the Polish forces pushed eastward into Lithuania, Belarus and Ukraine by taking advantage of the Russian preoccupation with the civil war, but they were soon confronted with the Soviet westward offensive of 1918–19. Western Ukraine was already a theater of the Polish–Ukrainian War, which eliminated the proclaimed West Ukrainian People's Republic in July 1919. In the autumn of 1919, Piłsudski rejected urgent pleas from the former Entente powers to support Anton Denikin's White movement in its advance on Moscow. The Polish–Soviet War proper began with the Polish Kiev Offensive in April 1920. Allied with the Directorate of Ukraine of the Ukrainian People's Republic, by June the Polish armies had advanced past Vilnius, Minsk and Kiev. At that time, a massive Soviet counter-offensive pushed the Poles out of most of Ukraine. On the northern front, the Soviet army reached the outskirts of Warsaw in early August. A Soviet triumph and the quick end of Poland seemed inevitable. However, the Poles scored a stunning victory at the Battle of Warsaw. Afterwards, more Polish military successes followed, the Soviets had to pull back and left swathes of territory occupied largely by Belarusians or Ukrainians to Polish rule. The new eastern boundary was finalized by the Peace of Riga in 1921.
The defeat of the Russian armies forced Vladimir Lenin and the Soviet leadership to postpone their strategic objective of linking up with the German and other European revolutionary-minded comrades and spread communist revolution. Lenin's hope of generating support for the Red Army in Poland had already failed to materialize. Piłsudski's seizure of Vilnius in October 1920 (known as Żeligowski's Mutiny) was a nail in the coffin of the already poor Polish–Lithuanian relations that had been strained by the Polish–Lithuanian War of 1919–20; both states would remain hostile to one another for the remainder of the interwar period. Piłsudski's planned Intermarium (an East European federation of states inspired by the tradition of the multiethnic Polish–Lithuanian Commonwealth that would include a hypothetical multinational successor state to the Grand Duchy of Lithuania) thus became incompatible with his assumption of Polish domination and encroachment on neighboring peoples' lands and aspirations at the time of rising national movements. It soon ceased to be a feature of Poland's politics. A larger federated structure was also opposed by Dmowski's National Democrats. Their representative at the Peace of Riga talks, Stanisław Grabski, opted for leaving Minsk, Berdychiv, Kamianets-Podilskyi and the surrounding areas on the Soviet side of the border, since the National Democrats did not want to permit population shifts that they considered politically undesirable, especially if the transfers would result in a reduced proportion of citizens who were ethnically Polish.
The Peace of Riga settled the eastern border by preserving for Poland a substantial portion of the old Commonwealth's eastern territories, at the cost of partitioning the lands of the former Grand Duchy of Lithuania (Lithuania and Belarus) and Ukraine. The Ukrainians ended up with no state of their own and felt betrayed by the Riga arrangements; their resentment gave rise to extreme nationalism and anti-Polish hostility. The Kresy (or borderland) territories in the east won by 1921 would form the basis for a swap arranged and carried out by the Soviets in 1943–45, who at that time compensated the re-emerging Polish state for the eastern lands lost to the Soviet Union with conquered areas of eastern Germany.
The successful outcome of the Polish–Soviet War gave Poland a false sense of its prowess as a self-sufficient military power and encouraged the government to try to resolve international problems through imposed unilateral solutions. The territorial and ethnic policies of the interwar period contributed to bad relations with most of Poland's neighbors and to uneasy cooperation with the more distant centers of power, including France, Britain and the League of Nations.
Democratic politics.
Among the chief difficulties faced by the government of the new Polish republic was the lack of an integrated infrastructure among the formerly separate partitions, a deficiency that disrupted industry, transportation, trade and other areas.
The first Polish legislative election for the re-established Sejm of the Republic of Poland took place in January 1919. A temporary Small Constitution was passed by the body the following month.
The rapidly growing population of Poland within its new boundaries was ¾ agricultural and ¼ urban; Polish was the primary language of only ⅔ of the inhabitants of the new country. The minorities had very little voice in the government. The permanent March Constitution of Poland was adopted in March 1921. At the insistence of the National Democrats, who were concerned how aggressively Józef Piłsudski might exercise presidential powers if he were elected to office, the constitution mandated limited prerogatives for the presidency.
The proclamation of the March Constitution was followed by a short and turbulent period of constitutional order and parliamentary democracy that lasted until 1926. The legislature remained fragmented, without stable majorities, and governments changed frequently. The open-minded Gabriel Narutowicz was elected president constitutionally (without a popular vote) by the National Assembly in 1922. However, members of the nationalist right-wing faction did not regard his elevation as legitimate. They viewed Narutowicz rather as a traitor whose election was pushed through by the votes of alien minorities. Narutowicz and his supporters were subjected to an intense harassment campaign, and the president was assassinated on December 16, 1922, after serving only five days in office.
Corruption was held to be commonplace in the political culture of the early Polish Republic. However, the investigations conducted by the new regime after the 1926 May Coup failed to uncover any major affair or corruption scheme within the state apparatus of its predecessors.
Land reform measures were passed in 1919 and 1925 under pressure from an impoverished peasantry. They were partially implemented, but resulted in the parcellation of only 20% of the great agricultural estates. Poland endured numerous economic calamities and disruptions in the early 1920s, including waves of workers' strikes such as the 1923 Kraków riot. The German–Polish customs war, initiated by Germany in 1925, was one of the most damaging external factors that put a strain on Poland's economy. On the other hand, there were also signs of progress and stabilization, for example a critical reform of finances carried out by the competent government of Władysław Grabski, which lasted almost two years. Certain other achievements of the democratic period having to do with the management of governmental and civic institutions necessary to the functioning of the reunited state and nation, were too easily overlooked. Lurking on the sidelines was a disgusted army officer corps, unwilling to subject itself to civilian control but ready to follow the retired Piłsudski, who was highly popular with Poles and just as dissatisfied with the Polish system of government as his former colleagues in the military.
Piłsudski's coup and the Sanation Era.
On May 12, 1926, Piłsudski staged the May Coup, a military overthrow of the civilian government mounted against President Stanisław Wojciechowski and the troops loyal to the legitimate government. Hundreds died in fratricidal fighting. Piłsudski was supported by several leftist factions, who ensured the success of his coup by blocking the railway transportation of government forces. He also had the support of the conservative great landowners, which left the right-wing National Democrats as the only major social force opposed to the takeover.
Following the coup, the new regime initially respected many parliamentary formalities, but gradually tightened its control and abandoned pretenses. Centrolew, a coalition of center-left parties, was formed in 1929, and in 1930 called for the "abolition of dictatorship". In 1930 the Sejm was dissolved and a number of opposition deputies were imprisoned at the Brest Fortress. Five thousand political opponents were arrested ahead of the Polish legislative election of 1930, which was rigged to award a majority of seats to the pro-regime Nonpartisan Bloc for Cooperation with the Government (BBWR).
The authoritarian "Sanation" regime (meant to denote a "healing" regime) that Piłsudski led until his death in 1935 (and would remain in place until 1939) reflected the dictator's evolution from his center-left past to conservative alliances. Political institutions and parties were allowed to function, but the electoral process was manipulated and those not willing to cooperate submissively were subjected to repression. From 1930, persistent opponents of the regime, many of the leftist persuasion, were imprisoned and subjected to staged legal processes with harsh sentences, such as the Brest trials, or else detained in the Bereza Kartuska prison and similar camps for political prisoners. About three thousand were detained without trial at different times at the Bereza concentration camp between 1934 and 1939. In 1936 for example, 369 activists were taken there, including 342 Polish communists. Rebellious peasants staged riots in 1932, 1933 and the 1937 peasant strike in Poland. Other civil disturbances were caused by striking industrial workers (e.g. events of the "Bloody Spring" of 1936), nationalist Ukrainians and the activists of the incipient Belarusian movement. All became targets of ruthless military pacification. Besides sponsoring political repression, the regime also fostered the Piłsudski cult of personality that had already existed long before he assumed dictatorial powers.
Piłsudski signed the Soviet–Polish Non-Aggression Pact in 1932 and the German–Polish Non-Aggression Pact in 1934, but in 1933 he insisted that there was no threat from the East or West and said that Poland's politics were focused on becoming fully independent without serving foreign interests. He initiated the policy of maintaining an equal distance and an adjustable middle course regarding the two great neighbors, later continued by Józef Beck. Piłsudski kept personal control of the army, but it was poorly equipped, poorly trained and had poor preparations in place for possible future conflicts. His only war plan was a defensive war against a Soviet invasion. The slow modernization after Piłsudski's death fell far behind the progress made by Poland's neighbors and measures to protect the western border, discontinued by Piłsudski from 1926, were not undertaken until March 1939.
Sanation deputies in the Sejm used a parliamentary maneuver to abolish the democratic March Constitution and push through a more authoritarian April Constitution in 1935; it reduced the powers of the Sejm, which Piłsudski despised. The process and the resulting document were seen as illegitimate by the anti-Sanation opposition, but during World War II, the Polish government-in-exile recognized the April Constitution in order to uphold the legal continuity of the Polish state.
When Marshal Piłsudski died in 1935, he retained the support of the main sections of Polish society even though he never risked testing his popularity in an honest election. His regime was dictatorial, but at that time only Czechoslovakia remained democratic in all of the regions neighboring Poland. Historians have taken widely divergent views of the meaning and consequences of the coup he perpetrated and his personal rule that followed.
Social and economic trends.
Independence stimulated the development of Polish culture in the Interbellum and intellectual achievement was high. Warsaw, whose population had almost doubled between World War I and World War II, was a restless, burgeoning metropolis. It outpaced Kraków, Lwów and Wilno, the other major population centers of the country.
Mainstream Polish society was not affected by the repressions of the Sanation authorities overall; many Poles enjoyed relative stability, and the economy improved markedly between 1926 and 1929, only to become caught up in the global Great Depression. After 1929, the country's industrial production and gross national income slumped by about 50%.
The Great Depression brought low prices for farmers and unemployment for workers. Social tensions increased, including rising antisemitism. A major economic transformation and multi-year state plan to achieve national industrial development, as embodied in the Central Industrial Region initiative launched in 1936, was led by Minister Eugeniusz Kwiatkowski. Motivated primarily by the need for a native arms industry, it was in progress at the time of the outbreak of World War II. Kwiatkowski was also the main architect of the earlier Gdynia seaport project.
The prevalent nationalism in political circles was fueled by the large size of Poland's minority populations and their separate agendas. According to the language criterion of the Polish census of 1931, the Poles constituted 69% of the population, Ukrainians 15%, Jews (defined as speakers of the Yiddish language) 8.5%, Belarusians 4.7%, Germans 2.2%, Lithuanians 0.25%, Russians 0.25% and Czechs 0.09%, with some geographical areas dominated by a particular minority. In time, the ethnic conflicts intensified, and the Polish state grew less tolerant of the interests of its national minorities. In interwar Poland, compulsory free general education substantially reduced illiteracy rates, but discrimination was practiced in a way that resulted in a dramatic decrease in the number of Ukrainian language schools and official restrictions on Jewish attendance at selected schools in the late 1930s.
The population grew steadily, reaching 35 million in 1939. However, the overall economic situation in the interwar period was one of stagnation. There was little money for investment inside Poland, and few foreigners were interested in investing there. Total industrial production barely increased between 1913 and 1939 (within the area delimited by the 1939 borders), but because of population growth (from 26.3 millions in 1919 to 34.8 millions in 1939), the "per capita" output actually decreased by 18%.
Conditions in the predominant agricultural sector kept deteriorating between 1929 and 1939, which resulted in rural unrest and a progressive radicalization of the Polish peasant movement that became increasingly inclined toward militant anti-state activities. It was firmly repressed by the authorities. According to Norman Davies, the failures of the Sanation regime (combined with the objective economic realities) caused a radicalization of the Polish masses by the end of the 1930s, but he warns against drawing parallels with the incomparably more destructive precedents of Nazi Germany or the Soviet Union under Stalin.
Final years.
After Piłsudski's death in 1935, Poland was governed until the German invasion of 1939 by old allies and subordinates known as "Piłsudski's colonels". They had neither the vision nor the resources to cope with the perilous situation facing Poland in the late 1930s. The colonels had gradually assumed greater powers during Piłsudski's life by manipulating the ailing marshal behind the scenes. Eventually they achieved an overt politicization of the army that did nothing to help prepare the country for war.
Foreign policy was the responsibility of Józef Beck, under whom Polish diplomacy attempted balanced approaches toward Germany and the Soviet Union, unfortunately without success, on the basis of a flawed understanding of the European geopolitics of his day. Beck had numerous foreign policy schemes and harbored illusions of Poland's status as a great power. He alienated most of Poland's neighbors, but is not blamed by historians for the ultimate failure of relations with Germany. The principal events of his tenure were concentrated in its last two years. In the case of the 1938 Polish ultimatum to Lithuania, the Polish action nearly resulted in a German takeover of southwest Lithuania. Also in 1938, the Polish government opportunistically undertook a hostile action against the Czechoslovak state as weakened by the Munich Agreement and annexed a small piece of territory on its borders. In this case, Beck's understanding of the consequences of the Polish military move turned out to be completely mistaken. In the end, the German occupation of Czechoslovakia ushered in by the Munich Agreement markedly weakened Poland's own position. Furthermore, Beck mistakenly believed that Nazi-Soviet ideological contradictions would preclude their cooperation.
At home, increasingly alienated minorities threatened unrest and violence and were suppressed. Extreme nationalist circles such as the National Radical Camp grew more outspoken. One of the groups, the Camp of National Unity, combined many nationalists with Sanation supporters and was connected to the new strongman, Marshal Edward Rydz-Śmigły, whose faction of the Sanation ruling movement was increasingly nationalistic.
In the late 1930s, the exile bloc Front Morges united several major Polish anti-Sanation figures, including Ignacy Paderewski, Władysław Sikorski, Wincenty Witos, Wojciech Korfanty and Józef Haller. It gained little influence inside Poland, but its spirit soon reappeared during World War II, within the Polish government-in-exile.
In October 1938, Joachim von Ribbentrop first proposed German-Polish territorial adjustments and Poland's participation in the Anti-Comintern Pact against the Soviet Union. The status of the Free City of Danzig was one of the key bones of contention. Approached by Ribbentrop again in March 1939, the Polish government expressed willingness to address issues causing German concern, but effectively rejected Germany's stated demands and thus refused to allow Poland to be turned by Adolf Hitler into a German puppet state. Hitler, incensed by the British and French declarations of support for Poland, abrogated the German–Polish Non-Aggression Pact in late April 1939.
To protect itself from an increasingly aggressive Nazi Germany, already responsible for the annexations of Austria (in the Anschluss of 1938), Czechoslovakia (in 1939) and a part of Lithuania after the 1939 German ultimatum to Lithuania, Poland entered into a military alliance with Britain and France (the 1939 Anglo-Polish military alliance and the earlier Franco-Polish military alliance of 1921, as updated in 1939). However, the two Western powers were defense-oriented and not in a strong position, either geographically or in terms of resources, to assist Poland. Attempts were therefore made by them to induce Soviet-Polish cooperation, which they viewed as the only militarily viable possibility. Diplomatic manoeuvers continued in the spring and summer of 1939, but in their final attempts, the Franco-British talks with the Soviets in Moscow on forming an anti-Nazi defensive military alliance failed. Warsaw's refusal to allow the Red Army to operate on Polish territory doomed the Western efforts. The final contentious Allied-Soviet exchanges took place on August 21 and 23, 1939. Stalin's regime was the target of an intense German counter-initiative and was concurrently involved in increasingly effective negotiations with Hitler's agents. On August 23, an outcome contrary to the exertions of the Allies became a reality: in Moscow, Germany and the Soviet Union hurriedly signed the Molotov–Ribbentrop non-aggression pact, which secretly provided for the dismemberment of Poland into Nazi and Soviet-controlled zones.
World War II.
Invasions and resistance.
On September 1, 1939, Hitler ordered the invasion of Poland, the opening event of World War II. Poland had signed an Anglo-Polish military alliance as recently as August 25, and had long been in alliance with France. The two Western powers soon declared war on Germany, but they remained largely inactive (the period early in the conflict became known as the Phoney War) and extended no aid to the attacked country. The numerically and technically superior "Wehrmacht" formations rapidly advanced eastwards and engaged massively in the murder of Polish civilians over the entire occupied territory. On September 17, a Soviet invasion of Poland began. The Soviet Union quickly occupied most of the areas of eastern Poland that contained large populations of Ukrainians and Belarusians. The two invading powers divided up the country as they had agreed in the secret provisions of the Molotov–Ribbentrop Pact. Poland's top government officials and military high command fled the war zone and arrived at the Romanian Bridgehead in mid-September. After the Soviet entry they sought refuge in Romania.
Among the military operations in which Poles held out the longest (until late September or early October) were the Siege of Warsaw, the Battle of Hel and the resistance of the Independent Operational Group Polesie. Warsaw fell on 27 September after a heavy German bombardment that killed about 40,000 civilians. Poland was ultimately partitioned between Germany and the Soviet Union according to the terms of the German–Soviet Treaty of Friendship, Cooperation and Demarcation signed by the two powers in Moscow on September 29.
Gerhard Weinberg has argued that the most significant Polish contribution to World War II was sharing its code-breaking results. This allowed the British to perform the cryptanalysis of the Enigma and decipher the main German military code, which gave the Allies a major advantage in the conflict. As regards actual military campaigns, some Polish historians have argued that simply resisting the initial invasion of Poland was the country's greatest contribution to the victory over Nazi Germany, despite its defeat. The Polish Army of nearly one million men significantly delayed the start of the Battle of France, planned for 1939. When the Nazi offensive in the West did happen, the delay caused it to be less effective, a possibly crucial factor in the victory of the Battle of Britain.
After Germany invaded the Soviet Union as part of its Operation Barbarossa in June 1941, the whole of pre-war Poland was overrun and occupied by German troops.
German-occupied Poland was divided from 1939 into two regions: Polish areas annexed by Nazi Germany directly into the German Reich and areas ruled under a so-called General Government of occupation. The Poles formed an underground resistance movement and a Polish government-in-exile that operated first in Paris, then, from July 1940, in London. Polish-Soviet diplomatic relations, broken since September 1939, were resumed in July 1941 under the Sikorski–Mayski agreement, which facilitated the formation of a Polish army (the Anders' Army) in the Soviet Union. In November 1941, Prime Minister Sikorski flew to the Soviet Union to negotiate with Stalin on its role on the Soviet-German front, but the British wanted the Polish soldiers in the Middle East. Stalin agreed, and the army was evacuated there.
The members of the Polish Underground State that functioned in Poland throughout the war were loyal to and formally under the Polish government-in-exile, acting through its Government Delegation for Poland. During World War II, about 400,000 Poles joined the underground Polish Home Army ("Armia Krajowa"), a part of the Polish Armed Forces of the government-in-exile. About 200,000 fought in the Western Front in Polish armed forces loyal to the government-in-exile, and about 300,000 in the Eastern Front. The pro-Soviet resistance movement, led by the Polish Workers' Party, was active from 1941. It was opposed by the gradually forming extreme nationalistic National Armed Forces.
Beginning in late 1939, hundreds of thousands of Poles from the Soviet-occupied areas were deported and taken east. Of the upper-ranking military personnel and others deemed uncooperative or potentially harmful by the Soviets, about 22,000 were secretly executed. 
In April 1943, the Soviet Union broke off deteriorating relations with the Polish government-in-exile after the German military announced the discovery of mass graves containing Polish army officers murdered by the Soviets at the Katyn massacre. The Soviets claimed that the Poles committed a hostile act by requesting that the Red Cross investigate these reports.
From 1941, the implementation of the Final Solution began, and the Holocaust in Poland proceeded with force. As the Jewish ghetto in occupied Warsaw was being liquidated by Nazi SS units, the city was the scene of the Warsaw Ghetto Uprising in April–May 1943. The elimination of Jewish ghettos in German-occupied Poland took place in a number of cities besides Warsaw and other uprisings were waged against impossible odds by desperate Jewish insurgents, whose people were being removed and exterminated.
Soviet advance 1944–45, Warsaw Uprising.
At a time of increasing cooperation between the Western Allies and the Soviet Union in the wake of the Nazi invasion of 1941, the influence of the Polish government-in-exile was seriously diminished by the death of Prime Minister Władysław Sikorski, its most capable leader, in a plane crash on July 4, 1943. His successors lacked the ability or willingness to negotiate effectively with the Soviets and proved equally ineffective in pressing for the interests of the Polish people with the Western Allies.
In July 1944, the Soviet Red Army and Soviet-controlled Polish People's Army entered the territory of future postwar Poland. In protracted fighting in 1944 and 1945, the Soviets and their Polish allies defeated and expelled the German army from Poland at a cost of over 600,000 Soviet and over 60,000 Polish soldiers lost.
The greatest single action of the Polish resistance movement in World War II and a major political event was the Warsaw Uprising that began on August 1, 1944. The uprising, in which most of the city's population participated, was instigated by the underground Home Army and approved by the Polish government-in-exile in an attempt to establish a non-communist Polish administration ahead of the arrival of the Red Army. The uprising was originally planned as a short-lived armed demonstration in expectation that the Soviet forces approaching Warsaw would assist in any battle to take the city. The Soviets had never agreed to an intervention, however, and they halted their advance at the Vistula River. The Germans used the opportunity to carry out a brutal suppression of the forces of the pro-Western Polish underground.
The bitterly fought uprising lasted for two months and resulted in the death or expulsion from the city of hundreds of thousands of civilians. After the Poles realised the hopelessness of the situation and surrendered on 2 October, the Germans carried out a planned destruction of Warsaw on Hitler's orders that obliterated the remaining infrastructure of the city. The Polish First Army, fighting alongside the Soviet Red Army, entered a devastated Warsaw on 17 January 1945.
Allied conferences, Polish governments.
From the time of the Tehran Conference in late 1943, there was broad agreement among the United States, Great Britain, and the Soviet Union that the locations of the borders between Germany and Poland and between Poland and the Soviet Union would be fundamentally changed after the conclusion of World War II. Stalin's proposal that Poland should be moved far to the west was readily accepted by the Polish communists, who were at that time in the early stages of forming a post-war government (the State National Council, a quasi-parliamentary body, was created). In July 1944, a communist-controlled Polish Committee of National Liberation was established in Lublin nominally to govern the areas liberated from German control, a move that prompted protests from Prime Minister Stanisław Mikołajczyk and his government-in-exile.
By the time of the Yalta Conference in February 1945, the communists had already established a Provisional Government of the Republic of Poland. The Soviet position at the conference was strong because of their decisive contribution to the war effort and as a result of their occupation of immense amounts of land in central and eastern Europe. The three Great Powers (the United States, Great Britain, and the Soviet Union) gave assurances that the communist provisional government would be converted into an entity that would include democratic forces from within the country and active abroad, but the London-based government-in-exile was not mentioned. A Provisional Government of National Unity and subsequent democratic elections were the agreed stated goals. The disappointing results of these plans and the failure of the Western powers to ensure the strong participation of non-communists in the immediate post-war Polish government were seen by many Poles as a manifestation of Western betrayal.
War losses, extermination of Jews.
A lack of accurate data makes it difficult to document numerically the extent of the human losses suffered by Polish citizens during World War II. Additionally, many assertions made in the past must be considered suspect due to flawed methodology and a desire to promote certain political agendas. The last available enumeration of ethnic Poles and the large ethnic minorities is the Polish census of 1931. Exact population figures for 1939 are therefore not known.
Modern research indicates that about 5 million Polish citizens were killed during the war, including 3 million Polish Jews. According to the United States Holocaust Memorial Museum, at least 1.9 to 2 million ethnic Poles and 3 million Polish Jews were killed. Millions of Polish citizens were deported to Germany for forced labor or to German death camps such as Treblinka, Auschwitz and Sobibor. According to another estimate, between 2.35 and 2.9 million Polish Jews and about 2 million ethnic Poles were killed. Nazi Germany intended to exterminate the Jews completely, in actions that have come to be described collectively as the Holocaust. The Poles were to be expelled from areas controlled by Nazi Germany through a process of resettlement that started in 1939 and was expected to be completed within 15 years.
In an attempt to incapacitate Polish society, the Nazis and the Soviets executed tens of thousands of members of the intelligentsia and community leadership during events such as the German AB-Aktion in Poland, Operation Tannenberg and the Katyn massacre. Over 95% of the Jewish losses and 90% of the ethnic Polish losses were caused directly by Nazi Germany, whereas 5% of the ethnic Polish losses were caused by the Soviets and 5% by Ukrainian nationalists. The large-scale Jewish presence in Poland that had endured for centuries was rather quickly put to an end by the policies of extermination implemented by the Nazis during the war. Waves of displacement and emigration that took place both during and after the war removed from Poland a majority of the Jews who survived. Further significant Jewish emigration followed events such as the Polish October political thaw of 1956 and the 1968 Polish political crisis. The magnitudes of the losses of Polish citizens of German, Ukrainian, Belarusian and other nationalities, which were also great, are not known.
In 1940–41, some 325,000 Polish citizens were deported by the Soviet regime. The number of Polish citizens who died at the hands of the Soviets is estimated at less than 100,000. In 1943–44, Ukrainian nationalists associated with the Organization of Ukrainian Nationalists (OUN) and the Ukrainian Insurgent Army perpetrated the Massacres of Poles in Volhynia and Eastern Galicia.
Approximately 90% of Poland's war casualties were the victims of prisons, death camps, raids, executions, the annihilation of ghettos, epidemics, starvation, excessive work and ill treatment. The war left one million children orphaned and 590,000 persons disabled. The country lost 38% of its national assets (whereas Britain lost only 0.8%, and France only 1.5%). Nearly half of pre-war Poland was expropriated by the Soviet Union, including the two great cultural centers of Lwów and Wilno.
Changing boundaries and population transfers.
By the terms of the 1945 Potsdam Agreement signed by the United States, the Soviet Union, and Great Britain, the Soviet Union retained most of the territories captured as a result of the Molotov–Ribbentrop Pact of 1939, including western Ukraine and western Belarus, and gained others. Lithuania and the Königsberg area of East Prussia were officially incorporated into the Soviet Union, in the case of the former without the recognition of the Western powers. Poland was compensated with the bulk of Silesia, including Breslau (Wrocław) and Grünberg (Zielona Góra), the bulk of Pomerania, including Stettin (Szczecin), and the greater southern portion of the former East Prussia, along with Danzig (Gdańsk). Collectively referred to as the "Recovered Territories", they were included in the reconstituted Polish state. With Germany's defeat, the re-established Polish state was thus shifted west to the area between the Oder–Neisse and Curzon lines. The Poles lost 70% of their pre-war oil capacity to the Soviets, but gained from the Germans a highly developed industrial base and infrastructure that made a diversified industrial economy possible for the first time in Polish history.
The flight and expulsion of Germans from what was eastern Germany prior to the war began before and during the Soviet conquest of those regions from the Nazis, and the process continued in the years immediately after the war. Of those who remained, many chose to emigrate to post-war Germany. On the other hand, 1.5–2 million Poles moved or were expelled from Polish areas annexed by the Soviet Union. The vast majority were resettled in the former German territories.
Many exiled Poles could not return to the country for which they had fought because they belonged to political groups incompatible with the new communist regimes, or because they originated from areas of pre-war eastern Poland that were incorporated into the Soviet Union (see Polish population transfers of the period 1944-46). Some were deterred from returning simply on the strength of warnings that anyone who had served in Western military units would be endangered under the new communist regimes. Many Poles were pursued, arrested, tortured and imprisoned by the Soviet authorities for belonging to the Home Army or other formations (see Anti-communist resistance in Poland during the period 1944-46), or were persecuted because they had fought on the Western front.
Territories on both sides of the new Polish-Ukrainian border were also "ethnically cleansed". Of the Ukrainians and Lemkos living in Poland within the new borders (about 700,000), close to 95% were forcibly moved to the Soviet Ukraine, or (in 1947) to the new territories in northern and western Poland under Operation Vistula. In Volhynia, 98% of the Polish pre-war population was either killed or expelled; in Eastern Galicia, the Polish population was reduced by 92%. In all, about 70,000 Poles and about 20,000 Ukrainians were killed in the ethnic violence that occurred in the 1940s, both during and after the war.
According to an estimate by Polish researchers, 40–60,000 of the 200–250,000 Polish Jews who escaped the Nazis survived without leaving Poland (the remainder perished). More were repatriated from the Soviet Union and elsewhere, and the February 1946 population census showed about 300,000 Jews within the new borders. Of the surviving Jews, many chose to emigrate or felt compelled to because of anti-Jewish violence in Poland.
Because of changing borders and the mass movements of people of various nationalities, the emerging communist Poland ended up with a mainly homogeneous, ethnically Polish population (97.6% according to the December 1950 census). Minority members were not encouraged by the authorities or their neighbors to emphasize their ethnic identities.
Polish People's Republic (1945–89).
Post-war struggle for power.
In response to the February 1945 Yalta Conference directives, a Polish Provisional Government of National Unity was formed in June 1945 under Soviet auspices; it was soon recognized by the United States and many other countries. Communist rule and Soviet domination were apparent from the beginning: sixteen prominent leaders of the Polish anti-Nazi underground were brought to trial in Moscow ("the Trial of the Sixteen") already in June 1945. In the immediate post-war years, emerging communist rule was challenged by opposition groups ("cursed soldiers"), and many thousands perished in the fight or were pursued by the Ministry of Public Security and executed. Such insurgents often pinned their hopes on expectations of the imminent outbreak of a World War III and the defeat of the Soviet Union. The Polish right-wing insurgency faded after the amnesty of February 1947.
The Polish people's referendum of June 1946 was arranged by the communist Polish Workers' Party to legitimize its dominance over Polish politics and claim widespread support for the Party's policies. Although the Yalta agreement called for free elections, the Polish legislative election of January 1947 was controlled by the communists. The Polish government-in-exile remained in continuous existence until 1990, although its influence declined.
Under Stalinism.
The Polish People's Republic ("Polska Rzeczpospolita Ludowa") was established under the rule of the communist Polish United Workers' Party. The name change from the Polish Republic was not officially adopted, however, until the proclamation of the Constitution of the Polish People's Republic in 1952.
The ruling party itself was formed by the forced amalgamation in December 1948 of the communist Polish Workers' Party and the historically non-communist Polish Socialist Party. The latter, re-established in 1944 by its left wing, had since been allied with the communists. The ruling communists, who in post-war Poland preferred to use the term "socialism" instead of "communism" to identify their ideological basis, needed to include the socialist junior partner to broaden their appeal, claim greater legitimacy and eliminate competition on the political Left. The socialists, who were losing their organization, were subjected to political pressure, ideological cleansing and purges in order to become suitable for unification on the terms of the "Workers' Party". The leading pro-communist leaders of the socialists were the prime ministers Edward Osóbka-Morawski and Józef Cyrankiewicz.
During the most oppressive phase of the Stalinist period (1948–53), terror was justified in Poland as necessary to eliminate reactionary subversion. Many thousands of perceived opponents of the regime were arbitrarily tried, and large numbers were executed. The People's Republic was led by discredited Soviet operatives such as Bolesław Bierut, Jakub Berman and Konstantin Rokossovsky. The independent Catholic Church in Poland was subjected to property confiscations and other curtailments from 1949, and in 1950 was pressured into signing an accord with the government. In 1953 and later, despite a partial thaw after the death of Joseph Stalin that year, the persecution of the Church intensified and its head, Cardinal Stefan Wyszyński, was detained. A key event in the persecution of the Polish church was the Stalinist show trial of the Kraków Curia in January 1953.
In the Warsaw Pact, formed in 1955, the army of the Polish People's Republic was the second largest, after the Soviet Army.
Economic and social developments.
In 1944, large agricultural holdings and former German property in Poland started to be redistributed through land reform and industry started to be nationalized. Communist restructuring and the imposition of work-space rules encountered active worker opposition already in the years 1945–47. The Three-Year Plan of 1947–49 continued with the rebuilding, socialization and socialist restructuring of the economy. It was followed by the Six-Year Plan of 1950–55 for heavy industry. The rejection of the Marshall Plan in 1947 made aspirations for catching up with West European standards of living unrealistic.
The government's highest economic priority was the development of heavy industry useful to the military. State-run or controlled institutions common in all the socialist countries of eastern Europe were imposed on Poland, including collective farms and worker cooperatives. The latter were dismantled in the late 1940s as not socialist enough, although they were later re-established; even small-scale private enterprises were eradicated. Stalinism introduced heavy political and ideological propaganda and indoctrination in social life, culture and education.
Great strides were made, however, in the areas of employment (which became nearly full), universal public education (which nearly eradicated adult illiteracy), health care and recreational amenities. Many historic sites, including the central districts of Warsaw and Gdańsk, both devastated during the war, were rebuilt at great cost.
The communist industrialization program led to increased urbanization and educational and career opportunities for the intended beneficiaries of the social transformation along the lines of the peasants-workers-working intelligentsia paradigm. The most significant improvement was accomplished in the lives of Polish peasants, many of whom were able to leave their impoverished and overcrowded village communities for better conditions in urban centers. Those who stayed behind took advantage of the implementation of the 1944 land reform decree of the Polish Committee of National Liberation, which terminated the antiquated, but widespread parafeudal socioeconomic relations in Poland. Under Stalinism, attempts were made at establishing collective farms; they generally failed. Due to urbanization, the national percentage of the rural population decreased in communist Poland by about 50%. A majority of Poland's residents of cities and towns still live in apartment blocks built during the communist era in part to accommodate migrants from rural areas.
Thaw.
In March 1956, after the 20th Soviet Party Congress in Moscow ushered in de-Stalinization, Edward Ochab was chosen to replace the deceased Bolesław Bierut as first secretary of the Polish United Workers' Party. As a result, Poland was rapidly overtaken by social restlessness and reformist undertakings; thousands of political prisoners were released and many people previously persecuted were officially rehabilitated. Worker riots in Poznań in June 1956 were violently suppressed, but they gave rise to the formation of a reformist current within the communist party.
Amidst continuing social and national upheaval, a further shakeup took place in the party leadership as part of what is known as the Polish October of 1956. While retaining most traditional communist economic and social aims, the regime led by the new Polish Party's First Secretary Władysław Gomułka liberalized internal life in Poland. The dependence on the Soviet Union was somewhat mollified, and the state's relationships with the Church and Catholic lay activists were put on a new footing. A repatriation agreement with the Soviet Union allowed the repatriation of hundreds of thousands of Poles who were still in Soviet hands, including many former political prisoners. Collectivization efforts were abandoned—agricultural land, unlike in other Comecon countries, mostly remained in the private ownership of farming families. State-mandated provisions of agricultural products at fixed, artificially low prices were reduced and, from 1972, eliminated.
Culture in the Polish People's Republic, to varying degrees linked to the intelligentsia's opposition to the authoritarian system, developed to a sophisticated level under Gomułka and his successors. The creative process was often compromised by state censorship, but significant works were created in fields such as literature, theater, cinema and music, among others. Journalism of veiled understanding and varieties of native and western popular culture were well represented. Uncensored information and works generated by émigré circles were conveyed through a variety of channels. The Paris-based Kultura magazine developed a conceptual framework for dealing with the issues of borders and the neighbors of a future free Poland, but Radio Free Europe was of foremost importance.
Stagnation and crackdown.
The legislative election of 1957 was followed by several years of political stability that was accompanied by economic stagnation and curtailment of reforms and reformists. One of the last initiatives of the brief reform era was a nuclear weapons–free zone in Central Europe proposed in 1957 by Adam Rapacki, Poland's foreign minister. One of the confirmations of the end of an era of greater tolerance was the expulsion from the communist party of several prominent "Marxist revisionists" in the 1960s.
In 1965, the Conference of Polish Bishops issued the Letter of Reconciliation of the Polish Bishops to the German Bishops, a gesture intended to heal bad mutual feelings left over from World War II. In 1966, the celebrations of the 1,000th anniversary of the Baptism of Poland led by Cardinal Stefan Wyszyński and other bishops turned into a huge demonstration of the power and popularity of the Catholic Church in Poland.
The post-1956 liberalizing trend, in decline for a number of years, was reversed in March 1968, when student demonstrations were suppressed during the 1968 Polish political crisis. Motivated in part by the Prague Spring movement, the Polish opposition leaders, intellectuals, academics and students used a historical-patriotic Dziady theater spectacle series in Warsaw (and its termination forced by the authorities) as a springboard for protests, which soon spread to other centers of higher education and turned nationwide. The authorities responded with a major crackdown on opposition activity, including the firing of faculty and the dismissal of students at universities and other institutions of learning. At the center of the controversy was also the small number of Catholic deputies in the Sejm (the Znak Association members) who attempted to defend the students.
In an official speech, Gomułka drew attention to the role of Jewish activists in the events taking place. This provided ammunition to a nationalistic and antisemitic communist party faction headed by Mieczysław Moczar that was opposed to Gomułka's leadership. Using the context of the military victory of Israel in the Six-Day War of 1967, some in the Polish communist leadership waged an antisemitic campaign against the remnants of the Jewish community in Poland. The targets of this campaign were accused of disloyalty and active sympathy with Israeli aggression. Branded "Zionists", they were scapegoated and blamed for the unrest in March, which eventually led to the emigration of much of Poland's remaining Jewish population (about 15,000 Polish citizens left the country).
With the active support of the Gomułka regime, the People's Army of Poland took part in the infamous Warsaw Pact invasion of Czechoslovakia in August 1968 after the informal announcement of the Brezhnev Doctrine.
In December 1970, the governments of Poland and West Germany signed the Treaty of Warsaw, which normalized their relations and made possible meaningful cooperation in a number of areas of bilateral interest. West Germany recognized the post-war "de facto" border between Poland and East Germany.
Worker revolts and Solidarity.
Price increases for essential consumer goods triggered the Polish protests of 1970. In December, there were disturbances and strikes in the port cities of Gdańsk, Gdynia, and Szczecin that reflected deep dissatisfaction with living and working conditions in the country. The activity was centered in the industrial shipyard areas of the three coastal cities. Dozens of protesting workers and bystanders were killed in police and military actions, generally under the authority of Gomułka and Minister of Defense Wojciech Jaruzelski. In the aftermath, Edward Gierek replaced Gomułka as first secretary of the communist party. The new regime was seen as more modern, friendly and pragmatic, and at first it enjoyed a degree of popular and foreign support.
Gierek's regime between 1970 and 1980 introduced wide-ranging (but ultimately unsuccessful) government reforms to revitalize the economy. Another attempt to raise food prices resulted in the June 1976 protests. Jacek Kuroń was among the activists who defended accused rioters from Radom and other towns. The Workers' Defence Committee (KOR), established in response to the crackdown, consisted of dissident intellectuals willing to support industrial workers, farmers and students who were struggling with and persecuted by the authorities throughout the late 1970s. During this period the opposition circles were emboldened by the Helsinki Conference processes.
In October 1978, the Archbishop of Kraków, Cardinal Karol Józef Wojtyła, became Pope John Paul II, head of the Roman Catholic Church. Catholics and others rejoiced at the elevation of a Pole to the papacy and greeted his June 1979 visit to Poland with an outpouring of emotion.
Fueled by large infusions of Western credit, Poland's economic growth rate was one of the world's highest during the first half of the 1970s, but much of the borrowed capital was misspent, and the centrally planned economy was unable to use the new resources effectively. The 1973 oil crisis caused recession and high interest rates in the West, to which the Polish government had to respond with sharp domestic consumer price increases. The growing debt burden became insupportable in the late 1970s, and negative economic growth set in by 1979.
Around July 1, 1980, with the Polish foreign debt standing at more than $20 billion, the government made another attempt to increase meat prices. Workers responded with escalating work stoppages that culminated in the 1980 general strikes in Lublin. In mid-August, labor protests at the Gdańsk Shipyard gave rise to a chain reaction of strikes that virtually paralyzed the Baltic coast by the end of the month and, for the first time, closed most coal mines in Silesia. The Inter-Enterprise Strike Committee coordinated the strike action across hundreds of workplaces and formulated the 21 demands as the basis for negotiations with the authorities. The Strike Committee was sovereign in its decision-making, but was aided by a team of "expert" advisers that included Bronisław Geremek and Tadeusz Mazowiecki, well-known intellectuals and dissidents.
On August 31, 1980, representatives of workers at the Gdańsk Shipyard, led by an electrician and activist Lech Wałęsa, signed the Gdańsk Agreement with the government that ended their strike. Similar agreements were concluded in Szczecin (the Szczecin Agreement) and in Silesia. The key provision of these agreements was the guarantee of the workers' right to form independent trade unions and the right to strike. Following the successful resolution of the largest labor confrontation in communist Poland's history, nationwide union organizing movements swept the country.
Edward Gierek was blamed by the Soviets for not following their "fraternal" advice, not shoring up the Party and the official trade unions and allowing "anti-socialist" forces to emerge. On September 5, 1980, Gierek was replaced by Stanisław Kania as first secretary.
Delegates of the emergent worker committees from all over Poland gathered in Gdańsk on September 17 and decided to form a single national union organization named "Solidarity" (the name was adopted following a suggestion by Karol Modzelewski).
While party–controlled courts took up the contentious issues of Solidarity's legal registration as a trade union (finalized by November 10), planning had already begun for the imposition of martial law. A parallel farmers' union was organized and strongly opposed by the regime, but Rural Solidarity was finally registered on May 12, 1981. In the meantime, a rapid deterioration of the authority of the communist party, the disintegration of state power and an escalation of demands and threats by the various Solidarity–affiliated groups were occurring. According to Kuroń, a "tremendous social democratization movement in all spheres" was taking place and could not be contained. Wałęsa had meetings with Kania, which brought no resolution to the impasse. Following the Warsaw Pact summit in Moscow, the Soviet Union proceeded with a massive military build-up along Poland's border in December 1980, but during the summit, Kania forcefully argued with Leonid Brezhnev and other allied communists leaders against the feasibility of an external military intervention, and no action was taken. The United States, under presidents Jimmy Carter and Ronald Reagan, repeatedly warned the Soviets about the consequences of a direct intervention, while discouraging an open insurrection in Poland and signaling to the Polish opposition that there would be no rescue by the NATO forces.
In February 1981, Defense Minister General Wojciech Jaruzelski assumed the position of prime minister. A World War II veteran with a generally positive image, Jaruzelski engaged in preparations for calming the Polish unrest by the use of force, utilizing ZOMO troops and other security forces backed up by the Polish and Soviet bloc military. The 1980–81 Solidarity social revolt had thus far been free of any major use of force, but in March 1981 in Bydgoszcz, three activists were beaten up by the secret police. A nationwide "warning strike" took place, in which the 9.5-million-strong Solidarity union was supported by the population at large. A general strike was called off by Wałęsa after the March 30 settlement with the government. Both Solidarity and the Party were badly split and the Soviets were losing patience. Kania was re-elected at the Party Congress in July, but the collapse of the economy continued and so did the general disorder.
At the first Solidarity National Congress in September–October 1981 in Gdańsk, Lech Wałęsa was elected national chairman of the Union with 55% of the vote. An appeal was issued to the workers of the other East European countries, urging them to follow in the footsteps of Solidarity. To the Soviets, the gathering was an "anti-socialist and anti-Soviet orgy" and the Polish communist leaders, increasingly led by Jaruzelski and General Czesław Kiszczak, were ready to apply force.
In October 1981, Jaruzelski was named the Party's first secretary, an unusual advancement for a military figure in the communist world. The Plenum's vote was 180 to 4, and he kept his government posts. Jaruzelski asked parliament to ban strikes and allow him to exercise extraordinary powers, but when neither request was granted, he decided to proceed with his plans anyway.
Martial law and end of communism.
On December 12–13, 1981, the regime declared martial law in Poland, under which the army and ZOMO riot police were used to crush Solidarity. In the Soviet reaction to the Polish crisis of 1980–81, the Soviet leaders insisted that Jaruzelski pacify the opposition with the forces at his disposal, without direct Soviet involvement or backup. Virtually all Solidarity leaders and many affiliated intellectuals were arrested or detained. Nine workers were killed in the Pacification of Wujek. The United States and other Western countries responded by imposing economic sanctions against Poland and the Soviet Union. Unrest in the country was subdued but continued.
During martial law, Poland was ruled by the so-called Military Council of National Salvation. The open or semi-open opposition communications, as recently practiced, were replaced by underground publishing (known in the eastern bloc as Samizdat), and Solidarity was reduced to a few thousand underground activists.
Having achieved some semblance of stability, the Polish regime relaxed and then rescinded martial law over several stages. By December 1982, martial law was suspended, and a small number of political prisoners, including Wałęsa, were released. Although martial law formally ended in July 1983 and a partial amnesty was enacted, several hundred political prisoners remained in jail. With the economic crisis unresolved and societal institutions dysfunctional, both the ruling establishment and the opposition led by Solidarity leading figures began looking for ways out of the stalemate. Facilitated by the indispensable mediation of the Catholic Church, exploratory contacts were established.
Student protests resumed from February 1988. The government's inability to forestall Poland's economic decline led to the 1988 Polish strikes across the country in April, May and August. The Soviet Union was becoming increasingly destabilized and unwilling to apply military and other pressure to prop up allied regimes in trouble. The Polish government felt compelled to negotiate with the opposition and in September 1988 preliminary talks with Solidarity leaders ensued in Magdalenka. Numerous meetings took place involving Wałęsa and General Kiszczak, among others, and the regime made a major public relations mistake by allowing a televised debate in November between Wałęsa and Alfred Miodowicz, chief of the All-Poland Alliance of Trade Unions, the official trade union organization. The fitful bargaining and intra-party squabbling led to the official Round Table Negotiations in the following year, followed by the Polish legislative election of 1989, a watershed event marking the fall of communism in Poland.
Third Polish Republic (1989–today).
Transition from communism.
The Polish Round Table Agreement of April 1989 called for local self-government, policies of job guarantees, legalization of independent trade unions and many wide-ranging reforms. The current Sejm promptly implemented the deal and agreed to partly open National Assembly elections that were set for June 4 and June 18. Only 35% of the seats in the Sejm (the national legislature's lower house) and all of the Senate seats were freely contested; the remaining Sejm seats (65%) were guaranteed for the communists and their allies.
The failure of the communists at the polls (almost all of the contested seats were won by the opposition) resulted in a political crisis. The new April constitutional agreement called for the re-establishment of the Polish presidency and on July 19 the National Assembly elected the communist leader General Wojciech Jaruzelski to that office. His election, seen at the time as politically necessary, was barely accomplished with tacit support from some Solidarity deputies, and the new president's position was not strong. Moreover, the unexpected definitiveness of the parliamentary election results created new dynamics and attempts by the communists to form a government failed.
On August 19, President Jaruzelski asked journalist and Solidarity activist Tadeusz Mazowiecki to form a government; on September 12, the Sejm voted approval of Prime Minister Mazowiecki and his cabinet. Mazowiecki decided to leave the economic reform entirely in the hands of economic liberals led by the new Deputy Prime Minister Leszek Balcerowicz, who proceeded with the design and implementation of his "shock therapy" policy. For the first time in post-war history, Poland had a government led by non-communists, setting a precedent soon to be followed by many other communist-ruled nations in a phenomenon known as the Revolutions of 1989. Mazowiecki's acceptance of the "thick line" formula meant no "witch-hunt", an absence of revenge seeking or exclusion from politics in regard to former communist officials.
In part because of the attempted indexation of wages, inflation reached 900% by the end of 1989, but was soon dealt with by the shock therapy. In December 1989, the Sejm approved the Balcerowicz Plan to transform the Polish economy rapidly from a centrally planned one to a free market economy. The Constitution of the Polish People's Republic was amended to eliminate references to the "leading role" of the communist party and the country was renamed the "Republic of Poland". The communist Polish United Workers' Party dissolved itself in January 1990, creating in its place a new party, Social Democracy of the Republic of Poland. "Territorial self-government", abolished in 1950, was legislated back in March 1990, to be led by locally elected officials; its fundamental unit was the administratively independent gmina.
In October 1990, the constitution was amended to curtail the term of President Jaruzelski.
In November 1990, Lech Wałęsa was elected president for a five-year term; in December, he became the first popularly elected president of Poland. Poland's first free parliamentary election was held in October 1991. 18 parties entered the new Sejm, but the largest representation received only 12% of the total vote.
Democratic constitution, NATO and European Union memberships.
Several post-Solidarity governments were in existence between the 1989 election and the 1993 election, after which the "post-communist" left-wing parties took over. In 1993, the formerly Soviet Northern Group of Forces, a vestige of past domination, left Poland.
In 1995, Aleksander Kwaśniewski of the social democratic party was elected president and remained in that capacity for the next ten years (two terms).
In 1997, the new Constitution of Poland was finalized and approved in a referendum; it replaced the Small Constitution of 1992, an amended version of the communist constitution.
Poland joined NATO in 1999. Elements of the Polish Armed Forces have since participated in the Iraq War and the Afghanistan War. Poland joined the European Union as part of its enlargement in 2004. The two memberships were indicative of the Third Polish Republic's integration with the West. Poland has not adopted the euro currency, however.
Notes.
"a."Piłsudski's family roots in the Polonized gentry of the Grand Duchy of Lithuania and the resulting perspective of seeing himself and people like him as legitimate Lithuanians put him in conflict with modern Lithuanian nationalists (who in Piłsudski's lifetime redefined the scope of the meaning of "Lithuanian"), and by extension with other nationalists and also with the Polish modern nationalist movement.
"b."In 1938 Poland and Romania refused to agree to a Franco-British proposal that in the event of war with Germany Soviet forces would be allowed to cross their territories to aid Czechoslovakia. The Polish ruling elites considered the Soviets in some ways more threatening than the Nazis.
The Soviet Union repeatedly declared its intention to fulfill its obligations under the 1935 treaty with Czechoslovakia and defend Czechoslovakia militarily. A transfer of land and air forces through Poland and/or Romania was required and the Soviets approached the French about it, who also had a treaty with Czechoslovakia (and with Poland and with the Soviet Union). Edward Rydz-Śmigły rebuked the French suggestion on that matter in 1936, and in 1938 Józef Beck pressured Romania not to allow even Soviet warplanes to fly over its territory. Like Hungary, Poland was looking into using the German-Czechoslovak conflict to settle its own territorial grievances, namely disputes over parts of Zaolzie, Spiš and Orava.
"c." In October 1939, the British Foreign Office notified the Soviets that the United Kingdom would be satisfied with a post-war creation of small ethnic Poland, patterned after the Duchy of Warsaw. An establishment of Poland restricted to "minimal size", according to ethnographic boundaries (such as the lands common to both the prewar Poland and postwar Poland), was planned by the Soviet People's Commissariat for Foreign Affairs in 1943–44. Such territorial reduction was recommended by Ivan Maisky to Vyacheslav Molotov in early 1944, because of what Maisky saw as Poland's historically unfriendly disposition toward Russia and the Soviet Union, likely in some way to continue. Joseph Stalin opted for a larger version, allowing a "swap" (territorial compensation for Poland), which involved the eastern lands gained by Poland at the Peace of Riga of 1921 and now lost, and eastern Germany conquered from the Nazis in 1944–45. In regard to the several major disputed areas: Lower Silesia west of the Oder and the Nysa Kłodzka rivers (the British wanted it to remain a part of the future German state), Stettin (in 1945 the German communists already established their administration there), "Zakerzonia" (western Red Ruthenia demanded by the Ukrainians), and the Białystok region (Białystok was claimed by the communists of the Byelorussian SSR), the Soviet leader made decisions that favored Poland.
Other territorial and ethnic scenarios were also possible, generally with outcomes less advantageous to Poland than its present form.
"d."Timothy Snyder spoke of about 100,000 Jews killed by Poles during the Nazi occupation, the majority probably by members of the collaborationist Blue Police. This number would have likely been many times higher had Poland entered into an alliance with Germany in 1939, as advocated by some Polish historians and others.
"e."Some may have falsely claimed Jewish identity hoping for permission to emigrate. The communist authorities, pursuing the concept of a Poland of single ethnicity (in accordance with the recent border changes and expulsions), were allowing the Jews to leave the country. For a discussion of early communist Poland's ethnic politics, see Timothy Snyder, "The Reconstruction of Nations", chapters on modern "Ukrainian Borderland".
"f."A Communist Party of Poland had existed in the past, but was eliminated in Stalin's purges in 1938.
"g."The Soviet leadership, which had previously ordered the crushing of the Uprising in East Germany, the Hungarian Revolution and the Prague Spring, now became worried about the demoralization of the Polish army, a crucial Warsaw Pact component, because of its deployment against Polish workers. The Soviets withdrew their support for Gomułka, who insisted on the use of force; he and his close associates were subsequently ousted from the Polish Politburo by the Polish Central Committee.
"h."East of the Molotov-Ribbentrop line, the population was 43% Polish, 33% Ukrainian, 8% Belarusian and 8% Jewish. The Soviet Union did not want to appear as an aggressor, and moved its troops to Eastern Poland under the pretext of offering protection to "the kindred Ukrainian and Belorussian people".
"i."Joseph Stalin at the 1943 Tehran Conference discussed with Winston Churchill and Franklin Roosevelt new post-war borders in central-eastern Europe, including the shape of a future Poland. He endorsed the Piast Concept, which justified a massive shift of Poland's frontiers to the west. Stalin resolved to secure and stabilize the western reaches of the Soviet Union and disable the future military potential of Germany by constructing a compact and ethnically defined Poland (along with the Soviet ethnic Ukraine, Belarus and Lithuania) and by radically altering the region's system of national borders. After 1945, the Polish communist regime wholeheartedly adopted and promoted the Piast Concept, making it the centerpiece of their claim to be the true inheritors of Polish nationalism. After all the killings and population transfers during and after the war the country was 99% "Polish".
"j.""All the currently available documents of Nazi administration show that, together with the Jews, the stratum of the Polish intelligentsia was marked for total extermination. In fact, Nazi Germany achieved this goal almost by half, since Poland lost 50 percent of her citizens with university diplomas and 35 percent of those with a gimnazium diploma."
"k."Decisive political events took place in Poland shortly before the Soviet intervention in Hungary. Władysław Gomułka, a reformist leader at that time, was reinstated to the Polish Politburo and the Eighth Plenum of the Party's Central Committee was announced to convene on October 19, 1956, all without seeking a Soviet approval. The Soviet Union responded with military moves and intimidation and its "military-political delegation", led by Nikita Khrushchev, quickly arrived in Warsaw. Gomułka tried to convince them of his loyalty but insisted on the reforms that he considered essential, including a replacement of Poland's Soviet-trusted minister of defense, Konstantin Rokossovsky. The disconcerted Soviets returned to Moscow, the Polish Plenum elected Gomułka first secretary and removed Rokossovsky from the Politburo. On October 21, the Soviet Presidium followed Khrushchev's lead and decided unanimously to "refrain from military intervention" in Poland, a decision likely influenced also by the ongoing preparations for the invasion of Hungary. The Soviet gamble paid off because Gomułka in the coming years turned out to be a very dependable Soviet ally and an orthodox communist.
Unlike the other Warsaw Pact countries, Poland did not endorse the Soviet armed intervention in Hungary. The Hungarian Uprising was intensely supported by the Polish public.
"l."The delayed reinforcements were coming and the government military commanders General Tadeusz Rozwadowski and Władysław Anders wanted to keep on fighting the coup perpetrators, but President Stanisław Wojciechowski and the government decided to surrender to prevent the imminent spread of civil war. The coup brought to power the "Sanation" regime under Józef Piłsudski and Edward Rydz-Śmigły after Piłsudski's death. The Sanation regime persecuted the opposition within the military and in general. Rozwadowski died after abusive imprisonment, according to some accounts murdered. According to Aleksandra Piłsudska, the marshal's wife, following the coup and for the rest of his life Piłsudski lost his composure and appeared over-burdened.
At the time of Rydz-Śmigły's command, the Sanation camp embraced the ideology of Roman Dmowski, Piłsudski's nemesis. Rydz-Śmigły did not allow General Władysław Sikorski, an anti-Sanation enemy, to participate as a soldier in the defense of the country in September 1939. During World War II in France and Britain the Polish government in exile became dominated by anti-Sanation politicians. The perceived Sanation followers were in turn persecuted (in exile) under prime ministers Sikorski and Stanisław Mikołajczyk.
"m."General Zygmunt Berling of the Soviet-allied First Polish Army attempted in mid-September a crossing of the Vistula and landing at Czerniaków to aid the insurgents, but the operation was defeated by the Germans and the Poles suffered heavy losses.
"n."The decision to launch the Warsaw Uprising resulted in the destruction of the city, its population and its elites and has been a source of lasting controversy. According to the historians Czesław Brzoza and Andrzej Leon Sowa, orders of further military offensives, issued at the end of August 1944 as a part of Operation Tempest, show the loss of a sense of responsibility for the country's fate on the part of the Polish leadership.
"o."One of the party leaders Mieczysław Rakowski, who abandoned his mentor Gomułka following the 1970 crisis, saw the demands of the demonstrating workers as "exclusively socialist" in character, because of the way they were phrased. Most people in communist Poland, including opposition activists, did not question the supremacy of socialism or the socialist idea; misconduct by party officials, such as not following the provisions of the constitution, was blamed. This assumed standard of political correctness was increasingly challenged in the decades that followed, when pluralism became a frequently used concept.
"p."The Polish Sanation authorities were provoked by the independence-seeking Organization of Ukrainian Nationalists (OUN). OUN engaged in political assassinations, terror and sabotage, to which the Polish state responded with a repressive campaign in the 1930s, as Józef Piłsudski and his successors imposed collective responsibility on the villagers in the affected areas. After the disturbances of 1933 and 1934, a prison camp in Bereza Kartuska was established, which became notorious for its brutal regime. The government brought Polish settlers and administrators to Volhynian areas with a centuries-old tradition of Ukrainian peasant rising against Polish land owners (and to Eastern Galicia). In the late 1930s, after Piłsudski's death, military persecution intensified and a policy of "national assimilation" was aggressively pursued. Military raids, public beatings, property confiscations and the closing and destruction of Orthodox churches aroused lasting enmity in Galicia and antagonized Ukrainian society in Volhynia at, according to Timothy Snyder, the worst possible moment. However, he also notes that "Ukrainian terrorism and Polish reprisals touched only part of the population, leaving vast regions unaffected" and "the OUN's nationalist prescription, a Ukrainian state for ethnic Ukrainians alone was far from popular". Halik Kochanski wrote of the legacy of bitterness between the Ukrainians and Poles that soon exploded in the context of the World War II. See also: History of the Ukrainian minority in Poland.
"r."Foreign policy was one of the few governmental areas in which Piłsudski took an active interest. He saw Poland's role and opportunity as lying in Eastern Europe and advocated passive relations with the West. He felt that a German attack should not be feared because, even if this unlikely event were to take place, the Western powers would be bound to restrain Germany and come to Poland's rescue.
"s."According to the researcher Jan Sowa, the Commonwealth failed as a state because it was not able to conform to the emerging new European order established at the Peace of Westphalia of 1648. Poland's elective kings, restricted by the self-serving but short-sighted nobility, could not impose a strong and efficient central government, with its characteristic post-Westphalian internal and external sovereignty. The inability of Polish kings to levy and collect taxes (and therefore sustain a standing army) and conduct independent foreign policy were among the chief obstacles to Poland competing effectively on the changed European scene, where absolutist power was a prerequisite for survival and became the foundation for the abolition of serfdom and gradual formation of parliamentarism.
"t."Besides the Home Army there were other major underground fighting formations: Bataliony Chłopskie, National Armed Forces (NSZ) and Gwardia Ludowa (later Armia Ludowa). From 1943, the leaders of the nationalistic NSZ collaborated with Nazi Germany in a case unique in occupied Poland. The NSZ conducted an anti-communist civil war. Before the arrival of the Soviets, the NSZ's Holy Cross Mountains Brigade left Poland under the protection of the German army. According to the historians Czesław Brzoza and Andrzej Leon Sowa, participation figures given for the underground resistance are often inflated. In the spring of 1944, the time of the most extensive involvement of the underground organizations, there were most likely considerably fewer than 500,000 military and civilian personnel participating, over the entire spectrum, from the right wing to the communists.
"u."According to Jerzy Eisler, about 1.1 million people may have been imprisoned or detained in 1944–56 and about 50,000 may have died because of the struggle and persecution, including about 7,000 soldiers of the right-wing underground killed in the 1940s. According to Adam Leszczyński, up to 30,000 people were killed by the communist regime during the first several years after the war.
"v."According to Andrzej Stelmachowski, one of the key participants of the Polish systemic transformation, Minister Leszek Balcerowicz pursued extremely liberal economic policies, often unusually painful for society. The December 1989 Sejm statute of credit relations reform introduced an "incredible" system of privileges for banks. Banks were allowed to alter unilaterally interest rates on already existing contracts. The exceedingly high rates they instantly introduced ruined many previously profitable enterprises and caused a complete breakdown of the apartment block construction industry, which had long-term deleterious effects on the state budget as well. Balcerowicz's policies also caused permanent damage to Polish agriculture, which Balcerowicz "did not understand", and to the often successful and useful Polish cooperative movement.
"w."Led by Władysław Anders, the Polish II Corps fought at the famous Battle of Monte Cassino in 1944, as part of the Allied Italian Campaign.
"x."The concept which had become known as the Piast Idea, the chief proponent of which was Jan Ludwik Popławski, was based on the statement that the Piast homeland was inhabited by so-called "native" aboriginal Slavs and Slavonic Poles since time immemorial and only later was "infiltrated" by "alien" Celts, Germans and others. After 1945, the so-called "autochthonous" or "aboriginal" school of Polish prehistory received official backing in Poland and a considerable degree of popular support. According to this view, the Lusatian Culture which archaeologists have identified between the Oder and the Vistula in the early Iron Age, was said to be Slavonic; all non-Slavonic tribes and peoples recorded in the area at various points in ancient times were dismissed as "migrants" and "visitors". In contrast, the critics of this theory, such as Marija Gimbutas, regarded it as an unproved hypotheses and for them the date and origin of the westward migration of the Slavs were largely uncharted; the Slavonic connections of the Lusatian Culture were entirely imaginary; and the presence of an ethnically mixed and constantly changing collection of peoples on the North European Plain was taken for granted.
Further reading.
More recent general history of Poland books in English
Published in Poland

</doc>
<doc id="13773" url="https://en.wikipedia.org/wiki?curid=13773" title="Hradčany">
Hradčany

Hradčany (common ; ), the Castle District, is the district of the city of Prague, Czech Republic, surrounding the Prague Castle.
The castle is said to be the biggest castle in the world at about 570 meters in length and an average of about 130 meters wide. Its history stretches back to the 9th century. St Vitus Cathedral is located in the castle area.
Most of the district consists of noble historical palaces. There are many other attractions for visitors: romantic nooks, peaceful places and beautiful lookouts.
Hradčany was an independent borough until 1784, when the four independent boroughs that had formerly constituted Prague were proclaimed a single city. The other three were Malá Strana (, ), Staré Město (, ) and Nové Město (, ).

</doc>
<doc id="13774" url="https://en.wikipedia.org/wiki?curid=13774" title="Houston">
Houston

Houston ( ) is the most populous city in Texas and the fourth-most populous city in the United States, located in Southeast Texas near the Gulf of Mexico. With a census-estimated 2014 population of 2.239 million within a land area of , it also is the largest city in the Southern United States, as well as the seat of Harris County. It is the principal city of Houston–The Woodlands–Sugar Land, which is the fifth-most populated metropolitan area in the United States.
Houston was founded in 1836 near the banks of Buffalo Bayou (now known as Allen's Landing) and incorporated as a city on June 5, 1837. The city was named after former General Sam Houston, who was president of the Republic of Texas and had commanded and won at the Battle of San Jacinto east of where the city was established. The burgeoning port and railroad industry, combined with oil discovery in 1901, has induced continual surges in the city's population. In the mid-20th century, Houston became the home of the Texas Medical Center—the world's largest concentration of healthcare and research institutions—and NASA's Johnson Space Center, where the Mission Control Center is located.
Houston's economy has a broad industrial base in energy, manufacturing, aeronautics, and transportation. It is also leading in health care sectors and building oilfield equipment; only New York City is home to more Fortune 500 headquarters within its city limits. The Port of Houston ranks first in the United States in international waterborne tonnage handled and second in total cargo tonnage handled. Nicknamed the "Space City", Houston is a global city, with strengths in business, international trade, entertainment, culture, media, fashion, science, sports, technology, education, medicine, and research. The city has a population from various ethnic and religious backgrounds and a large and growing international community. Houston is the most diverse city in Texas and has been described as the most diverse in the United States. It is home to many cultural institutions and exhibits, which attract more than 7 million visitors a year to the Museum District. Houston has an active visual and performing arts scene in the Theater District and offers year-round resident companies in all major performing arts.
History.
In August 1836, two real estate entrepreneurs from New York—Augustus Chapman Allen and John Kirby Allen—purchased of land along Buffalo Bayou with the intent of founding a city. The Allen brothers decided to name the city after Sam Houston, the popular general at the Battle of San Jacinto, who was elected President of Texas in September 1836. The great majority of slaves in Texas came with their owners from the older slave states. Sizable numbers, however, came through the domestic slave trade. New Orleans was the center of this trade in the Deep South, but slave dealers were in Houston. Thousands of enslaved African Americans lived near the city before the Civil War. Many of them near the city worked on sugar and cotton plantations, while most of those in the city limits had domestic and artisan jobs. In 1860, 49% of the city's population was enslaved. A few slaves, perhaps as many as 2,000 between 1835 and 1865, came through the illegal African trade. Post-war Texas grew rapidly as migrants poured into the cotton lands; they also brought or purchased enslaved African Americans, whose numbers nearly tripled in the state from 1850 to 1860, from 58,000 to 182,566.
Houston was granted incorporation on June 5, 1837, with James S. Holman becoming its first mayor. In the same year, Houston became the county seat of Harrisburg County (now Harris County) and the temporary capital of the Republic of Texas. In 1840, the community established a chamber of commerce in part to promote shipping and waterborne business at the newly created port on Buffalo Bayou.
By 1860, Houston had emerged as a commercial and railroad hub for the export of cotton. Railroad spurs from the Texas inland converged in Houston, where they met rail lines to the ports of Galveston and Beaumont. During the American Civil War, Houston served as a headquarters for General John Bankhead Magruder, who used the city as an organization point for the Battle of Galveston. After the Civil War, Houston businessmen initiated efforts to widen the city's extensive system of bayous so the city could accept more commerce between downtown and the nearby port of Galveston. By 1890, Houston was the railroad center of Texas.
In 1900, after Galveston was struck by a devastating hurricane, efforts to make Houston into a viable deep-water port were accelerated. The following year, oil discovered at the Spindletop oil field near Beaumont prompted the development of the Texas petroleum industry. In 1902, President Theodore Roosevelt approved a $1 million improvement project for the Houston Ship Channel. By 1910, the city's population had reached 78,800, almost doubling from a decade before. African Americans formed a large part of the city's population, numbering 23,929 people, or nearly one-third of the residents.
President Woodrow Wilson opened the deep-water Port of Houston in 1914, seven years after digging began. By 1930, Houston had become Texas' most populous city and Harris County the most populous county. In 1940, the Census Bureau reported Houston's population as 77.5% white and 22.4% black.
When World War II started, tonnage levels at the port decreased and shipping activities were suspended; however, the war did provide economic benefits for the city. Petrochemical refineries and manufacturing plants were constructed along the ship channel because of the demand for petroleum and synthetic rubber products by the defense industry during the war. Ellington Field, initially built during World War I, was revitalized as an advanced training center for bombardiers and navigators. The Brown Shipbuilding Company was founded in 1942 to build ships for the U.S. Navy during World War II. Due to the boom in defense jobs, thousands of new workers migrated to the city, both blacks and whites competing for the higher-paying jobs. President Roosevelt had established a policy of nondiscrimination for defense contractors, and blacks gained some opportunities, especially in shipbuilding, although not without resistance from whites and increasing social tensions that erupted into occasional violence. Economic gains of blacks who entered defense industries continued in the postwar years.
In 1945, the M.D. Anderson Foundation formed the Texas Medical Center. After the war, Houston's economy reverted to being primarily port-driven. In 1948, the city annexed several unincorporated areas, more than doubling its size. Houston proper began to spread across the region.
In 1950, the availability of air conditioning provided impetus for many companies to relocate to Houston, where wages were lower than the North; this resulted in an economic boom and produced a key shift in the city's economy toward the energy sector. 
The increased production of the expanded shipbuilding industry during World War II spurred Houston's growth, as did the establishment in 1961 of NASA's "Manned Spacecraft Center" (renamed the Lyndon B. Johnson Space Center in 1973). This was the stimulus for the development of the city's aerospace industry. The Astrodome, nicknamed the "Eighth Wonder of the World", opened in 1965 as the world's first indoor domed sports stadium.
During the late 1970s, Houston had a population boom as people from the Rust Belt states moved to Texas in large numbers. The new residents came for numerous employment opportunities in the petroleum industry, created as a result of the Arab oil embargo. With the increase in professional jobs, Houston has become a destination for many college-educated persons, including African Americans in a reverse Great Migration from northern areas.
One wave of the population boom ended abruptly in the mid-1980s, as oil prices fell precipitously. The space industry also suffered in 1986 after the Space Shuttle "Challenger" disintegrated shortly after launch. A cutback in some activities existed for a period. In the late 1980s, the city's economy suffered from the nationwide recession. After the early 1990s recession, Houston made efforts to diversify its economy by focusing on aerospace and health care/biotechnology, and reduced its dependence on the petroleum industry. Since the increase of oil prices in the 2000s, the petroleum industry has again increased its share of the local economy.
In 1997, Houstonians elected Lee P. Brown as the city's first African American mayor.
In June 2001, Tropical Storm Allison dumped up to of rain on parts of Houston, causing the worst flooding in the city's history. The storm cost billions of dollars in damage and killed 20 people in Texas. By December of that same year, Houston-based energy company Enron collapsed into the third-largest ever U.S. bankruptcy during an investigation surrounding fabricated partnerships that were allegedly used to hide debt and inflate profits.
In August 2005, Houston became a shelter to more than 150,000 people from New Orleans, who evacuated from Hurricane Katrina. One month later, about 2.5 million Houston-area residents evacuated when Hurricane Rita approached the Gulf Coast, leaving little damage to the Houston area. This was the largest urban evacuation in the history of the United States. In September 2008, Houston was hit by Hurricane Ike. As many as 40% refused to leave Galveston Island because they feared the traffic problems that happened after Hurricane Rita.
During the floods in 2015 and 2016, parts of the city were covered in several inches of water.
Geography.
Houston is located east of Austin, west of the Louisiana border, and south of Dallas. According to the United States Census Bureau, the city has a total area of ; this comprises of land and covered by water. The Piney Woods are north of Houston. Most of Houston is located on the gulf coastal plain, and its vegetation is classified as temperate grassland and forest. Much of the city was built on forested land, marshes, swamp, or prairie which resembles the Deep South, and are all still visible in surrounding areas. The flatness of the local terrain, when combined with urban sprawl, has made flooding a recurring problem for the city. Downtown stands about above sea level, and the highest point in far northwest Houston is about in elevation. The city once relied on groundwater for its needs, but land subsidence forced the city to turn to ground-level water sources such as Lake Houston, Lake Conroe, and Lake Livingston. The city owns surface water rights for 1.20 billion gallons of water a day in addition to 150 million gallons a day of groundwater.
Houston has four major bayous passing through the city. Buffalo Bayou runs through downtown and the Houston Ship Channel, and has three tributaries: White Oak Bayou, which runs through the Houston Heights community northwest of Downtown and then towards Downtown; Brays Bayou, which runs along the Texas Medical Center; and Sims Bayou, which runs through the south of Houston and downtown Houston. The ship channel continues past Galveston and then into the Gulf of Mexico.
Geology.
Underpinning Houston's land surface are unconsolidated clays, clay shales, and poorly cemented sands up to several miles deep. The region's geology developed from river deposits formed from the erosion of the Rocky Mountains. These sediments consist of a series of sands and clays deposited on decaying organic marine matter, that over time, transformed into oil and natural gas. Beneath the layers of sediment is a water-deposited layer of halite, a rock salt. The porous layers were compressed over time and forced upward. As it pushed upward, the salt dragged surrounding sediments into salt dome formations, often trapping oil and gas that seeped from the surrounding porous sands. The thick, rich, sometimes black, surface soil is suitable for rice farming in suburban outskirts where the city continues to grow.
The Houston area has over 150 active faults (estimated to be 300 active faults) with an aggregate length of up to , including the Long Point–Eureka Heights fault system which runs through the center of the city. No significant historically recorded earthquakes have occurred in Houston, but researchers do not discount the possibility of such quakes having occurred in the deeper past, nor occurring in the future. Land in some areas southeast of Houston is sinking because water has been pumped out of the ground for many years. It may be associated with slip along the faults; however, the slippage is slow and not considered an earthquake, where stationary faults must slip suddenly enough to create seismic waves. These faults also tend to move at a smooth rate in what is termed "fault creep", which further reduces the risk of an earthquake.
Climate.
Houston's climate is classified as humid subtropical ("Cfa" in the Köppen climate classification system), typical of the lower South. While not located in "Tornado Alley", like much of the rest of Texas, spring supercell thunderstorms sometimes bring tornadoes to the area. Prevailing winds are from the south and southeast during most of the year, which bring heat and moisture from the nearby Gulf of Mexico.
During the summer, temperatures commonly reach over , with an average of 106.5 days per year, including a majority from June to September, with a high of 90 °F or above and 4.6 days at or over . However, humidity usually yields a higher heat index. Summer mornings average over 90% relative humidity. Winds are often light in the summer and offer little relief, except in the far southeastern outskirts near the Gulf Coast and Galveston. To cope with the strong humidity and heat, people use air conditioning in nearly every vehicle and building. In 1980, Houston was described as the "most air-conditioned place on earth". Officially, the hottest temperature ever recorded in Houston is , which was reached both on September 4, 2000, and August 28, 2011.
Houston has mild winters in contrast to most areas of the United States. In January, the normal mean temperature at Intercontinental Airport is , while that station has an average of 13 days with a low at or below freezing. Snowfall is rare. Recent snow events in Houston include a storm on December 24, 2004 when 1.0 in (2.5 cm) of snow accumulated in parts of the metro area. Falls of at least 1.0 in on both December 10, 2008, and December 4, 2009, marked the first time measurable snowfall had occurred in two consecutive years in the city's recorded history. The coldest temperature officially recorded in Houston was on January 18, 1930. Houston has historically received an ample amount of rainfall, averaging about annually per 1981–2010 normals. Localized flooding often occurs, owing to the extremely flat topography and widespread typical clay-silt prairie soils, which do not drain quickly.
Houston has excessive ozone levels and is routinely ranked among the most ozone-polluted cities in the United States. Ground-level ozone, or smog, is Houston's predominant air pollution problem, with the American Lung Association rating the metropolitan area's ozone level sixth on the "Top 10 Most Ozone-Polluted Cities" in 2014. The industries located along the ship channel are a major cause of the city's air pollution. In 2006, Houston's air quality was comparable to that of Los Angeles.
Cityscape.
Houston was incorporated in 1837 under the ward system of representation. The ward designation is the progenitor of the 11 current-day geographically oriented Houston City Council districts. Locations in Houston are generally classified as either being inside or outside the Interstate 610 Loop. The inside encompasses the central business district and many residential neighborhoods that antedate World War II. More recently, high-density residential areas have been developed within the loop. The city's outlying areas, suburbs, and enclaves are located outside of the loop. Beltway 8 encircles the city another farther out.
Though Houston is the largest city in the United States without formal zoning regulations, it has developed similarly to other Sun Belt cities because the city's land use regulations and legal covenants have played a similar role. Regulations include mandatory lot size for single-family houses and requirements that parking be available to tenants and customers. Such restrictions have had mixed results. Though some have blamed the city's low density, urban sprawl, and lack of pedestrian-friendliness on these policies, the city's land use has also been credited with having significant affordable housing, sparing Houston the worst effects of the 2008 real estate crisis. The city issued 42,697 building permits in 2008 and was ranked first in the list of healthiest housing markets for 2009.
Voters rejected efforts to have separate residential and commercial land-use districts in 1948, 1962, and 1993. Consequently, rather than a single central business district as the center of the city's employment, multiple districts have grown throughout the city in addition to downtown which include Uptown, Texas Medical Center, Midtown, Greenway Plaza, Memorial City, Energy Corridor, Westchase, and Greenspoint.
Architecture.
Houston has the fourth-tallest skyline in North America (after New York City, Chicago, and Toronto) and 12th-tallest in the world, . A seven-mile (11 km) system of tunnels and skywalks links downtown buildings containing shops and restaurants, enabling pedestrians to avoid summer heat and rain while walking between buildings.
In the 1960s, Downtown Houston consisted of a collection of midrise office structures. Downtown was on the threshold of an energy industryled boom in 1970. A succession of skyscrapers was built throughout the 1970s—many by real estate developer Gerald D. Hines—culminating with Houston's tallest skyscraper, the 75-floor, -tall JPMorgan Chase Tower (formerly the Texas Commerce Tower), completed in 1982. It is the tallest structure in Texas, 15th tallest building in the United States, and the 85th-tallest skyscraper in the world, based on highest architectural feature. In 1983, the 71-floor, -tall Wells Fargo Plaza (formerly Allied Bank Plaza) was completed, becoming the second-tallest building in Houston and Texas. Based on highest architectural feature, it is the 17th-tallest in the United States and the 95th-tallest in the world. In 2007, downtown Houston had over 43 million square feet (4,000,000 m²) of office space.
Centered on Post Oak Boulevard and Westheimer Road, the Uptown District boomed during the 1970s and early 1980s when a collection of midrise office buildings, hotels, and retail developments appeared along Interstate 610 West. Uptown became one of the most prominent instances of an edge city. The tallest building in Uptown is the 64-floor, -tall, Philip Johnson and John Burgee designed landmark Williams Tower (known as the Transco Tower until 1999). At the time of construction, it was believed to be the world's tallest skyscraper outside of a central business district. The new 20-story Skanska building are the newest office buildings built in Uptown after 30 years. The Uptown District is also home to buildings designed by noted architects I. M. Pei, César Pelli, and Philip Johnson. In the late 1990s and early 2000s, a mini-boom of midrise and highrise residential tower construction occurred, with several over 30 stories tall. Since 2000 more than 30 high-rise buildings have gone up in Houston; all told, 72 high-rises tower over the city, which adds up to about 8,300 units. In 2002, Uptown had more than 23 million square feet (2,100,000 m²) of office space with 16 million square feet (1,500,000 m²) of class A office space.
Demographics.
Houston is multicultural, in part because of its many academic institutions and strong industries, as well as being a major port city. Over 90 languages are spoken in the city. It has among the youngest populations in the nation, partly due to an influx of immigrants into Texas. An estimated 400,000 undocumented immigrants reside in the Houston area.
According to the 2010 Census, Whites made up 51% of Houston's population; 26% of the total population was non-Hispanic Whites. Blacks or African Americans made up 25% of Houston's population. American Indians made up 0.7% of the population. Asians made up 6% (1.7% Vietnamese, 1.3% Chinese, 1.3% Indian, 0.9% Pakistani, 0.4% Filipino, 0.3% Korean, 0.1% Japanese), while Pacific Islanders made up 0.1%. Individuals from some other race made up 15.2% of the city's population, of which 0.2% were non-Hispanic. Individuals from two or more races made up 3.3% of the city. 
At the 2000 Census, 1,953,631 people inhabited the city, and the population density was 3,371.7 people per square mile (1,301.8/km²). The racial makeup of the city was 49.3% White, 25.3% African American, 5.3% Asian, 0.7% American Indian, 0.1% Pacific Islander, 16.5% from some other race, and 3.1% from two or more races. In addition, Hispanics made up 37.4% of Houston's population, while non-Hispanic Whites made up 30.8%, down from 62.4% in 1970.
The median income for a household in the city was $37,000, and for a family was $40,000. Males had a median income of $32,000 versus $27,000 for females. The per capita income was $20,000. About 19% of the population and 16% of families were below the poverty line. Of the total population, 26% of those under the age of 18 and 14% of those 65 and older were living below the poverty line.
According to a 2014 study by the Pew Research Center, 73% of the population of the city identified themselves as Christians, with 50% professing attendance at a variety of churches that could be considered Protestant, and 19% professing Roman Catholic beliefs. while 20% claim no religious affiliation. The same study says that other religions (including Judaism, Buddhism, Islam, and Hinduism) collectively make up about 7% of the population
Economy.
Houston is recognized worldwide for its energy industry—particularly for oil and natural gas—as well as for biomedical research and aeronautics. Renewable energy sources—wind and solar—are also growing economic bases in the city. The Houston Ship Channel is also a large part of Houston's economic base. Because of these strengths, Houston is designated as a global city by the Globalization and World Cities Study Group and Network and global management consulting firm A.T. Kearney. The Houston area is the top U.S. market for exports, surpassing New York City in 2013, according to data released by the U.S. Department of Commerce's International Trade Administration. In 2012, the Houston–The Woodlands–Sugar Land area recorded $110.3 billion in merchandise exports. Petroleum products, chemicals, and oil and gas extraction equipment accounted for roughly two-thirds of the metropolitan area's exports last year. The top three destinations for exports were Mexico, Canada, and Brazil.
The Houston area is a leading center for building oilfield equipment. Much of its success as a petrochemical complex is due to its busy ship channel, the Port of Houston. In the United States, the port ranks first in international commerce and 10th among the largest ports in the world. Unlike most places, high oil and gasoline prices are beneficial for Houston's economy, as many of its residents are employed in the energy industry. Houston is the beginning or end point of numerous oil, gas, and products pipelines:
The Houston–The Woodlands–Sugar Land MSA's gross domestic product (GDP) in 2012 was $489 billion, making it the fourth-largest of any metropolitan area in the United States and larger than Austria's, Venezuela's, or South Africa's GDP. Only 26 countries other than the United States have a gross domestic product exceeding Houston's regional gross area product (GAP). In 2010, mining (which consists almost entirely of exploration and production of oil and gas in Houston) accounted for 26.3% of Houston's GAP up sharply in response to high energy prices and a decreased worldwide surplus of oil production capacity, followed by engineering services, health services, and manufacturing.
The University of Houston System's annual impact on the Houston area's economy equates to that of a major corporation: $1.1 billion in new funds attracted annually to the Houston area, $3.13 billion in total economic benefit, and 24,000 local jobs generated. This is in addition to the 12,500 new graduates the U.H. System produces every year who enter the workforce in Houston and throughout the state of Texas. These degree-holders tend to stay in Houston. After five years, 80.5% of graduates are still living and working in the region.
In 2006, the Houston metropolitan area ranked first in Texas and third in the U.S. within the category of "Best Places for Business and Careers" by "Forbes" magazine. Foreign governments have established 92 consular offices in Houston's metropolitan area, the third-highest in the nation. Forty foreign governments maintain trade and commercial offices here with 23 active foreign chambers of commerce and trade associations. Twenty-five foreign banks representing 13 nations operate in Houston, providing financial assistance to the international community.
In 2008, Houston received top ranking on "Kiplinger's Personal Finance" Best Cities of 2008 list, which ranks cities on their local economy, employment opportunities, reasonable living costs, and quality of life. The city ranked fourth for highest increase in the local technological innovation over the preceding 15 years, according to "Forbes" magazine. In the same year, the city ranked second on the annual "Fortune" 500 list of company headquarters, first for "Forbes" magazine's Best Cities for College Graduates, and first on their list of Best Cities to Buy a Home. In 2010, the city was rated the best city for shopping, according to "Forbes".
In 2012, the city was ranked number one for paycheck worth by "Forbes" and in late May 2013, Houston was identified as America's top city for employment creation.
In 2013, Houston was identified as the number one U.S. city for job creation by the U.S. Bureau of Statistics after it was not only the first major city to regain all the jobs lost in the preceding economic downturn, but also after the crash, more than two jobs were added for every one lost. Economist and vice president of research at the Greater Houston Partnership Patrick Jankowski attributed Houston's success to the ability of the region's real estate and energy industries to learn from historical mistakes. Furthermore, Jankowski stated that "more than 100 foreign-owned companies relocated, expanded or started new businesses in Houston" between 2008 and 2010, and this openness to external business boosted job creation during a period when domestic demand was problematically low. Also in 2013, Houston again appeared on "Forbes"' list of Best Places for Business and Careers.
Culture.
Located in the American South, Houston is a diverse city with a large and growing international community. The metropolitan area is home to an estimated 1.1 million (21.4 percent) residents who were born outside the United States, with nearly two-thirds of the area's foreign-born population from south of the United States–Mexico border. Additionally, more than one in five foreign-born residents are from Asia. The city is home to the nation's third-largest concentration of consular offices, representing 86 countries.
Many annual events celebrate the diverse cultures of Houston. The largest and longest-running is the annual Houston Livestock Show and Rodeo, held over 20 days from early to late March, and is the largest annual livestock show and rodeo in the world. Another large celebration is the annual night-time Houston Pride Parade, held at the end of June. Other annual events include the Houston Greek Festival, Art Car Parade, the Houston Auto Show, the Houston International Festival, and the Bayou City Art Festival, which is considered to be one of the top five art festivals in the United States.
Houston received the official nickname of "Space City" in 1967 because it is the location of NASA's Lyndon B. Johnson Space Center. Other nicknames often used by locals include "Bayou City", "Clutch City", "Magnolia City", "New Houston" (a tribute to the cultural contributions of New Orleans natives who left their city during the 2005 Hurricane Katrina catastrophe), and "H-Town".
Arts and theater.
The Houston Theater District, located downtown, is home to nine major performing arts organizations and six performance halls. It is the second-largest concentration of theater seats in a downtown area in the United States. Houston is one of few United States cities with permanent, professional, resident companies in all major performing arts disciplines: opera (Houston Grand Opera), ballet (Houston Ballet), music (Houston Symphony Orchestra), and theater (The Alley Theatre). Houston is also home to folk artists, art groups and various small progressive arts organizations. Houston attracts many touring Broadway acts, concerts, shows, and exhibitions for a variety of interests. Facilities in the Theater District include the Jones Hall—home of the Houston Symphony Orchestra and Society for the Performing Arts—and the Hobby Center for the Performing Arts.
The Museum District's cultural institutions and exhibits attract more than 7 million visitors a year. Notable facilities include The Museum of Fine Arts, Houston Museum of Natural Science, the Contemporary Arts Museum Houston, the Station Museum of Contemporary Art, Holocaust Museum Houston, and the Houston Zoo. Located near the Museum District are The Menil Collection, Rothko Chapel, and the Byzantine Fresco Chapel Museum.
Bayou Bend is a facility of the Museum of Fine Arts that houses one of America's best collections of decorative art, paintings, and furniture. Bayou Bend is the former home of Houston philanthropist Ima Hogg.
The National Museum of Funeral History is located in Houston near the George Bush Intercontinental Airport. The museum houses the original Popemobile used by Pope John Paul II in the 1980s along with numerous hearses, embalming displays, and information on famous funerals.
Venues across Houston regularly host local and touring rock, blues, country, dubstep, and Tejano musical acts. While Houston has never been widely known for its music scene, Houston hip-hop has become a significant, independent music scene that is influential nationwide.
Tourism and recreation.
The Theater District is a 17-block area in the center of downtown Houston that is home to the Bayou Place entertainment complex, restaurants, movies, plazas, and parks. Bayou Place is a large multilevel building containing full-service restaurants, bars, live music, billiards, and Sundance Cinema. The Bayou Music Center stages live concerts, stage plays, and stand-up comedy.
Space Center Houston is the official visitors' center of NASA's Lyndon B. Johnson Space Center. The Space Center has many interactive exhibits including moon rocks, a shuttle simulator, and presentations about the history of NASA's manned space flight program. Other tourist attractions include the Galleria (Texas's largest shopping mall, located in the Uptown District), Old Market Square, the Downtown Aquarium, and Sam Houston Race Park.
Of worthy mention are Houston's current Chinatown and the Mahatma Gandhi District. Both areas offer a picturesque view of Houston's multicultural makeup. Restaurants, bakeries, traditional-clothing boutiques, and specialty shops can be found in both areas.
Houston is home to 337 parks, including Hermann Park, Terry Hershey Park, Lake Houston Park, Memorial Park, Tranquility Park, Sesquicentennial Park, Discovery Green, and Sam Houston Park. Within Hermann Park are the Houston Zoo and the Houston Museum of Natural Science. Sam Houston Park contains restored and reconstructed homes which were originally built between 1823 and 1905. A proposal has been made to open the city's first botanic garden at Herman Brown Park.
Of the 10 most populous U.S. cities, Houston has the most total area of parks and green space, . The city also has over 200 additional green spaces—totaling over that are managed by the city—including the Houston Arboretum and Nature Center. The Lee and Joe Jamail Skatepark is a public skatepark owned and operated by the city of Houston, and is one of the largest skateparks in Texas consisting of a 30,000-ft2 (2,800 m2)in-ground facility. The Gerald D. Hines Waterwall Park—located in the Uptown District of the city—serves as a popular tourist attraction and for weddings and various celebrations. A 2011 study by Walk Score ranked Houston the 23rd most walkable of the 50 largest cities in the United States. Wet'n'Wild SplashTown is a water park located north of Houston.
The Bayport Cruise Terminal on the Houston Ship Channel is port of call for both Princess Cruises and Norwegian Cruise Line.
Sports.
Houston has sports teams for every major professional league except the National Hockey League. The Houston Astros are a Major League Baseball expansion team formed in 1962 (known as the "Colt .45s" until 1965) that made one World Series appearance in 2005. The Houston Rockets are a National Basketball Association franchise based in the city since 1971. They have won two NBA Championships: in 1994 and 1995 under star players Hakeem Olajuwon, Otis Thorpe, Clyde Drexler, Vernon Maxwell, and Kenny Smith. The Houston Texans are a National Football League expansion team formed in 2002. The Houston Dynamo is a Major League Soccer franchise that has been based in Houston since 2006 after it won two MLS Cup titles in 2006 and 2007. The Houston Dash team plays in the National Women's Soccer League. The Scrap Yard Dawgs, a women's professional softball team, are expected to play in the National Pro Fastpitch from 2016.
Minute Maid Park (home of the Astros) and Toyota Center (home of the Rockets), are located in downtown Houston. Houston has the NFL's first retractable-roof stadium with natural grass, NRG Stadium (home of the Texans). Minute Maid Park is also a retractable-roof stadium. Toyota Center also has the largest screen for an indoor arena in the United States built to coincide with the arena's hosting of the 2013 NBA All-Star Game. BBVA Compass Stadium is a soccer-specific stadium for the Houston Dynamo, the Texas Southern University football team, and Houston Dash, located in East Downtown. In addition, NRG Astrodome was the first indoor stadium in the world, built in 1965. Other sports facilities include Hofheinz Pavilion (Houston Cougars basketball), Rice Stadium (Rice Owls football), and Reliant Arena. TDECU Stadium is where the University of Houston Houston Cougars football team plays.
Houston has hosted several major sports events: the 1968, 1986 and 2004 Major League Baseball All-Star Games; the 1989, 2006 and 2013 NBA All-Star Games; Super Bowl VIII and Super Bowl XXXVIII, as well as hosting the 2005 World Series and 1981, 1986, 1994 and 1995 NBA Finals, winning the latter two. Super Bowl LI is currently slated to be hosted in NRG Stadium in 2017.
The city has hosted several major professional and college sporting events, including the annual Houston Open golf tournament. Houston hosts the annual NCAA College Baseball Classic every February and NCAA football's Texas Bowl in December.
The Grand Prix of Houston, an annual auto race on the IndyCar Series circuit is held on a 1.7-mile temporary street circuit in Reliant Park. The October 2013 event was held using a tweaked version of the 2006–2007 course. The event has a 5-year race contract through 2017 with IndyCar. In motorcycling, the Astrodome hosted an AMA Supercross Championship round from 1974 to 2003 and the NRG Stadium since 2003.
Government and politics.
The city of Houston has a strong mayoral form of municipal government. Houston is a home rule city and all municipal elections in the state of Texas are nonpartisan. The City's elected officials are the mayor, city controller and 16 members of the Houston City Council. The current mayor of Houston is Sylvester Turner, a Democrat elected on a nonpartisan ballot. Houston's mayor serves as the city's chief administrator, executive officer, and official representative, and is responsible for the general management of the city and for seeing that all laws and ordinances are enforced.
The original city council line-up of 14 members (nine district-based and five at-large positions) was based on a U.S. Justice Department mandate which took effect in 1979. At-large council members represent the entire city. Under the city charter, once the population in the city limits exceeded 2.1 million residents, two additional districts were to be added. The city of Houston's official 2010 census count was 600 shy of the required number; however, as the city was expected to grow beyond 2.1 million shortly thereafter, the two additional districts were added for, and the positions filled during, the August 2011 elections.
The city controller is elected independently of the mayor and council. The controller's duties are to certify available funds prior to committing such funds and processing disbursements. The city's fiscal year begins on July 1 and ends on June 30. Ronald Green is the city controller, serving his first term as of January 2010.
As the result of a 2015 referendum in Houston, a mayor is elected for a four-year term, and can be elected to as many as two consecutive terms. The term limits were spearheaded by conservative political activist Clymer Wright. The city controller and city council members are also subject to the same two-year, three-term limitations.
Houston is considered to be a politically divided city whose balance of power often sways between Republicans and Democrats. Much of the city's wealthier areas vote Republican while the city's working class and minority areas vote Democratic. According to the 2005 Houston Area Survey, 68 percent of non-Hispanic whites in Harris County are declared or favor Republicans while 89 percent of non-Hispanic blacks in the area are declared or favor Democrats. About 62 percent Hispanics (of any race) in the area are declared or favor Democrats. The city has often been known to be the most politically diverse city in Texas, a state known for being generally conservative. As a result, the city is often a contested area in statewide elections. In 2009, Houston became the first US city with a population over 1 million citizens to elect a gay mayor, by electing Annise Parker.
Crime.
Houston's murder rate ranked 46th of U.S. cities with a population over 250,000 in 2005 (per capita rate of 16.3 murders per 100,000 population). In 2010, the city's murder rate (per capita rate of 11.8 murders per 100,000 population) was ranked sixth among U.S. cities with a population of over 750,000 (behind New York City, Chicago, Detroit, Dallas, and Philadelphia) according to the FBI.
Murders fell by 37 percent from January to June 2011, compared with the same period in 2010. Houston's total crime rate including violent and nonviolent crimes decreased by 11 percent.
Houston is a significant hub for trafficking of cocaine, cannabis, heroin, MDMA, and methamphetamine due to its size and proximity to major illegal drug exporting nations. Houston is one of the country's largest hubs for human trafficking.
In the early 1970s, Houston, Pasadena and several coastal towns were the site of the Houston Mass Murders, which at the time were the deadliest case of serial killing in American history.
Education.
Seventeen school districts exist within the city of Houston. The Houston Independent School District (HISD) is the seventh-largest school district in the United States. HISD has 112 campuses that serve as magnet or vanguard schools—specializing in such disciplines as health professions, visual and performing arts, and the sciences. There are also many charter schools that are run separately from school districts. In addition, some public school districts also have their own charter schools.
The Houston area encompasses more than 300 private schools, many of which are accredited by Texas Private School Accreditation Commission recognized agencies. The Houston Area Independent Schools offer education from a variety of different religious as well as secular viewpoints. The Houston area Catholic schools are operated by the Archdiocese of Galveston-Houston.
Colleges and universities.
Several private institutions of higher learning—ranging from liberal arts colleges, such as The University of St. Thomas, Houston's only Catholic university, to Rice University, the nationally recognized research university—are located within the city. Rice, with a total enrollment of slightly more than 6,000 students, has a number of distinguished graduate programs and research institutes, such as the James A. Baker Institute for Public Policy. Houston Baptist University, affiliated with the Baptist General Convention of Texas, offers bachelor's and graduate degrees. It was founded in 1960 and is located in the Sharpstown area in Southwest Houston.
Three community college districts exist with campuses in and around Houston. The Houston Community College System serves most of Houston. The northwestern through northeastern parts of the city are served by various campuses of the Lone Star College System, while the southeastern portion of Houston is served by San Jacinto College, and a northeastern portion is served by Lee College. The Houston Community College and Lone Star College systems are within the 10 largest institutions of higher learning in the United States.
Media.
The primary network-affiliated television stations are KPRC-TV (NBC), KHOU-TV (CBS), KTRK-TV (ABC), KRIV (Fox), KIAH (The CW), and KTXH (MyNetworkTV). KTRK-TV, KRIV and KTXH operate as owned-and-operated stations of their networks.
The Houston–The Woodlands–Sugar Land metropolitan area is served by one public television station and two public radio stations. KUHT ("HoustonPBS") is a PBS member station and is the first public television station in the United States. Houston Public Radio is listener-funded and comprises two NPR member stations: KUHF ("KUHF News") and KUHA ("Classical 91.7"). KUHF is news/talk radio and KUHA is a classical music station. The University of Houston System owns and holds broadcasting licenses to KUHT, KUHF, and KUHA. The stations broadcast from the Melcher Center for Public Broadcasting, located on the campus of the University of Houston.
Houston is served by the "Houston Chronicle", its only major daily newspaper with wide distribution. The Hearst Corporation, which owns and operates the "Houston Chronicle", bought the assets of the "Houston Post"—its long-time rival and main competition—when "Houston Post" ceased operations in 1995. The "Houston Post" was owned by the family of former Lieutenant Governor Bill Hobby of Houston. The only other major publication to serve the city is the "Houston Press"—a free alternative weekly with a weekly readership of more than 300,000.
Infrastructure.
Healthcare.
Houston is the seat of the internationally renowned Texas Medical Center, which contains the world's largest concentration of research and healthcare institutions. All 49 member institutions of the Texas Medical Center are non-profit organizations. They provide patient and preventive care, research, education, and local, national, and international community well-being.
Employing more than 73,600 people, institutions at the medical center include 13 hospitals and two specialty institutions, two medical schools, four nursing schools, and schools of dentistry, public health, pharmacy, and virtually all health-related careers. It is where one of the first—and still the largest—air emergency service, Life Flight, was created, and a very successful inter-institutional transplant program was developed. More heart surgeries are performed at the Texas Medical Center than anywhere else in the world.
Some of the academic and research health institutions at the center include MD Anderson Cancer Center, Baylor College of Medicine, UT Health Science Center, Memorial Hermann Hospital, The Methodist Hospital, Texas Children's Hospital, and University of Houston College of Pharmacy.
The Baylor College of Medicine has annually been considered within the top ten medical schools in the nation; likewise, the MD Anderson Cancer Center has consistently ranked as one of the top two U.S. hospitals specializing in cancer care by "U.S. News & World Report" since 1990. The Menninger Clinic, a renowned psychiatric treatment center, is affiliated with Baylor College of Medicine and The Methodist Hospital System. With hospital locations nationwide and headquarters in Houston, the Triumph Healthcare hospital system is the third largest long term acute care provider nationally.
Transportation.
Highways.
71.7 percent of residents drive alone to work. Houston's freeway system comprises of freeways and expressways in a ten-county metropolitan area. However, the Texas Transportation Institute's annual Urban Mobility Report found that Houston had the fourth-worst congestion in the country with commuters spending an average of 58 hours in traffic in 2009.
Houston's highway system has a hub-and-spoke freeway structure serviced by multiple loops. The innermost loop is Interstate 610, which encircles downtown, the medical center, and many core neighborhoods with around a diameter. Beltway 8 and its freeway core, the Sam Houston Tollway, form the middle loop at a diameter of roughly . A proposed highway project, State Highway 99 (Grand Parkway), will form a third loop outside of Houston, totaling 180 miles in length and making an almost-complete circumference, with the exception of crossing the ship channel. As of June 2014, two of eleven segments of State Highway 99 have been completed to the west of Houston, and three northern segments totaling 38 miles. In addition to the Sam Houston Tollway loop mentioned above, the Harris County Toll Road Authority currently operates four spoke tollways: The Katy Managed Lanes of Interstate 10, the Hardy Toll Road, the Westpark Tollway, and the Fort Bend Parkway Extension. Other spoke roads either planned or under construction include Crosby Freeway, and the future Alvin Freeway.
Houston's freeway system is monitored by Houston TranStar—a partnership of four government agencies that are responsible for providing transportation and emergency management services to the region.
Transit systems.
The Metropolitan Transit Authority of Harris County (METRO) provides public transportation in the form of buses, light rail, and lift vans.
METRO began light rail service on January 1, 2004, with the inaugural track ("Red Line") running about from the University of (UHD), which traverses through the Texas Medical Center and terminates at NRG Park. METRO is currently in the design phase of a 10-year expansion plan that will add five more lines. and expand the current Red Line. Amtrak, the national passenger rail system, provides service three times a week to Houston via the (Los Angeles–New Orleans), which stops at a train station on the north side of the downtown area. The station saw 14,891 boardings and alightings in fiscal year 2008. In 2012, there was a 25 percent increase in ridership to 20,327 passengers embarking from the Houston Amtrak station.
Cycling.
Houston has the largest number of bike commuters in Texas with over 160 miles of dedicated bikeways. The city is currently in the process of expanding its on and off street bikeway network. A new Bicycle sharing system known as Houston B-Cycle currently operates 29 different stations in downtown and neighboring areas
Airports.
Houston is served by three airports, two of which are commercial that served 52 million passengers in 2007 and managed by the Houston Airport System. The Federal Aviation Administration and the state of Texas selected the "Houston Airport System as Airport of the Year" for 2005, largely because of its multi-year, $3.1 billion airport improvement program for both major airports in Houston.
The primary city airport is George Bush Intercontinental Airport (IAH), the tenth-busiest in the United States for total passengers, and twenty eighth-busiest worldwide. Bush Intercontinental currently ranks fourth in the United States for non-stop domestic and international service with 182 destinations. In 2006, the United States Department of Transportation named IAH the fastest-growing of the top ten airports in the United States. The Houston Air Route Traffic Control Center stands on the George Bush Intercontinental Airport grounds.
Houston was the headquarters of Continental Airlines until its 2010 merger with United Airlines with headquarters in Chicago; regulatory approval for the merger was granted in October of that year. Bush Intercontinental became United Airlines' largest airline hub. The airline retained a significant operational presence in Houston while offering more than 700 daily departures from the city. In early 2007, Bush Intercontinental Airport was named a model "port of entry" for international travelers by U.S. Customs and Border Protection.
The second-largest commercial airport is William P. Hobby Airport (named Houston International Airport until 1967) which operates primarily short- to medium-haul domestic flights. However, in 2015 Southwest Airlines launched service from a new international terminal at Hobby airport to several destinations in Mexico, Central America, and the Caribbean. These were the first international flights flown from Hobby since 1969. Houston's aviation history is showcased in the 1940 Air Terminal Museum located in the old terminal building on the west side of the airport. Hobby Airport has been recognized with two awards for being one of the top five performing airports in the world and for customer service by Airports Council International.
Houston's third municipal airport is Ellington Airport (a former U.S. Air Force base) used by military, government, NASA, and general aviation sectors.
Sister cities.
The Houston Office of Protocol and International Affairs is the city's liaison to Houston's sister cities and to the national governing organization, Sister Cities International. Through their official city-to-city relationships, these volunteer associations promote people-to-people diplomacy and encourage citizens to develop mutual trust and understanding through commercial, cultural, educational, and humanitarian exchanges.

</doc>
<doc id="13776" url="https://en.wikipedia.org/wiki?curid=13776" title="Head (disambiguation)">
Head (disambiguation)

The head is the part of an animal that usually comprises the brain, eyes, ears, nose, and mouth.
Head may also refer to:

</doc>
<doc id="13777" url="https://en.wikipedia.org/wiki?curid=13777" title="Hard disk drive">
Hard disk drive

A hard disk drive (HDD), hard disk, hard drive or fixed disk is a data storage device used for storing and retrieving digital information using one or more rigid ("hard") rapidly rotating disks (platters) coated with magnetic material. The platters are paired with magnetic heads arranged on a moving actuator arm, which read and write data to the platter surfaces. Data is accessed in a random-access manner, meaning that individual blocks of data can be stored or retrieved in any order and not only sequentially. HDDs are a type of non-volatile memory, retaining stored data even when powered off.
Introduced by IBM in 1956, HDDs became the dominant secondary storage device for general-purpose computers by the early 1960s. Continuously improved, HDDs have maintained this position into the modern era of servers and personal computers. More than 200 companies have produced HDD units, though most current units are manufactured by Seagate, Toshiba and Western Digital. , HDD production (exabytes per year) and areal density are growing, although unit shipments are declining.
The primary characteristics of an HDD are its capacity and performance. Capacity is specified in unit prefixes corresponding to powers of : a 1-terabyte (TB) drive has a capacity of gigabytes (GB; where 1 gigabyte = bytes). Typically, some of an HDD's capacity is unavailable to the user because it is used by the file system and the computer operating system, and possibly inbuilt redundancy for error correction and recovery. Performance is specified by the time required to move the heads to a track or cylinder (average access time) plus the time it takes for the desired sector to move under the head (average latency, which is a function of the physical rotational speed in revolutions per minute), and finally the speed at which the data is transmitted (data rate).
The two most common form factors for modern HDDs are 3.5-inch, for desktop computers, and 2.5-inch, primarily for laptops. HDDs are connected to systems by standard interface cables such as PATA (Parallel ATA), SATA (Serial ATA), USB or SAS (Serial attached SCSI) cables.
, the primary competing technology for secondary storage is flash memory in the form of solid-state drives (SSDs), which have higher data transfer rates, better reliability, and significantly lower latency and access times, but HDDs remain the dominant medium for secondary storage due to advantages in cost per unit of storage. However, SSDs are replacing HDDs where speed, power consumption and durability are more important considerations. Hybrid drive products, also known by the initialism SSHD, have been available since 2007, made as a combination of HDD and SSD technology in a single device.
History.
Hard disk drives were introduced in 1956 as data storage for an IBM real-time transaction processing computer and were developed for use with general-purpose mainframe and minicomputers. The first IBM drive, the 350 RAMAC, was approximately the size of two refrigerators and stored five million six-bit characters (3.75 megabytes) on a stack of 50 disks.
The IBM 350 RAMAC disk storage unit was superseded by the IBM 1301 disk storage unit, which consisted of 50 platters, each about 1/8-inch thick and 24 inches in diameter. Whereas the IBM 350 used two read/write heads, pneumatically actuated and moving through two dimensions, the 1301 was one of the first disk storage units to use an array of heads, one per platter, moving as a single unit. Cylinder-mode read/write operations were supported, while the heads flew about 250 micro-inches above the platter surface. Motion of the head array depended upon a binary adder system of hydraulic actuators which assured repeatable positioning. The 1301 cabinet was about the size of three home refrigerators placed side by side, storing the equivalent of about 21 million eight-bit bytes. Access time was about 200 milliseconds.
In 1962, IBM introduced the model 1311 disk drive, which was about the size of a washing machine and stored two million characters on a removable disk pack. Users could buy additional packs and interchange them as needed, much like reels of magnetic tape. Later models of removable pack drives, from IBM and others, became the norm in most computer installations and reached capacities of 300 megabytes by the early 1980s. Non-removable HDDs were called "fixed disk" drives.
Some high-performance HDDs were manufactured with one head per track (e.g. IBM 2305) so that no time was lost physically moving the heads to a track. Known as fixed-head or head-per-track disk drives they were very expensive and are no longer in production.
In 1973, IBM introduced a new type of HDD codenamed "Winchester". Its primary distinguishing feature was that the disk heads were not withdrawn completely from the stack of disk platters when the drive was powered down. Instead, the heads were allowed to "land" on a special area of the disk surface upon spin-down, "taking off" again when the disk was later powered on. This greatly reduced the cost of the head actuator mechanism, but precluded removing just the disks from the drive as was done with the disk packs of the day. Instead, the first models of "Winchester technology" drives featured a removable disk module, which included both the disk pack and the head assembly, leaving the actuator motor in the drive upon removal. Later "Winchester" drives abandoned the removable media concept and returned to non-removable platters.
Like the first removable pack drive, the first "Winchester" drives used platters in diameter. A few years later, designers were exploring the possibility that physically smaller platters might offer advantages. Drives with non-removable eight-inch platters appeared, and then drives that used a form factor (a mounting width equivalent to that used by contemporary floppy disk drives). The latter were primarily intended for the then-fledgling personal computer (PC) market.
As the 1980s began, HDDs were a rare and very expensive additional feature in PCs, but by the late 1980s their cost had been reduced to the point where they were standard on all but the cheapest computers.
Most HDDs in the early 1980s were sold to PC end users as an external, add-on subsystem. The subsystem was not sold under the drive manufacturer's name but under the subsystem manufacturer's name such as Corvus Systems and Tallgrass Technologies, or under the PC system manufacturer's name such as the Apple ProFile. The IBM PC/XT in 1983 included an internal 10 MB HDD, and soon thereafter internal HDDs proliferated on personal computers.
External HDDs remained popular for much longer on the Apple Macintosh. Many Macintosh computers made between 1986 and 1998 featured a SCSI port on the back, making external expansion simple. Older compact Macintosh computers did not have user-accessible hard drive bays (indeed, the Macintosh 128K, Macintosh 512K, and Macintosh Plus did not feature a hard drive bay at all), so on those models external SCSI disks were the only reasonable option for expanding upon any internal storage.
The 2011 Thailand floods damaged the manufacturing plants and impacted hard disk drive cost adversely between 2011 and 2013.
Driven by ever increasing areal density since their invention, HDDs have continuously improved their characteristics; a few highlights are listed in the table above. At the same time, market application expanded from mainframe computers of the late 1950s to most mass storage applications including computers and consumer applications such as storage of entertainment content.
Technology.
Magnetic recording.
An HDD records data by magnetizing a thin film of ferromagnetic material on a disk. Sequential changes in the direction of magnetization represent binary data bits. The data is read from the disk by detecting the transitions in magnetization. User data is encoded using an encoding scheme, such as run-length limited encoding, which determines how the data is represented by the magnetic transitions.
A typical HDD design consists of a "" that holds flat circular disks, also called platters, which hold the recorded data. The platters are made from a non-magnetic material, usually aluminum alloy, glass, or ceramic, and are coated with a shallow layer of magnetic material typically 10–20 nm in depth, with an outer layer of carbon for protection. For reference, a standard piece of copy paper is .
The platters in contemporary HDDs are spun at speeds varying from 4,200 rpm in energy-efficient portable devices, to 15,000 rpm for high-performance servers. The first HDDs spun at 1,200 rpm and, for many years, 3,600 rpm was the norm. As of December 2013, the platters in most consumer-grade HDDs spin at either 5,400 rpm or 7,200 rpm.
Information is written to and read from a platter as it rotates past devices called read-and-write heads that are positioned to operate very close to the magnetic surface, with their flying height often in the range of tens of nanometers. The read-and-write head is used to detect and modify the magnetization of the material passing immediately under it.
In modern drives, there is one head for each magnetic platter surface on the spindle, mounted on a common arm. An actuator arm (or access arm) moves the heads on an arc (roughly radially) across the platters as they spin, allowing each head to access almost the entire surface of the platter as it spins. The arm is moved using a voice coil actuator or in some older designs a stepper motor. Early hard disk drives wrote data at some constant bits per second, resulting in all tracks having the same amount of data per track but modern drives (since the 1990s) use zone bit recording—increasing the write speed from inner to outer zone and thereby storing more data per track in the outer zones.
In modern drives, the small size of the magnetic regions creates the danger that their magnetic state might be lost because of thermal effects, thermally induced magnetic instability which is commonly known as the "superparamagnetic limit". To counter this, the platters are coated with two parallel magnetic layers, separated by a 3-atom layer of the non-magnetic element ruthenium, and the two layers are magnetized in opposite orientation, thus reinforcing each other. Another technology used to overcome thermal effects to allow greater recording densities is perpendicular recording, first shipped in 2005, and as of 2007 the technology was used in many HDDs.
In 2004, a new concept was introduced to allow further increase of the data density in magnetic recording, using recording media consisting of coupled soft and hard magnetic layers. That so-called "exchange spring media", also known as "exchange coupled composite media", allows good writability due to the write-assist nature of the soft layer. However, the thermal stability is determined only by the hardest layer and not influenced by the soft layer.
Components.
A typical HDD has two electric motors; a spindle motor that spins the disks and an actuator (motor) that positions the read/write head assembly across the spinning disks. The disk motor has an external rotor attached to the disks; the stator windings are fixed in place. Opposite the actuator at the end of the head support arm is the read-write head; thin printed-circuit cables connect the read-write heads to amplifier electronics mounted at the pivot of the actuator. The head support arm is very light, but also stiff; in modern drives, acceleration at the head reaches 550 "g".
The "" is a permanent magnet and moving coil motor that swings the heads to the desired position. A metal plate supports a squat neodymium-iron-boron (NIB) high-flux magnet. Beneath this plate is the moving coil, often referred to as the "voice coil" by analogy to the coil in loudspeakers, which is attached to the actuator hub, and beneath that is a second NIB magnet, mounted on the bottom plate of the motor (some drives only have one magnet).
The voice coil itself is shaped rather like an arrowhead, and made of doubly coated copper magnet wire. The inner layer is insulation, and the outer is thermoplastic, which bonds the coil together after it is wound on a form, making it self-supporting. The portions of the coil along the two sides of the arrowhead (which point to the actuator bearing center) interact with the magnetic field, developing a tangential force that rotates the actuator. Current flowing radially outward along one side of the arrowhead and radially inward on the other produces the tangential force. If the magnetic field were uniform, each side would generate opposing forces that would cancel each other out. Therefore, the surface of the magnet is half north pole and half south pole, with the radial dividing line in the middle, causing the two sides of the coil to see opposite magnetic fields and produce forces that add instead of canceling. Currents along the top and bottom of the coil produce radial forces that do not rotate the head.
The HDD's electronics control the movement of the actuator and the rotation of the disk, and perform reads and writes on demand from the disk controller. Feedback of the drive electronics is accomplished by means of special segments of the disk dedicated to servo feedback. These are either complete concentric circles (in the case of dedicated servo technology), or segments interspersed with real data (in the case of embedded servo technology). The servo feedback optimizes the signal to noise ratio of the GMR sensors by adjusting the voice-coil of the actuated arm. The spinning of the disk also uses a servo motor. Modern disk firmware is capable of scheduling reads and writes efficiently on the platter surfaces and remapping sectors of the media which have failed.
Error rates and handling.
Modern drives make extensive use of error correction codes (ECCs), particularly Reed–Solomon error correction. These techniques store extra bits, determined by mathematical formulas, for each block of data; the extra bits allow many errors to be corrected invisibly. The extra bits themselves take up space on the HDD, but allow higher recording densities to be employed without causing uncorrectable errors, resulting in much larger storage capacity. For example, a typical 1 TB hard disk with 512-byte sectors provides additional capacity of about 93 GB for the ECC data.
In the newest drives, as of 2009, low-density parity-check codes (LDPC) were supplanting Reed-Solomon; LDPC codes enable performance close to the Shannon Limit and thus provide the highest storage density available.
Typical hard disk drives attempt to "remap" the data in a physical sector that is failing to a spare physical sector provided by the drive's "spare sector pool" (also called "reserve pool"), while relying on the ECC to recover stored data while the amount of errors in a bad sector is still low enough. The S.M.A.R.T (Self-Monitoring, Analysis and Reporting Technology) feature counts the total number of errors in the entire HDD fixed by ECC (although not on all hard drives as the related S.M.A.R.T attributes "Hardware ECC Recovered" and "Soft ECC Correction" are not consistently supported), and the total number of performed sector remappings, as the occurrence of many such errors may predict an HDD failure.
The "No-ID Format", developed by IBM in the mid-1990s, contains information about which sectors are bad and where remapped sectors have been located.
Only a tiny fraction of the detected errors ends up as not correctable. For example, specification for an enterprise SAS disk (a model from 2013) estimates this fraction to be one uncorrected error in every 1016 bits, and another SAS enterprise disk from 2013 specifies similar error rates. Another modern (as of 2013) enterprise SATA disk specifies an error rate of less than 10 non-recoverable read errors in every 1016 bits. An enterprise disk with a Fibre Channel interface, which uses 520 byte sectors to support the Data Integrity Field standard to combat data corruption, specifies similar error rates in 2005.
The worst type of errors are those that go unnoticed, and are not even detected by the disk firmware or the host operating system. These errors are known as silent data corruption, some of which may be caused by hard disk drive malfunctions.
Future development.
The rate of areal density advancement was similar to Moore's law (doubling every two years) through 2010: 60% per year during 1988–1996, 100% during 1996–2003 and 30% during 2003–2010. Gordon Moore (1997) called the increase "flabbergasting," while observing later that growth cannot continue forever. Areal density advancement slowed to 10% per year during 2011–2014, due to difficulty in migrating from perpendicular recording to newer technologies.
Areal density is the inverse of bit cell size, so an increase in areal density corresponds to a decrease in bit cell size. In 2013, a production desktop 3 TB HDD (with four platters) would have had an areal density of about 500 Gbit/in2 which would have amounted to a bit cell comprising about 18 magnetic grains (11 by 1.6 grains). Since the mid-2000s areal density progress has increasingly been challenged by a superparamagnetic trilemma involving grain size, grain magnetic strength and ability of the head to write. In order to maintain acceptable signal to noise smaller grains are required; smaller grains may self-reverse (thermal instability) unless their magnetic strength is increased, but known write head materials are unable to generate a magnetic field sufficient to write the medium. Several new magnetic storage technologies are being developed to overcome or at least abate this trilemma and thereby maintain the competitiveness of HDDs with respect to products such as flash memory-based solid-state drives (SSDs).
In 2013, Seagate introduced one such technology, shingled magnetic recording (SMR). Additionally, SMR comes with design complexities that may cause reduced write performance. Other new recording technologies that, , still remain under development include heat-assisted magnetic recording (HAMR), microwave-assisted magnetic recording (MAMR), two-dimensional magnetic recording (TDMR), bit-patterned recording (BPR), and "current perpendicular to plane" giant magnetoresistance (CPP/GMR) heads.
The rate of areal density growth has dropped below the historical Moore's law rate of 40% per year, and the deceleration is expected to persist through at least 2020. Depending upon assumptions on feasibility and timing of these technologies, the median forecast by industry observers and analysts for 2020 and beyond for areal density growth is 20% per year with a range of 10–30%. The achievable limit for the HAMR technology in combination with BPR and SMR may be 10 Tbit/in2, which would be 20 times higher than the 500 Gbit/in2 represented by 2013 production desktop HDDs. As of 2015, HAMR HDDs have been delayed several years, and are expected in 2018. They require a different architecture, with redesigned media and read/write heads, new lasers, and new near-field optical transducers.
Capacity.
The capacity of a hard disk drive, as reported by an operating system to the end user, is smaller than the amount stated by the manufacturer, which has several reasons: the operating system using some space, use for data redundancy, and use for file system structures. The difference in capacity reported in true SI-based units vs. binary prefixes can lead to a false impression of missing capacity. 
Calculation.
Modern hard disk drives appear to their host controller as a contiguous set of logical blocks. The gross drive capacity is calculated by multiplying the number of blocks by the block size. This information is available from the manufacturer's product specification, and from the drive itself through use of operating system functions that invoke low-level drive commands.
The gross capacity of older HDDs is calculated as the product of the number of cylinders per recording zone, the number of bytes per sector (most commonly 512), and the count of zones of the drive. Some modern SATA drives also report cylinder-head-sector (CHS) capacities, but these are not physical parameters because the reported values are constrained by historic operating system interfaces. The C/H/S scheme has been replaced by logical block addressing (LBA), which is a simple linear addressing scheme that locates blocks by an integer index, starting at LBA 0 for the first block and incremented thereafter. When using the C/H/S method to describe modern large drives, the number of heads is often set to 64, although , a typical hard disk has only one to four platters.
In modern HDDs, spare capacity for defect management is not included in the published capacity; however, in many early HDDs a certain number of sectors were reserved as spares, thereby reducing the capacity available to the operating system.
For RAID subsystems, data integrity and fault-tolerance requirements also reduce the realized capacity. For example, a RAID1 subsystem has about half the total capacity as a result of data mirroring. RAID5 subsystems with x drives, lose 1/x of capacity to parity. RAID subsystems are multiple drives that appear to be one drive or more drives to the user, but provide fault-tolerance. Most RAID vendors use checksums to improve data integrity at the block level. Some vendors design systems using HDDs with sectors of 520 bytes to contain 512 bytes of user data and eight checksum bytes or using separate 512-byte sectors for the checksum data.
Some systems may use hidden partitions for system recovery, reducing the capacity available to the end user.
System use.
The presentation of a hard disk drive to its host is determined by the disk controller. The actual presentation may differ substantially from the drive's native interface, particularly in mainframes or servers. Modern HDDs, such as SAS and SATA drives, appear at their interfaces as a contiguous set of logical blocks that are typically 512 bytes long, though the industry is in the process of changing to the 4,096-byte logical blocks layout, known as the Advanced Format (AF).
The process of initializing these logical blocks on the physical disk platters is called "low-level formatting", which is usually performed at the factory and is not normally changed in the field. As a next step in preparing an HDD for use, "high-level formatting" writes partition and file system structures into selected logical blocks to make the remaining logical blocks available to the host's operating system and its applications. The file system uses some of the disk space to structure the HDD and organize files, recording their file names and the sequence of disk areas that represent the file. Examples of data structures stored on disk to retrieve files include the File Allocation Table (FAT) in the DOS file system and inodes in many UNIX file systems, as well as other operating system data structures (also known as metadata). As a consequence, not all the space on an HDD is available for user files, but this system overhead is usually negligible.
Units.
The total capacity of HDDs is given by manufacturers in SI-based units such as gigabytes (1 GB = 1,000,000,000 bytes) and terabytes (1 TB = 1,000,000,000,000 bytes). The practice of using SI-based prefixes (denoting powers of 1,000) in the hard disk drive and computer industries dates back to the early days of computing; by the 1970s, "million", "mega" and "M" were consistently used in the decimal sense for drive capacity. However, capacities of memory (RAM, ROM) and CDs are traditionally quoted using a binary interpretation of the prefixes, i.e. using powers of 1024 instead of 1000.
Internally, computers do not represent either hard disk drive or memory capacity in powers of 1,024, but reporting it in this manner is a convention. The Microsoft Windows family of operating systems uses the binary convention when reporting storage capacity, so an HDD offered by its manufacturer as a 1 TB drive is reported by these operating systems as a 931 GB HDD. OS X 10.6 ("Snow Leopard") uses decimal convention when reporting HDD capacity. The default behavior of the command-line utility on Linux is to report the HDD capacity as a number of 1024-byte units.
The difference between the decimal and binary prefix interpretation caused some consumer confusion and led to class action suits against HDD manufacturers. The plaintiffs argued that the use of decimal prefixes effectively misled consumers while the defendants denied any wrongdoing or liability, asserting that their marketing and advertising complied in all respects with the law and that no class member sustained any damages or injuries.
Price evolution.
HDD price per byte improved at the rate of −40% per year during 1988–1996, −51% per year during 1996–2003, and −34% per year during 2003–2010. The price improvement decelerated to −13% per year during 2011–2014, as areal density increase slowed and the 2011 Thailand floods damaged manufacturing facilities.
Form factors.
IBM's first hard drive, the IBM 350, used a stack of fifty 24-inch platters and was of a size comparable to two large refrigerators. In 1962, IBM introduced its model 1311 disk, which used six 14-inch (nominal size) platters in a removable pack and was roughly the size of a washing machine. This became a standard platter size and drive form-factor for many years, used also by other manufacturers. The IBM 2314 used platters of the same size in an eleven-high pack and introduced the "drive in a drawer" layout, although the "drawer" was not the complete drive.
Later drives were designed to fit entirely into a chassis that would mount in a 19-inch rack. Digital's RK05 and RL01 were early examples using single 14-inch platters in removable packs, the entire drive fitting in a 10.5-inch-high rack space (six rack units). In the mid-to-late 1980s the similarly sized Fujitsu Eagle, which used (coincidentally) 10.5-inch platters, was a popular product.
Such large platters were never used with microprocessor-based systems. With increasing sales of microcomputers having built in floppy-disk drives (FDDs), HDDs that would fit to the FDD mountings became desirable. Thus HDD "Form factors", initially followed those of 8-inch, 5.25-inch, and 3.5-inch floppy disk drives. Because there were no smaller floppy disk drives, smaller HDD form factors developed from product offerings or industry standards.
, 2.5-inch and 3.5-inch hard disks were the most popular sizes.
By 2009, all manufacturers had discontinued the development of new products for the 1.3-inch, 1-inch and 0.85-inch form factors due to falling prices of flash memory, which has no moving parts.
While these sizes are customarily described by an approximately correct figure in inches, actual sizes have long been specified in millimeters.
Performance characteristics.
Time to access data.
The factors that limit the time to access the data on an HDD are mostly related to the mechanical nature of the rotating disks and moving heads. Seek time is a measure of how long it takes the head assembly to travel to the track of the disk that contains data. Rotational latency is incurred because the desired disk sector may not be directly under the head when data transfer is requested. These two delays are on the order of milliseconds each. The bit rate or data transfer rate (once the head is in the right position) creates delay which is a function of the number of blocks transferred; typically relatively small, but can be quite long with the transfer of large contiguous files. Delay may also occur if the drive disks are stopped to save energy.
An HDD's "Average Access Time" is its average seek time which technically is the time to do all possible seeks divided by the number of all possible seeks, but in practice is determined by statistical methods or simply approximated as the time of a seek over one-third of the number of tracks.
Defragmentation is a procedure used to minimize delay in retrieving data by moving related items to physically proximate areas on the disk. Some computer operating systems perform defragmentation automatically. Although automatic defragmentation is intended to reduce access delays, performance will be temporarily reduced while the procedure is in progress.
Time to access data can be improved by increasing rotational speed (thus reducing latency) or by reducing the time spent seeking. Increasing areal density increases throughput by increasing data rate and by increasing the amount of data under a set of heads, thereby potentially reducing seek activity for a given amount of data. The time to access data has not kept up with throughput increases, which themselves have not kept up with growth in bit density and storage capacity.
Seek time.
Average seek time ranges from under 4 ms for high-end server drives to 15 ms for mobile drives, with the most common mobile drives at about 12 ms and the most common desktop type typically being around 9 ms. The first HDD had an average seek time of about 600 ms; by the middle of 1970s HDDs were available with seek times of about 25 ms. Some early PC drives used a stepper motor to move the heads, and as a result had seek times as slow as 80–120 ms, but this was quickly improved by voice coil type actuation in the 1980s, reducing seek times to around 20 ms. Seek time has continued to improve slowly over time.
Some desktop and laptop computer systems allow the user to make a tradeoff between seek performance and drive noise. Faster seek rates typically require more energy usage to quickly move the heads across the platter, causing louder noises from the pivot bearing and greater device vibrations as the heads are rapidly accelerated during the start of the seek motion and decelerated at the end of the seek motion. Quiet operation reduces movement speed and acceleration rates, but at a cost of reduced seek performance.
Latency.
Latency is the delay for the rotation of the disk to bring the required disk sector under the read-write mechanism. It depends on rotational speed of a disk, measured in revolutions per minute (rpm). Average rotational latency is shown in the table on the right, based on the statistical relation that the average latency in milliseconds for such a drive is one-half the rotational period. Average latency (in miliseconds) is computed as 30,000 divided by rotational speed (in rpm).
Data transfer rate.
, a typical 7,200-rpm desktop HDD has a sustained "disk-to-buffer" data transfer rate up to 1,030 Mbits/sec. This rate depends on the track location; the rate is higher for data on the outer tracks (where there are more data sectors per rotation) and lower toward the inner tracks (where there are fewer data sectors per rotation); and is generally somewhat higher for 10,000-rpm drives. A current widely used standard for the "buffer-to-computer" interface is 3.0 Gbit/s SATA, which can send about 300 megabyte/s (10-bit encoding) from the buffer to the computer, and thus is still comfortably ahead of today's disk-to-buffer transfer rates. Data transfer rate (read/write) can be measured by writing a large file to disk using special file generator tools, then reading back the file. Transfer rate can be influenced by file system fragmentation and the layout of the files.
HDD data transfer rate depends upon the rotational speed of the platters and the data recording density. Because heat and vibration limit rotational speed, advancing density becomes the main method to improve sequential transfer rates. Higher speeds require a more powerful spindle motor, which creates more heat. While areal density advances by increasing both the number of tracks across the disk and the number of sectors per track, only the latter increases the data transfer rate for a given rpm. Since data transfer rate performance only tracks one of the two components of areal density, its performance improves at a lower rate.
Other considerations.
Other performance considerations include quality-adjusted price, power consumption, audible noise, and both operating and non-operating shock resistance.
The Federal Reserve Board has a quality-adjusted price index for large scale enterprise storage systems including three or more enterprise HDDs and associated controllers, racks and cables. Prices for these large scale storage systems improved at the rate of ‒30% per year during 2004–2009 and ‒22% per year during 2009–2014.
Access and interfaces.
HDDs are accessed over one of a number of bus types, including parallel ATA (PATA, also called IDE or EIDE; described before the introduction of SATA as ATA), Serial ATA (SATA), SCSI, Serial Attached SCSI (SAS), and Fibre Channel. Bridge circuitry is sometimes used to connect HDDs to buses with which they cannot communicate natively, such as IEEE 1394, USB and SCSI.
Modern HDDs present a consistent interface to the rest of the computer, no matter what data encoding scheme is used internally. Typically a DSP in the electronics inside the HDD takes the raw analog voltages from the read head and uses PRML and Reed–Solomon error correction to decode the sector boundaries and sector data, then sends that data out the standard interface. That DSP also watches the error rate detected by error detection and correction, and performs bad sector remapping, data collection for Self-Monitoring, Analysis, and Reporting Technology, and other internal tasks.
Modern interfaces connect an HDD to a host bus interface adapter (today typically integrated into the "south bridge") with one data/control cable. Each drive also has an additional power cable, usually direct to the power supply unit.
Integrity and failure.
Due to the extremely close spacing between the heads and the disk surface, HDDs are vulnerable to being damaged by a head crash—a failure of the disk in which the head scrapes across the platter surface, often grinding away the thin magnetic film and causing data loss. Head crashes can be caused by electronic failure, a sudden power failure, physical shock, contamination of the drive's internal enclosure, wear and tear, corrosion, or poorly manufactured platters and heads.
The HDD's spindle system relies on air density inside the disk enclosure to support the heads at their proper flying height while the disk rotates. HDDs require a certain range of air densities in order to operate properly. The connection to the external environment and density occurs through a small hole in the enclosure (about 0.5 mm in breadth), usually with a filter on the inside (the "breather filter"). If the air density is too low, then there is not enough lift for the flying head, so the head gets too close to the disk, and there is a risk of head crashes and data loss. Specially manufactured sealed and pressurized disks are needed for reliable high-altitude operation, above about . Modern disks include temperature sensors and adjust their operation to the operating environment. Breather holes can be seen on all disk drives—they usually have a sticker next to them, warning the user not to cover the holes. The air inside the operating drive is constantly moving too, being swept in motion by friction with the spinning platters. This air passes through an internal recirculation (or "recirc") filter to remove any leftover contaminants from manufacture, any particles or chemicals that may have somehow entered the enclosure, and any particles or outgassing generated internally in normal operation. Very high humidity present for extended periods of time can corrode the heads and platters.
For giant magnetoresistive (GMR) heads in particular, a minor head crash from contamination (that does not remove the magnetic surface of the disk) still results in the head temporarily overheating, due to friction with the disk surface, and can render the data unreadable for a short period until the head temperature stabilizes (so called "thermal asperity", a problem which can partially be dealt with by proper electronic filtering of the read signal).
When the logic board of a hard disk fails, the drive can often be restored to functioning order and the data recovered by replacing the circuit board with one of an identical hard disk. In the case of read-write head faults, they can be replaced using specialized tools in a dust-free environment. If the disk platters are undamaged, they can be transferred into an identical enclosure and the data can be copied or cloned onto a new drive. In the event of disk-platter failures, disassembly and imaging of the disk platters may be required. For logical damage to file systems, a variety of tools, including fsck on UNIX-like systems and CHKDSK on Windows, can be used for data recovery. Recovery from logical damage can require file carving.
A common expectation is that hard disk drives designed and marketed for server use will fail less frequently than consumer-grade drives usually used in desktop computers. However, two independent studies by Carnegie Mellon University and Google found that the "grade" of a drive does not relate to the drive's failure rate.
A 2011 summary of research into SSD and magnetic disk failure patterns by Tom's Hardware summarized research findings as follows:
Manufacturers and sales.
More than 200 companies have manufactured HDDs over time. But consolidations have concentrated production into just three manufacturers today: Western Digital, Seagate, and Toshiba.
Worldwide revenues for disk storage were $32 billion in 2013, down about 3% from 2012. Annualized shipments worldwide were 470 million units during the first three quarters of 2015, down 16% and 30% from the corresponding quarters of 2014 and 2011. Another source reports 552 million units shipped in 2013, compared to 578 million in 2012, and 622 million in 2011. The estimated 2015 market shares are about 40–45% each for Seagate and Western Digital and 13–16% for Toshiba. The two largest manufacturers report that the average sales price is $60 per HDD unit in 2015.
External hard disk drives.
External hard disk drives typically connect via USB; variants using USB 2.0 interface generally have slower data transfer rates when compared to internally mounted hard drives connected through SATA. Plug and play drive functionality offers system compatibility and features large storage options and portable design. , available capacities for external hard disk drives range from 500 GB to 8 TB.
External hard disk drives are usually available as pre-assembled integrated products, but may be also assembled by combining an external enclosure (with USB or other interface) with a separately purchased drive. They are available in 2.5-inch and 3.5-inch sizes; 2.5-inch variants are typically called "portable external drives", while 3.5-inch variants are referred to as "desktop external drives". "Portable" drives are packaged in smaller and lighter enclosures than the "desktop" drives; additionally, "portable" drives use power provided by the USB connection, while "desktop" drives require external power bricks.
Features such as biometric security or multiple interfaces (for example, Firewire) are available at a higher cost. There are pre-assembled external hard disk drives that, when taken out from their enclosures, cannot be used internally in a laptop or desktop computer due to embedded USB interface on their printed circuit boards, and lack of SATA (or Parallel ATA) interfaces.
Visual representation.
Hard disk drives are traditionally symbolized as a stylized stack of platters or as a cylinder, and are as such found in various diagrams; sometimes, they are depicted with small lights to indicate data access. In most modern graphical user environments (GUIs), hard disk drives are represented by an illustration or photograph of the drive enclosure.

</doc>
<doc id="13782" url="https://en.wikipedia.org/wiki?curid=13782" title="Hebrew calendar">
Hebrew calendar

The Hebrew or Jewish calendar (, "ha'luach ha'ivri") is a lunisolar calendar used today predominantly for Jewish religious observances. It determines the dates for Jewish holidays and the appropriate public reading of Torah portions, "yahrzeits" (dates to commemorate the death of a relative), and daily Psalm readings, among many ceremonial uses. In Israel, it is used for religious purposes, provides a time frame for agriculture and is an official calendar for civil purposes, although the latter usage has been steadily declining in favor of the Gregorian calendar.
The present Hebrew calendar is the product of evolution, including a Babylonian influence. Until the Tannaitic period (approximately 10–220 CE) the calendar employed a new crescent moon, with an additional month normally added every two or three years to correct for the difference between twelve lunar months and the solar year. When to add it was based on observation of natural agriculture-related events. Through the Amoraic period (200–500 CE) and into the Geonic period, this system was gradually displaced by the mathematical rules used today. The principles and rules were fully codified by Maimonides in the "Mishneh Torah" in the 12th century. Maimonides' work also replaced counting "years since the destruction of the Temple" with the modern creation-era "Anno Mundi."
The Hebrew lunar year is about eleven days shorter than the solar cycle and uses the 19-year Metonic cycle to bring it into line with the solar cycle, with the addition of an intercalary month every two or three years, for a total of seven times per 19 years. Even with this intercalation, the average Hebrew calendar year is longer by about 6 minutes and 40 seconds than the current mean tropical year, so that every 216 years the Hebrew calendar will fall a day behind the current mean solar year; and about every 231 years it will fall a day behind the Gregorian calendar year.
The era used since the Middle Ages is the "Anno Mundi" epoch (Latin for "in the year of the world"; Hebrew: , "from the creation of the world"). As with "Anno Domini" ("A.D." or "AD"), the words or abbreviation for "Anno Mundi" ("A.M." or "AM") for the era should properly "precede" the date rather than follow it.
AM began at sunset on and will end at sunset on .
Components.
Day and hours.
The Jewish day is of no fixed length. The Jewish day is modeled on the reference to "...there was evening and there was morning..." in the creation account in the first chapter of Genesis. Based on the classic rabbinic interpretation of this text, a day in the rabbinic Hebrew calendar runs from sunset (start of "the evening") to the next sunset. (In most populated parts of the world this is always approximately 24 standard hours, but depending on the season of the year it can be slightly less or slightly more.) The time between sunset and the time when three stars are visible (known as 'tzait ha'kochavim') is known as 'bein hashmashot' and for some uses it is debated as what day it is.
There is no clock in the Jewish scheme, so that a civil clock is used. Though the civil clock, including the one in use in Israel, incorporates local adoptions of various conventions such as time zones, standard times and daylight saving, these have no place in the Jewish scheme. The civil clock is used only as a reference point – in expressions such as: "Shabbat starts at ...". The steady progression of sunset around the world and seasonal changes results in gradual civil time changes from one day to the next based on observable astronomical phenomena (the sunset) and not on man-made laws and conventions.
In Judaism, an hour is defined as 1/12 of the time from sunrise to sunset, so during the winter, an hour can be much less than 60 minutes, and during the summer, it can be much more than 60 minutes. This proportional hour is known as a 'sha'ah z'manit' (lit. a timely hour).
Instead of the international date line convention, there are varying opinions as to where the day changes. One opinion uses the antimeridian of Jerusalem. (Jerusalem is 35°13’ east of the prime meridian, so the antimeridian is at 144°47' W, passing through eastern Alaska.) Other opinions exist as well.
Every hour is divided into 1080 "halakim" (singular: "helek") or parts. A part is 3⅓ seconds or 1/18 minute. The ultimate ancestor of the helek was a small Babylonian time period called a "barleycorn", itself equal to 1/72 of a Babylonian "time degree" (1° of celestial rotation).
The weekdays start with Sunday (day 1, or "Yom Rishon") and proceed to Saturday (day 7), Shabbat. Since some calculations use division, a remainder of 0 signifies Saturday.
While calculations of days, months and years are based on fixed hours equal to 1/24 of a day, the beginning of each "halachic" day is based on the local time of sunset. The end of the Shabbat and other Jewish holidays is based on nightfall ("Tzeth haKochabim") which occurs some amount of time, typically 42 to 72 minutes, after sunset. According to Maimonides, nightfall occurs when three medium-sized stars become visible after sunset. By the 17th century this had become three second-magnitude stars. The modern definition is when the center of the sun is 7° below the geometric (airless) horizon, somewhat later than civil twilight at 6°. The beginning of the daytime portion of each day is determined both by dawn and sunrise. Most "halachic" times are based on some combination of these four times and vary from day to day throughout the year and also vary significantly depending on location. The daytime hours are often divided into "Sha`oth Zemaniyoth" or "Halachic hours" by taking the time between sunrise and sunset or between dawn and nightfall and dividing it into 12 equal hours. The nighttime hours are similarly divided into 12 equal portions, albeit a different amount of time than the "hours" of the daytime. The earliest and latest times for Jewish services, the latest time to eat chametz on the day before Passover and many other rules are based on "Sha`oth Zemaniyoth". For convenience, the modern day using "Sha`oth Zemaniyoth" is often discussed as if sunset were at 6:00pm, sunrise at 6:00am and each hour were equal to a fixed hour. For example, "halachic" noon may be after 1:00pm in some areas during daylight saving time. Within the Mishnah, however, the numbering of the hours starts with the "first" hour after the start of the day.
Weeks.
Shavua [שבוע] is a weekly cycle of seven days, mirroring the seven-day period of the Book of Genesis in which the world is created. The names for the days of the week, like those in the creation account, are simply the day number within the week, with Shabbat being the seventh day. Each day of the week runs from sunset to the following sunset and is figured locally.
Names of weekdays.
The Hebrew calendar follows a seven-day weekly cycle, which runs concurrently with but independently of the monthly and annual cycles. The names for the days of the week are simply the day number within the week. In Hebrew, these names may be abbreviated using the numerical value of the Hebrew letters, for example ("Day 1", or Yom Rishon ()):
The names of the days of the week are modeled on the seven days mentioned in the creation story. For example, "... And there was evening and there was morning, one day". "One day" () in Genesis 1:15 is translated in JPS as "first day", and in some other contexts (including KJV) as "day one". In subsequent verses the Hebrew refers to the days using ordinal numbers, e.g., 'second day', 'third day', and so forth, but with the sixth and seventh days the Hebrew includes the definite article ("the").
The Jewish Shabbat has a special role in the Jewish weekly cycle. There are many special rules which relate to the Shabbat, discussed more fully in the Talmudic tractate Shabbat.
In (Talmudic) Hebrew, the word "Shabbat" () can also mean "week", so that in ritual liturgy a phrase like "Yom Reviʻi bəShabbat" means "the fourth day in the week".
Days of week of holidays.
The period from 1 Adar (or Adar II, in leap years) to 29 Marcheshvan contains all of the festivals specified in the Bible – Purim (14 Adar), Pesach (15 Nisan), Shavuot (6 Sivan), Rosh Hashanah (1 Tishrei), Yom Kippur (10 Tishrei), Sukkot (15 Tishrei), and Shemini Atzeret (22 Tishrei). This period is fixed, during which no adjustments are made.
There are additional rules in the Hebrew calendar to prevent certain holidays from falling on certain days of the week. (See Rosh Hashanah postponement rules, below.) These rules are implemented by adding an extra day to Marcheshvan (making it 30 days long) or by removing one day from Kislev (making it 29 days long). Accordingly, a common Hebrew calendar year can have a length of 353, 354 or 355 days, while a leap Hebrew calendar year can have a length of 383, 384 or 385 days.
Months.
The Hebrew calendar is a lunisolar calendar, meaning that months are based on lunar months, but years are based on solar years. The calendar year features twelve lunar months of twenty-nine or thirty days, with an intercalary lunar month added periodically to synchronize the twelve lunar cycles with the longer solar year. (These extra months are added seven times every nineteen years. See Leap months, below.) The beginning of each Jewish lunar month is based on the appearance of the new moon. Although originally the new lunar crescent had to be observed and certified by witnesses, the moment of the true new moon is now approximated arithmetically as the molad, which is the mean new moon to a precision of one part.
The mean period of the lunar month (precisely, the synodic month) is very close to 29.5 days. Accordingly, the basic Hebrew calendar year is one of twelve lunar months alternating between 29 and 30 days:
In leap years (such as 5774) an additional month, Adar I (30 days) is added after Shevat, while the regular Adar is referred to as "Adar II."
The insertion of the leap month mentioned above is based on the requirement that Passover—the festival celebrating the Exodus from Egypt, which took place in the spring—always occurs in the hemisphere's spring season. Since the adoption of a fixed calendar, intercalations in the Hebrew calendar have been assigned to fixed points in a 19-year cycle. Prior to this, the intercalation was determined empirically:
The year may be intercalated on three grounds: 'aviv ripeness of barley, fruits of trees, and the equinox. On two of these grounds it should be intercalated, but not on one of them alone.
Importance of lunar months.
From very early times, the Mesopotamian lunisolar calendar was in wide use by the countries of the western Asia region. The structure, which was also used by the Israelites, was based on lunar months with the intercalation of an additional month to bring the cycle closer to the solar cycle, although there is no evidence of a thirteenth month mentioned anywhere in the Hebrew Bible.
According to the "Mishnah" and Tosefta, in the Maccabean, Herodian, and Mishnaic periods, new months were determined by the sighting of a new crescent, with two eyewitnesses required to testify to the Sanhedrin to having seen the new lunar crescent at sunset. The practice in the time of Gamaliel II (c. 100 CE) was for witnesses to select the appearance of the moon from a collection of drawings that depicted the crescent in a variety of orientations, only a few of which could be valid in any given month. These observations were compared against calculations.
At first the beginning of each Jewish month was signaled to the communities of Israel and beyond by fires lit on mountaintops, but after the Samaritans began to light false fires, messengers were sent. The inability of the messengers to reach communities outside Israel before mid-month High Holy Days (Succot and Passover) led outlying communities to celebrate scriptural festivals for two days rather than one, observing the second feast-day of the Jewish diaspora because of uncertainty of whether the previous month ended after 29 or 30 days.
In his work "Mishneh Torah" (1178), Maimonides included a chapter "Sanctification of the New Moon", in which he discusses the calendrical rules and their scriptural basis. He notes, "By how much does the solar year exceed the lunar year? By approximately 11 days. Therefore, whenever this excess accumulates to about 30 days, or a little more or less, one month is added and the particular year is made to consist of 13 months, and this is the so-called embolismic (intercalated) year. For the year could not consist of twelve months plus so-and-so many days, since it is said: throughout the months of the year (), which implies that we should count the year by months and not by days."
Names of months.
Both the Syrian calendar, currently used in the Arabic-speaking countries of the Fertile crescent, and the modern Assyrian calendar share many of the names for months with the Hebrew calendar, such as Nisan, Iyyar, Tammuz, Ab, Elul, Tishri and Adar, indicating a common origin. The origin is thought to be the Babylonian calendar. The modern Turkish calendar includes the names Şubat (February), Nisan (April), Temmuz (July) and Eylul (September). The former name for October was Tesrin.
Biblical references to the pre-Jewish calendar include ten months identified by number rather than by name. In parts of the Torah portion "Noach" ("Noah") (specifically, , , ) it is implied that the months are thirty days long. There is also an indication that there were twelve months in the annual cycle (, ). Prior to the Babylonian exile, the names of only four months are referred to in the Tanakh:
All of these are believed to be Canaanite names. These names are only mentioned in connection with the building of the First Temple. Håkan Ulfgard suggests that the use of what are rarely used Canaanite (or in the case of Ethanim perhaps Northwest-semitic) names indicates that "the author is consciously utilizing an archaizing terminology, thus giving the impression of an ancient story...".
In a regular ("kesidran") year, Marcheshvan has 29 days and Kislev has 30 days. However, because of the Rosh Hashanah postponement rules (see below) Kislev may lose a day to have 29 days, and the year is called a short ("chaser") year, or Marcheshvan may acquire an additional day to have 30 days, and the year is called a full ("maleh") year. The calendar rules have been designed to ensure that Rosh Hashanah does not fall on a Sunday, Wednesday or Friday. This is to ensure that Yom Kippur does not directly precede or follow Shabbat, which would create practical difficulties, and that Hoshana Rabbah is not on a Shabbat, in which case certain ceremonies would be lost for a year. Hebrew names and romanized transliteration may somewhat differ, as they do for Marcheshvan/Cheshvan () or Kislev (): the Hebrew words shown here are those commonly indicated, for example, in newspapers.
Leap months.
The solar year is about eleven days longer than twelve lunar months. The Bible does not directly mention the addition of "embolismic" or intercalary months. However, without the insertion of embolismic months, Jewish festivals would gradually shift outside of the seasons required by the Torah. This has been ruled as implying a requirement for the insertion of embolismic months to reconcile the lunar cycles to the seasons, which are integral to solar yearly cycles.
When the observational form of the calendar was in use, whether or not an embolismic month was announced after the "last month" (Adar) depended on 'aviv the ripeness of barley, fruits of trees, and the equinox. On two of these grounds it should be intercalated, but not on one of them alone. It may be noted that in the Bible the name of the first month, "Aviv", literally means "spring". Thus, if Adar was over and spring had not yet arrived, an additional month was observed.
Traditionally, for the Babylonian and Hebrew lunisolar calendars, the years 3, 6, 8, 11, 14, 17, and 19 are the long (13-month) years of the Metonic cycle. This cycle forms the basis of the Christian ecclesiastical calendar and the Hebrew calendar and is used for the computation of the date of Easter each year
During leap years Adar I (or Adar Aleph — "first Adar") is added before the regular Adar. Adar I is actually considered to be the extra month, and has 30 days. Adar II (or Adar Bet — "second Adar") is the "real" Adar, and has the usual 29 days. For this reason, holidays such as Purim are observed in Adar II, not Adar I.
Constellations.
Chronology was a chief consideration in the study of astronomy among the Jews; sacred time was based upon the cycles of the Sun and the Moon. The Talmud identified the twelve constellations of the zodiac with the twelve months of the Hebrew calendar. The correspondence of the constellations with their names in Hebrew and the months is as follows:
Some scholars identified the 12 signs of the zodiac with the 12 sons of Jacob/twelve tribes of Israel.
It should be noted that the 12 lunar months of the Hebrew calendar are the normal months from new moon to new: the year normally contains twelve months averaging 29.52 days each. The discrepancy compared to the mean synodic month of 29.53 days is due to Adar I in a leap year always having thirty days. This means that the calendar year normally contains 354 days.
Years.
The Hebrew calendar year conventionally begins on Rosh Hashanah. However, other dates serve as the beginning of the year for different religious purposes.
There are three qualities that distinguish one year from another: whether it is a leap year or a common year, on which of four permissible days of the week the year begins, and whether it is a deficient, regular, or complete year. Mathematically, there are 24 (2×4×3) possible combinations, but only 14 of them are valid. Each of these patterns is called a "keviyah" (Hebrew קביעה for "a setting" or "an established thing"), and is encoded as a series of two or three Hebrew letters. See Four gates.
In Hebrew there are two common ways of writing the year number: with the thousands, called ("major era"), and without the thousands, called ("minor era").
Anno Mundi.
In 1178 CE, Maimonides wrote in the "Mishneh Torah", "Sanctification of the Moon" (11.16), that he had chosen the epoch from which calculations of all dates should be as "the third day of Nisan in this present year ... which is the year 4938 of the creation of the world" (March 22, 1178 CE). He included all the rules for the calculated calendar and their scriptural basis, including the modern epochal year in his work, and beginning formal usage of the "anno mundi" era. From the eleventh century, "anno mundi" dating became dominant throughout most of the world's Jewish communities. Today, the rules detailed in Maimonides' calendrical code are those generally used by Jewish communities throughout the world.
Since the codification by Maimonides in 1178 CE, the Jewish calendar has used the Anno Mundi epoch (Latin for “in the year of the world,” abbreviated "AM" or "A.M.;" Hebrew ), sometimes referred to as the “Hebrew era”, to distinguish it from other systems based on some computation of creation, such as the Byzantine calendar.
There is also reference in the Talmud to years since the creation based on the calculation in the "Seder Olam Rabbah" of Rabbi Jose ben Halafta in about 160 CE. By his calculation, based on the Masoretic Text, Adam was created in 3760 BCE, later confirmed by the Muslim chronologist al-Biruni as 3448 years before the Seleucid era. An example is the c. 8th century Baraita of Samuel.
According to rabbinic reckoning, the beginning of "year 1" is "not" Creation, but about one year before Creation, with the new moon of its first month (Tishrei) to be called "molad tohu" (the mean new moon of chaos or nothing). The Jewish calendar's epoch (reference date), 1 Tishrei AM 1, is equivalent to Monday, 7 October 3761 BC/BCE in the proleptic Julian calendar, the equivalent tabular date (same daylight period) and is about one year "before" the traditional Jewish date of Creation on 25 Elul AM 1, based upon the "Seder Olam Rabbah". Thus, adding 3760 before Rosh Hashanah or 3761 after to a Julian year number starting from 1 CE (AD 1) will yield the Hebrew year. For earlier years there may be a discrepancy (see: Missing years (Jewish calendar)).
The "Seder Olam Rabbah" also recognized the importance of the Jubilee and Sabbatical cycles as a long-term calendrical system, and attempted at various places to fit the Sabbatical and Jubilee years into its chronological scheme.
Previous systems.
Before the adoption of the current AM year numbering system, other systems were in use. In early times, the years were counted from some significant historic event. (e.g., ) During the period of the monarchy, it was the widespread practice in western Asia to use era year numbers according to the accession year of the monarch of the country involved. This practice was also followed by the united kingdom of Israel (e.g., ), kingdom of Judah (e.g., ), kingdom of Israel (e.g., ), Persia (e.g., ) and others. Besides, the author of Kings coordinated dates in the two kingdoms by giving the accession year of a monarch in terms of the year of the monarch of the other kingdom, (e.g., ) though some commentators note that these dates do not always synchronise. Other era dating systems have been used at other times. For example, Jewish communities in the Babylonian diaspora counted the years from the first deportation from Israel, that of Jehoiachin in 597 BCE, (e.g., ). The era year was then called "year of the captivity of Jehoiachin". (e.g., )
During the Hellenistic Maccabean period, Seleucid era counting was used, at least in the Greek-influenced area of Israel. The Books of the Maccabees used Seleucid era dating exclusively (e.g., , , , , ). Josephus writing in the Roman period also used Seleucid era dating exclusively. During the Talmudic era, from the 1st to the 10th century, the center of world Judaism was in the Middle East, primarily in the Talmudic Academies of Iraq and Palestine. Jews in these regions used Seleucid era dating (also known as the "Era of Contracts"). The Avodah Zarah states:
Rav Aha b. Jacob then put this question: How do we know that our Era Documents is connected with the Kingdom of Greece at all? Why not say that it is reckoned from the Exodus from Egypt, omitting the first thousand years and giving the years of the next thousand? In that case, the document is really post-dated!<br> Said Rav Nahman: In the Diaspora the Greek Era alone is used. He questioner thought that Rav Nahman wanted to dispose of him anyhow, but when he went and studied it thoroughly he found that it is indeed taught a Baraita: In the Diaspora the Greek Era alone is used.
The use of the era of documents (i.e., Seleucid era) continued till the 16th century in the East, and was employed even in the 19th century among the Jews of Yemen.
Occasionally in Talmudic writings, reference was made to other starting points for eras, such as destruction era dating, being the number of years since the 70 CE destruction of the Second Temple. In the 8th and 9th centuries, as the center of Jewish life moved from Babylonia to Europe, counting using the Seleucid era "became meaningless". There is indication that Jews of the Rhineland in the early Middle Ages used the "years after the destruction of the Temple" (e.g., ).
New year.
Nisan 1 is referred to as the "ecclesiastical new year".
In ancient Israel, the start of the ecclesiastical new year for the counting of months and festivals (i.e., Nisan) was determined by reference to Passover. Passover is on 15 Nisan, () which corresponds to the full moon of Nisan. As Passover is a spring festival, it should fall on a full moon day around, and normally just after, the vernal (northward) equinox. If the twelfth full moon after the previous Passover is too early compared to the equinox, a leap month is inserted near the end of the previous year before the new year is set to begin. According to normative Judaism, the verses in require that the months be determined by a proper court with the necessary authority to sanctify the months. Hence the court, not the astronomy, has the final decision.
According to some Christian and Karaite sources, the tradition in ancient Israel was that 1 Nisan would not start until the barley is ripe, being the test for the onset of spring. If the barley was not ripe an intercalary month would be added before Nisan.
The day most commonly referred to as the "New Year" is 1 Tishrei, which actually begins in the seventh month of the ecclesiastical year. On that day the formal New Year for the counting of years (such as Shmita and Yovel), Rosh Hashanah ("head of the year") is observed. (see , which uses the phrase "beginning of the year".) This is the civil new year, and the date on which the year number advances. Certain agricultural practices are also marked from this date.
In the 1st century, Josephus stated that while –
Moses...appointed Nisan...as the first month for the festivals...the commencement of the year for everything relating to divine worship, but for selling and buying and other ordinary affairs he preserved the ancient order e. the year beginning with Tishrei."
Edwin Thiele has concluded that the ancient northern Kingdom of Israel counted years using the ecclesiastical new year starting on 1 Aviv (Nisan), while the southern Kingdom of Judah counted years using the civil new year starting on 1 Tishrei. The practice of the Kingdom of Israel was also that of Babylon, as well as other countries of the region. The practice of Judah is still followed.
In fact the Jewish calendar has a multiplicity of new years for different purposes. The use of these dates has been in use for a long time. The use of multiple starting dates for a year is comparable to different starting dates for civil "calendar years", "tax or fiscal years", "academic years", "religious cycles", etc. By the time of the redaction of the "Mishnah", (c. 200 CE), jurists had identified four new-year dates:
The 1st of Nisan is the new year for kings and feasts; the 1st of Elul is the new year for the tithe of cattle... the 1st of Tishri is the new year for years, of the years of release and jubilee years, for the planting and for vegetables; and the 1st of Shevat is the new year for trees-so the school of Shammai; and the school of Hillel say: On the 15th thereof.
The month of Elul is the new year for counting animal tithes ("ma'aser behemah"). "Tu Bishvat" ("the 15th of Shevat") marks the new year for trees (and agricultural tithes).
For the dates of the Jewish New Year see Jewish and Israeli holidays 2000–2050 or calculate using the section "Conversion between Jewish and civil calendars".
Leap years.
The Jewish calendar is based on the Metonic cycle of 19 years, of which 12 are common (non-leap) years of 12 months and 7 are leap years of 13 months. To determine whether a Jewish year is a leap year, one must find its position in the 19-year Metonic cycle. This position is calculated by dividing the Jewish year number by 19 and finding the remainder. For example, the Jewish year divided by 19 results in a remainder of , indicating that it is year of the Metonic cycle. Since there is no year 0, a remainder of 0 indicates that the year is year 19 of the cycle.
Years 3, 6, 8, 11, 14, 17, and 19 of the Metonic cycle are leap years. To assist in remembering this sequence, some people use the mnemonic Hebrew word GUCHADZaT , where the Hebrew letters "gimel-vav-het aleph-dalet-zayin-tet" are used as Hebrew numerals equivalent to 3, 6, 8, 1, 4, 7, 9. The "keviyah" records whether the year is leap or common: פ for "peshuta" (פשוטה), meaning simple and indicating a common year, and מ indicating a leap year (me'uberet, מעוברת).
Another memory aid notes that intervals of the major scale follow the same pattern as do Jewish leap years, with "do" corresponding to year 19 (or 0): a whole step in the scale corresponds to two common years between consecutive leap years, and a half step to one common year between two leap years. This connection with the major scale is more plain in the context of 19 equal temperament: counting the tonic as 0, the notes of the major scale in 19 equal temperament are numbers 0 (or 19), 3, 6, 8, 11, 14, 17, the same numbers as the leap years in the Hebrew calendar.
A simple rule for determining whether a year is a leap year has been given above. However, there is another rule which not only tells whether the year is leap but also gives the fraction of a month by which the calendar is behind the seasons, useful for agricultural purposes. To determine whether year "n" of the calendar is a leap year, find the remainder on dividing [(7 × "n") + 1] by 19. If the remainder is 6 or less it is a leap year; if it is 7 or more it is not. For example, the The This works because as there are seven leap years in nineteen years the difference between the solar and lunar years increases by 7/19 month per year. When the difference goes above 18/19 month this signifies a leap year, and the difference is reduced by one month.
Rosh Hashanah postponement rules.
To calculate the day on which Rosh Hashanah of a given year will fall, it is necessary first to calculate the expected molad (moment of lunar conjunction or new moon) of Tishrei in that year, and then to apply a set of rules to determine whether the first day of the year must be postponed. The molad can be calculated by multiplying the number of months that will have elapsed since some (preceding) molad whose weekday is known by the mean length of a (synodic) lunar month, which is 29 days, 12 hours, and 793 parts (there are 1080 "parts" in an hour, so that one part is equal to 3 seconds). The very first molad, the molad tohu, fell on Sunday evening at 11.11, or in Jewish terms Day 2, 5 hours, and 204 parts.
In calculating the number of months that will have passed since the known molad that one uses as the starting point, one must remember to include any leap month(s) that falls within the elapsed interval, according to the cycle of leap years. A 19-year cycle of 235 synodic months has 991 weeks 2 days 16 hours 595 parts, a common year of 12 synodic months has 50 weeks 4 days 8 hours 876 parts, while a leap year of 13 synodic months has 54 weeks 5 days 21 hours 589 parts.
The two months whose numbers of days may be adjusted, Marcheshvan and Kislev, are the eighth and ninth months of the Hebrew year, whereas Tishrei is the seventh month (in the traditional counting of the months, even though it is the first month of a new calendar year). Any adjustments needed to postpone Rosh Hashanah must be made to the adjustable months in the year that precedes the year of which the Rosh Hashanah will be the first day.
Just four potential conditions are considered to determine whether the date of Rosh Hashanah must be postponed. These are called the Rosh Hashanah postponement rules, or "deḥiyyot":
The first of these rules (deḥiyyah "molad zaken") is referred to in the Talmud. Nowadays, molad zaken is used as a device to prevent the molad falling on the second day of the month. The second rule, (deḥiyyah "lo ADU"), is applied for religious reasons.
Another two rules are applied much less frequently and serve to prevent impermissible year lengths. Their names are Hebrew acronyms that refer to the ways they are calculated:
At the innovation of the sages, the calendar was arranged to ensure that Yom Kippur would not fall on a Friday or Sunday, and Hoshana Rabbah would not fall on Shabbat. These rules have been instituted because Shabbat restrictions also apply to Yom Kippur, so that if Yom Kippur were to fall on Friday, it would not be possible to make necessary preparations for Shabbat (such as candle lighting). Similarly, if Yom Kippur fell on a Sunday, it would not be possible to make preparations for Yom Kippur because the preceding day is Shabbat. Additionally, the laws of Shabbat override those of Hoshana Rabbah, so that if Hoshana Rabbah were to fall on Shabbat certain rituals that are a part of the Hoshana Rabbah service (such as carrying willows, which is a form of work) could not be performed.
To prevent Yom Kippur (10 Tishrei) from falling on a Friday or Sunday, Rosh Hashanah (1 Tishrei) cannot fall on Wednesday or Friday. Likewise, to prevent Hoshana Rabbah (21 Tishrei) from falling on a Saturday, Rosh Hashanah cannot fall on a Sunday. This leaves only four days on which Rosh Hashanah can fall: Monday, Tuesday, Thursday, and Saturday, which are referred to as the "four gates". Each day is associated with a number (its order in the week, beginning with Sunday as day 1). Numbers in Hebrew have been traditionally denominated by Hebrew letters. Thus the "keviyah" uses the letters ה ,ג ,ב and ז (representing 2, 3, 5, and 7, for Monday, Tuesday, Thursday, and Saturday) to denote the starting day of the year.
Deficient, regular, and complete years.
The postponement of the year is compensated for by adding a day to the second month or removing one from the third month. A Jewish common year can only have 353, 354, or 355 days. A leap year is always 30 days longer, and so can have 383, 384, or 385 days.
Whether a year is deficient, regular, or complete is determined by the time between two adjacent Rosh Hashanah observances and the leap year. While the "keviyah" is sufficient to describe a year, a variant specifies the day of the week for the first day of Pesach (Passover) in lieu of the year length.
A Metonic cycle equates to 235 lunar months in each 19-year cycle. This gives an average of 6939 days, 16 hours, and 595 parts for each cycle. But due to the Rosh Hashanah postponement rules (preceding section) a cycle of 19 Jewish years can be either 6939, 6940, 6941, or 6942 days in duration. Since none of these values is evenly divisible by seven, the Jewish calendar repeats exactly only following 36,288 Metonic cycles, or 689,472 Jewish years. There is a near-repetition every 247 years, except for an excess of about 50 minutes (905 parts).
Four gates.
The annual calendar of a numbered Hebrew year, displayed as 12 or 13 months partitioned into weeks, can be determined by consulting the table of Four gates, whose inputs are the year's position in the 19-year cycle and its molad Tishrei. The resulting "keviyah" of the desired year in the body of the table is a triple consisting of two numbers and a letter (written left-to-right in English). The left number of each triple is the day of the week of , Rosh Hashanah ; the letter indicates whether that year is deficient (D), regular (R), or complete (C), the number of days in Chesvan and Kislev; while the right number of each triple is the day of the week of , the first day of Passover or Pesach , within the same Hebrew year (next Julian/Gregorian year). The "keviyah" in Hebrew letters are written right-to-left, so their days of the week are reversed, the right number for and the left for . The year within the 19-year cycle alone determines whether that year has one or two Adars.
This table numbers the days of the week and hours for the limits of molad Tishrei in the Hebrew manner for calendrical calculations, that is, both begin at , thus is noon Saturday. The years of a 19-year cycle are organized into four groups: common years after a leap year but before a common year ; common years between two leap years ; common years after a common year but before a leap year ; and leap years , all between common years. The oldest surviving table of Four gates was written by Saadia Gaon (892–942). It is so named because it identifies the four allowable days of the week on which can occur.
Comparing the days of the week of molad Tishrei with those in the "keviyah" shows that during 39% of years is not postponed beyond the day of the week of its molad Tishrei, 47% are postponed one day, and 14% are postponed two days. This table also identifies the seven types of common years and seven types of leap years. Most are represented in any 19-year cycle, except one or two may be in neighboring cycles. The most likely type of year is 5R7 in 18.1% of years, whereas the least likely is 5C1 in 3.3% of years. The day of the week of is later than that of by one, two or three days for common years and three, four or five days for leap years in deficient, regular or complete years, respectively.
Holidays.
See Jewish and Israeli holidays 2000–2050
History.
Mishnaic period.
The Tanakh contains several commandments related to the keeping of the calendar and the lunar cycle, and records changes that have taken place to the Hebrew calendar.
It has been noted that the procedures described in the Mishnah and Tosefta are all plausible procedures for regulating an empirical lunar calendar. Fire-signals, for example, or smoke-signals, are known from the pre-exilic Lachish ostraca. Furthermore, the Mishnah contains laws that reflect the uncertainties of an empirical calendar. Mishnah Sanhedrin, for example, holds that when one witness holds that an event took place on a certain day of the month, and another that the same event took place on the following day, their testimony can be held to agree, since the length of the preceding month was uncertain. Another Mishnah takes it for granted that it cannot be known in advance whether a year's lease is for twelve or thirteen months. Hence it is a reasonable conclusion that the Mishnaic calendar was actually used in the Mishnaic period.
The accuracy of the Mishnah's claim that the Mishnaic calendar was also used in the late Second Temple period is less certain. One scholar has noted that there are no laws from Second Temple period sources that indicate any doubts about the length of a month or of a year. This led him to propose that the priests must have had some form of computed calendar or calendrical rules that allowed them to know in advance whether a month would have 30 or 29 days, and whether a year would have 12 or 13 months.
Modern calendar.
Between 70 and 1178 CE, the observation-based calendar was gradually replaced by a mathematically calculated one. Except for the epoch year number, the calendar rules reached their current form by the beginning of the 9th century, as described by the Persian Muslim astronomer al-Khwarizmi (c. 780–850 CE) in 823.
One notable difference between the calendar of that era and the modern form was the date of the epoch (the fixed reference point at the beginning of year 1), which at that time was one year later than the epoch of the modern calendar.
Most of the present rules of the calendar were in place by 823, according to a treatise by al-Khwarizmi. Al-Khwarizmi's study of the Jewish calendar, "Risāla fi istikhrāj taʾrīkh al-yahūd" "Extraction of the Jewish Era" describes the 19-year intercalation cycle, the rules for determining on what day of the week the first day of the month Tishrī shall fall, the interval between the Jewish era (creation of Adam) and the Seleucid era, and the rules for determining the mean longitude of the sun and the moon using the Jewish calendar. Not all the rules were in place by 835.
In 921, Aaron ben Meïr proposed changes to the calendar. Though the proposals were rejected, they indicate that all of the rules of the modern calendar (except for the epoch) were in place before that date. In 1000, the Muslim chronologist al-Biruni described all of the modern rules of the Hebrew calendar, except that he specified three different epochs used by various Jewish communities being one, two, or three years later than the modern epoch.
There is a tradition, first mentioned by Hai Gaon (died 1038 CE), that Hillel b. R. Yehuda "in the year 670 of the Seleucid era" (i.e., 358–359 CE) was responsible for the new calculated calendar with a fixed intercalation cycle. Later writers, such as Nachmanides, explained Hai Gaon's words to mean that the entire computed calendar was due to Hillel b. Yehuda in response to persecution of Jews. Maimonides, in the 12th century, stated that the Mishnaic calendar was used "until the days of Abaye and Rava", who flourished c. 320–350 CE, and that the change came when "the land of Israel was destroyed, and no permanent court was left." Taken together, these two traditions suggest that Hillel b. Yehuda (whom they identify with the mid-4th-century Jewish patriarch Ioulos, attested in a letter of the Emperor Julian, and the Jewish patriarch Ellel, mentioned by Epiphanius) instituted the computed Hebrew calendar because of persecution. H. Graetz linked the introduction of the computed calendar to a sharp repression following a failed Jewish insurrection that occurred during the rule of the Christian emperor Constantius and Gallus. A later writer, S. Lieberman, argued instead that the introduction of the fixed calendar was due to measures taken by Christian Roman authorities to prevent the Jewish patriarch from sending calendrical messengers.
Both the tradition that Hillel b. Yehuda instituted the complete computed calendar, and the theory that the computed calendar was introduced due to repression or persecution, have been questioned. Furthermore, two Jewish dates during post-Talmudic times (specifically in 506 and 776) are impossible under the rules of the modern calendar, indicating that its arithmetic rules were developed in Babylonia during the times of the Geonim (7th to 8th centuries). The Babylonian rules required the delay of the first day of Tishrei when the new moon occurred after noon.
The Talmuds do, however, indicate at least the beginnings of a transition from a purely empirical to a computed calendar. According to a statement attributed to Yose, an Amora who lived during the second half of the 3rd century, the feast of Purim, 14 Adar, could not fall on a Sabbath nor a Monday, lest 10 Tishrei (Yom Kippur) fall on a Friday or a Sunday. This indicates that, by the time of the redaction of the Jerusalem Talmud (c. 400 CE), there were a fixed number of days in all months from Adar to Elul, also implying that the extra month was already a second Adar added before the regular Adar. In another passage, a sage is reported to have counseled "those who make the computations" not to set the first day of Tishrei or the Day of the Willow on the sabbath. This indicates that there was a group who "made computations" and were in a position to control, to some extent, the day of the week on which Rosh Hashanah would fall.
Usage in contemporary Israel.
Early Zionist pioneers were impressed by the fact that the calendar preserved by Jews over many centuries in far-flung diasporas, as a matter of religious ritual, was geared to the climate of their original country: the Jewish New Year marks the transition from the dry season to the rainy one, and major Jewish holidays such as Sukkot, Passover, and Shavuot correspond to major points of the country's agricultural year such as planting and harvest.
Accordingly, in the early 20th century the Hebrew calendar was re-interpreted as an agricultural rather than religious calendar. The Kibbutz movement was especially inventive in creating new rituals fitting this interpretation.
After the creation of the State of Israel, the Hebrew calendar became one of the official calendars of Israel, along with the Gregorian calendar. Holidays and commemorations not derived from previous Jewish tradition were to be fixed according to the Hebrew calendar date. For example, the Israeli Independence Day falls on 5 Iyar, Jerusalem Reunification Day on 28 Iyar, and the Holocaust Commemoration Day on 27 Nisan.
Nevertheless, since the 1950s usage of the Hebrew calendar has steadily declined, in favor of the Gregorian calendar. At present, Israelis—except for the religiously observant—conduct their private and public life according to the Gregorian calendar, although the Hebrew calendar is still widely acknowledged, appearing in public venues such as banks (where it is legal for use on cheques and other documents, though only rarely do people make use of this option) and on the mastheads of newspapers.
The Jewish New Year (Rosh Hashanah) is a two-day public holiday in Israel. However, since the 1980s an increasing number of secular Israelis celebrate the Gregorian New Year (usually known as "Silvester Night"—"ליל סילבסטר") on the night between 31 December and 1 January. Prominent rabbis have on several occasions sharply denounced this practice, but with no noticeable effect on the secularist celebrants.
Wall calendars commonly used in Israel are hybrids. Most are organised according to Gregorian rather than Jewish months, but begin in September, when the Jewish New Year usually falls, and provide the Jewish date in small characters.
Other practices.
Outside of Rabbinic Judaism, evidence shows a diversity of practice.
Karaite calendar.
Karaites use the lunar month and the solar year, but the Karaite calendar differs from the current Rabbinic calendar in a number of ways. The Karaite calendar is identical to the Rabbinic calendar used before the Sanhedrin changed the Rabbinic calendar from the lunar, observation based calendar, to the current mathematically based calendar used in Rabbinic Judaism today.
In the lunar Karaite calendar, the beginning of each month, the Rosh Chodesh, can be calculated, but is confirmed by the observation in Israel of the first sightings of the new moon. This may result in an occasional variation of a maximum of one day, depending on the inability to observe the new moon. The day is usually "picked up" in the next month.
The addition of the leap month (Adar II) is determined by observing in Israel the ripening of barley at a specific stage (defined by Karaite tradition) (called aviv), rather than using the calculated and fixed calendar of rabbinic Judaism. Occasionally this results in Karaites being one month ahead of other Jews using the calculated rabbinic calendar. The "lost" month would be "picked up" in the next cycle when Karaites would observe a leap month while other Jews would not.
Furthermore, the seasonal drift of the rabbinic calendar is avoided, resulting in the years affected by the drift starting one month earlier in the Karaite calendar.
Also, the four rules of postponement of the rabbinic calendar are not applied, since they are not mentioned in the Tanakh. This can affect the dates observed for all the Jewish holidays in a particular year by one day.
In the Middle Ages many Karaite Jews outside Israel followed the calculated rabbinic calendar, because it was not possible to retrieve accurate aviv barley data from the land of Israel. However, since the establishment of the State of Israel, and especially since the Six Day War, the Karaite Jews that have made "aliyah" can now again use the observational calendar.
The Qumran calendar.
Many of the Dead Sea (Qumran) Scrolls have references to a unique calendar, used by the people there, who are often assumed to be Essenes.
The year of this calendar used the ideal Mesopotamian calendar of twelve 30-day months, to which were added 4 days at the equinoxes and solstices (cardinal points), making a total of 364 days.
There was some ambiguity as to whether the cardinal days were at the beginning of the months or at the end, but the clearest calendar attestations give a year of four seasons, each having three months of 30, 30, and 31 days with the cardinal day the extra day at the end, for a total of 91 days, or exactly 13 weeks. Each season started on the 4th day of the week (Wednesday), every year. (Ben-Dov, "Head of All Years", pp. 16–17)
With only 364 days, it is clear that the calendar would after a few years be very noticeably different from the actual seasons, but there is nothing to indicate what was done about this problem. Various suggestions have been made by scholars. One is that nothing was done and the calendar was allowed to change with respect to the seasons. Another suggestion is that changes were made irregularly, only when the seasonal anomaly was too great to be ignored any longer. (Ben-Dov, "Head of All Years", pp. 19–20)
The writings often discuss the moon, but the calendar was not based on the movement of the moon any more than indications of the phases of the moon on a modern western calendar indicate that that is a lunar calendar.
The calendrical documents 4Q320 and 4Q321 from the Dead Sea Scrolls outlining the 364-day solar calendar, six-year cycle of priestly courses, and 354-day lunar year cycles may be found here. In addition, an abbreviated Jubilee calendar from 4Q319 along with the priestly course serving on 1 Abib (the first day of the year) each year may be found here.
Persian civil calendar.
Calendrical evidence for the postexilic Persian period is found in papyri from the Jewish colony at Elephantine, in Egypt. These documents show that the Jewish community of Elephantine used the Egyptian and Babylonian calendars.
The Sardica paschal table shows that the Jewish community of some eastern city, possibly Antioch, used a calendrical scheme that kept Nisan 14 within the limits of the Julian month of March. Some of the dates in the document are clearly corrupt, but they can be emended to make the sixteen years in the table consistent with a regular intercalation scheme. Peter, the bishop of Alexandria (early 4th century CE), mentions that the Jews of his city "hold their Passover according to the course of the moon in the month of Phamenoth, or according to the intercalary month every third year in the month of Pharmuthi", suggesting a fairly consistent intercalation scheme that kept Nisan 14 approximately between Phamenoth 10 (March 6 in the 4th century CE) and Pharmuthi 10 (April 5). Jewish funerary inscriptions from Zoar, south of the Dead Sea, dated from the 3rd to the 5th century, indicate that when years were intercalated, the intercalary month was at least sometimes a repeated month of Adar. The inscriptions, however, reveal no clear pattern of regular intercalations, nor do they indicate any consistent rule for determining the start of the lunar month.
In 1178, Maimonides included all the rules for the calculated calendar and their scriptural basis, including the modern epochal year in his work, "Mishneh Torah". Today, the rules detailed in Maimonides' code are those generally used by Jewish communities throughout the world.
Astronomical calculations.
Synodic month – the molad interval.
A "new moon" (astronomically called a lunar conjunction and in Hebrew called a molad) is the moment at which the sun and moon are aligned horizontally with respect to a north-south line (technically, they have the same ecliptical longitude). The period between two new moons is a synodic month. The actual length of a synodic month varies from about 29 days 6 hours and 30 minutes (29.27 days) to about 29 days and 20 hours (29.83 days), a variation range of about 13 hours and 30 minutes. Accordingly, for convenience, a long-term average length, identical to the mean synodic month of ancient times (also called the molad interval) is used. The molad interval is formula_1 days, or 29 days, 12 hours, and 793 parts (44 1/18 minutes) (i.e., 29.530594 days), and is the same value determined by the Babylonians in their System B about 300 BCE and was adopted by the Greek astronomer Hipparchus in the 2nd century BCE and by the Alexandrian astronomer Ptolemy in the "Almagest" four centuries later (who cited Hipparchus as his source). Its remarkable accuracy (less than one second from the true value) is thought to have been achieved using records of lunar eclipses from the 8th to 5th centuries BCE.
This value is as close to the correct value of 29.530589 days as it is possible for a value to come that is rounded off to whole parts (1/18 minute). The discrepancy makes the molad interval about 0.6 seconds too long. Put another way, if the molad is taken as the time of mean conjunction at some reference meridian, then this reference meridian is drifting slowly eastward. If this drift of the reference meridian is traced back to the mid-4th century, the traditional date of the introduction of the fixed calendar, then it is found to correspond to a longitude midway between the Nile and the end of the Euphrates. The modern molad moments match the mean solar times of the lunar conjunction moments near the meridian of Kandahar, Afghanistan, more than 30° east of Jerusalem.
Furthermore, the discrepancy between the molad interval and the mean synodic month is accumulating at an accelerating rate, since the mean synodic month is progressively shortening due to gravitational tidal effects. Measured on a strictly uniform time scale, such as that provided by an atomic clock, the mean synodic month is becoming gradually longer, but since the tides slow Earth's rotation rate even more, the mean synodic month is becoming gradually shorter in terms of mean solar time.
Seasonal drift.
The mean year of the current mathematically based Hebrew calendar is 365 days 5 hours 55 minutes and 25+25/57 seconds (365.2468 days) – computed as the molad/monthly interval of 29.530594 days × 235 months in a 19-year metonic cycle ÷ 19 years per cycle. In relation to the Gregorian calendar, the mean Gregorian calendar year is 365 days 5 hours 49 minutes and 12 seconds (365.2425 days), and the drift of the Hebrew calendar in relation to it is about a day every 231 years.
Implications for Jewish ritual.
Although the molad of Tishrei is the only molad moment that is not ritually announced, it is actually the only one that is relevant to the Hebrew calendar, for it determines the provisional date of Rosh Hashanah, subject to the Rosh Hashanah postponement rules. The other monthly molad moments are announced for mystical reasons. With the moladot on average almost 100 minutes late, this means that the molad of Tishrei lands one day later than it ought to in (100 minutes) ÷ (1440 minutes per day) = 5 of 72 years or nearly 7% of years.
Therefore, the seemingly small drift of the moladot is already significant enough to affect the date of Rosh Hashanah, which then cascades to many other dates in the calendar year and sometimes, due to the Rosh Hashanah postponement rules, also interacts with the dates of the prior or next year. The molad drift could be corrected by using a progressively shorter molad interval that corresponds to the actual mean lunar conjunction interval at the original molad reference meridian. Furthermore, the molad interval determines the calendar mean year, so using a progressively shorter molad interval would help correct the excessive length of the Hebrew calendar mean year, as well as helping it to "hold onto" the northward equinox for the maximum duration.
When the 19-year intercalary cycle was finalised in the 4th century, the earliest Passover (in year 16 of the cycle) coincided with the northward equinox, which means that Passover fell near the "first" full moon after the northward equinox, or that the northward equinox landed within one lunation before 16 days after the "molad" of "Nisan". This is still the case in about 80% of years, but in about 20% of years Passover is a month late by these criteria (as it was in AM 5765 and 5768, the 8th and 11th years of the 19-year cycle = Gregorian 2005 and 2008 CE). Presently this occurs after the "premature" insertion of a leap month in years 8, 11, and 19 of each 19-year cycle, which causes the northward equinox to land on exceptionally early Hebrew dates in such years. This problem will get worse over time, and so beginning in AM 5817 (2057 CE), year 3 of each 19-year cycle will also be a month late. If the calendar is not amended then Passover will start to land on or after the summer solstice around AM 16652 (12892 CE). (The exact year when this will begin to occur depends on uncertainties in the future tidal slowing of the Earth rotation rate, and on the accuracy of predictions of precession and Earth axial tilt.)
The seriousness of the spring equinox drift is widely discounted on the grounds that Passover will remain in the spring season for many millennia, and the text of the Torah is generally not interpreted as having specified tight calendrical limits. Of course, the Hebrew calendar also drifts with respect to the autumn equinox, and at least part of the harvest festival of Sukkot is already more than a month after the equinox in years 1, 9, and 12 of each 19-year cycle; beginning in AM 5818 (2057 CE), this will also be the case in year 4. (These are the same year numbers as were mentioned for the spring season in the previous paragraph, except that they get incremented at Rosh Hashanah.) This progressively increases the probability that Sukkot will be cold and wet, making it uncomfortable or impractical to dwell in the traditional "succah" during Sukkot. The first winter seasonal prayer for rain is not recited until "Shemini Atzeret", after the end of Sukkot, yet it is becoming increasingly likely that the rainy season in Israel will start before the end of Sukkot.
No equinox or solstice will ever be more than a day or so away from its mean date according to the solar calendar, while nineteen Jewish years average 6939d 16h 33m 03s compared to the 6939d 14h 26m 15s of nineteen mean tropical years. This discrepancy has mounted up to six days, which is why the earliest Passover currently falls on 26 March (as in AM 5773 / 2013 CE).
Worked example.
Given the length of the year, the length of each month is fixed as described above, so the real problem in determining the calendar for a year is determining the number of days in the year. In the modern calendar this is determined in the following manner.
The day of Rosh Hashanah and the length of the year are determined by the time and the day of the week of the Tishrei "molad", that is, the moment of the average conjunction. Given the Tishrei "molad" of a certain year, the length of the year is determined as follows:
First, one must determine whether each year is an ordinary or leap year by its position in the 19-year Metonic cycle. Years 3, 6, 8, 11, 14, 17, and 19 are leap years.
Secondly, one must determine the number of days between the starting Tishrei "molad" (TM1) and the Tishrei "molad" of the next year (TM2). For calendar descriptions in general the day begins at 6 p.m., but for the purpose of determining Rosh Hashanah, a "molad" occurring on or after noon is treated as belonging to the next day (the first "deḥiyyah"). All months are calculated as 29d, 12h, 44m, 3s long (MonLen). Therefore, in an ordinary year TM2 occurs 12 × MonLen days after TM1. This is usually 354 calendar days after TM1, but if TM1 is on or after 3:11:20 a.m. and before noon, it will be 355 days. Similarly, in a leap year, TM2 occurs 13 × MonLen days after TM1. This is usually 384 days after TM1, but if TM1 is on or after noon and before 2:27:16 p.m., TM2 will be only 383 days after TM1. In the same way, from TM2 one calculates TM3. Thus the four natural year lengths are 354, 355, 383, and 384 days.
However, because of the holiday rules, Rosh Hashanah cannot fall on a Sunday, Wednesday, or Friday, so if TM2 is one of those days, Rosh Hashanah in year 2 is postponed by adding one day to year 1 (the second "deḥiyyah"). To compensate, one day is subtracted from year 2. It is to allow these adjustments that the system allows 385-day years (long leap) and 353-day years (short ordinary) besides the four natural year lengths.
But how can year 1 be lengthened if it is already a long ordinary year of 355 days or year 2 be shortened if it is a short leap year of 383 days? That is why the third and fourth "deḥiyyah"s are needed.
If year 1 is already a long ordinary year of 355 days, there will be a problem if TM1 is on a Tuesday, as that means TM2 falls on a Sunday and will have to be postponed, creating a 356-day year. In this case, Rosh Hashanah in year 1 is postponed from Tuesday (the third "deḥiyyah"). As it cannot be postponed to Wednesday, it is postponed to Thursday, and year 1 ends up with 354 days.
On the other hand, if year 2 is already a short year of 383 days there will be a problem if TM2 is on a Wednesday. because Rosh Hashanah in year 2 will have to be postponed from Wednesday to Thursday and this will cause year 2 to be only 382 days long. In this case, year 2 is extended by one day by postponing Rosh Hashanah in year 3 from Monday to Tuesday (the fourth "deḥiyyah" ), and year 2 will have 383 days.
Rectifying the Hebrew calendar.
The attribution of the fixed arithmetic Hebrew calendar solely to Hillel II has, however, been questioned by a few authors, such as Sasha Stern, who claim that the calendar rules developed gradually over several centuries.
Given the importance in Jewish ritual of establishing the accurate timing of monthly and annual times, some futurist writers and researchers have considered whether a "corrected" system of establishing the Hebrew date is required. The mean year of the current mathematically based Hebrew calendar has "drifted" an average of 7–8 days late relative to the equinox relationship that it originally had. It is not possible, however, for any individual Hebrew date to be a week or more "late", because Hebrew months always begin within a day or two of the "molad" moment. What happens instead is that the traditional Hebrew calendar "prematurely" inserts a leap month one year before it "should have been" inserted, where "prematurely" means that the insertion causes the spring equinox to land more than 30 days before the latest acceptable moment, thus causing the calendar to run "one month late" until the time when the leap month "should have been" inserted prior to the following spring. This presently happens in 4 years out of every 19-year cycle (years 3, 8, 11, and 19), implying that the Hebrew calendar currently runs "one month late" more than 21% of the time.
Dr. Irv Bromberg has proposed a 353-year cycle of 4366 months, which would include 130 leap months, along with use of a progressively shorter "molad" interval, which would keep an amended fixed arithmetic Hebrew calendar from drifting for more than seven millennia. It takes about 3 centuries for the spring equinox to drift an average of th of a "molad" interval earlier in the Hebrew calendar. That is a very important time unit, because it can be cancelled by simply truncating a 19-year cycle to 11 years, omitting 8 years including three leap years from the sequence. That is the essential feature of the 353-year leap cycle ().
Religious questions abound about how such a system might be implemented and administered throughout the diverse aspects of the world Jewish community.
Conversion between Jewish and civil calendars.
The list below gives a time which can be used to determine the day the Jewish ecclesiastical (spring) year starts over a period of nineteen years:
Every nineteen years this time is 2 days, 16 hours, 33 1/18 minutes later in the week. That is either the same or the previous day in the civil calendar, depending on whether the difference in the day of the week is three or two days. If 29 February is included fewer than five times in the nineteen - year period the date will be later by the number of days which corresponds to the difference between the actual number of insertions and five. If the year is due to start on Sunday, it actually begins on the following Tuesday if the following year is due to start on Friday morning. If due to start on Monday, Wednesday or Friday it actually begins on the following day. If due to start on Saturday, it actually begins on the following day if the previous year was due to begin on Monday morning.
The table below lists, for a Jewish year commencing on 23 March, the civil date of the first day of each month. If the year does not begin on 23 March, each month's first day will differ from the date shown by the number of days that the start of the year differs from 23 March. The correct column is the one which shows the correct starting date for the following year in the last row. If 29 February falls within a Jewish month the first day of later months will be a day earlier than shown.
For long period calculations, dates should be reduced to the Julian calendar and converted back to the civil calendar at the end of the calculation. The civil calendar used here (Exigian) is correct to one day in 44,000 years and omits the leap day in centennial years which do not give remainder 200 or 700 when divided by 900. It is identical to the Gregorian calendar between 15 October 1582 CE and 28 February 2400 CE (both dates inclusive).
To find how many days the civil calendar is ahead of the Julian in any year from 301 BCE (the calendar is proleptic up to 1582 CE) add 300 to the year, multiply the hundreds by 7, divide by 9 and subtract 4. Ignore any fraction of a day. When the difference between the calendars changes the calculated value applies on and from March 1 (civil date) for conversions to Julian. For earlier dates reduce the calculated value by one. For conversions to the civil date the calculated value applies on and from February 29 (Julian date). Again, for earlier dates reduce the calculated value by one. The difference is applied to the calendar one is converting into. A negative value indicates that the Julian date is ahead of the civil date. In this case it is important to remember that when calculating the civil equivalent of February 29 (Julian), February 29 is discounted. Thus if the calculated value is -4 the civil equivalent of this date is February 24. Before 1 CE use astronomical years rather than years BCE. The astronomical year is (year BCE) - 1.
Up to the 4th century CE these tables give the day of the Jewish month to within a day or so and the number of the month to within a month or so. From the 4th century the number of the month is given exactly and from the 9th century the day of the month is given exactly as well.
In the Julian calendar, every 76 years the Jewish year is due to start 5h 47 14/18m earlier, and 3d 18h 12 4/18m later in the week.
On what civil date does the eighth month begin in CE 20874-5?
20874=2026+(248x76). In (248x76) Julian years the Jewish year is due to start (248x3d 18h 12 4/18m) later in the week, which is 932d 2h 31 2/18m or 1d 2h 31 2/18m later after removing complete weeks. Allowing for the current difference of thirteen days between the civil and Julian calendars the Julian date is 13+(248x0d 5h 47 4/18m) earlier, which is 72d 21h 28 16/18m earlier. Convert back to the civil calendar by applying the formula.
So in 20874 CE the Jewish year is due to begin 87d 2h 31 2/18m later than in 2026 CE and 1d 2h 31 2/18m later in the week. In 20874 CE, therefore, the Jewish year is due to begin at 11.30 3/18 A.M. on Friday, 14 June. Because of the displacements it actually begins on Saturday, 15 June. Odd months have 30 days and even months 29, so the starting dates are 2, 15 July; 3, 13 August; 4, 12 September; 5, 11 October; 6, 10 November; 7, 9 December, and 8, 8 January.
The rules are based on the theory which Maimonides explains in his book "Rabbinical Astronomy" - no allowance is made for the secular (centennial) decrease of ½ second in the length of the mean tropical year and increase of about four yards in the distance between the earth and the moon resulting from tidal friction because astronomy was not sufficiently developed in the 12th century (when Maimonides wrote his book) to detect this.
Bibliography.
723–730.

</doc>
<doc id="13786" url="https://en.wikipedia.org/wiki?curid=13786" title="The Holocaust Industry">
The Holocaust Industry

The Holocaust Industry: Reflections on the Exploitation of Jewish Suffering is a 2000 book by Norman G. Finkelstein in which he argues that the American Jewish establishment exploits the memory of the Nazi Holocaust for political and financial gain, as well as to further the interests of Israel. According to Finkelstein, this "Holocaust industry" has corrupted Jewish culture and the authentic memory of the Holocaust.
Finkelstein on the book.
Finkelstein states that his consciousness of "the Nazi holocaust" is rooted in his parents' experiences in the Warsaw Ghetto; with the exception of his parents themselves, "every family member on both sides was exterminated by the Nazis". Nonetheless, during his childhood, no one ever asked any questions about what his mother and father had suffered. He suggests, "This was not a respectful silence. It was indifference." It was only after the establishment of "the Holocaust industry", he suggests, that outpourings of anguish over the plight of the Jews in World War II began. This ideology in turn served to endow Israel with a status as "'victim' state" despite its "horrendous" human rights record.
According to Finkelstein, his book is "an anatomy and an indictment of the Holocaust industry". He argues that "'The Holocaust' is an ideological representation of the Nazi holocaust".
In the foreword to the first paperback edition, Finkelstein notes that the first hardback edition had been a considerable hit in several European countries and many languages, but had been largely ignored in the United States. He sees "The New York Times" as the main promotional vehicle of the "Holocaust industry", and notes that the 1999 Index listed 273 entries for the Holocaust and just 32 entries for the entire continent of Africa.
Chapters.
The second (2003) edition contained 100 pages of new material, primarily in chapter 3 on the World Jewish Congress lawsuit against Swiss banks. Finkelstein set out to provide a guide to the relevant sections of the case. He feels that the presiding judge elected not to docket crucial documents, and that the Claims Resolution Tribunal could no longer be trusted. Finkelstein claims the CRT was on course to vindicate the Swiss banks before it changed tack in order to "protect the blackmailers' reputation".
Reviews and critiques.
The critical response has been varied. In addition to prominent supporters, such as Noam Chomsky and Alexander Cockburn, the Holocaust historian Raul Hilberg is on record as praising Finkelstein's book: 
On the other hand, many have argued that "The Holocaust Industry" is an unscholarly work that promotes antisemitic stereotypes. For example, according to Israeli journalist Yair Sheleg, in August 2000, German historian Hans Mommsen called it "a most trivial book, which appeals to easily aroused anti-Semitic prejudices." Wolfgang Benz stated to "Le Monde": "It is impossible to learn anything from Finkelstein's book. At best, it is interesting for a psychotherapist." The reviewer of this daily added that Norman Finkelstein "hardly cares about nuance" and Rony Brauman wrote in the preface to the French edition ("L'Industrie de l'Holocauste", Paris, La Fabrique, 2001) that some assertions of N. Finkelstein (especially on the impact of the Six-days war) are wrong, others being pieces of "propaganda".
University of Chicago Professor Peter Novick, whose work Finkelstein described as providing the "initial stimulus" for "The Holocaust Industry", asserted in the July 28, 2000 "Jewish Chronicle" (London) that the book is replete with "false accusations", "egregious misrepresentations", "absurd claims" and "repeated mis-statements" ("A charge into darkness that sheds no light"). Finkelstein replied to the allegations by Novick on his homepage.
Hasia Diner has accused Peter Novick and Finkelstein of being "harsh critics of American Jewry from the left," and challenges the notion reflected in their books that American Jews did not begin to commemorate the Holocaust until post 1967.
Andrew Ross, reviewing the book for Salon magazine, wrote:
Finkelstein's response to the critics.
Finkelstein responded to his critics in the foreword to the second edition:
Other topics.
Fraudulent writings on the Holocaust.
Finkelstein describes two known frauds, that of "The Painted Bird" by Polish writer Jerzy Kosinski and "Fragments" by Binjamin Wilkomirski, and how they were defended by people even after they had been exposed. He identifies some of these people as members of the "Holocaust Industry", and notes that they also support each other. Elie Wiesel supported Kosinski; Israel Gutman and Daniel Goldhagen (see below) supported Wilkomirski; Wiesel and Gutman support Goldhagen.
Holocaust Industry defends itself.
Finkelstein has published heavy criticisms of several books in his career, as he did to "Hitler's Willing Executioners" by Daniel Johnah Goldhagen, which he calls "replete with gross misinterpretations of source material and internal contradictions", and says "the book is devoid of scholarly value". Independently, Ruth Bettina Birn (the world's leading authority on the archives that Goldhagen had consulted and chief historian for War Crimes with the Canadian Department of Justice) did the same - she and Finkelstein worked together on "A Nation on Trial: The Goldhagen Thesis and Historical Truth". Goldhagen refused the journal's invitation for a full rebuttal, and instead enlisted a London law firm to sue Birn and the Cambridge University Press. Protests were made to Birn's employer, calling her "a member of the perpetrator race" (she is German-born), prompting an official investigation of her.(p. 66) 
Other genocides.
Finkelstein scathingly compared the media treatment of the Holocaust and the media treatment of other genocides such as the Holodomor and the Armenian Genocide, particularly by members of what he calls "The Holocaust Industry". 1 to 1.5 million Armenians died in the years between 1915 and 1917/1923 - denial includes the claim that they were the result of a Civil War within World War I, or refusal to accept there were deaths. In 2001, Israeli Foreign Minister Shimon Peres went so far as to dismiss it as "allegations". However, by this time historical consensus was changing, and he was "angrily compared ... to a holocaust denier" by Israel Charny, executive director of the Institute on the Holocaust and Genocide in Jerusalem.
In August 2007, the Elie Wiesel Foundation for Humanity produced a letter signed by 53 Nobel Laureates re-affirming the Genocide Scholars' conclusion that the 1915 killings of Armenians constituted genocide. However, Wiesel's organization asserted there would be no legal "basis for reparations or territorial claims", anticipating Turkish anxieties that it could prompt financial or property claims. Abraham Foxman of the Anti-Defamation League announced: "Upon reflection, the consequences of those actions were indeed tantamount to genocide".
Other forms of Holocaust denial.
According to Finkelstein, Elie Wiesel characterizes any suggestion that he has profited from the "Holocaust Industry", or even any criticism at all, as Holocaust denial. Questioning a survivor's testimony, denouncing the role of Jewish collaborators, suggesting that Germans suffered during the bombing of Dresden or that any state except Germany committed crimes in World War II are all evidence of Holocaust denial – according to Deborah Lipstadt – and the most "insidious" forms of Holocaust denial are "immoral equivalencies", denying the uniqueness of The Holocaust. Finkelstein examines the implications of applying this standard to another member of the "Holocaust Industry", Daniel Goldhagen, who argued that Serbian actions in Kosovo "are, in their essence, different from those of Nazi Germany only in scale".
Holocaust deniers in real life.
According to Finkelstein, Deborah Lipstadt claims there is widespread Holocaust denial - yet in "Denying the Holocaust" (1993) her prime example is Arthur Butz, author of "The Hoax of the Twentieth Century". The chapter on him is entitled "Entering the Mainstream" - but Finkelstein considers that, were it not for the likes of Lipstadt, no one would ever have heard of Arthur Butz. Holocaust deniers have as much influence in the US as the Flat Earth Society (p. 69). Finkelstein believes there to be only one "truly mainstream" holocaust denier—Bernard Lewis, who was convicted in France of denying the Armenian genocide. Since Lewis is pro-Israel, "this instance ... raises no hackles in the United States."
Publishing history.
Publishing history of "The Holocaust Industry":

</doc>
<doc id="13787" url="https://en.wikipedia.org/wiki?curid=13787" title="Hermetic Order of the Golden Dawn">
Hermetic Order of the Golden Dawn

The Hermetic Order of the Golden Dawn (; or, more commonly, The Golden Dawn ("Aurora Aurea")) was an organization devoted to the study and practice of the occult, metaphysics, and paranormal activities during the late 19th and early 20th centuries. Known as a magical order, the Hermetic Order of the Golden Dawn was active in Great Britain and focused its practices on theurgy and spiritual development. Many present-day concepts of ritual and magic that are at the centre of contemporary traditions, such as Wicca and Thelema, were inspired by the Golden Dawn, which became one of the largest single influences on 20th-century Western occultism.
The three founders, William Robert Woodman, William Wynn Westcott, and Samuel Liddell MacGregor Mathers, were Freemasons and members of Societas Rosicruciana in Anglia (S.R.I.A.). Westcott appears to have been the initial driving force behind the establishment of the Golden Dawn.
The Golden Dawn system was based on hierarchy and initiation like the Masonic Lodges; however women were admitted on an equal basis with men. The "Golden Dawn" was the first of three Orders, although all three are often collectively referred to as the "Golden Dawn". The First Order taught esoteric philosophy based on the Hermetic Qabalah and personal development through study and awareness of the four Classical Elements as well as the basics of astrology, tarot divination, and geomancy. The Second or "Inner" Order, the "Rosae Rubeae et Aureae Crucis" (the Ruby Rose and Cross of Gold), taught proper magic, including scrying, astral travel, and alchemy. The Third Order was that of the "Secret Chiefs", who were said to be highly skilled; they supposedly directed the activities of the lower two orders by spirit communication with the Chiefs of the Second Order.
History.
Cipher Manuscripts.
The foundational documents of the original Order of the Golden Dawn, known as the Cipher Manuscripts, are written in English using the Trithemius cipher. The manuscripts give the specific outlines of the Grade Rituals of the Order and prescribe a curriculum of graduated teachings that encompass the Hermetic Qabalah, astrology, occult tarot, geomancy, and alchemy.
According to the records of the Order, the manuscripts passed from Kenneth R. H. Mackenzie, a Masonic scholar, to the Rev. A. F. A. Woodford, whom British occult writer Francis King describes as the fourth founder (although Woodford died shortly after the Order was founded). The documents did not excite Woodford, and in February 1886 he passed them on to Freemason William Wynn Westcott, who managed to decode them in 1887. Westcott, pleased with his discovery, called on fellow Freemason Samuel Liddell MacGregor Mathers for a second opinion. Westcott asked for Mathers' help to turn the manuscripts into a coherent system for lodge work. Mathers in turn asked fellow Freemason William Robert Woodman to assist the two, and he accepted. Mathers and Westcott have been credited with developing the ritual outlines in the Cipher Manuscripts into a workable format. Mathers, however, is generally credited with the design of the curriculum and rituals of the Second Order, which he called the "Rosae Rubae et Aureae Crucis" ("Ruby Rose and Golden Cross" or the "RR et AC").
Founding of first temple.
In October 1887, Westcott claimed to have written to a German countess and prominent Rosicrucian named Anna Sprengel, whose address was said to have been found in the decoded Cipher Manuscripts. According to Westcott, Sprengel claimed the ability to contact certain supernatural entities, known as the Secret Chiefs, that were considered the authorities over any magical order or esoteric organization. Westcott purportedly received a reply from Sprengel granting permission to establish a Golden Dawn temple and conferring honorary grades of Adeptus Exemptus on Westcott, Mathers, and Woodman. The temple was to consist of the five grades outlined in the manuscripts.
In 1888, the Isis-Urania Temple was founded in London. In contrast to the S.R.I.A. and Masonry, women were allowed and welcome to participate in the Order in "perfect equality" with men. The Order was more of a philosophical and metaphysical teaching order in its early years. Other than certain rituals and meditations found in the Cipher manuscripts and developed further, "magical practices" were generally not taught at the first temple.
For the first four years, the Golden Dawn was one cohesive group later known as "the Outer Order" or "First Order." An "Inner Order" was established and became active in 1892. The Inner Order consisted of members known as "adepts," who had completed the entire course of study for the Outer Order. This group of adepts eventually became known as the Second Order.
Eventually, the Osiris temple in Weston-super-Mare, the Horus temple in Bradford (both in 1888), and the Amen-Ra temple in Edinburgh (1893) were founded. In 1893 Mathers founded the Ahathoor temple in Paris.
Secret Chiefs.
In 1891, Westcott's alleged correspondence with Anna Sprengel suddenly ceased. He claimed to have received word from Germany that she was either dead or that her companions did not approve of the founding of the Order and no further contact was to be made. If the founders were to contact the Secret Chiefs, apparently, it had to be done on their own. In 1892, Mathers professed that a link to the Secret Chiefs had been established. Subsequently, he supplied rituals for the Second Order, calling them the Red Rose and Cross of Gold. The rituals were based on the tradition of the tomb of Christian Rosenkreuz, and a "Vault of Adepts" became the controlling force behind the Outer Order. Later in 1916, Westcott claimed that Mathers also constructed these rituals from materials he received from Frater Lux ex Tenebris, a purported "Continental Adept".
Some followers of the Golden Dawn tradition believe that the Secret Chiefs were not human or supernatural beings but, rather, symbolic representations of actual or legendary sources of spiritual esotericism. The term came to stand for a great leader or teacher of a spiritual path or practice that found its way into the teachings of the Order.
Golden Age.
By the mid-1890s, the Golden Dawn was well established in Great Britain, with over one hundred members from every class of Victorian society. Many celebrities belonged to the Golden Dawn, such as the actress Florence Farr, the Irish revolutionary Maud Gonne, the Irish poet William Butler Yeats, the Welsh author Arthur Machen, and the English authors Evelyn Underhill and Aleister Crowley.
In 1896 or 1897, Westcott broke all ties to the Golden Dawn, leaving Mathers in control. It has been speculated that his departure was due to his having lost a number of occult-related papers in a hansom cab. Apparently, when the papers were found, Westcott's connection to the Golden Dawn was discovered and brought to the attention of his employers. He may have been told to either resign from the Order or to give up his occupation as coroner. After Westcott's departure, Mathers appointed Florence Farr to be Chief Adept in Anglia. Dr. Henry B. Pullen Burry succeeded Westcott as Cancellarius—one of the three Chiefs of the Order.
Mathers was the only active founding member after Westcott's departure. Due to personality clashes with other members and frequent absences from the center of Lodge activity in Great Britain, however, challenges to Mathers's authority as leader developed among the members of the Second Order.
Revolt.
Toward the end of 1899, the Adepts of the Isis-Urania and Amen-Ra temples had become dissatisfied with Mathers' leadership, as well as his growing friendship with Aleister Crowley. They had also become anxious to make contact with the Secret Chiefs themselves, instead of relying on Mathers as an intermediary. Within the Isis-Urania temple, disputes were arising between Farr's "The Sphere", a secret society within the Isis-Urania, and the rest of the Adepti Minores.
Crowley was refused initiation into the Adeptus Minor grade by the London officials. Mathers overrode their decision and quickly initiated him at the Ahathoor temple in Paris on January 16, 1900. Upon his return to the London temple, Crowley requested from Miss Cracknell, the acting secretary, the papers acknowledging his grade, to which he was now entitled. To the London Adepts, this was the final straw. Farr, already of the opinion that the London temple should be closed, wrote to Mathers expressing her wish to resign as his representative, although she was willing to carry on until a successor was found. Mathers believed Westcott was behind this turn of events and replied on February 16. On March 3, a committee of seven Adepts was elected in London, and requested a full investigation of the matter. Mathers sent an immediate reply, declining to provide proof, refusing to acknowledge the London temple, and dismissing Farr as his representative on March 23. In response, a general meeting was called on March 29 in London to remove Mathers as chief and expel him from the Order.
Splinters.
In 1901, W. B. Yeats privately published a pamphlet titled "Is the Order of R. R. & A. C. to Remain a Magical Order?"
After the Isis-Urania temple claimed its independence, there were even more disputes, leading to Yeats resigning. A committee of three was to temporarily govern, which included P.W. Bullock, M.W. Blackden and J. W. Brodie-Innes. After a short time, Bullock resigned, and Dr. Robert Felkin took his place.
In 1903, A.E. Waite and Blackden joined forces to retain the name Isis-Urania, while Felkin and other London members formed the Stella Matutina. Yeats remained in the Stella Matutina until 1921, while Brodie-Innes continued his Amen-Ra membership in Edinburgh.
Reconstruction.
Once Mathers realised that reconciliation was impossible, he made efforts to reestablish himself in London. The Bradford and Weston-super-Mare temples remained loyal to him, but their numbers were few. He then appointed Edward Berridge as his representative. According to Francis King, historical evidence shows that there were "twenty three members of a flourishing Second Order under Berridge-Mathers in 1913."
J.W. Brodie-Innes continued leading the Amen-Ra temple, deciding that the revolt was unjustified. By 1908, Mathers and Brodie-Innes were in complete accord. According to sources that differ regarding the actual date, sometime between 1901 and 1913 Mathers renamed the branch of the Golden Dawn remaining loyal to his leadership to Alpha et Omega. Brodie-Innes assumed command of the English and Scottish temples, while Mathers concentrated on building up his Ahathoor temple and extending his American connections. According to occultist Israel Regardie, the Golden Dawn had spread to the United States of America before 1900 and a Thoth-Hermes temple had been founded in Chicago. By the beginning of the First World War in 1914, Mathers had established two to three American temples.
Most temples of the Alpha et Omega and Stella Matutina closed or went into abeyance by the end of the 1930s, with the exceptions of two Stella Matutina temples: Hermes Temple in Bristol, which operated sporadically until 1970, and the Smaragdum Thallasses Temple (commonly referred to as Whare Ra) in Havelock North, New Zealand, which operated regularly until its closure in 1978.
Structure and grades.
Much of the hierarchical structure for the Golden dawn came from the Societas Rosicruciana in Anglia, which was itself derived from the Order of the Golden and Rosy Cross.
The paired numbers attached to the Grades relate to positions on the Tree of Life. The Neophyte Grade of "0=0" indicates no position on the Tree. In the other pairs, the first numeral is the number of steps up from the bottom (Malkuth), and the second numeral is the number of steps down from the top (Kether).
The First Order Grades were related to the four elements of Earth, Air, Water, and Fire, respectively. The Aspirant to a Grade received instruction on the metaphysical meaning of each of these Elements and had to pass a written examination and demonstrate certain skills to receive admission to that Grade.
The Portal Grade was an "Invisible" or in-between grade separating the First Order from the Second Order. The Circle of existing Adepts from the Second Order had to consent to allow an Aspirant to be initiated as an Adept and join the Second Order.
The Second Order was not, properly, part of the "Golden Dawn", but a separate Order in its own right, known as the R.R. et A.C. The Second Order directed the teachings of the First Order and was the governing force behind the First Order.
After passing the Portal, the Aspirant was instructed in the techniques of practical magic. When another examination was passed, and the other Adepts consented, the Aspirant attained the Grade of Adeptus Minor (5=6). There were also four sub-Grades of instruction for the Adeptus Minor, again relating to the four Outer Order grades.
A member of the Second Order had the power and authority to initiate aspirants to the First Order, though usually not without the permission of the Chiefs of his or her Lodge.
"The Golden Dawn" book.
The encyclopedic text "The Golden Dawn", by Israel Regardie, has been the most intensively used source for modern western occult and magical practice.
Contemporary Golden Dawn orders.
While no temples in the original chartered lineage of the Golden Dawn survived past the 1970s, several organizations have since revived its teachings and rituals. Among these, the following are notable:

</doc>
<doc id="13790" url="https://en.wikipedia.org/wiki?curid=13790" title="Hash function">
Hash function

A hash function is any function that can be used to map data of arbitrary size to data of fixed size. The values returned by a hash function are called hash values, hash codes, hash sums, or simply hashes. 
One use is a data structure called a hash table, widely used in computer software for rapid data lookup. Hash functions accelerate table or database lookup by detecting duplicated records in a large file. An example is finding similar stretches in DNA sequences. They are also useful in cryptography. A cryptographic hash function allows one to easily verify that some input data maps to a given hash value, but if the input data is unknown, it is deliberately difficult to reconstruct it (or equivalent alternatives) by knowing the stored hash value. This is used for assuring integrity of transmitted data, and is the building block for HMACs, which provide message authentication.
Hash functions are related to (and often confused with) checksums, check digits, fingerprints, randomization functions, error-correcting codes, and ciphers. Although these concepts overlap to some extent, each has its own uses and requirements and is designed and optimized differently. The Hash Keeper database maintained by the American National Drug Intelligence Center, for instance, is more aptly described as a catalogue of file fingerprints than of hash values.
Uses.
Hash tables.
Hash functions are primarily used in hash tables, to quickly locate a data record (e.g., a dictionary definition) given its search key (the headword). Specifically, the hash function is used to map the search key to an index; the index gives the place in the hash table where the corresponding record should be stored. Hash tables, in turn, are used to implement associative arrays and dynamic sets.
Typically, the domain of a hash function (the set of possible keys) is larger than its range (the number of different table indices), and so it will map several different keys to the same index. Therefore, each slot of a hash table is associated with (implicitly or explicitly) a set of records, rather than a single record. For this reason, each slot of a hash table is often called a "bucket", and hash values are also called "bucket indices".
Thus, the hash function only hints at the record's location — it tells where one should start looking for it. Still, in a half-full table, a good hash function will typically narrow the search down to only one or two entries.
Caches.
Hash functions are also used to build caches for large data sets stored in slow media. A cache is generally simpler than a hashed search table, since any collision can be resolved by discarding or writing back the older of the two colliding items. This is also used in file comparison.
Bloom filters.
Hash functions are an essential ingredient of the Bloom filter, a space-efficient probabilistic data structure that is used to test whether an element is a member of a set.
Finding duplicate records.
When storing records in a large unsorted file, one may use a hash function to map each record to an index into a table "T", and to collect in each bucket "T"["i"] a list of the numbers of all records with the same hash value "i". Once the table is complete, any two duplicate records will end up in the same bucket. The duplicates can then be found by scanning every bucket "T"["i"] which contains two or more members, fetching those records, and comparing them. With a table of appropriate size, this method is likely to be much faster than any alternative approach (such as sorting the file and comparing all consecutive pairs).
Protecting data.
A hash value can be used to uniquely identify secret information. This requires that the hash function is collision-resistant, which means that it is very hard to find data that generate the same hash value. These functions are categorized into cryptographic hash functions and provably secure hash functions. Functions in the second category are the most secure but also too slow for most practical purposes. Collision resistance is accomplished in part by generating very large hash values. For example, SHA-1, one of the most widely used cryptographic hash functions, generates 160 bit values.
Finding similar records.
Hash functions can also be used to locate table records whose key is similar, but not identical, to a given key; or pairs of records in a large file which have similar keys. For that purpose, one needs a hash function that maps similar keys to hash values that differ by at most "m", where "m" is a small integer (say, 1 or 2). If one builds a table "T" of all record numbers, using such a hash function, then similar records will end up in the same bucket, or in nearby buckets. Then one need only check the records in each bucket "T"["i"] against those in buckets "T"["i"+"k"] where "k" ranges between −"m" and "m".
This class includes the so-called acoustic fingerprint algorithms, that are used to locate similar-sounding entries in large collection of audio files. For this application, the hash function must be as insensitive as possible to data capture or transmission errors, and to trivial changes such as timing and volume changes, compression, etc.
Finding similar substrings.
The same techniques can be used to find equal or similar stretches in a large collection of strings, such as a document repository or a genomic database. In this case, the input strings are broken into many small pieces, and a hash function is used to detect potentially equal pieces, as above.
The Rabin–Karp algorithm is a relatively fast string searching algorithm that works in O("n") time on average. It is based on the use of hashing to compare strings.
Geometric hashing.
This principle is widely used in computer graphics, computational geometry and many other disciplines, to solve many proximity problems in the plane or in three-dimensional space, such as finding closest pairs in a set of points, similar shapes in a list of shapes, similar images in an image database, and so on. In these applications, the set of all inputs is some sort of metric space, and the hashing function can be interpreted as a partition of that space into a grid of "cells". The table is often an array with two or more indices (called a "grid file", "grid index", "bucket grid", and similar names), and the hash function returns an index tuple. This special case of hashing is known as geometric hashing or "the grid method". Geometric hashing is also used in telecommunications (usually under the name vector quantization) to encode and compress multi-dimensional signals.
Standard uses of hashing in cryptography.
Some standard applications that employ hash functions include authentication, message integrity (using an HMAC (Hashed MAC)), message fingerprinting, data corruption detection, and digital signature efficiency.
Properties.
Good hash functions, in the original sense of the term, are usually required to satisfy certain properties listed below. The exact requirements are dependent on the application, for example a hash function well suited to indexing data will probably be a poor choice for a cryptographic hash function.
Determinism.
A hash procedure must be deterministic—meaning that for a given input value it must always generate the same hash value. In other words, it must be a function of the data to be hashed, in the mathematical sense of the term. This requirement excludes hash functions that depend on external variable parameters, such as pseudo-random number generators or the time of day. It also excludes functions that depend on the memory address of the object being hashed in cases that the address may change during execution (as may happen on systems that use certain methods of garbage collection), although sometimes rehashing of the item is possible.
The determinism is in the context of the reuse of the function. For example, Python adds the feature that hash functions are randomized seed that is decided once, when the process starts. It is still a valid hash function when used in within a single run. But if the values are persisted (for example, written to disk) they can no longer be treated as valid hash values, since in the next run the random value might differ.
Uniformity.
A good hash function should map the expected inputs as evenly as possible over its output range. That is, every hash value in the output range should be generated with roughly the same probability. The reason for this last requirement is that the cost of hashing-based methods goes up sharply as the number of "collisions"—pairs of inputs that are mapped to the same hash value—increases. If some hash values are more likely to occur than others, a larger fraction of the lookup operations will have to search through a larger set of colliding table entries.
Note that this criterion only requires the value to be "uniformly distributed", not "random" in any sense. A good randomizing function is (barring computational efficiency concerns) generally a good choice as a hash function, but the converse need not be true.
Hash tables often contain only a small subset of the valid inputs. For instance, a club membership list may contain only a hundred or so member names, out of the very large set of all possible names. In these cases, the uniformity criterion should hold for almost all typical subsets of entries that may be found in the table, not just for the global set of all possible entries.
In other words, if a typical set of "m" records is hashed to "n" table slots, the probability of a bucket receiving many more than "m"/"n" records should be vanishingly small. In particular, if "m" is less than "n", very few buckets should have more than one or two records. (In an ideal "perfect hash function", no bucket should have more than one record; but a small number of collisions is virtually inevitable, even if "n" is much larger than "m" – see the birthday paradox).
When testing a hash function, the uniformity of the distribution of hash values can be evaluated by the chi-squared test.
Defined range.
It is often desirable that the output of a hash function have fixed size (but see below). If, for example, the output is constrained to 32-bit integer values, the hash values can be used to index into an array. Such hashing is commonly used to accelerate data searches. On the other hand, cryptographic hash functions produce much larger hash values, in order to ensure the computational complexity of brute-force inversion. For example, SHA-1, one of the most widely used cryptographic hash functions, produces a 160-bit value.
Producing fixed-length output from variable length input can be accomplished by breaking the input data into chunks of specific size. Hash functions used for data searches use some arithmetic expression which iteratively processes chunks of the input (such as the characters in a string) to produce the hash value. In cryptographic hash functions, these chunks are processed by a one-way compression function, with the last chunk being padded if necessary. In this case, their size, which is called "block size", is much bigger than the size of the hash value. For example, in SHA-1, the hash value is 160 bits and the block size 512 bits.
Variable range.
In many applications, the range of hash values may be different for each run of the program, or may change along the same run (for instance, when a hash table needs to be expanded). In those situations, one needs a hash function which takes two parameters—the input data "z", and the number "n" of allowed hash values.
A common solution is to compute a fixed hash function with a very large range (say, 0 to 232 − 1), divide the result by "n", and use the division's remainder. If "n" is itself a power of 2, this can be done by bit masking and bit shifting. When this approach is used, the hash function must be chosen so that the result has fairly uniform distribution between 0 and "n" − 1, for any value of "n" that may occur in the application. Depending on the function, the remainder may be uniform only for certain values of "n", e.g. odd or prime numbers.
We can allow the table size "n" to not be a power of 2 and still not have to perform any remainder or division operation, as these computations are sometimes costly. For example, let "n" be significantly less than 2"b". Consider a pseudorandom number generator (PRNG) function "P"(key) that is uniform on the interval 2"b" − 1. A hash function uniform on the interval n-1 is "n" "P"(key)/2"b". We can replace the division by a (possibly faster) right bit shift: "nP"(key) Â» "b".
Variable range with minimal movement (dynamic hash function).
When the hash function is used to store values in a hash table that outlives the run of the program, and the hash table needs to be expanded or shrunk, the hash table is referred to as a dynamic hash table.
A hash function that will relocate the minimum number of records when the table is – where "z" is the key being hashed and "n" is the number of allowed hash values – such that "H"("z","n" + 1) = "H"("z","n") with probability close to "n"/("n" + 1).
Linear hashing and spiral storage are examples of dynamic hash functions that execute in constant time but relax the property of uniformity to achieve the minimal movement property.
Extendible hashing uses a dynamic hash function that requires space proportional to "n" to compute the hash function, and it becomes a function of the previous keys that have been inserted.
Several algorithms that preserve the uniformity property but require time proportional to "n" to compute the value of "H"("z","n") have been invented.
Data normalization.
In some applications, the input data may contain features that are irrelevant for comparison purposes. For example, when looking up a personal name, it may be desirable to ignore the distinction between upper and lower case letters. For such data, one must use a hash function that is compatible with the data equivalence criterion being used: that is, any two inputs that are considered equivalent must yield the same hash value. This can be accomplished by normalizing the input before hashing it, as by upper-casing all letters.
Continuity.
"A hash function that is used to search for similar (as opposed to equivalent) data must be as continuous as possible; two inputs that differ by a little should be mapped to equal or nearly equal hash values."
Note that continuity is usually considered a fatal flaw for checksums, cryptographic hash functions, and other related concepts. Continuity is desirable for hash functions only in some applications, such as hash tables used in Nearest neighbor search.
Non-invertible.
In cryptographic applications, hash functions are typically expected to be practically non-invertible, meaning that it is not realistic to reconstruct the input datum from its hash value () alone without spending great amounts of computing time (see also One-way function).
Hash function algorithms.
For most types of hashing functions, the choice of the function depends strongly on the nature of the input data, and their probability distribution in the intended application.
Trivial hash function.
If the data to be hashed is small enough, one can use the data itself (reinterpreted as an integer) as the hashed value. The cost of computing this "trivial" (identity) hash function is effectively zero. This hash function is perfect, as it maps each input to a distinct hash value.
The meaning of "small enough" depends on the size of the type that is used as the hashed value. For example, in Java, the hash code is a 32-bit integer. Thus the 32-bit integer codice_1 and 32-bit floating-point codice_2 objects can simply use the value directly; whereas the 64-bit integer codice_3 and 64-bit floating-point codice_4 cannot use this method.
Other types of data can also use this perfect hashing scheme. For example, when mapping character strings between upper and lower case, one can use the binary encoding of each character, interpreted as an integer, to index a table that gives the alternative form of that character ("A" for "a", "8" for "8", etc.). If each character is stored in 8 bits (as in ASCII or ISO Latin 1), the table has only 28 = 256 entries; in the case of Unicode characters, the table would have 17×216 = 1114112 entries.
The same technique can be used to map two-letter country codes like "us" or "za" to country names (262=676 table entries), 5-digit zip codes like 13083 to city names (100000 entries), etc. Invalid data values (such as the country code "xx" or the zip code 00000) may be left undefined in the table, or mapped to some appropriate "null" value.
Perfect hashing.
A hash function that is injective—that is, maps each valid input to a different hash value—is said to be perfect. With such a function one can directly locate the desired entry in a hash table, without any additional searching.
Minimal perfect hashing.
A perfect hash function for "n" keys is said to be minimal if its range consists of "n" "consecutive" integers, usually from 0 to "n"−1. Besides providing single-step lookup, a minimal perfect hash function also yields a compact hash table, without any vacant slots. Minimal perfect hash functions are much harder to find than perfect ones with a wider range.
Hashing uniformly distributed data.
If the inputs are bounded-length strings and each input may independently occur with uniform probability (such as telephone numbers, car license plates, invoice numbers, etc.), then a hash function needs to map roughly the same number of inputs to each hash value. For instance, suppose that each input is an integer "z" in the range 0 to "N"−1, and the output must be an integer "h" in the range 0 to "n"−1, where "N" is much larger than "n". Then the hash function could be "h" = "z" mod "n" (the remainder of "z" divided by "n"), or "h" = ("z" × "n") ÷ "N" (the value "z" scaled down by "n"/"N" and truncated to an integer), or many other formulas.
Hashing data with other distributions.
These simple formulas will not do if the input values are not equally likely, or are not independent. For instance, most patrons of a supermarket will live in the same geographic area, so their telephone numbers are likely to begin with the same 3 to 4 digits. In that case, if "m" is 10000 or so, the division formula ("z" × "m") ÷ "M", which depends mainly on the leading digits, will generate a lot of collisions; whereas the remainder formula "z" mod "m", which is quite sensitive to the trailing digits, may still yield a fairly even distribution.
Hashing variable-length data.
When the data values are long (or variable-length) character strings—such as personal names, web page addresses, or mail messages—their distribution is usually very uneven, with complicated dependencies. For example, text in any natural language has highly non-uniform distributions of characters, and character pairs, very characteristic of the language. For such data, it is prudent to use a hash function that depends on all characters of the string—and depends on each character in a different way.
In cryptographic hash functions, a Merkle–Damgård construction is usually used. In general, the scheme for hashing such data is to break the input into a sequence of small units (bits, bytes, words, etc.) and combine all the units "b""b"[2, …, "b"["m"] sequentially, as follows
This schema is also used in many text checksum and fingerprint algorithms. The state variable "S" may be a 32- or 64-bit unsigned integer; in that case, "S0" can be 0, and "G"("S","n") can be just "S" mod "n". The best choice of "F" is a complex issue and depends on the nature of the data. If the units "b"["k"] are single bits, then "F"("S","b") could be, for instance
Here "highbit"("S") denotes the most significant bit of "S"; the '*' operator denotes unsigned integer multiplication with lost overflow; '^' is the bitwise exclusive or operation applied to words; and "P" is a suitable fixed word.
Special-purpose hash functions.
In many cases, one can design a special-purpose (heuristic) hash function that yields many fewer collisions than a good general-purpose hash function. For example, suppose that the input data are file names such as FILE0000.CHK, FILE0001.CHK, FILE0002.CHK, etc., with mostly sequential numbers. For such data, a function that extracts the numeric part "k" of the file name and returns "k" mod "n" would be nearly optimal. Needless to say, a function that is exceptionally good for a specific kind of data may have dismal performance on data with different distribution.
Rolling hash.
In some applications, such as substring search, one must compute a hash function "h" for every "k"-character substring of a given "n"-character string "t"; where "k" is a fixed integer, and "n" is "k". The straightforward solution, which is to extract every such substring "s" of "t" and compute "h"("s") separately, requires a number of operations proportional to "k"·"n". However, with the proper choice of "h", one can use the technique of rolling hash to compute all those hashes with an effort proportional to "k" + "n".
Universal hashing.
A universal hashing scheme is a randomized algorithm that selects a hashing function "h" among a family of such functions, in such a way that the probability of a collision of any two distinct keys is 1/"n", where "n" is the number of distinct hash values desired—independently of the two keys. Universal hashing ensures (in a probabilistic sense) that the hash function application will behave as well as if it were using a random function, for any distribution of the input data. It will however have more collisions than perfect hashing, and may require more operations than a special-purpose hash function. See also Unique Permutation Hashing.
Hashing with checksum functions.
One can adapt certain checksum or fingerprinting algorithms for use as hash functions. Some of those algorithms will map arbitrary long string data "z", with any typical real-world distribution—no matter how non-uniform and dependent—to a 32-bit or 64-bit string, from which one can extract a hash value in 0 through "n" − 1.
This method may produce a sufficiently uniform distribution of hash values, as long as the hash range size "n" is small compared to the range of the checksum or fingerprint function. However, some checksums fare poorly in the avalanche test, which may be a concern in some applications. In particular, the popular CRC32 checksum provides only 16 bits (the higher half of the result) that are usable for hashing. Moreover, each bit of the input has a deterministic effect on each bit of the CRC32, that is one can tell without looking at the rest of the input, which bits of the output will flip if the input bit is flipped; so care must be taken to use all 32 bits when computing the hash from the checksum.
Multiplicative hashing.
Multiplicative hashing is a simple type of hash function often used by teachers introducing students to hash tables. Multiplicative hash functions are simple and fast, but have higher collision rates in hash tables than more sophisticated hash functions.
In many applications, such as hash tables, collisions make the system a little slower but are otherwise harmless.
In such systems, it is often better to use hash functions based on multiplication -- such as MurmurHash and the SBoxHash -- or even simpler hash functions such as CRC32 -- and tolerate more collisions; rather than use a more complex hash function that avoids many of those collisions but takes longer to compute. Multiplicative hashing is susceptible to a "common mistake" that leads to poor diffusion—higher-value input bits do not affect lower-value output bits.
Hashing with cryptographic hash functions.
Some cryptographic hash functions, such as SHA-1, have even stronger uniformity guarantees than checksums or fingerprints, and thus can provide very good general-purpose hashing functions.
In ordinary applications, this advantage may be too small to offset their much higher cost. However, this method can provide uniformly distributed hashes even when the keys are chosen by a malicious agent. This feature may help to protect services against denial of service attacks.
Hashing by nonlinear table lookup.
Tables of random numbers (such as 256 random 32 bit integers) can provide high-quality nonlinear functions to be used
as hash functions or for other purposes such as cryptography. The key to be hashed would be split into 8-bit (one byte) parts and each part will be used as an index for the nonlinear table. The table values will be added by arithmetic or XOR addition to the hash output value. Because the table is just 1024 bytes in size, it will fit into the cache of modern microprocessors and allow for very fast execution of the hashing algorithm. As the table value is on average much longer than 8 bits, one bit of input will affect nearly all output bits.
This algorithm has proven to be very fast and of high quality for hashing purposes (especially hashing of integer number keys).
Efficient hashing of strings.
Modern microprocessors will allow for much faster processing, if 8-bit character strings are not hashed by processing one character at a time, but by interpreting the string as an array of 32 bit or 64 bit integers and hashing/accumulating these "wide word" integer values by means of arithmetic operations (e.g. multiplication by constant and bit-shifting). The remaining characters of the string which are smaller than the word length of the CPU must be handled differently (e.g. being processed one character at a time).
This approach has proven to speed up hash code generation by a factor of five or more on modern microprocessors of
a word size of 64 bit.
Another approach is to convert strings to a 32 or 64 bit numeric value and then apply a hash function. One method that avoids the problem of strings having great similarity ("Aaaaaaaaaa" and "Aaaaaaaaab") is to use a Cyclic redundancy check (CRC) of the string to compute a 32- or 64-bit value. While it is possible that two different strings will have the same CRC, the likelihood is very small and only requires that one check the actual string found to determine whether one has an exact match. CRCs will be different for strings such as "Aaaaaaaaaa" and "Aaaaaaaaab". Although, CRC codes can be used as hash values they are not cryptographically secure since they are not collision-resistant.
Locality-sensitive hashing.
Locality-sensitive hashing (LSH) is a method of performing probabilistic dimension reduction of high-dimensional data. The basic idea is to hash the input items so that similar items are mapped to the same buckets with high probability (the number of buckets being much smaller than the universe of possible input items). This is different from the conventional hash functions, such as those used in cryptography, as in this case the goal is to maximize the probability of "collision" of similar items rather than to avoid collisions.
One example of LSH is MinHash algorithm used for finding similar documents (such as web-pages):
Let "h" be a hash function that maps the members of and to distinct integers, and for any set "S" define to be the member of with the minimum value of . Then exactly when the minimum hash value of the union lies in the intersection .
Therefore,
In other words, if is a random variable that is one when and zero otherwise, then is an unbiased estimator of , although it has too high a variance to be useful on its own. The idea of the MinHash scheme is to reduce the variance by averaging together several variables constructed in the same way.
Origins of the term.
The term "hash" comes by way of analogy with its non-technical meaning, to "chop and mix", see hash (food). Indeed, typical hash functions, like the mod operation, "chop" the input domain into many sub-domains that get "mixed" into the output range to improve the uniformity of the key distribution.
Donald Knuth notes that Hans Peter Luhn of IBM appears to have been the first to use the concept, in a memo dated January 1953, and that Robert Morris used the term in a survey paper in CACM which elevated the term from technical jargon to formal terminology.

</doc>
<doc id="13791" url="https://en.wikipedia.org/wiki?curid=13791" title="High jump">
High jump

The high jump is a track and field event in which competitors must jump unaided over a horizontal bar placed at measured heights without dislodging it. In its modern most practised format, a bar is placed between two standards with a crash mat for landing. At the elite level, athletes run towards the bar and use the Fosbury Flop method of jumping, leaping head first with their back to the bar. Performed since ancient times, competitors have introduced increasingly more effective techniques to arrive at the current form.
The discipline is, alongside the pole vault, one of two vertical clearance events to feature on the Olympic athletics programme. It is contested at the World Championships in Athletics and IAAF World Indoor Championships, and is a common occurrence at track and field meetings. The high jump was among the first events deemed acceptable for women, having been held at the 1928 Olympic Games.
Javier Sotomayor (Cuba) is the current men's record holder with a jump of set in 1993 – the longest standing record in the history of the men's high jump. Stefka Kostadinova (Bulgaria) has held the women's world record at since 1987, also the longest-held record in the event.
Rules.
Jumpers must take off on one foot.
The rules for the high jump are set internationally by the International Association of Athletics Federations (IAAF). A jump is considered a failure if the bar is dislodged by the action of the jumper whilst jumping or the jumper touches the ground or breaks the plane of the near edge of the bar before clearance. The technique one uses for the jump must be almost flawless in order to have a chance of clearing a high bar.
Competitors may begin jumping at any height announced by the chief judge, or may pass, at their own discretion. Three consecutive missed jumps, at any height or combination of heights, will eliminate the jumper from competition.
The victory goes to the jumper who clears the greatest height during the final. If two or more jumpers tie for first place, the tie-breakers are: 1) The fewest misses at the height at which the tie occurred; and 2) The fewest misses throughout the competition.
If the event remains tied for first place (or a limited advancement position to a subsequent meet), the jumpers have a jump-off, beginning at the next greater height. Each jumper has one attempt. The bar is then alternately lowered and raised until only one jumper succeeds at a given height.
History.
The first recorded high jump event took place in Scotland in the 19th century. Early jumpers used either an elaborate straight-on approach or a scissors technique. In the later years, the bar was approached diagonally, and the jumper threw first the inside leg and then the other over the bar in a scissoring motion. Around the turn of the 20th century, techniques began to modernise, starting with the Irish-American Michael Sweeney's "Eastern cut-off". By taking off like the scissors, but extending his back and flattening out over the bar, Sweeney achieved a more economic clearance and raised the world record to in 1895.
Another American, George Horine, developed an even more efficient technique, the "Western roll". In this style, the bar again is approached on a diagonal, but the inner leg is used for the take-off, while the outer leg is thrust up to lead the body sideways over the bar. Horine increased the world standard to in 1912. His technique was predominant through the Berlin Olympics of 1936, in which the event was won by Cornelius Johnson at .
American and Soviet jumpers held the playing field for the next four decades, and they pioneered the evolution of the straddle technique. Straddle jumpers took off as in the Western roll, but rotated their (belly-down) torso around the bar, obtaining the most economical clearance up to that time. Straddle-jumper Charles Dumas was the first to clear 7 feet (2.13 m), in 1956, and American John Thomas pushed the world mark to in 1960. Valeriy Brumel took over the event for the next four years. The elegant Soviet jumper radically sped up his approach run, took the record up to , and won the Olympic gold medal in 1964, before a motorcycle accident ended his career.
American coaches, including two-time NCAA champion Frank Costello of the University of Maryland, flocked to Russia to learn from Brumel and his coaches. However, it would be a solitary innovator at Oregon State University, Dick Fosbury, who would bring the high jump into the next century. Taking advantage of the raised, softer landing areas by then in use, Fosbury added a new twist to the outmoded Eastern Cut-off. He directed himself over the bar head and shoulders first, sliding over on his back and landing in a fashion which would likely have broken his neck in the old, sawdust landing pits. After he used this Fosbury flop to win the 1968 Olympic gold medal, the technique began to spread around the world, and soon "floppers" were dominating international high jump competitions. The last straddler to set a world record was Vladimir Yashchenko, who cleared in 1977 and then indoors in 1978.
Among renowned high jumpers following Fosbury's lead were Americans Dwight Stones and his rival, tall Franklin Jacobs of Paterson, NJ, who cleared , over his head (a feat equaled 27 years later by Sweden's Stefan Holm); Chinese record-setters Ni-chi Chin and Zhu Jianhua; Germans Gerd Wessig and Dietmar Mögenburg; Swedish Olympic medalist and world record holder Patrik Sjöberg; and female jumpers Iolanda Balaş of Romania, Ulrike Meyfarth of Germany and Italy's Sara Simeoni.
Technical aspects.
The approach.
The approach of the high jump may actually be more important than the take-off. If a high jumper runs with bad timing or without enough aggression, clearing a high bar becomes more of a challenge. The approach requires a certain shape or curve, the right amount of speed, and the correct number of strides. The approach angle is also critical for optimal height.
Most great straddle jumpers have a run at angles of about 30 to 40 degrees. The length of the run is determined by the speed of the person's approach. A slower run requires about 8 strides. However, a faster high jumper might need about 13 strides. A greater run speed allows a greater part of the body's forward momentum to be converted upward .
The J type approach, favored by Fosbury floppers, allows for horizontal speed, the ability to turn in the air (centripetal force), and good take-off position. The approach should be a hard controlled stride so that a person does not fall from creating an angle with speed. Athletes should run tall and lean on the curve, from the ankles and not the hips. 
The take-off.
Unlike the classic straddle technique, where the take-off foot is "planted" in the same spot at every height, flop-style jumpers must adjust their take-off as the bar is raised. Their J approach run must be adjusted slightly so that their take-off spot is slightly further out from the bar in order to allow their hips to clear the bar while still maintaining enough momentum to carry their legs across the bar. Jumpers attempting to reach record heights commonly fail when most of their energy is directed into the vertical effort, and they brush the bar off the standards with the backs of their legs as they stall out in mid-air.
An effective approach shape can be derived from physics. For example, the rate of backward spin required as the jumper crosses the bar to facilitate shoulder clearance on the way up and foot clearance on the way down can be determined by computer simulation. This rotation rate can be back-calculated to determine the required angle of lean away from the bar at plant, based on how long the jumper is on the take-off foot. This information, together with the jumper's speed in the curve, can be used to calculate the radius of the curved part of the approach. This is a lot of work and requires measurements of running speed and time of take-off foot on the ground. However, one can work in the opposite direction by assuming an approach radius and watching the resulting backward rotation. This only works if some basic rules are followed in how one executes the approach and take-off. 
Drills can be practiced to solidify the approach. One drill is to run in a straight line (the linear part of the approach) and then run two to three circles spiraling into one another. Another is to run or skip a circle of any size, two to three times in a row. It is important to train to leap upwards without first leaning into the bar, allowing the momentum of the J approach to carry the body across the bar.
Winner declaration.
In competition the winner is the person who cleared the highest height. In case of a tie, fewer failed attempts at that height are better: "i.e.", the jumper who makes a height on his or her first attempt is placed ahead of someone who clears the same height on the second or third attempt. If there still is a tie, all the failed attempts at lower heights are added up, and the one with the fewest total misses is declared the winner. If still tied, a playoff is held. Starting height is the next higher height after the overjumped one. If all the competitors clear the height, the bar is raised , and if they fail, the bar is lowered 2 cm. That continues until only one competitor succeeds in overjumping that height, and he or she is declared the winner.
Athletes with most medals.
Athletes who have won multiple titles at the two most important competitions, the Olympic Games and the World Championships:
Kostadinova and Sotomayor are the only high jumpers to have been Olympic Champion, World Champion and broken the world record.
Season's bests.
As of June 5, 2015
Height differentials.
All time lists of athletes with the highest recorded jumps above their own height.
Female two metres club.
, 65 different female athletes had ever been able to jump . The following table shows the only ten countries from which more than one athlete has cleared that mark.
National records.
Updated April 2016.

</doc>
<doc id="13792" url="https://en.wikipedia.org/wiki?curid=13792" title="Heraclitus">
Heraclitus

Heraclitus of Ephesus (; , ; c. 535 – c. 475 BCE) was a pre-Socratic Greek philosopher, a native of the Greek city Ephesus, Ionia, on the coast of Asia Minor. He was of distinguished parentage. Little is known about his early life and education, but he regarded himself as self-taught and a pioneer of wisdom. From the lonely life he led, and still more from the apparently riddled and allegedly paradoxical nature of his philosophy and his stress upon the needless unconsciousness of humankind, he was called "The Obscure" and the "Weeping Philosopher".
Heraclitus was famous for his insistence on ever-present change as being the fundamental essence of the universe, as stated in the famous saying, "No man ever steps in the same river twice" (see panta rhei, below). This position was complemented by his stark commitment to a unity of opposites in the world, stating that "the path up and down are one and the same". Through these doctrines Heraclitus characterized all existing entities by pairs of contrary properties, whereby no entity may ever occupy a single state at a single time. This, along with his cryptic utterance that "all entities come to be in accordance with this "Logos"" (literally, "word", "reason", or "account") has been the subject of numerous interpretations.
Life.
The main source for the life of Heraclitus is Diogenes Laërtius, although some have questioned the validity of his account as ""a tissue of Hellenistic anecdotes, most of them obviously fabricated on the basis of statements in the preserved fragments"." Diogenes said that Heraclitus flourished in the 69th Olympiad, 504–501 BCE. All the rest of the evidence — the people Heraclitus is said to have known, or the people who were familiar with his work — confirms the "floruit". His dates of birth and death are based on a life span of 60 years, the age at which Diogenes says he died, with the floruit in the middle.
Heraclitus was born to an aristocratic family in Ephesus, Anatolia, in what is now called present-day Efes, Turkey. His father was named either Blosôn or Herakôn. Diogenes says that he abdicated the kingship ("basileia") in favor of his brother and Strabo confirms that there was a ruling family in Ephesus descended from the Ionian founder, Androclus, which still kept the title and could sit in the chief seat at the games, as well as a few other privileges. How much power the king had is another question. Ephesus had been part of the Persian Empire since 547 and was ruled by a satrap, a more distant figure, as the Great King allowed the Ionians considerable autonomy. Diogenes says that Heraclitus used to play knucklebones with the youths in the temple of Artemis and when asked to start making laws he refused saying that the constitution ("politeia") was "ponêra", which can mean either that it was fundamentally wrong or that he considered it toilsome. Two extant letters between Heraclitus and Darius I, quoted by Diogenes, are undoubtedly later forgeries.
With regard to education, Diogenes says that Heraclitus was "wondrous" ("thaumasios", which, as Plato explains in the "Theaetetus" and elsewhere, is the beginning of philosophy) from childhood. Diogenes relates that Sotion said he was a "hearer" of Xenophanes, which contradicts Heraclitus' statement (so says Diogenes) that he had taught himself by questioning himself. Burnet states in any case that "... Xenophanes left Ionia before Herakleitos was born." Diogenes relates that as a boy Heraclitus had said he "knew nothing" but later claimed to "know everything." His statement that he "heard no one" but "questioned himself," can be placed alongside his statement that "the things that can be seen, heard and learned are what I prize the most."
Diogenes relates that Heraclitus had a poor opinion of human affairs. He believed that Hesiod and Pythagoras lacked understanding though learned and that Homer and Archilochus deserved to be beaten. Laws needed to be defended as though they were city walls. Timon is said to have called him a "mob-reviler." Heraclitus hated the Athenians and his fellow Ephesians, wishing the latter wealth in punishment for their wicked ways. Says Diogenes: "Finally, he became a hater of his kind ("misanthrope") and wandered the mountains ... making his diet of grass and herbs."
Heraclitus' life as a philosopher was interrupted by dropsy. The physicians he consulted were unable to prescribe a cure. Diogenes lists various stories about Heraclitus' death: In two versions, Heraclitus was cured of the dropsy and died of another disease. In one account, however, the philosopher "buried himself in a cowshed, expecting that the noxious damp humour would be drawn out of him by the warmth of the manure", while another says he treated himself with a liniment of cow manure and, after a day prone in the sun, died and was interred in the marketplace. According to Neathes of Cyzicus, after smearing himself with dung, Heraclitus was devoured by dogs.
Works.
Diogenes states that Heraclitus' work was "a continuous treatise "On Nature", but was divided into three discourses, one on the universe, another on politics, and a third on theology." Theophrastus says (in Diogenes) "...some parts of his work half-finished, while other parts [made a strange medley."
Diogenes also tells us that Heraclitus deposited his book as a dedication in the great temple of Artemis, the Artemisium, one of the largest temples of the 6th century BCE and one of the Seven Wonders of the Ancient World. Ancient temples were regularly used for storing treasures, and were open to private individuals under exceptional circumstances; furthermore, many subsequent philosophers in this period refer to the work. Says Kahn: "Down to the time of Plutarch and Clement, if not later, the little book of Heraclitus was available in its original form to any reader who chose to seek it out." Diogenes says: "the book acquired such fame that it produced partisans of his philosophy who were called Heracliteans."
As with other pre-Socratics, his writings survive now only in fragments quoted by other authors.
Ancient characterizations.
"The Obscure".
At some time in antiquity he acquired this epithet denoting that his major sayings were difficult to understand. According to Diogenes Laërtius, Timon of Phlius called him "the riddler" ( "ainiktēs"), and explained that Heraclitus wrote his book "rather unclearly" ("asaphesteron") so that only the "capable" should attempt it. By the time of Cicero he had become "the dark" ( — ) because he had spoken "nimis obscurē", "too obscurely", concerning nature and had done so deliberately in order to be misunderstood. The customary English translation of follows the Latin, "the Obscure."
The "weeping philosopher".
Diogenes Laërtius ascribes the theory that Heraclitus did not complete some of his works because of melancholia to Theophrastus. Later he was referred to as the "weeping philosopher," as opposed to Democritus, who is known as the "laughing philosopher." If Stobaeus writes correctly, Sotion in the early 1st century CE was already combining the two in the imaginative duo of weeping and laughing philosophers: "Among the wise, instead of anger, Heraclitus was overtaken by tears, Democritus by laughter." The view is expressed by the satirist Juvenal:
The motif was also adopted by Lucian of Samosata in his "Sale of Creeds," in which the duo is sold together as a complementary product in the satirical auction of philosophers. Subsequently they were considered an indispensable feature of philosophic landscapes. Montaigne proposed two archetypical views of human affairs based on them, selecting Democritus' for himself. The weeping philosopher may have been mentioned in William Shakespeare's "The Merchant of Venice". Donato Bramante painted a fresco, "Democritus and Heraclitus," in Casa Panigarola in Milan.
Philosophy.
"Logos".
"The idea that all things come to pass in accordance with this "Logos"" and "the "Logos" is common," is expressed in two famous but obscure fragments:
This "Logos" holds always but humans always prove unable to understand it, both before hearing it and when they have first heard it. For though all things come to be in accordance with this "Logos", humans are like the inexperienced when they experience such words and deeds as I set out, distinguishing each in accordance with its nature and saying how it is. But other people fail to notice what they do when awake, just as they forget what they do while asleep. (DK 22B1)
For this reason it is necessary to follow what is common. But although the "Logos" is common, most people live as if they had their own private understanding. (DK 22B2)
The meaning of "Logos" also is subject to interpretation: "word", "account", "principle", "plan", "formula", "measure", "proportion", "reckoning." Though Heraclitus "quite deliberately plays on the various meanings of "logos"", there is no compelling reason to suppose that he used it in a special technical sense, significantly different from the way it was used in ordinary Greek of his time.
The later Stoics understood it as "the account which governs everything," and Hippolytus, in the 3rd century CE, identified it as meaning the Christian "Word of God".
"Panta rhei", "everything flows".
The phrase ("panta rhei") "everything flows" either was spoken by Heraclitus or survived as a quotation of his. This famous aphorism used to characterize Heraclitus' thought comes from Simplicius, a neoplatonist, and from Plato's "Cratylus". The word "rhei" (cf. rheology) is the Greek word for "to stream", and is etymologically related to Rhea according to Plato's "Cratylus".
The philosophy of Heraclitus is summed up in his cryptic utterance: 
<br>"Potamoisi toisin autoisin embainousin, hetera kai hetera hudata epirrei"<br>"Ever-newer waters flow on those who step into the same rivers." 
The quote from Heraclitus appears in Plato's "Cratylus" twice; in 401d as:
<br>"Ta onta ienai te panta kai menein ouden"<br>"All entities move and nothing remains still"and in 402a
"Everything changes and nothing remains still ... and ... you cannot step twice into the same stream"
Instead of "flow" Plato uses "chōrei", "to change place" (χῶρος "chōros").
The assertions of flow are coupled in many fragments with the enigmatic river image:
<br>"We both step and do not step in the same rivers. We are and are not."
Compare with the Latin adages "Omnia mutantur" and "Tempora mutantur" () and the Japanese tale "Hōjōki," () which contains the same image of the changing river, and the central Buddhist doctrine of impermanence.
However, the German classicist and philosopher Karl-Martin Dietz interprets this fragment as an indication by Heraclitus, for the world as a steady constant: "You will not find anything, in which the river remains constant. ... Just the fact, that there is a particular river bed, that there is a source and a estuary etc. is something, that stays identical. And this is ... the concept of a river"
"Hodos ano kato", "the way up and the way down".
In the structure "anō katō" is more accurately translated as a hyphenated word: "the upward-downward path." They go on simultaneously and instantaneously and result in "hidden harmony". A way is a series of transformations: the , "turnings of fire," first into sea, then half of sea to earth and half to rarefied air.
The transformation is a replacement of one element by another: "The death of fire is the birth of air, and the death of air is the birth of water."
This world, which is the same for all, no one of gods or men has made. But it always was and will be: an ever-living fire, with measures of it kindling, and measures going out. This latter phraseology is further elucidated:All things are an interchange for fire, and fire for all things, just like goods for gold and gold for goods.
Heraclitus considered fire as the most fundamental element. He believed fire gave rise to the other elements and thus to all things. He regarded the soul as being a mixture of fire and water, with fire being the noble part of the soul, and water the ignoble part. A soul should therefore aim toward becoming more full of fire and less full of water: a "dry" soul was best. According to Heraclitus, worldly pleasures made the soul "moist", and he considered mastering one's worldly desires to be a noble pursuit which purified the soul's fire. Norman Melchert interpreted Heraclitus as using "fire" metaphorically, in lieu of "Logos", as the origin of all things.
"Dike eris", "strife is justice".
If objects are new from moment to moment so that one can never touch the same object twice, then each object must dissolve and be generated continually momentarily and an object is a harmony between a building up and a tearing down. Heraclitus calls the oppositional processes ἔρις "eris", "strife", and hypothesizes that the apparently stable state, δίκη "dikê", or "justice," is a harmony of it:We must know that war (πόλεμος "polemos") is common to all and strife is justice, and that all things come into being through strife necessarily.
As Diogenes explains:All things come into being by conflict of opposites, and the sum of things (τὰ ὅλα "ta hola", "the whole") flows like a stream.
In the bow metaphor Heraclitus compares the resultant to a strung bow held in shape by an equilibrium of the string tension and spring action of the bow:There is a harmony in the bending back (παλίντροπος "palintropos") as in the case of the bow and the lyre.
"Hepesthai to koino", "follow the common".
People must "follow the common" ( "hepesthai tō koinō") and not live having "their own judgement ("phronēsis")". He distinguishes between human laws and divine law (τοῦ θείου "tou theiou" "of God"). By "God" Heraclitus does not mean the Judeo-Christian version of a single God as primum mobile of all things, God as Creator, but the divine as opposed the human, the immortal (which we tend to confuse with the "eternal") as opposed to the mortal, the cyclical as opposed to the transient. It is more accurate to speak of "the god" and not of "God".
He removes the human sense of justice from his concept of God; i.e., humanity is not the image of God: "To God all things are fair and good and just, but people hold some things wrong and some right." God's custom has wisdom but human custom does not, and yet both humans and God are childish (inexperienced): "human opinions are children's toys" and "Eternity is a child moving counters in a game; the kingly power is a child's."
Wisdom is "to know the thought by which all things are steered through all things", which must not imply that people are or can be wise. Only Zeus is wise. To some degree then Heraclitus seems to be in the mystic's position of urging people to follow God's plan without much of an idea what that may be. In fact there is a note of despair: "The fairest universe ( "kallistos kosmos") is but a heap of rubbish ( "sarma", sweepings) piled up (κεχυμένον "kechumenon", i.e. "poured out") at random ( "eikê", "aimlessly")."
"Ethos anthropoi daimon", "character is fate".
This influential quote by Heraclitus "ἦθος ἀνθρώπῳ δαίμων" (DK 22B119) has led to numerous interpretations. Whether in this context, "daimon" can indeed be translated to mean "fate" is disputed, however, it lends much sense to Heraclitus' observations and conclusions about human nature in general. While the translation with "fate" is generally accepted as in Kahn's "a man's character is his divinity", in some cases, it may also stand for the soul of the departed.
Influence.
Plato.
In Heraclitus a perceived object is a harmony between two fundamental units of change, a waxing and a waning. He typically uses the ordinary word "to become" ("gignesthai" or "ginesthai", present tense or aorist tense of the verb, with the root sense of "being born"), which led to his being characterized as the philosopher of becoming rather than of being. He recognizes the fundamental changing of objects with the flow of time.
Plato argues against Heraclitus as follows:How can that be a real thing which is never in the same state? ... for at the moment that the observer approaches, then they become other ... so that you cannot get any further in knowing their nature or state ... but if that which knows and that which is known exist ever ... then I do not think they can resemble a process or flux ...
In Plato one experienced unit is a state, or object existing, which can be observed. The time parameter is set at "ever"; that is, the state is to be presumed present between observations. Change is to be deduced by comparing observations and is thus presumed a function that "happens to" objects already in being, rather than something ontologically essential to them (such that something that does not change cannot exist) as in Heraclitus. In Plato, no matter how many of those experienced units you are able to tally, you cannot get through the mysterious gap between them to account for the change that must be occurring there. This limitation is considered a fundamental limitation of reality by Plato and in part underpins his differentiation between imperfect experience from more perfect Forms. The fact that this is no limitation for Heraclitus motivates Plato's condemnation. 
Stoics.
Stoicism was a philosophical school which flourished between the 3rd century BCE and about the 3rd century CE. It began among the Greeks and became the major philosophy of the Roman Empire before declining with the rise of Christianity in the 3rd century.
Throughout their long tenure the Stoics believed that the major tenets of their philosophy derived from the thought of Heraclitus. According to Long, "the importance of Heraclitus to later Stoics is evident most plainly in Marcus Aurelius." Explicit connections of the earliest Stoics to Heraclitus showing how they arrived at their interpretation are missing but they can be inferred from the Stoic fragments, which Long concludes are "modifications of Heraclitus."
The Stoics were interested in Heraclitus' treatment of fire. In addition to seeing it as the most fundamental of the four elements and the one that is quantified and determines the quantity ("logos") of the other three, he presents fire as the cosmos, which was not made by any of the gods or men, but "was and is and ever shall be ever-living fire." Fire is both a substance and a motivator of change, it is active in altering other things quantitatively and performing an activity Heraclitus describes as "the judging and convicting of all things." It is "the thunderbolt that steers the course of all things." There is no reason to interpret the judgement, which is actually "to separate" (κρίνειν "krinein"), as outside of the context of "strife is justice" (see subsection above).
The earliest surviving Stoic work, the "Hymn to Zeus" of Cleanthes, though not explicitly referencing Heraclitus, adopts what appears to be the Heraclitean logos modified. Zeus rules the universe with law ("nomos") wielding on its behalf the "forked servant", the "fire" of the "ever-living lightning." So far nothing has been said that differs from the Zeus of Homer. But then, says Cleanthes, Zeus uses the fire to "straighten out the common logos" that travels about ("phoitan", "to frequent") mixing with the greater and lesser lights (heavenly bodies). This is Heraclitus' logos, but now it is confused with the "common "nomos"", which Zeus uses to "make the wrong ("perissa", left or odd) right ("artia", right or even)" and "order ("kosmein") the disordered ("akosma")."
The Stoic modification of Heraclitus' idea of the Logos was also influential on Jewish philosophers such as Philo of Alexandria, who connected it to "Wisdom personified" as God's creative principle. Philo uses the term Logos throughout his treatises on Hebrew Scripture in a manner clearly influenced by the Stoics.
Church fathers.
The church fathers were the leaders of the early Christian Church during its first five centuries of existence, roughly contemporaneous to Stoicism under the Roman Empire. The works of dozens of writers in hundreds of pages have survived.
All of them had something to say about the Christian form of the Logos. The Catholic Church found it necessary to distinguish between the Christian logos and that of Heraclitus as part of its ideological distancing from paganism. The necessity to convert by defeating paganism was of paramount importance. Hippolytus of Rome therefore identifies Heraclitus along with the other Pre-Socratics (and Academics) as sources of heresy. Church use of the methods and conclusions of ancient philosophy as such was as yet far in the future, even though many were converted philosophers.
In "Refutation of All Heresies" Hippolytus says: "What the blasphemous folly is of Noetus, and that he devoted himself to the tenets of Heraclitus the Obscure, not to those of Christ." Hippolytus then goes on to present the inscrutable DK B67: "God ("theos") is day and night, winter and summer, ... but he takes various shapes, just as fire, when it is mingled with spices, is named according to the savor of each." The fragment seems to support pantheism if taken literally. German physicist and philosopher Max Bernard Weinstein classed these views with pandeism.
Hippolytus condemns the obscurity of it. He cannot accuse Heraclitus of being a heretic so he says instead: "Did not (Heraclitus) the Obscure anticipate Noetus in framing a system ...?" The apparent pantheist deity of Heraclitus (if that is what DK B67 means) must be equal to the union of opposites and therefore must be corporeal and incorporeal, divine and not-divine, dead and alive, etc., and the Trinity can only be reached by some sort of illusory shape-shifting.
The Apologist Justin Martyr, however, took a much more positive view of him. In his First Apology, he said both Socrates and Heraclitus were Christians before Christ: "those who lived reasonably are Christians, even though they have been thought atheists; as, among the Greeks, Socrates and Heraclitus, and men like them." 
See also.
The following articles on other topics contain non-trivial information that relates to Heraclitus in some way.

</doc>
<doc id="13793" url="https://en.wikipedia.org/wiki?curid=13793" title="Harrison Schmitt">
Harrison Schmitt

Harrison Hagan "Jack" Schmitt (born July 3, 1935) is an American geologist, retired NASA astronaut, university professor and former U.S. senator from New Mexico.
In December 1972, as one of the crew on board Apollo 17, Schmitt became the first member of NASA's first scientist-astronaut group to fly in space. As Apollo 17 was the last of the Apollo missions, he also became the twelfth person to set foot on the Moon, and , the second-to-last person to step off of the Moon (he boarded the Lunar Module shortly before commander Eugene Cernan). Schmitt also remains the first and only professional scientist to have flown beyond low Earth orbit and to have visited the Moon. He was influential within the community of geologists supporting the Apollo program and, before starting his own preparations for an Apollo mission, had been one of the scientists training those Apollo astronauts chosen to visit the lunar surface.
Schmitt resigned from NASA in August 1975 in order to run for election to the United States Senate as a member from New Mexico. As the Republican candidate in the 1976 election, he defeated the two-term Democrat incumbent Joseph Montoya, but, running for re-election in 1982, was himself defeated, by Democrat Jeff Bingaman.
Biography.
Early life and education.
Born in Santa Rita, New Mexico, Schmitt grew up in nearby Silver City, and he is a graduate of the Western High School (class of 1953). He received a B.S. degree in geology from the California Institute of Technology in 1957 and then spent a year studying geology at the University of Oslo in Norway. He received a Ph.D. in geology from Harvard University in 1964, based on his geological field studies in Norway.
NASA career.
Before joining NASA as a member of the first group of scientist-astronauts in June 1965, he worked at the U.S. Geological Survey's Astrogeology Center at Flagstaff, Arizona, developing geological field techniques that would be used by the Apollo crews. Following his selection, Schmitt spent his first year at Air Force UPT learning to become a jet pilot. Upon his return to the astronaut corps in Houston, he played a key role in training Apollo crews to be geologic observers when they were in lunar orbit and competent geologic field workers when they were on the lunar surface. After each of the landing missions, he participated in the examination and evaluation of the returned lunar samples and helped the crews with the scientific aspects of their mission reports.
Schmitt spent considerable time becoming proficient in the CSM and LM systems. In March 1970 he became the first of the scientist-astronauts to be assigned to space flight, joining Richard F. Gordon, Jr. (Commander) and Vance Brand (Command Module Pilot) on the Apollo 15 backup crew. The flight rotation put these three in line to fly as prime crew on the third following mission, Apollo 18. When Apollo flights 18 and 19 were cancelled in September 1970, the community of lunar geologists supporting Apollo felt so strongly about the need to land a professional geologist on the Moon, that they pressured NASA to reassign Schmitt to a remaining flight. As a result, Schmitt was assigned in August 1971 to fly on the last mission, Apollo 17, replacing Joe Engle as Lunar Module Pilot. Schmitt landed on the Moon with commander Gene Cernan in December 1972.
Schmitt claims to have taken the photograph of the Earth known as "The Blue Marble", one of the most widely distributed photographic images in existence. (NASA officially credits the image to the entire Apollo 17 crew.)
While on the Moon's surface, Schmitt — the only geologist in the astronaut corps — collected the rock sample designated Troctolite 76535, which has been called "without doubt the most interesting sample returned from the Moon". Among other distinctions, it is the central piece of evidence suggesting that the Moon once possessed an active magnetic field.
As he returned to the Lunar Module before Cernan, Schmitt is the next-to-last person to have walked on the Moon's surface.
After the completion of Apollo 17, Schmitt played an active role in documenting the Apollo geologic results and also took on the task of organizing NASA's Energy Program Office.
Post-NASA career.
In August 1975, Schmitt resigned from NASA to seek election as a Republican to the United States Senate representing New Mexico in the 1976 election. Schmitt faced two-term Democratic incumbent, Joseph Montoya, whom he defeated 57% to 42%. He served one term and, notably, was the ranking Republican member of the Science, Technology, and Space Subcommittee. He sought a second term in the 1982, but due to a deep recession and concerns that he was not paying attention to local matters, he was defeated in a re-election bid by the state Attorney General Jeff Bingaman by a 54% to 46% margin. Bingaman's campaign slogan asked, "What on Earth has he done for you lately?" Following his Senate term, Schmitt has been a consultant in business, geology, space, and public policy.
During his term in the Senate, Schmitt sat at the chamber's candy desk.
Schmitt is an adjunct professor of engineering physics at the University of Wisconsin–Madison, and has long been a proponent of lunar resource utilization. In 1997 he proposed the Interlune InterMars Initiative, listing among its goals the advancement of private sector acquisition and use of lunar resources, particularly lunar helium-3 as a fuel for notional nuclear fusion reactors.
Schmitt was chair of the NASA Advisory Council, whose mandate is to provide technical advice to the NASA Administrator, from November 2005 until his abrupt resignation on October 16, 2008. In November 2008, he quit the Planetary Society over policy advocacy differences, citing the organization's statements on "focusing on Mars as the driving goal of human spaceflight" (Schmitt said that going back to the Moon would speed progress toward a manned Mars mission), on "accelerating research into global climate change through more comprehensive Earth observations" (Schmitt voiced objections to the notion of a present "scientific consensus" on climate change as any policy guide), and on international cooperation (which he felt would retard rather than accelerate progress), among other points of divergence.
In January, 2011, he was appointed as Secretary of the New Mexico Energy, Minerals and Natural Resources Department in the cabinet of Governor Susana Martinez, but was forced to give up the appointment the following month after refusing to submit to a required background investigation. "El Paso Times" called him the "most celebrated" candidate for New Mexico energy secretary.
Schmitt wrote a book entitled "Return to the Moon: Exploration, Enterprise, and Energy in the Human Settlement of Space" in 2006.
He lives in Silver City, New Mexico, and spends some of his summer at his northern Minnesota lake cabin.
Senator Schmitt is also involved in several civic projects, including the improvement of the Senator Harrison H. Schmitt Big Sky Hang Glider Park in Albuquerque, NM.
Views on global warming.
Schmitt's view on climate change diverges from the prevailing scientific opinion on climate change as he emphasizes natural over human factors as driving climate. In repeated comments, Schmitt has expressed his view that the risks posed by climate change are overrated, and suggests instead that climate change is a tool for people who are trying to increase the size of government. He resigned his membership in the Planetary Society because of its stance on the subject, writing in his resignation letter that the "global warming scare is being used as a political tool to increase government control over American lives, incomes and decision-making." He spoke at the March 2009 International Conference on Climate Change sponsored by the Heartland Institute. He appeared in December that year on the Fox Business Network, saying "he CO2 scare is a red herring".
In a 2009 interview with libertarian talk-radio host Alex Jones, Schmitt asserted a link between Soviet Communism and the American environmental movement: "I think the whole trend really began with the fall of the Soviet Union. Because the great champion of the opponents of liberty, namely communism, had to find some other place to go and they basically went into the environmental movement." At the Heartland Institute's sixth International Conference on Climate Change Schmitt said that climate change was a stalking horse for National Socialism.
Schmitt co-authored a May 8, 2013 "Wall Street Journal" opinion column with William Happer, contending that increasing levels of carbon dioxide in the atmosphere are not significantly correlated with global warming, attributing the "single-minded demonization of this natural and essential atmospheric gas" to advocates of government control of energy production. Noting a positive relationship between crop resistance to drought and increasing carbon dioxide levels, the authors argued, "Contrary to what some would have us believe, increased carbon dioxide in the atmosphere will benefit the increasing population on the planet by increasing agricultural productivity."
Media.
Schmitt is one of the astronauts featured in the documentary "In the Shadow of the Moon". He also contributed to the book "NASA's Scientist-Astronauts" by David Shayler and Colin Burgess.

</doc>
<doc id="13795" url="https://en.wikipedia.org/wiki?curid=13795" title="Hilaire Rouelle">
Hilaire Rouelle

Hilaire Marin Rouelle (February 1718, Mathieu–April 7, 1779, Paris) was a French chemist. Commonly cited as the 1773 discoverer of urea, he was not the first to do so. Dutch scientist Herman Boerhaave had discovered this chemical as early as 1727. Rouelle is known as "le cadet" (the younger) to distinguish him from his older brother, Guillaume-François Rouelle, who was also a chemist.

</doc>
<doc id="13798" url="https://en.wikipedia.org/wiki?curid=13798" title="Halon">
Halon

Halon may refer to:

</doc>
<doc id="13800" url="https://en.wikipedia.org/wiki?curid=13800" title="Harrisonburg">
Harrisonburg

Harrisonburg may refer to a place in the United States:

</doc>
<doc id="13802" url="https://en.wikipedia.org/wiki?curid=13802" title="Hammer">
Hammer

A hammer is a tool that delivers a blow (a sudden impact) to an object. Most hammers are hand tools used to drive nails, fit parts, forge metal, and break apart objects. Hammers vary in shape, size, and structure, depending on their purposes.
Hammers are basic tools in many trades. The usual features are a head (most often made of steel) and a handle (also called a helve or haft). Most hammers are hand tools, but there are also many powered versions, called power hammers (such as steam hammers and trip hammers) for heavier uses, such as forging.
Some hammers have other names, such as "sledgehammer", "mallet" and "gavel". The term "hammer" also applies to other devices that deliver blows, such as the hammer of a firearm or the hammer of a piano.
History.
The use of simple hammers dates to about 2,600,000 BCE when various shaped stones were used to strike wood, bone, or other stones to break them apart and shape them. Stones attached to sticks with strips of leather or animal sinew were being used as hammers with handles by about 30,000 BCE during the middle of the Paleolithic Stone Age.
The hammer's archeological record shows that it may be the oldest tool for which definite evidence exists of its early existence.
Construction and materials.
A traditional hand-held hammer consists of a separate head and a handle, fastened together by means of a special wedge made for the purpose, or by glue, or both. This two-piece design is often used, to combine a dense metallic striking head with a non-metallic mechanical-shock-absorbing handle (to reduce user fatigue from repeated strikes). If wood is used for the handle, it is often hickory or ash, which are tough and long-lasting materials that can dissipate shock waves from the hammer head. Rigid fiberglass resin may be used for the handle; this material does not absorb water or decay, but does not dissipate shock as well as wood.
A loose hammer head is hazardous because it can literally "fly off the handle" when in use, becoming a dangerous uncontrolled missile. Wooden handles can often be replaced when worn or damaged; specialized kits are available covering a range of handle sizes and designs, plus special wedges for attachment.
Some hammers are one-piece designs made primarily of a single material. A one-piece metallic hammer may optionally have its handle coated or wrapped in a resilient material such as rubber, for improved grip and reduced user fatigue.
The hammer head may be surfaced with a variety of materials, including brass, bronze, wood, plastic, rubber, or leather. Some hammers have interchangeable striking surfaces, which can be selected as needed or replaced when worn out.
Designs and variations.
A large hammer-like tool is a "maul" (commander, beetle), a wood or rubber headed hammer is a "mallet", and a hammer-like tool with a cutting blade is usually called a "hatchet". The essential part of a hammer is the head, a compact solid mass that is able to deliver the blow to the intended target without itself deforming. The opposite side may have a ball shape, as in the ball-peen hammer. Some upholstery hammers have a magnetized face, to pick up tacks. In the hatchet, the hammer head is secondary to the cutting edge of the tool.
The impact between steel hammer heads and the objects being hit can create sparks, which may ignite flammable or explosive gases. These are a hazard in some industries such as underground coal mining (methane gas), or in other hazardous environments such as petroleum refineries and chemical plants. In these environments, a variety of non-sparking metal tools are used, primarily aluminium or beryllium copper hammers.
In recent years, the handles have been made of durable plastic or rubber, though wood is still widely used.
Mechanically-powered hammers.
Mechanically-powered hammers often look quite different from the hand tools, but nevertheless most of them work on the same principle. They include:
In professional framing carpentry, the manual hammer has almost been completely replaced by the nail gun. In professional upholstery, its chief competitor is the staple gun.
Physics of hammering.
Hammer as a force amplifier.
A hammer is basically a force amplifier that works by converting mechanical work into kinetic energy and back.
In the swing that precedes each blow, the hammer head stores a certain amount of kinetic energy—equal to the length "D" of the swing times the force "f" produced by the muscles of the arm and by gravity. When the hammer strikes, the head is stopped by an opposite force coming from the target, equal and opposite to the force applied by the head to the target. If the target is a hard and heavy object, or if it is resting on some sort of anvil, the head can travel only a very short distance "d" before stopping. Since the stopping force "F" times that distance must be equal to the head's kinetic energy, it follows that "F" is much greater than the original driving force "f"—roughly, by a factor "D"/"d". In this way, great strength is not needed to produce a force strong enough to bend steel, or crack the hardest stone.
Effect of the head's mass.
The amount of energy delivered to the target by the hammer-blow is equivalent to one half the mass of the head times the square of the head's speed at the time of impact (formula_1). While the energy delivered to the target increases linearly with mass, it increases quadratically with the speed (see the effect of the handle, below). High tech titanium heads are lighter and allow for longer handles, thus increasing velocity and delivering the same energy with less arm fatigue than that of a heavier steel head hammer. A titanium head has about 3% recoil energy and can result in greater efficiency and less fatigue when compared to a steel head with up to 30% recoil. Dead blow hammers use special rubber or steel shot to absorb recoil energy, rather than bouncing the hammer head after impact. 
Effect of the handle.
The handle of the hammer helps in several ways. It keeps the user's hands away from the point of impact. It provides a broad area that is better-suited for gripping by the hand. Most importantly, it allows the user to maximize the speed of the head on each blow. The primary constraint on additional handle length is the lack of space to swing the hammer. This is why sledge hammers, largely used in open spaces, can have handles that are much longer than a standard carpenter's hammer. The second most important constraint is more subtle. Even without considering the effects of fatigue, the longer the handle, the harder it is to guide the head of the hammer to its target at full speed.
Most designs are a compromise between practicality and energy efficiency. With too long a handle, the hammer is inefficient because it delivers force to the wrong place, off-target. With too short a handle, the hammer is inefficient because it doesn't deliver enough force, requiring more blows to complete a given task. Modifications have also been made with respect to the effect of the hammer on the user. Handles made of shock-absorbing materials or varying angles attempt to make it easier for the user to continue to wield this age-old device, even as nail guns and other powered drivers encroach on its traditional field of use.
As hammers must be used in many circumstances, where the position of the person using them cannot be taken for granted, trade-offs are made for the sake of practicality. In areas where one has plenty of room, a long handle with a heavy head (like a sledge hammer) can deliver the maximum amount of energy to the target. It is not practical to use such a large hammer for all tasks, however, and thus the overall design has been modified repeatedly to achieve the optimum utility in a wide variety of situations.
Effect of gravity.
Gravity exerts a force on the hammer head. If hammering downwards, gravity increases the acceleration during the hammer stroke and increases the energy delivered with each blow. If hammering upwards, gravity reduces the acceleration during the hammer stroke and therefore reduces the energy delivered with each blow. Some hammering methods, such as traditional mechanical pile drivers, rely entirely on gravity for acceleration on the down stroke.
Ergonomics and injury risks.
Both manual and powered hammers can cause peripheral neuropathy or a variety of other ailments when used improperly. Awkward handles can cause repetitive stress injury (RSI) to hand and arm joints, and uncontrolled shock waves from repeated impacts can injure nerves and the skeleton.
War hammers.
A war hammer is a late medieval weapon of war intended for close combat action.
Symbolic hammers.
The hammer, being one of the most used tools by "Homo sapiens", has been used very much in symbols such as flags and heraldry. In the Middle Ages, it was used often in blacksmith guild logos, as well as in many family symbols. The hammer and pick is used as a symbol of mining.
A well known symbol with a hammer in it is the Hammer and Sickle, which was the symbol of the former Soviet Union and is very interlinked with communism and early socialism. The hammer in this symbol represents the industrial working class (and the sickle represents the agricultural working class). The hammer is used in some coat of arms in (former) socialist countries like East Germany. Similarly, the Hammer and Sword symbolizes Strasserism, a strand of National Socialism seeking to appeal to the working class.
The gavel, a small wooden mallet, is used to symbolize a mandate to preside over a meeting or judicial proceeding, and a graphic image of one is used as a symbol of legislative or judicial decision-making authority.
In Norse mythology, Thor, the god of thunder and lightning, wields a hammer named Mjölnir. Many artifacts of decorative hammers have been found, leading modern practitioners of this religion to often wear reproductions as a sign of their faith.
Judah Maccabee, "The Hammer", possibly in recognition of his ferocity in battle. The name Maccabee may derive from the Aramaic "maqqaba". (see ).
In American folkore, the hammer of John Henry represents the strength and endurance of a man.
The hammer in the song "If I Had a Hammer" represents a relentless message of justice broadcast across the land. The song became a symbol of the American Civil Rights movement.

</doc>
<doc id="13804" url="https://en.wikipedia.org/wiki?curid=13804" title="Hiragana">
Hiragana

Hiragana and katakana are both kana systems. With one or two minor exceptions, each sound in the Japanese language (strictly, each mora) is represented by one character (or one digraph) in each system. This may be either a vowel such as ""a"" (hiragana あ); a consonant followed by a vowel such as ""ka"" (か); or ""n"" (ん), a nasal sonorant which, depending on the context, sounds either like English "m", "n", or "ng" (), or like the nasal vowels of French. Because the characters of the kana do not represent single consonants (except in the case of ん "n"), the kana are referred to as syllabaries and not alphabets.
Hiragana is used to write native words for which there are no kanji, including grammatical particles such as "kara" "from". Likewise, hiragana is used to write words whose kanji form is obscure, not known to the writer or readers, or too formal for the writing purpose. There is also some flexibility for words that have common kanji renditions to be optionally written instead in hiragana, according to an individual author's preference. Verb and adjective inflections, as, for example, "be-ma-shi-ta" in , are written in hiragana, often following a verb or adjective root (here, "") that is written in kanji. When Hiragana is used to show the pronunciation of kanji characters as reading aid, it is referred to as "furigana".
There are two main systems of ordering hiragana: the old-fashioned iroha ordering and the more prevalent gojūon ordering.
Writing system.
The modern hiragana syllabary consists of 46 characters (49 if three obsolete characters are included):
These are conceived as a 5×10 grid ("gojūon", , lit. "Fifty Sounds"), as illustrated in the adjacent table, with the extra character being the anomalous singular consonant ん ("N").
Romanisation of the kana does not always strictly follow the consonant-vowel scheme laid out in the table. For example, ち, nominally "ti", is very often romanised as "chi" in an attempt to better represent the actual sound in Japanese.
These basic characters can be modified in various ways. By adding a "dakuten" marker ( ゛), a voiceless consonant is turned into a voiced consonant: "k"→"g", "ts/s"→"z", "t"→"d", "h"→"b" and "ch"/"sh"→"j". Hiragana beginning with an "h" can also add a "handakuten" marker ( ゜) changing the "h" to a "p".
A small version of the hiragana for "ya", "yu" or "yo" (ゃ, ゅ or ょ respectively) may be added to hiragana ending in "i". This changes the "i" vowel sound to a glide (palatalization) to "a", "u" or "o". Addition of the small "y" kana is called yōon. For example, き ("ki") plus ゃ (small "ya") becomes ("kya").
A small "tsu" っ, called a "sokuon", indicates that the following consonant is geminated (doubled). For example, compare "saka" "hill" with "sakka" "author". It also sometimes appears at the end of utterances, where it denotes a glottal stop, as in ( Ouch!). However, it cannot be used to double the "na", "ni", "nu", "ne", "no" syllables' consonants – to double them, the singular "n" (ん) is added in front of the syllable.
Hiragana usually spells long vowels with the addition of a second vowel kana. The chōonpu (long vowel mark) (ー) used in katakana is rarely used with hiragana, for example in the word , "rāmen", but this usage is considered non-standard. In informal writing, small versions of the five vowel kana are sometimes used to represent trailing off sounds ( "haa", "nee"). Standard and voiced iteration marks are written in hiragana as ゝ and ゞ respectively.
Table of hiragana.
The following table shows the complete hiragana together with the Hepburn romanization and IPA transcription in the "gojūon" order. Hiragana with "dakuten" or "handakuten" follow the "gojūon" kana without them, with the "yōon" kana following. Obsolete and normally unused kana are shown in gray. For all syllables besides ん, the pronunciation indicated is for word-initial syllables, for mid-word pronunciations see below.
&#x1B001; ) in pre-Classical Japanese (prior to the advent of kana), but is generally represented for purposes of reconstruction by the kanji 江, and its hiragana form is not present in any known orthography. In modern orthography, "ye" can also be written as いぇ (イェ in katakana).
It's true that in early periods of kana, hiragana and katakana letters for "ye" were used, but soon after the distinction between /ye/ and /e/ went away, and letters and glyphs were not established.
This topic is too much advanced, and it's better to write in Early Middle Japanese.
In the middle of words, the "g" sound (normally ) often turns into a velar nasal and less often (although increasing recently) into the voiced velar fricative . An exception to this is numerals; 15 "juugo" is considered to be one word, but is pronounced as if it was "jū" and "go" stacked end to end: .
Additionally, the "j" sound (normally ) can be pronounced in the middle of words. For example, "sūji" 'number'.
In archaic forms of Japanese, there existed the "kwa" ( ) and "gwa" ( ) digraphs. In modern Japanese, these phonemes have been phased out of usage and only exist in the extended katakana digraphs for approximating foreign language words.
The singular "n" is pronounced before "t", "ch", "ts", "n", "r", "z", "j" and "d", before "m", "b" and "p", before "k" and "g", at the end of utterances, before vowels, palatal approximants ("y"), consonants "s", "sh", "h", "f" and "w", and finally after the vowel "i" if another vowel, palatal approximant or consonant "s", "sh", "h", "f" or "w" follows.
In kanji readings, the diphthongs "ou" and "ei" are today usually pronounced (long o) and (long e) respectively. For example, (lit. "toukyou") is pronounced 'Tokyo', and "sensei" is 'teacher'. However, "tou" is pronounced 'to inquire', because the "o" and "u" are considered distinct, "u" being the infinitive verb ending. Similarly, "shite iru" is pronounced 'is doing'.
For a more thorough discussion on the sounds of Japanese, please refer to Japanese phonology.
Spelling rules.
With a few exceptions for sentence particles は, を, and へ (Normally ha, wo, and he, but instead pronounced as "wa", "o", and "e", respectively), and a few other arbitrary rules, Japanese, when written in kana, is phonemically orthographic, i.e. there is a one-to-one correspondence between kana characters and sounds, leaving only words' pitch accent unrepresented. This has not always been the case: a previous system of spelling, now referred to as historical kana usage, differed substantially from pronunciation; the three above-mentioned exceptions in modern usage are the legacy of that system. The old spelling is referred to as .
There are two hiragana pronounced "ji" (じ and ぢ) and two hiragana pronounced "zu" (ず and づ), but to distinguish them, sometimes "ぢ" is written as "di" and "づ" is written as "dzu". These pairs are not interchangeable. Usually, "ji" is written as じ and "zu" is written as ず. There are some exceptions. If the first two syllables of a word consist of one syllable without a "dakuten" and the same syllable with a "dakuten", the same hiragana is used to write the sounds. For example, "chijimeru" ('to boil down' or 'to shrink') is spelled ちぢめる and "tsudzuku" ('to continue') is . For compound words where the dakuten reflects "rendaku" voicing, the original hiragana is used. For example, "chi" ( 'blood') is spelled ち in plain hiragana. When "hana" ('nose') and "chi" ('blood') combine to make "hanaji" ( 'nose bleed'), the sound of changes from "chi" to "dji". So "hanadji" is spelled according to ち: the basic hiragana used to transcribe . Similarly, "tsukau" (; 'to use') is spelled in hiragana, so "kanazukai" (; 'kana use', or 'kana orthography') is spelled in hiragana.
However, this does not apply when kanji are used phonetically to write words that do not relate directly to the meaning of the kanji (see also ateji). The Japanese word for 'lightning', for example, is "inazuma" (). The component means 'rice plant', is written in hiragana and is pronounced: "ina". The component means 'wife' and is pronounced "tsuma" (つま) when written in isolation—or frequently as "zuma" when it features after another syllable. Neither of these components have anything to do with 'lightning', but together they do when they compose the word for 'lightning'. In this case, the default spelling in hiragana rather than is used.
Officially, ぢ and づ do not occur word-initially pursuant to modern spelling rules. There were words such as "jiban" 'ground' in the historical kana usage, but they were unified under じ in the modern kana usage in 1946, so today it is spelled exclusively . However, "zura" 'wig' (from "katsura") and "zuke" (a sushi term for lean tuna soaked in soy sauce) are examples of word-initial づ today. Some people write the word for hemorrhoids as ぢ (normally じ) for emphasis.
No standard Japanese words begin with the kana ん ("n"). This is the basis of the word game shiritori. ん "n" is normally treated as its own syllable and is separate from the other "n"-based kana ("na", "ni" etc.). A notable exception to this is the colloquial negative verb conjugation; for example "wakaranai" meaning " don't understand" is rendered as "wakaran". It is however not a contraction of the former, but instead comes from the classic negative verb conjugation ぬ "nu" ( "wakaranu").
ん is sometimes directly followed by a vowel ("a", "i", "u", "e" or "o") or a palatal approximant ("ya", "yu" or "yo"). These are clearly distinct from the "na", "ni" etc. syllables, and there are minimal pairs such as "kin'en" 'smoking forbidden', "kinen" 'commemoration', "kinnen" 'recent years'. In Hepburn romanization, they are distinguished with an apostrophe, but not all romanization methods make the distinction. For example, past prime minister Junichiro Koizumi's first name is actually "Jun'ichirō" pronounced 
There are a few hiragana that are rarely used. 𛀁 "ye" is obsolete, and ゐ "wi" and ゑ "we" are obsolete outside of Okinawan dialects. ゔ "vu" is a modern addition used to represent the /v/ sound in foreign languages such as English, but since Japanese from a phonological standpoint does not have a /v/ sound, it is pronounced as /b/ and mostly serves as a more accurate indicator of a word's pronunciation in its original language. However, it is rarely seen because loanwords and transliterated words are usually written in katakana, where the corresponding character would be written as ヴ. , , for "ja"/"ju"/"jo" are theoretically possible in rendaku, but are practically never used. For example, 'throughout Japan' could be written , but is practically always 
The "myu" kana is extremely rare in originally Japanese words; linguist Haruhiko Kindaichi raises the example of the Japanese family name Omamyūda and claims it is the only occurrence amongst pure Japanese words. Its katakana counterpart is used in many loanwords, however.
History.
Hiragana developed from "man'yōgana", Chinese characters used for their pronunciations, a practice that started in the 5th century. The oldest example of Man'yōgana is the Inariyama Sword, an iron sword excavated at the Inariyama Kofun in 1968. This sword is thought to be made in year of (which is A.D. 471 in commonly accepted theory).
The forms of the hiragana originate from the cursive script style of Chinese calligraphy. The figure below shows the derivation of hiragana from manyōgana via cursive script. The upper part shows the character in the regular script form, the center character in red shows the cursive script form of the character, and the bottom shows the equivalent hiragana. Note also that the cursive script forms are not strictly confined to those in the illustration.
Male authors came to write literature using hiragana. Hiragana was used for unofficial writing such as personal letters, while katakana and Chinese were used for official documents. In modern times, the usage of hiragana has become mixed with katakana writing. Katakana is now relegated to special uses such as recently borrowed words (i.e., since the 19th century), names in transliteration, the names of animals, in telegrams, and for emphasis.
Originally, for all syllables there was more than one possible hiragana. In 1900, the system was simplified so each syllable had only one hiragana. The deprecated hiragana are now known as .
The pangram poem "Iroha-uta" ("ABC song/poem"), which dates to the 10th century, uses every hiragana once (except "n" ん, which was just a variant of む before the Muromachi era).
Stroke order and direction.
The following table shows the method for writing each hiragana character. It is arranged in the traditional way, beginning top right and reading columns down. The numbers and arrows indicate the stroke order and direction respectively. <br>
Unicode.
Hiragana was added to the Unicode Standard in October, 1991 with the release of version 1.0.
The Unicode block for Hiragana is U+3040–U+309F:
The Unicode hiragana block contains precomposed characters for all hiragana in the modern set, including small vowels and yōon kana for compound syllables, plus the archaic ゐ "wi" and ゑ "we" and the rare ゔ "vu"; the archaic 𛀁 "ye" is included in plane 1 at U+1B001 (see below). All combinations of hiragana with "dakuten" and "handakuten" used in modern Japanese are available as precomposed characters, and can also be produced by using a base hiragana followed by the combining dakuten and handakuten characters (U+3099 and U+309A, respectively). This method is used to add the diacritics to kana that are not normally used with them, for example applying the dakuten to a pure vowel or the handakuten to a kana not in the h-group.
Characters U+3095 and U+3096 are small か ("ka") and small け ("ke"), respectively. U+309F is a ligature of より ("yori") occasionally used in vertical text. U+309B and U+309C are spacing (non-combining) equivalents to the combining dakuten and handakuten characters, respectively.
Historic and variant forms of Japanese kana characters were added to the Unicode Standard in October, 2010 with the release of version 6.0.
The Unicode block for Kana Supplement is U+1B000–U+1B0FF:
Furthermore, as of Unicode 8.0, the following combinatory sequences are also possible, despite having no precomposed symbols in the hiragana block.

</doc>
<doc id="13805" url="https://en.wikipedia.org/wiki?curid=13805" title="Hohenstaufen">
Hohenstaufen

The Hohenstaufen, also called the Staufer or Staufen, were a dynasty of German kings (1138–1254) during the Middle Ages. Besides Germany, they also ruled the Kingdom of Sicily (1194–1268). In Italian historiography, they are known as the "Svevi" (Swabians), since they were (successive) dukes of Swabia from 1079. Three members of the dynasty—Frederick I, Henry VI and Frederick II—were crowned Holy Roman Emperor.
Name.
The name "Hohenstaufen", meaning "high Staufen", originates in the 14th century, when it was first used to distinguish the conical hill named Staufen in the Swabian Jura, in the district of Göppingen, from the village of the same name in the valley below. The name "Staufen" derives from "Stauf" (formerly "stouf"), meaning "chalice", and was commonly applied to conical hills in Swabia in the Middle Ages. The family derives its name from the castle which the first Swabian duke of the lineage built there in the latter half of the 11th century. Staufen castle was only finally called Hohenstaufen by historians in the 19th century, to distinguish it from other castles of the same name. The name of the dynasty followed, but in recent decades the trend in German historiography has been to prefer the name Staufer.
Origins.
The noble family first appeared in the late 10th century in the Swabian "Riesgau" region around the former Carolingian court of Nördlingen. A local count Frederick (d. about 1075) is mentioned as progenitor in a pedigree drawn up by Abbot Wibald of Stavelot at the behest of Emperor Frederick Barbarossa in 1153. He held the office of a Swabian count palatine; his son Frederick of Buren (c.1020–1053) married Hildegard of Egisheim-Dagsburg (d. 1094/95), a niece of Pope Leo IX. Their son Frederick I was appointed Duke of Swabia at Hohenstaufen Castle by the Salian king Henry IV of Germany in 1079. 
At the same time, Duke Frederick I was engaged to the king's approximately seventeen-year-old daughter, Agnes. Nothing is known about Frederick's life before this event, but he proved to be an imperial ally throughout Henry's struggles against other Swabian lords, namely Rudolf of Rheinfelden, Frederick's predecessor, and the Zähringen and Welf lords. Frederick's brother Otto was elevated to the Strasbourg bishopric in 1082.
Upon Frederick's death, he was succeeded by his son, Duke Frederick II, in 1105. Frederick II remained a close ally of the Salians, he and his younger brother Conrad were named the king's representatives in Germany when the king was in Italy. Around 1120, Frederick II married Judith of Bavaria from the rival House of Welf.
Ruling in Germany.
When the last male member of the Salian dynasty, Emperor Henry V, died without heirs in 1125, a controversy arose about the succession. Duke Frederick II and Conrad, the two current male Staufers, by their mother Agnes were grandsons of late Emperor Henry IV and nephews of Henry V. Frederick attempted to succeed to the throne of the Holy Roman Emperor (formally known as the King of the Romans) through a customary election, but lost to the Saxon duke Lothair of Supplinburg. A civil war between Frederick's dynasty and Lothair's ended with Frederick's submission in 1134. After Lothair's death in 1137, Frederick's brother Conrad was elected King as Conrad III.
Because the Welf duke Henry the Proud, son-in-law and heir of Lothair and the most powerful prince in Germany, who had been passed over in the election, refused to acknowledge the new king, Conrad III deprived him of all his territories, giving the Duchy of Saxony to Albert the Bear and that of Bavaria to Leopold IV, Margrave of Austria. In 1147, Conrad heard Bernard of Clairvaux preach the Second Crusade at Speyer, and he agreed to join King Louis VII of France in a great expedition to the Holy Land which failed.
Conrad's brother Duke Frederick II died in 1147, and was succeeded in Swabia by his son, Duke Frederick III. When King Conrad III died without adult heir in 1152, Frederick also succeeded him, taking both German royal and Imperial titles.
Frederick Barbarossa.
Frederick I, known as Frederick Barbarossa because of his red beard, struggled throughout his reign to restore the power and prestige of the German monarchy against the dukes, whose power had grown both before and after the Investiture Controversy under his Salian predecessors. As royal access to the resources of the church in Germany was much reduced, Frederick was forced to go to Italy to find the finances needed to restore the king's power in Germany. He was soon crowned emperor in Italy, but decades of warfare on the peninsula yielded scant results. The Papacy and the prosperous city-states of the Lombard League in northern Italy were traditional enemies, but the fear of Imperial domination caused them to join ranks to fight Frederick. Under the skilled leadership of Pope Alexander III, the alliance suffered many defeats but ultimately was able to deny the emperor a complete victory in Italy. Frederick returned to Germany. He had vanquished one notable opponent, his Welf cousin, Duke Henry the Lion of Saxony and Bavaria in 1180, but his hopes of restoring the power and prestige of the monarchy seemed unlikely to be met by the end of his life.
During Frederick's long stays in Italy, the German princes became stronger and began a successful colonization of Slavic lands. Offers of reduced taxes and manorial duties enticed many Germans to settle in the east in the course of the "Ostsiedlung". In 1163 Frederick waged a successful campaign against the Kingdom of Poland in order to re-install the Silesian dukes of the Piast dynasty. With the German colonization, the Empire increased in size and came to include the Duchy of Pomerania as well as Bohemia and the March of Moravia. A quickening economic life in Germany increased the number of towns and Imperial cities, and gave them greater importance. It was also during this period that castles and courts replaced monasteries as centers of culture. Growing out of this courtly culture, Middle High German literature reached its peak in lyrical love poetry, the Minnesang, and in narrative epic poems such as "Tristan", "Parzival", and the "Nibelungenlied".
Henry VI.
Frederick died in 1190 while on the Third Crusade and was succeeded by his son, Henry VI. Elected king even before his father's death, Henry went to Rome to be crowned emperor. He married Queen Constance of Sicily, and a death in his wife's family in 1194 gave him possession of the Kingdom of Sicily, a source of vast wealth. Henry failed to make royal and Imperial succession hereditary, but in 1196 he succeeded in gaining a pledge that his infant son Frederick would receive the German crown. Faced with difficulties in Italy and confident that he would realize his wishes in Germany at a later date, Henry returned to the south, where it appeared he might unify the peninsula under the Hohenstaufen name. After a series of military victories, however, he fell ill and died of natural causes in Sicily in 1197. His underage son Frederick could only succeed him in Sicily and Malta, while in the Empire the struggle between the House of Staufen and the House of Welf erupted once again.
Philip of Swabia.
Because the election of a three-year-old boy to be German king appeared likely to make orderly rule difficult, the boy's uncle, Duke Philip of Swabia, brother of late Henry VI, was designated to serve in his place. Other factions however favoured a Welf candidate. In 1198, two rival kings were chosen: the Hohenstaufen Philip of Swabia and the son of the deprived Duke Henry the Lion, the Welf Otto IV. A long civil war began; Philip was about to win when he was murdered by the Bavarian count palatine Otto VIII of Wittelsbach in 1208. Pope Innocent III initially had supported the Welfs, but when Otto, now sole elected monarch, moved to appropriate Sicily, Innocent changed sides and accepted young Frederick II and his ally, King Philip II of France, who defeated Otto at the 1214 Battle of Bouvines. Frederick had returned to Germany in 1212 from Sicily, where he had grown up, and was elected king in 1215. When Otto died in 1218, Fredrick became the undisputed ruler, and in 1220 was crowned Holy Roman Emperor.
Philip changed the coat of arms from a black lion on a gold shield to three leopards, probably derived from the arms of his Welf rival Otto IV.
Ruling in Italy.
The conflict between the Staufer dynasty and the Welf had irrevocably weakened the Imperial authority and the Norman kingdom of Sicily became the base for Staufer rule.
Frederick II.
Emperor Frederick II spent little time in Germany as his main concerns lay in Southern Italy. He founded the University of Naples in 1224 to train future state officials and reigned over Germany primarily through the allocation of royal prerogatives, leaving the sovereign authority and imperial estates to the ecclesiastical and secular princes. He made significant concessions to the German nobles, such as those put forth in an imperial statute of 1232, which made princes virtually independent rulers within their territories. These measures favoured the further fragmentation of the Empire.
By the 1226 Golden Bull of Rimini, Frederick had assigned the military order of the Teutonic Knights to complete the conquest and conversion of the Prussian lands. A reconciliation with the Welfs took place in 1235, whereby Otto the Child, grandson of the late Saxon duke Henry the Lion, was named Duke of Brunswick and Lüneburg. The power struggle with the popes continued and resulted in Fredrick's excommunication in 1227. In 1239, Pope Gregory IX excommunicated Fredrick again, and in 1245 he was condemned as a heretic by a church council. Although Frederick was one of the most energetic, imaginative, and capable rulers of the time, he was not concerned with drawing the disparate forces in Germany together. His legacy was thus that local rulers had more authority after his reign than before it. The clergy also had become more powerful.
By the time of Frederick's death in 1250, little centralized power remained in Germany. The Great Interregnum, a period in which there were several elected rival kings, none of whom was able to achieve any position of authority, followed the death of Frederick's son King Conrad IV of Germany in 1254. The German princes vied for individual advantage and managed to strip many powers away from the diminished monarchy. Rather than establish sovereign states however, many nobles tended to look after their families. Their many male heirs created more and smaller estates, and from a largely free class of officials previously formed, many of these assumed or acquired hereditary rights to administrative and legal offices. These trends compounded political fragmentation within Germany. The period was ended in 1273 with the election of Rudolph of Habsburg, a godson of Frederick.
End of the Staufer dynasty.
Conrad IV was succeeded as duke of Swabia by his only son, two-year-old Conradin. By this time, the office of duke of Swabia had been fully subsumed into the office of the king, and without royal authority had become meaningless. In 1261, attempts to elect young Conradin king were unsuccessful. He also had to defend Sicily against an invasion, sponsored by Pope Urban IV (Jacques Pantaléon) and Pope Clement IV (Guy Folques), by Charles of Anjou, a brother of the French king. Charles had been promised by the popes the Kingdom of Sicily, where he would replace the relatives of Frederick II. Charles had defeated Conradin's uncle Manfred, King of Sicily in the Battle of Benevento on 26 February 1266. The king himself, refusing to flee, rushed into the midst of his enemies and was killed. Conradin's campaign to retake control ended with his defeat in 1268 at the Battle of Tagliacozzo, after which he was handed over to Charles, who had him publicly executed at Naples. With Conradin, the direct line of the Dukes of Swabia finally ceased to exist, though most of the later emperors were descended from the Staufer dynasty indirectly.
During the political decentralization of the late Staufer period, the population had grown from an estimated 8 million in 1200 to about 14 million in 1300, and the number of towns increased tenfold. The most heavily urbanized areas of Germany were located in the south and the west. Towns often developed a degree of independence, but many were subordinate to local rulers if not immediate to the emperor. Colonization of the east also continued in the thirteenth century, most notably through the efforts of the Teutonic Knights. German merchants also began trading extensively on the Baltic.
Members of the Hohenstaufen family.
Holy Roman Emperors and Kings of the Romans.
The first ruling Hohenstaufen, Conrad III, like the last one, Conrad IV, was never crowned emperor. After a 20-year period (Great interregnum 1254–1273), the first Habsburg was elected king.
Kings of Italy.
"Note: The following kings are already listed above as German Kings"
Kings of Sicily.
"Note: Some of the following kings are already listed above as German Kings"
Dukes of Swabia.
"Note: Some of the following dukes are already listed above as German Kings"
See also.
Modern history

</doc>
<doc id="13806" url="https://en.wikipedia.org/wiki?curid=13806" title="History of Malaysia">
History of Malaysia

Malaysia is a Southeast Asian country located on strategic sea-lane that exposes it to global trade and foreign culture. Hinduism from India and Buddhism from China dominated early regional history, reaching their peak during the reign of the Sumatra-based Srivijaya civilisation, whose influence extended through Sumatra, Java, the Malay Peninsula and much of Borneo from the 7th to the 14th centuries.
Although Muslims had passed through the Malay Peninsula as early as the 10th century, it was not until the 14th and 15th centuries that Islam first firmly established itself. The adoption of Islam by the 15th century saw the rise of a number of sultanates, the most prominent of which was the Sultanate of Malacca. Islam has had a profound influence on the Malay people, but has also been influenced by them. The Portuguese were the first European colonial powers to establish themselves on the Malay Peninsula and Southeast Asia, capturing Malacca in 1511, followed by the Dutch in 1641. However, it was the British who, after initially establishing bases at Jesselton, Kuching, Penang and Singapore, ultimately secured their hegemony across the territory that is now Malaysia. The Anglo-Dutch Treaty of 1824 defined the boundaries between British Malaya and the Netherlands East Indies (which became Indonesia). A fourth phase of foreign influence was immigration of Chinese and Indian workers to meet the needs of the colonial economy created by the British in the Malay Peninsula and Borneo.
Japanese invasion during World War II ended British domination in Malaysia. The subsequent occupation of Malaya, North Borneo and Sarawak from 1942 to 1945 unleashed nationalism. In the Peninsula, the Malayan Communist Party took up arms against the British. A tough military response was needed to end the insurgency and bring about the establishment of an independent, multi-racial Federation of Malaya on 31 August 1957. On 31 August 1963, the British territories in North Borneo and Singapore were granted independence and formed Malaysia with the Peninsular states on 16 September 1963. Approximately two years later, the Malaysian parliament passed a bill to separate Singapore from the Federation. A confrontation with Indonesia occurred in the early-1960s. Race riots in 1969 led to the imposition of emergency rule, and a curtailment of political life and civil liberties which has never been fully reversed. Since 1970 the "National Front coalition" headed by United Malays National Organisation (UMNO) has governed Malaysia. Economic growth dramatically increased living standards by the 1990s. This growing prosperity helped minimise political discontent.
Prehistory.
Stone hand-axes from early hominoids, probably Homo erectus, have been unearthed in Lenggong. They date back 1.83 million years, the oldest evidence of hominid habitation in Southeast Asia. The earliest evidence of modern human habitation in Malaysia is the 40,000-year-old skull excavated from the Niah Caves in Borneo in 1958. A study of Asian genetics points to the idea that the original humans in East Asia came from Southeast Asia. The oldest complete skeleton found in Malaysia is 11,000-year-old Perak Man unearthed in 1991. The indigenous groups on the peninsula can be divided into three ethnicities, the Negritos, the Senoi, and the proto-Malays. The first inhabitants of the Malay Peninsula were most probably Negritos. These Mesolithic hunters were probably the ancestors of the Semang, an ethnic Negrito group who have a long history in the Malay Peninsula.
The Senoi appear to be a composite group, with approximately half of the maternal mitochondrial DNA lineages tracing back to the ancestors of the Semang and about half to later ancestral migrations from Indochina. Scholars suggest they are descendants of early Austroasiatic-speaking agriculturalists, who brought both their language and their technology to the southern part of the peninsula approximately 4,000 years ago. They united and coalesced with the indigenous population.
The Proto Malays have a more diverse origin and had settled in Malaysia by 1000 BC. Although they show some connections with other inhabitants in Maritime Southeast Asia, some also have an ancestry in Indochina around the time of the Last Glacial Maximum about 20,000 years ago. Anthropologists support the notion that the Proto-Malays originated from what is today Yunnan, China.A history of Malaya and her neighbours – Page 21 – by Francis Joseph Moorhead, published by Longmans of Malaysia, 1965India and ancient Malaya (from the earliest times to circa A.D. 1400) – Page 3 – by D. Devahuti, Published by D. Moore for Eastern Universities Press, 1965The making of modern Malaya: a history from earliest times to independence – Page 5 – by N. J. Ryan, Oxford University Press, 1965The cultural heritage of Malaya – Page 2 – by N. J. Ryan published by Longman Malaysia, 1971A history of Malaysia and Singapore – Page 5 – by N. J. Ryan published by Oxford University Press, 1976"How the dominoes fell": Southeast Asia in perspective – Page 7 – by Mae H. Esterline, Hamilton Press, 1986A design guide of public parks in Malaysia – Page 38 – by Jamil Abu Bakar published by Penerbit UTM, 2002, ISBN 983-52-0274-5, ISBN 978-983-52-0274-2An introduction to the Malaysian legal system – Page 1 – by Min Aun Wu, Heinemann Educational Books (Asia), 1975A short history of Malaysia – Page 22 – by Harry Miller published by F.A. Praeger, 1966Malaya and its history – Page 14 – by Sir Richard Olaf Winstedt published by Hutchinson University Library, 1962Southeast Asia, past & present – Page 10 – by D. R. SarDesai published by Westview Press, 1994Malaya – Page 17 – by Norton Sydney Ginsburg, Chester F. Roberts published by University of Washington Press, 1958Asia: a social study – Page 43 – by David Tulloch published by Angus and Robertson, 1969Area handbook on Malaya University of Chicago, Chester F. Roberts, Bettyann Carner published by University of Chicago for the Human Relations Area Files, 1955Thailand into the 80's – Page 12 – by Samnak Nāyok Ratthamontrī published by the Office of the Prime Minister, Kingdom of Thailand, 1979Man in Malaya – Page 22 – by B. W. Hodder published by Greenwood Press, 1973The modern anthropology of South-East Asia: an introduction, Volume 1 of The modern anthropology of South-East Asia, RoutledgeCurzon Research on Southeast Asia Series – Page 54 – by Victor T. King, William D. Wilder published by Routledge, 2003, ISBN 0-415-29751-6, ISBN 978-0-415-29751-6Journal of the Malaysian Branch of the Royal Asiatic Society – Page 17 – by Royal Asiatic Society of Great Britain and Ireland. Malaysian Branch, Singapore, 1936Malay and Indonesian leadership in perspective – Page 9 – by Ahmad Kamar 1984The Malay peoples of Malaysia and their languages – Page 36 – by Asmah Haji Omar published by Dewan Bahasa dan Pustaka, Kementerian Pelajaran Malaysia, 1983Encyclopedia of world cultures Volume 5 – Page 174 – by David Levinson – History – 1993 published by G.K. Hall, 1993Indigenous peoples of Asia – Page 274 – by Robert Harrison Barnes, Andrew Gray, Benedict Kingsbury published by the Association for Asian Studies, 1995Peoples of the Earth: Indonesia, Philippines and Malaysia edited by Edward Evan Evans-Pritchard published by Danbury Press, 1973American anthropologist Vol 60 – Page 1228 – by American Anthropological Association, Anthropological Society of Washington (Washington, D.C.), American Ethnological Society, 1958Encyclopaedia Of Southeast Asia (set Of 5 Vols.) – Page 4 – by Brajendra Kumar published by Akansha Publishing House, 2006, ISBN 81-8370-073-X, ISBN 978-81-8370-073-3</ref> This was followed by an early-Holocene dispersal through the Malay Peninsula into the Malay Archipelago. Around 300 BC, they were pushed inland by the Deutero-Malays, an Iron Age or Bronze Age people descended partly from the Chams of Cambodia and Vietnam. The first group in the peninsula to use metal tools, the Deutero-Malays were the direct ancestors of today's Malaysian Malays, and brought with them advanced farming techniques. The Malays remained politically fragmented throughout the Malay archipelago, although a common culture and social structure was shared.
Early kingdoms.
In the first millennium CE, Malays became the dominant race on the peninsula. The small early states that were established were greatly influenced by Indian culture. Indian influence in the region dates back to at least the 3rd century BCE. South Indian culture was spread to Southeast Asia by the south Indian Pallava dynasty in the 4th and 5th century.
The Malay Peninsula was known to ancient Tamils as "Suvarnadvipa" or the "Golden Peninsula". It was shown on Ptolemy's map as the "Golden Khersonese". He referred to the Straits of Melaka as "Sinus Sabaricus". Trade relations with China and India were established in the 1st century BC. Shards of Chinese pottery have been found in Borneo dating from the 1st century following the southward expansion of the Han Dynasty. In the early centuries of the first millennium, the people of the Malay Peninsula adopted the Indian religions of Hinduism and Buddhism, religions which had a major effect on the language and culture of those living in Malaysia. The Sanskrit writing system was used as early as the 4th century.
There were numerous Malay kingdoms in the 2nd and 3rd century, as many as 30, mainly based on the Eastern side of the Malay peninsula. Among the earliest kingdoms known to have been based in what is now Malaysia is the ancient empire of Langkasuka, located in the northern Malay Peninsula and based somewhere in Kedah. It was closely tied to Funan in Cambodia, which also ruled part of northern Malaysia until the 6th century. According to the Sejarah Melayu ("Malay Annals"), the Khmer prince Raja Ganji Sarjuna founded the kingdom of Gangga Negara (modern-day Beruas, Perak) in the 700s. Chinese chronicles of the 5th century CE speak of a great port in the south called Guantoli, which is thought to have been in the Straits of Malacca. In the 7th century, a new port called Shilifoshi is mentioned, and this is believed to be a Chinese rendering of Srivijaya.
Between the 7th and the 13th century, much of the Malay peninsula was under the Buddhist Srivijaya empire. The site of Srivijaya's centre is thought be at a river mouth in eastern Sumatra, based near what is now Palembang. For over six centuries the Maharajahs of Srivijaya ruled a maritime empire that became the main power in the archipelago. The empire was based around trade, with local kings (dhatus or community leaders) swearing allegiance to the central lord for mutual profit.
The relation between Srivijaya and the Chola Empire of south India was friendly during the reign of Raja Raja Chola I but during the reign of Rajendra Chola I the Chola Empire attacked Srivijaya cities.
In 1025 and 1026 Gangga Negara was attacked by Rajendra Chola I of the Chola Empire, the Tamil emperor who is now thought to have laid Kota Gelanggi to waste. Kedah—known as "Kedaram", "Cheh-Cha" (according to "I-Ching") or "Kataha", in ancient Pallava or Sanskrit—was in the direct route of the invasions and was ruled by the Cholas from 1025. A second invasion was led by Virarajendra Chola of the Chola dynasty who conquered Kedah in the late 11th century. The senior Chola's successor, Vira Rajendra Chola, had to put down a Kedah rebellion to overthrow other invaders. The coming of the Chola reduced the majesty of Srivijaya, which had exerted influence over Kedah, Pattani and as far as Ligor. During the reign of Kulothunga Chola I Chola overlordship was established over the Sri Vijaya province kedah in the late 11th century. The expedition of the Chola Emperors had such a great impression to the Malay people of the medieval period that their name was mentioned in the corrupted form as Raja Chulan in the medieval Malay chronicle Sejarah Melaya. Even today the Chola rule is remembered in Malaysia as many Malaysian princes have names ending with Cholan or Chulan, one such was the Raja of Perak called Raja Chulan.
Pattinapalai, a Tamil poem of the 2nd century CE, describes goods from Kedaram heaped in the broad streets of the Chola capital. A 7th-century Indian drama, "Kaumudhimahotsva", refers to Kedah as Kataha-nagari. The "Agnipurana" also mentions a territory known as Anda-Kataha with one of its boundaries delineated by a peak, which scholars believe is Gunung Jerai. Stories from the "Katasaritasagaram" describe the elegance of life in Kataha. The Buddhist kingdom of Ligor took control of Kedah shortly after. Its king Chandrabhanu used it as a base to attack Sri Lanka in the 11th century and ruled the northern parts, an event noted in a stone inscription in Nagapattinum in Tamil Nadu and in the Sri Lankan chronicles, "Mahavamsa".
At times, the Khmer kingdom, the Siamese kingdom, and even Cholas kingdom tried to exert control over the smaller Malay states. The power of Srivijaya declined from the 12th century as the relationship between the capital and its vassals broke down. Wars with the Javanese caused it to request assistance from China, and wars with Indian states are also suspected. In the 11th century, the centre of power shifted to Melayu, a port possibly located further up the Sumatran coast at near the Jambi River. The power of the Buddhist Maharajas was further undermined by the spread of Islam. Areas which were converted to Islam early, such as Aceh, broke away from Srivijaya’s control. By the late 13th century, the Siamese kings of Sukhothai had brought most of Malaya under their rule. In the 14th century, the Hindu Java-based Majapahit empire came into possession of the peninsula.
Arrival of Islam.
Islam came to the Malay Archipelago via Arabs and Indian traders in the 13th century, ending the age of Hinduism and Buddhism. It arrived in the region gradually, and became the religion of the elite before it spread to the commoners. The Islam in Malaysia was influenced by previous religions and was originally not orthodox.
The port of Malacca on the west coast of the Malay Peninsula was founded in 1402 by Parameswara, a Srivijaya prince fleeing Temasek (now Singapore), who was claimed in the Sejarah Melayu to be a descendant of Alexander the Great. Parameswara in particular sailed to Temasek to escape persecution. There he came under the protection of Temagi, a Malay chief from Patani who was appointed by the king of Siam as regent of Temasek. Within a few days, Parameswara killed Temagi and appointed himself regent. Some five years later he had to leave Temasek, due to threats from Siam. During this period, a Javanese fleet from Majapahit attacked Temasek.
Parameswara headed north to found a new settlement. At Muar, Parameswara considered siting his new kingdom at either Biawak Busuk or at Kota Buruk. Finding that the Muar location was not suitable, he continued his journey northwards. Along the way, he reportedly visited Sening Ujong (former name of present-day Sungai Ujong) before reaching a fishing village at the mouth of the Bertam River (former name of the Melaka River), and founded what would become the Malacca Sultanate. Over time this developed into modern-day Malacca Town. According to the "Malay Annals", here Parameswara saw a mouse deer outwitting a dog resting under a Malacca tree. Taking this as a good omen, he decided to establish a kingdom called Malacca. He built and improved facilities for trade. The Malacca Sultanate is commonly considered the first independent state in the peninsula.
At the time of Melaka's founding, the emperor of Ming Dynasty China was sending out fleets of ships to expand trade. Admiral Zheng He called at Malacca and brought Parameswara with him on his return to China, a recognition of his position as legitimate ruler of Malacca. In exchange for regular tribute, the Chinese emperor offered Melaka protection from the constant threat of a Siamese attack. The Chinese and Indians who settled in the Malay Peninsula before and during this period are the ancestors of today's Baba-Nyonya and Chetti community. According to one theory, Parameswara became a Muslim when he married a Princess of Pasai and he took the fashionable Persian title "Shah", calling himself Iskandar Shah. Chinese chronicles mention that in 1414, the son of the first ruler of Malacca visited the Ming emperor to inform them that his father had died. Parameswara's son was then officially recognised as the second ruler of Melaka by the Chinese Emperor and styled Raja Sri Rama Vikrama, Raja of Parameswara of Temasek and Malacca and he was known to his Muslim subjects as Sultan Sri Iskandar Zulkarnain Shah or Sultan Megat Iskandar Shah. He ruled Malacca from 1414 to 1424. Through the influence of Indian Muslims and, to a lesser extent, Hui people from China, Islam became increasingly common during the 15th century.
After an initial period paying tribute to the Ayutthaya, the kingdom rapidly assumed the place previously held by Srivijaya, establishing independent relations with China, and exploiting its position dominating the Straits to control the China-India maritime trade, which became increasingly important when the Mongol conquests closed the overland route between China and the west.
Within a few years of its establishment, Malacca officially adopted Islam. Parameswara became a Muslim, and due to the fact Malacca was under a Muslim Prince the conversion of Malays to Islam accelerated in the 15th century. The political power of the Malacca Sultanate helped Islam’s rapid spread through the archipelago. Malacca was an important commercial centre during this time, attracting trade from around the region. By the start of the 16th century, with the Malacca Sultanate in the Malay peninsula and parts of Sumatra, the Demak Sultanate in Java, and other kingdoms around the Malay archipelago increasingly converting to Islam, it had become the dominant religion among Malays, and reached as far as the modern-day Philippines, leaving Bali as an isolated outpost of Hinduism today.
Malacca's reign lasted little more than a century, but during this time became the established centre of Malay culture. Most future Malay states originated from this period. Malacca became a cultural centre, creating the matrix of the modern Malay culture: a blend of indigenous Malay and imported Indian, Chinese and Islamic elements. Malacca's fashions in literature, art, music, dance and dress, and the ornate titles of its royal court, came to be seen as the standard for all ethnic Malays. The court of Malacca also gave great prestige to the Malay language, which had originally evolved in Sumatra and been brought to Malacca at the time of its foundation. In time Malay came to be the official language of all the Malaysian states, although local languages survived in many places. After the fall of Malacca, the Sultanate of Brunei became the major centre of Islam.
Struggles for hegemony.
The closing of the overland route from Asia to Europe by the Ottoman Empire and the claim towards trade monopoly with India and southeast Asia by Arab traders, led European powers to look for a maritime route. In 1511, Afonso de Albuquerque led an expedition to Malaya which seized Malacca with the intent of using it as a base for activities in southeast Asia. This was the first colonial claim on what is now Malaysia. The son of the last Sultan of Malacca, Sultan Alauddin Riayat Shah II fled to the southern tip of the peninsula, where he founded a state that which became the Sultanate of Johor. Another son created the Perak Sultanate to the north. By the late 16th century, the tin mines of northern Malaya had been discovered by European traders, and Perak grew wealthy on the proceeds of tin exports. Portuguese influence was strong, as they aggressively tried to convert the population of Malacca to Catholicism. In 1571, the Spanish captured Manila and established a colony in the Philippines, reducing the Sultanate of Brunei's power.
After the fall of Malacca to Portugal, the Johor Sultanate and the Sultanate of Aceh on northern Sumatra moved to fill the power vacuum left behind. The three powers struggled to dominate the Malay peninsula and the surrounding islands. Johor founded in the wake of Malacca's conquest grew powerful enough to rival the Portuguese, although it was never able to recapture the city. Instead it expanded in other directions, building in 130 years one of the largest Malay states. In this time the numerous attempts to recapture Malacca led to a strong backlash from the Portuguese, whose raids even reached Johor's capital of Johor Lama in 1587.
In 1607, the Sultanate of Aceh rose as the powerful and wealthiest state in the Malay archipelago. Under Iskandar Muda's reign, the sultanate's control was extended over a number of Malay states. A notable conquest was Perak, a tin-producing state on the Peninsula. In Iskandar Muda's disastrous campaign against Malacca in 1629, the combined Portuguese and Johor forces managed to destroy all the ships of his formidable fleet and 19,000 troops according to a Portuguese account. Aceh forces were not destroyed, however, as Aceh was able to conquer Kedah within the same year and took many of its citizens to Aceh. The Sultan's son-in-law, Iskandar Thani, the former prince of Pahang later became Iskandar Muda's successor. The conflict over control of the straits went on until 1641, when the Dutch (allied to Johor) gained control of Malacca.
In the early 17th century, the Dutch East India Company ("Vereenigde Oost-Indische Compagnie", or VOC) was established. During this time the Dutch were at war with Spain, which absorbed the Portuguese Empire due to the Iberian Union. The Dutch expanded across the archipelago, forming an alliance with Johor and using this to push the Portuguese out of Malacca in 1641. Backed by the Dutch, Johor established a loose hegemony over the Malay states, except Perak, which was able to play off Johor against the Siamese to the north and retain its independence. The Dutch did not interfere in local matters in Malacca, but at the same time diverted most trade to its colonies on Java.
The weakness of the small coastal Malay states led to the immigration of the Bugis, escaping from Dutch colonisation of Sulawesi, who established numerous settlements on the peninsula which they used to interfere with Dutch trade. They seized control of Johor following the assassination of the last Sultan of the old Melaka royal line in 1699. Bugis expanded their power in the states of Johor, Kedah, Perak, and Selangor. The Minangkabau from central Sumatra migrated into Malaya, and eventually established their own state in Negeri Sembilan. The fall of Johor left a power vacuum on the Malay Peninsula which was partly filled by the Siamese kings of Ayutthaya kingdom, who made the five northern Malay states—Kedah, Kelantan, Patani, Perlis, and Terengganu — their vassals. Johor’s eclipse also left Perak as the unrivalled leader of the Malay states.
The economic importance of Malaya to Europe grew rapidly during the 18th century. The fast-growing tea trade between China and United Kingdom increased the demand for high-quality Malayan tin, which was used to line tea-chests. Malayan pepper also had a high reputation in Europe, while Kelantan and Pahang had gold mines. The growth of tin and gold mining and associated service industries led to the first influx of foreign settlers into the Malay world — initially Arabs and Indians, later Chinese — who colonised the towns and soon dominated economic activities. 
British influence.
English traders had been present in Malay waters since the 17th century. However, with the arrival of the British, European power became dominant in Malaysia. Before the mid-19th-century British interests in the region were predominantly economic, with little interest in territorial control. Already the most powerful coloniser in India, the British were looking towards southeast Asia for new resources. The growth of the China trade in British ships increased the East India Company’s desire for bases in the region. Various islands were used for this purpose, but the first permanent acquisition was Penang, leased from the Sultan of Kedah in 1786. This was followed soon after by the leasing of a block of territory on the mainland opposite Penang (known as Province Wellesley). In 1795, during the Napoleonic Wars, the British with the consent of the Netherlands occupied Dutch Melaka to forestall possible French encroachment in the area.
When Malacca was handed back to the Dutch in 1815, the British governor, Stamford Raffles, looked for an alternative base, and in 1819 he acquired Singapore from the Sultan of Johor. The exchange of the British colony of Bencoolen for Malacca with the Dutch left the British as the sole colonial power on the peninsula. The territories of the British were set up as free ports, attempting to break the monopoly held by other colonial powers at the time, and making them large bases of trade. They allowed Britain to control all trade through the straits of Malacca. British influence was increased by Malayan fears of Siamese expansionism, to which Britain made a useful counterweight. During the 19th century the Malay Sultans aligned themselves with the British Empire, due to the benefits of associations with the British and the belief in superior British civilisation.
In 1824, British hegemony in Malaya (before the name Malaysia) was formalised by the Anglo-Dutch Treaty, which divided the Malay archipelago between Britain and the Netherlands. The Dutch evacuated Melaka and renounced all interest in Malaya, while the British recognised Dutch rule over the rest of the East Indies. By 1826 the British controlled Penang, Malacca, Singapore, and the island of Labuan, which they established as the crown colony of the Straits Settlements, administered first under the East India Company until 1867, when they were transferred to the Colonial Office in London.
Federated and Unfederated Malay States.
Initially, the British followed a policy of non-intervention in relations between the Malay states. The commercial importance of tin mining in the Malay states to merchants in the Straits Settlements led to infighting between the aristocracy on the peninsula. The destabilisation of these states damaged the commerce in the area, causing British intervention. The wealth of Perak’s tin mines made political stability there a priority for British investors, and Perak was thus the first Malay state to agree to the supervision of a British resident. British gunboat diplomacy was employed to bring about a peaceful resolution to civil disturbances caused by Chinese and Malay gangsters employed in a political fight between Ngah Ibrahim and Raja Muda Abdullah. The Pangkor Treaty of 1874 paved the way for the expansion of British influence in Malaya. The British concluded treaties with some Malay states, installing “residents” who advised the Sultans and soon became the effective rulers of their states. These advisors held power in everything except to do with Malay religion and customs.
Johor alone resisted, by modernising and giving British and Chinese investors legal protection. By the turn of the 20th century, the states of Pahang, Selangor, Perak, and Negeri Sembilan, known together as the Federated Malay States, had British advisors. In 1909 the Siamese kingdom was compelled to cede Kedah, Kelantan, Perlis and Terengganu, which already had British advisors, over to the British. Sultan Abu Bakar of Johor and Queen Victoria were personal acquaintances who recognised each other as equals. It was not until 1914 that Sultan Abu Bakar's successor, Sultan Ibrahim, accepted a British adviser. The four previously Thai states and Johor were known as the Unfederated Malay States. The states under the most direct British control developed rapidly, becoming the largest suppliers in the world of first tin, then rubber.
By 1910, the pattern of British rule in the Malay lands was established. The Straits Settlements were a Crown colony, ruled by a governor under the supervision of the Colonial Office in London. Their population was about half Chinese, but all residents, regardless of race, were British subjects. The first four states to accept British residents, Perak, Selangor, Negeri Sembilan, and Pahang, were termed the Federated Malay States: while technically independent, they were placed under a Resident-General in 1895, making them British colonies in all but name. The Unfederated Malay States (Johore, Kedah, Kelantan, Perlis, and Terengganu) had a slightly larger degree of independence, although they were unable to resist the wishes of their British residents for long. Johor, as Britain's closest ally in Malay affairs, had the privilege of a written constitution, which gave the Sultan the right to appoint his own Cabinet, but he was generally careful to consult the British first.
19th-century Borneo.
During the late 19th century the British also gained control of the north coast of Borneo, where Dutch rule had never been established. Development on the Peninsula and Borneo were generally separate until the 19th century. The eastern part of this region (now Sabah) was under the nominal control of the Sultan of Sulu, who later became a vassal of the Spanish East Indies. The rest was the territory of the Sultanate of Brunei. In 1841, British adventurer James Brooke helped the Sultan of Brunei suppress a revolt, and in return received the title of raja and the right to govern the Sarawak River District. In 1846, his title was recognised as hereditary, and the "White Rajahs" began ruling Sarawak as a recognised independent state. The Brookes expanded Sarawak at the expense of Brunei.
In 1881, the British North Borneo Company was granted control of the territory of British North Borneo, appointing a governor and legislature. It was ruled from the office in London. Its status was similar to that of a British Protectorate, and like Sarawak it expanded at the expense of Brunei. Until the Philippine independence on 1946, seven British-controlled islands in the north-eastern part of Borneo named Turtle Islands and Cagayan de Tawi-Tawi were ceded to the Philippine government by the Crown colony government of North Borneo. The Philippines then under its irredentism motive since the administration of President Diosdado Macapagal laying claim to eastern Sabah in a basis the territory was part of the present-defunct Sultanate of Sulu’s territory. In 1888, what was left of Brunei was made a British protectorate, and in 1891 another Anglo-Dutch treaty formalised the border between British and Dutch Borneo.
Race relations.
Unlike some colonial powers, the British always saw their empire as primarily an economic concern, and its colonies were expected to turn a profit for British shareholders. Malaya’s obvious attractions were its tin and gold mines, but British planters soon began to experiment with tropical plantation crops—tapioca, gambier, pepper, and coffee. But in 1877 the rubber plant was introduced from Brazil, and rubber soon became Malaya’s staple export, stimulated by booming demand from European industry. Rubber was later joined by palm oil as an export earner. All these industries required a large and disciplined labour force, and the British did not regard the Malays as reliable workers. The solution was the importation of plantation workers from India, mainly Tamil-speakers from South India. The mines, mills and docks also attracted a flood of immigrant workers from southern China. Soon towns like Singapore, Penang, and Ipoh were majority Chinese, as was Kuala Lumpur, founded as a tin-mining centre in 1857. By 1891, when Malaya’s first census was taken, Perak and Selangor, the main tin-mining states, had Chinese majorities.
The Chinese mostly arrived poor; yet, their belief in industriousness and frugality, their emphasis in their children's education and their maintenance of Confucian family hierarchy, as well as their voluntary connection with tightly knit networks of mutual aid societies (run by "Hui-Guan" 會館, or non-profit organisations with nominal geographic affiliations from different parts of China) all contributed to their prosperity. In the 1890s Yap Ah Loy, who held the title of Kapitan China of Kuala Lumpur, was the richest man in Malaya, owning a chain of mines, plantations and shops. Malaya’s banking and insurance industries were run by the Chinese from the start, and Chinese businesses, usually in partnership with London firms, soon had a stranglehold on the economy. Since the Malay Sultans tended to spend well beyond their means, they were soon indebted to Chinese bankers, and this gave the Chinese political as well as economic leverage. At first the Chinese immigrants were mostly men, and many intended to return home when they had made their fortunes. Many did go home, but many more stayed. At first they married Malay women, producing a community of Sino-Malayans or baba people, but soon they began importing Chinese brides, establishing permanent communities and building schools and temples.
The Indians were initially less successful, since unlike the Chinese they came mainly as indentured labourers to work in the rubber plantations, and had few of the economic opportunities that the Chinese had. They were also a less united community, since they were divided between Hindus and Muslims and along lines of language and caste. An Indian commercial and professional class emerged during the early 20th century, but the majority of Indians remained poor and uneducated in rural ghettos in the rubber-growing areas.
Traditional Malay society had great difficulty coping with both the loss of political sovereignty to the British and of economic power to the Chinese. By the early 20th century it seemed possible that the Malays would become a minority in their own country. The Sultans, who were seen as collaborators with both the British and the Chinese, lost some of their traditional prestige, particularly among the increasing number of Malays with a western education, but the mass of rural Malays continued to revere the Sultans and their prestige was thus an important prop for colonial rule. A small class of Malay nationalist intellectuals began to emerge during the early 20th century, and there was also a revival of Islam in response to the perceived threat of other imported religions, particularly Christianity. In fact few Malays converted to Christianity, although many Chinese did. The northern regions, which were less influenced by western ideas, became strongholds of Islamic conservatism, as they have remained.
The one consolation to Malay pride was that the British allowed them a virtual monopoly of positions in the police and local military units, as well as a majority of those administrative positions open to non-Europeans. While the Chinese mostly built and paid for their own schools and colleges, importing teachers from China, the colonial government fostered education for Malays, opening Malay College in 1905 and creating the Malay Administrative Service in 1910. (The college was dubbed “Bab ud-Darajat” – the Gateway to High Rank.) A Malay Teachers College followed in 1922, and a Malay Women’s Training College in 1935. All this reflected the official British policy that Malaya belonged to the Malays, and that the other races were but temporary residents. This view was increasingly out of line with reality, and contained the seeds of much future trouble.
The Malay teacher's college had lectures and writings that nurtured Malay nationalism and anti-colonialist sentiments. Due to this it is known as the birthplace of Malay nationalism. In 1938, Ibrahim Yaacob, an alumnus of Sultan Idris College, established the Kesatuan Melayu Muda (Young Malays Union or KMM) in Kuala Lumpur. It was the first nationalist political organisation in British Malaya, advocating for the union of all Malays regardless of origin, and fighting for Malay rights and against British Imperialism. A specific ideal the KMM held was "Panji Melayu Raya", which called for the unification of British Malaya and Dutch East Indies.
In the years before World War II, the British were concerned with finding the balance between a centralised state and maintaining the power of the Sultans in Malaya. There were no moves to give Malaya a unitary government, and in fact in 1935 the position of Resident-General of the Federated States was abolished, and its powers decentralised to the individual states. With their usual tendency to racial stereotyping, the British regarded the Malays as amiable but unsophisticated and rather lazy, incapable of self-government, although making good soldiers under British officers. They regarded the Chinese as clever but dangerous—and indeed during the 1920s and 1930s, reflecting events in China, the Chinese Nationalist Party (the Kuomintang) and the Communist Party of China built rival clandestine organisations in Malaya, leading to regular disturbances in the Chinese towns. The British saw no way that Malaya’s disparate collection of states and races could become a nation, let alone an independent one.
War and emergency.
Malaya saw little action during World War I, except for the sinking of the Russian cruiser Zhemchug by the German cruiser Emden on 28 October 1914 during the Battle of Penang.
The outbreak of war in the Pacific in December 1941 found the British in Malaya completely unprepared. During the 1930s, anticipating the rising threat of Japanese naval power, they had built a great naval base at Singapore, but never anticipated an invasion of Malaya from the north. Because of the demands of the war in Europe, there was virtually no British air capacity in the Far East. The Japanese were thus able to attack from their bases in French Indo-China with impunity, and despite stubborn resistance from British, Australian, and Indian forces, they overran Malaya in two months. Singapore, with no landward defences, no air cover, and no water supply, was forced to surrender in February 1942, doing irreparable damage to British prestige. British North Borneo and Brunei were also occupied.
The Japanese had a racial policy just as the British did. They regarded the Malays as a colonial people liberated from British imperialist rule, and fostered a limited form of Malay nationalism, which gained them some degree of collaboration from the Malay civil service and intellectuals. (Most of the Sultans also collaborated with the Japanese, although they maintained later that they had done so unwillingly.) The Malay nationalist Kesatuan Melayu Muda, advocates of "Melayu Raya", collaborated with the Japanese, based on the understanding that Japan would unite the Dutch East Indies, Malaya and Borneo and grant them independence. The occupiers regarded the Chinese, however, as enemy aliens, and treated them with great harshness: during the so-called "sook ching" (purification through suffering), up to 80,000 Chinese in Malaya and Singapore were killed. Chinese businesses were expropriated and Chinese schools either closed or burned down. Not surprisingly the Chinese, led by the Malayan Communist Party (MCP), became the backbone of the Malayan Peoples' Anti-Japanese Army (MPAJA), which with British assistance became the most effective resistance force in the occupied Asian countries.
Although the Japanese argued that they supported Malay nationalism, they offended Malay nationalism by allowing their ally Thailand to re-annex the four northern states, Kedah, Perlis, Kelantan, and Terengganu that had been surrendered to the British in 1909. The loss of Malaya’s export markets soon produced mass unemployment which affected all races and made the Japanese increasingly unpopular.
During occupation, ethnic tensions were raised and nationalism grew. The Malayans were thus on the whole glad to see the British back in 1945, but things could not remain as they were before the war, and a stronger desire for independence grew. Britain was bankrupt and the new Labour government was keen to withdraw its forces from the East as soon as possible. Colonial self-rule and eventual independence were now British policy. The tide of colonial nationalism sweeping through Asia soon reached Malaya. But most Malays were more concerned with defending themselves against the MCP which was mostly made up of Chinese, than with demanding independence from the British; indeed, their immediate concern was that the British not leave and abandon the Malays to the armed Communists of the MPAJA, which was the largest armed force in the country.
In 1944, the British drew up plans for a Malayan Union, which would turn the Federated and Unfederated Malay States, plus Penang and Malacca (but not Singapore), into a single Crown colony, with a view towards independence. The Bornean territories and Singapore were left out as it was thought this would make union more difficult to achieve. There was however strong opposition from the Malays, who opposed the weakening of the Malay rulers and the granting of citizenship to the ethnic Chinese and other minorities. The British had decided on equality between races as they perceived the Chinese and Indians as more loyal to the British during the war than the Malays. The Sultans, who had initially supported it, backed down and placed themselves at the head of the resistance.
In 1946, the United Malays National Organisation (UMNO) was founded by Malay nationalists led by Dato Onn bin Jaafar, the Chief Minister of Johor. UMNO favoured independence for Malaya, but only if the new state was run exclusively by the Malays. Faced with implacable Malay opposition, the British dropped the plan for equal citizenship. The Malayan Union was thus established in 1946, and was dissolved in 1948 and replaced by the Federation of Malaya, which restored the autonomy of the rulers of the Malay states under British protection.
Meanwhile, the Communists were moving towards open insurrection. The MPAJA had been disbanded in December 1945, and the MCP organised as a legal political party, but the MPAJA’s arms were carefully stored for future use. The MCP policy was for immediate independence with full equality for all races. This meant it recruited very few Malays. The Party’s strength was in the Chinese-dominated trade unions, particularly in Singapore, and in the Chinese schools, where the teachers, mostly born in China, saw the Communist Party of China as the leader of China’s national revival. In March 1947, reflecting the international Communist movement’s “turn to left” as the Cold War set in, the MCP leader Lai Tek was purged and replaced by the veteran MPAJA guerrilla leader Chin Peng, who turned the party increasingly to direct action. These rebels, under the leadership of the MCP, launched guerrilla operations designed to force the British out of Malaya. In July, following a string of assassinations of plantation managers, the colonial government struck back, declaring a State of Emergency, banning the MCP and arresting hundreds of its militants. The Party retreated to the jungle and formed the Malayan Peoples’ Liberation Army, with about 13,000 men under arms, all Chinese.
The Malayan Emergency as it was known, lasted from 1948 to 1960, and involved a long anti-insurgency campaign by Commonwealth troops in Malaya. The British strategy, which proved ultimately successful, was to isolate the MCP from its support base by a combination of economic and political concessions to the Chinese and the resettlement of Chinese squatters into “New Villages” in “white areas” free of MCP influence. The effective mobilisation of the Malays against the MCP was also an important part of the British strategy. From 1949 the MCP campaign lost momentum and the number of recruits fell sharply. Although the MCP succeeded in assassinating the British High Commissioner, Sir Henry Gurney, in October 1951, this turn to terrorist tactics alienated many moderate Chinese from the Party. The arrival of Lt.-Gen Sir Gerald Templer as British commander in 1952 was the beginning of the end of the Emergency. Templer invented the techniques of counter-insurgency warfare in Malaya and applied them ruthlessly. Although the insurgency was defeated Commonwealth troops remained with the backdrop of the Cold War. Against this backdrop, independence for the Federation within the Commonwealth was granted on 31 August 1957, with Tunku Abdul Rahman as the first prime minister.
Towards Malaysia.
Chinese reaction against the MCP was shown by the formation of the Malayan Chinese Association (MCA) in 1949 as a vehicle for moderate Chinese political opinion. Its leader Tan Cheng Lock favoured a policy of collaboration with UMNO to win Malayan independence on a policy of equal citizenship, but with sufficient concessions to Malay sensitivities to ease nationalist fears. Tan formed a close collaboration with Tunku (Prince) Abdul Rahman, the Chief Minister of Kedah and from 1951 successor to Datuk Onn as leader of UMNO. Since the British had announced in 1949 that Malaya would soon become independent whether the Malayans liked it or not, both leaders were determined to forge an agreement their communities could live with as a basis for a stable independent state. The UMNO-MCA Alliance, which was later joined by the Malayan Indian Congress (MIC), won convincing victories in local and state elections in both Malay and Chinese areas between 1952 and 1955.
The introduction of elected local government was another important step in defeating the Communists. After Joseph Stalin’s death in 1953, there was a split in the MCP leadership over the wisdom of continuing the armed struggle. Many MCP militants lost heart and went home, and by the time Templer left Malaya in 1954 the Emergency was over, although Chin Peng led a diehard group that lurked in the inaccessible country along the Thai border for many years.
During 1955 and 1956 UMNO, the MCA and the British hammered out a constitutional settlement for a principle of equal citizenship for all races. In exchange, the MCA agreed that Malaya’s head of state would be drawn from the ranks of the Malay Sultans, that Malay would be the official language, and that Malay education and economic development would be promoted and subsidised. In effect this meant that Malaya would be run by the Malays, particularly since they continued to dominate the civil service, the army and the police, but that the Chinese and Indians would have proportionate representation in the Cabinet and the parliament, would run those states where they were the majority, and would have their economic position protected. The difficult issue of who would control the education system was deferred until after independence. This came on 31 August 1957, when Tunku Abdul Rahman became the first Prime Minister of independent Malaya.
This left the unfinished business of the other British-ruled territories in the region. After the Japanese surrender the Brooke family and the British North Borneo Company gave up their control of Sarawak and North Borneo respectively, and these became British Crown Colonies. They were much less economically developed than Malaya, and their local political leaderships were too weak to demand independence. Singapore, with its large Chinese majority, achieved autonomy in 1955, and in 1959 the young socialist leader Lee Kuan Yew became Prime Minister. The Sultan of Brunei remained as a British client in his oil-rich enclave. Between 1959 and 1962 the British government orchestrated complex negotiations between these local leaders and the Malayan government.
On 24 April 1961 Lee Kuan Yew proposed the idea of forming Malaysia during a meeting to Tunku Abdul Rahman, after which Tunku invited Lee to prepare a paper elaborating on this idea. On 9 May, Lee sent the final version of the paper to Tunku and then deputy Malayan Prime Minister Abdul Razak. There were doubts about the practicality of the idea but Lee assured the Malayan government of continued Malay political dominance in the new federation. Razak supported the idea of the new federation and worked to convince Tunku to back it. On 27 May 1961, Abdul Rahman proposed the idea of forming "Malaysia", which would consist of Brunei, Malaya, North Borneo, Sarawak, and Singapore, all except Malaya still under British rule. It was states that this would allow the central government to better control and combat communist activities, especially in Singapore. It was also feared that if Singapore became independent, it would become a base for Chinese chauvinists to threaten Malayan sovereignty. The proposed inclusion of British territories besides Singapore was intended to keep the ethnic composition of the new nation similar to that of Malaya, with the Malay and indigenous populations of the other territories cancelling out the Chinese majority in Singapore.
Although Lee Kuan Yew supported the proposal, his opponents from the Singaporean Socialist Front resisted, arguing that this was a ploy for the British to continue controlling the region. Most political parties in Sarawak were also against the merger, and in North Borneo, where there were no political parties, community representatives also stated their opposition. Although the Sultan of Brunei supported the merger, the Parti Rakyat Brunei opposed it as well. At the Commonwealth Prime Ministers Conference in 1961, Abdul Rahman explained his proposal further to its opponents. In October, he obtained agreement from the British government to the plan, provided that feedback be obtained from the communities involved in the merger.
The Cobbold Commission, named after its head, Lord Cobbold, conducted a study in the Borneo territories and approved a merger with North Borneo and Sarawak; however, it was found that a substantial number of Bruneians opposed merger. North Borneo drew up a list of points, referred to as the 20-point agreement, proposing terms for its inclusion in the new federation. Sarawak prepared a similar memorandum, known as the 18-point agreement. Some of the points in these agreements were incorporated into the eventual constitution, some were instead accepted orally. These memoranda are often cited by those who believe that Sarawak's and North Borneo's rights have been eroded over time. A referendum was conducted in Singapore to gauge opinion, and 70% supported merger with substantial autonomy given to the state government. The Sultanate of Brunei withdrew from the planned merger due to opposition from certain segments of its population as well as arguments over the payment of oil royalties and the status of the sultan in the planned merger. Additionally, the Bruneian Parti Rakyat Brunei staged an armed revolt, which, though it was put down, was viewed as potentially destabilising to the new nation.
After reviewing the Cobbold Commission's findings, the British government appointed the Landsdowne Commission to draft a constitution for Malaysia. The eventual constitution was essentially the same as the 1957 constitution, albeit with some rewording; for instance, giving recognition to the special position of the natives of the Borneo States. North Borneo, Sarawak and Singapore were also granted some autonomy unavailable to the states of Malaya. After negotiations in July 1963, it was agreed that Malaysia would come into being on 31 August 1963, consisting of Malaya, North Borneo, Sarawak and Singapore. The date was to coincide with the independence day of Malaya and the British giving self-rule to Sarawak and North Borneo. However, the Philippines and Indonesia strenuously objected to this development, with Indonesia claiming Malaysia represented a form of "neocolonialism" and the Philippines claiming North Borneo as its territory. The opposition from the Indonesian government led by Sukarno and attempts by the Sarawak United People's Party delayed the formation of Malaysia. Due to these factors, an eight-member UN team was formed to re-ascertain whether North Borneo and Sarawak truly wanted to join Malaysia. Malaysia formally came into being on 16 September 1963, consisting of Malaya, North Borneo, Sarawak, and Singapore. In 1963 the total population of Malaysia was about 10 million.
Challenges of independence.
At the time of independence Malaya had great economic advantages. It was among the world’s leading producers of three valuable commodities, rubber, tin, and palm oil, and also a significant iron ore producer. These export industries gave the Malayan government a healthy surplus to invest in industrial development and infrastructure projects. Like other developing nations in the 1950s and 1960s, Malaya (and later Malaysia) placed great stress on state planning, although UMNO was never a socialist party. The First and Second Malayan Plans (1956–60 and 1961–65 respectively) stimulated economic growth through state investment in industry and repairing infrastructure such as roads and ports, which had been damaged and neglected during the war and the Emergency. The government was keen to reduce Malaya’s dependence on commodity exports, which put the country at the mercy of fluctuating prices. The government was also aware that demand for natural rubber was bound to fall as the production and use of synthetic rubber expanded. Since a third of the Malay workforce worked in the rubber industry it was important to develop alternative sources of employment. Competition for Malaya’s rubber markets meant that the profitability of the rubber industry increasingly depended on keeping wages low, which perpetuated rural Malay poverty.
Foreign objection.
Indonesian President Sukarno, backed by the powerful Communist Party of Indonesia (PKI), chose to regard Malaysia as a "neocolonialist" plot against his country, and backed a Communist insurgency in Sarawak, mainly involving elements of the local Chinese community. Indonesian irregular forces were infiltrated into Sarawak, where they were contained by Malaysian and Commonwealth of Nations forces. This period of "Konfrontasi", an economic, political, and military confrontation lasted until the downfall of Sukarno in 1966. The Philippines objected to the formation of the federation, claiming North Borneo was part of Sulu, and thus the Philippines. In 1966 the new president, Ferdinand Marcos, dropped the claim, although it has since been revived and is still a point of contention marring Philippine-Malaysian relations.
Racial strife.
The Depression of the 1930s, followed by the outbreak of the Sino-Japanese War, had the effect of ending Chinese emigration to Malaya. This stabilised the demographic situation and ended the prospect of the Malays becoming a minority in their own country. At the time of independence in 1957, Malays comprised 55% of the population, Chinese 35% and Indians 10%. This balance was altered by the inclusion of the majority-Chinese Singapore, upsetting many Malays. The federation increased the Chinese proportion to close to 40%. Both UMNO and the MCA were nervous about the possible appeal of Lee's People's Action Party (then seen as a radical socialist party) to voters in Malaya, and tried to organise a party in Singapore to challenge Lee's position there. Lee in turn threatened to run PAP candidates in Malaya at the 1964 federal elections, despite an earlier agreement that he would not do so (see PAP-UMNO Relations). Racial tensions intensified as PAP created an opposition alliance aiming for equality between races. This provoked Tunku Abdul Rahman to demand that Singapore withdraw from Malaysia, which it did in August 1965.
The most vexed issues of independent Malaysia were education and the disparity of economic power among the ethnic communities. The Malays felt unhappy with the wealth of the Chinese community, even after the expulsion of Singapore. Malay political movements emerged based around this. However, since there was no effective opposition party, these issues were contested mainly within the coalition government, which won all but one seat in the first post-independence Malayan Parliament. The two issues were related, since the Chinese advantage in education played a large part in maintaining their control of the economy, which the UMNO leaders were determined to end. The MCA leaders were torn between the need to defend their own community's interests and the need to maintain good relations with UMNO. This produced a crisis in the MCA in 1959, in which a more assertive leadership under Lim Chong Eu defied UMNO over the education issue, only to be forced to back down when Tunku Abdul Rahman threatened to break up the coalition.
The Education Act of 1961 put UMNO's victory on the education issue into legislative form. Henceforward Malay and English would be the only teaching languages in secondary schools, and state primary schools would teach in Malay only. Although the Chinese and Indian communities could maintain their own Chinese and Tamil-language primary schools, all their students were required to learn Malay, and to study an agreed "Malayan curriculum". Most importantly, the entry exam to the University of Malaya (which moved from Singapore to Kuala Lumpur in 1963) would be conducted in Malay, even though most teaching at the university was in English until the 1970s. This had the effect of excluding many Chinese students. At the same time Malay schools were heavily subsidised, and Malays were given preferential treatment. This obvious defeat for the MCA greatly weakened its support in the Chinese community.
As in education, the UMNO government's unspoken agenda in the field of economic development aimed to shift economic power away from the Chinese and towards the Malays. The two Malayan Plans and the First Malaysian Plan (1966–1970) directed resources heavily into developments which would benefit the rural Malay community, such as village schools, rural roads, clinics, and irrigation projects. Several agencies were set up to enable Malay smallholders to upgrade their production and to increase their incomes. The Federal Land Development Authority (FELDA) helped many Malays to buy farms or to upgrade ones they already owned. The state also provided a range of incentives and low-interest loans to help Malays start businesses, and government tendering systematically favoured Malay companies, leading many Chinese-owned businesses to "Malayanise" their management. All this certainly tended to reduce to gap between Chinese and Malay standards of living, although some argued that this would have happened anyway as Malaysia's trade and general prosperity increased.
The crisis of 1969.
The collaboration of the MCA and the MIC in these policies weakened their hold on the Chinese and Indian electorates. At the same time, the effects of the government’s affirmative action policies of the 1950s and 1960s had been to create a discontented class of educated but underemployed Malays. This was a dangerous combination, and led to the formation of a new party, the Malaysian People’s Movement (Gerakan Rakyat Malaysia) in 1968. Gerakan was a deliberately non-communal party, bringing in Malay trade unionists and intellectuals as well as Chinese and Indian leaders. At the same time, an Islamist party, the Islamic Party of Malaysia (PAS) and a Chinese socialist party, the Democratic Action Party (DAP), gained increasing support, at the expense of UMNO and the MCA respectively.
At the May 1969 federal elections, the UMNO-MCA-MIC Alliance polled only 48% of the vote, although it retained a majority in the legislature. The MCA lost most of the Chinese-majority seats to Gerakan or DAP candidates. The victorious opposition celebrated by holding a motorcade on the main streets of Kuala Lumpur with supporters holding up brooms as a signal of its intention to make sweeping changes. Fear of what the changes might mean for them (as much of the country's businesses were Chinese-owned), a Malay backlash resulted, leading rapidly to riots and inter-communal violence in which about 6,000 Chinese homes and businesses were burned and at least 184 people were killed. The government declared a state of emergency, and a National Operations Council, headed by Deputy Prime Minister Tun Abdul Razak, took power from the government of Tunku Abdul Rahman, who, in September 1970, was forced to retire in favour of Abdul Razak. It consisted of nine members, mostly Malay, and wielded full political and military power.
Using the Emergency-era Internal Security Act (ISA), the new government suspended Parliament and political parties, imposed press censorship and placed severe restrictions on political activity. The ISA gave the government power to intern any person indefinitely without trial. These powers were widely used to silence the government’s critics, and have never been repealed. The Constitution was changed to make illegal any criticism, even in Parliament, of the Malaysian monarchy, the special position of Malays in the country, or the status of Malay as the national language.
In 1971 Parliament reconvened, and a new government coalition, the National Front (Barisan Nasional), was formed in 1973 to replace the Alliance party. The coalition consisted of UMNO, the MCA, the MIC, Gerakan, PPP, and regional parties in Sabah and Sarawak. The PAS also joined the Front but was expelled in 1977. The DAP was left outside as the only significant opposition party. Abdul Razak held office until his death in 1976. He was succeeded by Datuk Hussein Onn, the son of UMNO’s founder Onn Jaafar, and then by Tun Mahathir Mohamad, who had been Education Minister since 1981, and who held power for 22 years. During these years policies were put in place which led to the rapid transformation of Malaysia’s economy and society, such as the controversial New Economic Policy, which was intended to increase proportionally the share of the economic "pie" of the bumiputras as compared to other ethnic groups—was launched by Prime Minister Tun Abdul Razak. Malaysia has since maintained a delicate ethno-political balance, with a system of government that has attempted to combine overall economic development with political and economic policies that promote equitable participation of all races.
Modern Malaysia.
In 1970 three quarters of Malaysians living below the poverty line were Malays, the majority of Malays were still rural workers, and Malays were still largely excluded from the modern economy. The government’s response was the New Economic Policy of 1971, which was to be implemented through a series of four five-year plans from 1971 to 1990. The plan had two objectives: the elimination of poverty, particularly rural poverty, and the elimination of the identification between race and prosperity. This latter policy was understood to mean a decisive shift in economic power from the Chinese to the Malays, who until then made up only 5% of the professional class.
Poverty was tackled through an agricultural policy which resettled 250,000 Malays on newly cleared farmland, more investment in rural infrastructure, and the creation of free trade zones in rural areas to create new manufacturing jobs. Little was done to improve the living standards of the low-paid workers in plantation agriculture, although this group steadily declined as a proportion of the workforce. By 1990 the poorest parts of Malaysia were rural Sabah and Sarawak, which lagged significantly behind the rest of the country. During the 1970s and ‘80s rural poverty did decline, particularly in the Malayan Peninsula, but critics of the government’s policy contend that this was mainly due to the growth of overall national prosperity (due in large part to the discovery of important oil and gas reserves) and migration of rural people to the cities rather than to state intervention. These years saw rapid growth in Malaysian cities, particularly Kuala Lumpur, which became a magnet for immigration both from rural Malaya and from poorer neighbours such as Indonesia, Bangladesh, Thailand and the Philippines. Urban poverty became a problem for the first time, with shanty towns growing up around the cities.
The second arm of government policy, driven mainly by Mahathir first as Education Minister and then as Prime Minister, was the transfer of economic power to the Malays. Mahathir greatly expanded the number of secondary schools and universities throughout the country, and enforced the policy of teaching in Malay rather than English. This had the effect of creating a large new Malay professional class. It also created an unofficial barrier against Chinese access to higher education, since few Chinese are sufficiently fluent in Malay to study at Malay-language universities. Chinese families therefore sent their children to universities in Singapore, Australia, Britain or the United States – by 2000, for example, 60,000 Malaysians held degrees from Australian universities. This had the unintended consequence of exposing large numbers of Malaysians to life in Western countries, creating a new source of discontent. Mahathir also greatly expanded educational opportunities for Malay women – by 2000 half of all university students were women.
To find jobs for all these new Malay graduates, the government created several agencies for intervention in the economy. The most important of these were PERNAS (National Corporation Ltd.), PETRONAS (National Petroleum Ltd.), and HICOM (Heavy Industry Corporation of Malaysia), which not only directly employed many Malays but also invested in growing areas of the economy to create new technical and administrative jobs which were preferentially allocated to Malays. As a result, the share of Malay equity in the economy rose from 1.5% in 1969 to 20.3% in 1990, and the percentage of businesses of all kinds owned by Malays rose from 39 percent to 68 percent. This latter figure was deceptive because many businesses that appeared to be Malay-owned were still indirectly controlled by Chinese, but there is no doubt that the Malay share of the economy considerably increased. The Chinese remained disproportionately powerful in Malaysian economic life, but by 2000 the distinction between Chinese and Malay business was fading as many new corporations, particularly in growth sectors such as information technology, were owned and managed by people from both ethnic groups.
Malaysia’s rapid economic progress since 1970, which was only temporarily disrupted by the Asian financial crisis of 1997, has not been matched by change in Malaysian politics. The repressive measures passed in 1970 remain in place. Malaysia has had regular elections since 1974, and although campaigning is reasonably free at election time, it is in effect a one-party state, with the UMNO-controlled National Front usually winning nearly all the seats, while the DAP wins some Chinese urban seats and the PAS some rural Malay ones. Since the DAP and the PAS have diametrically opposed policies, they have been unable to form an effective opposition coalition. There is almost no criticism of the government in the media and public protest remains severely restricted. The ISA continues to be used to silence dissidents, and the members of the UMNO youth movement are deployed to physically intimidate opponents.
Mahathir administration.
The restoration of democracy after the 1969 crisis caused disputes in the UMNO, a struggle of power which increased after the death of Tun Abdul Razak. The ailing Datuk Hussein Bin Onn replaced him, but the fight for control shifted to appointing the deputy prime minister. Mahathir Mohamad was chosen, an advocate of Bumiputra who also tried to benefit the other ethnic communities.
Under the premiership of Mahathir Mohamad, Malaysia experienced economic growth from the 1980s, a 1985–86 property market depression, and returned to growth through to the mid-1990s. Mahathir increased privatisation and introduced the New Development Policy (NDP), designed to increase economic wealth for all Malaysians, rather than just Malays. The period saw a shift from an agriculture-based economy to one based on manufacturing and industry in areas such as computers and consumer electronics. It was during this period, too, that the physical landscape of Malaysia changed with the emergence of numerous mega-projects. Notable amongst these projects were the construction of the Petronas Twin Towers (at the time the tallest building in the world, and, as of 2010, still the tallest twin building), Kuala Lumpur International Airport (KLIA), the North-South Expressway, the Sepang International Circuit, the Multimedia Super Corridor (MSC), the Bakun hydroelectric dam, and Putrajaya, the new federal administrative capital.
Under Mahathir Mohamad’s long Prime Ministership (1981–2003), Malaysia’s political culture became increasingly centralised and authoritarian, due to Mahathir's belief that the multiethnic Malaysia could only remain stable through controlled democracy. In 1986–87, he faced leadership challenges among his own party. There were also attacks by the government on several non-governmental organisations (NGO) which were critical of various government policies. There were also issues such the questioning by MCA's Lee Kim Sai over the use of the term "pendatang" (immigrants) that was seen as challenging Malay's bumiputra status, as well as rumours of forced conversion to or from Islam. Mahathir initiated a crackdown on opposition dissidents with the use of the Internal Security Act named Operation Lalang. The Internal Security Act was invoked in October 1987 arresting 106 people, including opposition leaders. The head of the judiciary and five members of the supreme court who had questioned his use of the ISA were also arrested, and a clampdown on Malaysia's press occurred.
This culminated in the dismissal and imprisonment on unsubstantiated charges of the Deputy Prime Minister, Anwar Ibrahim, in 1997 after an internal dispute within the government. The complicity of the judiciary in this piece of persecution was seen as a particularly clear sign of the decline of Malaysian democracy. The Anwar affair led to the formation of a new party, the People's Justice Party, or Keadilan, led by Anwar’s wife, Wan Azizah Wan Ismail. At the 1999 elections Keadilan formed a coalition with the DAP and the PAS known as the Alternative Front (Barisan Alternatif). The result of this was that the PAS won a number of Malay seats from UMNO, but many Chinese voters disapproved of this unnatural alliance with the Islamist PAS, causing the DAP to lose many of its seats to the MCA, including that of its veteran leader, Lim Kit Siang. Wan Azizah won her husband’s former constituency in Penang but otherwise Keadilan made little impact.
In the late 1990s, Malaysia was shaken by the Asian financial crisis, which damaged Malaysia's assembly line-based economy. Mahathir combated it initially with IMF approved policies. However, the devaluation of the Ringgit and the deepening recession caused him to create his own programme, based on protecting Malaysia from foreign investors and reinvigorating the economy through construction projects and the lowering of interest rates. The policies caused Malaysia's economy to rebound by 2002, but brought disagreement between Mahathir and his deputy, Anwar Ibrahim, who backed the IMF policies. This led to the sacking of the Anwar, causing political unrest. Anwar was arrested and banned from politics on what are considered trumped up charges. In 2003 Mahathir, Malaysia's longest serving prime minister, voluntarily retired in favour of his new deputy, Abdullah Ahmad Badawi. In November 2007 two anti-government rallies occurred, precipitated by allegations of corruption and discrepancies in the election system that heavily favoured the ruling political party, National Front, which has been in power since Malaya achieved independence.
Badawi administration.
Dato Seri Abdullah Ahmad Badawi freed Anwar, which was seen as a portent of a mild liberalisation. At the 2004 election, the National Front led by Abdullah had a massive victory, virtually wiping out the PAS and Keadilan, although the DAP recovered the seats it had lost in 1999. This victory was seen as the result mainly of Abdullah's personal popularity and the strong recovery of Malaysia’s economy, which has lifted the living standards of most Malaysians to almost first world standards, coupled with an ineffective opposition. The government's objective is for Malaysia to become a fully developed country by 2020 as expressed in "Wawasan 2020". It leaves unanswered, however, the question of when and how Malaysia will acquire a first world political system (a multi-party democracy, a free press, an independent judiciary and the restoration of civil and political liberties) to go with its new economic maturity.
In November 2007, Malaysia was rocked by two anti-government rallies. The 2007 Bersih Rally which was attended by 40,000 people was held in Kuala Lumpur on 10 November 2007, to campaign for electoral reform. It was precipitated by allegations of corruption and discrepancies in the Malaysian election system that heavily favour the ruling political party, Barisan Nasional, which has been in power since Malaysia achieved its independence in 1957. Another rally was held on 25 November 2007, in Kuala Lumpur led by HINDRAF. The rally organiser, the Hindu Rights Action Force, had called the protest over alleged discriminatory policies favouring ethnic Malays. The crowd was estimated to be between 5,000 and 30,000. In both cases the government and police tried to prevent the gatherings from taking place.
On 16 October 2008, HINDRAF was banned when the government labelled the organisation as "a threat to national security".
Najib administration.
Najib Razak entered office as Prime Minister with a sharp focus on domestic economic issues and political reform. On his first day as Prime Minister, Najib announced as his first actions the removal of bans on two opposition newspapers, "Suara Keadilan" and "Harakahdaily", run by the opposition leader Datuk Seri Anwar Ibrahim-led People's Justice Party and the Pan Islamic Party, respectively, and the release of 13 people held under the Internal Security Act. Among the released detainees were two ethnic Indian activists who were arrested in December 2007 for leading an anti-government campaign, three foreigners and eight suspected Islamic militants. Najib also pledged to conduct a comprehensive review of the much-criticised law which allows for indefinite detention without trial. In the speech, he emphasised his commitment to tackling poverty, restructuring Malaysian society, expanding access to quality education for all, and promoting renewed "passion for public service". He also deferred and abandoned the digital television transition plan of all free-to-air broadcasters such as Radio Televisyen Malaysia.

</doc>
