<doc id="14006" url="https://en.wikipedia.org/wiki?curid=14006" title="Haemophilia">
Haemophilia

Haemophilia (also spelled hemophilia) is a group of hereditary genetic disorders that impairs the body's ability to control blood clotting, which is used to stop bleeding when a blood vessel is broken. Haemophilia A (clotting factor VIII deficiency) is the most common form of the disorder, present in about 1 in 5,000–10,000 male births. Haemophilia B (factor IX deficiency) occurs in around 1 in about 20,000–34,000 male births.
Like other recessive sex-linked, X chromosome disorders, haemophilia is more likely to occur in males than females. This is because females have two X chromosomes while males have only one, so the defective gene is guaranteed to manifest in any male who carries it. Because females have two X chromosomes and haemophilia is rare, the chance of a female having two defective copies of the gene is very remote, so females are almost exclusively asymptomatic carriers of the disorder. Female carriers can inherit the defective gene from either their mother or father, or it may be a new mutation. Although it is not impossible for a female to have haemophilia, it is unusual: daughters which are the product of both a male with haemophilia A or B and a female carrier will possess a 50% chance of having haemophilia, while the non-sex-linked haemophilia C due to coagulant factor XI deficiency, which can affect either sex, is more common in Jews of Ashkenazi (east European) descent but rare in other population groups.
People with haemophilia have lower clotting factor level of blood plasma or impaired activity of the coagulation factors needed for a normal clotting process. Thus when a blood vessel is injured, a temporary scab does form, but the missing coagulation factors prevent fibrin formation, which is necessary to maintain the blood clot. A haemophiliac does not bleed more intensely than a person without it, but can bleed for a much longer time. In severe haemophiliacs even a minor injury can result in blood loss lasting days or weeks, or even never healing completely. In areas such as the brain or inside joints, this can be fatal or permanently debilitating. The word is from the Greek "haima" αἷμα 'blood' and "philia" φιλία 'love'.
Signs and symptoms.
Characteristic symptoms vary with severity. In general symptoms are internal or external bleeding episodes, which are called "bleeds". People with more severe haemophilia suffer more severe and more frequent bleeds, while people with mild haemophilia usually suffer more minor symptoms except after surgery or serious trauma. Moderate haemophiliacs have variable symptoms which manifest along a spectrum between severe and mild forms.
In both haemophilia A and B, there is spontaneous bleeding but a normal bleeding time, normal prothrombin time, normal thrombin time, but prolonged partial thromboplastin time. Internal bleeding is common in people with severe haemophilia and some individuals with moderate haemophilia. The most characteristic type of internal bleed is a joint bleed where blood enters into the joint spaces. This is most common with severe haemophiliacs and can occur spontaneously (without evident trauma). If not treated promptly, joint bleeds can lead to permanent joint damage and disfigurement. Bleeding into soft tissues such as muscles and subcutaneous tissues is less severe but can lead to damage and requires treatment.
Children with mild to moderate haemophilia may not have any signs or symptoms at birth especially if they do not undergo circumcision. Their first symptoms are often frequent and large bruises and haematomas from frequent bumps and falls as they learn to walk. Swelling and bruising from bleeding in the joints, soft tissue, and muscles may also occur. Children with mild haemophilia may not have noticeable symptoms for many years. Often, the first sign in very mild haemophiliacs is heavy bleeding from a dental procedure, an accident, or surgery. Females who are carriers usually have enough clotting factors from their one normal gene to prevent serious bleeding problems, though some may present as mild haemophiliacs.
Complications.
Severe complications are much more common in severe and moderate haemophiliacs. Complications may be both directly from the disease or from its treatment:
Haemophilic arthropathy is characterized by chronic proliferative synovitis and cartilage destruction. If an intra-articular bleed is not drained early, it may cause apoptosis of chondrocytes and affect the synthesis of proteoglycans. The hypertrophied and fragile synovial lining while attempting to eliminate excessive blood may be more likely to easily rebleed, leading to a vicious cycle of hemarthrosis-synovitis-hemarthrosis. In addition, iron deposition in the synovium may induce an inflammatory response activating the immune system and stimulating angiogenesis, resulting in cartilage and bone destruction.
Life expectancy.
Like most aspects of the disorder, life expectancy varies with severity and adequate treatment. People with severe haemophilia who don't receive adequate, modern treatment have greatly shortened lifespans and often do not reach maturity. Prior to the 1960s when effective treatment became available, average life expectancy was only 11 years. By the 1980s the life span of the average haemophiliac receiving appropriate treatment was 50–60 years. Today with appropriate treatment, males with haemophilia typically have a near normal quality of life with an average lifespan approximately 10 years shorter than an unaffected male.
Since the 1980s the primary leading cause of death of people with severe haemophilia has shifted from haemorrhage to HIV/AIDS acquired through treatment with contaminated blood products. The second leading cause of death related to severe haemophilia complications is intracranial haemorrhage which today accounts for one third of all deaths of people with haemophilia. Two other major causes of death include hepatitis infections causing cirrhosis and obstruction of air or blood flow due to soft tissue haemorrhage.
Causes.
Genetics.
Females possess two X-chromosomes, males have one X and one Y-chromosome. Since the mutations causing the disease are X-linked, a woman carrying the defect on one of her X-chromosomes may not be affected by it, as the equivalent allele on her other chromosome should express itself to produce the necessary clotting factors, due to X inactivation. However, the Y-chromosome in men has no gene for factors VIII or IX. If the genes responsible for production of factor VIII or factor IX present on a male's X-chromosome are deficient there is no equivalent on the Y-chromosome to cancel it out, so the deficient gene is not masked and he will develop the illness.
Since a male receives his single X-chromosome from his mother, the son of a healthy female silently carrying the deficient gene will have a 50% chance of inheriting that gene from her and with it the disease; and if his mother is affected with haemophilia, he will have a 100% chance of being a haemophiliac. In contrast, for a female to inherit the disease, she must receive two deficient X-chromosomes, one from her mother and the other from her father (who must therefore be a haemophiliac himself). Hence haemophilia is far more common among males than females. However, it is possible for female carriers to become mild haemophiliacs due to lyonisation (inactivation) of the X-chromosomes. Haemophiliac daughters are more common than they once were, as improved treatments for the disease have allowed more haemophiliac males to survive to adulthood and become parents. Adult females may experience menorrhagia (heavy periods) due to the bleeding tendency. The pattern of inheritance is criss-cross type. This type of pattern is also seen in colour blindness.
A mother who is a carrier has a 50% chance of passing the faulty X-chromosome to her daughter, while an affected father will always pass on the affected gene to his daughters. A son cannot inherit the defective gene from his father. This is a recessive trait and can be passed on if cases are more severe with carrier.
Genetic testing and genetic counselling is recommended for families with haemophilia. Prenatal testing, such as amniocentesis, is available to pregnant women who may be carriers of the condition.
As with all genetic disorders, it is of course also possible for a human to acquire it spontaneously through mutation, rather than inheriting it, because of a new mutation in one of their parents' gametes. Spontaneous mutations account for about 33% of all cases of haemophilia A. About 30% of cases of haemophilia B are the result of a spontaneous gene mutation.
If a female gives birth to a haemophiliac child, either the female is a carrier for the blood disorder or the haemophilia was the result of a spontaneous mutation. Until modern direct DNA testing, however, it was impossible to determine if a female with only healthy children was a carrier or not. Generally, the more healthy sons she bore, the higher the probability that she was not a carrier.
If a male is afflicted with the disease and has children with a female who is not even a carrier, his daughters will be carriers of haemophilia. His sons, however, will not be affected with the disease. The disease is X-linked and the father cannot pass haemophilia through the Y-chromosome. Males with the disorder are then no more likely to pass on the gene to their children than carrier females, though all daughters they sire will be carriers and all sons they father will not have haemophilia (unless the mother is a carrier).
Severity.
There are numerous different mutations which cause each type of haemophilia. Due to differences in changes to the genes involved, people with haemophilia often have some level of active clotting factor. Individuals with less than 1% active factor are classified as having severe haemophilia, those with 1-5% active factor have moderate haemophilia, and those with mild haemophilia have between 5-40% of normal levels of active clotting factor.
Diagnosis.
Haemophilia A can be mimicked by von Willebrand disease.
Additionally, severe cases of vitamin K deficiency can present similar symptoms to haemophilia. This is because vitamin K is necessary for the human body to produce several protein clotting factors. This vitamin deficiency is rare in adults and older children but is common in newborns. Infants are born with naturally low levels of vitamin K and do not yet have the symbiotic gut flora to properly synthesise their own vitamin K. Bleeding issues due to vitamin K deficiency in infants is known as "haemorrhagic disease of the newborn". To avoid this complication, newborns are routinely injected with vitamin K supplements.
Management.
Though there is no cure for haemophilia, it can be controlled with regular infusions of the deficient clotting factor, i.e. factor VIII in haemophilia A or factor IX in haemophilia B. Factor replacement can be either isolated from human blood serum, recombinant, or a combination of the two. Some haemophiliacs develop antibodies (inhibitors) against the replacement factors given to them, so the amount of the factor has to be increased or non-human replacement products must be given, such as porcine factor VIII.
If a person becomes refractory to replacement coagulation factor as a result of circulating inhibitors, this may be partially overcome with recombinant human factor VII (NovoSeven), which is registered for this indication in many countries.
In early 2008, the US Food and Drug Administration (FDA) approved Xyntha (Wyeth) anti-haemophilic factor, genetically engineered from the genes of Chinese hamster ovary cells. Since 1993 (Dr. Mary Nugent) recombinant factor products (which are typically cultured in Chinese hamster ovary (CHO) tissue culture cells and involve little, if any human plasma products) have been available and have been widely used in wealthier western countries. While recombinant clotting factor products offer higher purity and safety, they are, like concentrate, extremely expensive, and not generally available in the developing world. In many cases, factor products of any sort are difficult to obtain in developing countries.
In Western countries, common standards of care fall into one of two categories: prophylaxis or on-demand. Prophylaxis involves the infusion of clotting factor on a regular schedule in order to keep clotting levels sufficiently high to prevent spontaneous bleeding episodes. On-demand treatment involves treating bleeding episodes once they arise. In 2007, a clinical trial was published in the "New England Journal of Medicine" comparing on-demand treatment of boys (< 30 months) with haemophilia A with prophylactic treatment (infusions of 25 IU/kg body weight of Factor VIII every other day) in respect to its effect on the prevention of joint-diseases. When the boys reached 6 years of age, 93% of those in the prophylaxis group and 55% of those in the episodic-therapy group had a normal index joint-structure on MRI. Prophylactic treatment, however, resulted in average costs of $300,000 per year. The author of an editorial published in the same issue of the "NEJM" supports the idea that prophylactic treatment not only is more effective than on demand treatment but also suggests that starting after the first serious joint-related haemorrhage may be more cost effective than waiting until the fixed age to begin. This study resulted in the first (October 2008) FDA approval to label any Factor VIII product to be used prophylactically. As a result, the factor product used in the study (Bayer's Kogenate) is now labelled for use to prevent bleeds, making it more likely that insurance carriers in the US will reimburse consumers who are prescribed and use this product prophylactically. Despite Kogenate only recently being "approved" for this use in the US, it and other factor products have been well studied and are often prescribed to treat Haemophilia prophylactically to prevent bleeds, especially joint bleeds.
Gene therapy.
On 10 December 2011, a team of British and American investigators reported the successful treatment of haemophilia B using gene therapy. The investigators inserted the "F9" gene into an adeno-associated virus-8 vector, which has a propensity for the liver, where factor 9 is produced, and remains outside the chromosomes so as not to disrupt other genes. The transduced virus was infused intravenously. To prevent rejection, the people were primed with steroids to suppress their immune response. In October 2013, the Royal Free London NHS Foundation Trust in London reported that after treating six people with haemophilia in early 2011 with the genetically modified adeno-associated virus, over two years later all were still producing blood plasma clotting factor.
Preventive exercises.
It is recommended that people affected with haemophilia do specific exercises to strengthen the joints, particularly the elbows, knees, and ankles. Exercises include elements which increase flexibility, tone, and strength of muscles, increasing their ability to protect joints from damaging bleeds. These exercises are recommended after an internal bleed occurs and on a daily basis to strengthen the muscles and joints to prevent new bleeding problems. Many recommended exercises include standard sports warm-up and training exercises such as stretching of the calves, ankle circles, elbow flexions, and quadriceps sets.
Contraindications.
Anticoagulants such as heparin and warfarin are contraindicated for people with haemophilia as these can aggravate clotting difficulties. Also contraindicated are those drugs which have "blood thinning" side effects. For instance, medicines which contain aspirin, ibuprofen, or naproxen sodium should not be taken because they are well known to have the side effect of prolonged bleeding.
Also contraindicated are activities with a high likelihood of trauma, such as motorcycling and skateboarding. Popular sports with very high rates of physical contact and injuries such as American football, hockey, boxing, wrestling, and rugby should be avoided by people with haemophilia. Other active sports like soccer, baseball, and basketball also have a high rate of injuries, but have overall less contact and should be undertaken cautiously and only in consultation with a doctor.
Epidemiology.
Haemophilia is rare, with only about 1 instance in every 10,000 births (or 1 in 5,000 male births) for haemophilia A and 1 in 50,000 births for haemophilia B. About 18,000 people in the United States have haemophilia. Each year in the US, about 400 babies are born with the disorder. Haemophilia usually occurs in males and less often in females. It is estimated that about 2500 Canadians have haemophilia A, and about 500 Canadians have haemophilia B.
History.
Scientific discovery.
The first medical professional to describe the disease was Abulcasis. In the tenth century he described families whose males died of bleeding after only minor traumas. While many other such descriptive and practical references to the disease appear throughout historical writings, scientific analysis did not begin until the start of the nineteenth century.
In 1803, Dr. John Conrad Otto, a Philadelphian physician, wrote an account about "a hemorrhagic disposition existing in certain families" in which he called the affected males "bleeders". He recognised that the disorder was hereditary and that it affected mostly males and was passed down by healthy females. His paper was the second paper to describe important characteristics of an X-linked genetic disorder (the first paper being a description of colour blindness by John Dalton who studied his own family). Otto was able to trace the disease back to a woman who settled near Plymouth, NH in 1720. The idea that affected males could pass the trait onto their unaffected daughters was not described until 1813 when John F. Hay, published an account in The New England Journal of Medicine.
In 1924, a Finnish doctor discovered a hereditary bleeding disorder similar to Haemophilia localised in the "Åland Islands", southwest of Finland. This bleeding disorder is called "Von Willebrand Disease".
The term "haemophilia" is derived from the term "haemorrhaphilia" which was used in a description of the condition written by Friedrich Hopff in 1828, while he was a student at the University of Zurich. In 1937, Patek and Taylor, two doctors from Harvard, discovered anti-haemophilic globulin. In 1947, Pavlosky, a doctor from Buenos Aires, found haemophilia A and haemophilia B to be separate diseases by doing a lab test. This test was done by transferring the blood of one haemophiliac to another haemophiliac. The fact that this corrected the clotting problem showed that there was more than one form of haemophilia.
European royalty.
Haemophilia has featured prominently in European royalty and thus is sometimes known as 'the royal disease'. Queen Victoria passed the mutation for Haemophilia B to her son Leopold and, through two of her daughters, Alice and Beatrice, to various royals across the continent, including the royal families of Spain, Germany, and Russia. In Russia, Tsarevich Alexei Nikolaevich, son of Nicholas II, was a descendant of Queen Victoria through his mother Empress Alexandra and suffered from haemophilia.
It was claimed that Rasputin was successful at treating Tsarevich Alexei's haemophilia. At the time, a common treatment administered by professional doctors was to use aspirin, which worsened rather than lessened the problem. It is believed that, by simply advising against the medical treatment, Rasputin could bring visible and significant improvement to the condition of Tsarevich Alexei.
In Spain, Queen Victoria's youngest daughter, Princess Beatrice, had a daughter Victoria Eugenie of Battenberg, who later became Queen of Spain. Two of her sons were haemophiliacs and both died from minor car accidents. Her eldest son, Prince Alfonso of Spain, Prince of Asturias, died at the age of 31 from internal bleeding after his car hit a telephone booth. Her youngest son, Infante Gonzalo, died at age 19 from abdominal bleeding following a minor car accident where he and his sister hit a wall while avoiding a cyclist. Neither appeared injured or sought immediate medical care and Gonzalo died two days later from internal bleeding.
Blood contamination issues.
Prior to 1985, there were no laws enacted within the U.S. to screen blood. As a result, many people with haemophilia who received untested and unscreened clotting factor prior to 1992 were at an extreme risk for contracting HIV and hepatitis C via these blood products. It is estimated that more than 50% of the haemophilia population, i.e. over 10,000 people, contracted HIV from the tainted blood supply in the United States alone.
As a direct result of the contamination of the blood supply in the late 1970s and early/mid-1980s with viruses such as hepatitis and HIV, new methods were developed in the production of clotting factor products. The initial response was to heat-treat (pasteurise) plasma-derived factor concentrate, followed by the development of monoclonal factor concentrates, which use a combination of heat treatment and affinity chromatography to inactivate any viral agents in the pooled plasma from which the factor concentrate is derived. The Lindsay Tribunal in Ireland investigated, among other things, the slow adoption of the new methods.

</doc>
<doc id="14008" url="https://en.wikipedia.org/wiki?curid=14008" title="Hickory (disambiguation)">
Hickory (disambiguation)

Hickory is a type of tree ("Carya" species) found in North America and East Asia.
Hickory may also refer to:
Place names.
In the United States:

</doc>
<doc id="14009" url="https://en.wikipedia.org/wiki?curid=14009" title="Hemicellulose">
Hemicellulose

A hemicellulose (also known as polyose) is any of several heteropolymers (matrix polysaccharides), such as arabinoxylans, present along with cellulose in almost all plant cell walls. While cellulose is crystalline, strong, and resistant to hydrolysis, hemicellulose has a random, amorphous structure with little strength. It is easily hydrolyzed by dilute acid or base as well as myriad hemicellulase enzymes.
Composition.
Hemicelluloses include xylan, glucuronoxylan, arabinoxylan, glucomannan, and xyloglucan.
These polysaccharides contain many different sugar monomers. In contrast, cellulose contains only anhydrous glucose. For instance, besides glucose, sugar monomers in hemicellulose can include xylose, mannose, galactose, rhamnose, and arabinose. Hemicelluloses contain most of the D-pentose sugars, and occasionally small amounts of L-sugars as well. Xylose is in most cases the sugar monomer present in the largest amount, although in softwoods mannose can be the most abundant sugar. Not only regular sugars can be found in hemicellulose, but also their acidified form, for instance glucuronic acid and galacturonic acid can be present.
Structural comparison to cellulose.
Unlike cellulose, hemicellulose (also a polysaccharide) consists of shorter chains – 500–3,000 sugar units as opposed to 7,000–15,000 glucose molecules per polymer seen in cellulose. In addition, hemicellulose is a branched polymer, while cellulose is unbranched.
Native structure.
Hemicelluloses are embedded in the cell walls of plants, sometimes in chains that form a 'ground' - they bind with pectin to cellulose to form a network of cross-linked fibres.
Biosynthesis.
Hemicelluloses are synthesised from sugar nucleotides in the cell's Golgi apparatus. Two models explain their synthesis: 1) a ‘2 component model' where modification occurs at two transmembrane proteins, and 2) a '1 component model' where modification occurs only at one transmembrane protein. After synthesis, hemicelluloses are transported to the plasma membrane via Golgi vesicles.
Applications.
As percent content of hemicellulose increases in animal feed, the voluntary feed intake decreases.
Hemicellulose is represented by the difference between neutral detergent fiber (NDF) and acid detergent fiber (ADF).
Functions.
Microfibrils are cross-linked together by hemicellulose homopolymers. Lignins assist and strengthen the attachment of hemicelluloses to microfibrils.
Hemicellulose from trees.
Hemicellulose found in hardwood trees is predominantly xylan with some glucomannan, while in softwoods it is mainly rich in galactoglucomannan and contains only a small amount of xylan. The average molecular weight is lower than that of cellulose at less than 30,000, as opposed to the 100,000 average molecular weight reported for cellulose.

</doc>
<doc id="14011" url="https://en.wikipedia.org/wiki?curid=14011" title="Hillbilly">
Hillbilly

Hillbilly is a term (often derogatory) for people who dwell in rural, mountainous areas in the United States, primarily in Appalachia and the Ozarks, Uwharrie Mountains and Caraway Mountains. Due to its strongly stereotypical connotations, the term can be offensive to those Americans of Appalachian or Ozark heritage. "Hillbilly" first appeared in print in a 1900 "New York Journal" article, with the definition: "a Hill-Billie is a free and untrammeled white citizen of Alabama, who lives in the hills, has no means to speak of, dresses as he can, talks as he pleases, drinks whiskey when he gets it, and fires off his revolver as the fancy takes him." The stereotype is two-fold in that it incorporates both positive and negative traits: “Hillbillies” are often considered independent and self-reliant individuals who resist the modernization of society, but at the same time they are also defined as backward and violent. Scholars argue this duality is reflective of the split ethnic identities in “white America." 
Etymology and history.
The Appalachian Mountains were settled in the 18th century by settlers primarily from the Province of Ulster in Ireland. The settlers from Ulster were mainly Protestants who migrated to Ireland during the Plantation of Ulster in the 17th century from Scotland and Northern England. Many further migrated to the American colonies beginning in the 1730s, and in America became known as the Scotch-Irish. Scholars argue that the term "hillbilly" originated from Scottish dialect. The term "hill-folk" referred to people that preferred isolation from the greater society and "billy" meant “comrade” or “companion." It is suggested that “hill-folk” and “billie” were combined when the Cameronians fled to the Highlands.
Others have suggested the term originated in 17th century Ireland, during the Williamite War, when Protestant supporters of King William III (""King Billy"") were often referred to as "Billy's Boys." Some scholars disagree with this theory. Michael Montgomery's "From Ulster to America: The Scotch-Irish Heritage of American English" states, "In Ulster in recent years it has sometimes been supposed that it was coined to refer to followers of King William III and brought to America by early Ulster emigrants, but this derivation is almost certainly incorrect… In America "hillbilly" was first attested only in 1898, which suggests a later, independent development." 
The term "hillbilly" spread in the years following the American Civil War. At this time, the country was developing both technologically and socially, but the Appalachian region was falling behind. Before the war, Appalachia was not distinctively different from other rural areas of the country. Post-war, although the frontier pushed farther west, the region maintained frontier characteristics. Appalachians themselves were perceived as backward, quick to violence and inbred in their isolation. Fueled by news stories of mountain feuds such as that in the 1880s between the Hatfields and McCoys, the hillbilly stereotype developed in the late 19th to early 20th century.
The "classic" hillbilly stereotype reached its current characterization during the years of the Great Depression when many mountaineers left their homes to find work in other areas of the country. The period of Appalachian out-migration, roughly from the 1930s through the 1950s, saw many mountain residents moving North to the Midwestern industrial cities of Chicago, Cleveland, Akron and Detroit. This movement North became known as the "Hillbilly Highway." The movement brought these previously isolated communities into mainstream United States culture. Poor white mountaineers became central characters in newspapers, pamphlets and eventually, motion pictures. Authors at this time were inspired by historical figures such as Davy Crockett and Daniel Boone. The mountaineer image transferred over to the 20th century where the “hillbilly” stereotype emerged.
In popular culture.
Pop culture has perpetuated the hillbilly stereotype. The misrepresentation of Appalachian people in the media has led to great cultural distortion of Appalachian beliefs, practices, and lifestyle. Scholarly works suggest that the media has exploited both the Appalachian region and people by classifying them as "hillbillies." These generalizations do not match the cultural experiences of Appalachians. Appalachians, like many other groups, do not subscribe to a single identity. One of the issues associated with stereotyping is that it is profitable. When “hillbilly” became a widely used term, entrepreneurs saw a window for potential revenue. They “recycled” the image and brought it to life through various forms of media.
Television and film have portrayed "hillbillies" in both derogatory and sympathetic terms. Films such as "Sergeant York" or the Ma and Pa Kettle series portrayed the hillbilly as wild but good-natured. Television programs of the 1960s such as "The Real McCoys", "The Andy Griffith Show", and especially "The Beverly Hillbillies", portrayed the hillbilly as backwards but with enough wisdom to outwit more sophisticated city folk. "Gunsmoke"'s Festus Haggen was portrayed as intelligent and quick-witted (but lacking "education"). The popular 1970s television variety show "Hee Haw" regularly lampooned the stereotypical hillbilly lifestyle. A darker image of the hillbilly is found in the film "Deliverance" (1972), based on a novel by James Dickey, which depicted some hillbillies as genetically deficient, inbred and murderous, while depicting others as helpful, friendly, and smart.
“Hillbillies” were at the center of reality television in the 21st century. Network television shows such as New Beverly Hillbillies, High Life, and The Simple Life displayed the “hillbilly” lifestyle for viewers in the United States. This sparked protests across the country with rural-minded individuals gathering to fight the stereotype. The Center for Rural Strategies started a nationwide campaign stating the stereotype was “politically incorrect.” The Kentucky-based organization engaged political figures in the movement such as Robert Byrd and Mike Huckabee. Both protestors argued that the discrimination of any other group in United States would not be tolerated, so neither should the discrimination against rural U.S. citizens. A 2003 piece published by The Cincinnati Enquirer read, “In this day of hypersensitivity to diversity and political correctness, Appalachians have been a group that it is still socially acceptable to demean and joke about…But rural folks have spoken up and said “enough” to the Hollywood mockers." 
Music.
"Hillbilly music" was at one time considered an acceptable label for what is now known as country music. The label, coined in 1925 by country pianist Al Hopkins, persisted until the 1950s.
The "hillbilly music" categorization covers a wide variety of musical genres including bluegrass, country western, and gospel. Appalachian folk song existed long before the "hillbilly" label. When the commercial industry was combined with "traditional Appalachian folksong," "hillbilly music" was formed. Some argue this is a "High Culture" issue where sophisticated individuals may see something considered "unsophisticated" as "trash."
In the early 20th century, artists began to utilize the "hillbilly" label. The York Brothers entitled one of their songs "Hillbilly Rose" and the Delmore Brothers followed with their song "Hillbilly Boogie." In 1927, the Gennett studios in Richmond, Indiana, made a recording of black fiddler Jim Booker. The recordings were labeled "made for Hillbilly" in the Gennett files and were marketed to a white audience. Columbia Records had much success with the "Hill Billies" featuring Al Hopkins and Fiddlin' Charlie Bowman.
By the late 1940s, radio stations started to use the "hillbilly music" label. Originally, "hillbilly" was used to describe fiddlers and string bands, but now it was used to describe traditional Appalachian music. Appalachians had never used this term to describe their own music. Popular songs whose style bore characteristics of both hillbilly and African American music were referred to as hillbilly boogie and "rockabilly". Elvis Presley was a prominent player of rockabilly and was known early in his career as the "Hillbilly Cat."
When the Country Music Association was founded in 1958, the term "hillbilly music" gradually fell out of use. The music industry merged hillbilly music, Western swing, and Cowboy music, to form the current category C&W, Country and Western.
Some artists and fans (notably Hank Williams Sr.) were offended by the "hillbilly music" label. While the term is not used as frequently today, it is still used on occasion to refer to old-time music or bluegrass. For example, WHRB broadcasts a popular weekly radio show entitled "Hillbilly at Harvard." The show is devoted to playing a mix of old-time music, bluegrass, and traditional country and western
Cultural implications.
The hillbilly stereotype has a traumatizing effect on some in the Appalachian region. Feelings of shame, self-hatred, and detachment are cited as a result of "culturally transmitted traumatic stress syndrome." Appalachian scholars say that the large-scale stereotyping has rewritten Appalachian history, making Appalachians feel particularly vulnerable. "Hillbilly" has now become part of Appalachian identity and some Appalachians feel they are constantly defending themselves against this image.
The stereotyping also has political implications for the region. There is a sense of "perceived history" that prevents many political issues from receiving adequate attention. Appalachians are often blamed for economic struggles. "Moonshiners, welfare cheats, and coal miners" are stereotypes stemming from the greater hillbilly stereotype in the region. This prejudice serves as a barrier for addressing some serious issues such as the economy and the environment.
Despite the political and social difficulties associated with stereotyping, Appalachians have organized to enact change. The War on Poverty is an example of one effort that allowed for Appalachian community organization. Grassroots movements, protests, and strikes are common in the area, though not always successful.
Intragroup versus intergroup usage.
The Springfield, Missouri Chamber of Commerce once presented dignitaries visiting the city with an "Ozark Hillbilly Medallion" and a certificate proclaiming the honoree a "hillbilly of the Ozarks." On June 7, 1952, President Harry S. Truman received the medallion after a breakfast speech at the Shrine Mosque for the 35th Division Association. Other recipients included US Army generals Omar Bradley and Matthew Ridgeway, J. C. Penney, Johnny Olsen and Ralph Story.
Hillbilly Days is an annual festival held in mid-April in Pikeville, Kentucky celebrating the best of Appalachian culture. The event began by local Shriners as a fundraiser to support the Shriners Children's Hospital. It has grown since its beginning in 1976 and now is the second largest festival held in the state of Kentucky. Artists and craftspeople showcase their talents and sell their works on display. Nationally renowned musicians as well as the best of the regional mountain musicians share six different stages located throughout the downtown area of Pikeville. Want-to-be hillbillies from across the nation compete to come up with the wildest Hillbilly outfit. The event has earned its name as the Mardi Gras of the Mountains. Fans of "mountain music" come from around the United States to hear this annual concentrated gathering of talent. Some refer to this event as the equivalent of a "Woodstock" for mountain music.

</doc>
<doc id="14012" url="https://en.wikipedia.org/wiki?curid=14012" title="Host">
Host

Host (masculine) and hostess (feminine) most often refer to a person responsible for guests at an event or providing hospitality during it, or to an event's presenter or master or mistress of ceremonies. Host or hosts may also refer to:

</doc>
<doc id="14013" url="https://en.wikipedia.org/wiki?curid=14013" title="Hernán Cortés">
Hernán Cortés

Hernán Cortés de Monroy y Pizarro, Marquis of the Valley of Oaxaca (; 1485 – December 2, 1547) was a Spanish "Conquistador" who led an expedition that caused the fall of the Aztec Empire and brought large portions of mainland Mexico under the rule of the King of Castile in the early 16th century. Cortés was part of the generation of Spanish colonizers who began the first phase of the Spanish colonization of the Americas.
Born in Medellín, Spain, to a family of lesser nobility, Cortés chose to pursue a livelihood in the New World. He went to Hispaniola and later to Cuba, where he received an "encomienda" and, for a short time, became alcalde (magistrate) of the second Spanish town founded on the island. In 1519, he was elected captain of the third expedition to the mainland, an expedition which he partly funded. His enmity with the Governor of Cuba, Diego Velázquez de Cuéllar, resulted in the recall of the expedition at the last moment, an order which Cortés ignored.
Arriving on the continent, Cortés executed a successful strategy of allying with some indigenous people against others. He also used a native woman, Doña Marina, as an interpreter; she would later bear Cortés a son. When the Governor of Cuba sent emissaries to arrest Cortés, he fought them and won, using the extra troops as reinforcements. Cortés wrote letters directly to the king asking to be acknowledged for his successes instead of punished for mutiny. After he overthrew the Aztec Empire, Cortés was awarded the title of "Marqués del Valle de Oaxaca", while the more prestigious title of Viceroy was given to a high-ranking nobleman, Antonio de Mendoza. In 1541 Cortés returned to Spain, where he died peacefully but embittered, six years later.
Because of the controversial undertakings of Cortés and the scarcity of reliable sources of information about him, it has become difficult to assert anything definitive about his personality and motivations. Early lionizing of the conquistadors did not encourage deep examination of Cortés. Later reconsideration of the conquistadors' character in the context of modern anti-colonial sentiment also did little to expand understanding of Cortés as an individual. As a result of these historical trends, descriptions of Cortés tend to be simplistic, and either damning or idealizing.
Name.
While now generally called "Hernán", Cortés himself used the form "Hernando" or "Fernando". All are equally correct.
Early life.
Cortés was born in 1485 in the town of Medellín, in modern-day Extremadura, Spain. His father, Martín Cortés de Monroy, born in 1449 to Rodrigo or Ruy Fernández de Monroy and his wife María Cortés, was an infantry captain of distinguished ancestry but slender means. Hernán's mother was Catalina Pizarro Altamirano.
Through his mother, Hernán was the second cousin once removed of Francisco Pizarro, who later conquered the Inca Empire of modern-day Peru (not to be confused with another Francisco Pizarro who joined Cortés to conquer the Aztecs), through her parents Diego Altamirano and wife and cousin Leonor Sánchez Pizarro Altamirano, first cousin of Pizarro's father. Through his father, Hernán was a twice distant relative of Nicolás de Ovando, the third Governor of Hispaniola. His paternal grandfather was a son of Rodrigo de Monroy y Almaraz, 5th Lord of Monroy, and wife Mencía de Orellana y Carvajal.
Hernán Cortés is described as a pale, sickly child by his biographer, chaplain, and friend Francisco López de Gómara. At the age of 14, Cortés was sent to study Latin under an uncle-in-law in Salamanca.
After two years, Cortés, tired of schooling, returned home to Medellín, much to the irritation of his parents, who had hoped to see him equipped for a profitable legal career. However, those two years at Salamanca, plus his long period of training and experience as a notary, first in Seville and later in Hispaniola, would give him a close acquaintance with the legal codes of Castile that helped him to justify his unauthorized conquest of Mexico.
At this point in his life, Cortés was described by Gómara as restless, haughty and mischievous. This was probably a fair description of a 16-year-old boy who had returned home only to find himself frustrated by life in his small provincial town. By this time, news of the exciting discoveries of Christopher Columbus in the New World was streaming back to Spain.
Early career in the New World.
Plans were made for Cortés to sail to the Americas with a family acquaintance and distant relative, Nicolás de Ovando, the newly appointed governor of Hispaniola (currently Haiti and the Dominican Republic), but an injury he sustained while hurriedly escaping from the bedroom of a married woman from Medellín prevented him from making the journey. Instead, he spent the next year wandering the country, probably spending most of his time in the heady atmosphere of Spain's southern ports of Cadiz, Palos, Sanlucar, and Seville, listening to the tales of those returning from the Indies, who told of discovery and conquest, gold, Indians, and strange unknown lands. He finally left for Hispaniola in 1504 where he became a colonist.
Arrival.
Cortés reached Hispaniola in a ship commanded by Alonso Quintero, who tried to deceive his superiors and reach the New World before them in order to secure personal advantages. Quintero's mutinous conduct may have served as a model for Cortés in his subsequent career. The history of the conquistadores is rife with accounts of rivalry, jockeying for positions, mutiny, and betrayal.
Upon his arrival in 1504 in Santo Domingo, the capital of Hispaniola, the 18-year-old Cortés registered as a citizen, which entitled him to a building plot and land to farm. Soon afterwards, Nicolás de Ovando, still the governor, gave him an "encomienda" and made him a notary of the town of Azua de Compostela. His next five years seemed to help establish him in the colony; in 1506, Cortés took part in the conquest of Hispaniola and Cuba, receiving a large estate of land and Indian slaves for his efforts from the leader of the expedition.
Cuba (1511–1519).
In 1511, Cortés accompanied Diego Velázquez de Cuéllar, an aide of the Governor of Hispaniola, in his expedition to conquer Cuba. Velázquez was appointed as governor. At the age of 26, Cortés was made clerk to the treasurer with the responsibility of ensuring that the Crown received the "quinto", or customary one fifth of the profits from the expedition.
The Governor of Cuba, Diego Velázquez, was so impressed with Cortés that he secured a high political position for him in the colony. He became secretary for Governor Velázquez. Cortés was twice appointed municipal magistrate ("alcalde") of Santiago. In Cuba, Cortés became a man of substance with an "encomienda" to provide Indian labor for his mines and cattle. This new position of power also made him the new source of leadership, which opposing forces in the colony could then turn to. In 1514, Cortés led a group which demanded that more Indians be assigned to the settlers.
As time went on, relations between Cortés and Governor Velázquez became strained. This began once news of Juan de Grijalva, establishing a colony on the mainland where there was a bonanza of silver and gold, reached Velázquez; it was decided to send him help. Cortés was appointed Captain-General of this new expedition in October 1518, but was advised to move fast before Velázquez changed his mind.
With Cortés's experience as an administrator, knowledge gained from many failed expeditions, and his impeccable rhetoric he was able to gather six ships and 300 men, within a month. Predictably, Velázquez's jealousy exploded and decided to place the leadership of the expedition in other hands. However, Cortés quickly gathered more men and ships in other Cuban ports.
Cortés also found time to become romantically involved with Catalina Xuárez (or Juárez), the sister-in-law of Governor Velázquez. Part of Velázquez's displeasure seems to have been based on a belief that Cortés was trifling with Catalina's affections. Cortés was temporarily distracted by one of Catalina's sisters but finally married Catalina, reluctantly, under pressure from Governor Velázquez. However, by doing so, he hoped to secure the good will of both her family and that of Velázquez.
It was not until he had been almost 15 years in the Indies, that Cortés began to look beyond his substantial status as mayor of the capital of Cuba and as a man of affairs in the thriving colony. He missed the first two expeditions, under the orders of Francisco Hernández de Córdoba and then Juan de Grijalva, sent by Diego Velázquez to Mexico in 1518.
Conquest of Mexico (1519–1521).
In 1518, Velázquez put Cortés in command of an expedition to explore and secure the interior of Mexico for colonization. At the last minute, due to the old argument between the two, Velázquez changed his mind and revoked Cortés's charter. He ignored the orders and, in an act of open mutiny, went anyway in February 1519. He stopped in Trinidad, Cuba, to hire more soldiers and obtain more horses. Accompanied by about 11 ships, 500 men, 13 horses, and a small number of cannon, Cortés landed on the Yucatan Peninsula in Mayan territory. There he encountered Geronimo de Aguilar, a Spanish Franciscan priest who had survived a shipwreck followed by a period in captivity with the Maya, before escaping. Aguilar had learned the Chontal Maya language and was able to translate for Cortés.
In March 1519, Cortés formally claimed the land for the Spanish crown. Then he proceeded to Tabasco, where he met with resistance and won a battle against the natives. He received twenty young indigenous women from the vanquished natives, and he converted them all to Christianity.
Among these women was La Malinche, his future mistress and mother of his son Martín. Malinche knew both the Nahuatl language and the Chontal Maya, thus enabling Cortés to communicate with the Aztecs through Aguilar. At San Juan de Ulúa on Easter Sunday 1519, Cortés met with Moctezuma II's Aztec Empire governors Tendile and Pitalpitoque.
In July 1519, his men took over Veracruz. By this act, Cortés dismissed the authority of the Governor of Cuba to place himself directly under the orders of King Charles. In order to eliminate any ideas of retreat, Cortés scuttled his ships.
March on Tenochtitlan.
In Veracruz, he met some of the tributaries of the Aztecs and asked them to arrange a meeting with Moctezuma II, the "tlatoani" (ruler) of the Aztec Empire. Moctezuma repeatedly turned down the meeting, but Cortés was determined. Leaving a hundred men in Veracruz, Cortés marched on Tenochtitlan in mid-August 1519, along with 600 soldiers, 15 horsemen, 15 cannons, and hundreds of indigenous carriers and warriors.
On the way to Tenochtitlan, Cortés made alliances with indigenous peoples such as the Totonacs of Cempoala and the Nahuas of Tlaxcala. The Otomis initially, and then the Tlaxcalans fought the Spanish a series of three battles from 2 Sept. to 5 Sept. 1519, and at one point Diaz remarked, "they surrounded us on every side". After Cortés continued to release prisoners with messages of peace, and realizing the Spanish were enemies of Montezuma, Xicotencatl the Elder, and Maxixcatzin, persuaded the Tlaxcalan warleader, Xicotencatl the Younger, that it would be better to ally with the newcomers than to kill them.
In October 1519, Cortés and his men, accompanied by about 1,000 Tlaxcalteca, marched to Cholula, the second largest city in central Mexico. Cortés, either in a pre-meditated effort to instill fear upon the Aztecs waiting for him at Tenochtitlan or (as he later claimed, when he was being investigated) wishing to make an example when he feared native treachery, massacred thousands of unarmed members of the nobility gathered at the central plaza, then partially burned the city.
By the time he arrived in Tenochtitlan the Spaniards had a large army. On November 8, 1519, they were peacefully received by Moctezuma II. Moctezuma deliberately let Cortés enter the Aztec capital, the island city of Tenochtitlan, hoping to get to know their weaknesses better and to crush them later.
Moctezuma gave lavish gifts of gold to the Spaniards which, rather than placating them, excited their ambitions for plunder. In his letters to King Charles, Cortés claimed to have learned at this point that he was considered by the Aztecs to be either an emissary of the feathered serpent god Quetzalcoatl or Quetzalcoatl himself – a belief which has been contested by a few modern historians. But quickly Cortés learned that several Spaniards on the coast had been killed by Aztecs while supporting the Totonacs, and decided to take Moctezuma as a hostage in his own palace, indirectly ruling Tenochtitlan through him.
Meanwhile, Velázquez sent another expedition, led by Pánfilo de Narváez, to oppose Cortés, arriving in Mexico in April 1520 with 1,100 men. Cortés left 200 men in Tenochtitlan and took the rest to confront Narváez. He overcame Narváez, despite his numerical inferiority, and convinced the rest of Narváez's men to join him. In Mexico, one of Cortés's lieutenants Pedro de Alvarado, committed the "massacre in the Great Temple", triggering a local rebellion.
Cortés speedily returned to Tenochtitlán. On July 1, 1520 Moctezuma was killed (the Spaniards claimed he was stoned to death by his own people; other claim he was murdered by the Spanish once they realized his inability to placate the locals). Faced with a hostile population, Cortés decided to flee for Tlaxcala. During the "Noche Triste" (June 30 – July 1, 1520), the Spaniards managed a narrow escape from Tenochtitlan across the Tlacopan causeway, while their backguard was being massacred. Much of the treasure looted by Cortés was lost (as well as his artillery) during this panicked escape from Tenochtitlán.
Destruction of Tenochtitlan.
After a battle in Otumba, they managed to reach Tlaxcala, having lost 870 men. With the assistance of their allies, Cortés's men finally prevailed with reinforcements arriving from Cuba. Cortés began a policy of attrition towards Tenochtitlan, cutting off supplies and subduing the Aztecs' allied cities. The siege of Tenochtitlán ended with Spanish victory and the destruction of the city.
In January 1521, Cortés countered a conspiracy against him, headed by Antonio de Villafana, who was hanged for the offense. Finally, with the capture of Cuauhtémoc, the "tlatoani" (ruler) of Tenochtitlán, on August 13, 1521, the Aztec Empire disappeared, and Cortés was able to claim it for Spain, thus renaming the city Mexico City. From 1521 to 1524, Cortés personally governed Mexico.
Appointment to governorship of Mexico and internal dissensions.
Many historical sources have conveyed an impression that Cortés was unjustly treated by the Spanish Crown, and that he received nothing but ingratitude for his role in establishing New Spain. This picture is the one Cortés presents in his letters and in the later biography written by Francisco López de Gómara. However, there may be more to the picture than this. Cortés's own sense of accomplishment, entitlement, and vanity may have played a part in his deteriorating position with the king:
King Charles appointed Cortés as governor, captain general and chief justice of the newly conquered territory, dubbed "New Spain of the Ocean Sea". But also, much to the dismay of Cortés, four royal officials were appointed at the same time to assist him in his governing – in effect, submitting him to close observation and administration. Cortés initiated the construction of Mexico City, destroying Aztec temples and buildings and then rebuilding on the Aztec ruins what soon became the most important European city in the Americas.
Cortés managed the founding of new cities and appointed men to extend Spanish rule to all of New Spain, imposing the "encomienda" system in 1524. He reserved many encomiendas for himself and for his retinue, which they considered just rewards for their accomplishment in conquering central Mexico. However, later arrivals and members of factions antipathetic to Cortés complained of the favoritism that excluded them.
In 1523, the Crown (possibly influenced by Cortés's enemy, Bishop Fonseca), sent a military force under the command of Francisco de Garay to conquer and settle the northern part of Mexico, the region of Pánuco. This was another setback for Cortés who mentioned this in his fourth letter to the King in which he describes himself as the victim of a conspiracy by his archenemies Diego Velázquez de Cuéllar, Diego Columbus and Bishop Fonseca as well as Francisco Garay. The influence of Garay was effectively stopped by this appeal to the King who sent out a decree forbidding Garay to interfere in the politics of New Spain, causing him to give up without a fight.
Granted Coat of Arms by the King, 1525.
Although Cortés had flouted the authority of Diego Velázquez in sailing to the mainland and then leading an expedition of conquest, Cortés's spectacular success was rewarded by the crown with a coat of arms, a mark of high honor, following the conqueror's request. The document granting the coat of arms summarizes Cortés's accomplishments in the conquest of Mexico. The proclamation of the king says in part We, respecting the many labors, dangers, and adventures which you underwent as stated above, and so that there might remain a perpetual memorial of you and your services and that you and your descendants might be more fully honored...it is our will that besides your coat of arms of your lineage, which you have, you may have and bear as your coat of arms, known and recognized, a shield... The grant specifies the iconography of the coat of arms, the central portion divided into quadrants. In the upper portion, there is a "black eagle with two heads on a white field, which are the arms of the empire." Below that is a "golden lion on a red field, in memory of the fact that you, the said Hernando Cortés, by your industry and effort brought matters to the state described above" (i.e., the conquest). The specificity of the other two quadrants is linked directly to Mexico, with one quadrant showing three crowns representing the three Aztec emperors of the conquest era, Moctezuma, Cuitlahuac, and Cuauhtemoc and the other showing the Aztec capital of Tenochtitlan. Encircling the central shield are symbols of the seven city-states around the lake and their lords that Cortés defeated, with the lords "to be shown as prisoners bound with a chain which shall be closed with a lock beneath the shield."
Death of his First Wife and Remarriage.
Cortés's wife Catalina Súarez arrived in New Spain from sometime around summer 1522, along with sister and brother. His marriage to Catalina was at this point extremely awkward, since she was a kinswoman of governor of Cuba Diego Velázquez, whose authority Cortés had thrown off and now his enemy. Catalina lacked the noble title of "doña," so at this point his alliance with her no longer raised his status. The marriage had been childless. Since Cortés had sired children with a variety of indigenous women, including a son ca. 1522 by his cultural translator, Doña Marina, Cortés knew he was capable of fathering children. Cortés's only male heir at this point was illegitimate, but nonetheless named after Cortés's father, Martín Cortés. This natural son Martín Cortés was sometimes called "El Mestizo." Cortés's wife, Catalina Suárez, died under mysterious circumstances the night of November 1–2, 1522. There were accusations at the time that Cortés had murdered his wife. There was an investigation into her death, interviewing a variety of household residents and others. The documentation of the investigation published in the nineteenth century in Mexico and archival documents uncovered in the twentieth century. The death of Catalina Suárez had produced a scandal and a major investigation, but weathering that Cortés was now free to marry someone of high status more appropriate to his wealth and power. In 1529 he had been accorded the noble designation of "don", but more importantly was given the noble title of Marquis of the Valley of Oaxaca and married the Spanish noblewoman Doña Juana de Zúñiga. The marriage produced three children, including another son, who was also named Martín. As the first-born legitimate son, Don Martín Cortés y Zúñiga was now Cortés's heir and succeeded his father as holder of the title and estate of the Marquisate of the Valley of Oaxaca. Cortés's legitimate daughters were Doña Maria, Doña Catalina, and Doña Juana.
Cortés and the "Spiritual Conquest" of Mexico.
Since the conversion to Christianity of indigenous peoples was an essential and integral part of the extension of Spanish power, making formal provisions for that conversion once the military conquest was completed was an important task for Cortés. During the Age of Discovery, the Catholic Church had seen early attempts at conversion in the Caribbean islands by Spanish friars, particularly mendicant orders. Cortés made a request to the Spanish monarch to send Franciscan and Dominican friars to Mexico to begin the daunting work of converting vast populations indigenous to Christianity. In his fourth letter to the king, Cortés pleaded for friars rather than diocesan or secular priests because those clerics were in his view a serious danger to the Indians' conversion. If these people were now to see the affairs of the Church and the service of God in the hands of canons or other dignitaries, and saw them indulge in the vices and profanities now common in Spain, knowing that such men were the ministers of God, it would bring our Faith into much harm that I believe any further preaching would be of no avail. He wished the mendicants to be the main evangelists. Mendicant friars did not usually have full priestly powers to perform all the sacraments needed for conversion of the Indians and growth of the neophytes in the Christian faith, so Cortés laid out a solution to this to the king. Your Majesty should likewise beseech His Holiness [the pope to grant these powers to the two principal persons in the religious orders that are to come here, and that they should be his delegates, one from the Order of St. Francis and the other from the Order of St. Dominic. They should bring the most extensive powers Your Majesty is able to obtain, for, because these lands are so far from the Church of Rome, and we, the Christians who now reside here and shall do so in the future, are so far from the proper remedies of our consciences and, as we are human, so subject to sin, it is essential that His Holiness should be generous with us and grant to these persons most extensive powers, to be handed down to persons actually in residence here whether it be given to the general of each order or to his provincials.
The Franciscans arrived in May of 1524, a symbolically powerful group of twelve known as the Twelve Apostles of Mexico, led by Fray Martín de Valencia. Franciscan Geronimo de Mendieta claimed that Cortés's most important deed was the way he met this first group of Franciscans. The conqueror himself was said to have met the friars as they approached the capital, kneeling at the feet of the friars who had walked from the coast. This story was used by Franciscans as a demonstration of Cortés's piety and humility was a powerful message to all, including the Indians, that Cortés's earthly power was subordinate to the spiritual power of the friars. However, one of the first twelve Franciscans, Fray Toribio de Benavente Motolinia does not mention it in his history. Cortés and the Franciscans had a particularly strong alliance in Mexico, with Franciscans seeing him as "the new Moses" for conquering Mexico and opening it to Christian evangelization. In Motolinia's 1555 response to Dominican Bartolomé de Las Casas, he praises Cortés. And as to those who murmur against the Marqués del Valle [Cortés], God rest him, and who try to blacken and obscure his deeds, I believe that before God their deeds are not as acceptable as those of the Marqués. Although as a human he was a sinner, he had faith and works of a good Christian, and a great desire to employ his life and property in widening and augmenting the fair of Jesus Christ, and dying for the conversion of these gentiles... Who has loved and defended the Indians of this new world like Cortés?... Through this captain, God opened the door for us to preach his holy gospel and it was he who caused the Indians to revere the holy sacraments and respect the ministers of the church.
In Fray Bernardino de Sahagún's 1585 revision of the conquest narrative first codified as Book XII of the Florentine Codex, there are laudatory references to Cortés that do not appear in the earlier text from the indigenous perspective. Whereas Book XII of the Florentine Codex concludes with an account of Spaniards' search for gold, in Sahagún's 1585 revised account, he ends with praise of Cortés for requesting the Franciscans be sent to Mexico to convert the Indians.
Expedition to Honduras and aftermath.
From 1524 to 1526, Cortés headed an expedition to Honduras where he defeated Cristóbal de Olid, who had claimed Honduras as his own under the influence of the Governor of Cuba Diego Velázquez. Fearing that Cuauhtémoc might head an insurrection in Mexico, he brought him with him to Honduras. In a controversial move, Cuauhtémoc was executed during the journey. Raging over Olid's treason, Cortés issued a decree to arrest Velázquez, whom he was sure was behind Olid's treason. This, however, only served to further estrange the Crown of Castile and the Council of Indies, both of which were already beginning to feel anxious about Cortés's rising power.
Cortés's fifth letter to King Charles attempts to justify his conduct, concludes with a bitter attack on "various and powerful rivals and enemies" who have "obscured the eyes of your Majesty." Charles, who was also Holy Roman Emperor, had little time for distant colonies (much of Charles's reign was taken up with wars with France, the German Protestants and the expanding Ottoman Empire), except insofar as they contributed to finance his wars. In 1521, year of the Conquest, Charles was attending to matters in his German domains and Bishop Adrian of Utrecht functioned as regent in Spain.
Velázquez and Fonseca persuaded the regent to appoint a commissioner with powers, (a "Juez de residencia", Luis Ponce de León), to investigate Cortés's conduct and even arrest him. Cortés was once quoted as saying that it was "more difficult to contend against (his) own countrymen than against the Aztecs." Governor Diego Velázquez continued to be a thorn in his side, teaming up with Bishop Juan Rodríguez de Fonseca, chief of the Spanish colonial department, to undermine him in the Council of the Indies.
A few days after Cortés's return from his expedition, Ponce de León suspended Cortés from his office of governor of New Spain. The Licentiate then fell ill and died shortly after his arrival, appointing Marcos de Aguilar as "alcalde mayor". The aged Aguilar also became sick and appointed Alonso de Estrada governor, who was confirmed in his functions by a royal decree in August 1527. Cortés, suspected of poisoning them, refrained from taking over the government.
Estrada sent Diego de Figueroa to the south. De Figueroa raided graveyards and extorted contributions, meeting his end when the ship carrying these treasures sank. Albornoz persuaded Alonso de Estrada to release Salazar and Chirinos. When Cortés complained angrily after one of his adherents' hands was cut off, Estrada ordered him exiled. Cortés sailed for Spain in 1528 to appeal to King Charles.
First return to Spain (1528) and Marquisate of the Valley of Oaxaca.
In 1528, Cortés returned to Spain to appeal to the justice of his master, Charles V. Juan Altamirano and Alonso Valiente stayed in Mexico and acted as Cortés' representatives during his absence. Cortés presented himself with great splendor before Charles V's court. By this time Charles had returned and Cortés forthrightly responded to his enemy's charges. Denying he had held back on gold due the crown, he showed that he had contributed more than the quinto (one-fifth) required. Indeed, he had spent lavishly to build the new capital of Mexico City on the ruins of the Aztec capital of Tenochtitlán, leveled during the siege that brought down the Aztec empire.
He was received by Charles with every distinction, and decorated with the order of Santiago. In return for his efforts in expanding the still young Spanish Empire, Cortés was rewarded in 1529 by being accorded the noble title of "don" but more importantly named the ""Marqués del Valle de Oaxaca"" Marquisate of the Valley of Oaxaca and married the Spanish noblewoman, Doña Juana Zúñiga, after the 1522 death of his much less distinguished first wife, Catalina Suárez. The noble title and senorial estate of the Marquesado was passed down to his descendants until 1811. The Oaxaca Valley was one of the wealthiest region of New Spain, and Cortés had 23,000 vassals in 23 named encomiendas in perpetuity.
Although confirmed in his land holdings and vassals, he was not reinstated as governor and was never again given any important office in the administration of New Spain. During his travel to Spain, his property was mismanaged by abusive colonial administrators. He sided with local natives in a lawsuit. The natives documented the abuses in the Huexotzinco Codex.
The entailed estate and title passed to his legitimate son Don Martín Cortés upon Cortés's death in 1547, who became the Second Marquis. Don Martín's association with the so-called Encomenderos' Conspiracy endangered the entailed holdings, but they were restored and remained the continuing reward for Hernán Cortés's family through the generations.
Return to Mexico.
Cortés returned to Mexico in 1530 with new titles and honors, but with diminished power. Although Cortés still retained military authority and permission to continue his conquests, viceroy Don Antonio de Mendoza was appointed in 1535 to administer New Spain's civil affairs. This division of power led to continual dissension, and caused the failure of several enterprises in which Cortés was engaged.
On returning to Mexico, Cortés found the country in a state of anarchy. There was a strong suspicion in court circles of an intended rebellion by Cortés, and a charge was brought against him that cast a fatal blight upon his character and plans. He was accused of murdering his first wife. The proceedings of the investigation were kept secret.
No report, either exonerating or condemning Cortés, was published. Had the Government declared him innocent, it would have greatly increased his popularity. Had it declared him a criminal, a crisis would have been precipitated by the accused and his party. Silence was the only safe policy, but that silence is suggestive that grave danger was feared from his influence.
After reasserting his position and reestablishing some sort of order, Cortés retired to his estates at Cuernavaca, about 30 miles (48 km) south of Mexico City. There he concentrated on the building of his palace and on Pacific exploration. Remaining in Mexico between 1530 and 1541, Cortés quarreled with Nuño Beltrán de Guzmán and disputed the right to explore the territory that is today California with Antonio de Mendoza, the first viceroy.
In 1536, Cortés explored the northwestern part of Mexico and discovered the Baja California Peninsula. Cortés also spent time exploring the Pacific coast of Mexico. The Gulf of California was originally named the "Sea of Cortes" by its discoverer Francisco de Ulloa in 1539. This was the last major expedition by Cortés.
Later life and death.
Second return to Spain.
After his exploration of Baja California, Cortés returned to Spain in 1541, hoping to confound his angry civilians, who had brought many lawsuits against him (for debts, abuse of power, etc.).
On his return he was utterly neglected, and could scarcely obtain an audience. On one occasion he forced his way through a crowd that surrounded the emperor's carriage, and mounted on the footstep. The emperor, astounded at such audacity, demanded of him who he was. "I am a man," replied Cortés proudly, "who has given you more provinces than your ancestors left you cities."
Expedition against Algiers.
The emperor finally permitted Cortés to join him and his fleet commanded by Andrea Doria at the great expedition against Algiers in the Barbary Coast in 1541, which was then part of the Ottoman Empire and was used as a base by Hayreddin Barbarossa, a famous Turkish corsair and Admiral-in-Chief of the Ottoman Fleet. During this unfortunate campaign, which was his last, Cortés was almost drowned in a storm that hit his fleet while he was pursuing Barbarossa.
Last years, death, and remains.
Having spent a great deal of his own money to finance expeditions, he was now heavily in debt. In February 1544 he made a claim on the royal treasury, but was given a royal runaround for the next three years. Disgusted, he decided to return to Mexico in 1547. When he reached Seville, he was stricken with dysentery. He died in Castilleja de la Cuesta, Seville province, on December 2, 1547, from a case of pleurisy at the age of 62.
Like Columbus, he died a wealthy but embittered man. He left his many mestizo and white children well cared for in his will, along with every one of their mothers. He requested in his will that his remains eventually be buried in Mexico. Before he died he had the Pope remove the "natural" status of three of his
children (legitimizing them in the eyes of the church), including Martin, the son he had with Doña Marina (also known as La Malinche), said to be his favourite.
After his death his body has been moved more than eight times for several reasons. On December 4, 1547 he was buried in the mausoleum of the Duke of Medina in the church of San Isidoro del Campo, Sevilla. Three years later (1550) due to the space being required by the duke, his body was moved to the altar of Santa Catarina in the same church. In his testament, Cortés asked for his body to be buried in the monastery he had ordered to be built in Coyoacan in México, ten years after his death, but the monastery was never built. So in 1566, his body was sent to New Spain and buried in the church of "San Francisco de Texcoco", where his mother and one of his sisters were buried.
In 1629, "Don Pedro Cortés fourth "Marquez del Valle", his last male descendant, died, so the viceroy decided to move the bones of Cortés along with those of his descendant to the Franciscan church in México. This was delayed for nine years, while his body stayed in the main room of the palace of the viceroy. Eventually it was moved to the Sagrario of Franciscan church, where it stayed for 87 years. In 1716, it was moved to another place in the same church. In 1794, his bones were moved to the "Hospital de Jesus" (founded by Cortés), where a statue by Tolsa and a mausoleum were made. There was a public ceremony and all the churches in the city rang their bells.
In 1823, after the independence of México, it seemed imminent that his body would be desecrated, so the mausoleum was removed, the statue and the coat of arms were sent to Palermo, Sicily, to be protected by the Duke of Terranova. The bones were hidden, and everyone thought that they had been sent out of México. In 1836, his bones were moved to another place in the same building.
It was not until November 24, 1946 that they were rediscovered, thanks to the discovery of a secret document by Lucas Alamán. His bones were put in charge of the Instituto Nacional de Antropología e Historia (INAH). The remains were authenticated by INAH. They were then restored to the same place, this time with a bronze inscription and his coat of arms. When the bones were first rediscovered, the supporters of the Hispanic tradition in Mexico were excited, but one supporter of an indigenist vision of Mexico "proposed that the remains be publicly burned in front of the statue of Cuauhtemoc, and the ashes flung into the air." Following the discovery and authentication of Cortés's remains, there was a discovery of what were described as the bones of Cuauhtémoc occurred, resulting in the so-called "battle of the bones" In 1981, when a copy of the bust by Tolsa was put in the church, there was a failed attempt to destroy his bones.
Disputed interpretation of his life.
There are relatively few sources to the early life of Cortés; his fame arose from his participation in the conquest of Mexico and it was only after this that people became interested in reading and writing about him.
Probably the best source is his letters to the king which he wrote during the campaign in Mexico, but they are written with the specific purpose of putting his efforts in a favourable light and so must be read critically. Another main source is the biography written by Cortés's private chaplain Lopez de Gómara, which was written in Spain several years after the conquest. Gómara never set foot in the Americas and knew only what Cortés had told him, and he had an affinity for knightly romantic stories which he incorporated richly in the biography. The third major source is written as a reaction to what its author calls "the lies of Gomara", the eyewitness account written by the Conquistador Bernal Díaz del Castillo does not paint Cortés as a romantic hero but rather tries to emphasize that Cortés's men should also be remembered as important participants in the undertakings in Mexico.
In the years following the conquest more critical accounts of the Spanish arrival in Mexico were written. The Dominican friar Bartolomé de Las Casas wrote his "A Short Account of the Destruction of the Indies" which raises strong accusations of brutality and heinous violence towards the Indians; accusations against both the conquistadors in general and Cortés in particular. The accounts of the conquest given in the Florentine Codex by the Franciscan Bernardino de Sahagún and his native informants are also less than flattering towards Cortés. The scarcity of these sources has led to a sharp division in the description of Cortés's personality and a tendency to describe him as either a vicious and ruthless person or a noble and honorable cavalier.
Representations in México.
In México there are few representations of Cortés. However, many landmarks still bear his name, from the castle in the city of Cuernavaca to some street names throughout the republic.
The only authentic monuments are in Mexico City at the pass between the volcanoes Iztaccíhuatl and Popocatépetl where Cortés took his soldiers on their march to Mexico City. It is known as the Paso de Cortés.
The muralist Diego Rivera painted several representation of him but the most famous, depicts him as a powerful and ominous figure along with Malinche in a mural in the National Palace in Mexico City.
In 1981, President Lopez Portillo tried to bring Cortés to public recognition. First, he made public a copy of the bust of Cortés made by Manuel Tolsá in the Hospital de Jesús Nazareno with an official ceremony, but soon a nationalist group tried to destroy it, so it had to be taken out of the public. Today the copy of bust is in the "Hospital de Jesús Nazareno" while the original is in Naples, Italy, in the Villa Pignatelli.
Later, another monument, known as "Monumento al Mestizaje" by Julián Martínez y M. Maldonado (1982) was commissioned by Mexican president José López Portillo to be put in the "Zócalo" (Main square) of Coyoacan, near the place of his country house, but it had to be removed to a little known park, the Jardín Xicoténcatl, Barrio de San Diego Churubusco, to quell protests. The statue depicts Cortés, Malinche and their son Martín.
There is another statue by Sebastián Aparicio, in Cuernavaca, was in a hotel "El casino de la selva". Cortés is barely recognizable, so it sparked little interest. The hotel was closed to make a commercial center, and the statue was put out of public display by Costco the builder of the commercial center.
Writings: the "Cartas de Relación".
Cortés' personal account of the conquest of Mexico is narrated in his five letters addressed to Charles V. These five letters, the "cartas de relación", are Cortés' only surviving writings. See "Letters and Dispatches of Cortés", translated by George Folsom (New York, 1843); Prescott's "Conquest of Mexico" (Boston, 1843); and Sir Arthur Helps's "Life of Hernando Cortes" (London, 1871).
As one specialist describes them:
His first letter is lost, and the one from the municipality of Veracruz has to take its place. It was published for the first time in volume IV of "Documentos para la Historia de España", and subsequently reprinted. The first "carta de relación" is available online at the University of Wisconsin.
The "Segunda Carta de Relacion", bearing the date of October 30, 1520, appeared in print at Seville in 1522. The third letter, dated May 15, 1522, appeared at Seville in 1523. The fourth, October 20, 1524, was printed at Toledo in 1525. The fifth, on the Honduras expedition, is contained in volume IV of the "Documentos para la Historia de España". The important letter mentioned in the text has been published under the heading of "Carta inédita de Cortés" by Ycazbalceta. A great number of minor documents, either by Cortés or others, for or against him, are dispersed through the voluminous collection above cited and through the "Colección de Documentos de Indias", as well as in the "Documentos para la Historia de México" of Ycazbalceta. There are a number of reprints and translations of Cortés's writings into various languages.
Children.
Natural children of Don Hernán Cortés
He married twice: firstly in Cuba to Catalina Suárez Marcaida, who died at Coyoacán in 1522 without issue, and secondly in 1529 to "doña" Juana Ramírez de Arellano de Zúñiga, daughter of "don" Carlos Ramírez de Arellano, 2nd Count of Aguilar and wife the Countess "doña" Juana de Zúñiga, and had:
Ancestors.
Ancestors of Hernán Cortés de Monroy y Pizarro, 1st Marquess of the Valley of Oaxaca
See also.
General:

</doc>
<doc id="14015" url="https://en.wikipedia.org/wiki?curid=14015" title="Herstory">
Herstory

Herstory is history written from a feminist perspective, emphasizing the role of women, or told from a woman's point of view. It is a neologism coined as a pun with the word "history", as part of a feminist critique of conventional historiography, which in their opinion is traditionally written as "his story", i.e., from the masculine point of view. (The word "history"—from the Ancient Greek ἱστορία, or historia, meaning "knowledge obtained by inquiry"—is etymologically unrelated to the possessive pronoun "his".)
The herstory movement has spawned women-centered presses, such as Virago Press in 1973, which publishes fiction and non-fiction by noted women authors like Janet Frame and Sarah Dunant.
Usage.
Robin Morgan, in a book of her selected writings states that the debut of the word "herstory" was in the byline of her article "Goodbye to All That", in early 1970, in the first issue of the "underground" New Left newspaper "Rat" after it was overtaken by women to clean it of sexism. She writes that she identified herself as a member of W.I.T.C.H., decoding the acronym as ""Women Inspired to Commit Herstory".
In 1976, Casey Miller and Kate Swift wrote in "Words & Women,"
During the 1970s and 1980s, second-wave feminists saw the study of history as a male-dominated intellectual enterprise and presented "herstory" as a means of compensation. The term, intended to be both serious and comic, became a rallying cry used on T-shirts and buttons as well as in academia.
Criticism.
Christina Hoff Sommers has been a vocal critic of the concept of herstory, and presented her argument against the movement in her 1994 book, "Who Stole Feminism?". Sommers defined herstory as an attempt to infuse education with ideology, at the expense of knowledge. The "gender feminists", as she termed them, were the band of feminists responsible for the movement, which she felt amounted to negationism. She regarded most attempts to make historical studies more female-inclusive as being artificial in nature, and an impediment to progress.
Professor and author Devoney Looser has criticized the concept of herstory for overlooking the contributions that some women made as historians before the twentieth century.
The Global Language Monitor, a nonprofit group that analyzes and tracks trends in language, named "herstory" the third most "politically incorrect" word of 2006—rivaled only by ""macaca"" and ""Global Warming Denier"."
Books.
Recent books published on the topic include:

</doc>
<doc id="14017" url="https://en.wikipedia.org/wiki?curid=14017" title="House of Cards (UK TV series)">
House of Cards (UK TV series)

House of Cards is a 1990 British political thriller television drama serial in four episodes, set after the end of Margaret Thatcher's tenure as Prime Minister of the United Kingdom. It was televised by the BBC from 18 November to 9 December 1990, to critical and popular acclaim.
Andrew Davies adapted the story from a novel written by Michael Dobbs, a former Chief of Staff at Conservative Party headquarters. Neville Teller also dramatised Dobbs's novel for BBC World Service in 1996, and it had two television sequels ("To Play the King" and "The Final Cut"). The opening and closing theme music for those TV series is entitled "Francis Urquhart's March."
"House of Cards" was ranked 84th in the British Film Institute list of the 100 Greatest British Television Programmes in 2000. In 2013, the serial and the Dobbs novel were the basis for a US adaptation set in Washington, D.C., commissioned and released by Netflix.
Overview.
The antihero of "House of Cards" is Francis Urquhart, a fictional Chief Whip of the Conservative Party, played by Ian Richardson. The plot follows his amoral and manipulative scheme to become leader of the governing party and, thus, Prime Minister of the United Kingdom.
Michael Dobbs did not envisage writing the second and third books, as Urquhart dies at the end of the first novel. The screenplay of the BBC's dramatisation of "House of Cards" differs from the book, and hence allows future series. Dobbs wrote two following books, "To Play the King" and "The Final Cut", which were televised in 1993 and 1995, respectively.
"House of Cards" was said to draw from Shakespeare's plays "Macbeth" and "Richard III", both of which feature main characters who are corrupted by power and ambition. Richardson has a Shakespearean background and said he based his characterization of Urquhart on Shakespeare's portrayal of Richard III.
Urquhart frequently talks through the camera to the audience, breaking the fourth wall using the aside.
Plot.
After Margaret Thatcher's resignation, the ruling Conservative Party is about to elect a new leader. Francis Urquhart (Ian Richardson), an MP and the Government Chief Whip in the House of Commons, introduces viewers to the contestants, from which Henry "Hal" Collingridge (David Lyon) emerges victorious. Urquhart is secretly contemptuous of the well-meaning but weak Collingridge, but expects a promotion to a senior position in the Cabinet. After the general election, which the party wins by a reduced majority, Urquhart submits his suggestions for a cabinet reshuffle that includes his desired promotion. However, Collingridge – citing Harold Macmillan's political demise after the 1962 Night of the Long Knives – effects no changes at all. Urquhart resolves to oust Collingridge, with encouragement from his wife, Elizabeth (Diane Fletcher).
At the same time, with Elizabeth's blessing, Urquhart begins an affair with Mattie Storin (Susannah Harker), a junior political reporter at a Conservative-leaning tabloid newspaper called "The Chronicle". The affair allows Urquhart to manipulate Mattie and indirectly skew her coverage of the Conservative leadership contest in his favour. Mattie has an apparent Electra complex; she finds appeal in Urquhart's much older age and later refers to him as "Daddy." Another unwitting pawn is Roger O'Neill (Miles Anderson), the party's cocaine-addicted public relations consultant.
Urquhart blackmails O'Neill into leaking information on budget cuts that humiliates Collingridge during the Prime Minister's Questions. Later, he blames Party chairman Lord "Teddy" Billsborough (Nicholas Selby) for leaking an internal poll showing a drop in Tory numbers, leading Collingridge to sack him. As Collingridge's image suffers, Urquhart encourages ultraconservative Foreign Secretary Patrick Woolton (Malcolm Tierney) and "Chronicle" owner Benjamin Landless to support his removal. Urquhart also poses as Collingridge's alcoholic brother, Charles, to trade shares in a chemical company about to benefit from advance information confidential to the government. Consequently, Collingridge becomes falsely accused of insider trading and is forced to resign.
In the ensuing leadership race, Urquhart initially feigns unwillingness to stand before announcing his candidacy. With the help of his underling, Tim Stamper (Colin Jeavons), Urquhart goes about making sure his competitors drop out of the race: Health Secretary Peter MacKenzie (Christopher Owen) accidentally runs his car over a disabled protester at a demonstration staged by Urquhart and is forced by the public outcry to withdraw, while Education Secretary Harold Earle (Kenneth Gilbert) is blackmailed into withdrawing when Urquhart anonymously sends pictures of him in the company of a rent boy whom Earle had paid for sex.
The first ballot leaves Urquhart to face Woolton and Michael Samuels, the moderate Environment Secretary supported by Billsborough. Urquhart eliminates Woolton by a prolonged scheme: at the party conference, he pressures O'Neill into persuading his personal assistant and lover, Penny Guy (Alphonsia Emmanuel), to have a one-night stand with Woolton in his suite, which Urquhart records via a bugged ministerial red box. When the tape is sent to Woolton, he is led to assume that Samuels is behind the scheme and backs Urquhart in the contest. Urquhart also receives support from Collingridge, who is unaware of Urquhart's role in his own downfall. Samuels is forced out of the running when the tabloids reveal that he backed leftist causes as a student at University of Cambridge.
Stumbling across contradictions in the allegations against Collingridge and his brother, Mattie begins to dig deeper. On Urquhart's orders, O'Neill arranges for her car and flat to be vandalised in a show of intimidation. However, O'Neill becomes increasingly uneasy with what he is being asked to do, and his cocaine addiction adds to his instability. Urquhart mixes O'Neill's cocaine with rat poison, causing him to kill himself when taking the cocaine in a motorway lavatory. Though initially blind to the truth of matters thanks to her relations with Urquhart, Mattie eventually deduces that Urquhart is responsible for O'Neill's death and is behind the unfortunate downfalls of Collingridge and all of Urquhart's rivals.
Mattie looks for Urquhart at the point when it seems his victory is certain. She eventually finds him on the roof garden of the Houses of Parliament, where she confronts him. He admits to O'Neill's murder and everything else he has done. He then asks whether he can trust Mattie, and, though she answers in the affirmative, he does not believe her and throws her off the roof onto a van parked below. An unseen person picks up Mattie's tape recorder, which she had been using to secretly record her conversations with Urquhart. The series ends with Urquhart defeating Samuels in the second leadership ballot and being driven to Buckingham Palace to be invited to form a government by Elizabeth II.
Deviations from the novel in the series.
In the first novel, but not in the television series:
When the series was reissued in 2013, to coincide with the release of the US version of "House of Cards", Dobbs rewrote portions of the novel to bring the series in line with the television mini-series and restore continuity among the three novels. In the 2013 version:
Reception.
The first installment of the TV series coincidentally aired two days before the Conservative Party leadership election. Author Dobbs said that John Major's leadership headquarters "came to a halt" to view the show. During a time of "disillusionment with politics", the series "caught the nation's mood".
Ian Richardson won a Best Actor BAFTA in 1991 for his role as Urquhart, and Andrew Davies won an Emmy for outstanding writing in a miniseries.
The series ranked 84th in the British Film Institute list of the 100 Greatest British Television Programmes.
Adaptation.
The Urquhart trilogy has been adapted in the United States as "House of Cards". The show stars Kevin Spacey as Francis "Frank" Underwood, the Majority Whip of the Democratic Party, who schemes and murders his way to becoming President of the United States. It is produced by David Fincher and Spacey's Trigger Street Productions, with the initial episodes directed by Fincher.
The series, produced and financed by independent studio Media Rights Capital, is one of Netflix's first forays into original programming. Season one was made available online on 1 February 2013. The series is filmed in Baltimore, Maryland. The first season was critically acclaimed and earned four Golden Globe Nominations, including Best Drama, actor, actress and supporting actor, with Robin Wright winning best actress. It also earned nine Primetime Emmy Award nominations, winning three, and was the first show to earn nominations that was broadcast solely via an internet streaming service.
In popular culture.
The drama introduced and popularised the phrase: "You might very well think that; I couldn't possibly comment." It was a non-confirmation confirmative statement, used by Urquhart whenever he could not be seen to agree with a leading statement, with the emphasis on either the "I" or the "possibly", depending on the situation. The phrase was even used in the House of Commons following the series.
A variation on the phrase was written into the TV adaptation of Terry Pratchett's "Hogfather" for Death, as an in-joke on the fact that he was voiced by Richardson.
A further variation was used by Nicola Murray, a fictional government minister, in the third series finale of "The Thick of It".
In the U.S. adaptation, the phrase is used by Frank Underwood in the first episode during his initial meeting with Zoe Barnes.

</doc>
<doc id="14018" url="https://en.wikipedia.org/wiki?curid=14018" title="Helen Gandy">
Helen Gandy

Helen W. Gandy (April 8, 1897 – July 7, 1988) was an American civil servant. She was the secretary to Federal Bureau of Investigation director J. Edgar Hoover for 54 years. Hoover called her "indispensable" and she exercised great behind-the-scenes influence on Hoover and the workings of the Bureau. Following Hoover's death in 1972, she spent weeks destroying his "Personal File," thought to be where the most incriminating material he used to manipulate and control the most powerful figures in Washington was kept.
Early life.
Gandy was born in Rockville, New Jersey, one of three children (two daughters and a son) born to Franklin Dallas and Annie (née Williams) Gandy. She grew up in New Jersey in Fairton or the Port Norris section of Commercial Township (sources differ) and graduated from Bridgeton High School in Bridgeton New Jersey. In 1918, aged 21, she moved to Washington, D.C., where she later took classes at Strayer Business College and George Washington University Law School.
Career.
Gandy briefly worked in a department store in Washington before she found a job as a file clerk at the Justice Department in 1918. Within weeks, she went to work as a typist for Hoover, effective March 25, 1918, having told Hoover in her interview she had "no immediate plans to marry." She, like Hoover, would never marry, both being completely devoted to the Bureau.
When Hoover went to the Bureau of Investigation (as it was then known) as its assistant director on August 22, 1921, he specifically requested Gandy return from vacation to help him in the new post. Hoover became director of the Bureau in 1924 and Gandy continued in his service. She was promoted to "office assistant" on August 23, 1937, and "executive assistant" on October 1, 1939. Though she would receive promotions in her civil service grade subsequently, she would retain her title as executive assistant to her retirement on May 2, 1972, the day Hoover died. Hoover said of her "if there is anyone in this Bureau whose services are indispensable I consider Miss Gandy to be that person." Despite this, Curt Gentry wrote:
Theirs was a rigidly formal relationship. He'd always called her 'Miss Gandy' (when angry, barking it out as one word). In all those fifty-four years he had never once called her by her first name.
Hoover biographers Theoharis and Cox would say "her stern face recalled Cerberus at the gate," a view echoed by Anthony Summers in his life of Hoover, who also pictured Gandy as Hoover's first line of defense against the outside world. When Attorney General Robert F. Kennedy, Hoover's superior, had a direct telephone line installed between their offices, Hoover refused to answer the phone. "Put that damn thing on Miss Gandy's desk where it belongs," Hoover would declare.
Gentry would describe her influence:
Her genteel manner and pleasant voice contrasted sharply with this domineering presence. Yet behind the politeness was a resolute firmness not unlike his, and no small amount of influence. Many a career in the Bureau had been quietly manipulated by her. Even those who disliked him, praised her, most often commenting on her remarkable ability to get along with all kinds of people. That she had held her position for fifty-four years was the best evidence of this, for it was a Bureau tradition that the closer you were to him, the more demanding he was.
William C. Sullivan, an agent with the Bureau for three decades, reported in his memoir when he worked in the public relations section answering mail from the public, he gave a correspondent the wrong measurements for Hoover's personal popover recipe, relying on memory rather than the files. Gandy, ever protective of her boss, caught the error and brought it to Hoover's attention. The director then placed an official letter of reprimand in Sullivan's file for the lapse. Mark Felt, deputy associate director of the Bureau, wrote in his memoir that Gandy "was bright and alert and quick-tempered—and completely dedicated to her boss."
The Files.
J. Edgar Hoover died during the night of May 1–2, 1972. According to Curt Gentry, who wrote the book "J Edgar Hoover: The Man and the Secrets", Hoover's body was not discovered by his live-in cook and general housekeeper, Annie Fields. Rather, it was discovered by James Crawford, who had been Hoover's chauffeur for 37 years. Crawford then yelled out to Fields and Tom Moton (Hoover's new chauffeur after Crawford had retired in January, 1972). Ms. Fields first called Hoover's personal physician, Dr. Robert Choisser, then used another phone to call Clyde Tolson's private number. Tolson then called Helen Gandy's private number with the news of Hoover's death along with orders to begin destroying the files. Within an hour, the "D List" ("d" standing for destruction) was being distributed and the destruction of files began. However, "The New York Times" quoted an anonymous F.B.I. source in spring 1975 that "Gandy had begun almost a year before Mr. Hoover's death and was instructed to purge the files that were then in his office."
Anthony Summers reported that G. Gordon Liddy stated his sources in the F.B.I. said "by the time Gray went in to get the files, Miss Gandy had already got rid of them." The day after Hoover died, Gray, who had been named acting director by President Richard Nixon upon Tolson's resignation from that position, went to Hoover's office. Gandy paused from her work to give Gray a tour. He found file cabinets open and packing boxes being filled with papers. She informed him the boxes contained personal papers of Hoover's. Gandy stated Gray flipped through a few files and approved her work, but Gray was to deny he looked at any papers. Gandy also told Gray it would be a week before she could clear Hoover's effects out so he could move into the suite.
Gray reported to Nixon that he had secured Hoover's office and its contents. However, he had sealed only Hoover's personal inner office, where no files were stored, not the entire suite of offices. Since 1957, Hoover's "Official/Confidential" files, containing material too sensitive to include in the Bureau's central files, had been kept in the outer office, where Gandy sat. Gentry reported that Gray would not have known where to look in Gandy's office for the files, as her office was lined floor to ceiling with filing cabinets; moreover, without her index to the files, he would not have been able to locate incriminating material, for files were deliberately mislabeled, e.g. President Nixon's file was labeled "Obscene Matters".
On May 4, she turned over twelve boxes of the "Official/Confidential", containing 167 files and 17,750 pages to Mark Felt. Many of them contained derogatory information. Gray told the press that afternoon that "there are no dossiers or secret files. There are just general files and I took steps to preserve their integrity." Gandy retained the "Personal File".
Gandy worked on going through Hoover's "Personal File" in the office until May 12. She then transferred at least thirty-two file drawers of material to the basement rec room of Hoover's Washington home at 4936 Thirtieth Place, Northwest, where she would continue her work from May 13 to July 17. Gandy later testified nothing official had been removed from the Bureau's offices, "not even his badge." There the destruction was overseen by John P. Mohr, the number three man in the Bureau after Hoover and Tolson. They were aided by James Jesus Angleton, the Central Intelligence Agency's counterintelligence chief, whom Hoover's neighbors saw removing boxes from Hoover's home. Mohr would claim the boxes Angleton removed were cases of spoiled wine.
When the House Committee on Government Oversight investigated the F.B.I.'s spying on and harassment of Martin Luther King, Jr. and others in 1975, Gandy was called to testify. "I tore them up, put them in boxes, and they were taken away to be shredded," she told the congressmen about the papers. The Bureau's Washington field office had F.B.I. drivers transport the material to Hoover's home, then once Gandy had gone through the material, the drivers transported it back to the field office in the Old Post Office Building on Pennsylvania Avenue where it was shredded and burned.
Gandy stated that Hoover had left standing instructions to destroy his personal papers upon his death and that this instruction was confirmed by Tolson and Gray. Gandy stated that she destroyed no official papers, that everything was personal papers of Hoover. The staff of the subcommittee did not believe her, but she told the committee "I have no reason to lie." Representative Andrew Maguire (D-New Jersey), a freshman member of the 94th Congress, said "I find your testimony very difficult to believe." Gandy held her ground: "That is your privilege."
"I can give you my word. I know what there was—letters to and from friends, personal friends, a lot of letters," she testified. Gandy also said the files she took to his home also included his financial papers, such as tax returns and investment statements, the deed to his home, and papers relating to his dogs' pedigrees.
Curt Gentry wrote
In "J. Edgar Hoover: The Man and His Secrets", Gentry describes the nature of the files: "... their contents included blackmail material on the patriarch of an American political dynasty, his sons, their wives, and other women; allegations of two homosexual arrests which Hoover leaked to help defeat a witty, urbane Democratic presidential candidate; the surveillance reports on one of America's best-known first ladies and her alleged lovers, both male and female, white and black; the child molestation documentation the director used to control and manipulate one of the Red-baiting proteges; a list of the Bureau's spies in the White House during the eight administrations when Hoover was FBI director; the forbidden fruit of hundreds of illegal wiretaps and bugs, containing, for example, evidence that an attorney general, Tom C. Clark, who later became Supreme Court justice, had received payoffs from the Chicago syndicate; as well as celebrity files, with all the unsavory gossip Hoover could amass on some of the biggest names in show business."
Later years and death.
While she officially retired the day Hoover died, she spent the next few weeks destroying his papers and Hoover left her $5,000 in his will. In 1961, she and her sister, Lucy G. Rodman, donated a portrait of their mother by Thomas Eakins to the Smithsonian American Art Museum. Gandy lived in Washington, D.C., until 1986, when she moved to DeLand, Florida, in Volusia County where a niece lived.
An avid trout fisherman, she died of a heart attack on July 7, 1988, either in DeLand (says her "New York Times" obituary) or in nearby Orange City, Florida (says her "Post" obituary).
In popular culture.
Gandy was portrayed by actresses Lee Kessler in the 1987 television film "J. Edgar Hoover", and Naomi Watts in the 2011 cinematic release "J. Edgar".

</doc>
<doc id="14019" url="https://en.wikipedia.org/wiki?curid=14019" title="Horsepower">
Horsepower

Horsepower (hp) is a unit of measurement of power (the rate at which work is done). There are many different standards and types of horsepower. The term was adopted in the late 18th century by Scottish engineer James Watt to compare the output of steam engines with the power of draft horses. It was later expanded to include the output power of other types of piston engines, as well as turbines, electric motors and other machinery. The definition of the unit varied between geographical regions. Most countries now use the SI unit "watt" for measurement of power. With the implementation of the EU Directive 80/181/EEC on January 1, 2010, the use of horsepower in the EU is permitted only as a supplementary unit.
Definitions of term.
Units called "horsepower" have differing definitions:
History of the unit.
The development of the steam engine provided a reason to compare the output of horses with that of the engines that could replace them. In 1702, Thomas Savery wrote in "The Miner's Friend":
So that an engine which will raise as much water as two horses, working together at one time in such a work, can do, and for which there must be constantly kept ten or twelve horses for doing the same. Then I say, such an engine may be made large enough to do the work required in employing eight, ten, fifteen, or twenty horses to be constantly maintained and kept for doing such a work…
The idea was later used by James Watt to help market his improved steam engine. He had previously agreed to take royalties of one third of the savings in coal from the older Newcomen steam engines. This royalty scheme did not work with customers who did not have existing steam engines but used horses instead.
Watt determined that a horse could turn a mill wheel 144 times in an hour (or 2.4 times a minute). The wheel was 12 feet (3.6576 meters) in radius; therefore, the horse travelled 2.4·2π·12 feet in one minute. Watt judged that the horse could pull with a force of 180 pounds. So:
Watt defined and calculated the horsepower as 32,572 ft·lbf/min, which was rounded to an even 33,000 ft·lbf/min.
Watt determined that a pony could lift an average per minute over a four-hour working shift. Watt then judged a horse was 50% more powerful than a pony and thus arrived at the 33,000 ft·lbf/min figure. "Engineering in History" recounts that John Smeaton initially estimated that a horse could produce 22,916 foot-pounds per minute. John Desaguliers had previously suggested 44,000 foot-pounds per minute and Tredgold 27,500 foot-pounds per minute. "Watt found by experiment in 1782 that a 'brewery horse' could produce 32,400 foot-pounds per minute." James Watt and Matthew Boulton standardized that figure at 33,000 the next year.
Most observers familiar with horses and their capabilities estimate that Watt was either a bit optimistic or intended to underpromise and overdeliver; few horses can maintain that effort for long. Regardless, comparison with a horse proved to be an enduring marketing tool.
A common legend states that the unit was created when one of Watt's first customers, a brewer, specifically demanded an engine that would match a horse, but tried to cheat by taking the strongest horse he had and driving it to the limit. Watt, while aware of the trick, accepted the challenge and built a machine which was actually even stronger than the figure achieved by the brewer, and it was the output of that machine which became the horsepower.
In 1993, R. D. Stevenson and R. J. Wassersug published an article calculating the upper limit to an animal's power output. The peak power over a few seconds has been measured to be as high as 14.9 hp. However, Stevenson and Wassersug observe that for sustained activity, a work rate of about 1 hp per horse is consistent with agricultural advice from both 19th and 20th century sources.
When considering human-powered equipment, a healthy human can produce about 1.2 hp briefly (see orders of magnitude) and sustain about 0.1 hp indefinitely; trained athletes can manage up to about 2.5 hp briefly
and 0.3 hp for a period of several hours. The Jamaican sprinter Usain Bolt produced 11.4 hp on average for the duration of 9.58 seconds when he set the 100m dash world record in 2009.
Calculating power.
When torque formula_2 is in pound-foot units, rotational speed formula_3 is in rpm and power is required in horsepower:
The constant 5252 is the rounded value of (33,000 ft·lbf/min)/(2π rad/rev).
When torque formula_2 is in inch pounds:
The constant 63,025 is the approximation of
If torque and rotational speed are expressed in coherent SI units, the power is calculated by ;
where formula_9 is power in watts when formula_10 is torque in newton-metres, and formula_11 is angular speed in radians per second. When using other units or if the speed is in revolutions per unit time rather than radians, a conversion factor has to be included.
Current definitions.
The following definitions have been widely used:
In certain situations it is necessary to distinguish between the various definitions of horsepower and thus a suffix is added: hp(I) for mechanical (or imperial) horsepower, hp(M) for metric horsepower, hp(S) for boiler (or steam) horsepower and hp(E) for electrical horsepower.
Hydraulic horsepower is equivalent to mechanical horsepower. The formula given above is for conversion to mechanical horsepower from the factors acting on a hydraulic system.
Mechanical horsepower.
Assuming the third CGPM (1901, CR 70) definition of standard gravity, "g"n=9.80665 m/s2, is used to define the pound-force as well as the kilogram force, and the international avoirdupois pound (1959), one mechanical horsepower is:
Or given that 1 hp = 550 ft·lbf/s, 1 ft = 0.3048 m, 1 lbf ≈ 4.448 N, 1 J = 1 N·m, 1 W = 1 J/s: 1 hp ≈ 746 W
Metric horsepower (PS, cv, hk, pk, ks, ch).
The various units used to indicate this definition ("PS", "cv", "hk", "pk", "ks" and "ch") all translate to "horse power" in English, so it is common to see these values referred to as "horsepower" or "hp" in the press releases or media coverage of the German, French, Italian, and Japanese automobile companies. British manufacturers often intermix metric horsepower and mechanical horsepower depending on the origin of the engine in question. Sometimes the metric horsepower rating of an engine is conservative enough so that the same figure can be used for both 80/1269/EEC with metric hp and SAE J1349 with imperial hp.
DIN 66036 defines one metric horsepower as the power to raise a mass of 75 kilograms against the earth's gravitational force over a distance of one metre in one second; this is equivalent to 735.49875 W or 98.6% of an imperial mechanical horsepower.
In 1972, the PS was rendered obsolete by EEC directives, when it was replaced by the kilowatt as the official power measuring unit. It is still in use for commercial and advertising purposes, in addition to the kW rating, as many customers are still not familiar with the use of kilowatts for engines.
Other names for the metric horsepower are the Dutch (pk), the French (ch), the Portuguese (cv), the Russian (лс), the Swedish (hk), the Finnish (hv), the Norwegian and Danish (hk), the Hungarian (LE), the Czech and Slovak (k or ks), the Bosnian/Croatian/Serbian (KS), the Bulgarian , the Macedonian (KC), the Polish (KM), Slovenian (KM) and the Romanian (CP), which all equal the German (PS).
In the 19th century, the French had their own unit, which they used instead of the CV or horsepower. It was called the poncelet and was abbreviated "p".
French and Italian tax horsepower (CV).
In addition, the capital form "CV" is used in Italy and France as a unit for tax horsepower, short for, respectively, and ("steam horses"). CV is a non-linear rating of a motor vehicle for tax purposes. The CV rating, or fiscal power, is formula_12, where "P" is the maximum power in kilowatts and "U" is the amount of carbon dioxide (CO2) emitted in grams per kilometre. The term for CO2 measurements has been included in the definition only since 1998, so older ratings in CV are not directly comparable. The fiscal power has found its way into naming of automobile models, such as the popular Citroën deux-chevaux. The (ch) unit should not be confused with the French (CV).
Electrical horsepower.
The horsepower used for electrical machines is defined as exactly 746 W. In the US, nameplates on electrical motors show their power output in hp, not their power input. Outside the United States watts or kilowatts are generally used for electric motor ratings and in such usage it is the output power that is stated.
Boiler horsepower.
Boiler horsepower is a boiler's capacity to deliver steam to a steam engine and is not the same unit of power as the 550 ft-lb/s definition. One boiler horsepower is equal to the thermal energy rate required to evaporate 34.5 lb of fresh water at 212 °F in one hour. In the early days of steam use, the boiler horsepower was roughly comparable to the horsepower of engines fed by the boiler.
The term "Boiler Horsepower" was originally developed at the Philadelphia Centennial Exhibition in 1876, where the best steam engines of that period were tested. The average steam consumption of those engines (per output horsepower) was determined to be the evaporation of 30 pounds of water per hour, based on feed water at 100 °F, and saturated steam generated at 70 PSIG. This original definition is equivalent to a boiler heat output of 33,485 Btu/hr. Years later in 1884, the ASME re-defined the boiler horsepower as the thermal output equal to the evaporation of 34.5 pounds per hour of water "from and at" 212 °F. This considerably simplified boiler testing, and provided more accurate comparisons of the boilers at that time. This revised definition is equivalent to a boiler heat output of 33,469 Btu/hr. Present industrial practice is to define "Boiler Horsepower" as a boiler thermal output equal to 33,475 Btu/hr, which is very close to the original and revised definitions.
Boiler horsepower is still used to measure boiler output in industrial boiler engineering in Australia, the US, and New Zealand. Boiler horsepower is abbreviated BHP, not to be confused with brake horsepower, below, which is also called BHP.
Drawbar horsepower.
Drawbar horsepower (dbhp) is the power a railway locomotive has available to haul a train or an agricultural tractor to pull an implement. This is a measured figure rather than a calculated one. A special railway car called a dynamometer car coupled behind the locomotive keeps a continuous record of the drawbar pull exerted, and the speed. From these, the power generated can be calculated. To determine the maximum power available, a controllable load is required; it is normally a second locomotive with its brakes applied, in addition to a static load.
If the drawbar force (formula_13) is measured in pounds-force (lbf) and speed (formula_14) is measured in miles per hour (mph), then the drawbar power (formula_9) in horsepower (hp) is:
Example: How much power is needed to pull a drawbar load of 2,025 pounds-force at 5 miles per hour?

</doc>
<doc id="14020" url="https://en.wikipedia.org/wiki?curid=14020" title="History of London">
History of London

London (the capital city of England and the United Kingdom) has a history dating back over 2,000 years. During this time, it has grown to become one of the most significant financial and cultural capitals of planet Earth. It has experienced plague, devastating fire, civil war, aerial bombardment, terrorist attacks, and widespread rioting. The City of London is its historic core and today is its primary financial district, though it now represents a tiny part of the wider metropolis of Greater London.
Legendary foundations and prehistoric London.
According to the legendary "Historia Regum Britanniae", of Geoffrey of Monmouth, London was founded by Brutus of Troy about 1000–1100 B.C. after he defeated the native giant Gogmagog; the settlement was known as ', ' (Latin for New Troy), which, according to a pseudo-etymology, was corrupted to "Trinovantum". Trinovantes were the Iron Age tribe who inhabited the area prior to the Romans. Geoffrey provides prehistoric London with a rich array of legendary kings, such as King Lud (see also Lludd, from Welsh Mythology) who, he claims, renamed the town "Caer Ludein", from which London was derived, and was buried at Ludgate.
However, despite intensive excavations, archaeologists have found no evidence of a prehistoric major settlement in the area. There have been scattered prehistoric finds, evidence of farming, burial and traces of habitation, but nothing more substantial. It is now considered unlikely that a pre-Roman city existed, but as some of the Roman city remains unexcavated, it is still just possible that some major settlement may yet be discovered. London was most likely a rural area with scattered settlement. Rich finds such as the Battersea Shield, found in the Thames near Chelsea, suggest the area was important; there may have been important settlements at Egham and Brentford, and there was a hillfort at Uphall Camp, Ilford, but no city in the area of the Roman London, the present day City of London.
Some discoveries indicate probable very early settlements near the Thames in the London area. In 2010 the foundations of a large timber structure, dated to 4000BC, were found on the Thames foreshore, south of Vauxhall Bridge. The function of the mesolithic structure is not known. In 1999, the remains of a Bronze Age bridge were found, again on the foreshore south of Vauxhall Bridge. This bridge either crossed the Thames, or went to a now lost island in the river. Dendrology dated the timbers to 1500BC. In 2001 a further dig found that the timbers were driven vertically into the ground on the south bank of the Thames west of Vauxhall Bridge. All these structures are on the south bank at a natural crossing point where the River Effra flows into the River Thames.
Numerous finds have been made of spear heads and weaponry from the Bronze and Iron ages near the banks of the Thames in the London area, many of which had clearly been used in battle. This suggests that the Thames was an important tribal boundary.
Early history.
Roman London (43-410 AD).
"Londinium" was established as a civilian town by the Romans about seven years after the invasion of AD 43. London, like Rome, was founded on the point of the river where it was narrow enough to bridge and the strategic location of the city provided easy access to much of Europe. Early Roman London occupied a relatively small area, roughly equivalent to the size of Hyde Park. In around AD 60, it was destroyed by the Iceni led by their queen Boudica. The city was quickly rebuilt as a planned Roman town and recovered after perhaps 10 years, the city growing rapidly over the following decades.
During the 2nd century "Londinium" was at its height and replaced Colchester as the capital of Roman Britain (Britannia). Its population was around 60,000 inhabitants. It boasted major public buildings, including the largest basilica north of the Alps, temples, bath houses, an amphitheatre and a large fort for the city garrison. Political instability and recession from the 3rd century onwards led to a slow decline.
At some time between 180 and 225 AD the Romans built the defensive London Wall around the landward side of the city. The wall was about long, high, and thick. The wall would survive for another 1,600 years and define the City of London's perimeters for centuries to come. The perimeters of the present City are roughly defined by the line of the ancient wall.
In the late 3rd century, Londinium was raided on several occasions by Saxon pirates. This led, from around 255 onwards, to the construction of an additional riverside wall. Six of the traditional seven city gates of London are of Roman origin, namely: Ludgate, Newgate, Aldersgate, Cripplegate, Bishopsgate and Aldgate (Moorgate is the exception, being of medieval origin).
By the 5th century the Roman Empire was in rapid decline, and in 410 AD the Roman occupation of Britain came to an end. Following this, the Roman city also went into rapid decline and by the end of the 5th century was practically abandoned.
Anglo-Saxon London (5th century – 1066 AD).
Until recently it was believed that Anglo-Saxon settlement initially avoided the area immediately around Londinium. However, the discovery in 2008 of an Anglo-Saxon cemetery at Covent Garden indicates that the incomers had begun to settle there at least as early as the 6th century and possibly in the 5th. The main focus of this settlement was outside the Roman walls, clustering a short distance to the west along what is now the Strand, between the Aldwych and Trafalgar Square. It was known as "Lundenwic", the "-wic" suffix here denoting a trading settlement. Recent excavations have also highlighted the population density and relatively sophisticated urban organisation of this earlier Anglo-Saxon London, which was laid out on a grid pattern and grew to house a likely population of 10-12,000.
Early Anglo-Saxon London belonged to a people known as the Middle Saxons, from whom the name of the county of Middlesex is derived, but who probably also occupied the approximate area of modern Hertfordshire and Surrey. However, by the early 7th century the London area had been incorporated into the kingdom of the East Saxons. In 604 King Saebert of Essex converted to Christianity and London received Mellitus, its first post-Roman bishop.
At this time Essex was under the overlordship of King Æthelberht of Kent, and it was under Æthelberht's patronage that Mellitus founded the first St. Paul's Cathedral, traditionally said to be on the site of an old Roman Temple of Diana (although Christopher Wren found no evidence of this). It would have only been a modest church at first and may well have been destroyed after he was expelled from the city by Saeberht's pagan successors.
The permanent establishment of Christianity in the East Saxon kingdom took place in the reign of King Sigeberht II in the 650s. During the 8th century the kingdom of Mercia extended its dominance over south-eastern England, initially through overlordship which at times developed into outright annexation. London seems to have come under direct Mercian control in the 730s.
Viking attacks dominated most of the 9th century, becoming increasingly common from around 830 onwards. London was sacked in 842 and again in 851. The Danish "Great Heathen Army", which had rampaged across England since 865, wintered in London in 871. The city remained in Danish hands until 886, when it was captured by the forces of King Alfred the Great of Wessex and reincorporated into Mercia, then governed under Alfred's sovereignty by his son-in-law Ealdorman Æthelred.
Around this time the focus of settlement moved within the old Roman walls for the sake of defence, and the city became known as "Hindenburg". The Roman walls were repaired and the defensive ditch re-cut, while the bridge was probably rebuilt at this time. A second fortified Borough was established on the south bank at Southwark, the "Suthringa Geworc" (defensive work of the men of Surrey). The old settlement of "Lundenwic" became known as the "ealdwic" or "old settlement", a name which survives today as Aldwich.
From this point, the City of London began to develop its own unique local government. Following Ethelred's death in 911 it was transferred to Wessex, preceding the absorption of the rest of Mercia in 918. Although it faced competition for political pre-eminence in the united Kingdom of England from the traditional West Saxon centre of Winchester, London's size and commercial wealth brought it a steadily increasing importance as a focus of governmental activity. King Athelstan held many meetings of the "witan" in London and issued laws from there, while King Æthelred the Unready issued the Laws of London there in 978.
Following the resumption of Viking attacks in the reign of Ethelred, London was unsuccessfully attacked in 994 by an army under King Sweyn Forkbeard of Denmark. As English resistance to the sustained and escalating Danish onslaught finally collapsed in 1013, London repulsed an attack by the Danes and was the last place to hold out while the rest of the country submitted to Sweyn, but by the end of the year it too capitulated and Æthelred fled abroad. Sweyn died just five weeks after having been proclaimed king and Æthelred was restored to the throne, but Sweyn's son Cnut returned to the attack in 1015.
After Æthelred's death at London in 1016 his son Edmund Ironside was proclaimed king there by the "witangemot" and left to gather forces in Wessex. London was then subjected to a systematic siege by Cnut but was relieved by King Edmund's army; when Edmund again left to recruit reinforcements in Wessex the Danes resumed the siege but were again unsuccessful. However, following his defeat at the Battle of Assandun Edmund ceded to Cnut all of England north of the Thames, including London, and his death a few weeks later left Cnut in control of the whole country.
A Norse saga tells of a battle when King Æthelred returned to attack Danish-occupied London. According to the saga, the Danes lined London Bridge and showered the attackers with spears. Undaunted, the attackers pulled the roofs off nearby houses and held them over their heads in the boats. Thus protected, they were able to get close enough to the bridge to attach ropes to the piers and pull the bridge down, thus ending the Viking occupation of London. This story presumably relates to Æthelred's return to power after Sweyn's death in 1014, but there is no strong evidence of any such struggle for control of London on that occasion.
Following the extinction of Cnut's dynasty in 1042 English rule was restored under Edward the Confessor. He was responsible for the foundation of Westminster Abbey and spent much of his time at Westminster, which from this time steadily supplanted the City itself as the centre of government. Edward's death at Westminster in 1066 without a clear heir led to a succession dispute and the Norman conquest of England. Earl Harold Godwinson was elected king by the "witangemot" and crowned in Westminster Abbey but was defeated and killed by William the Bastard, Duke of Normandy at the Battle of Hastings. The surviving members of the "witan" met in London and elected King Edward's young nephew Edgar the Ætheling as king.
The Normans advanced to the south bank of the Thames opposite London, where they defeated an English attack and burned Southwark but were unable to storm the bridge. They moved upstream and crossed the river at Wallingford before advancing on London from the north-west. The resolve of the English leadership to resist collapsed and the chief citizens of London went out together with the leading members of the Church and aristocracy to submit to William at Berkhamstead, although according to some accounts there was a subsequent violent clash when the Normans reached the city. Having occupied London, William was crowned king in Westminster Abbey.
Norman and Medieval London (1066 – late 15th century).
The new Norman regime established new fortresses within the city to dominate the native population. By far the most important of these was the Tower of London at the eastern end of the city, where the initial wooden fortification was rapidly replaced by the construction of the first stone castle in England. The smaller forts of Baynard's Castle and Montfichet's Castle were also established along the waterfront. King William also granted a charter in 1067 confirming the city's existing rights, privileges and laws. Its growing self-government was consolidated by the election rights granted by King John in 1199 and 1215.
In 1097 William Rufus, the son of William the Conqueror began the construction of 'Westminster Hall', which became the focus of the Palace of Westminster.
In 1176 construction began of the most famous incarnation of London Bridge (completed in 1209) which was built on the site of several earlier wooden bridges. This bridge would last for 600 years, and remained the only bridge across the River Thames until 1739.
In 1216 during the First Barons' War London was occupied by Prince Louis of France, who had been called in by the baronial rebels against King John and was acclaimed as King of England in St Paul's Cathedral. However, following John's death in 1217 Louis's supporters reverted to their Plantagenet allegiance, rallying round John's son Henry III, and Louis was forced to withdraw from England.
Over the following centuries, London would shake off the heavy French cultural and linguistic influence which had been there since the times of the Norman conquest. The city would figure heavily in the development of Early Modern English.
During the Peasants' Revolt of 1381 London was invaded by rebels led by Wat Tyler. A group of peasants stormed the Tower of London and executed the Lord Chancellor, Archbishop Simon Sudbury, and the Lord Treasurer. The peasants looted the city and set fire to numerous buildings. Tyler was stabbed to death by the Lord Mayor William Walworth in a confrontation at Smithfield and the revolt collapsed.
Trade increased steadily during the Middle Ages, and London grew rapidly as a result. In 1100 London's population was somewhat more than 15,000. By 1300 it had grown to roughly 80,000. London lost at least half of its population during the Black Death in the mid-14th century, but its economic and political importance stimulated a rapid recovery despite further epidemics. Trade in London was organised into various guilds, which effectively controlled the city, and elected the Lord Mayor of the City of London.
Medieval London was made up of narrow and twisting streets, and most of the buildings were made from combustible materials such as wood and straw, which made fire a constant threat, while sanitation in cities was poor.
Modern history.
Tudor London (1485–1603).
During the Reformation, London was the principal early centre of Protestantism in England. Its close commercial connections with the Protestant heartlands in northern continental Europe, large foreign mercantile communities, disproportionately large number of literate inhabitants and role as the centre of the English print trade all contributed to the spread of the new ideas of religious reform. Before the Reformation, more than half of the area of London was the property of monasteries, nunneries and other religious houses.
Henry VIII's "Dissolution of the Monasteries" had a profound effect on the city as nearly all of this property changed hands. The process started in the mid 1530s, and by 1538 most of the larger monastic houses had been abolished. Holy Trinity Aldgate went to Lord Audley, and the Marquess of Winchester built himself a house in part of its precincts. The Charterhouse went to Lord North, Blackfriars to , the leper hospital of St Giles to Lord Dudley, while the king took for himself the leper hospital of St James, which was rebuilt as St James's Palace.
The period saw London rapidly rising in importance amongst Europe's commercial centres. Trade expanded beyond Western Europe to Russia, the Levant, and the Americas. This was the period of mercantilism and monopoly trading companies such as the Muscovy Company (1555) and the British East India Company (1600) were established in London by Royal Charter. The latter, which ultimately came to rule India, was one of the key institutions in London, and in Britain as a whole, for two and a half centuries. Immigrants arrived in London not just from all over England and Wales, but from abroad as well, for example Huguenots from France; the population rose from an estimated 50,000 in 1530 to about 225,000 in 1605. The growth of the population and wealth of London was fuelled by a vast expansion in the use of coastal shipping.
The late 16th and early 17th century saw the great flourishing of drama in London whose preeminent figure was William Shakespeare. During the mostly calm later years of Elizabeth's reign, some of her courtiers and some of the wealthier citizens of London built themselves country residences in Middlesex, Essex and Surrey. This was an early stirring of the villa movement, the taste for residences which were neither of the city nor on an agricultural estate, but at the time of Elizabeth's death in 1603, London was still very compact.
Xenophobia was rampant in London, and increased after the 1580s. Many immigrants became disillusioned by routine threats of violence and molestation, attempts at expulsion of foreigners, and the great difficulty in acquiring English citizenship. Dutch cities proved more hospitable, and many left London permanently.
Stuart London (1603–1714).
London's expansion beyond the boundaries of the City was decisively established in the 17th century. In the opening years of that century the immediate environs of the City, with the principal exception of the aristocratic residences in the direction of Westminster, were still considered not conducive to health. Immediately to the north was Moorfields, which had recently been drained and laid out in walks, but it was frequented by beggars and travellers, who crossed it in order to get into London. Adjoining Moorfields were Finsbury Fields, a favourite practising ground for the archers, Mile End, then a common on the Great Eastern Road and famous as a rendezvous for the troops.
The preparations for King James I becoming king were interrupted by a severe plague epidemic, which may have killed over thirty thousand people. The Lord Mayor's Show, which had been discontinued for some years, was revived by order of the king in 1609. The dissolved monastery of the Charterhouse, which had been bought and sold by the courtiers several times, was purchased by Thomas Sutton for £13,000. The new hospital, chapel, and schoolhouse were begun in 1611. Charterhouse School was to be one of the principal public schools in London until it moved to Surrey in Victorian times, and the site is still used as a medical school.
The general meeting-place of Londoners in the day-time was the nave of Old St. Paul's Cathedral. Merchants conducted business in the aisles, and used the font as a counter upon which to make their payments; lawyers received clients at their particular pillars; and the unemployed looked for work. St Paul's Churchyard was the centre of the book trade and Fleet Street was a centre of public entertainment. Under James I the theatre, which established itself so firmly in the latter years of Elizabeth, grew further in popularity. The performances at the public theatres were complemented by elaborate masques at the royal court and at the inns of court.
Charles I acceded to the throne in 1625. During his reign, aristocrats began to inhabit the West End in large numbers. In addition to those who had specific business at court, increasing numbers of country landowners and their families lived in London for part of the year simply for the social life. This was the beginning of the "London season". Lincoln's Inn Fields was built about 1629. The piazza of Covent Garden, designed by England's first classically trained architect Inigo Jones followed in about 1632. The neighbouring streets were built shortly afterwards, and the names of Henrietta, Charles, James, King and York Streets were given after members of the royal family.
In January 1642 five members of parliament whom the King wished to arrest were granted refuge in the City. In August of the same year the King raised his banner at Nottingham, and during the English Civil War London took the side of the parliament. Initially the king had the upper hand in military terms and in November he won the Battle of Brentford a few miles to the west of London. The City organised a new makeshift army and Charles hesitated and retreated. Subsequently an extensive system of fortifications was built to protect London from a renewed attack by the Royalists. This comprised a strong earthen rampart, enhanced with bastions and redoubts. It was well beyond the City walls and encompassed the whole urban area, including Westminster and Southwark. London was not seriously threatened by the royalists again, and the financial resources of the City made an important contribution to the parliamentarians' victory in the war.
The unsanitary and overcrowded City of London has suffered from the numerous outbreaks of the plague many times over the centuries, but in Britain it is the last major outbreak which is remembered as the "Great Plague" It occurred in 1665 and 1666 and killed around 60,000 people, which was one fifth of the population. Samuel Pepys chronicled the epidemic in his diary. On 4 September 1665 he wrote "I have stayed in the city till above 7400 died in one week, and of them about 6000 of the plague, and little noise heard day or night but tolling of bells."
Great Fire of London (1666).
The Great Plague was immediately followed by another catastrophe, albeit one which helped to put an end to the plague. On the Sunday, 2 September 1666 the Great Fire of London broke out at one o'clock in the morning at a bakery in Pudding Lane in the southern part of the City. Fanned by an eastern wind the fire spread, and efforts to arrest it by pulling down houses to make firebreaks were disorganised to begin with. On Tuesday night the wind fell somewhat, and on Wednesday the fire slackened. On Thursday it was extinguished, but on the evening of that day the flames again burst forth at the Temple. Some houses were at once blown up by gunpowder, and thus the fire was finally mastered. The Monument was built to commemorate the fire: for over a century and a half it bore an inscription attributing the conflagration to a ""popish frenzy"".
The fire destroyed about 60% of the City, including Old St Paul's Cathedral, 87 parish churches, 44 livery company halls and the Royal Exchange. However, the number of lives lost was surprisingly small; it is believed to have been 16 at most. Within a few days of the fire, three plans were presented to the king for the rebuilding of the city, by Christopher Wren, John Evelyn and Robert Hooke.
Wren proposed to build main thoroughfares north and south, and east and west, to insulate all the churches in conspicuous positions, to form the most public places into large piazzas, to unite the halls of the 12 chief livery companies into one regular square annexed to the Guildhall, and to make a fine quay on the bank of the river from Blackfriars to the Tower of London. Wren wished to build the new streets straight and in three standard widths of thirty, sixty and ninety feet. Evelyn's plan differed from Wren's chiefly in proposing a street from the church of St Dunstan's in the East to the St Paul's, and in having no quay or terrace along the river. These plans were not implemented, and the rebuilt city generally followed the streetplan of the old one, and most of it has survived into the 21st century.
Nonetheless, the new City was different from the old one. Many aristocratic residents never returned, preferring to take new houses in the West End, where fashionable new districts such as St. James's were built close to the main royal residence, which was Whitehall Palace until it was destroyed by fire in the 1690s, and thereafter St. James's Palace. The rural lane of Piccadilly sprouted courtiers mansions such as Burlington House. Thus the separation between the middle class mercantile City of London, and the aristocratic world of the court in Westminster became complete.
In the City itself there was a move from wooden buildings to stone and brick construction to reduce the risk of fire. Parliament's Rebuilding of London Act 1666 stated ""building with brick not only more comely and durable, but also more safe against future perils of fire"". From then on only doorcases, window-frames and shop fronts were allowed to be made of wood.
Christopher Wren's plan for a new model London came to nothing, but he was appointed to rebuild the ruined parish churches and to replace St Paul's Cathedral. His domed baroque cathedral was the primary symbol of London for at least a century and a half. As city surveyor, Robert Hooke oversaw the reconstruction of the City's houses. The East End, that is the area immediately to the east of the city walls, also became heavily populated in the decades after the Great Fire. London's docks began to extend downstream, attracting many working people who worked on the docks themselves and in the processing and distributive trades. These people lived in Whitechapel, Wapping, Stepney and Limehouse, generally in slum conditions.
In the winter of 1683–4 a frost fair was held on the Thames. The frost, which began about seven weeks before Christmas and continued for six weeks after, was the greatest on record. The Revocation of the Edict of Nantes in 1685 led to a large migration on Huguenots to London. They established a silk industry at Spitalfields.
At this time the Bank of England was founded, and the British East India Company was expanding its influence. Lloyd's of London also began to operate in the late 17th century. In 1700 London handled 80% of England's imports, 69% of its exports and 86% of its re-exports. Many of the goods were luxuries from the Americas and Asia such as silk, sugar, tea and tobacco. The last figure emphasises London's role as an entrepot: while it had many craftsmen in the 17th century, and would later acquire some large factories, its economic prominence was never based primarily on industry. Instead it was a great trading and redistribution centre. Goods were brought to London by England's increasingly dominant merchant navy, not only to satisfy domestic demand, but also for re-export throughout Europe and beyond.
William III, a Dutchman, cared little for London, the smoke of which gave him asthma, and after the first fire at Whitehall Palace (1691) he purchased Nottingham House and transformed it into Kensington Palace. Kensington was then an insignificant village, but the arrival of the court soon caused it to grow in importance. The palace was rarely favoured by future monarchs, but its construction was another step in the expansion of the bounds of London. During the same reign Greenwich Hospital, then well outside the boundary of London, but now comfortably inside it, was begun; it was the naval complement to the Chelsea Hospital for former soldiers, which had been founded in 1681. During the reign of Queen Anne an act was passed authorising the building of 50 new churches to serve the greatly increased population living outside the boundaries of the City of London.
18th century.
The 18th century was a period of rapid growth for London, reflecting an increasing national population, the early stirrings of the Industrial Revolution, and London's role at the centre of the evolving British Empire.
In 1707 an Act of Union was passed merging the Scottish and the English Parliaments, thus establishing the Kingdom of Great Britain. A year later, in 1708 Christopher Wren's masterpiece, St Paul's Cathedral was completed on his birthday. However, the first service had been held on 2 December 1697; more than 10 years earlier. This Cathedral replaced the original St. Paul's which had been completely destroyed in the Great Fire of London. This building is considered one of the finest in Britain and a fine example of Baroque architecture.
Many tradesmen from different countries came to London to trade goods and merchandise. Also, more immigrants moved to London making the population greater. More people also moved to London for work and for business making London an altogether bigger and busier city. Britain's victory in the Seven Years' War increased the country's international standing and opened large new markets to British trade, further boosting London's prosperity.
During the Georgian period London spread beyond its traditional limits at an accelerating pace. This is shown in a series of detailed maps, particularly John Rocque's 1741–45 map "(see below)" and his 1746 Map of London. New districts such as Mayfair were built for the rich in the West End, new bridges over the Thames encouraged an acceleration of development in South London and in the East End, the Port of London expanded downstream from the City. During this period was also the uprising of the American colonies. In 1780, the Tower of London held its only American prisoner, former President of the Continental Congress, Henry Laurens. In 1779 he was the Congress's representative of Holland, and got the country's support for the Revolution. On his return voyage back to America, the Royal Navy captured him and charged him with treason after finding evidence of a reason of war between Great Britain and the Netherlands. He was released from the Tower on 21 December 1781 in exchange for General Lord Cornwallis.
In 1762 George III acquired Buckingham Palace (then called Buckingham House) from the Duke of Buckingham. It was enlarged over the next 75 years by architects such as John Nash.
A phenomenon of 18th-century London was the coffeehouse, which became a popular place to debate ideas. Growing literacy and the development of the printing press meant that news became widely available. Fleet Street became the centre of the embryonic British press during the century.
18th-century London was dogged by crime, the Bow Street Runners were established in 1750 as a professional police force. Penalties for crime were harsh, with the death penalty being applied for fairly minor crimes. Public hangings were common in London, and were popular public events.
In 1780 London was rocked by the Gordon Riots, an uprising by Protestants against Roman Catholic emancipation led by Lord George Gordon. Severe damage was caused to Catholic churches and homes, and 285 rioters were killed.
In the year 1787, freed slaves from London, America, and many of Britain's colonies founded Freetown in modern-day Sierra Leone.
Up until 1750, London Bridge was the only crossing over the Thames, but in that year Westminster Bridge was opened and, for the first time in history, London Bridge, in a sense, had a rival. In 1798, Frankfurt banker Nathan Mayer Rothschild arrived in London and set up a banking house in the city, with a large sum of money given to him by his father, Amschel Mayer Rothschild. The Rothschilds also had banks in Paris and Vienna. The bank financed numerous large-scale projects, especially regarding railways around the world and the Suez Canal.
The 18th century saw the breakaway of the American colonies and many other unfortunate events in London, but also great change and Enlightenment. This all led into the beginning of modern times, the 19th century.
19th century.
During the 19th century, London was transformed into the world's largest city and capital of the British Empire. Its population expanded from 1 million in 1800 to 6.7 million a century later. During this period, London became a global political, financial, and trading capital. In this position, it was largely unrivalled until the latter part of the century, when Paris and New York began to threaten its dominance.
While the city grew wealthy as Britain's holdings expanded, 19th-century London was also a city of poverty, where millions lived in overcrowded and unsanitary slums. Life for the poor was immortalised by Charles Dickens in such novels as Oliver Twist In 1810, after the death of Sir Francis Baring and Abraham Goldsmid, Rothschild emerges as the major banker in London.
In 1829 the then Home Secretary (and future prime minister) Robert Peel established the Metropolitan Police as a police force covering the entire urban area. The force gained the nickname of "bobbies" or "peelers" named after Robert Peel.
19th-century London was transformed by the coming of the railways. A new network of metropolitan railways allowed for the development of suburbs in neighbouring counties from which middle-class and wealthy people could commute to the centre. While this spurred the massive outward growth of the city, the growth of greater London also exacerbated the class divide, as the wealthier classes emigrated to the suburbs, leaving the poor to inhabit the inner city areas.
The first railway to be built in London was a line from London Bridge to Greenwich, which opened in 1836. This was soon followed by the opening of great rail termini which linked London to every corner of Britain. These included Euston station (1837), Paddington station (1838), Fenchurch Street station (1841), Waterloo station (1848), King's Cross station (1850), and St Pancras station (1863). From 1863, the first lines of the London Underground were constructed.
The urbanised area continued to grow rapidly, spreading into Islington, Paddington, Belgravia, Holborn, Finsbury, Shoreditch, Southwark and Lambeth. Towards the middle of the century, London's antiquated local government system, consisting of ancient parishes and vestries, struggled to cope with the rapid growth in population. In 1855 the Metropolitan Board of Works (MBW) was created to provide London with adequate infrastructure to cope with its growth. One of its first tasks was addressing London's sanitation problems. At the time, raw sewage was pumped straight into the River Thames. This culminated in The Great Stink of 1858. Parliament finally gave consent for the MBW to construct a large system of sewers. The engineer put in charge of building the new system was Joseph Bazalgette. In what was one of the largest civil engineering projects of the 19th century, he oversaw construction of over 2100 km of tunnels and pipes under London to take away sewage and provide clean drinking water. When the London sewerage system was completed, the death toll in London dropped dramatically, and epidemics of cholera and other diseases were curtailed. Bazalgette's system is still in use today.
One of the most famous events of 19th-century London was the Great Exhibition of 1851. Held at The Crystal Palace, the fair attracted 6 million visitors from across the world and displayed Britain at the height of its Imperial dominance.
As the capital of a massive empire, London became a magnet for immigrants from the colonies and poorer parts of Europe. A large Irish population settled in the city during the Victorian period, with many of the newcomers refugees from the Great Famine (1845–1849). At one point, Catholic Irish made up about 20% of London's population; they typically lived in overcrowded slums. London also became home to a sizable Jewish community, which was notable for its entrepreneurship in the clothing trade and merchandising.
In 1888, the new County of London was established, administered by the London County Council. This was the first elected London-wide administrative body, replacing the earlier Metropolitan Board of Works, which had been made up of appointees. The County of London covered broadly what was then the full extent of the London conurbation, although the conurbation later outgrew the boundaries of the county. In 1900, the county was sub-divided into 28 metropolitan boroughs, which formed a more local tier of administration than the county council.
Many famous buildings and landmarks of London were constructed during the 19th century including:
20th century.
1900 to World War I.
London entered the 20th century at the height of its influence as the capital of one of the largest empires in history, but the new century was to bring many challenges.
London's population continued to grow rapidly in the early decades of the century, and public transport was greatly expanded. A large tram network was constructed by the London County Council, through the LCC Tramways; the first motorbus service began in the 1900s. Improvements to London's overground and underground rail network, including large scale electrification were progressively carried out.
During World War I, London experienced its first bombing raids carried out by German zeppelin airships; these killed around 700 people and caused great terror, but were merely a foretaste of what was to come. The city of London would experience many more terrors as a result of both World Wars. The largest explosion in London occurred during World War I: the Silvertown explosion, when a munitions factory containing 50 tons of TNT exploded, killing 73 and injuring 400.
The period between the two World Wars saw London's geographical extent growing more quickly than ever before or since. A preference for lower density suburban housing, typically semi-detached, by Londoners seeking a more "rural" lifestyle, superseded Londoners' old predilection for terraced houses. This was facilitated not only by a continuing expansion of the rail network, including trams and the Underground, but also by slowly widening car ownership. London's suburbs expanded outside the boundaries of the County of London, into the neighbouring counties of Essex, Hertfordshire, Kent, Middlesex and Surrey.
Like the rest of the country, London suffered severe unemployment during the Great Depression of the 1930s. In the East End during the 1930s, politically extreme parties of both right and left flourished. The Communist Party of Great Britain and the British Union of Fascists both gained serious support. Clashes between right and left culminated in the Battle of Cable Street in 1936. The population of London reached an all-time peak of 8.6 million in 1939.
Large numbers of Jewish immigrants fleeing from Nazi Germany, settled in London during the 1930s, mostly in the East End.
In World War II.
During World War II, London, as many other British cities, suffered severe damage, being bombed extensively by the "Luftwaffe" as a part of The Blitz. Prior to the bombing, hundreds of thousands of children in London were evacuated to the countryside to avoid the bombing. Civilians took shelter from the air raids in underground stations.
The heaviest bombing took place during The Blitz between 7 September 1940 and 10 May 1941. During this period, London was subjected to 71 separate raids receiving over 18,000 tonnes of high explosive. One raid in December 1940, which became known as the Second Great Fire of London saw a firestorm engulf much of the City of London and destroy many historic buildings. St Paul's Cathedral however remained unscathed; A photograph showing the Cathedral shrouded in smoke became a famous image of the war.
Having failed to defeat Britain, Hitler turned his attention to the Eastern front and regular bombing raids ceased. They began again, but on a smaller scale with the "Little Blitz" in early 1944. Towards the end of the war, during 1944/45 London again came under heavy attack by pilotless V-1 flying bombs and V-2 rockets, which were fired from Nazi occupied Europe. These attacks only came to an end when their launch sites were captured by advancing Allied forces.
London suffered severe damage and heavy casualties, the worst hit part being the Docklands area. By the war's end, just under 30,000 Londoners had been killed by the bombing, and over 50,000 seriously injured, tens of thousands of buildings were destroyed, and hundreds of thousands of people were made homeless.
1945–2000.
Three years after the war, the 1948 Summer Olympics were held at the original Wembley Stadium, at a time when the city had barely recovered from the war. London's rebuilding was slow to begin. However, in 1951 the Festival of Britain was held, which marked an increasing mood of optimism and forward looking.
In the immediate postwar years housing was a major issue in London, due to the large amount of housing which had been destroyed in the war. The authorities decided upon high-rise blocks of flats as the answer to housing shortages. During the 1950s and 1960s the skyline of London altered dramatically as tower blocks were erected, although these later proved unpopular. In a bid to reduce the number of people living in overcrowded housing, a policy was introduced of encouraging people to move into newly built new towns surrounding London.
Through the 19th and in the early half of the 20th century, Londoners used coal for heating their homes, which produced large amounts of smoke. In combination with climatic conditions this often caused a characteristic smog, and London became known for its typical "London Fog", also known as "Pea Soupers". London was sometimes referred to as "The Smoke" because of this. In 1952 this culminated in the disastrous Great Smog of 1952 which lasted for five days and killed over 4,000 people. In response to this, the Clean Air Act 1956 was passed, mandating the creating of "smokeless zones" where the use of "smokeless" fuels was required (this was at a time when most households still used open fires); the Act was effective.
Starting in the mid-1960s, and partly as a result of the success of such UK musicians as the Beatles and the Rolling Stones, London became a centre for the worldwide youth culture, exemplified by the Swinging London subculture which made Carnaby Street a household name of youth fashion around the world. London's role as a trendsetter for youth fashion was revived strongly in the 1980s during the new wave and punk eras. In the mid-1990s this was revived to some extent with the emergence of the Britpop era.
From the 1950s onwards London became home to a large number of immigrants, largely from Commonwealth countries such as Jamaica, India, Bangladesh, Pakistan, which dramatically changed the face of London, turning it into one of the most diverse cities in Europe. However, the integration of the new immigrants was not always easy. Racial tensions emerged in events such as the Brixton Riots in the early 1980s.
From the beginning of "The Troubles" in Northern Ireland in the early 1970s until the mid-1990s, London was subjected to repeated terrorist attacks by the Provisional IRA.
The outward expansion of London was slowed by the war, and the introduction of the Metropolitan Green Belt. Due to this outward expansion, in 1965 the old County of London (which by now only covered part of the London conurbation) and the London County Council were abolished, and the much larger area of Greater London was established with a new Greater London Council (GLC) to administer it, along with 32 new London boroughs.
Greater London's population declined steadily in the decades after World War II, from an estimated peak of 8.6 million in 1939 to around 6.8 million in the 1980s. However, it then began to increase again in the late 1980s, encouraged by strong economic performance and an increasingly positive image.
London's traditional status as a major port declined dramatically in the post-war decades as the old Docklands could not accommodate large modern container ships. The principal ports for London moved downstream to the ports of Felixstowe and Tilbury. The docklands area had become largely derelict by the 1980s, but was redeveloped into flats and offices from the mid-1980s onwards. The Thames Barrier was completed in the 1980s to protect London against tidal surges from the North Sea.
In the early 1980s political disputes between the GLC run by Ken Livingstone and the Conservative government of Margaret Thatcher led to the GLC's abolition in 1986, with most of its powers relegated to the London boroughs. This left London as the only large metropolis in the world without a central administration.
In 2000, London-wide government was restored, with the creation of the Greater London Authority (GLA) by Tony Blair's government, covering the same area of Greater London. The new authority had similar powers to the old GLC, but was made up of a directly elected Mayor and a London Assembly. The first election took place on 4 May, with Ken Livingstone comfortably regaining his previous post. London was recognised as one of the nine regions of England. In global perspective, it was emerging as a World city widely compared to New York and Tokyo.
21st century.
Around the start of the 21st century, London hosted the much derided Millennium Dome at Greenwich, to mark the new century. Other Millennium projects were more successful. One was the largest observation wheel in the world, the "Millennium Wheel", or the London Eye, which was erected as a temporary structure, but soon became a fixture, and draws four million visitors a year. The National Lottery also released a flood of funds for major enhancements to existing attractions, for example the roofing of the Great Court at the British Museum.
The London Plan, published by the Mayor of London in 2004, estimated that the population would reach 8.1 million by 2016, and continue to rise thereafter. This was reflected in a move towards denser, more urban styles of building, including a greatly increased number of tall buildings, and proposals for major enhancements to the public transport network. However, funding for projects such as Crossrail remained a struggle.
On 6 July 2005 London won the right to host the 2012 Olympics and Paralympics making it the first city to host the modern games three times. However, celebrations were cut short the following day when the city was rocked by a series of terrorist attacks. More than 50 were killed and 750 injured in three bombings on London Underground trains and a fourth on a double decker bus near King's Cross.
In the public there was ambivalence leading-up to the Olympics, though public sentiment changed strongly in their favour following a successful opening ceremony and when the anticipated organisational and transport problems never occurred.

</doc>
<doc id="14021" url="https://en.wikipedia.org/wiki?curid=14021" title="History of astronomy">
History of astronomy

Astronomy is the oldest of the natural sciences, dating back to antiquity, with its origins in the religious, mythological, cosmological, calendrical, and astrological beliefs and practices of pre-history: vestiges of these are still found in astrology, a discipline long interwoven with public and governmental astronomy, and not completely disentangled from it until a few centuries ago in the Western World (see astrology and astronomy). In some cultures, astronomical data was used for astrological prognostication.
Ancient astronomers were able to differentiate between stars and planets, as stars remain relatively fixed over the centuries while planets will move an appreciable amount during a comparatively short time.
Early history.
Early cultures identified celestial objects with gods and spirits. They related these objects (and their movements) to phenomena such as rain, drought, seasons, and tides. It is generally believed that the first astronomers were priests, and that they understood celestial objects and events to be manifestations of the divine, hence early astronomy's connection to what is now called astrology. Ancient structures with possibly astronomical alignments (such as Stonehenge) probably fulfilled astronomical, religious, and social functions.
Calendars of the world have often been set by observations of the Sun and Moon (marking the day, month and year), and were important to agricultural societies, in which the harvest depended on planting at the correct time of year. The most common modern calendar is based on the Roman calendar, which broke the traditional link of the month to the phases of the moon and divided the year into twelve months, alternately comprising thirty and thirty-one days. In 46 BC, Julius Caesar instigated calendar reform and adopted what is now known as the Julian calendar, based upon the 365 day year length originally proposed by the 4th century BC Greek astronomer Callippus.
Prehistoric Europe.
Since 1990 our understanding of prehistoric Europeans has been radically changed by discoveries of ancient astronomical artifacts throughout Europe. The artifacts demonstrate that Neolithic and Bronze Age Europeans had a sophisticated knowledge of mathematics and astronomy.
Among the discoveries are:
Mesopotamia.
The origins of Western astronomy can be found in Mesopotamia, the "land between the rivers" Tigris and Euphrates, where the ancient kingdoms of Sumer, Assyria, and Babylonia were located. A form of writing known as cuneiform emerged among the Sumerians around 3500–3000 BC. Our knowledge of Sumerian astronomy is indirect, via the earliest Babylonian star catalogues dating from about 1200 BC. The fact that many star names appear in Sumerian suggests a continuity reaching into the Early Bronze Age. Astral theology, which gave planetary gods an important role in Mesopotamian mythology and religion, began with the Sumerians. They also used a sexagesimal (base 60) place-value number system, which simplified the task of recording very large and very small numbers. The modern practice of dividing a circle into 360 degrees, of 60 minutes each, began with the Sumerians. For more information, see the articles on Babylonian numerals and mathematics.
Classical sources frequently use the term Chaldeans for the astronomers of Mesopotamia, who were, in reality, priest-scribes specializing in astrology and other forms of divination.
The first evidence of recognition that astronomical phenomena are periodic and of the application of mathematics to their prediction is Babylonian. Tablets dating back to the Old Babylonian period document the application of mathematics to the variation in the length of daylight over a solar year. Centuries of Babylonian observations of celestial phenomena are recorded in the series of cuneiform tablets known as the "Enūma Anu Enlil". The oldest significant astronomical text that we possess is Tablet 63 of the "Enūma Anu Enlil", the Venus tablet of Ammi-saduqa, which lists the first and last visible risings of Venus over a period of about 21 years and is the earliest evidence that the phenomena of a planet were recognized as periodic. The MUL.APIN, contains catalogues of stars and constellations as well as schemes for predicting heliacal risings and the settings of the planets, lengths of daylight measured by a water clock, gnomon, shadows, and intercalations. The Babylonian GU text arranges stars in 'strings' that lie along declination circles and thus measure right-ascensions or time-intervals, and also employs the stars of the zenith, which are also separated by given right-ascensional differences.
A significant increase in the quality and frequency of Babylonian observations appeared during the reign of Nabonassar (747–733 BC). The systematic records of ominous phenomena in Babylonian astronomical diaries that began at this time allowed for the discovery of a repeating 18-year cycle of lunar eclipses, for example. The Greek astronomer Ptolemy later used Nabonassar's reign to fix the beginning of an era, since he felt that the earliest usable observations began at this time.
The last stages in the development of Babylonian astronomy took place during the time of the Seleucid Empire (323–60 BC). In the third century BC, astronomers began to use "goal-year texts" to predict the motions of the planets. These texts compiled records of past observations to find repeating occurrences of ominous phenomena for each planet. About the same time, or shortly afterwards, astronomers created mathematical models that allowed them to predict these phenomena directly, without consulting past records. A notable Babylonian astronomer from this time was Seleucus of Seleucia, who was a supporter of the heliocentric model.
Babylonian astronomy was the basis for much of what was done in Greek and Hellenistic astronomy, in classical Indian astronomy, in Sassanian Iran, in Byzantium, in Syria, in Islamic astronomy, in Central Asia, and in Western Europe.
India.
Astronomy in the Indian subcontinent dates back to the period of Indus Valley Civilization during 3rd millennium BCE, when it was used to create calendars. As the Indus Valley civilization did not leave behind written documents, the oldest extant Indian astronomical text is the Vedanga Jyotisha, dating from the Vedic period. Vedanga Jyotisha describes rules for tracking the motions of the Sun and the Moon for the purposes of ritual. During 6th century AD, astronomy was influenced by the Greek and Byzantine astronomical traditions.
Aryabhata (476–550), in his magnum opus "Aryabhatiya" (499), propounded a computational system based on a planetary model in which the Earth was taken to be spinning on its axis and the periods of the planets were given with respect to the Sun. He accurately calculated many astronomical constants, such as the periods of the planets, times of the solar and lunar eclipses, and the instantaneous motion of the Moon. Early followers of Aryabhata's model included Varahamihira, Brahmagupta, and Bhaskara II.
Astronomy was advanced during the Shunga Empire and many star catalogues were produced during this time. The Shunga period is known as the "Golden age of astronomy in India".
It saw the development of calculations for the motions and places of various planets, their rising and setting, conjunctions, and the calculation of eclipses.
Bhāskara II (1114–1185) was the head of the astronomical observatory at Ujjain, continuing the mathematical tradition of Brahmagupta. He wrote the "Siddhantasiromani" which consists of two parts: "Goladhyaya" (sphere) and "Grahaganita" (mathematics of the planets). He also calculated the time taken for the Earth to orbit the sun to 9 decimal places. The Buddhist University of Nalanda at the time offered formal courses in astronomical studies.
Other important astronomers from India include Madhava of Sangamagrama, Nilakantha Somayaji and Jyeshtadeva, who were members of the Kerala school of astronomy and mathematics from the 14th century to the 16th century. Nilakantha Somayaji, in his "Aryabhatiyabhasya", a commentary on Aryabhata's "Aryabhatiya", developed his own computational system for a partially heliocentric planetary model, in which Mercury, Venus, Mars, Jupiter and Saturn orbit the Sun, which in turn orbits the Earth, similar to the Tychonic system later proposed by Tycho Brahe in the late 16th century. Nilakantha's system, however, was mathematically more effient than the Tychonic system, due to correctly taking into account the equation of the centre and latitudinal motion of Mercury and Venus. Most astronomers of the Kerala school of astronomy and mathematics who followed him accepted his planetary model.
Greece and Hellenistic world.
The Ancient Greeks developed astronomy, which they treated as a branch of mathematics, to a highly sophisticated level. The first geometrical, three-dimensional models to explain the apparent motion of the planets were developed in the 4th century BC by Eudoxus of Cnidus and Callippus of Cyzicus. Their models were based on nested homocentric spheres centered upon the Earth. Their younger contemporary Heraclides Ponticus proposed that the Earth rotates around its axis.
A different approach to celestial phenomena was taken by natural philosophers such as Plato and Aristotle. They were less concerned with developing mathematical predictive models than with developing an explanation of the reasons for the motions of the Cosmos. In his "Timaeus", Plato described the universe as a spherical body divided into circles carrying the planets and governed according to harmonic intervals by a world soul. Aristotle, drawing on the mathematical model of Eudoxus, proposed that the universe was made of a complex system of concentric spheres, whose circular motions combined to carry the planets around the earth. This basic cosmological model prevailed, in various forms, until the 16th century AD.
In the 3rd century BC Aristarchus of Samos was the first to suggest a heliocentric system, although only fragmentary descriptions of his idea survive. Eratosthenes, using the angles of shadows created at widely separated regions, estimated the circumference of the Earth with great accuracy.
Greek geometrical astronomy developed away from the model of concentric spheres to employ more complex models in which an eccentric circle would carry around a smaller circle, called an epicycle which in turn carried around a planet. The first such model is attributed to Apollonius of Perga and further developments in it were carried out in the 2nd century BC by Hipparchus of Nicea. Hipparchus made a number of other contributions, including the first measurement of precession and the compilation of the first star catalog in which he proposed our modern system of apparent magnitudes.
The Antikythera mechanism, an ancient Greek astronomical observational device for calculating the movements of the Sun and the Moon, possibly the planets, dates from about 150–100 BC, and was the first ancestor of an astronomical computer. It was discovered in an ancient shipwreck off the Greek island of Antikythera, between Kythera and Crete. The device became famous for its use of a differential gear, previously believed to have been invented in the 16th century AD, and the miniaturization and complexity of its parts, comparable to a clock made in the 18th century. The original mechanism is displayed in the Bronze collection of the National Archaeological Museum of Athens, accompanied by a replica.
Depending on the historian's viewpoint, the acme or corruption of physical Greek astronomy is seen with Ptolemy of Alexandria, who wrote the classic comprehensive presentation of geocentric astronomy, the "Megale Syntaxis" (Great Synthesis), better known by its Arabic title "Almagest", which had a lasting effect on astronomy up to the Renaissance. In his "Planetary Hypotheses", Ptolemy ventured into the realm of cosmology, developing a physical model of his geometric system, in a universe many times smaller than the more realistic conception of Aristarchus of Samos four centuries earlier.
Egypt.
The precise orientation of the Egyptian pyramids affords a lasting demonstration of the high degree of technical skill in watching the heavens attained in the 3rd millennium BC. It has been shown the Pyramids were aligned towards the pole star, which, because of the precession of the equinoxes, was at that time Thuban, a faint star in the constellation of Draco. Evaluation of the site of the temple of Amun-Re at Karnak, taking into account the change over time of the obliquity of the ecliptic, has shown that the Great Temple was aligned on the rising of the midwinter sun. The length of the corridor down which sunlight would travel would have limited illumination at other times of the year.
Astronomy played a considerable part in religious matters for fixing the dates of festivals and determining the hours of the night. The titles of several temple books are preserved recording the movements and phases of the sun, moon and stars. The rising of Sirius (Egyptian: Sopdet, Greek: Sothis) at the beginning of the inundation was a particularly important point to fix in the yearly calendar.
Writing in the Roman era, Clement of Alexandria gives some idea of the importance of astronomical observations to the sacred rites:
And after the Singer advances the Astrologer (ὡροσκόπος), with a "horologium" (ὡρολόγιον) in his hand, and a "palm" (φοίνιξ), the symbols of astrology. He must know by heart the Hermetic astrological books, which are four in number. Of these, one is about the arrangement of the fixed stars that are visible; one on the positions of the sun and moon and five planets; one on the conjunctions and phases of the sun and moon; and one concerns their risings.
The Astrologer's instruments ("horologium" and "palm") are a plumb line and sighting instrument. They have been identified with two inscribed objects in the Berlin Museum; a short handle from which a plumb line was hung, and a palm branch with a sight-slit in the broader end. The latter was held close to the eye, the former in the other hand, perhaps at arms length. The "Hermetic" books which Clement refers to are the Egyptian theological texts, which probably have nothing to do with Hellenistic Hermetism.
From the tables of stars on the ceiling of the tombs of Rameses VI and Rameses IX it seems that for fixing the hours of the night a man seated on the ground faced the Astrologer in such a position that the line of observation of the pole star passed over the middle of his head. On the different days of the year each hour was determined by a fixed star culminating or nearly culminating in it, and the position of these stars at the time is given in the tables as in the centre, on the left eye, on the right shoulder, etc. According to the texts, in founding or rebuilding temples the north axis was determined by the same apparatus, and we may conclude that it was the usual one for astronomical observations. In careful hands it might give results of a high degree of accuracy.
China.
The astronomy of East Asia began in China. Solar term was completed in Warring States period. The knowledge of Chinese astronomy was introduced into East Asia.
Astronomy in China has a long history. Detailed records of astronomical observations were kept from about the 6th century BC, until the introduction of Western astronomy and the telescope in the 17th century. Chinese astronomers were able to precisely predict eclipses.
Much of early Chinese astronomy was for the purpose of timekeeping. The Chinese used a lunisolar calendar, but because the cycles of the Sun and the Moon are different, astronomers often prepared new calendars and made observations for that purpose.
Astrological divination was also an important part of astronomy. Astronomers took careful note of "guest stars" which suddenly appeared among the fixed stars. They were the first to record a supernova, in the Astrological Annals of the Houhanshu in 185 A.D. Also, the supernova that created the Crab Nebula in 1054 is an example of a "guest star" observed by Chinese astronomers, although it was not recorded by their European contemporaries. Ancient astronomical records of phenomena like supernovae and comets are sometimes used in modern astronomical studies.
The world's first star catalogue was made by Gan De, a Chinese astronomer, in the 4th century BC.
Mesoamerica.
Maya astronomical codices include detailed tables for calculating phases of the Moon, the recurrence of eclipses, and the appearance and disappearance of Venus as morning and evening star. The Maya based their calendrics in the carefully calculated cycles of the Pleiades, the Sun, the Moon, Venus, Jupiter, Saturn, Mars, and also they had a precise description of the eclipses as depicted in the Dresden Codex, as well as the ecliptic or zodiac, and the Milky Way was crucial in their Cosmology. A number of important Maya structures are believed to have been oriented toward the extreme risings and settings of Venus. To the ancient Maya, Venus was the patron of war and many recorded battles are believed to have been timed to the motions of this planet. Mars is also mentioned in preserved astronomical codices and early mythology.
Although the Maya calendar was not tied to the Sun, John Teeple has proposed that the Maya calculated the solar year to somewhat greater accuracy than the Gregorian calendar. Both astronomy and an intricate numerological scheme for the measurement of time were vitally important components of Maya religion.
Medieval Islamic world.
The Arabic and the Persian world under Islam had become highly cultured, and many important works of knowledge from Greek astronomy and Indian astronomy and Persian astronomy were translated into Arabic, used and stored in libraries throughout the area. An important contribution by Islamic astronomers was their emphasis on observational astronomy This led to the emergence of the first astronomical observatories in the Muslim world by the early 9th century. Zij star catalogues were produced at these observatories.
In the 10th century, Abd al-Rahman al-Sufi (Azophi) carried out observations on the stars and described their positions, magnitudes, brightness, and colour and drawings for each constellation in his "Book of Fixed Stars". He also gave the first descriptions and pictures of "A Little Cloud" now known as the Andromeda Galaxy. He mentions it as lying before the mouth of a Big Fish, an Arabic constellation. This "cloud" was apparently commonly known to the Isfahan astronomers, very probably before 905 AD. The first recorded mention of the Large Magellanic Cloud was also given by al-Sufi. In 1006, Ali ibn Ridwan observed SN 1006, the brightest supernova in recorded history, and left a detailed description of the temporary star.
In the late 10th century, a huge observatory was built near Tehran, Iran, by the astronomer Abu-Mahmud al-Khujandi who observed a series of meridian transits of the Sun, which allowed him to calculate the tilt of the Earth's axis relative to the Sun. He noted that measurements by earlier (Indian, then Greek) astronomers had found higher values for this angle, possible evidence that the axial tilt is not constant but was in fact decreasing. In 11th-century Persia, Omar Khayyám compiled many tables and performed a reformation of the calendar that was more accurate than the Julian and came close to the Gregorian.
Other Muslim advances in astronomy included the collection and correction of previous astronomical data, resolving significant problems in the Ptolemaic model, the development of the universal latitude-independent astrolabe by Arzachel, the invention of numerous other astronomical instruments, Ja'far Muhammad ibn Mūsā ibn Shākir's belief that the heavenly bodies and celestial spheres were subject to the same physical laws as Earth, the first elaborate experiments related to astronomical phenomena, the introduction of exacting empirical observations and experimental techniques, and the introduction of empirical testing by Ibn al-Shatir, who produced the first model of lunar motion which matched physical observations.
Natural philosophy (particularly Aristotelian physics) was separated from astronomy by Ibn al-Haytham (Alhazen) in the 11th century, by Ibn al-Shatir in the 14th century, and Qushji in the 15th century, leading to the development of an astronomical physics.
Medieval Western Europe.
After the significant contributions of Greek scholars to the development of astronomy, it entered a relatively static era in Western Europe from the Roman era through the 12th century. This lack of progress has led some astronomers to assert that nothing happened in Western European astronomy during the Middle Ages. Recent investigations, however, have revealed a more complex picture of the study and teaching of astronomy in the period from the 4th to the 16th centuries.
Western Europe entered the Middle Ages with great difficulties that affected the continent's intellectual production. The advanced astronomical treatises of classical antiquity were written in Greek, and with the decline of knowledge of that language, only simplified summaries and practical texts were available for study. The most influential writers to pass on this ancient tradition in Latin were Macrobius, Pliny, Martianus Capella, and Calcidius. In the 6th Century Bishop Gregory of Tours noted that he had learned his astronomy from reading Martianus Capella, and went on to employ this rudimentary astronomy to describe a method by which monks could determine the time of prayer at night by watching the stars.
In the 7th Century the English monk Bede of Jarrow published an influential text, On the Reckoning of Time, providing churchmen with the practical astronomical knowledge needed to compute the proper date of Easter using a procedure called the "computus". This text remained an important element of the education of clergy from the 7th century until well after the rise of the Universities in the 12th century.
The range of surviving ancient Roman writings on astronomy and the teachings of Bede and his followers began to be studied in earnest during the revival of learning sponsored by the emperor Charlemagne. By the 9th century rudimentary techniques for calculating the position of the planets were circulating in Western Europe; medieval scholars recognized their flaws, but texts describing these techniques continued to be copied, reflecting an interest in the motions of the planets and in their astrological significance.
Building on this astronomical background, in the 10th century European scholars such as Gerbert of Aurillac began to travel to Spain and Sicily to seek out learning which they had heard existed in the Arabic-speaking world. There they first encountered various practical astronomical techniques concerning the calendar and timekeeping, most notably those dealing with the astrolabe. Soon scholars such as Hermann of Reichenau were writing texts in Latin on the uses and construction of the astrolabe and others, such as Walcher of Malvern, were using the astrolabe to observe the time of eclipses in order to test the validity of computistical tables.
By the 12th century, scholars were traveling to Spain and Sicily to seek out more advanced astronomical and astrological texts, which they translated into Latin from Arabic and Greek to further enrich the astronomical knowledge of Western Europe. The arrival of these new texts coincided with the rise of the universities in medieval Europe, in which they soon found a home. Reflecting the introduction of astronomy into the universities, John of Sacrobosco wrote a series of influential introductory astronomy textbooks: the Sphere, a Computus, a text on the Quadrant, and another on Calculation.
In the 14th century, Nicole Oresme, later bishop of Liseux, showed that neither the scriptural texts nor the physical arguments advanced against the movement of the Earth were demonstrative and adduced the argument of simplicity for the theory that the earth moves, and "not" the heavens. However, he concluded "everyone maintains, and I think myself, that the heavens do move and not the earth: For God hath established the world which shall not be moved." In the 15th century, cardinal Nicholas of Cusa suggested in some of his scientific writings that the Earth revolved around the Sun, and that each star is itself a distant sun. He was not, however, describing a scientifically verifiable theory of the universe.
Renaissance Period.
The renaissance came to astronomy with the work of Nicolaus Copernicus, who proposed a heliocentric system, in which the planets revolved around the Sun and not the Earth. His "De revolutionibus" provided a full mathematical discussion of his system, using the geometrical techniques that had been traditional in astronomy since before the time of Ptolemy. His work was later defended, expanded upon and modified by Galileo Galilei and Johannes Kepler.
Galileo was considered the father of observational astronomy. He was among the first to use a telescope to observe the sky and after constructing a 20x refractor telescope he discovered the four largest moons of Jupiter in 1610. This was the first observation of satellites orbiting another planet. He also found that our Moon had craters and observed (and correctly explained) sunspots. Galileo noted that Venus exhibited a full set of phases resembling lunar phases. Galileo argued that these observations supported the Copernican system and were, to some extent, incompatible with the favored model of the Earth at the center of the universe. He may have even observed the planet Neptune in 1612 and 1613, over 200 years before it was discovered, but it is unclear if he was aware of what he was looking at.
Uniting physics and astronomy.
Although the motions of celestial bodies had been qualitatively explained in physical terms since Aristotle introduced celestial movers in his Metaphysics and a fifth element in his On the Heavens, Johannes Kepler was the first to attempt to derive mathematical predictions of celestial motions from assumed physical causes. Combining his physical insights with the unprecedentedly accurate naked-eye observations made by Tycho Brahe, Kepler discovered the three laws of planetary motion that now carry his name.
Isaac Newton developed further ties between physics and astronomy through his law of universal gravitation. Realising that the same force that attracted objects to the surface of the Earth held the moon in orbit around the Earth, Newton was able to explain – in one theoretical framework – all known gravitational phenomena. In his Philosophiae Naturalis Principia Mathematica, he derived Kepler's laws from first principles. Newton's theoretical developments lay many of the foundations of modern physics.
Completing the solar system.
Outside of England, Newton's theory took some time to become established. Descartes' theory of vortices held sway in France, and Huygens, Leibniz and Cassini accepted only parts of Newton's system, preferring their own philosophies. It wasn't until Voltaire published a popular account in 1738 that the tide changed. In 1748, the French Academy of Sciences offered a reward for solving the perturbations of Jupiter and Saturn which was eventually solved by Euler and Lagrange. Laplace completed the theory of the planets towards the end of the century.
Edmund Halley succeeded Flamsteed as Astronomer Royal in England and succeeded in predicting the return in 1758 of the comet that bears his name. Sir William Herschel found the first new planet, Uranus, to be observed in modern times in 1781. The gap between the planets Mars and Jupiter disclosed by the Titius–Bode law was filled by the discovery of the asteroids Ceres and Pallas in 1801 with many more following.
At first, astronomical thought in America was based on Aristotelian philosophy, but interest in the new astronomy began to appear in Almanacs as early as 1659.
Modern astronomy.
In the 19th century it was discovered that, when decomposing the light from the Sun, a multitude of spectral lines were observed (regions where there was less or no light). Experiments with hot gases showed that the same lines could be observed in the spectra of gases, specific lines corresponding to unique elements. It was proved that the chemical elements found in the Sun (chiefly hydrogen and helium) were also found on Earth.
During the 20th century spectroscopy (the study of these lines) advanced, especially because of the advent of quantum physics, that was necessary to understand the observations.
Although in previous centuries noted astronomers were exclusively male, at the turn of the 20th century women began to play a role in the great discoveries. In this period prior to modern computers, women at the United States Naval Observatory (USNO), Harvard University, and other astronomy research institutions began to be hired as human "computers," who performed the tedious calculations while scientists performed research requiring more background knowledge. [http://maia.usno.navy.mil/women_history/history.html] A number of discoveries in this period were originally noted by the women "computers" and reported to their supervisors. For example, at the Harvard Observatory Henrietta Swan Leavitt discovered the cepheid variable star period-luminosity relation which she further developed into a method of measuring distance outside of our solar system. Annie Jump Cannon, also at Harvard, organized the stellar spectral types according to stellar temperature. In 1847, Maria Mitchell discovered a comet using a telescope. According to Lewis D. Eigen, Cannon alone, "in only 4 years discovered and catalogued more stars than all the men in history put together."
Most of these women received little or no recognition during their lives due to their lower professional standing in the field of astronomy. Although their discoveries and methods are taught in classrooms around the world, few students of astronomy can attribute the works to their authors or have any idea that there were active female astronomers at the end of the 19th century.
Cosmology and the expansion of the universe.
Most of our current knowledge was gained during the 20th century. With the help of the use of photography, fainter objects were observed. Our sun was found to be part of a galaxy made up of more than 1010 stars (10 billion stars). The existence of other galaxies, one of the matters of "the great debate", was settled by Edwin Hubble, who identified the Andromeda nebula as a different galaxy, and many others at large distances and receding, moving away from our galaxy.
Physical cosmology, a discipline that has a large intersection with astronomy, made huge advances during the 20th century, with the model of the hot big bang heavily supported by the evidence provided by astronomy and physics, such as the redshifts of very distant galaxies and radio sources, the cosmic microwave background radiation, Hubble's law and cosmological abundances of elements.
New windows into the Cosmos open.
In the 19th century, scientists began discovering forms of light which were invisible to the naked eye: X-Rays, gamma rays, radio waves, microwaves, ultraviolet radiation, and infrared radiation. This had a major impact on astronomy, spawning the fields of infrared astronomy, radio astronomy, x-ray astronomy and finally gamma-ray astronomy. With the advent of spectroscopy it was proven that other stars were similar to our own sun, but with a range of temperatures, masses and sizes. The existence of our galaxy, the Milky Way, as a separate group of stars was only proven in the 20th century, along with the existence of "external" galaxies, and soon after, the expansion of the universe seen in the recession of most galaxies from us.

</doc>
<doc id="14022" url="https://en.wikipedia.org/wiki?curid=14022" title="Haber process">
Haber process

The Haber process, also called the Haber–Bosch process, is an artificial nitrogen fixation process and is the main industrial procedure for the production of ammonia today. It is named after its inventors, the German chemists Fritz Haber and Carl Bosch, who developed it in the first half of the 20th century. The process converts atmospheric nitrogen (N2) to ammonia (NH3) by a reaction with hydrogen (H2) using a metal catalyst under high temperatures and pressures:
Before the development of the Haber process, ammonia had been difficult to produce on an industrial scale with early methods such as the Birkeland–Eyde process and Frank–Caro process all being highly inefficient.
Although the Haber process is mainly used to produce fertilizer today, during WWI it provided Germany with a source of ammonia for the production of explosives, compensating for the Allied trade blockade on Chilean saltpeter.
History.
Throughout the 19th century the demand for nitrates and ammonia for use as fertilizers and industrial feedstocks had been steadily increasing. The main source was mining niter deposits. At the beginning of the 20th century it was being predicted that these reserves could not satisfy future demand and research into new potential sources of ammonia became more important. The obvious source was atmospheric nitrogen (N2), comprising nearly 80% of the air, however N2 is exceptionally stable and will not readily react with other chemicals. Converting N2 into ammonia posed a chemical challenge for chemists globally.
Haber, with his assistant Robert Le Rossignol, developed the high-pressure devices and catalysts to demonstrate the Haber process at laboratory scale. They demonstrated their process in the summer of 1909 by producing ammonia from air, drop by drop, at the rate of about per hour. The process was purchased by the German chemical company BASF, which assigned Carl Bosch the task of scaling up Haber's tabletop machine to industrial-level production. He succeeded in 1910. Haber and Bosch were later awarded Nobel prizes, in 1918 and 1931 respectively, for their work in overcoming the chemical and engineering problems of large-scale, continuous-flow, high-pressure technology.
Ammonia was first manufactured using the Haber process on an industrial scale in 1913 in BASF's Oppau plant in Germany, reaching 20 tonnes per day the following year. During WWI, the synthetic ammonia was used for the production of nitric acid, a precursor to munitions. The Allies had access to large amounts of sodium nitrate deposits in Chile (so called "Chile saltpetre") that belonged almost totally to British industries. As Germany lacked access to such readily available natural resources, the Haber process proved essential to the German war effort.
The process.
This conversion is typically conducted at and between , as the gases are passed over four beds of catalyst, with cooling between each pass so as to maintain a reasonable equilibrium constant. On each pass only about 15% conversion occurs, but any unreacted gases are recycled, and eventually an overall conversion of 97% is achieved.
The steam reforming, shift conversion, carbon dioxide removal, and methanation steps each operate at pressures of about , and the ammonia synthesis loop operates at pressures ranging from , depending upon which proprietary process is used.
Sources of hydrogen.
The major source of hydrogen is methane from natural gas. The conversion, steam reforming, is conducted with steam in a high temperature and pressure tube inside a reformer with a nickel catalyst, separating the carbon and hydrogen molecules in the natural gas. The hydrogen and oxygen molecules in the steam are also separated, with the resultant products of hydrogen and carbon monoxide, by the reaction CH4 + H2O(g) --> 3H2 + CO. Originally Bosch obtained hydrogen by the electrolysis of water.
Reaction rate and equilibrium.
Nitrogen (N2) is very unreactive because the molecules are held together by strong triple bonds. The Haber process relies on catalysts that accelerate the scission of this triple bond.
Two opposing considerations are relevant to this synthesis: the position of the equilibrium and the rate of reaction. At room temperature, the equilibrium is strongly in favor of ammonia, but the reaction doesn't proceed at a detectable rate. The obvious solution is to raise the temperature, but because the reaction is exothermic, the equilibrium constant (using bar or atm units) becomes 1 around . (See Le Chatelier's principle.)
Above this temperature, the equilibrium quickly becomes quite unfavorable at atmospheric pressure, according to the Van 't Hoff equation. Thus one might suppose that a low temperature is to be used and some other means to increase rate. However, the catalyst itself requires a temperature of at least 400 °C to be efficient.
Pressure is the obvious choice to favor the forward reaction because there are 4 moles of reactant for every 2 moles of product (see entropy), and the pressure used () alters the equilibrium concentrations to give a profitable yield.
Economically, pressure is an expensive commodity. Pipes and reaction vessels need to be strengthened, valves more rigorous, and there are safety considerations of working at 20 MPa. In addition, running pumps and compressors takes considerable energy. Thus the compromise used gives a single pass yield of around 15%.
Another way to increase the yield of the reaction would be to remove the product (i.e. ammonia gas) from the system. In practice, gaseous ammonia is not removed from the reactor itself, since the temperature is too high; it is removed from the equilibrium mixture of gases leaving the reaction vessel. The hot gases are cooled enough, whilst maintaining a high pressure, for the ammonia to condense and be removed as liquid. Unreacted hydrogen and nitrogen gases are then returned to the reaction vessel to undergo further reaction.
Catalysts.
The most popular catalysts are based on iron promoted with K2O, CaO, SiO2, and Al2O3. The original Haber–Bosch reaction chambers used osmium as the catalyst, but it was available in extremely small quantities. Haber noted uranium was almost as effective and easier to obtain than osmium. Under Bosch's direction in 1909, the BASF researcher Alwin Mittasch discovered a much less expensive iron-based catalyst, which is still used today. Some ammonia production utilizes ruthenium-based catalysts (the KAAP process). Ruthenium forms more active catalysts that allows milder operating pressures. Such catalysts are prepared by decomposition of triruthenium dodecacarbonyl on graphite.
In industrial practice, the iron catalyst is obtained from finely ground iron powder, which is usually obtained by reduction of high purity magnetite (Fe3O4). The pulverized iron metal is burnt (oxidized) to give magnetite of a defined particle size. The magnetite particles are then partially reduced, removing some of the oxygen in the process. The resulting catalyst particles consist of a core of magnetite, encased in a shell of wüstite (FeO), which in turn is surrounded by an outer shell of iron metal. The catalyst maintains most of its bulk volume during the reduction, resulting in a highly porous high surface area material, which enhances its effectiveness as a catalyst. Other minor components of the catalyst include calcium and aluminium oxides, which support the iron catalyst and help it maintain its surface area. These oxides of Ca, Al, K, and Si are immune to reduction by the hydrogen.
The reaction mechanism, involving the heterogeneous catalyst, is believed to involve the following steps:
Reaction 5 occurs in three steps, forming NH, NH2, and then NH3. Experimental evidence points to reaction 2 as being the slow, rate-determining step. This is not unexpected since the bond broken, the nitrogen triple bond, is the strongest of the bonds that must be broken.
A major contributor to the elucidation of this mechanism is Gerhard Ertl.
Economic and environmental aspects.
When it was first invented, the Haber process needed to compete against another industrial process, the Cyanamide process. However, the Cyanamide process consumed large amounts of electrical power and was more labor-intensive than the Haber process.
The Haber process now produces of nitrogen fertilizer per year, mostly in the form of anhydrous ammonia, ammonium nitrate, and urea. 3–5% of the world's natural gas production is consumed in the Haber process (~1–2% of the world's annual energy supply). In combination with pesticides, these fertilizers have quadrupled the productivity of agricultural land:
Due to its dramatic impact on the human ability to grow food, the Haber process served as the "detonator of the population explosion", enabling the global population to increase from 1.6 billion in 1900 to today's 7 billion. Nearly 80% of the nitrogen found in human tissues originated from the Haber-Bosch process. Since nitrogen use efficiency is typically less than 50%, our heavy use of industrial nitrogen fixation is disruptive to our biological habitat.

</doc>
<doc id="14023" url="https://en.wikipedia.org/wiki?curid=14023" title="Hot or Not">
Hot or Not

Hot or Not is a rating site that allows users to rate the attractiveness of photos submitted voluntarily by others. The site offers a matchmaking engine called 'Meet Me' and an extended profile feature called "Hotlists". The domain hotornot.com is currently owned by Hot Or Not Limited, and was previously owned by Avid Life Media. 'Hot or Not' was a significant influence on the people who went on to create the social media sites Facebook and YouTube.
Description.
Users would submit photographs of themselves to the site for the purpose of other users to rate said person's attractiveness on a scale of 1 - 10, with the cumulative average acting as the overall score for a given photograph.
History.
The site was founded in October 2000 by James Hong and Jim Young, two friends and Silicon Valley-based engineers. Both graduated from the University of California, Berkeley in electrical engineering, with Young pursuing a Ph.D at the time. It was inspired on some other developer's ideas. 
The site was a technical solution to a disagreement they made one day over a passing woman's attractiveness. The site was originally called "Am I Hot or Not". Within a week of launching, it had reached almost two million page views per day. Within a few months, the site was immediately behind CNET and NBCi on NetNielsen Rating's Top 25 advertising domains. To keep up with rising costs Hong and Young added a matchmaking component to their website called "Meet Me at Hot or Not", i.e. a system of range voting. The matchmaking service has been especially successful and the site continues to generate most of its revenue through subscriptions. In the December 2006 issue of "Time" magazine, the founders of YouTube stated that they originally set out to make a version of Hot or Not with Video before developing their more inclusive site. Mark Zuckerberg of Facebook similarly got his start by creating a Hot or Not type site called FaceMash, where he posted photos from Harvard's Facebook for the university's community to rate.
Hot or Not was sold for a rumored $20 million on February 8, 2008 to Avid Life Media, owners of Ashley Madison. Annual revenue reached $7.5 million, with net profits of $5.5 million. They initially started off $60,000 in debt due to tuition fees James paid for his MBA. On July 31, 2008, Hot or Not launched Hot or Not Gossip and a Baresi rate box (a "hot meter") – a subdivision to expand their market, run by former radio DJ turned celebrity blogger Zack Taylor.
Predecessors and spin-offs.
Hot or Not was preceded by the rating sites RateMyFace, which was registered a year earlier in the summer of 1999, and AmIHot.com, which was registered in January 2000 by MIT freshman Daniel Roy. Regardless, despite any head starts of its predecessors, Hot or Not quickly became the most popular. Since AmIHotOrNot.com's launch, the concept has spawned many imitators. The concept always remained the same, but the subject matter varied greatly. The concept has also been integrated with a wide variety of dating and matchmaking systems. In 2007 BecauseImHot.com launched and deleted anyone with a rating below 7 after a voting audit or the first 50 votes (whichever is first).
Variations on the Hot or Not concept include voting via a Condorcet method where a candidate is compared with other candidates in a series of pairwise comparisons in order to gauge their popularity. Another variation used a four-way comparison of candidates to gauge their popularity and show a 'type' match for candidates who most closely match the average preferences shown by the user making the choices.
Research.
In 2005, as an example of using image morphing methods to study the effects of averageness, imaging researcher Pierre Tourigny created a composite of about 30 faces to find out the current standard of good looks on the Internet (as shown above). On the Hot or Not web site, people rate others' attractiveness on a scale of 1 to 10. An average score based on hundreds or even thousands of individual ratings takes only a few days to emerge. To make this hot or not palette of morphed images, photos from the site were sorted by rank and used SquirlzMorph to create multi-morph composites from them. Unlike projects like Face of Tomorrow, where the subjects are posed for the purpose, the portraits are blurry because the source images are low resolution with differences in posture, hair styles, glasses, etc., so that here images could use only 36 control points for the morphs. A similar study was done with Miss Universe contestants, as shown in the averageness article, as well as one for age, as shown in youthfulness article.
A 2006 "hot" or "not" style study, involving 264 women and 18 men, at the Washington University School of Medicine, as published online in the journal "Brain Research", indicates that a person's brain determines whether an image is erotic long before the viewer is even aware they are seeing the picture. Moreover, according to these researchers, one of the basic functions of the brain is to classify images into a hot or not type categorization. The study's researchers also discovered that sexy shots induce a uniquely powerful reaction in the brain, equal in effect for both men and women, and that erotic images produced a strong reaction in the hypothalamus.

</doc>
<doc id="14024" url="https://en.wikipedia.org/wiki?curid=14024" title="H.263">
H.263

H.263 is a video compression standard originally designed as a low-bit-rate compressed format for videoconferencing. It was developed by the ITU-T Video Coding Experts Group (VCEG) in a project ending in 1995/1996 as one member of the H.26x family of video coding standards in the domain of the ITU-T, and it was later extended to add various additional enhanced features in 1998 and 2000. Smaller additions were also made in 1997 and 2001, and a unified edition was produced in 2005.
The H.263 standard was first designed to be utilized in H.324 based systems (PSTN and other circuit-switched network videoconferencing and videotelephony), but it also found use in H.323 (RTP/IP-based videoconferencing), H.320 (ISDN-based videoconferencing), RTSP (streaming media) and SIP (IP-based videoconferencing) solutions.
H.263 is a required video coding format in ETSI 3GPP technical specifications for IP Multimedia Subsystem (IMS), Multimedia Messaging Service (MMS) and Transparent end-to-end Packet-switched Streaming Service (PSS). In 3GPP specifications, H.263 video is usually used in 3GP container format.
H.263 also found many applications on the internet: much Flash Video content (as used on sites such as YouTube, Google Video, MySpace, etc.) used to be encoded in Sorenson Spark format (an incomplete implementation of H.263). The original version of the RealVideo codec was based on H.263 up until the release of RealVideo 8.
H.263 was developed as an evolutionary improvement based on experience from H.261, the previous ITU-T standard for video compression, and the MPEG-1 and MPEG-2 standards. Its first version was completed in 1995 and provided a suitable replacement for H.261 at all bit rates. It was further enhanced in projects known as H.263v2 (also known as H.263+ or H.263 1998), MPEG-4 Part 2 and H.263v3 (also known as H.263++ or H.263 2000). MPEG-4 Part 2 is H.263 compatible in the sense that basic "baseline" H.263 bitstreams are correctly decoded by an MPEG-4 Video decoder.
The next enhanced format developed by ITU-T VCEG (in partnership with MPEG) after H.263 was the H.264 standard, also known as AVC and MPEG-4 part 10. As H.264 provides a significant improvement in capability beyond H.263, the H.263 standard is now considered a legacy design. Most new videoconferencing products now include H.264 as well as H.263 and H.261 capabilities. An even-newer standard format, HEVC, has also been developed by VCEG and MPEG, and has begun to emerge in some applications.
Versions.
Since the original ratification of H.263 in March 1996 (approving a document that was produced in November 1995), there have been two subsequent additions which improved on the original standard by additional optional extensions (for example, the H.263v2 project added a deblocking filter in its Annex J).
Version 1 and Annex I.
The original version of H.263 specified the following annexes:
The first version of H.263 supported a limited set of picture sizes:
In March 1997, an informative Appendix I describing Error Tracking – an encoding technique for providing improved robustness to data losses and errors, was approved to provide information for the aid of implementers having an interest in such techniques.
H.263v2 (H.263+).
H.263v2 (also known as "H.263+", or as "the 1998 version of H.263") is the informal name of the second edition of the ITU-T H.263 international video coding standard. It retained the entire technical content of the original version of the standard, but enhanced H.263 capabilities by adding several annexes which can substantially improve encoding efficiency and provide other capabilities (such as enhanced robustness against data loss in the transmission channel). The H.263+ project was ratified by the ITU in February 1998. It added the following Annexes:
H.263v2 also added support for flexible customized picture formats and custom picture clock frequencies. As noted above, the only picture formats previously supported in H.263 had been Sub-QCIF, QCIF, CIF, 4CIF, and 16CIF, and the only picture clock frequency had been 30000/1001 (approximately 29.97) clock ticks per second.
H.263v2 specified a set of recommended modes in an informative appendix (Appendix II, since deprecated):
H.263v3 (H.263++) and Annex X.
The definition of H.263v3 (also known as H.263++ or as the 2000 version of H.263) added three annexes. These annexes and an additional annex that specified profiles (approved the following year) were originally published as separate documents from the main body of the standard itself. The additional annexes specified are:
The prior informative Appendix II (recommended optional enhancement) was obsoleted by the creation of the normative Annex X.
In June 2001, another informative appendix (Appendix III, Examples for H.263 encoder/decoder implementations) was approved. It describes techniques for encoding and for error/loss concealment by decoders.
In January 2005, a unified H.263 specification document was produced (with the exception of Appendix III, which remains as a separately-published document).
In August 2005, an implementors guide was approved to correct a small error in the seldom-used Annex Q reduced-resolution update mode.
Open-source implementation.
In countries without software patents, H.263 video can be legally encoded and decoded with the free LGPL-licensed libavcodec library (part of the FFmpeg project) which is used by programs such as ffdshow, VLC media player and MPlayer.

</doc>
<doc id="14026" url="https://en.wikipedia.org/wiki?curid=14026" title="House of Orange (disambiguation)">
House of Orange (disambiguation)

House of Orange may refer to:

</doc>
<doc id="14029" url="https://en.wikipedia.org/wiki?curid=14029" title="Histone">
Histone

In biology, histones are highly alkaline proteins found in eukaryotic cell nuclei that package and order the DNA into structural units called nucleosomes. They are the chief protein components of chromatin, acting as spools around which DNA winds, and playing a role in gene regulation. Without histones, the unwound DNA in chromosomes would be very long (a length to width ratio of more than 10 million to 1 in human DNA). For example, each human cell has about 1.8 meters of DNA, (~6 ft) but wound on the histones it has about 90 micrometers (0.09 mm) of chromatin, which, when duplicated and condensed during mitosis, result in about 120 micrometers of chromosomes.
Classes.
Five major families of histones exist: H1/H5, H2A, H2B, H3 and H4. Histones H2A, H2B, H3 and H4 are known as the core histones, while histones H1 and H5 are known as the linker histones.
The core histones all exist as dimers, which are similar in that they all possess the histone fold domain; three alpha helices linked by two loops. It is this helical structure that allows for interaction between distinct dimers, particularly in a head-tail fashion (also called the handshake motif). The resulting four distinct dimers then come together to form one octameric nucleosome core, approximately 63 Angstroms in diameter (a solenoid (DNA)-like particle). 147 base pairs of DNA wrap around this core particle 1.65 times in a left-handed super-helical turn to give a particle of around 100 Angstroms across. The linker histone H1 binds the nucleosome at the entry and exit sites of the DNA, thus locking the DNA into place and allowing the formation of higher order structure. The most basic such formation is the 10 nm fiber or beads on a string conformation. This involves the wrapping of DNA around nucleosomes with approximately 50 base pairs of DNA separating each pair of nucleosomes (also referred to as linker DNA). Higher-order structures include the 30 nm fiber (forming an irregular zigzag) and 100 nm fiber, these being the structures found in normal cells. During mitosis and meiosis, the condensed chromosomes are assembled through interactions between nucleosomes and other regulatory proteins.
The following is a list of human histone proteins:
Structure.
The nucleosome core is formed of two H2A-H2B dimers and a H3-H4 tetramer, forming two nearly symmetrical halves by tertiary structure (C2 symmetry; one macromolecule is the mirror image of the other). The H2A-H2B dimers and H3-H4 tetramer also show pseudodyad symmetry. The 4 'core' histones (H2A, H2B, H3 and H4) are relatively similar in structure and are highly conserved through evolution, all featuring a 'helix turn helix turn helix' motif (which allows the easy dimerisation). They also share the feature of long 'tails' on one end of the amino acid structure - this being the location of post-translational modification (see below).
It has been proposed that histone proteins are evolutionarily related to the helical part of the extended AAA+ ATPase domain, the C-domain, and to the N-terminal substrate recognition domain of Clp/Hsp100 proteins. Despite the differences in their topology, these three folds share a homologous helix-strand-helix (HSH) motif.
Using an electron paramagnetic resonance spin-labeling technique, British researchers measured the distances between the spools around which eukaryotic cells wind their DNA. They determined the spacings range from 59 to 70 Å.
In all, histones make five types of interactions with DNA:
The highly basic nature of histones, aside from facilitating DNA-histone interactions, contributes to their water solubility.
Histones are subject to post translational modification by enzymes primarily on their N-terminal tails, but also in their globular domains. Such modifications include methylation, citrullination, acetylation, phosphorylation, SUMOylation, ubiquitination, and ADP-ribosylation. This affects their function of gene regulation.
In general, genes that are active have less bound histone, while inactive genes are highly associated with histones during interphase. It also appears that the structure of histones has been evolutionarily conserved, as any deleterious mutations would be severely maladaptive. All histones have a highly positively charged N-terminus with many lysine and arginine residues.
History.
Histones were discovered in 1884 by Albrecht Kossel. The word "histone" dates from the late 19th century and is from the German word ""Histon"", a word itself of uncertain origin - perhaps from the Greek "histanai" or "histos". Until the early 1990s, histones were dismissed by most as inert packing material for eukaryotic nuclear DNA, a view based in part on the models of Mark Ptashne and others, who believed that transcription was activated by protein-DNA and protein-protein interactions on largely naked DNA templates, as is the case in bacteria.
During the 1980s, Yahli Lorch and Roger Kornberg showed that a nucleosome on a core promoter prevents the initiation of transcription in vitro, and Michael Grunstein demonstrated that histones repress transcription in vivo, leading to the idea of the nucleosome as a general gene repressor. Relief from repression is believed to involve both histone modification and the action of chromatin-remodeling complexes. Vincent Allfrey and Alfred Mirsky earlier proposed a role of histone modification in transcriptional activation, regarded as a molecular manifestation of epigenetics. Michael Grunstein and David Allis found support for this proposal, in the importance of histone acetylation for transcription in yeast and the activity of the transcriptional activator Gcn5 as a histone acetyltransferase.
The discovery of the H5 histone appears to date back to the 1970s, and it is now considered an isoform of Histone H1.
Conservation across species.
Histones are found in the nuclei of eukaryotic cells, and in certain Archaea, namely Thermoproteales and Euryarchaea, but not in bacteria. The unicellular algae known as dinoflagellates are the only eukaryotes that are known to completely lack histones.
Archaeal histones may well resemble the evolutionary precursors to eukaryotic histones. Histone proteins are among the most highly conserved proteins in eukaryotes, emphasizing their important role in the biology of the nucleus. In contrast mature sperm cells largely use protamines to package their genomic DNA, most likely because this allows them to achieve an even higher packaging ratio.
Core histones are highly conserved proteins; that is, there are very few differences among the amino acid sequences of the histone proteins of different species.
There are some "variant" forms in some of the major classes. They share amino acid sequence homology and core structural similarity to a specific class of major histones but also have their own feature that is distinct from the major histones. These "minor histones" usually carry out specific functions of the chromatin metabolism. For example, histone H3-like CenpA is associated with only the centromere region of the chromosome. Histone H2A variant H2A.Z is associated with the promoters of actively transcribed genes and also involved in the prevention of the spread of silent heterochromatin. Furthermore, H2A.Z has roles in chromatin for genome stability. Another H2A variant H2A.X is phosphorylated at S139 in regions around double-strand breaks and marks the region undergoing DNA repair. Histone H3.3 is associated with the body of actively transcribed genes.
Function.
Compacting DNA strands.
Histones act as spools around which DNA winds. This enables the compaction necessary to fit the large genomes of eukaryotes inside cell nuclei: the compacted molecule is 40,000 times shorter than an unpacked molecule.
Chromatin regulation.
Histones undergo posttranslational modifications that alter their interaction with DNA and nuclear proteins. The H3 and H4 histones have long tails protruding from the nucleosome, which can be covalently modified at several places. Modifications of the tail include methylation, acetylation, phosphorylation, ubiquitination, SUMOylation, citrullination, and ADP-ribosylation. The core of the histones H2A, H2B, and H3 can also be modified. Combinations of modifications are thought to constitute a code, the so-called "histone code". Histone modifications act in diverse biological processes such as gene regulation, DNA repair, chromosome condensation (mitosis) and spermatogenesis (meiosis).
The common nomenclature of histone modifications is:
So H3K4me1 denotes the monomethylation of the 4th residue (a lysine) from the start (i.e., the N-terminal) of the H3 protein.
Examples of histone modifications in transcription regulation include:
Functions of histone modifications.
A huge catalogue of histone modifications have been described, but a functional understanding of most is still lacking. Collectively, it is thought that histone modifications may underlie a histone code, whereby combinations of histone modifications have specific meanings. However, most functional data concerns individual prominent histone modifications that are biochemically amenable to detailed study.
Chemistry of histone modifications.
Lysine methylation.
The addition of one, two or three methyl groups to lysine has little effect on the chemistry of the histone; methylation leaves the charge of the lysine intact and adds a minimal number of atoms so steric interactions are mostly unaffected. However, proteins containing Tudor, chromo or PHD domains, amongst others, can recognise lysine methylation with exquisite sensitivity and differentiate mono, di and tri-methyl lysine, to the extent that, for some lysines (e.g.: H4K20) mono, di and tri-methylation appear to have different meanings. Because of this, lysine methylation tends to be a very informative mark and dominates the known histone modification functions.
Arginine methylation.
What was said above of the chemistry of lysine methylation also applies to arginine methylation, and some protein domains—e.g., Tudor domains—can be specific for methyl arginine instead of methyl lysine. Arginine is known to be mono- or di-methylated, and methylation can be symmetric or asymmetric, potentially with different meanings.
Arginine citrullination.
Enzymes called peptidylarginine deiminases (PADs) hydrolyze the imine group of arginines and attach a keto group, so that there is one less positive charge on the amino acid residue. This process has been involved in the activation of gene expression by making the modified histones less tightly bound to DNA and thus making the chromatin more accessible. PADs can also produce the opposite effect by removing or inhibiting mono-methylation of arginine residues on histones and thus antagonizing the positive effect arginine methylation has on transcriptional activity.
Lysine acetylation.
Addition of an acetyl group has a major chemical effect on lysine as it neutralises the positive charge. This reduces electrostatic attraction between the histone and the negatively charged DNA backbone, loosening the chromatin structure; highly acetylated histones form more accessible chromatin and tend to be associated with active transcription. Lysine acetylation appears to be less precise in meaning than methylation, in that histone acetyltransferases tend to act on more than one lysine; presumably this reflects the need to alter multiple lysines to have a significant effect on chromatin structure.
Serine/threonine/tyrosine phosphorylation.
Addition of a negatively charged phosphate group can lead to major changes in protein structure, leading to the well-characterised role of phosphorylation in controlling protein function. It is not clear what structural implications histone phosphorylation has, but histone phosphorylation has clear functions as a post-translational modification, and binding domains such as BRCT have been characterised.
Functions in transcription.
Most well-studied histone modifications are involved in control of transcription.
Actively transcribed genes.
Two histone modifications are particularly associated with active transcription:
Repressed genes.
Three histone modifications are particularly associated with repressed genes:
Bivalent promoters.
Analysis of histone modifications in embryonic stem cells (and other stem cells) revealed many gene promoters carrying both H3K4Me3 and H3K27Me3, in other words these promoters display both activating and repressing marks simultaneously. This peculiar combination of modifications marks genes that are poised for transcription; they are not required in stem cells, but are rapidly required after differentiation into some lineages. Once the cell starts to differentiate, these bivalent promoters are resolved to either active or repressive states depending on the chosen lineage.
Other functions.
DNA damage.
Marking sites of DNA damage is an important function for histone modifications. It also protects DNA from getting destroyed by ultraviolet radiation of sun.

</doc>
<doc id="14031" url="https://en.wikipedia.org/wiki?curid=14031" title="Hierarchical organization">
Hierarchical organization

A hierarchical organization is an organizational structure where every entity in the organization, except one, is subordinate to a single other entity. This arrangement is a form of a hierarchy. In an organization, the hierarchy usually consists of a singular/group of power at the top with subsequent levels of power beneath them. This is the dominant mode of organization among large organizations; most corporations, governments, and organized religions are hierarchical organizations with different levels of management, power or authority. For example, the broad, top-level overview of the general organization of the Catholic Church consists of the Pope, then the Cardinals, then the Archbishops, and so on. 
Members of hierarchical organizational structures chiefly communicate with their immediate superior and with their immediate subordinates. Structuring organizations in this way is useful partly because it can reduce the communication overhead by limiting information flow; this is also its major limitation.
Visualization.
A hierarchy is typically visualized as a pyramid, where the height of the ranking or person depicts their power status and the width of that level represents how many people or business divisions are at that level relative to the whole—the highest-ranking people are at the apex, and there are very few of them; the base may include thousands of people who have no subordinates. These hierarchies are typically depicted with a tree or triangle diagram, creating an organizational chart or organigram. Those nearest the top have more power than those nearest the bottom, and there being fewer people at the top than at the bottom. As a result, superiors in a hierarchy generally have higher status and command greater rewards than their subordinates.
Common models.
All governments and most companies have similar structures. Traditionally, the monarch was the pinnacle of the state. In many countries, feudalism and manorialism provided a formal social structure that established hierarchical links at every level of society, with the monarch at the top. 
In modern post-feudal states the nominal top of the hierarchy still remains the head of state, which may be a president or a constitutional monarch, although in many modern states the powers of the head of state are delegated among different bodies. Below the head, there is commonly a senate, parliament or congress, which in turn often delegate the day-to-day running of the country to a prime minister. In many democracies, the people are considered to be the notional top of the hierarchy, over the head of state; in reality, the people's power is restricted to voting in elections.
In business, the business owner traditionally occupied the pinnacle of the organization. In most modern large companies, there is now no longer a single dominant shareholder, and the collective power of the business owners is for most purposes delegated to a board of directors, which in turn delegates the day-to-day running of the company to a managing director or CEO. Again, although the shareholders of the company are the nominal top of the hierarchy, in reality many companies are run at least in part as personal fiefdoms by their management; corporate governance rules are an attempt to mitigate this tendency.
Studies.
The organizational development theorist Elliott Jacques identified a special role for hierarchy in his concept of requisite organization. 
The iron law of oligarchy, introduced by Robert Michels, describes the inevitable tendency of hierarchical organizations to become oligarchic in their decision making.
The Peter Principle is a term coined by Laurence J. Peter in which the selection of a candidate for a position in an hierarchical organization is based on the candidate's performance in their current role, rather than on abilities relevant to the intended role. Thus, employees only stop being promoted once they can no longer perform effectively, and managers in an hierarchical organization "rise to the level of their incompetence." 
Hierarchiology is another term coined by Laurence J. Peter, described in his humorous book of the same name, to refer to the study of hierarchical organizations and the behavior of their members.
The IRG Solution – hierarchical incompetence and how to overcome it argued that hierarchies were inherently incompetent, and were only able to function due to large amounts of informal lateral communication fostered by private informal networks.
Criticism and alternatives.
In the work of diverse theorists such as William James (1842–1910), Michel Foucault (1926–1984) and Hayden White, important critiques of hierarchical epistemology are advanced. James famously asserts in his work "Radical Empiricism" that clear distinctions of type and category are a constant but unwritten goal of scientific reasoning, so that when they are discovered, success is declared. But if aspects of the world are organized differently, involving inherent and intractable ambiguities, then scientific questions are often considered unresolved. A hesitation to declare success upon the discovery of ambiguities leaves heterarchy at an artificial and subjective disadvantage in the scope of human knowledge. This bias is an artifact of an aesthetic or pedagogical preference for hierarchy, and not necessarily an expression of objective observation.
Hierarchies and hierarchical thinking has been criticized by many people, including Susan McClary and one political philosophy which is vehemently opposed to hierarchical organization: anarchism is generally opposed to hierarchical organization in any form of human relations. Heterarchy is the most commonly proposed alternative to hierarchy and this has been combined with responsible autonomy by Gerard Fairtlough in his work on Triarchy theory.
Amidst constant innovation in information and communication technologies, hierarchical authority structures are giving way to greater decision-making latitude for individuals and more flexible definitions of job activities and this new style of work presents a challenge to existing organizational forms, with some research studies contrasting traditional organizational forms against groups that operate as online communities that are characterized by personal motivation and the satisfaction of making one's own decisions. With all levels of an organization having access to information and communication via digital means, power structures align more as a wirearchy, enabling the flow of power and authority to be based not on hierarchical levels, but on information, trust, credibility, and a focus on results.

</doc>
<doc id="14033" url="https://en.wikipedia.org/wiki?curid=14033" title="Harry Secombe">
Harry Secombe

Sir Harry Donald Secombe, CBE (8 September 1921 – 11 April 2001) was a Welsh comedian and singer. He played Neddie Seagoon, a central character in the BBC radio comedy series "The Goon Show" (1951–60). He also appeared in musicals and films and, in his later years, was a presenter of television shows incorporating hymns and other devotional songs.
Early life.
Secombe was born in St Thomas, Swansea, the third of four children of Nellie Jane Gladys (née Davies), a shop manageress, and Frederick Ernest Secombe, a grocer. From the age of 11 he attended Dynevor School, a state secondary school in central Swansea.
His family were regular churchgoers, belonging to the congregation of St Thomas Church. A member of the choir, Secombe would – from the age of 12 – perform a sketch entitled "The Welsh Courtship" at church socials, acting as "feed" to his sister Carol. His elder brother, Fred Secombe, was the author of several books about his experiences as an Anglican priest and rector.
British Army.
After leaving school in 1937, Secombe became a pay clerk at Baldwin's store. With war looming, he decided in 1938 that he would join the Territorial Army (United Kingdom). Very short sighted, he got a friend to tell him the sight test, and then learnt it by heart. He served as a Lance Bombardier in No.132 Field Regiment of the Royal Artillery. He would refer to the unit in which he served during World War II in the North African Campaign, Sicily, and Italy, as "The Five-Mile Snipers". While in North Africa Secombe met Spike Milligan for the first time. In Sicily he joined a concert party and developed his own comedy routines to entertain the troops.
When Secombe visited the Falkland Islands to entertain the troops after the 1982 Falklands War, his old regiment promoted him to the rank of sergeant – 37 years after he had been demobbed.
As an entertainer.
He made his first radio broadcast in May 1944 on a variety show aimed at the services. Following the end of fighting in the war but prior to demobilisation Secombe joined a pool of entertainers in Naples and formed a comedy duo with Spike Milligan.
Secombe joined the cast of the Windmill Theatre in 1946, using a routine he had developed in Italy about how people shaved. Secombe always claimed that his ability to sing could always be counted on to save him when he bombed. Both Milligan and Sellers credited him with keeping the act on the bill when club owners had wanted to sack them.
After a regional touring career, his first break came in radio when he was chosen as resident comedian for the Welsh series "Welsh Rarebit," followed by appearances on "Variety Bandbox" and a regular role in "Educating Archie".
Secombe met Michael Bentine at the Windmill Theatre, and was introduced to Peter Sellers by his agent Jimmy Grafton. Together with Spike Milligan, the four wrote a comedy radio script, and "Those Crazy People" was commissioned and first broadcast on 28 May 1951. Produced by Peter Ross, this would soon become "The Goon Show" and the show remained on the air until 1960. Secombe mainly played Neddie Seagoon, around whom the show's absurd plots developed.
With the success of "The Goon Show", Secombe developed a dual career as both a comedy actor and a singer. At the beginning of his career as an entertainer, his act would end with a joke version of the duet "Sweethearts," in which he sang both the baritone and falsetto parts. Trained under Italian maestro Manlio di Veroli, he emerged as a "bel canto" tenor (characteristically, he insisted that in his case this meant "can belto") and had a long list of best-selling record albums to his credit.
In 1958 he appeared in the film "Jet Storm," which starred Dame Sybil Thorndike and Richard Attenborough and in the same year Secombe starred in the title role in "Davy", one of Ealing Studios' last films.
The power of his voice allowed Secombe to appear in many stage musicals. This included 1963's "Pickwick," based on Charles Dickens' "The Pickwick Papers", which gave him the number 18 hit single "If I Ruled the World" – his later signature tune. In 1965 the show was produced on tour in the United States, where on Broadway he garnered a nomination for a Tony Award for Best Actor in a Musical. He also appeared in the musical "The Four Musketeers" (1967), as Mr. Bumble in Carol Reed's film of "Oliver!" (1968), and in the Envy segment of "The Magnificent Seven Deadly Sins" (1971).
He would go on to star in his own television show, "The Harry Secombe Show", which debuted on Christmas Day 1968 on BBC 1 and ran for thirty one episodes until 1973. A sketch comedy show featuring Julian Orchard as Secombe's regular sidekick, the series also featured guest appearances by fellow Goon Spike Milligan as well as leading performers such as Ronnie Barker and Arthur Lowe. Secombe later starred in similar vehicles such as "Sing a Song of Secombe" and ITV's "Secombe with Music" during the 1970s.
Later career.
Later in life, Secombe (whose brother Fred Secombe was a priest in the Church in Wales, part of the Anglican Communion) attracted new audiences as a presenter of religious programmes, such as the BBC's "Songs of Praise" and ITV's "Stars on Sunday" and "Highway". He was also a special programming consultant to Harlech Television. and hosted a Thames Television programme in 1979 entitled "Cross on the Donkey's Back". In the latter half of the 1980s, Secombe personally sponsored a football team for boys aged 9–11 in the local West Sutton Little League, 'Secombes Knights'.
In 1990, he was one of a few to be honoured by a second appearance on "This Is Your Life", when he was surprised by Michael Aspel at a book signing in a London branch of WH Smith. Secombe had been a subject of the show previously in March 1958 when Eamonn Andrews surprised him at the BBC Television Theatre.
Honours.
In 1963 he was appointed a Commander of the Order of the British Empire (CBE).
He was knighted in 1981, and jokingly referred to himself as Sir Cumference (in recognition of his rotund figure). The motto he chose for his coat of arms was "GO ON", a reference to goon.
Later life and death.
Secombe suffered from peritonitis in 1980. He had a stroke in 1997, from which he made a slow recovery. He was then diagnosed with prostate cancer in September 1998. After suffering a second stroke in 1999, he was forced to abandon his television career, but made a documentary about his condition in the hope of giving encouragement to other sufferers. Secombe had diabetes in the latter part of his life.
Secombe died on 11 April 2001 at the age of 79, from prostate cancer, in hospital in Guildford, Surrey. His ashes are interred at the parish church of Shamley Green, and a later memorial service to celebrate his life was held at Westminster Abbey on 26 October 2001. As well as family members and friends, the service was also attended by Charles, Prince of Wales and representatives of Prince Philip, Duke of Edinburgh, Anne, Princess Royal, Princess Margaret, Countess of Snowdon and Prince Edward, Duke of Kent. On his tombstone is the inscription: "To know him was to love him."
Upon hearing of his old friend's death, Spike Milligan quipped, "I'm glad he died before me, because I didn't want him to sing at my funeral." But Secombe would have the last laugh: upon Milligan's own death the following year, a recording of Secombe singing was played at Spike's memorial service.
The Secombe Theatre at Sutton, London, bears his name in memory of this former local personality. He is also fondly remembered at the London Welsh Centre, where he opened the bar on St Patrick's Day (17 March) 1971.
Family.
Secombe met Myra Atherton at the Mumbles dance hall. The couple were married from 1948 until his death, and had four children:

</doc>
<doc id="14034" url="https://en.wikipedia.org/wiki?curid=14034" title="Heroin">
Heroin

Heroin is an opioid pain killer. It is also used less commonly as a cough suppressant and as an antidiarrhoeal. Heroin is used as a recreational drug for its euphoric effects. Frequent and regular administration is associated with tolerance and physical dependence. In some countries it is available for prescription to long-term users as a form of opioid replacement therapy alongside counseling.
Administered intravenously by injection, heroin is two to four times more potent than morphine and is faster in its onset of action. Illicit heroin is sometimes available in a matte-white powder freebase form. Because of its lower boiling point, the freebase form of heroin is smokable. It is the 3,6-diacetyl ester of morphine.
Heroin was first made by C. R. Alder Wright in 1874 by adding two acetyl groups to the molecule morphine, a natural product of the opium poppy. Internationally, heroin is controlled under Schedules I and IV of the Single Convention on Narcotic Drugs. It is generally illegal to manufacture, possess, or sell heroin without a license. In 2004 Afghanistan produced roughly 87% of the world supply in illicit raw opium. However, the production rate in Mexico rose sixfold from 2007 to 2011, making Mexico the second-largest opium producer in the world.
Uses.
Medical uses.
Under the generic name diamorphine, heroin is prescribed as a strong pain medication in the United Kingdom, where it is given via subcutaneous, intramuscular, intrathecal or intravenous route. Its use includes treatment for acute pain, such as in severe physical trauma, myocardial infarction, post-surgical pain, and chronic pain, including end-stage cancer and other terminal illnesses. In other countries it is more common to use morphine or other strong opioids in these situations. In 2004 the National Institute for Health and Clinical Excellence, a non-departmental public body of the Department of Health in the United Kingdom, produced guidance on the management of caesarian section, which recommended the use of intrathecal or epidural diamorphine for post-operative pain relief.
In 2005 there was a shortage of diamorphine in the UK, because of a problem at the main UK manufacturers. Because of this, many hospitals changed to using morphine instead of diamorphine. Although there is no longer a problem with the manufacturing of diamorphine in the UK, some hospitals there have continued to use morphine. The majority, however, continue to use diamorphine, and diamorphine tablets are supplied for pain management.
Diamorphine continues to be widely used in palliative care in the UK, where it is commonly given by the subcutaneous route, often via a syringe driver, if patients cannot easily swallow oral morphine solution. The advantage of diamorphine over morphine is that diamorphine is more fat soluble and therefore more potent by injection, so smaller doses of it are needed for the same analgesic effect. Both of these factors are advantageous if giving high doses of opioids via the subcutaneous route, which is often necessary in palliative care.
The medical use of diamorphine, in common with other strong opioids such as morphine, fentanyl and oxycodone, is controlled in the UK by the Misuse of Drugs Act 1971. In the UK it is a class A controlled drug and as such is subject to guidelines surrounding its storage, administration and destruction. Possession of diamorphine without a prescription is an arrestable offence. When diamorphine is prescribed in a hospital or similar environment, its administration must be supervised by two people who must then complete and sign a controlled drugs register (CD register) detailing the patient's name, amount, time, date and route of administration. In the case of a physician administering diamorphine, then he/she may administer the drug alone, however the rule requiring two registered practitioners, such as a nurse, midwife or another physician to sign the CD register still applies. The use of a witness when administering diamorphine is to avoid the possibility of the drug being diverted onto the black market.
For safety reasons, many UK National Health Service hospitals now only permit the administration of intravenous diamorphine in designated areas. In practice this usually means a critical care unit, an accident and emergency department, operating theatres by an anaesthetist or nurse anaesthetist or other such areas where close monitoring and support from senior staff is immediately available. However, administration by other routes is permitted in other areas of the hospital. This includes subcutaneous, intramuscular, intravenously as part of a patient controlled analgesia setup, and as an already established epidural infusion pump. Subcutaneous infusion, along with subcutaneous and intramuscular injection (bolus administration), is often used in the patient's own home, in order to treat severe pain in terminal illness.
Recreational.
Diamorphine, almost always still called by its original trade name of heroin in non-medical settings, is used as a recreational drug for the intense euphoria it induces. Anthropologist Michael Agar once described heroin as "the perfect whatever drug." Tolerance develops quickly, and increased doses are needed in order to achieve the same effects. Its popularity with recreational drug users, compared to morphine, reportedly stems from its perceived different effects. In particular, users report an intense rush, an acute transcendent state of euphoria, which occurs while diamorphine is being metabolized into 6-monoacetylmorphine (6-MAM) and morphine in the brain. Some believe that heroin produces more euphoria than other opioids; one possible explanation is the presence of 6-monoacetylmorphine, a metabolite unique to heroin – although a more likely explanation is the rapidity of onset. While other opioids of recreational use produce only morphine, heroin also leaves 6-MAM, also a psycho-active metabolite. However, this perception is not supported by the results of clinical studies comparing the physiological and subjective effects of injected heroin and morphine in individuals formerly addicted to opioids; these subjects showed no preference for one drug over the other. Equipotent injected doses had comparable action courses, with no difference in subjects' self-rated feelings of euphoria, ambition, nervousness, relaxation, drowsiness, or sleepiness.
Short-term addiction studies by the same researchers demonstrated that tolerance developed at a similar rate to both heroin and morphine. When compared to the opioids hydromorphone, fentanyl, oxycodone, and pethidine (meperidine), former addicts showed a strong preference for heroin and morphine, suggesting that heroin and morphine are particularly susceptible to abuse and addiction. Morphine and heroin were also much more likely to produce euphoria and other positive subjective effects when compared to these other opioids.
Some researchers have attempted to explain heroin use and the culture that surrounds it through the use of sociological theories. In "Righteous Dopefiend", Philippe Bourgois and Jeff Schonberg use anomie theory to explain why people begin using heroin. By analyzing a community in San Francisco, they demonstrated that heroin use was caused in part by internal and external factors such as violent homes and parental neglect. This lack of emotional, social, and financial support causes strain and influences individuals to engage in deviant acts, including heroin usage. They further found that heroin users practiced "retreatism", a behavior first described by Howard Abadinsky, in which those suffering from such strain reject society's goals and institutionalized means of achieving them.
Maintenance prescription.
Diamorphine is also used as a maintenance drug to assist the treatment of opiate addiction, normally in long-term chronic intravenous (IV) heroin users. It is only prescribed following exhaustive efforts at treatment via other means. It is sometimes thought that heroin users can walk into a clinic and walk out with a prescription, but the process takes many weeks before a prescription for diamorphine is issued. Though this is somewhat controversial among proponents of a zero-tolerance drug policy, it has proven superior to methadone in improving the social and health situations of addicts.
The UK Department of Health's Rolleston Committee Report in 1926 established the British approach to diamorphine prescription to users, which was maintained for the next 40 years: dealers were prosecuted, but doctors could prescribe diamorphine to users when withdrawing from it would cause harm or severe distress to the patient. This "policing and prescribing" policy effectively controlled the perceived diamorphine problem in the UK until 1959 when the number of diamorphine addicts doubled every 16 months during the ten years from 1959 to 1968. In 1964 the Brain Committee recommended that only selected approved doctors working at approved specialised centres be allowed to prescribe diamorphine and benzoylmethylecgonine (cocaine) to users. The law was made more restrictive in 1968. Beginning in the 1970s, the emphasis shifted to abstinence and the use of methadone; currently only a small number of users in the UK are prescribed diamorphine.
In 1994 Switzerland began a trial diamorphine maintenance program for users that had failed multiple withdrawal programs. The aim of this program was to maintain the health of the user by avoiding medical problems stemming from the illicit use of diamorphine. The first trial in 1994 involved 340 users, although enrollment was later expanded to 1000 based on the apparent success of the program.
The trials proved diamorphine maintenance to be superior to other forms of treatment in improving the social and health situation for this group of patients. It has also been shown to save money, despite high treatment expenses, as it significantly reduces costs incurred by trials, incarceration, health interventions and delinquency. Patients appear twice daily at a treatment center, where they inject their dose of diamorphine under the supervision of medical staff. They are required to contribute about 450 Swiss francs per month to the treatment costs. A national referendum in November 2008 showed 68% of voters supported the plan, introducing diamorphine prescription into federal law. The previous trials were based on time-limited executive ordinances. The success of the Swiss trials led German, Dutch, and Canadian cities to try out their own diamorphine prescription programs. Some Australian cities (such as Sydney) have instituted legal diamorphine supervised injecting centers, in line with other wider harm minimization programs.
Since January 2009, Denmark has prescribed diamorphine to a few addicts that have tried methadone and subutex without success. Beginning in February 2010, addicts in Copenhagen and Odense became eligible to receive free diamorphine. Later in 2010 other cities including Århus and Esbjerg joined the scheme. It was supposed that around 230 addicts would be able to receive free diamorphine.
However, Danish addicts would only be able to inject heroin according to the policy set by Danish National Board of Health. Of the estimated 1500 drug users who did not benefit from the then-current oral substitution treatment, approximately 900 would not be in the target group for treatment with injectable diamorphine, either because of "massive multiple drug abuse of non-opioids" or "not wanting treatment with injectable diamorphine".
In July 2009 the German Bundestag passed a law allowing diamorphine prescription as a standard treatment for addicts; a large-scale trial of diamorphine prescription had been authorized in that country in 2002.
Routes of administration.
The onset of heroin's effects depends upon the route of administration. Studies have shown that the subjective pleasure of drug use (the reinforcing component of addiction) is proportional to the rate at which the blood level of the drug increases. Intravenous injection is the fastest route of drug administration, causing blood concentrations to rise the most quickly, followed by smoking, suppository (anal or vaginal insertion), insufflation (snorting), and ingestion (swallowing).
Ingestion does not produce a rush as forerunner to the high experienced with the use of heroin, which is most pronounced with intravenous use. While the onset of the rush induced by injection can occur in as little as a few seconds, the oral route of administration requires approximately half an hour before the high sets in. Thus, with both higher the dosage of heroin used and faster the route of administration used, the higher potential risk for psychological addiction.
Large doses of heroin can cause fatal respiratory depression, and the drug has been used for suicide or as a murder weapon. The serial killer Harold Shipman used diamorphine on his victims, and the subsequent Shipman Inquiry led to a tightening of the regulations surrounding the storage, prescribing and destruction of controlled drugs in the UK. John Bodkin Adams is also known to have used heroin as a murder weapon.
Because significant tolerance to respiratory depression develops quickly with continued use and is lost just as quickly during withdrawal, it is often difficult to determine whether a heroin lethal overdose was accidental, suicide or homicide. Examples include the overdose deaths of Sid Vicious, Janis Joplin, Tim Buckley, Hillel Slovak, Layne Staley, Bradley Nowell, Ted Binion, and River Phoenix.
Chronic use of heroin and other opioids has been shown to be a potential cause of hyponatremia, resultant because of excess vasopressin secretion.
Oral.
Oral use of heroin is less common than other methods of administration, mainly because there is little to no "rush", and the effects are less potent. Heroin is entirely converted to morphine by means of first-pass metabolism, resulting in deacetylation when ingested. Heroin's oral bioavailability is both dose-dependent (as is morphine's) and significantly higher than oral use of morphine itself, reaching up to 64.2% for high doses and 45.6% for low doses; opiate-naive users showed far less absorption of the drug at low doses, having bioavailabilities of only up to 22.9%. The maximum plasma concentration of morphine following oral administration of heroin was around twice as much as that of oral morphine.
Injection.
Injection, also known as "slamming", "banging", "shooting up", "digging" or "mainlining", is a popular method which carries relatively greater risks than other methods of administration. Heroin base (commonly found in Europe), when prepared for injection, will only dissolve in water when mixed with an acid (most commonly citric acid powder or lemon juice) and heated. Heroin in the east-coast United States is most commonly found in the hydrochloride salt form, requiring just water (and no heat) to dissolve. Users tend to initially inject in the easily accessible arm veins, but as these veins collapse over time, users resort to more dangerous areas of the body, such as the femoral vein in the groin. Users who have used this route of administration often develop a deep vein thrombosis. Intravenous users can use a various single dose range using a hypodermic needle. The dose of heroin used for recreational purposes is dependent on the frequency and level of use: thus a first-time user may use between 5 and 20 mg, while an established addict may require several hundred mg per day. As with the injection of any drug, if a group of users share a common needle without sterilization procedures, blood-borne diseases, such as HIV or hepatitis, can be transmitted.
The use of a common dispenser for water for the use in the preparation of the injection, as well as the sharing of spoons and/or filters can also cause the spread of blood-borne diseases. Many countries now supply small sterile spoons and filters for single use in order to prevent the spread of disease.
Smoking.
Smoking heroin refers to vaporizing it to inhale the resulting fumes, not burning it to inhale the resulting smoke. It is commonly smoked in glass pipes made from glassblown Pyrex tubes and light bulbs. It can also be smoked off aluminium foil, which is heated underneath by a flame and the resulting smoke is inhaled through a tube of rolled up foil, This method is also known as "chasing the dragon" (whereas smoking methamphetamine is known as "chasing the "white" dragon").
Insufflation.
Another popular route to intake heroin is insufflation (snorting), where a user crushes the heroin into a fine powder and then gently inhales it (sometimes with a straw or a rolled-up banknote, as with cocaine) into the nose, where heroin is absorbed through the soft tissue in the mucous membrane of the sinus cavity and straight into the bloodstream. This method of administration redirects first-pass metabolism, with a quicker onset and higher bioavailability than oral administration, though the duration of action is shortened. This method is sometimes preferred by users who do not want to prepare and administer heroin for injection or smoking, but still experience a fast onset. Snorting heroin becomes an often unwanted route, once a user begins to inject the drug. The user may still get high on the drug from snorting, and experience a nod, but will not get a rush. A "rush" is caused by a large amount of heroin entering the body at once. When the drug is taken in through the nose, the user does not get the rush because the drug is absorbed slowly rather than instantly.
Suppository.
Little research has been focused on the suppository (anal insertion) or pessary (vaginal insertion) methods of administration, also known as "plugging". These methods of administration are commonly carried out using an oral syringe. Heroin can be dissolved and withdrawn into an oral syringe which may then be lubricated and inserted into the anus or vagina before the plunger is pushed. The rectum or the vaginal canal is where the majority of the drug would likely be taken up, through the membranes lining their walls.
Adverse effects.
Like most opioids, unadulterated heroin does not cause many long-term complications other than dependence and constipation. The average purity of street heroin in the UK varies between 30% and 50% and heroin that has been seized at the border has purity levels between 40% and 60%; this variation has led to people suffering from overdoses as a result of the heroin missing a stage on its journey from port to end user, as each set of hands that the drug passes through adds further adulterants, the strength of the drug reduces, with the effect that if steps are missed, the purity of the drug reaching the end user is higher than they are used to. Intravenous use of heroin (and any other substance) with non-sterile needles and syringes or other related equipment may lead to:
Many countries and local governments have begun funding programs that supply sterile needles to people who inject illegal drugs in an attempt to reduce these contingent risks, and especially the spread of blood-borne diseases. The Drug Policy Alliance reports that up to 75% of new AIDS cases among women and children are directly or indirectly a consequence of drug use by injection. The United States federal government does not operate needle exchanges, although some state and local governments do support such programs.
Anthropologists Philippe Bourgois and Jeff Schonberg performed a decade of fieldwork among homeless heroin and cocaine addicts in San Francisco, published in 2009. They reported that the African-American addicts they observed were more inclined to "direct deposit" heroin into a vein, while "skin-popping" was a far more widespread practice: "By the midpoint of our fieldwork, most of the whites had given up searching for operable veins and skin-popped. They sank their needles perfunctorily, often through their clothing, into their fatty tissue." Bourgois and Schonberg describes how the cultural difference between the African-Americans and the whites leads to this contrasting behavior, and also points out that the two different ways to inject heroin comes with different health risks. Skin-popping more often results in abscesses, and direct injection more often leads to fatal overdose and also to hepatitis C and HIV infection.
A small percentage of heroin smokers, and occasionally IV users, may develop symptoms of toxic leukoencephalopathy. The cause has yet to be identified, but one speculation is that the disorder is caused by an uncommon adulterant that is only active when heated. Symptoms include slurred speech and difficulty walking.
Cocaine is sometimes used in combination with heroin, and is referred to as a speedball when injected or "moonrocks" when smoked together. Cocaine acts as a stimulant, whereas heroin acts as a depressant. Coadministration provides an intense rush of euphoria with a high that combines both effects of the drugs, while excluding the negative effects, such as anxiety and sedation. The effects of cocaine wear off far more quickly than heroin, so if an overdose of heroin was used to compensate for cocaine, the end result is fatal respiratory depression.
Withdrawal.
The withdrawal syndrome from heroin (the so-called "cold turkey") may begin within 6–24 hours of discontinuation of the drug; however, this time frame can fluctuate with the degree of tolerance as well as the amount of the last consumed dose. Symptoms may include: sweating, malaise, anxiety, depression, akathisia, priapism, extra sensitivity of the genitals in females, general feeling of heaviness, excessive yawning or sneezing, tears, rhinorrhea, sleep difficulties (insomnia), cold sweats, chills, severe muscle and bone aches, nausea, vomiting, diarrhea, cramps, watery eyes, fever and cramp-like pains and involuntary spasms in the limbs (thought to be an origin of the term "kicking the habit").
Overdose.
Heroin overdose is usually treated with an opioid antagonist, such as naloxone (Narcan), or naltrexone. This reverses the effects of heroin and other opioids and causes an immediate return of consciousness but may result in withdrawal symptoms. The half-life of naloxone is shorter than most opioids, so that it has to be administered multiple times until the opioid has been metabolized by the body.
Depending on drug interactions and numerous other factors, death from overdose can take anywhere from several minutes to several hours. Death usually occurs due to lack of oxygen resulting from the lack of breathing caused by the opioid. Heroin overdoses can occur because of an unexpected increase in the dose or purity or because of diminished opioid tolerance. However, many fatalities reported as overdoses are probably caused by interactions with other depressant drugs such as alcohol or benzodiazepines. It should also be noted that since heroin can cause nausea and vomiting, a significant number of deaths attributed to heroin overdose are caused by aspiration of vomit by an unconscious person. Some sources quote the median lethal dose (for an average 75 kg opiate-naive individual) as being between 75 and 600  mg. Illicit heroin is of widely varying and unpredictable purity. This means that the user may prepare what they consider to be a moderate dose while actually taking far more than intended. Also, tolerance typically decreases after a period of abstinence. If this occurs and the user takes a dose comparable to their previous use, the user may experience drug effects that are much greater than expected, potentially resulting in an overdose. It has been speculated that an unknown portion of heroin-related deaths are the result of an overdose or allergic reaction to quinine, which may sometimes be used as a cutting agent.
Pharmacology.
When taken orally, heroin undergoes extensive first-pass metabolism via deacetylation, making it a prodrug for the systemic delivery of morphine. When the drug is injected, however, it avoids this first-pass effect, very rapidly crossing the blood–brain barrier because of the presence of the acetyl groups, which render it much more fat soluble than morphine itself. Once in the brain, it then is deacetylated variously into the inactive 3-monoacetylmorphine and the active 6-monoacetylmorphine (6-MAM), and then to morphine, which bind to μ-opioid receptors, resulting in the drug's euphoric, analgesic (pain relief), and anxiolytic (anti-anxiety) effects; heroin itself exhibits relatively low affinity for the μ receptor. Unlike hydromorphone and oxymorphone, however, administered intravenously, heroin creates a larger histamine release, similar to morphine, resulting in the feeling of a greater subjective "body high" to some, but also instances of pruritus (itching) when they first start using.
Both morphine and 6-MAM are μ-opioid agonists that bind to receptors present throughout the brain, spinal cord, and gut of all mammals. The μ-opioid receptor also binds endogenous opioid peptides such as β-endorphin, Leu-enkephalin, and Met-enkephalin. Repeated use of heroin results in a number of physiological changes, including an increase in the production of μ-opioid receptors (upregulation). These physiological alterations lead to tolerance and dependence, so that stopping heroin use results in uncomfortable symptoms including pain, anxiety, muscle spasms, and insomnia called the opioid withdrawal syndrome. Depending on usage it has an onset 4–24 hours after the last dose of heroin. Morphine also binds to δ- and κ-opioid receptors.
There is also evidence that 6-MAM binds to a subtype of μ-opioid receptors that are also activated by the morphine metabolite morphine-6β-glucuronide but not morphine itself. The third subtype of third opioid type is the mu-3 receptor, which may be a commonality to other six-position monoesters of morphine. The contribution of these receptors to the overall pharmacology of heroin remains unknown.
A subclass of morphine derivatives, namely the 3,6 esters of morphine, with similar effects and uses, includes the clinically used strong analgesics nicomorphine (Vilan), and dipropanoylmorphine; there is also the latter's dihydromorphine analogue, diacetyldihydromorphine (Paralaudin). Two other 3,6 diesters of morphine invented in 1874–75 along with diamorphine, dibenzoylmorphine and acetylpropionylmorphine, were made as substitutes after it was outlawed in 1925 and, therefore, sold as the first "designer drugs" until they were outlawed by the League of Nations in 1930.
Physical and chemical properties.
Detection in body fluids.
The major metabolites of diamorphine, 6-MAM, morphine, morphine-3-glucuronide and morphine-6-glucuronide, may be quantitated in blood, plasma or urine to monitor for abuse, confirm a diagnosis of poisoning or assist in a medicolegal death investigation. Most commercial opiate screening tests cross-react appreciably with these metabolites, as well as with other biotransformation products likely to be present following usage of street-grade diamorphine such as 6-acetylcodeine and codeine. However, chromatographic techniques can easily distinguish and measure each of these substances. When interpreting the results of a test, it is important to consider the diamorphine usage history of the individual, since a chronic user can develop tolerance to doses that would incapacitate an opiate-naive individual, and the chronic user often has high baseline values of these metabolites in his system. Furthermore, some testing procedures employ a hydrolysis step before quantitation that converts many of the metabolic products to morphine, yielding a result that may be 2 times larger than with a method that examines each product individually.
History.
The opium poppy was cultivated in lower Mesopotamia as long ago as 3400 BCE. The chemical analysis of opium in the 19th century revealed that most of its activity could be ascribed to two alkaloids, codeine and morphine.
Diamorphine was first synthesized in 1874 by C. R. Alder Wright, an English chemist working at St. Mary's Hospital Medical School in London. He had been experimenting with combining morphine with various acids. He boiled anhydrous morphine alkaloid with acetic anhydride for several hours and produced a more potent, acetylated form of morphine, now called "diacetylmorphine" or "morphine diacetate". The compound was sent to F. M. Pierce of Owens College in Manchester for analysis. Pierce told Wright:
Wright's invention did not lead to any further developments, and diamorphine became popular only after it was independently re-synthesized 23 years later by another chemist, Felix Hoffmann. Hoffmann, working at Bayer pharmaceutical company in Elberfeld, Germany, was instructed by his supervisor Heinrich Dreser to acetylate morphine with the objective of producing codeine, a constituent of the opium poppy, pharmacologically similar to morphine but less potent and less addictive. Instead, the experiment produced an acetylated form of morphine one and a half to two times more potent than morphine itself. The head of Bayer's research department reputedly coined the drug's new name, "heroin," based on the German "heroisch", which means "heroic, strong" (from the ancient greek word "heros, ήρως"). Bayer scientists were not the first to make heroin, but their scientists discovered ways to make it, and Bayer led commercialization of heroin.
From 1898 through to 1910, diamorphine was marketed under the trademark name Heroin as a non-addictive morphine substitute and cough suppressant. In the 11th edition of Encyclopedia Britannica (1910), the article on morphine states: "In the cough of phthisis minute doses morphine are of service, but in this particular disease morphine is frequently better replaced by codeine or by heroin, which checks irritable coughs without the narcotism following upon the administration of morphine."
In the U.S., the Harrison Narcotics Tax Act was passed in 1914 to control the sale and distribution of diacetylmorphine and other opioids, which allowed the drug to be prescribed and sold for medical purposes. In 1924, the United States Congress banned its sale, importation, or manufacture. It is now a Schedule I substance, which makes it illegal for non-medical use in signatory nations of the Single Convention on Narcotic Drugs treaty, including the United States.
The Health Committee of the League of Nations banned diacetylmorphine in 1925, although it took more than three years for this to be implemented. In the meantime, the first designer drugs, viz. 3,6 diesters and 6 monoesters of morphine and acetylated analogues of closely related drugs like hydromorphone and dihydromorphine, were produced in massive quantities to fill the worldwide demand for diacetylmorphine—this continued until 1930 when the Committee banned diacetylmorphine analogues with no therapeutic advantage over drugs already in use, the first major legislation of this type.
Later, as with Aspirin, Bayer lost some of its trademark rights to heroin under the 1919 Treaty of Versailles following the German defeat in World War I.
Etymology.
In 1895, the German drug company Bayer marketed diacetylmorphine as an over-the-counter drug under the trademark name Heroin. The name was derived from the Greek word "heros" because of its perceived "heroic" effects upon a user. It was developed chiefly as a morphine substitute for cough suppressants that did not have morphine's addictive side-effects. Morphine at the time was a popular recreational drug, and Bayer wished to find a similar but non-addictive substitute to market. However, contrary to Bayer's advertising as a "non-addictive morphine substitute," heroin would soon have one of the highest rates of addiction among its users.
Society and culture.
Legal status.
Asia.
In Hong Kong, diamorphine is regulated under Schedule 1 of Hong Kong's Chapter 134 "Dangerous Drugs Ordinance". It is available by prescription. Anyone supplying diamorphine without a valid prescription can be fined $10,000 (HKD). The penalty for trafficking or manufacturing diamorphine is a $50,000 (HKD) fine and life imprisonment. Possession of diamorphine without a license from the Department of Health is illegal with a $10,000 (HKD) fine and/or 7 years of jail time.
Europe.
In the Netherlands, diamorphine is a List I drug of the Opium Law. It is available for prescription under tight regulation exclusively to long-term addicts for whom methadone maintenance treatment has failed. It cannot be used to treat severe pain or other illnesses.
In the United Kingdom, diamorphine is available by prescription, though it is a restricted Class A drug. According to the 50th edition of the British National Formulary (BNF), diamorphine hydrochloride may be used in the treatment of acute pain, myocardial infarction, acute pulmonary oedema, and chronic pain. The treatment of chronic non-malignant pain must be supervised by a specialist. The BNF notes that all opioid analgesics cause dependence and tolerance but that this is "no deterrent in the control of pain in terminal illness". When used in the palliative care of cancer patients, diamorphine is often injected using a syringe driver.
Australia.
In Australia diamorphine is listed as a schedule 9 prohibited substance under the Poisons Standard (October 2015). A schedule 9 drug is outlined in the Poisons Act 1964 as "Substances which may be abused or misused, the manufacture, possession, sale or use of which should be prohibited by law except when required for medical or scientific research, or for analytical, teaching or training purposes with approval of the CEO." 
North America.
In Canada, diamorphine is a controlled substance under Schedule I of the Controlled Drugs and Substances Act (CDSA). Any person seeking or obtaining diamorphine without disclosing authorization 30 days before obtaining another prescription from a practitioner is guilty of an indictable offense and subject to imprisonment for a term not exceeding seven years. Possession of diamorphine for the purpose of trafficking is an indictable offense and subject to imprisonment for life.
In the United States, diamorphine is a Schedule I drug according to the Controlled Substances Act of 1970, making it illegal to possess without a DEA license. Possession of more than 100 grams of diamorphine or a mixture containing diamorphine is punishable with a minimum mandatory sentence of 5 years of imprisonment in a federal prison.
Economics.
Production.
Diamorphine is produced from acetylation of morphine derived from natural opium sources. Numerous mechanical and chemical means are used to purify the final product. The final products have a different appearance depending on purity and have different names.
Heroin grades.
Heroin purity has been classified into four grades. No.4 is the purest form – white powder (salt) to be easily dissolved and injected. No.3 is "brown sugar" for smoking (base). No.1 and No.2 are unprocessed raw heroin (salt or base).
Trafficking.
Traffic is heavy worldwide, with the biggest producer being Afghanistan. According to a U.N. sponsored survey, in 2004, Afghanistan accounted for production of 87 percent of the world's diamorphine. Afghan opium kills around 100,000 people annually.
In 2003 "The Independent" reported:
Opium production in that country has increased rapidly since, reaching an all-time high in 2006. War in Afghanistan once again appeared as a facilitator of the trade. Some 3.3 million Afghans are involved in producing opium.
At present, opium poppies are mostly grown in Afghanistan, and in Southeast Asia, especially in the region known as the Golden Triangle straddling Burma, Thailand, Vietnam, Laos and Yunnan province in China. There is also cultivation of opium poppies in the Sinaloa region of Mexico and in Colombia. According to the DEA, the majority of the heroin consumed in the United States comes from Mexico (50%) and Colombia (43-45%) via Mexican criminal cartels such as Sinaloa Cartel. 
, the Sinaloa Cartel is the most active drug cartel involved in smuggling illicit drugs such as heroin into the United States and trafficking them throughout the United States. According to the United Nations Office on Drugs and Crime (UNODC), Pakistan has over of opium poppies under cultivation concentrated in the areas bordering Afghanistan and is the destination and transit point for 40 percent of the opiates produced in that country.
Conviction for trafficking heroin carries the death penalty in most Southeast Asian, some East Asian and Middle Eastern countries (see Use of death penalty worldwide for details), among which Malaysia, Singapore and Thailand are the most strict. The penalty applies even to citizens of countries where the penalty is not in place, sometimes causing controversy when foreign visitors are arrested for trafficking, for example the arrest of nine Australians in Bali, the death sentence given to Nola Blake in Thailand in 1987, or the hanging of an Australian citizen Van Tuong Nguyen in Singapore.
Trafficking history.
The origins of the present international illegal heroin trade can be traced back to laws passed in many countries in the early 1900s that closely regulated the production and sale of opium and its derivatives including heroin. At first, heroin flowed from countries where it was still legal into countries where it was no longer legal. By the mid-1920s, heroin production had been made illegal in many parts of the world. An illegal trade developed at that time between heroin labs in China (mostly in Shanghai and Tianjin) and other nations. The weakness of government in China and conditions of civil war enabled heroin production to take root there. Chinese triad gangs eventually came to play a major role in the illicit heroin trade. The French Connection route started in the 1930s.
Heroin trafficking was virtually eliminated in the U.S. during World War II because of temporary trade disruptions caused by the war. Japan's war with China had cut the normal distribution routes for heroin and the war had generally disrupted the movement of opium.
After World War II, the Mafia took advantage of the weakness of the postwar Italian government and set up heroin labs in Sicily. The Mafia took advantage of Sicily's location along the historic route opium took westward into Europe and the United States.
Large-scale international heroin production effectively ended in China with the victory of the communists in the civil war in the late 1940s. The elimination of Chinese production happened at the same time that Sicily's role in the trade developed.
Although it remained legal in some countries until after World War II, health risks, addiction, and widespread recreational use led most western countries to declare heroin a controlled substance by the latter half of the 20th century.
In late 1960s and early 1970s, the CIA supported anti-Communist Chinese Nationalists settled near the Sino-Burmese border and Hmong tribesmen in Laos. This helped the development of the Golden Triangle opium production region, which supplied about one-third of heroin consumed in US after the 1973 American withdrawal from Vietnam. In 1999, Burma, the heartland of the Golden Triangle, was the second largest producer of heroin, after Afghanistan.
The Soviet-Afghan war led to increased production in the Pakistani-Afghan border regions, as U.S.-backed mujaheddin militants raised money for arms from selling opium, contributing heavily to the modern Golden Crescent creation. By 1980, 60 percent of heroin sold in the U.S. originated in Afghanistan. It increased international production of heroin at lower prices in the 1980s. The trade shifted away from Sicily in the late 1970s as various criminal organizations violently fought with each other over the trade. The fighting also led to a stepped-up government law enforcement presence in Sicily.
Following the discovery at a Jordanian airport of a toner cartridge that had been modified into an improvised explosive device, the resultant increased level of airfreight scrutiny led to a major shortage (drought) of heroin from October 2010 until April 2011. This was reported in most of mainland Europe and the UK which led to a price increase of approximately 30 percent in the cost of street heroin and an increased demand for diverted methadone. The number of addicts seeking treatment also increased significantly during this period.
Other heroin droughts (shortages) have been attributed to cartels restricting supply in order to force a price increase and also to a fungus that attacked the opium crop of 2009. Many people thought that the American government had introduced pathogens into the Afghanistan atmosphere in order to destroy the opium crop and thus starve insurgents of income.
On 13 March 2012, Haji Bagcho, with ties to the Taliban, was convicted by a U.S. District Court of conspiracy, distribution of heroin for importation into the United States and narco-terrorism. Based on heroin production statistics compiled by the United Nations Office on Drugs and Crime, in 2006, Bagcho's activities accounted for approximately 20 percent of the world's total production for that year.
Street price.
The European Monitoring Centre for Drugs and Drug Addiction reports that the retail price of brown heroin varies from €14.5 per gram in Turkey to €110 per gram in Sweden, with most European countries reporting typical prices of €35–40 per gram. The price of white heroin is reported only by a few European countries and ranged between €27 and €110 per gram.
The United Nations Office on Drugs and Crime claims in its 2008 World Drug Report that typical US retail prices are US$172 per gram.
Harm reduction.
Harm reduction is a public health philosophy that seeks to reduce the harms associated with the use of diamorphine. One aspect of harm reduction initiatives focuses on the behaviour of individual users. This includes promoting safer means of taking the drug, such as smoking, nasal use, oral or rectal insertion. This attempts to avoid the higher risks of overdose, infections and blood-borne viruses associated with injecting the drug. Other measures include using a small amount of the drug first to gauge the strength, and minimize the risks of overdose. For the same reason, poly drug use (the use of two or more drugs at the same time) is discouraged. Injecting diamorphine users are encouraged to use new needles, syringes, spoons/steri-cups and filters every time they inject and not share these with other users. Users are also encouraged to not use it on their own, as others can assist in the event of an overdose.
Governments that support a harm reduction approach usually fund needle and syringe exchange programs, which supply new needles and syringes on a confidential basis, as well as education on proper filtering before injection, safer injection techniques, safe disposal of used injecting gear and other equipment used when preparing diamorphine for injection may also be supplied including citric acid sachets/vitamin C sachets, steri-cups, filters, alcohol pre-injection swabs, sterile water ampules and tourniquets (to stop use of shoe laces or belts).
Another harm reduction measure employed for example in Europe, Canada and Australia are safe injection sites where users can inject diamorphine and cocaine under the supervision of medically trained staff. Safe injection sites are low threshold and allow social services to approach problem users that would otherwise be hard to reach.
In the UK the Criminal Justice System has a protocol in place that requires that any individual that is arrested and is suspected of having a substance misuse problem be offered the chance to enter a treatment program. This has had the effect of drastically reducing an area's crime rate as individuals arrested for theft in order to supply the funds for their drugs are no longer in the position of having to steal to purchase heroin because they have been placed onto a methadone program, quite often more quickly than would have been possible had they not been arrested. This aspect of harm reduction is seen as being beneficial to both the individual and the community at large, who are then protected from the possible theft of their goods.
During the late 1980s and early 1990s, Swiss authorities ran the ZIPP-AIDS (Zurich Intervention Pilot Project), handing out free syringes in the officially tolerated drug scene in Platzspitz park. In 1994, Zurich started a pilot project using prescription heroin in heroin-assisted treatment (HAT) which allowed users to obtain heroin and inject it under medical supervision. The HAT program proved to be cost-beneficial to society and improve patients overall health and social stability and has since been introduced in multiple European countries.
Popular culture.
Heroin is mentioned in hundreds of films. Sometimes the use or trafficking of the drug is the central theme of the film but many times it is almost incidental as part of a crime in a police drama, for example.
Use of heroin by jazz musicians in particular was prevalent in the mid-twentieth century, including Billie Holiday, sax legends Charlie Parker and Art Pepper, guitarist Joe Pass and piano player/singer Ray Charles; a "staggering number of jazz musicians were addicts". It was also a problem with many rock musicians, particularly from the late 1960s through the 1990s. Pete Doherty is also a self-confessed user of heroin. Nirvana frontman Kurt Cobain's heroin addiction was well documented. Pantera frontman, Phil Anselmo, turned to heroin while touring during the 1990s to cope with his back pain.
Research.
Researchers are attempting to reproduce the biosynthetic pathway that produces morphine in genetically engineered yeast. In June 2015 the "S"-reticuline could be produced from sugar and "R"-reticuline could be converted to morphine, but the intermediate reaction could not be performed.

</doc>
<doc id="14035" url="https://en.wikipedia.org/wiki?curid=14035" title="Hellas Verona F.C.">
Hellas Verona F.C.

Hellas Verona Football Club (commonly known simply as Verona, or Hellas within the city of Verona itself) is a professional Italian football club, based in Verona, Veneto. The team won the Serie A Championship in 1984–85, and currently plays in Serie A.
History.
Origins and early history.
Founded in 1903 by a group of high school students, the club was named "Hellas" (the Greek word for Greece), at the request of a professor of classics. At a time in which football was played seriously only in the larger cities of the northwest of Italy, most of Verona was indifferent to the growing sport. However, when in 1906 two city teams chose the city's Roman amphitheatre as a venue to showcase the game, crowd enthusiasm and media interest began to rise.
During these first few years, Hellas was one of three or four area teams playing mainly at a municipal level while fighting against city rivals Bentegodi to become the city's premier football outfit. By the 1907–08 season, Hellas was playing against regional teams and an intense rivalry with Vicenza that lasts to this day was born.
From 1898 to 1926, Italian football was organised into regional groups. In this period, Hellas was one of the founding teams of the early league and often among its top final contenders. In 1911, the city helped Hellas replace the early, gritty football fields with a proper venue. This allowed the team to take part in its first regional tournament, which until 1926, was the qualifying stage for the national title.
In 1919, following a return to activity after a four-year suspension of all football competition in Italy during World War I, the team merged with city rival Verona and changed its name to Hellas Verona. Between 1926 and 1929, the elite ""Campionato Nazionale"" assimilated the top sides from the various regional groups and Hellas Verona joined the privileged teams, yet struggled to remain competitive.
Serie A, as it is structured today, began in 1929, when the "Campionato Nazionale" turned into a professional league. Still an amateur team, Hellas merged with two city rivals, Bentegodi and Scaligera, to form AC Verona. Hoping to build a first class contender for future years the new team debuted in Serie B in 1929. It would take the "gialloblu" 28 years to finally achieve their goal. After first being promoted to Serie A for one season in 1957–58, in 1959, the team merged with another city rival (called Hellas) and commemorated its beginnings by changing its name to Hellas Verona AC.
Success in the 1970s and 1980s.
Coached by Nils Liedholm, the team returned to Serie A in 1968 and remained in the elite league almost without interruption until 1990. Along the way, it scored a famous 5–3 win in the 1972–73 season that cost Milan the "scudetto" (the Serie A title). The fact that the result came late during the last matchday of the season makes the sudden and unexpected end to the "rossoneri"'s title ambitions all the more memorable.
In 1973–74, Hellas finished the season in fourth-last, just narrowly avoiding relegation, but were nonetheless sent down to Serie B during the summer months as a result of a scandal involving team president Saverio Garonzi. After a year in Serie B, Hellas returned to Serie A.
In the 1975–76 season, the team had a successful run in the Coppa Italia, eliminating highly-rated teams such as Torino, Cagliari and Internazionale from the tournament. However, in their first ever final in the competition, Hellas were trounced 4–0 by Napoli.
Under the leadership of coach Osvaldo Bagnoli, in 1982–83 the team secured a fourth-place in Serie A (its highest finish at the time) and even lead the Serie A standings for a few weeks. The same season Hellas again reached the Coppa Italia final. After a 2–0 home victory, Hellas then travelled to Turin to play Juventus but were defeated 3–0 after extra time.
Further disappointment followed in the 1983–84 season when the team again reached the Coppa Italia final, only to lose the Cup in the final minutes of the return match against defending Serie A champions Roma
The team made its first European appearance in the 1983-84 UEFA Cup and were knocked out in the second round of the tournament by Sturm Graz. Hellas were eliminated from the 1985–86 European Cup in the second round by defending champions and fellow Serie A side Juventus after a contested game, the result of a scandalous arbitrage by the French Wurtz, having beaten PAOK of Greece in the first round.
In 1988, the team had their best international result when they reached the UEFA Cup quarterfinals with four victories and three draws. The decisive defeat came from German side Werder Bremen.
1984–1985 "Scudetto".
Although the 1984–85 season squad was made up of a mix of emerging players and mature stars, at the beginning of the season no one would have regarded the team as having the necessary ingredients to make it to the end. Certainly, the additions of Hans-Peter Briegel in midfield and of Danish striker Preben Elkjær to an attack that already featured the wing play of Pietro Fanna, the creative abilities of Antonio Di Gennaro and the scoring touch of Giuseppe Galderisi were to prove crucial.
To mention a few of the memorable milestones on the road to the "scudetto": a decisive win against Juventus (2–0), with a goal scored by Elkjær after having lost a boot in a tackle just outside the box, set the stage early in the championship; an away win over Udinese (5–3) ended any speculation that the team was losing energy at the midway point; three straight wins (including a hard fought 1–0 victory against a strong Roma side) served notice that the team had kept its polish and focus intact during their rival's final surge; and a 1–1 draw in Bergamo against Atalanta secured the title with a game in hand.
Hellas finished the year with a 15–13–2 record and 43 points, four points ahead of Torino with Internazionale and Sampdoria rounding out the top four spots. This unusual final table of the Serie A (with the most successful Italian teams of the time, Juventus and Roma, ending up much lower than expected) has led to many speculations. The 1984–85 season was the only season when referees were assigned to matches by way of a random draw. Before then each referee had always been assigned to a specific match by a special commission of referees ("designatori arbitrali"). After the betting scandal of the early 1980 (the Calcio Scommesse scandal), it was decided to clean up the image of Italian football by assigning referees randomly instead of picking them, to clear up all the suspicions and accusations always accompanying Italy's football life. This resulted in a quieter championship and in a completely unexpected final table.
In the following season, won again by Juventus, the choice of the referees went back in the hands of the "designatori arbitrali". In 2006, a major scandal in Italian football revealed that certain clubs had been illegally influencing the referee selection process in an attempt to ensure that certain referees were assigned to their matches.
Between Serie A and Serie B.
These were more than mere modest achievements for a mid-size city with a limited appeal to fans across the nation. But soon enough financial difficulties caught up with team managers. In 1991 the team folded and was reborn as Verona, regularly moving to and fro between Serie A and Serie B for several seasons. In 1995 the name was officially changed back to Hellas Verona.
After a three-year stay, their last stint in Serie A ended in grief in 2002. That season emerging international talents such as Adrian Mutu, Mauro Camoranesi, Alberto Gilardino, Martin Laursen, Massimo Oddo, Marco Cassetti and coach Alberto Malesani failed to capitalise on an excellent start and eventually dropped into fourth-to-last place for the first time all season on the very last matchday, enforcing relegation into Serie B.
Decline and Serie A comeback (2002–2016).
Following the 2002 relegation to Serie B, team fortunes continued to slip throughout the decade. In the 2003–04 season Hellas Verona struggled in Serie B and spent most of the season fighting off an unthinkable relegation to Serie C1. Undeterred, the fans supported their team and a string of late season wins eventually warded off the danger. Over 5,000 of them followed Hellas to Como on the final day of the season to celebrate.
In 2004–05, things looked much brighter for the team. After a rocky start, Hellas put together a string of results and climbed to third spot. The "gialloblù" held on to the position until January 2005, when transfers weakened the team, yet they managed to take the battle for Serie A to the last day of the season.
The 2006–07 Serie B seemed to start well, due to the club takeover by Pietro Arvedi D'Emilei, which ended nine years of controversial leadership under chairman Gianbattista Pastorello, heavily contested by the supporters in his later years at Verona. However, Verona was immediately involved in the relegation battle, and Massimo Ficcadenti was replaced in December 2006 by Giampiero Ventura. Despite a recovery in the results, Verona ended in an 18th place, thus being forced to play a two-legged playoff against 19th-placed Spezia to avert relegation. A 2–1 away loss in the first leg at La Spezia was followed by a 0–0 home tie, and Verona were relegated to Serie C1 after 64 years of play in the two highest divisions.
Verona appointed experienced coach Franco Colomba for the new season with the aim to return to Serie B as soon as possible. However, despite being widely considered the division favourite, the "gialloblù" spent almost the entire season in last place. After seven matches, club management sacked Colomba in early October and replaced him with youth team coach (and former Verona player) Davide Pellegrini. A new property acquired the club in late 2007, appointing in December Giovanni Galli as new director of football and Maurizio Sarri as new head coach. Halfway through the 2007–08 season, the team remained at the bottom of Serie C1, on the brink of relegation to the fourth level (Serie C2). In response, club management sacked Sarri and brought back Pellegrini. Thanks to a late-season surge the "scaligeri" avoided direct relegation by qualifying for the relegation play-off, and narrowly averted dropping to Lega Pro Seconda Divisione in the final game, beating Pro Patria 2–1 on aggregate. However, despite the decline in results, attendance and season ticket sales remained on 15,000 average.
For the 2008–09 season, Verona appointed former Sassuolo and Piacenza manager Gian Marco Remondina with the aim to win promotion to Serie B. However, the season did not start impressively, with Verona being out of the playoff zone by mid-season, and club chairman Pietro Arvedi D'Emilei entering into a coma after being involved in a car crash on his way back from a league match in December 2008. Arvedi died in March 2009, two months after the club was bought by new chairman Giovanni Martinelli.
The following season looked promising, as new transfer players were brought aboard, and fans enthusiastically embraced the new campaign. Season ticket figures climbed to over 10,000, placing Verona ahead of several Serie A teams and all but Torino in Serie B attendance. The team led the standings for much of the season, accumulating a seven-point lead by early in the spring. However, the advantage was gradually squandered, and the team dropped to second place on the second-last day of the season, with a chance to regain first place in the final regular season match against Portogruaro on home soil. Verona, however, disappointed a crowd of over 25,000 fans and, with the loss, dropped to third place and headed towards the play-offs. A managerial change for the post-season saw the firing of Remondina and the arrival of Giovanni Vavassori. After eliminating Rimini in the semi-finals (1–0; 0–0) Verona lost the final to Pescara (2–2 on home soil and 0–1 in the return match) and were condemned to a fourth-straight year of third division football.
Former 1990 World Cup star Giuseppe Giannini (a famous captain of Roma for many years) signed as manager for the 2010–11 campaign. Once again, the team was almost entirely revamped during the transfer season. The squad struggled in the early months and Giannini was eventually sacked and replaced by former Internazionale defender Andrea Mandorlini, who succeeded in reorganising the team's play and bringing discipline both on and off the pitch. In the second half of the season, Verona climbed back from the bottom of the division to clinch a play-off berth (fifth place) on the last day of the regular season. The team advanced to the play-off final after eliminating Sorrento in the semi-finals 3–1 on aggregate. Following the play-off final, after four years of Lega Pro football, Verona were promoted back to Serie B after a 2–1 aggregate win over Salernitana on 19 June 2011.
On 18 May 2013, Verona finished second in Serie B and were promoted to Serie A after an 11-year absence. Their return to the top flight began against title contenders Milan and Roma, beating the former 2–1 and losing to the latter 3–0. The team continued at a steady pace, finishing the first half of the season with 32 points and sitting in sixth place—11 points behind the closest UEFA Champions League spot—and tied with Internazionale for the final UEFA Europa League spot. Verona, however, ultimately finished the year in tenth.
During the 2015–16 season, Verona hadn't won a single match since the beginning of the campaign until the club edged Atalanta 2–1 on 3 February 2016 in a win at home; coming twenty-three games into the season. Consequently, Verona were relegated from Serie A.
Colours and badge.
The team's colours are yellow and blue and "gialloblu" (literally, "yellow-blue" in Italian) is the team's most widely used nickname. The colours represent the city itself and Verona's emblem (a yellow cross on a blue shield) appears on most team apparel. Two more team nicknames are "Mastini" (the mastiffs) and "Scaligeri", both references to Mastino I della Scala of the Della Scala princes that ruled the city during the 13th and 14th centuries.
The Scala family coat of arms is depicted on the team's jersey and on its trademark logo as a stylised image of two large, powerful mastiffs facing opposite directions. In essence, the term ""scaligeri"" is synonymous with Veronese, and therefore can describe anything or anyone from Verona (e.g., Chievo Verona, a different team that also links itself to the Scala family – specifically to Cangrande I della Scala).
Stadium.
Since 1963, the club have played at the Stadio Marc'Antonio Bentegodi, which has a capacity of 39,211. The ground is shared with Hellas' rivals, Chievo Verona. It was used as a venue for the 1990 FIFA World Cup.
Derby with Chievo Verona.
The intercity fixtures against Chievo Verona are known as the "Derby della Scala." The name refers to the Scaligeri or della Scala aristocratic family, who were rulers of Verona during the Middle Ages and early Renaissance. In the season 2001–02, both Hellas Verona and the city rivals of Chievo Verona were playing in the Serie A. The first ever derby of Verona in Serie A took place on 18 November 2001, while both teams were ranked among the top four. The match was won by Hellas, 3–2. Chievo got revenge in the return match in spring 2002, winning 2–1. Verona became so the fifth city in Italy, after Milan, Rome, Turin and Genoa to host a cross-city derby in Serie A.

</doc>
<doc id="14036" url="https://en.wikipedia.org/wiki?curid=14036" title="Hinayana">
Hinayana

Hīnayāna is a Sanskrit term literally meaning: the "Smaller Vehicle", applied to the "Śrāvakayāna", the Buddhist path followed by a śrāvaka who wishes to become an arhat. The term appeared around the first or second century. Hīnayāna is often contrasted with Mahāyāna, which means the "Great Vehicle."
There are a variety of interpretations as to who or what the term Hīnayāna refers. Kalu Rinpoche stated the "lesser" or "greater" designation "does not refer to economic or social status, but concerns the spiritual capacities of the practitioner".
The Chinese monk Yijing, who visited India in the 7th century, distinguished Mahāyāna from Hīnayāna as follows:
The term was widely used in the past by Western scholars to cover "the earliest system of Buddhist doctrine", as the Monier-Williams Sanskrit-English Dictionary put it. It has been used as a synonym for the Theravada tradition, which continues as the main form of Buddhism in Sri Lanka and Southeast Asia, but some scholars deny that the term included Theravada Buddhism. In 1950 the World Fellowship of Buddhists declared that the term Hīnayana should not be used when referring to any form of Buddhism existing today.
Etymology.
The word Hīnayāna is formed of "hīna": "little," "poor," "inferior," "abandoned," "deficient," "defective;" and "yāna" (यान): "vehicle", where "vehicle" means "a way of going to enlightenment". The Pali Text Society's Pali-English Dictionary (1921–25) defines "hīna" in even stronger terms, with a semantic field that includes "poor, miserable; vile, base, abject, contemptible," and "despicable."
The term was translated by Kumārajīva and others into Classical Chinese as "small vehicle" (小 meaning "small", 乘 meaning "vehicle"), although earlier and more accurate translations of the term also exist. In Mongolian ("Baga Holgon") the term for Hinayana also means "small" or "lesser" vehicle, while in Tibetan there are at least two words to designate the term, "theg chung" meaning "small vehicle" and "theg dman" meaning "inferior vehicle" or "inferior spiritual approach".
Thrangu Rinpoche has emphasized that "hinayana" is in no way implying "inferior". In his translation and commentary of Asanga's "Distinguishing Dharma from Dharmata", he writes, "all three traditions of hinayana, mahayana, and vajrayana were practiced in Tibet and that the hinayana which literally means "lesser vehicle" is in no way inferior to the mahayana."
Origins.
According to Jan Nattier, it is most likely that the term Hīnayāna postdates the term Mahāyāna and was only added at a later date due to antagonism and conflict between the bodhisattva and śrāvaka ideals. The sequence of terms then began with the term "Bodhisattvayāna" "bodhisattva-vehicle", which was given the epithet Mahāyāna "Great Vehicle". It was only later, after attitudes toward the bodhisattva teachings had become more critical, that the term Hīnayāna was created as a back-formation, contrasting with the already established term Mahāyāna. The earliest Mahāyāna texts often use the term Mahāyāna as an epithet and synonym for Bodhisattvayāna, but the term Hīnayāna is comparatively rare in early texts, and is usually not found at all in the earliest translations. Therefore, the often-perceived symmetry between Mahāyāna and Hīnayāna can be deceptive, as the terms were not actually coined in relation to one another in the same era.
According to Paul Williams, "the deep-rooted misconception concerning an unfailing, ubiquitous fierce criticism of the Lesser Vehicle by the [Mahāyāna] is not supported by our texts." Williams states that while evidence of conflict is present in some cases, there is also substantial evidence demonstrating peaceful coexistence between the two traditions.
Mahāyāna members of the early Buddhist schools.
Although the 18–20 early Buddhist schools are sometimes loosely classified as Hīnayāna in modern times, this is not necessarily accurate. There is no evidence that Mahāyāna ever referred to a separate formal school of Buddhism but rather as a certain set of ideals, and later doctrines. Paul Williams has also noted that the Mahāyāna never had nor ever attempted to have a separate vinaya or ordination lineage from the early Buddhist schools, and therefore bhikṣus and bhikṣuṇīs adhering to the Mahāyāna formally adheres to the vinaya of an early school. This continues today with the Dharmaguptaka ordination lineage in East Asia and the Mūlasarvāstivāda ordination lineage in Tibetan Buddhism. Mahāyāna was never a separate sect of the early schools. From Chinese monks visiting India, we now know that both Mahāyāna and non-Mahāyāna monks in India often lived in the same monasteries side by side.
The Chinese Buddhist monk and pilgrim Yijing wrote about the relationship between the various "vehicles" and the early Buddhist schools in India. He wrote, "There exist in the West numerous subdivisions of the schools which have different origins, but there are only four principal schools of continuous tradition." These schools are the Mahāsāṃghika Nikāya, Sthavira nikāya, Mūlasarvāstivāda Nikāya, and Saṃmitīya Nikāya. Explaining their doctrinal affiliations, he then writes, "Which of the four schools should be grouped with the Mahāyāna or with the Hīnayāna is not determined." That is to say, there was no simple correspondence between a Buddhist school and whether its members learn "Hīnayāna" or "Mahāyāna" teachings.
To identify entire schools as "Hīnayāna" that contained not only śrāvakas and pratyekabuddhas but also Mahāyāna bodhisattvas would be attacking the schools of their fellow Mahāyānists as well as their own. Instead, what is demonstrated in the definition of "Hīnayāna" given by Yijing is that the term referred to individuals based on doctrinal differences.
Hīnayāna as Śrāvakayāna.
Scholar Isabelle Onians asserts that although "the Mahāyāna ... very occasionally referred to earlier Buddhism as the Hinayāna, the Inferior Way, [...] the preponderance of this name in the secondary literature is far out of proportion to occurrences in the Indian texts." She notes that the term Śrāvakayāna was "the more politically correct and much more usual" term used by Mahāyānists. Jonathan Silk has argued that the term "Hinayana" was used to refer to whomever one wanted to criticize on any given occasion, and did not refer to any definite grouping of Buddhists.
Hīnayāna and Theravāda.
Views of Chinese pilgrims.
In the 7th century, the Chinese Buddhist monk Xuanzang describes the concurrent existence of the Mahāvihara and the Abhayagiri vihāra in Sri Lanka. He refers to the monks of the Mahāvihara as the "Hīnayāna Sthaviras" and the monks of Abhayagiri vihāra as the "Mahāyāna Sthaviras". Xuanzang further writes, "The Mahāvihāravāsins reject the Mahāyāna and practice the Hīnayāna, while the Abhayagirivihāravāsins study both Hīnayāna and Mahāyāna teachings and propagate the "Tripiṭaka"."
Philosophical differences.
Mahayanists were primarily in philosophical dialectic with the Vaibhāṣika school of Sarvāstivāda, which had by far the most "comprehensive edifice of doctrinal systematics" of the nikāya schools. With this in mind it is sometimes argued that the Theravada would not have been considered a "Hinayana" school by Mahayanists because, unlike the now-extinct Sarvastivada school, the primary object of Mahayana criticism, the Theravada school does not claim the existence of independent dharmas; in this it maintains the attitude of early Buddhism. Additionally, the concept of the bodhisattva as one who puts off enlightenment rather than reaching awakening as soon as possible, has no roots in Theravada textual or cultural contexts, current or historical. Aside from the Theravada schools being geographically distant from the Mahayana, the Hinayana distinction is used in reference to certain views and practices that had become found within the Mahayana tradition itself. Theravada, as well as Mahayana schools stress the urgency of one's own awakening in order to end suffering. Some contemporary Theravadin figures have thus indicated a sympathetic stance toward the Mahayana philosophy found in the "Heart Sutra" and the "Mūlamadhyamakakārikā".
The Mahayanists were bothered by the substantialist thought of the Sarvāstivādins and Sautrāntikins, and in emphasizing the doctrine of śūnyatā, David Kalupahana holds that they endeavored to preserve the early teaching. The Theravadins too refuted the Sarvāstivādins and Sautrāntikins (and followers of other schools) on the grounds that their theories were in conflict with the non-substantialism of the canon. The Theravada arguments are preserved in the "Kathavatthu".
Opinions of scholars.
Most western scholars regard the Theravada school to be one of the Hinayana schools referred to in Mahayana literature, or regard Hinayana as a synonym for Theravada. These scholars understand the term to refer to schools of Buddhism that did not accept the teachings of the Mahāyāna sūtras as authentic teachings of the Buddha. At the same time, scholars have objected to the pejorative connotation of the term Hinayana and some scholars do not use it for any school.

</doc>
<doc id="14045" url="https://en.wikipedia.org/wiki?curid=14045" title="Humphrey Bogart">
Humphrey Bogart

Humphrey DeForest Bogart (; December 25, 1899January 14, 1957) was an American screen actor whose performances in iconic 1940s films noir such as "The Maltese Falcon", "Casablanca", and "The Big Sleep" earned him status as a cultural icon.
Bogart began acting in 1921 after a hitch in the U.S. Navy in World War I and little success in various jobs in finance and the production side of the theater. Gradually he became a regular in Broadway shows in the 1920s and 1930s. When the stock market crash of 1929 reduced the demand for plays, Bogart turned to film. His first great success was as Duke Mantee in "The Petrified Forest" (1936), and this led to a period of typecasting as a gangster with films such as "Angels with Dirty Faces" (1938) and B-movies like "The Return of Doctor X" (1939).
Bogart's breakthrough as a leading man came in 1941 with "High Sierra" and "The Maltese Falcon." The next year, his performance in "Casablanca" (1943; Oscar nomination) raised him to the peak of his profession and, at the same time, cemented his trademark film persona, that of the hard-boiled cynic who ultimately shows his noble side. Other successes followed, including "To Have and Have Not" (1944), "The Big Sleep" (1946), "Dark Passage" (1947), and "Key Largo" (1948), all four with his wife Lauren Bacall; "The Treasure of the Sierra Madre" (1948); "In a Lonely Place" (1950); "The African Queen" (1951; Oscar winner); "Sabrina" (1954); and "The Caine Mutiny" (1954; Oscar nomination). His last film was "The Harder They Fall" (1956).
During a film career of almost 30 years, Bogart appeared in more than 75 feature films. In 1999, the American Film Institute ranked Bogart as the greatest male star of Classic American cinema. Over his career, he received three Academy Award nominations for Best Actor, winning one (for "The African Queen").
Early life.
Bogart was born on Christmas Day, 1899, in New York City, the eldest child of Dr. Belmont DeForest Bogart (July 1867, Watkins Glen, New York – September 8, 1934, New York City) and Maud Humphrey (1868–1940). Belmont was the only child of the unhappy marriage of Adam Watkins Bogart, a Canandaigua, New York innkeeper, and his wife, Julia, a wealthy heiress. The name "Bogart" comes from the Dutch surname "Bogaert". Belmont and Maud married in June 1898, he was Presbyterian of English and Dutch descent, she an Episcopalian of English heritage. Young Humphrey was raised in the Episcopal faith, but was non-practicing for most of his adult life.
The precise date of Bogart's birth was long a matter of dispute, but has been cleared up. Warner Bros listed his birthdate as Christmas Day, 1899, throughout his career; but film historian Clifford McCarty later maintained that the Warner publicity department had altered it from January 23, 1900 "...to foster the view that a man born on Christmas Day couldn't really be as villainous as he appeared to be on screen". The "corrected" January birthdate subsequently appeared—and in some cases, remains—in many otherwise authoritative sources. Biographers A.M. Sperber and Eric Lax documented, however, that Bogart always celebrated his birthday on December 25, and consistently listed it as such on official records, such as his marriage license.
Lauren Bacall confirmed in her autobiography that his birthday was always celebrated on Christmas Day, adding that he joked that he was cheated out of a present every year because of it. Sperber and Lax also noted that a birth announcement, printed in the "Ontario County Times" on January 10, 1900, effectively rules out the possibility of a January 23 birthdate; and state and federal census records from 1900 report a Christmas 1899 birthdate as well.
Bogart's father, Belmont, was a cardiopulmonary surgeon. His mother, Maud, was a commercial illustrator who received her art training in New York and France, including study with James McNeill Whistler. Later she became art director of the fashion magazine "The Delineator" and a militant suffragette. She used a drawing of baby Humphrey in a well-known advertising campaign for Mellins Baby Food. In her prime, she made over $50,000 a year, then a vast sum and far more than her husband's $20,000. The Bogarts lived in a fashionable Upper West Side apartment, and had an elegant cottage on a 55-acre estate on Canandaigua Lake in upstate New York. As a youngster, Humphrey's gang of friends at the lake would put on theatricals.
Humphrey had two younger sisters, Frances ("Pat") and Catherine Elizabeth ("Kay"). His parents were busy in their careers and frequently fought. Very formal, they showed little emotion towards their children. Maud told her offspring to call her "Maud" not "Mother", and showed little if any physical affection for them. When pleased she "lapped you on the shoulder, almost the way a man does", Bogart recalled. "I was brought up very unsentimentally but very straightforwardly. A kiss, in our family, was an event. Our mother and father didn't glug over my two sisters and me."
As a boy, Bogart was teased for his curls, tidiness, the "cute" pictures his mother had him pose for, the Little Lord Fauntleroy clothes she dressed him in, and even for the name "Humphrey". From his father, Bogart inherited a tendency to needle, fondness for fishing, lifelong love of boating, and an attraction to strong-willed women.
Bogart attended the private Delancey School until fifth grade, then the prestigious Trinity School. He was an indifferent, sullen student who showed no interest in after-school activities. Later he went to the equally elite boarding school Phillips Academy, where he was admitted based on family connections. His parents hoped he would go on to Yale, but in 1918 Bogart was expelled. Several reasons have been given: one claims that it was for throwing the headmaster (or a groundskeeper) into Rabbit Pond on campus.
Another cites smoking, drinking, poor academic performance, and possibly some inappropriate comments made to the staff. A third has him withdrawn by his father for failing to improve his grades. Whatever caused his premature departure, his parents were deeply dismayed and rued their failed plans for his future.
Navy.
With no viable career options, Bogart followed his passion for the sea and enlisted in the United States Navy in the spring of 1918. He recalled later, "At eighteen, war was great stuff. Paris! Sexy French girls! Hot damn!" Bogart is recorded as a model sailor who spent most of his sea time after the Armistice ferrying troops back from Europe.
It was during his naval stint that Bogart may have received his trademark scar and developed his characteristic lisp, though the actual circumstances are unclear. In one account his lip was cut by shrapnel when his ship, the "", was shelled, although some claim Bogart did not make it to sea until after the Armistice had been signed. Another version, which Bogart's long-time friend, author Nathaniel Benchley, holds to, is that Bogart was injured while taking a prisoner to Portsmouth Naval Prison in Kittery, Maine.
Changing trains in Boston the handcuffed prisoner allegedly asked Bogart for a cigarette, then while Bogart looked for a match, the prisoner smashed him across the mouth with the cuffs, cutting Bogart's lip and fleeing. Recaptured, the prisoner was taken to jail. An alternate version has Bogart struck in the mouth by a handcuff loosened while freeing his charge, the other still around the prisoner's wrist.
By the time Bogart was treated by a doctor, a scar had already formed. David Niven said that when he first asked Bogart about his scar, he said it was caused by a childhood accident. "Goddamn doctor", Bogart later told Niven, "instead of stitching it up, he screwed it up." Niven claims the stories that Bogart got the scar during wartime were made up by the studios to inject glamour.
His post-service physical makes no mention of the lip scar, even though it mentions many smaller scars. When actress Louise Brooks met Bogart in 1924, he had some scar-tissue on his upper lip, which Brooks said that Bogart may have had partially repaired before entering films in 1930. She believed his scar had nothing to do with his distinctive speech pattern, and said his "lip wound gave him no speech impediment, either before or after it was mended. Over the years, Bogart practiced all kinds of lip gymnastics, accompanied by nasal tones, snarls, lisps and slurs. His painful wince, his leer, his fiendish grin were the most accomplished ever seen on film."
Early career.
Bogart returned home to find his father suffering from poor health, his medical practice faltering, and much of the family's wealth lost on bad investments in timber. During his naval days, Bogart's character and values developed independently of family influence, and he began to rebel somewhat against their values. He came to be a liberal who hated pretensions, phonies, and snobs, and at times defied conventional behavior and authority, traits he displayed in both life and the movies. He did not, however, forsake good manners, articulateness, punctuality, modesty, and a dislike of being touched. After his naval service, he worked as a shipper and then bond salesman. He joined the Naval Reserve.
Bogart resumed his friendship with boyhood pal Bill Brady, Jr., whose father had show business connections. Eventually Bogart got an office job working for William A. Brady Sr.'s new company, World Films. Bogart was able to try his hand at screenwriting, directing, and production, but excelled at none. For a while he was stage manager for Brady's daughter Alice's play "A Ruined Lady". A few months later he made his stage debut as a Japanese butler in Alice's 1921 play "Drifting", nervously speaking one line of dialog. Several appearances followed in her subsequent plays.
While Bogart had been raised to believe that acting was beneath a gentleman, he liked the late hours actors kept and enjoyed the attention gotten on stage. He stated, "I was born to be indolent and this was the softest of rackets." He spent a lot of his free time in speakeasies and became a heavy drinker. A barroom brawl during this time joins the list of purported causes of Bogart's lip damage, and coincides better with the Brooks account.
Preferring to learn as he went, Bogart never took acting lessons. He was persistent and worked steadily at his craft, appearing in at least seventeen Broadway productions between 1922 and 1935. He played juveniles or romantic second-leads in drawing room comedies, and is said to have been the first actor to ask "Tennis, anyone?" on stage. Critic Alexander Woollcott wrote of Bogart's early work that he "is what is usually and mercifully described as inadequate." Some reviews were kinder.
Heywood Broun, reviewing "Nerves" wrote, "Humphrey Bogart gives the most effective performance ... both dry and fresh, if that be possible". He played juvenile lead, reporter Gregory Brown, in the comedy "Meet the Wife", written by Lynn Starling, which had a successful run of 232 performances at the Klaw Theatre from November 1923 through July 1924. Bogart loathed these trivial, effeminate parts he had to play early in his career, calling them "White Pants Willie" roles.
Early in his career, while playing double roles in the play "Drifting" at the Playhouse Theatre in 1922, Bogart met actress Helen Menken. They were married on May 20, 1926, at the Gramercy Park Hotel in New York City. Divorced on November 18, 1927, they remained friends. On April 3, 1928, he married Mary Philips, whom he'd met when they appeared in the play "Nerves" during its very brief run at the Comedy Theatre in September 1924, at her mother's apartment in Hartford, Connecticut. She, like Menken, had a fiery temper, and, like every other Bogart spouse, was an actress.
After the stock market crash of 1929, stage production dropped off sharply, and many of the more photogenic actors headed for Hollywood. Bogart's film debut was with Helen Hayes in the 1928 two-reeler "The Dancing Town", of which a complete copy has never been found. He also appeared with Joan Blondell and Ruth Etting in a Vitaphone short, "Broadway's Like That" (1930) which was re-discovered in 1963.
Bogart then signed a contract with Fox Film Corporation for $750 a week. There he met Spencer Tracy, a serious Broadway actor whom Bogart liked and admired, and they became close friends and drinking companions. It was Tracy, in 1930, who first called him "Bogie." Tracy made his film debut in the only film in which he and Bogart appeared together, John Ford's early sound film "Up the River" (1930). Both had major roles as inmates. Tracy received top billing and Bogart's face was featured on the film's posters instead of Tracy's.
Bogart then had a minor supporting role in "Bad Sister" with Bette Davis in 1931. Decades later, Tracy and Bogart planned to make "The Desperate Hours" together, but both sought top billing, so Tracy dropped out and was replaced by Fredric March.
Bogart shuttled back and forth between Hollywood and the New York stage from 1930 to 1935, suffering long periods without work. His parents had separated, his father dying in 1934 in debt, which Bogart eventually paid off. Bogart inherited his father's gold ring which he always wore, even in many of his films. At his father's deathbed, Bogart finally told him how much he loved him. His second marriage was on the rocks, and he was less than happy with his acting career. He became depressed, irritable, and drank heavily.
"The Petrified Forest".
Bogart starred in the Broadway play "Invitation to a Murder" at the Theatre Masque, now the John Golden Theatre, in 1934. The producer Arthur Hopkins heard the play from off-stage and sent for Bogart to play escaped murderer Duke Mantee in Robert E. Sherwood's new play, "The Petrified Forest". Hopkins recalled:
The play had 197 performances at the Broadhurst Theatre in New York in 1935. Leslie Howard, though, was the star. "New York Times" critic Brooks Atkinson said of the play, "a peach ... a roaring Western melodrama ... Humphrey Bogart does the best work of his career as an actor." Bogart said the play "marked my deliverance from the ranks of the sleek, sybaritic, stiff-shirted, swallow-tailed 'smoothies' to which I seemed condemned to life." However, he was still feeling insecure.
Warner Bros. bought the screen rights to "The Petrified Forest". The play seemed perfect for the studio, which was famous for its socially realistic, urban, low-budget action pictures, especially for a public entranced by real-life criminals like John Dillinger (whom Bogart resembled) and Dutch Schultz. Bette Davis and Leslie Howard were cast. Howard, who held production rights, made it clear he wanted Bogart to star with him.
The studio tested several Hollywood veterans for the Duke Mantee role, and chose Edward G. Robinson, who had first-rank star appeal and was due to make a film to fulfill his expensive contract. Bogart cabled news of this to Howard in Scotland, who replied: "Att: Jack Warner Insist Bogart Play Mantee No Bogart No Deal L.H.". When Warner Bros. saw Howard would not budge, they gave in and cast Bogart. Jack Warner, famous for butting heads with his stars, tried to get Bogart to adopt a stage name, but Bogart stubbornly refused.
The film was highly successful, earning $500,000 at the box office, and making Bogart a star. He never forgot Howard's favor, and in 1952 named his only daughter "Leslie Howard Bogart" after Howard, who had died in World War II under mysterious circumstances. Robert E. Sherwood remained a close friend of Bogart's.
Early film career.
The film version of "The Petrified Forest" was released in 1936. Bogart's performance was called "brilliant", "compelling", and "superb." Despite his success in an "A movie," Bogart received a tepid twenty-six week contract at $550 per week and was typecast as a gangster in a series of "B movie" crime dramas. Bogart was proud of his success, but the fact that it came from playing a gangster weighed on him. He once said: "I can't get in a mild discussion without turning it into an argument. There must be something in my tone of voice, or this arrogant face—something that antagonizes everybody. Nobody likes me on sight. I suppose that's why I'm cast as the heavy."
Bogart's roles were not only repetitive, but physically demanding and draining (studios were not yet air-conditioned), and his regimented, tightly scheduled job at Warners was anything but the indolent and "peachy" actor's life he hoped for. However, he was always professional and generally respected by other actors. He used these "B movie" years to start developing his enduring film persona—the wounded, stoical, cynical, charming, vulnerable, self-mocking loner with a code of honor.
In spite of his success, Warner Bros. had no interest in making Bogart a top star. Shooting on a new movie might begin days or only hours after the previous one wrapped. The studio system, then at its most entrenched, restricted actors to their home lot, with only occasional loan-outs. Any actor who refused a role could be suspended without pay. Bogart disliked the roles chosen for him, but he worked steadily. Between 1936 and 1940 he averaged a movie every two months, at times working on two simultaneously.
Amenities at Warners were few compared to the prestigious Metro-Goldwyn-Mayer. Bogart thought that the Warners wardrobe department was cheap, and often wore his own suits in his movies. In "High Sierra", Bogart used his own pet dog Zero to play his character's dog, Pard. Bogart's disputes with Warner Bros. over roles and money were similar to those the studio waged with other high-spirited, less-than-obedient stars such as Bette Davis, James Cagney, Errol Flynn, and Olivia de Havilland.
The leading men ahead of Bogart at Warner Bros. included not only such marquee names as James Cagney and Edward G. Robinson, but also journeymen leads such as Victor McLaglen, George Raft, and Paul Muni. Most of the studio's better movie scripts went to them, leaving Bogart with what was left. He made films like "Racket Busters", "San Quentin", and "You Can't Get Away with Murder". The only substantial leading role he got during this period was in "Dead End" (1937), while loaned to Samuel Goldwyn, where he portrayed a gangster modeled after Baby Face Nelson.
Bogart played violent roles so often that in Nevil Shute's 1939 novel "What Happened to the Corbetts" the protagonist, when asked whether he knows how to operate an automatic weapon, jokes "I've seen Humphrey Bogart with one often enough ...". He did play a variety of interesting supporting roles, such as in "Angels with Dirty Faces" (1938) (in which his character got shot by James Cagney's). Bogart was gunned down on film repeatedly by Cagney and Edward G. Robinson, among others. In "Black Legion" (1937), for a change, he played a good man caught up and destroyed by a racist organization, a movie Graham Greene described as "intelligent and exciting, if rather earnest".
In 1938, Warner Bros. put Bogart in a "hillbilly musical" called "Swing Your Lady" as a wrestling promoter; he later apparently considered this his worst film performance. In 1939, Bogart played a mad scientist in "The Return of Doctor X". He cracked, "If it'd been Jack Warner's blood ... I wouldn't have minded so much. The trouble was they were drinking mine and I was making this stinking movie." During this time his wife Mary had a stage hit in "A Touch of Brimstone" (1935), and refused to give up her Broadway career to go to Hollywood. After the play closed she relented, but insisted on continuing her career and the couple divorced in 1937.
On August 21, 1938, Bogart entered into a disastrous third marriage, with actress Mayo Methot, a lively, friendly woman when sober but paranoid and physical when drunk. She became convinced Bogart was cheating on her. The more the two drifted apart, the more she drank, in her fury throwing plants, crockery, anything close at hand, at him. She set their house on fire, stabbed him with a knife, and slashed her wrists on several occasions. Bogart for his part needled her mercilessly and seemed to enjoy confrontation. Sometimes he turned violent. The press accurately dubbed them "the Battling Bogarts."
"The Bogart-Methot marriage was the sequel to the Civil War," said their friend Julius Epstein. A wag observed that there was "madness in his Methot." During this time, Bogart bought a motor launch, which he named "Sluggy," his nickname for hot-tempered Methot. Despite his proclamations that, "I like a jealous wife," "We get on so well together (because) we don't have illusions about each other," and, "I wouldn't give you two cents for a dame without a temper," it was a highly destructive relationship.
Bogart had a lifelong disgust for the pretentious, fake or phoney. Sensitive yet caustic, he was once again disgusted by the inferior movies he was performing in. He rarely saw his own films and avoided premieres. He even issued phony press releases about his private life to satisfy the curiosity of newspapers and the public. When he thought an actor, director, or a movie studio had done something shoddy, he spoke up about it and was willing to be quoted. He advised Robert Mitchum that the only way to stay alive in Hollywood was to be an "againster." As a result, he was not the most popular of actors, and some in the Hollywood community shunned him privately to avoid trouble with the studios. But the Hollywood press, unaccustomed to candor, was delighted. Bogart once said:
Rise to stardom.
"High Sierra".
"High Sierra", a 1941 film directed by Raoul Walsh, had a screenplay written by Bogart's friend and drinking partner, John Huston, adapted from the novel by W. R. Burnett ("Little Caesar", etc.). Both Paul Muni and George Raft turned down the lead role, giving Bogart the opportunity to play a character of some depth, although legendary director Walsh initially fought the casting of supporting player Bogart as a leading man, much preferring Raft for the part. The film was Bogart's last major film playing a gangster (only a supporting role in 1942's "The Big Shot" followed). Bogart worked well with Ida Lupino, and her relationship with him was close, provoking jealousy from Bogart's wife, Mayo.
The film cemented a strong personal and professional connection between Bogart and Huston. Bogart admired and somewhat envied Huston for his skill as a writer. Though a poor student, Bogart was a lifelong reader. He could quote Plato, Pope, Ralph Waldo Emerson, and over a thousand lines of Shakespeare. He subscribed to the "Harvard Law Review". He admired writers, and some of his best friends were screenwriters, including Louis Bromfield, Nathaniel Benchley, and Nunnally Johnson. Bogart enjoyed intense, provocative conversation and stiff drinks, as did Huston. Both were rebellious and liked to play childish pranks. Huston was reported to be easily bored during production, and admired Bogart (also bored easily off camera) not just for his acting talent but for his intense concentration on the set.
"The Maltese Falcon".
Now regarded as a classic film noir, the "The Maltese Falcon" (1941) was John Huston's directorial debut. Originally a novel written by Dashiell Hammett, it was first published in the pulp magazine "Black Mask" in 1929, and had also served as the basis of two other movie versions including "Satan Met a Lady" (1936) starring Bette Davis. Producer Hal Wallis initially offered the leading man role to George Raft, a more established box office name than Bogart whose contract stipulated he did not have to appear in remakes. Fearing it would be no more than a cleaned-up version of the pre-Production Code "The Maltese Falcon" (1931), Raft turned it down in order to make "Manpower" with director Raoul Walsh and cast members Edward G. Robinson and Marlene Dietrich. Eagerly, Huston accepted Bogart as his Sam Spade.
Complementing Bogart were co-stars Sydney Greenstreet, Peter Lorre, Elisha Cook, Jr., and Mary Astor as the treacherous female foil. Bogart's sharp timing and facial expressions were praised by the cast and director as vital to the quick action and rapid-fire dialogue. The film was a huge hit in theaters and a major triumph for Huston. Bogart was unusually happy with it, remarking, "it is practically a masterpiece. I don't have many things I'm proud of ... but that's one".
"Casablanca".
Bogart gained his first real romantic lead in 1942's "Casablanca", playing Rick Blaine, a hard-pressed expatriate nightclub owner hiding from a shady past while negotiating a fine line among Nazis, the French underground, the Vichy prefect and unresolved feelings for his ex-girlfriend. The film was directed by Michael Curtiz and produced by Hal Wallis, and featured Ingrid Bergman, Claude Rains, Sydney Greenstreet, Paul Henreid, Conrad Veidt, Peter Lorre and Dooley Wilson. An avid chess player, Bogart reportedly had the idea that Rick Blaine be portrayed as one, a metaphor for the sparring relationship he maintained with friends, enemies, and tenuous allies. In real life Bogart played tournament level chess one division below master, often enjoying games with crew members and cast, but finding his better in the superior Paul Henreid.
The on-screen magic of Bogart and Bergman was the result of two actors working at their best, not any real-life sparks, though Bogart's perennially jealous wife assumed otherwise. Off the set, the co-stars hardly spoke. Bergman, who had a reputation for affairs with her leading men, later said of Bogart, "I kissed him but I never knew him." Because Bergman was taller, Bogart had blocks attached to his shoes in certain scenes.
"Casablanca" won the 1943 Academy Award for Best Picture. Bogart was nominated for Best Actor in a Leading Role, but lost to Paul Lukas for his performance in "Watch on the Rhine". The film vaulted Bogart from fourth place to first in the studio's roster, finally overtaking James Cagney. By 1946 he'd more than double his annual salary to over $460,000, making him the highest-paid actor in the world.
World War II.
During part of 1943 and 1944, Bogart went on USO and War Bond tours accompanied by Methot, enduring arduous travels to Italy and North Africa, including Casablanca.
Bogart and Bacall.
"To Have and Have Not".
Bogart met Lauren Bacall while filming "To Have and Have Not" (1944), a loose adaptation of the Ernest Hemingway novel. The movie has many similarities with "Casablanca"—the same enemies, the same kind of hero, even a piano player sidekick (played by Hoagy Carmichael). When they met, Bacall was 19 and Bogart 44. He nicknamed her "Baby." She had been a model since 16 and had acted in two failed plays. Bogart was drawn to Bacall's high cheekbones, green eyes, tawny blond hair, and lean body, as well as her poise and earthy, outspoken honesty. Reportedly he said, "I just saw your test. We'll have a lot of fun together". Their physical and emotional rapport was very strong from the start, their age difference and disparity in acting experience allowing the dynamic of a mentor-student relationship to emerge. Quite contrary to Hollywood norm, their affair was Bogart's first with a leading lady. He was still married and his early meetings with Bacall were discreet and brief, their separations bridged by ardent love letters. The relationship made it much easier for the newcomer to make her first film, and Bogart did his best to put her at ease with jokes and quiet coaching. He let her steal scenes and even encouraged it. Howard Hawks, for his part, also did his best to boost her performance and highlight her role, and found Bogart easy to direct.
Hawks at some point began to disapprove of the pair. He considered himself Bacall's protector and mentor, and Bogart was usurping that role. Married, and not usually drawn to his starlets, he too fell for Bacall, telling her she meant nothing to Bogart and even threatening to send her to Monogram, the worst studio in Hollywood. Bogart calmed her down and then went after Hawks. Jack Warner settled the dispute and filming resumed. Hawks said of Bacall: "Bogie fell in love with the character she played, so she had to keep playing it the rest of her life."
"The Big Sleep".
Just months after wrapping the film, Bogart and Bacall were reunited for an encore, the film noir "The Big Sleep", based on the novel by Raymond Chandler, again with script help from William Faulkner. Chandler thoroughly admired Bogart's performance: "Bogart can be tough without a gun. Also, he has a sense of humor that contains that grating undertone of contempt." The film was completed and slated for release in 1945, then withdrawn and substantially re-edited to add new, juiced-up scenes exploiting both the box office chemistry that shone between Bogart and Bacall in "To Have and Have Not", and the notoriety of their personal relationship.
At director Howard Hawks' urging production partner Charles K. Feldman agreed to Bacall's scenes being re-written to heighten the 'insolent' quality that had intrigued critics and audiences in that film. By chance, a 35-mm nitrate composite master positive (fine grain) of the 1945 version survived. The UCLA Film Archive, in association with Turner Entertainment and with funding provided by Hugh Hefner, restored and released it in 1996.
Throughout filming Bogart was still torn between his new love and his sense of duty to his marriage. The mood on the set was tense, the actors both emotionally exhausted as Bogart tried to find a way out of his dilemma. The dialogue, especially in the newly shot scenes, was full of sexual innuendo supplied by Hawks, and Bogart proves convincing and enduring as private detective Philip Marlowe. In the end, the film was successful, though some critics found the plot confusing and overcomplicated. Reportedly a bemused Chandler himself could not answer baffled screenwriters' question over who killed the limousine driver early in the story.
Marriage.
Bogart filed for divorce from Methot in February 1945. He and Bacall married in a small ceremony at the country home of Bogart's close friend, Pulitzer Prize-winning author Louis Bromfield, at Malabar Farm near Lucas, Ohio, on May 21, 1945.
Bogart and Bacall moved into a $160,000 ($ today) white brick mansion in an exclusive neighborhood in Los Angeles's Holmby Hills. The marriage proved a happy one, though there were tensions due to their differences. Bogart's drinking sometimes inflamed tensions. He was a homebody and she liked nightlife; he loved the sea, which made her seasick.
In California in 1945, Bogart bought a sailing yacht, the "Santana", from actor Dick Powell. The sea was his sanctuary, spending about thirty weekends a year on the water, with a particular fondness for sailing around Catalina Island. He once said, "An actor needs something to stabilize his personality, something to nail down what he really is, not what he is currently pretending to be." He also joined the Coast Guard Temporary Reserve offering the use of his own yacht, Santana, for Coast Guard use. It was rumored Bogart attempted to enlist but was turned down because of his age.
"Dark Passage" and "Key Largo".
The suspenseful "Dark Passage" (1947) was Bogart and Bacall's next pairing.
Its first third is shot from the Bogart's character's point of view, with the camera seeing what he sees. After his plastic surgery, the rest of the movie is shot normally, with Bogart intent on finding the real murderer in a crime he was blamed for and sentenced to prison.
The couple next starred in the future classic, "Key Largo". Directed by John Huston, the film highlighted Edward G. Robinson as gangster "Johnny Rocco," a seething older synthesis of many of his vicious early bad guy roles. The characters are trapped during a spectacular hurricane in a hotel owned by Bacall's screen father-in-law, played by Lionel Barrymore. Claire Trevor won an Academy Award for Best Supporting Actress for her heart-wrenching performance as Rocco's physically abused alcoholic girlfriend. Though Robinson had always had top billing over Bogart in their previous films together, this time Robinson's name appears to the right of Bogart's, but placed a little higher on the posters and in the film's opening credits, to signify Robinson's near-equal status. Robinson's image was also markedly larger and centered on the original poster, with Bogart relegated to the background.
In the film's trailer, Bogart is repeatedly mentioned first, but Robinson's name is listed above Bogart's in a cast list at the trailer's end. Robinson's role is evocative of Duke Mantee in "The Petrified Forest" (1936), a Bogart leading man breakthrough the studio had originally earmarked for Robinson.
Children.
Bogart became a first-time father at age 49 when Bacall gave birth to Stephen Humphrey Bogart on January 6, 1949, during the filming of "Tokyo Joe". The name was drawn from Bogart's character's nickname in "To Have and Have Not", "Steve". Stephen would go on to become an author and biographer, later hosting a television special about his father on Turner Classic Movies. Three years later the couple's daughter, Leslie Howard Bogart, would draw her name from Bogart's friend and "The Petrified Forest" co-star, British actor Leslie Howard.
Later career.
The enormous success of "Casablanca" redefined Bogart's career. For the first time, Bogart could be cast successfully as both a tough, strong man and vulnerable love interest. Despite his elevated standing, he did not yet have a contractual right of script refusal. When he got weak scripts he simply dug in his heels and locked horns again with the front office, as he did on the film "Conflict" (1945). Though he submitted to Jack Warner on it, he successfully turned down "God is My Co-Pilot" (1945).
"The Treasure of the Sierra Madre".
Riding high in 1947 with a new contract which provided limited script refusal and the right to form his own production company, Bogart reunited with John Huston for "The Treasure of the Sierra Madre", a stark tale of greed played out by three gold prospectors in Mexico. Without either a love interest or happy ending it was deemed a risky project. Bogart later said of co-star (and John Huston's father) Walter Huston, "He's probably the only performer in Hollywood to whom I'd gladly lost a scene".
The film was shot in the heat of summer for greater realism and atmosphere, proving grueling to make. James Agee wrote, "Bogart does a wonderful job with this character ... miles ahead of the very good work he has done before". John Huston won the Academy Award for direction and screenplay and his father won Best Supporting Actor, but the film had mediocre box office results. Bogart complained, "An intelligent script, beautifully directed—something different—and the public turned a cold shoulder on it".
House Un-American Activities Committee.
Bogart, a liberal Democrat, organized a delegation to Washington, D.C., called the Committee for the First Amendment, against what he perceived to be the House Un-American Activities Committee's harassment of Hollywood screenwriters and actors. He subsequently wrote an article "I'm No Communist" in the March 1948 edition of "Photoplay" magazine in which he distanced himself from The Hollywood Ten to counter the negative publicity resulting from his appearance. Bogart wrote: "The ten men cited for contempt by the House Un-American Activities Committee were not defended by us."
Santana Productions.
In addition to being offered better, more diverse roles, Bogart started his own production company in 1948, Santana Productions, named after his sailing yacht (which also loaned her name to the cabin cruiser featured in the climax of that year's smash, "Key Largo"). Earning the right to create his own production company had left Warner Bros. head Jack Warner furious, and afraid other stars would do the same and further erode the major studios' power. In addition to the pressure they were bearing from freelancing actors like Bogart, James Stewart, Henry Fonda and others, they were beginning to buckle from the eroding impact of television and enforcement of anti-trust laws breaking up theater chains. Bogart performed in his final films for Warners, "Chain Lightning", released early in 1950, and "The Enforcer", early in 1951.
Bogart's Santana Productions released its films through Columbia Pictures. Without letting up, Bogart starred in "Knock on Any Door" (1949), "Tokyo Joe" (1949), "In a Lonely Place" (1950), "Sirocco" (1951) and "Beat the Devil" (1954). Santana made two other films without him: "And Baby Makes Three" (1949) and "The Family Secret" (1951).
While the majority lost money at the box office, ultimately forcing Santana's sale, at least two are well remembered today: "In a Lonely Place" is considered by many a high point in film noir. Bogart plays embittered writer Dixon Steele, whose history of violence lands him as top suspect in a murder case. At the same time he falls in love with an alluring but failed actress played by Gloria Grahame. It is considered among his best performances, and many Bogart biographers and actress/writer Louise Brooks feel the role is the closest to the real Bogart of any he played. She wrote that the film "gave him a role that he could play with complexity, because the film character's pride in his art, his selfishness, drunkenness, lack of energy stabbed with lightning strokes of violence were shared by the real Bogart". The character even mimics some of Bogart's personal habits, including twice ordering Bogart's favorite meal of ham and eggs.
Something of a parody of "The Maltese Falcon", "Beat the Devil" (1953), was Bogart's last film with his close friend and favorite director John Huston. Co-written by Truman Capote, the eccentrically filmed tale follows an amoral group of rogues chasing an unattainable treasure. It enjoys a cult following.
Bogart sold his interest in Santana to Columbia for over $1 million in 1955.
"The African Queen".
Working outside of his own Santana Productions, Bogart starred with Katharine Hepburn in the John Huston directed "The African Queen" in 1951. The C.S. Forester novel on which it was based was overlooked and left undeveloped for fifteen years until producer Sam Spiegel and Huston bought the rights. Spiegel sent Katharine Hepburn the book and she suggested Bogart for the male lead, firmly believing that "he was the only man who could have played that part". Huston's love of adventure, deep, longstanding friendship–and success–with Bogart, and a chance to work with Hepburn, convinced the actor to leave the comfortable confines of Hollywood for a difficult shoot on location in the Belgian Congo in Africa. Bogart was to get 30 percent of the profits and Hepburn 10 percent, plus a relatively small salary for both. The stars met up in London and announced the happy prospect of working together.
Bacall came for the four-month-plus duration, leaving their young child to be cared for in L.A. The Bogarts started the trip with a junket through Europe, including a visit with Pope Pius XII. Later, the glamor would be gone and Bacall would make herself useful as a cook, nurse and clothes washer, earning her husband's praise: "I don't know what we'd have done without her. She Luxed my undies in darkest Africa". Just about everyone in the cast came down with dysentery except Bogart and Huston, who subsisted on canned food and alcohol. Bogart explained: "All I ate was baked beans, canned asparagus and Scotch whisky. Whenever a fly bit Huston or me, it dropped dead." Hepburn, a teetotaler in and out of character, fared worse in the difficult conditions, losing weight and at one point falling very ill. Bogart resisted Huston's insistence on using real leeches in a key scene where Charlie has to drag his tramp steamer through an infested marsh, until reasonable fakes were employed. In the end, the crew overcame illness, soldier ant invasions, leaking boats, poor food, attacking hippos, poor water filters, fierce heat, isolation, and a boat fire to complete a memorable film. Despite the discomfort of jumping from the boat into swamps, rivers and marshes the film apparently rekindled Bogart's early love of boats. On his return to California he bought a classic mahogany Hacker-Craft runabout, which he kept until his death.
The role of cantankerous skipper Charlie Allnutt won Bogart his only Academy Award in three nominations, for Best Actor in a Leading Role in 1951. Bogart considered his performance to be the best of his film career. He had vowed to friends that if he won, his speech would break the convention of thanking everyone in sight. He advised Claire Trevor, when she had been nominated for "Key Largo", to "just say you did it all yourself and don't thank anyone". But when Bogart won the Academy Award, which he truly coveted despite his well-advertised disdain for Hollywood, he said "It's a long way from the Belgian Congo to the stage of this theatre. It's nicer to be here. Thank you very much ... No one does it alone. As in tennis, you need a good opponent or partner to bring out the best in you. John and Katie helped me to be where I am now". Despite the thrilling win and the recognition, Bogart later commented, "The way to survive an Oscar is never to try to win another one ... too many stars ... win it and then figure they have to top themselves ... they become afraid to take chances. The result: A lot of dull performances in dull pictures".
"The African Queen" was the first Technicolor film in which Bogart appeared. He appeared in relatively few color films of any kind during the rest of his career, which continued for another five years.
Final roles.
Just three years after his Best Actor triumph in "African Queen", Bogart dropped his asking price to get the role of Captain Queeg in Edward Dmytryk's 1954 drama "The Caine Mutiny". Though he griped with some of his old bitterness about having to do so, he delivered a strong performance in the lead, earning him his final Oscar nomination as well as being the subject of the cover story in the June 7, 1954 issue of TIME. Yet for all his success, Bogart was still his melancholy old self, grumbling and feuding with the studio, while his health was beginning to deteriorate. The character of Queeg mirrored in some ways those Bogart had played in "The Maltese Falcon", "Casablanca" and "The Big Sleep"–the wary loner who trusts no one—but without either the warmth or humor of those roles. Like his portrayal of Fred C. Dobbs in "The Treasure of the Sierra Madre", Bogart played a paranoid, self-pitying character whose small-mindedness eventually destroyed him. Three months before the film's release, Bogart appeared as Queeg on the cover of "TIME" magazine, while on Broadway Henry Fonda was starring in the stage version (in a different role), both of which generated strong publicity for the film.
In "Sabrina", Billy Wilder wished to cast Cary Grant as the older male lead. Unable, he chose Bogart to play the elder, conservative brother who competes with his younger playboy sibling (William Holden) for the affection of the Cinderella-like Sabrina (Audrey Hepburn). Bogart was lukewarm about the part, but agreed to it on a handshake with Wilder, sans finished script but with the director's assurances he would take good care of Bogart during the filming. Nevertheless, Bogart got on poorly with his director and co-stars. He complained about the script and its last-minute drafting and delivery, and accused Wilder of favoring Hepburn and Holden on and off the set. At the root was Wilder being the opposite of Bogart's ideal director, John Huston, in both style and personality. Bogart groused to the press that Wilder was "overbearing" and "is the kind of Prussian German with a riding crop. He is the type of director I don't like to work with ... the picture is a crock of crap. I got sick and tired of who gets Sabrina." Wilder later claimed, "We parted as enemies but finally made up." Despite the acrimony, the film was successful. "The New York Times" crowed that Bogart was "incredibly adroit ... the skill with which this old rock-ribbed actor blends the gags and such duplicities with a manly manner of melting is one of the incalculable joys of the show."
"The Barefoot Contessa", directed by Joseph Mankiewicz, was filmed in Rome, and released in 1954. In this Hollywood back-story Bogart is again a broken-down man, the cynical director-narrator who saves his career by making a star of a flamenco dancer modeled on real life movie sex goddess Rita Hayworth. Bogart was uneasy with Ava Gardner in the female lead, as she had just split from close "Rat Pack" buddy Frank Sinatra and was carrying on an affair with bullfighter Luis Miguel Dominguín. Bogart told her, "Half the world's female population would throw themselves at Frank's feet and here you are flouncing around with guys who wear capes and little ballerina slippers." He was also annoyed by her inexperienced performance. Later, Gardner credited Bogart with helping her both on and offscreen. Bogart's performance was generally praised as the strongest part of the film. During the filming, while Bacall was home, Bogart resumed his discreet affair with Verita Peterson, his long-time studio assistant, whom he took sailing and enjoyed drinking with. When his wife suddenly arrived on the scene discovering them together, she took it quite well, extracting an expensive shopping spree from her husband, the three traveling together after the shooting.
Bogart could be generous with actors, particularly those who were blacklisted, down on their luck, or having personal problems. During the filming of the Edward Dmytryk directed "The Left Hand of God" (1955), he noticed his co-star Gene Tierney having a hard time remembering her lines and behaving oddly. He coached Tierney, feeding her lines. He was familiar with mental illness from his sister's bouts of depression, and encouraged Tierney to seek treatment. He also stood behind Joan Bennett and insisted on her as his co-star in Michael Curtiz's "We're No Angels" when an ugly public scandal made her persona non grata with Jack Warner.
Bogart rounded out 1955 with "The Desperate Hours", directed by William Wyler. Mark Robson's "The Harder They Fall" (1956) was his last film.
Television and radio.
While Bogart rarely performed on television, he and Bacall appeared on Edward R. Murrow's "Person to Person" in which they disagreed in answering every question. Bogart was also featured on "The Jack Benny Show". The surviving kinescope of the live telecast captures him in his only TV sketch comedy outing.
Bogart and Bacall also worked together on an early color telecast in 1955, an NBC adaptation of "The Petrified Forest" for "Producers' Showcase", with Bogart receiving top billing and Henry Fonda playing Leslie Howard's role; a black and white kinescope of the live telecast has also survived.
Bogart performed radio adaptations of some of his best known films, such as "Casablanca" and "The Maltese Falcon". He also recorded a radio series called "Bold Venture" with Lauren Bacall.
In 1995 newly developed digital technology allowed Bogart's image to be inserted in the "Tales from the Crypt" television episode "You, Murderer" as one of its many "Casablanca" references. The "Ingrid Bergman" character was played by her daughter Isabella Rossellini.
Personal life.
The Rat Pack.
Bogart was a founding member and original leader of the so-called Hollywood Rat Pack. In the spring of 1955, after a long party in Las Vegas attended by Frank Sinatra, Judy Garland, her husband Sid Luft, Mike Romanoff and wife Gloria, David Niven, Angie Dickinson and others, Lauren Bacall surveyed the wreckage and declared, "You look like a goddamn rat pack."
The name stuck and was made official at Romanoff's in Beverly Hills. Sinatra was tabbed Pack Leader, Bacall Den Mother, Bogie Director of Public Relations, and Sid Luft Acting Cage Manager. When asked by columnist Earl Wilson what the group's purpose was, Bacall stated "to drink a lot of bourbon and stay up late."
Death.
Once, after signing a long-term deal with Warner Bros., Bogart had predicted with glee that his teeth and hair would fall out before the contract ended. By the mid-1950s, well established as an independent producer, the sometime actor's health was failing. In the wake of Santana Productions he had formed a new company and had anxious plans for a film "Melville Goodwin, U.S.A.", in which he would play a general and Bacall a press magnate. However, his persistent cough and difficulty eating became too serious to ignore and he dropped the project.
Bogart, a heavy smoker and drinker, had developed cancer of the esophagus. He almost never spoke of his failing health and refused to see a doctor until January 1956. A diagnosis was made several weeks later, but by then removal of his esophagus, two lymph nodes, and a rib on March 1, 1956, was too late to halt the disease, even with chemotherapy. He underwent corrective surgery in November 1956 after the cancer had spread. With time, he grew too weak to walk up and down stairs, valiantly fighting the pain yet still able to joke: "Put me in the dumbwaiter and I'll ride down to the first floor in style." It was then altered to accommodate his wheelchair. Frank Sinatra was a frequent visitor, as were Katharine Hepburn and Spencer Tracy. In an interview, Hepburn described the last time she and Tracy saw their dear friend, on January 13, 1957:
Bogart fell into a coma and died in his bed the next day. He had just turned 57 twenty days prior and weighed only 80 pounds (36 kg). His simple funeral was held at All Saints Episcopal Church, with musical selections from favorite composers Johann Sebastian Bach and Claude Debussy. The ceremony was attended by some of Hollywood's biggest stars, including Hepburn, Tracy, Judy Garland, David Niven, Ronald Reagan, James Mason, Bette Davis, Danny Kaye, Joan Fontaine, Marlene Dietrich, James Cagney, Errol Flynn, Gregory Peck and Gary Cooper, as well as Billy Wilder and Jack Warner. Bacall had asked Tracy to give the eulogy, but he was too upset, so John Huston spoke instead. He reminded the gathered mourners that while Bogart's life had ended far too soon, it had been a rich one:
Bogart's cremated remains were interred in Forest Lawn Memorial Park Cemetery, Glendale, California in the Garden of Memory, Columbarium of Eternal Light, Garden Niche 647. He was buried with a small, gold whistle once part of a charm bracelet he had given to Lauren Bacall before they married. On it was inscribed an allusion to a line from their first movie together, in 1944, "To Have and Have Not", where Bacall had said to him shortly after their first meeting: "You know how to whistle don't you Steve? You just put your lips together and blow". The inscription read: "If you want anything, just whistle."
The probate value of Bogart's estate was $910,146 gross and $737,668 net ($ million and $ million today).
Legacy and tributes.
After his death, a "Bogie Cult" formed at the Brattle Theatre in Cambridge, Massachusetts, as well as Greenwich Village, Manhattan, New York, and in France, which contributed to his spike in popularity in the late 1950s and 1960s. In 1997, "Entertainment Weekly" magazine named Bogart the number one movie legend of all time. In 1999, the American Film Institute ranked him the Greatest Male Star of Classic Hollywood.
Jean-Luc Godard's "Breathless" (1960) was the first film to pay tribute to Bogart. Later, in Woody Allen's comic paean to Bogart, "Play It Again, Sam" (1972), Bogart's ghost comes to the aid of Allen's bumbling character, a movie critic with women troubles whose "sex life has turned into the 'Petrified Forest'".
Awards and honors.
On August 21, 1946, Bogart was honored in a ceremony at Grauman's Chinese Theater to record his hand and footprints in cement. On February 8, 1960, he was posthumously given a star on the Hollywood Walk of Fame at 6322 Hollywood Boulevard. During his career, Bogart was nominated for several awards including the BAFTA award for best foreign actor in 1952 for "The African Queen" and three Academy Awards.
In 1997, the United States Postal Service honored Bogart with a stamp bearing his image in its "Legends of Hollywood" series as the third figure to be recognized. At a formal ceremony attended by Lauren Bacall, and the Bogart children, Stephen and Leslie, Tirso del Junco, the chairman of the governing board of the USPS, provided an eloquent tribute:
"Today, we mark another chapter in the Bogart legacy. With an image that is small and yet as powerful as the ones he left in celluloid, we will begin today to bring his artistry, his power, his unique star quality, to the messages that travel the world."
On June 24, 2006, a section of 103rd Street, between Broadway and West End Avenue, in New York City was renamed "Humphrey Bogart Place." Lauren Bacall and her son Stephen Bogart were present at the commemorative event. "Bogie would never have believed it," Lauren Bacall expressed to the assembled group of city officials and onlookers in attendance.
In popular culture.
Humphrey Bogart's life has inspired writers and others:
Quotations.
Bogart is credited with five of the American Film Institute's top 100 quotations in American cinema, the most by any actor:
Bogart is also credited with one of the top movie misquotations, "Play it again, Sam". In "Casablanca", neither his Rick Blaine character nor anyone else says the line, although it is widely credited to him and is the verbatim title of a Woody Allen tribute movie.
When Blaine's former love, Ilsa (Ingrid Bergman), first enters his "Café Americain", she spots Sam, the piano player (Dooley Wilson), and asks him to "Play it once, Sam, for old times' sake." When he feigns ignorance, she persists, "Play it, Sam. Play 'As Time Goes By.'" Later that night, alone with Sam, Rick demands, "You played it for her - you can play it for me." Sam once again resists, prompting Blaine to shout: "If she can stand it, I can! Play it!"

</doc>
<doc id="14051" url="https://en.wikipedia.org/wiki?curid=14051" title="History painting">
History painting

History painting is a genre in painting defined by its subject matter rather than artistic style. History paintings usually depict a moment in a narrative story, rather than a specific and static subject, as in a portrait. The term is derived from the wider senses of the word "historia" in Latin and Italian, and essentially means "story painting." In modern English, historical painting is sometimes used to describe the painting of scenes from history in its narrower sense (excluding religious, mythological and allegorical subjects, which are included in the broader term history painting), especially for 19th-century art. History paintings almost always contain a number of figures, often a large number. The genre includes depictions of moments in religious narratives, above all the "Life of Christ", as well as narrative scenes from mythology, and also allegorical scenes. These groups were for long the most frequently painted; works such as Michelangelo's Sistine Chapel ceiling are therefore history paintings, as are most very large paintings before the 19th century. The term covers large paintings in oil on canvas or fresco produced between the Renaissance and the late 19th century, after which the term is generally not used even for the many works that still meet the basic definition.
History painting may be used interchangeably with historical painting, and was especially so used before the 20th century. Where a distinction is made "historical painting" is the painting of scenes from secular history, whether specific episodes or generalized scenes. In the 19th century historical painting in this sense became a distinct genre. In phrases such as "historical painting materials", "historical" means in use before about 1900, or some earlier date.
History paintings were traditionally regarded as the highest form of Western painting, occupying the most prestigious place in the hierarchy of genres, and considered the equivalent to the epic in literature. In his "De Pictura" of 1436, Leon Battista Alberti had argued that multi-figure history painting was the noblest form of art, as being the most difficult, which required mastery of all the others, because it was a visual form of history, and because it had the greatest potential to move the viewer. He placed emphasis on the ability to depict the interactions between the figures by gesture and expression.
This view remained general until the 19th century, when artistic movements began to struggle against the establishment institutions of academic art, which continued to adhere to it. At the same time there was from the latter part of the 18th century an increased interest in depicting in the form of history painting moments of drama from recent or contemporary history, which had long largely been confined to battle-scenes and scenes of formal surrenders and the like. Scenes from ancient history had been popular in the early Renaissance, and once again became common in the Baroque and Rococo periods, and still more so with the rise of Neoclassicism. In some 19th or 20th century contexts, the term may refer specifically to paintings of scenes from secular history, rather than those from religious narratives, literature or mythology.
Development.
The term is generally not used in art history in speaking of medieval painting, although the Western tradition was developing in large altarpieces, fresco cycles, and other works, as well as miniatures in illuminated manuscripts. It comes to the fore in Italian Renaissance painting, where a series of increasingly ambitious works were produced, many still religious, but several, especially in Florence, which did actually feature near-contemporary historical scenes such as the set of three huge canvases on "The Battle of San Romano" by Paolo Uccello, the abortive "Battle of Cascina" by Michelangelo and the "Battle of Anghiari" by Leonardo da Vinci, neither of which were completed. Scenes from ancient history and mythology were also popular. Writers such as Alberti and the following century Giorgio Vasari in his "Lives of the Artists", followed public and artistic opinion in judging the best painters above all on their production of large works of history painting (though in fact the only modern (post-classical) work described in "De Pictura" is Giotto's huge "Navicella" in mosaic). Artists continued for centuries to strive to make their reputation by producing such works, often neglecting genres to which their talents were better suited.
There was some objection to the term, as many writers preferred terms such as "poetic painting" ("poesia"), or wanted to make a distinction between the "true" "istoria", covering history including biblical and religious scenes, and the "fabula", covering pagan myth, allegory, and scenes from fiction, which could not be regarded as true. The large works of Raphael were long considered, with those of Michelangelo, as the finest models for the genre.
In the Raphael Rooms in the Vatican Palace, allegories and historical scenes are mixed together, and the Raphael Cartoons show scenes from the Gospels, all in the Grand Manner that from the High Renaissance became associated with, and often expected in, history painting. In the Late Renaissance and Baroque the painting of actual history tended to degenerate into panoramic battle-scenes with the victorious monarch or general perched on a horse accompanied with his retinue, or formal scenes of ceremonies, although some artists managed to make a masterpiece from such unpromising material, as Velázquez did with his "The Surrender of Breda".
An influential formulation of the hierarchy of genres, confirming the history painting at the top, was made in 1667 by André Félibien, a historiographer, architect and theoretician of French classicism became the classic statement of the theory for the 18th century:Celui qui fait parfaitement des païsages est au-dessus d'un autre qui ne fait que des fruits, des fleurs ou des coquilles. Celui qui peint des animaux vivants est plus estimable que ceux qui ne représentent que des choses mortes & sans mouvement ; & comme la figure de l'homme est le plus parfait ouvrage de Dieu sur la Terre, il est certain aussi que celui qui se rend l'imitateur de Dieu en peignant des figures humaines, est beaucoup plus excellent que tous les autres ... un Peintre qui ne fait que des portraits, n'a pas encore cette haute perfection de l'Art, & ne peut prétendre à l'honneur que reçoivent les plus sçavans. Il faut pour cela passer d'une seule figure à la représentation de plusieurs ensemble ; il faut traiter l'histoire & la fable ; il faut représenter de grandes actions comme les historiens, ou des sujets agréables comme les Poëtes ; & montant encore plus haut, il faut par des compositions allégoriques, sçavoir couvrir sous le voile de la fable les vertus des grands hommes, & les mystères les plus relevez.
He who produces perfect landscapes is above another who only produces fruit, flowers or seashells. He who paints living animals is more than those who only represent dead things without movement, and as man is the most perfect work of God on the earth, it is also certain that he who becomes an imitator of God in representing human figures, is much more excellent than all the others ... a painter who only does portraits still does not have the highest perfection of his art, and cannot expect the honour due to the most skilled. For that he must pass from representing a single figure to several together; history and myth must be depicted; great events must be represented as by historians, or like the poets, subjects that will please, and climbing still higher, he must have the skill to cover under the veil of myth the virtues of great men in allegories, and the mysteries they reveal".
By the late 18th century, with both religious and mytholological painting in decline, there was an increased demand for paintings of scenes from history, including contemporary history. This was in part driven by the changing audience for ambitious paintings, which now increasingly made their reputation in public exhibitions rather than by impressing the owners of and visitors to palaces and public buildings. Classical history remained popular, but scenes from national histories were often the best-received. From 1760 onwards, the Society of Artists of Great Britain, the first body to organize regular exhibitions in London, awarded two generous prizes each year to paintings of subjects from British history. 
The unheroic nature of modern dress was regarded as a serious difficulty. When, in 1770, Benjamin West proposed to paint "The Death of General Wolfe" in contemporary dress, he was firmly instructed to use classical costume by many people. He ignored these comments and showed the scene in modern dress. Although George III refused to purchase the work, West succeeded both in overcoming his critics' objections and inaugurating a more historically accurate style in such paintings. Other artists depicted scenes, regardless of when they occurred, in classical dress and for a long time, especially during the French Revolution, history painting often focused on depictions of the heroic male nude.
The large production, using the finest French artists, of propaganda paintings glorifying the exploits of Napoleon, were matched by works, showing both victories and losses, from the anti-Napoleonic alliance by artists such as Goya and J.M.W. Turner. Théodore Géricault's "The Raft of the Medusa" (1818–1819) was a sensation, appearing to update the history painting for the 19th century, and showing anonymous figures famous only for being victims of what was then a famous and controversial disaster at sea. Conveniently their clothes had been worn away to classical-seeming rags by the point the painting depicts. At the same time the demand for traditional large religious history paintings very largely fell away.
In the mid-nineteenth century there arose a style known as historicism, which marked a formal imitation of historical styles and/or artists. Another development in the nineteenth century was the treatment of historical subjects, often on a large scale, with the values of genre painting, the depiction of scenes of everyday life, and anecdote. Grand depictions of events of great public importance were supplemented with scenes depicting more personal incidents in the lives of the great, or of scenes centred on unnamed figures involved in historical events, as in the Troubadour style. At the same time scenes of ordinary life with moral, political or satirical content became often the main vehicle for expressive interplay between figures in painting, whether given a modern or historical setting.
By the later 19th century, history painting was often explicitly rejected by avant-garde movements such as the Impressionists (except for Édouard Manet) and the Symbolists, and according to one recent writer "Modernism was to a considerable extent built upon the rejection of History Painting... All other genres are deemed capable of entering, in one form or another, the 'pantheon' of modernity considered, but History Painting is excluded".
History painting and historical painting.
The terms.
Initially, "history painting" and "historical painting" were used interchangeably in English, as when Sir Joshua Reynolds in his fourth "Discourse" uses both indiscriminately to cover "history painting", while saying "...it ought to be called poetical, as in reality it is", reflecting the French term "peinture historique", one equivalent of "history painting". The terms began to separate in the 19th century, with "historical painting" becoming a sub-group of "history painting" restricted to subjects taken from history in its normal sense. In 1853 John Ruskin asked his audience: "What do you at present "mean" by historical painting? Now-a-days it means the endeavour, by the power of imagination, to portray some historical event of past days." So for example Harold Wethey's three-volume catalogue of the paintings of Titian (Phaidon, 1969–75) is divided between "Religious Paintings", "Portraits", and "Mythological and Historical Paintings", though both volumes I and III cover what is included in the term "History Paintings". This distinction is useful but is by no means generally observed, and the terms are still often used in a confusing manner. Because of the potential for confusion modern academic writing tends to avoid the phrase "historical painting", talking instead of "historical subject matter" in history painting, but where the phrase is still used in contemporary scholarship it will normally mean the painting of subjects from history, very often in the 19th century. "Historical painting" may also be used, especially in discussion of painting techniques in conservation studies, to mean "old", as opposed to modern or recent painting.
In 19th-century British writing on art the terms "subject painting" or "anecdotic" painting were often used for works in a line of development going back to William Hogarth of monoscenic depictions of crucial moments in an implied narrative with unidentified characters, such as William Holman Hunt's 1853 painting "The Awakening Conscience" or Augustus Egg's "Past and Present", a set of three paintings, updating sets by Hogarth such as "Marriage à-la-mode".
19th century.
History painting was the dominant form of academic painting in the various national academies in the 18th century, and for most of the 19th, and increasingly historical subjects dominated. During the Revolutionary and Napoleonic periods the heroic treatment of contemporary history in a frankly propagandistic fashion by Antoine-Jean, Baron Gros, Jacques-Louis David, Carle Vernet and others was supported by the French state, but after the fall of Napoleon in 1815 the French governments were not regarded as suitable for heroic treatment and many artists retreated further into the past to find subjects, though in Britain depicting the victories of the Napoleonic Wars mostly occurred after they were over. Another path was to choose contemporary subjects that were oppositional to government either at home and abroad, and many of what were arguably the last great generation of history paintings were protests at contemporary episodes of repression or outrages at home or abroad: Goya's "The Third of May 1808" (1814), Théodore Géricault's "The Raft of the Medusa" (1818–19), Eugène Delacroix's "The Massacre at Chios" (1824) and "Liberty Leading the People" (1830). These were heroic, but showed heroic suffering by ordinary civilians.
Romantic artists such as Géricault and Delacroix, and those from other movements such as the English Pre-Raphaelite Brotherhood continued to regard history painting as the ideal for their most ambitious works. Others such as Jan Matejko in Poland, Vasily Surikov in Russia, and Paul Delaroche in France became specialized painters of large historical subjects. The "style troubadour" ("troubadour style") was a somewhat derisive French term for earlier paintings of medieval and Renaissance scenes, which were often small and depicting moments of anecdote rather than drama; Ingres, Richard Parkes Bonington and Henri Fradelle painted such works. Sir Roy Strong calls this type of work the "Intimate Romantic", and in French it was known as the "peinture de genre historique" or "peinture anecdotique" ("historical genre painting" or "anecdotal painting").
Church commissions for large group scenes from the Bible had greatly reduced, and historical painting became very significant. Especially in the early 19th century, much historical painting depicted specific moments from historical literature, with the novels of Sir Walter Scott a particular favourite, in France and other European countries as much as Great Britain. By the middle of the century medieval scenes were expected to be very carefully researched, using the work of historians of costume, architecture and all elements of decor that were becoming available. The provision of examples and expertise for artists, as well as revivalist industrial designers, was one of the motivations for the establishment of museums like the Victoria and Albert Museum in London. New techniques of printmaking such as the chromolithograph made good quality monochrome print reproductions both relatively cheap and very widely accessible, and also hugely profitable for artist and publisher, as the sales were so large. Historical painting often had a close relationship with Nationalism, and painters like Matejko in Poland could play an important role in fixing the prevailing historical narrative of national history in the popular mind. In France, "L'art Pompier" ("Fireman art") was a derisory term for official academic historical painting, and in a final phase, "History painting of a debased sort, scenes of brutality and terror, purporting to illustrate episodes from Roman and Moorish history, were Salon sensations. On the overcrowded walls of the exhibition galleries, the paintings that shouted loudest got the attention". Orientalist painting was an alternative genre that offered similar exotic costumes and decor, and at least as much opportunity to depict sex and violence.

</doc>
<doc id="14052" url="https://en.wikipedia.org/wiki?curid=14052" title="Hyperbola">
Hyperbola

In mathematics, a hyperbola (plural "hyperbolas" or "hyperbolae") is a type of smooth curve lying in a plane, defined by its geometric properties or by equations for which it is the solution set. A hyperbola has two pieces, called connected components or branches, that are mirror images of each other and resemble two infinite bows. The hyperbola is one of the three kinds of conic section, formed by the intersection of a plane and a double cone. (The other conic sections are the parabola and the ellipse. A circle is a special case of an ellipse). If the plane intersects both halves of the double cone but does not pass through the apex of the cones, then the conic is a hyperbola.
Hyperbolas arise in many ways: as the curve representing the function formula_1 in the Cartesian plane, as the path followed by the shadow of the tip of a sundial, as the shape of an open orbit (as distinct from a closed elliptical orbit), such as the orbit of a spacecraft during a gravity assisted swing-by of a planet or more generally any spacecraft exceeding the escape velocity of the nearest planet, as the path of a single-apparition comet (one travelling too fast ever to return to the solar system), as the scattering trajectory of a subatomic particle (acted on by repulsive instead of attractive forces but the principle is the same), and so on.
Each branch of the hyperbola has two arms which become straighter (lower curvature) further out from the center of the hyperbola. Diagonally opposite arms, one from each branch, tend in the limit to a common line, called the asymptote of those two arms. So there are two asymptotes, whose intersection is at the center of symmetry of the hyperbola, which can be thought of as the mirror point about which each branch reflects to form the other branch. In the case of the curve formula_1 the asymptotes are the two coordinate axes.
Hyperbolas share many of the ellipses' analytical properties such as eccentricity, focus, and directrix. Typically the correspondence can be made with nothing more than a change of sign in some term. Many other mathematical objects have their origin in the hyperbola, such as hyperbolic paraboloids (saddle surfaces), hyperboloids ("wastebaskets"), hyperbolic geometry (Lobachevsky's celebrated non-Euclidean geometry), hyperbolic functions (sinh, cosh, tanh, etc.), and gyrovector spaces (a geometry proposed for use in both relativity and quantum mechanics which is not Euclidean).
Etymology and history.
The word "hyperbola" derives from the Greek , meaning "over-thrown" or "excessive", from which the English term hyperbole also derives. Hyperbolae were discovered by Menaechmus in his investigations of the problem of doubling the cube, but were then called sections of obtuse cones. The term hyperbola is believed to have been coined by Apollonius of Perga (c. 262–c. 190 BC) in his definitive work on the conic sections, the "Conics". For comparison, the other two general conic sections, the ellipse and the parabola, derive from the corresponding Greek words for "deficient" and "comparable"; these terms may refer to the eccentricity of these curves, which is greater than one (hyperbola), less than one (ellipse) and exactly one (parabola).
Nomenclature and features.
Similar to a parabola, a hyperbola is an open curve, meaning that it continues indefinitely to infinity, rather than closing on itself as an ellipse does. A hyperbola consists of two disconnected curves called its arms or branches.
The points on the two branches that are closest to each other are called the vertices; they are the points where the curve has its smallest radius of curvature. The line segment connecting the vertices is called the "transverse axis" or "major axis", corresponding to the major diameter of an ellipse. The midpoint of the transverse axis is known as the hyperbola's "center". The distance "a" from the center to each vertex is called the semi-major axis. Outside of the transverse axis but on the same line are the two "focal points (foci)" of the hyperbola. The line through these five points is one of the two principal axes of the hyperbola, the other being the perpendicular bisector of the transverse axis. The hyperbola has mirror symmetry about its principal axes, and is also symmetric under a 180° turn about its center.
At large distances from the center, the hyperbola approaches two lines, its asymptotes, which intersect at the hyperbola's center. A hyperbola approaches its asymptotes arbitrarily closely as the distance from its center increases, but it never intersects them; however, a degenerate hyperbola consists only of its asymptotes. Consistent with the symmetry of the hyperbola, if the transverse axis is aligned with the "x"-axis of a Cartesian coordinate system, the slopes of the asymptotes are equal in magnitude but opposite in sign, ±, where "b" = "a" × tan(θ) and where θ is the angle between the transverse axis and either asymptote. The distance "b" (not shown) is the length of the perpendicular segment from either vertex to the asymptotes.
A "conjugate axis" of length 2"b", corresponding to the "minor axis" of an ellipse, is sometimes drawn on the non-transverse principal axis; its endpoints ±"b" lie on the minor axis at the height of the asymptotes over/under the hyperbola's vertices. Because of the minus sign in some of the formulas below, it is also called the "imaginary axis" of the hyperbola.
If , the angle 2θ between the asymptotes equals 90° and the hyperbola is said to be "rectangular" or "equilateral". In this special case, the rectangle joining the four points on the asymptotes directly above and below the vertices is a square, since the lengths of its sides "2a" "2b".
If the transverse axis of any hyperbola is aligned with the "x"-axis of a Cartesian coordinate system and is centered on the origin, the equation of the hyperbola can be written as
A hyperbola aligned in this way is called an "East-West opening hyperbola". Likewise, a hyperbola with its transverse axis aligned with the "y"-axis is called a "North–South opening hyperbola" and has equation
Every hyperbola is congruent to the origin-centered East-West opening hyperbola sharing its same scale and eccentricity "e" (its shape, or degree of "spread"), and is also congruent to the origin-centered North–South opening hyperbola with identical scale and eccentricity "e" — that is, it can be rotated so that it opens in the desired direction and can be translated (rigidly moved in the plane) so that it is centered at the origin. For convenience, hyperbolas are usually analyzed in terms of their centered East-West opening form.
If formula_5 is the distance from the center to either focus, then formula_6.
The shape of a hyperbola is defined entirely by its eccentricity "e", which is a dimensionless number always greater than one. The distance "c" from the center to the foci equals "ae". The eccentricity can also be defined as the ratio of the distances to either focus and to a corresponding line known as the directrix; hence, the distance from the center to the directrices equals "a"/"e". In terms of the parameters "a", "b", "c" and the angle θ, the eccentricity equals
For example, the eccentricity of a rectangular hyperbola , equals the square root of two: "e" =  formula_8.
Every hyperbola has a conjugate hyperbola, in which the transverse and conjugate axes are exchanged without changing the asymptotes. The equation of the conjugate hyperbola of formula_9 is formula_10. If the graph of the conjugate hyperbola is rotated 90° to restore the east-west opening orientation (so that "x" becomes "y" and vice versa), the equation of the resulting rotated conjugate hyperbola is the same as the equation of the original hyperbola except with "a" and "b" exchanged. For example, the angle θ of the conjugate hyperbola equals 90° minus the angle of the original hyperbola. Thus, the angles in the original and conjugate hyperbolas are complementary angles, which implies that they have different eccentricities unless θ = 45° (a rectangular hyperbola). Hence, the conjugate hyperbola does "not" in general correspond to a 90° rotation of the original hyperbola; the two hyperbolas are generally different in shape.
A few other lengths are used to describe hyperbolas. Consider a line perpendicular to the transverse axis (i.e., parallel to the conjugate axis) that passes through one of the hyperbola's foci. The line segment connecting the two intersection points of this line with the hyperbola is known as the "latus rectum" and has a length formula_11. The "semi-latus rectum" "l" is half of this length, i.e., formula_12. The "focal parameter" "p" is the distance from a focus to its corresponding directrix, and equals formula_13.
Mathematical definitions.
A hyperbola can be defined mathematically in several equivalent ways.
Conic section.
A hyperbola may be defined as the curve of intersection between a right circular conical surface and a plane that cuts through both halves of the cone. The other major types of conic sections are the ellipse and the parabola; in these cases, the plane cuts through only one half of the double cone. If the plane passes through the central apex of the double cone a degenerate hyperbola results — two straight lines that cross at the apex point.
Difference of distances to foci.
A hyperbola may be defined equivalently as the locus of points where the absolute value of the "difference" of the distances to the two foci is a constant equal to 2"a", the distance between its two vertices. This definition accounts for many of the hyperbola's applications, such as multilateration; this is the problem of determining position from the "difference" in arrival times of synchronized signals, as in GPS.
This definition may be expressed also in terms of tangent circles. The center of any circles externally tangent to two given circles lies on a hyperbola, whose foci are the centers of the given circles and where the vertex distance 2"a" equals the difference in radii of the two circles. As a special case, one given circle may be a point located at one focus; since a point may be considered as a circle of zero radius, the other given circle—which is centered on the other focus—must have radius 2"a". This provides a simple technique for constructing a hyperbola, as shown below. It follows from this definition that a tangent line to the hyperbola at a point P bisects the angle formed with the two foci, i.e., the angle F1P F2. Consequently, the feet of perpendiculars drawn from each focus to such a tangent line lies on a circle of radius "a" that is centered on the hyperbola's own center.
A proof that this characterization of the hyperbola is equivalent to the conic-section characterization can be done without coordinate geometry by means of Dandelin spheres.
Directrix and focus.
A hyperbola can be defined as the locus of points for which the ratio of the distances to one focus and to a line (called the directrix) is a constant "e" that is larger than 1. This constant is the eccentricity of the hyperbola. The eccentricity equals the secant of half the angle between the asymptotes of the hyperbola, so the eccentricity of the hyperbola "xy" = 1 equals the square root of 2.
By symmetry a hyperbola has two directrices, which are parallel to the conjugate axis and are between it and the tangent to the hyperbola at a vertex. One directrix and its focus is enough to produce both arms of the hyperbola.
Reciprocation of a circle.
The reciprocation of a circle "B" in a circle "C" always yields a conic section such as a hyperbola. The process of "reciprocation in a circle "C"" consists of replacing every line and point in a geometrical figure with their corresponding pole and polar, respectively. The "pole" of a line is the inversion of its closest point to the circle "C", whereas the polar of a point is the converse, namely, a line whose closest point to "C" is the inversion of the point.
The eccentricity of the conic section obtained by reciprocation is the ratio of the distances between the two circles' centers to the radius "r" of reciprocation circle "C". If B and C represent the points at the centers of the corresponding circles, then
Since the eccentricity of a hyperbola is always greater than one, the center B must lie outside of the reciprocating circle "C".
This definition implies that the hyperbola is both the locus of the poles of the tangent lines to the circle "B", as well as the envelope of the polar lines of the points on "B". Conversely, the circle "B" is the envelope of polars of points on the hyperbola, and the locus of poles of tangent lines to the hyperbola. Two tangent lines to "B" have no (finite) poles because they pass through the center C of the reciprocation circle "C"; the polars of the corresponding tangent points on "B" are the asymptotes of the hyperbola. The two branches of the hyperbola correspond to the two parts of the circle "B" that are separated by these tangent points.
Quadratic equation.
A hyperbola can also be defined as a second-degree equation in the Cartesian coordinates ("x", "y") of the plane
provided that the constants "A""xx", "A""xy", "A""yy", "B""x", "B""y", and "C" satisfy the determinant condition
A special case of a hyperbola—the "degenerate hyperbola" consisting of two intersecting lines—occurs when another determinant is zero
This determinant Δ is sometimes called the discriminant of the conic section.
Given the above general parametrization of the hyperbola in Cartesian coordinates, the eccentricity can be found using the formula in Conic section#Eccentricity in terms of parameters of the quadratic form.
The center ("x""c", "y""c") of the hyperbola may be determined from the formulae
In terms of new coordinates, and , the defining equation of the hyperbola can be written
The principal axes of the hyperbola make an angle Φ with the positive "x"-axis that is given by
Rotating the coordinate axes so that the "x"-axis is aligned with the transverse axis brings the equation into its canonical form
The major and minor semiaxes "a" and "b" are defined by the equations
where λ1 and λ2 are the roots of the quadratic equation
For comparison, the corresponding equation for a degenerate hyperbola is
The tangent line to a given point ("x"0, "y"0) on the hyperbola is defined by the equation
where "E", "F" and "G" are defined by
The normal line to the hyperbola at the same point is given by the equation
The normal line is perpendicular to the tangent line, and both pass through the same point ("x"0, "y"0).
From the equation
and for a point on the left branch that
can be proved as follows:
If x,y is a point on the hyperbola the distance to the left focal point is
To the right focal point the distance is
If x,y is a point on the right branch of the hyperbola then formula_41 and
Subtracting these equations one gets
If x,y is a point on the left branch of the hyperbola then formula_45 and
Subtracting these equations one gets
True anomaly.
In the section above it is shown that using the coordinate system in which the equation of the hyperbola takes its canonical form
the distance formula_50 from a point formula_51 on the left branch of the hyperbola to the left focal point formula_52 is
Introducing polar coordinates formula_54 with origin at the left focal point, the coordinates relative to the canonical coordinate system are
and the equation above takes the form
from which it follows that
This is the representation of the near branch of a hyperbola in polar coordinates with respect to a focal point.
The polar angle formula_59 of a point on a hyperbola relative to the near focal point as described above is called the true anomaly of the point.
Geometrical constructions.
Similar to the ellipse, a hyperbola can be constructed using a taut thread. A straightedge of length "S" is attached to one focus F1 at one of its corners A so that it is free to rotate about that focus. A thread of length "L" = "S" - 2"a" is attached between the other focus F2 and the other corner B of the straightedge. A sharp pencil is held up against the straightedge, sandwiching the thread tautly against the straightedge. Let the position of the pencil be denoted as P. The total length "L" of the thread equals the sum of the distances "L"2 from F2 to P and "L"B from P to B. Similarly, the total length "S" of the straightedge equals the distance "L"1 from F1 to P and "L"B. Therefore, the difference in the distances to the foci, equals the constant 2"a"
A second construction uses intersecting circles, but is likewise based on the constant difference of distances to the foci. Consider a hyperbola with two foci F1 and F2, and two vertices P and Q; these four points all lie on the transverse axis. Choose a new point T also on the transverse axis and to the right of the rightmost vertex P; the difference in distances to the two vertices, = 2"a", since 2"a" is the distance between the vertices. Hence, the two circles centered on the foci F1 and F2 of radius QT and PT, respectively, will intersect at two points of the hyperbola.
A third construction relies on the definition of the hyperbola as the reciprocation of a circle. Consider the circle centered on the center of the hyperbola and of radius "a"; this circle is tangent to the hyperbola at its vertices. A line "g" drawn from one focus may intersect this circle in two points M and N; perpendiculars to "g" drawn through these two points are tangent to the hyperbola. Drawing a set of such tangent lines reveals the envelope of the hyperbola.
A fourth construction is using the parallelogram method. It is similar to such method for parabola and ellipse construction: certain equally spaced points lying on parallel lines are connected with each other by two straight lines and their intersection point lies on the hyperbola.
Reflections and tangent lines.
The ancient Greek geometers recognized a reflection property of hyperbolas. If a ray of light emerges from one focus and is reflected from either branch of the hyperbola, the light-ray appears to have come from the other focus. Equivalently, by reversing the direction of the light, rays directed at one of the foci are reflected towards the other focus. This property is analogous to the property of ellipses that a ray emerging from one focus is reflected from the ellipse directly "towards" the other focus (rather than "away" as in the hyperbola). Expressed mathematically, lines drawn from each focus to the same point on the hyperbola intersect it at equal angles; the tangent line to a hyperbola at a point P bisects the angle formed with the two foci, F1PF2.
Tangent lines to a hyperbola have another remarkable geometrical property. If a tangent line at a point T intersects the asymptotes at two points K and L, then T bisects the line segment KL, and the product of distances to the hyperbola's center, OK×OL is a constant.
Hyperbolic functions and equations.
Just as the sine and cosine functions give a parametric equation for the ellipse, so the hyperbolic sine and hyperbolic cosine give a parametric equation for the hyperbola.
As
one has for any hyperbolic angle formula_62 that the point
satisfies the equation
which is the equation of a hyperbola relative its canonical coordinate system.
When "μ" varies over the interval formula_66 one gets with this formula all points formula_67 on the right branch of the hyperbola.
The left branch for which formula_68 is in the same way obtained as
In the figure the points formula_71 given by
for
on the left branch of a hyperbola with eccentricity 1.2 are marked as dots.
Relation to other conic sections.
There are three types of conic sections: hyperbolas, ellipses and parabolas. Since the parabola may be seen as a limiting case poised exactly between an ellipse and a hyperbola, there are only two non-limiting types, ellipses and hyperbolas. These two types are related in that formulae for one type can often be applied to the other.
The canonical equation for a hyperbola is
Any hyperbola can be rotated so that it is east-west opening and positioned with its center at the origin, so that the equation describing it is this canonical equation.
The canonical equation for the hyperbola may be seen as a version of the corresponding ellipse equation
in which the semi-minor axis length "b" is imaginary. That is, if in the ellipse equation "b" is replaced by "ib" where "b" is real, one obtains the hyperbola equation.
Similarly, the parametric equations for a hyperbola and an ellipse are expressed in terms of hyperbolic and trigonometric functions, respectively, which are again related by an imaginary circular angle, for example,
Hence, many formulae for the ellipse can be extended to hyperbolas by adding the imaginary unit "i" in front of the semi-minor axis "b" and the angle. For example, the arc length of a segment of an ellipse can be determined using an incomplete elliptic integral of the second kind. The corresponding arclength of a hyperbola is given by the same function with imaginary parameters "b" and μ, namely, "ib E(iμ, c)".
Conic section analysis of the hyperbolic appearance of circles.
Besides providing a uniform description of circles, ellipses, parabolas, and hyperbolas, conic sections can also be understood as a natural model of the geometry of perspective in the case where the scene being viewed consists of a circle, or more generally an ellipse. The viewer is typically a camera or the human eye. In the simplest case the viewer's lens is just a pinhole; the role of more complex lenses is merely to gather far more light while retaining as far as possible the simple pinhole geometry in which all rays of light from the scene pass through a single point. Once through the lens, the rays then spread out again, in air in the case of a camera, in the vitreous humor in the case of the eye, eventually distributing themselves over the film, imaging device, or retina, all of which come under the heading of image plane. The lens plane is a plane parallel to the image plane at the lens; all rays pass through a single point on the lens plane, namely the lens itself.
When the circle directly faces the viewer, the viewer's lens is on-axis, meaning on the line normal to the circle through its center (think of the axle of a wheel). The rays of light from the circle through the lens to the image plane then form a cone with circular cross section whose apex is the lens. The image plane concretely realizes the abstract cutting plane in the conic section model.
When in addition the viewer directly faces the circle, the circle is rendered faithfully on the image plane without perspective distortion, namely as a scaled-down circle. When the viewer turns attention or gaze away from the center of the circle the image plane then cuts the cone in an ellipse, parabola, or hyperbola depending on how far the viewer turns, corresponding exactly to what happens when the surface cutting the cone to form a conic section is rotated.
A parabola arises when the lens plane is tangent to (touches) the circle. A viewer with perfect 180-degree wide-angle vision will see the whole parabola; in practice this is impossible and only a finite portion of the parabola is captured on the film or retina.
When the viewer turns further so that the lens plane cuts the circle in two points, the shape on the image plane becomes that of a hyperbola. The viewer still sees only a finite curve, namely a portion of one branch of the hyperbola, and is unable to see the second branch at all, which corresponds to the portion of the circle behind the viewer, more precisely, on the same side of the lens plane as the viewer. In practice the finite extent of the image plane makes it impossible to see any portion of the circle near where it is cut by the lens plane. Further back however one could imagine rays from the portion of the circle well behind the viewer passing through the lens, were the viewer transparent. In this case the rays would pass through the image plane before the lens, yet another impracticality ensuring that no portion of the second branch could possibly be visible.
The tangents to the circle where it is cut by the lens plane constitute the asymptotes of the hyperbola. Were these tangents to be drawn in ink in the plane of the circle, the eye would perceive them as asymptotes to the visible branch. Whether they converge in front of or behind the viewer depends on whether the lens plane is in front of or behind the center of the circle respectively.
If the circle is drawn on the ground and the viewer gradually transfers gaze from straight down at the circle up towards the horizon, the lens plane eventually cuts the circle producing first a parabola then a hyperbola on the image plane as shown in Figure 10. As the gaze continues to rise the asymptotes of the hyperbola, if realized concretely, appear coming in from left and right, swinging towards each other and converging at the horizon when the gaze is horizontal. Further elevation of the gaze into the sky then brings the point of convergence of the asymptotes towards the viewer.
By the same principle with which the back of the circle appears on the image plane were all the physical obstacles to its projection to be overcome, the portion of the two tangents behind the viewer appear on the image plane as an extension of the visible portion of the tangents in front of the viewer. Like the second branch this extension materializes in the sky rather than on the ground, with the horizon marking the boundary between the physically visible (scene in front) and invisible (scene behind), and the visible and invisible parts of the tangents combining in a single X shape. As the gaze is raised and lowered about the horizon, the X shape moves oppositely, lowering as the gaze is raised and vice versa but always with the visible portion being on the ground and stopping at the horizon, with the center of the X being on the horizon when the gaze is horizontal.
All of the above was for the case when the circle faces the viewer, with only the viewer's gaze varying. When the circle starts to face away from the viewer the viewer's lens is no longer on-axis. In this case the cross section of the cone is no longer a circle but an ellipse (never a parabola or hyperbola). However the principle of conic sections does not depend on the cross section of the cone being circular, and applies without modification to the case of eccentric cones.
It is not difficult to see that even in the off-axis case a circle can appear circular, namely when the image plane (and hence lens plane) is parallel to the plane of the circle. That is, to see a circle as a circle when viewing it obliquely, look not at the circle itself but at the plane in which it lies. From this it can be seen that when viewing a plane filled with many circles, all of them will appear circular simultaneously when the plane is looked at directly.
A common misperception about the hyperbola is that it is a mathematical curve rarely if ever encountered in daily life. The reality is that one sees a hyperbola whenever catching sight of portion of a circle cut by one's lens plane (and a parabola when the lens plane is tangent to, i.e. just touches, the circle). The inability to see very much of the arms of the visible branch, combined with the complete absence of the second branch, makes it virtually impossible for the human visual system to recognize the connection with hyperbolas such as "y" = 1/"x" where both branches are on display simultaneously.
Derived curves.
Several other curves can be derived from the hyperbola by inversion, the so-called inverse curves of the hyperbola. If the center of inversion is chosen as the hyperbola's own center, the inverse curve is the lemniscate of Bernoulli; the lemniscate is also the envelope of circles centered on a rectangular hyperbola and passing through the origin. If the center of inversion is chosen at a focus or a vertex of the hyperbola, the resulting inverse curves are a limaçon or a strophoid, respectively.
Coordinate systems.
Cartesian coordinates.
An east-west opening hyperbola centered at ("h","k") has the equation
The major axis runs through the center of the hyperbola and intersects both arms of the hyperbola at the vertices (bend points) of the arms. The foci lie on the extension of the major axis of the hyperbola.
The minor axis runs through the center of the hyperbola and is perpendicular to the major axis.
In both formulas "a" is the semi-major axis (half the distance between the two arms of the hyperbola measured along the major axis), and "b" is the semi-minor axis (half the distance between the asymptotes along a line tangent to the hyperbola at a vertex).
If one forms a rectangle with vertices on the asymptotes and two sides that are tangent to the hyperbola, the sides tangent to the hyperbola are "2b" in length while the sides that run parallel to the line between the foci (the major axis) are "2a" in length. Note that "b" may be larger than "a" despite the names "minor" and "major".
If one calculates the distance from any point on the hyperbola to each focus, the absolute value of the difference of those two distances is always "2a".
The eccentricity is given by
If "c" equals the distance from the center to either focus, then
where
The distance "c" is known as the linear eccentricity of the hyperbola. The distance between the foci is 2"c" or 2"ae".
The foci for an east-west opening hyperbola are given by
and for a north-south opening hyperbola are given by
The directrices for an east-west opening hyperbola are given by
and for a north-south opening hyperbola are given by
Polar coordinates.
The polar coordinates used most commonly for the hyperbola are defined relative to the Cartesian coordinate system that has its origin in a focus and its x-axis pointing towards the origin of the "canonical coordinate system" as illustrated in the figure of the section True anomaly.
Relative to this coordinate system one has that
and the range of the true anomaly formula_87 is
With polar coordinate relative to the "canonical coordinate system"
one has that
For the right branch of the hyperbola the range of formula_92 is
Parametric equations.
"East-west opening hyperbola:"
"North-south opening hyperbola:"
In all formulae ("h","k") are the center coordinates of the hyperbola, "a" is the length of the semi-major axis, and "b" is the length of the semi-minor axis.
Elliptic coordinates.
A family of confocal hyperbolas is the basis of the system of elliptic coordinates in two dimensions. These hyperbolas are described by the equation
where the foci are located at a distance "c" from the origin on the "x"-axis, and where θ is the angle of the asymptotes with the "x"-axis. Every hyperbola in this family is orthogonal to every ellipse that shares the same foci. This orthogonality may be shown by a conformal map of the Cartesian coordinate system "w" = "z" + 1/"z", where "z"= "x" + "iy" are the original Cartesian coordinates, and "w"="u" + "iv" are those after the transformation.
Other orthogonal two-dimensional coordinate systems involving hyperbolas may be obtained by other conformal mappings. For example, the mapping "w" = "z"2 transforms the Cartesian coordinate system into two families of orthogonal hyperbolas.
Rectangular hyperbola.
A rectangular hyperbola, equilateral hyperbola, or right hyperbola is a hyperbola for which the asymptotes are perpendicular.
Rectangular hyperbolas with the coordinate axes parallel to their asymptotes have the equation
Rectangular hyperbolas have eccentricity formula_98 with semi-major axis and semi-minor axis given by formula_99.
The simplest example of rectangular hyperbolas occurs when the center ("h", "k") is at the origin:
describing quantities "x" and "y" that are inversely proportional. By rotating the coordinate axes counterclockwise by 45 degrees, with the new coordinate axes labelled formula_101 the equation of the hyperbola is given by canonical form
If the scale factor "m"=1/2, then this canonical rectangular hyperbola is the unit hyperbola.
A circumconic passing through the orthocenter of a triangle is a rectangular hyperbola.
Applications.
Sundials.
Hyperbolas may be seen in many sundials. On any given day, the sun revolves in a circle on the celestial sphere, and its rays striking the point on a sundial traces out a cone of light. The intersection of this cone with the horizontal plane of the ground forms a conic section. At most populated latitudes and at most times of the year, this conic section is a hyperbola. In practical terms, the shadow of the tip of a pole traces out a hyperbola on the ground over the course of a day (this path is called the "declination line"). The shape of this hyperbola varies with the geographical latitude and with the time of the year, since those factors affect the cone of the sun's rays relative to the horizon. The collection of such hyperbolas for a whole year at a given location was called a "pelekinon" by the Greeks, since it resembles a double-bladed axe.
Multilateration.
A hyperbola is the basis for solving multilateration problems, the task of locating a point from the differences in its distances to given points — or, equivalently, the difference in arrival times of synchronized signals between the point and the given points. Such problems are important in navigation, particularly on water; a ship can locate its position from the difference in arrival times of signals from a LORAN or GPS transmitters. Conversely, a homing beacon or any transmitter can be located by comparing the arrival times of its signals at two separate receiving stations; such techniques may be used to track objects and people. In particular, the set of possible positions of a point that has a distance difference of 2"a" from two given points is a hyperbola of vertex separation 2"a" whose foci are the two given points.
Path followed by a particle.
The path followed by any particle in the classical Kepler problem is a conic section. In particular, if the total energy "E" of the particle is greater than zero (i.e., if the particle is unbound), the path of such a particle is a hyperbola. This property is useful in studying atomic and sub-atomic forces by scattering high-energy particles; for example, the Rutherford experiment demonstrated the existence of an atomic nucleus by examining the scattering of alpha particles from gold atoms. If the short-range nuclear interactions are ignored, the atomic nucleus and the alpha particle interact only by a repulsive Coulomb force, which satisfies the inverse square law requirement for a Kepler problem.
Korteweg-de Vries equation.
The hyperbolic trig function formula_109 appears as one solution to the Korteweg-de Vries equation which describes the motion of a soliton wave in a canal.
Angle trisection.
As shown first by Apollonius of Perga, a hyperbola can be used to trisect any angle, a well studied problem of geometry. Given an angle, first draw a circle centered at its vertex O, which intersects the sides of the angle at points A and B. Next draw the line segment with endpoints A and B and its perpendicular bisector formula_110. Construct a hyperbola of eccentricity "e"=2 with formula_110 as directrix and B as a focus. Let P be the intersection (upper) of the hyperbola with the circle. Angle POB trisects angle AOB. To prove this, reflect the line segment OP about the line formula_110 obtaining the point P' as the image of P. Segment AP' has the same length as segment BP due to the reflection, while segment PP' has the same length as segment BP due to the eccentricity of the hyperbola. As OA, OP', OP and OB are all radii of the same circle (and so, have the same length), the triangles OAP', OPP' and OPB are all congruent. Therefore, the angle has been trisected, since 3×POB = AOB.
Efficient portfolio frontier.
In portfolio theory, the locus of mean-variance efficient portfolios (called the efficient frontier) is the upper half of the east-opening branch of a hyperbola drawn with the portfolio return's standard deviation plotted horizontally and its expected value plotted vertically; according to this theory, all rational investors would choose a portfolio characterized by some point on this locus.
Extensions.
The three-dimensional analog of a hyperbola is a hyperboloid. Hyperboloids come in two varieties, those of one sheet and those of two sheets. A simple way of producing a hyperboloid is to rotate a hyperbola about the axis of its foci or about its symmetry axis perpendicular to the first axis; these rotations produce hyperboloids of two and one sheet, respectively.

</doc>
<doc id="14055" url="https://en.wikipedia.org/wiki?curid=14055" title="Humayun">
Humayun

Humayun (; OS 7 March 1508OS 27 January 1556) was the second Mughal Emperor who ruled over territory in what is now Afghanistan, Pakistan, and parts of northern India from 15311540 and again from 15551556. Like his father, Babur, he lost his kingdom early, but regained it with the aid of the Safavid dynasty of Persia, with additional territory. At the time of his death in 1556, the Mughal Empire spanned almost one million square kilometers.
Humayun succeeded his father in December 1530, as ruler of the Mughal territories in the Indian subcontinent. At the age of 23, Humayun was an inexperienced ruler when he came to power. His half-brother Kamran Mirza inherited Kabul and Lahore, the northernmost parts of their father's empire. Mirza was to become a bitter rival of Humayun.
Humayun lost Mughal territories to the Pashtun noble, Sher Shah Suri, and, with Persian (Safavid) aid, regained them 15 years later. Humayun's return from Persia was accompanied by a large retinue of Persian noblemen and signaled an important change in Mughal court culture. The Central Asian origins of the dynasty were largely overshadowed by the influences of Persian art, architecture, language and literature. There are many stone carvings and thousands of Persian manuscripts in India dating from the time of Humayun.
Subsequently, in a very short time, Humayun was able to expand the Empire further, leaving a substantial legacy for his son, Akbar. His peaceful personality, patience and non-provocative methods of speech earned him the title "’Insān-i-Kamil" (Perfect Man), among the Mughals.
Background.
Babur's decision to divide the territories of his empire between two of his sons was unusual in India although it had been a common Central Asian practice since the time of Genghis Khan. Unlike most monarchies which practised primogeniture, the Timurids, following Genghis Khan's example, did not leave an entire kingdom to the eldest son. Although under that system only a Chingissid could claim sovereignty and khanal authority, any male Chinggisid within a given sub-branch had an equal right to the throne. (Although the Timurids were not Chinggisid in their paternal ancestry) While Genghis Khan's Empire had been peacefully divided between his sons upon his death, almost every Chinggisid succession since had resulted in fratricide.
Timur himself had divided his territories among Pir Muhammad, Miran Shah, Khalil Sultan and Shah Rukh, which resulted in inter-family warfare. Upon Babur's death, Humayun's territories were the least secure. He had ruled only four years, and not all "umarah" (nobles) viewed Humayun as the rightful ruler. Indeed, earlier, when Babur had become ill, some of the nobles had tried to install his uncle, Mahdi Khwaja, as ruler. Although this attempt failed, it was a sign of problems to come.
Early reign.
When Humayun came to the throne several of his brothers revolted against him. Another brother Khalil Mirza (1509–30) supported Humayun but was assassinated. The Emperor commenced construction of a tomb for his brother in 1538 but this was uncompleted when Humayun was forced to flee to Persia. Sher Shah destroyed the structure and no further work was done on it after Humayun's restoration.
Humayun had two major rivals for his lands — Sultan Bahadur of Gujarat to the south west and Sher Shah Suri (Sher Khan) settled along the river Ganges in Bihar to the east. Humayun’s first campaign was to confront Sher Khan Suri. Halfway through this offensive Humayun had to abandon it and concentrate on Gujarat, where a threat from Ahmed Shah had to be met. Humayun was victorious annexing Gujarat, Malwa, Champaner and the great fort of Mandu.
During the first five years of Humayun's reign, Bahadur and Sher Khan extended their rule, although Sultan Bahadur faced pressure in the east from sporadic conflicts with the Portuguese. While the Mughals had obtained firearms via the Ottoman Empire, Bahadur's Gujarat had acquired them through a series of contracts drawn up with the Portuguese, allowing the Portuguese to establish a strategic foothold in north western India.
Humayun was made aware that the Sultan of Gujarat was planning an assault on the Mughal territories with Portuguese aid. Humayun gathered an army and marched on Bahadur. Within a month he had captured the forts of Mandu and Champaner. However, instead of pressing his attack, Humayun ceased the campaign and consolidated his newly conquered territory. Sultan Bahadur, meanwhile escaped and took up refuge with the Portuguese.
Sher Shah Suri.
Shortly after Humayun had marched on Gujarat, Sher Shah saw an opportunity to wrest control of Agra from the Mughals. He began to gather his army together hoping for a rapid and decisive siege of the Mughal capital. Upon hearing this alarming news, Humayun quickly marched his troops back to Agra allowing Bahadur to easily regain control of the territories Humayun had recently taken. A few months later, however, Bahadur was dead, killed when a botched plan to kidnap the Portuguese viceroy ended in a fire-fight which the Sultan lost.
Whilst Humayun succeeded in protecting Agra from Sher Shah, the second city of the Empire, Gaur the capital of the "vilayat" of Bengal, was sacked. Humayun's troops had been delayed while trying to take Chunar, a fort occupied by Sher Shah's son, in order to protect his troops from an attack from the rear. The stores of grain at Gauri, the largest in the empire, were emptied and Humayun arrived to see corpses littering the roads. The vast wealth of Bengal was depleted and brought East giving Sher Shah a substantial war chest.
Sher Shah withdrew to the east, but Humayun did not follow: instead he "shut himself up for a considerable time in his Harem, and indulged himself in every kind of luxury." Hindal, Humayun's 19-year-old brother, had agreed to aid him in this battle and protect the rear from attack but abandoned his position and withdrew to Agra where he decreed himself acting emperor. When Humayun sent the grand "Mufti", Sheikh Buhlul, to reason with him, the Sheikh was killed. Further provoking the rebellion, Hindal ordered that the "Khutba" or sermon in the main mosque at Agra be read in his name, a sign of assumption of sovereignty. When Hindal withdrew from protecting the rear of Humayun's troops, Sher Shah's troop quickly reclaimed these positions, leaving Humayun surrounded.
Humayun's other brother, Kamran, marched from his territories in the Punjab, ostensibly to aid Humayun. However, his return home had treacherous motives as he intended to stake a claim for Humayun's apparently collapsing empire. He brokered a deal with Hindal which provided that his brother would cease all acts of disloyalty in return for a share in the new empire which Kamran would create once Humayun was deposed.
Sher Shah met Humayun in battle on the banks of the Ganges, near Buxar, in Chausa. This was to become an entrenched battle in which both sides spent a lot of time digging themselves into positions. The major part of the Mughal army, the artillery, was now immobile, and Humayun decided to engage in some diplomacy using Muhammad Aziz as ambassador. Humayun agreed to allow Sher Shah to rule over Bengal and Bihar, but only as provinces granted to him by his Emperor, Humayun, falling short of outright sovereignty. The two rulers also struck a bargain in order to save face: Humayun's troops would charge those of Sher Shah whose forces then retreat in feigned fear. Thus honour would, supposedly, be satisfied.
Once the Army of Humayun had made its charge and Sher Shah's troops made their agreed-upon retreat, the Mughal troops relaxed their defensive preparations and returned to their entrenchments without posting a proper guard. Observing the Mughals' vulnerability, Sher Shah reneged on his earlier agreement. That very night, his army approached the Mughal camp and finding the Mughal troops unprepared with a majority asleep, they advanced and killed most of them. The Emperor survived by swimming across the Ganges using an air filled "water skin," and quietly returned to Agra. Humayun was assisted across the Ganges by Shams al-Din Muhammad.
In Agra.
When Humayun returned to Agra, he found that all three of his brothers were present. Humayun once again not only pardoned his brothers for plotting against him, but even forgave Hindal for his outright betrayal. With his armies travelling at a leisurely pace, Sher Shah was gradually drawing closer and closer to Agra. This was a serious threat to the entire family, but Humayun and Kamran squabbled over how to proceed. Kamran withdrew after Humayun refused to make a quick attack on the approaching enemy, instead opting to build a larger army under his own name.
When Kamran returned to Lahore, his troops followed him shortly afterwards, and Humayun, with his other brothers Askari and Hindal, marched to meet Sher Shah just east of Agra at the Battle of Kanauj on 17 May 1540. The battle once again saw Humayun make some tactical errors, and his army was soundly defeated. He and his brothers quickly retreated back to Agra, but they chose not to stay in Agra, and retreated to Lahore, since Sher Shah followed them. The founding of the short-lived Sur Dynasty (which contained only him and his son)of northern India with its capital at Delhi, resulted in Humayun's exile for 15 years in the court of Shah Tahmasp I.
In Lahore.
The four brothers were united in Lahore, but every day they were informed that Sher Shah was getting closer and closer. When he reached Sirhind, Humayun sent an ambassador carrying the message "I have left you the whole of Hindustan ("i.e." the lands to the East of Punjab, comprising most of the Ganges Valley). Leave Lahore alone, and let Sirhind be a boundary between you and me." Sher Shah, however, replied "I have left you Kabul. You should go there." Kabul was the capital of the empire of Humayun's brother Kamran Mirza, who was far from willing to hand over any of his territories to his brother. Instead, Kamran approached Sher Shah, and proposed that he actually revolt against his brother and side with Sher Shah in return for most of the Punjab. Sher Shah dismissed his help, believing it not to be required, though word soon spread to Lahore about the treacherous proposal and Humayun was urged to make an example of Kamran and kill him. Humayun refused, citing the last words of his father, Babur "Do nothing against your brothers, even though they may deserve it."
Withdrawing further.
Humayun decided that it would be wise to withdraw still further, Humayun and his army rode out through and across the Thar Desert, when the Hindu ruler Rao Maldeo Rathore allied himself with Sher Shah Suri against the Mughal Empire. In many accounts Humayun mentions how he and his pregnant wife, had to trace their steps through the desert at the hottest time of year. All the wells had been filled with sand by the nearby Hindu inhabitants in order to starve and exhaust the Mughals further, leaving them with nothing but berries to eat. When Hamida's horse died, no one would lend the Queen (who was now eight months pregnant) a horse, so Humayun did so himself, resulting in him riding a camel for six kilometeres (four miles), although Khaled Beg then offered him his mount. Humayun was later to describe this incident as the lowest point in his life.
He asked that his brothers join him as he fell back into Sindh. While the previously rebellious Hindal Mirza remained loyal and was ordered to join his brothers in Kandahar. Kamran Mirza and Askari Mirza instead decided to head to the relative peace of Kabul. This was to be a definitive schism in the family.
Humayun expected aid from the Emir of Sindh, Hussein Umrani, whom he had appointed and who owed him his allegiance. The Emir Hussein Umrani welcomed Humayun's presence and was loyal to Humayun just as he had been loyal to Babur against the renegade Arghuns. Whilst in the oasis garrison of Umerkot in Sindh, Hamida daughter of noble Sindhi, gave birth to Akbar on 15 October 1542, the heir-apparent to the 34-year-old Humayun. The date was special because Humayun consulted his Astronomer to utilize the astrolabe and check the location of the planets.
While in Sindh, Humayun alongside Emir Hussein Umrani, gathered horses and weapons and formed new alliances that helped regain lost territories. Until finally Humayun had gathered hundreds of Sindhi and Baloch tribesmen alongside his Mughals and then marched towards Kandahar and later Kabul, thousands more gathered by his side as Humayun continually declared himself the rightful Timurid heir of the first Mughal Emperor Babur.
Retreat to Kabul.
After Humayun set out from his expedition in Sindh, along with 300 camels (mostly wild) and 2000 loads of grain, he set off to join his brothers in Kandahar after crossing the Indus River on 11 July 1543 along with the ambition to regain the Mughal Empire and overthrow the Suri dynasty. Among the tribes that had sworn allegiance to Humayun were the Magsi, Rind and many others.
In Kamran Mirza's territory, Hindal Mirza had been placed under house arrest in Kabul after refusing to have the "Khutba" recited in Kamran Mirza's name. His other brother Askari Mirza was now ordered to gather an army and march on Humayun. When Humayun received word of the approaching hostile army he decided against facing them, and instead sought refuge elsewhere. Akbar was left behind in camp close to Kandahar for, as it was December it would have been too cold and dangerous to include the 14-month-old toddler in the forthcoming march through the dangerous and snowy mountains of the Hindu Kush. Askari Mirza found Akbar in the camp, and embraced him, and allowed his own wife to parent him, she apparently started treating him as her own.
Once again Humayun turned toward Kandahar where his brother Kamran Mirza was in power, but he received no help and had to seek refuge with the Shah of Persia.
Refuge in Persia.
Humayun fled to the refuge of the Safavid Empire in Persia, marching with 40 men and his wife and her companion through mountains and valleys. Amongst other trials the Imperial party were forced to live on horse meat boiled in the soldiers' helmets. These indignities continued during the month it took them to reach Herat, however after their arrival they were reintroduced to the finer things in life. Upon entering the city his army was greeted with an armed escort, and they were treated to lavish food and clothing. They were given fine accommodations and the roads were cleared and cleaned before them. Shah Tahmasp, unlike Humayun's own family, actually welcomed the Mughal, and treated him as a royal visitor. Here Humayun went sightseeing and was amazed at the Persian artwork and architecture he saw: much of this was the work of the Timurid Sultan Husayn Bayqarah and his ancestor, princess Gauhar Shad, thus he was able to admire the work of his relatives and ancestors at first hand.
He was introduced to the work of the Persian miniaturists, and Kamaleddin Behzad had two of his pupils join Humayun in his court. Humayun was amazed at their work and asked if they would work for him if he were to regain the sovereignty of Hindustan: they agreed. With so much going on Humayun did not even meet the Shah until July, some six months after his arrival in Persia. After a lengthy journey from Herat the two met in Qazvin where a large feast and parties were held for the event. The meeting of the two monarchs is depicted in a famous wall-painting in the Chehel Sotoun (Forty Columns) palace in Esfahan.
The Shah urged that Humayun convert from Sunni to Shia Islam, and Humayun eventually and reluctantly accepted, in order to keep himself and several hundred followers alive. Although the Mughals initially disagreed to their conversion they knew that with this outward acceptance of Shi'ism, Shah Tahmasp was eventually prepared to offer Humayun more substantial support. When Humayun's brother, Kamran Mirza, offered to cede Kandahar to the Persians in exchange for Humayun, dead or alive, Shah Tahmasp refused. Instead the Shah staged a celebration for Humayun, with 300 tents, an imperial Persian carpet, 12 musical bands and "meat of all kinds". Here the Shah announced that all this, and 12,000 elite cavalry were his to lead an attack on his brother Kamran. All that Shah Tahmasp asked for was that, if Humayun's forces were victorious, Kandahar would be his.
Kandahar and onwards.
With this Persian Safavid aid Humayun took Kandahar from Askari Mirza after a two-week siege. He noted how the nobles who had served Askari Mirza quickly flocked to serve him, "in very truth the greater part of the inhabitants of the world are like a flock of sheep, wherever one goes the others immediately follow". Kandahar was, as agreed, given to the Shah of Persia who sent his infant son, Murad, as the Viceroy. However, the baby soon died and Humayun thought himself strong enough to assume power.
Humayun now prepared to take Kabul, ruled by his brother Kamran Mirza. In the end, there was no actual siege. Kamran Mirza was detested as a leader and as Humayun's Persian army approached the city hundreds of Kamran Mirza's troops changed sides, flocking to join Humayun and swelling his ranks. Kamran Mirza absconded and began building an army outside the city. In November 1545, Hamida and Humayun were reunited with their son Akbar, and held a huge feast. They also held another, larger, feast in the child's honour when he was circumcised.
However, while Humayun had a larger army than his brother and had the upper hand, on two occasions his poor military judgement allowed Kamran Mirza to retake Kabul and Kandahar, forcing Humayun to mount further campaigns for their recapture. He may have been aided in this by his reputation for leniency towards the troops who had defended the cities against him, as opposed to Kamran Mirza, whose brief periods of possession were marked by atrocities against the inhabitants who, he supposed, had helped his brother.
His youngest brother, Hindal Mirza, formerly the most disloyal of his siblings, died fighting on his behalf. His brother Askari Mirza was shackled in chains at the behest of his nobles and aides. He was allowed go on Hajj, and died en route in the desert outside Damascus.
Humayun's other brother, Kamran Mirza, had repeatedly sought to have Humayun killed. In 1552 Kamran Mirza attempted to make a pact with Islam Shah, Sher Shah's successor, but was apprehended by a Gakhar. The Gakhars were one of the minority of tribal groups who had consistently remained loyal to their oath to the Mughals. Sultan Adam of the Gakhars handed Kamran Mirza over to Humayun. Humayun was inclined to forgive his brother. However he was warned that allowing Kamran Mirza's repeated acts of treachery to go unpunished could foment rebellion amongst his own supporters. So, instead of killing his brother, Humayun had Kamran Mirza blinded which would end any claim by the latter to the throne. Humayun sent Kamran Mirza on Hajj, as he hoped to see his brother thereby absolved of his offences. However Kamran Mirza died close to Mecca in the Arabian Peninsula in 1557.
Restoration of the Mughal Empire.
Sher Shah Suri had died in 1545; his son and successor Islam Shah died too, in 1554. These two deaths left the dynasty reeling and disintegrating. Three rivals for the throne all marched on Delhi, while in many cities leaders tried to stake a claim for independence. This was a perfect opportunity for the Mughals to march back to India.
The Mughal Emperor Humayun, gathered a vast army and attempted the challenging task of retaking the throne in Delhi. Humayun placed the army under the able leadership of Bairam Khan. This was a wise move given Humayun's own record of military ineptitude, and turned out to be prescient, as Bairam was to prove himself a great tactician.
Marriage relations with the Khanzadas.
The "Gazetteer of Ulwur" states:
Bairam Khan led the army through the Punjab virtually unopposed. The fort of Rohtas, which was built in 1541–43 by Sher Shah Suri to crush the Gakhars who were loyal to Humayun, was surrendered without a shot by a treacherous commander. The walls of the Rohtas Fort measure up to 12.5 meters in thickness and up to 18.28 meters in height. They extend for 4 km and feature 68 semi-circular bastions. Its sandstone gates, both massive and ornate, are thought to have exerted a profound influence on Mughal military architecture.
The only major battle faced by Humayun's armies was against Sikander Suri in Sirhind, where Bairam Khan employed a tactic whereby he engaged his enemy in open battle, but then retreated quickly in apparent fear. When the enemy followed after them they were surprised by entrenched defensive positions and were easily annihilated.
From here on most towns and villages chose to welcome the invading army as it made its way to the capital. On 23 July 1555, Humayun once again sat on Babur's throne in Delhi.
Ruling Kashmir.
With all of Humayun's brothers now dead, there was no fear of another usurping his throne during his military campaigns. He was also now an established leader, and could trust his generals. With this new-found strength Humayun embarked on a series of military campaigns aimed at extending his reign over areas in eastern and western India. His sojourn in exile seems to have reduced Humayun's reliance on astrology, and his military leadership came to imitate the more effective methods that he had observed in Persia.
Trusted generals.
After defeating Bahadur Shah's confederacy in Gujarat, Humayun placed the following generals in Gujarat:
Foreign relations.
In the year 1540, the Mughal Emperor Humayun met the Ottoman Admiral Seydi Ali Reis. During their discussions in the Durbar, Humayun asked which of the two empires was bigger and Seydi Ali Reis, stated that the Ottoman Empire was "ten times bigger", Humayun was very inspired and he turned towards his nobles and remarked without resentment: "Indeed Suleiman the Magnificent, deserves to be called the only Padshah on Earth".
Death and legacy.
On 27 January 1556, Humayun, with his arms full of books, was descending the staircase from his library when the muezzin announced the Azaan (the call to prayer). It was his habit, wherever he heard the summons, to bow his knee in holy reverence. Trying to kneel, he caught his foot in his robe, tumbled down several steps and hit his temple on a rugged stone edge. He died three days later. His body was laid to rest in Purana Quila initially, but because of attack by Hemu on Delhi and capture of Purana Qila, Humayun's body was exhumed by the fleeing army and transferred to Kalanaur in Punjab where Akbar was coronated. His tomb stands in Delhi, where he was later buried in a grand way.
Full title.
His full title as Emperor of the Mughal Empire was:
"Al-Sultan al-'Azam wal Khaqan al-Mukarram, Jam-i-Sultanat-i-haqiqi wa Majazi, Sayyid al-Salatin, Abu'l Muzaffar Nasir ud-din Muhammad Humayun Padshah Ghazi, Zillu'llah"

</doc>
<doc id="14056" url="https://en.wikipedia.org/wiki?curid=14056" title="Prince-elector">
Prince-elector

The prince-electors (or simply electors) of the Holy Roman Empire ( (), pl. "Kurfürsten", , ) were the members of the electoral college of the Holy Roman Empire. 
From the 13th century onwards, the Prince-Electors had the privilege of electing the King of the Romans, who would be crowned by the Pope as Holy Roman Emperor. Charles V was the last to be a crowned Emperor (elected 1519, crowned 1530); his successors were elected Emperors directly by the electoral college, each being titled "Elected Emperor of the Romans" (; ). In practice, all but one Emperor after 1440 came from the Austrian House of Habsburg, and the Electors merely ratified the Habsburg succession.
The dignity of Elector carried great prestige and was considered to be second only to that of King or Emperor. The Electors had exclusive privileges that were not shared with the other princes of the Empire, and they continued to hold their original titles alongside that of Elector. The heir apparent to a secular prince-elector was known as an electoral prince (). 
History.
The German practice of electing monarchs began when ancient Germanic tribes formed "ad hoc" coalitions and elected the leaders thereof. Elections were irregularly held by the Franks, whose successor states include France and Germany. The French monarchy eventually became hereditary, but the German monarchy remained elective. While all free men originally exercised the right to vote in such elections, suffrage eventually came to be limited to the leading men of the realm. In the election of Lothar II in 1125, a small number of eminent nobles chose the monarch and then submitted him to the remaining magnates for their approbation.
Soon, the right to choose the monarch was settled on an exclusive group of princes, and the procedure of seeking the approval of the remaining nobles was abandoned. The college of electors was mentioned in 1152 and again in 1198. The composition of electors at that time is unclear, but appears to have included representatives of the church and the dukes of the four nations of Germany: the Franks (Duchy of Franconia), Swabians (Duchy of Swabia), Saxons (Duchy of Saxony) and Bavarians (Duchy of Bavaria).
1257 to Thirty Years' War.
A letter of Pope Urban IV suggests that by "immemorial custom", seven princes had the right to elect the King and future Emperor. The seven have been mentioned as the vote-casters in the election of 1257 that resulted in two kings becoming elected.
The three Archbishops oversaw the most venerable and powerful sees in Germany, while the other four were supposed to represent the dukes of the four nations. The Count Palatine of the Rhine held most of the former Duchy of Franconia after the last Duke died in 1039. The Margrave of Brandenburg became an Elector when the Duchy of Swabia was dissolved after the last Duke of Swabia was beheaded in 1268. Saxony, even with diminished territory, retained its eminent position.
The Palatinate and Bavaria were originally held by the same individual, but in 1253, they were divided between two members of the House of Wittelsbach. The other electors refused to allow two princes from the same dynasty to have electoral rights, so a heated rivalry arose between the Count Palatine and the Duke of Bavaria over who should hold the Wittelsbach seat.
Meanwhile, the King of Bohemia, who held the ancient imperial office of Arch-Cupbearer, asserted his right to participate in elections. Sometimes he was challenged on the grounds that his kingdom was not German, though usually he was recognized, instead of Bavaria which after all was just a younger line of Wittelsbachs.
The Declaration of Rhense issued in 1338 had the effect that election by the majority of the electors automatically conferred the royal title and rule over the empire, without papal confirmation. The Golden Bull of 1356 finally resolved the disputes among the electors. Under it, the Archbishops of Mainz, Trier, and Cologne, as well as the King of Bohemia, the Count Palatine of the Rhine, the Duke of Saxony, and the Margrave of Brandenburg held the right to elect the King.
The college's composition remained unchanged until the 17th century, although the Electorate of Saxony was transferred from the senior to the junior branch of the Wettin family in 1547, in the aftermath of the Schmalkaldic War.
Thirty Years' War to Napoleon.
In 1621, the Elector Palatine, Frederick V, came under the imperial ban after participating in the Bohemian Revolt (a part of the Thirty Years' War). The Elector Palatine's seat was conferred on the Duke of Bavaria, the head of a junior branch of his family. Originally, the Duke held the electorate personally, but it was later made hereditary along with the duchy. When the Thirty Years' War concluded with the Peace of Westphalia in 1648, a new electorate was created for the Count Palatine of the Rhine. Since the Elector of Bavaria retained his seat, the number of electors increased to eight; the two Wittelsbach lines now sufficiently estranged so as not to pose a combined potential threat.
In 1685, the religious composition of the College of Electors was disrupted when a Catholic branch of the Wittelsbach family inherited the Palatinate. A new Protestant electorate was created in 1692 for the Duke of Brunswick-Lüneburg, who became known as the Elector of Hanover (the Imperial Diet officially confirmed the creation in 1708). The Elector of Saxony converted to Catholicism in 1697 so that he could become King of Poland, but no additional Protestant electors were created. Although the Elector of Saxony was personally Catholic, the Electorate itself remained officially Protestant, and the Elector even remained the leader of the Protestant body in the Reichstag.
In 1706, the Elector of Bavaria and Archbishop of Cologne were banned during the War of the Spanish Succession, but both were restored in 1714 after the Peace of Baden. In 1777, the number of electors was reduced to eight when the Elector Palatine inherited Bavaria.
Many changes to the composition of the college were necessitated by Napoleon's aggression during the early 19th century. The Treaty of Lunéville (1801), which ceded territory on the Rhine's left bank to France, led to the abolition of the archbishoprics of Trier and Cologne, and the transfer of the remaining spiritual Elector from Mainz to Regensburg. In 1803, electorates were created for the Duke of Württemberg, the Margrave of Baden, the Landgrave of Hesse-Kassel, and the Duke of Salzburg, bringing the total number of electors to ten. When Austria annexed Salzburg under the Treaty of Pressburg (1805), the Duke of Salzburg moved to the Grand Duchy of Würzburg and retained his electorate. None of the new electors, however, had an opportunity to cast votes, as the Holy Roman Empire was abolished in 1806, and the new electorates were never confirmed by the Emperor.
After the Empire.
After the abolition of the Holy Roman Empire in August 1806, the Electors continued to reign over their territories, many of them taking higher titles. The Electors of Bavaria, Württemberg, and Saxony styled themselves Kings, while the Electors of Baden, Hesse-Darmstadt, Regensburg, and Würzburg became Grand Dukes. The Elector of Hesse-Kassel, however, retained the meaningless title "Elector of Hesse", thus distinguishing himself from other Hessian princes (the Grand Duke of Hesse-Darmstadt and the Landgrave of Hesse-Homburg). Napoleon soon exiled him and Kassel was annexed to the Kingdom of Westphalia, a new creation. The King of Great Britain remained at war with Napoleon and continued to style himself Elector of Hanover, while the Hanoverian government continued to operate in London.
The Congress of Vienna accepted the Electors of Bavaria, Württemberg, and Saxony as Kings, along with the newly-created Grand Dukes. The Elector of Hanover finally joined his fellow Electors by declaring himself the King of Hanover. The restored Elector of Hesse, a Napoleonic creation, tried to be recognized as the King of the Chatti. However, the European powers refused to acknowledge this title at the Congress of Aix-la-Chapelle (1818) and instead listed him with the grand dukes as a "Royal Highness." Believing the title of Prince-Elector to be superior in dignity to that of Grand Duke, the Elector of Hesse-Kassel chose to remain an Elector, even though there was no longer a Holy Roman Emperor to elect. Hesse-Kassel remained the only Electorate in Germany until 1866, when the country backed the losing side in the Austro-Prussian War and was absorbed into Prussia.
Etymology of "Kurfürst".
The German element "Kur-" is based on the Middle High German irregular verb "kiesen" and is related etymologically to the English word "choose" (cf. Old English "ceosan" , participle "coren" 'having been chosen' and Gothic "kiusan"). In English, the "s"/"r" mix in the Germanic verb conjugation has been regularized to "s" throughout, while German retains the "r" in "Kur-". There is also a modern German verb "küren" which means 'to choose' in a ceremonial sense. "Fürst" is German for 'prince', but while the German language distinguishes between the head of a principality ("der Fürst") and the son of a monarch ("der Prinz"), English uses "prince" for both concepts. "Fürst" itself is related to English "first" and is thus the 'foremost' person in his realm. Note that 'prince' derives from Latin "princeps", which carried the same meaning.
Rights and privileges.
Electors were "reichsstände" (Imperial Estates), enjoying precedence over the other princes. They were, until the 18th century, exclusively entitled to be addressed with the title "Durchlaucht" (Serene Highness). In 1742, the electors became entitled to the superlative "Durchläuchtigste" (Most Serene Highness), while other princes were promoted to "Durchlaucht".
As Imperial Estates, the electors enjoyed all the privileges of the other princes enjoying that status, including the right to enter into alliances, autonomy in relation to dynastic affairs and precedence over other subjects. The Golden Bull had granted them the Privilegium de non appellando, which prevented their subjects from lodging an appeal to a higher Imperial court. However, while this privilege, and some others, were automatically granted to Electors, they were not exclusive to them and many of the larger Imperial Estates were also to be individually granted some or all those rights and privileges.
Imperial Diet.
The electors, like the other princes ruling States of the Empire, were members of the Imperial Diet, which was divided into three "collegia": the Council of Electors, the Council of Princes, and the Council of Cities. In addition to being members of the Council of Electors, several lay electors were therefore members of the Council of Princes as well by virtue of other territories they possessed. In many cases, the lay electors ruled numerous States of the Empire, and therefore held several votes in the Council of Princes. In 1792, the King of Bohemia held three votes, the Elector of Bavaria six votes, the Elector of Brandenburg eight votes, and the Elector of Hanover six votes. Thus, of the hundred votes in the Council of Princes in 1792, twenty-three belonged to electors. The lay electors therefore exercised considerable influence, being members of the small Council of Electors and holding a significant number of votes in the Council of Princes. The assent of both bodies was required for important decisions affecting the structure of the Empire, such as the creation of new electorates or States of the Empire.
In addition to voting by colleges or councils, the Imperial Diet also voted on religious lines, as provided for by the Peace of Westphalia. The Archbishop of Mainz presided over the Catholic body, or "corpus catholicorum", while the Elector of Saxony presided over the Protestant body, or "corpus evangelicorum". The division into religious bodies was on the basis of the official religion of the state, and not of its rulers. Thus, even when the Electors of Saxony were Catholics during the eighteenth century, they continued to preside over the "corpus evangelicorum", since the state of Saxony was officially Protestant.
Elections.
The individual chosen by the electors assumed the title "King of the Romans", though he actually reigned in Germany. The King of the Romans became Holy Roman Emperor only when crowned by the Pope. On many occasions, a Pope refused to crown a king with whom he was engaged in a dispute, but a lack of a papal coronation deprived a king of only the title Emperor and not of the power to govern (cf Declaration of Rhense). The Habsburg dynasty stopped the practice of papal coronations. After Charles V, all individuals chosen by the electors were merely "Emperors elect".
The electors were originally summoned by the Archbishop of Mainz within one month of an Emperor's death, and met within three months of being summoned. During the "interregnum", imperial power was exercised by two imperial vicars. Each vicar, in the words of the Golden Bull, was "the administrator of the empire itself, with the power of passing judgments, of presenting to ecclesiastical benefices, of collecting returns and revenues and investing with fiefs, of receiving oaths of fealty for and in the name of the holy empire". The Elector of Saxony was vicar in areas operating under Saxon law (Saxony, Westphalia, Hanover, and northern Germany), while the Elector Palatine was vicar in the remainder of the Empire (Franconia, Swabia, the Rhine, and southern Germany). The Elector of Bavaria replaced the Elector Palatine in 1623, but when the latter was granted a new electorate in 1648, there was a dispute between the two as to which was vicar. In 1659, both purported to act as vicar, but the other vicar recognised the Elector of Bavaria. Later, the two electors made a pact to act as joint vicars, but the Imperial Diet rejected the agreement. In 1711, while the Elector of Bavaria was under the ban of the Empire, the Elector Palatine again acted as vicar, but his cousin was restored to his position upon his restoration three years later. Finally, in 1745, the two agreed to alternate as vicars, with Bavaria starting first. This arrangement was upheld by the Imperial Diet in 1752. In 1777 the question became moot when the Elector Palatine inherited Bavaria. On many occasions, however, there was no interregnum, as a new king had been elected during the lifetime of the previous Emperor.
Frankfurt regularly served as the site of the election from the fifteenth century on, but elections were also held at Cologne (1531), Regensburg (1575 and 1636), and Augsburg (1653 and 1690). An elector could appear in person or could appoint another elector as his proxy. More often, an electoral suite or embassy was sent to cast the vote; the credentials of such representatives were verified by the Archbishop of Mainz, who presided over the ceremony. The deliberations were held at the city hall, but voting occurred in the cathedral. In Frankfurt, a special electoral chapel, or "Wahlkapelle", was used for elections. Under the Golden Bull, a majority of electors sufficed to elect a king, and each elector could cast only one vote. Electors were free to vote for whomsoever they pleased (including themselves), but dynastic considerations played a great part in the choice. Electors drafted a "Wahlkapitulation", or electoral capitulation, which was presented to the king-elect. The capitulation may be described as a contract between the princes and the king, the latter conceding rights and powers to the electors and other princes. Once an individual swore to abide by the electoral capitulation, he assumed the office of King of the Romans.
In the 10th and 11th centuries, princes often acted merely to confirm hereditary succession in the Saxon Ottonian dynasty and Franconian Salian dynasty. But with the actual formation of the prince-elector class, elections became more open, starting with the election of Lothair II in 1125. The Staufen dynasty managed to get its sons formally elected in their fathers' lifetimes almost as a formality. After these lines ended in extinction, the electors began to elect kings from different families so that the throne would not once again settle within a single dynasty. For some two centuries, the monarchy was elective both in theory and in practice; the arrangement, however, did not last, since the powerful House of Habsburg managed to secure succession within their dynasty during the fifteenth century. All kings elected from 1438 onwards were from among the Habsburg Archdukes of Austria (and later Kings of Hungary and Bohemia) until 1740, when the archduchy was inherited by a woman, Maria Theresa, sparking the War of the Austrian Succession. A representative of the House of Wittelsbach was elected for a short period of time, but in 1745, Maria Theresa's husband, Francis I of the Habsburg-Lorraine dynasty, became King. All of his successors were also from the same family. Hence, for the greater part of the Empire's history, the role of the electors was largely ceremonial.
High offices.
Each elector held a "High Office of the Empire" ("Reichserzämter") and was a member of the (ceremonial) Imperial Household. The three spiritual electors were all Arch-Chancellors (, ): the Archbishop of Mainz was Arch-Chancellor of Germany, the Archbishop of Cologne was Arch-Chancellor of Italy, and the Archbishop of Trier was Arch-Chancellor of Burgundy. The other offices were as follows:
When the Duke of Bavaria replaced the Elector Palatine in 1623, he assumed the latter's office of Arch-Steward. When the Count Palatine was granted a new electorate, he assumed the position of Arch-Treasurer of the Empire. When the Duke of Bavaria was banned in 1706, the Elector Palatine returned to the office of Arch-Steward, and in 1710 the Elector of Hanover was promoted to the post of Arch-Treasurer. Matters were complicated by the Duke of Bavaria's restoration in 1714; the Elector of Bavaria resumed the office of Arch-Steward, while the Elector Palatine returned to the post of Arch-Treasurer, and the Elector of Hanover was given the new office of Archbannerbearer. The Electors of Hanover, however, continued to be styled Arch-Treasurers, though the Elector Palatine was the one who actually exercised the office until 1777, when he inherited Bavaria and the Arch-Stewardship. After 1777, no further changes were made to the Imperial Household; new offices were planned for the Electors admitted in 1803, but the Empire was abolished before they could be created. The Duke of Württemberg, however, started to adopt the trappings of the Arch-Bannerbearer.
Many High Officers were entitled to use augmentations on their coats of arms; these augmentations, which were special marks of honour, appeared in the centre of the electors' shields (as shown in the image above) above the other charges (in heraldic terms, the augmentations appeared in the form of inescutcheons). The Arch-Steward used "gules an orb Or" (a gold orb on a red field). The Arch-Marshal utilised the more complicated "per fess sable and argent, two swords in saltire gules" (two red swords arranged in the form of a saltire, on a black and white field). The Arch-Chamberlain's augmentation was "azure a sceptre palewise Or" (a gold sceptre on a blue field), while the Arch-Treasurer's was "gules the crown of Charlemagne Or" (a gold crown on a red field). As noted above, the Elector Palatine and the Elector of Hanover styled themselves Arch-Treasurer from 1714 until 1777; during this time, both electors used the corresponding augmentations. The three Arch-Chancellors and the Arch-Cupbearer did not use any augmentations.
The electors discharged the ceremonial duties associated with their offices only during coronations, where they bore the crown and regalia of the Empire. Otherwise, they were represented by holders of corresponding "Hereditary Offices of the Household". The Arch-Butler was represented by the Butler (Cupbearer) (the Count of Althann), the Arch-Seneschal by the Steward (the Count of Waldburg), the Arch-Chamberlain by the Chamberlain (the Count of Hohenzollern), the Arch-Marshal by the Marshal (the Count of Pappenheim), and the Arch-Treasurer by the Treasurer (the Count of Sinzendorf). The Duke of Württemberg assigned the count of Zeppelin-Aschhausen as hereditary Bannerbearer.
Coats of arms.
Coat of arms of the states granted the electoral dignity:
Three ecclesiastic/spiritual electors (archbishops):
Four secular electors:
Electors added in 17th century:
During the collapse of the Holy Roman Empire, between 1803 and 1806:

</doc>
