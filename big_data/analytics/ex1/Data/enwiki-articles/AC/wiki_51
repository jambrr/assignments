<doc id="15622" url="https://en.wikipedia.org/wiki?curid=15622" title="James Cameron">
James Cameron

James Francis Cameron (born August 16, 1954) is a Canadian filmmaker, director, producer, screenwriter, inventor, engineer, philanthropist, and deep-sea explorer. He first found major success with the science fiction action film "The Terminator" (1984). He then became a popular Hollywood director and was hired to write and direct "Aliens" (1986); three years later he followed up with "The Abyss" (1989).
He found further critical acclaim for his use of special effects in "" (1991). After his film "True Lies" (1994) Cameron took on his biggest film at the time, "Titanic" (1997), which earned him Academy Awards for Best Picture, Best Director and Film Editing.
After "Titanic", Cameron began a project that took almost 10 years to make: his science-fiction epic "Avatar" (2009), which was in particular a landmark for 3D technology, and for which he received nominations for the same three Academy Awards. In the time between making "Titanic" and "Avatar", Cameron spent several years creating many documentary films (specifically underwater documentaries) and co-developed the digital 3D Fusion Camera System. Described by a biographer as part scientist and part artist, Cameron has also contributed to underwater filming and remote vehicle technologies. On March 26, 2012, Cameron reached the bottom of the Mariana Trench, the deepest part of the ocean, in the "Deepsea Challenger" submersible. He is the first person to do this in a solo descent, and is only the third person to do so ever.
In total, Cameron's directorial efforts have grossed approximately US$2 billion in North America and US$6 billion worldwide. Not adjusted for inflation, Cameron's "Titanic" and "Avatar" are the two highest-grossing films of all time at $2.19 billion and $2.78 billion respectively. Cameron also holds the achievement of having directed two of the three films in history to gross over $2 billion worldwide. In March 2011, he was named Hollywood's top earner by "Vanity Fair", with estimated 2010 earnings of $257 million. In October 2013, a new species of frog "Pristimantis jamescameroni" from Venezuela was named after him in recognition of his efforts in environmental awareness, in addition to his public promotion of veganism.
Early life.
Cameron was born in 1954 in Kapuskasing, Ontario, the son of Shirley (née Lowe), an artist and nurse, and Phillip Cameron, an electrical engineer. His paternal great-great-great-grandfather emigrated from Balquhidder, Scotland, in 1825.
Cameron grew up in Chippawa, Ontario, and attended Stamford Collegiate School in Niagara Falls, Ontario. His family moved to Brea, California in 1971, when Cameron was 17 years old. He dropped out of Sonora High School, then attended Brea Olinda High School to further his secondary education.
Cameron enrolled at Fullerton College, a two-year community college, in 1973 to study physics. He switched to English, then dropped out before the start of the fall 1974 semester. Next, he worked several jobs, including as a truck driver, writing when he had time. During this period he taught himself about special effects: "I'd go down to the USC library and pull any thesis that graduate students had written about optical printing, or front screen projection, or dye transfers, anything that related to film technology. That way I could sit down and read it, and if they'd let me photocopy it, I would. If not, I'd make notes."
Cameron quit his job as a truck driver to enter the film industry after seeing "Star Wars" in 1977. When Cameron read Syd Field's book "Screenplay", it occurred to him that integrating science and art was possible, and he wrote a 10-minute science-fiction script with two friends, titled "Xenogenesis". They raised money, rented camera, lenses, film stock and studio then shot it in 35mm. They dismantled the camera to understand how to operate it and spent the first half-day of the shoot trying to figure out how to get it running.
Early career.
He was the director, writer, producer, and production designer for "Xenogenesis" (1978). He then became a production assistant on a film called "Rock and Roll High School", though uncredited, in 1979. While continuing to educate himself in film-making techniques, Cameron started working as a miniature-model maker at Roger Corman Studios. Making rapidly produced, low-budget productions taught Cameron to work efficiently and effectively. He soon found employment as an art director in the sci-fi movie "Battle Beyond the Stars" (1980). He did special effects work design and direction on John Carpenter's "Escape from New York" (1981), acted as production designer on "Galaxy of Terror" (1981), and consulted on the design of "Android" (1982).
Cameron was hired as the special effects director for the sequel to "Piranha", entitled "" in 1981. The original director, Miller Drake, left the project due to creative differences with producer Ovidio Assonitis, who then gave Cameron his first job as director. The interior scenes were filmed in Rome, Italy, while the underwater sequences were shot at Grand Cayman Island.
The movie was to be produced in Jamaica. On location, production slowed due to numerous problems and adverse weather. James Cameron was fired after failing to get a close up of Carole Davis in her opening scene. Ovidio ordered Cameron to do the close-up the next day before he started on that day’s shooting. Cameron spent the entire day sailing around the resort to reproduce the lighting but still failed to get the close-up. After he was fired, Ovidio invited Cameron to stay on location and assist in the shooting. Once in Rome, Ovidio took over the editing when Cameron was stricken with food poisoning. During his illness, Cameron had a nightmare about an invincible robot hitman sent from the future to kill him, giving him the idea for "The Terminator", which later catapulted his film career.
Major films.
"The Terminator" (1984).
After completing a screenplay for "The Terminator", Cameron decided to sell it so that he could direct the movie. However, the production companies he contacted, while expressing interest in the project, were unwilling to let a largely inexperienced feature film director make the movie. Finally, Cameron found a company called Hemdale Pictures, which was willing to let him direct. Gale Anne Hurd, who had started her own production company, Pacific Western Productions, had previously worked with Cameron in Roger Corman's company and agreed to buy Cameron's screenplay for one dollar, on the condition that Cameron direct the film. Hurd was signed on as producer, and Cameron finally got his first break as director. Orion Pictures distributed the film. Hurd and Cameron were married from 1985 to 1989.
For the role of the Terminator, Cameron envisioned a man who was not exceptionally muscular, who could "blend into" a crowd. Lance Henriksen, who had starred in "Piranha II: The Spawning", was considered for the title role, but when Arnold Schwarzenegger and Cameron first met over lunch to discuss Schwarzenegger's playing the role of Kyle Reese, both came to the conclusion that the cyborg villain would be the more compelling role for the Austrian bodybuilder; Henriksen got the smaller part of LAPD detective Hal Vukovich and the role of Kyle Reese went to Michael Biehn. In addition, Linda Hamilton first appeared in this film in her iconic role of Sarah Connor, and later married Cameron.
"The Terminator" was a box-office hit, breaking expectations by Orion Pictures executives that the film would be regarded as no more than a sci-fi film and only last a week in theaters. It was a low-budget film which cost $6.5 million to make, cutting expenses in such ways as recording the audio track in mono. However, "The Terminator" eventually earned over $78 million worldwide.
"Rambo: First Blood Part II" (1985).
During the early 1980s, Cameron wrote three screenplays simultaneously: "The Terminator", "Aliens", and the first draft of '. While Cameron continued with "The Terminator" and "Aliens", Sylvester Stallone eventually took over the script of ', creating a final draft which differed radically from Cameron's initial vision.
"Aliens" (1986).
Cameron next began the sequel to "Alien", the 1979 film by Ridley Scott. Cameron named the sequel "Aliens" and again cast Sigourney Weaver in the iconic role of Ellen Ripley. According to Cameron, the crew on "Aliens" was hostile to him, regarding him as a poor substitute for Ridley Scott. Cameron sought to show them "The Terminator" but the majority of the crew refused to watch it and remained skeptical of his direction throughout production. Despite this and other off-screen problems (such as clashing with an uncooperative camera man and having to replace one of the lead actors when Michael Biehn of "Terminator" took James Remar's place as Corporal Hicks), "Aliens" became a box-office success. It received Academy Award nominations for Best Actress in a Leading Role for Weaver, Best Art Direction, Best Film Editing, Best Original Score, Best Sound, and won awards for Best Sound Effects Editing and Best Visual Effects. In addition, the film and its lead actress made the cover of "TIME" magazine as a result of its numerous and extensive scenes of women in combat; these were almost without precedent and expressed the feminist theme of the film very strongly.
"The Abyss" (1989).
Cameron's next project stemmed from an idea that had come up during a high school biology class. The story of oil-rig workers who discover otherworldly underwater creatures became the basis of Cameron's screenplay for "The Abyss", which cast Ed Harris, Mary Elizabeth Mastrantonio and Michael Biehn. Initially budgeted at $41 million U.S. (though the production ran considerably over budget), it was considered to be one of the most expensive films of its time and required cutting-edge effects technology. Because much of the filming took place underwater and the technology wasn't advanced enough to digitally create an underwater environment, Cameron chose to shoot much of the movie "reel-for-real", at depths of up to . For creation of the sets, the containment building of an unfinished nuclear power plant was converted, and two huge tanks were used. The main tank was filled with of water and the second with . The cast and crew resided there for much of the filming.
"Terminator 2: Judgment Day" (1991).
After the success of "The Terminator", there had been talk about a sequel to continue the story of Sarah Connor and her struggle against machines from the future. Although Cameron had come up with a core idea for the sequel and Schwarzenegger expressed interest in continuing the story, there were still problems regarding who had the rights to the story, as well as the logistics of the special effects needed to make the sequel. Finally, in late-1980s, Mario Kassar of Carolco Pictures secured the rights to the sequel, allowing Cameron to greenlight production of the film, now called "".
For the film, Linda Hamilton reprised her iconic role of Sarah Connor. In addition, Schwarzenegger also returned in his role as The Terminator, but this time as a protector. Unlike Schwarzenegger's character—the T-800 Terminator which is made of a metal endoskeleton—the new villain of the sequel, called the T-1000, is a more-advanced Terminator made of liquid metal, and with polymorphic abilities. The T-1000 would also be much less bulky than the T-800. For the role, Cameron cast Robert Patrick, a sharp contrast to Schwarzenegger. Cameron explained, "I wanted someone who was extremely fast and agile. If the T-800 is a human Panzer tank, then the T-1000 is a Porsche."
Cameron had originally wanted to incorporate this advanced-model Terminator into the first film, but the special effects at the time were not advanced enough. The ground-breaking effects used in "The Abyss" to digitally depict the water tentacle convinced Cameron that his liquid metal villain was now possible.
TriStar Pictures agreed to distribute the film, but under a locked release date only about one year after the start of shooting. The movie, co-written by Cameron and his longtime friend, William Wisher, Jr., had to go from screenplay to finished film in just that amount of time. Like Cameron's previous film, it was one of the most expensive films of its era, with a budget of about $100 million. The biggest challenge of the movie was the special effects used in creating the T-1000. Nevertheless, the film was finished on time and released to theaters on July 3, 1991.
"Terminator 2", or "T2", as it was abbreviated, broke box-office records (including the opening weekend record for an R-rated film), earning over $200 million in the United States and Canada, and over $300 million in other territories, and became the highest-grossing film of that year. It won four Academy Awards: Best Makeup, Best Sound, Best Sound Effects Editing, and Best Visual Effects. It was also nominated for Best Cinematography and Best Film Editing, but lost both Awards to "JFK".
James Cameron announced a third "Terminator" film many times during the 1990s, but without coming out with any finished scripts. Kassar and Vajna purchased the rights to the Terminator franchise from a bankruptcy sale of Carolco's assets. "" was eventually made and released in July 2003 without Cameron's involvement. Jonathan Mostow directed the film and Schwarzenegger returned as the Terminator.
Cameron reunited with the main cast of "Terminator 2" to film "", an attraction at Universal Studios Florida, Universal Studios Hollywood and Universal Studios Japan. It was released in 1996 and was a mini-sequel to "Terminator 2: Judgment Day". The show is in two parts: a prequel segment in which a spokesperson talks about Cyberdyne, and a main feature, in which the performers interact with a 3-D movie.
"True Lies" (1994).
Before the release of "T2", Schwarzenegger came to Cameron with the idea of remaking the French comedy "La Totale!" Titled "True Lies", with filming beginning after "T2"'s release, the story revolves around a secret-agent spy who leads a double life as a married man, whose wife believes he is a computer salesman. Schwarzenegger was cast as Harry Tasker, a spy charged with stopping a plan by a terrorist to use nuclear weapons against the United States. Jamie Lee Curtis and Eliza Dushku played the character's family, and Tom Arnold the sidekick.
Cameron's Lightstorm Entertainment signed on with Twentieth Century Fox for production of "True Lies". Made on a budget of $115 million and released in 1994, the film earned $146 million in North America, and $232 million abroad. The film received an Academy Award nomination for Best Visual Effects.
"Strange Days" (1995).
An American science-fiction action thriller film directed by Kathryn Bigelow. It was co-written and produced by her ex-husband James Cameron and co-written by Jay Cocks.
"Titanic" (1997).
Cameron expressed interest in the famous sinking of the ship . He decided to script and film his next project based on this event. The picture revolved around a fictional romance story between two young lovers from different social classes who meet on board. Before production began, he took dives to the bottom of the Atlantic and shot actual footage of the ship underwater, which he inserted into the final film. Much of the film's dialogue was also written during these dives.
Subsequently, Cameron cast Leonardo DiCaprio, Kate Winslet, Billy Zane, Kathy Bates, Frances Fisher, Gloria Stuart, Bernard Hill, Jonathan Hyde, Victor Garber, Danny Nucci, David Warner, Suzy Amis, and Bill Paxton as the film's principal cast. Cameron's budget for the film reached about $200 million, making it the most expensive movie ever made at the time. Before its release, the film was widely ridiculed for its expense and protracted production schedule.
Released to theaters on December 19, 1997, "Titanic" grossed less in its first weekend ($28.6 million) than in its second ($35.4 million), an increase of 23.8%. This is unheard of for a widely released film, which is a testament to the movie's appeal. This was especially noteworthy, considering that the film's running time of more than three hours limited the number of showings each theater could schedule. It held the No. 1 spot on the box-office charts for months, eventually grossing a total of $600.8 million in the United States and Canada and more than $1.84 billion worldwide. "Titanic" became the highest-grossing film of all time, both worldwide and in the United States and Canada, and was also the first film to gross more than $1 billion worldwide. It remained the highest-grossing film since 1998, until Cameron's 2009 film "Avatar" surpassed its gross in 2010.
The CG visuals surrounding the sinking and destruction of the ship were considered spectacular. Despite criticism during production of the film, it received a record-tying 14 Oscar nominations (tied with "All About Eve") at the 1998 Academy Awards. It won 11 Oscars (also tying the record for most Oscar wins with "Ben-Hur" and later ""), including: Best Picture, Best Director, Best Art Direction, Best Cinematography, Best Visual Effects, Best Film Editing, Best Costume Design, Best Sound, Best Sound Effects Editing, Best Original Dramatic Score, Best Original Song. Upon receiving the Best Director Oscar, Cameron exclaimed, "I'm king of the world!", in reference to one of the main characters' lines from the film. After receiving the Best Picture Oscar along with Jon Landau, Cameron asked for a moment of silence for the 1,500 men, women, and children who died when the ship sank.
In March 2010, Cameron revealed that "Titanic" would be re-released in 3D in April 2012, to commemorate the 100th anniversary of the sinking of the real ship. On March 27, 2012, Cameron attended the world première with Kate Winslet at the Royal Albert Hall in London. Following the re-release, "Titanic's" domestic total was pushed to $658.6 million and more than $2.18 billion worldwide. It became the second film to gross more than $2 billion worldwide (the first being Avatar).
"Spider-Man" and "Dark Angel" (2000–2002).
Cameron had initially next planned to do a film of the comic-book character Spider-Man, a project developed by Menahem Golan of Cannon Films. Columbia hired David Koepp to adapt Cameron's treatment into a screenplay, and Koepp's first draft is taken often word-for-word from Cameron's story, though later drafts were heavily rewritten by Koepp himself, Scott Rosenberg, and Alvin Sargent. Columbia preferred to credit David Koepp solely, and none of the scripts before or after his were ever examined by the Writers Guild of America, East to determine proper credit attribution. Cameron and other writers objected, but Columbia and the WGA prevailed. In its release in 2002, "Spider-Man" had its screenplay credited solely to Koepp.
Unable to make "Spider-Man", Cameron moved to television and created "Dark Angel", a superheroine-centered series influenced by cyberpunk, biopunk, contemporary superhero franchises, and third-wave feminism. Co-produced with Charles H. Eglee, "Dark Angel" starred Jessica Alba as Max Guevara, a genetically enhanced super-soldier created by a secretive organization. Cameron's work was said to "bring empowered female warriors back to television screens[...] by mixing the sober feminism of his "The Terminator" and "Aliens" characters with the sexed-up Girl Power of a Britney Spears concert." While a success in its first season, low ratings in the second led to its cancellation. Cameron himself directed the series finale, a two-hour episode wrapping up many of the series' loose ends.
Documentaries (2002–2012).
In 1998 James and John David Cameron formed a digital media company, earthship.tv, which became Earthship Productions. The company produced live multimedia documentaries from the depths of the Atlantic and Pacific oceans. With Earthship Productions, John Cameron's recent projects have included undersea documentaries on the ("", 2002) and the ("Ghosts of the Abyss" (2003, in IMAX 3D) and "Tony Robinson's Titanic Adventure" (2005)). He was a producer on the 2002 film "Solaris", and narrated "The Exodus Decoded".
Cameron is an advocate for stereoscopic digital 3-D films. In a 2003 interview about his IMAX 2D documentary "Ghosts of the Abyss", he mentioned that he is "going to do everything in 3D now". He has made similar statements in other interviews. "Ghosts of the Abyss" and "Aliens of the Deep" (also an IMAX documentary) were both shot in 3-D and released by Walt Disney Pictures and Walden Media, and Cameron did the same for his new project, "Avatar" for 20th Century Fox & Sony Pictures' Columbia Pictures. He intends to use the same technology for "The Dive", "Sanctum" and an adaptation of the manga series "Battle Angel Alita".
Cameron was the co-founder and CEO of Digital Domain, a visual-effects production and technology company.
In addition, he plans to create a 3-D project about the first trip to Mars. ("I've been very interested in the Humans to Mars movement—the 'Mars Underground'—and I've done a tremendous amount of personal research for a novel, a miniseries, and a 3-D film.") He is on the science team for the 2011 Mars Science Laboratory.
Cameron announced on February 26, 2007, that he, along with his director, Simcha Jacobovici, have documented the unearthing of the Talpiot Tomb, which is alleged to be the tomb of Jesus. Unearthed in 1981 by Israeli construction workers, the names on the tomb are claimed, in the documentary, to correlate with the names of Jesus and several individuals closely associated with him. The documentary, named "The Lost Tomb of Jesus", was broadcast on the Discovery Channel on March 4, 2007.
As a National Geographic explorer-in-residence, Cameron re-investigated the sinking of the "Titanic" with eight experts in 2012. The investigation was featured in the TV documentary special "Titanic: The Final Word with James Cameron", which premiered on April 8 on the National Geographic Channel. In the conclusion of the analysis, the consensus revised the CGI animation of the sinking conceived in 1995.
"Avatar" (2009).
In June 2005, Cameron was announced to be working on a project tentatively titled "Project 880" (now known to be "Avatar") in parallel with another project, "Battle Angel" (an adaptation of the manga series Battle Angel Alita). Both movies were to be shot in 3D. By December, Cameron stated that he wanted to film "Battle Angel" first, followed by "Avatar". However, in February 2006, he switched goals for the two film projects and decided to film "Avatar" first. He mentioned that if both films were successful, he would be interested in seeing a trilogy being made for both.
"Avatar" had an estimated budget of over $300 million and was released on December 18, 2009. This marked his first feature film since 1997's "Titanic". It is composed almost entirely of computer-generated animation, using a more-advanced version of the "performance capture" technique used by director Robert Zemeckis in "The Polar Express". James Cameron had written an 80-page scriptment for "Avatar" in 1995 and announced in 1996 that he would make the film after completing "Titanic". In December 2006, Cameron explained that the delay in producing the film since the 1990s had been to wait until the technology necessary to create his project was advanced enough, since at the time no studio would finance for the development of the visual effects. The film was originally scheduled to be released in May 2009 but was pushed back to December 2009 to allow more time for post-production on the complex CGI and to give more time for theatres worldwide to install 3D projectors. Cameron originally intended "Avatar" to be 3D-only.
"Avatar" broke several box office records during its initial theatrical run. It grossed $749.7 million in the United States and Canada and more than $2.74 billion worldwide, becoming the highest-grossing film of all time in the United States and Canada, surpassing Cameron's "Titanic". "Avatar" also became the first movie to ever earn more than $2 billion worldwide. Including revenue from the re-release of "Avatar" featuring extended footage, it grossed $760.5 million in the U.S. and Canada and more than $2.78 billion worldwide. It was nominated for nine Academy Awards, including Best Picture and Best Director, and won three for Best Art Direction, Best Cinematography and Best Visual Effects.
Avatar's success made Cameron the highest earner in Hollywood for 2010, netting him $257 million as reported by "Vanity Fair".
Disney announced in September 2011 that it would adapt James Cameron's film "Avatar" into Pandora–The World of Avatar, a themed area at Disney's Animal Kingdom in Lake Buena Vista, Florida.
On April 14, 2016, during CinemaCon Cameron announced there will be four upcoming sequels to the Avatar franchise and not the originally planned three. Cameron said each of the four sequels will be able to stand alone, but will together create a saga. His goal is to release Avatar 2 during the holiday season of 2018 and then a new film in 2020, 2022 and 2023.
"Sanctum" (2011).
Cameron served as the executive producer of "Sanctum", a film detailing the expedition of a team of underwater cave divers who find themselves trapped in a cave, their exit blocked and with no known way to reach the surface either in person or by radio contact.
Planned films.
In August 2013, Cameron announced his intention to film three sequels to "Avatar" simultaneously, to be released in December 2016, 2017, and 2018. However, on January 14, 2015, Cameron announced that the release dates for the three sequels were each delayed a year with the first sequel scheduled to be released in December 2017. His original plans were to do "Battle Angel" next, but he changed his mind due to "Avatar"'s success; "My intention when I made "Avatar" was to do "Battle Angel" next. However, the positive feedback for "Avatar" and the support of the message of "Avatar", encouraged me to do more of those films." Cameron's Lightstorm Entertainment bought the film rights to the Taylor Stevens novel "The Informationist" in October 2012 with plans for Cameron to direct it. A screenwriter will be hired to adapt the novel while Cameron works on the "Avatar" sequels. Another project Cameron has announced is a personal commitment to shoot a film on the atomic bombings of Hiroshima and Nagasaki as told through the story of Tsutomu Yamaguchi, a man who survived both attacks. Cameron met with Yamaguchi just days before he died in 2010.
In 1996, James Cameron decided to produce the new installment in the Planet of the Apes franchise, but it was cancelled before the Tim Burton version was made.
On April 14th, 2016, Cameron announced that there would be 4 sequels to the movie Avatar, with release dates planned for 2018, 2020, 2022, and 2023.
Personal life.
Cameron has been married five times to the following spouses: Sharon Williams (1978–1984), Gale Anne Hurd (1985–1989), director Kathryn Bigelow (1989–1991), Linda Hamilton (1997–1999, daughter Josephine born in 1993), and Suzy Amis (2000–present). Cameron had dated Hamilton since 1991. Eight months after the marriage, however, they separated, and within days of Cameron's Oscar victory with "Titanic," the couple announced their divorce. As part of the divorce settlement, Cameron was ordered to pay Hamilton $50 million. Hamilton later revealed that the reason for their divorce was not only Cameron's blind devotion to his work to the exclusion of almost everything else, but also that he had been having an affair with Suzy Amis, an actress he cast as Lizzy Calvert in "Titanic." He married Amis in 2000, and they have one son and two daughters. Cameron lives in New Zealand, a country he fell in love with when he was filming "Avatar".
Hurd was the producer of Cameron's "The Terminator", "Aliens", and "The Abyss", and the executive producer of "". Hamilton played the role of Sarah Connor in both "Terminator" films. Amis played the part of Lizzy Calvert, Rose's granddaughter, in "Titanic". Both Cameron ("Avatar") and Bigelow ("The Hurt Locker") were nominated for the Oscar, the Golden Globe, and the BAFTA Award for Best Director for films released in 2009. Cameron won the Golden Globe, while Bigelow won the Oscar and the BAFTA for Best Director, becoming the first woman to win either.
Cameron is a member of the NASA Advisory Council and is working on the project to put cameras on an upcoming manned Mars mission. Cameron has also given speeches and raised money for the Mars Society, a non-profit organization lobbying for the colonization of Mars.
Cameron became an expert on deep-sea exploration in conjunction with his research and underwater filming for "The Abyss" (1989) and "Titanic" (1997). In June 2010, Cameron met in Washington with the EPA to discuss possible solutions to the 2010 Deepwater Horizon (BP) oil spill. Later that week at the All Things Digital Conference, he attracted some notoriety when he stated, "Over the last few weeks I've watched...and been thinking, 'Those morons don't know what they're doing'." Reportedly, Cameron had offered BP help to plug the oil well, but it declined. The oil spill was eventually stopped using techniques similar to those Cameron recommended.
Although Cameron had resided in the United States since 1971, he remains a Canadian citizen. Cameron applied for American citizenship but withdrew his application after George W. Bush won the presidential election in 2004.
Cameron calls himself "Converted Agnostic", and says "I've sworn off agnosticism, which I now call cowardly atheism". As a child he described the Lord's Prayer as being a "tribal chant".
In June 2013, British artist Roger Dean filed a legal action at a court in New York against Cameron. Dean accused Cameron of "wilful and deliberate copying, dissemination and exploitation" of his original images, relating to Cameron's 2009 film "Avatar" and sought damages of $50m. Dean subsequently lost the case.
Early in 2014, Cameron purchased the Beaufort Vineyard and Estate Winery in Courtenay, British Columbia, at a price of $2.7 million, as well as a number of other businesses in the area, including cattle ranching operations, to pursue his passion for sustainable agribusiness.
Deep sea dives.
On March 7, 2012, Cameron took the "Deepsea Challenger" submersible to the bottom of the New Britain Trench in a five-mile-deep solo dive. On March 26, 2012, Cameron reached the Challenger Deep, the deepest part of the Mariana Trench. He spent more than three hours exploring the ocean floor before returning to the surface. Cameron is the first person to accomplish the trip solo. He was preceded by unmanned dives in 1995 and 2009 and by Jacques Piccard and Don Walsh, who were the first men to reach the bottom of the Mariana Trench aboard the Bathyscaphe Trieste in 1960. Cameron has made a three-dimensional film of his dive. During his dive to the Challenger Deep, the data he collected resulted in interesting new finds in the field of marine biology, including new species of sea cucumber, squid worm, and giant single-celled amoeba, which are exciting finds due to the harshness of the environment. Cameron is also one of the two men in history to stand on Challenger Deep.
Veganism.
In 2012, Cameron, his wife and his children adopted a vegan diet. Cameron explains that "By changing what you eat, you will change the entire contract between the human species and the natural world".
When asked what’s the best thing an individual can do to fight climate change, Cameron said, “Stop eating animals.”
MUSE School.
In 2006 Cameron's wife co-founded MUSE School, in 2015 the school became the first K-12 vegan school in USA.
Influence.
Cameron's directorial style has provided great influence throughout the film industry. "Buffy the Vampire Slayer" and "Firefly" creator Joss Whedon stated that Cameron's approach to action scenes was influential to those in "The Avengers". He also cited Cameron as "the leader and the teacher and the Yoda". Michael Bay considers Cameron an idol and was convinced by him to use 3D in "". Cameron's approach to 3D also inspired Baz Luhrmann to use it in "The Great Gatsby". Other directors that have drawn inspiration from Cameron include Peter Jackson, Neill Blomkamp and Quentin Tarantino.
Reputation.
In 1999, Cameron was labeled selfish and cruel by one collaborator, author Orson Scott Card, who had been hired a decade earlier to work with Cameron on the novelization of "The Abyss". Card said the experience was "hell on wheels. He was very nice to me, because I could afford to walk away. But he made everyone around him miserable, and his unkindness did nothing to improve the film in any way. Nor did it motivate people to work faster or better. And unless he changes his way of working with people, I hope he never directs anything of mine. In fact, now that this is in print, I can fairly guarantee that he will never direct anything of mine. Life is too short to collaborate with selfish, cruel people." He later alluded to Cameron in his review of "Me and Orson Welles", where he described witnessing a famous director chew out an assistant for his own error.
After working with Cameron on "Titanic", Kate Winslet decided she would not work with Cameron again unless she earned "a lot of money". She said that Cameron was a nice man, but she found his temper difficult to deal with. In an editorial, the British newspaper "The Independent" said that Cameron "is a nightmare to work with. Studios have come to fear his habit of straying way over schedule and over budget. He is notorious on set for his uncompromising and dictatorial manner, as well as his flaming temper."
Sam Worthington, who worked with Cameron, stated on "The Jay Leno Show" that Cameron had very high expectations from everyone: he would use a nail gun to nail the film crew's cell phones to a wall above an exit door in retaliation for unwanted ringing during production. Other actors, such as Bill Paxton and Sigourney Weaver, have praised Cameron's perfectionism. Weaver said of Cameron: "He really does want us to risk our lives and limbs for the shot, but he doesn't mind risking his own." Michael Biehn has also praised Cameron, claiming "Jim is a really passionate person. He cares more about his movies than other directors care about their movies", but added "I’ve never seen him yell at anybody." However, Biehn did claim Cameron is "not real sensitive when it comes to actors."
Composer James Horner refused to work with Cameron for a decade following their strained working relationship on 1986's "Aliens"; they eventually settled their differences, and Horner went on to score both "Titanic" and "Avatar".
An episode of "South Park" from its 16th season depicted the director as being self-obsessed. He is seen going deep sea diving while playing his own theme song and appearing oblivious to his overtly bored ship's crew. However, the episode also credits him for "raising the bar" on entertainment.
In 2014, Cameron was the keynote speaker at the first annual Fame and Philanthropy, a charity fundraiser which raised money for several high profile celebrity charities. Cameron was one of several guest speakers at the event along with Charlize Theron and Halle Berry.
In a 2015 interview together, actresses Sigourney Weaver and Jamie Lee Curtis, who both worked with Cameron, commented very positively on him. Curtis stated, "the truth is he can do every other job acting. I'm talking about every single department, from art direction to props to wardrobe to cameras, he knows more than everyone doing the job." Weaver answered "There are very few geniuses in the world, let alone in our business, and he's certainly one of them." She also said, "he's misunderstood in the industry, somewhat. He is so generous to actors."
Awards.
Cameron received the inaugural Bradbury Award from the Science Fiction and Fantasy Writers of America in 1992 for "" ("Avatar" would be a finalist in 2010).
Cameron did not receive any major mainstream filmmaking awards prior to "Titanic". For "Titanic" he won several including Academy Awards for Best Picture (shared with Jon Landau), Best Director and Best Film Editing (shared with Conrad Buff and Richard A. Harris). Cameron is one of the few filmmakers to win three Oscars in a single evening and Golden Globes for Best Motion Picture – Drama and Best Director.
In recognition of "a distinguished career as a Canadian filmmaker", Carleton University, Ottawa, awarded Cameron the honorary degree of Doctor of Fine Arts on June 13, 1998. Cameron accepted the degree in person and gave the Convocation Address.
He also received an honorary doctorate in October 1998 from Brock University in St. Catharines, Ontario, for his accomplishments in the international film industry.
In 1998, Cameron attended convocation to receive an honorary doctorate of Laws from Ryerson University, Toronto. The university awards its highest honor to those who have made extraordinary contributions in Canada, or internationally.
In 1999, Cameron received the honorary Doctor of Fine Arts degree from California State University, Fullerton, where he had been a student in the 1970s. He received the degree at the university's annual Commencement exercises that year, where he gave the keynote speech.
In recognition of his contributions to underwater filming and remote vehicle technology, the University of Southampton awarded Cameron the honorary degree of Doctor of the University. Cameron did not attend the Engineering Sciences graduation ceremony in July 2004 where the degree was awarded but instead received it in person at the National Oceanography Centre.
On June 3, 2008, it was announced that he would be inducted into Canada's Walk of Fame. On December 18, 2009, the same day "Avatar" was released worldwide, Cameron received the 2,396th star on the Hollywood Walk of Fame. After the release of "Avatar", on February 28, 2010, Cameron was also honored with a Visual Effects Society (VES) Lifetime Achievement Award.
For "Avatar", Cameron won numerous awards as well, including: Golden Globes for Best Motion Picture – Drama (shared with Jon Landau) and Best Director. He was nominated for three Academy Awards: Best Picture, Best Director and Best Film Editing (shared with John Refoua and Stephen E. Rivkin). However, Cameron and "Avatar" lost to his former wife Kathryn Bigelow and her film, "The Hurt Locker".
On September 24, 2010, James Cameron was named Number 1 in The 2010 Guardian Film Power 100 list. In a list compiled by the British magazine "New Statesman" in September 2010, he was listed 30th in the list of "The World's 50 Most Influential Figures 2010".
The Science Fiction Hall of Fame inducted Cameron in June 2012.
Awards.
Cameron has received numerous awards; mainly for "Titanic" and "Avatar".
Collaborations.
Cameron has consistently worked with Bill Paxton, Michael Biehn, Lance Henriksen, Jenette Goldstein and Arnold Schwarzenegger.
1 Apart from acting, Wisher Jr. also collaborated with Cameron in writing credits.
2 Biehn's reprise of the role of Kyle Reese was cut from the theatrical release, but was restored in the Special Edition on DVD/Blu-ray.
Recurring themes.
Cameron's films have recurring themes and subtexts. These include the conflicts between humanity and technology, the dangers of corporate greed, strong female characters, and a strong romance subplot. In almost all films, the main characters usually get into dramatic crisis situations with significant threats to their own life or even the threat of an impending apocalypse.
"The Abyss" dealt with deep sea exploration (shot in an unfinished nuclear reactor filled with water) and Cameron himself became an expert in the field of deep-sea wreckage exploration, exploring the wreckage of the "Titanic" and the "Bismarck". Cameron will return to this theme with "The Dive", shooting from a minisub.
Filmography.
Cameron has contributed to many projects as a writer, director, and producer, or as a combination of the three.
Cameron's first film was the 1978 science fiction short film "Xenogenesis", which he directed, wrote and produced. Cameron's films have grossed a total of over $7 billion worldwide.
In addition to works of fiction, Cameron has directed and appeared in several documentaries including "Ghosts of the Abyss" and "Aliens of the Deep". He also contributed to a number of television series including "Dark Angel" and "Entourage". He plans to shoot a small drama film after the Avatar trilogy, just to prove that 3D works even for domestic dramas.
Reception.
Critical, public and commercial reception to films James Cameron has directed as of May 7, 2015.

</doc>
<doc id="15624" url="https://en.wikipedia.org/wiki?curid=15624" title="Judaism">
Judaism

Judaism (from , derived from Greek , originally from Hebrew , "Yehudah", "Judah"; in Hebrew: , "Yahadut", the distinctive characteristics of the Judean ethnos) encompasses the religion, philosophy, culture and way of life of the Jewish people. Judaism is an ancient monotheistic religion, with the Torah as its foundational text (part of the larger text known as the Tanakh or Hebrew Bible), and supplemental oral tradition represented by later texts such as the Midrash and the Talmud. Judaism is considered by religious Jews to be the expression of the covenantal relationship that God established with the Children of Israel.
Judaism includes a wide corpus of texts, practices, theological positions, and forms of organization. Within Judaism there are a variety of movements, most of which emerged from Rabbinic Judaism, which holds that God revealed his laws and commandments to Moses on Mount Sinai in the form of both the Written and Oral Torah. Historically, this assertion was challenged by various groups such as the Sadducees and Hellenistic Judaism during the Second Temple period; the Karaites and Sabbateans during the early and later medieval period; and among segments of the modern non-Orthodox denominations. Modern branches of Judaism such as Humanistic Judaism may be nontheistic. Today, the largest Jewish religious movements are Orthodox Judaism (Haredi Judaism and Modern Orthodox Judaism), Conservative Judaism and Reform Judaism. Major sources of difference between these groups are their approaches to Jewish law, the authority of the Rabbinic tradition, and the significance of the State of Israel. Orthodox Judaism maintains that the Torah and Jewish law are divine in origin, eternal and unalterable, and that they should be strictly followed. Conservative and Reform Judaism are more liberal, with Conservative Judaism generally promoting a more "traditional" interpretation of Judaism's requirements than Reform Judaism. A typical Reform position is that Jewish law should be viewed as a set of general guidelines rather than as a set of restrictions and obligations whose observance is required of all Jews. Historically, special courts enforced Jewish law; today, these courts still exist but the practice of Judaism is mostly voluntary. Authority on theological and legal matters is not vested in any one person or organization, but in the sacred texts and rabbis and scholars who interpret them.
The history of Judaism spans more than 3,000 years. Judaism has its roots as a structured religion in the Middle East during the Bronze Age. Judaism is considered one of the oldest monotheistic religions. The Hebrews and Israelites were already referred to as "Jews" in later books of the Tanakh such as the Book of Esther, with the term Jews replacing the title "Children of Israel". Judaism's texts, traditions and values strongly influenced later Abrahamic religions, including Christianity, Islam and the Baha'i Faith. Many aspects of Judaism have also directly or indirectly influenced secular Western ethics and civil law.
Jews are an ethnoreligious group and include those born Jewish and converts to Judaism. In 2012, the world Jewish population was estimated at about 14 million, or roughly 0.2% of the total world population. About 42% of all Jews reside in Israel and another 42% reside in North America, with most of the remainder living in Europe, and other minority groups spread throughout South America, Asia, Africa, and Australia.
Defining characteristics and principles of faith.
Defining characteristics.
Unlike other ancient Near Eastern gods, the Hebrew God is portrayed as unitary and solitary; consequently, the Hebrew God's principal relationships are not with other gods, but with the world, and more specifically, with the people he created. Judaism thus begins with ethical monotheism: the belief that God is one and is concerned with the actions of humankind. According to the Tanakh (Hebrew Bible), God promised Abraham to make of his offspring a great nation. Many generations later, he commanded the nation of Israel to love and worship only one God; that is, the Jewish nation is to reciprocate God's concern for the world. He also commanded the Jewish people to love one another; that is, Jews are to imitate God's love for people. These commandments are but two of a large corpus of commandments and laws that constitute this covenant, which is the substance of Judaism.
Thus, although there is an esoteric tradition in Judaism (Kabbalah), Rabbinic scholar Max Kadushin has characterized normative Judaism as "normal mysticism", because it involves everyday personal experiences of God through ways or modes that are common to all Jews. This is played out through the observance of the Halakha and given verbal expression in the Birkat Ha-Mizvot, the short blessings that are spoken every time a positive commandment is to be fulfilled.
Whereas Jewish philosophers often debate whether God is immanent or transcendent, and whether people have free will or their lives are determined, Halakha is a system through which any Jew acts to bring God into the world.
Ethical monotheism is central in all sacred or normative texts of Judaism. However, monotheism has not always been followed in practice. The Jewish Bible (Tanakh) records and repeatedly condemns the widespread worship of other gods in ancient Israel. In the Greco-Roman era, many different interpretations of monotheism existed in Judaism, including the interpretations that gave rise to Christianity.
Moreover, some have argued that Judaism is a non-creedal religion that does not require one to believe in God. For some, observance of Jewish law is more important than belief in God "per se". In modern times, some liberal Jewish movements do not accept the existence of a personified deity active in history.
Core tenets.
Scholars throughout Jewish history have proposed numerous formulations of Judaism's core tenets, all of which have met with criticism. The most popular formulation is Maimonides' thirteen principles of faith, developed in the 12th century. According to Maimonides, any Jew who rejects even one of these principles would be considered an apostate and a heretic. Jewish scholars have held points of view diverging in various ways from Maimonides' principles.
In Maimonides' time, his list of tenets was criticized by Hasdai Crescas and Joseph Albo. Albo and the Raavad argued that Maimonides' principles contained too many items that, while true, were not fundamentals of the faith.
Along these lines, the ancient historian Josephus emphasized practices and observances rather than religious beliefs, associating apostasy with a failure to observe Jewish law and maintaining that the requirements for conversion to Judaism included circumcision and adherence to traditional customs. Maimonides' principles were largely ignored over the next few centuries. Later, two poetic restatements of these principles (""Ani Ma'amin"" and ""Yigdal"") became integrated into many Jewish liturgies, leading to their eventual near-universal acceptance.
In modern times, Judaism lacks a centralized authority that would dictate an exact religious dogma. Because of this, many different variations on the basic beliefs are considered within the scope of Judaism. Even so, all Jewish religious movements are, to a greater or lesser extent, based on the principles of the Hebrew Bible and various commentaries such as the Talmud and Midrash. Judaism also universally recognizes the Biblical Covenant between God and the Patriarch Abraham as well as the additional aspects of the Covenant revealed to Moses, who is considered Judaism's greatest prophet. In the Mishnah, a core text of Rabbinic Judaism, acceptance of the Divine origins of this covenant is considered an essential aspect of Judaism and those who reject the Covenant forfeit their share in the World to Come.
Jewish religious texts.
The following is a basic, structured list of the central works of Jewish practice and thought.
Jewish legal literature.
The basis of Jewish law and tradition (halakha) is the Torah (also known as the Pentateuch or the Five Books of Moses). According to rabbinic tradition there are 613 commandments in the Torah. Some of these laws are directed only to men or to women, some only to the ancient priestly groups, the Kohanim and Leviyim (members of the tribe of Levi), some only to farmers within the Land of Israel. Many laws were only applicable when the Temple in Jerusalem existed, and fewer than 300 of these commandments are still applicable today.
While there have been Jewish groups whose beliefs were claimed to be based on the written text of the Torah alone (e.g., the Sadducees, and the Karaites), most Jews believed in what they call the oral law. These oral traditions were transmitted by the Pharisee sect of ancient Judaism, and were later recorded in written form and expanded upon by the rabbis.
Rabbinic Judaism (which derives from the Pharisees) has always held that the books of the Torah (called the written law) have always been transmitted in parallel with an oral tradition. To justify this viewpoint, Jews point to the text of the Torah, where many words are left undefined, and many procedures mentioned without explanation or instructions; this, they argue, means that the reader is assumed to be familiar with the details from other, i.e., oral, sources. This parallel set of material was originally transmitted orally, and came to be known as "the oral law".
By the time of Rabbi Judah haNasi (200 CE), after the destruction of Jerusalem, much of this material was edited together into the Mishnah. Over the next four centuries this law underwent discussion and debate in both of the world's major Jewish communities (in Israel and Babylonia), and the commentaries on the Mishnah from each of these communities eventually came to be edited together into compilations known as the two Talmuds. These have been expounded by commentaries of various Torah scholars during the ages.
Halakha, the rabbinic Jewish way of life, then, is based on a combined reading of the Torah, and the oral tradition—the Mishnah, the halakhic Midrash, the Talmud and its commentaries. The Halakha has developed slowly, through a precedent-based system. The literature of questions to rabbis, and their considered answers, is referred to as responsa (in Hebrew, "Sheelot U-Teshuvot".) Over time, as practices develop, codes of Jewish law are written that are based on the responsa; the most important code, the Shulchan Aruch, largely determines Orthodox religious practice today.
Jewish philosophy.
Jewish philosophy refers to the conjunction between serious study of philosophy and Jewish theology. Major Jewish philosophers include Solomon ibn Gabirol, Saadia Gaon, Judah Halevi, Maimonides, and Gersonides. Major changes occurred in response to the Enlightenment (late 18th to early 19th century) leading to the post-Enlightenment Jewish philosophers. Modern Jewish philosophy consists of both Orthodox and non-Orthodox oriented philosophy. Notable among Orthodox Jewish philosophers are Eliyahu Eliezer Dessler, Joseph B. Soloveitchik, and Yitzchok Hutner. Well-known non-Orthodox Jewish philosophers include Martin Buber, Franz Rosenzweig, Mordecai Kaplan, Abraham Joshua Heschel, Will Herberg, and Emmanuel Lévinas.
Related Topics
Rabbinic hermeneutics.
Orthodox and many other Jews do not believe that the revealed Torah consists solely of its written contents, but of its interpretations as well. The study of Torah (in its widest sense, to include both poetry, narrative, and law, and both the Hebrew Bible and the Talmud) is in Judaism itself a sacred act of central importance. For the sages of the Mishnah and Talmud, and for their successors today, the study of Torah was therefore not merely a means to learn the contents of God's revelation, but an end in itself. According to the Talmud,
In Judaism, "the study of Torah can be a means of experiencing God". Reflecting on the contribution of the Amoraim and Tanaim to contemporary Judaism, Professor Jacob Neusner observed:
To study the Written Torah and the Oral Torah in light of each other is thus also to study "how" to study the word of God.
In the study of Torah, the sages formulated and followed various logical and hermeneutical principles. According to David Stern, all Rabbinic hermeneutics rest on two basic axioms:
These two principles make possible a great variety of interpretations. According to the Talmud,
Observant Jews thus view the Torah as dynamic, because it contains within it a host of interpretations
According to Rabbinic tradition, all valid interpretations of the written Torah were revealed to Moses at Sinai in oral form, and handed down from teacher to pupil (The oral revelation is in effect coextensive with the Talmud itself). When different rabbis forwarded conflicting interpretations, they sometimes appealed to hermeneutic principles to legitimize their arguments; some rabbis claim that these principles were themselves revealed by God to Moses at Sinai.
Thus, Hillel called attention to seven commonly used hermeneutical principles in the interpretation of laws (baraita at the beginning of Sifra); R. Ishmael, thirteen (baraita at the beginning of Sifra; this collection is largely an amplification of that of Hillel). Eliezer b. Jose ha-Gelili listed 32, largely used for the exegesis of narrative elements of Torah. All the hermeneutic rules scattered through the Talmudim and Midrashim have been collected by Malbim in "Ayyelet ha-Shachar," the introduction to his commentary on the Sifra. Nevertheless, R. Ishmael's 13 principles are perhaps the ones most widely known; they constitute an important, and one of Judaism's earliest, contributions to logic, hermeneutics, and jurisprudence. Judah Hadassi incorporated Ishmael's principles into Karaite Judaism in the 12th century. Today R. Ishmael's 13 principles are incorporated into the Jewish prayer book to be read by observant Jews on a daily basis.
Jewish identity.
Origin of the term "Judaism".
The term Judaism derives from "Iudaismus", a Latinized form of the Ancient Greek Ἰουδαϊσμός or "Ioudaïsmos" (from the verb , "to side with or imitate the "), and it was ultimately inspired by the Hebrew יהודה, "Yehudah", "Judah"; in Hebrew: יַהֲדוּת, "Yahadut". The term "Ἰουδαϊσμός" first appears in the Hellenistic Greek book of 2 Maccabees in the 2nd century BCE. In the context of the age and period it meant "seeking or forming part of a cultural entity" and resembled its antonym "hellenismos", a word that signified a people's submission unto Hellenic (Greek) cultural norms. The conflict between "iudaismos" and "hellenismos" lay behind the Maccabean revolt and hence the invention of the term "iudaismos". Shaye J. D. Cohen writes in his book "The Beginnings of Jewishness":
We are tempted, of course, to translate ["Ioudaïsmos"] as "Judaism," but this translation is too narrow, because in this first occurrence of the term, "Ioudaïsmos" has not yet be reduced to designation of a religion. It means rather "the aggregate of all those characteristics that makes Judaeans Judaean (or Jews Jewish)." Among these characteristics, to be sure, are practices and beliefs that we would today call "religious," but these practices and beliefs are not the sole content of the term. Thus "Ioudaïsmos" should be translated not as "Judaism" but as Judaeanness.
The earliest instance in Europe where the term was used to mean "the profession or practice of the Jewish religion; the religious system or polity of the Jews"{cn} is Robert Fabyan's "The newe cronycles of Englande and of Fraunce a 1513". "Judaism" as a direct translation of the Latin "Iudaismus" first occurred in a 1611 English translation of the Apocrypha (Deuterocanon in Catholic and Eastern Orthodoxy), 2 Macc. ii. 21: "Those that behaved themselues manfully to their honour for Iudaisme."
Distinction between Jews as a people and Judaism.
According to Daniel Boyarin, the underlying distinction between religion and ethnicity is foreign to Judaism itself, and is one form of the dualism between spirit and flesh that has its origin in Platonic philosophy and that permeated Hellenistic Judaism. Consequently, in his view, Judaism does not fit easily into conventional Western categories, such as religion, ethnicity, or culture. Boyarin suggests that this in part reflects the fact that much of Judaism's more than 3,000-year history predates the rise of Western culture and occurred outside the West (that is, Europe, particularly medieval and modern Europe). During this time, Jews experienced slavery, anarchic and theocratic self-government, conquest, occupation, and exile. In the Diaspora, they were in contact with, and influenced by, ancient Egyptian, Babylonian, Persian, and Hellenic cultures, as well as modern movements such as the Enlightenment (see Haskalah) and the rise of nationalism, which would bear fruit in the form of a Jewish state in their ancient homeland, the Land of Israel. They also saw an elite population convert to Judaism (the Khazars), only to disappear as the centers of power in the lands once occupied by that elite fell to the people of Rus and then the Mongols. Thus, Boyarin has argued that "Jewishness disrupts the very categories of identity, because it is not national, not genealogical, not religious, but all of these, in dialectical tension."
In contrast to this point of view, practices such as Humanistic Judaism reject the religious aspects of Judaism, while retaining certain cultural traditions.
Who is a Jew?
According to Rabbinic Judaism, a Jew is anyone who was either born of a Jewish mother or who converted to Judaism in accordance with Jewish Law. Reconstructionist Judaism and the larger denominations of worldwide Progressive Judaism (also known as Liberal or Reform Judaism) accept the child as Jewish if one of the parents is Jewish, if the parents raise the child with a Jewish identity, but not the smaller regional branches. All mainstream forms of Judaism today are open to sincere converts, although conversion has traditionally been discouraged since the time of the Talmud. The conversion process is evaluated by an authority, and the convert is examined on his or her sincerity and knowledge. Converts are called "ben Abraham" or "bat Abraham", (son or daughter of Abraham). Conversions have on occasion been overturned. In 2008, Israel's highest religious court invalidated the conversion of 40,000 Jews, mostly from Russian immigrant families, even though they had been approved by an Orthodox rabbi.
Rabbinical Judaism maintains that a Jew, whether by birth or conversion, is a Jew forever. Thus a Jew who claims to be an atheist or converts to another religion is still considered by traditional Judaism to be Jewish. According to some sources, the Reform movement has maintained that a Jew who has converted to another religion is no longer a Jew, and the Israeli Government has also taken that stance after Supreme Court cases and statutes. However, the Reform movement has indicated that this is not so cut and dried, and different situations call for consideration and differing actions. For example, Jews who have converted under duress may be permitted to return to Judaism "without any action on their part but their desire to rejoin the Jewish community" and "A proselyte who has become an apostate remains, nevertheless, a Jew". (p. 100–106).
Karaite Judaism believes that Jewish identity can only be transmitted by patrilineal descent. Although a minority of modern Karaites believe that Jewish identity requires that both parents be Jewish, and not only the father. They argue that only patrilineal descent can transmit Jewish identity on the grounds that all descent in the Torah went according to the male line.
The question of what determines Jewish identity in the State of Israel was given new impetus when, in the 1950s, David Ben-Gurion requested opinions on "mihu Yehudi" ("Who is a Jew") from Jewish religious authorities and intellectuals worldwide in order to settle citizenship questions. This is still not settled, and occasionally resurfaces in Israeli politics.
Jewish demographics.
The total number of Jews worldwide is difficult to assess because the definition of "who is a Jew" is problematic; not all Jews identify themselves as Jewish, and some who identify as Jewish are not considered so by other Jews. According to the "Jewish Year Book" (1901), the global Jewish population in 1900 was around 11 million. The latest available data is from the World Jewish Population Survey of 2002 and the Jewish Year Calendar (2005). In 2002, according to the Jewish Population Survey, there were 13.3 million Jews around the world. The Jewish Year Calendar cites 14.6 million. Jewish population growth is currently near zero percent, with 0.3% growth from 2000 to 2001.
Jewish religious movements.
Rabbinic Judaism.
Rabbinic Judaism (or in some Christian traditions, Rabbinism) (Hebrew: "Yahadut Rabanit" – יהדות רבנית) has been the mainstream form of Judaism since the 6th century CE, after the codification of the Talmud. It is characterised by the belief that the Written Torah (Written Law) cannot be correctly interpreted without reference to the Oral Torah and the voluminous literature specifying what behavior is sanctioned by the Law.
The Jewish Enlightenment of the late 18th century resulted in the division of Ashkenazi (Western) Jewry into religious movements or denominations, especially in North America and Anglophone countries. The main denominations today outside Israel (where the situation is rather different) are Orthodox, Conservative, and Reform.
Jewish movements in Israel.
Most Jewish Israelis classify themselves as "secular" ("hiloni"), "traditional" ("masorti"), "religious" ("dati") or "Haredi". The term "secular" is more popular as a self-description among Israeli families of western (European) origin, whose Jewish identity may be a very powerful force in their lives, but who see it as largely independent of traditional religious belief and practice. This portion of the population largely ignores organized religious life, be it of the official Israeli rabbinate (Orthodox) or of the liberal movements common to diaspora Judaism (Reform, Conservative).
The term "traditional" ("masorti") is most common as a self-description among Israeli families of "eastern" origin (i.e., the Middle East, Central Asia, and North Africa). This term, as commonly used, has nothing to do with the Conservative Judaism, which also names itself "Masorti" outside North America. There is a great deal of ambiguity in the ways "secular" and "traditional" are used in Israel: they often overlap, and they cover an extremely wide range in terms of worldview and practical religious observance. The term "Orthodox" is not popular in Israeli discourse, although the percentage of Jews who come under that category is far greater than in the diaspora. What would be called "Orthodox" in the diaspora includes what is commonly called "dati" (religious) or "haredi" (ultra-Orthodox) in Israel. The former term includes what is called "Religious Zionism" or the "National Religious" community, as well as what has become known over the past decade or so as "haredi-leumi" (nationalist "haredi"), or "Hardal", which combines a largely "haredi" lifestyle with nationalist ideology. (Some people, in Yiddish, also refer to observant Orthodox Jews as "frum", as opposed to "frei" (more liberal Jews)).
"Haredi" applies to a populace that can be roughly divided into three separate groups along both ethnic and ideological lines: (1) "Lithuanian" (non-hasidic) "haredim" of Ashkenazic origin; (2) Hasidic "haredim" of Ashkenazic origin; and (3) Sephardic "haredim".
Karaites and Samaritans.
Karaite Judaism defines itself as the remnants of the non-Rabbinic Jewish sects of the Second Temple period, such as the Sadducees. The Karaites ("Scripturalists") accept only the Hebrew Bible and what they view as the Peshat ("simple" meaning); they do not accept non-biblical writings as authoritative. Some European Karaites do not see themselves as part of the Jewish community at all, although most do.
The Samaritans, a very small community located entirely around Mount Gerizim in the Nablus/Shechem region of the West Bank and in Holon, near Tel Aviv in Israel, regard themselves as the descendants of the Israelites of the Iron Age kingdom of Israel. Their religious practices are based on the literal text of the written Torah (Five Books of Moses), which they view as the only authoritative scripture (with a special regard also for the Samaritan Book of Joshua).
Jewish observances.
Jewish ethics.
Jewish ethics may be guided by halakhic traditions, by other moral principles, or by central Jewish virtues. Jewish ethical practice is typically understood to be marked by values such as justice, truth, peace, loving-kindness (chesed), compassion, humility, and self-respect. Specific Jewish ethical practices include practices of charity (tzedakah) and refraining from negative speech (lashon hara). Proper ethical practices regarding sexuality and many other issues are subjects of dispute among Jews.
Prayers.
Traditionally, Jews recite prayers three times daily, Shacharit, Mincha, and Ma'ariv with a fourth prayer, Mussaf added on Shabbat and holidays. At the heart of each service is the "Amidah" or "Shemoneh Esrei". Another key prayer in many services is the declaration of faith, the "Shema Yisrael" (or "Shema"). The "Shema" is the recitation of a verse from the Torah (Deuteronomy 6:4): "Shema Yisrael Adonai Eloheinu Adonai Echad"—"Hear, O Israel! The Lord is our God! The Lord is One!"
Most of the prayers in a traditional Jewish service can be recited in solitary prayer, although communal prayer is preferred. Communal prayer requires a quorum of ten adult Jews, called a "minyan". In nearly all Orthodox and a few Conservative circles, only male Jews are counted toward a "minyan"; most Conservative Jews and members of other Jewish denominations count female Jews as well.
In addition to prayer services, observant traditional Jews recite prayers and benedictions throughout the day when performing various acts. Prayers are recited upon waking up in the morning, before eating or drinking different foods, after eating a meal, and so on.
The approach to prayer varies among the Jewish denominations. Differences can include the texts of prayers, the frequency of prayer, the number of prayers recited at various religious events, the use of musical instruments and choral music, and whether prayers are recited in the traditional liturgical languages or the vernacular. In general, Orthodox and Conservative congregations adhere most closely to tradition, and Reform and Reconstructionist synagogues are more likely to incorporate translations and contemporary writings in their services. Also, in most Conservative synagogues, and all Reform and Reconstructionist congregations, women participate in prayer services on an equal basis with men, including roles traditionally filled only by men, such as reading from the Torah. In addition, many Reform temples use musical accompaniment such as organs and mixed choirs.
Religious clothing.
A "kippah" (Hebrew: כִּפָּה, plural "kippot"; Yiddish: יאַרמלקע, "yarmulke") is a slightly rounded brimless skullcap worn by many Jews while praying, eating, reciting blessings, or studying Jewish religious texts, and at all times by some Jewish men. In Orthodox communities, only men wear kippot; in non-Orthodox communities, some women also wear kippot. "Kippot" range in size from a small round beanie that covers only the back of the head, to a large, snug cap that covers the whole crown.
"Tzitzit" (Hebrew: צִיציִת) (Ashkenazi pronunciation: "tzitzis") are special knotted "fringes" or "tassels" found on the four corners of the "tallit" (Hebrew: טַלִּית) (Ashkenazi pronunciation: "tallis"), or prayer shawl. The "tallit" is worn by Jewish men and some Jewish women during the prayer service. Customs vary regarding when a Jew begins wearing a tallit. In the Sephardi community, boys wear a tallit from bar mitzvah age. In some Ashkenazi communities it is customary to wear one only after marriage. A "tallit katan" (small tallit) is a fringed garment worn under the clothing throughout the day. In some Orthodox circles, the fringes are allowed to hang freely outside the clothing.
Tefillin (Hebrew: תְפִלִּין), known in English as phylacteries (from the Greek word φυλακτήριον, meaning "safeguard" or "amulet"), are two square leather boxes containing biblical verses, attached to the forehead and wound around the left arm by leather straps. They are worn during weekday morning prayer by observant Jewish men and some Jewish women.
A "kittel" (Yiddish: קיטל), a white knee-length overgarment, is worn by prayer leaders and some observant traditional Jews on the High Holidays. It is traditional for the head of the household to wear a kittel at the Passover seder in some communities, and some grooms wear one under the wedding canopy. Jewish males are buried in a "tallit" and sometimes also a "kittel" which are part of the "tachrichim" (burial garments).
Jewish holidays.
Jewish holidays are special days in the Jewish calendar, which celebrate moments in Jewish history, as well as central themes in the relationship between God and the world, such as creation, revelation, and redemption.
Shabbat.
"Shabbat", the weekly day of rest lasting from shortly before sundown on Friday night to nightfall Saturday night, commemorates God's day of rest after six days of creation. It plays a pivotal role in Jewish practice and is governed by a large corpus of religious law. At sundown on Friday, the woman of the house welcomes the Shabbat by lighting two or more candles and reciting a blessing. The evening meal begins with the Kiddush, a blessing recited aloud over a cup of wine, and the Mohtzi, a blessing recited over the bread. It is customary to have challah, two braided loaves of bread, on the table. During Shabbat Jews are forbidden to engage in any activity that falls under 39 categories of "melakhah", translated literally as "work". In fact the activities banned on the Sabbath are not "work" in the usual sense: They include such actions as lighting a fire, writing, using money and carrying in the public domain. The prohibition of lighting a fire has been extended in the modern era to driving a car, which involves burning fuel, and using electricity.
Three pilgrimage festivals.
Jewish holy days ("chaggim"), celebrate landmark events in Jewish history, such as the Exodus from Egypt and the giving of the Torah, and sometimes mark the change of seasons and transitions in the agricultural cycle. The three major festivals, Sukkot, Passover and Shavuot, are called "regalim" (derived from the Hebrew word "regel", or foot). On the three regalim, it was customary for the Israelites to make pilgrimages to Jerusalem to offer sacrifices in the Temple.
High Holy Days.
The High Holidays ("Yamim Noraim" or "Days of Awe") revolve around judgment and forgiveness.
Purim.
Purim (Hebrew: "Pûrîm" "lots") is a joyous Jewish holiday that commemorates the deliverance of the Persian Jews from the plot of the evil Haman, who sought to exterminate them, as recorded in the biblical Book of Esther. It is characterized by public recitation of the Book of Esther, mutual gifts of food and drink, charity to the poor, and a celebratory meal (Esther 9:22). Other customs include drinking wine, eating special pastries called hamantashen, dressing up in masks and costumes, and organizing carnivals and parties.
Purim is celebrated annually on the 14th of the Hebrew month of Adar, which occurs in February or March of the Gregorian calendar.
Hanukkah.
Hanukkah (, "dedication") also known as the Festival of Lights, is an eight-day Jewish holiday that starts on the 25th day of Kislev (Hebrew calendar). The festival is observed in Jewish homes by the kindling of lights on each of the festival's eight nights, one on the first night, two on the second night and so on.
The holiday was called Hanukkah (meaning "dedication") because it marks the re-dedication of the Temple after its desecration by Antiochus IV Epiphanes. Spiritually, Hanukkah commemorates the "Miracle of the Oil". According to the Talmud, at the re-dedication of the Temple in Jerusalem following the victory of the Maccabees over the Seleucid Empire, there was only enough consecrated oil to fuel the eternal flame in the Temple for one day. Miraculously, the oil burned for eight days – which was the length of time it took to press, prepare and consecrate new oil.
Hanukkah is not mentioned in the Bible and was never considered a major holiday in Judaism, but it has become much more visible and widely celebrated in modern times, mainly because it falls around the same time as Christmas and has national Jewish overtones that have been emphasized since the establishment of the State of Israel.
Other days.
Tisha B'Av ( or , "the Ninth of Av") is a day of mourning and fasting commemorating the destruction of the First and Second Temples, and in later times, the expulsion of the Jews from Spain.
The modern holidays of Yom Ha-shoah (Holocaust Remembrance Day) and Yom Ha'atzmaut (Israeli Independence Day) commemorate the horrors of the Holocaust and the achievement of Israel independence, respectively.
Torah readings.
The core of festival and Shabbat prayer services is the public reading of the Torah, along with connected readings from the other books of the Tanakh, called Haftarah. Over the course of a year, the whole Torah is read, with the cycle starting over in the autumn, on Simchat Torah.
Synagogues and religious buildings.
Synagogues are Jewish houses of prayer and study. They usually contain separate rooms for prayer (the main sanctuary), smaller rooms for study, and often an area for community or educational use. There is no set blueprint for synagogues and the architectural shapes and interior designs of synagogues vary greatly. The Reform movement mostly refer to their synagogues as temples. Some traditional features of a synagogue are:
In addition to synagogues, other buildings of significance in Judaism include yeshivas, or institutions of Jewish learning, and mikvahs, which are ritual baths.
Dietary laws: "kashrut".
The Jewish dietary laws are known as "kashrut". Food prepared in accordance with them is termed kosher, and food that is not kosher is also known as "treifah" or "treif". People who observe these laws are colloquially said to be "keeping kosher".
Many of the laws apply to animal-based foods. For example, in order to be considered kosher, mammals must have split hooves and chew their cud. The pig is arguably the most well-known example of a non-kosher animal. Although it has split hooves, it does not chew its cud. For seafood to be kosher, the animal must have fins and scales. Certain types of seafood, such as shellfish, crustaceans, and eels, are therefore considered non-kosher. Concerning birds, a list of non-kosher species is given in the Torah. The exact translations of many of the species have not survived, and some non-kosher birds' identities are no longer certain. However, traditions exist about the "kashrut" status of a few birds. For example, both chickens and turkeys are permitted in most communities. Other types of animals, such as amphibians, reptiles, and most insects, are prohibited altogether.
In addition to the requirement that the species be considered kosher, meat and poultry (but not fish) must come from a healthy animal slaughtered in a process known as "shechitah". Without the proper slaughtering practices even an otherwise kosher animal will be rendered "treif". The slaughtering process is intended to be quick and relatively painless to the animal. Forbidden parts of animals include the blood, some fats, and the area in and around the sciatic nerve.
Jewish law also forbids the consumption of meat and dairy products together. The waiting period between eating meat and eating dairy varies by the order in which they are consumed and by community, and can extend for up to six hours. Based on the Biblical injunction against cooking a kid in its mother's milk, this rule is mostly derived from the Oral Torah, the Talmud and Rabbinic law. Chicken and other kosher birds are considered the same as meat under the laws of "kashrut", but the prohibition is Rabbinic, not Biblical.
The use of dishes, serving utensils, and ovens may make food "treif" that would otherwise be kosher. Utensils that have been used to prepare non-kosher food, or dishes that have held meat and are now used for dairy products, render the food "treif" under certain conditions.
Furthermore, all Orthodox and some Conservative authorities forbid the consumption of processed grape products made by non-Jews, due to ancient pagan practices of using wine in rituals. Some Conservative authorities permit wine and grape juice made without rabbinic supervision.
The Torah does not give specific reasons for most of the laws of "kashrut". However, a number of explanations have been offered, including maintaining ritual purity, teaching impulse control, encouraging obedience to God, improving health, reducing cruelty to animals and preserving the distinctness of the Jewish community. The various categories of dietary laws may have developed for different reasons, and some may exist for multiple reasons. For example, people are forbidden from consuming the blood of birds and mammals because, according to the Torah, this is where animal souls are contained. In contrast, the Torah forbids Israelites from eating non-kosher species because "they are unclean". The Kabbalah describes sparks of holiness that are released by the act of eating kosher foods, but are too tightly bound in non-kosher foods to be released by eating.
Survival concerns supersede all the laws of "kashrut", as they do for most halakhot.
Laws of ritual purity.
The Tanakh describes circumstances in which a person who is "tahor" or ritually pure may become "tamei" or ritually impure. Some of these circumstances are contact with human corpses or graves, seminal flux, vaginal flux, menstruation, and contact with people who have become impure from any of these. In Rabbinic Judaism, Kohanim, members of the hereditary caste that served as priests in the time of the Temple, are mostly restricted from entering grave sites and touching dead bodies. During the Temple period, such priests (Kohanim) were required to eat their bread offering (Terumah) in a state of ritual purity, which laws eventually led to more rigid laws being enacted, such as hand-washing which became a requisite of all Jews before consuming ordinary bread.
Family purity.
An important subcategory of the ritual purity laws relates to the segregation of menstruating women. These laws are also known as "niddah", literally "separation", or family purity. Vital aspects of halakha for traditionally observant Jews, they are not usually followed by Jews in liberal denominations.
Especially in Orthodox Judaism, the Biblical laws are augmented by Rabbinical injunctions. For example, the Torah mandates that a woman in her normal menstrual period must abstain from sexual intercourse for seven days. A woman whose menstruation is prolonged must continue to abstain for seven more days after bleeding has stopped. The Rabbis conflated ordinary "niddah" with this extended menstrual period, known in the Torah as "zavah", and mandated that a woman may not have sexual intercourse with her husband from the time she begins her menstrual flow until seven days after it ends. In addition, Rabbinical law forbids the husband from touching or sharing a bed with his wife during this period. Afterwards, purification can occur in a ritual bath called a mikveh.
Traditional Ethiopian Jews keep menstruating women in separate huts and, similar to Karaite practice, do not allow menstruating women into their temples because of a temple's special sanctity. Emigration to Israel and the influence of other Jewish denominations have led to Ethiopian Jews adopting more normative Jewish practices.
Life-cycle events.
Life-cycle events, or rites of passage, occur throughout a Jew's life that serve to strengthen Jewish identity and bind him/her to the entire community.
Community leadership.
Classical priesthood.
The role of the priesthood in Judaism has significantly diminished since the destruction of the Second Temple in 70 CE, when priests attended to the Temple and sacrifices. The priesthood is an inherited position, and although priests no longer have any but ceremonial duties, they are still honored in many Jewish communities. Many Orthodox Jewish communities believe that they will be needed again for a future Third Temple and need to remain in readiness for future duty.
Prayer leaders.
From the time of the Mishnah and Talmud to the present, Judaism has required specialists or authorities for the practice of very few rituals or ceremonies. A Jew can fulfill most requirements for prayer by himself. Some activities—reading the Torah and "haftarah" (a supplementary portion from the Prophets or Writings), the prayer for mourners, the blessings for bridegroom and bride, the complete grace after meals—require a "minyan", the presence of ten Jews.
The most common professional clergy in a synagogue are:
Jewish prayer services do involve two specified roles, which are sometimes, but not always, filled by a rabbi or hazzan in many congregations. In other congregations these roles are filled on an ad-hoc basis by members of the congregation who lead portions of services on a rotating basis:
Many congregations, especially larger ones, also rely on a:
The three preceding positions are usually voluntary and considered an honor. Since the Enlightenment large synagogues have often adopted the practice of hiring rabbis and hazzans to act as "shatz" and "baal kriyah", and this is still typically the case in many Conservative and Reform congregations. However, in most Orthodox synagogues these positions are filled by laypeople on a rotating or ad-hoc basis. Although most congregations hire one or more Rabbis, the use of a professional hazzan is generally declining in American congregations, and the use of professionals for other offices is rarer still.
History.
Origins.
At its core, the Tanakh is an account of the Israelites' relationship with God from their earliest history until the building of the Second Temple (c. 535 BCE). Abraham is hailed as the first Hebrew and the father of the Jewish people. As a reward for his act of faith in one God, he was promised that Isaac, his second son, would inherit the Land of Israel (then called Canaan). Later, the descendants of Isaac's son Jacob were enslaved in Egypt, and God commanded Moses to lead the Exodus from Egypt. At Mount Sinai they received the Torah—the five books of Moses. These books, together with Nevi'im and Ketuvim are known as "Torah Shebikhtav" as opposed to the Oral Torah, which refers to the Mishnah and the Talmud. Eventually, God led them to the land of Israel where the tabernacle was planted in the city of Shiloh for over 300 years to rally the nation against attacking enemies. As time went on, the spiritual level of the nation declined to the point that God allowed the Philistines to capture the tabernacle. The people of Israel then told Samuel the prophet that they needed to be governed by a permanent king, and Samuel appointed Saul to be their King. When the people pressured Saul into going against a command conveyed to him by Samuel, God told Samuel to appoint David in his stead.
Once King David was established, he told the prophet Nathan that he would like to build a permanent temple, and as a reward for his actions, God promised David that he would allow his son, Solomon, to build the First Temple and the throne would never depart from his children.
Rabbinic tradition holds that the details and interpretation of the law, which are called the "Oral Torah" or "oral law", were originally an unwritten tradition based upon what God told Moses on Mount Sinai. However, as the persecutions of the Jews increased and the details were in danger of being forgotten, these oral laws were recorded by Rabbi Judah HaNasi (Judah the Prince) in the Mishnah, redacted "circa" 200 CE. The Talmud was a compilation of both the Mishnah and the Gemara, rabbinic commentaries redacted over the next three centuries. The Gemara originated in two major centers of Jewish scholarship, Palestine and Babylonia. Correspondingly, two bodies of analysis developed, and two works of Talmud were created. The older compilation is called the Jerusalem Talmud. It was compiled sometime during the 4th century in Palestine. The Babylonian Talmud was compiled from discussions in the houses of study by the scholars Ravina I, Ravina II, and Rav Ashi by 500 CE, although it continued to be edited later.
Some critical scholars oppose the view that the sacred texts, including the Hebrew Bible, were divinely inspired. Many of these scholars accept the general principles of the documentary hypothesis and suggest that the Torah consists of inconsistent texts edited together in a way that calls attention to divergent accounts. Many suggest that during the First Temple period, the people of Israel believed that each nation had its own god, but that their god was superior to other gods. Some suggest that strict monotheism developed during the Babylonian Exile, perhaps in reaction to Zoroastrian dualism. In this view, it was only by the Hellenic period that most Jews came to believe that their god was the only god, and that the notion of a clearly bounded Jewish nation identical with the Jewish religion formed.
John Day argues that the origins of biblical Yahweh, El, Asherah, and Ba'al, may be rooted in earlier Canaanite religion, which was centered on a pantheon of gods much like the Greek pantheon.
Antiquity.
According to the Hebrew Bible, the United Monarchy was established under Saul and continued under King David and Solomon with its capital in Jerusalem. After Solomon's reign the nation split into two kingdoms, the Kingdom of Israel (in the north) and the Kingdom of Judah (in the south). The Kingdom of Israel was conquered by the Assyrian ruler Sargon II in the late 8th century BCE with many people from the capital Samaria being taken captive to Media and the Khabur River valley. The Kingdom of Judah continued as an independent state until it was conquered by a Babylonian army in the early 6th century BCE, destroying the First Temple that was at the center of ancient Jewish worship. The Judean elite were exiled to Babylonia and this is regarded as the first Jewish Diaspora. Later many of them returned to their homeland after the subsequent conquest of Babylonia by the Persians seventy years later, a period known as the Babylonian Captivity. A new Second Temple was constructed, and old religious practices were resumed.
During the early years of the Second Temple, the highest religious authority was a council known as the Great Assembly, led by Ezra of the Book of Ezra. Among other accomplishments of the Great Assembly, the last books of the Bible were written at this time and the canon sealed.
Hellenistic Judaism spread to Ptolemaic Egypt from the 3rd century BCE. After the Great Revolt (66–73 CE), the Romans destroyed the Temple. Hadrian built a pagan idol on the Temple grounds and prohibited circumcision; these acts of ethnocide provoked the Bar Kokhba revolt 132–136 CE after which the Romans banned the study of the Torah and the celebration of Jewish holidays, and forcibly removed virtually all Jews from Judea. In 200 CE, however, Jews were granted Roman citizenship and Judaism was recognized as a "religio licita" ("legitimate religion"), until the rise of Gnosticism and Early Christianity in the fourth century.
Following the destruction of Jerusalem and the expulsion of the Jews, Jewish worship stopped being centrally organized around the Temple, prayer took the place of sacrifice, and worship was rebuilt around the community (represented by a minimum of ten adult men) and the establishment of the authority of rabbis who acted as teachers and leaders of individual communities (see Jewish diaspora).
Historical Jewish groupings (to 1700).
Around the 1st century CE there were several small Jewish sects: the Pharisees, Sadducees, Zealots, Essenes, and Christians. After the destruction of the Second Temple in 70 CE, these sects vanished. Christianity survived, but by breaking with Judaism and becoming a separate religion; the Pharisees survived but in the form of Rabbinic Judaism (today, known simply as "Judaism"). The Sadducees rejected the divine inspiration of the Prophets and the Writings, relying only on the Torah as divinely inspired. Consequently, a number of other core tenets of the Pharisees' belief system (which became the basis for modern Judaism), were also dismissed by the Sadducees. (The Samaritans practiced a similar religion, which is traditionally considered separate from Judaism.)
Like the Sadducees who relied only on the Torah, some Jews in the 8th and 9th centuries rejected the authority and divine inspiration of the oral law as recorded in the Mishnah (and developed by later rabbis in the two Talmuds), relying instead only upon the Tanakh. These included the Isunians, the Yudganites, the Malikites, and others. They soon developed oral traditions of their own, which differed from the rabbinic traditions, and eventually formed the Karaite sect. Karaites exist in small numbers today, mostly living in Israel. Rabbinical and Karaite Jews each hold that the others are Jews, but that the other faith is erroneous.
Over a long time, Jews formed distinct ethnic groups in several different geographic areas — amongst others, the Ashkenazi Jews (of central and Eastern Europe), the Sephardi Jews (of Spain, Portugal, and North Africa), the Beta Israel of Ethiopia, and the Yemenite Jews from the southern tip of the Arabian Peninsula. Many of these groups have developed differences in their prayers, traditions and accepted canons; however these distinctions are mainly the result of their being formed at some cultural distance from normative (rabbinic) Judaism, rather than based on any doctrinal dispute.
Persecutions.
Antisemitism arose during the Middle Ages, in the form of persecutions, pogroms, forced conversion, expulsions, social restrictions and ghettoization.
This was different in quality to any repressions of Jews in ancient times. Ancient repression was politically motivated and Jews were treated the same way as any other ethnic group would have been. With the rise of the Churches, attacks on Jews became motivated instead by theological considerations specifically deriving from Christian views about Jews and Judaism. During the Middle Ages, Jewish people under Muslim rule generally experienced tolerance and integration, but there were occasional outbreaks of violence like Almohad's persecutions.
Hasidism.
Hasidic Judaism was founded by Yisroel ben Eliezer (1700–1760), also known as the "Ba'al Shem Tov" (or "Besht"). It originated in a time of persecution of the Jewish people, when European Jews had turned inward to Talmud study; many felt that most expressions of Jewish life had become too "academic", and that they no longer had any emphasis on spirituality or joy. His disciples attracted many followers; they themselves established numerous Hasidic sects across Europe. Hasidic Judaism eventually became the way of life for many Jews in Europe. Waves of Jewish immigration in the 1880s carried it to the United States.
The movement itself claims to be nothing new, but a "refreshment" of original Judaism. Or as some have put it: " "they merely re-emphasized that which the generations had lost"". Nevertheless, early on there was a serious schism between Hasidic and non-Hasidic Jews. European Jews who rejected the Hasidic movement were dubbed by the Hasidim as Misnagdim, (lit. "opponents"). Some of the reasons for the rejection of Hasidic Judaism were the overwhelming exuberance of Hasidic worship, its untraditional ascriptions of infallibility and alleged miracle-working to their leaders, and the concern that it might become a messianic sect. Since then differences between the Hasidim and their opponents have slowly diminished and both groups are now considered part of Haredi Judaism.
The Enlightenment and new religious movements.
In the late 18th century CE, Europe was swept by a group of intellectual, social and political movements known as the Enlightenment. The Enlightenment led to reductions in the European laws that prohibited Jews to interact with the wider secular world, thus allowing Jews access to secular education and experience. A parallel Jewish movement, Haskalah or the "Jewish Enlightenment", began, especially in Central Europe and Western Europe, in response to both the Enlightenment and these new freedoms. It placed an emphasis on integration with secular society and a pursuit of non-religious knowledge through reason. With the promise of political emancipation many Jews saw no reason to continue to observe Jewish law and increasing numbers of Jews assimilated into Christian Europe. Modern religious movements of Judaism all formed in reaction to this trend.
In Central Europe, followed by Great Britain and the United States, Reform (or Liberal) Judaism developed, relaxing legal obligations (especially those that limited Jewish relations with non-Jews), emulating Protestant decorum in prayer, and emphasizing the ethical values of Judaism's Prophetic tradition. Modern Orthodox Judaism developed in reaction to Reform Judaism, by leaders who argued that Jews could participate in public life as citizens equal to Christians, while maintaining the observance of Jewish law. Meanwhile, in the United States, wealthy Reform Jews helped European scholars, who were Orthodox in practice but critical (and skeptical) in their study of the Bible and Talmud, to establish a seminary to train rabbis for immigrants from Eastern Europe. These left-wing Orthodox rabbis were joined by right-wing Reform rabbis who felt that Jewish law should not be entirely abandoned, to form the Conservative movement. Orthodox Jews who opposed the Haskalah formed Haredi Orthodox Judaism. After massive movements of Jews following The Holocaust and the creation of the state of Israel, these movements have competed for followers from among traditional Jews in or from other countries.
Spectrum of observance.
Countries such as the United States, Israel, Canada, United Kingdom, Argentina and South Africa contain large Jewish populations. Jewish religious practice varies widely through all levels of observance. According to the 2001 edition of the National Jewish Population Survey, in the United States' Jewish community—the world's second largest—4.3 million Jews out of 5.1 million had some sort of connection to the religion. Of that population of connected Jews, 80% participated in some sort of Jewish religious observance, but only 48% belonged to a synagogue, and fewer than 16% attend regularly.
Birth rates for American Jews have dropped from 2.0 to 1.7. (Replacement rate is 2.1.) Intermarriage rates range from 40-50% in the US, and only about a third of children of intermarried couples are raised as Jews. Due to intermarriage and low birth rates, the Jewish population in the US shrank from 5.5 million in 1990 to 5.1 million in 2001. This is indicative of the general population trends among the Jewish community in the Diaspora, but a focus on total population obscures growth trends in some denominations and communities, such as Haredi Judaism. The Baal teshuva movement is a movement of Jews who have "returned" to religion or become more observant.
Judaism and other religions.
Christianity and Judaism.
Christianity was originally a sect of Second Temple Judaism, but the two religions diverged in the first century. The differences between Christianity and Judaism originally centered on whether Jesus was the Jewish Messiah, but eventually became irreconcilable. Major differences between the two faiths include the nature of the Messiah, of atonement and sin, the status of God's commandments to Israel, and perhaps most significantly of the nature of God himself. Due to these differences, Judaism traditionally regards Christianity as Shituf, or worship of the God of Israel which is not monotheistic. Christianity has traditionally regarded Judaism as obsolete with the invention of Christianity and Jews as a people replaced by the Church, though a Christian belief in dual-covenant theology emerged as a phenomenon following Christian reflection on how their theology influenced the Nazi Holocaust.
Until their emancipation in the late 18th and the 19th century, Jews in Christian lands were subject to humiliating legal restrictions and limitations. They included provisions requiring Jews to wear specific and identifying clothing such as the Jewish hat and the yellow badge, restricting Jews to certain cities and towns or in certain parts of towns (ghettos), and forbidding Jews to enter certain trades (for example selling new clothes in medieval Sweden). Disabilities also included special taxes levied on Jews, exclusion from public life, restraints on the performance of religious ceremonies, and linguistic censorship. Some countries went even further and completely expelled Jews, for example England in 1290 (Jews were readmitted in 1655) and Spain in 1492 (readmitted in 1868). The first Jewish settlers in North America arrived in the Dutch colony of New Amsterdam in 1654; they were forbidden to hold public office, open a retail shop, or establish a synagogue. When the colony was seized by the British in 1664 Jewish rights remained unchanged, but by 1671 Asser Levy was the first Jew to serve on a jury in North America.
In 1791, Revolutionary France was the first country to abolish disabilities altogether, followed by Prussia in 1848. Emancipation of the Jews in the United Kingdom was achieved in 1858 after an almost 30-year struggle championed by Isaac Lyon Goldsmid with the ability of Jews to sit in parliament with the passing of the Jews Relief Act 1858. The newly united German Empire in 1871 abolished Jewish disabilities in Germany, which were reinstated in the Nuremberg Laws in 1935.
Jewish life in Christian lands was marked by frequent blood libels, expulsions, forced conversions and massacres. An underlying source of prejudice against Jews in Europe was religious. Christian rhetoric and antipathy towards Jews developed in the early years of Christianity and was reinforced by ever increasing anti-Jewish measures over the ensuing centuries. The action taken by Christians against Jews included acts of violence, and murder culminating in the Holocaust. These attitudes were reinforced in Christian preaching, art and popular teaching for two millennia, containing contempt for Jews, as well as statutes which were designed to humiliate and stigmatise Jews.
Islam and Judaism.
Both Judaism and Islamic religion arose from the patriarch Abraham, and are therefore considered Abrahamic religions. In both Jewish and Muslim tradition, the Jewish and Arab peoples are descended from the two sons of Abraham—Isaac and Ishmael, respectively. While both religions are monotheistic and share many commonalities, they differ in that Jews do not consider Jesus or Muhammad to be prophets. The religions' adherents have interacted with each other since the 7th century, when Islam originated and spread in the Arabian peninsula. Indeed, the years 712 to 1066 CE under the Ummayad and the Abbasid rulers have been called the Golden age of Jewish culture in Spain. Non-Muslim monotheists living in these countries, including Jews, were known as dhimmis. Dhimmis were allowed to practice their religion and to administer their internal affairs, but they were subject to certain restrictions that were not imposed on Muslims. For example, they had to pay the jizya, a per capita tax imposed on free adult non-Muslim males, and they were also forbidden to bear arms or testify in court cases involving Muslims. Many of the laws regarding dhimmis were highly symbolic. For example, dhimmis in some countries were required to wear distinctive clothing, a practice not found in either the Qur'an or hadiths but invented in early medieval Baghdad and inconsistently enforced. Jews in Muslim countries were not entirely free from persecution—for example, many were killed, exiled or forcibly converted in the 12th century, in Persia, and by the rulers of the Almohad dynasty in North Africa and Al-Andalus, as well as by the Zaydi imams of Yemen in the 17th century (see: Mawza Exile). At times, Jews were also restricted in their choice of residence—in Morocco, for example, Jews were confined to walled quarters (mellahs) beginning in the 15th century and increasingly since the early 19th century.
In the mid-20th century, Jews were expelled from nearly all of the Arab countries. Most have chosen to live in Israel. Today, antisemitic themes including Holocaust denial have become commonplace in the propaganda of Islamic movements such as Hizbullah and Hamas, in the pronouncements of various agencies of the Islamic Republic of Iran, and even in the newspapers and other publications of Refah Partisi.
Syncretic movements incorporating Judaism.
There are some movements that combine elements of Judaism with those of other religions. The most well-known of these is Messianic Judaism, a religious movement, which arose in the 1960s, that incorporates elements of Judaism with the tenets of Christianity. The movement states that Jesus is the Jewish Messiah, and generally that he is part of the Trinity, and salvation is only achieved through acceptance of Jesus as one's savior. Some members argue that Messianic Judaism is a sect of Judaism. Jewish organizations of every denomination reject this, stating that Messianic Judaism is a Christian sect, as it harbors identical creeds to that of Pauline Christianity.
Other examples of syncretism include Semitic neopaganism, a loosely organized sect which incorporates pagan or Wiccan beliefs with some Jewish religious practices; Jewish Buddhists, another loosely organized group that incorporates elements of Asian spirituality in their faith; and some Renewal Jews who borrow freely and openly from Buddhism, Sufism, Native American religion, and other faiths.
The Kabbalah Centre, which employs teachers from multiple religions, is a New Age movement that claims to popularize the kabbalah, part of the Jewish esoteric tradition.
Bibliography.
Jews in Islamic countries:
External links.
See also Torah database for links to more Judaism e-texts.
Text study projects at Wikisource. In many instances, the Hebrew versions of these projects are more fully developed than the English.

</doc>
<doc id="15626" url="https://en.wikipedia.org/wiki?curid=15626" title="John Stuart Mill">
John Stuart Mill

John Stuart Mill (20 May 1806 – 8 May 1873) was an English philosopher, political economist, feminist, and civil servant. One of the most influential thinkers in the history of liberalism, he contributed widely to social theory, political theory and political economy. He has been called "the most influential English-speaking philosopher of the nineteenth century." Mill's conception of liberty justified the freedom of the individual in opposition to unlimited state control.
Mill expresses his view on freedom by illustrating how an individual's drive to better their station, and for self-improvement, is the sole source of true freedom. Only when an individual is able to attain such improvements, without impeding others in their own efforts to do the same, can true freedom prevail. Mill's linking of freedom and self-improvement has inspired many. By establishing that individual efforts to excel have worth, Mill was able to show how they should achieve self-improvement without harming others, or society at large.
Among his philosophical achievements, he was a proponent of utilitarianism, an ethical theory developed by Jeremy Bentham, and he worked on the theory of the scientific method. Mill was also a Member of Parliament belonging to the Liberal Party.
Biography.
John Stuart Mill was born on Rodney Street in the Pentonville area of London, the eldest son of the Scottish philosopher, historian and economist James Mill, and Harriet Burrow. John Stuart was educated by his father, with the advice and assistance of Jeremy Bentham and Francis Place. He was given an extremely rigorous upbringing, and was deliberately shielded from association with children his own age other than his siblings. His father, a follower of Bentham and an adherent of associationism, had as his explicit aim to create a genius intellect that would carry on the cause of
utilitarianism and its implementation after he and Bentham had died.
Mill was a notably precocious child. He describes his education in his autobiography. At the age of three he was taught Greek. By the age of eight, he had read "Aesop's Fables", Xenophon's "Anabasis", and the whole of Herodotus, and was acquainted with Lucian, Diogenes Laërtius, Isocrates and six dialogues of Plato. He had also read a great deal of history in English and had been taught arithmetic, physics and astronomy.
At the age of eight, Mill began studying Latin, the works of Euclid, and algebra, and was appointed schoolmaster to the younger children of the family. His main reading was still history, but he went through all the commonly taught Latin and Greek authors and by the age of ten could read Plato and Demosthenes with ease. His father also thought that it was important for Mill to study and compose poetry. One of Mill's earliest poetry compositions was a continuation of the Iliad. In his spare time, he also enjoyed reading about natural sciences and popular novels, such as "Don Quixote" and "Robinson Crusoe".
His father's work, "The History of British India" was published in 1818; immediately thereafter, about the age of twelve, Mill began a thorough study of the scholastic logic, at the same time reading Aristotle's logical treatises in the original language. In the following year he was introduced to political economy and studied Adam Smith and David Ricardo with his father, ultimately completing their classical economic view of factors of production. Mill's "comptes rendus" of his daily economy lessons helped his father in writing "Elements of Political Economy" in 1821, a textbook to promote the ideas of Ricardian economics; however, the book lacked popular support. Ricardo, who was a close friend of his father, used to invite the young Mill to his house for a walk in order to talk about political economy.
At the age of fourteen, Mill stayed a year in France with the family of Sir Samuel Bentham, brother of Jeremy Bentham. The mountain scenery he saw led to a lifelong taste for mountain landscapes. The lively and friendly way of life of the French also left a deep impression on him. In Montpellier, he attended the winter courses on chemistry, zoology, logic of the "Faculté des Sciences", as well as taking a course of the higher mathematics. While coming and going from France, he stayed in Paris for a few days in the house of the renowned economist Jean-Baptiste Say, a friend of Mill's father. There he met many leaders of the Liberal party, as well as other notable Parisians, including Henri Saint-Simon.
Mill went through months of sadness and pondered suicide at twenty years of age. According to the opening paragraphs of Chapter V, of his autobiography, he had asked himself whether the creation of a just society, his life's objective, would actually make him happy. His heart answered "no" and unsurprisingly, he lost the happiness of striving towards this objective. Eventually, the poetry of William Wordsworth showed him beauty generates compassion for others and stimulates joy. [http://www.victorianweb.org/philosophy/mill/crisis.html] With renewed joy, he continued to work towards a just society, but with more relish for the journey. He considered this one of the most pivotal shifts in his thinking. In fact, many of the differences between him and his father stemmed from this expanded source of joy.
Mill had been engaged in a pen-friendship with Auguste Comte, the founder of positivism and sociology, since Mill first contacted Comte in November 1841. Comte's "sociologie" was more an early philosophy of science than we perhaps know it today, and the "positive" philosophy aided in Mill's broad rejection of Benthamism.
As a nonconformist who refused to subscribe to the Thirty-Nine Articles of the Church of England, Mill was not eligible to study at the University of Oxford or the University of Cambridge. Instead he followed his father to work for the East India Company, and attended University College, London, to hear the lectures of John Austin, the first Professor of Jurisprudence. He was elected a Foreign Honorary Member of the American Academy of Arts and Sciences in 1856.
Mill's career as a colonial administrator at the British East India Company spanned from when he was 17 years old in 1823 until 1858, when the Company was abolished in favor of direct rule by the British crown over India. In 1836, he was promoted to the Company's Political Department, where he was responsible for correspondence pertaining to the Company's relations with the princely states, and in 1856, was finally promoted to the position of Examiner of Indian Correspondence. In "On Liberty", "A Few Words on Non-Intervention", and other works, Mill defended British imperialism by arguing that a fundamental distinction existed between civilized and barbarous peoples. Mill viewed countries such as India and China as having once been progressive, but that were now stagnant and barbarous, thus legitimizing British rule as benevolent despotism, "provided the end is barbarians' improvement." When the crown proposed to take direct control over the colonies in India, he was tasked with defending Company rule, penning "Memorandum on the Improvements in the Administration of India during the Last Thirty Years" among other petitions. He was offered a seat on the Council of India, the body created to advise the new Secretary of State for India, but declined, citing his disapproval of the new system of rule.
In 1851, Mill married Harriet Taylor after 21 years of an intimate friendship. Taylor was married when they met, and their relationship was close but generally believed to be chaste during the years before her first husband died. Brilliant in her own right, Taylor was a significant influence on Mill's work and ideas during both friendship and marriage. His relationship with Harriet Taylor reinforced Mill's advocacy of women's rights. He cites her influence in his final revision of "On Liberty", which was published shortly after her death. Taylor died in 1858 after developing severe lung congestion, after only seven years of marriage to Mill.
Between the years 1865 and 1868 Mill served as Lord Rector of the University of St. Andrews. During the same period, 1865–68, he was a Member of Parliament for City and Westminster, sitting for the Liberal Party. During his time as an MP, Mill advocated easing the burdens on Ireland. In 1866, Mill became the first person in the history of Parliament to call for women to be given the right to vote, vigorously defending this position in subsequent debate. Mill became a strong advocate of such social reforms as labour unions and farm cooperatives. In "Considerations on Representative Government", Mill called for various reforms of Parliament and voting, especially proportional representation, the Single Transferable Vote, and the extension of suffrage.
He was godfather to the philosopher Bertrand Russell.
In his views on religion, Mill was an atheist.
Mill died in 1873 of erysipelas in Avignon, France, where he was buried alongside his wife.
Works.
Theory of liberty.
Mill's "On Liberty" addresses the nature and limits of the power that can be legitimately exercised by society over the individual. However Mill is clear that his concern for liberty does not extend to all individuals and all societies. He states that "Despotism is a legitimate mode of government in dealing with barbarians".
Mill states that it is acceptable to harm oneself as long as the person doing so is not harming others. He also argues that individuals should be prevented from doing lasting, serious harm to themselves or their property by the harm principle. Because no one exists in isolation, harm done to oneself may also harm others, and destroying property deprives the community as well as oneself. Mill excuses those who are "incapable of self-government" from this principle, such as young children or those living in "backward states of society".
Though this principle seems clear, there are a number of complications. For example, Mill explicitly states that "harms" may include acts of omission as well as acts of commission. Thus, failing to rescue a drowning child counts as a harmful act, as does failing to pay taxes, or failing to appear as a witness in court. All such harmful omissions may be regulated, according to Mill. By contrast, it does not count as harming someone if – without force or fraud – the affected individual consents to assume the risk: thus one may permissibly offer unsafe employment to others, provided there is no deception involved. (Mill does, however, recognise one limit to consent: society should not permit people to sell themselves into slavery). In these and other cases, it is important to bear in mind that the arguments in "On Liberty" are grounded on the principle of Utility, and not on appeals to natural rights.
The question of what counts as a self-regarding action and what actions, whether of omission or commission, constitute harmful actions subject to regulation, continues to exercise interpreters of Mill. It is important to emphasise that Mill did not consider giving offence to constitute "harm"; an action could not be restricted because it violated the conventions or morals of a given society.
"On Liberty" involves an impassioned defense of free speech. Mill argues that free discourse is a necessary condition for intellectual and social progress. We can never be sure, he contends, that a silenced opinion does not contain some element of the truth. He also argues that allowing people to air false opinions is productive for two reasons. First, individuals are more likely to abandon erroneous beliefs if they are engaged in an open exchange of ideas. Second, by forcing other individuals to re-examine and re-affirm their beliefs in the process of debate, these beliefs are kept from declining into mere dogma. It is not enough for Mill that one simply has an unexamined belief that happens to be true; one must understand why the belief in question is the true one. Along those same lines Mill wrote, "unmeasured vituperation, employed on the side of prevailing opinion, really does deter people from expressing contrary opinions, and from listening to those who express them."
Social liberty and tyranny of majority.
Mill believed that "the struggle between Liberty and Authority is the most conspicuous feature in the portions of history." For him, liberty in antiquity was a "contest... between subjects, or some classes of subjects, and the government." Mill defined "social liberty" as protection from "the tyranny of political rulers." He introduced a number of different concepts of the form tyranny can take, referred to as social tyranny, and tyranny of the majority.
Social liberty for Mill meant putting limits on the ruler's power so that he would not be able to use his power on his own wishes and make decisions which could harm society; in other words, people should have the right to have a say in the government's decisions. He said that social liberty was "the nature and limits of the power which can be legitimately exercised by society over the individual". It was attempted in two ways: first, by obtaining recognition of certain immunities, called political liberties or rights; second, by establishment of a system of "constitutional checks".
However, in Mill's view, limiting the power of government was not enough. He stated, "Society can and does execute its own mandates: and if it issues wrong mandates instead of right, or any mandates at all in things with which it ought not to meddle, it practices a social tyranny more formidable than many kinds of political oppression, since, though not usually upheld by such extreme penalties, it leaves fewer means of escape, penetrating much more deeply into the details of life, and enslaving the soul itself."
Liberty.
John Stuart Mill's view on liberty, which was influenced by Joseph Priestley and Josiah Warren, is that the individual ought to be free to do as he wishes unless he harms others. Individuals are rational enough to make decisions about their well being. Government should interfere when it is for the protection of society. Mill explained:
The sole end for which mankind are warranted, individually or collectively, in interfering with the liberty of action of any of their number, is self-protection. That the only purpose for which power can be rightfully exercised over any member of a civilized community, against his will, is to prevent harm to others. His own good, either physical or moral, is not sufficient warrant. He cannot rightfully be compelled to do or forbear because it will be better for him to do so, because it will make him happier, because, in the opinion of others, to do so would be wise, or even right...The only part of the conduct of anyone, for which he is amenable to society, is that which concerns others. In the part which merely concerns him, his independence is, of right, absolute. Over himself, over his own body and mind, the individual is sovereign.
Mill added: "Despotism is a legitimate mode of government in dealing with barbarians, provided the end be their improvement, and the means justified by actually effecting that end. Liberty, as a principle, has no application to any state of things anterior to the time when mankind have become capable of being improved by free and equal discussion."
Freedom of speech.
An influential advocate of freedom of speech, Mill objected to censorship. He says:
I choose, by preference the cases which are least favourable to me – In which the argument opposing freedom of opinion, both on truth and that of utility, is considered the strongest. Let the opinions impugned be the belief of God and in a future state, or any of the commonly received doctrines of morality... But I must be permitted to observe that it is not the feeling sure of a doctrine (be it what it may) which I call an assumption of infallibility. It is the undertaking to decide that question "for others", without allowing them to hear what can be said on the contrary side. And I denounce and reprobate this pretension not the less if it is put forth on the side of my most solemn convictions. However, positive anyone's persuasion may be, not only of the faculty but of the pernicious consequences, but (to adopt expressions which I altogether condemn) the immorality and impiety of opinion. – yet if, in pursuance of that private judgement, though backed by the public judgement of his country or contemporaries, he prevents the opinion from being heard in its defence, he assumes infallibility. And so far from the assumption being less objectionable or less dangerous because the opinion is called immoral or impious, this is the case of all others in which it is most fatal.
Mill outlines the benefits of 'searching for and discovering the truth' as a way to further knowledge. He argued that even if an opinion is false, the truth can be better understood by refuting the error. And as most opinions are neither completely true nor completely false, he points out that allowing free expression allows the airing of competing views as a way to preserve partial truth in various opinions. Worried about minority views being suppressed, Mill also argued in support of freedom of speech on political grounds, stating that it is a critical component for a representative government to have in order to empower debate over public policy. Mill also eloquently argued that freedom of expression allows for personal growth and self-realization. He said that freedom of speech was a vital way to develop talents and realise a person's potential and creativity. He repeatedly said that eccentricity was preferable to uniformity and stagnation.
Colonialism.
Mill, an employee for the British East India Company from 1823 to 1858, argued in support of what he called a 'benevolent despotism' with regard to the colonies. Mill argued that "To suppose that the same international customs, and the same rules of international morality, can obtain between one civilized nation and another, and between civilized nations and barbarians, is a grave error...To characterize any conduct whatever towards a barbarous people as a violation of the law of nations, only shows that he who so speaks has never considered the subject."
Slavery.
In 1850, Mill sent an anonymous letter (which came to be known under the title "The Negro Question"), in rebuttal to Thomas Carlyle's anonymous letter to "Fraser's Magazine for Town and Country" in which Carlyle argued for slavery. Mill supported abolition in the United States.
In Mill's essay from 1869, "The Subjection of Women", he expressed his opposition to slavery:
This absolutely extreme case of the law of force, condemned by those who can tolerate almost every other form of arbitrary power, and which, of all others, presents features the most revolting to the feeling of all who look at it from an impartial position, was the law of civilized and Christian England within the memory of persons now living: and in one half of Angle-Saxon America three or four years ago, not only did slavery exist, but the slave trade, and the breeding of slaves expressly for it, was a general practice between slave states. Yet not only was there a greater strength of sentiment against it, but, in England at least, a less amount either of feeling or of interest in favour of it, than of any other of the customary abuses of force: for its motive was the love of gain, unmixed and undisguised: and those who profited by it were a very small numerical fraction of the country, while the natural feeling of all who were not personally interested in it, was unmitigated abhorrence.
Domenico Losurdo argues that "In Mill's view, 'any means were licit for those who took on the task of educating 'savage tribes'; 'slavery' was sometimes a mandatory stage for inducing them to work and making them useful to civilization and progress."
Women's rights.
Mill's view of history was that right up until his time "the whole of the female" and "the great majority of the male sex" were simply "slaves". He countered arguments to the contrary, arguing that relations between sexes simply amounted to "the legal subordination of one sex to the other — is wrong itself, and now one of the chief hindrances to human improvement; and that it ought to be replaced by a principle of perfect equality." With this, Mill can be considered among the earliest women's rights advocates. His book "The Subjection of Women" (1861, published 1869) is one of the earliest written on this subject by a male author. In "The Subjection of Women" Mill attempts to make a case for perfect equality. He talks about the role of women in marriage and how it needed to be changed. There, Mill comments on three major facets of women's lives that he felt are hindering them: society and gender construction, education, and marriage. He argued that the oppression of women was one of the few remaining relics from ancient times, a set of prejudices that severely impeded the progress of humanity.
Mill's ideas were opposed by Ernest Belfort Bax in his treatise, "The Legal Subjection of Men".
Utilitarianism.
The canonical statement of Mill's utilitarianism can be found in "Utilitarianism." This philosophy has a long tradition, although Mill's account is primarily influenced by Jeremy Bentham and Mill's father James Mill.
Jeremy Bentham's famous formulation of utilitarianism is known as the "greatest-happiness principle". It holds that one must always act so as to produce the greatest aggregate happiness among all sentient beings, within reason. Mill's major contribution to utilitarianism is his argument for the qualitative separation of pleasures. Bentham treats all forms of happiness as equal, whereas Mill argues that intellectual and moral pleasures (higher pleasures) are superior to more physical forms of pleasure (lower pleasures). Mill distinguishes between happiness and contentment, claiming that the former is of higher value than the latter, a belief wittily encapsulated in the statement that "it is better to be a human being dissatisfied than a pig satisfied; better to be Socrates dissatisfied than a fool satisfied. And if the fool, or the pig, are of a different opinion, it is because they only know their own side of the question."
Mill defines the difference between higher and lower forms of pleasure with the principle that those who have experienced both tend to prefer one over the other. This is, perhaps, in direct contrast with Bentham's statement that "Quantity of pleasure being equal, push-pin is as good as poetry", that, if a simple child's game like hopscotch causes more pleasure to more people than a night at the opera house, it is more imperative upon a society to devote more resources to propagating hopscotch than running opera houses. Mill's argument is that the "simple pleasures" tend to be preferred by people who have no experience with high art, and are therefore not in a proper position to judge. Mill also argues that people who, for example, are noble or practice philosophy, benefit society more than those who engage in individualist practices for pleasure, which are lower forms of happiness. It is not the agent's own greatest happiness that matters "but the greatest amount of happiness altogether".
The qualitative account of happiness that Mill advocates thus sheds light on his account presented in "On Liberty". As Mill suggests in that text, utility is to be conceived in relation to humanity "as a progressive being", which includes the development and exercise of rational capacities as we strive to achieve a "higher mode of existence". The rejection of censorship and paternalism is intended to provide the necessary social conditions for the achievement of knowledge and the greatest ability for the greatest number to develop and exercise their deliberative and rational capacities.
Economic philosophy.
Mill's early economic philosophy was one of free markets. However, he accepted interventions in the economy, such as a tax on alcohol, if there were sufficient utilitarian grounds. He also accepted the principle of legislative intervention for the purpose of animal welfare. Mill originally believed that "equality of taxation" meant "equality of sacrifice" and that progressive taxation penalised those who worked harder and saved more and was therefore "a mild form of robbery".
Given an equal tax rate regardless of income, Mill agreed that inheritance should be taxed. A utilitarian society would agree that everyone should be equal one way or another. Therefore, receiving inheritance would put one ahead of society unless taxed on the inheritance.
Those who donate should consider and choose carefully where their money goes—some charities are more deserving than others. Considering public charities boards such as a government will disburse the money equally. However, a private charity board like a church would disburse the monies fairly to those who are in more need than others.
Later he altered his views toward a more socialist bent, adding chapters to his Principles of Political Economy in defence of a socialist outlook, and defending some socialist causes. Within this revised work he also made the radical proposal that the whole wage system be abolished in favour of a co-operative wage system. Nonetheless, some of his views on the idea of flat taxation remained, albeit altered in the third edition of the "Principles of Political Economy" to reflect a concern for differentiating restrictions on "unearned" incomes, which he favoured, and those on "earned" incomes, which he did not favour.
Mill's "Principles", first published in 1848, was one of the most widely read of all books on economics in the period. As Adam Smith's "Wealth of Nations" had during an earlier period, Mill's "Principles" dominated economics teaching. In the case of Oxford University it was the standard text until 1919, when it was replaced by Marshall's "Principles of Economics".
Economic democracy.
Mill promoted economic democracy instead of capitalism, in the manner of substituting capitalist businesses with worker cooperatives. He says:
The form of association, however, which if mankind continue to improve, must be expected in the end to predominate, is not that which can exist between a capitalist as chief, and work-people without a voice in the management, but the association of the labourers themselves on terms of equality, collectively owning the capital with which they carry on their operations, and working under managers elected and removable by themselves.
Political democracy.
Mill's major work on political democracy, "Considerations on Representative Government", defends two fundamental principles, extensive participation by citizens and enlightened competence of rulers. The two values are obviously in tension, and some readers have concluded that he is an elitist democrat, while others count him as an earlier participatory democrat. In one section he appears to defend plural voting, in which more competent citizens are given extra votes (a view he later repudiated). But in chapter 3 he presents what is still one of the most eloquent cases for the value of participation by all citizens. He believed that the incompetence of the masses could eventually be overcome if they were given a chance to take part in politics, especially at the local level.
Mill is one of the few political philosophers ever to serve in government as an elected official. In his three years in Parliament, he was more willing to compromise than the "radical" principles expressed in his writing would lead one to expect.
The environment.
Mill demonstrated an early insight into the value of the natural world – in particular in Book IV, chapter VI of "Principles of Political Economy": "Of the Stationary State" in which Mill recognised wealth beyond the material, and argued that the logical conclusion of unlimited growth was destruction of the environment and a reduced quality of life. He concluded that a stationary state could be preferable to unending economic growth:
I cannot, therefore, regard the stationary states of capital and wealth with the unaffected aversion so generally manifested towards it by political economists of the old school.
If the earth must lose that great portion of its pleasantness which it owes to things that the unlimited increase of wealth and population would extirpate from it, for the mere purpose of enabling it to support a larger, but not a better or a happier population, I sincerely hope, for the sake of posterity, that they will be content to be stationary, long before necessity compel them to it.
Economic development.
Mill regarded economic development as a function of land, labour and capital. While land and labour are the two original factors of production, capital is "a stock, previously accumulated of the products of former labour." Increase in wealth is possible only if land and capital help to increase production faster than the labour force. It is productive labour that is productive of wealth and capital accumulation. "The rate of capital accumulation is the function of the proportion of the labour force employed ' productively. Profits earned by employing unproductive labours are merely transfers of income; unproductive labour does not generate wealth or income" . It is productive labourers who do productive consumption. Productive consumption is that "which maintains and increase the productive capacity of the community." It implies that productive consumption is an input necessary to maintain productive labourers.
Control of population growth.
Mill supported the Malthusian theory of population. By population he meant the number of the working class only. He was therefore concerned about the growth in number of labourers who worked for hire. He believed that population control was essential for improving the condition of the working class so that they might enjoy the fruits of the technological progress and capital accumulation. Mill advocated birth control. In 1823 Mill and a friend were arrested while distributing pamphlets on birth control by Francis Place to women in working class areas.
Wage fund.
According to Mill, supply is very elastic in response to wages. Wages generally exceed the minimum subsistence level, and are paid out of capital. Hence, wages are limited by existing capital for paying wages. Thus, wage per worker can be derived by dividing the total circulating capital by the size of the working population. Wages can increase by an increase in the capital used in paying wages, or by decrease in the number of workers. If wages rise, supply of labour will rise. Competition among workers not only brings down wages, but also keeps some workers out of employment. This is based on Mill's notion that "demand for commodities is not demand for labourers". It means that income invested as advances of wages to labour creates employment, and not income spent on consumer goods. An increase in consumption causes a decline in investment. So increased investment leads to increases in the wage fund and to economic progress.
In 1869, Mill recanted his support of the Wage-Fund Doctrine due to recognition that capital is not necessarily fixed in that it can be supplemented through “income of the employer which might otherwise go into savings or be spent on consumption.” (Spiegel, p. 390) Walker also states in “The Wages Question” that the limits on capital and the growth in population “were accidental, not essential” to the formation of the doctrine. The limitation on the growth of industrial capacity placed a limit on the number of workers who could be accommodated more than the limit on capital. Furthermore, English agriculture “had reached the condition of diminishing returns." (Walker); therefore, each additional worker was not providing more output than he needed for himself for survival. Given the improvements in technology and productivity that followed 1848, the original reasons that gave rise to the doctrine were seen to be unusual and not the basis for a universal law.
Rate of capital accumulation.
According to Mill, the rate of capital accumulation depends on: (1) "the amount of fund from which saving can be made" or "the size of the net produce of the industry", and (2) the "disposition to save". Capital is the result of savings, and the savings come from the "abstinence from present consumption for the sake of future goods". Although capital is the result of saving, it is nevertheless consumed. This means saving is spending. Since saving depends on the net produce of the industry, it grows with profits and rent which go into making the net produce. On the other hand, the disposition to save depends on (1) the rate of profit and (2) the desire to save, or what Mill called "effective desire of accumulation". However, profit also depends on the cost of labour, and the rate of profit is the ratio of profits to wages. When profits rise or wages fall, the rate of profits increases, which in turn increases the rate of capital accumulation. Similarly, it is the desire to save which tends to increase the rate of capital accumulation.
Rate of profit.
According to Mill, the ultimate tendency in an economy is for the rate of profit to decline due to diminishing returns in agriculture and increase in population at a Malthusian rate.
In popular culture.
<poem>
John Stuart Mill,
By a mighty effort of will,
Overcame his natural bonhomie
And wrote "Principles of Political Economy".
</poem>

</doc>
<doc id="15627" url="https://en.wikipedia.org/wiki?curid=15627" title="Junk science">
Junk science

The expression junk science is used to describe scientific data, research, or analysis considered by the person using the phrase to be spurious or fraudulent. The concept is often invoked in political and legal contexts where facts and scientific results have a great amount of weight in making a determination. It usually conveys a pejorative connotation that the research has been untowardly driven by political, ideological, financial, or otherwise unscientific motives.
The concept was popularized in the 1990s in relation to expert testimony in civil litigation. More recently, invoking the concept has been a tactic to criticize research on the harmful environmental or public health effects of corporate activities, and occasionally in response to such criticism. The term has been used by proponents of both sides of such political debates. Author Dan Agin in his book "Junk Science" harshly criticized those who deny the basic premise of global warming, while former Fox News commentator Steven Milloy has extensively denounced research linking the fossil fuel industry to climate change, on his website "junkscience.com".
In some contexts, junk science is counterposed to the "sound science" or "solid science" that favors one's own point of view. This dichotomy has been particularly promoted by Steven Milloy and the Advancement of Sound Science Center, and is somewhat different from pseudoscience and fringe science.
History.
The phrase "junk science" appears to have been in use prior to 1985. A 1985 United States Department of Justice report by the Tort Policy Working Group noted:
"The use of such invalid scientific evidence (commonly referred to as 'junk science') has resulted in findings of causation which simply cannot be justified or understood from the standpoint of the current state of credible scientific or medical knowledge."
In 1989, the climate scientist Jerry Mahlman (Director of the Geophysical Fluid Dynamics Laboratory) characterized the theory that global warming was due to solar variation (presented in "Scientific Perspectives on the Greenhouse Problem" by Frederick Seitz et al.) as "noisy junk science."
Peter W. Huber popularized the term with respect to litigation in his 1991 book "Galileo's Revenge: Junk Science in the Courtroom." The book has been cited in over 100 legal textbooks and references; as a consequence, some sources cite Huber as the first to coin the term. By 1997, the term had entered the legal lexicon as seen in an opinion by Supreme Court of the United States Justice John Paul Stevens: 
"An example of 'junk science' that should be excluded under the Daubert standard as too unreliable would be the testimony of a phrenologist who would purport to prove a defendant's future dangerousness based on the contours of the defendant's skull." Lower courts have subsequently set guidelines for identifying junk science, such as the 2005 opinion of United States Court of Appeals for the Seventh Circuit Judge Easterbrook:
"Positive reports about magnetic water treatment are not replicable; this plus the lack of a physical explanation for any effects are hallmarks of junk science."
As the subtitle of Huber's book, "Junk Science in the Courtroom", suggests, his emphasis was on the use or misuse of expert testimony in civil litigation. One prominent example cited in the book was litigation over casual contact in the spread of AIDS. A California school district sought to prevent a young boy with AIDS, Ryan Thomas, from attending kindergarten. The school district produced an expert witness, Dr. Steven Armentrout, who testified that a possibility existed that AIDS could be transmitted to schoolmates through yet undiscovered "vectors." However, five experts testified on behalf of Thomas that AIDS is not transmitted through casual contact, and the court affirmed the "solid science" (as Mr. Huber called it) and rejected Dr. Armentrout's argument.
In 1999, Paul Ehrlich and others advocated public policies to improve the dissemination of valid environmental scientific knowledge and discourage junk science: 
"The Intergovernmental Panel on Climate Change reports offer an antidote to junk science by articulating the current consensus on the prospects for climate change, by outlining the extent of the uncertainties, and by describing the potential benefits and costs of policies to address climate change."
In a 2003 study about changes in environmental activism regarding the Crown of the Continent Ecosystem, Pedynowski noted that junk science can undermine the credibility of science over a much broader scale because misrepresentation by special interests casts doubt on more defensible claims and undermines the credibility of all research.
In his 2006 book "Junk Science", Dan Agin emphasized two main causes of junk science: fraud, and ignorance. In the first case, Agin discussed falsified results in the development of organic transistors: 
"As far as understanding junk science is concerned, the important aspect is that both Bell Laboratories and the international physics community were fooled until someone noticed that noise records published by Jan Hendrik Schön in several papers were identical—which means physically impossible."
In the second case, he cites an example that demonstrates ignorance of statistical principles in the lay press: 
"Since no such proof is possible genetically modified food is harmless, the article in The New York Times was what is called a "bad rap" against the U.S. Department of Agriculture—a bad rap based on a junk-science belief that it's possible to prove a null hypothesis."
Agin asks the reader to step back from the rhetoric, as "how things are labeled does not make a science junk science." In its place, he offers that junk science is ultimately motivated by the desire to hide undesirable truths from the public.
Use as corporate PR.
John Stauber and Sheldon Rampton of "PR Watch" say the concept of junk science has come to be invoked in attempts to dismiss scientific findings that stand in the way of short-term corporate profits. In their book "Trust Us, We're Experts" (2001), they write that industries have launched multimillion-dollar campaigns to position certain theories as junk science in the popular mind, often failing to employ the scientific method themselves. For example, the tobacco industry has described research demonstrating the harmful effects of smoking and second-hand smoke as junk science, through the vehicle of various astroturf groups.
Theories more favorable to corporate activities are portrayed in words as "sound science." Past examples where "sound science" was used include the research into the toxicity of Alar, which was heavily criticized by antiregulatory advocates, and Herbert Needleman's research into low dose lead poisoning. Needleman was accused of fraud and personally attacked.
Fox News commentator Steven Milloy often invokes the concept of junk science to attack the results of credible scientific research on topics like global warming, ozone depletion, and passive smoking. The credibility of Milloy's website junkscience.com was questioned by Paul D. Thacker, a writer for "The New Republic", in the wake of evidence that Milloy had received funding from Philip Morris, RJR Tobacco, and Exxon Mobil. Thacker also noted that Milloy was receiving almost $100,000 a year in consulting fees from Philip Morris while he criticized the evidence regarding the hazards of second-hand smoke as junk science. Following the publication of this article, the Cato Institute, which had hosted the junkscience.com site, ceased its association with the site and removed Milloy from its list of adjunct scholars.
Tobacco industry documents reveal that Philip Morris executives conceived of the "Whitecoat Project" in the 1980s as a response to emerging scientific data on the harmfulness of second-hand smoke. The goal of the Whitecoat Project, as conceived by Philip Morris and other tobacco companies, was to use ostensibly independent "scientific consultants" to spread doubt in the public mind about scientific data through invoking concepts like junk science. According to epidemiologist David Michaels, Assistant Secretary of Energy for Environment, Safety, and Health in the Clinton Administration, the tobacco industry invented the "sound science" movement in the 1980s as part of their campaign against the regulation of second-hand smoke.
David Michaels has argued that, since the U.S. Supreme Court ruling in "Daubert v. Merrell Dow Pharmaceuticals, Inc.", lay judges have become "gatekeepers" of scientific testimony and, as a result, respected scientists have sometimes been unable to provide testimony so that corporate defendants are "increasingly emboldened" to accuse adversaries of practicing junk science.
Use by scientists.
In 1995, the Union of Concerned Scientists launched the Sound Science Initiative, a national network of scientists committed to debunking junk science through media outreach, lobbying, and developing joint strategies to participate in town meetings or public hearings. In its newsletter on Science and Technology in Congress, the American Association for the Advancement of Science also recognized the need for increased understanding between scientists and lawmakers: "Although most individuals would agree that sound science is preferable to junk science, fewer recognize what makes a scientific study 'good' or 'bad'." The American Dietetic Association, criticizing marketing claims made for food products, has created a list of "Ten Red Flags of Junk Science."
Individual scientists have also invoked the concept.

</doc>
<doc id="15628" url="https://en.wikipedia.org/wiki?curid=15628" title="Java (disambiguation)">
Java (disambiguation)

Java is an island of Indonesia.
Java may also refer to:

</doc>
<doc id="15630" url="https://en.wikipedia.org/wiki?curid=15630" title="James Cook">
James Cook

Captain James Cook (7 November 172814 February 1779) was a British explorer, navigator, cartographer, and captain in the Royal Navy. Cook made detailed maps of Newfoundland prior to making three voyages to the Pacific Ocean, during which he achieved the first recorded European contact with the eastern coastline of Australia and the Hawaiian Islands, and the first recorded circumnavigation of New Zealand.
Cook joined the British merchant navy as a teenager and joined the Royal Navy in 1755. He saw action in the Seven Years' War, and subsequently surveyed and mapped much of the entrance to the Saint Lawrence River during the siege of Quebec. This helped bring Cook to the attention of the Admiralty and Royal Society. This notice came at a crucial moment in both Cook's career and the direction of British overseas exploration, and led to his commission in 1766 as commander of for the first of three Pacific voyages.
In three voyages Cook sailed thousands of miles across largely uncharted areas of the globe. He mapped lands from New Zealand to Hawaii in the Pacific Ocean in greater detail and on a scale not previously achieved. As he progressed on his voyages of discovery he surveyed and named features, and recorded islands and coastlines on European maps for the first time. He displayed a combination of seamanship, superior surveying and cartographic skills, physical courage and an ability to lead men in adverse conditions.
Cook was killed in Hawaii in a fight with Hawaiians during his third exploratory voyage in the Pacific in 1779. He left a legacy of scientific and geographical knowledge which was to influence his successors well into the 20th century and numerous memorials worldwide have been dedicated to him.
Early life and family.
James Cook was born on 27 October 1728 in the village of Marton in Yorkshire and baptised on 3 November in the local church of S. Cuthbert, where his name can be seen in the church register. He was the second of eight children of James Cook, a Scottish farm labourer from Ednam in Roxburghshire, and his locally born wife, Grace Pace, from Thornaby-on-Tees. In 1736, his family moved to Airey Holme farm at Great Ayton, where his father's employer, Thomas Skottowe, paid for him to attend the local school. In 1741, after five years schooling, he began work for his father, who had by now been promoted to farm manager. For leisure, he would climb a nearby hill, Roseberry Topping, enjoying the opportunity for solitude. Cooks' Cottage, his parents' last home, which he is likely to have visited, is now in Melbourne, having been moved from England and reassembled, brick by brick, in 1934.
In 1745, when he was 16, Cook moved to the fishing village of Staithes, to be apprenticed as a shop boy to grocer and haberdasher William Sanderson. Historians have speculated that this is where Cook first felt the lure of the sea while gazing out of the shop window.
After 18 months, not proving suitable for shop work, Cook travelled to the nearby port town of Whitby to be introduced to friends of Sanderson's, John and Henry Walker. The Walkers were prominent local ship-owners and Quakers, and were in the coal trade. Their house is now the Captain Cook Memorial Museum. Cook was taken on as a merchant navy apprentice in their small fleet of vessels, plying coal along the English coast. His first assignment was aboard the collier "Freelove", and he spent several years on this and various other coasters, sailing between the Tyne and London. As part of his apprenticeship, Cook applied himself to the study of algebra, geometry, trigonometry, navigation and astronomy—all skills he would need one day to command his own ship.
His three-year apprenticeship completed, Cook began working on trading ships in the Baltic Sea. After passing his examinations in 1752, he soon progressed through the merchant navy ranks, starting with his promotion in that year to mate aboard the collier brig "Friendship". In 1755, within a month of being offered command of this vessel, he volunteered for service in the Royal Navy, when Britain was re-arming for what was to become the Seven Years' War. Despite the need to start back at the bottom of the naval hierarchy, Cook realised his career would advance more quickly in military service and entered the Navy at Wapping on 17 June 1755.
Cook married Elizabeth Batts (1742–1835), the daughter of Samuel Batts, keeper of the Bell Inn, Wapping and one of his mentors, on 21 December 1762 at St Margaret's Church in Barking, Essex. The couple had six children: James (1763–94), Nathaniel (1764–80, lost aboard which foundered with all hands in a hurricane in the West Indies), Elizabeth (1767–71), Joseph (1768–68), George (1772–72) and Hugh (1776–93), the last of whom died of scarlet fever while a student at Christ's College, Cambridge. When not at sea, Cook lived in the East End of London. He attended St Paul's Church, Shadwell, where his son James was baptised. Cook has no known direct descendants—all his recorded children either pre-deceased him or died without issue.
Start of Royal Navy career.
Cook's first posting was with , serving as able seaman and master's mate under Captain Joseph Hamar for his first year aboard, and Captain Hugh Palliser thereafter. In October and November 1755 he took part in "Eagle"'s capture of one French warship and the sinking of another, following which he was promoted to boatswain in addition to his other duties. His first temporary command was in March 1756 when he was briefly master of the "Cruizer", a small cutter attached to the "Eagle" while on patrol.
In June 1757 Cook passed his master's examinations at Trinity House, Deptford, which qualified him to navigate and handle a ship of the King's fleet. He then joined the frigate as master under Captain Robert Craig.
Conquest of Canada (1758–63).
During the Seven Years' War, Cook served in North America as master of . In 1758 he took part in the major amphibious assault that captured the Fortress of Louisbourg from the French, after which he participated in the siege of Quebec City and then the Battle of the Plains of Abraham in 1759. He showed a talent for surveying and cartography, and was responsible for mapping much of the entrance to the Saint Lawrence River during the siege, thus allowing General Wolfe to make his famous stealth attack on the Plains of Abraham.
Cook's surveying ability was put to good use mapping the jagged coast of Newfoundland in the 1760s, aboard . He surveyed the north-west stretch in 1763 and 1764, the south coast between the Burin Peninsula and Cape Ray in 1765 and 1766, and the west coast in 1767. At this time Cook employed local pilots to point out the "rocks and hidden dangers" along the south and west coasts. During the 1765 season, four pilots were engaged at a daily pay of 4 shillings each: John Beck for the coast west of "Great St Lawrence", Morgan Snook for Fortune Bay, John Dawson for Connaigre and Hermitage Bay, and John Peck for the "Bay of Despair."
His five seasons in Newfoundland produced the first large-scale and accurate maps of the island's coasts and were the first scientific, large scale, hydrographic surveys to use precise triangulation to establish land outlines. They also gave Cook his mastery of practical surveying, achieved under often adverse conditions, and brought him to the attention of the Admiralty and Royal Society at a crucial moment both in his career and in the direction of British overseas discovery. Cook's map would be used into the 20th century—copies of it being referenced by those sailing Newfoundland's waters for 200 years.
Following on from his exertions in Newfoundland, it was at this time that Cook wrote that he intended to go not only ""farther than any man has been before me, but as far as I think it is possible for a man to go.""
Voyages of exploration.
First voyage (1768–71).
In 1766, Admiralty engaged Cook to command a scientific voyage to the Pacific Ocean. The purpose of the voyage was to observe and record the transit of Venus across the Sun for the benefit of Royal Society inquiry into a means of determining longitude. Cook, at the age of 39, was promoted to lieutenant to grant him sufficient status to take the command. For its part the Royal Society agreed that Cook would receive a one hundred guinea gratuity in addition to his Naval pay.
The expedition sailed from England on 26 August 1768, rounded Cape Horn and continued westward across the Pacific to arrive at Tahiti on 13 April 1769, where the observations of the Venus Transit were made. However, the result of the observations was not as conclusive or accurate as had been hoped. Once the observations were completed, Cook opened the sealed orders which were additional instructions from the Admiralty for the second part of his voyage: to search the south Pacific for signs of the postulated rich southern continent of "Terra Australis".
Cook then sailed to New Zealand and mapped the complete coastline, making only some minor errors. He then voyaged west, reaching the south-eastern coast of Australia on 19 April 1770, and in doing so his expedition became the first recorded Europeans to have encountered its eastern coastline.
On 23 April he made his first recorded direct observation of indigenous Australians at Brush Island near Bawley Point, noting in his journal: "…and were so near the Shore as to distinguish several people upon the Sea beach they appear'd to be of a very dark or black Colour but whether this was the real colour of their skins or the Cothes they might have on I know not." On 29 April Cook and crew made their first landfall on the mainland of the continent at a place now known as the Kurnell Peninsula. Cook originally christened the area as "Stingray Bay", but he later crossed it out and named it "Botany Bay" after the unique specimens retrieved by the botanists Joseph Banks and Daniel Solander. It is here that James Cook made first contact with an aboriginal tribe known as the Gweagal.
After his departure from Botany Bay he continued northwards. On 11 June a mishap occurred when ran aground on a shoal of the Great Barrier Reef, and then "nursed into a river mouth on 18 June 1770". The ship was badly damaged and his voyage was delayed almost seven weeks while repairs were carried out on the beach (near the docks of modern Cooktown, Queensland, at the mouth of the Endeavour River). The voyage then continued, sailing through Torres Strait and on 22 August Cook landed on Possession Island, where he claimed the entire coastline that he had just explored as British territory. He returned to England via Batavia (modern Jakarta, Indonesia where many in his crew succumbed to malaria), the Cape of Good Hope, and arriving on the island of Saint Helena on 12 July 1771.
Interlude.
Cook's journals were published upon his return, and he became something of a hero among the scientific community. Among the general public, however, the aristocratic botanist Joseph Banks was a greater hero. Banks even attempted to take command of Cook's second voyage, but removed himself from the voyage before it began, and Johann Reinhold Forster and his son Georg Forster were taken on as scientists for the voyage. Cook's son George was born five days before he left for his second voyage.
Second voyage (1772–75).
Shortly after his return from the first voyage, Cook was promoted in August 1771, to the rank of commander. In 1772 he was commissioned to lead another scientific expedition on behalf of the Royal Society, to search for the hypothetical Terra Australis. On his first voyage, Cook had demonstrated by circumnavigating New Zealand that it was not attached to a larger landmass to the south. Although he charted almost the entire eastern coastline of Australia, showing it to be continental in size, the Terra Australis was believed to lie further south. Despite this evidence to the contrary, Alexander Dalrymple and others of the Royal Society still believed that a massive southern continent should exist.
Cook commanded on this voyage, while Tobias Furneaux commanded its companion ship, . Cook's expedition circumnavigated the globe at an extreme southern latitude, becoming one of the first to cross the Antarctic Circle (17 January 1773). In the Antarctic fog, "Resolution" and "Adventure" became separated. Furneaux made his way to New Zealand, where he lost some of his men during an encounter with Māori, and eventually sailed back to Britain, while Cook continued to explore the Antarctic, reaching 71°10'S on 31 January 1774.
Cook almost encountered the mainland of Antarctica, but turned towards Tahiti to resupply his ship. He then resumed his southward course in a second fruitless attempt to find the supposed continent. On this leg of the voyage he brought a young Tahitian named Omai, who proved to be somewhat less knowledgeable about the Pacific than Tupaia had been on the first voyage. On his return voyage to New Zealand in 1774, Cook landed at the Friendly Islands, Easter Island, Norfolk Island, New Caledonia, and Vanuatu.
Before returning to England, Cook made a final sweep across the South Atlantic from Cape Horn and surveyed, mapped and took possession for Britain of South Georgia, which had been explored by Anthony de la Roché in 1675. Cook also discovered and named Clerke Rocks and the South Sandwich Islands ("Sandwich Land"). He then turned north to South Africa, and from there continued back to England. His reports upon his return home put to rest the popular myth of Terra Australis.
Cook's second voyage marked a successful employment of Larcum Kendall's K1 copy of John Harrison's H4 marine chronometer, which enabled Cook to calculate his longitudinal position with much greater accuracy. Cook's log was full of praise for this time-piece which he used to make charts of the southern Pacific Ocean that were so remarkably accurate that copies of them were still in use in the mid-20th century.
Upon his return, Cook was promoted to the rank of post-captain and given an honorary retirement from the Royal Navy, with a posting as an officer of the Greenwich Hospital. He reluctantly accepted, insisting that he be allowed to quit the post if an opportunity for active duty should arise. His fame now extended beyond the Admiralty; he was made a Fellow of the Royal Society, and awarded the Copley Gold Medal for completing his second voyage without losing a man to scurvy. Nathaniel Dance-Holland painted his portrait; he dined with James Boswell; he was described in the House of Lords as "the first navigator in Europe". But he could not be kept away from the sea. A third voyage was planned and Cook volunteered to find the Northwest Passage. He travelled to the Pacific and hoped to travel east to the Atlantic, while a simultaneous voyage travelled the opposite route.
Third voyage (1776–79).
On his last voyage, Cook again commanded HMS "Resolution", while Captain Charles Clerke commanded . The voyage was ostensibly planned to return the Pacific Islander, Omai to Tahiti, or so the public were led to believe. The trip's principal goal was to locate a Northwest Passage around the American continent. After dropping Omai at Tahiti, Cook travelled north and in 1778 became the first European to begin formal contact with the Hawaiian Islands. After his initial landfall in January 1778 at Waimea harbour, Kauai, Cook named the archipelago the "Sandwich Islands" after the fourth Earl of Sandwich—the acting First Lord of the Admiralty.
From the Sandwich Islands Cook sailed north and then north-east to explore the west coast of North America north of the Spanish settlements in Alta California. He made landfall on the Oregon coast at approximately 44°30′ north latitude, naming his landing point Cape Foulweather. Bad weather forced his ships south to about 43° north before they could begin their exploration of the coast northward. He unknowingly sailed past the Strait of Juan de Fuca, and soon after entered Nootka Sound on Vancouver Island. He anchored near the First Nations village of Yuquot. Cook's two ships remained in Nootka Sound from 29 March to 26 April 1778, in what Cook called Ship Cove, now Resolution Cove, at the south end of Bligh Island, about east across Nootka Sound from Yuquot, lay a Nuu-chah-nulth village (whose chief Cook did not identify but may have been Maquinna). Relations between Cook's crew and the people of Yuquot were cordial if sometimes strained. In trading, the people of Yuquot demanded much more valuable items than the usual trinkets that had worked in Hawaii. Metal objects were much desired, but the lead, pewter, and tin traded at first soon fell into disrepute. The most valuable items which the British received in trade were sea otter pelts. During the stay, the Yuquot "hosts" essentially controlled the trade with the British vessels; the natives usually visited the British vessels at Resolution Cove instead of the British visiting the village of Yuquot at Friendly Cove.
After leaving Nootka Sound, Cook explored and mapped the coast all the way to the Bering Strait, on the way identifying what came to be known as Cook Inlet in Alaska. In a single visit, Cook charted the majority of the North American north-west coastline on world maps for the first time, determined the extent of Alaska, and closed the gaps in Russian (from the West) and Spanish (from the South) exploratory probes of the Northern limits of the Pacific.
By the second week of August 1778 Cook was through the Bering Strait, sailing into the Chukchi Sea. He headed north-east up the coast of Alaska until he was blocked by sea ice. His furthest north was 70 degrees 44 minutes. Cook then sailed west to the Siberian coast, and then south-east down the Siberian coast back to the Bering Strait. By early September 1778 he was back in the Bering Sea to begin the trip to the Sandwich (Hawaiian) Islands. He became increasingly frustrated on this voyage, and perhaps began to suffer from a stomach ailment; it has been speculated that this led to irrational behaviour towards his crew, such as forcing them to eat walrus meat, which they had pronounced inedible.
Return to Hawaii.
Cook returned to Hawaii in 1779. After sailing around the archipelago for some eight weeks, he made landfall at Kealakekua Bay, on 'Hawaii Island', largest island in the Hawaiian Archipelago. Cook's arrival coincided with the "Makahiki", a Hawaiian harvest festival of worship for the Polynesian god Lono. Coincidentally the form of Cook's ship, HMS "Resolution", or more particularly the mast formation, sails and rigging, resembled certain significant artefacts that formed part of the season of worship. Similarly, Cook's clockwise route around the island of Hawaii before making landfall resembled the processions that took place in a clockwise direction around the island during the Lono festivals. It has been argued (most extensively by Marshall Sahlins) that such coincidences were the reasons for Cook's (and to a limited extent, his crew's) initial deification by some Hawaiians who treated Cook as an incarnation of Lono. Though this view was first suggested by members of Cook's expedition, the idea that any Hawaiians understood Cook to be Lono, and the evidence presented in support of it, were challenged in 1992.
Death.
After a month's stay, Cook attempted to resume his exploration of the Northern Pacific. Shortly after leaving Hawaii Island, however, the "Resolution"'s foremast broke, so the ships returned to Kealakekua Bay for repairs.
Tensions rose, and a number of quarrels broke out between the Europeans and Hawaiians at Kealakekua Bay. An unknown group of Hawaiians took one of Cook's small boats. The evening when the cutter was taken, the people had become "insolent" even with threats to fire upon them. Cook was forced into a wild goose chase that ended with his return to the ship frustrated. He attempted to kidnap and ransom the King of Hawaiʻi, Kalaniʻōpuʻu.
That following day, 14 February 1779, Cook marched through the village to retrieve the King. Cook took the King (aliʻi nui) by his own hand and led him willingly away. One of Kalaniʻōpuʻu's favorite wives, Kanekapolei and two chiefs approached the group as they were heading to boats. They pleaded with the king not to go until he stopped and sat where he stood. An old Kahuna (priest), chanting rapidly while holding out a coconut, attempted to distract Cook and his men as a large crowd began to form at the shore. The king began to understand that Cook was his enemy. As Cook turned his back to help launch the boats, he was struck on the head by the villagers and then stabbed to death as he fell on his face in the surf. He was first struck on the head with a club by a chief named Kalaimanokahoʻowaha or Kanaʻina (namesake of Charles Kana'ina) and then stabbed by one of the king's attendants, Nuaa. The Hawaiians carried his body away towards the back of the town, still visible to the ship through their spyglass. Four marines, Corporal James Thomas, Private Theophilus Hinks, Private Thomas Fatchett and Private John Allen, were also killed and two others were wounded in the confrontation.
Aftermath.
The esteem which the islanders nevertheless held for Cook caused them to retain his body. Following their practice of the time, they prepared his body with funerary rituals usually reserved for the chiefs and highest elders of the society. The body was disembowelled, baked to facilitate removal of the flesh, and the bones were carefully cleaned for preservation as religious icons in a fashion somewhat reminiscent of the treatment of European saints in the Middle Ages. Some of Cook's remains, thus preserved, were eventually returned to his crew for a formal burial at sea.
Clerke assumed leadership of the expedition, and made a final attempt to pass through the Bering Strait. Following the death of Clerke, "Resolution" and "Discovery" returned home in October 1780 commanded by John Gore, a veteran of Cook's first voyage, and Captain James King. After their arrival in England, King completed Cook's account of the voyage.
David Samwell, who sailed with Cook on "Resolution", wrote of him: "He was a modest man, and rather bashful; of an agreeable lively conversation, sensible and intelligent. In temper he was somewhat hasty, but of a disposition the most friendly, benevolent and humane. His person was above six feet high: and, though a good looking man, he was plain both in dress and appearance. His face was full of expression: his nose extremely well shaped: his eyes which were small and of a brown cast, were quick and piercing; his eyebrows prominent, which gave his countenance altogether an air of austerity."
Legacy.
Ethnographic collections.
The Australian Museum acquired its Cook Collection in 1894 from the Government of New South Wales. At that time the collection consisted of 115 artefacts collected on Cook's three voyages throughout the Pacific Ocean, during the period 1768–1780, along with documents and memorabilia related to these voyages. Many of the ethnographic artifacts were collected at a time of first contact between Pacific Peoples and Europeans. In 1935 most of the documents and memorabilia were transferred to the Mitchell Library in the State Library of New South Wales. The provenance of the collection shows that the objects remained in the hands of Cook's widow Elizabeth Cook, and her descendants, until 1886. In this year John Mackrell, the great-nephew of Isaac Smith, Elizabeth Cook's cousin, organised the display of this collection at the request of the NSW Government at the Colonial and Indian Exhibition in London. In 1887 the London-based Agent-General for the New South Wales Government, Saul Samuel, bought John Mackrell's items and also acquired items belonging to the other relatives Reverend Canon Frederick Bennett, Mrs Thomas Langton, H. M. C. Alexander, and William Adams. The collection remained with the Colonial Secretary of NSW until 1894, when it was transferred to the Australian Museum.
Navigation and science.
Cook's 12 years sailing around the Pacific Ocean contributed much to European knowledge of the area. Several islands such as Sandwich Islands (Hawaii) were encountered for the first time by Europeans, and his more accurate navigational charting of large areas of the Pacific was a major achievement.
To create accurate maps, latitude and longitude must be accurately determined. Navigators had been able to work out latitude accurately for centuries by measuring the angle of the sun or a star above the horizon with an instrument such as a backstaff or quadrant. Longitude was more difficult to measure accurately because it requires precise knowledge of the time difference between points on the surface of the earth. The Earth turns a full 360 degrees relative to the sun each day. Thus longitude corresponds to time: 15 degrees every hour, or 1 degree every 4 minutes.
Cook gathered accurate longitude measurements during his first voyage due to his navigational skills, the help of astronomer Charles Green and by using the newly published Nautical Almanac tables, via the lunar distance method—measuring the angular distance from the moon to either the sun during daytime or one of eight bright stars during night-time to determine the time at the Royal Observatory, Greenwich, and comparing that to his local time determined via the altitude of the sun, moon, or stars. On his second voyage Cook used the K1 chronometer made by Larcum Kendall, which was the shape of a large pocket watch, in diameter. It was a copy of the H4 clock made by John Harrison, which proved to be the first to keep accurate time at sea when used on the ship "Deptford's" journey to Jamaica, 1761–62.
Cook succeeded in circumnavigating the world on his first voyage without losing a single man to scurvy, an unusual accomplishment at the time. He tested several preventive measures but the most important was frequent replenishment of fresh food. It was for presenting a paper on this aspect of the voyage to the Royal Society that he was presented with the Copley Medal in 1776. Ever the observer, Cook was the first European to have extensive contact with various people of the Pacific. He correctly postulated a link among all the Pacific peoples, despite their being separated by great ocean stretches (see Malayo-Polynesian languages). Cook theorised that Polynesians originated from Asia, which scientist Bryan Sykes later verified. In New Zealand the coming of Cook is often used to signify the onset of colonisation.
Cook carried several scientists on his voyages; they made several significant observations and discoveries. Two botanists, Joseph Banks, and Swede Daniel Solander, were on the first Cook voyage. The two collected over 3,000 plant species. Banks subsequently strongly promoted British settlement of Australia.
Several artists also sailed on Cook's first voyage. Sydney Parkinson was heavily involved in documenting the botanists' findings, completing 264 drawings before his death near the end of the voyage. They were of immense scientific value to British botanists. Cook's second expedition included William Hodges, who produced notable landscape paintings of Tahiti, Easter Island, and other locations.
Several officers who served under Cook went on to distinctive accomplishments. William Bligh, Cook's sailing master, was given command of in 1787 to sail to Tahiti and return with breadfruit. Bligh is most known for the mutiny of his crew which resulted in his being set adrift in 1789. He later became governor of New South Wales, where he was subject of another mutiny—the only successful armed takeover of an Australian government. George Vancouver, one of Cook's midshipmen, later led a voyage of exploration to the Pacific Coast of North America from 1791 to 1794. In honour of his former commander, Vancouver's new ship was also christened . George Dixon sailed under Cook on his third expedition, and later commanded his own expedition. A lieutenant under Cook, Henry Roberts, spent many years after that voyage preparing the detailed charts that went into Cook's posthumous Atlas, published around 1784.
Cook's contributions to knowledge were internationally recognised during his lifetime. In 1779, while the American colonies were fighting Britain for their independence, Benjamin Franklin wrote to captains of colonial warships at sea, recommending that if they came into contact with Cook's vessel, they were to "not consider her an enemy, nor suffer any plunder to be made of the effects contained in her, nor obstruct her immediate return to England by detaining her or sending her into any other part of Europe or to America; but that you treat the said Captain Cook and his people with all civility and kindness, ... as common friends to mankind." Unknown to Franklin, Cook had met his death a month before this "passport" was written.
Cook's voyages were involved in another unusual first. The first recorded circumnavigation of the world by an animal was by Cook's goat, who made that memorable journey twice; the first time on HMS "Dolphin", under Samuel Wallis and then aboard "Endeavour". When they returned to England, Cook had the goat presented with a silver collar engraved with lines from Samuel Johnson: "Perpetui, ambita bis terra, praemia lactis Haec habet altrici Capra secunda Jovis.". She was put to pasture on Cook's farm outside London, and also was reportedly admitted to the privileges of the Royal Naval hospital at Greenwich. Cook's journal recorded the date of The Goat's death: 28 March 1772.
Memorials.
A US coin, the 1928 Hawaiian Sesquicentennial half dollar carries Cook's image. Minted for the 150th anniversary of his discovery of the islands, its low mintage (10,008) has made this example of Early United States commemorative coins both scarce and expensive. The site where he was killed in Hawaii was marked in 1874 by a white obelisk set on of chained-off beach. This land, although in Hawaii, was deeded to the United Kingdom. A nearby town is named Captain Cook, Hawaii; several Hawaiian businesses also carry his name. The Apollo 15 Command/Service Module "Endeavour" was named after Cook's ship, , as was the space shuttle . Another shuttle, "Discovery", was named after Cook's .
The first institution of higher education in North Queensland, Australia was named after him, with James Cook University opening in Townsville in 1970. In Australian rhyming slang the expression ""Captain Cook"" means ""look"". Numerous institutions, landmarks and place names reflect the importance of Cook's contributions, including the Cook Islands, the Cook Strait, Cook Inlet, and the Cook crater on the Moon. Aoraki/Mount Cook, the highest summit in New Zealand, is named for him. Another Mount Cook is on the border between the US state of Alaska and the Canadian Yukon Territory, and is designated Boundary Peak 182 as one of the official Boundary Peaks of the Hay–Herbert Treaty.
One of the earliest monuments to Cook in the United Kingdom is located at The Vache, erected in 1780 by Admiral Hugh Palliser, a contemporary of Cook and one-time owner of the estate. A huge obelisk was built in 1827 as a monument to Cook on Easby Moor overlooking his boyhood village of Great Ayton, along with a smaller monument at the former location of Cook's cottage. There is also a monument to Cook in the church of St Andrew the Great, St Andrew's Street, Cambridge, where his son Hugh, a student at Christ's College, was buried. Cook's widow Elizabeth was also buried in the church and in her will left money for the memorial's upkeep. The 250th anniversary of Cook's birth was marked at the site of his birthplace in Marton, by the opening of the Captain Cook Birthplace Museum, located within Stewart Park (1978). A granite vase just to the south of the museum marks the approximate spot where he was born. Tributes also abound in post-industrial Middlesbrough, including a primary school, shopping square and the "Bottle 'O Notes", a public artwork by Claes Oldenburg, that was erected in the town's Central Gardens in 1993. Also named after Cook is the James Cook University Hospital, a major teaching hospital which opened in 2003.
The Royal Research Ship RRS "James Cook" was built in 2006 to replace the RRS "Charles Darwin" in the UK's Royal Research Fleet, and Stepney Historical Trust placed a plaque on Free Trade Wharf in the Highway, Shadwell to commemorate his life in the East End of London. In 2002 Cook was placed at number 12 in the BBC's poll of the 100 Greatest Britons.
Popular culture.
The death of Cook was interesting to artists and writers of the time because its brutality was a contrast to the idea that primitive humans were naturally good-natured – the "noble savage". There were no eye-witness accounts of the death and so the incident was open to artistic interpretation and myth-making.

</doc>
<doc id="15632" url="https://en.wikipedia.org/wiki?curid=15632" title="John Baskerville">
John Baskerville

John Baskerville (28 January 1706 – 8 January 1775) was an English businessman, in areas including japanning and papier-mâché, but he is best remembered as a printer and type designer.
Life.
Baskerville was born in the village of Wolverley, near Kidderminster in Worcestershire and was a printer in Birmingham, England. He was a member of the Royal Society of Arts, and an associate of some of the members of the Lunar Society. He directed his punchcutter, John Handy, in the design of many typefaces of broadly similar appearance. In 1757, Baskerville published a remarkable quarto edition of Virgil on wove paper, using his own type. It took three years to complete, but it made such an impact that he was appointed printer to the University of Cambridge the following year.
John Baskerville printed works for the University of Cambridge in 1758 and, although an atheist, printed a splendid folio Bible in 1763. His typefaces were greatly admired by Benjamin Franklin, a printer and fellow member of the Royal Society of Arts, who took the designs back to the newly created United States, where they were adopted for most federal government publishing. Baskerville's work was criticised by jealous competitors and soon fell out of favour, but since the 1920s many new fonts have been released by Linotype, Monotype, and other type foundries – revivals of his work and mostly called 'Baskerville'. Emigre released a popular revival of this typeface in 1996 called Mrs Eaves, named for Baskerville's wife, Sarah Eaves. Baskerville's most notable typeface Baskerville represents the peak of transitional type face and bridges the gap between Old Style and Modern type design.
Baskerville also was responsible for significant innovations in printing, paper and ink production. He developed a technique which produced a smoother whiter paper which showcased his strong black type. Baskerville also pioneered a completely new style of typography adding wide margins and leading between each line.
Death and interments.
Baskerville died in January 1775 at his home, "Easy Hill". He requested that his body be placed
However, in 1821 a canal was built through the land and his body was placed on show by the landowner until Baskerville's family and friends arranged to have it moved to the crypt of Christ Church, Birmingham. Christ Church was demolished in 1897 so his remains were then moved, with other bodies from the crypt, to consecrated catacombs at Warstone Lane Cemetery. In 1963 a petition was presented to Bimingham City Council requesting that he be reburied in unconsecrated ground according to his wishes.
Baskerville House was built on the grounds of "Easy Hill".
Commemoration.
A Portland stone sculpture of the Baskerville typeface, "Industry and Genius", in his honour stands in front of Baskerville House in Centenary Square, Birmingham. It was created by local artist David Patten.
Gallery.
Some examples of volumes published by Baskerville.
External links.
Death:

</doc>
<doc id="15640" url="https://en.wikipedia.org/wiki?curid=15640" title="John Young">
John Young

John Young may refer to:

</doc>
<doc id="15641" url="https://en.wikipedia.org/wiki?curid=15641" title="Joseph Stalin">
Joseph Stalin

Joseph Vissarionovich Stalin (; birth surname: Jughashvili; 18 December 1878 – 5 March 1953) was the leader of the Soviet Union from the mid-1920s until his death in 1953. Holding the post of the General Secretary of the Central Committee of the Communist Party of the Soviet Union, he was effectively the dictator of the state.
Stalin was one of the seven members of the first Politburo, founded in 1917 in order to manage the Bolshevik Revolution, alongside Lenin, Zinoviev, Kamenev, Trotsky, Sokolnikov and Bubnov. Among the Bolshevik revolutionaries who took part in the Russian Revolution of 1917, Stalin was appointed General Secretary of the party's Central Committee in 1922. He subsequently managed to consolidate power following the 1924 death of Vladimir Lenin by suppressing Lenin's criticisms (in the postscript of his testament) and expanding the functions of his role, all the while eliminating any opposition. He remained General Secretary until the post was abolished in 1952, concurrently serving as the Premier of the Soviet Union from 1941 onward.
Under Stalin's rule, the concept of "Socialism in One Country" became a central tenet of Soviet society, contrary to Leon Trotsky's view that socialism must be spread through continuous international revolutions. He replaced the New Economic Policy introduced by Lenin in the early 1920s with a highly centralised command economy, launching a period of industrialization and collectivization that resulted in the rapid transformation of the USSR from an agrarian society into an industrial power. However, the economic changes coincided with the imprisonment of millions of people in Gulag labour camps. The initial upheaval in agriculture disrupted food production and contributed to the catastrophic Soviet famine of 1932–33, known in Ukraine as the Holodomor. Between 1934 and 1939 he organized and led a massive purge (known as "Great Purge") of the party, government, armed forces and intelligentsia, in which millions of so-called "enemies of the working class" were imprisoned, exiled or executed, often without due process. Major figures in the Communist Party and government, and many Red Army high commanders, were killed after being convicted of treason in show trials.
In August 1939, after failed attempts to conclude anti-Hitler pacts with other major European powers, Stalin entered into a non-aggression pact with Nazi Germany that divided their influence and territory within Eastern Europe, resulting in their invasion of Poland in September of that year, but Germany later violated the agreement and launched a massive invasion of the Soviet Union in June 1941. Despite heavy human and territorial losses, Soviet forces managed to halt the Nazi incursion after the decisive Battles of Moscow and Stalingrad. After defeating the Axis powers on the Eastern Front, the Red Army captured Berlin in May 1945, effectively ending the war in Europe for the Allies. The Soviet Union subsequently emerged as one of two recognized world superpowers, the other being the United States. Communist governments loyal to the Soviet Union were established in most countries freed from German occupation by the Red Army, which later constituted the Eastern Bloc. Stalin also had close relations with Mao Zedong in China and Kim Il-sung in North Korea.
Stalin led the Soviet Union through its post-war reconstruction phase, which saw a significant rise in tension with the Western world that would later be known as the Cold War. During this period, the USSR became the second country in the world to successfully develop a nuclear weapon, as well as launching the Great Plan for the Transformation of Nature in response to another widespread famine and the Great Construction Projects of Communism. In the years following his death, Stalin and his regime have been condemned on numerous occasions, most notably in 1956 when his successor Nikita Khrushchev denounced his legacy and initiated a process of de-Stalinization. Stalin remains a controversial figure today, with many regarding him as a tyrant. However, popular opinion within the Russian Federation is mixed. The exact number of deaths caused by Stalin's regime is still a subject of debate, but it is widely agreed to be in the order of millions.
Early life.
Stalin's birth name in Georgian was Ioseb Besarionis dze Jughashvili (). He was born an ethnic Georgian; Georgia was then part of the Russian Empire. The Russian-language version of his birth name was Iosif Vissarionovich Dzhugashvili ().
Ioseb was born on 18 December 1878 in the town of Gori in the Tiflis Governorate of the Russian Empire (today in Georgia). His father was Besarion Jughashvili, a cobbler, while his mother was Ketevan Geladze, a housemaid. As a child, Ioseb was plagued with numerous health issues. He was born with two adjoined toes on his left foot, and his face was permanently scarred by smallpox at the age of 7. At age 12, he injured his left arm in an accident involving a horse-drawn carriage, rendering it shorter and stiffer than its counterpart.
Ioseb's father slid into alcoholism, which made him abusive to his family and caused his business to fail. When Ioseb's mother enrolled him into a Greek Orthodox priesthood school against her husband's wishes, Ioseb's enraged father went on a drunken rampage. He was banished from Gori after assaulting the police chief. Besarion moved to Tiflis, leaving his wife and son behind in Gori.
When Ioseb was sixteen, he received a scholarship to attend the Tiflis Spiritual Seminary, the leading Russian Orthodox seminary in Tiflis; the language of instruction was Russian. Despite being trained as a priest, he became an atheist in his first year. He was a voracious reader and became a Georgian cultural nationalist. He anonymously published poetry in Georgian in the local press and engaged in student politics. Although his performance had been good, he was expelled in 1899 after missing his final exams. The seminary's records also suggest that he was unable to pay his tuition fees. Around this time, Ioseb discovered the writings of Vladimir Lenin and joined the Russian Social-Democratic Labour Party, a Marxist group.
Out of school, Jughashvili briefly worked as a part-time clerk in a meteorological office, but after a state crackdown on revolutionaries, he went underground and became a full-time revolutionary, living off donations.
When Lenin formed the Bolsheviks, Jughashvili eagerly joined him. Jughashvili proved to be a very effective organizer of men as well as a capable intellectual. Among other activities, he wrote and distributed propaganda, organized strikes, and raised funds through bank robberies, kidnappings, extortion, and assassinations. Jughashvili was arrested and exiled to Siberia numerous times, but often escaped. His skill, charm, and street-smarts won him the respect of Lenin, and he rose rapidly through the ranks of the Bolsheviks.
Jughashvili married his first wife, Ekaterina Svanidze, in 1906, who bore him a son. She died the following year of typhus. In 1911, he met his future second wife, Nadezhda Alliluyeva, during one of his many exiles in Siberia.
Sometime between 1910 and 1912, he began using the alias "Stalin" in his writings.
Revolution and war.
World War I.
During WWI, upon his exile in Siberia, Stalin was drafted into the Russian Army but his damaged left arm disqualified him for service. He had to travel to Achinsk, only 100 km from the Trans-Siberian Railway for his medical exam, and was allowed to stay there after the army rejected him.
Russian Revolution of 1917.
After returning to Petrograd from his final exile, Stalin ousted Vyacheslav Molotov and Alexander Shlyapnikov as editors of "Pravda". He then took a position in favor of supporting Alexander Kerensky's provisional government. However, after Lenin prevailed at the April 1917 Communist Party conference, Stalin and "Pravda" shifted to opposing the provisional government. At this conference, Stalin was elected to the Bolshevik Central Committee. In October 1917, the Bolshevik Central Committee voted in favor of an insurrection. On 7 November, from the Smolny Institute, Leon Trotsky, Lenin and the rest of the Central Committee coordinated the insurrection against Kerensky in the 1917 October Revolution. By 8 November, the Bolsheviks had stormed the Winter Palace and Kerensky's Cabinet had been arrested.
Russian Civil War, 1917–1919.
Upon the October Revolution, Stalin was appointed People's Commissar for Nationalities' Affairs. Thereafter, civil war broke out in Russia, pitting Lenin's Red Army against the White Army, a loose alliance of anti-Bolshevik forces. Lenin formed a five-member Politburo, which included Stalin and Trotsky. In May 1918, Lenin dispatched Stalin to the city of Tsaritsyn. Through his new allies, Kliment Voroshilov and Semyon Budyonny, Stalin imposed his influence on the military.
Stalin challenged many of the decisions of Trotsky, ordered the killings of many counter-revolutionaries and former Tsarist officers in the Red Army and burned villages in order to intimidate the peasantry into submission and discourage bandit raids on food shipments. In May 1919, in order to stem mass desertions on the Western front, Stalin had deserters and renegades publicly executed as traitors.
Polish–Soviet War, 1919–21.
As Bolshevik victories in the Russian Civil War of 1917–1922 established the Bolshevik position more securely, Soviet Russia started a push towards world revolution, which formed part of the communist ideology to transform the whole world into socialist states. (Tukhachevsky: There can be no doubt that if we had been victorious on the Vistula (i.e. in Poland), the revolutionary fires would have reached the entire continent.). Looking toward Western Europe, the Bolsheviks encountered the newly reborn independent — and expansionist-minded — state of Poland. Conflicts began in what became known as the Polish–Soviet War of 1919–1921. After the Polish Army achieved initial successes, the Bolsheviks pushed the Polish forces back into central Poland in the summer of 1920. As the people's commissar to the high command of the southern front, Stalin was determined to take the then Polish city of Lwów (now Lviv in Ukraine). This conflicted with the general strategy set by Lenin and Trotsky, which focused on the capture of Warsaw further north.
Tukhachevsky's forces engaged those of Polish commanders Józef Piłsudski and Władysław Sikorski at the pivotal Battle of Warsaw (12–25 August 1920), but Stalin refused to redirect his troops from Lwów to help Tukhachevsky. Consequently, the Poles totally routed the four invading armies of Soviet Russia fighting for the Polish capital. The Bolsheviks lost the battles for both Lwów and Warsaw, and Stalin was blamed. In August 1920 Stalin returned to Moscow, where he defended himself and resigned his military command. At the Ninth Party Conference of March–April 1920, on 22 September 1920, Trotsky openly criticized Stalin's behavior.
Rise to power.
Stalin played a decisive role in the 1921 Red Army invasion of Georgia, after which he adopted particularly hardline, centralist policies towards Soviet Georgia. This led to the Georgian Affair of 1922 and other repressions. Stalin's actions in Georgia created a rift with Lenin, who believed that all the Soviet states should stand equal.
Lenin nonetheless considered Stalin a loyal ally, and when he got mired in squabbles with Trotsky and other politicians, he decided to support Stalin. With the help of Lev Kamenev, Lenin appointed Stalin General Secretary in 1922. This post enabled Stalin to appoint many of his allies to government positions.
Lenin suffered strokes in May and December 1922, forcing him into semi-retirement in Gorki. Stalin visited him often, acting as his intermediary with the outside world, but the pair quarreled and their relationship deteriorated. Lenin dictated increasingly disparaging notes on Stalin in what would become his testament. He criticized Stalin's political views, rude manners, and excessive power and ambition, and suggested that Stalin should be removed from the position of general secretary. During Lenin's semi-retirement, Stalin forged an alliance with Kamenev and Grigory Zinoviev against Trotsky. These allies prevented "Lenin's Testament" from being revealed to the Twelfth Party Congress in April 1923 (after Lenin's death the testament was read to selected groups of deputies to the Thirteenth Party Congress in May 1924 but it was forbidden to be mentioned at the plenary assemblies or any documents of the Congress).
Lenin died of a stroke on 21 January 1924. Following Lenin's death, a power struggle began, which involved the following seven Politburo members: Nikolai Bukharin, Lev Kamenev, Alexei Rykov, Joseph Stalin, Mikhail Tomsky, Leon Trotsky, and Grigory Zinoviev.
Again, Kamenev and Zinoviev helped to keep Lenin's Testament from going public. Thereafter, Stalin's disputes with Kamenev and Zinoviev intensified. Trotsky, Kamenev and Zinoviev grew increasingly isolated, and were eventually ejected from the Central Committee and then from the Party itself. Kamenev and Zinoviev were later readmitted, but Trotsky was exiled from the Soviet Union.
The Northern Expedition in China became a point of contention over foreign policy by Stalin and Trotsky. Stalin wanted the Communist Party of China to ally itself with the Nationalist Kuomintang, rather than attempt to implement a communist revolution. Trotsky urged the party to oppose the Kuomintang and launch a full-scale revolution. Stalin funded the KMT during the expedition. Stalin countered Trotsky's criticisms by making a secret speech in which he said that the Kuomintang were the only ones capable of defeating the imperialists, that Chiang Kai-shek had funding from the rich merchants, and that his forces were to be utilized until squeezed for all usefulness like a lemon before being discarded. However, Chiang quickly reversed the tables in the Shanghai massacre of 1927 by massacring the membership of the Communist party in Shanghai midway through the Northern Expedition.
Stalin pushed for more rapid industrialization and central control of the economy, contravening Lenin's New Economic Policy (NEP). At the end of 1927, a critical shortfall in grain supplies prompted Stalin to push for the collectivisation of agriculture and order the seizure of grain hoards from kulak farmers. Nikolai Bukharin and Premier Alexey Rykov opposed these policies and advocated a return to the NEP, but the rest of the Politburo sided with Stalin and removed Bukharin from the Politburo in November 1929. Rykov was fired the following year and was replaced by Vyacheslav Molotov on Stalin's recommendation.
In December 1934, the popular Communist Party boss in Leningrad, Sergei Kirov, was murdered. Stalin blamed Kirov's murder on a vast conspiracy of saboteurs and Trotskyites. He launched a massive purge against these internal enemies, putting them on rigged show trials and then having them executed or imprisoned in Siberian Gulags. Among these victims were old enemies, including Bukharin, Rykov, Kamenev and Zinoviev. Stalin made the loyal Nikolai Yezhov head of the secret police, the NKVD, and had him purge the NKVD of veteran Bolsheviks. With no serious opponents left in power, Stalin ended the purges in 1938. Yezhov was held to blame for the excesses of the Great Terror. He was dismissed from office and later executed.
Changes to Soviet society, 1927–1939.
Bolstering Soviet secret service and intelligence.
Stalin vastly increased the scope and power of the state's secret police and intelligence agencies. Under his guiding hand, Soviet intelligence forces began to set up intelligence networks in most of the major nations of the world, including Germany (the famous "Rote Kappelle" spy ring), Great Britain, France, Japan, and the United States. Stalin made considerable use of the Communist International movement in order to and to ensure that foreign Communist parties remained pro-Soviet and pro-Stalin.
One of the best examples of Stalin's ability to integrate secret police and foreign espionage came in 1940, when he gave approval to the secret police to have Leon Trotsky assassinated in Mexico.
Cult of personality.
A cult of personality developed in the Soviet Union around both Stalin and Lenin. Many personality cults in history have been frequently measured and compared to his. Numerous towns, villages and cities were renamed after the Soviet leader (see List of places named after Stalin) and the Stalin Prize and Stalin Peace Prize were named in his honor. He accepted grandiloquent titles (e.g., "Coryphaeus of Science," "Father of Nations," "Brilliant Genius of Humanity," "Great Architect of Communism," "Gardener of Human Happiness," and others), and helped rewrite Soviet history to provide himself a more significant role in the revolution of 1917. At the same time, according to Nikita Khrushchev, he insisted that he be remembered for "the extraordinary modesty characteristic of truly great people." Although statues of Stalin depict him at a height and build approximating the very tall Tsar Alexander III, sources suggest he was approximately 5 ft 4 in (163 cm).
Trotsky criticized the cult of personality built around Stalin. It reached new levels during World War II, with Stalin's name included in the new Soviet national anthem. Stalin became the focus of literature, poetry, music, paintings and film that exhibited fawning devotion. He was sometimes credited with almost god-like qualities, including the suggestion that he single-handedly won the Second World War. The degree to which Stalin himself relished the cult surrounding him is debatable. The Finnish communist Arvo Tuominen records a sarcastic toast proposed by Stalin at a New Year's Party in 1935 in which he said "Comrades! I want to propose a toast to our Patriarch, life and sun, liberator of nations, architect of socialism rattled off all the appellations applied to him in those days – Josef Vissarionovich Stalin, and I hope this is the first and last speech made to that genius this evening."
In a 1956 speech, Nikita Khrushchev denounced Stalin's cult of personality with these words: "It is impermissible and foreign to the spirit of Marxism-Leninism to elevate one person, to transform him into a superman possessing supernatural characteristics akin to those of a god." Khrushchev's speech and especially the confirmation reflected in the decisions of the 22nd Congress of the Communist Party of the Soviet Union in 1961 led to the destruction of thousands of Stalin monuments not only in the Soviet Union but in many other Socialist countries in the following years. In November 1961, for example, the large Stalin Statue on Berlin's monumental Stalinallee (promptly renamed Karl-Marx-Allee) was removed in a clandestine operation.
Purges and deportations.
Purges and executions.
Stalin, as head of the Politburo of the Central Committee of the Communist Party of the Soviet Union, consolidated near-absolute power in the 1930s with a Great Purge of the party that was justified as an attempt to expel "opportunists" and "counter-revolutionary infiltrators". Those targeted by the purge were often expelled from the party, however more severe measures ranged from banishment to the Gulag labor camps to execution after trials held by NKVD troikas.
In the 1930s, Stalin apparently became increasingly worried about the growing popularity of the Leningrad party boss Sergey Kirov. At the 1934 Party Congress where the vote for the new Central Committee was held, Kirov received only three negative votes, the fewest of any candidate, while Stalin received over one hundred negative votes. After the assassination of Kirov, which may have been orchestrated by Stalin, Stalin invented a detailed scheme to implicate opposition leaders in the murder, including Trotsky, Kamenev and Zinoviev. The investigations and trials expanded. Stalin passed a new law on "terrorist organizations and terrorist acts" that were to be investigated for no more than ten days, with no prosecution, defense attorneys or appeals, followed by a sentence to be executed "quickly."
Thereafter, several trials known as the Moscow Trials were held, but the procedures were replicated throughout the country. Article 58 of the legal code, which listed prohibited anti-Soviet activities as counterrevolutionary crime, was applied in the broadest manner. The flimsiest pretexts were often enough to brand someone an "enemy of the people", starting the cycle of public persecution and abuse, often proceeding to interrogation, torture and deportation, if not death. The Russian word troika gained a new meaning: a quick, simplified trial by a committee of three subordinated to NKVD—NKVD troika—with sentencing carried out within 24 hours. Stalin's hand-picked executioner, Vasily Blokhin, was entrusted with carrying out some of the high profile executions in this period.
Many military leaders were convicted of treason and a large-scale purge of Red Army officers followed. The repression of so many formerly high-ranking revolutionaries and party members led Leon Trotsky to claim that a "river of blood" separated Stalin's regime from that of Lenin. In August 1940, Trotsky was assassinated in Mexico, where he had lived in exile since January 1937; this eliminated the last of Stalin's opponents among the former Party leadership.
With the exception of Vladimir Milyutin (who died in prison in 1937) and Joseph Stalin himself, all of the members of Lenin's original cabinet who had not succumbed to death from natural causes before the purge were executed.
Mass operations of the NKVD also targeted "national contingents" (foreign ethnicities) such as Poles, ethnic Germans, Koreans, etc. A total of 350,000 (144,000 of them Poles) were arrested and 247,157 (110,000 Poles) were executed. Many Americans who had emigrated to the Soviet Union during the worst of the Great Depression were executed; others were sent to prison camps or gulags. Concurrent with the purges, efforts were made to rewrite the history in Soviet textbooks and other propaganda materials. Notable people executed by NKVD were removed from the texts and photographs as though they never existed. Gradually, the history of revolution was transformed to a story about just two key characters: Lenin and Stalin.
In light of revelations from Soviet archives, historians now estimate that nearly 700,000 people (353,074 in 1937 and 328,612 in 1938) were executed in the course of the terror, with the great mass of victims merely "ordinary" Soviet citizens: workers, peasants, homemakers, teachers, priests, musicians, soldiers, pensioners, ballerinas, and beggars. Many of the executed were interred in mass graves, with some of the major killing and burial sites being Bykivnia, Kurapaty and Butovo.
Some Western experts believe the evidence released from the Soviet archives is understated, incomplete or unreliable.
Stalin personally signed 357 proscription lists in 1937 and 1938 that condemned to execution some 40,000 people, and about 90% of these are confirmed to have been shot. At the time, while reviewing one such list, Stalin reportedly muttered to no one in particular: "Who's going to remember all this riff-raff in ten or twenty years' time? No one. Who remembers the names now of the boyars Ivan the Terrible got rid of? No one." In addition, Stalin dispatched a contingent of NKVD operatives to Mongolia, established a Mongolian version of the NKVD troika, and unleashed a bloody purge in which tens of thousands were executed as "Japanese Spies." Mongolian ruler Khorloogiin Choibalsan closely followed Stalin's lead.
During the 1930s and 1940s, the Soviet leadership sent NKVD squads into other countries to murder defectors and other opponents of the Soviet regime. Victims of such plots included Yevhen Konovalets, Ignace Poretsky, Rudolf Klement, Alexander Kutepov, Evgeny Miller, Leon Trotsky and the Workers' Party of Marxist Unification (POUM) leadership in Catalonia (e.g. Andreu Nin).
Deportations.
Shortly before, during and immediately after World War II, Stalin conducted a series of deportations on a huge scale that profoundly affected the ethnic map of the Soviet Union. It is estimated that between 1941 and 1949 nearly 3.3 million people were deported to Siberia and the Central Asian republics. By some estimates up to 43% of the resettled population died of diseases and malnutrition.
Separatism, resistance to Soviet rule and collaboration with the invading Germans were cited as the official reasons for the deportations, rightly or wrongly. Individual circumstances of those spending time in German-occupied territories were not examined. After the brief Nazi occupation of the Caucasus, the entire population of five of the small highland peoples and the Crimean Tatars – more than a million people in total – were deported without notice or any opportunity to take their possessions.
As a result of Stalin's lack of trust in the loyalty of particular ethnicities, ethnic groups such as the Soviet Koreans, the Volga Germans, the Crimean Tatars, the Chechens, and many Poles were forcibly moved out of strategic areas and relocated to places in the central Soviet Union, especially Kazakhstan in Soviet Central Asia. By some estimates, hundreds of thousands of deportees may have died en route.
According to official Soviet estimates, more than 14 million people passed through the "Gulag" from 1929 to 1953, with a further 7 to 8 million being deported and exiled to remote areas of the Soviet Union (including the entire nationalities in several cases).
In February 1956, Nikita Khrushchev condemned the deportations as a violation of Leninism, and reversed some of them, although it was not until 1991 that the Tatars, Meskhetians and Volga Germans were allowed to return "en masse" to their homelands. The memory of the deportations has played a major part in the separatist movements in the Baltic States, Tatarstan and Chechnya.
Collectivization.
Stalin's regime moved to force collectivization of agriculture. This was intended to increase agricultural output from large-scale mechanized farms, to bring the peasantry under more direct political control, and to make tax collection more efficient. Collectivization brought social change on a scale not seen since the abolition of serfdom in 1861 and alienation from control of the land and its produce. Collectivization also meant a drastic drop in living standards for many peasants, and it faced violent reaction among the peasantry.
In the first years of collectivization it was estimated that industrial production would rise by 200% and agricultural production by 50%, but these expectations were not realized. Stalin blamed this unanticipated failure on kulaks (rich peasants), who resisted collectivization. However, kulaks proper made up only 4% of the peasant population; the "kulaks" that Stalin targeted included the slightly better-off peasants who took the brunt of violence from the OGPU and the Komsomol. These peasants were about 60% of the population. Those officially defined as "kulaks", "kulak helpers", and, later, "ex-kulaks" were to be shot, placed into Gulag labor camps, or deported to remote areas of the country, depending on the charge. Archival data indicates that 20,201 people were executed during 1930, the year of Dekulakization.
The two-stage progress of collectivization—interrupted for a year by Stalin's famous editorials, "Dizzy with Success" and "Reply to Collective Farm Comrades"—is a prime example of his capacity for tactical political withdrawal followed by intensification of initial strategies.
Famines.
Famine affected Ukraine, southern Russia and other parts of the USSR. The death toll from famine in the Soviet Union at this time is estimated at between 5 and 10 million people. The worst crop failure of late tsarist Russia, in 1892, had caused 375,000 to 400,000 deaths. Most modern scholars agree that the famine was caused by the policies of the government of the Soviet Union under Stalin, rather than by natural reasons. According to Alan Bullock, "the total Soviet grain crop was no worse than that of 1931 ... it was not a crop failure but the excessive demands of the state, ruthlessly enforced, that cost the lives of as many as five million Ukrainian peasants." Stalin refused to release large grain reserves that could have alleviated the famine, while continuing to export grain; he was convinced that the Ukrainian peasants had hidden grain away and strictly enforced draconian new collective-farm theft laws in response. Other historians hold it was largely the insufficient harvests of 1931 and 1932 caused by a variety of natural disasters that resulted in famine, with the successful harvest of 1933 ending the famine. Soviet and other historians have argued that the rapid collectivization of agriculture was necessary in order to achieve an equally rapid industrialization of the Soviet Union and ultimately win World War II. Alec Nove claims that the Soviet Union industrialized in spite of, rather than because of, its collectivized agriculture.
The USSR also experienced a major famine in 1947 as a result of war damage and severe droughts, but economist Michael Ellman argues that it could have been prevented if the government had not mismanaged its grain reserves. The famine cost an estimated 1 to 1.5 million lives as well as secondary population losses due to reduced fertility.
Ukrainian famine.
The Holodomor famine is sometimes referred to as the Ukrainian Genocide, implying it was engineered by the Soviet government, specifically targeting the Ukrainian people to destroy the Ukrainian nation as a political factor and social entity. While historians continue to disagree whether the policies that led to Holodomor fall under the legal definition of genocide, twenty-six countries have officially recognized the Holodomor as such. On 28 November 2006, the Ukrainian Parliament approved a bill declaring the Soviet-era forced famine an act of genocide against the Ukrainian people. Professor Michael Ellman concludes that Ukrainians were victims of genocide in 1932–33 according to a more relaxed definition that is favored by some specialists in the field of genocide studies. He asserts that Soviet policies greatly exacerbated the famine's death toll. Although 1.8 million tonnes of grain were exported during the height of the starvation — enough to feed 5 million people for one year — the use of torture and execution to extract grain under the Law of Spikelets, the use of force to prevent starving peasants from fleeing the worst-affected areas, and the refusal to import grain or secure international humanitarian aid to alleviate conditions led to incalculable human suffering in the Ukraine. It would appear that Stalin intended to use the starvation as a cheap and efficient means (as opposed to deportations and shootings) to kill off those deemed to be "counterrevolutionaries," "idlers," and "thieves," but not to annihilate the Ukrainian peasantry as a whole. Ellman also claims that, while this was not the only Soviet genocide (e.g., the Polish operation of the NKVD), it was the worst in terms of mass casualties.
Current estimates on the total number of casualties within Soviet Ukraine range mostly from 2.2 million
to 4 to 5 million.
A Ukrainian court found Josef Stalin, Lazar Kaganovich, Stanislav Kosior and other leaders of the former Soviet Union guilty of genocide by "organizing mass famine in Ukraine in 1932–1933" in January 2010. However, the court "dropped criminal proceedings over the suspects' deaths".
Industrialization.
The Russian Civil War and wartime communism had a devastating effect on the country's economy. Industrial output in 1922 was 13% of that in 1914. A recovery followed under the New Economic Policy, which allowed a degree of market flexibility within the context of socialism. Under Stalin's direction, this was replaced by a system of centrally ordained "Five-Year Plans" in the late 1920s. These called for a highly ambitious program of state-guided crash industrialization and the collectivization of agriculture.
With seed capital unavailable because of international reaction to Communist policies, little international trade, and virtually no modern infrastructure, Stalin's government financed industrialization both by restraining consumption on the part of ordinary Soviet citizens to ensure that capital went for re-investment into industry and by ruthless extraction of wealth from the kulaks.
In 1933 workers' real earnings sank to about one-tenth of the 1926 level. Common and political prisoners in labor camps were forced to perform unpaid labor, and communists and Komsomol members were frequently "mobilized" for various construction projects. The Soviet Union used numerous foreign experts to design new factories, supervise construction, instruct workers, and improve manufacturing processes. The most notable foreign contractor was Albert Kahn's firm that designed and built 521 factories between 1930 and 1932. As a rule, factories were supplied with imported equipment.
In spite of early breakdowns and failures, the first two Five-Year Plans achieved rapid industrialization from a very low economic base. While it is generally agreed that the Soviet Union achieved significant levels of economic growth under Stalin, the precise rate of growth is disputed. It is not disputed, however, that these gains were accomplished at the cost of millions of lives. Official Soviet estimates stated the annual rate of growth at 13.9%; Russian and Western estimates gave lower figures of 5.8% and even 2.9%. Indeed, one estimate is that Soviet growth became temporarily much higher after Stalin's death.
According to Robert Lewis, the Five-Year Plan substantially helped to modernize the previously backward Soviet economy. New products were developed, and the scale and efficiency of existing production greatly increased. Some innovations were based on indigenous technical developments, others on imported foreign technology. Despite its costs, the industrialization effort allowed the Soviet Union to fight, and ultimately win, World War II.
Science.
Science in the Soviet Union was under strict ideological control by Stalin and his government, along with art and literature. There was significant progress in "ideologically safe" domains, owing to the free Soviet education system and state-financed research. However, the most notable legacy during Stalin's time was his public endorsement of the agronomist Trofim Lysenko, who rejected Mendelian genetics as "bourgeois pseudoscience" and instead advocated Lamarckian inheritance and hybridization theories (which had been discredited by most Western countries by the 1920s in favor of Darwinian Evolution), that caused widespread agricultural destruction and major setbacks in Soviet knowledge in biology. Many scientists came out publicly against his views, but the majority of them, including Nikolai Vavilov (who was later hailed as a pioneer in modern Genetics), were imprisoned or executed. Some areas of physics were criticized.
Social services.
Under the Soviet government people benefited from some social liberalization. Girls were given an adequate, equal education and women had equal rights in employment, improving lives for women and families. Stalinist development also contributed to advances in health care, which significantly increased the lifespan and quality of life of the typical Soviet citizen. Stalin's policies granted the Soviet people universal access to healthcare and education, effectively creating the first generation free from the fear of typhus, cholera, and malaria. The occurrences of these diseases dropped to record low numbers, increasing life spans by decades.
Soviet women under Stalin were the first generation of women in this country able to give birth in the safety of a hospital with access to prenatal care. Education was also an example of an increase in the standard of living after economic development. The generation born during Stalin's rule was the first in the USSR to achieve widespread literacy. Engineers were sent abroad to learn industrial technology, and hundreds of foreign engineers were brought to Russia on contract. Transport links were improved and many new railways built. Workers who exceeded their quotas, "Stakhanovites", received many incentives for their work; they could afford to buy the goods that were mass-produced by the rapidly expanding Soviet economy.
The increase in demand due to industrialization and the decrease in the workforce due to World War II and repressions generated a major expansion in job opportunities for the survivors, especially for women.
Culture.
Although Stalin was Georgian by birth, after he became involved in politics he promoted Russian nationalism and significantly promoted Russian history, language, and Russian national heroes, particularly during the 1930s and 1940s. There are also claims that he held the Russian people up as the elder brothers of the non-Russian minorities.
During Stalin's reign, the official and long-lived style of Socialist Realism was established for painting, sculpture, music, drama and literature. Previously fashionable "revolutionary" expressionism, abstract art, and avant-garde experimentation were discouraged or denounced as "formalism".
The degree of Stalin's personal involvement in general, and in specific instances, has been the subject of discussion. Stalin's favorite novel "Pharaoh", shared similarities with Sergei Eisenstein's film, "Ivan the Terrible", produced under Stalin's tutelage.
In architecture, a Stalinist Empire Style (basically, updated neoclassicism on a very large scale, exemplified by the Seven Sisters of Moscow) replaced the constructivism of the 1920s. Stalin's rule had a largely disruptive effect on indigenous cultures within the Soviet Union, though the politics of Korenizatsiya and forced development were possibly beneficial to the integration of later generations of indigenous cultures.
Religion.
Raised in the Georgian Orthodox faith, Stalin later denounced religion and became an atheist. His government promoted atheism through special atheistic education in schools, anti-religious propaganda, the anti-religious work of public institutions (Society of the Godless), discriminatory laws, and a terror campaign against religious believers. By the late 1930s, it had become dangerous to be publicly associated with religion.
Stalin's role in the fortunes of the Russian Orthodox Church is complex. Continuous persecution in the 1930s resulted in its near-extinction as a public institution: by 1939, active parishes numbered in the low hundreds (down from 54,000 in 1917), many churches had been leveled, and tens of thousands of priests, monks and nuns were persecuted and killed. Over 100,000 were shot during the purges of 1937–1938. During World War II, the Church was allowed a revival as a patriotic organization, and thousands of parishes were reactivated until a further round of suppression during Khrushchev's rule. The Russian Orthodox Church Synod's recognition of the Soviet government and of Stalin personally led to a schism with the Russian Orthodox Church Outside Russia.
All other religions in the Soviet Union, including the Roman Catholic Church, Eastern Catholic Churches, Baptists, Islam, Buddhism, and Judaism underwent similar ordeals: thousands of monks were persecuted, and hundreds of churches, synagogues, mosques, temples, sacred monuments, monasteries and other religious buildings were razed.
Theorist.
Stalin and his supporters have highlighted the notion that socialism can be built and consolidated by a country ("Socialism in One Country") as underdeveloped as Russia during the 1920s. Indeed, this might be the only means in which it could be built in a hostile environment. In 1933, Stalin put forward the theory of aggravation of the class struggle along with the development of socialism, arguing that the further the country would move forward, the more acute forms of struggle will be used by the doomed remnants of exploiter classes in their last desperate efforts – and that, therefore, political repression was necessary.
In 1936, Stalin announced that the society of the Soviet Union consisted of two non-antagonistic classes: workers and kolkhoz peasantry. These corresponded to the two different forms of property over the means of production that existed in the Soviet Union: state property (for the workers) and collective property (for the peasantry). In addition to these, Stalin distinguished the stratum of intelligentsia. The concept of "non-antagonistic classes" was entirely new to Leninist theory. Among Stalin's contributions to Communist theoretical literature were "Dialectical and Historical Materialism," "Marxism and the National Question", "Trotskyism or Leninism", and "The Principles of Leninism."
Calculating the number of victims.
Before the 1991 dissolution of the Soviet Union, researchers who attempted to count the number of people killed under Stalin's regime produced estimates ranging from 3 to 60 million. After the Soviet Union dissolved, evidence from the Soviet archives also became available, containing official records of 799,455 executions (1921–1953), around 1.7 million deaths in the Gulag and some 390,000 deaths during kulak forced resettlement – with a total of about 2.9 million officially recorded victims in these categories.
The official Soviet archival records do not contain comprehensive figures for some categories of victims, such as those of ethnic deportations or of German population transfers in the aftermath of World War II. Eric D. Weitz wrote, "By 1948, according to Nicolas Werth, the mortality rate of the 600,000 people deported from the Caucasus between 1943 and 1944 had reached 25%." Other notable exclusions from NKVD data on repression deaths include the Katyn massacre, other killings in the newly occupied areas, and the mass shootings of Red Army personnel (deserters and so-called deserters) in 1941. The Soviets executed 158,000 soldiers for desertion during the war, and the "blocking detachments" of the NKVD shot thousands more. Also, the official statistics on Gulag mortality exclude deaths of prisoners taking place shortly after their release but which resulted from the harsh treatment in the camps. Some historians also believe that the official archival figures of the categories that were recorded by Soviet authorities are unreliable and incomplete. In addition to failures regarding comprehensive recordings, as one additional example, Robert Gellately and Simon Sebag Montefiore argue that the many suspects beaten and tortured to death while in "investigative custody" were likely not to have been counted amongst the executed.
Historians working after the Soviet Union's dissolution have estimated victim totals ranging from approximately 4 million to nearly 10 million, not including those who died in famines. Russian writer Vadim Erlikman, for example, makes the following estimates: executions, 1.5 million; gulags, 5 million; deportations, 1.7 million out of 7.5 million deported; and POWs and German civilians, 1 million – a total of about 9 million victims of repression.
Some have also included the deaths of 6 to 8 million people in the 1932–1933 famine among the victims of Stalin's repression. This categorization is controversial however, as historians differ as to whether the famine was a deliberate part of the campaign of repression against kulaks and others, or simply an unintended consequence of the struggle over forced collectivization.
Accordingly, if famine victims are included, a minimum of around 10 million deaths—6 million from famine and 4 million from other causes—are attributable to the regime, with a number of recent historians suggesting a likely total of around 20 million, citing much higher victim totals from executions, Gulag camps, deportations and other causes. Adding 6–8 million famine victims to Erlikman's estimates above, for example, would yield a total of between 15 and 17 million victims. Researcher Robert Conquest, meanwhile, has revised his original estimate of up to 30 million victims down to 20 million. In his most recent edition of "The Great Terror" (2007), Conquest states that while exact numbers may never be known with complete certainty, at least 15 million people were either executed or worked to death in the camps. RJ Rummel maintains that the earlier higher victim total estimates are correct, although he includes those killed by the Soviet government in other Eastern European countries as well.
World War II, 1939–45.
Pact with Hitler.
After a failed attempt to sign an anti-German military alliance with France and Britain and talks with Germany regarding a potential political deal, on 23 August 1939, the Soviet Union entered into a non-aggression pact with Nazi Germany, negotiated by Soviet foreign minister Vyacheslav Molotov and German foreign minister Joachim von Ribbentrop. Officially a non-aggression treaty only, an appended secret protocol, also reached on 23 August 1939, divided the whole of eastern Europe into German and Soviet spheres of influence.
The eastern part of Poland, Latvia, Estonia, Finland and part of Romania were recognized as parts of the Soviet sphere of influence, with Lithuania added in a second secret protocol in September 1939. Stalin and Ribbentrop traded toasts on the night of the signing discussing past hostilities between the countries. German-Soviet trade agreements completely undermined the British blockade of Germany. Economic cooperation was so considerable that in 1939 Trotsky called Stalin "Hitler's quartermaster".
Implementing the division of Eastern Europe and other invasions.
On 1 September 1939, the German invasion of its agreed upon portion of Poland started World War II. On 17 September the Red Army invaded eastern Poland and occupied the Polish territory assigned to it by the Molotov-Ribbentrop Pact, followed by co-ordination with German forces in Poland. Eleven days later, the secret protocol of the Molotov-Ribbentrop Pact was modified, allotting Germany a larger part of Poland, while ceding most of Lithuania to the Soviet Union.
After Stalin declared that he was going to "solve the Baltic problem", by June 1940, Lithuania, Latvia and Estonia were merged into the Soviet Union, after repressions and actions therein brought about the deaths of over 160,000 citizens of these states. After facing stiff resistance in an invasion of Finland, an interim peace was entered, granting the Soviet Union the eastern region of Karelia (10% of Finnish territory).
After this campaign, Stalin took actions to bolster the Soviet military, modify training and improve propaganda efforts in the Soviet military. In June 1940, Stalin directed the Soviet annexation of Bessarabia and northern Bukovina, proclaiming this formerly Romanian territory part of the Moldavian Soviet Socialist Republic. But in annexing northern Bukovina, Stalin had gone beyond the agreed limits of the secret protocol.
After the Tripartite Pact was signed by Axis Powers Germany, Japan and Italy, in October 1940, Stalin traded letters with Ribbentrop, with Stalin writing about entering an agreement regarding a "permanent basis" for their "mutual interests." After a conference in Berlin between Hitler, Molotov and Ribbentrop, Germany presented Molotov with a proposed written agreement for Axis entry. On 25 November, Stalin responded with a proposed written agreement for Axis entry which was never answered by Germany. Shortly thereafter, Hitler issued a secret directive on the eventual attempts to invade the Soviet Union. In an effort to demonstrate peaceful intentions toward Germany, on 13 April 1941, Stalin oversaw the signing of a neutrality pact with Axis power Japan.
On 6 May, Stalin replaced Molotov as Premier of the Soviet Union. Although Stalin had been the "de facto" head of government for a decade and a half, he had concluded relations with Nazi Germany had deteriorated to such an extent that he needed to deal with the problem as "de jure" head of government as well.
Hitler breaks the pact.
In the early morning of 22 June 1941, Adolf Hitler broke the pact by implementing Operation Barbarossa, the German invasion of the Soviet Union that began the war on the Eastern Front. Already in autumn 1940 Stalin received a warning from the Dutch Communist Party, via the network of the Red Orchestra, that Hitler was preparing for a winter war by allowing the construction of thousands of snow landing gears for the Junkers Ju 52 transport planes. Although Stalin had received warnings from spies and his generals, he felt that Germany would not attack the Soviet Union until Germany had defeated Britain. In the initial hours after the German attack commenced, Stalin hesitated, wanting to ensure that the German attack was sanctioned by Hitler, rather than the unauthorized action of a rogue general.
Accounts by Nikita Khrushchev and Anastas Mikoyan claim that, after the invasion, Stalin retreated to his dacha in despair for several days and did not participate in leadership decisions. However, documentary evidence of orders given by Stalin contradicts these accounts, leading some historians to speculate that Khrushchev's account is inaccurate. By the end of 1941, the Soviet military had suffered 4.3 million casualties and German forces had advanced 1,050 miles (1,690 kilometers).
Soviets stop the Germans.
While the Germans pressed forward, Stalin was confident of an eventual Allied victory over Germany. In September 1941, Stalin told British diplomats that he wanted two agreements: (1) a mutual assistance/aid pact and (2) a recognition that, after the war, the Soviet Union would gain the territories in countries that it had taken pursuant to its division of Eastern Europe with Hitler in the Molotov–Ribbentrop Pact. The British agreed to assistance but refused to agree upon the territorial gains, which Stalin accepted months later as the military situation deteriorated somewhat in mid-1942. By December 1941, Hitler's troops had advanced to within 20 miles of the Kremlin in Moscow. On 5 December, the Soviets launched a counteroffensive, pushing German troops back 40–50 miles from Moscow, the Wehrmacht's first significant defeat of the war.
In 1942, Hitler shifted his primary goal from an immediate victory in the East, to the more long-term goal of securing the southern Soviet Union to conquer oil fields vital to a long-term German war effort. In July 1942, Hitler praised the efficiency of the Soviet military industry and Stalin:
While Red Army generals saw evidence that Hitler would shift efforts south, Stalin considered this to be a flanking campaign in efforts to take Moscow. During the war, "Time" magazine named Stalin Time Person of the Year twice and he was also one of the nominees for "Time" Person of the Century title.
Soviet push to Germany.
The Soviets repulsed the important German strategic southern campaign and, although there were 2.5 million Soviet casualties in that effort, it permitted the Soviets to take the offensive for most of the rest of the war on the Eastern Front.
Germany attempted an encirclement attack at Kursk, which was successfully repulsed by the Soviets. Kursk marked the beginning of a period where Stalin became more willing to listen to the advice of his generals. By the end of 1943, the Soviets occupied half of the territory taken by the Germans from 1941 to 1942. Soviet military industrial output also had increased substantially from late 1941 to early 1943 after Stalin had moved factories well to the East of the front, safe from German invasion and air attack.
In November 1943, Stalin met with Churchill and Roosevelt in Tehran. The parties later agreed that Britain and America would launch a cross-channel invasion of France in May 1944, along with a separate invasion of southern France. Stalin insisted that, after the war, the Soviet Union should incorporate the portions of Poland it occupied pursuant to the Molotov-Ribbentrop Pact with Germany, which Churchill opposed.
In 1944, the Soviet Union made significant advances across Eastern Europe toward Germany, including Operation Bagration, a massive offensive in Belorussia against the German Army Group Centre.
Final victory.
By April 1945, Nazi Germany faced its last days with 1.9 million German soldiers in the East fighting 6.4 million Red Army soldiers while 1 million German soldiers in the West battled 4 million Western Allied soldiers. While initial talk existed of a race to Berlin by the Allies, after Stalin successfully lobbied for Eastern Germany to fall within the Soviet "sphere of influence" at Yalta, no plans were made by the Western Allies to seize the city by a ground operation.
On 30 April, Hitler and Eva Braun committed suicide, after which Soviet forces found their remains, which had been burned at Hitler's directive. German forces surrendered a few days later. Despite the Soviets' possession of Hitler's remains, Stalin refused to believe that his old nemesis was actually dead, a belief that remained with him for years after the war ended.
Fending off the German invasion and pressing to victory in the East required a tremendous sacrifice by the Soviet Union. Soviet military casualties totaled approximately 35 million (official figures 28.2 million) with approximately 14.7 million killed, missing or captured (official figures 11.285 million). Although figures vary, the Soviet civilian death toll probably reached 20 million. One in four Soviets was killed or wounded. Some 1,710 towns and 70,000 villages were destroyed. Thereafter, Stalin was at times referred to as one of the most influential men in human history.
Nobel Peace Prize nominations.
Stalin was nominated for the Nobel Peace Prize in 1945 and 1948.
Human rights abuses.
After taking around 300,000 Polish prisoners in 1939 and early 1940, 25,700 Polish POWs were executed on 5 March 1940, pursuant to a note to Stalin from Lavrenty Beria, in what became known as the Katyn massacre. While Stalin personally told a Polish general they'd "lost track" of the officers in Manchuria, Polish railroad workers found the mass grave after the 1941 Nazi invasion. The massacre became a source of political controversy, with the Soviets eventually claiming that Germany committed the executions when the Soviet Union retook Poland in 1944. The Soviets did not admit responsibility until 1990.
Stalin introduced controversial military orders, such as Order No. 270 in August 1941, requiring superiors to shoot deserters on the spot while their family members were subject to arrest. Thereafter, Stalin also conducted a purge of several military commanders that were shot for "cowardice" without a trial. Stalin issued Order No. 227 in July 1942, directing that commanders permitting retreat without permission to be subject to a military tribunal, and soldiers guilty of disciplinary procedures to be forced into "penal battalions", which were sent to the most dangerous sections of the front lines. From 1942 to 1945, 427,910 soldiers were assigned to penal battalions. The order also directed "blocking detachments" to shoot fleeing panicked troops at the rear.
In June 1941, weeks after the German invasion began, Stalin also directed employing a scorched earth policy of destroying the infrastructure and food supplies of areas before the Germans could seize them, and that partisans were to be set up in evacuated areas. He also ordered the NKVD to murder around one hundred thousand political prisoners in areas where the Wehrmacht approached, while others were deported east.
After the capture of Berlin, Soviet troops reportedly raped from tens of thousands to two million women, and 50,000 during and after the occupation of Budapest. Many of these women died or committed suicide as a result of rape. In former Axis countries, such as Germany, Romania and Hungary, Red Army officers generally viewed cities, villages and farms as being open to pillaging and looting.
In the Soviet Occupation Zone of post-war Germany, the Soviets set up ten NKVD-run "special camps" subordinate to the gulag. These "special camps" were former Stalags, prisons, or Nazi concentration camps such as Sachsenhausen (special camp number 7) and Buchenwald (special camp number 2). According to German government estimates, "65,000 people died in those Soviet-run camps or in transportation to them."
According to recent figures, of an estimated four million POWs taken by the Soviets, including Germans, Japanese, Hungarians, Romanians and others, some 580,000 never returned, presumably victims of privation or the Gulags. German estimates put the actual death toll of German POWs in the USSR at about 1 million, they maintain that among those reported as missing were men who actually died as POWs. Soviet POWs and forced laborers who survived German captivity were sent to special "transit" or "filtration" camps to determine which were potential traitors.
Of the approximately 4 million to be repatriated 2,660,013 were civilians and 1,539,475 were former POWs. Of the total, 2,427,906 were sent home and 801,152 were reconscripted into the armed forces. 608,095 were enrolled in the work battalions of the defense ministry. 272,867 were transferred to the authority of the NKVD for punishment, which meant a transfer to the Gulag system. 89,468 remained in the transit camps as reception personnel until the repatriation process was finally wound up in the early 1950s.
Allied conferences on post-war Europe.
Stalin met in several conferences with British Prime Minister Winston Churchill (and later Clement Attlee) and/or U.S. President Franklin D. Roosevelt (and later Harry Truman) to plan military strategy and, later, to discuss Europe's postwar reorganization. Very early conferences, such as that with British diplomats in Moscow in 1941 and with Churchill and American diplomats in Moscow in 1942, focused mostly upon war planning and supply, though some preliminary postwar reorganization discussion also occurred. In 1943, Stalin met with Churchill and Roosevelt in the Tehran Conference. In 1944, Stalin met with Churchill in the Moscow Conference. Beginning in late 1944, the Red Army occupied much of Eastern Europe during these conferences and the discussions shifted to a more intense focus on the reorganization of postwar Europe.
In February 1945, at the conference at Yalta, Stalin demanded a Soviet sphere of political influence in Eastern Europe. Stalin eventually was convinced by Churchill and Roosevelt not to dismember Germany. Stalin also stated that the Polish government-in-exile demands for self-rule were not negotiable, such that the Soviet Union would keep the territory of eastern Poland they had already taken by invasion with German consent in 1939, and wanted the pro-Soviet Polish government installed. After resistance by Churchill and Roosevelt, Stalin promised a re-organization of the current Communist puppet government on a broader democratic basis in Poland. He stated the new government's primary task would be to prepare elections.
The parties at Yalta further agreed that the countries of liberated Europe and former Axis satellites would be allowed to "create democratic institutions of their own choice", pursuant to "the right of all peoples to choose the form of government under which they will live." The parties also agreed to help those countries form interim governments "pledged to the earliest possible establishment through free elections" and "facilitate where necessary the holding of such elections." After the re-organization of the Provisional Government of the Republic of Poland, the parties agreed that the new party shall "be pledged to the holding of free and unfettered elections as soon as possible on the basis of universal suffrage and secret ballot." One month after Yalta, the Soviet NKVD arrested 16 Polish leaders wishing to participate in provisional government negotiations, for alleged "crimes" and "diversions", which drew protest from the West. The fraudulent Polish elections, held in January 1947 resulted in Poland's official transformation to undemocratic communist state by 1949.
At the Potsdam Conference from July to August 1945, though Germany had surrendered months earlier, instead of withdrawing Soviet forces from Eastern European countries, Stalin had not moved those forces. At the beginning of the conference, Stalin repeated previous promises to Churchill that he would refrain from a "Sovietization" of Eastern Europe. Stalin pushed for reparations from Germany without regard to the base minimum supply for German citizens' survival, which worried Truman and Churchill who thought that Germany would become a financial burden for Western powers.
In addition to reparations, Stalin pushed for "war booty", which would permit the Soviet Union to directly seize property from conquered nations without quantitative or qualitative limitation, and a clause was added permitting this to occur with some limitations. By July 1945, Stalin's troops effectively controlled the Baltic States, Poland, Czechoslovakia, Hungary, Bulgaria, and Romania, and refugees were fleeing out of these countries fearing a Communist take-over. The western allies, and especially Churchill, were suspicious of the motives of Stalin, who had already installed communist governments in the central European countries under his influence.
In these conferences, his first appearances on the world stage, Stalin proved to be a formidable negotiator. Anthony Eden, the British Foreign Secretary noted: "Marshal Stalin as a negotiator was the toughest proposition of all. Indeed, after something like thirty years' experience of international conferences of one kind and another, if I had to pick a team for going into a conference room, Stalin would be my first choice. Of course the man was ruthless and of course he knew his purpose. He never wasted a word. He never stormed, he was seldom even irritated."
War against Japan.
At the Tehran and Yalta conferences, Stalin agreed to enter the war against Japan after Germany's defeat. In April 1945, the Soviet Government officially denounced the Soviet–Japanese Neutrality Pact and on 9 August, in-between the atomic bombings of Hiroshima and Nagasaki, the Soviet army invaded Japanese occupied Manchuria and quickly defeated the Kwantung Army. These events led to the Japanese surrender and the complete end of World War II. The Soviets took Sakhalin Island and the Kuril Islands and Stalin had planned to invade the Japanese island of Hokkaido but cancelled the operation after strong American protest. Stalin felt dismayed in not being given much influence in occupied Japan.
Post-war era, 1945–1953.
The Iron Curtain and the Eastern Bloc.
After Soviet forces remained in Eastern and Central European countries, with the beginnings of communist puppet regimes in those countries, Churchill referred to the region as being behind an "Iron Curtain" of control from Moscow. The countries under Soviet control in Eastern and Central Europe were sometimes called the "Eastern bloc" or "Soviet Bloc".
In Soviet-controlled East Germany, the major task of the ruling communist party in Germany was to channel Soviet orders down to both the administrative apparatus and the other bloc parties pretending that these were initiatives of its own, with deviations potentially leading to reprimands, imprisonment, torture and even death. Property and industry were nationalized.
The German Democratic Republic was declared on 7 October 1949, with a new constitution which enshrined socialism and gave the Soviet-controlled Socialist Unity Party (SED) control. In Berlin, after citizens strongly rejected communist candidates in an election, in June 1948, the Soviet Union blockaded West Berlin, the portion of Berlin not under Soviet control, cutting off all supply of food and other items. The blockade failed due to the unexpected massive aerial resupply campaign carried out by the Western powers known as the Berlin Airlift. In 1949, Stalin conceded defeat and ended the blockade.
While Stalin had promised at the Yalta Conference that free elections would be held in Poland, after an election failure in "3 times YES" elections, vote rigging was employed to win a majority in the carefully controlled poll. Following the forged referendum, the Polish economy started to become nationalized.
In Hungary, when the Soviets installed a communist government, Mátyás Rákosi, who described himself as "Stalin's best Hungarian disciple" and "Stalin's best pupil", took power. Rákosi employed "salami tactics", slicing up these enemies like pieces of salami, to battle the initial postwar political majority ready to establish a democracy. Rákosi, employed Stalinist political and economic programs, and was dubbed the "bald murderer" for establishing one of the harshest dictatorships in Europe. Approximately 350,000 Hungarian officials and intellectuals were purged from 1948 to 1956.
During World War II, in Bulgaria, the Red Army crossed the border and created the conditions for a communist coup d'état on the following night. The Soviet military commander in Sofia assumed supreme authority, and the communists whom he instructed, including Kimon Georgiev, took full control of domestic politics.
In 1949, the Soviet Union, Bulgaria, Czechoslovakia, Hungary, Poland, and Romania founded the Comecon in accordance with Stalin's desire to enforce Soviet domination of the lesser states of Central Europe and to mollify some states that had expressed interest in the Marshall Plan, and which were now, increasingly, cut off from their traditional markets and suppliers in Western Europe. Czechoslovakia, Hungary, and Poland had remained interested in Marshall aid despite the requirements for a convertible currency and market economies. In July 1947, Stalin ordered these communist-dominated governments to pull out of the Paris Conference on the European Recovery Programme. This has been described as "the moment of truth" in the post–World War II division of Europe.
In Greece, Britain and the United States supported the anti-communists in the Greek Civil War and suspected the Soviets of supporting the Greek communists, although Stalin refrained from getting involved in Greece, dismissing the movement as premature. Albania remained an ally of the Soviet Union, but Yugoslavia broke with the USSR in 1948.
In Stalin's last year of life, one of his last major foreign policy initiatives was the 1952 Stalin Note for German reunification and Superpower disengagement from Central Europe, but Britain, France, and the United States viewed this with suspicion and rejected the offer.
China and Korea.
In Asia, the Red Army had overrun Manchuria in the last month of the war and then also occupied Korea above the 38th parallel north. Mao Zedong's Communist Party of China, though receptive to minimal Soviet support, defeated the pro-Western and heavily American-assisted Chinese Nationalist Party (Kuomintang, KMT) in the Chinese Civil War.
There was friction between Stalin and Mao from the beginning. During World War II Stalin had supported the dictator of China, Chiang Kai-Shek, as a bulwark against Japan and had turned a blind eye to Chiang's mass killings of communists. He generally put his alliance with Chiang against Japan ahead of helping his ideological allies in China in his priorities. Even after the war Stalin concluded a non-aggression pact between the USSR and Chiang's KMT regime in China and instructed Mao and the Chinese communists to cooperate with Chiang and the KMT after the war. Mao did not follow Stalin's instructions though and started a communist revolution against Chiang. Stalin did not believe Mao would be successful so he was less than enthusiastic in helping Mao. The USSR continued to maintain diplomatic relations with Chiang's KMT regime until 1949 when it became clear Mao would win.
Stalin supported the Turkic Muslims known today as Uyghur in seeking their own state, Second East Turkestan Republic during the Ili Rebellion against the Republic of China. He backed the Uyghur Communist Muslim leader Ehmetjan Qasim against the anti Communist Chinese Kuomintang forces.
Stalin did conclude a new friendship and alliance treaty with Mao after he defeated Chiang. But there was still a lot of tension between the two leaders and resentment by Mao for Stalin's less than enthusiastic help during the civil war in China.
The Communists controlled mainland China while the Nationalists held a rump state on the island of Taiwan. The Soviet Union soon after recognized Mao's People's Republic of China, which it regarded as a new ally. The People's Republic claimed Taiwan, though it had never held authority there.
Diplomatic relations between the Soviet Union and China reached a high point with the signing of the 1950 Sino-Soviet Treaty of Friendship and Alliance. Both countries provided military support to a new friendly state in North Korea. After various Korean border conflicts, war broke out with U.S.-allied South Korea in 1950, starting the Korean War.
The North Korean Army struck in the pre-dawn hours of Sunday, 25 June 1950, crossing the 38th parallel behind a firestorm of artillery, beginning their invasion of South Korea. During the Korean War, Soviet pilots flew Soviet aircraft from Chinese bases against United Nations aircraft defending South Korea. Post-Cold War research in Soviet Archives has revealed that the Korean War was begun by Kim Il-sung with the express permission of Stalin.
Israel.
Stalin originally supported the creation of Israel in 1948. The USSR was one of the first nations to recognize the new country. Golda Meir came to Moscow as the first Israeli Ambassador to the USSR that year. However, after providing war materiel for Israel through Czechoslovakia from 1947 to 1949, Stalin later changed his mind and came out against Israel.
"Falsifiers of History".
In 1948, Stalin personally edited and rewrote by hand sections of the cold war book "Falsifiers of History". "Falsifiers" was published in response to the documents made public in "Nazi-Soviet Relations, 1939–1941: Documents from the Archives of The German Foreign Office", which included the secret protocols of the Molotov-Ribbentrop Pact and other secret German-Soviet relations documents. "Falsifiers" originally appeared as a series of articles in "Pravda" in February 1948, and was subsequently published in numerous languages and distributed worldwide.
The book did not attempt to directly counter or deal with the documents published in "Nazi-Soviet Relations" and rather, focused upon Western culpability for the outbreak of war in 1939. It argues that "Western powers" aided Nazi rearmament and aggression, including that American bankers and industrialists provided capital for the growth of German war industries, while deliberately encouraging Hitler to expand eastward. It depicted the Soviet Union as striving to negotiate a collective security against Hitler, while being thwarted by double-dealing Anglo-French appeasers who, despite appearances, had no intention of a Soviet alliance and were secretly negotiating with Berlin. It casts the Munich agreement, not just as Anglo-French short-sightedness or cowardice, but as a "secret" agreement that was "a highly important phase in their policy aimed at goading the Hitlerite aggressors against the Soviet Union." The book also included the claim that, during the Pact's operation, Stalin rejected Hitler's offer to share in a division of the world, without mentioning the Soviet offers to join the Axis. Historical studies, official accounts, memoirs and textbooks published in the Soviet Union used that depiction of events until the Soviet Union's dissolution.
Domestic support.
Domestically, Stalin was seen as a great wartime leader who had led the Soviets to victory against the Nazis.
An increasingly nationalistic emphasis on Russian history and achievements became a salient feature of Soviet culture in the 1940s. At the end of May 1945, Stalin proposed a victory toast to the Soviet people, and to the virtues of the Russian majority in particular:
"Doctors' plot".
The "Doctors' plot" was a plot outlined by Stalin and Soviet officials in 1952 and 1953 whereby several doctors (over half of whom were Jewish) allegedly attempted to kill Soviet officials. The prevailing opinion of many scholars outside the Soviet Union is that Stalin intended to use the resulting doctors' trial to launch a massive party purge. The plot is also viewed by many historians as an antisemitic provocation. It followed on the heels of the 1952 show trials of the Jewish Anti-Fascist Committee and the secret execution of thirteen members on Stalin's orders in the Night of the Murdered Poets.
Thereafter, in a December Politburo session, Stalin announced that "Every Jewish nationalist is the agent of the American intelligence service. Jewish nationalists think that their nation was saved by the United States (there you can become rich, bourgeois, etc.). They think they're indebted to the Americans. Among doctors, there are many Jewish nationalists." To mobilize the Soviet people for his campaign, Stalin ordered "TASS" and "Pravda" to issue stories along with Stalin's alleged uncovering of a "Doctors Plot" to assassinate top Soviet leaders, including Stalin, in order to set the stage for show trials.
The next month, Pravda published stories with text regarding the purported "Jewish bourgeois-nationalist" plotters. Nikita Khrushchev wrote that Stalin hinted him to incite anti-Semitism in the Ukraine, telling him that "the good workers at the factory should be given clubs so they can beat the hell out of those Jews." Stalin also ordered falsely accused physicians to be tortured "to death". Regarding the origins of the plot, people who knew Stalin, such as Khrushchev, suggest that Stalin had long harbored negative sentiments toward Jews, and anti-Semitic trends in the Kremlin's policies were further fueled by the exile of Leon Trotsky. In 1946, Stalin allegedly said privately that "every Jew is a potential spy." At the end of January 1953, Stalin's personal physician Miron Vovsi (cousin of Solomon Mikhoels, who was assassinated in 1948 at the orders of Stalin) was arrested within the frame of the plot. Vovsi was released by Beria after Stalin's death in 1953, as was his son-in-law, the composer Mieczyslaw Weinberg.
Some historians have argued that Stalin was also planning to send millions of Jews to four large newly built labor camps in Western Russia using a "Deportation Commission" that would purportedly act to save Soviet Jews from an enraged Soviet population after the Doctors Plot trials. Others argue that any charge of an alleged mass deportation lacks specific documentary evidence. Regardless of whether a plot to deport Jews was planned, in his "Secret Speech" in 1956, Soviet Premier Nikita Khrushchev stated that the Doctors Plot was "fabricated ... set up by Stalin", that Stalin told the judge to beat confessions from the defendants and had told Politburo members "You are blind like young kittens. What will happen without me? The country will perish because you do not know how to recognize enemies."
Death and legacy.
Stalin's health deteriorated towards the end of World War II. He suffered from atherosclerosis from his heavy smoking, a mild stroke around the time of the Victory Parade, and a severe heart attack in October 1945.
In the early morning hours of 1 March 1953, after an all-night dinner and a movie, Stalin arrived at his Kuntsevo residence 15 km west of Moscow centre, with interior minister Lavrentiy Beria and future premiers Georgy Malenkov, Nikolai Bulganin, and Nikita Khrushchev, where he retired to his bedroom to sleep. At dawn, Stalin did not emerge from his room.
Although his guards thought that it was strange not to see him awake at his usual time, they were strictly instructed not to bother him and left him alone the entire day. At around 10 p.m., he was discovered by Peter Lozgachev, the Deputy Commandant of Kuntsevo, who entered his bedroom to check on him and recalled the scene of Stalin lying on his back on the floor of his room beside his bed, wearing pyjama bottoms and an undershirt, with his clothes soaked in stale urine. A frightened Lozgachev asked Stalin what happened to him, but all he could get out of him was unintelligible responses that sounded like "Dzhhhhh." Lozgachev used the bedroom telephone to frantically call a few party officials; he told them that Stalin may have had a stroke and asked them to send good doctors to the Kuntsevo residence immediately. Lavrentiy Beria was informed and arrived a few hours afterwards. The doctors arrived in the early morning of 2 March when they changed Stalin's bedclothes and tended to him. They diagnosed him with a cerebral hemorrhage (stroke) caused by hypertension (high blood pressure), with stomach hemorrhage facilitating. He was treated in his dacha with leeches, as was customary at the time. On March 3 his double Felix Dadaev was recalled from vacation to Moscow "to be ready to stand in for Stalin if needed", but he never needed to. On March 4 Stalin's illness was broadcast in the media with surprising detail such as pulse, blood pressure and urinalysis; for convenience the time of his stroke was said to be March 2 and his location as Moscow. The bedridden Stalin died on 5 March 1953, at the age of 74.
Suggestions of assassination.
The political memoirs of Vyacheslav Molotov, published in 1993, claimed that Beria had boasted to Molotov that he poisoned Stalin: "I took him out."
Stomach hemorrhage is usually not caused by high blood pressure, but is, along with stroke, consistent with overdose of warfarin, a colorless, tasteless, anticoagulant drug. In the treating physicians' final report submitted to the Central Committee in July 1953, any mention of the stomach hemorrhage was "deleted or vastly subordinated to other information." In 2004, American historian Jonathan Brent and Russia's Presidential Commission for the Rehabilitation of Repressed Persons executive secretary Vladimir Naumov published a book proposing that Beria, with the complicity of Khrushchev, slipped warfarin into Stalin's wine on the night of his death.
Stalin's autopsy, conducted by the Soviet Ministry of Health in March 1953 but not released until 2011, confirmed the cause of death as stroke resulting from high blood pressure, and that hypertension had caused cardiac hemorrhage (not usually caused by high blood pressure) and gastrointestinal hemorrhage as well. In 2011, Miguel A. Faria, President of Mercer University School of Medicine, retired clinical professor of neurosurgery and adjunct professor of medical history, interpreted the autopsy's composition as the examiners' desire to demonstrate for posterity that they had fulfilled their professional duties as best they could by mentioning the non-cerebral hemorrhages. At the same time they would have provided themselves political cover by purposely attributing the hemorrhages to hypertension instead of poisoning by warfarin. Faria noted that when the autopsy was performed, "Stalin was worshipped as a demigod, and his assassination would have been unacceptable to the Russian populace." He also notes that Stalin experienced renal hemorrhages during his death, which is unlikely to be caused by high blood pressure.
Announcement and Reactions.
Yuri Levitan, the announcer who during the war brought the Soviet people news of victories—but never of defeats—announced Stalin's death. Slowly, solemnly, with a voice brimming over with emotion, he read:
After a visitation of 1.5 million people, his embalmed body was laid to rest on March 9, 1953 in Lenin's Mausoleum. On 31 October 1961 his body was removed from the mausoleum and buried in the Kremlin Wall Necropolis next to the Kremlin walls as part of the process of de-Stalinization.
The Chinese government instituted a period of official mourning for Stalin's death. Mao ordered the flag be flown at half-mast, and banned recreation for three days; he also eulogized Stalin in an article "as a great leader, a Marxist theorist, and a friend of China". On March 9, the country observed a five-minute period of silence in Stalin's memory.
Aftermath.
His demise arrived at a convenient time for Lavrentiy Beria and others, who feared being swept away in yet another purge. It is believed that Stalin felt Beria's power was too great and threatened his own.
After Stalin's death a power struggle for his vacant position took place between the following eight senior members of the Presidium of the Central Committee of the Communist Party of the Soviet Union listed according to the order of precedence presented formally on 5 March 1953: Georgy Malenkov, Lavrentiy Beria, Vyacheslav Molotov, Klim Voroshilov, Nikita Khrushchev, Nikolai Bulganin, Lazar Kaganovich, Anastas Mikoyan.
This struggle lasted until 1958 and eventually Khrushchev won, having defeated all his potential rivals in the Presidium.
Reaction by successors.
The harshness with which Soviet affairs were conducted during Stalin's rule was subsequently repudiated by his successors in the Communist Party leadership, most notably by Nikita Khrushchev's repudiation of Stalinism in February 1956. In his "Secret Speech", "On the Personality Cult and its Consequences", delivered to a closed session of the 20th Congress of the Communist Party of the Soviet Union, Khrushchev denounced Stalin for his cult of personality, and his regime for "violation of Leninist norms of legality".
A 1974 Soviet work describes Stalin's leadership in the following manner:
Views on Stalin in contemporary Russia.
Results of a controversial poll taken in 2006 stated that over 35% of Russians would vote for Stalin if he were still alive. Fewer than a third of all Russians regarded Stalin as a "murderous tyrant"; however, a Russian court in 2009, ruling on a suit by Stalin's grandson Yevgeny Dzhugashvili against the newspaper "Novaya Gazeta", ruled that referring to Stalin as a "bloodthirsty cannibal" was not libel. In a July 2007 poll, 54% of the Russian youth agreed that Stalin did more good than bad while 46% (of them) disagreed that Stalin was a "cruel tyrant". Half of the respondents, aged from 16 to 19, agreed Stalin was a wise leader.
In December 2008, Stalin was voted third in the nationwide television show "Name of Russia", (Russia TV channel aimed to elect the most notable personality in Russian history by voting via the Internet, radio and television) narrowly behind 13th-century prince Alexander Nevsky and Pyotr Stolypin, one of Nicholas II's prime ministers. The Communist Party accused the Kremlin in rigging the poll in order to prevent him or Lenin being given first place.
On 3 July 2009, Russia's delegates walked out of an Organization for Security and Co-operation in Europe session to demonstrate their objections to a resolution for a remembrance day for the "victims of both Nazism and Stalinism". Only eight out of 385 assembly members voted against the resolution.
In a Kremlin video blog posted on 29 October 2009, Russian President Dmitry Medvedev denounced the efforts of people seeking to rehabilitate Stalin's image. He said the mass extermination during the Stalin era cannot be justified.
In a 2013 Q&A session, when asked whether Russia should restore statues of its Soviet-era leaders, Russian President Vladimir Putin replied "What is the essential difference between (Oliver) Cromwell and (Joseph) Stalin? Can you tell me? No difference...(Cromwell's) monument is standing, (and) no one is going to remove it. The essence is not in these symbols, but in the need to treat with respect every period of our history."
In a March 2016 Levada Center poll 54% of Russians believe that Stalin played a positive role in the history and believed Stalin was a wise leader who led the Soviet Union to prosperity. 60% of respondents did not want to live under a head of state who resembled Stalin. In 2008 this number was 74%. Two thirds of all polled regarded Stalin as a "murderous tyrant" while 23% stated to have "positive feelings" for Stalin.
Views on Stalin in other former Soviet states.
In 2013 a survey by Tbilisi University, commissioned by the Carnegie Endowment for International Peace, found that 45% of Georgians expressed "a positive attitude to Stalin".
In a poll taken by the Kiev International Institute of Sociology in February 2013, 37% of all Ukrainians had "a negative attitude to the figure of Stalin" and 22% "a positive name=KIISS1313/> Positive attitudes prevailed in East Ukraine (36%) and South Ukraine (27%), and negative attitudes in West Ukraine (64%) and Central Ukraine (39%). In the age group 18–29, 16% had positive feelings towards Stalin.
Early 2010 a Ukrainian court convicted Stalin of genocide against the Ukrainian nation during the Soviet famine of 1932–1933.Yushchenko brings Stalin to court over genocide, RT (14 January 2010)Yushchenko Praises Guilty Verdict Against Soviet Leaders For Famine, Radio Free Europe/Radio Liberty (14 January 2010)</ref>
In the spring of 2010 a new monument in honor of Stalin was erected in Zaporizhia. In late December 2010 the statue had his head cut off by unidentified vandals and the following New Year's Eve it was completely destroyed in an explosion. On 25 February 2011 Ukrainian President Viktor Yanukovych stated "Ukraine will definitely not revise its negative view" on Stalin. Ukraine and Poland unveiled a memorial (outside Kiev) to the thousands of Ukrainians, Poles and others killed by Stalin's secret police ahead of World War II in September 2012.
In a Kiev International Institute of Sociology poll taken in February 2016 of all respondents 38% had a negative attitude to Stalin, 26% a neutral one and 17% a positive (19% refused to answer).
According to a 2012 study, 72% of Armenians do not want to live in a country led by someone like Stalin.
Personal life.
Origin of name, nicknames and pseudonyms.
Stalin's (Russian: Ио́сиф Виссарио́нович Ста́лин, ) original Georgian name is transliterated as "Ioseb Besarionis dze Jughashvili" (Georgian: იოსებ ბესარიონის ძე ჯუღაშვილი ). The Russian transliteration of his name Ио́сиф Виссарио́нович Джугашви́ли is in turn transliterated to English as "Iosif Vissarionovich Dzhugashvili". Like other Bolsheviks, he became commonly known by one of his revolutionary "noms de guerre", of which "Stalin" was only the last. "Stalin" is based on the Russian word сталь "stal", meaning "steel", and the name as a whole is supposed to mean "man of steel". Prior nicknames included "Koba", "Soselo", "Ivanov" and many others.
Stalin is believed to have started using the name "K. Stalin" sometime in 1912 as a pen name.
During Stalin's reign his nicknames included:
Appearance.
While photographs and portraits portray Stalin as physically massive and majestic (he had several painters shot who did not depict him "right"), he was only tall (President Harry S. Truman, who stood himself, described Stalin as "a little squirt"). His mustached face was pock-marked from small-pox during childhood. After a carriage accident in his youth, his left arm was shortened and stiffened at the elbow, while his right hand was thinner than his left and frequently hidden. Bronze casts made in 1990 from plaster death mask and plaster casts of his hands clearly show a normal right hand and a withered left hand. He could be charming and polite, mainly towards visiting statesmen. In movies, Stalin was often played by Mikheil Gelovani and, less frequently, by Aleksei Dikiy.
Marriages and family.
Stalin married his first wife Ekaterina Svanidze in 1906, with whom he had a son, Yakov. Yakov shot himself because of Stalin's harshness toward him, but survived. After this, Stalin said, "He can't even shoot straight." Yakov served in the Red Army during World War II and was captured by the Germans. They offered to exchange him for Field Marshal Friedrich Paulus, who had surrendered after Stalingrad, but Stalin turned the offer down, stating, "You have in your hands not only my son Yakov, but millions of my sons. Either you free them all or my son will share their fate." Afterwards, Yakov is said to have committed suicide, running into an electric fence in Sachsenhausen concentration camp, where he was being held. Yakov had a son Yevgeny, who has recently defended his grandfather's legacy in Russian courts. Yevgeny is married to a Georgian woman, has two sons, and seven grandchildren.
With his second wife Nadezhda Alliluyeva Stalin had a son, Vasiliy, and a daughter, Svetlana. Nadezhda died in 1932, officially of illness. She may have committed suicide by shooting herself after a quarrel with Stalin, leaving a suicide note which according to their daughter was "partly personal, partly political." According to A&E Biography, there is also a belief among some Russians that Stalin himself murdered his wife after the quarrel, which apparently took place at a dinner in which Stalin tauntingly flicked cigarettes across the table at her.
Vasiliy rose through the ranks of the Soviet Air Force, officially dying of alcoholism in 1962; however, this is still in question. He distinguished himself in World War II as a capable airman. Svetlana defected to the United States in 1967, where she later married William Wesley Peters, the apprentice of Frank Lloyd Wright. She died in Richland Center, Wisconsin on November 22, 2011, from complications of colon cancer. Olga, her daughter with Peters, now goes by the name Chrese Evans and lives in Portland, Oregon.
In March 2001, Russian Independent Television NTV interviewed a previously unknown grandson living in Novokuznetsk, Yuri Davydov, who stated that his father had told him of his lineage, but, was told to keep quiet because of the campaign against Stalin's cult of personality.
Beside his suite in the Kremlin, Stalin had numerous domiciles. In 1919, he started with a country house near Usovo, he added dachas at Zuvalova and Kuntsevo ("Blizhny dacha" built by Miron Merzhanov). Before World War II he added the Lipki estate and Semyonovskaya, and had at least four dachas in the south by 1937, including one near Sochi. A luxury villa near Gagri was given to him by Beria. In Abkhazia he maintained a mountain retreat. After the war he added dachas at Novy Afon, near Sukhumi, in the Valdai Hills, and at Lake Mitsa. Another estate was near Zelyony Myss on the Black Sea. All these dachas, estates, and palaces were staffed, well-furnished and equipped, kept safe by security forces, and were mainly used privately, rarely for diplomatic purposes. Between places Stalin would travel by car or train, never by air; he flew only once when attending the 1943 Tehran conference.
Other relationships.
While exiled in Siberia, Stalin had an affair with Maria Kuzakova who had an out-of-wedlock child, Konstantin Kuzakov, born after his escape. According to Radzinsky Monefiore, he also had long-term relationship with his housekeeper Valentina Istomina since 1934 The relationship was not officially admitted by the Soviet authorities. Vyacheslav Molotov once said: "Whether or not she was Stalin's wife is nobody else's business".
Habits.
Stalin enjoyed drinking, and would often force those around him to join in. He preferred Georgian wine over Russian vodka, but usually ate traditional Russian food.
Khrushchev reports in his memoirs that Stalin was fond of American cowboy movies. He would often sleep until evening in his dacha, and after waking up summon high-ranking Soviet politicians to watch foreign movies with him in the Kremlin movie theater. The movies, being in foreign languages, were given a running translation by Ivan Bolshakov, people's commissar of cinema. The translations were hilarious for the audience as Bolshakov spoke very basic English. His favourite films were westerns and Charlie Chaplin episodes. He banned any hint of nudity. When Ivan showed a film with a naked woman Stalin shouted, "Are you making a brothel here, Bolshakov?" After a movie had ended, Stalin often invited the audience for dinner, even though the clock was usually past midnight. In the aftermath of the war, he took control over all of Joseph Goebbels' films.
Stalin was an accomplished billiards player, and could read 500 pages a day, having a library of over 20,000 books.
Stalin was also afraid of flying, which is one reason he chose the site of the Yalta conference so he could travel by train.
Religion.
Stalin was raised in the Georgian Orthodox faith, but later gave it up. Stalin had a complex relationship with religious institutions in the Soviet Union. Historians Vladislav Zubok and Constantine Pleshakov have suggested that "[Stalin's] atheism remained rooted in some vague idea of a God of nature."
During the Second World War, Stalin reopened the churches. One reason could have been to motivate the majority of the population, who had Christian beliefs. The reasoning behind this is that by changing the official policy of the party and the state towards religion, the Church and its clergymen could be at his disposal in mobilizing the war effort. On 4 September 1943, Stalin invited Metropolitan Sergius, Metropolitan Alexius and Metropolitan Nicholas to the Kremlin and proposed to reestablish the Moscow Patriarchate, which had been suspended since 1925, and elect the Patriarch. On 8 September 1943, Metropolitan Sergius was elected patriarch.
The CPSU Central Committee continued to promote atheism and the elimination of religion during the remainder of Stalin's lifetime after the 1943 concordat. Stalin's greater tolerance for religion after 1943 was limited by party machinations. Whether persecutions after World War II were more aimed at certain sections of society over and above detractors is disputed.
Controversies about Stalin.
There are conflicting accounts of Stalin's birth, who before coming to power in 1922 listed his birth year in various documents as 1878. The phrase "death of one man is a tragedy, death of a million is a statistic" is sometimes attributed to Stalin, although there is no proof of his saying that. In addition, hypotheses and popular rumors exist that Stalin's biological father was explorer Nicolay Przhevalsky. Some Bolsheviks and others have accused Stalin of being an agent for the Okhrana. It is also widely believed that the Red Terror was begun by Stalin.
Works.
Stalin was also a well-regarded poet in his youth. Some of his poems were published in Ilia Chavchavadze's journal "Iveria" and later anthologized.

</doc>
<doc id="15642" url="https://en.wikipedia.org/wiki?curid=15642" title="January">
January

January is the first month of the year in the Julian and Gregorian calendars and one of seven months with the length of 31 days. The first day of the month is known as New Year's Day. It is, on average, the coldest month of the year within most of the Northern Hemisphere (where it is the second month of winter) and the warmest month of the year within most of the Southern Hemisphere (where it is the second month of summer). In the Southern hemisphere, January is the seasonal equivalent of July in the Northern hemisphere and vice versa.
History.
January (in Latin, "Ianuarius") is named after the Latin word for door ("ianua") since January is the door to the year. The month is conventionally thought of as being named after Janus, the god of beginnings and transitions in Roman mythology, but according to ancient Roman farmers' almanacs Juno was the tutelary deity of the month.
Traditionally, the original Roman calendar consisted of 10 months totaling 304 days, winter being considered a month-less period. Around 713 BC, the semi-mythical successor of Romulus, King Numa Pompilius, is supposed to have added the months of January and February, so that the calendar covered a standard lunar year (354 days). Although March was originally the first month in the old Roman calendar, January became the first month of the calendar year either under Numa or under the Decemvirs about 450 BC (Roman writers differ). In contrast, each specific calendar year was identified by the names of the two consuls, who entered office on May 1 or March 15 until 153 BC, from when they entered office on January 1.
Various Christian feast dates were used for the New Year in Europe during the Middle Ages, including March 25 (Feast of the Annunciation) and December 25. However, medieval calendars were still displayed in the Roman fashion with twelve columns from January to December. Beginning in the 16th century, European countries began officially making January 1 the start of the New Year once again—sometimes called "Circumcision Style" because this was the date of the Feast of the Circumcision, being the seventh day after December 25.
Historical names for January include its original Roman designation, Ianuarius, the Saxon term "Wulf-monath" (meaning wolf month) and Charlemagne's designation Wintarmanoth (winter / cold month). In Slovene, it is traditionally called "január". The name, associated with millet bread and the act of asking for something, was first written in 1466 in the Škofja Loka manuscript.
According to Theodor Mommsen, 1 January became the first day of the year in 600 AUC of the Roman calendar (153 BC), due to disasters in the Lusitanian War. A Lusitanian chief called Punicus invaded the Roman territory, defeated two Roman governors, and killed their troops. The Romans resolved to send a consul to Hispania, and in order to accelerate the dispatch of aid, "they even made the new consuls enter into office two months and a half before the legal time" (March 15).
Month-long observances.
Food Months in the United States.
"This list does not necessarily imply either official status nor general observance."
Non-Gregorian observances, 2016.
"This list does not necessarily imply either official status nor general observance."
Movable observances, 2016 dates.
"This list does not necessarily imply either official status nor general observance."
First Friday - January 1 
First Monday - January 4
Second Saturday - January 9
Second Monday - January 11
Third Friday - January 15
Friday before third Monday - January 15
Third full week of January - January 17–23
Third Sunday - January 17
Third Monday - January 18
Friday between January 19–25 - January 22
Last week of January - 24-30
Fourth Monday - January 25
'"Last Saturday - January 30"
Last Sunday - January 31
January 30 or the nearest Sunday - January 31
Movable Western Christian Observances, 2016.
"This list does not necessarily imply either official status nor general observance."
Fixed observances.
"This list does not necessarily imply either official status nor general observance."

</doc>
<doc id="15644" url="https://en.wikipedia.org/wiki?curid=15644" title="Johnny Unitas">
Johnny Unitas

John Constantine Unitas (; May 7, 1933 – September 11, 2002), nicknamed "Johnny U", or "The Golden Arm", was an American professional football player from the 1950s through the 1970s. He spent the majority of his career playing for the Baltimore Colts. He was a record-setting quarterback, and the National Football League's most valuable player in 1959, 1964 and 1967. For 52 years he held the record for most consecutive games with a touchdown pass (which he set between 1956–1960), until New Orleans Saints quarterback Drew Brees broke his long standing record on October 7, 2012. Unitas was the prototype of the modern era marquee quarterback with a strong passing game, media fanfare, and widespread popularity. He has been consistently listed as one of the greatest NFL players of all time.
Early life.
John Constantine Unitas was born to Francis J. Unitas and Helen Superfisky, who both were of Lithuanian descent, in Pittsburgh, in 1933, and grew up in the Mount Washington neighborhood. His father died when Johnny was five years old of cardiovascular renal disease complicated by pneumonia, and he was raised by his mother, who worked two jobs to support the family. His unusual surname was a result of a phonetic transliteration of a common Lithuanian last name "Jonaitis". Attending St. Justin's High School in Pittsburgh, Unitas played halfback and quarterback. After high school, Unitas looked for an opportunity to play college football.
The University of Louisville came through and Unitas left home for Kentucky.
College career.
In his younger years, Unitas dreamed about being part of the Fighting Irish of Notre Dame but when he tried out for them, the coach simply said that he was just too skinny and he'd "get murdered" if he was put on the field.
In his four-year career as a Louisville Cardinal, Unitas completed 245 passes for 3,139 yards and 27 touchdowns. Reportedly, the 6-foot-1 Unitas weighed 145 pounds on his first day of practice at Louisville. Unitas' first start was in the fifth game of the 1951 season against St. Bonaventure. That game, the freshman threw 11 consecutive passes and three touchdowns to give the Cardinals a 21–19 lead. Though Louisville lost the game 22–21 on a disputed field goal, it had found a talented quarterback. Unitas completed 12 of 19 passes for 240 yards and four touchdowns in a 35–28 victory over Houston. The team finished the season 5–5 overall and 4–1 with Unitas as the starting quarterback. As a freshman, Unitas completed 46 of 99 passes for 602 yards and nine touchdowns (44).
By the 1952 season, the university decided to de-emphasize sports. The new president at Louisville, Dr. Philip Grant Davidson, reduced the amount of athletic aid, and tightened academic standards for athletes. As a result, 15 returning players could not meet the new standards and lost their scholarships. But Unitas maintained his scholarship by taking on a new elective: square dancing. In 1952 Coach Camp switched the team to two-way football. Unitas not only played safety or linebacker on defense and quarterback on offense but returned kicks and punts on special teams. The Cards won their first game against Wayne State, and then Florida State in the second game. Unitas completed 16 of 21 passes for 198 yards and three touchdowns. It was said that Unitas put on such a show at the Florida State game that he threw a pass under his legs for 15 yards. The rest of the season was a struggle for the Cards, who finished 3–5. Unitas completed 106 of 198 passes for 1,540 yards and 12 touchdowns in his sophomore year.
The team won their first game in 1953, against Murray State, and lost the rest for a record of 1–7. One of the most memorable games of the season came in a 59–6 loss against Tennessee. Unitas completed 9 of 19 passes for 73 yards, rushed 9 times for 52 yards, returned 6 kickoffs for eighty-five yards, 1 punt for three yards, and had 86 percent of the team's tackles. The only touchdown the team scored was in the fourth quarter when Unitas made a fake pitch to the running back and ran the ball 23 yards for a touchdown. Unitas was hurt later in the fourth quarter while trying to run the ball. On his way off the field, he received a standing ovation. When he got to the locker room he was so worn that his jersey and shoulder pads had to be cut off because he could not lift his arms. Louisville ended the season with a 20–13 loss to Eastern Kentucky. In his junior year, Unitas completed 49 of 95 passes for 470 yards and three touchdowns.
Unitas was elected captain for the 1954 season, but due to an early injury did not see much playing time. His first start was the third game of the season, against Florida State. Of the 34-man team, 21 were freshmen. The 1954 m Cardinals went 3–6, with their last win at home against Morehead State. Unitas was slowed by so many injuries his senior year his 527 passing yards ended second to Jim Houser's 560.
Professional career.
Pittsburgh Steelers.
After college, the Pittsburgh Steelers of the NFL drafted Unitas in the ninth round. However, Unitas was released before the season began as the odd man out among four quarterbacks trying to fill three spots. Steelers Head Coach Walt Kiesling had made up his mind about Unitas; he thought he was not smart enough to quarterback an NFL team, and Unitas was not given any snaps in practice with the Steelers. Among those edging out Unitas was Ted Marchibroda, future NFL quarterback and longtime NFL head coach. Out of pro football, Unitas—by this time married—worked in construction in Pittsburgh to support his family. On the weekends, he played quarterback, safety and punter on a local semi-professional team called the Bloomfield Rams for $6 a game.
Baltimore Colts.
In 1956, Unitas joined the Baltimore Colts of the NFL under legendary coach Weeb Ewbank, after being asked at the last minute to join Bloomfield Rams lineman Jim Deglau, a Croatian steel worker with a life much like Unitas', at the latter's scheduled Colts tryout. The pair borrowed money from friends to pay for the gas to make the trip. Deglau later told a reporter after Unitas' death, "uncle told him not to come. [He was worried that if he came down and the Colts passed on him, it would look bad (to other NFL teams)." The Colts signed Unitas, much to the chagrin of the Cleveland Browns, who had hoped to claim the rejected Steeler quarterback.
Unitas made his NFL debut with an inauspicious "mop-up" appearance against Detroit, going 0-2 with one interception. Two weeks later starting quarterback George Shaw suffered a broken leg against the Chicago Bears. In his first serious action, Unitas' initial pass was intercepted and returned for a touchdown. Then he botched a hand-off on his next play, a fumble recovered by the Bears. Unitas rebounded quickly from that 58–27 loss, leading the Colts to an upset of Green Bay and their first win over Cleveland. He threw nine touchdown passes that year, including one in the season finale that started his record 47-game streak. His 55.6-percent completion mark was a rookie record.
In 1957, his first season as the Colts full-time starter at quarterback, Unitas finished first in the NFL in passing yards (2,550) and touchdown passes (24) as he helped lead the Colts to a 7–5 record, the first winning record in franchise history. At season's end, Unitas was named the NFL's Most Valuable Player by the Newspaper Enterprise Association (NEA).
1958: "The Greatest Game Ever Played".
Unitas continued his prowess in 1958 passing for 2,007 yards and 19 touchdowns as the Colts won the Western Conference title. The Colts won the NFL championship under Unitas' leadership on December 28, 1958, by defeating the New York Giants 23–17 in sudden death overtime. It was the first overtime game in NFL history, and is often referred to as the "greatest game ever played." The game, nationally televised by NBC, has been credited for sparking the rise in popularity of professional football during the 1960s.
First MVP in 1959.
In 1959, Unitas was named the NFL's MVP by the Associated Press (AP) and UPI (See: National Football League Most Valuable Player Award) for the first time, leading the NFL in passing yards (2,899), touchdown passes (32) and completions (193). Unitas then led the Colts to a repeat championship, beating the Giants again 31–16 in the title game.
The AP Award was considered the MVP award, adopting that verbiage in 1961. The Associated Press had always called the pre-1961 'Players of the Year"' their MVP until 2008, when it was revealed to them that they had made certain errors in their listings, namely that Jim Brown was the 1958 MVP/Player of the Year rather than Gino Marchetti and that Unitas was the true winner in 1959, not Y.A. Tittle. Rather than correct the winners, the AP "disavowed" that the pre-1961 winners were indeed "MVPs" claiming that it was a different award. Nonetheless, UPI also voted Unitas the top player award and there is little doubt that through sports history "MVP" and "Player of the Year" are terms that are interchangeable.
Beginning of the 1960s.
As the 1960s began, the Colts' fortunes (and win totals) declined. Injuries to key players such as Alan Ameche, Raymond Berry and Lenny Moore were a contributing factor. Unitas' streak of 47 straight games with at least one touchdown pass ended against the Los Angeles Rams in week 11 of the 1960 season. In spite of this, Unitas topped the 3000 yard passing mark for the first time and lead the league in touchdown passes for the fourth consecutive season.
After three middle-of-the-pack seasons, Colts owner Carroll Rosenbloom fired Weeb Ewbank and replaced him with Don Shula, who at the time was the youngest head coach in NFL history (33 years of age when he was hired). The Colts finished 8–6 in Shula's first season at the helm, good enough for only third place in the NFL's Western Conference but they did end the season on a strong note by winning their final three games. The season was very successful for Unitas personally as he led the NFL in passing yards with a career-best total of 3,481 and also led in completions with 237.
Second MVP in 1964.
The 1964 season would see the Colts return to the top of the Western Conference. After dropping their season opener to the Vikings, the Colts ran off 10 straight victories to finish with a 12–2 record. The season was one of Unitas' best as he finished with 2,824 yards passing, a league-best 9.26 yards per pass attempt, 19 touchdown passes and only 6 interceptions. He was named the NFL's Most Valuable Player by the AP and UPI for a second time. However, the season would end on a disappointing note for the Colts as they were upset by the Cleveland Browns in the 1964 NFL Championship Game, losing 27–0.
Unitas resumed his torrid passing in 1965, as he threw for 2,530 yards, 23 touchdowns and finished with a league-high and career best 97.1 passer rating. But he was lost for the balance of the season due to a knee injury in a week 12 loss to the Bears. More postseason heartbreak would follow in 1965. The Colts and Packers finished in a tie for first place in the Western Conference and a one-game playoff was played in Green Bay to decide who would be the conference representative in the 1965 NFL Championship Game. The Colts lost in overtime 13–10 due in large part to a game-tying field goal by Don Chandler that many say was incorrectly ruled good. Backup quarterback Gary Cuozzo also suffered a season-ending injury the following week and it would be running back Tom Matte who filled in as the emergency QB for the regular-season finale and the playoff loss to the Packers.
Unitas, healthy once more, threw for 2748 yards and 22 touchdowns in 1966 in a return to Pro Bowl form. However he posted a league-high 24 interceptions.
Third MVP in 1967.
After once again finishing 2nd in the Western Conference in 1966, the Colts rebounded to finish 11–1–2 in 1967 tying the Los Angeles Rams for the NFL's best record. In winning his third MVP awards from the AP and UPI in 1967 (and his second from the NEA), Unitas had a league-high 58.5 completion percentage and passed for 3,428 yards and 20 touchdowns. He openly complained about having tennis elbow and he threw eight interceptions and only three touchdown passes in the final five games. Once again the season ended in heartbreak for the Colts, as they were shut out of the newly instituted four team NFL playoff after losing the divisional tiebreaker to the Rams, a 34–10 rout in the regular season finale.
Super Bowls and final Colt years.
In the final game of the 1968 preseason, the muscles in Unitas' arm were torn when he was hit by a member of the Dallas Cowboys defense. Unitas wrote in his autobiography that he felt his arm was initially injured by the use of the "night ball" that the NFL was testing for better TV visibility during night games. In a post-game interview the previous year, he noted having constant pain in his elbow for several years prior. He would spend most of the season sitting on the bench. But the Colts still marched to a league-best 13–1 record behind backup quarterback and ultimate 1968 NFL MVP Earl Morrall. Although he was injured through most of the season, Unitas came off the bench to play in Super Bowl III, the famous game where Joe Namath guaranteed a New York Jets win despite conventional wisdom. Unitas' insertion was a desperation move in an attempt to retrieve dominance of the NFL over the upstart AFL. Although the Colts won an NFL Championship in 1968, they lost the Super Bowl to the AFL Champion New York Jets, thus becoming the first ever NFL champions that were not also deemed world champions. Unitas helped put together the Colts' only score, a touchdown late in the game. Unitas also drove the Colts into scoring position following the touchdown and successful onside kick, but for reasons that to this day are unknown, head coach Don Shula eschewed a field goal attempt, which (if successful) would have cut the Jets' lead to 16-10. Despite not playing until late in the third quarter, Unitas still finished the game with more passing yards than the team's starter, Earl Morrall.
After an off-season of rehabilitation on his elbow, Unitas rebounded in 1969, passing for 2342 yards and twelve touchdowns with 20 interceptions. But the Colts finished with a disappointing 8-5-1 record, and missed the playoffs.
In 1970, the NFL and AFL had merged into one league, and the Colts moved to the new American Football Conference, along with the Cleveland Browns and the Pittsburgh Steelers. Unitas threw for 2213 yards and 14 touchdowns while leading the Colts to an 11-2-1 season. In their first rematch with the Jets, Unitas and Namath threw a combined nine interceptions in a 29-22 Colts win. Namath threw 62 passes and broke his hand on the final play of the game, ending his season.
Unitas played well in the AFC playoffs, throwing for 390 yards, three touchdowns, and no interceptions in victories over the Cincinnati Bengals and the Oakland Raiders. In Super Bowl V against the Dallas Cowboys, he was knocked out of the game with a rib injury in the second quarter, soon after throwing a 75-yard touchdown pass (setting a then-Super Bowl record) to John Mackey. However, he had also tossed two interceptions before his departure from the game. Earl Morrall came in to lead the team to a last second, 16-13 victory.
Final years.
In 1971 Unitas split playing time with Morrall, throwing only three touchdown passes. He started both playoff games, a win over the Cleveland Browns that sent the Colts to the AFC Championship game against the Miami Dolphins, which they lost by a score of 21–0. Unitas threw three interceptions in the game, one being returned for a touchdown by safety Dick Anderson.
1972 saw the Colts declining into mediocrity. After losing the season opener, Unitas was involved in the second and final regular season head-to-head meeting with "Broadway" Joe Namath. The first was in 1970 (won by the Colts, 29–22). The last meeting was a memorable one, which took place on September 24, 1972, at Memorial Stadium. Unitas threw for 376 yards and three touchdowns, but Namath upstaged him again, bombing the Colts for 496 yards and six touchdowns in a 44–34 Jets victory – their first over Baltimore since the 1970 merger. After losing four of their first five games, the Colts fired Head Coach Don McCafferty, and benched Unitas.
One of the more memorable moments in football history came on Unitas' last game in a Colts uniform at Memorial Stadium, in a game against the Buffalo Bills. Unitas was not the starter for this game, but the Colts were blowing the Bills out by a score of 28–0 behind Marty Domres; Unitas entered the game due to the fans chanting, "We want Unitas!!!", and a plan devised by head coach John Sandusky to convince Unitas that the starting quarterback was injured. Unitas came onto the field, and proceeded to throw his last pass at home as a Colts player. It was a short pass that wide receiver Eddie Hinton was able to turn into a long touchdown, and the Colts won the game by a score of 35–7.
San Diego, retirement and records.
Unitas was traded to the San Diego Chargers in 1973 after posting a 5-9 record in 1972 with Baltimore, but he was far past his prime. He replaced former Chargers quarterback John Hadl. He was replaced in Baltimore by Marty Domres, who had been acquired from the San Diego Chargers in August, 1972. Domres was ultimately replaced by LSU's Bert Jones, drafted with the number two overall pick in 1973. Unitas started the season with a 38-0 loss to the Washington Redskins. He threw for just 55 yards and 3 interceptions, and was sacked 8 times. His final victory as a starter came against the Buffalo Bills in week two. Unitas was 10-18 for 175 yards, two touchdown passes, and no interceptions in a 34-7 Chargers rout . After the victory against Buffalo, it looked like Unitas still had a chance to win for San Diego. Unitas was clearly not the same player he was years ago, and many were questioning his role as a starter after a loss to the Bengals in week three. Two weeks later, he threw two first-half interceptions, passed for only for 19 yards, and went 2-for-9 against the Pittsburgh Steelers. He was then replaced by rookie quarterback, future Hall-of-Famer Dan Fouts. After having posted a 1-3 record as a starter, Unitas retired in the preseason of 1974.
Unitas finished his 18 NFL seasons with 2,830 completions in 5,186 attempts for 40,239 yards and 290 touchdowns, with 253 interceptions. He also rushed for 1,777 yards and 13 touchdowns. Plagued by arm trouble in his later seasons, he threw more interceptions (64) than touchdowns (38) in 1968-1973. After averaging 215.8 yards per game in his first twelve seasons, his production fell to 124.4 in his final six. His Passer Rating plummeted from 82.9 to 60.4 for the same periods. Even so, Unitas set many passing records during his career. He was the first quarterback to throw for more than 40,000 yards, despite playing during an era when NFL teams played shorter seasons of 12 or 14 games (as opposed to today's 16-game seasons) and prior to modern passing-friendly rules implemented in 1978. His 32 touchdown passes in 1959 were a record at the time, making Unitas the first QB to hit the 30 touchdown mark in a season. His 47-game consecutive touchdown streak between 1956 and 1960 was a record considered by many to be unbreakable. The streak stood for 52 years before being broken by New Orleans Saints quarterback Drew Brees in a game against the San Diego Chargers on October 7, 2012.
Post-playing days.
After his playing days were finished, Unitas settled in Baltimore where he raised his family while also pursuing a career in broadcasting, doing color commentary for NFL games on CBS in the 1970s. He was elected to the Pro Football Hall of Fame in 1979. After Robert Irsay moved the Colts franchise to Indianapolis in 1984, a move reviled to this day in Baltimore as "Bob Irsay's Midnight Ride," Unitas was so outraged that he cut all ties to the relocated team (though his #19 jersey is still retired by the Colts). Some other prominent old-time Colts followed his lead, although many attended the 1975 team's reunion at Lucas Oil Stadium in Indianapolis in 2009. A total of 39 Colts players from that 1975 team attended said reunion in Indianapolis, including Bert Jones and Lydell Mitchell. Unitas asked the Pro Football Hall of Fame on numerous occasions (including on Roy Firestone's "Up Close") to remove his display unless it was listed as belonging to the Baltimore Colts. The Hall-of-Fame has never complied with the request. Unitas donated his Colts memorabilia to the Babe Ruth Museum in Baltimore; they are now on display in the Sports Legends Museum at Camden Yards.
Johnny Unitas was inducted into the American Football Association's Semi Pro Football Hall of Fame in 1987.
Unitas actively lobbied for another NFL team to come to Baltimore. After the Cleveland Browns moved to Baltimore in 1996 and changed their name to the Ravens, Unitas and some of the other old-time Colts attended the Ravens' first game ever against the Raiders on Opening Day at Memorial Stadium. Unitas was frequently seen on the Ravens' sidelines at home games (most prominently in 1998 when the now-Indianapolis Colts played the Ravens) and received a thunderous ovation every time he was pictured on each of the huge widescreens at M&T Bank Stadium. He was often seen on the 30-yard line on the Ravens side. When the NFL celebrated its first 50 years, Unitas was voted the league's best player. Retired Bears quarterback Sid Luckman said of Unitas, "He was better than me, better than Sammy Baugh, better than anyone."
Unitas lived most of the final years of his life severely hobbled. Due to an elbow injury suffered during his playing career, he was unable to use his right hand, and could not perform any physical activity more strenuous than golf due to his artificial knees.
Personal life.
At the age of 21, Unitas was married by his uncle to his high school sweetheart Dorothy Hoelle on November 20, 1954; they lived in Towson and had five children before divorcing. Unitas' second wife was Sandra Lemon, whom he married on June 26, 1972; they had three children, lived in Baldwin, and remained married until Unitas' death on September 11, 2002.
Death.
On September 11, 2002, Unitas died suddenly of a heart attack while working out at the Kernan Physical Therapy Center (now The University of Maryland Rehabilitation & Orthopaedic Institute) in Baltimore. After his death, many fans of the Baltimore Ravens petitioned the renaming of the Ravens' home stadium (owned by the State of Maryland) after Unitas. These requests were unsuccessful since the lucrative naming rights had already been leased by the Ravens to Buffalo-based M&T Bank. However, a statue of Unitas was erected as the centerpiece of the plaza in front of the stadium named in Unitas' honor. Large banners depicting the NFL Hall of Famer in his Baltimore Colts heyday flank the entrance to the stadium. Towson University, where Unitas was a major fund-raiser and which his children attended, named its football and lacrosse complex Johnny Unitas Stadium in recognition of both his football career and service to the University.
Toward the end of his life, Unitas brought media attention to the many permanent physical disabilities that he and his fellow players suffered during their careers before heavy padding and other safety features became popular. Unitas himself lost almost total use of his right hand, with the middle finger and thumb noticeably disfigured from being repeatedly broken during games.
He is buried at Dulaney Valley Memorial Gardens in Timonium, Maryland.

</doc>
<doc id="15645" url="https://en.wikipedia.org/wiki?curid=15645" title="John Jacob Astor">
John Jacob Astor

John Jacob Astor (July 17, 1763 – March 29, 1848), born Johann Jakob Astor, was a German–American businessman, merchant, fur trader, and investor who was the first prominent member of the Astor family and the first multi-millionaire in the United States. He was the creator of the first trust in the United States. 
Born in Germany, Astor emigrated to England as a teenager and worked as a musical instrument manufacturer. He moved to the United States after the American Revolutionary War. He entered the fur trade and built a monopoly, managing a business empire that extended to the Great Lakes region and Canada, and later expanded into the American West and Pacific coast. Seeing the decline of demand, he got out of the fur trade in 1830, diversifying by investing in New York City real estate and later becoming a famed patron of the arts. 
Biography.
Early life.
Johann Jakob Astor was born in Walldorf near Heidelberg in the old Palatinate. He was the youngest son of Johann Jakob Astor (July 7, 1724 – April 18, 1816) and Maria Magdalena Vorfelder (1730–1766). His three elder brothers were Georg Peter (later "George"; April 28, 1752 – December 1813), Heinrich (later "Henry"; 1754–1833), and Melchior (1759–1829). Astor's father was a butcher; Johann first worked in his father's shop and as a dairy salesman. In 1779, at the age of 16, he moved to London to join his brother George in working for an uncle's piano and flute manufactory, Astor & Broadwood. While there, he learned English and anglicized his name.
Immigration to the United States.
In 1783 or March 1784, Astor immigrated to New York City, just following the end of the American Revolution, He rented a room from Sarah Cox Todd, a widow and began a flirtation with his landlady's daughter, Sara Cox Todd. His intent was to join his brother Henry, who had established a butcher shop there, but a chance meeting with a fur trader on his voyage inspired him to join the North American fur trade as well. After working at his brother's shop for a time he began to purchase raw hides from Native Americans, prepare them himself, and then resell them in London and elsewhere at great profit. He opened his own fur goods shop in New York in the late 1780s and also served as the New York agent of his uncle's musical instrument business.
Fortune from fur trade.
Astor took advantage of the Jay Treaty between England and the United States in 1794, which opened new markets in Canada and the Great Lakes region. In London, Astor at once made a contract with the North West Company, who from Montreal rivaled the trade interests of the Hudson's Bay Company, then based in London. 
Astor imported furs from Montreal to New York and shipped them to Europe. By 1800, he had amassed almost a quarter of a million dollars, and had become one of the leading figures in the fur trade. His agents worked throughout the western areas and were ruthless in competition. In 1800, following the example of the "Empress of China", the first American trading vessel to China, Astor traded furs, teas, and sandalwood with Canton in China, and greatly benefited from it.
The U.S. Embargo Act in 1807, however, disrupted Astor's import/export business because it closed off trade with Canada. With the permission of President Thomas Jefferson, Astor established the American Fur Company on April 6, 1808. He later formed subsidiaries: the Pacific Fur Company, and the Southwest Fur Company (in which Canadians had a part), in order to control fur trading in the Great Lakes areas and Columbia River region. His Columbia River trading post at Fort Astoria (established in April 1811) was the first United States community on the Pacific coast. He financed the overland Astor Expedition in 1810–12 to reach the outpost. Members of the expedition were to discover South Pass, through which hundreds of thousands of settlers on the Oregon, Mormon, and California trails passed through the Rocky Mountains.
Astor's fur trading ventures were disrupted during the War of 1812, when the English captured his trading posts. In 1816, he joined the opium-smuggling trade. His American Fur Company purchased ten tons of Turkish opium, then shipped the contraband item to Canton on the packet ship "Macedonian". Astor later left the China opium trade and sold solely to England.
Astor's business rebounded in 1817 after the U.S. Congress passed a protectionist law that barred foreign fur traders from U.S. territories. The American Fur Company came to dominate trading in the area around the Great Lakes. John Jacob Astor had a townhouse at 233 Broadway in Manhattan and a country estate, Hellgate in Northern New York City. In 1822, Astor established the Astor House on Mackinac Island where his Broadway townhouse had once stood, as headquarters for the reorganized American Fur Company, making the island a metropolis of the fur trade. A lengthy description based on documents, diaries, etc. was given by Washington Irving in his travelogue "Astoria". Astor's commercial connections extended over the entire globe, and his ships were found in every sea. And he and Sarah moved to a townhouse on Prince Street in Manhattan, New York.
Real estate and retirement.
Astor began buying land in New York in 1799 and acquired sizable holdings along the waterfront. After the start of the 19th century, flush with China trade profits, he became more systematic, ambitious, and calculating by investing in New York real estate. In 1803, he bought a 70-acre farm that ran west of Broadway to the Hudson River between 42nd and 46th streets. That same year, and the following year, he bought considerable holdings from the disgraced Aaron Burr.
In the 1830s, Astor foresaw that the next big boom would be the build-up of New York, which would soon emerge as one of the world's greatest cities. Astor withdrew from the American Fur Company, as well as all his other ventures, and used the money to buy and develop large tracts of Manhattan real estate. Astor correctly predicted New York's rapid growth northward on Manhattan Island, and he purchased more and more land beyond the then-existing city limits. Astor rarely built on his land, but leased it to others for rent and their use. After retiring from his business, Astor spent the rest of his life as a patron of culture. He supported the ornithologist John James Audubon in his studies, art work, and travels, and the presidential campaign of Henry Clay.
Marriage and family.
On September 19, 1785, Astor married Sarah Cox Todd (1762–1842), the daughter of Scottish immigrants Adam Todd and Sarah Cox. Although she brought him a dowry of only $300, she possessed a frugal mind and a business judgment that he declared better than that of most merchants. She assisted him in the practical details of his business.
They had eight children:
Fraternal organizations.
Astor was a Freemason, and served as Master of Holland Lodge #8, New York City in 1788. Later he served as Grand Treasurer for the Grand Lodge of New York.
Legacy.
At the time of his death in 1848, Astor was the wealthiest person in the United States, leaving an estate estimated to be worth at least $20 million. His estimated net worth, if calculated as a fraction of the U.S. gross domestic product at the time, would have been equivalent to $110.1 billion in 2006 U.S. dollars, making him the fifth-richest person in American history. 
In his will, Astor bequeathed $400,000 to build the Astor Library for the New York public, which was later consolidated with other libraries to form the New York Public Library. He also left $50,000 for a poorhouse and orphanage in his German hometown of Walldorf. The "Astorhaus" is now operated as a museum honoring Astor and serves as a renowned fest hall for marriages. Astor donated $25,000 to the German Society of the City of New York, whose chairman he was from 1837 until 1841. Also, he bequeathed $30,000 for a professor's chair in German literature at Columbia University, but due to differences he had with the deanship, he erased this donation from the testament.
Astor left the bulk of his fortune to his second son William, because his eldest son, John Jr., was sickly and mentally unstable. Astor left enough money to care for John Jr. for the rest of his life. Astor is buried in Trinity Church Cemetery in Manhattan, New York. Many members of his family had joined its congregation but Astor remained a member of the local German Reformed congregation to his death. Herman Melville used Astor as a symbol of men who made the earliest fortunes in New York in his novella, "Bartleby, the Scrivener".
The pair of marble lions that sit by the entrance of the New York Public Library at Fifth Avenue and 42nd Street were originally named Leo Astor and Leo Lenox, after Astor and James Lenox, who founded the library from his own collection. Next, they were called Lord Astor and Lady Lenox (both lions are males). Mayor Fiorello La Guardia renamed them "Patience" and "Fortitude" during the Great Depression.
In 1908, when the association football club FC Astoria Walldorf was formed in Astor's birthplace in Germany, the group added "Astoria" to its name in his, and the family's, honor.

</doc>
<doc id="15651" url="https://en.wikipedia.org/wiki?curid=15651" title="Julian calendar">
Julian calendar

The Julian calendar, introduced by Julius Caesar in 46 BC (708 AUC), was a reform of the Roman calendar. It took effect in 45 BC (709 AUC), shortly after the Roman conquest of Egypt. It was the predominant calendar in the Roman world, most of Europe, and in European settlements in the Americas and elsewhere, until it was refined and gradually replaced by the Gregorian calendar, promulgated in 1582 by Pope Gregory XIII. The Julian calendar gains against the mean tropical year at the rate of one day in 128 years. For the Gregorian the figure is one day in 3,226 years. The difference in the average length of the year between Julian (365.25 days) and Gregorian (365.2425 days) is 0.002%.
The Julian calendar has a regular year of 365 days divided into 12 months, as listed in Table of months. A leap day is added to February every four years. The Julian year is, therefore, on average 365.25 days long. It was intended to approximate the tropical (solar) year. Although Greek astronomers had known, at least since Hipparchus, a century before the Julian reform, that the tropical year was a few minutes shorter than 365.25 days, the calendar did not compensate for this difference. As a result, the year gained about three days every four centuries compared to observed equinox times and the seasons. This discrepancy was corrected by the Gregorian reform of 1582. The Gregorian calendar has the same months and month lengths as the Julian calendar, but, in the Gregorian calendar, years evenly divisible by 100 are not leap years, except that years evenly divisible by 400 remain leap years. Consequently, the Julian calendar is currently (since the beginning of March 1900 and until the end of February 2100) 13 days behind the Gregorian calendar; for instance, 1 January in the Julian calendar is 14 January in the Gregorian.
The Julian calendar has been replaced as the civil calendar by the Gregorian calendar in all countries which formerly used it, although it continued to be the civil calendar of some countries into the 20th century. Among the last countries to convert to the Gregorian calendar were Russia (in 1918) and Greece (in 1923). By 1930, all countries that were using the Julian calendar had discontinued it. Most Christian denominations in the West and areas evangelized by Western churches have also replaced the Julian calendar with the Gregorian as the basis for their liturgical calendars. However, most branches of the Eastern Orthodox Church still use the Julian calendar for calculating the dates of moveable feasts, including Easter (Pascha). Some Orthodox churches have adopted the Revised Julian calendar for the observance of fixed feasts, while other Orthodox churches retain the Julian calendar for all purposes. The Julian calendar is still used by the Berber people of North Africa, and on Mount Athos. In the form of the Alexandrian calendar, it is the basis for the Ethiopian calendar, which is the civil calendar of Ethiopia.
During the changeover between calendars and for some time afterwards, dual dating was used in documents and gave both old style and new style dates.
Motivation.
The ordinary year in the previous Roman calendar consisted of 12 months, for a total of 355 days. In addition, a 27- or 28-day intercalary month, the Mensis Intercalaris, was sometimes inserted between February and March. This intercalary month was formed by inserting 22 or 23 days after the first 23 days of February; the last five days of February, which counted down toward the start of March, became the last five days of Intercalaris. The net effect was to add 22 or 23 days to the year, forming an intercalary year of 377 or 378 days. Some say the "mensis intercalaris" always had 27 days and began on either the first or the second day after the Terminalia (23 February).
According to the later writers Censorinus and Macrobius, the ideal intercalary cycle consisted of ordinary years of 355 days alternating with intercalary years, alternately 377 and 378 days long. In this system, the average Roman year would have had days over four years, giving it an average drift of one day per year relative to any solstice or equinox. Macrobius describes a further refinement whereby, in one 8-year period within a 24-year cycle, there were only three intercalary years, each of 377 days (thus 11 intercalary years out of 24). This refinement averages the length of the year to 365.25 days over 24 years.
In practice, intercalations did not occur systematically according to any of these ideal systems, but were determined by the Pontifices. So far as can be determined from the historical evidence, they were much less regular than these ideal schemes suggest. They usually occurred every second or third year, but were sometimes omitted for much longer, and occasionally occurred in two consecutive years.
If managed correctly this system could have allowed the Roman year to stay roughly aligned to a tropical year. However, since the Pontifices were often politicians, and because a Roman magistrate's term of office corresponded with a calendar year, this power was prone to abuse: a Pontifex could lengthen a year in which he or one of his political allies was in office, or refuse to lengthen one in which his opponents were in power.
If too many intercalations were omitted, as happened after the Second Punic War and during the Civil Wars, the calendar would drift out of alignment with the tropical year. Moreover, because intercalations were often determined quite late, the average Roman citizen often did not know the date, particularly if he or she was some distance from the city. For these reasons, the last years of the pre-Julian calendar were later known as "years of confusion". The problems became particularly acute during the years of Julius Caesar's pontificate before the reform, 63–46 BC, when there were only five intercalary months (instead of eight), none of which were during the five Roman years before 46 BC.
Caesar's reform was intended to solve this problem permanently, by creating a calendar that remained aligned to the sun without any human intervention. This proved useful very soon after the new calendar came into effect. Varro used it in 37 BC to fix calendar dates for the start of the four seasons, which would have been impossible only 8 years earlier. A century later, when Pliny dated the winter solstice to 25 December because the sun entered the 8th degree of Capricorn on that date, this stability had become an ordinary fact of life.
Context of the reform.
The approximation of days for the tropical year had been known for a long time but was not used directly since ancient calendars were not solar although the Egyptian calendar and Iranian calendar did come very close with unadjusted years of 365 days. The octaeteris, a cycle of 8 lunar years popularized by Cleostratus (and also commonly attributed to Eudoxus) which was used in some early Greek calendars, notably in Athens, is 1.53 days longer than eight Julian years. The length of nineteen years in the cycle of Meton was 6,940 days, six hours longer than the mean Julian year. The mean Julian year was the basis of the 76-year cycle devised by Callippus (a student under Eudoxus) to improve the Metonic cycle.
In Persia (Iran) after the reform in the Persian calendar by introduction of the Persian Zoroastrian (i. e. Young Avestan) calendar in 503 BC and afterwards, the first day of the year (1 Farvardin=Nowruz) slipped against the vernal equinox at the rate of approximately one day every four years.
Likewise in Egypt, a fixed year of 365 days was in use, drifting by one day against the sun in four years. An unsuccessful attempt to add an extra day every fourth year was made in 238 BC (Decree of Canopus). Caesar probably experienced this "wandering" or "vague" calendar in that country. He landed in the Nile delta in October 48 BC and soon became embroiled in the Ptolemaic dynastic war, especially after Cleopatra managed to be "introduced" to him in Alexandria.
Caesar imposed a peace, and a banquet was held to celebrate the event. Lucan depicted Caesar talking to a wise man called Acoreus during the feast, stating his intention to create a calendar more perfect than that of Eudoxus (Eudoxus was popularly credited with having determined the length of the year to be days). But the war soon resumed and Caesar was attacked by the Egyptian army for several months until he achieved victory. He then enjoyed a long cruise on the Nile with Cleopatra before leaving the country in June 47 BC.
Caesar returned to Rome in 46 BC and, according to Plutarch, called in the best philosophers and mathematicians of his time to solve the problem of the calendar. Pliny says that Caesar was aided in his reform by the astronomer Sosigenes of Alexandria who is generally considered the principal designer of the reform. Sosigenes may also have been the author of the astronomical almanac published by Caesar to facilitate the reform. Eventually, it was decided to establish a calendar that would be a combination between the old Roman months, the fixed length of the Egyptian calendar, and the days of the Greek astronomy. According to Macrobius, Caesar was assisted in this by a certain Marcus Flavius.
Julian reform.
Realignment of the year.
The first step of the reform was to realign the start of the calendar year (1 January) to the tropical year by making 46 BC (708 AUC) 445 days long, compensating for the intercalations which had been missed during Caesar's pontificate. This year had already been extended from 355 to 378 days by the insertion of a regular intercalary month in February. When Caesar decreed the reform, probably shortly after his return from the African campaign in late Quintilis (July), he added 67 more days by inserting two extraordinary intercalary months between November and December.
These months are called "Intercalaris Prior" and "Intercalaris Posterior" in letters of Cicero written at the time; there is no basis for the statement sometimes seen that they were called "Undecimber" and "Duodecimber". Their individual lengths are unknown, as is the position of the Nones and Ides within them.
Because 46 BC was the last of a series of irregular years, this extra-long year was, and is, referred to as the "last year of confusion". The new calendar began operation after the realignment had been completed, in 45 BC.
Changes to the months.
The Julian months were formed by adding ten days to a regular pre-Julian Roman year of 355 days, creating a regular Julian year of 365 days. Two extra days were added to January, Sextilis (August) and December, and one extra day was added to April, June, September and November. February was not changed in ordinary years, and so continued to be the traditional 28 days. Thus, the ordinary (i.e., non leap year) lengths of all of the months were set by the Julian calendar to the same values they still hold today. (See Sacrobosco's theory on month lengths below for stories purporting otherwise).
The Julian reform did not change the method used to account days of the month in the pre-Julian calendar, based on the Kalends, Nones and Ides, nor did it change the positions of these three dates within the months. Macrobius states that the extra days were added immediately before the last day of each month to avoid disturbing the position of the established religious ceremonies relative to the Nones and Ides of the month. However, since Roman dates after the Ides of the month counted down toward the start of the next month, the extra days had the effect of raising the initial value of the count of the day following the Ides in the lengthened months. Thus, in January, Sextilis and December the 14th day of the month became a.d. XIX Kal. instead of a.d. XVII Kal., while in April, June, September and November it became a.d. XVIII Kal.
Romans of the time born after the Ides of a month responded differently to the effect of this change on their birthdays. Mark Antony kept his birthday on 14 January, which changed its date from a.d. XVII Kal. Feb to a.d. XIX Kal. Feb, a date that had previously not existed. Livia kept the date of her birthday unchanged at a.d. III Kal. Feb, which moved it from 28 to 30 January, a day that had previously not existed. Augustus kept his on 23 September, but both the old date (a.d. VIII Kal. Oct.) and the new (a.d. IX Kal. Oct.) were celebrated in some places.
The inserted days were all initially characterised as "dies fasti" (F – see Roman calendar). The character of a few festival days was changed. In the early Julio-Claudian period a large number of festivals were decreed to celebrate events of dynastic importance, which caused the character of the associated dates to be changed to NP. However, this practice was discontinued around the reign of Claudius, and the practice of characterising days fell into disuse around the end of the first century AD: the Antonine jurist Gaius speaks of "dies nefasti" as a thing of the past.
Intercalation.
The old intercalary month was abolished. The new leap day was dated as "ante diem bis sextum Kalendas Martias" ('the sixth doubled before the Kalends of March'), usually abbreviated as "a.d. bis VI Kal. Mart."; hence it is called in English the bissextile day. The year in which it occurred was termed "annus bissextus", in English the bissextile year.
There is debate about the exact position of the bissextile day in the early Julian calendar. The earliest direct evidence is a statement of the 2nd century jurist Celsus, who states that there were two halves of a 48-hour day, and that the intercalated day was the "posterior" half. An inscription from AD 168 states that "a.d. V Kal. Mart." was the day after the bissextile day. The 19th century chronologist Ideler argued that Celsus used the term "posterior" in a technical fashion to refer to the earlier of the two days, which requires the inscription to refer to the whole 48-hour day as the bissextile. Some later historians share this view. Others, following Mommsen, take the view that Celsus was using the ordinary Latin (and English) meaning of "posterior". A third view is that neither half of the 48-hour "bis sextum" was originally formally designated as intercalated, but that the need to do so arose as the concept of a 48-hour day became obsolete.
There is no doubt that the bissextile day eventually became the earlier of the two days for most purposes. In 238 Censorinus stated that it was inserted after the Terminalia (23 February) and was followed by the last five days of February, i.e., a.d. VI, V, IV, III and prid. Kal. Mart. (which would be 24 to 28 February in a common year and the 25th to 29th in a leap year). Hence he regarded the bissextum as the first half of the doubled day. All later writers, including Macrobius about 430, Bede in 725, and other medieval computists (calculators of Easter) followed this rule, as does the liturgical calendar of the Roman Catholic Church. However, Celsus' definition continued to be used for legal purposes. It was incorporated into Justinian's Digest, and in the English statute "De anno et die bissextili" of 1236, which was not formally repealed until 1879.
The effect of the bissextile day on the nundinal cycle is not discussed in the sources. According to Dio Cassius, a leap day was inserted in 41 BC to ensure that the first market day of 40 BC did not fall on 1 January, which implies that the old 8-day cycle was not immediately affected by the Julian reform. However, he also reports that in AD 44, and on some previous occasions, the market day was changed to avoid a conflict with a religious festival. This may indicate that a single nundinal letter was assigned to both halves of the 48-hour bissextile day by this time, so that the Regifugium and the market day might fall on the same date but on different days. In any case, the 8-day nundinal cycle began to be displaced by the 7-day week in the first century AD, and dominical letters began to appear alongside nundinal letters in the fasti.
During the late Middle Ages days in the month came to be numbered in consecutive day order. Consequently, the leap day was considered to be the last day in February in leap years, i.e., 29 February, which is its current position.
Sacrobosco's theory on month lengths.
The Julian reform set the lengths of the months to their modern values. However, a 13th-century scholar, Sacrobosco, proposed a different explanation for the lengths of Julian months which is still widely repeated but is certainly wrong.
According to Sacrobosco, the month lengths for ordinary years in the Roman Republican calendar, from January to December, were:
Sacrobosco then thought that Julius Caesar added one day to every month except February, a total of 11 more days to regular months, giving the ordinary Julian year of 365 days. A single leap day could now be added to this extra short February:
He then said Augustus changed this, by taking one day from February to add it to Sextilis, and then modifying the alternation of the following months, to:
so that the length of "Augustus" (August) would not be shorter than (and therefore inferior to) the length of "Iulius" (July), giving us the irregular month lengths which are still in use.
There is abundant evidence disproving this theory. First, a wall painting of a Roman calendar predating the Julian reform has survived, which confirms the literary accounts that the months were already irregular before Julius Caesar reformed them, with an ordinary year of 355 days, not 354, with month lengths arranged as:
Also, the Julian reform did not change the dates of the Nones and Ides. In particular, the Ides were late (on the 15th rather than 13th) in March, May, July and October, showing that these months always had 31 days in the Roman calendar, whereas Sacrobosco's theory requires that March, May and July were originally 30 days long and that the length of October was changed from 29 to 30 days by Caesar and to 31 days by Augustus. Further, Sacrobosco's theory is explicitly contradicted by the 3rd and 5th century authors Censorinus and Macrobius, and it is inconsistent with seasonal lengths given by Varro, writing in 37 BC, before Sextilis was renamed for Augustus in 8 BC, with the 31-day Sextilis given by an Egyptian papyrus from 24 BC, and with the 28-day February shown in the "Fasti Caeretani", which is dated before 12 BC.
Adoption of the Julian calendar.
Caesar's reform only applied to the Roman calendar. However, in the following decades many of the local civic and provincial calendars of the empire and neighbouring client kingdoms were aligned to the Julian calendar by transforming them into calendars with years of 365 days with an extra day intercalated every four years. The reformed calendars typically retained many features of the unreformed calendars. In many cases, the New Year was not on 1 January, the leap day was not on the bissextile day, the old month names were retained, the lengths of the reformed months did not match the lengths of Julian months, and, even if they did, their first days did not match the first day of the corresponding Julian month. Nevertheless, since the reformed calendars had fixed relationships to each other and to the Julian calendar, the process of converting dates between them became quite straightforward, through the use of conversion tables known as "hemerologia". Several of the reformed calendars are only known through surviving hemerologia.
The three most important of these calendars are the Alexandrian calendar, the Asian calendar and the Syro-Macedonian calendar. Other reformed calendars are known from Cappadocia, Cyprus and the cities of Syria and Palestine. Most reformed calendars were adopted under Augustus, though the calendar of Nabatea was reformed after the kingdom became the Roman province of Arabia in AD 106. There is no evidence that local calendars were aligned to the Julian calendar in the Western empire. Unreformed calendars continued to be used in Gaul, Greece, Macedon, the Balkans and parts of Palestine, most notably in Judea.
The Alexandrian calendar adapted the Egyptian calendar by adding a 6th epagomenal day as the last day of the year in every fourth year, falling on 29 August preceding a bissextile day. It was otherwise identical to the Egyptian calendar. The first leap day was in 22 BC, and they occurred every four years from the beginning, even though Roman leap days occurred every three years at this time (see Leap year error). This calendar influenced the structure of several other reformed calendars, such as those of the cities of Gaza and Ascalon in Palestine, Salamis in Cyprus, and the province of Arabia. It was adopted by the Coptic church and remains in use both as the liturgical calendar of the Coptic church and as the civil calendar of Ethiopia.
The Asian calendar was an adaptation of the Macedonian calendar used in the province of Asia and, with minor variations, in nearby cities and provinces. It is known in detail through the survival of decrees promulgating it issued in 8BC by the proconsul Paullus Fabius Maximus. It renamed the first month Dios as "Kaisar", and arranged the months such that each month started on the ninth day before the kalends of the corresponding Roman month; thus the year began on 23 September, Augustus' birthday. Since Greek months typically had 29 or 30 days, the 31st day of 31 day months was named "Sebaste"—the emperor's day—and was the first day of these months. The leap day was a second Sebaste day in the month of Xandikos, i.e., 24 February. This calendar remained in use at least until the middle of the fifth century AD.
The SyroMacedonian calendar was an adaptation of the Macedonian calendar used in Antioch and other parts of Syria. The months were exactly aligned to the Julian calendar, but they retained their Macedonian names and the year began in Dios = November until the fifth century, when the start of the year was moved to Gorpiaios = September.
These reformed calendars generally remained in use until the fifth or sixth century. Around that time most of them were replaced as civil calendars by the Julian calendar, but with a year starting in September to reflect the year of the indiction cycle.
The Julian calendar spread beyond the borders of the Roman Empire through its use as the Christian liturgical calendar. When a people or a country was converted to Christianity, they generally also adopted the Christian calendar of the church responsible for conversion. Thus, Christian Nubia and Ethiopia adopted the Alexandrian calendar, while Christian Europe adopted the Julian calendar, in either the Catholic or Orthodox variant. Starting in the 16th century, European settlements in the Americas and elsewhere likewise inherited the Julian calendar of the mother country, until they adopted the Gregorian reform. The last country to adopt the Julian calendar was the Ottoman Empire, which had used it for financial purposes for some time, but adopted it officially as the civil calendar (the "Rumi calendar") in 1840.
Leap year error.
Although the new calendar was much simpler than the pre-Julian calendar, the pontifices initially added a leap day every three years, instead of every four. There are accounts of this in Solinus, Ammianus, and Censorinus.
Macrobius gives the following account of the introduction of the Julian calendar:
"Caesar’s regulation of the civil year to accord with his revised measurement was proclaimed publicly by edict, and the arrangement might have continued to stand had not the correction itself of the calendar led the priests to introduce a new error of their own; for they proceeded to insert the intercalary day, which represented the four quarter-days, at the beginning of each fourth year instead of at its end, although the intercalation ought to have been made at the end of each fourth year and before the beginning of the fifth.
"This error continued for thirty-six years by which time twelve intercalary days had been inserted instead of the number actually due, namely nine. But when this error was at length recognised, it too was corrected, by an order of Augustus, that twelve years should be allowed to pass without an intercalary day, since the sequence of twelve such years would account for the three days which, in the course of thirty-six years, had been introduced by the premature actions of the priests."
So, according to Macrobius,
Some people have had different ideas as to how the leap years went. The above scheme is that of Scaliger in the table below. He established that the Augustan reform was instituted in 8 BC. The table shows for each reconstruction the implied proleptic Julian date for the first day of Caesar's reformed calendar (Kal. Ian. AUC 709) and the first Julian date on which the Roman calendar date matches the Julian calendar after the completion of Augustus' reform.
Alexander Jones claims that the correct Julian calendar was in use in Egypt in 24 BC, implying that the first day of the reform in both Egypt and Rome, , was the Julian date 1 January if 45 BC was a leap year and 2 January if it was not. This necessitates fourteen leap days up to and including AD 8 if 45 BC was a leap year and thirteen if it was not.
Pierre Brind'Amour argued that "only one day was intercalated between 1/1/45 and 1/1/40 (disregarding a momentary 'fiddling' in December of 41 to avoid the nundinum falling on Kal. Ian."
In 1999 a papyrus was discovered which gives the dates of astronomical phenomena in 24 BC in both the Egyptian and Roman calendars. From Egypt had two calendars: the old Egyptian in which every year had 365 days and the new Alexandrian in which every fourth year had 366 days. Up to the date in both calendars was the same. The dates in the Alexandrian and Julian calendars are in one-to-one correspondence except for the period from 29 August in the year preceding a Julian leap year to the following 24 February. concluded that the Egyptian astronomers (as opposed to travellers from Rome) used the correct Julian calendar.
An inscription has been discovered which orders a new calendar to be used in Asia to replace the previous Greek lunar calendar. According to one translation
"Intercalation shall commence on the day after 14 Peritius IX Kal. Feb, which would have been 15 Peritius as it is currently constituted in the third year following promulgation of the decree. Xanthicus shall have 32 days in this intercalary year."
This is historically correct. It was decreed by the proconsul that the first day of the year in the new calendar shall be Augustus' birthday, a.d. IX Kal. Oct. Every month begins on the ninth day before the kalends. The date of introduction, the day after 14 Peritius, was 1 Dystrus, the next month. The month after that was Xanthicus. Thus Xanthicus began on a.d. IX Kal. Mart., and normally contained 31 days. In leap year, however, it contained an extra "Sebaste day", the Roman leap day, and thus had 32 days. From the lunar nature of the old calendar we can fix the starting date of the new one as 24 January, in the Julian calendar, which was a leap year. Thus from inception the dates of the reformed Asian calendar are in one-to-one correspondence with the Julian.
Another translation of this inscription is
"Intercalation shall commence on the day after the fourteenth day in the current month of Peritius IX Kal. Feb, occurring every third year. Xanthicus shall have 32 days in this intercalary year."
This would move the starting date back three years to 8 BC, and from the lunar synchronism back to 26 January (Julian). But since the corresponding Roman date in the inscription is 24 January, this must be according to the incorrect calendar which in 8 BC Augustus had ordered to be corrected by the omission of leap days. As the authors of the previous paper point out, with the correct four-year cycle being used in Egypt and the three-year cycle abolished in Rome it is unlikely that Augustus would have ordered the three-year cycle to be introduced in Asia.
Month names.
The Julian reform did not immediately cause the names of any months to be changed. The old intercalary month was abolished and replaced with a single intercalary day at the same point (i.e., five days before the end of February). January continued to be the first month of the year.
The Romans later renamed months after Julius Caesar and Augustus, renaming Quintilis as "Iulius" (July) in 44 BC and Sextilis as "Augustus" (August) in 8 BC. Quintilis was renamed to honour Caesar because it was the month of his birth. According to a "senatus consultum" quoted by Macrobius, Sextilis was renamed to honour Augustus because several of the most significant events in his rise to power, culminating in the fall of Alexandria, occurred in that month.
Other months were renamed by other emperors, but apparently none of the later changes survived their deaths. In AD 37, Caligula renamed September as "Germanicus" after his father; in AD 65, Nero renamed April as "Neroneus", May as "Claudius" and June as "Germanicus"; and in AD 84 Domitian renamed September as "Germanicus" and October as "Domitianus". Commodus was unique in renaming all twelve months after his own adopted names (January to December): "Amazonius", "Invictus", "Felix", "Pius", "Lucius", "Aelius", "Aurelius", "Commodus", "Augustus", "Herculeus", "Romanus", and "Exsuperatorius". The emperor Tacitus is said to have ordered that September, the month of his birth and accession, be renamed after him, but the story is doubtful since he did not become emperor before November 275. Similar honorific month names were implemented in many of the provincial calendars that were aligned to the Julian calendar.
Other name changes were proposed but were never implemented. Tiberius rejected a senatorial proposal to rename September as "Tiberius" and October as "Livius", after his mother Livia. Antoninus Pius rejected a senatorial decree renaming September as "Antoninus" and November as "Faustina", after his empress.
Much more lasting than the ephemeral month names of the post-Augustan Roman emperors were the Old High German names introduced by Charlemagne. According to his biographer, Charlemagne renamed all of the months agriculturally into German. These names were used until the 15th century, over 700 years after his rule, and continued, with some modifications, to see some use as "traditional" month names until the late 18th century. The names (January to December) were: "Wintarmanoth" ("winter month"), "Hornung", "Lentzinmanoth" ("spring month", "Lent month"), "Ostarmanoth" ("Easter month"), "Wonnemanoth" ("joy-month", a corrpution of "Winnimanoth" "pasture-month"), "Brachmanoth" ("fallow-month"), "Heuuimanoth" ("hay month"), "Aranmanoth" ("reaping month"), "Witumanoth" ("wood month"), "Windumemanoth" ("vintage month"), "Herbistmanoth" ("harvest month"), and "Heilagmanoth" ("holy month").
The calendar month names used in western and northern Europe, in Byzantium, and by the Berbers, were derived from the Latin names. However, in eastern Europe older seasonal month names continued to be used into the 19th century, and in some cases are still in use, in many languages, including: Belarusian, Bulgarian, Croatian, Czech, Finnish, Georgian, Lithuanian, Macedonian, Polish, Romanian, Slovene, Ukrainian. When the Ottoman Empire adopted the Julian calendar, in the form of the Rumi calendar, the month names reflected Ottoman tradition.
Year numbering.
The principal method that the Romans used to identify a year for dating purposes was to name it after the two consuls who took office in it, so this eponymous or named year was called the consular year. Since 153 BC, they had taken office on 1 January, the start of the calendar year. The calendar year is the order that the months were listed in calendars or "fasti" displayed on painted walls or on stone tablets, and has been January to December since about 450 BC according to Ovid or since about 713 BC according to Macrobius and Plutarch (see Roman calendar). Julius Caesar did not change the beginning of either the consular year or the calendar year. In addition to consular years, the Romans sometimes used the regnal year of the emperor, and by the late 4th century documents were also being dated according to the 15-year cycle of the indiction. In 537, Justinian required that henceforth the date must include the name of the emperor and his regnal year, in addition to the indiction and the consul, while also allowing the use of local eras.
In 309 and 310, and from time to time thereafter, no consuls were appointed. When this happened, the consular date was given a count of years since the last consul (so-called "post-consular" dating). After 541, only the reigning emperor held the consulate, typically for only one year in his reign, and so post-consular dating became the norm. Similar post-consular dates were also known in the West in the early 6th century. The system of consular dating, long obsolete, was formally abolished in the law code of Leo VI, issued in 888.
Only rarely did the Romans number the year from the founding of the city (of Rome), "ab urbe condita" (AUC). This method was used by Roman historians to determine the number of years from one event to another, not to date a year. Different historians had several different dates for the founding. The "Fasti Capitolini", an inscription containing an official list of the consuls which was published by Augustus, used an epoch of 752 BC. The epoch used by Varro, 753 BC, has been adopted by modern historians. Indeed, Renaissance editors often added it to the manuscripts that they published, giving the false impression that the Romans numbered their years. Most modern historians tacitly assume that it began on the day the consuls took office, and ancient documents such as the "Fasti Capitolini" which use other AUC systems do so in the same way. However, Censorinus, writing in the 3rd century AD, states that, in his time, the AUC year began with the Parilia, celebrated on 21 April, which was regarded as the actual anniversary of the foundation of Rome.
Many local eras, such as the Era of Actium and the Spanish Era, were adopted for the Julian calendar or its local equivalent in the provinces and cities of the Roman Empire. Some of these were used for a considerable time. Perhaps the best known is the Era of Martyrs, sometimes also called "Anno Diocletiani" (after Diocletian), which was associated with the Alexandrian calendar and often used by the Alexandrian Christians to number their Easters during the 4th and 5th centuries, and continues to be used by the Coptic and Ethiopian churches.
In the Eastern Mediterranean, the efforts of Christian chronographers such as Annianus of Alexandria to date the Biblical creation of the world led to the introduction of Anno Mundi eras based on this event. The most important of these was the Etos Kosmou, used throughout the Byzantine world from the 10th century and in Russia until 1700. In the West, the kingdoms succeeding the empire initially used indictions and regnal years, alone or in combination. The chronicler Prosper of Aquitaine, in the fifth century, used an era dated from the Passion of Christ, but this era was not widely adopted. Dionysius Exiguus proposed the system of Anno Domini in 525. This era gradually spread through the western Christian world, once the system was adopted by Bede.
The Julian calendar was also used in some Muslim countries. The Rumi calendar, the Julian calendar used in the later years of the Ottoman Empire, adopted an era derived from the lunar AH year equivalent to AD 1840, i.e., the effective Rumi epoch was AD 585. In recent years, some users of the Berber calendar have adopted an era starting in 950 BC, the approximate date that the Libyan pharaoh Sheshonq I came to power in Egypt.
New Year's Day.
The Roman calendar began the year on 1 January, and this remained the start of the year after the Julian reform. However, even after local calendars were aligned to the Julian calendar, they started the new year on different dates. The Alexandrian calendar in Egypt started on 29 August (30 August after an Alexandrian leap year). Several local provincial calendars were aligned to start on the birthday of Augustus, 23 September. The indiction caused the Byzantine year, which used the Julian calendar, to begin on :1 September; this date is still used in the Eastern Orthodox Church for the beginning of the liturgical year. When the Julian calendar was adopted in AD 988 by Vladimir I of Kiev, the year was numbered Anno Mundi 6496, beginning on 1 March, six months after the start of the Byzantine Anno Mundi year with the same number. In 1492 (AM 7000), Ivan III, according to church tradition, realigned the start of the year to 1 September, so that AM 7000 only lasted for six months in Russia, from 1 March to 31 August 1492.
During the Middle Ages 1 January retained the name "New Year's Day" (or an equivalent name) in all Western European countries (affiliated with the Roman Catholic Church), since the medieval calendar continued to display the months from January to December (in twelve columns containing 28 to 31 days each), just as the Romans had. However, most of those countries began their numbered year on 25 December (the Nativity of Jesus), 25 March (the Incarnation of Jesus), or even Easter, as in France (see the Liturgical year article for more details).
In Anglo-Saxon England, the year most commonly began on 25 December, which, as the winter solstice, had marked the start of the year in pagan times, though 25 March is occasionally documented in the 11th century. Sometimes the start of the year was reckoned as 24 September, the start of the so-called "western indiction" introduced by Bede. These practices changed after the Norman conquest. From 1087 to 1155 the English year began on 1 January, and from 1155 to 1751 on 25 March. In 1752 it was moved back to 1 January.
Even before 1752, 1 January was sometimes treated as the start of the new year – for example by Pepys – while the "year starting 25th March was called the Civil or Legal Year". To reduce misunderstandings on the date, it was not uncommon for a date between 1 January and 24 March to be written as "1661/62". This was to explain to the reader that the year was 1661 counting from March and 1662 counting from January as the start of the year.
Most Western European countries shifted the first day of their numbered year to 1 January while they were still using the Julian calendar, "before" they adopted the Gregorian calendar, many during the 16th century. The following table shows the years in which various countries adopted 1 January as the start of the year. Eastern European countries, with populations showing allegiance to the Orthodox Church, began the year on 1 September from about 988. The Rumi calendar used in the Ottoman Empire began the civil year on 1 March until 1918.
From Julian to Gregorian.
The Julian calendar was in general use in Europe and Northern Africa until 1582, when Pope Gregory XIII promulgated the Gregorian calendar. Reform was required because too many leap days are added with respect to the astronomical seasons on the Julian scheme. On average, the astronomical solstices and the equinoxes advance by about 11 minutes per year against the Julian year. As a result, the calculated date of Easter gradually moved out of alignment with the March equinox.
While Hipparchus and presumably Sosigenes were aware of the discrepancy, although not of its correct value, it was evidently felt to be of little importance at the time of the Julian reform. However, it accumulated significantly over time: the Julian calendar gained a day about every 134 years. By 1582, it was ten days out of alignment from where it supposedly had been in 325 during the Council of Nicaea.
The Gregorian calendar was soon adopted by most Catholic countries (e.g., Spain, Portugal, Poland, most of Italy). Protestant countries followed later, and the countries of Eastern Europe adopted the "new calendar" even later. In the British Empire (including the American colonies), Wednesday was followed by Thursday . For 12 years from 1700 Sweden used a modified Julian calendar, and adopted the Gregorian calendar in 1753, but Russia remained on the Julian calendar until 1918 ( became ), while Greece continued to use it until 1 March 1923 (Gregorian).
Since the Julian and Gregorian calendars were long used simultaneously, although in different places, calendar dates in the transition period are often ambiguous, unless it is specified which calendar was being used. In some circumstances, double dates might be used, one in each calendar. The notation "Old Style" (O.S.) is sometimes used to indicate a date in the Julian calendar, as opposed to "New Style" (N.S.), which either represents the Julian date with the start of the year as 1 January or a full mapping onto the Gregorian calendar. This notation is used to clarify dates from countries which continued to use the Julian calendar after the Gregorian reform, such as Great Britain, which did not switch to the reformed calendar until 1752, or Russia, which did not switch until 1918.
Throughout the long transition period, the Julian calendar has continued to diverge from the Gregorian. This has happened in whole-day steps, as leap days which were dropped in certain centennial years in the Gregorian calendar continued to be present in the Julian calendar. Thus, in the year 1700 the difference increased to 11 days; in 1800, 12; and in 1900, 13. Since 2000 was a leap year according to both the Julian and Gregorian calendars, the difference of 13 days did not change in that year: (Gregorian) fell on (Julian). This difference will persist until the last day of February 2100 (Gregorian), since 2100 is "not" a Gregorian leap year, but "is" a Julian leap year. Monday (Gregorian) falls on Monday (Julian).
Eastern Orthodox usage.
Although all Eastern Orthodox countries (most of them in Eastern or Southeastern Europe) had adopted the Gregorian calendar by 1924, their national churches had not. The "Revised Julian calendar" was proposed during a synod in Constantinople in May 1923, consisting of a solar part which was and will be identical to the Gregorian calendar until the year 2800, and a lunar part which calculated Pascha (Easter) astronomically at Jerusalem. All Orthodox churches refused to accept the lunar part, so almost all Orthodox churches continue to celebrate Pascha according to the Julian calendar (with the exception of the Estonian Orthodox Church
and the Finnish Orthodox Church).
The solar part of the Revised Julian calendar was accepted by only some Orthodox churches. Those that did accept it, with hope for improved dialogue and negotiations with the Western denominations, were the Ecumenical Patriarchate of Constantinople, the Patriarchates of Alexandria, Antioch, the Orthodox Churches of Greece, Cyprus, Romania, Poland (from 1924 to 2014 but remain permitted to use the Revised Julian calendar in parishes that want it), Bulgaria (the last in 1963), and the Orthodox Church in America (although some OCA parishes are permitted to use the Julian calendar). Thus these churches celebrate the Nativity on the same day that Western Christians do, 25 December Gregorian until 2799.
The Orthodox Churches of Jerusalem, Russia, Serbia, Montenegro, Poland (from 15 June 2014), Macedonia, Georgia, Ukraine, and the Greek Old Calendarists and other groups continue to use the Julian calendar, thus they celebrate the Nativity on 25 December Julian (which is 7 January Gregorian until 2100). The Russian Orthodox Church has some parishes in the West which celebrate the Nativity on 25 December Gregorian.
Parishes of the Orthodox Church in America Bulgarian Diocese, both before and after the 1976 transfer of that diocese from the Russian Orthodox Church Outside Russia to the Orthodox Church in America, were permitted to use the 25 December Gregorian date. Some Old Calendarist groups which stand in opposition to the state churches of their homelands will use the Great Feast of the Theophany (6 January Julian/19 January Gregorian) as a day for religious processions and the Great Blessing of Waters, to publicize their cause.
The Oriental Orthodox Churches generally use the local calendar of their homelands. However, when calculating the Nativity Feast, most observe the Julian calendar. This was traditionally for the sake of unity throughout Christendom. In the West, some Oriental Orthodox Churches either use the Gregorian calendar or are permitted to observe the Nativity according to it. The Armenian Apostolic Orthodox Church celebrates the Nativity as part of the Feast of Theophany according to its traditional calendar.

</doc>
<doc id="15654" url="https://en.wikipedia.org/wiki?curid=15654" title="John Quincy Adams">
John Quincy Adams

John Quincy Adams (; July 11, 1767 – February 23, 1848) was an American statesman who served as the sixth President of the United States from 1825 to 1829. He also served as a diplomat, a Senator and member of the House of Representatives. He was a member of the Federalist, Democratic-Republican, National Republican, and later Anti-Masonic and Whig parties.
In his biography, Samuel Flagg Bemis argues that Adams was able to "gather together, formulate, and practice the fundamentals of American foreign-policy – self-determination, independence, noncolonization, nonintervention, nonentanglement in European politics, Freedom of the Seas, freedom of commerce."
John Quincy Adams was the son of President John Adams and Abigail Adams. As a diplomat, Adams played an important role in negotiating key treaties, most notably the Treaty of Ghent, which ended the War of 1812. As Secretary of State, he negotiated with Britain over the United States' northern border with Canada, negotiated with Spain the annexation of Florida, and drafted the Monroe Doctrine. Historians agree he was one of the greatest diplomats and secretaries of state in American history.
As president he sought to modernize the American economy and promote education. Adams enacted a part of his agenda and paid off much of the national debt. However he was stymied time and again by a Congress controlled by his enemies, and his lack of patronage networks helped politicians eager to undercut him. He lost his 1828 bid for re-election to Andrew Jackson.
Adams is best known as a diplomat who shaped U.S. foreign policy in line with his ardently nationalist commitment to U.S. republican values. More recently, he has been portrayed as the exemplar and moral leader in an era of modernization. During Adams' lifetime, technological innovations and new means of communication spread messages of religious revival, social reform, and party politics. Goods, money, and people traveled more rapidly and efficiently than ever before.
Adams was elected as U.S. Representative from Massachusetts after leaving office, serving for the last 17 years of his life with far greater acclaim than he had achieved as president. Animated by his growing revulsion against slavery, Adams became a leading opponent of the Slave Power. He predicted that if a civil war were to break out, the president could abolish slavery by using his powers. Adams also predicted the Union's dissolution over the slavery issue, but said that if the South became independent there would be a series of bloody slave revolts.
Early life, education, and early career.
John Quincy Adams was born on July 11, 1767, to John Adams and his wife Abigail Adams (née Smith) in a part of Braintree, Massachusetts that is now Quincy. John Quincy Adams did not attend school, but was tutored by his cousin James Thax and his father's law clerk, Nathan Rice. He was named for his mother's maternal grandfather, Colonel John Quincy, after whom Quincy, Massachusetts, is named. His namesake great-grandfather died only two days after he was born.
Adams first learned of the Declaration of Independence from the letters his father wrote his mother from the Second Continental Congress in Philadelphia. In 1779, Adams began a diary that he kept until just before he died in 1848. The massive fifty volumes are one of the most extensive collections of first-hand information from the period of the early republic and are widely cited by modern historians.
Much of Adams' youth was spent accompanying his father overseas. John Adams served as an American envoy to France from 1778 until 1779 and to the Netherlands from 1780 until 1782, and the younger Adams accompanied his father on these diplomatic missions. Adams acquired an education at institutions such as Leiden University. He matriculated in Leiden January 10, 1781. For nearly three years, beginning at the age of 14, he accompanied Francis Dana as a secretary on a mission to Saint Petersburg, Russia, to obtain recognition of the new United States. He spent time in Finland, Sweden, and Denmark and, in 1804, published a travel report of Silesia. During these years overseas, Adams became fluent in French and Dutch and became familiar with German and other European languages. Adams, mainly through the influence of his father, had also excelled in classical studies and reached high fluency of Latin and Greek. Upon entering Harvard he had already translated Virgil, Horace, Plutarch, and Aristotle and within six months memorized his Greek grammar and translated the New Testament. He entered Harvard College and graduated in 1787 with a Bachelor of Arts degree, Phi Beta Kappa. Adams House at Harvard College is named in honor of Adams and his father. He later earned an M.A. from Harvard in 1790. He apprenticed as an attorney with Theophilus Parsons in Newburyport, Massachusetts, from 1787 to 1789. He gained admittance to the bar in 1791 and began practicing law in Boston.
Early political career (1796–1817).
Washington administration.
Adams first won national recognition when he published a series of widely read articles supporting Washington's decision to keep America out of the growing hostilities surrounding the French Revolution. Soon after, George Washington appointed Adams minister to the Netherlands (at the age of 26) in 1793. He did not want the position, preferring to maintain his quiet life of reading in Massachusetts, and probably would have rejected it if his father had not persuaded him to take it. On his way to the Netherlands, he was to deliver a set of documents to John Jay, who was negotiating the Jay Treaty. After spending some time with Jay, Adams wrote home to his father, in support of the emerging treaty because he thought America should stay out of European affairs. Historian Paul Nagel has noted that this letter reached Washington, and that parts of it were used by Washington when drafting his farewell address. 
While going back and forth between The Hague and London, he met and proposed to his future wife, Louisa Catherine Johnson. Though he wanted to return to private life at the end of his appointment, Washington appointed him minister to Portugal in 1796, where he was soon appointed to the Berlin Legation. Though his talents were far greater than his desire to serve, he was finally convinced to remain in public service when he learned how highly Washington thought of his abilities. Washington called Adams "the most valuable of America's officials abroad," and Nagel believes that it was at this time that Adams first came to terms with a lifetime of public service.
He became a Fellow of the American Academy of Arts and Sciences in 1797. When the elder Adams became president, he appointed his son in 1797 as Minister to Prussia at Washington's urging. There Adams signed the renewal of the very liberal Prussian-American Treaty of Amity and Commerce after negotiations with Prussian Foreign Minister Count Karl-Wilhelm Finck von Finckenstein. He served at that post until 1801.
While serving abroad, in 1797 Adams also married Louisa Catherine Johnson, the daughter of a poor American merchant, in a ceremony at the church of All Hallows-by-the-Tower, London. Adams remains the only president to have married a First Lady born outside of the United States.
Massachusetts politics.
On his return to the United States, Adams was appointed a Commissioner of Monetary Affairs in Boston by a Federal District Judge; however, Thomas Jefferson rescinded this appointment. He again tried his hand as an attorney, but shortly afterward entered politics. John Quincy Adams was elected a member of the Massachusetts State Senate in April 1802. In November 1802 he ran as a Federalist for the United States House of Representatives and lost.
The Massachusetts General Court elected Adams as a Federalist to the U.S. Senate soon after, and he served from March 4, 1803, until 1808, when he broke with the Federalist Party. Adams, as a senator, had supported the Louisiana Purchase and Jefferson's Embargo Act, actions which made him very unpopular with Massachusetts Federalists. The Federalist-controlled Massachusetts Legislature chose a replacement for Adams on June 3, 1808, several months early. On June 8, Adams broke with the Federalists, resigned his Senate seat, and became a Republican.
Harvard professor.
While a member of the Senate, Adams also served as a professor of logic at Brown University. Disowned by the Federalists and not fully accepted by the Republicans, Adams used his Boylston Professorship of Rhetoric and Oratory at Harvard as a new base. Adams' devotion to classical rhetoric shaped his response to public issues. He remained inspired by classical rhetorical ideals long after the neo-classicalism and deferential politics of the founding generation had been eclipsed by the commercial ethos and mass democracy of the Jacksonian Era. Many of Adams' idiosyncratic positions were rooted in his abiding devotion to the Ciceronian ideal of the citizen-orator "speaking well" to promote the welfare of the polis.
Adams was influenced by the classical republican ideal of civic eloquence espoused by British philosopher David Hume. Adams adapted these classical republican ideals of public oratory to America, viewing the multilevel political structure as ripe for "the renaissance of Demosthenic eloquence." Adams' "Lectures on Rhetoric and Oratory" (1810) looks at the fate of ancient oratory, the necessity of liberty for it to flourish, and its importance as a unifying element for a new nation of diverse cultures and beliefs. Just as civic eloquence failed to gain popularity in Britain, in the United States interest faded in the second decade of the 19th century as the "public spheres of heated oratory" disappeared in favor of the private sphere.
First U.S. minister to Russia.
President James Madison appointed Adams as the first ever United States Minister to Russia in 1809 (though Francis Dana and William Short had previously been nominated to the post, neither presented his credentials at Saint Petersburg). After resigning his post at Harvard, Adams and his wife Louisa boarded a merchant ship in Boston on Aug. 5, 1809. Their youngest son was with them during the long and tedious voyage to St. Petersburg. Their voyage was temporarily interrupted outside the southern coast of Norway due to the Gunboat War. They were at first boarded by a British officer who examined their papers and then, later that day, by a Norwegian officer who ordered the ship to Christiansand. In Christiansand, Adams discovered thirty-eight U.S. vessels which had been detained by the Norwegians and decided to take whatever action necessary to gain the release of both ships and crew as soon as possible. The voyage to St. Petersburg resumed but was once again stopped by a British squadron. Adams showed his commission to Admiral Albermarle Bertie, the commander of the Squadron who recognized the usage of nations and Adams as an ambassador. The usage of nations is common laws of nations founded on custom. Because of the many delays, the Adamses did not arrive in St. Petersburg until October 23, 1809.
Diplomatic relations.
Count Nikolay Rumyantsev, Chancellor of the empire, formally received Adams, and requested a copy of his credential letter. Romanzoff assured Adams that his appointment pleased him personally. Adams presentation to the emperor was postponed however because of the temporary indisposition of Alexander I. Rumyantsev immediately invited Adams to a diplomatic dinner which included the French ambassador, Armand Augustin Louis de Caulaincourt, Duke of Vicenza, numerous foreign ministers then at the Russian Court, and many of the nobility. This was the same mansion Adams had dined in 1781, as secretary of Francis Dana.
Tsar Alexander I received Adams alone in his cabinet where he expressed his pleasure at Adams' appointment. Adams told Alexander that "the President of the United States had desired him to express the hope that his mission would be considered as a proof of respect for the person and character of his majesty, as an acknowledgment of the many testimonies of good-will he had already given to the United States, and of a desire to strengthen commercial relations between them and his provinces." Alexander replied, that, "in everything depending on him, he should be happy to contribute to the increase of their friendly relations; that it was his wish to establish a just system of maritime rights, and that he should adhere invariably to those he had declared." After these official diplomatic greetings, Alexander and Adams discussed several other issues such as the policies of the different European powers, trade and commerce, and other mutually beneficial prospects, and that Russia and U.S. could be very useful to each other.
Adams was also given private audiences with the empress and the dowager empress, who also received Louisa Adams. While not officially a diplomat, Louisa Adams did serve an invaluable role as wife-of-diplomat, becoming a favorite of the czar and making up for her husband's utter lack of charm. She was an indispensable part of the American mission.
Adams requested Rumyantsev to ask Alexander to act on behalf of the United States in securing the release of the American sailors and ships being held by the Danish. The Tsar ordered the Chancellor to request the release of the American property as soon as possible, which the Danish government complied with. Adams spent a great deal of time securing the release of American vessels and seamen from various "seizures and sequestrations."
In 1811, Adams received a commission from the Secretary of State as an Associate Justice of the Supreme Court of the United States. Adams immediately declined and remained in St. Petersburg. In 1812, Adams reported the news of Napoleon's invasion of Russia and Napoleon's disastrous retreat. Also in 1812, Rumyantsev asked if he should request Alexander to mediate a pacification of hostilities between the United States and Great Britain. The U.S. accepted the offer and in July 1813, two associates of Adams, Albert Gallatin and James A. Bayard arrived in St. Petersburg to begin negotiations under mediation by Alexander. Gallatin was at that time Secretary of Treasury and the Senate rejected his appointment to the diplomatic mission as incompatible under the Constitution. However, this rejection did not occur until after Gallatin and Bayard had already left for St. Petersburg. In September, Lord William Cathcart delivered a British memoir to Alexander explaining their reasons for declining the mediation. Thus ended President Madison's hope that Alexander could end the war.
Adams in Russian society.
Adams was well liked by the Russian Court and often would be met on walks by Alexander. The Tsar asked Adams if he would be taking a house in the country over the summer. When Adams hesitated, the emperor stated with good humor that perhaps it was a financial consideration and Adams was able to respond in kind that it was in large part. Adams was a man who endeavored to live within the means provided by the American government.
The Adamses were also provided with several invitations to various entertainments. "The formalities of these court presentations," Mr. Adams remarked, "are so trifling and insignificant in themselves, and so important in the eyes of princes and courtiers, that they are much more embarrassing to an American than business of greater importance. It is not safe or prudent to despise them, nor practicable for a person of rational understanding to value them." Adams was concerned that the various balls and parties given by the Tsar and the foreign ministers, took too much time away from his official duties. Often these events would last until 4 a.m., so Adams stopped attending those he was able to avoid. The numerous diplomatic visits also annoyed Adams who wrote "I have been engaged the whole forenoon; and though I rise at six o'clock, I am sometimes unable to find time to write only part of a private letter in the course of the day. These visits take up so much of my time, that I sometimes think of taking a resolution not to receive them; but, on the other hand, so much information important to be possessed, and particularly relative to current political events, is to be collected from them, that they are rather to be encouraged than discountenanced."
In 1814, Adams was recalled from Russia to serve as chief negotiator of the U.S. commission for the Treaty of Ghent, which ended the War of 1812 between the United States and United Kingdom.
Minister to the Court of St. James's.
Finally, he was sent to be minister to the Court of St. James's (Britain) from 1815 until 1817, a post that was first held by his father. The name is derived from its location at St. James's Palace. In London, Adams was part of a U.S. Legation consisting of himself, two young secretaries and a small office in Craven Street, London WC2. Since they were not particularly well paid, Adams and his wife Louisa lived in Ealing, at that time a village in the countryside, in order to maintain the expensive carriages and liveries which social appearance demanded.
U.S. Secretary of State (1817–1825).
Adams served as Secretary of State in the Cabinet of President James Monroe from 1817 until 1825. Typically, his views concurred with those espoused by Monroe. As Secretary of State, he negotiated the Adams–Onís Treaty (which acquired Florida for the United States), the Treaty of 1818, and wrote the Monroe Doctrine. Many historians regard him as one of the greatest Secretaries of State in American history.
The Floridas, still a Spanish territory but with no Spanish presence to speak of, became a refuge for runaway slaves and Native Americans. Monroe sent in General Andrew Jackson who pushed the Seminole south, executed two British merchants who were supplying weapons, deposed one governor and named another, and left an American garrison in occupation. President Monroe and all his cabinet, except Adams, believed Jackson had exceeded his instructions. Adams argued that since Spain had proved incapable of policing her territories, the United States was obliged to act in self-defense. Adams so ably justified Jackson's conduct that he silenced protests from either Spain or Britain; Congress refused to punish Jackson. Adams used the events that had unfolded in Florida to negotiate the Florida Treaty with Spain in 1819 that turned Florida over to the U.S. and resolved border issues regarding the Louisiana Purchase.
With the ongoing Oregon boundary dispute, Adams sought to negotiate a settlement with England to decide the border between the western United States and Canada. This would become the Treaty of 1818. Along with the Rush–Bagot Treaty of 1817, this marked the beginning of improved relations between the British Empire and its former colonies, and paved the way for better relations between the U.S. and Canada. The treaty had several provisions, but in particular it set the boundary between British North America and the United States along the 49th parallel through the Rocky Mountains. This settled a boundary dispute caused by ignorance of actual geography in the boundary agreed to in the 1783 Treaty of Paris that ended the American Revolutionary War. That earlier treaty had used the Mississippi River to determine the border, but assumed that the river extended further north than it did, and so that earlier settlement was unworkable.
By the time Monroe became president, several European powers, in particular Spain, were attempting to re-establish control over South America. On Independence Day 1821, in response to those who advocated American support for independence movements in many South American countries, Adams gave a speech in which he said that American policy was moral support for independence movements but not armed intervention. Adams foresaw what would befall the United States if it sacrificed its republican spirit on the altar of empire. He stated that America "goes not abroad in search of monsters to destroy" lest she "involve herself beyond power of extrication, in all wars of interest and intrigue, of individual avarice, envy, and ambition, which assume the colors and usurp the standard of freedom. The fundamental maxims of her policy would insensibly change from liberty to force." The United States, Adams warned, might "become the dictatress of the world she would be no longer the ruler of her own spirit." From this, Adams authored what came to be known as the Monroe Doctrine, which was introduced on December 2, 1823. It stated that further efforts by European countries to colonize land or interfere with states in the Americas would be viewed as acts of aggression requiring U.S. intervention. The United States, reflecting concerns raised by the United Kingdom, ultimately hoped to avoid having any European power take over Spain's colonies. It became a defining moment in the foreign policy of the United States and one of its longest-standing tenets, and would be invoked by many U.S. statesmen and several U.S. presidents, including Theodore Roosevelt, Calvin Coolidge, Herbert Hoover, John F. Kennedy, Ronald Reagan and others.
1824 presidential election.
As the 1824 election drew near people began looking for candidates. New England voters admired Adams' patriotism and political skills and it was mainly due to their support that he entered the race. The old caucus system of the Democratic-Republican Party had collapsed; indeed the entire First Party System had collapsed and the election was a fight based on regional support. Adams had a strong base in New England. His opponents included John C. Calhoun, William H. Crawford, Henry Clay, and the hero of New Orleans, Andrew Jackson. During the campaign Calhoun dropped out, and Crawford fell ill giving further support to the other candidates. When Election Day arrived, Andrew Jackson won, although narrowly, pluralities of the popular and electoral votes, but not the necessary majority of electoral votes. With just over one-fourth voter turnout for the election, combined with Adams receiving less than one-third of the popular vote, Adams scored only 113,142 votes.
Under the terms of the Twelfth Amendment, the presidential election fell to the House of Representatives, which was to choose from the top three candidates: Jackson, Adams, and Crawford. Clay had come in fourth place and thus was not on the ballot, but he retained considerable power and influence as Speaker of the House.
Clay's personal dislike for Jackson and the similarity of his American System to Adams' position on tariffs and internal improvements caused him to throw his support to Adams, who was elected by the House on February 9, 1825, on the first ballot. Adams' victory shocked Jackson, who had won the most electoral and popular votes and fully expected to be elected president. When Adams appointed Clay as Secretary of State—the position that Adams and his three predecessors had held before becoming president—Jacksonian Democrats were outraged, and claimed that Adams and Clay had struck a "corrupt bargain". This contention overshadowed Adams' term and greatly contributed to Adams' loss to Jackson four years later, in the 1828 election.
Presidency (1825–1829).
Adams served as the sixth President of the United States from March 4, 1825, to March 4, 1829. He took the oath of office on a book of constitutional law, instead of the more traditional Bible. Adams proposed an elaborate program of internal improvements (roads, ports and canals), a national university, and federal support for the arts and sciences. He favored a high tariff to encourage the building of factories, and restricted land sales to slow the movement west. Opposition from the states' rights faction of a hostile congress killed many of his proposals. He also reduced the national debt from $16 million to $5 million, the remainder of which was paid off by his immediate successor, Andrew Jackson.
Paul Nagel argues that his political acumen was not any less developed than others were in his day, and notes that Henry Clay, one of the era's most astute politicians, was a principal adviser to Adams and supporter throughout his presidency. Nagel argues that Adams' political problems were the result of an unusually hostile Jacksonian faction, and Adams' own dislike of the office. Although a product of the political culture of his day, he refused to play politics according to the usual rules and was not as aggressive in courting political support as he could have been. He was attacked by the followers of Jackson, who accused him of being a partner to a "corrupt bargain" to obtain Clay's support in the election and then appoint him Secretary of State. Jackson defeated Adams in 1828, and created the modern Democratic party thus inaugurating the Second Party System.
Domestic policies.
During his term, Adams worked on transforming America into a world power through "internal improvements," as a part of the "American System". It consisted of a high tariff to fund internal improvements such as road-building, and a national bank to encourage productive enterprise and form a national currency. In his first annual message to Congress, Adams presented an ambitious program for modernization that included roads, canals, a national university, an astronomical observatory, and other initiatives. The support for his proposals was mixed, mainly due to opposition from Jackson's followers.
Some of his proposals were adopted, specifically the extension of the Cumberland Road into Ohio with surveys for its continuation west to St. Louis; the beginning of the Chesapeake and Ohio Canal, the construction of the Chesapeake and Delaware Canal and the Louisville and Portland Canal around the falls of the Ohio; the connection of the Great Lakes to the Ohio River system in Ohio and Indiana; and the enlargement and rebuilding of the Dismal Swamp Canal in North Carolina. One of the issues which divided the administration was protective tariffs, of which Henry Clay was a leading advocate. After Adams lost control of Congress in 1827, the situation became more complicated. By signing into law the Tariff of 1828, quite unpopular in parts of the south, he further antagonized the Jacksonians.
Adams' generous policy toward Native Americans caused him trouble. Settlers on the frontier, who were constantly seeking to move westward, cried for a more expansionist policy. When the federal government tried to assert authority on behalf of the Cherokees, the governor of Georgia took up arms. Adams defended his domestic agenda as continuing Monroe's policies. In contrast, Andrew Jackson and Martin Van Buren instigated the policy of Indian removal to the west (i.e. the Trail of Tears).
Foreign policies.
Adams is regarded as one of the greatest diplomats in American history, and during his tenure as Secretary of State, he was the chief designer of the Monroe Doctrine. He had witnessed the First Barbary War and the Second Barbary War against the Arab pirates of North Africa, and the Greek War of Independence from the Ottoman Turks. Public opinion in the U.S. strongly favored the Greek cause and such leaders as Henry Clay called for intervention. Adams strongly opposed any entanglement in European affairs. According to Charles Edel, Adams believed that, "Intervention would accomplish little, retard the cause of republicanism, and distract the country from its primary goal of continental expansion". Moreover, fearful that U.S. intentions would outstrip its capabilities, Adams thought that projecting U.S. power abroad would weaken its gravitational force on the North American continent.
On July 4, 1821, he gave an address to Congress:
... But she United States of America goes not abroad, in search of monsters to destroy. She is the well-wisher to the freedom and independence of all. She is the champion and vindicator only of her own.
During his term as president, however, Adams achieved little of long-term consequence in foreign affairs. A reason for this was the opposition he faced in Congress, where his rivals prevented him from succeeding. Among his diplomatic achievements were treaties of reciprocity with a number of nations, including Denmark, Mexico, the Hanseatic League, the Scandinavian countries, Prussia and Austria. However, thanks to the successes of Adams' diplomacy during his previous eight years as secretary of state, most of the foreign policy issues he would have faced had been resolved by the time he became president.
Departure.
John Quincy Adams left office on March 4, 1829, after losing the election of 1828 to Andrew Jackson. Adams did not attend the inauguration of his successor, Andrew Jackson, who had openly snubbed him by refusing to pay the traditional "courtesy call" to the outgoing president during the weeks before his own inauguration. He was one of only four presidents who chose not to attend their respective successor's inauguration; the others were his father, Andrew Johnson, and Richard Nixon.
1828 presidential election.
After the inauguration of Adams in 1825, Jackson resigned from his senate seat. For four years he worked hard, with help from his supporters in Congress, to defeat Adams in the presidential election of 1828. The campaign was very much a personal one. As was the tradition of the day and age in American presidential politics, neither candidate personally campaigned, but their political followers organized many campaign events. Both candidates were rhetorically attacked in the press. This reached a low point when the press accused Jackson's wife Rachel of bigamy. She died a few weeks after the elections. Jackson said he would forgive those who insulted him, but he would never forgive the ones who had attacked his wife.
Adams lost the election by a decisive margin. He won all the same states that his father had won in the election of 1800: the New England states, New Jersey, and Delaware, as well as parts of New York and a majority of Maryland. Jackson won the rest of the states, picking up 178 electoral votes to Adams' 83 votes, and succeeded him. Adams and his father were the only U.S. presidents to serve a single term during the first 48 years of the Presidency (1789–1837). Historian Thomas Bailey observed, "Seldom has the public mind been so successfully poisoned against an honest and high-minded man."
Later congressional career (1830–1848).
Adams did not retire after leaving office. Instead he ran for and won a seat in the United States House of Representatives in the 1830 elections. This went against the generally held opinion that former Presidents should not run for public office. He was the first President to serve in Congress after his term of office, and one of only two former presidents to do so (Andrew Johnson later served in the Senate). He was elected to nine terms, serving as a Representative for 17 years, from 1831 until his death.
Adams ran for Governor of Massachusetts in 1833 on the Anti-Masonic ticket. Incumbent National Republican Governor Levi Lincoln, Jr. was retiring and Adams faced National Republican John Davis, Democrat Marcus Morton and Samuel L. Allen of the Working Men's Party. Davis won a plurality, 40%, and Adams took 29% with Morton taking 25% and Allen 6%. Because no candidate had won a majority, the election was sent to the state legislature to decide. Adams withdrew and endorsed Davis, preferring him over Morton, and Davis was chosen by the legislature in January 1834.
In authoring a change to the Tariff of 1828, he was instrumental to the compromise that ended the Nullification Crisis. When James Smithson died and left his estate to the U.S. government to build an institution of learning, many in Congress wanted to use the money for other purposes. Adams was key to ensuring that the money was instead used to build the Smithsonian Institution. In 1839, he was elected a member of the American Antiquarian Society.
Committee assignments.
In Congress, he was chair of the Committee on Commerce and Manufactures, the Committee on Indian Affairs and the Committee on Foreign Affairs.
Slavery.
A longtime opponent of slavery, Adams used his new role in Congress to fight it. In 1836, Southern Representatives voted in a "gag rule" that immediately tabled any petitions about slavery, thus preventing any discussion or debate of the slavery issue. He became a forceful opponent of this rule and conceived a way around it, attacking slavery in the House for two weeks.
The gag rule prevented him from bringing slavery petitions to the floor, but he brought one anyway. It was a petition from a Georgia citizen urging disunion due to the continuation of slavery in the South. Though he certainly did not support it and made that clear at the time, his intent was to antagonize the pro-slavery faction of Congress into an open fight on the matter. The plan worked.
The petition infuriated his Congressional enemies, many of whom were agitating for disunion themselves. They moved for his censure over the matter, enabling Adams to discuss slavery openly during his subsequent defense. Taking advantage of his right to defend himself, Adams delivered prepared and impromptu remarks against slavery and in favor of abolition. Knowing that he would probably be acquitted, he changed the focus from his own actions to those of the slaveholders, speaking against the slave trade and the ownership of slaves. He decided that if he were censured, he would merely resign, run for the office again, and probably win easily. When his opponents realized that they played into his political strategy, they tried to bury the censure. Adams made sure this did not happen, and the debate continued. He attacked slavery and slaveholders as immoral and condemned the institution while calling for it to end. After two weeks, a vote was held, and he was not censured. He delighted in the misery he was inflicting on the slaveholders he so hated, and prided himself on being "obnoxious to the slave faction."
Although the censure of Adams over the slavery petition was ultimately abandoned, the House did address the issue of petitions from enslaved persons at a later time. Adams again argued that the right to petition was a universal right, granted by God, so that those in the weakest positions might always have recourse to those in the most powerful. Adams also called into question the actions of a House that would limit its own ability to debate and resolve questions internally. After this debate, the gag rule was ultimately retained.
The discussion ignited by his actions and the attempts of others to quiet him raised questions of the right to petition, the right to legislative debate, and the morality of slavery. During the censure debate, Adams said that he took delight in the fact that southerners would forever remember him as "the acutest, the astutest, the archest enemy of southern slavery that ever existed".
In 1844, he chaired a committee for reform of the rules of Congress, and he used this opportunity to try once again to repeal the gag rule. He spent two months building support for this move, but due to northern opposition, the rule narrowly survived. He fiercely criticized northern Representatives and Senators, in particular Stephen A. Douglas, who seemed to cater to the slave faction in exchange for southern support. His opposition to slavery made him, along with Henry Clay, one of the leading opponents of Texas annexation and the Mexican–American War. He correctly predicted that both would contribute to civil war. After one of his reelection victories, he said that he must "bring about a day prophesied when slavery and war shall be banished from the face of the earth." He wrote in his private journal in 1820:
In 1841, at the request of Lewis Tappan and Ellis Gray, Adams joined the case of "United States v. The Amistad". Adams went before the Supreme Court on behalf of African slaves who had revolted and seized the Spanish ship "Amistad". Adams appeared on 24 February 1841, and spoke for four hours. His argument succeeded; the Court ruled in favor of the Africans, who were declared free and returned to their homes. Among his opponents was US President Martin Van Buren who had carried the fight over the Amistad slaves to the US Supreme Court}. In the following years, the Spanish government continued to press the US for compensation for the ship, cargo and slaves. Several southern lawmakers introduced resolutions into the United States Congress to appropriate money for such payment but failed to gain passage, although supported by Democratic presidents James K. Polk and James Buchanan.
Photography.
In 1843, Adams sat for the earliest confirmed photograph still in existence of a U.S. president, although other sources contend that William Henry Harrison had posed even earlier for his portrait, in 1841. The original daguerreotype is in the collection of the National Portrait Gallery of the Smithsonian Institution.
Living Bridge between American Revolution and Civil War.
Although there is no indication that the two were close, Adams met Abraham Lincoln during the latter's sole term as a member of the House of Representatives, from 1847 until Adams' death. Thus, it has been suggested that Adams is the only major figure in American history who knew both the Founding Fathers and Abraham Lincoln; however, this is not so as Martin Van Buren met Founding Fathers Thomas Jefferson and John Adams, knew Founder Aaron Burr (Van Buren's mentor), and met the young Lincoln while on a campaign trip through Illinois.
Nullification crisis.
Besides his opposition to slavery and the gag rule (discussed above), his congressional career is remembered for several other key accomplishments. Shortly after Adams entered Congress, the Nullification Crisis threatened civil war over the Tariff of 1828. Adams authored an alteration to the tariff, which weakened it and diffused the crisis. Congress also passed the Force Bill which authorized President Andrew Jackson to use military force if Adams' compromise bill did not force the belligerent states to capitulate. There was no need, however, because Adams' compromise defused the issue. The compromise actually did not alter the tariff as much as the southern states had hoped, though they agreed not to continue pursuing the issue for fear of civil war.
Advancement of science.
Adams also became a leading force for the advancement of science. As president, he had proposed a national observatory, which did not win much support. In 1829 British scientist James Smithson died, and left his fortune for the "increase and diffusion of knowledge." In Smithson's will, he stated that should his nephew, Henry James Hungerford, die without heirs, the Smithson estate would go to the government of the United States to create an "Establishment for the increase & diffusion of Knowledge among men." After the nephew died without heirs in 1835, President Andrew Jackson informed Congress of the bequest, which amounted to about US$500,000 ($75,000,000 in 2008 U.S. dollars after inflation). Adams realized that this might allow the United States to realize his dream of building a national institution of science and learning. Adams thus became Congress' primary supporter of what would become the Smithsonian Institution. He also relentlessly pursued support for astronomical efforts and observatories, seeking a national observatory for the United States. In 1825 Adams signed a bill for the creation of a national observatory just before leaving presidential office. His efforts led to the founding in 1830 of what is now the United States oldest, still-operational scientific institution, the United States Naval Observatory. Adams in fact spent many nights at the Observatory, with celebrated national astronomer and oceanographer Matthew Fontaine Maury, watching and charting the stars, which had always been one of Adams' avocations.
As for efforts to found the Smithsonian Institution, the money was invested in shaky state bonds, which quickly defaulted. After heated debate in Congress, Adams successfully argued to restore the lost funds with interest. Though Congress wanted to use the money for other purposes, Adams successfully persuaded Congress to preserve the money for an institution of science and learning. Congress also debated whether the federal government had the authority to accept the gift, though with Adams leading the initiative, Congress decided to accept the legacy bequeathed to the nation and pledged the faith of the United States to the charitable trust on July 1, 1836.
Likewise Adams project on a national University would not be realized in his lifetime; the American University would be chartered in 1893.
Death.
In 1846, the 78-year-old former president suffered a stroke that left him partially paralyzed. After a few months of rest, he made a full recovery and resumed his duties in Congress. When Adams entered the House chamber, everyone "stood up and applauded." On February 21, 1848, the House of Representatives was discussing the matter of honoring U.S. Army officers who served in the Mexican–American War. Adams had been a vehement critic of the war, and as Congressmen rose up to say, "Aye!" in favor of the measure, he instead yelled, "No!" He rose to answer a question put forth by the Speaker of the House. Immediately thereafter, Adams collapsed, having suffered a massive cerebral hemorrhage. Two days later, on February 23, he died with his wife and youngest son at his side in the Speaker's Room inside the Capitol Building in Washington, D.C. His last words were "This is the last of earth. I am content." He died at 7:20 p.m. An obscure one-term congressman named Abraham Lincoln was assigned to the committee making the funeral arrangements. 
His original interment was temporary, in the public vault at the Congressional Cemetery in Washington, D.C. Later, he was interred in the family burial ground in Quincy, Massachusetts, across from the First Parish Church, called Hancock Cemetery. After Louisa's death in 1852, his son Charles Francis Adams had his parents reinterred in the expanded family crypt in the United First Parish Church across the street, next to John and Abigail. Both tombs are viewable by the public. Adams' original tomb at Hancock Cemetery is still there and marked simply "J.Q. Adams".
Personal life.
John Quincy Adams and Louisa Catherine Adams had three sons and a daughter. Their daughter, Louisa, was born in 1811 but died in 1812 while the family was in Russia. They named their first son George Washington Adams (1801–1829) after the first president. Both George and their second son, John (1803–1834), led troubled lives and died in early adulthood. (George committed suicide and John was expelled from Harvard before his 1823 graduation.)
Adams' youngest son, Charles Francis Adams (who named his own son John Quincy), also pursued a career in diplomacy and politics. In 1870 Charles Francis built the first memorial presidential library in the United States, to honor his father. The Stone Library includes over 14,000 books written in twelve languages. The library is located in the "Old House" at Adams National Historical Park in Quincy, Massachusetts.
John Adams and John Quincy Adams were the only father and son to serve as presidents until George H. W. Bush (1989–1993) and George W. Bush (2001–2009).
Adams has been determined to have the highest I.Q. of any U.S. President.
Legacy.
John Quincy Adams Birthplace is now part of Adams National Historical Park and open to the public. The name Quincy has been used for at least nineteen other places in the United States. Those places were either directly or indirectly named for John Quincy Adams (for example, Quincy, Illinois, was named in honor of Adams while Quincy, California, was named for Quincy, Illinois).
Adams was the first president to have his photograph taken. He also became the first president to adopt a short haircut instead of long hair tied in a queue and to regularly wear long trousers instead of knee breeches. He is probably best known as a diplomat who shaped America's foreign policy in accordance with his ardently nationalist views, and is widely considered by historians to have been one of the greatest diplomats in American history. He was key to the negotiation of several important treaties, such as the Treaty of Ghent, which ended the War of 1812, and the Florida Treaty, which resulted in the annexation of Florida. He also formulated the Monroe Doctrine, which is invoked to the present day. He is viewed by many as the exemplar and moral leader in an era of modernization. During this era, new technologies and networks of infrastructure and communication brought to the people messages of religious revival, social reform, and party politics, as well as moving goods, money, and people ever more rapidly and efficiently.
Though he was always quite hostile to slavery, nearly to be point of being an abolitionist (although he doubted the abolitionists could successfully end slavery), he grew even more hostile to it later in life. Adams became a leading opponent of slave power and articulated a theory whereby the president could abolish slavery by using his war powers, a correct prediction of Abraham Lincoln's use of the Emancipation Proclamation in 1863. Adams predicted the likelihood of the Union's dissolution over the slavery issue, and was a key opponent of the Mexican–American War for this reason. Though he later described his presidency as the unhappiest time of his life, scholars rate John Quincy Adams in the second quartile in the majority of historical presidential rankings.
Historians have often included Adams among the leading conservatives of his day. Russell Kirk, however, sees Adams as a flawed conservative who was imprudent in opposing slavery.
Diaries.
One of Adams' most important legacies is his massive diary, which he began at age 11 with the simple entry "A journal, by me, J.Q.A." The Diary, housed at the Massachusetts Historical Society, covers, in extraordinary detail, Adams' life and experiences up to his death in 1848. The massive fifty volumes are one of the most extensive collections of first-hand information from that period of the early American republic, and are cited by historians in a wide range of matters pertaining to that period.
Personality.
Adams' personality was much like that of his father, as were his political beliefs. Throughout his life, he always preferred reading in seclusion to attending social engagements, and several times had to be pressured by others to remain in public service. Historian Paul Nagel argues that, like Abraham Lincoln after him, Adams suffered from depression for much of his life. Early in his life he sought some form of treatment. Adams thought his depression was due to the high expectations demanded of him by his father and mother. Throughout his life he felt inadequate and socially awkward because of his depression, and was constantly bothered by his physical appearance. He was closer to his father, whom he spent much of his early life with abroad, than he was to his mother. When he was younger and the American Revolution was going on, his mother told her children what their father was doing, and what he was risking, and because of this Adams grew to greatly respect his father. His relationship with his mother was rocky; she had high expectations of him and was afraid her children might end up a dead alcoholic like her brother. As Abigail Adams had feared, John Quincy's brother, Charles, would eventually follow this fate.
John Quincy fell in love shortly after he finished school, but his mother did not approve of his considering marriage when he was still dependent on his parents for support, and the relationship ended. When he fell in love with his future wife, Louisa Johnson, his mother disapproved of this relationship as well. His biographer, Nagel, concludes that this disapproval motivated him to marry Johnson in 1797, despite Adams' reservations that Johnson, like his mother, had a strong personality.
Marquis de Lafayette once gave Adams an alligator as a gift, which he lodged for months in the unfinished East Room of the White House, before building it its own lodge. Reports indicate he enjoyed showing it to visitors.
Antislavery advocacy.
Before 1820, Adams was best known as an exponent of American nationalism. Later in life, especially after his election to the House, he was famous as the most prominent national leader opposing slavery. He was not an abolitionist, say biographers Nagel and Parsons. Remini notes that Adams thought the end of slavery would come by either civil war or the consent of the slave South, but definitely not through the work of abolitionists.
The turning point came with the debate on the Missouri Compromise in 1820 when he broke with his friend John C. Calhoun, who became the most outspoken national leader in favor of slavery. They became bitter enemies. Adams vilified slavery as a terrible evil and preached total abolition, while Calhoun countered that the right to own slaves had to be protected from interference from the federal government to keep the nation alive. Adams said slavery contradicted the principles of republicanism, while Calhoun said that slavery was essential to American democracy, for it made all white men equal. Both men pulled away from nationalism, and started to consider dissolution of the Union as a way of resolving the slavery predicament. Adams predicted that if the South formed a new nation, it would be torn apart by an extremely violent slave insurrection. If the two nations went to war, Adams predicted the president of the United States would use his war powers to abolish slavery. The two men became ideological leaders of the North and the South. In the House Adams became a champion of free speech, demanding that petitions against slavery be heard despite a "gag rule" that said they could not be heard.
In 1841, Adams had the case of a lifetime, representing the defendants in "United States v. The Amistad Africans" in the Supreme Court of the United States. He successfully argued that the Africans, who had seized control of a Spanish ship on which they were being transported illegally as slaves, should not be extradited or deported to Cuba (a Spanish colony where slavery was legal) but should be considered free. Under President Martin Van Buren, the government argued the Africans should be deported for having mutinied and killed officers on the ship. Adams won their freedom, with the chance to stay in the United States or return to Africa. Adams made the argument because the U.S. had prohibited the international slave trade, although it allowed internal slavery. He never billed for his services in the "Amistad" case. The speech was directed not only at the justices of this Supreme Court hearing the case, but also to the broad national audience he instructed in the evils of slavery.
Adams repeatedly spoke out against the "Slave Power", that is the organized political power of the slave owners who dominated all the southern states and their representation in Congress. He vehemently attacked the annexation of Texas (1845) and the Mexican War (1846–48) as part of a "conspiracy" to extend slavery.
Film and television.
Adams occasionally is featured in the mass media. In the PBS miniseries "The Adams Chronicles" (1976), he was portrayed by David Birney, William Daniels, Marcel Trenchard, Steven Grover and Mark Winkworth. He was also portrayed by Anthony Hopkins in the 1997 film "Amistad", and again by Ebon Moss-Bachrach and Steven Hinkle in the 2008 HBO television miniseries "John Adams"; the HBO series received criticism for needless historical and temporal distortions in its portrayal.

</doc>
<doc id="15655" url="https://en.wikipedia.org/wiki?curid=15655" title="Jurassic">
Jurassic

The Jurassic (; from Jura Mountains) is a geologic period and system that extends from 199.6 Mya (million years ago) to 145.5 Mya; from the end of the Triassic to the beginning of the Cretaceous. The Jurassic constitutes the middle period of the Mesozoic Era, also known as the Age of Reptiles. The start of the period is marked by the major Triassic–Jurassic extinction event. Two other extinction events occurred during the period: the Pliensbachian/Toarcian event in the Early Jurassic, and the Tithonian event at the end; however, neither event ranks among the "Big Five" mass extinctions. 
The Jurassic is named after the Jura Mountains within the European Alps, where limestone strata from the period were first identified.
By the beginning of the Jurassic, the supercontinent Pangaea had begun rifting into two landmasses, Laurasia to the north and Gondwana to the south. This created more coastlines and shifted the continental climate from dry to humid, and many of the arid deserts of the Triassic were replaced by lush rainforests. On land, the fauna transitioned from the Triassic fauna, dominated by both dinosauromorph and crocodylomorph archosaurs, to one dominated by dinosaurs alone. The first birds also appeared during the Jurassic, having evolved from a branch of theropod dinosaurs. Other major events include the appearance of the earliest lizards, and the evolution of therian mammals, including primitive placentals. Crocodilians made the transition from a terrestrial to an aquatic mode of life. The oceans were inhabited by marine reptiles such as ichthyosaurs and plesiosaurs, while pterosaurs were the dominant flying vertebrates.
Etymology.
The chronostratigraphic term "Jurassic" is directly linked to the Jura Mountains. Alexander von Humboldt recognized the mainly limestone dominated mountain range of the Jura Mountains as a separate formation that had not been included in the established stratigraphic system defined by Abraham Gottlob Werner, and he named it "Jurakalk" in 1795. The name "Jura" is derived from the Celtic root "jor", which was Latinised into "juria", meaning forest (i.e., ""Jura"" is forest mountains).
Divisions.
The Jurassic period is divided into the Early Jurassic, Middle, and Late Jurassic epochs. The Jurassic System, in stratigraphy, is divided into the Lower Jurassic, Middle, and Upper Jurassic series of rock formations, also known as "Lias", "Dogger" and "Malm" in Europe. The separation of the term Jurassic into three sections goes back to Leopold von Buch. The faunal stages from youngest to oldest are:
Paleogeography and tectonics.
During the early Jurassic period, the supercontinent Pangaea broke up into the northern supercontinent Laurasia and the southern supercontinent Gondwana; the Gulf of Mexico opened in the new rift between North America and what is now Mexico's Yucatan Peninsula. The Jurassic North Atlantic Ocean was relatively narrow, while the South Atlantic did not open until the following Cretaceous period, when Gondwana itself rifted apart. The Tethys Sea closed, and the Neotethys basin appeared. Climates were warm, with no evidence of glaciation. As in the Triassic, there was apparently no land near either pole, and no extensive ice caps existed.
The Jurassic geological record is good in western Europe, where extensive marine sequences indicate a time when much of the continent was submerged under shallow tropical seas; famous locales include the Jurassic Coast World Heritage Site in southern England and the renowned late Jurassic "lagerstätten" of Holzmaden and Solnhofen in Germany. In contrast, the North American Jurassic record is the poorest of the Mesozoic, with few outcrops at the surface. Though the epicontinental Sundance Sea left marine deposits in parts of the northern plains of the United States and Canada during the late Jurassic, most exposed sediments from this period are continental, such as the alluvial deposits of the Morrison Formation.
The Jurassic was a time of calcite sea geochemistry in which low-magnesium calcite was the primary inorganic marine precipitate of calcium carbonate. Carbonate hardgrounds were thus very common, along with calcitic ooids, calcitic cements, and invertebrate faunas with dominantly calcitic skeletons (Stanley and Hardie, 1998, 1999).
The first of several massive batholiths were emplaced in the northern Cordillera beginning in the mid-Jurassic, marking the Nevadan orogeny. Important Jurassic exposures are also found in Russia, India, South America, Japan, Australasia and the United Kingdom.
In Africa, Early Jurassic strata are distributed in a similar fashion to Late Triassic beds, with more common outcrops in the south and less common fossil beds which are predominated by tracks to the north. As the Jurassic proceeded, larger and more iconic groups of dinosaurs like sauropods and ornithopods proliferated in Africa. Middle Jurassic strata are neither well represented nor well studied in Africa. Late Jurassic strata are also poorly represented apart from the spectacular Tendeguru fauna in Tanzania. The Late Jurassic life of Tendeguru is very similar to that found in western North America's Morrison Formation.
Fauna.
Aquatic and marine.
During the Jurassic period, the primary vertebrates living in the sea were fish and marine reptiles. The latter include ichthyosaurs, who were at the peak of their diversity, plesiosaurs, pliosaurs, and marine crocodiles of the families Teleosauridae and Metriorhynchidae. Numerous turtles could be found in lakes and rivers.
In the invertebrate world, several new groups appeared, including rudists (a reef-forming variety of bivalves) and belemnites. Calcareous sabellids ("Glomerula") appeared in the Early Jurassic. The Jurassic also had diverse encrusting and boring (sclerobiont) communities, and it saw a significant rise in the bioerosion of carbonate shells and hardgrounds. Especially common is the ichnogenus (trace fossil) "Gastrochaenolites".
During the Jurassic period, about four or five of the twelve clades of planktonic organisms that exist in the fossil record either experienced a massive evolutionary radiation or appeared for the first time.
Terrestrial.
On land, various archosaurian reptiles remained dominant. The Jurassic was a golden age for the large herbivorous dinosaurs known as the sauropods—"Camarasaurus", "Apatosaurus", "Diplodocus", "Brachiosaurus", and many others—that roamed the land late in the period; their mainstays were either the prairies of ferns, palm-like cycads and bennettitales, or the higher coniferous growth, according to their adaptations. They were preyed upon by large theropods, such as "Ceratosaurus", "Megalosaurus", "Torvosaurus" and "Allosaurus". All these belong to the 'lizard hipped' or saurischian branch of the dinosaurs.
During the Late Jurassic, the first avialans, like "Archaeopteryx", evolved from small coelurosaurian dinosaurs. Ornithischian dinosaurs were less predominant than saurischian dinosaurs, although some, like stegosaurs and small ornithopods, played important roles as small and medium-to-large (but not sauropod-sized) herbivores. In the air, pterosaurs were common; they ruled the skies, filling many ecological roles now taken by birds. Within the undergrowth were various types of early mammals, as well as tritylodonts, lizard-like sphenodonts, and early lissamphibians.
There was actually no Tyrannosaurus rex, despite what the movies of "Jurassic Park" show. The rest of the Lissamphibia evolved in this period, introducing the first salamanders and caecilians.
Flora.
The arid, continental conditions characteristic of the Triassic steadily eased during the Jurassic period, especially at higher latitudes; the warm, humid climate allowed lush jungles to cover much of the landscape. Gymnosperms were relatively diverse during the Jurassic period. The Conifers in particular dominated the flora, as during the Triassic; they were the most diverse group and constituted the majority of large trees.
Extant conifer families that flourished during the Jurassic included the Araucariaceae, Cephalotaxaceae, Pinaceae, Podocarpaceae, Taxaceae and Taxodiaceae. The extinct Mesozoic conifer family Cheirolepidiaceae dominated low latitude vegetation, as did the shrubby Bennettitales. Cycads, similar to palm trees, were also common, as were ginkgos and Dicksoniaceous tree ferns in the forest. Smaller ferns were probably the dominant undergrowth. Caytoniaceous seed ferns were another group of important plants during this time and are thought to have been shrub to small-tree sized. Ginkgo plants were particularly common in the mid- to high northern latitudes. In the Southern Hemisphere, podocarps were especially successful, while Ginkgos and Czekanowskiales were rare.
In the oceans, modern coralline algae appeared for the first time. However, they were a part of another major extinction that happened within the next major time period.

</doc>
<doc id="15656" url="https://en.wikipedia.org/wiki?curid=15656" title="John Wyndham">
John Wyndham

John Wyndham Parkes Lucas Beynon Harris (; 10 July 1903 – 11 March 1969) was an English science fiction writer who usually used the pen name John Wyndham, although he also used other combinations of his names, such as John Beynon and Lucas Parkes. Many of his works were set in post-apocalyptic landscapes.
Biography.
Early life.
Wyndham was born in the village of Dorridge near Knowle, Warwickshire (now West Midlands), England, the son of George Beynon Harris, a barrister, and Gertrude Parkes, the daughter of a Birmingham ironmaster.
His early childhood was spent in Edgbaston in Birmingham, but when he was 8 years old his parents separated and he and his brother, the writer Vivian Beynon Harris, spent the rest of their childhood at a number of English preparatory and public schools, including Blundell's School in Tiverton, Devon, during World War I. His longest and final stay was at Bedales School near Petersfield in Hampshire (1918–21), which he left at the age of 18, and where he blossomed and was happy.
After leaving school, Wyndham tried several careers, including farming, law, commercial art and advertising, but mostly relied on an allowance from his family. He eventually turned to writing for money in 1925 and, by 1931, was selling short stories and serial fiction to American science fiction magazines, most under the pen names of "John Beynon" or "John Beynon Harris", although he also wrote some detective stories.
World War II.
During World War II, Wyndham first served as a censor in the Ministry of Information, then joined the British Army, serving as a Corporal cipher operator in the Royal Corps of Signals. He participated in the Normandy landings, although he was not involved in the first days of the operation.
Postwar.
After the war, Wyndham returned to writing, inspired by the success of his brother, who had four novels published. He altered his writing style; and, by 1951, using the John Wyndham pen name for the first time, he wrote the novel "The Day of the Triffids". His pre-war writing career was not mentioned in the book's publicity, and people were allowed to assume that it was a first novel from a previously unknown writer.
The book proved to be an enormous success and established Wyndham as an important exponent of science fiction. During his lifetime, he wrote and published six more novels under the name John Wyndham. In 1963, he married Grace Wilson, whom he had known for more than 20 years; the couple remained married until he died. He and Grace lived for several years in separate rooms at the Penn Club, London and later lived near Petersfield, Hampshire, just outside the grounds of Bedales School.
He died in 1969, aged 65, at his home in Petersfield, survived by his wife and his brother. Subsequently, some of his unsold work was published; and his earlier work was re-published. His archive was acquired by Liverpool University.
Books.
Critical reception.
His reputation rests mainly on the first four of the novels published in his lifetime as by John Wyndham. "The Day of the Triffids" remains his best-known, but some of his readers consider that "The Chrysalids" was really his best.
He also penned several short stories, ranging from hard science fiction to whimsical fantasy. A few have been filmed: "Consider Her Ways", "Random Quest", "Dumb Martian", "Jizzle" (filmed as "Maria") and "Time to Rest" (filmed as "No Place Like Earth"). There is also a radio version of "Survival".
Most of Wyndham's novels have a contemporary 1950s English middle-class setting. Brian Aldiss, another British science fiction writer, has disparagingly labelled some of them as "cosy catastrophes", especially his novel "The Day of the Triffids". The critic LJ Hurst dismissed Aldiss's accusations, pointing out that in "Triffids" the main character witnesses several murders, suicides, and misadventures, and is frequently in mortal danger himself.
References.
Notes
Citations

</doc>
<doc id="15657" url="https://en.wikipedia.org/wiki?curid=15657" title="Jerzy Kosiński">
Jerzy Kosiński

Jerzy Kosiński (; June 14, 1933 – May 3, 1991), born Józef Lewinkopf, was an award-winning Polish-American novelist and two-time President of the American Chapter of P.E.N., who wrote primarily in English. Born in Poland, he survived World War II and, as a young man, emigrated to the U.S., where he became a citizen.
He was known for various novels, among them "The Painted Bird" (1965) and "Being There" (1971), which was adapted as an Academy Award-winning film (1979).
Early life, teaching, and marriage.
Kosiński was born Józef Lewinkopf to Jewish parents in Łódź, Poland. As a child during World War II, he lived in central Poland under a false identity, Jerzy Kosiński, which his father gave to him. A Roman Catholic priest issued him a forged baptismal certificate, and the Lewinkopf family survived the Holocaust thanks to local villagers who offered assistance to Polish Jews, often at great risk. (The penalty for helping Jews in Nazi- Germany-occupied Poland was death for all members of a family and sometimes for the inhabitants of the village). Kosiński's father was assisted not only by town leaders and clergymen, but also by individuals such as Marianna Pasiowa, a member of an underground network that helped Jews evade capture. The family lived openly in Dąbrowa Rzeczycka, near Stalowa Wola, and attended church in nearby Wola Rzeczycka, with the support of villagers in Kępa Rzeczycka. For a time, they were sheltered by a Catholic family in Rzeczyca Okrągła. Jerzy even served as an altar boy in the local church.
After the war ended, Kosiński and his parents moved to Jelenia Góra. By age 22 he had earned two graduate degrees, in history and sociology, at the University of Łódź. He then became an associate professor at the Polish Academy of Sciences. Kosinski also studied in the Soviet Union, and served as a sharpshooter in the Polish Army.
In order to emigrate to the United States in 1957, he created a fake foundation, which supposedly sponsored him. He later claimed he forged the letters from prominent communist authorities guaranteeing his loyal return to Poland, as were then required for anyone leaving the country.
Kosiński first worked at odd jobs to get by, including driving a truck, and he managed to graduate from Columbia University. He became an American citizen in 1965. He also received grants from the Guggenheim Fellowship in 1967 and the Ford Foundation in 1968. In 1970, he won the American Academy of Arts and Letters award for literature. The grants allowed him to write a political non-fiction book that opened new doors of opportunity. He became a lecturer at Yale, Princeton, Davenport, and Wesleyan Universities.
In 1962, Kosiński married an American steel heiress, Mary Hayward Weir. They divorced four years later. Weir died in 1968 from brain cancer, leaving Kosiński out of her will. He would fictionalize his marriage in his novel "Blind Date", speaking of Weir under pseudonym Mary-Jane Kirkland. Kosiński later married Katherina "Kiki" von Fraunhofer, a marketing consultant and a member of the Bavarian nobility.
Death.
Kosiński suffered from multiple illnesses towards the end of his life, and he was under attack from journalists who accused him of plagiarism. By his late 50s, he was suffering from an irregular heartbeat, as well as severe physical and nervous exhaustion.
Kosiński committed suicide on May 3, 1991, by ingesting a lethal amount of alcohol and drugs and wrapping a plastic bag around his head and suffocating to death. His suicide note read: "I am going to put myself to sleep now for a bit longer than usual. Call it Eternity."
Novels.
Kosiński's novels have appeared on "The New York Times" Best Seller list, and have been translated into over 30 languages, with total sales estimated at 70 million in 1991.
"The Painted Bird".
"The Painted Bird", Kosiński's controversial 1965 novel, is a fictional account that depicts the personal experiences of a boy of unknown religious and ethnic background who wanders around unidentified areas of Eastern Europe during World War II and takes refuge among a series of people, many of whom are brutally cruel and abusive, either to him or to others.
Soon after the book was published in the US, Kosiński was accused by the then-Communist Polish government of being anti-Polish, especially following the regime's 1968 anti-Semitic campaign. The book was banned in Poland from its initial publication until the fall of the Communist government in 1989. When it was finally printed, thousands of Poles in Warsaw lined up for as long as eight hours to purchase copies of the work autographed by Kosiński. Polish literary critic and University of Warsaw professor Paweł Dudziak remarked that "in spite of the unclear role of its author,"The Painted Bird" is an achievement in English literature." He stressed that since the book is a work of fiction and does not document real-world events, accusations of anti-Polish sentiment may result only from taking it too literally.
The book received recommendations from Elie Wiesel who wrote in "The New York Times Book Review" that it was "one of the best ... Written with deep sincerity and sensitivity." Richard Kluger, reviewing it for "Harper's Magazine" wrote: "Extraordinary ... literally staggering ... one of the most powerful books I have ever read." Jonathon Yardley, reviewing it for "The Miami Herald", wrote: "Of all the remarkable fiction that emerged from World War II, nothing stands higher than Jerzy Kosiński's "The Painted Bird". A magnificent work of art, and a celebration of the individual will. No one who reads it will forget it; no one who reads it will be unmoved by it."
However, reception of the book was not uniformly positive. After being translated into Polish, it was read by the people with whom the Lewinkopf family lived during the war. They recognized names of Jewish children sheltered by them (who also survived the war), depicted in the novel as victims of abuse by characters based on them. Also, according to Iwo Cyprian Pogonowski, "The Painted Bird" was Kosiński's most successful attempt at profiteering from the Holocaust by maintaining an aura of a chronicle. In addition, several claims that Kosiński committed plagiarism in writing "The Painted Bird" were leveled against him. (See 'Criticism' section, below.)
"Steps".
"Steps" (1968), a novel comprising scores of loosely connected vignettes, won the U.S. National Book Award for Fiction.
American novelist David Foster Wallace described "Steps" as a "collection of unbelievably creepy little allegorical tableaux done in a terse elegant voice that's like nothing else anywhere ever". Wallace continued in praise: "Only Kafka's fragments get anywhere close to where Kosiński goes in this book, which is better than everything else he ever did combined." Samuel Coale, in a 1974 discussion of Kosiński's fiction, wrote that "the narrator of "Steps" for instance, seems to be nothing more than a disembodied voice howling in some surrealistic wilderness."
"Being There".
One of Kosiński's most significant works is "Being There" (1971), a satirical view of the absurd reality of America's media culture. It is the story of Chance the gardener, a man with few distinctive qualities who emerges from nowhere and suddenly becomes the heir to the throne of a Wall Street tycoon and a presidential policy adviser. His simple and straightforward responses to popular concerns are praised as visionary despite the fact that no one actually understands what he is really saying. Many questions surround his mysterious origins, and filling in the blanks in his background proves impossible.
The novel was made into a 1979 movie directed by Hal Ashby, starring Peter Sellers, who was nominated for an Academy Award for the role and Melvyn Douglas, who won the award for Best Supporting Actor. The screenplay was co-authored by award-winning screenwriter Robert C. Jones with Kosiński. The film won the 1981 British Academy of Film and Television Arts (Film) Best Screenplay Award, as well as the 1980 Writers Guild of America Award (Screen) for Best Comedy Adapted from Another Medium. It was also nominated for the 1980 Golden Globes Best Screenplay Award (Motion Picture).
Criticism.
According to Eliot Weinberger, an American writer, essayist, editor and translator, Kosiński was not the author of "The Painted Bird". Weinberger alleged in his 2000 book "Karmic Traces" that Kosiński was not fluent in English at the time of its writing.
In a review of "Jerzy Kosiński: A Biography" by James Park Sloan, D. G. Myers, Associate Professor of English at Texas A&M University wrote "For years Kosinski passed off "The Painted Bird" as the true story of his own experience during the Holocaust. Long before writing it he regaled friends and dinner parties with macabre tales of a childhood spent in hiding among the Polish peasantry. Among those who were fascinated was Dorothy de Santillana, a senior editor at Houghton Mifflin, to whom Kosinski confided that he had a manuscript based on his experiences. Upon accepting the book for publication Santillana said, "It is my understanding that, fictional as the material may sound, it is straight autobiography." Although he backed away from this claim, Kosinski never wholly disavowed it.
M. A. Orthofer addressed Weinberger's assertion by saying: 
"Kosinski was, in many respects, a fake – possibly near as genuine a one as Weinberger could want. (One aspect of the best fakes is the lingering doubt that, possibly, there is some authenticity behind them – as is the case with Kosinski.) Kosinski famously liked to pretend he was someone he wasn't (as do many of the characters in his books), he occasionally published under a pseudonym, and, apparently, he plagiarized and forged left and right."
Kosiński himself addressed these claims in the introduction to the 1976 reissue of "The Painted Bird", saying that "Well-intentioned writers, critics, and readers sought facts to back up their claims that the novel was autobiographical. They wanted to cast me in the role of spokesman for my generation, especially for those who had survived the war; but for me survival was an individual action that earned the survivor the right to speak only for himself. Facts about my life and my origins, I felt, should not be used to test the book's authenticity, any more than they should be used to encourage readers to read "The Painted Bird". Furthermore, I felt then, as I do now, that fiction and autobiography are very different modes."
Plagiarism allegations.
In June 1982, a "Village Voice" report by Geoffrey Stokes and Eliot Fremont-Smith accused Kosiński of plagiarism, claiming that much of his work was derivative of prewar books unfamiliar to English readers, and that "Being There" was a plagiarism of "Kariera Nikodema Dyzmy" — "The Career of Nicodemus Dyzma" — a 1932 Polish bestseller by Tadeusz Dołęga-Mostowicz. They also alleged Kosiński wrote "The Painted Bird" in Polish, and had it secretly translated into English. The report claimed that Kosiński's books had actually been ghost-written by "assistant editors", finding stylistic differences among Kosiński's novels. Kosiński, according to them, had depended upon his free-lance editors for "the sort of composition that we usually call writing." American biographer James Sloan notes that New York poet, publisher and translator, George Reavey, claimed to have written "The Painted Bird" for Kosiński.
The article found a more realistic picture of Kosiński's life during the Holocaust — a view which was supported by biographers Joanna Siedlecka and Sloan. The article asserted that "The Painted Bird," assumed by some to be semi-autobiographical, was largely a work of fiction. The information showed that rather than wandering the Polish countryside, as his fictional character did, Kosiński spent the war years in hiding with Polish Catholics.
Terence Blacker, a profitable English publisher (who helped publish Kosiński's books) and author of children's books and mysteries for adults, wrote in his article published in "The Independent" in 2002: "The significant point about Jerzy Kosiński was that ... his books ... had a vision and a voice consistent with one another and with the man himself. The problem was perhaps that he was a successful, worldly author who played polo, moved in fashionable circles and even appeared as an actor in Warren Beatty's "Reds". He seemed to have had an adventurous and rather kinky sexuality which, to many, made him all the more suspect. All in all, he was a perfect candidate for the snarling pack of literary hangers-on to turn on. There is something about a storyteller becoming rich and having a reasonably full private life that has a powerful potential to irritate so that, when things go wrong, it causes a very special kind of joy."
D. G. Myers responded to Blacker's assertions in his review of "Jerzy Kosinski: A Biography" by James Park Sloan: 
"This theory explains much: the reckless driving, the abuse of small dogs, the thirst for fame, the fabrication of personal experience, the secretiveness about how he wrote, the denial of his Jewish identity. 'There was a hollow space at the center of Kosinski that had resulted from denying his past,' Sloan writes, 'and his whole life had become a race to fill in that hollow space before it caused him to implode, collapsing inward upon himself like a burnt-out star.' On this theory, Kosinski emerges as a classic borderline personality, frantically defending himself against ... all-out psychosis.
Journalist John Corry wrote a 6,000-word feature article in "The New York Times" in November 1982, responding and defending Kosiński, which appeared on the front page of the Arts and Leisure section. Among other things, Corry alleged that reports claiming that "Kosinski was a plagiarist in the pay of the C.I.A. were the product of a Polish Communist disinformation campaign."
Kosiński himself responded that he had never maintained that the book was autobiographical, even though years earlier he confided to Houghton Mifflin editor Santillana that his manuscript "draws upon a childhood spent, by the casual chances of war, in the remotest villages of Eastern Europe." In 1988, he wrote "The Hermit of 69th Street", in which he sought to demonstrate the absurdity of investigating prior work by inserting footnotes for practically every term in the book. "Ironically," wrote theatre critic Lucy Komisar, "possibly his only true book ... about a successful author who is shown to be a fraud."
Despite repudiation of the "Village Voice" allegations in detailed articles in "The New York Times", "The Los Angeles Times", and other publications, Kosiński remained tainted. "I think it contributed to his death," said Zbigniew Brzezinski, a friend and fellow Polish emigrant.
TV, radio, film, and newspaper appearances.
Kosiński appeared 12 times on "The Tonight Show Starring Johnny Carson" during 1971–73, and "The Dick Cavett Show" in 1974, was a guest on the talk radio show of Long John Nebel, posed half-naked for a cover photograph by Annie Leibovitz for "The New York Times Magazine" in 1982, and presented the Oscar for screenwriting in 1982.
He also played the role of Bolshevik revolutionary and Politburo member Grigory Zinoviev in Warren Beatty's film "Reds". The "Time" magazine critic wrote: "As Reed's Soviet nemesis, novelist Jerzy Kosinski acquits himself nicely–a tundra of ice against Reed's all-American fire." "Newsweek" complimented Kosinski's "delightfully abrasive" performance.
Friendships.
Kosiński was friends with Roman Polanski, with whom he attended the National Film School in Łódź, and said he narrowly missed being at Polanski and Sharon Tate's house on the night Tate was murdered by Charles Manson's followers in 1969, due to lost luggage. His novel "Blind Date" discussed the Manson murders.
In 1984, Polanski denied Kosiński's story in his autobiography. Journalist John Taylor of New York Magazine believes Polanski was mistaken. "Although it was a single sentence in a 461-page book, reviewers focused on it. But the accusation was untrue: Jerzy and Kiki "had" been invited to stay with Tate the night of the Manson murders, and they missed being killed as well only because they stopped in New York en route from Paris because their luggage had been misdirected." The reason why Taylor believes this, is that "a friend of Kosiński's wrote a letter to the "Times", which was published in the "Book Review", describing the detailed plans he and Jerzy had made to meet that weekend at Polanski's house on Cielo Drive. Few people saw the letter." The NYM article does not contain the name of this friend, nor the particular issue of the "Book Review" in which this letter is supposed to have been published, nor names of the 'few' who may have read the letter.
Kosiński was also friends with Wojciech Frykowski and Abigail Folger. He introduced the couple.
Kosiński wrote his novel "Pinball" (1982) for his friend George Harrison, having conceived of the idea for the book at least ten years before writing it.
Interests.
Kosiński practiced the photographic arts, with one-man exhibitions to his credit in Warsaw's Crooked Circle Gallery (1957), and in the Andre Zarre Gallery in New York (1988). He watched surgeries, and read to terminally ill patients.
Kosiński was also very interested in polo, and compared himself to a character from his novel "Passion Play": "The character, Fabian, is at the mercy of his aging and his sexual obsession. It's my calling card. I'm 46. I'm like Fabian."
Further reading.
Biographical accounts.
He is the subject of the off-Broadway play "More Lies About Jerzy" (2001), written by Davey Holmes and originally starring Jared Harris as Kosinski-inspired character "Jerzy Lesnewski". The most recent production being produced at the New End Theatre in London starring George Layton.
He also appears as one of the 'literary golems' (ghosts) in Thane Rosenbaum's novel "The Golems of Gotham"

</doc>
<doc id="15658" url="https://en.wikipedia.org/wiki?curid=15658" title="Jeep">
Jeep

Jeep is a brand of American automobiles that is a division of FCA US LLC (formerly Chrysler Group, LLC), a wholly owned subsidiary of Fiat Chrysler Automobiles. The former Chrysler Corporation acquired the Jeep brand, along with the remaining assets of its owner American Motors, in 1987. The division is headquartered in Toledo, Ohio. Jeep's current product range consists solely of sport utility vehicles and off-road vehicles, but has also included pickup trucks in the past.
The original Jeep was the prototype Bantam BRC. Willys MB Jeeps went into production in 1941 specifically for the military, arguably making them the oldest four-wheel drive mass-production vehicles now known as SUVs. The Jeep became the primary light 4-wheel-drive vehicle of the United States Army and the Allies during World War II, as well as the postwar period. The term became common worldwide in the wake of the war. Doug Stewart notes:
The first civilian models were produced in 1945. It inspired a number of other light utility vehicles, such as the Land Rover. Many Jeep variants serving similar military and civilian roles have since been designed in other nations.
Development.
Bantam Reconnaissance Car.
When it became obvious that the United States was eventually going to become involved in the war raging in Europe, the U.S. Army contacted 135 companies asking for working prototypes of a four-wheel-drive reconnaissance car. Only two companies responded to the request: American Bantam Car Company and Willys-Overland. The Army had set what seemed like an impossible deadline of 49 days to supply a working prototype. Willys asked for more time, but was refused. The bankrupt American Bantam Car Company had no engineering staff left on the payroll and solicited Karl Probst, a talented freelance designer from Detroit. After turning down Bantam's initial request, Probst responded to an Army request and commenced work, initially without salary, on July 17, 1940.
Probst laid out full plans for the Bantam prototype, known as the BRC or Bantam Reconnaissance Car, in just two days, working up a cost estimate the next. Bantam's bid was submitted, complete with blueprints, on July 22. While much of the vehicle could be assembled from off-the-shelf automotive parts, custom four-wheel drivetrain components were to be supplied by Spicer. The hand-built prototype was completed in Butler, Pennsylvania, and driven to Camp Holabird, Maryland, for Army testing September 21. The vehicle met all the Army's criteria except engine torque.
Willys MA and Ford GP.
The Army felt that the Bantam company was too small to supply the number of vehicles it needed, so it supplied the Bantam design to Willys and Ford, who were encouraged to make their own changes and modifications. The resulting Ford "Pygmy" and Willys "Quad" prototypes looked very similar to the Bantam BRC prototype, and Spicer supplied very similar four-wheel drivetrain components to all three manufacturers.
Fifteen hundred of each of the three models (Bantam BRC-40, Ford GP, and Willys MA) were built and extensively field-tested. Delmar "Barney" Roos, Willys-Overland's chief engineer, made design changes to meet a revised weight specification (a maximum of , including oil and water). He was thus able to use the powerful but comparatively heavy Willys "Go Devil" engine, and win the initial production contract. The Willys version of the car would become the standardized Jeep design, designated the model MB and was built at their plant in Toledo, Ohio. The familiar pressed-metal Jeep grille was actually a Ford design feature and incorporated in the final design by the Army.
Since the War Department required a large number of vehicles to be manufactured in a relatively short time, Willys-Overland granted the United States Government a non-exclusive license to allow another company to manufacture vehicles using Willys' specifications. The Army chose Ford as the second supplier, building Jeeps to the Willys' design. Willys supplied Ford with a complete set of plans and specifications. American Bantam, the creators of the first Jeep, built approximately 2700 of them to the BRC-40 design, but then spent the rest of the war building heavy-duty trailers for the Army.
Origin of the name.
Many explanations of the origin of the word "jeep" have proven difficult to verify. The most widely held theory is that the military designation "GP" (for "Government Purposes" or "General Purpose") was slurred into the word "Jeep" in the same way that the contemporary "HMMWV" (for "High-Mobility Multi-purpose Wheeled Vehicle") has become known as the Humvee. Joe Frazer, Willys-Overland President from 1939 to 1944, claimed to have coined the word "jeep" by slurring the initials G.P. There are no contemporaneous uses of "GP" before later attempts to create a ""backronym.""
A more detailed view, popularized by R. Lee Ermey on his television series "Mail Call", disputes this "slurred GP" origin, saying that the vehicle was designed for specific duties, and was never referred to as "General Purpose" and it is highly unlikely that the average jeep-driving GI would have been familiar with this designation. The Ford GPW abbreviation actually meant G for government use, P to designate its wheelbase and W to indicate its Willys-Overland designed engine. Ermey suggests that soldiers at the time were so impressed with the new vehicles that they informally named it after Eugene the Jeep, a character in the Popeye comic strip and cartoons created by E. C. Segar, as early as mid-March 1936. Eugene the Jeep was Popeye's "jungle pet" and was "small, able to move between dimensions and could solve seemingly impossible problems."
The word "jeep", however, was used as early as 1914 by US Army mechanics assigned to test new vehicles. In 1937, tractors which were supplied by Minneapolis Moline to the US Army were called jeeps. A precursor of the Boeing B-17 Flying Fortress was also referred to as the jeep.
"Words of the Fighting Forces" by Clinton A. Sanders, a dictionary of military slang, published in 1942, in the library at The Pentagon gives this definition:
This definition is supported by the use of the term "jeep carrier" to refer to the Navy's small escort carriers.
Early in 1941, Willys-Overland demonstrated the vehicle's off-road capability by having it drive up the steps of the United States Capitol, driven by Willys test driver Irving "Red" Haussman, who had recently heard soldiers at Fort Holabird calling it a "jeep." When asked by syndicated columnist Katharine Hillyer for the "Washington Daily News" (or by a bystander, according to another account) what it was called, Irving answered, "It's a jeep."
Katharine Hillyer's article was published nationally on February 19, 1941, and included a picture of the vehicle with the caption:
Although the term was also military slang for vehicles that were untried or untested, this exposure caused all other jeep references to fade, leaving the 4x4 with the name.
The original trademark brand-name application was filed in February 1943 by Willys-Overland. It is also used as a generic term as the lowercase "jeep" for vehicles inspired by the Jeep that are suitable for use on rough terrain.
World War II Jeeps.
Final production version Jeeps built by Willys-Overland were the Model MB, while those built by Ford were the Model GPW ("G"=government vehicle, "P" designated the 80" wheelbase, and "W" = the Willys engine design). There were subtle differences between the two. The versions produced by Ford had every component (including bolt heads) marked with an "F". Willys also followed the Ford pattern by stamping its name into some body parts, but stopped this in 1942. The cost per vehicle trended upwards as the war continued from the price under the first contract from Willys at US$648.74 (Ford's was $782.59 per unit). Willys-Overland and Ford, under the direction of Charles E. Sorensen (Vice-President of Ford during World War II), produced about 640,000 Jeeps towards the war effort, which accounted for approximately 18% of all the wheeled military vehicles built in the U.S. during the war.
Jeeps were used by every service of the U.S. military. An average of 145 were supplied to every Army infantry regiment. Jeeps were used for many purposes, including cable laying, saw milling, as firefighting pumpers, field ambulances, tractors and, with suitable wheels, would even run on railway tracks. An amphibious jeep, the model GPA, or "seep" (Sea Jeep) was built for Ford in modest numbers but it could not be considered a huge success—it was neither a good off-road vehicle nor a good boat. As part of the war effort, nearly 30% of all Jeep production was supplied to Great Britain and to the Soviet Red Army.
Post-war military Jeeps.
The Jeep has been widely imitated around the world, including in France by Delahaye and by Hotchkiss et Cie (after 1954, Hotchkiss manufactured Jeeps under license from Willys), and in Japan by Mitsubishi Motors and Toyota. The utilitarian good looks of the original Jeep have been hailed by industrial designers and museum curators alike. The Museum of Modern Art described the Jeep as a masterpiece of functionalist design, and has periodically exhibited the Jeep as part of its collection. Ernie Pyle called the Jeep, along with the Coleman G.I. Pocket Stove, "the two most important pieces of noncombat equipment ever developed." Jeeps became even more famous following the war, as they became available on the surplus market. Some ads claimed to offer "Jeeps still in the factory crate." This legend persisted for decades, despite the fact that Jeeps were never shipped from the factory in crates (although Ford did "knock down" Jeeps for easier shipping, which may have perpetuated the myth).
The "Jeepney" is a unique type of taxi or bus created in the Philippines. The first Jeepneys were military-surplus MBs and GPWs, left behind in the war-ravaged country following World War II and Filipino independence. Jeepneys were built from Jeeps by lengthening and widening the rear "tub" of the vehicle, allowing them to carry more passengers. Over the years, Jeepneys have become the most ubiquitous symbol of the modern Philippines, even as they have been decorated in more elaborate and flamboyant styles by their owners. Most Jeepneys today are scratch-built by local manufacturers, using different powertrains. Some are even constructed from stainless steel.
In the United States military, the Jeep has been supplanted by a number of vehicles (e.g. Ford's M151 MUTT) of which the latest is the Humvee.
The CJ-V35/U.
After World War II, Jeep began to experiment with new designs, including a model that could drive under water. On February 1, 1950, contract N8ss-2660 was approved for 1,000 units "especially adapted for general reconnaissance or command communications" and "constructed for short period underwater operation such as encountered in landing and fording operations." The engine was modified with a snorkel system so that the engine could properly breathe under water.
The M715.
In 1965, Jeep developed the M715 1.25-ton army truck, a militarized version of the civilian J-series Jeep truck, which served extensively in the Vietnam War. It had heavier full-floating axles and a foldable, vertical, flat windshield. Today, it serves other countries, and is still being produced by Kia under license.
The CJ ("Civilian Jeep") series began in 1945 with the CJ-2A, followed by the CJ-3B in 1953. These early Jeeps are commonly referred to as "flatfenders" because their front fenders were flat across the front, the same as their military precedents, the Willys MB and identical Ford GPW models. The CJ-4 exists only as a 1951 prototype, and is the "missing" link between the flat-fendered CJ-2A and CJ-3B and the round-fendered CJ-5 first introduced in 1955.
The Jeep brand.
The brand has gone through many owners, starting with Willys, which produced the first Civilian Jeep (CJ) in 1945. As the only company that continually produced Jeep vehicles after the war, in June 1950 Willys-Overland was granted the privilege of owning the name "Jeep" as a registered trademark. Willys was sold to Kaiser Motors in 1953, which became Kaiser-Jeep in 1963. American Motors Corporation (AMC) purchased Kaiser's money-losing Jeep operations in 1970. The utility vehicles complemented AMC's passenger car business by sharing components, achieving volume efficiencies, as well as capitalizing on Jeep's international and government markets.
The French automaker Renault began investing in AMC in 1979. However, by 1987, the automobile markets had changed and even Renault itself was experiencing financial troubles. At the same time, Chrysler Corporation wanted to capture the Jeep brand, as well as other assets of AMC. Chrysler bought out AMC in 1987, shortly after the Jeep CJ-7 was replaced with the AMC-designed Jeep Wrangler or YJ. Chrysler merged with Daimler-Benz in 1998 to form DaimlerChrysler. DaimlerChrysler eventually sold most of their interest in Chrysler to a private equity company in 2007. Chrysler and the Jeep division operated under Chrysler Group LLC, until December 15, 2014, when the name was changed to FCA US LLC.
Jeeps have been built under licence by various manufacturers around the world, including Mahindra in India, EBRO in Spain, and several in South America. Mitsubishi built more than 30 different Jeep models in Japan between 1953 and 1998. Most of them were based on the CJ-3B model of the original Willys-Kaiser design.
Toledo, Ohio has been the headquarters of the Jeep brand since its inception, and the city has always been proud of this heritage. Although no longer produced in the same Toledo Complex as the World War II originals, two streets in the vicinity of the old plant are named Willys Parkway and Jeep Parkway. The Jeep Wrangler and Jeep Cherokee are built in the city currently, in separate facilities, not far from the site of the original Willys-Overland plant.
American Motors set up the first automobile-manufacturing joint venture in the People's Republic of China on January 15, 1984. The result was Beijing Jeep Corporation, Ltd., in partnership with Beijing Automobile Industry Corporation, to produce the Jeep Cherokee (XJ) in Beijing. Manufacture continued after Chrysler's buyout of AMC. This joint venture is now part of DaimlerChrysler and DaimlerChrysler China Invest Corporation. The original 1984 XJ model was updated and called the "Jeep 2500" toward the end of its production that ended after 2005.
A division of FCA US LLC, the most recent successor company to the Jeep brand, now holds trademark status on the name "Jeep" and the distinctive 7-slot front grille design. The original 9-slot grille associated with all World War II jeeps was designed by Ford for their GPW, and because it weighed less than the original "Slat Grille" of Willys (an arrangement of flat bars), was incorporated into the "standardized jeep" design.
The history of the HMMWV (Humvee) has ties with Jeep. In 1971, Jeep's Defense and Government Products Division was turned into AM General, a wholly owned subsidiary of American Motors Corporation, which also owned Jeep. In 1979, while still owned by American Motors, AM General began the first steps toward designing the Humvee. AM General also continued manufacturing the two-wheel-drive DJ, which Jeep created in 1953. The General Motors Hummer and Chrysler Jeep have been waging battle in U.S. courts over the right to use seven slots in their respective radiator grilles. Chrysler Jeep claims it has the exclusive rights to use the seven vertical slits since it is the sole remaining assignee of the various companies since Willys gave their postwar jeeps seven slots instead of Ford's nine-slot design for the Jeep.
Off-road abilities.
Jeep advertising has always emphasized the vehicle's off-road capabilities. Today, the Wrangler is one of the few remaining four-wheel-drive vehicles with solid front and rear axles. These axles are known for their durability, strength, and articulation. New Wranglers come with a Dana 44 rear differential and a Dana 30 front differential. The upgraded Rubicon model of the JK Wrangler is equipped with electronically activated locking differentials, Dana 44 axles front and rear with 4.10 gears, a 4:1 transfer case, electronic sway bar disconnect and heavy duty suspension.
Another benefit of solid axle vehicles is they tend to be easier and cheaper to "lift" with aftermarket suspension systems. This increases the distance between the axle and chassis of the vehicle. By increasing this distance, larger tires can be installed, which will increase the ground clearance, allowing it to traverse even larger and more difficult obstacles. In addition to higher ground clearance, many owners aim to increase suspension articulation or "flex" to give their Jeeps greatly improved off-road capabilities. Good suspension articulation keeps all four wheels in contact with the ground and maintains traction.
Useful features of the smaller Jeeps are their short wheelbases, narrow frames, and ample approach, breakover, and departure angles, allowing them to fit into places where full-size four-wheel drives have difficulty.
Jeep model list.
Current models.
The Jeep brand currently produces five models, but 9 vehicles are under the brand name or use the Jeep logo:
Jeeps around the world.
Jeeps have been built and/or assembled around the world by various companies.
Jeep apparel and sponsorships.
Jeep is also a brand of apparel of outdoor lifestyle sold under license. It is reported that there are between 600 and 1,500 such outlets in China, vastly outnumbering the number of Jeep auto dealers in the country.
In April 2012 Jeep signed a shirt sponsorship deal worth €35m ($45m) with Italian football club Juventus.
In August 2014 Jeep signed a sponsorship deal with Greek football club AEK Athens F.C..

</doc>
