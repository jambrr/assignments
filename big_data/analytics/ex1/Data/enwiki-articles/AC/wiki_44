<doc id="15155" url="https://en.wikipedia.org/wiki?curid=15155" title="I. M. Pei">
I. M. Pei

Ieoh Ming Pei (born April 26, 1917), commonly known as I. M. Pei, is a Chinese American architect. In 1948, Pei was recruited by New York real estate magnate William Zeckendorf. There he spent seven years before establishing his own independent design firm I. M. Pei & Associates in 1955, which became I.M. Pei & Partners in 1966 and later in 1989 became Pei Cobb Freed & Partners. Pei retired from full-time practice in 1990. Since then, he has taken on work as an architectural consultant primarily from his sons' architectural firm Pei Partnership Architects. His first major recognition came with the National Center for Atmospheric Research in Colorado; his new stature led to his selection as chief architect for the John F. Kennedy Library in Massachusetts. He went on to design Dallas City Hall and the East Building of the National Gallery of Art.
He returned to China for the first time in 1975 to design a hotel at Fragrant Hills, and designed Bank of China Tower, Hong Kong, a skyscraper in Hong Kong for the Bank of China fifteen years later. In the early 1980s, Pei was the focus of controversy when he designed a glass-and-steel pyramid for the Musée du Louvre in Paris. He later returned to the world of the arts by designing the Morton H. Meyerson Symphony Center in Dallas, the Miho Museum in Japan, the Suzhou Museum in Suzhou, and the Museum of Islamic Art in Qatar.
Pei has won a wide variety of prizes and awards in the field of architecture, including the AIA Gold Medal in 1979, the first "Praemium Imperiale" for Architecture in 1989, and the Lifetime Achievement Award from the Cooper-Hewitt, National Design Museum in 2003. In 1983, he won the Pritzker Prize, sometimes called the Nobel Prize of architecture.
Childhood.
Pei's ancestry traces back to the Ming Dynasty, when his family moved from Anhui province to Suzhou. Finding wealth in the sale of medicinal herbs, the family stressed the importance of helping the less fortunate. Ieoh Ming Pei was born on April 26, 1917 to Tsuyee Pei and Lien Kwun, and the family moved to Hong Kong one year later. The family eventually included five children. As a boy, Pei was very close to his mother, a devout Buddhist who was recognized for her skills as a flautist. She invited him, his brothers, and his sisters to join her on meditation retreats. His relationship with his father was less intimate. Their interactions were respectful but distant.
Pei's ancestors' success meant that the family lived in the upper echelons of society, but Pei said his father was "not cultivated in the ways of the arts". The younger Pei, drawn more to music and other cultural forms than to his father's domain of banking, explored art on his own. "I have cultivated myself", he said later.
At the age of ten, Pei moved with his family to Shanghai after his father was promoted. Pei attended Saint Johns Middle School, run by Protestant missionaries. Academic discipline was rigorous; students were allowed only one half-day each month for leisure. Pei enjoyed playing billiards and watching Hollywood movies, especially those of Buster Keaton and Charlie Chaplin. He also learned rudimentary English skills by reading the Bible and novels by Charles Dickens.
Shanghai's many international elements gave it the name "Paris of the East". The city's global architectural flavors had a profound influence on Pei, from the Bund waterfront area to the Park Hotel, built in 1934. He was also impressed by the many gardens of Suzhou, where he spent the summers with extended family and regularly visited a nearby ancestral shrine. The Shizilin Garden, built in the 14th century by a Buddhist monk, was especially influential. Its unusual rock formations, stone bridges, and waterfalls remained etched in Pei's memory for decades. He spoke later of his fondness for the garden's blending of natural and human-built structures.
Soon after the move to Shanghai, Pei's mother developed cancer. As a pain reliever, she was prescribed opium, and assigned the task of preparing her pipe to Pei. She died shortly after his thirteenth birthday, and he was profoundly upset. The children were sent to live with extended family; their father became more consumed by his work and more physically distant. Pei said: "My father began living his own separate life pretty soon after that." His father later married a woman named Aileen, who moved to New York later in her life.
Education and formative years.
As Pei neared the end of his secondary education, he decided to study at an overseas university. He was accepted to a number of schools, but decided to enroll at the University of Pennsylvania. Pei's choice had two roots. While studying in Shanghai, he had closely examined the catalogs for various institutions of higher learning around the world. The architectural program at the University of Pennsylvania stood out to him. The other major factor was Hollywood. Pei was fascinated by the representations of college life in the films of Bing Crosby, which differed tremendously from the academic atmosphere in China. "College life in the U.S. seemed to me to be mostly fun and games", he said in 2000. "Since I was too young to be serious, I wanted to be part of it ... You could get a feeling for it in Bing Crosby's movies. College life in America seemed very exciting to me. It's not real, we know that. Nevertheless, at that time it was very attractive to me. I decided that was the country for me."
In 1935 Pei boarded the SS "President Coolidge" and sailed to San Francisco, then traveled by train to Philadelphia. What he found, however, differed vastly from his expectations. Professors at the University of Pennsylvania based their teaching in the Beaux-Arts style, rooted in the classical traditions of Greece and Rome. Pei was more intrigued by modern architecture, and also felt intimidated by the high level of drafting proficiency shown by other students. He decided to abandon architecture and transferred to the engineering program at Massachusetts Institute of Technology (MIT). Once he arrived, however, the dean of the architecture school commented on his eye for design and convinced Pei to return to his original major.
MIT's architecture faculty was also focused on the Beaux-Arts school, and Pei found himself uninspired by the work. In the library he found three books by the Swiss-French architect Charles-Édouard Jeanneret-Gris, better known as Le Corbusier. Pei was inspired by the innovative designs of the new International style, characterized by simplified form and the use of glass and steel materials. Le Corbusier visited MIT in , an occasion which powerfully affected Pei: "The two days with Le Corbusier, or 'Corbu' as we used to call him, were probably the most important days in my architectural education." Pei was also influenced by the work of US architect Frank Lloyd Wright. In 1938 he drove to Spring Green, Wisconsin, to visit Wright's famous Taliesin building. After waiting for two hours, however, he left without meeting Wright.
Although he disliked the Beaux-Arts emphasis at MIT, Pei excelled in his studies. "I certainly don't regret the time at MIT", he said later. "There I learned the science and technique of building, which is just as essential to architecture." Pei received his Bachelor of Architecture degree in 1940. He planned to return to China immediately afterward, but the Second Sino-Japanese War forced him to change his plans. Pei's father urged him to remain in the United States, and he worked for two years at the Boston engineering firm of Stone & Webster.
While visiting New York City in the late '30s, Pei met a Wellesley College student named Eileen Loo. They began dating and they married in the spring of 1942. She enrolled in the landscape architecture program at Harvard University, and Pei was thus introduced to members of the faculty at Harvard's Graduate School of Design (GSD). He was excited by the lively atmosphere, and joined the GSD in .
Less than a month later, Pei suspended his work at Harvard to join the National Defense Research Committee, which coordinated scientific research into US weapons technology during World War II. Pei's background in architecture was seen as a considerable asset; one member of the committee told him: "If you know how to build you should also know how to destroy." The fight against Germany was ending, so he focused on the Pacific War. The US realized that its bombs used against the stone buildings of Europe would be ineffective against Japanese cities, mostly constructed from wood and paper; Pei was assigned to work on incendiary bombs. Pei spent two and a half years with the NDRC, but has revealed few details.
In 1945 Eileen gave birth to a son, T'ing Chung; she withdrew from the landscape architecture program in order to care for him. Pei returned to Harvard in the autumn of 1945, and received a position as assistant professor of design. The GSD was developing into a hub of resistance to the Beaux-Arts orthodoxy. At the center were members of the Bauhaus, a European architectural movement that had advanced the cause of modernist design. The Nazi regime had condemned the Bauhaus school, and its leaders left Germany. Two of these, Walter Gropius and Marcel Breuer, took positions at the Harvard GSD. Their iconoclastic focus on modern architecture appealed to Pei, and he worked closely with both men.
One of Pei's design projects at the GSD was a plan for an art museum in Shanghai. He wanted to create a mood of Chinese authenticity in the architecture without using traditional materials or styles. The design was based on straight modernist structures, organized around a central courtyard garden, with other similar natural settings arranged nearby. It was very well received; Gropius, in fact, called it "the best thing done in master class". Pei received his master's degree in 1946, and taught at Harvard for another two years.
Career.
1948–56: Early career with Webb and Knapp.
In the spring of 1948 Pei was recruited by New York real estate magnate William Zeckendorf to join a staff of architects for his firm of Webb and Knapp to design buildings around the country. Pei found Zeckendorf's personality the opposite of his own; his new boss was known for his loud speech and gruff demeanor. Nevertheless, they became good friends and Pei found the experience personally enriching. Zeckendorf was well connected politically, and Pei enjoyed learning about the social world of New York's city planners.
His first project for Webb and Knapp was an apartment building with funding from the Housing Act of 1949. Pei's design was based on a circular tower with concentric rings. The areas closest to the supporting pillar handled utilities and circulation; the apartments themselves were located toward the outer edge. Zeckendorf loved the design and even showed it off to Le Corbusier when they met. The cost of such an unusual design was too high, however, and the building never moved beyond the model stage.
Pei finally saw his architecture come to life in 1949, when he designed a two-story corporate building for Gulf Oil in Atlanta, Georgia. The building was demolished in February 2013 although the front facade will be retained as part of an apartment development. His use of marble for the exterior curtain wall brought praise from the journal "Architectural Forum". Pei's designs echoed the work of Mies van der Rohe in the beginning of his career as also shown in his own weekend-house in Katonah in 1952. Soon Pei was so inundated with projects that he asked Zeckendorf for assistants, which he chose from his associates at the GSD, including Henry N. Cobb and Ulrich Franzen. They set to work on a variety of proposals, including the Roosevelt Field Shopping Mall. The team also redesigned the Webb and Knapp office building, transforming Zeckendorf's office into a circular space with teak walls and a glass clerestory. They also installed a control panel into the desk that allowed their boss to control the lighting in his office. The project took one year and exceeded its budget, but Zeckendorf was delighted with the results.
In 1952 Pei and his team began work on a series of projects in Denver, Colorado. The first of these was the Mile High Center, which compressed the core building into less than twenty-five percent of the total site; the rest is adorned with an exhibition hall and fountain-dotted plazas. One block away, Pei's team also redesigned Denver's Courthouse Square, which combined office spaces, commercial venues, and hotels. These projects helped Pei conceptualize architecture as part of the larger urban geography. "I learned the process of development," he said later, "and about the city as a living organism." These lessons, he said, became essential for later projects. The hyperbolic paraboloid structure was removed when the Denver Pavilions facility was constructed. In 1982, he returned to design the 16th Street Mall, essentially converting a street into a pedestrian only shopping district.
Pei and his team also designed a united urban area for Washington, D.C., L'Enfant Plaza (named for French-American architect Pierre Charles L'Enfant). Pei's associate Araldo Cossutta was the lead architect for the plaza's North Building (955 L'Enfant Plaza SW) and South Building (490 L'Enfant Plaza SW). Vlastimil Koubek was the architect for the East Building (L'Enfant Plaza Hotel, located at 480 L'Enfant Plaza SW), and for the Center Building (475 L'Enfant Plaza SW; now the United States Postal Service headquarters). The team set out with a broad vision that was praised by both the "Washington Post" and "Washington Star" (which rarely agreed on anything), but funding problems forced revisions and a significant reduction in scale.
In 1955 Pei's group took a step toward institutional independence from Webb and Knapp by establishing a new firm called I. M. Pei & Associates. (The name changed later to I. M. Pei & Partners.) They gained the freedom to work with other companies, but continued working primarily with Zeckendorf. The new firm distinguished itself through the use of detailed architectural models. They took on the Kips Bay residential area on the east side of Manhattan, where Pei set up Kips Bay Towers, two large long towers of apartments with recessed windows (to provide shade and privacy) in a neat grid, adorned with rows of trees. Pei involved himself in the construction process at Kips Bay, even inspecting the bags of concrete to check for consistency of color.
The company continued its urban focus with the Society Hill project in central Philadelphia. Pei designed the Society Hill Towers, a three-building residential block injecting cubist design into the 18th-century milieu of the neighborhood. As with previous projects, abundant green spaces were central to Pei's vision, which also added traditional townhouses to aid the transition from classical to modern design.
From 1958 to 1963 Pei and Ray Affleck developed a key downtown block of Montreal in a phased process that involved one of Pei's most admired structures in the commonwealth, the cruciform tower known as the Royal Bank Plaza (Place Ville Marie). According to the Canadian Encyclopedia "its grand plaza and lower office buildings, designed by internationally famous US architect I.M. Pei, helped to set new standards for architecture in Canada in the 1960s ... The tower's smooth aluminum and glass surface and crisp unadorned geometric form demonstrate Pei's adherence to the mainstream of 20th-century modern design."
Although these projects were satisfying, Pei wanted to establish an independent name for himself. In 1959 he was approached by MIT to design a building for its Earth science program. The Green Building continued the grid design of Kips Bay and Society Hill. The pedestrian walkway at the ground floor, however, was prone to sudden gusts of wind, which embarrassed Pei. "Here I was from MIT," he said, "and I didn't know about wind-tunnel effects." At the same time, he designed the Luce Memorial Chapel in at Tunghai University in Taichung, Taiwan. The soaring structure, commissioned by the same organisation that had run his middle school in Shanghai, broke severely from the cubist grid patterns of his urban projects.
The challenge of coordinating these projects took an artistic toll on Pei. He found himself responsible for acquiring new building contracts and supervising the plans for them. As a result, he felt disconnected from the actual creative work. "Design is something you have to put your hand to," he said. "While my people had the luxury of doing one job at a time, I had to keep track of the whole enterprise." Pei's dissatisfaction reached its peak at a time when financial problems began plaguing Zeckendorf's firm. I. M. Pei and Associates officially broke from Webb and Knapp in 1960, which benefited Pei creatively but pained him personally. He had developed a close friendship with Zeckendorf, and both men were sad to part ways.
NCAR and Related Projects.
Pei was able to return to hands-on design when he was approached in 1961 by Walter Orr Roberts to design the new Mesa Laboratory for the National Center for Atmospheric Research outside Boulder, Colorado. The project differed from Pei's earlier urban work; it would rest in an open area in the foothills of the Rocky Mountains. He drove with his wife around the region, visiting assorted buildings and surveying the natural environs. He was impressed by the United States Air Force Academy in Colorado Springs, but felt it was "detached from nature".
The conceptualization stages were important for Pei, presenting a need and an opportunity to break from the Bauhaus tradition. He later recalled the long periods of time he spent in the area: "I recalled the places I had seen with my mother when I was a little boy—the mountaintop Buddhist retreats. There in the Colorado mountains, I tried to listen to the silence again—just as my mother had taught me. The investigation of the place became a kind of religious experience for me."
Pei also drew inspiration from the Mesa Verde cliff dwellings of the Ancient Pueblo Peoples; he wanted the buildings to exist in harmony with their natural surroundings. To this end, he called for a rock-treatment process that could color the buildings to match the nearby mountains. He also set the complex back on the mesa overlooking the city, and designed the approaching road to be long, winding, and indirect.
Roberts disliked Pei's initial designs, referring to them as "just a bunch of towers". Roberts intended his comments as typical of scientific experimentation, rather than artistic critique; still, Pei was frustrated. His second attempt, however, fit Roberts' vision perfectly: a spaced-out series of clustered buildings, joined by lower structures and complemented by two underground levels. The complex uses many elements of cubist design, and the walkways are arranged to increase the probability of casual encounters among colleagues.
Once the laboratory was built, several problems with its construction became apparent. Leaks in the roof caused difficulties for researchers, and the shifting of clay soil beneath caused cracks in the buildings which were expensive to repair. Still, both architect and project manager were pleased with the final result. Pei refers to the NCAR complex as his "breakout building", and he remained a friend of Roberts until the scientist died in .
The success of NCAR brought renewed attention to Pei's design acumen. He was recruited to work on a variety of projects, including the S. I. Newhouse School of Public Communications at Syracuse University, the Sundrome terminal at John F. Kennedy International Airport in New York City, and dormitories at New College of Florida.
Kennedy Library.
After President John F. Kennedy was assassinated in , his family and friends discussed how to construct a library that would serve as a fitting memorial. A committee was formed to advise Kennedy's widow Jacqueline, who would make the final decision. The group deliberated for months and considered many famous architects. Eventually, Kennedy chose Pei to design the library, based on two considerations. First, she appreciated the variety of ideas he had used for earlier projects. "He didn't seem to have just one way to solve a problem," she said. "He seemed to approach each commission thinking only of it and then develop a way to make something beautiful." Ultimately, however, Kennedy made her choice based on her personal connection with Pei. Calling it "really an emotional decision", she explained: "He was so full of promise, like Jack; they were born in the same year. I decided it would be fun to take a great leap with him."
The project was plagued with problems from the outset. The first was scope. President Kennedy had begun considering the structure of his library soon after taking office, and he wanted to include archives from his administration, a museum of personal items, and a political science institute. After the assassination, the list expanded to include a fitting memorial tribute to the slain president. The variety of necessary inclusions complicated the design process and caused significant delays.
Pei's first proposed design included a large glass pyramid that would fill the interior with sunlight, meant to represent the optimism and hope that Kennedy's administration had symbolized for so many in the US. Mrs. Kennedy liked the design, but resistance began in Cambridge, the first proposed site for the building, as soon as the project was announced. Many community members worried that the library would become a tourist attraction, causing particular problems with traffic congestion. Others worried that the design would clash with the architectural feel of nearby Harvard Square. By the mid-70s, Pei tried proposing a new design, but the library's opponents resisted every effort. These events pained Pei, who had sent all three of his sons to Harvard, and although he rarely discussed his frustration, it was evident to his wife. "I could tell how tired he was by the way he opened the door at the end of the day," she said. "His footsteps were dragging. It was very hard for I. M. to see that so many people didn't want the building."
Finally the project moved to Columbia Point, near the University of Massachusetts Boston. The new site was less than ideal; it was located on an old landfill, and just over a large sewage pipe. Pei's architectural team added more fill to cover the pipe and developed an elaborate ventilation system to conquer the odor. A new design was unveiled, combining a large square glass-enclosed atrium with a triangular tower and a circular walkway.
The John F. Kennedy Presidential Library and Museum was dedicated on October 20, 1979. Critics generally liked the finished building, but the architect himself was unsatisfied. The years of conflict and compromise had changed the nature of the design, and Pei felt that the final result lacked its original passion. "I wanted to give something very special to the memory of President Kennedy," he said in 2000. "It could and should have been a great project." Pei's work on the Kennedy project boosted his reputation as an architect of note.
"Pei Plan" in Oklahoma City.
The Pei Plan was an urban redevelopment initiative designed for downtown Oklahoma City, Oklahoma, in the 1960s and 1970s. It is the informal name for two related commissions by Pei – namely the Central Business District General Neighborhood Renewal Plan (design completed 1964) and the Central Business District Project I-A Development Plan (design completed 1966). It was formally adopted in 1965, and implemented in various public and private phases throughout the 1960s and 1970s.
The plan called for the demolition of hundreds of old downtown structures in favor of renewed parking, office building, and retail developments, in addition to public projects such as the Myriad Convention Center and the Myriad Botanical Gardens. It was the dominant template for downtown development in Oklahoma City from its inception through the 1970s. The plan generated mixed results and opinion, largely succeeding in re-developing office building and parking infrastructure but failing to attract its anticipated retail and residential development. Significant public resentment also developed as a result of the destruction of multiple historic structures. As a result, Oklahoma City's leadership avoided large-scale urban planning for downtown throughout the 1980s and early 1990s, until the passage of the Metropolitan Area Projects (MAPS) initiative in 1993.
Dallas City Hall.
Kennedy's assassination led indirectly to another commission for Pei's firm. In 1964 the acting mayor, Erik Jonsson, began working to change the community's image. Dallas was known and disliked as the city where the president had been killed, but Jonsson began a program designed to initiate a community renewal. One of the goals was a new city hall, which could be a "symbol of the people". Jonsson, a co-founder of Texas Instruments, learned about Pei from his associate Cecil Howard Green, who had recruited the architect for MIT's Earth Sciences building.
Pei's approach to the new Dallas City Hall mirrored those of other projects; he surveyed the surrounding area and worked to make the building fit. In the case of Dallas, he spent days meeting with residents of the city and was impressed by their civic pride. He also found that the skyscrapers of the downtown business district dominated the skyline, and sought to create a building which could face the tall buildings and represent the importance of the public sector. He spoke of creating "a public-private dialogue with the commercial high-rises".
Working with his associate Theodore Musho, Pei developed a design centered on a building with a top much wider than the bottom; the facade leans at an angle of 34 degrees. A plaza stretches out before the building, and a series of support columns holds it up. It was influenced by Le Corbusier's High Court building in Chandigarh, India; Pei sought to use the significant overhang to unify building and plaza. The project cost much more than initially expected, and took 11 years. Revenue was secured in part by including a subterranean parking garage. The interior of the city hall is large and spacious; windows in the ceiling above the eighth floor fill the main space with light.
The city of Dallas received the building well, and a local television news crew found unanimous approval of the new city hall when it officially opened to the public in 1978. Pei himself considered the project a success, even as he worried about the arrangement of its elements. He said: "It's perhaps stronger than I would have liked; it's got more strength than finesse." He felt that his relative lack of experience left him without the necessary design tools to refine his vision, but the community liked the city hall enough to invite him back. Over the years he went on to design five additional buildings in the Dallas area.
Hancock Tower, Boston.
While Pei and Musho were coordinating the Dallas project, their associate Henry Cobb had taken the helm for a commission in Boston. John Hancock Insurance chairman Robert Slater hired I. M. Pei & Partners to design a building that could overshadow the Prudential Tower, erected by their rival.
After the firm's first plan was discarded due to a need for more office space, Cobb developed a new plan around a towering parallelogram, slanted away from the Trinity Church and accented by a wedge cut into each narrow side. To minimize the visual impact, the building was covered in large reflective glass panels; Cobb said this would make the building a "background and foil" to the older structures around it. When the Hancock Tower was finished in 1976, it was the tallest building in New England.
Serious issues of execution became evident in the tower almost immediately. Many glass panels fractured in a windstorm during construction in 1973. Some detached and fell to the ground, causing no injuries but sparking concern among Boston residents. In response, the entire tower was reglazed with smaller panels. This significantly increased the cost of the project. Hancock sued the glass manufacturers, Libbey-Owens-Ford, as well as I. M. Pei & Partners, for submitting plans that were "not good and workmanlike". LOF countersued Hancock for defamation, accusing Pei's firm of poor use of their materials; I. M. Pei & Partners sued LOF in return. All three companies settled out of court in 1981.
The project became an albatross for Pei's firm. Pei himself refused to discuss it for many years. The pace of new commissions slowed and the firm's architects began looking overseas for opportunities. Cobb worked in Australia and Pei took on jobs in Singapore, Iran, and Kuwait. Although it was a difficult time for everyone involved, Pei later reflected with patience on the experience. "Going through this trial toughened us," he said. "It helped to cement us as partners; we did not give up on each other."
National Gallery East Building, Washington, DC.
In the mid-1960s, directors of the National Gallery of Art in Washington, D.C., declared the need for a new building. Paul Mellon, a primary benefactor of the gallery and a member of its building committee, set to work with his assistant J. Carter Brown (who became gallery director in 1969) to find an architect. The new structure would be located to the east of the original building, and tasked with two functions: offer a large space for public appreciation of various popular collections; and house office space as well as archives for scholarship and research. They likened the scope of the new facility to the Library of Alexandria. After inspecting Pei's work at the Des Moines Art Center in Iowa and the Johnson Museum at Cornell University, they offered him the commission.
Pei took to the project with vigor, and set to work with two young architects he had recently recruited to the firm, William Pedersen and Yann Weymouth. Their first obstacle was the unusual shape of the building site, a trapezoid of land at the intersection of Constitution and Pennsylvania Avenues. Inspiration struck Pei in 1968, when he scrawled a rough diagram of two triangles on a scrap of paper. The larger building would be the public gallery; the smaller would house offices and archives. This triangular shape became a singular vision for the architect. As the date for groundbreaking approached, Pedersen suggested to his boss that a slightly different approach would make construction easier. Pei simply smiled and said: "No compromises."
The growing popularity of art museums presented unique challenges to the architecture. Mellon and Pei both expected large crowds of people to visit the new building, and they planned accordingly. To this end, he designed a large lobby roofed with enormous skylights. Individual galleries are located along the periphery, allowing visitors to return after viewing each exhibit to the spacious main room. A large mobile sculpture by American artist Alexander Calder was later added to the lobby. Pei hoped the lobby would be exciting to the public in the same way as the central room of the Guggenheim Museum in New York. The modern museum, he said later, "must pay greater attention to its educational responsibility, especially to the young".
Materials for the building's exterior were chosen with careful precision. To match the look and texture of the original gallery's marble walls, builders re-opened the quarry in Knoxville, Tennessee, from which the first batch of stone had been harvested. The project even found and hired Malcolm Rice, a quarry supervisor who had overseen the original 1941 gallery project. The marble was cut into three-inch-thick panels and arranged over the concrete foundation, with darker blocks at the bottom and lighter blocks on top.
The East Building was honored on May 30, 1978, two days before its public unveiling, with a black-tie party attended by celebrities, politicians, benefactors, and artists. When the building opened, popular opinion was enthusiastic. Large crowds visited the new museum, and critics generally voiced their approval. Ada Louise Huxtable wrote in "The New York Times" that Pei's building was "a palatial statement of the creative accommodation of contemporary art and architecture". The sharp angle of the smaller building has been a particular note of praise for the public; over the years it has become stained and worn from the hands of visitors.
Some critics disliked the unusual design, however, and criticized the reliance on triangles throughout the building. Others took issue with the large main lobby, particularly its attempt to lure casual visitors. In his review for "Artforum", critic Richard Hennessy described a "shocking fun-house atmosphere" and "aura of ancient Roman patronage". One of the earliest and most vocal critics, however, came to appreciate the new gallery once he saw it in person. Allan Greenberg had scorned the design when it was first unveiled, but wrote later to J. Carter Brown: "I am forced to admit that you are right and I was wrong! The building is a masterpiece."
Starting in 2005, the joints attaching the marble panels to the walls began to show signs of strain, creating a risk of panels falling off the building onto the public below. In 2008 officials decided that it would be necessary to remove and reinstall "all" the panels. The project is scheduled for completion in 2013.
Fragrant Hills, China.
After US President Richard Nixon made his famous 1972 visit to China, a wave of exchanges took place between the two countries. One of these was a delegation of the American Institute of Architects in 1974, which Pei joined. It was his first trip back to China since leaving in 1935. He was favorably received, returned the welcome with positive comments, and a series of lectures ensued. Pei noted in one lecture that since the 1950s Chinese architects had been content to imitate Western styles; he urged his audience in one lecture to search China's native traditions for inspiration.
In 1978 Pei was asked to initiate a project for his home country. After surveying a number of different locations, Pei fell in love with a valley that had once served as an imperial garden and hunting preserve known as Fragrant Hills. The site housed a decrepit hotel; Pei was invited to tear it down and build a new one. As usual, he approached the project by carefully considering the context and purpose. Likewise, he considered modernist styles inappropriate for the setting. Thus, he said, it was necessary to find "a third way".
After visiting his ancestral home in Suzhou, Pei created a design based on some simple but nuanced techniques he admired in traditional residential Chinese buildings. Among these were abundant gardens, integration with nature, and consideration of the relationship between enclosure and opening. Pei's design included a large central atrium covered by glass panels that functioned much like the large central space in his East Building of the National Gallery. Openings of various shapes in walls invited guests to view the natural scenery beyond. Younger Chinese who had hoped the building would exhibit some of Cubist flavor for which Pei had become known were disappointed, but the new hotel found more favour with government officials and architects.
The hotel, with 325 guest rooms and a four-story central atrium, was designed to fit perfectly into its natural habitat. The trees in the area were of special concern, and particular care was taken to cut down as few as possible. He worked with an expert from Suzhou to preserve and renovate a water maze from the original hotel, one of only five in the country. Pei was also meticulous about the arrangement of items in the garden behind the hotel; he even insisted on transporting of rocks from a location in southwest China to suit the natural aesthetic. An associate of Pei's said later that he never saw the architect so involved in a project.
During construction, a series of mistakes collided with the nation's lack of technology to strain relations between architects and builders. Whereas 200 or so workers might have been used for a similar building in the US, the Fragrant Hill project employed over 3,000 workers. This was mostly because the construction company lacked the sophisticated machines used in other parts of the world. The problems continued for months, until Pei had an uncharacteristically emotional moment during a meeting with Chinese officials. He later explained that his actions included "shouting and pounding the table" in frustration. The design staff noticed a difference in the manner of work among the crew after the meeting. As the opening neared, however, Pei found the hotel still needed work. He began scrubbing floors with his wife and ordered his children to make beds and vacuum floors. The project's difficulties took an emotional and physical strain on the Pei family.
The Fragrant Hill Hotel opened on October 17, 1982 but quickly fell into disrepair. A member of Pei's staff returned for a visit several years later and confirmed the dilapidated condition of the hotel. He and Pei attributed this to the country's general unfamiliarity with deluxe buildings. The Chinese architectural community at the time gave the structure little attention, as their interest at the time centered on the work of American postmodernists such as Michael Graves.
Javits Convention Center, New York.
As the Fragrant Hill project neared completion, Pei began work on the Jacob K. Javits Convention Center in New York City, for which his associate James Freed served as lead designer. Hoping to create a vibrant community institution in a run-down neighborhood on Manhattan's west side, Freed developed a glass-coated structure with an intricate space frame of interconnected metal rods and spheres.
The convention center was plagued from the start by budget problems and construction blunders. City regulations forbid a general contractor having final authority over the project, so architects and program manager Richard Kahan had to coordinate the wide array of builders, plumbers, electricians, and other workers. The forged steel globes to be used in the space frame came to the site with hairline cracks and other defects; 12,000 were rejected. These and other problems led to media comparisons with the disastrous Hancock Tower. One New York City official blamed Kahan for the difficulties, indicating that the building's architectural flourishes were responsible for delays and financial crises. The Javits Center opened on April 3, 1986, to a generally positive reception. During the inauguration ceremonies, however, neither Freed nor Pei was recognized for their role in the project.
Le Grand Louvre, Paris.
When François Mitterrand was elected President of France in 1981, he laid out an ambitious plan for a variety of construction projects. One of these was the renovation of the Louvre Museum. Mitterrand appointed a civil servant named Emile Biasini to oversee it. After visiting museums in Europe and the United States, including the US National Gallery, he asked Pei to join the team. The architect made three secretive trips to Paris, to determine the feasibility of the project; only one museum employee knew why he was there. Pei finally agreed that a reconstruction project was not only possible, but necessary for the future of the museum. He thus became the first foreign architect to work on the Louvre.
The heart of the new design included not only a renovation of the "Cour Napoléon" in the midst of the buildings, but also a transformation of the interiors. Pei proposed a central entrance, not unlike the lobby of the National Gallery East Building, which would link the three major buildings. Below would be a complex of additional floors for research, storage, and maintenance purposes. At the center of the courtyard he designed a glass and steel pyramid, first proposed with the Kennedy Library, to serve as entrance and anteroom skylight. It was mirrored by another inverted pyramid underneath, to reflect sunlight into the room. These designs were partly an homage to the fastidious geometry of the famous French landscape architect André Le Nôtre (1613–1700). Pei also found the pyramid shape best suited for stable transparency, and considered it "most compatible with the architecture of the Louvre, especially with the faceted planes of its roofs".
Biasini and Mitterrand liked the plans, but the scope of the renovation displeased Louvre director André Chabaud. He resigned from his post, complaining that the project was "unfeasible" and posed "architectural risks". The public also reacted harshly to the design, mostly because of the proposed pyramid. One critic called it a "gigantic, ruinous gadget"; another charged Mitterrand with "despotism" for inflicting Paris with the "atrocity". Pei estimated that 90 percent of Parisians opposed his design. "I received many angry glances in the streets of Paris," he said. Some condemnations carried nationalistic overtones. One opponent wrote: "I am surprised that one would go looking for a Chinese architect in America to deal with the historic heart of the capital of France."
Soon, however, Pei and his team won the support of several key cultural icons, including the conductor Pierre Boulez and Claude Pompidou, widow of former French President Georges Pompidou, after whom another controversial museum was named. In an attempt to soothe public ire, Pei took a suggestion from then-mayor of Paris Jacques Chirac and placed a full-sized cable model of the pyramid in the courtyard. During the four days of its exhibition, an estimated 60,000 people visited the site. Some critics eased their opposition after witnessing the proposed scale of the pyramid.
To minimize the impact of the structure, Pei demanded a method of glass production that resulted in clear panes. The pyramid was constructed at the same time as the subterranean levels below, which caused difficulties during the building stages. As they worked, construction teams came upon an abandoned set of rooms containing 25,000 historical items; these were incorporated into the rest of the structure to add a new exhibition zone.
The new Louvre courtyard was opened to the public on October 14, 1988, and the Pyramid entrance was opened the following March. By this time, public opinion had softened on the new installation; a poll found a fifty-six percent approval rating for the pyramid, with twenty-three percent still opposed. The newspaper "Le Figaro" had vehemently criticized Pei's design, but later celebrated the tenth anniversary of its magazine supplement at the pyramid. Prince Charles of Britain surveyed the new site with curiosity, and declared it "marvelous, very exciting". A writer in "Le Quotidien de Paris" wrote: "The much-feared pyramid has become adorable." The experience was exhausting for Pei, but also rewarding. "After the Louvre," he said later, "I thought no project would be too difficult." The Louvre Pyramid has become Pei's most famous structure.
Meyerson Symphony Center, Dallas.
The opening of the Louvre Pyramid coincided with four other projects on which Pei had been working, prompting architecture critic Paul Goldberger to declare 1989 "the year of Pei" in "The New York Times". It was also the year in which Pei's firm changed its name to Pei Cobb Freed & Partners, to reflect the increasing stature and prominence of his associates. At the age of seventy-two, Pei had begun thinking about retirement, but continued working long hours to see his designs come to light.
One of the projects took Pei back to Dallas, Texas, to design the Morton H. Meyerson Symphony Center. The success of city's performing artists, particularly the Dallas Symphony Orchestra then being led by conductor Eduardo Mata, led to interest by city leaders in creating a modern center for musical arts that could rival the best halls in Europe. The organizing committee contacted 45 architects, but at first Pei did not respond, thinking that his work on the Dallas City Hall had left a negative impression. One of his colleagues from that project, however, insisted that he meet with the committee. He did and, although it would be his first concert hall, the committee voted unanimously to offer him the commission. As one member put it: "We were convinced that we would get the world's greatest architect putting his best foot forward."
The project presented a variety of specific challenges. Because its main purpose was the presentation of live music, the hall needed a design focused on acoustics first, then public access and exterior aesthetics. To this end, a professional sound technician was hired to design the interior. He proposed a shoebox auditorium, used in the acclaimed designs of top European symphony halls such as the Amsterdam Concertgebouw and Vienna Musikverein. Pei drew inspiration for his adjustments from the designs of the German architect Johann Balthasar Neumann, especially the Basilica of the Fourteen Holy Helpers. He also sought to incorporate some of the panache of the Paris Opéra designed by Charles Garnier.
Pei's design placed the rigid shoebox at an angle to the surrounding street grid, connected at the north end to a long rectangular office building, and cut through the middle with an assortment of circles and cones. The design attempted to reproduce with modern features the acoustic and visual functions of traditional elements like filigree. The project was risky: its goals were ambitious and any unforeseen acoustic flaws would be virtually impossible to remedy after the hall's completion. Pei admitted that he did not completely know how everything would come together. "I can imagine only 60 percent of the space in this building," he said during the early stages. "The rest will be as surprising to me as to everyone else." As the project developed, costs rose steadily and some sponsors considered withdrawing their support. Billionaire tycoon Ross Perot made a donation of US$10 million, on the condition that it be named in honor of Morton H. Meyerson, the longtime patron of the arts in Dallas.
The building opened and immediately garnered widespread praise, especially for its acoustics. After attending a week of performances in the hall, a music critic for "The New York Times" wrote an enthusiastic account of the experience and congratulated the architects. One of Pei's associates told him during a party before the opening that the symphony hall was "a very mature building"; he smiled and replied: "Ah, but did I have to wait this long?"
Bank of China, Hong Kong.
A new offer had arrived for Pei from the Chinese government in 1982. With an eye toward the transfer of sovereignty of Hong Kong from the British in 1997, authorities in China sought Pei's aid on a new tower for the local branch of the Bank of China. The Chinese government was preparing for a new wave of engagement with the outside world and sought a tower to represent modernity and economic strength. Given the elder Pei's history with the bank before the Communist takeover, government officials visited the 89-year-old man in New York to gain approval for his son's involvement. Pei then spoke with his father at length about the proposal. Although the architect remained pained by his experience with Fragrant Hill, he agreed to accept the commission.
The proposed site in Hong Kong's Central District was less than ideal; a tangle of highways lined it on three sides. The area had also been home to a headquarters for Japanese military police during World War II, and was notorious for prisoner torture. The small parcel of land made a tall tower necessary, and Pei had usually shied away from such projects; in Hong Kong especially, the skyscrapers lacked any real architectural character. Lacking inspiration and unsure of how to approach the building, Pei took a weekend vacation to the family home in Katonah, New York. There he found himself experimenting with a bundle of sticks until he happened upon a cascading sequence.
The design that Pei developed for the Bank of China Tower was not only unique in appearance, but also sound enough to pass the city's rigorous standards for wind-resistance. The tower was planned around a visible truss structure, which distributed stress to the four corners of the base. Using the reflective glass that had become something of a trademark for him, Pei organized the facade around a series of boxed X shapes. At the top, he designed the roofs at sloping angles to match the rising aesthetic of the building. Some influential advocates of "feng shui" in Hong Kong and China criticized the design, and Pei and government officials responded with token adjustments.
As the tower neared completion, Pei was shocked to witness the government's massacre of unarmed civilians at the Tiananmen Square protests of 1989. He wrote an opinion piece for "The New York Times" titled "China Won't Ever Be the Same", in which he said that the killings "tore the heart out of a generation that carries the hope for the future of the country". The massacre deeply disturbed his entire family, and he wrote that "China is besmirched."
1990–present: museum projects.
As the 1990s began, Pei transitioned into a role of decreased involvement with his firm. The staff had begun to shrink, and Pei wanted to dedicate himself to smaller projects allowing for more creativity. Before he made this change, however, he set to work on his last major project as active partner: The Rock and Roll Hall of Fame in Cleveland, Ohio. Considering his work on such bastions of high culture as the Louvre and US National Gallery, some critics were surprised by his association with what many considered a tribute to low culture. The sponsors of the hall, however, sought Pei for specifically this reason; they wanted the building to have an aura of respectability from the beginning. As in the past, Pei accepted the commission in part because of the unique challenge it presented.
Using a glass wall for the entrance, similar in appearance to his Louvre pyramid, Pei coated the exterior of the main building in white metal, and placed a large cylinder on a narrow perch to serve as a performance space. The combination of off-centered wraparounds and angled walls was, Pei said, designed to provide "a sense of tumultuous youthful energy, rebelling, flailing about".
The building opened in 1995, and was received with moderate praise. "The New York Times" called it "a fine building", but Pei was among those who felt disappointed with the results. The museum's early beginnings in New York combined with an unclear mission created a fuzzy understanding among project leaders for precisely what was needed. Although the city of Cleveland benefited greatly from the new tourist attraction, Pei was unhappy with it.
At the same time, Pei designed a new museum for Luxembourg, the "Musée d'art moderne Grand-Duc Jean", commonly known as the Mudam. Drawing from the original shape of the Fort Thüngen walls where the museum was located, Pei planned to remove a portion of the original foundation. Public resistance to the historical loss forced a revision of his plan, however, and the project was nearly abandoned. The size of the building was halved, and it was set back from the original wall segments to preserve the foundation. Pei was disappointed with the alterations, but remained involved in the building process even during construction.
In 1995 Pei was hired to design an extension to the "Deutsches Historisches Museum", or German Historical Museum in Berlin. Returning to the challenge of the East Building of the US National Gallery, Pei worked to combine a modernist approach with a classical main structure. He described the glass cylinder addition as a "beacon", and topped it with a glass roof to allow plentiful sunlight inside. Pei had difficulty working with German government officials on the project; their utilitarian approach clashed with his passion for aesthetics. "They thought I was nothing but trouble", he said.
Pei also worked at this time on two projects for a new Japanese religious movement called "Shinji Shumeikai". He was approached by the movement's spiritual leader, Kaishu Koyama, who impressed the architect with her sincerity and willingness to give him significant artistic freedom. One of the buildings was a bell tower, designed to resemble the "bachi" used when playing traditional instruments like the "shamisen". Pei was unfamiliar with the movement's beliefs, but explored them in order to represent something meaningful in the tower. As he said: "It was a search for the sort of expression that is not at all technical."
The experience was rewarding for Pei, and he agreed immediately to work with the group again. The new project was the Miho Museum, to display Koyama's collection of tea ceremony artifacts. Pei visited the site in Shiga Prefecture, and during their conversations convinced Koyama to expand her collection. She conducted a global search and acquired more than 300 items showcasing the history of the Silk Road.
One major challenge was the approach to the museum. The Japanese team proposed a winding road up the mountain, not unlike the approach to the NCAR building in Colorado. Instead, Pei ordered a hole cut through a nearby mountain, connected to a major road via a bridge suspended from ninety-six steel cables and supported by a post set into the mountain. The museum itself was built into the mountain, with 80 percent of the building underground.
When designing the exterior, Pei borrowed from the tradition of Japanese temples, particularly those found in nearby Kyoto. He created a concise spaceframe wrapped into French limestone and covered with a glass roof. Pei also oversaw specific decorative details, including a bench in the entrance lobby, carved from a 350-year-old "keyaki" tree. Because of Koyama's considerable wealth, money was rarely considered an obstacle; estimates at the time of completion put the cost of the project at US$350 million.
During the first decade of the 2000s, Pei designed a variety of buildings, including the Suzhou Museum near his childhood home. He also designed the Museum of Islamic Art in Doha, Qatar at the request of the Al-Thani Family. Although it was originally planned for the corniche road along Doha Bay, Pei convinced project coordinators to build a new island to provide the needed space. He then spent six months touring the region and surveying mosques in Spain, Syria, and Tunisia. He was especially impressed with the elegant simplicity of the Mosque of Ibn Tulun in Cairo.
Once again, Pei sought to combine new design elements with the classical aesthetic most appropriate for the location of the building. The rectangular boxes rotate evenly to create a subtle movement, with small arched windows at regular intervals into the limestone exterior. The museum's coordinators were pleased with the project; its official website describes its "true splendour unveiled in the sunlight", and speaks of "the shades of colour and the interplay of shadows paying tribute to the essence of Islamic architecture".
The Macao Science Center in Macau was designed by Pei Partnership Architects in association with I. M. Pei. The project to build the science center was conceived in 2001 and construction started in 2006. The center was completed in 2009 and opened by the Chinese President Hu Jintao.
The main part of the building is a distinctive conical shape with a spiral walkway and large atrium inside, similar to the Solomon R. Guggenheim Museum in New York. Galleries lead off the walkway, mainly consisting of interactive exhibits aimed at science education.
The building is in a prominent position by the sea and is now a landmark of Macau.
Style and method.
Pei's style is described as thoroughly modernist, with significant cubist themes. He is known for combining traditional architectural elements with progressive designs based on simple geometric patterns. As one critic writes: "Pei has been aptly described as combining a classical sense of form with a contemporary mastery of method." In 2000, biographer Carter Wiseman called Pei "the most distinguished member of his Late-Modernist generation still in practice". At the same time, Pei himself rejects simple dichotomies of architectural trends. He once said: "The talk about modernism versus post-modernism is unimportant. It's a side issue. An individual building, the style in which it is going to be designed and built, is not that important. The important thing, really, is the community. How does it affect life?"
Pei's work is celebrated throughout the world of architecture. His colleague John Portman once told him: "Just once, I'd like to do something like the East Building." But this originality does not always bring large financial reward; as Pei replied to the successful architect: "Just once, I'd like to make the kind of money you do." His concepts, moreover, are too individualized and dependent on context to give rise to a particular school of design. Pei refers to his own "analytical approach" when explaining the lack of a "Pei School". "For me," he said, "the important distinction is between a stylistic approach to the design; and an analytical approach giving the process of due consideration to time, place, and purpose ... My analytical approach requires a full understanding of the three essential elements ... to arrive at an ideal balance among them."
On a matter of personal style and his method of business negotiation, Mr. Pei once told a television reporter during an interview about an amusing event that happened in his career. A client was inclined to disburse less treasure for a particular design, and the architect replied "My name is I. M. Pei, not I am Not Pay." The client paid the asking price.
Awards and honors.
In the words of his biographer, Pei has won "every award of any consequence in his art", including the Arnold Brunner Award from the National Institute of Arts and Letters (1963), the Gold Medal for Architecture from the American Academy of Arts and Letters (1979), the AIA Gold Medal (1979), the first "Praemium Imperiale" for Architecture from the Japan Art Association (1989), the Lifetime Achievement Award from the Cooper-Hewitt, National Design Museum, and the 2010 Royal Gold Medal from the Royal Institute of British Architects. In 1983 he was awarded the Pritzker Prize, sometimes called the Nobel Prize of architecture. In its citation, the jury said: "Ieoh Ming Pei has given this century some of its most beautiful interior spaces and exterior forms ... His versatility and skill in the use of materials approach the level of poetry." The prize was accompanied by a US$100,000 award, which Pei used to create a scholarship for Chinese students to study architecture in the US, on the condition that they return to China to work. In being awarded the 2003 Henry C. Turner Prize by the National Building Museum, then-museum board chair Carolyn Brody praised his impact on construction innovation: "His magnificent designs have challenged engineers to devise innovative structural solutions, and his exacting expectations for construction quality have encouraged contractors to achieve high standards."
In 1992, Pei was awarded the Presidential Medal of Freedom by President George H.W. Bush.
Personal life.
Pei's wife of over seventy years, Eileen Loo, predeceased him in June 2014. They had three sons, T'ing Chung (1946–2003), Chien Chung (b. 1946) and Li Chung (b. 1949), and a daughter, Liane (b. 1960). T'ing Chung was an urban planner and alumnus of his father's "alma mater" MIT and Harvard. Chieng Chung and Li Chung, who are both Harvard Graduate School of Design alumni, founded and run Pei Partnership Architects. Liane is a lawyer.
References.
Notes
Bibliography

</doc>
<doc id="15156" url="https://en.wikipedia.org/wiki?curid=15156" title="ICD (disambiguation)">
ICD (disambiguation)

ICD may refer to:
In health and medicine:
Organizations:
In technology:
Other uses:

</doc>
<doc id="15158" url="https://en.wikipedia.org/wiki?curid=15158" title="Islamic Jihad">
Islamic Jihad

Islamic Jihad may refer to:

</doc>
<doc id="15161" url="https://en.wikipedia.org/wiki?curid=15161" title="Intel 80486">
Intel 80486

The Intel 486 (""four-eight-six""), also known as the i486 or 80486 was a higher performance follow-up to the Intel 80386 microprocessor. The 486 was introduced in 1989 and was the first tightly pipelined x86 design as well as the first x86 chip to use more than a million transistors, due to a large on-chip cache and an integrated floating-point unit. It represents a fourth generation of binary compatible CPUs since the original 8086 of 1978.
A 50 MHz 486 executes around 40 million instructions per second on average and is able to reach 50 MIPS peak performance.
The i486 does not have the usual 80-prefix because of a court ruling that prohibits trademarking numbers (such as 80486). Later, with the introduction of the Pentium brand, Intel began branding its chips with words rather than numbers.
Background.
The 486 was announced at Spring Comdex in April 1989. At the announcement, Intel stated that samples would be available in the third quarter of 1989 and production quantities would ship in the fourth quarter of 1989. The first 486-based PCs were announced in late 1989, but some advised that people wait until 1990 to purchase a 486 PC because there were early reports of bugs and software incompatibilities.
Improvements.
The instruction set of the i486 is very similar to its predecessor, the Intel 80386, with the addition of only a few extra instructions, such as CMPXCHG which implements a compare-and-swap atomic operation and XADD, a fetch-and-add atomic operation returning the original value (unlike a standard ADD which "returning s" flags only).
From a performance point of view, the architecture of the i486 is a vast improvement over the 80386. It has an on-chip unified instruction and data cache, an on-chip floating-point unit (FPU) and an enhanced bus interface unit. Due to the tight pipelining, sequences of simple instructions (such as ALU reg,reg and ALU reg,im) could sustain a single clock cycle throughput (one instruction completed every clock). These improvements yielded a rough doubling in integer ALU performance over the 386 at the same clock rate. A 16-MHz 486 therefore had a performance similar to a 33-MHz 386, and the older design had to reach 50 MHz to be comparable with a 25-MHz 486 part.
Differences between i386 and i486.
Just as in the 80386, a simple flat 4 GB memory model could be implemented by setting all "segment selector" registers to a neutral value in protected mode, or setting (the same) "segment registers" to zero in real mode, and using only the 32-bit "offset registers" (x86-terminology for general CPU registers used as address registers) as a linear 32-bit virtual address bypassing the segmentation logic. Virtual addresses were then normally mapped onto physical addresses by the paging system except when it was disabled. ("Real" mode had no "virtual" addresses.) Just as with the 80386, circumventing memory segmentation could substantially improve performance in some operating systems and applications.
On a typical PC motherboard, either four matched 30-pin (8-bit) SIMMs or one 72-pin (32-bit) SIMM per bank were required to fit the 486's 32-bit data bus. The address bus used 30-bits (A31..A2) complemented by four byte-select pins (instead of A0,A1) to allow for any 8/16/32-bit selection. This meant that the limit of directly addressable physical memory was 4 gigabytes as well,(230 "32-bit" words = 232 "8-bit" words).
Models.
There are several suffixes and variants. (see Table). Other variants include:
The specified maximum internal clock frequency (on Intel's versions) ranged from 16 to 100 MHz. The 16 MHz i486SX model was used by Dell Computers.
One of the few 486 models specified for a 50 MHz bus (486DX-50) initially had overheating problems and was moved to the 0.8 micrometre fabrication process. However, problems continued when the 486DX-50 was installed in local bus systems due to the high bus speed, making it rather unpopular with mainstream consumers as local bus video was considered a requirement at the time, though it remained popular with users of EISA systems. The 486DX-50 was soon eclipsed by the clock-doubled i486DX2 which instead ran the CPU logic at twice the external bus speed which actually means it was slower due to the bus running at only 25 or 33 MHz
More powerful 486 iterations such as the OverDrive and DX4 were less popular (the latter available as an OEM part only), as they came out after Intel had released the next generation P5 Pentium processor family. Certain steppings of the DX4 also officially supported 50 MHz bus operation but was a seldom used feature.
"WT" = Write-Through cache strategy, "WB" = Write-Back cache strategy
Other makers of 486-like CPUs.
486 compatible processors have been produced by other companies such as IBM, Texas Instruments, AMD, Cyrix, UMC, and SGS Thompson. Some were clones (identical at the microarchitectural level), others were clean room implementations of the Intel instruction-set. (IBM's multiple source requirement is one of the reasons behind its x86-manufacturing since the 80286.) The 486 was, however, covered by many of Intel's patents covering new R&D as well as that of the prior 80386. Intel and IBM have broad cross-licenses of these patents, and AMD was granted rights to the relevant patents in the 1995 settlement of a lawsuit between the companies.
AMD produced several clones of the 486 using a 40 MHz bus (486DX-40, 486DX/2-80, and 486DX/4-120) which had no equivalent available from Intel, as well as a part specified for 90 MHz, using a 30 MHz external clock, that was sold only to OEMs. The fastest running 486 CPU, the Am5x86, ran at 133 MHz and was released by AMD in 1995. 150 MHz and 160 MHz parts were planned but never officially released.
Cyrix made a variety of 486-compatible processors, positioned at the cost-sensitive desktop and low-power (laptop) markets. Unlike AMD's 486 clones, the Cyrix processors were the result of clean-room reverse-engineering. Cyrix's early offerings included the 486DLC and 486SLC, two hybrid chips which plugged into 386DX or SX sockets respectively, and offered 1 KB of cache (versus 8 KB for the then-current Intel/AMD parts). Cyrix also made "real" 486 processors, which plugged into the i486's socket and offered 2 or 8 KB of cache. Clock-for-clock, the Cyrix-made chips were generally slower than their Intel/AMD equivalents, though later products with 8 KB caches were more competitive, if late to market.
The Motorola 68040 (best known for its use in the Macintosh Quadra series), while not compatible with the 486, was often positioned as the 486's equivalent in features and performance. Clock-for-clock basis the Motorola 68040 could significantly outperform the Intel 80486 chip. However, the 486 had the ability to be clocked significantly faster without suffering from overheating problems. The Motorola 68040 performance lagged behind the later production 486 systems.
Motherboards and buses.
Early 486 machines were equipped with several ISA slots (using an emulated PC/AT-bus) and sometimes one or two 8-bit–only slots (compatible with the PC/XT-bus). Many motherboards enabled overclocking of these up from the default 6 or 8 MHz to perhaps 16.7 or 20 MHz (half the i486 bus clock) in a number of steps, often from within the BIOS setup. Especially older peripheral cards normally worked well at such speeds as they often used standard MSI chips instead of slower (at the time) custom VLSI designs. This could give significant performance gains (such as for old video cards moved from a 386 or 286 computer, for example). However, operation beyond 8 or 10 MHz could sometimes lead to stability problems, at least in systems equipped with SCSI or sound cards.
Some motherboards came equipped with a 32-bit bus called EISA that was backward compatible with the ISA-standard. EISA offered a number of attractive features such as increased bandwidth, extended addressing, IRQ sharing, and card configuration through software (rather than through jumpers, DIP switches, etc.) However, EISA cards were expensive and therefore mostly employed in servers and workstations. Consumer desktops often used the simpler but faster VESA Local Bus (VLB), unfortunately somewhat prone to electrical and timing-based instability; typical consumer desktops had ISA slots combined with a single VLB slot for a video card. VLB was gradually replaced by PCI during the final years of the 486 period. Few Pentium class motherboards had VLB support as VLB was based directly on the i486 bus; it was no trivial matter adapting it to the quite different P5 Pentium-bus. ISA persisted through the P5 Pentium generation and was not completely displaced by PCI until the Pentium III era.
Late 486 boards were normally equipped with both PCI and ISA slots, and sometimes a single VLB slot as well. In this configuration VLB or PCI throughput suffered depending on how buses were bridged. Initially, the VLB slot in these systems was usually fully compatible only with video cards (quite fitting as "VESA" stands for "Video Electronics Standards Association"); VLB-IDE, multi I/O, or SCSI cards could have problems on motherboards with PCI slots. The VL-Bus operated at the same clock speed as the i486-bus (basically "being" a local 486-bus) while the PCI bus also usually depended on the i486 clock but sometimes had a divider setting available via the BIOS. This could be set to 1/1 or 1/2, sometimes even 2/3 (for 50 MHz CPU clocks). Some motherboards limited the PCI clock to the specified maximum of 33 MHz and certain network cards depended on this frequency for correct bit-rates. The ISA clock was typically generated by a divider of the CPU/VLB/PCI clock (as implied above).
One of the earliest complete systems to use the 486 chip was the Apricot VX FT, produced by United Kingdom hardware manufacturer Apricot Computers. Even overseas in the United States it was popularised as "The World's First 486" in the September 1989 issue of "Byte" magazine (shown right).
Later 486 boards also supported Plug-And-Play, a specification designed by Microsoft that began as a part of Windows 95 to make component installation easier for consumers.
Gaming.
The 486DX2 66 MHz processor was popular on home-oriented PCs during the early to mid 1990s, toward the end of the MS-DOS gaming era. It was often coupled with a VESA Local Bus video card.
The introduction of 3D computer graphics spelled the end of the 486's reign, because 3D graphics make heavy use of floating point calculations and require a faster CPU cache and more memory bandwidth. Developers began to target the P5 Pentium processor family almost exclusively with x86 assembly language optimizations (e.g., "Quake") which led to the usage of terms like "Pentium compatible processor" for software requirements. Many of these games required the speed of the P5 Pentium processor family's double-pipelined architecture.
Obsolescence.
The AMD Am5x86, up to 133 MHz, and Cyrix Cx5x86, up to 120 MHz, were the last 486 processors that were often used in late generation 486 motherboards with PCI slots and 72-pin SIMMs that are designed to be able to run Windows 95, and also often used as upgrades for older 486 motherboards. While the Cyrix Cx5x86 faded quite quickly when the Cyrix 6x86 took over, the AMD Am5x86 was important during the time when the AMD K5 was delayed.
In the general purpose desktop computer role, 486-based machines remained in use into the early-2000s, especially as Windows 95, Windows 98, and Windows NT 4.0 were the latest Microsoft operating systems to officially support installation on a 486-based system. However, as Windows 95/98 and Windows NT 4.0 were eventually overtaken by newer operating systems, 486 systems likewise fell out of use. Still, a number of 486 machines have remained in use today, mostly for backward compatibility with older programs (most notably games), especially since many of them have problems running on newer operating systems. However, DOSBox is also available for current operating systems and provides emulation of the 486 instruction set, as well as full compatibility with most DOS-based programs.
Although the 486 was eventually overtaken by the Pentium for personal computer applications, Intel had continued production for use in embedded systems. In May 2006 Intel announced that production of the 80486 would stop at the end of September 2007.

</doc>
<doc id="15164" url="https://en.wikipedia.org/wiki?curid=15164" title="Intel 80486SX">
Intel 80486SX

Intel's i486SX was a modified Intel 486DX microprocessor with its floating-point unit (FPU) disabled. It was intended as a lower-cost CPU for use in low-end systems. Computer manufacturers that used these processors include Packard Bell, Compaq, ZEOS and IBM.
In the early 1990s, common applications did not need or benefit from an FPU. Among the rare exceptions were CAD applications, which could often simulate floating point operations in software, but benefited from a hardware floating point unit immensely. Intel wanted to provide a lower cost i486 CPU for system integrators, but without sacrificing the better profit margins of a "full" i486. This was accomplished through a debug feature called Disable Floating Point (DFP), by grounding a certain bond wire in the CPU package. The i486SX was introduced in mid-1991, 18 months after the i486DX. Later (late 1992) versions of the i486SX had the FPU entirely removed for cost cutting reasons.
Some systems allowed the user to upgrade the i486SX to a CPU with the FPU enabled. The upgrade was shipped as the i487, which was a full blown i486DX chip with an extra pin. The extra pin prevents the chip from being installed incorrectly. The NC# pin, one of the standard 168 pins, was used to shut off the i486SX. Although i486SX devices were not used at all when the i487 was installed, they were hard to remove because the i486SX was typically installed in non-ZIF sockets or in a plastic package that was surface mounted on the motherboard.
External links.
Intel Datasheets

</doc>
<doc id="15165" url="https://en.wikipedia.org/wiki?curid=15165" title="Ivory">
Ivory

Ivory is a hard, white material from the tusks (traditionally elephant's) and teeth of animals, that can be used in art or manufacturing. It consists mainly of dentine (inorganic formula Ca10(PO4)6(CO3).H2O)), one of the physical structures of teeth and tusks. The chemical structure of the teeth and tusks of mammals is the same, regardless of the species of origin. The trade in certain teeth and tusks other than elephant is well established and widespread; therefore, "ivory" can correctly be used to describe any mammalian teeth or tusks of commercial interest which are large enough to be carved or scrimshawed. It has been valued since ancient times for making a range of items, from ivory carvings to false teeth, fans, and dominoes. Elephant ivory is the most important source, but ivory from mammoth, walrus, hippopotamus, sperm whale, killer whale, narwhal and wart hog are used as well. Elk also have two ivory teeth, which are believed to be the remnants of tusks from their ancestors.
The national and international trade in ivory of threatened species such as African and Asian elephants is illegal. The word "ivory" ultimately derives from the ancient Egyptian "âb, âbu" ("elephant"), through the Latin "ebor-" or "ebur".
Uses.
Both the Greek and Roman civilizations practiced ivory carving to make large quantities of high value works of art, precious religious objects, and decorative boxes for costly objects. Ivory was often used to form the white of the eyes of statues.
The Syrian and North African elephant populations were reduced to extinction, probably due to the demand for ivory in the Classical world.
The Chinese have long valued ivory for both art and utilitarian objects. Early reference to the Chinese export of ivory is recorded after the Chinese explorer Zhang Qian ventured to the west to form alliances to enable the eventual free movement of Chinese goods to the west; as early as the first century BC, ivory was moved along the Northern Silk Road for consumption by western nations. Southeast Asian kingdoms included tusks of the Indian elephant in their annual tribute caravans to China. Chinese craftsmen carved ivory to make everything from images of deities to the pipe stems and end pieces of opium pipes.
The Buddhist cultures of Southeast Asia, including Myanmar, Thailand, Laos and Cambodia, traditionally harvested ivory from their domesticated elephants. Ivory was prized for containers due to its ability to keep an airtight seal. It was also commonly carved into elaborate seals utilized by officials to "sign" documents and decrees by stamping them with their unique official seal.
In Southeast Asian countries, where Muslim Malay peoples live, such as Malaysia, Indonesia and the Philippines, ivory was the material of choice for making the handles of kris daggers. 
In the Philippines, ivory was also used to craft the faces and hands of Catholic icons and images of saints prevalent in the Santero culture.
Tooth and tusk ivory can be carved into a vast variety of shapes and objects. Examples of modern carved ivory objects are okimono, netsukes, jewelry, flatware handles, furniture inlays, and piano keys. Additionally, warthog tusks, and teeth from sperm whales, orcas and hippos can also be scrimshawed or superficially carved, thus retaining their morphologically recognizable shapes.
Ivory usage in the last thirty years has moved towards mass production of souvenirs and jewelry. In Japan, the increase in wealth sparked consumption of solid ivory "hanko" – name seals – which before this time had been made of wood. These "hanko" can be carved out in a matter of seconds using machinery and were partly responsible for massive African elephant decline in the 1980s, when the African elephant population went from 1.3 million to around 600,000 in ten years.
Consumption before plastics.
Prior to the introduction of plastics, ivory had many ornamental and practical uses, mainly because of the white color it presents when processed. It was formerly used to make cutlery handles, billiard balls, piano keys, Scottish bagpipes, buttons and a wide range of ornamental items.
Synthetic substitutes for ivory in the use of most of these items have been developed since 1800: the billiard industry challenged inventors to come up with an alternative material that could be manufactured; the piano industry abandoned ivory as a key covering material in the 1970s.
Ivory can be taken from dead animals – however, most ivory came from elephants that were killed for their tusks. For example, in 1930 to acquire 40 tons of ivory required the killing of approximately 700 elephants. Other animals which are now endangered were also preyed upon, for example, hippos, which have very hard white ivory prized for making artificial teeth. In the first half of the 20th century, Kenyan elephant herds were devastated because of demand for ivory, to be used for piano keys.
During the Art Deco era from 1912 to 1940, dozens (if not hundreds) of European artists used ivory in the production of chryselephantine statues. Two of the most frequent users of ivory in their sculptured artworks were Ferdinand Preiss and Claire Colinet.
Availability.
Owing to the rapid decline in the populations of the animals that produce it, the importation and sale of ivory in many countries is banned or severely restricted. In the ten years preceding a decision in 1989 by CITES to ban international trade in African elephant ivory, the population of African elephants declined from 1.3 million to around 600,000. It was found by investigators from the Environmental Investigation Agency (EIA) that CITES sales of stockpiles from Singapore and Burundi (270 tonnes and 89.5 tonnes respectively) had created a system that increased the value of ivory on the international market, thus rewarding international smugglers and giving them the ability to control the trade and continue smuggling new ivory.
Since the ivory ban, some southern African countries have claimed their elephant populations are stable or increasing, and argued that ivory sales would support their conservation efforts. Other African countries oppose this position, stating that renewed ivory trading puts their own elephant populations under greater threat from poachers reacting to demand. CITES allowed the sale of 49 tonnes of ivory from Zimbabwe, Namibia and Botswana in 1997 to Japan.
In 2007 eBay, under pressure from the International Fund for Animal Welfare, banned all international sales of elephant-ivory products. The decision came after several mass slaughters of African elephants, most notably the 2006 Zakouma elephant slaughter in Chad. The IFAW found that up to 90% of the elephant-ivory transactions on eBay violated their own wildlife policies and could potentially be illegal. In October 2008, eBay expanded the ban, disallowing any sales of ivory on eBay.
A more recent sale in 2008 of 108 tonnes from the three countries and South Africa took place to Japan and China. The inclusion of China as an "approved" importing country created enormous controversy, despite being supported by CITES, the World Wide Fund for Nature and Traffic. They argued that China had controls in place and the sale might depress prices. However, the price of ivory in China has skyrocketed. Some believe this may be due to deliberate price fixing by those who bought the stockpile, echoing the warnings from the Japan Wildlife Conservation Society on price-fixing after sales to Japan in 1997, and monopoly given to traders who bought stockpiles from Burundi and Singapore in the 1980s.
Despite arguments prevailing on the ivory trade for the last thirty years through CITES, there is one fact upon which virtually all informed parties now agree – poaching of African elephants for ivory is now seriously on the increase.
The debate surrounding ivory trade has often been depicted as Africa vs the West. However, in reality the southern Africans have always been in a minority within the African elephant range states. To reiterate this point, 19 African countries signed the "Accra Declaration" in 2006 calling for a total ivory trade ban, and 20 range states attended a meeting in Kenya calling for a 20-year moratorium in 2007.
In Asia, wild elephant populations are now a fraction of what they were in historic times, and poaching of elephants continues. Elephants are now close to extinction in China, Vietnam, Laos, Cambodia and Indonesia. Instances of theft of even domestic elephants for their ivory have been recorded in Myanmar.
Controversy and conservation issues.
The use and trade of elephant ivory have become controversial because they have contributed to seriously declining elephant populations in many countries. It is estimated that consumption in Great Britain alone in 1831 amounted to the deaths of nearly 4,000 elephants. In 1975, the Asian elephant was placed on Appendix One of the Convention on International Trade in Endangered Species (CITES), which prevents international trade between member countries. The African elephant was placed on Appendix One in January 1990. Since then, some southern African countries have had their populations of elephants "downlisted" to Appendix Two, allowing sale of some stockpiles.
In June of 2015 more than a ton of confiscated ivory was crushed in New York's Times Square by the Wildlife Conservation Society to send a message that the illegal trade will not be tolerated. The ivory, confiscated in New York and Philadelphia, was sent up a conveyor belt into a rock crusher. The Wildlife Conservation Society has pointed out that the global ivory trade leads to the slaughter of up to 35,000 elephants a year in Africa.
China was the biggest market for poached ivory but announced they would phase out the legal domestic manufacture and sale of ivory products in May, 2015, and in September 2015 China and the U.S.A. "said they would enact a nearly complete ban on the import and export of ivory." [http://www.theguardian.com/environment/2015/sep/26/china-and-us-agree-on-ivory-ban-in-bid-to-end-illegal-trade-globally].
Alternative sources.
Trade in the ivory from the tusks of dead mammoths has occurred for 300 years and continues to be legal. Mammoth ivory is used today to make handcrafted knives and similar implements. Mammoth ivory is rare and costly, because mammoths have been extinct for millennia, and scientists are loath to sell museum-worthy specimens in pieces. Some estimates suggest that 10 million mammoths are still buried in Siberia.
A species of hard nut is gaining popularity as a replacement for ivory, although its size limits its usability. It is sometimes called vegetable ivory, or tagua, and is the seed endosperm of the ivory nut palm commonly found in coastal rainforests of Ecuador, Peru and Colombia.

</doc>
<doc id="15166" url="https://en.wikipedia.org/wiki?curid=15166" title="Infantry fighting vehicle">
Infantry fighting vehicle

An infantry fighting vehicle (IFV), or mechanized infantry combat vehicle (MICV), is a type of armoured fighting vehicle used to carry infantry into battle and provide direct fire support. The Treaty on Conventional Armed Forces in Europe defines an infantry fighting vehicle as "an armoured combat vehicle which is designed and equipped primarily to transport a combat infantry squad, which is armed with an integral or organic cannon of at least 20 millimeters calibre and sometimes an antitank missile launcher."
Infantry fighting vehicles are distinct from armoured personnel carriers (APCs), which are transport vehicles armed only for self-defense and not specifically engineered to fight on their own. Consequently, they possess heavier armament and the attached rifle squad fights mounted more often than in an APC. IFVs also often have improved armour and some have ports which allow the infantry to fire personal weapons while on board.
They are typically armed with a 20 to 40 mm caliber autocannon, a coaxial machine gun and sometimes anti-tank guided missiles (ATGMs). IFVs are usually tracked, but there are some wheeled vehicles too. IFVs are much less heavily armed and protected than main battle tanks, but when equipped with larger cannon or ATGMs may pose a significant threat to all but the heaviest armoured fighting vehicles.
History.
The first mass-produced IFV was the West German Schützenpanzer 12-3 which served in the Bundeswehr from 1958 until the early 1980s. The SPz 12-3 mounted a 20 mm autocannon in a small turret and carried a half-squad of five armoured infantrymen.
Western powers were surprised when the Soviet Union paraded the BMP-1, in 1967. The BMP possessed a very low profile and was armed with both a 73 mm smoothbore gun and an AT-3 Sagger ATGM, making it the first combat vehicle with mounted cannon and missiles in addition to a mounted infantry squad. Its steeply-sloped front armour offered full protection against NATO's standard .50 calibre machine gun and partial protection against a 20mm Oerlikon autocannon both in a 60 degree frontal arc, while its armament posed a threat to lighter NATO APCs and even main battle tanks. Soviet infantrymen could thus enter a hypothetical engagement in a vehicle that possessed formidable fighting capability in its own right. This brought combined arms integration to the lowest tactical level.
In 1971, the Ratel was designed to a South African specification for a wheeled combat vehicle suited to the demands of a high-speed offensive that combined maximum mobility and firepower. The emphasis was on mobility in particular, as it had to keep pace with a rapid mechanised advance and cross hostile ground quickly. South Africa's motorised units had carried out prior deployments on unprotected Bedford MK and Unimog trucks, but these were deemed unsuitable for the harsh African terrain. They also offered few advantages in mine protection, while the Ratel's blastproof hull was developed to withstand even the most catastrophic anti-tank mine explosions. During the South African Border War, Ratels equipped with a 90 mm gun adopted from the Eland-90 were utilised as improvised tank destroyers to varying degrees of success.
After the BMP and the Ratel, several other countries followed suit in embracing the concept, including the United States, which adopted the M2 Bradley, the United Kingdom the Warrior, and West Germany the Marder. Other examples include the Japanese 89FV, the Chinese ZBD-97, the Indian Abhay, the Canadian LAV III, the Spanish ASCOD Pizarro, the Italian Dardo, the French AMX-10P and VBCI, and the Swedish Combat Vehicle 90. In most Western IFVs, the comparatively large smoothbore guns on the BMP and later variants of the Ratel have now been discarded in favour of smaller calibre autocannon. While the former enjoyed a tank-killing capability that most autocannon lack, their low rate of fire and mediocre individual accuracy offset this advantage.
Combat applications in close-combat environments are likely to drive up survivability requirements necessitating the same protection level required by most tanks.
Doctrine.
In times of asymmetrical warfare, local crises, and urban combat, the IFV is more important than ever. The IFV offers a viable compromise between mobility, protection, and firepower. They can be used in high and low intensity conflicts as well as peacekeeping operations. The latest vehicles, like the Patria AMV, have been designed with an emphasis on modularity that improves their ability to be repaired in the field. 
Design.
Infantry fighting vehicles are typically well armoured, although usually with less protection than main battle tanks. Typical armament is an autocannon and machine guns. IFVs have a door for dismounts.
Protection.
Generally, IFVs have thinner and less complex armour than tanks to ensure mobility. Most IFVs are resistant against heavy machine guns, artillery fragments, and small arms. The IFV's mission does not include anti-tank duties except in support of tank units or in emergencies; therefore, it needs less protection from heavy weapons fire. Instead, the IFV, as its name implies, is supposed to carry riflemen and their weapons into the battlefield where they dismount and fight outside the vehicle with the support of the IFV's main armament.
In IFVs, the thickness of armour varies widely between models. Some vehicles are proof against .50 in (12. mm) bullets while others, such as Sweden's CV90 and Russia's BMP-3, can withstand frontal hits from 30 mm autocannon. The sides, roof, and floor of IFVs have thinner armour. Vehicles must also protect crew against anti-personnel mines and against anti-tank mines.
Newer vehicles like the Finnish Patria AMV use armour made in interchangeable modules of various thickness. This permits the vehicle to be tailored for particular missions such as decreasing the weight of vehicle for air transportation or strengthening the protection if it engages in dangerous missions. The latest models of the Russian BMP-3 use the Arena active protection system (APS) that protects the vehicle from missiles and rockets with velocities from 70 to 700 metres per second. Israeli IFVs will soon employ the Iron Fist (countermeasure) which can defeat kinetic APFSDS tank rounds.
The most common countermeasures are smoke grenade dischargers. These help IFVs to avoid attacks from ATGMs by providing a smoke screen. Some vehicles, such as the French VBCI, employ infra-red jamming flare dispensers. These are effective against missiles with IR guidance systems.
The term Heavy Infantry Fighting Vehicle (HIFV) is often applied to IFVs that are as heavily armoured as a main battle tank. An example of a HIFV is the Russian T-15 Armata, which is based on the same chassis as the T-14 main battle tank. The Israeli Namer is based on the Merkava IV main battle tank chassis and, although officially an armored personnel carrier, does have the capability to be equipped with an autocannon in a remote unmanned turret, thus making it a true HIFV.
Weaponry.
The primary weapon on most IFVs is an autocannon, usually of a calibre between 20–40 mm, although the BMP-3, ZBD-97 and BMD-4 carry a 100 mm gun-launcher along with a 30 mm autocannon. It is effective against a wide range of targets such as unarmoured and lightly armoured vehicles, infantry, helicopters and low-flying aircraft. It can fire several types of munitions, including high explosive, incendiary, and kinetic penetrator rounds. Germany's Puma and Sweden's CV90 can fire air burst munitions that contain hundreds of tungsten rods that are effective against vehicles, helicopters, and stationary strong points. IFV cannons can elevate their barrels by as much as 70 degrees to permit their crews to engage aircraft.
On many IFVs, a coaxial machine gun is mounted on the turret along with the main armament. The most common calibre is 7.62 mm (.30 cal); usually in form of 7.62×51mm NATO or 7.62×54mmR. Some vehicles mount more machine guns; for example, on the German Marder, one machine gun fires from the rear of the vehicle, and the Russian BMP-3 has two machine guns mounted in the hull facing forward.
Many IFVs utilize firing ports allowing infantry to fire without leaving the protection of the vehicle.
Some IFVs are equipped with anti-tank missiles. These missiles are mostly medium range (2000–4000 m), though some such as 9M117M1 Arkan has a range of 5,500 m. Others may carry surface-to-air missiles or a combination of the two, such as the 2T Stalker.
Some new vehicles, such as the BTR-90 or Puma, come equipped with 30, 40, 76, or 81 mm grenade launchers. Most IFVs also have a smoke grenade dischargers for concealment.
Mobility.
Some IFVs are amphibious and air transportable. A few, such as the Soviet BMD series or the Chinese ZBD2000, can be deployed by a parachute system from aircraft directly into battle.
Wheeled IFVs can travel great distances on their own without needing to be transported by flat-bed trucks and railway, as required by tracked IFVs. Tracked IFVs need to have their treads serviced or replaced on a regular basis. The tracks themselves and the weight of the IFVs tend to be tough on road surfaces, wearing them down more quickly than a wheeled IFV. Many wheeled IFVs can extract themselves from a battlefield with specialised run-flat tires when not operating in difficult terrain, while an IFV with damaged tracks would require its tracks repaired in the field or an armoured recovery vehicle to tow it out. However, tracks cannot be punctured like a tire so are more difficult to immobilize. A weapon that destroys a track will do significantly more damage to a wheeled vehicle (such as destroying the axles in which case the vehicle will have to be towed away as well). Tracks also offer far greater off-road mobility, greater obstacle climbing capabilities and greater maneuverability than wheels. Unlike a wheeled vehicle, a tracked vehicle will not be stopped by a spike strip or roadblock. A tracked vehicle can reach places a wheeled vehicle can not. As IFVs often operate alongside tanks, being tracked means that the vehicle can go everywhere that the tanks go, which a wheeled vehicle could not. In this case wheeled vehicles would either have to take another route (leaving the tanks without IFV support and the IFVs without tank protection) or the tanks would have to stay where the wheeled IFVs can go and drastically limit their own tactical mobility. Tracked vehicles can attack and operate from and in areas which a wheeled vehicle could not which is a great tactical advantage. A wheeled vehicle is limited to roads and less difficult terrain which may enable the enemy to predict where they will come from. Generally, therefore, tracked vehicles have greater mobility except on roads where wheeled vehicles have a greater top speed. Many countries only use wheeled armour for support functions or low-risk peacekeeping missions whereas combat functions are performed by tracked vehicles. A compromise between wheels and tracks is the band track, which combines some of the advantages of wheels and tracks.
The mobility offered by IFVs is an asset in close combat, especially against fixed artillery. American military simulations in the 1980s concluded a three to five-fold increase in artillery defence effectiveness when IFVs were deployed.
Throughout its life cycle, an IFV is expected to gain 30% more weight from armour additions. This complicates the design by requiring additional power.

</doc>
<doc id="15167" url="https://en.wikipedia.org/wiki?curid=15167" title="ICQ">
ICQ

ICQ is an open source instant messaging computer program that was first developed and popularized by the Israeli company Mirabilis in 1996. The name ICQ stands for "I Seek You". Its ownership was passed to AOL in 1998 and to Mail.Ru Group in 2010.
The ICQ client application and service were initially released in November 1996 and the client was freely available to download. Users could register an account and would be assigned a number, like a phone number, for others to be able to contact them (users could also provide handles). ICQ was the first stand-alone instant messenger and the first online instant messenger service as such — while real-time chat was not in itself new to the internet (IRC being the most common platform at the time), the concept of a fully centralized service with individual user accounts focused on one-on-one conversations set the blueprint for later instant messaging services like AIM, and its influence is seen in modern social media applications.
At its peak around 2001, ICQ had over 100 million accounts registered. At the time of the Mail.Ru acquisition in 2010, there were around 42 million daily users. Since 2013 ICQ has 11 million monthly users.
Features.
Additional products.
ICQ provides all users additional services and content products:
Also, users can choose and select their own avatar for their profile page. In this way, they can protect their privacy.
UIN.
ICQ users are identified by numbers called "UIN", distributed in sequential order. The UIN was first invented by Mirabilis, as the user name assigned to each user upon registration. The meaning is either "Universal Internet Number" or "Unified Identification Number". Issued UINs started at number "10000" (5 digits) and every user receives a UIN when first registering to ICQ. As of ICQ6, users are also able to log in using the specific e-mail address they associated with their UIN during the registration process.
Unlike other instant messaging software or web applications, on ICQ the only permanent user info is the UIN, although it is possible to search for other users using their associated e-mail address or any other detail they have made public (by updating it in the public profile). In addition, the user can change all of his or her personal information, including screen name and e-mail address, without having to re-register. Since 2000, ICQ and AIM users are able to add each other to their contact list without the need for any external clients. As a response to UIN theft or sale of attractive UINs, ICQ started to store email addresses previously associated to a UIN. As such, UINs that are stolen can sometimes be reclaimed. This applies only if (since 1999 onwards) a valid primary email address was entered into the user profile.
History.
Mirabilis was first established in June 1996 by five Israelis: Yair Goldfinger, Sefi Vigiser, Amnon Amir, and Arik Vardi, plus Arik's father Yossi Vardi. They recognized that many people were accessing the internet through non-UNIX operating systems such as Microsoft Windows, and those users were unfamiliar with established chat technologies, e.g. IRC.
The technology Mirabilis developed for ICQ was distributed free of charge. The technology's success encouraged AOL to acquire Mirabilis on June 8, 1998, for $287 million up front and $120 million in additional payments over three years based on performance levels. At the time, this was the highest price ever paid to purchase an Israeli technology company. In 2002, AOL successfully patented the technology.
After the purchase, the product was initially managed by Ariel Yarnitsky and Avi Shechter. ICQ's management changed at the end of 2003. Under the leadership of the new CEO, Orey Gilliam, who also assumed the responsibility for all of AOL's messaging business in 2007, ICQ resumed its growth; it was not only a highly profitable company, but one of AOL's most successful businesses. Eliav Moshe replaced Gilliam in 2009 and became ICQ's managing director.
In April 2010, AOL sold ICQ to Digital Sky Technologies, headed by Alisher Usmanov, for $187.5 million. While ICQ was displaced by AOL Instant Messenger, Google Talk, and other competitors in the U.S. and many other countries over the 2000s, it remained the most popular instant messaging network in Russian-speaking countries, and an important part of online culture. Popular UINs demanded over 11,000 rubles in 2010.
In September of that year, Digital Sky Technologies changed its name to Mail.Ru Group. Since the acquisition, Mail.ru has invested in turning ICQ from a desktop client to a mobile messaging system. As of 2013, around half of ICQ’s users were using its mobile apps, and in 2014, the number of users began growing for the first time since the purchase.
In March 2016 the source code of the client was released under the Apache license released on github.com.
Criticism.
According to security analyst Jeffrey Carr, use of ICQ may cause security problems because it was purchased by Russian investment company Digital Sky Technologies. ICQ has fallen out of fashion in the US and the UK, but it remains popular in Eastern Europe and Russia. Carr says the new ownership may be used by Russia's powerful secret service, the FSB (formerly the KGB) since Russian law, like the American law, requires ICQ to open its logs whenever they want. Similar concerns apply to other instant messengers (see Skype security, YIM SPIM, AIM vulnerabilities etc.).
Privacy and copyright.
When accepting "ICQ Terms Of Service—Acceptable Use Policy" (2000), a user gives all the copyright in the posted information to ICQ Inc. This implies that ICQ Inc. may publish, distribute etc. any messages sent through the system that could be meant to be private:
You agree that by posting any material or information anywhere on the ICQ Services and Information you surrender your copyright and any other proprietary right in the posted material or information. You further agree that ICQ Inc. is entitled to use at its own discretion any of the posted material or information in any manner it deems fit, including, but not limited to, publishing the material or distributing it.
ICQ accounts may be deleted by user. The web page no longer functions, giving "Invalid ICQ UIN, email address, or password. Please fill out the form again" error.
Pressure on alternative clients.
AOL pursued an aggressive policy regarding alternative ("unauthorized") ICQ clients.
System Message
On icq.com there is an "important message" for Russian-speaking ICQ users: "ICQ осуществляет поддержку только авторизированных версий программ: ICQ Lite и ICQ 6.5." ("ICQ supports only authorized versions of programs: ICQ Lite and ICQ 6.5.")
Clients.
AOL's OSCAR network protocol used by ICQ is proprietary and using a third party client is a violation of ICQ Terms of Service, nevertheless a number of third-party clients have been created by using reverse-engineering and protocol descriptions. These clients include:
AOL supported clients include:

</doc>
<doc id="15169" url="https://en.wikipedia.org/wiki?curid=15169" title="Impressionism">
Impressionism

Impressionism is a 19th-century art movement that originated with a group of Paris-based artists whose independent exhibitions brought them to prominence during the 1870s and 1880s. Impressionist painting characteristics include relatively small, thin, yet visible brush strokes, open composition, emphasis on accurate depiction of light in its changing qualities (often accentuating the effects of the passage of time), ordinary subject matter, inclusion of "movement" as a crucial element of human perception and experience, and unusual visual angles.
The Impressionists faced harsh opposition from the conventional art community in France. The name of the style derives from the title of a Claude Monet work, "Impression, soleil levant" ("Impression, Sunrise"), which provoked the critic Louis Leroy to coin the term in a satirical review published in the Parisian newspaper "Le Charivari".
The development of Impressionism in the visual arts was soon followed by analogous styles in other media that became known as impressionist music and impressionist literature.
Overview.
Impressionism emerged in France at the same time that a number of other painters, including the Italian artists known as the Macchiaioli, and Winslow Homer in the United States, were also exploring "plein-air" painting. The Impressionists, however, developed new techniques specific to the style. Encompassing what its adherents argued was a different way of seeing, it is an art of immediacy and movement, of candid poses and compositions, of the play of light expressed in a bright and varied use of colour.
The public, at first hostile, gradually came to believe that the Impressionists had captured a fresh and original vision, even if the art critics and art establishment disapproved of the new style.
By recreating the sensation in the eye that views the subject, rather than delineating the details of the subject, and by creating a welter of techniques and forms, Impressionism is a precursor of various painting styles, including Neo-Impressionism, Post-Impressionism, Fauvism, and Cubism.
Beginnings.
In the middle of the 19th century—a time of change, as Emperor Napoleon III rebuilt Paris and waged war—the Académie des Beaux-Arts dominated French art. The Académie was the preserver of traditional French painting standards of content and style. Historical subjects, religious themes, and portraits were valued; landscape and still life were not. The Académie preferred carefully finished images that looked realistic when examined closely. Paintings in this style were made up of precise brush strokes carefully blended to hide the artist's hand in the work. Colour was restrained and often toned down further by the application of a golden varnish.
The Académie had an annual, juried art show, the Salon de Paris, and artists whose work was displayed in the show won prizes, garnered commissions, and enhanced their prestige. The standards of the juries represented the values of the Académie, represented by the works of such artists as Jean-Léon Gérôme and Alexandre Cabanel.
In the early 1860s, four young painters—Claude Monet, Pierre-Auguste Renoir, Alfred Sisley, and Frédéric Bazille—met while studying under the academic artist Charles Gleyre. They discovered that they shared an interest in painting landscape and contemporary life rather than historical or mythological scenes. Following a practice that had become increasingly popular by mid-century, they often ventured into the countryside together to paint in the open air, but not for the purpose of making sketches to be developed into carefully finished works in the studio, as was the usual custom. By painting in sunlight directly from nature, and making bold use of the vivid synthetic pigments that had become available since the beginning of the century, they began to develop a lighter and brighter manner of painting that extended further the Realism of Gustave Courbet and the Barbizon school. A favourite meeting place for the artists was the Café Guerbois on Avenue de Clichy in Paris, where the discussions were often led by Édouard Manet, whom the younger artists greatly admired. They were soon joined by Camille Pissarro, Paul Cézanne, and Armand Guillaumin.
During the 1860s, the Salon jury routinely rejected about half of the works submitted by Monet and his friends in favour of works by artists faithful to the approved style. In 1863, the Salon jury rejected Manet's "The Luncheon on the Grass" "(Le déjeuner sur l'herbe)" primarily because it depicted a nude woman with two clothed men at a picnic. While the Salon jury routinely accepted nudes in historical and allegorical paintings, they condemned Manet for placing a realistic nude in a contemporary setting. The jury's severely worded rejection of Manet's painting appalled his admirers, and the unusually large number of rejected works that year perturbed many French artists.
After Emperor Napoleon III saw the rejected works of 1863, he decreed that the public be allowed to judge the work themselves, and the Salon des Refusés (Salon of the Refused) was organized. While many viewers came only to laugh, the Salon des Refusés drew attention to the existence of a new tendency in art and attracted more visitors than the regular Salon.
Artists' petitions requesting a new Salon des Refusés in 1867, and again in 1872, were denied. In December 1873, Monet, Renoir, Pissarro, Sisley, Cézanne, Berthe Morisot, Edgar Degas and several other artists founded the "Société Anonyme Coopérative des Artistes Peintres, Sculpteurs, Graveurs" ("Cooperative and Anonymous Association of Painters, Sculptors, and Engravers") to exhibit their artworks independently. Members of the association were expected to forswear participation in the Salon. The organizers invited a number of other progressive artists to join them in their inaugural exhibition, including the older Eugène Boudin, whose example had first persuaded Monet to adopt "plein air" painting years before. Another painter who greatly influenced Monet and his friends, Johan Jongkind, declined to participate, as did Édouard Manet. In total, thirty artists participated in their first exhibition, held in April 1874 at the studio of the photographer Nadar.
The critical response was mixed. Monet and Cézanne received the harshest attacks. Critic and humorist Louis Leroy wrote a scathing review in the newspaper "Le Charivari" in which, making wordplay with the title of Claude Monet's "Impression, Sunrise" "(Impression, soleil levant)", he gave the artists the name by which they became known. Derisively titling his article "", Leroy declared that Monet's painting was at most, a sketch, and could hardly be termed a finished work.
He wrote, in the form of a dialog between viewers,
The term "Impressionist" quickly gained favour with the public. It was also accepted by the artists themselves, even though they were a diverse group in style and temperament, unified primarily by their spirit of independence and rebellion. They exhibited together—albeit with shifting membership—eight times between 1874 and 1886. The Impressionists' style, with its loose, spontaneous brushstrokes, would soon become synonymous with modern life.
Monet, Sisley, Morisot, and Pissarro may be considered the "purest" Impressionists, in their consistent pursuit of an art of spontaneity, sunlight, and colour. Degas rejected much of this, as he believed in the primacy of drawing over colour and belittled the practice of painting outdoors. Renoir turned away from Impressionism for a time during the 1880s, and never entirely regained his commitment to its ideas. Édouard Manet, although regarded by the Impressionists as their leader, never abandoned his liberal use of black as a colour, and never participated in the Impressionist exhibitions. He continued to submit his works to the Salon, where his painting "Spanish Singer" had won a 2nd class medal in 1861, and he urged the others to do likewise, arguing that "the Salon is the real field of battle" where a reputation could be made.
Among the artists of the core group (minus Bazille, who had died in the Franco-Prussian War in 1870), defections occurred as Cézanne, followed later by Renoir, Sisley, and Monet, abstained from the group exhibitions so they could submit their works to the Salon. Disagreements arose from issues such as Guillaumin's membership in the group, championed by Pissarro and Cézanne against opposition from Monet and Degas, who thought him unworthy. Degas invited Mary Cassatt to display her work in the 1879 exhibition, but also insisted on the inclusion of Jean-François Raffaëlli, Ludovic Lepic, and other realists who did not represent Impressionist practices, causing Monet in 1880 to accuse the Impressionists of "opening doors to first-come daubers". The group divided over invitations to Paul Signac and Georges Seurat to exhibit with them in 1886. Pissarro was the only artist to show at all eight Impressionist exhibitions.
The individual artists achieved few financial rewards from the Impressionist exhibitions, but their art gradually won a degree of public acceptance and support. Their dealer, Durand-Ruel, played a major role in this as he kept their work before the public and arranged shows for them in London and New York. Although Sisley died in poverty in 1899, Renoir had a great Salon success in 1879. Monet became secure financially during the early 1880s and so did Pissarro by the early 1890s. By this time the methods of Impressionist painting, in a diluted form, had become commonplace in Salon art.
Impressionist techniques.
French painters who prepared the way for Impressionism include the Romantic colourist Eugène Delacroix, the leader of the realists Gustave Courbet, and painters of the Barbizon school such as Théodore Rousseau. The Impressionists learned much from the work of Jean-Baptiste-Camille Corot and Eugène Boudin, who painted from nature in a direct and spontaneous style that prefigured Impressionism, and who befriended and advised the younger artists.
A number of identifiable techniques and working habits contributed to the innovative style of the Impressionists. Although these methods had been used by previous artists—and are often conspicuous in the work of artists such as Frans Hals, Diego Velázquez, Peter Paul Rubens, John Constable, and J. M. W. Turner—the Impressionists were the first to use them all together, and with such consistency. These techniques include:
New technology played a role in the development of the style. Impressionists took advantage of the mid-century introduction of premixed paints in tin tubes (resembling modern toothpaste tubes), which allowed artists to work more spontaneously, both outdoors and indoors. Previously, painters made their own paints individually, by grinding and mixing dry pigment powders with linseed oil, which were then stored in animal bladders.
Many vivid synthetic pigments became commercially available to artists for the first time during the 19th century. These included cobalt blue, viridian, cadmium yellow, and synthetic ultramarine blue, all of which were in use by the 1840s, before Impressionism. The Impressionists' manner of painting made bold use of these pigments, and of even newer colours such as cerulean blue, which became commercially available to artists in the 1860s.
The Impressionists' progress toward a brighter style of painting was gradual. During the 1860s, Monet and Renoir sometimes painted on canvases prepared with the traditional red-brown or grey ground. By the 1870s, Monet, Renoir, and Pissarro usually chose to paint on grounds of a lighter grey or beige colour, which functioned as a middle tone in the finished painting. By the 1880s, some of the Impressionists had come to prefer white or slightly off-white grounds, and no longer allowed the ground colour a significant role in the finished painting.
Content and composition.
Prior to the Impressionists, other painters, notably such 17th-century Dutch painters as Jan Steen, had emphasized common subjects, but their methods of composition were traditional. They arranged their compositions so that the main subject commanded the viewer's attention. The Impressionists relaxed the boundary between subject and background so that the effect of an Impressionist painting often resembles a snapshot, a part of a larger reality captured as if by chance. Photography was gaining popularity, and as cameras became more portable, photographs became more candid. Photography inspired Impressionists to represent momentary action, not only in the fleeting lights of a landscape, but in the day-to-day lives of people. 
The development of Impressionism can be considered partly as a reaction by artists to the challenge presented by photography, which seemed to devalue the artist's skill in reproducing reality. Both portrait and landscape paintings were deemed somewhat deficient and lacking in truth as photography "produced lifelike images much more efficiently and reliably".
In spite of this, photography actually inspired artists to pursue other means of creative expression, and rather than compete with photography to emulate reality, artists focused "on the one thing they could inevitably do better than the photograph—by further developing into an art form its very subjectivity in the conception of the image, the very subjectivity that photography eliminated". The Impressionists sought to express their perceptions of nature, rather than create exact representations. This allowed artists to depict subjectively what they saw with their "tacit imperatives of taste and conscience". Photography encouraged painters to exploit aspects of the painting medium, like colour, which photography then lacked: "The Impressionists were the first to consciously offer a subjective alternative to the photograph".
Another major influence was Japanese ukiyo-e art prints (Japonism). The art of these prints contributed significantly to the "snapshot" angles and unconventional compositions that became characteristic of Impressionism. An example is Monet's "Jardin à Sainte-Adresse", 1867, with its bold blocks of colour and composition on a strong diagonal slant showing the influence of Japanese prints
Edgar Degas was both an avid photographer and a collector of Japanese prints. His "The Dance Class" "(La classe de danse)" of 1874 shows both influences in its asymmetrical composition. The dancers are seemingly caught off guard in various awkward poses, leaving an expanse of empty floor space in the lower right quadrant. He also captured his dancers in sculpture, such as the "Little Dancer of Fourteen Years".
Main Impressionists.
The central figures in the development of Impressionism in France, listed alphabetically, were:
Timeline: Lives of the Impressionists.
The Impressionists
Associates and influenced artists.
Among the close associates of the Impressionists were several painters who adopted their methods to some degree. These include Giuseppe De Nittis, an Italian artist living in Paris who participated in the first Impressionist exhibit at the invitation of Degas, although the other Impressionists disparaged his work. Federico Zandomeneghi was another Italian friend of Degas who showed with the Impressionists. Eva Gonzalès was a follower of Manet who did not exhibit with the group. James Abbott McNeill Whistler was an American-born painter who played a part in Impressionism although he did not join the group and preferred grayed colours. Walter Sickert, an English artist, was initially a follower of Whistler, and later an important disciple of Degas; he did not exhibit with the Impressionists. In 1904 the artist and writer Wynford Dewhurst wrote the first important study of the French painters published in English, "Impressionist Painting: its genesis and development", which did much to popularize Impressionism in Great Britain.
By the early 1880s, Impressionist methods were affecting, at least superficially, the art of the Salon. Fashionable painters such as Jean Béraud and Henri Gervex found critical and financial success by brightening their palettes while retaining the smooth finish expected of Salon art. Works by these artists are sometimes casually referred to as Impressionism, despite their remoteness from Impressionist practice.
The influence of the French Impressionists lasted long after most of them had died. Artists like J.D. Kirszenbaum were borrowing Impressionist techniques throughout the twentieth century.
Beyond France.
As the influence of Impressionism spread beyond France, artists, too numerous to list, became identified as practitioners of the new style. Some of the more important examples are:
Sculpture, photography and film.
The sculptor Auguste Rodin is sometimes called an Impressionist for the way he used roughly modeled surfaces to suggest transient light effects.
Pictorialist photographers whose work is characterized by soft focus and atmospheric effects have also been called Impressionists.
French Impressionist Cinema is a term applied to a loosely defined group of films and filmmakers in France from 1919–1929, although these years are debatable. French Impressionist filmmakers include Abel Gance, Jean Epstein, Germaine Dulac, Marcel L’Herbier, Louis Delluc, and Dmitry Kirsanoff.
Music and literature.
Musical Impressionism is the name given to a movement in European classical music that arose in the late 19th century and continued into the middle of the 20th century. Originating in France, musical Impressionism is characterized by suggestion and atmosphere, and eschews the emotional excesses of the Romantic era. Impressionist composers favoured short forms such as the nocturne, arabesque, and prelude, and often explored uncommon scales such as the whole tone scale. Perhaps the most notable innovations of Impressionist composers were the introduction of major 7th chords and the extension of chord structures in 3rds to five- and six-part harmonies.
The influence of visual Impressionism on its musical counterpart is debatable. Claude Debussy and Maurice Ravel are generally considered the greatest Impressionist composers, but Debussy disavowed the term, calling it the invention of critics. Erik Satie was also considered in this category, though his approach was regarded as less serious, more musical novelty in nature. Paul Dukas is another French composer sometimes considered an Impressionist, but his style is perhaps more closely aligned to the late Romanticists. Musical Impressionism beyond France includes the work of such composers as Ottorino Respighi (Italy) Ralph Vaughan Williams, Cyril Scott, and John Ireland (England), and Manuel De Falla, and Isaac Albeniz (Spain).
The term Impressionism has also been used to describe works of literature in which a few select details suffice to convey the sensory impressions of an incident or scene. Impressionist literature is closely related to Symbolism, with its major exemplars being Baudelaire, Mallarmé, Rimbaud, and Verlaine. Authors such as Virginia Woolf, D.H. Lawrence, and Joseph Conrad have written works that are Impressionistic in the way that they describe, rather than interpret, the impressions, sensations and emotions that constitute a character's mental life.
Post-Impressionism.
Post-Impressionism developed from Impressionism. During the 1880s several artists began to develop different precepts for the use of colour, pattern, form, and line, derived from the Impressionist example: Vincent van Gogh, Paul Gauguin, Georges Seurat, and Henri de Toulouse-Lautrec. These artists were slightly younger than the Impressionists, and their work is known as post-Impressionism. Some of the original Impressionist artists also ventured into this new territory; Camille Pissarro briefly painted in a pointillist manner, and even Monet abandoned strict "plein air" painting. Paul Cézanne, who participated in the first and third Impressionist exhibitions, developed a highly individual vision emphasising pictorial structure, and he is more often called a post-Impressionist. Although these cases illustrate the difficulty of assigning labels, the work of the original Impressionist painters may, by definition, be categorised as Impressionism.

</doc>
<doc id="15172" url="https://en.wikipedia.org/wiki?curid=15172" title="Internet slang">
Internet slang

Internet slang (Internet shorthand, Cyber-slang, netspeak, or chatspeak) refers to a variety of slang languages used by different people on the Internet. It is difficult to provide a standardized definition of Internet slang due to the constant changes made to its nature. However, it can be understood to be a type of slang that Internet users have popularized, and in many cases, have coined. Such terms often originate with the purpose of saving keystrokes or to compensate for small character limits. Many people use the same abbreviations in texting and instant messaging, and social networking websites. Acronyms, keyboard symbols and abbreviations are common types of Internet slang. New dialects of slang, such as leet or Lolspeak, develop as ingroup internet memes rather than time savers.
Creation and evolution.
Origins.
Internet slang originated in the early days of the Internet with some terms predating the Internet. Internet slang is used in chat rooms, social networking services, online games, video games and in the online community. Since 1979, users of communications networks like Usenet created their own shorthand.
In pop culture.
In Japanese, the term moe has come into common use among slang users to mean something extremely cute and appealing.
Aside from the more frequent abbreviations, acronyms, and emoticons, Internet slang also uses archaic words or the lesser-known meanings of mainstream terms. Regular words can also be altered into something with a similar pronunciation but altogether different meaning, or attributed new meanings altogether. Phonetic transcriptions of foreign words, such as the transformation of "impossible" into "impossibru" in Japanese and then back to English, also occur. In places where logographic languages are used, such as China, a visual Internet slang exists, giving characters dual meanings, one direct and one implied.
Motivations.
The primary motivation for using a slang unique to the Internet is to ease communication. However, while Internet slang shortcuts save time for the writer, they take two times as long for the reader to understand, according to a study by the University of Tasmania. On the other hand, similar to the use of slang in traditional face-to-face speech or written language, slang on the Internet is often a way of indicating group membership.
Internet slang provides a channel which facilitates and constrains our ability to communicate in ways that are fundamentally different from those found in other semiotic situations. Many of the expectations and practices which we associate with spoken and written language are no longer applicable. The Internet itself is ideal for new slang to emerge because of the richness of the medium and the availability of information. Slang is also thus motivated for the “creation and sustenance of online communities”. These communities in turn play a role in solidarity or identification or an exclusive or common cause.
Crystal distinguishes among five Internet situations: The Web, email, asynchronous chat (for example, mailing lists), synchronous chat (for example, Internet Relay Chat), and virtual worlds. The electronic character of the channel has a fundamental influence on the language of the medium. The options of communication for the user are constrained by the nature of the hardware needed in order to gain Internet access. Thus, productive linguistic capacity (the type of information that can be sent) is determined by the preassigned characters on a keyboard, and receptive linguistic capacity (the type of information that can be seen) is determined by the size and configuration of the screen. Additionally, both sender and receiver are constrained linguistically by the properties of the internet software, computer hardware, and networking hardware linking them. Electronic discourse refers to writing that is "very often reads as if it were being spoken – that is, as if the sender were writing talking".
Types of slang.
Internet slang does not constitute a homogeneous language variety. Rather, it differs according to the user and type of Internet situation. However, within the language of Internet slang, there is still an element of prescriptivism, as seen in style guides, for example "Wired Style", which are specifically aimed at usage on the Internet. Even so, few users consciously heed these prescriptive recommendations on CMC, but rather adapt their styles based on what they encounter online. Although it is difficult to produce a clear definition of Internet slang, the following types of slang may be observed. This list is not exhaustive.
Views on Internet slang.
There have been ongoing debates about how the use of slang on the Internet influences language usage outside of technology. Even though the direct causal relationship between the Internet and language has yet to be proven by any scientific research, Internet slang has invited split views on its influence on the standard of language use in non-computer-mediated communications.
Prescriptivists tend to have the widespread belief that the Internet has a negative influence on the future of language, and that it would lead to a degradation of standard. Some would even attribute any declination of standard formal English to the increase in usage of electronic communication. It has also been suggested that the linguistic differences between Standard English and CMC can have implications for literacy education. This is illustrated by the widely reported example of a school essay submitted by a Scottish teenager, which contained many abbreviations and acronyms likened to SMS language. There was great condemnation of this style by the mass media as well as educationists, who expressed that this showed diminishing literacy or linguistic abilities.
On the other hand, descriptivists have counter-argued that the Internet allows better expressions of a language. Rather than established linguistic conventions, linguistic choices sometimes reflect personal taste. It has also been suggested that as opposed to intentionally flouting language conventions, Internet slang is a result of a lack of motivation to monitor speech online. Hale and Scalon describe language in Emails as being derived from "writing the way people talk", and that there is no need to insist on 'Standard' English. English users, in particular, have an extensive tradition of etiquette guides, instead of traditional prescriptive treatises, that offer pointers on linguistic appropriateness. Using and spreading Internet slang also adds onto the cultural currency of a language. It is important to the speakers of the language due to the foundation it provides for identifying within a group, and also for defining a person’s individual linguistic and communicative competence. The result is a specialized subculture based on its use of slang.
In scholarly research, attention has, for example, been drawn to the effect of the use of Internet slang in ethnography, and more importantly to how conversational relationships online change structurally because slang is used.
In German, there is already considerable controversy regarding the use of anglicisms outside of CMC. This situation is even more problematic within CMC, since the jargon of the medium is dominated by English terms. An extreme example of an anti-anglicisms perspective can be observed from the chatroom rules of a Christian site, which bans all anglicisms ("Das Verwenden von Anglizismen ist strengstens untersagt!"), and also translates even fundamental terms into German equivalents.
Journalism.
In April 2014, Gawker's editor-in-chief Max Read instituted new writing style guidelines banning internet slang for his writing staff.
Use beyond computer-mediated communication.
Internet slang has crossed from being mediated by the computer into other non-physical domains. Here, these domains are taken to refer to any domain of interaction where interlocutors need not be geographically proximate to one another, and where the Internet is not primarily used. Internet slang is now prevalent in telephony, mainly through short messages (SMS) communication. Abbreviations and interjections, especially, have been popularized in this medium, perhaps due to the limited character space for writing messages on mobile phones. Another possible reason for this spread is the convenience of transferring the existing mappings between expression and meaning into a similar space of interaction.
At the same time, Internet slang has also taken a place as part of everyday offline language, among those with digital access. The nature and content of online conversation is brought forward to direct offline communication through the telephone and direct talking, as well as through written language, such as in writing notes or letters. In the case of interjections, such as numerically based and abbreviated Internet slang, are not pronounced as they are written physically or replaced by any actual action. Rather, they become lexicalized and spoken like non-slang words in a “stage direction” like fashion, where the actual action is not carried out but substituted with a verbal signal. The notions of flaming and trolling have also extended outside of the computer, and are used in the same circumstances of deliberate or unintentional implicatures.
The expansion of Internet slang has been furthered through codification and the promotion of digital literacy. The subsequently existing and growing popularity of such references among those online as well as offline has thus advanced Internet slang literacy and globalized it. Awareness and proficiency in manipulating Internet slang in both online and offline communication indicates digital literacy and teaching materials have even been developed to further this knowledge. A South Korean publisher, for example, has published a textbook that details the meaning and context of use for common Internet slang instances and is targeted at young children who will soon be using the Internet. Similarly, Internet slang has been recommended as language teaching material in second language classrooms in order to raise communicative competence by imparting some of the cultural value attached to a language that is available only in slang.
Meanwhile, well-known dictionaries such as the OED and Merriam-Webster have been updated with a significant and growing body of slang jargon. Besides the all too common examples, lesser known slang and slang with a non-English etymology have also found place in standardized linguistic references. Along with these instances, literature in user-contributed dictionaries such as Urban Dictionary has also been added on to. Codification seems to be qualified through frequency of use, and novel creations are often not accepted by other users of slang.
Internet slang today.
Although Internet slang began as a means of "opposition" to mainstream language, its popularity with today's globalized digitally literate population has shifted it into a part of everyday language, where it also leaves a profound impact.
Frequently used slang also have become conventionalised into memetic "unit of cultural information". These memes in turn are further spread through their use on the Internet, prominently through websites. The Internet as an "information superhighway" is also catalysed through slang. The evolution of slang has also created a 'slang union' as part of a unique, specialised subculture. Such impacts are, however, limited and requires further discussion especially from the non-English world. This is because Internet slang is prevalent in languages more actively used on the Internet, like English, which is the Internet’s lingua franca.
Internet slang around the world.
The Internet has helped people from all over the world to become connected to one another, enabling "global" relationships to be formed. As such, it is important for the various types of slang used online to be recognizable for everyone. It is also important to do so because of how other languages are quickly catching up with English on the Internet, following the increase in Internet usage in countries predominantly non-English speaking. In fact, as of May 31, 2011, only approximately 27% of the online population is made up of English speakers.
Different cultures tend to have different motivations behind their choice of slang, on top of the difference in language used. For example, in China, because of the tough Internet regulations imposed, users tend to use certain slang to talk about issues deemed as sensitive to the government. These include using symbols to separate the characters of a word into other to avoid detection and hence resulting in censorship. An outstanding example is the use of the term river crab to denote censorship. River crab (hexie) is pronounced the same as "harmony"—the official discourse used to justify political discipline and censorship. As such Chinese netizens reappropriates the official language in a sarcastic way.
Abbreviations are popular across different cultures, including countries like Japan, China, France, Portugal, etc., and are used according to the particular language the Internet users speak. Significantly, this same style of slang creation is also found in non-alphabetical languages as, for example, a form of 'e gao' or alternative political discourse.
The difference in language often results in miscommunication, as seen in an Onomatopoeic example, "555", which sounds like "crying" in Chinese, and "laughing" in Thai. A similar example is between the English "haha" and the Spanish "jaja", where both are onomatopoeic expressions of laughter, but the difference in language also meant a different consonant for the same sound to be produced. For more examples of how other languages express "laughing out loud", see also: LOL
In terms of culture, in Chinese, the numerically based onomatopoeia "770880" (), which means to 'kiss and hug you', is used. This is comparable to "XOXO", which many Internet users use. In French, "pkoi" is used in the place of pourquoi, which means why. This is an example of a combination of onomatopoeia and shortening of the original word for convenience when writing online.
In conclusion, every different country has their own language background and cultural differences and hence they tend to have their own rules and motivations for their own Internet slang. However, at present, there is still a lack of studies done by researchers on some differences between the countries.
On the whole, the popular use of Internet slang has resulted in a unique online and offline community as well as a couple sub-categories of "special internet slang which is different from other slang spread in the whole internet… similar to jargon … usually decided by the sharing community". It has also led to virtual communities marked by the specific slang they use and led to a more homogenized yet diverse online culture.

</doc>
<doc id="15174" url="https://en.wikipedia.org/wiki?curid=15174" title="Impi">
Impi

Impi is a Zulu word for any armed body of men. However, in English it is often used to refer to a Zulu regiment, which is called an "ibutho" in Zulu. Its beginnings lie far back in historic tribal warfare customs, when groups of armed men called "impis" battled. They were systematised radically by the Zulu king Shaka, who was then only the exiled illegitimate son of king Senzangakona, but already showing much prowess as a general in the army of Mthethwa king Dingiswayo in the Mthethwa-Ndwandwe war in the early 1810s.
Genesis of the impi.
The Zulu impi is popularly identified with the ascent of Shaka, ruler of the relatively small Zulu tribe before its explosion across the landscape of southern Africa, but its earliest shape as a purposeful instrument of statecraft lies in the innovations of the Mwetha chieftain Dingiswayo, according to some historians (Morris 1965). These innovations in turn drew upon existing tribal customs, such as the "iNtanga". This was an age grade tradition common among many of the Bantu peoples of the continent's southern region. Youths were organised into age groups, with each cohort responsible for certain duties and tribal ceremonies. Periodically, the older age grades were summoned to the kraals of sub-chieftains, or "inDunas", for consultations, assignments, and an induction ceremony that marked their transition from boys to full-fledged adults and warriors, the "ukuButbwa". Kraal or settlement elders generally handled local disputes and issues. Above them were the inDunas, and above the inDunas stood the chief of a particular clan lineage or tribe. The inDunas handled administrative matters for their chiefs – ranging from settlement of disputes, to the collection of taxes. In time of war, the inDunas supervised the fighting men in their areas, forming leadership of the military forces deployed for combat. The age grade "iNtangas", under the guidance of the inDunas, formed the basis for the systematic regimental organisation that would become known worldwide as the impi.
Limited nature of early tribal warfare.
Militarily warfare was mild among the Bantu prior to the rise of Shaka, though it occurred frequently. Objectives were typically limited to such matters as recovering cattle, avenging some personal insult, or resolving disputes over segments of grazing land. Generally a loose mob, called an "impi" participated in these melees. There were no campaigns of extermination against the defeated. They simply moved on to other open spaces on the veldt, and equilibrium was restored. The bow and arrow were known but seldom used. Warfare, like the hunt, depended on skilled spearmen and trackers. The primary weapon was a thin 6-foot throwing spear, the "assegai". Several were carried into combat. Defensive weapons included a small cowhide shield, which was later improved by King Shaka. Many battles were prearranged, with the clan warriors meeting at an assigned place and time, while women and children of the clan watched the festivities from some distance away. Ritualized taunts, single combats and tentative charges were the typical pattern. If the affair did not dissipate before, one side might find enough courage to mount a sustained attack, driving off their enemies. Casualties were usually light. The defeated clan might pay in lands or cattle and have captives to be ransomed, but extermination and mass casualties were rare. Tactics were rudimentary. Outside the ritual battles, the quick raid was the most frequent combat action, marked by burning kraals, seizure of captives, and the driving off of cattle. Pastoral herders and light agriculturalists, the Bantu did not usually build permanent fortifications to fend off enemies. A clan under threat simply packed their meager material possessions, rounded up their cattle and fled until the marauders were gone. If the marauders did not stay to permanently dispossess them of grazing areas, the fleeing clan might return to rebuild in a day or two. The genesis of the Zulu impi thus lies in tribal structures existing long before the coming of Europeans or the Shaka era.
Rise of Dingiswayo.
In the early 19th century, a combination of factors began to change the customary pattern. These included rising populations, the growth of white settlement and slaving that dispossessed native peoples both at the Cape and in Portuguese Mozambique, and the rise of ambitious "new men." One such man, a warrior called Dingiswayo ("the Troubled One") of the Mtetwa rose to prominence. Historians such as Donald Morris hold that his political genius laid the basis for a relatively light hegemony. This was established through a combination of diplomacy and conquest, using not extermination or slavery, but strategic reconciliation and judicious force of arms. This hegemony reduced the frequent feuding and fighting among the small clans in the Metetwa's orbit, transferring their energies to more centralised forces. Under Dingiswayo the age grades came to be regarded as military drafts, deployed more frequently to maintain the new order. It was from these small clans, including among them the eLangeni and the Zulu, that Shaka sprung.
Ascent and innovations of Shaka.
Shaka proved himself to be one of Dingiswayo's most able warriors after the military call up of his age grade to serve in the Mthethwa forces. He fought with his iziCwe regiment wherever he was assigned during this early period, but from the beginning, Shaka's approach to battle did not fit the traditional mould. He began to implement his own individual methods and style, designing the famous short stabbing spear the "iKlwa", a larger, stronger shield, and discarding the oxhide sandals that he felt slowed him down. These methods proved effective on a small scale, but Shaka himself was restrained by his overlord. His conception of warfare was far more extreme that the reconcilitory methods of Dingiswayo. He sought to bring combat to a swift and bloody decision, as opposed to duels of individual champions, scattered raids, or limited skirmishes where casualties were comparatively light. While his mentor and overlord Dingiswayo lived, Shakan methods were reined in, but the removal of this check gave the Zulu chieftain much broader scope. It was under his rule that a much more rigorous mode of tribal warfare came into being. This newer, brutal focus demanded changes in weapons, organisation and tactics.
Weapons and shields.
Shaka is credited with introducing a new variant of the traditional weapon, demoting the long, spindly throwing spear in favour of a heavy-bladed, short-shafted stabbing spear. He is also said to have introduced a larger, heavier cowhide shield ("isihlangu"), and trained his forces to thus close with the enemy in more effective hand-to-hand combat. The throwing spear was not discarded, but standardised like the stabbing implement and carried as a missile weapon, typically discharged at the foe, before close contact. These weapons changes integrated with and facilitated an aggressive mobility and tactical organisation.
As weapons, the Zulu warrior carried the "iklwa" stabbing spear (losing one could result in execution) and a club or cudgel fashioned from dense hardwood known in Zulu as the "iwisa", usually called the knobkerrie in English, for beating an enemy in the manner of a mace. Zulu officers often carried the Zulu Axe, but this weapon was more of a symbol to show their rank. The iklwa – so named because of the sucking sound it made when withdrawn from a human body – with its long (c. 25 cm in) and broad blade was an invention of Shaka that superseded the older thrown "ipapa" (so named because of the "pa-pa" sound it made as it flew through the air). It could theoretically be used both in melee and as a thrown weapon, but warriors were forbidden in Shaka's day from throwing it, which would disarm them and give their opponents something to throw back. Moreover, Shaka felt it discouraged warriors from closing into hand-to-hand combat. Shaka's brother, and successor, Dingane reintroduced greater use of the throwing spear, perhaps as a counter to Boer firearms.
As early as Shaka's reign small numbers of firearms, often obsolete muskets and rifles, were obtained by the Zulus from Europeans by trade. In the aftermath of the defeat of the British at the Battle of Isandlwana many Martini-Henry rifles were captured by the Zulus together with considerable amounts of ammunition. The advantage of this capture is debatable due to the alleged tendency of Zulu warriors to close their eyes when firing such weapons. The possession of firearms did little to change Zulu tactics, which continued to rely on a swift approach to the enemy to bring him into close combat.
All warriors carried a shield made of oxhide, which retained the hair, with a central stiffening shaft of wood, the "mgobo". Shields were the property of the king; they were stored in specialised structures, raised off the ground for protection from vermin, when not issued to the relevant regiment. The large "isihlangu" shield of Shaka's day was about five feet in length and was later partially replaced by the smaller "umbumbuluzo," a shield of identical manufacture but around three and a half feet in length. Close combat relied on co-ordinated use of the "iklwa" and shield. The warrior sought to get the edge of his shield behind the edge of his enemy's, so that he could pull the enemy's shield to the side thus opening him to a thrust with the "iklwa" deep into the abdomen or chest.
Logistics.
The fast-moving host, like all military formations, needed supplies. These were provided by young boys, who were attached to a force and carried rations, cooking pots, sleeping mats, extra weapons and other material. Cattle were sometimes driven on the hoof as a movable larder. Again, such arrangements in the local context were probably nothing unusual. What was different was the systematisation and organisation, a pattern yielding major benefits when the Zulu were dispatched on raiding missions.
Age-grade regimental system.
Age-grade groupings of various sorts were common in the Bantu tribal culture of the day, and indeed are still important in much of Africa. Age grades were responsible for a variety of activities, from guarding the camp, to cattle herding, to certain rituals and ceremonies. It was customary in Zulu culture for young men to provide limited service to their local chiefs until they were married and recognised as official householders. Shaka manipulated this system, transferring the customary service period from the regional clan leaders to himself, strengthening his personal hegemony. Such groupings on the basis of age, did not constitute a permanent, paid military in the modern Western sense, nevertheless they did provide a stable basis for sustained armed mobilisation, much more so than ad hoc tribal levies or war parties.
Shaka organised the various age grades into regiments, and quartered them in special military kraals, with each regiment having its own distinctive names and insignia. Some historians argue that the large military establishment was a drain on the Zulu economy and necessitated continual raiding and expansion. This may be true since large numbers of the society's men were isolated from normal occupations, but whatever the resource impact, the regimental system clearly built on existing tribal cultural elements that could be adapted and shaped to fit an expansionist agenda.
After their 20th birthdays, young men would be sorted into formal "ibutho" (plural "amabutho") or regiments. They would build their "i=handa" (often referred to as a 'homestead', as it was basically a stockaded group of huts surrounding a corral for cattle), their gathering place when summoned for active service. Active service continued until a man married, a privilege only the king bestowed. The amabutho were recruited on the basis of age rather than regional or tribal origin. The reason for this was to enhance the centralised power of the Zulu king at the expense of clan and tribal leaders. They swore loyalty to the king of the Zulu nation.
Mobility, training and insignia.
Shaka discarded sandals to enable his warriors to run faster. Initially the move was unpopular, but those who objected were simply killed, a practice that quickly concentrated the minds of remaining personnel. Zulu tradition indicates that Shaka hardened the feet of his troops by having them stamp thorny tree and bush branches flat. Shaka drilled his troops frequently, implementing forced marches covering more than fifty miles a day. He also drilled the troops to carry out encirclement tactics (see below). Such mobility gave the Zulu a significant impact in their local region and beyond. Upkeep of the regimental system and training seems to have continued after Shaka's death, although Zulu defeats by the Boers, and growing encroachment by British colonists, sharply curtailed raiding operations prior to the War of 1879. Morris (1965, 1982) records one such mission under King Mpande to give green warriors of the uThulwana regiment experience: a raid into Swaziland, dubbed ""Fund' uThulwana"" by the Zulu, or "Teach the uThulwana".
Impi warriors were trained as early as age six, joining the army as "udibi" porters at first, being enrolled into same-age groups ("intanga"). Until they were "buta'"d, Zulu boys accompanied their fathers and brothers on campaign as servants. Eventually, they would go to the nearest "ikhanda" to "kleza" (literally, "to drink directly from the udder"), at which time the boys would become "inkwebane", cadets. They would spend their time training until they were formally enlisted by the king. They would challenge each other to stick fights, which had to be accepted on pain of dishonor.
In Shaka's day, warriors often wore elaborate plumes and cow tail regalia in battle, but by the Anglo-Zulu War of 1879, many warriors wore only a loin cloth and a minimal form of headdress. The later period Zulu soldier went into battle relatively simply dressed, painting his upper body and face with chalk and red ochre, despite the popular conception of elaborately panoplied warriors. Each "ibutho" had a singular arrangement of headdress and other adornments, so that the Zulu army could be said to have had regimental uniforms; latterly the 'full-dress' was only worn on festive occasions. The men of senior regiments would wear, in addition to their other headdress, the head-ring ("isicoco") denoting their married state. A gradation of shield colour was found, junior regiments having largely dark shields the more senior ones having shields with more light colouring; Shaka's personal regiment "Fasimba" (The Haze) having white shields with only a small patch of darker colour. This shield uniformity was facilitated by the custom of separating the king's cattle into herds based on their coat colours.
Certain adornments were awarded to individual warriors for conspicuous courage in action; these included a type of heavy brass arm-ring ("ingxotha") and an intricate necklace composed of interlocking wooden pegs.
Tactics.
The Zulu typically took the offensive, deploying in the well-known "buffalo horns" formation (). It comprised three elements:
Encirclement tactics are not unique in warfare, and historians note that attempts to surround an enemy were not unknown even in the ritualised battles. The use of separate manoeuvre elements to support a stronger central group is also well known in pre-mechanised tribal warfare, as is the use of reserve echelons farther back. What was unique about the Zulu was the degree of organisation, consistency with which they used these tactics, and the speed at which they executed them. Developments and refinements may have taken place after Shaka's death, as witnessed by the use of larger groupings of regiments by the Zulu against the British in 1879. Missions, available manpower and enemies varied, but whether facing native spear, or European bullet, the impis generally fought in and adhered to the classical buffalo horns pattern.
Organisation and leadership of the Zulu forces.
Regiments and corps. The Zulu forces were generally grouped into three levels: regiments, corps of several regiments, and "armies" or bigger formations, although the Zulu did not use these terms in the modern sense. Although size distinctions were taken account of, any grouping of men on a mission could collectively be called an impi, whether a raiding party of 100 or horde of 10,000. Numbers were not uniform but dependent on a variety of factors, including assignments by the king, or the manpower mustered by various clan chiefs or localities. A regiment might be 400 or 4000 men. These were grouped into corps that took their name from the military kraals where they were mustered, or sometimes the dominant regiment of that locality. There were 4 basic ranks: herdboy assistants, warriors, inDunas and higher ranked supremos for a particular mission.
Higher command and unit leadership. Leadership was not a complicated affair. An inDuna guided each regiment, and he in turn answered to senior izinduna who controlled the corps grouping. Overall guidance of the host was furnished by elder izinduna usually with many years of experience. One or more of these elder chiefs might accompany a big force on an important mission, but there was no single "field marshal" in supreme command of all Zulu forces. Regimental izinduna, like the non-coms of today's army, and yesterday's Roman Centurions, were extremely important to morale and discipline. This was shown during the battle of Isandhlwana. Blanketed by a hail of British bullets, rockets and artillery, the advance of the Zulu faltered. Echoing from the mountain, however, were the shouted cadences and fiery exhortations of their regimental izinduna, who reminded the warriors that their king did not send them to run away. Thus encouraged, the encircling regiments remained in place, maintaining continual pressure, until weakened British dispositions enabled the host to make a final surge forward. (See Morris ref below—"The Washing of the Spears").
Summary of the Shakan reforms.
As noted above, Shaka was neither the originator of the impi, or the age grade structure, nor the concept of a bigger grouping than the small clan system. His major innovations were to blend these traditional elements in a new way, to systematise the approach to battle, and to standardise organization, methods and weapons, particularly in his adoption of the "ilkwa" – the Zulu thrusting spear, unique long-term regimental units, and the "buffalo horns" formation. Dingswayo's approach was of a loose federation of allies under his hegemony, combining to fight, each with their own contingents, under their own leaders. Shaka dispensed with this, insisting instead on a standardised organisation and weapons package that swept away and replaced old clan allegiances with loyalty to himself. This uniform approach also encouraged the loyalty and identification of warriors with their own distinctive military regiments. In time, these warriors, from many conquered tribes and clans came to regard themselves as one nation- the Zulu. The Marian reforms of Rome in the military sphere are referenced by some writers as similar. While other ancient powers such as the Carthaginians maintained a patchwork of force types, and the legions retained such phalanx-style holdovers like the "triarii," Marius implemented one consistent standardised approach for all the infantry. This enabled more disciplined formations and efficient execution of tactics over time against a variety of enemies. As one military historian notes: 
The Impi in battle.
The impi, in its Shakan form, is best known among Western readers from the Anglo-Zulu War of 1879, particularly the famous Zulu victory at Isandhlwana, but its development was over 60 years in coming before that great clash. To understand the full scope of the impi's performance in battle, military historians of the Zulu typically look to its early operations against internal African enemies, not merely the British interlude. In terms of numbers, the operations of the impi would change- from the Western equivalent of small company and battalion size forces, to manoeuvres in multi-divisional strength of between 10,000 and 40,000 men. The victory won by Zulu king Cetawasyo at Ndondakusuka, for example, two decades before the British invasion involved a deployment of 30,000 troops. These were sizeable formations in regional context but represented the bulk of prime Zulu fighting strength. Few impi-style formations were to routinely achieve this level of mobilisation for a single battle. For example, at Cannae, the Romans deployed 80,000 men, and generally could put tens of thousands more into smaller combat actions). The popular notion of countless attacking black spearmen is a distorted one. Manpower supplies on the continent were often limited. In the words of one historian: "The savage hordes of popular lore seldom materialized on African battlefields." This limited resource base would hurt the Zulu when they confronted technologically advanced world powers such as Britain. The advent of new weapons like firearms would also have a profound impact on the African battlefield, but as will be seen, the impi-style forces largely eschewed firearms, or used them in a minor way. Whether facing native spear or European bullet, impis largely fought as they had since the days of Shaka, from Zululand to Zimbabwe, and from Mozambique to Tanzania.
The starting period: Clash at Gqokli Hill.
Upon his accession to power, Shaka was confronted by two potent threats, the Ndwandwes under Zwide, and the Qwabes. Both clans were twice as large as the Zulu. The first key test of the "new model" Shakan impis would be against the Ndwandwe, and the battle offers insight into both Shaka as a commander and the performance of his reorganised combat team. The Zulu king deployed his troops in a strong position on top of Gqokli Hill, using a deep depression on the summit to hide a large central reserve, while grouping his other warriors forward in defensive formation. Shaka also made a decoy gambit- sending the Zulu cattle off with a small escort, luring Zwide into splitting his force. The battle began in the early morning as the Ndwandwe, under Zwide's son Nomahlanjana, made a series of frontal attacks up the steep hill. Slowed by the incline, and armed only with traditional throwing spears, they were badly mauled by Shaka's men in close quarters fighting. By mid-afternoon, the Ndwandwe were exhausted and their force weakened further by small groups of men going off in search of water. Shaka however had cunningly positioned himself so that his troops had access to a small stream nearby. In the late afternoon the Ndwandwe made a final attack. Leaving a part of their army surrounding the bottom of the hill, they pushed a huge column up to the top, hoping to drive the Zulu down into the blocking forces below. Shaka waited until the column was almost at the top, then ordered his fresh reserves to make a flanking "horn" attack, sprinting down both sides of the hill to encircle and liquidate the ascending Ndwandwe. The rest of the enemy force, which could not clearly see what was happening on the summit was next attacked in another encircling manoeuvre that sent it fleeing. In its first major battle, the Shakan impi had pulled off a multiple envelopment. On the negative side, the Ndwandwe remnants had been able to withdraw intact, and all the Zulu cattle were captured. Shaka furthermore was forced eventually to recall and pull back the warriors to his kraal at kwaBulawayo. Nevertheless, the impi had badly mauled an enemy force over twice its size, killing 5 of Zwide's sons in the process and succeeding in its first major test. A period of rebuilding now commenced and new recruits, either by conquest or alliance were incorporated into the growing Shakan force. Among the newcomers was one Mzilikazi, a small-time chieftain of the Kumalo, and a grandson of Zwide whose father had been killed by Zwide. Mzilikazi would eventually fall out with Shaka, and in fleeing, would extend the concept of the impi even further across the landscape of southern and eastern Africa.
The period of consolidation: the Zulu impi and its variants.
In this period Shaka's power grew, defeating several powerful local rivals and creating a vast monolith that was the most powerful nation in its region.
Shaka's success was to spawn several offshoots of the impi-style formation. Chief among these was the Matebele, under Mzilkhazi and the Shangaan, under the redoubtable Shoshangane. The greatest expansion of the impi outside the Zululand/Zimbabwe area however was to come in East Africa, where bands of Ngoni fighting men, conquered large swathes of territory, using the methods first laid down by Shaka.
The first challenge of Europe: African impi versus the Boer Commando.
The impi clashed with another tactical system introduced by European settlers: the horse-gun system of the Boer Commando. This conflict is often popularly conceived of in terms of the well known battles between Zulu King Dingane and the Boers, most notably at the Battle of Blood River. As will be seen however, this tells only part of the story. The impi was to clash with the mobile commando on the open fields of the high veldt in a series of epic confrontations, in which each force both suffered defeat and enjoyed victory, and both sides acquitted themselves well.
The second challenge of Europe: African impi versus the British Empire.
Nearly 35,000 strong, well motivated and supremely confident, the Zulu were a formidable force on their own home ground, despite the almost total lack of modern weaponry. Their greatest assets were their morale, unit leadership, mobility and numbers. Tactically the Zulu acquitted themselves well in at least 3 encounters, Isandhlwana, Hlobane and the smaller Intombi action. Their stealthy approach march, camouflage and noise discipline at Isandhlwana, while not perfect, put them within excellent striking distance of their opponents, where they were able to exploit weaknesses in the camp layout. At Hlobane they caught a British column on the move rather than in the usual fortified position, partially cutting off its retreat and forcing it to withdraw.
Strategically (and perhaps understandably in their own traditional tribal context) they lacked any clear vision of fighting their most challenging war, aside from smashing the three British columns by the weight and speed of their regiments. Despite the Isandhlwana victory, tactically there were major problems as well. They rigidly and predictably applied their three-pronged "buffalo horns" attack, paradoxically their greatest strength, but also their greatest weakness when facing concentrated firepower. The Zulu failed to make use of their superior mobility by attacking the British rear area such as Natal or in interdicting vulnerable British supply lines. However, an important consideration, which King Cetshwayo appreciated, was that there was a clear difference between defending one's territory, and encroaching on another, regardless of the fact that they are at war with the holder of that land. The King realised that peace would be impossible if a real invasion of Natal was launched, and that it would only provoke a more concerted effort on the part of the British against them. The attack on Rorke's Drift, in Natal, was an opportunist raid, as opposed to a real invasion. When they did, they achieved some success, such as the liquidation of a supply detachment at the Intombi River. A more expansive mobile strategy might have cut British communications and brought their lumbering advance to a halt, bottling up the redcoats in scattered strongpoints while the impis ran rampant between them. Just such a scenario developed with the No. 1 British column, which was penned up static and immobile in garrison for over two months at Eshowe.
The Zulu also allowed their opponents too much time to set up fortified strongpoints, assaulting well defended camps and positions with painful losses. A policy of attacking the redcoats while they were strung out on the move, or crossing difficult obstacles like rivers, might have yielded more satisfactory results. For example, four miles past the Ineyzane River, after the British had comfortably crossed, and after they had spent a day consolidating their advance, the Zulu finally launched a typical "buffalo horn" encirclement attack that was seen off with withering fire from not only breech-loading Martini-Henry rifles, but 7-pounder artillery and Gatling guns. In fairness, the Zulu commanders could not conjure regiments out of thin air at the optimum time and place. They too needed time to marshal, supply and position their forces, and sort out final assignments to the three-prongs of attack. Still, the Battle of Hlobane Mountain offers just a glimpse of an alternative mobile scenario, where the manoeuvering Zulu "horns" cut off and drove back Buller's column when it was dangerously strung out on the mountain.
Command and control.
Command and control of the impis was problematic at times. Indeed, the Zulu attacks on the British strongpoints at Rorke's Drift and at Kambula, (both bloody defeats) seemed to have been carried out by over-enthusiastic leaders and warriors despite contrary orders of the Zulu King, Cetshwayo. Popular film re-enactments display a grizzled izinduna directing the host from a promontory with elegant sweeps of the hand. This might have happened during the initial marshaling of forces from a jump off point, or the deployment of reserves, but once the great encircling sweep of frenzied warriors in the "horns" and "chest" was in motion, the izinduna could not generally exercise detailed control.
Handling of reserve forces.
Although the "loins" or reserves were on hand to theoretically correct or adjust an unfavorable situation, a shattered attack could make the reserves irrelevant. Against the Boers at Blood River, massed gunfire broke the back of the Zulu assault, and the Boers were later able to mount a cavalry sweep in counterattack that became a turkey shoot against fleeing Zulu remnants. Perhaps the Zulu threw everything forward and had little left. In similar manner, after exhausting themselves against British firepower at Kambula and Ulindi, few of the Zulu reserves were available to do anything constructive, although the tribal warriors still remained dangerous at the guerrilla level when scattered. At Isandhlwana however, the "classical" Zulu system struck gold, and after liquidating the British position, it was a relatively fresh reserve force that swept down on Rorke's Drift.
Use of modern arms.
The Zulu had greater numbers than their opponents, but greater numbers massed together in compact arrays simply presented easy targets in the age of modern firearms and artillery. African tribes that fought in smaller guerrilla detachments typically held out against European invaders for a much longer time, as witnessed by the 7-year resistance of the Lobi against the French in West Africa, or the operations of the Berbers in Algeria against the French.
When the Zulu did acquire firearms, most notably captured stocks after the great victory at Isandhlwana, they lacked training and used them ineffectively, consistently firing high to give the bullets "strength." Southern Africa, including the areas near Natal, was teeming with bands like the Griquas who had learned to use guns. Indeed, one such group not only mastered the way of the gun, but became proficient horsemen as well, skills that helped build the Basotho tribe, in what is now the nation of Lesotho. In addition, numerous European renegades or adventurers (both Boer and non-Boer) skilled in firearms were known to the Zulu. Some had even led detachments for the Zulu kings on military missions.
The Zulu thus had clear scope and opportunity to master and adapt the new weaponry. They also had already experienced defeat against the Boers, by concentrated firearms. They had had at least four decades to adjust their tactics to this new threat. A well-drilled corps of gunmen or grenadiers, or a battery of artillery operated by European mercenaries for example, might have provided much needed covering fire as the regiments manoeuvred into position.
No such adjustments were on hand when they faced the redcoats. Immensely proud of their system, and failing to learn from their earlier defeats, they persisted in "human wave" attacks against well defended European positions where massed firepower devastated their ranks. The ministrations of an "isAngoma" (plural: "izAngoma") Zulu diviner or "witch doctor", and the bravery of individual regiments were ultimately of little use against the volleys of modern rifles, Gatling guns and artillery at the Ineyzane River, Rorke's Drift, Kambula, Gingingdlovu and finally Ulindi.
A tough challenge.
Undoubtedly, Cetshwayo and his war leaders faced a tough and extremely daunting task – overcoming the challenge of concentrated rifled, Gatling gun, and artillery fire on the battlefield. It was one that also taxed European military leaders, as the carnage of the American Civil War and the later Boer War attests. Nevertheless, Shaka's successors could argue that within the context of their experience and knowledge, they had done the best they could, following his classical template, which had advanced the Zulu from a small, obscure tribe to a respectable regional power known for its fierce warriors.
Demise of the Impi.
The demise of the impi finally came about with the success of European colonisation of Africa- first in southern Africa by the British, and finally in East Africa as German colonialists defeated the last of the impi-style formations under Mkawawa, chief of the Hehe of Tanzania. The Boers, another major challenger to the impi, also saw defeat by imperial forces, in the Boer War of 1902. In its relatively brief history the impi inspired anger, scorn (During the Anglo-Zulu War, British commander Lord Chelmsford complained that they did not 'fight fair'), and even a grudging admiration by its opponents, epitomised in Kipling's poem "Fuzzy Wuzzy":
Today the impi lives on in popular lore and culture, even in the West. While the term "impi" has become synonymous with the Zulu nation in international popular culture, it appears in various video games such as "Civilization III, ', ', and ', where the Impi is the unique unit for the Zulu faction with Shaka as their leader and also as an appearance as unique unit of the Bantu nation in "Rise of Nations" (Zulus are among many tribes who make up the Bantu people) . 'Impi' is also the title of a very famous South Africa song by Johnny Clegg and the band Juluka which has become something of an unofficial national anthem, especially at major international sports events and especially when the opponent is England.
Lyrics:
Before stage seven of the 2013 Tour de France, the Orica-GreenEDGE cycling team played 'Impi' on their team bus in honor of teammate Daryl Impey, the first South African Tour de France leader.

</doc>
<doc id="15175" url="https://en.wikipedia.org/wiki?curid=15175" title="Irish mythology">
Irish mythology

The mythology of pre-Christian Ireland did not entirely survive the conversion to Christianity. However, much of it was preserved in medieval Irish literature, though it was shorn of its religious meanings. This literature represents the most extensive and best preserved of all the branches of Celtic mythology. Although many of the manuscripts have not survived and much more material was probably never committed to writing, there is enough remaining to enable the identification of distinct, if overlapping, cycles: the Mythological Cycle, the Ulster Cycle, the Fenian Cycle and the Historical Cycle. There are also a number of extant mythological texts that do not fit into any of the cycles. Additionally, there are a large number of recorded folk tales that, while not strictly mythological, feature personages from one or more of these four cycles.
The sources.
The three main manuscript sources for Irish mythology are the late 11th/early 12th century "Lebor na hUidre" which is in the library of the Royal Irish Academy, the early 12th century "Book of Leinster" in the Library of Trinity College, Dublin, and the Rawlinson manuscript B 502 ("Rawl."), housed in the Bodleian Library at Oxford University. Despite the dates of these sources, most of the material they contain predates their composition. The earliest of the prose can be dated on linguistic grounds to the 8th century, and some of the verse may be as old as the 6th century.
Other important sources include a group of four manuscripts originating in the west of Ireland in the late 14th or early 15th century: "The Yellow Book of Lecan", "The Great Book of Lecan", "The Book of Hy Many", and "The Book of Ballymote". The first of these contains part of the earliest known version of the "Táin Bó Cúailnge" ("The Driving-off of Cattle of Cooley") and is housed in Trinity College. The other three are in the Royal Academy. Other 15th-century manuscripts, such as "The Book of Fermoy" also contain interesting materials, as do such later syncretic works such as Geoffrey Keating's "Foras Feasa ar Éirinn" ("The History of Ireland") (ca. 1640), particularly as these later compilers and writers may have had access to manuscript sources that have since disappeared.
When using these sources, it is, as always, important to question the impact of the circumstances in which they were produced. Most of the manuscripts were created by Christian monks, who may well have been torn between the desire to record their native culture and their religious hostility to pagan beliefs resulting in some of the gods being euhemerised. Many of the later sources may also have formed part of a propaganda effort designed to create a history for the people of Ireland that could bear comparison with the mythological descent of their British invaders from the founders of Rome that was promulgated by Geoffrey of Monmouth and others. There was also a tendency to rework Irish genealogies to fit into the known schema of Greek or Biblical genealogy.
It was once unquestioned that medieval Irish literature preserved truly ancient traditions in a form virtually unchanged through centuries of oral tradition back to the ancient Celts of Europe. Kenneth Jackson famously described the Ulster Cycle as a "window on the Iron Age", and Garret Olmsted has attempted to draw parallels between "Táin Bó Cuailnge", the Ulster Cycle epic, and the iconography of the Gundestrup Cauldron. However, this "nativist" position has been challenged by "revisionist" scholars who believe that much of it was created in Christian times in deliberate imitation of the epics of classical literature that came with Latin learning. The revisionists would indicate passages apparently influenced by the Iliad in "Táin Bó Cuailnge", and the existence of "Togail Troí", an Irish adaptation of Dares Phrygius' "De excidio Troiae historia", found in the Book of Leinster, and note that the material culture of the stories is generally closer to the time of the stories' composition than to the distant past. A consensus has emerged which encourages the critical reading of the material.
Mythological cycle.
The Mythological Cycle, comprising stories of the former gods and origins of the Irish, is the least well preserved of the four cycles. The most important sources are the "Metrical Dindshenchas" or "Lore of Places" and the "Lebor Gabála Érenn" or "Book of Invasions". Other manuscripts preserve such mythological tales as "The Dream of Aengus", "The Wooing Of Étain" and "Cath Maige Tuireadh", "The (second) Battle of Magh Tuireadh". One of the best known of all Irish stories, "Oidheadh Clainne Lir", or "The Tragedy of the Children of Lir", is also part of this cycle.
"Lebor Gabála Érenn" is a pseudo-history of Ireland, tracing the ancestry of the Irish back to before Noah. It tells of a series of invasions or "takings" of Ireland by a succession of peoples, the fifth of whom was the people known as the Tuatha Dé Danann ("Peoples of the Goddess Danu"), who were believed to have inhabited the island before the arrival of the Gaels, or Milesians. They faced opposition from their enemies, the Fomorians, led by Balor of the Evil Eye. Balor was eventually slain by Lug Lámfada (Lug of the Long Arm) at the second battle of Magh Tuireadh. With the arrival of the Gaels, the Tuatha Dé Danann retired underground to become the fairy people of later myth and legend.
The "Metrical Dindshenchas" is the great onomastic work of early Ireland, giving the naming legends of significant places in a sequence of poems. It includes a lot of important information on Mythological Cycle figures and stories, including the Battle of Tailtiu, in which the Tuatha Dé Danann were defeated by the Milesians.
It is important to note that by the Middle Ages the Tuatha Dé Danann were not viewed so much as gods as the shape-shifting magician population of an earlier Golden Age Ireland. Texts such as "Lebor Gabála Érenn" and "Cath Maige Tuireadh" present them as kings and heroes of the distant past, complete with death-tales. However, there is considerable evidence, both in the texts and from the wider Celtic world, that they were once considered deities.
Even after they are displaced as the rulers of Ireland, characters such as Lug, the Mórrígan, Aengus and Manannan appear in stories set centuries later, betraying their immortality. A poem in the Book of Leinster lists many of the Tuatha Dé, but ends "Although author enumerates them, he does not worship them". Goibniu, Creidhne and Luchta are referred to as "Trí Dé Dána" ("three gods of craftsmanship"), and the Dagda's name is interpreted in medieval texts as "the good god". Nuada is cognate with the British god Nodens; Lug is a reflex of the pan-Celtic deity Lugus, the name of whom may indicate "Light"; Tuireann may be related to the Gaulish Taranis; Ogma to Ogmios; the Badb to Catubodua.
Ulster cycle.
The Ulster Cycle is traditionally set around the first century CE, and most of the action takes place in the provinces of Ulster and Connacht. It consists of a group of heroic tales dealing with the lives of Conchobar mac Nessa, king of Ulster, the great hero Cú Chulainn, the son of Lug (Lugh), and of their friends, lovers, and enemies. These are the Ulaid, or people of the North-Eastern corner of Ireland and the action of the stories centres round the royal court at Emain Macha (known in English as Navan Fort), close to the modern town of Armagh. The Ulaid had close links with the Irish colony in Scotland, and part of Cú Chulainn's training takes place in that colony.
The cycle consists of stories of the births, early lives and training, wooings, battles, feastings, and deaths of the heroes and reflects a warrior society in which warfare consists mainly of single combats and wealth is measured mainly in cattle. These stories are written mainly in prose. The centrepiece of the Ulster Cycle is the "Táin Bó Cúailnge". Other important Ulster Cycle tales include "The Tragic Death of Aife's only Son", "Bricriu's Feast", and "The Destruction of Da Derga's Hostel". "The Exile of the Sons of Usnach", better known as the tragedy of Deirdre and the source of plays by John Millington Synge, William Butler Yeats, and Vincent Woods, is also part of this cycle.
This cycle is, in some respects, close to the mythological cycle. Some of the characters from the latter reappear, and the same sort of shape-shifting magic is much in evidence, side by side with a grim, almost callous realism. While we may suspect a few characters, such as Medb or Cú Roí, of once being deities, and Cú Chulainn in particular displays superhuman prowess, the characters are mortal and associated with a specific time and place. If the Mythological Cycle represents a Golden Age, the Ulster Cycle is Ireland's Heroic Age.
Fenian cycle.
Like the Ulster Cycle, the Fenian Cycle is concerned with the deeds of Irish heroes. The stories of the Fenian Cycle appear to be set around the 3rd century and mainly in the provinces of Leinster and Munster. They differ from the other cycles in the strength of their links with the Irish-speaking community in Scotland and there are many extant Fenian texts from that country. They also differ from the Ulster Cycle in that the stories are told mainly in verse and that in tone they are nearer to the tradition of romance than the tradition of epic. The stories concern the doings of Fionn mac Cumhaill and his band of soldiers, the Fianna.
The single most important source for the Fenian Cycle is the "Acallam na Senórach" ("Colloquy of the Old Men"), which is found in two 15th-century manuscripts, the "Book of Lismore" and Laud 610, as well as a 17th-century manuscript from Killiney, County Dublin. The text is dated from linguistic evidence to the 12th century. The text records conversations between Caílte mac Rónáin and Oisín, the last surviving members of the Fianna, and Saint Patrick, and consists of about 8,000 lines. The late dates of the manuscripts may reflect a longer oral tradition for the Fenian stories.
The Fianna of the story are divided into the Clann Baiscne, led by Fionn mac Cumhaill (often rendered as "Finn MacCool", Finn Son of Cumhall), and the Clann Morna, led by his enemy, Goll mac Morna. Goll killed Fionn's father, Cumhal, in battle and the boy Fionn was brought up in secrecy. As a youth, while being trained in the art of poetry, he accidentally burned his thumb while cooking the Salmon of Knowledge, which allowed him to suck or bite his thumb to receive bursts of stupendous wisdom. He took his place as the leader of his band and numerous tales are told of their adventures. Two of the greatest of the Irish tales, "Tóraigheacht Dhiarmada agus Ghráinne" ("The Pursuit of Diarmuid and Gráinne)" and "Oisín in Tír na nÓg" form part of the cycle. The Diarmuid and Grainne story, which is one of the few Fenian prose tales, is a probable source of "Tristan and Iseult".
The world of the Fenian Cycle is one in which professional warriors spend their time hunting, fighting, and engaging in adventures in the spirit world. New entrants into the band are expected to be knowledgeable in poetry as well as undergo a number of physical tests or ordeals.
There is not any religious element in these tales unless it is one of hero-worship.
Historical cycle.
It was part of the duty of the medieval Irish bards, or court poets, to record the history of the family and the genealogy of the king they served. This they did in poems that blended the mythological and the historical to a greater or lesser degree. The resulting stories form what has come to be known as the Historical Cycle, or more correctly Cycles, as there are a number of independent groupings.
The kings that are included range from the almost entirely mythological Labraid Loingsech, who allegedly became High King of Ireland around 431 BC, to the entirely historical Brian Boru. However, the greatest glory of the Historical Cycle is the "Buile Shuibhne" ("The Frenzy of Sweeney"), a 12th-century tale told in verse and prose. Suibhne, king of Dál nAraidi, was cursed by St Ronan and became a kind of half-man, half bird, condemned to live out his life in the woods, fleeing from his human companions. The story has captured the imaginations of contemporary Irish poets and has been translated by Trevor Joyce and Seamus Heaney.
Other tales.
Adventures.
The adventures, or "echtrae", are a group of stories of visits to the Irish Other World (which may be westward across the sea, underground, or simply invisible to mortals). The most famous, "Oisin in Tir na nÓg" belongs to the Fenian Cycle, but several free-standing adventures survive, including "The Adventure of Conle", "The Voyage of Bran mac Ferbail", and "The Adventure of Lóegaire".
Voyages.
The voyages, or "immrama", are tales of sea journeys and the wonders seen on them that may have resulted from the combination of the experiences of fishermen combined and the Other World elements that inform the adventures. Of the seven "immrama" mentioned in the manuscripts, only three have survived: the "Voyage of Mael Dúin", the "Voyage of the Uí Chorra", and the "Voyage of Snedgus and Mac Riagla". "The Voyage of Mael Duin" is the forerunner of the later "Voyage of St. Brendan".
Folk tales.
During the first few years of the 20th Century, Herminie T. Kavanagh wrote down many Irish folk tales which she published in magazines and in two books. Twenty-six years after her death, the tales from her two books, "Darby O'Gill and the Good People", and "Ashes of Old Wishes" were made in to the film Darby O'Gill and the Little People. Noted Irish playwright Lady Gregory also collected folk stories to preserve Irish history. Eddie Lenihan (b-1950), first author of "Meeting the Other Crowd" as well as writer of numerous other books, has a growing reputation as a modern Irish folklorist.

</doc>
<doc id="15176" url="https://en.wikipedia.org/wiki?curid=15176" title="Insurance">
Insurance

Insurance is a means of protection from financial loss. It is a form of risk management primarily used to hedge against the risk of a contingent, uncertain loss.
An entity which provides insurance is known as an insurer, insurance company, or insurance carrier. A person or entity who buys insurance is known as an insured or policyholder. The insurance transaction involves the insured assuming a guaranteed and known relatively small loss in the form of payment to the insurer in exchange for the insurer's promise to compensate the insured in the event of a covered loss. The loss may or may not be financial, but it must be reducible to financial terms, and must involve something in which the insured has an insurable interest established by ownership, possession, or preexisting relationship. The insured receives a contract, called the insurance policy, which details the conditions and circumstances under which the insured will be financially compensated. The amount of money charged by the insurer to the insured for the coverage set forth in the insurance policy is called the premium. If the insured experiences a loss which is potentially covered by the insurance policy, the insured submits a claim to the insurer for processing by a claims adjuster.
History.
Early methods.
Methods for transferring or distributing risk were practiced by Chinese and Babylonian traders as long ago as the 3rd and 2nd millennia BC, respectively. Chinese merchants travelling treacherous river rapids would redistribute their wares across many vessels to limit the loss due to any single vessel's capsizing. The Babylonians developed a system which was recorded in the famous Code of Hammurabi, c. 1750 BC, and practiced by early Mediterranean sailing merchants. If a merchant received a loan to fund his shipment, he would pay the lender an additional sum in exchange for the lender's guarantee to cancel the loan should the shipment be stolen or lost at sea.
At some point in the 1st millennium BC, the inhabitants of Rhodes created the 'general average'. This allowed groups of merchants to pay to insure their goods being shipped together. The collected premiums would be used to reimburse any merchant whose goods were jettisoned during transport, whether to storm or sinkage.
Separate insurance contracts (i.e., insurance policies not bundled with loans or other kinds of contracts) were invented in Genoa in the 14th century, as were insurance pools backed by pledges of landed estates. The first known insurance contract dates from Genoa in 1347, and in the next century maritime insurance developed widely and premiums were intuitively varied with risks. These new insurance contracts allowed insurance to be separated from investment, a separation of roles that first proved useful in marine insurance.
Modern insurance.
Insurance became far more sophisticated in Enlightenment era Europe, and specialized varieties developed.
Property insurance as we know it today can be traced to the Great Fire of London, which in 1666 devoured more than 13,000 houses. The devastating effects of the fire converted the development of insurance "from a matter of convenience into one of urgency, a change of opinion reflected in Sir Christopher Wren's inclusion of a site for 'the Insurance Office' in his new plan for London in 1667". A number of attempted fire insurance schemes came to nothing, but in 1681, economist Nicholas Barbon and eleven associates established the first fire insurance company, the "Insurance Office for Houses", at the back of the Royal Exchange to insure brick and frame homes. Initially, 5,000 homes were insured by his Insurance Office.
At the same time, the first insurance schemes for the underwriting of business ventures became available. By the end of the seventeenth century, London's growing importance as a center for trade was increasing demand for marine insurance. In the late 1680s, Edward Lloyd opened a coffee house, which became the meeting place for parties in the shipping industry wishing to insure cargoes and ships, and those willing to underwrite such ventures. These informal beginnings led to the establishment of the insurance market Lloyd's of London and several related shipping and insurance businesses.
The first life insurance policies were taken out in the early 18th century. The first company to offer life insurance was the Amicable Society for a Perpetual Assurance Office, founded in London in 1706 by William Talbot and Sir Thomas Allen. Edward Rowe Mores established the Society for Equitable Assurances on Lives and Survivorship in 1762.
It was the world's first mutual insurer and it pioneered age based premiums based on mortality rate laying "the framework for scientific insurance practice and development" and "the basis of modern life assurance upon which all life assurance schemes were subsequently based".
In the late 19th century, "accident insurance" began to become available. This operated much like modern disability insurance. The first company to offer accident insurance was the Railway Passengers Assurance Company, formed in 1848 in England to insure against the rising number of fatalities on the nascent railway system.
By the late 19th century, governments began to initiate national insurance programs against sickness and old age. Germany built on a tradition of welfare programs in Prussia and Saxony that began as early as in the 1840s. In the 1880s Chancellor Otto von Bismarck introduced old age pensions, accident insurance and medical care that formed the basis for Germany's welfare state. In Britain more extensive legislation was introduced by the Liberal government in the 1911 National Insurance Act. This gave the British working classes the first contributory system of insurance against illness and unemployment. This system was greatly expanded after the Second World War under the influence of the Beveridge Report, to form the first modern welfare state.
Principles.
Insurance involves pooling funds from "many" insured entities (known as exposures) to pay for the losses that some may incur. The insured entities are therefore protected from risk for a fee, with the fee being dependent upon the frequency and severity of the event occurring. In order to be an insurable risk, the risk insured against must meet certain characteristics. Insurance as a financial intermediary is a commercial enterprise and a major part of the financial services industry, but individual entities can also self-insure through saving money for possible future losses.
Insurability.
Risk which can be insured by private companies typically shares seven common characteristics:
Legal.
When a company insures an individual entity, there are basic legal requirements and regulations. Several commonly cited legal principles of insurance include:
Indemnification.
To "indemnify" means to make whole again, or to be reinstated to the position that one was in, to the extent possible, prior to the happening of a specified event or peril. Accordingly, life insurance is generally not considered to be indemnity insurance, but rather "contingent" insurance (i.e., a claim arises on the occurrence of a specified event). There are generally three types of insurance contracts that seek to indemnify an insured:
From an insured's standpoint, the result is usually the same: the insurer pays the loss and claims expenses.
If the Insured has a "reimbursement" policy, the insured can be required to pay for a loss and then be "reimbursed" by the insurance carrier for the loss and out of pocket costs including, with the permission of the insurer, claim expenses.
Under a "pay on behalf" policy, the insurance carrier would defend and pay a claim on behalf of the insured who would not be out of pocket for anything. Most modern liability insurance is written on the basis of "pay on behalf" language which enables the insurance carrier to manage and control the claim.
Under an "indemnification" policy, the insurance carrier can generally either "reimburse" or "pay on behalf of", whichever is more beneficial to it and the insured in the claim handling process.
An entity seeking to transfer risk (an individual, corporation, or association of any type, etc.) becomes the 'insured' party once risk is assumed by an 'insurer', the insuring party, by means of a contract, called an insurance policy. Generally, an insurance contract includes, at a minimum, the following elements: identification of participating parties (the insurer, the insured, the beneficiaries), the premium, the period of coverage, the particular loss event covered, the amount of coverage (i.e., the amount to be paid to the insured or beneficiary in the event of a loss), and exclusions (events not covered). An insured is thus said to be "indemnified" against the loss covered in the policy.
When insured parties experience a loss for a specified peril, the coverage entitles the policyholder to make a claim against the insurer for the covered amount of loss as specified by the policy. The fee paid by the insured to the insurer for assuming the risk is called the premium. Insurance premiums from many insureds are used to fund accounts reserved for later payment of claims – in theory for a relatively few claimants – and for overhead costs. So long as an insurer maintains adequate funds set aside for anticipated losses (called reserves), the remaining margin is an insurer's profit.
Social effects.
Insurance can have various effects on society through the way that it changes who bears the cost of losses and damage. On one hand it can increase fraud; on the other it can help societies and individuals prepare for catastrophes and mitigate the effects of catastrophes on both households and societies.
Insurance can influence the probability of losses through moral hazard, insurance fraud, and preventive steps by the insurance company. Insurance scholars have typically used moral hazard to refer to the increased loss due to unintentional carelessness and insurance fraud to refer to increased risk due to intentional carelessness or indifference. Insurers attempt to address carelessness through inspections, policy provisions requiring certain types of maintenance, and possible discounts for loss mitigation efforts. While in theory insurers could encourage investment in loss reduction, some commentators have argued that in practice insurers had historically not aggressively pursued loss control measures—particularly to prevent disaster losses such as hurricanes—because of concerns over rate reductions and legal battles. However, since about 1996 insurers have begun to take a more active role in loss mitigation, such as through building codes.
Methods of insurance.
In accordance with study books of The Chartered Insurance Institute, there are the following types of insurance:
Insurers' business model.
Underwriting and investing.
The business model is to collect more in premium and investment income than is paid out in losses, and to also offer a competitive price which consumers will accept. Profit can be reduced to a simple equation:
Insurers make money in two ways:
The most complicated aspect of the insurance business is the actuarial science of ratemaking (price-setting) of policies, which uses statistics and probability to approximate the rate of future claims based on a given risk. After producing rates, the insurer will use discretion to reject or accept risks through the underwriting process.
At the most basic level, initial ratemaking involves looking at the frequency and severity of insured perils and the expected average payout resulting from these perils. Thereafter an insurance company will collect historical loss data, bring the loss data to present value, and compare these prior losses to the premium collected in order to assess rate adequacy. Loss ratios and expense loads are also used. Rating for different risk characteristics involves at the most basic level comparing the losses with "loss relativities"—a policy with twice as many losses would therefore be charged twice as much. More complex multivariate analyses are sometimes used when multiple characteristics are involved and a univariate analysis could produce confounded results. Other statistical methods may be used in assessing the probability of future losses.
Upon termination of a given policy, the amount of premium collected minus the amount paid out in claims is the insurer's underwriting profit on that policy. Underwriting performance is measured by something called the "combined ratio", which is the ratio of expenses/losses to premiums. A combined ratio of less than 100% indicates an underwriting profit, while anything over 100 indicates an underwriting loss. A company with a combined ratio over 100% may nevertheless remain profitable due to investment earnings.
Insurance companies earn investment profits on "float". Float, or available reserve, is the amount of money on hand at any given moment that an insurer has collected in insurance premiums but has not paid out in claims. Insurers start investing insurance premiums as soon as they are collected and continue to earn interest or other income on them until claims are paid out. The Association of British Insurers (gathering 400 insurance companies and 94% of UK insurance services) has almost 20% of the investments in the London Stock Exchange.
In the United States, the underwriting loss of property and casualty insurance companies was $142.3 billion in the five years ending 2003. But overall profit for the same period was $68.4 billion, as the result of float. Some insurance industry insiders, most notably Hank Greenberg, do not believe that it is forever possible to sustain a profit from float without an underwriting profit as well, but this opinion is not universally held.
Naturally, the float method is difficult to carry out in an economically depressed period. Bear markets do cause insurers to shift away from investments and to toughen up their underwriting standards, so a poor economy generally means high insurance premiums. This tendency to swing between profitable and unprofitable periods over time is commonly known as the underwriting, or insurance, cycle.
Claims.
Claims and loss handling is the materialized utility of insurance; it is the actual "product" paid for. Claims may be filed by insureds directly with the insurer or through brokers or agents. The insurer may require that the claim be filed on its own proprietary forms, or may accept claims on a standard industry form, such as those produced by ACORD.
Insurance company claims departments employ a large number of claims adjusters supported by a staff of records management and data entry clerks. Incoming claims are classified based on severity and are assigned to adjusters whose settlement authority varies with their knowledge and experience. The adjuster undertakes an investigation of each claim, usually in close cooperation with the insured, determines if coverage is available under the terms of the insurance contract, and if so, the reasonable monetary value of the claim, and authorizes payment.
The policyholder may hire their own public adjuster to negotiate the settlement with the insurance company on their behalf. For policies that are complicated, where claims may be complex, the insured may take out a separate insurance policy add-on, called loss recovery insurance, which covers the cost of a public adjuster in the case of a claim.
Adjusting liability insurance claims is particularly difficult because there is a third party involved, the plaintiff, who is under no contractual obligation to cooperate with the insurer and may in fact regard the insurer as a deep pocket. The adjuster must obtain legal counsel for the insured (either inside "house" counsel or outside "panel" counsel), monitor litigation that may take years to complete, and appear in person or over the telephone with settlement authority at a mandatory settlement conference when requested by the judge.
If a claims adjuster suspects under-insurance, the condition of average may come into play to limit the insurance company's exposure.
In managing the claims handling function, insurers seek to balance the elements of customer satisfaction, administrative handling expenses, and claims overpayment leakages. As part of this balancing act, fraudulent insurance practices are a major business risk that must be managed and overcome. Disputes between insurers and insureds over the validity of claims or claims handling practices occasionally escalate into litigation (see insurance bad faith).
Marketing.
Insurers will often use insurance agents to initially market or underwrite their customers. Agents can be captive, meaning they write only for one company, or independent, meaning that they can issue policies from several companies. The existence and success of companies using insurance agents is likely due to improved and personalized service.
Types.
Any risk that can be quantified can potentially be insured. Specific kinds of risk that may give rise to claims are known as perils. An insurance policy will set out in detail which perils are covered by the policy and which are not. Below are non-exhaustive lists of the many different types of insurance that exist. A single policy may cover risks in one or more of the categories set out below. For example, vehicle insurance would typically cover both the property risk (theft or damage to the vehicle) and the liability risk (legal claims arising from an accident). A home insurance policy in the United States typically includes coverage for damage to the home and the owner's belongings, certain legal claims against the owner, and even a small amount of coverage for medical expenses of guests who are injured on the owner's property.
Business insurance can take a number of different forms, such as the various kinds of professional liability insurance, also called professional indemnity (PI), which are discussed below under that name; and the business owner's policy (BOP), which packages into one policy many of the kinds of coverage that a business owner needs, in a way analogous to how homeowners' insurance packages the coverages that a homeowner needs.
Auto insurance.
Auto insurance protects the policyholder against financial loss in the event of an incident involving a vehicle they own, such as in a traffic collision.
Coverage typically includes:
Gap insurance.
Gap insurance covers the excess amount on your auto loan in an instance where your insurance company does not cover the entire loan. Depending on the company's specific policies it might or might not cover the deductible as well. This coverage is marketed for those who put low down payments, have high interest rates on their loans, and those with 60-month or longer terms. Gap insurance is typically offered by a finance company when the vehicle owner purchases their vehicle, but many auto insurance companies offer this coverage to consumers as well.
Health insurance.
Health insurance policies cover the cost of medical treatments. Dental insurance, like medical insurance, protects policyholders for dental costs. In most developed countries, all citizens receive some health coverage from their governments, paid for by taxation. In most countries, health insurance is often part of an employer's benefits.
Casualty.
Casualty insurance insures against accidents, not necessarily tied to any specific property. It is a broad spectrum of insurance that a number of other types of insurance could be classified, such as auto, workers compensation, and some liability insurances.
Life.
Life insurance provides a monetary benefit to a decedent's family or other designated beneficiary, and may specifically provide for income to an insured person's family, burial, funeral and other final expenses. Life insurance policies often allow the option of having the proceeds paid to the beneficiary either in a lump sum cash payment or an annuity. In most states, a person cannot purchase a policy on another person without their knowledge.
Annuities provide a stream of payments and are generally classified as insurance because they are issued by insurance companies, are regulated as insurance, and require the same kinds of actuarial and investment management expertise that life insurance requires. Annuities and pensions that pay a benefit for life are sometimes regarded as insurance against the possibility that a retiree will outlive his or her financial resources. In that sense, they are the complement of life insurance and, from an underwriting perspective, are the mirror image of life insurance.
Certain life insurance contracts accumulate cash values, which may be taken by the insured if the policy is surrendered or which may be borrowed against. Some policies, such as annuities and endowment policies, are financial instruments to accumulate or liquidate wealth when it is needed.
In many countries, such as the United States and the UK, the tax law provides that the interest on this cash value is not taxable under certain circumstances. This leads to widespread use of life insurance as a tax-efficient method of saving as well as protection in the event of early death.
In the United States, the tax on interest income on life insurance policies and annuities is generally deferred. However, in some cases the benefit derived from tax deferral may be offset by a low return. This depends upon the insuring company, the type of policy and other variables (mortality, market return, etc.). Moreover, other income tax saving vehicles (e.g., IRAs, 401(k) plans, Roth IRAs) may be better alternatives for value accumulation.
Burial insurance.
Burial insurance is a very old type of life insurance which is paid out upon death to cover final expenses, such as the cost of a funeral. The Greeks and Romans introduced burial insurance c. 600 CE when they organized guilds called "benevolent societies" which cared for the surviving families and paid funeral expenses of members upon death. Guilds in the Middle Ages served a similar purpose, as did friendly societies during Victorian times.
Property.
Property insurance provides protection against risks to property, such as fire, theft or weather damage. This may include specialized forms of insurance such as fire insurance, flood insurance, earthquake insurance, home insurance, inland marine insurance or boiler insurance.
The term "property insurance" may, like casualty insurance, be used as a broad category of various subtypes of insurance, some of which are listed below:
Liability.
Liability insurance is a very broad superset that covers legal claims against the insured. Many types of insurance include an aspect of liability coverage. For example, a homeowner's insurance policy will normally include liability coverage which protects the insured in the event of a claim brought by someone who slips and falls on the property; automobile insurance also includes an aspect of liability insurance that indemnifies against the harm that a crashing car can cause to others' lives, health, or property. The protection offered by a liability insurance policy is twofold: a legal defense in the event of a lawsuit commenced against the policyholder and indemnification (payment on behalf of the insured) with respect to a settlement or court verdict. Liability policies typically cover only the negligence of the insured, and will not apply to results of wilful or intentional acts by the insured.
Often a commercial insured's liability insurance program consists of several layers. The first layer of insurance generally consists of primary insurance, which provides first dollar indemnity for judgments and settlements up to the limits of liability of the primary policy. Generally, primary insurance is subject to a deductible and obligates the insured to defend the insured against lawsuits, which is normally accomplished by assigning counsel to defend the insured. In many instances, a commercial insured may elect to self-insure. Above the primary insurance or self-insured retention, the insured may have one or more layers of excess insurance to provide coverage additional limits of indemnity protection. There are a variety of types of excess insurance, including "stand-alone" excess policies (policies that contain their own terms, conditions, and exclusions), "follow form" excess insurance (policies that follow the terms of the underlying policy except as specifically provided), "umbrella" insurance policies (excess insurance that in some circumstances could provide coverage that is broader than the underlying insurance), and "surplus lines" insurance (policies written by non-admitted carriers).
Credit.
Credit insurance repays some or all of a loan when the borrower is insolvent.
Closed community and governmental self-insurance.
Some communities prefer to create virtual insurance amongst themselves by other means than contractual risk transfer, which assigns explicit numerical values to risk. A number of religious groups, including the Amish and some Muslim groups, depend on support provided by their communities when disasters strike. The risk presented by any given person is assumed collectively by the community who all bear the cost of rebuilding lost property and supporting people whose needs are suddenly greater after a loss of some kind. In supportive communities where others can be trusted to follow community leaders, this tacit form of insurance can work. In this manner the community can even out the extreme differences in insurability that exist among its members. Some further justification is also provided by invoking the moral hazard of explicit insurance contracts.
In the United Kingdom, The Crown (which, for practical purposes, meant the civil service) did not insure property such as government buildings. If a government building was damaged, the cost of repair would be met from public funds because, in the long run, this was cheaper than paying insurance premiums. Since many UK government buildings have been sold to property companies, and rented back, this arrangement is now less common and may have disappeared altogether.
In the United States, the most prevalent form of self-insurance is governmental risk management pools. They are self-funded cooperatives, operating as carriers of coverage for the majority of governmental entities today, such as county governments, municipalities, and school districts. Rather than these entities independently self-insure and risk bankruptcy from a large judgment or catastrophic loss, such governmental entities form a risk pool. Such pools begin their operations by capitalization through member deposits or bond issuance. Coverage (such as general liability, auto liability, professional liability, workers compensation, and property) is offered by the pool to its members, similar to coverage offered by insurance companies. However, self-insured pools offer members lower rates (due to not needing insurance brokers), increased benefits (such as loss prevention services) and subject matter expertise. Of approximately 91,000 distinct governmental entities operating in the United States, 75,000 are members of self-insured pools in various lines of coverage, forming approximately 500 pools. Although a relatively small corner of the insurance market, the annual contributions (self-insured premiums) to such pools have been estimated up to 17 billion dollars annually.
Insurance companies.
Insurance companies may be classified into two groups:
General insurance companies can be further divided into these sub categories.
In most countries, life and non-life insurers are subject to different regulatory regimes and different tax and accounting rules. The main reason for the distinction between the two types of company is that life, annuity, and pension business is very long-term in nature – coverage for life assurance or a pension can cover risks over many decades. By contrast, non-life insurance cover usually covers a shorter period, such as one year.
In the United States, standard line insurance companies are insurers that have received a license or authorization from a state for the purpose of writing specific kinds of insurance in that state, such as automobile insurance or homeowners' insurance. They are typically referred to as "admitted" insurers. Generally, such an insurance company must submit its rates and policy forms to the state's insurance regulator to receive his or her prior approval, although whether an insurance company must receive prior approval depends upon the kind of insurance being written. Standard line insurance companies usually charge lower premiums than excess line insurers and may sell directly to individual insureds. They are regulated by state laws, which include restrictions on rates and forms, and which aim to protect consumers and the public from unfair or abusive practices. These insurers also are required to contribute to state guarantee funds, which are used to pay for losses if an insurer becomes insolvent.
Excess line insurance companies (also known as Excess and Surplus) typically insure risks not covered by the standard lines insurance market, due to a variety of reasons (e.g., new entity or an entity that does not have an adequate loss history, an entity with unique risk characteristics, or an entity that has a loss history that does not fit the underwriting requirements of the standard lines insurance market). They are typically referred to as non-admitted or unlicensed insurers. Non-admitted insurers are generally not licensed or authorized in the states in which they write business, although they must be licensed or authorized in the state in which they are domiciled. These companies have more flexibility and can react faster than standard line insurance companies because they are not required to file rates and forms. However, they still have substantial regulatory requirements placed upon them.
Most states require that excess line insurers submit financial information, articles of incorporation, a list of officers, and other general information. They also may not write insurance that is typically available in the admitted market, do not participate in state guarantee funds (and therefore policyholders do not have any recourse through these funds if an insurer becomes insolvent and cannot pay claims), may pay higher taxes, only may write coverage for a risk if it has been rejected by three different admitted insurers, and only when the insurance producer placing the business has a surplus lines license. Generally, when an excess line insurer writes a policy, it must, pursuant to state laws, provide disclosure to the policyholder that the policyholder's policy is being written by an excess line insurer.
On July 21, 2010, President Barack Obama signed into law the Nonadmitted and Reinsurance Reform Act of 2010 ("NRRA"), which took effect on July 21, 2011, and was part of the Dodd-Frank Wall Street Reform and Consumer Protection Act. The NRRA changed the regulatory paradigm for excess line insurance. Generally, under the NRRA, only the insured's home state may regulate and tax the excess line transaction.
Insurance companies are generally classified as either mutual or proprietary companies. Mutual companies are owned by the policyholders, while shareholders (who may or may not own policies) own proprietary insurance companies.
Demutualization of mutual insurers to form stock companies, as well as the formation of a hybrid known as a mutual holding company, became common in some countries, such as the United States, in the late 20th century. However, not all states permit mutual holding companies.
Other possible forms for an insurance company include reciprocals, in which policyholders reciprocate in sharing risks, and Lloyd's organizations.
Insurance companies are rated by various agencies such as A. M. Best. The ratings include the company's financial strength, which measures its ability to pay claims. It also rates financial instruments issued by the insurance company, such as bonds, notes, and securitization products.
Reinsurance companies are insurance companies that sell policies to other insurance companies, allowing them to reduce their risks and protect themselves from very large losses. The reinsurance market is dominated by a few very large companies, with huge reserves. A reinsurer may also be a direct writer of insurance risks as well.
Captive insurance companies may be defined as limited-purpose insurance companies established with the specific objective of financing risks emanating from their parent group or groups. This definition can sometimes be extended to include some of the risks of the parent company's customers. In short, it is an in-house self-insurance vehicle. Captives may take the form of a "pure" entity (which is a 100% subsidiary of the self-insured parent company); of a "mutual" captive (which insures the collective risks of members of an industry); and of an "association" captive (which self-insures individual risks of the members of a professional, commercial or industrial association). Captives represent commercial, economic and tax advantages to their sponsors because of the reductions in costs they help create and for the ease of insurance risk management and the flexibility for cash flows they generate. Additionally, they may provide coverage of risks which is neither available nor offered in the traditional insurance market at reasonable prices.
The types of risk that a captive can underwrite for their parents include property damage, public and product liability, professional indemnity, employee benefits, employers' liability, motor and medical aid expenses. The captive's exposure to such risks may be limited by the use of reinsurance.
Captives are becoming an increasingly important component of the risk management and risk financing strategy of their parent. This can be understood against the following background:
There are also companies known as "insurance consultants". Like a mortgage broker, these companies are paid a fee by the customer to shop around for the best insurance policy amongst many companies. Similar to an insurance consultant, an 'insurance broker' also shops around for the best insurance policy amongst many companies. However, with insurance brokers, the fee is usually paid in the form of commission from the insurer that is selected rather than directly from the client.
Neither insurance consultants nor insurance brokers are insurance companies and no risks are transferred to them in insurance transactions. Third party administrators are companies that perform underwriting and sometimes claims handling services for insurance companies. These companies often have special expertise that the insurance companies do not have.
The financial stability and strength of an insurance company should be a major consideration when buying an insurance contract. An insurance premium paid currently provides coverage for losses that might arise many years in the future. For that reason, the viability of the insurance carrier is very important. In recent years, a number of insurance companies have become insolvent, leaving their policyholders with no coverage (or coverage only from a government-backed insurance pool or other arrangement with less attractive payouts for losses). A number of independent rating agencies provide information and rate the financial viability of insurance companies.
Across the world.
Global insurance premiums grew by 2.7% in inflation-adjusted terms in 2010 to $4.3 trillion, climbing above pre-crisis levels. The return to growth and record premiums generated during the year followed two years of decline in real terms. Life insurance premiums increased by 3.2% in 2010 and non-life premiums by 2.1%. While industrialised countries saw an increase in premiums of around 1.4%, insurance markets in emerging economies saw rapid expansion with 11% growth in premium income. The global insurance industry was sufficiently capitalised to withstand the financial crisis of 2008 and 2009 and most insurance companies restored their capital to pre-crisis levels by the end of 2010. With the continuation of the gradual recovery of the global economy, it is likely the insurance industry will continue to see growth in premium income both in industrialised countries and emerging markets in 2011.
Advanced economies account for the bulk of global insurance. With premium income of $1.62 trillion, Europe was the most important region in 2010, followed by North America $1.409 trillion and Asia $1.161 trillion. Europe has however seen a decline in premium income during the year in contrast to the growth seen in North America and Asia. The top four countries generated more than a half of premiums. The United States and Japan alone accounted for 40% of world insurance, much higher than their 7% share of the global population. Emerging economies accounted for over 85% of the world's population but only around 15% of premiums. Their markets are however growing at a quicker pace. The country expected to have the biggest impact on the insurance share distribution across the world is China. According to Sam Radwan of ENHANCE International LLC, low premium penetration (insurance premium as a % of GDP), an ageing population and the largest car market in terms of new sales, premium growth has averaged 15–20% in the past five years, and China is expected to be the largest insurance market in the next decade or two.
Regulatory differences.
In the United States, insurance is regulated by the states under the McCarran-Ferguson Act, with "periodic proposals for federal intervention", and a nonprofit coalition of state insurance agencies called the National Association of Insurance Commissioners works to harmonize the country's different laws and regulations. The National Conference of Insurance Legislators (NCOIL) also works to harmonize the different state laws.
In the European Union, the Third Non-Life Directive and the Third Life Directive, both passed in 1992 and effective 1994, created a single insurance market in Europe and allowed insurance companies to offer insurance anywhere in the EU (subject to permission from authority in the head office) and allowed insurance consumers to purchase insurance from any insurer in the EU. As far as insurance in the United Kingdom, the Financial Services Authority took over insurance regulation from the General Insurance Standards Council in 2005; laws passed include the Insurance Companies Act 1973 and another in 1982, and reforms to warranty and other aspects under discussion .
The insurance industry in China was nationalized in 1949 and thereafter offered by only a single state-owned company, the People's Insurance Company of China, which was eventually suspended as demand declined in a communist environment. In 1978, market reforms led to an increase in the market and by 1995 a comprehensive Insurance Law of the People's Republic of China was passed, followed in 1998 by the formation of China Insurance Regulatory Commission (CIRC), which has broad regulatory authority over the insurance market of China.
In India IRDA is insurance regulatory authority. As per the section 4 of IRDA Act 1999, Insurance Regulatory and Development Authority (IRDA), which was constituted by an act of parliament. National Insurance Academy, Pune is apex insurance capacity builder institute promoted with support from Ministry of Finance and by LIC, Life & General Insurance companies.
Controversies.
Does not reduce the risk.
Some analysts argue that insurance does not reduce the risk, because insurance costs the policyholder, due to the premiums that they have to pay. Insurance is a risk for both the insurance company and the insured. The insurance company understands the risk involved and will perform a risk assessment when writing the policy. As a result, the premiums may go up if they determine that the policyholder will file a claim. If a person is financially stable and plans for life's unexpected events, they may be able to go without insurance. However, they must have enough to cover a total and complete loss of employment and of their possessions. Some states will accept a surety bond, a government bond, or even making a cash deposit with the state.
Insurance insulates too much.
An insurance company may inadvertently find that its insureds may not be as risk-averse as they might otherwise be (since, by definition, the insured has transferred the risk to the insurer), a concept known as moral hazard. This 'insulates' many from the
true costs of living with risk, potentially negating measures that can mitigate or adapt to risk and leading some to describe insurance schemes as potentiall maladaptive. To reduce their own financial exposure, insurance companies have contractual clauses that mitigate their obligation to provide coverage if the insured engages in behavior that grossly magnifies their risk of loss or liability.
For example, life insurance companies may require higher premiums or deny coverage altogether to people who work in hazardous occupations or engage in dangerous sports. Liability insurance providers do not provide coverage for liability arising from intentional torts committed by or at the direction of the insured. Even if a provider desired to provide such coverage, it is against the public policy of most countries to allow such insurance to exist, and thus it is usually illegal.
Complexity of insurance policy contracts.
Insurance policies can be complex and some policyholders may not understand all the fees and coverages included in a policy. As a result, people may buy policies on unfavorable terms. In response to these issues, many countries have enacted detailed statutory and regulatory regimes governing every aspect of the insurance business, including minimum standards for policies and the ways in which they may be advertised and sold.
For example, most insurance policies in the English language today have been carefully drafted in plain English; the industry learned the hard way that many courts will not enforce policies against insureds when the judges themselves cannot understand what the policies are saying. Typically, courts construe ambiguities in insurance policies against the insurance company and in favor of coverage under the policy.
Many institutional insurance purchasers buy insurance through an insurance broker. While on the surface it appears the broker represents the buyer (not the insurance company), and typically counsels the buyer on appropriate coverage and policy limitations, in the vast majority of cases a broker's compensation comes in the form of a commission as a percentage of the insurance premium, creating a conflict of interest in that the broker's financial interest is tilted towards encouraging an insured to purchase more insurance than might be necessary at a higher price. A broker generally holds contracts with many insurers, thereby allowing the broker to "shop" the market for the best rates and coverage possible.
Insurance may also be purchased through an agent. A tied agent, working exclusively with one insurer, represents the insurance company from whom the policyholder buys (while a free agent sells policies of various insurance companies). Just as there is a potential conflict of interest with a broker, an agent has a different type of conflict. Because agents work directly for the insurance company, if there is a claim the agent may advise the client to the benefit of the insurance company. Agents generally cannot offer as broad a range of selection compared to an insurance broker.
An independent insurance consultant advises insureds on a fee-for-service retainer, similar to an attorney, and thus offers completely independent advice, free of the financial conflict of interest of brokers and/or agents. However, such a consultant must still work through brokers and/or agents in order to secure coverage for their clients.
Limited consumer benefits.
In United States, economists and consumer advocates generally consider insurance to be worthwhile for low-probability, catastrophic losses, but not for high-probability, small losses. Because of this, consumers are advised to select high deductibles and to not insure losses which would not cause a disruption in their life. However, consumers have shown a tendency to prefer low deductibles and to prefer to insure relatively high-probability, small losses over low-probability, perhaps due to not understanding or ignoring the low-probability risk. This is associated with reduced purchasing of insurance against low-probability losses, and may result in increased inefficiencies from moral hazard.
Redlining.
Redlining is the practice of denying insurance coverage in specific geographic areas, supposedly because of a high likelihood of loss, while the alleged motivation is unlawful discrimination. Racial profiling or redlining has a long history in the property insurance industry in the United States. From a review of industry underwriting and marketing materials, court documents, and research by government agencies, industry and community groups, and academics, it is clear that race has long affected and continues to affect the policies and practices of the insurance industry.
In July 2007, The Federal Trade Commission (FTC) released a report presenting the results of a study concerning credit-based insurance scores in automobile insurance. The study found that these scores are effective predictors of risk. It also showed that African-Americans and Hispanics are substantially overrepresented in the lowest credit scores, and substantially underrepresented in the highest, while Caucasians and Asians are more evenly spread across the scores. The credit scores were also found to predict risk within each of the ethnic groups, leading the FTC to conclude that the scoring models are not solely proxies for redlining. The FTC indicated little data was available to evaluate benefit of insurance scores to consumers. The report was disputed by representatives of the Consumer Federation of America, the National Fair Housing Alliance, the National Consumer Law Center, and the Center for Economic Justice, for relying on data provided by the insurance industry.
All states have provisions in their rate regulation laws or in their fair trade practice acts that prohibit unfair discrimination, often called redlining, in setting rates and making insurance available.
In determining premiums and premium rate structures, insurers consider quantifiable factors, including location, credit scores, gender, occupation, marital status, and education level. However, the use of such factors is often considered to be unfair or unlawfully discriminatory, and the reaction against this practice has in some instances led to political disputes about the ways in which insurers determine premiums and regulatory intervention to limit the factors used.
An insurance underwriter's job is to evaluate a given risk as to the likelihood that a loss will occur. Any factor that causes a greater likelihood of loss should theoretically be charged a higher rate. This basic principle of insurance must be followed if insurance companies are to remain solvent. Thus, "discrimination" against (i.e., negative differential treatment of) potential insureds in the risk evaluation and premium-setting process is a necessary by-product of the fundamentals of insurance underwriting. For instance, insurers charge older people significantly higher premiums than they charge younger people for term life insurance. Older people are thus treated differently from younger people (i.e., a distinction is made, discrimination occurs). The rationale for the differential treatment goes to the heart of the risk a life insurer takes: Old people are likely to die sooner than young people, so the risk of loss (the insured's death) is greater in any given period of time and therefore the risk premium must be higher to cover the greater risk. However, treating insureds differently when there is no actuarially sound reason for doing so is unlawful discrimination.
Insurance patents.
New assurance products can now be protected from copying with a business method patent in the United States.
A recent example of a new insurance product that is patented is Usage Based auto insurance. Early versions were independently invented and patented by a major US auto insurance company, Progressive Auto Insurance () and a Spanish independent inventor, Salvador Minguijon Perez ().
Many independent inventors are in favor of patenting new insurance products since it gives them protection from big companies when they bring their new insurance products to market. Independent inventors account for 70% of the new U.S. patent applications in this area.
Many insurance executives are opposed to patenting insurance products because it creates a new risk for them. The Hartford insurance company, for example, recently had to pay $80 million to an independent inventor, Bancorp Services, in order to settle a patent infringement and theft of trade secret lawsuit for a type of corporate owned life insurance product invented and patented by Bancorp.
There are currently about 150 new patent applications on insurance inventions filed per year in the United States. The rate at which patents have been issued has steadily risen from 15 in 2002 to 44 in 2006.
Inventors can now have their insurance US patent applications reviewed by the public in the Peer to Patent program. The first insurance patent to be granted was including another example of an application posted was US2009005522 "risk assessment company". It was posted on March 6, 2009. This patent application describes a method for increasing the ease of changing insurance companies.
Insurance industry and rent-seeking.
Certain insurance products and practices have been described as rent-seeking by critics. That is, some insurance products or practices are useful primarily because of legal benefits, such as reducing taxes, as opposed to providing protection against risks of adverse events. Under United States tax law, for example, most owners of variable annuities and variable life insurance can invest their premium payments in the stock market and defer or eliminate paying any taxes on their investments until withdrawals are made. Sometimes this tax deferral is the only reason people use these products. Another example is the legal infrastructure which allows life insurance to be held in an irrevocable trust which is used to pay an estate tax while the proceeds themselves are immune from the estate tax.
Religious concerns.
Muslim scholars have varying opinions about life insurance. Life insurance policies that earn interest (or guaranteed bonus/NAV) are generally considered to be a form of "riba" (usury) and some consider even policies that do not earn interest to be a form of "gharar" (speculation). Some argue that "gharar" is not present due to the actuarial science behind the underwriting.
Jewish rabbinical scholars also have expressed reservations regarding insurance as an avoidance of God's will but most find it acceptable in moderation.
Some Christians believe insurance represents a lack of faith and there is a long history of resistance to commercial insurance in Anabaptist communities (Mennonites, Amish, Hutterites, Brethren in Christ) but many participate in community-based self-insurance programs that spread risk within their communities.
See also.
Country-specific articles:

</doc>
<doc id="15179" url="https://en.wikipedia.org/wiki?curid=15179" title="Indira Gandhi">
Indira Gandhi

Indira Priyadarshini Gandhi (; Nehru; 19 November 1917 – 31 October 1984) was a key 20th century stateswoman, a central figure of the Indian National Congress party, and to date the only female Prime Minister of India. Indira Gandhi was the only child of India's first Prime Minister, Jawaharlal Nehru. She served as Prime Minister from 1966 to 1977 and then again from 1980 until her assassination in 1984, making her the second-longest-serving Prime Minister after her father. 
Gandhi served as her father's personal assistant and hostess during his tenure as prime minister between 1947 and 1964. She was elected Congress President in 1959. Upon her father's death in 1964, Gandhi refused to enter Congress party leadership contest and instead chose to become a cabinet minister in the government led by Lal Bahadur Shastri. In Congress' party parliamentary leadership election held in early 1966 upon the death of Shastri, she defeated her rival, Morarji Desai, to become leader and thus succeed Shastri as the prime minister of India.
As the Prime Minister of India, Gandhi was known for her political ruthlessness and unprecedented centralisation of power. She went to war with Pakistan in support of the independence movement and war of independence in East Pakistan, which resulted in an Indian victory and the creation of Bangladesh, as well as increasing India's influence to the point where it became the regional hegemon of South Asia. Gandhi also presided over a controversial state of emergency from 1975 to 1977 during which she ruled by decree. She was assassinated in 1984 by her Sikh bodyguards a few months after she ordered the storming of the Harmandir Sahib in Amritsar to counter the Punjab insurgency.
Early life and career.
Indira Gandhi was born Indira Nehru in a Kashmiri Pandit family on 19 November 1917 in Allahabad. Her father, Jawaharlal Nehru, was a leading figure in India's political struggle for independence from British rule, and became the first Prime Minister of the Union (and later Republic) of India. She was the only child (a younger brother was born, but died young), and grew up with her mother, Kamala Nehru, at the Anand Bhavan; a large family estate in Allahabad. She had a lonely and unhappy childhood. Her father was often away, directing political activities or being incarcerated in prison, while her mother was frequently bed-ridden with illness, and later suffered an early death from tuberculosis. She had limited contact with her father, mostly through letters.
Gandhi was mostly taught at home by tutors, and intermittently attended school until matriculation in 1934. She was a student at the Modern School in Delhi, St Cecilia's and St Mary's Christian convent schools in Allahabad, the International School of Geneva, the Ecole Nouvelle in Bex, and the Pupils' Own School in Poona and Bombay. She and her mother Kamala Nehru moved to Belur Math headquarters of Ramakrishna Mission where Swami Ranganathananda was her guardian later she went on to study at the Viswa Bharati University in Shantiniketan. It was during her interview that Rabindranath Tagore named her Priyadarshini, and she came to be known as Indira Priyadarshini Nehru. A year later, however, she had to leave university to attend to her ailing mother in Europe. While there, it was decided that Gandhi would continue her education at the University of Oxford. After her mother died, she briefly attended the Badminton School before enrolling at Somerville College in 1937 to study history. Gandhi had to take the entrance examination twice, having failed at her first attempt with a poor performance in Latin. At Oxford, she did well in history, political science and economics, but her grades in Latin—a compulsory subject—remained poor. She did, however, have an active part within the student life of the university, such as the Oxford Majlis Asian Society.
During her time in Europe, Gandhi was plagued with ill-health and was constantly attended by doctors. She had to make repeated trips to Switzerland to recover, disrupting her studies. She was being treated there in 1940, when the Nazi armies rapidly conquered Europe. Gandhi tried to return to England through Portugal but was left stranded for nearly two months. She managed to enter England in early 1941, and from there returned to India without completing her studies at Oxford. The university later awarded her an honorary degree. In 2010, Oxford further honoured her by selecting her as one of the ten Oxasians, illustrious Asian graduates from the University of Oxford.
During her stay in Great Britain, young Gandhi frequently met her future husband Feroze Gandhi (no relation to Mahatma Gandhi), whom she knew from Allahabad, and who was studying at the London School of Economics. The marriage took place in Allahabad according to Adi Dharm rituals though Feroze belonged to a Zorastrian Parsi family of Gujarat.
In the 1950s, Indira, now Mrs. Indira Gandhi after her marriage, served her father unofficially as a personal assistant during his tenure as the first Prime Minister of India. Towards the end of the 1950s, Indira Gandhi served as the President of the Congress. In that capacity, she was instrumental in getting the Communist led Kerala State Government dismissed in 1959. That government had the distinction of being India's first ever elected Communist Government.
After her father's death in 1964 she was appointed as a member of the Rajya Sabha (upper house) and became a member of Lal Bahadur Shastri's cabinet as Minister of Information and Broadcasting. In 1966, after Shastri's death, the Congress legislative party elected Indira Gandhi over Morarji Desai as their leader. Congress party veteran, Kamaraj was instrumental in achieving Indira's victory.
First term as Prime Minister between 1966 and 1971.
Following a poor showing in the 1967 general election, Indira Gandhi started progressively moving to the left in the political spectrum. In 1969, after falling out with senior party leaders on a number of issues, the party president S. Nijalingappa expelled her from the party. Gandhi, in turn floated her own faction of the Congress party and managed to retain most of the Congress MPs on her side with only 65 on the side of Congress (O) faction. The policies of the Congress under Indira Gandhi, prior to the 1971 elections, also included proposals for the abolition of Privy Purse to former rulers of the Princely states and the 1969 nationalization of the fourteen largest banks in India.
The internal structure of the Congress Party had withered following its numerous splits, leaving it entirely dependent on her leadership for its election fortunes. "Garibi Hatao" (Eradicate Poverty) was the theme for Gandhi's 1971 bid. On the other hand, the combined opposition alliance had a two word manifesto of "Indira Hatao" (Remove Indira).
The Garibi Hatao slogan and the proposed anti-poverty programs that came with it were designed to give Gandhi an independent national support, based on rural and urban poor. This would allow her to bypass the dominant rural castes both in and of state and local governments; likewise the urban commercial class. And, for their part, the previously voiceless poor would at last gain both political worth and political weight. The programs created through Garibi Hatao, though carried out locally, were funded and developed by the Central Government in New Delhi. The program was supervised and staffed by the Indian National Congress party. "These programs also provided the central political leadership with new and vast patronage resources to be disbursed... throughout the country.",
The biggest achievement of Indira Gandhi after the 1971 election was India's decisive victory in the Indo-Pakistani War of 1971, that led to the formation of independent Bangladesh. Some sources claim she was hailed as Goddess Durga by opposition leader Atal Bihari Vajpayee at that time.
In the elections held for State assemblies across India in March 1972, the Congress(R) swept to power in most states riding on the post-war "Indira wave".
Despite the victory against Pakistan, the Congress government faced numerous problems during this term. Some of these were due to high inflation which was in turn caused by war time expenses, drought in some parts of the country and more importantly, the 1973 oil crisis. The opposition to Gandhi in 1973-75 period, after the Indira wave had receded, was strongest in Bihar and Gujarat. In Bihar, Jayaprakash Narayan, the veteran leader came out of retirement to lead the protest movement there.
Verdict on electoral malpractice.
On 12 June 1975 the High Court of Allahabad declared Indira Gandhi's election to the Lok Sabha void on grounds of electoral malpractice. In an election petition filed by Raj Narain (who later on defeated her in 1977 parliamentary election from Rae Bareily), he had alleged several major as well as minor instances of using government resources for campaigning. The court thus ordered her stripped of her parliamentary seat and banned from running for any office for six years. The Prime Minister must be a member of either the Lok Sabha (the lower house in the Parliament of India) or the Rajya Sabha (the upper house). Thus, this decision effectively removed her from office. Gandhi had asked one of her colleagues in government, Mr Ashoke Kumar Sen to defend her in court.
But Gandhi rejected calls to resign and announced plans to appeal to the Supreme Court. The verdict was delivered by Mr Justice Jagmohanlal Sinha at Allahabad High Court. It came almost four years after the case was brought by Raj Narain, the premier's defeated opponent in the 1971 parliamentary election. Gandhi, who gave evidence in her defence during the trial, was found guilty of dishonest election practices, excessive election expenditure, and of using government machinery and officials for party purposes. The judge, however, rejected more serious charges of bribery against her.
Gandhi insisted that the conviction did not undermine her position, despite having been unseated from the lower house of parliament, Lok Sabha, by order of the High Court. She said: "There is a lot of talk about our government not being clean, but from our experience the situation was very much worse when parties were forming governments". And she dismissed criticism of the way her Congress Party raised election campaign money, saying all parties used the same methods. The prime minister retained the support of her party, which issued a statement backing her. After news of the verdict spread, hundreds of supporters demonstrated outside her house, pledging their loyalty. Indian High Commissioner BK Nehru said Gandhi's conviction would not harm her political career. "Mrs Gandhi has still today overwhelming support in the country," he said. "I believe the prime minister of India will continue in office until the electorate of India decides otherwise".
State of Emergency (1975–1977).
Gandhi moved to restore order by ordering the arrest of most of the opposition participating in the unrest. Her Cabinet and government then recommended that President Fakhruddin Ali Ahmed declare a state of emergency because of the disorder and lawlessness following the Allahabad High Court decision. Accordingly, Ahmed declared a State of Emergency caused by internal disorder, based on the provisions of Article 352(1) of the Constitution, on 25 June 1975.
Rule by decree.
Within a few months, President's Rule was imposed on the two opposition party ruled states of Gujarat and Tamil Nadu thereby bringing the entire country under direct Central rule or by governments led by the ruling Congress party.
Police were granted powers to impose curfews and indefinitely detain citizens and all publications were subjected to substantial censorship by the Ministry of Information and Broadcasting. Finally, the impending legislative assembly elections were indefinitely postponed, with all opposition-controlled state governments being removed by virtue of the constitutional provision allowing for a dismissal of a state government on recommendation of the state's governor.
Indira Gandhi used the emergency provisions to change conflicting party members.
President Ahmed issued ordinances that did not require debate in the Parliament, allowing Gandhi to rule by decree.
Rise of Sanjay.
The Emergency saw the entry of Gandhi's younger son, Sanjay, into Indian Politics. Sanjay wielded tremendous power during the emergency without holding any Government office. According to Mark Tully, "His inexperience did not stop him from using the Draconian powers his mother, Indira Gandhi, had taken to terrorise the administration, setting up what was in effect a police state."
It was said that during the Emergency he virtually ran India along with his friends, especially Bansi Lal. It was also quipped that Sanjay Gandhi had total control over his mother and that the government was run by the PMH (Prime Minister House) rather than the PMO (Prime Minister Office).
1977 election and opposition years.
In 1977, after extending the state of emergency twice, Indira Gandhi called elections to give the electorate a chance to vindicate her rule. Gandhi may have grossly misjudged her popularity by reading what the heavily censored press wrote about her. In any case, she was opposed by the Janata alliance of Opposition parties. Janata alliance, with Jai Prakash Narayan as its spiritual guide, claimed the elections were the last chance for India to choose between "democracy and dictatorship." 
The Congress Party split during the election campaign of 1977: veteran Gandhi supporters like Jagjivan Ram, Hemwati Nandan Bahuguna and Nandini Satpathy were compelled to part ways and form a new political entity, CFD (Congress for Democracy), primarily due to intra-party politicking and also due to circumstances created by Sanjay Gandhi. The prevailing rumour was that Sanjay had intentions of dislodging Gandhi and the trio stood between that.
Gandhi's Congress party was crushed soundly in the elections. The public realized the statement and motto of the Janata Party alliance. Indira and Sanjay Gandhi both lost their seats, and Congress was cut down to 153 seats (compared with 350 in the previous Lok Sabha), 92 of which were in the South. 
The Janata alliance, under the leadership of Morarji Desai, came into power after the State of Emergency was lifted. The alliance parties later merged to form the Janata Party under the guidance of Gandhian leader, Jayaprakash Narayan. The other leaders of the Janata Party were Charan Singh, Raj Narain, George Fernandes and Atal Bihari Vajpayee.
In opposition and return to power.
Since Gandhi had lost her seat in the election, the defeated Congress party appointed Yashwantrao Chavan as their parliamentary party leader. Soon afterwards, the Congress party split again with Gandhi floating her own Congress (I) faction. She won a bye-election from the Chikamagalur constituency to the Lok sabha in 1978. However, the Janata government's Home Minister, Choudhary Charan Singh, ordered the arrest of her and Sanjay Gandhi on several charges, none of which would be easy to prove in an Indian court. The arrest meant that Indira Gandhi was automatically expelled from Parliament. These allegations included that she "‘had planned or thought of killing all opposition leaders in jail during the Emergency’". In response to her arrest, Indira Gandhi's supporters hijacked an Indian Airlines jet and demanded her immediate release. However, this strategy backfired disastrously. Her arrest and long-running trial gained her great sympathy from many people. The Janata coalition was only united by its hatred of Gandhi (or "that woman" as some called her). The party included right wing Hindu Nationalists, Socialists and former Congress party members. With so little in common, the Morarji Desai government was bogged down by infighting. In 1979, the government started to unravel over the issue of dual loyalties of some members to Janata and the RSS. The ambitious Union Finance minister, Charan Singh, who as the Union Home Minister during the previous year had ordered arrest of Gandhi, took advantage of this and started courting the Congress (I). After a significant exodus from the party to Charan Singh's faction, Desai resigned in July 1979. Charan Singh was appointed Prime Minister, by President Reddy, after Indira and Sanjay Gandhi promised Singh that Congress (I) would support his government from outside on certain conditions., The conditions included dropping all charges against Indira and Sanjay. Since Charan Singh refused to drop the charges, Congress (I) withdrew its support and President Reddy dissolved Parliament in August 1979.
Before the 1980 elections Gandhi approached the then Shahi Imam of Jama Masjid, Syed Abdullah Bukhari and entered into an agreement with him on the basis of 10-point programme to secure the support of the Muslim votes. In the elections held in January, Congress returned to power with a landslide majority.
In 1979, when she visited Madurai, some hooligans attacked her. Nedumaran saved her from the attacks.
1980 elections and third term.
The Congress (I) under Gandhi swept back to power in January 1980. Elections soon after to State assemblies across the country also brought back Congress ministries in the state with Sanjay Gandhi choosing loyalists to lead the states. Sanjay soon died in an air crash early into this term. Sanjay Gandhi died instantly from head wounds in an air crash on 23 June 1980 near Safdarjung Airport in New Delhi.] He was flying a new aircraft of the Delhi Flying club, and, while performing an aerobatic manoeuvre over his office, lost control and crashed. The only passenger in the plane, Captain Subhash Saxena, also died in the crash. Gandhi by this stage only trusted her family members and therefore decided to bring in her reluctant pilot son, Rajiv into politics.
Operation Blue Star (attack on Golden temple).
In the 1977 elections, a coalition led by the Sikh-majority Akali Dal came to power in the northern Indian state of Punjab. In an effort to split the Akali Dal and gain popular support among the Sikhs, Indira Gandhi's Congress helped bring the orthodox religious leader Jarnail Singh Bhindranwale to prominence in Punjab politics. Later, Bhindranwale's organisation Damdami Taksal became embroiled in violence with another religious sect called the Sant Nirankari Mission, and he was accused of instigating the murder of the Congress leader Jagat Narain. After being arrested in this matter, Bhindranwale disassociated himself from Congress and joined hands with the Akali Dal. In July 1982, he led the campaign for the implementation of the Anandpur Sahib Resolution, which demanded greater autonomy for the Sikh-majority state. Meanwhile, a small section of the Sikhs, including some of Bhindranwale's followers, turned to militancy after being targeted by government officials and police in support of the Resolution. After several futile negotiations, Indira Gandhi ordered the Indian army to enter the Golden Temple in order to subdue Bhindranwale and his followers. Indian army used heavy artillery such as tanks and canons and machine guns in addition with helicopters to crush the Sikhs of Harmindar Sahib. In the resulting Operation Blue Star, the shrine was damaged and many civilians were brutally massacred. Which started a swim of riot in Punjab and as a result many religious Sikh leaders as well as many devotees of Harminder Sahib were arrested, tortured and killed. Still there is a long list of missing persons from Sikh religious community of Punjab being abducted by state agencies, police and army. The State of Punjab was closed to international media, its phone and communication lines shut. To this day the events remain controversial with a disputed number of victims; Many Sikhs seeing the attack as unjustified and Bhindrawale being declared the greatest Sikh martyr of the 20th century by Akal Takht (Sikh Political Authority) in 2003.
Assassination.
The day before her death (30 October 1984) Indira Gandhi visited Orissa where she gave her last speech:
Indira Gandhi delivered her last speech at the then Parade Ground in front of the Secretariat of Orissa. After her death, the Parade Ground was converted to the Indira Gandhi Park which was inaugurated by her son, Rajiv Gandhi.
On 31 October 1984, two of Gandhi's bodyguards, Satwant Singh and Beant Singh, shot her with their service weapons in the garden of the Prime Minister's residence at 1 Safdarjung Road, New Delhi. The shooting occurred as she was walking past a wicket gate guarded by Satwant and Beant. She was to have been interviewed by the British actor Peter Ustinov, who was filming a documentary for Irish television. Beant Singh shot her three times using his side-arm and Satwant Singh fired 30 rounds. Beant Singh and Satwant Singh dropped their weapons and surrendered. Afterwards they were taken away by other guards into a closed room where Beant Singh was shot dead. Kehar Singh was later arrested for conspiracy in the attack. Both Satwant and Kehar were sentenced to death and hanged in Delhi's Tihar Jail.
Indira Gandhi was brought at 9:30 AM to the All India Institute of Medical Sciences where doctors operated on her. She was declared dead at 2:20 PM. The post-mortem examination was conducted by a team of doctors headed by Dr. T.D. Dogra. Dr. Dogra stated that as many as 30 bullet wounds were sustained by Indira Gandhi, from two sources, a Sten gun and a pistol. The assailants had fired 31 bullets at her, of which 30 had hit; 23 had passed through her body while 7 were trapped inside her. Dr. Dogra extracted bullets to establish the identity of the weapons and to match each weapon with the bullets recovered by ballistic examination. The bullets were matched with respective weapons at CFSL Delhi. Subsequently Dr. Dogra appeared in the court of Shri Mahesh Chandra as an expert witness (PW-5), and his testimony lasted several sessions. The cross examination was conducted by Shri P. N. Lekhi, the defence counsel. Salma Sultan gave the first news of assassination of Indira Gandhi on Doordarshan's evening news on 31 October 1984, more than 10 hours after she was shot. She died two weeks and five days before her 67th birthday.
Gandhi was cremated on 3 November near Raj Ghat. The site where she was cremated is today known as Shakti Sthala. Her funeral was televised live on domestic and international stations, including the BBC. Following her cremation, millions of Sikhs were displaced and nearly three thousand were killed in anti-Sikh riots. Rajiv Gandhi on a live TV show said of the carnage, "When a big tree falls, the earth shakes."
Foreign policy.
South Asia.
In 1971, Gandhi intervened in the Pakistani Civil War in support of East Pakistan. India emerged victorious in the resulting conflict to become the regional hegemon of South Asia. India had signed a treaty with the Soviet Union promising mutual assistance in the case of war, while Pakistan received active support from the United States during the conflict. U.S. President Richard Nixon disliked Gandhi personally, referring to her as a "witch" and "clever fox" in his private communication with Secretary of State Henry Kissinger. Relations with the U.S. became distant as Gandhi developed closer ties with the Soviet Union after the war. The latter grew to become India's largest trading partner and its biggest arms supplier for much of Gandhi's premiership. Nixon later wrote of the war: "suckered [America. Suckered us...this woman suckered us."
India's new hegemonic position as articulated under the "Indira Doctrine" led to attempts to bring the Himalayan states under the Indian sphere of influence. Nepal and Bhutan remained aligned with India, while in 1975, after years of building up support, Gandhi annexed Sikkim to India. This was denounced as a "despicable act" by China.
India maintained close ties with neighbouring Bangladesh (formerly East Pakistan) following the Liberation War. Prime Minister Sheikh Mujibur Rahman recognized Gandhi's contributions to the independence of Bangladesh. However, Mujibur Rahman's pro-India policies antagonised many in Bangladeshi politics and the military, who feared that Bangladesh had become a client state of India. The Assassination of Mujibur Rahman in 1975 led to the establishment of Islamist military regimes that sought to distance the country from India. Gandhi's relationship with the military regimes was strained, due to her alleged support of anti-Islamist leftist guerrilla forces in Bangladesh. Generally, however, there was a rapprochement between Gandhi and the Bangladeshi regimes, although issues such as border disputes and the Farakka Dam remained an irritant in bilateral ties. In 2011, the Government of Bangladesh conferred its highest state award posthumously on Gandhi for her "outstanding contribution" to the country's independence.
Gandhi's approach to dealing with Sri Lanka's ethnic problems was initially accommodating. She enjoyed cordial relations with Prime Minister Sirimavo Bandaranaike. In 1974, India ceded the tiny islet of Kachchatheevu to Sri Lanka in order to save Bandaranaike's socialist government from a political disaster. However, relations soured over Sri Lanka's turn away from socialism under Junius Jayewardene, whom Gandhi despised as a "western puppet." India under Gandhi was alleged to have supported LTTE militants in the 1980s to put pressure on Jayewardene to abide by Indian interests. Nevertheless, Gandhi rejected demands to invade Sri Lanka in the aftermath of Black July 1983, an anti-Tamil pogrom carried out by Sinhalese mobs. Gandhi made a statement emphasizing that she stood for the territorial integrity of Sri Lanka, although she also stated that India cannot "remain a silent spectator to any injustice done to the Tamil community."
India's relationship with Pakistan remained strained following the Shimla Accord in 1972. Gandhi's authorization of the detonation of a nuclear device at Pokhran in 1974 was viewed by Pakistani leader Zulfikar Ali Bhutto as an attempt to intimidate Pakistan into accepting India's hegemony in the subcontinent. However, in May 1976, Gandhi and Bhutto both agreed to reopen diplomatic establishments and normalize relations. Following the rise to power of General Zia-ul-Haq in Pakistan in 1978, India's relations with its neighbour reached a nadir. Gandhi accused General Zia of supporting Khalistani militants in Punjab. Military hostilities recommenced in 1984 following Gandhi's authorization of Operation Meghdoot. India was victorious in the resulting Siachen conflict against Pakistan.
Middle East.
Gandhi remained a staunch supporter of Palestinians in the Arab-Israeli conflict and was critical of the Middle East diplomacy sponsored by the United States. Israel was viewed as a religious state and thus an analogue to India's arch rival Pakistan. Indian diplomats also hoped to win Arab support in countering Pakistan in Kashmir. Nevertheless, Gandhi authorized the development of a secret channel of contact and security assistance with Israel in the late 1960s. Her lieutenant, Narasimha Rao, later became Prime Minister and approved full diplomatic ties with Israel in 1992.
India's pro-Arab policy had mixed success. Establishment of close ties with the socialist and secular Baathist regimes to some extent neutralized Pakistani propaganda against India. However, the Indo-Pakistani War of 1971 put the Arab and Muslim states of the Middle East in a dilemma as the war was fought by two states both friendly to the Arabs. The progressive Arab regimes in Egypt, Syria, and Algeria chose to remain neutral, while the conservative pro-American Arab monarchies in Jordan, Saudi Arabia, Kuwait, and United Arab Emirates openly supported Pakistan. Egypt's stance was met with dismay by the Indians, who had come to expect close co-operation with the Baathist regimes. But, the death of Nasser in 1970 and Sadat's growing friendship with Riyadh, and his mounting differences with Moscow, constrained Egypt to a policy of neutrality. Gandhi's overtures to Muammar Gaddafi were rebuffed. Libya agreed with the Arab monarchies in believing that Gandhi's intervention in East Pakistan was an attack against Islam.
The 1971 war temporarily became a stumbling block in growing Indo-Iranian ties. Although Iran had earlier characterized the Indo-Pakistani war in 1965 as Indian aggression, the Shah had launched an effort at rapprochement with India in 1969 as part of his effort to secure support for a larger Iranian role in the Persian Gulf. Gandhi's tilt towards Moscow and her dismemberment of Pakistan was perceived by the Shah as part of a larger anti-Iran conspiracy involving India, Iraq, and the Soviet Union. Nevertheless, Iran had resisted Pakistani pressure to activate the Baghdad Pact and draw in the Central Treaty Organisation (CENTO) into the conflict. Gradually, Indian and Iranian disillusionment with their respective regional allies led to a renewed partnership between the nations. Gandhi was unhappy with the lack of support from India's Arab allies during the war with Pakistan, while the Shah was apprehensive at the growing friendship between Pakistan and the Gulf states, specially Saudi Arabia, and the growing influence of Islam in Pakistani society. There was an increase in Indian economic and military co-operation with Iran during the 1970s. The 1974 India-Iranian agreement led to Iran supplying nearly 75 percent of India's crude oil demands. Gandhi appreciated the Shah's disregard of Pan-Islamism in diplomacy.
Asia-Pacific.
One of the major developments in Southeast Asia during Gandhi's premiership was the formation of the Association of Southeast Asian Nations (ASEAN) in 1967. Relations between ASEAN and India was mutually antagonistic. ASEAN in the Indian perception was linked to the Southeast Asia Treaty Organization (SEATO), and it was therefore, seen as a pro-American organisation. On their part, the ASEAN nations were unhappy with Gandhi's sympathy for the Viet Cong and India's strong links with the USSR. Furthermore, they were also apprehensions in the region about Gandhi's future plans, particularly after India played a big role in breaking up Pakistan and facilitating in the emergence of Bangladesh as a sovereign country in 1971. India's entry into the nuclear weapons club in 1974 contributed to tensions in Southeast Asia. Relations only began to improve following Gandhi's endorsement of the ZOPFAN declaration and the disintegration of the SEATO alliance in the aftermath of Pakistani and American defeats in the region. Nevertheless, Gandhi's close relations with reunified Vietnam and her decision to recognize the People's Republic of Kampuchea in 1980 meant that India and ASEAN were not able to develop a viable partnership.
Africa.
Although independent India was initially viewed as a champion of anti-colonialism, its cordial relationship with the Commonwealth of Nations and liberal views of British colonial policies in East Africa had harmed its image as a staunch supporter of the anti-colonial movements. Indian condemnation of militant struggles in Kenya and Algeria was in sharp contrast to China, who had supported armed struggle to win African independence. After reaching a high diplomatic point in the aftermath of Nehru's role in the Suez Crisis, India's isolation from Africa was complete when only four nations; Ethiopia, Kenya, Nigeria and Libya supported her during the Sino-Indian War in 1962. After Gandhi became Prime Minister, diplomatic and economic relations with the states which had sided with India during the Sino-Indian War were expanded. Gandhi began negotiations with the Kenyan government to establish the Africa-India Development Cooperation. The Indian government also started considering the possibility of bringing Indians settled in Africa within the framework of its policy goals to help recover its declining geo-strategic influence. Gandhi declared the people of Indian origin settled in Africa as "Ambassors of India." Efforts to rope in the Asian community to join Indian diplomacy, however, came to naught, partly because of the unwilligness of Indians to remain in politically insecure surroundings and partly due to the exodus of African Indians to Britain with the passing of the Commonwealth Immigrants Act in 1968. In Uganda, the African Indian community even suffered persecution and eventually expulsion under the government of Idi Amin.
Foreign and domestic policy successes in the 1970s enabled Gandhi to rebuild India's image in the eyes of African states. Victory over Pakistan and India's possession of nuclear weapons showed the degree of India's progress. Furthermore, the conclusion of the Indo-Soviet treaty in 1971 and threatening gestures by the major western power, the United States, to send its nuclear armed Task Force 74 into the Bay of Bengal at the height of the East Pakistan crisis had enabled India to regain its anti-imperialist image. Gandhi firmly tied Indian anti-imperialist interests in Africa to those of the Soviet Union. Unlike Nehru, she openly and enthusiastically supported liberation struggles in Africa. At the same time, Chinese influence in Africa had declined owing to its incessant quarrels with the Soviet Union. These developments permanently halted India's decline in Africa and helped reestablish its geo-strategic presence.
Economic policy.
Gandhi presided over three Five-Year plans as Prime Minister, two of which succeeded in meeting the targeted growth.
There is considerable debate regarding whether Gandhi was a socialist on principle or out of political expediency. S. K. Datta-Ray described her as "a master of rhetoric...often more posture than policy", while "The Times" journalist, Peter Hazelhurst, famously quipped that Gandhi's socialism was "slightly left of self-interest." Critics have focused on the contradictions in the evolution of her stance towards communism; Gandhi being known for her anti-communist stance in the 1950s with Meghnad Desai even describing her as "the scourge of Communist Party." Yet, she later forged close relations with Indian communists even while using the army to break the Naxalites. In this context, Gandhi was accused of formulating populist policies to suit her political needs; being seemingly against the rich and big business while preserving the status quo in order to manipulate the support of the left at times of political insecurity, such as the late 1960s. Although Gandhi came to be viewed in time as the scourge of the right-wing and reactionary political elements of India, leftist opposition to her policies emerged. As early as 1969, critics had begun accusing her of insincerity and machiavellism. "The Indian Libertarian" wrote that: "it would be difficult to find a more machiavellian leftist than Mrs Indira Gandhi...for here is Machiavelli at its best in the person of a suave, charming and astute politician." Rosser wrote that "some have even seen the declaration of emergency rule in 1975 as a move to suppress [leftist dissent against Gandhi's policy shift to the right." In the 1980s, Gandhi was accused of "betraying socialism" after the beginning of "Operation Forward", an attempt at economic reform. Nevertheless, others were more convinced of Gandhi's sincerity and devotion to socialism. Pankaj Vohra noted that "even the late prime minister’s critics would concede that the maximum number of legislations of social significance was brought about during her tenure...that she lives in the hearts of millions of Indians who shared her concern for the poor and weaker sections and who supported her politics."
In summarizing the biographical works on Gandhi, Blema S. Steinberg concluded she was decidedly non-ideological. Only 7.4% (24) of the total 330 biographical extractions posit ideology as a reason for her policy choices. Steinberg noted Gandhi's association with socialism was superficial; only having a general and traditional commitment to the ideology, by way of her political and family ties. Gandhi personally had a fuzzy concept of socialism. In one of the early interviews she had given as Prime Minister, Gandhi had ruminated: "I suppose you could call me a socialist, but you have understand what we mean by that term...we used the word because it came closest to what we wanted to do here – which is to eradicate poverty. You can call it socialism; but if by using that word we arouse controversy, I don't see why we should use it. I don't believe in words at all." Regardless of the debate over her ideology or lack of thereof, Gandhi remains a left-wing icon. She has been described as the "arguably the greatest mass leader of the last century." Her campaign slogan, Garibi Hatao (Eng: Remove Poverty), has become the iconic motto of the Indian National Congress. To the rural and urban poor, untouchables, minorities and women in India, Gandhi was "Indira Amma or Mother Indira."
Green Revolution and the Fourth Five Year Plan.
Gandhi inherited a weak and troubled economy. Fiscal problems associated with the war with Pakistan in 1965, along with a drought-induced food crisis that spawned famines, had plunged India into the sharpest recession since independence. The government responded by taking steps to liberalize the economy, and by agreeing to the devaluation of the currency in return for the restoration of foreign aid. The economy managed to recover in 1966 and ended up growing at 4.1% over 1966–1969. But, much of that growth was offset by the fact that the external aid promised by the United States government and the International Bank for Reconstruction and Development (IBRD), meant to ease the short-run costs of adjustment to a liberalized economy, never materialized. American policy makers had complained of continued restrictions imposed on the economy. At the same time, Indo-US relations were straining due to Gandhi's criticism of the American bombing campaign in Vietnam. While it was thought, at the time, and for decades after, that President Johnson's policy of withholding food grain shipments was to coerce Indian support for the war, in fact, it was to offer India rainmaking technology that he wanted to use as a counterweight to China's possession of the atomic bomb. In light of the circumstances, liberalization became politically suspect and was soon abandoned. Grain diplomacy and currency devaluation became matters of intense national pride in India. After the bitter experience with Johnson, Gandhi decided not to request food aid in the future. Moreover, Gandhi's government resolved never again to become "so vulnerably dependent" on aid, and painstakingly began building up substantial foreign exchange reserves. When food stocks slumped after poor harvests in 1972, the government made it a point to use foreign exchange to buy US wheat commercially rather than seek resumption of food aid.
The period of 1967–75 was characterized by socialist ascendency in India which culminated in 1976 with the official declaration of state socialism. Gandhi not only abandoned the short lived liberalization programme but also aggressively expanded the public sector with new licensing requirements and other restrictions for industry. She began a new course by launching the "Fourth Five-Year Plan" in 1969. The government targeted growth at 5.7% while stating as its goals, "growth with stability and progressive achievement of self-reliance." The rationale behind the overall plan was Gandhi's "Ten Point Programme" of 1967. This had been her first economic policy formulation, six months after coming to office. The programme emphasized greater state control of the economy with the understanding that government control assured greater welfare than private control. Related to this point were a set of policies which were meant to regulate the private sector. By the end of the 1960s, the reversal of the liberalization process was complete, and India's policies were characterised as "protectionist as ever."
To deal with India's food problems, Gandhi expanded the emphasis on production of inputs to agriculture that had already been initiated by her father, Jawaharlal Nehru. The Green Revolution in India subsequently culminated under her government in the 1970s and transformed the country from a nation heavily reliant on imported grains and prone to famine to being largely able to feed itself, and become successful in achieving its goal of food security. Gandhi had a personal motive in pursuing agricultural self-sufficiency, having found India's dependency on the U.S. for shipments of grains humiliating.
The economic period of 1967–75 became significant for its major wave of nationalisations amidst the increased regulation of the private sector.
Some of the other objectives of the economic plan for the period was to provide for the minimum needs of the community through a rural works program and the removal of the privy purses of the nobility. Both these, and many other goals of the 1967 program were accomplished by 1974–75. Nevertheless, the success of the overall economic plan was tempered by the fact that annual growth at 3.3–3.4% over 1969–74 fell short of the targeted figure.
State of Emergency and the Fifth Five Year Plan.
The Fifth Five Year Plan (1974–79) was enacted in the backdrop of the state of emergency and the "Twenty Point Program" of 1975. The latter was the economic rationale of the emergency, a political act which has often been justified on economic grounds. In contrast to the reception of Gandhi's earlier economic plan, this one was criticized for being a "hastily thrown together wish list." Gandhi promised to reduce poverty by targeting the consumption levels of the poor and enact wide ranging social and economic reforms. The government additionally targeted an annual growth of 4.4% over the period of the plan.
The measures of the emergency regime was able to halt the economic trouble of the early to mid-1970s, which had been marred by harvest failures, fiscal contraction, and the breakdown of the Bretton Woods system of fixed exchanged rate; the resulting turbulence in the foreign exchange markets being further accentuated by the oil shock of 1973. The government was even able to exceed the targeted growth figure with an annual growth rate of 5.0–5.2% over the five-year period of the plan (1974–79). The economy grew at the rate of 9% in 1975–76 alone, and the Fifth Plan, became the first plan during which the per capita income of the economy grew by over 5%.
Operation Forward and the Sixth Five Year Plan.
Gandhi inherited a weak economy when she again became Prime Minister in 1980. The preceding year in 1979–80 under the Janata Party government had led to the strongest recession (−5.2%) in the history of modern India with inflation rampant at 18.2%. Gandhi proceeded to abrogate the Janata Party government's Five Year Plan in 1980 and launched the Sixth Five Year Plan (1980–85). The government targeted an average growth of 5.2% over the period of the plan. Measures to check the inflation were also taken; by the early 1980s inflation was under control at an annual rate of about 5%.
Although Gandhi continued professing socialist beliefs, the Sixth Five Year Plan was markedly different from the years of Garibi Hatao. Populist programs and policies were replaced by pragmatism. There was an emphasis on tightening public expenditures, greater efficiency of the State Owned Enterprises (SOE), which Gandhi qualified as a "sad thing", and in stimulating the private sector through deregulation and liberation of the capital market. The government subsequently launched "Operation Forward" in 1982, the first cautious attempt at reform. The Sixth Plan went on to become the most successful of the Five Year plans yet; showing an average growth of 5.7% over 1980–85.
Inflation and unemployment.
During Lal Bahadur Shastri's last full year in office (1965), inflation averaged 7.7%, compared to 5.2% at the end of Gandhi's first stint in office (1977). On average, inflation in India had remained below 7% through the 1950s and 1960s. But, it then accelerated sharply in the 1970s, from 5.5% in 1970–71 to over 20% by 1973–74, due to the international oil crisis. Gandhi declared inflation the gravest of problems in 1974 (at 25.2%) and devised a severe anti-inflation program. The government was successful in bringing down inflation during the emergency; achieving negative figures of −1.1% by the end of 1975–76.
Gandhi inherited a tattered economy in her second term; harvest failures and a second oil shock in the late 1970s had again caused inflation to rise. During Charan Singh's last year in office (1980), inflation averaged 18.2%, compared to 6.5% during Gandhi's last year in office (1984). General economic recovery under Gandhi led to an average inflation at 6.5% from 1981–82 to 1985–86; the lowest since the beginning of India's inflation problems in the 1960s.
Unemployment stayed constant at 9% over a nine-year period (1971–80) before declining to 8.3% in 1983.
Domestic policy.
Nationalisation.
Despite the provisions, control and regulations of Reserve Bank of India, most banks in India had continued to be owned and operated by private persons. Businessmen who owned the banks were often accused of channeling the deposits into their own companies, and ignoring the priority sector. Furthermore, there was a great resentment against "class" banking in India, which had left the poor (the majority population) unbanked. After becoming Prime Minister, Gandhi expressed the intention of nationalising the banks in a paper titled, ""Stray thoughts on Bank Nationalisation"" in order to alleviate poverty. The paper received the overwhelming support of the public. In 1969, Gandhi moved to nationalise fourteen major commercial banks. After the nationalisation of banks, the branches of the public sector banks in India rose to approximate 800 percent in deposits, and advances took a huge jump by 11,000 percent. Nationalisation also resulted in a significant growth in the geographical coverage of banks; the number of bank branches rose from 8,200 to over 62,000, most of which were opened in the unbanked, rural areas. The nationalization drive not only helped to increase household savings, but it also provided considerable investments in the informal sector, in small and medium-sized enterprises, and in agriculture, and contributed significantly to regional development and to the expansion of India’s industrial and agricultural base. Jayaprakash Narayan, who became famous for leading the opposition to Gandhi in the 1970s, was solid in his praise for her bank nationalisations.
Having been re-elected in 1971 on a nationalisation platform, Gandhi proceeded to nationalise the coal, steel, copper, refining, cotton textiles, and insurance industries. Most of these nationalisations were made to protect employment and the interest of the organised labour. The remaining private sector industries were placed under strict regulatory control.
During the Indo-Pakistani War of 1971, foreign-owned private oil companies had refused to supply fuel to the Indian Navy and Indian Air Force. In response, Gandhi nationalised oil companies in 1973. After nationalisation the oil majors such as the Indian Oil Corporation (IOC), the Hindustan Petroleum Corporation (HPCL) and the Bharat Petroleum Corporation (BPCL) had to keep a minimum stock level of oil, to be supplied to the military when needed.
Administration.
In 1966, Gandhi accepted the demands of the Akali's to reorganize Punjab on linguistic lines. The Hindi-speaking southern half of Punjab became a separate state, Haryana, while the Pahari speaking hilly areas in the north east were joined to Himachal Pradesh. In doing so, she had hoped to ward off the growing political conflict between Hindu and Sikh groups in the region. However, a contentious issue that was considered unresolved by the Akali's was the status of Chandigarh, a prosperous city on the Punjab-Haryana border, which Gandhi declared a union territory to be shared as a capital by both the states.
Victory over Pakistan in 1971 consolidated Indian power in Kashmir. Gandhi indicated that she would make no major concessions on Kashmir. The most prominent of the Kashmiri separatists, Sheikh Abdullah, had to recognize India's control over Kashmir in light of the new order in South Asia. The situation was normalized in the years following the war after Abdullah agreed to an accord with Gandhi, by giving up the demand for a plebiscite in return for a special autonomous status for Kashmir. In 1975, Gandhi declared the state of Jammu and Kashmir as a constituent unit of India. The Kashmir conflict remained largely peaceful if frozen under Gandhi's premiership.
In 1972, Gandhi granted statehood to Meghalaya, Manipur and Tripura, while the North-East Frontier Agency was declared a union territory and renamed Arunachal Pradesh. The transition to statehood for these territories was successfully overseen by her administration. This was followed by the annexation of Sikkim in 1975.
Social reform.
The principle of equal pay for equal work for both men and women was enshrined in the Indian Constitution under the Gandhi administration.
Gandhi questioned the continued existence of a privy purse for Indian monarchs. She argued the case for abolition based on equal rights for all citizens and the need to reduce the government's revenue deficit. The nobility responded by rallying around the Jana Sangh and other right-wing parties that stood in opposition to Gandhi's attempts to abolish royal privileges. The motion to abolish privy purses, and the official recognition of the titles, was originally brought before the Parliament in 1970. It was passed in the Lok Sabha but felt short of the two-thirds majority in the Rajya Sabha by a single vote. Gandhi responded by having a Presidential proclamation issued; de-recognizing the princes; with this withdrawal of recognition, their claims to privy purses were also legally lost. However, the proclamation was struck down by the Supreme Court of India. In 1971, Gandhi again motioned to abolish the privy purse. This time, it was successfully passed as the 26th Amendment to the Constitution of India. Many royals tried to protest the abolition of the privy purse, primarily through campaigns to contest seats in elections. They, however, received a final setback when many of them were defeated by huge margins.
Gandhi claimed that only "clear vision, iron will and the strictest discipline" can remove poverty. She justified the imposition of the state of emergency in 1975 in the name of the socialist mission of the Congress. Armed with the power to rule by decree and without constitutional constraints, Gandhi embarked on a massive redistribution program. The provisions included rapid enforcement of land ceilings, housing for landless labourers, the abolition of bonded labour and a moratorium on the debts of the poor. North India was at the centre of the reforms; millions of acres of land were acquired and redistributed. The government was also successful in procuring houses for landless labourers; according to Frankel, three-fourths of the targeted four million houses was achieved in 1975 alone. Nevertheless, others have disputed the success of the program and criticized Gandhi for not doing enough to reform land ownership. The political economist, Jyotindra Das Gupta, cryptically questioned "...whether or not the real supporters of land-holders were in jail or in power?" Critics also accused Gandhi of choosing to "talk left and act right", referring to her concurrent pro-business decisions and endeavours. Rosser wrote that "some have even seen the declaration of emergency rule in 1975 as a move to suppress dissent against Gandhi's policy shift to the right." Regardless of the controversy over the nature of the reforms, the long-term effects of the social changes gave rise to prominence of middle-ranking farmers from intermediate and lower castes in North India. The rise of these newly empowered social classes challenged the political establishment of the Hindi Belt in the years to come.
Language policy.
Under the Indian Constitution of 1950, Hindi was to have become the official national language by 1965. This was not acceptable to many non-Hindi speaking states, who wanted the continued use of English in government. In 1967, Gandhi made a constitutional amendment that guaranteed the de facto use of both Hindi and English as official languages. This established the official government policy of bilingualism in India and satisfied the non-Hindi speaking Indian states. Gandhi thus put herself forward as a leader with a pan-Indian vision. Nevertheless, critics alleged that her stance was actually meant to weaken the position of rival Congress leaders from the northern states such as Uttar Pradesh, where there had been strong, sometimes violent, pro-Hindi agitations. Gandhi came out of the language conflicts with the strong support of the south Indian populace.
National security.
In the late 1960s and 1970s, Gandhi had the Indian army crush militant Communist uprisings in the Indian state of West Bengal. The communist insurgency in India was completely suppressed during the state of emergency.
Gandhi considered the north-eastern regions important, because of its strategic situation. In 1966, the Mizo uprising took place against the government of India and overran almost the whole of the Mizoram region. Gandhi ordered the Indian army to launch massive retaliatory strikes in response. The rebellion was suppressed with the Indian Air Force even carrying out airstrikes in Aizawl; this remains the only instance of India carrying out an airstrike in its own civilian territory. The defeat of Pakistan in 1971 and the secession of East Pakistan as pro-India Bangladesh led to the collapse of the Mizo separatist movement. In 1972, after the less extremist Mizo leaders came to the negotiating table, Gandhi upgraded Mizoram to the status of a union territory. A small-scale insurgency by some militants continued into the late 1970s but was successfully dealt with by the government. The Mizo conflict was definitively resolved during the administration of Gandhi's son Rajiv. Today, Mizoram is considered as one of the most peaceful states in the north-east.
Responding to the insurgency in Nagaland, Gandhi "unleashed a powerful military offensive" in the 1970s. Finally, a massive crackdown on the insurgents took place during the state of emergency ordered by Gandhi. The insurgents soon agreed to surrender and signed the Shillong Accord in 1975. While the agreement was considered a victory for the Indian government and ended large-scale conflicts, there has since been spurts of violence by rebel holdouts and ethnic conflict amongst the tribes.
Nuclear Program of India.
Gandhi contributed and further carried out the vision of Jawarharalal Nehru, former Premier of India to develop the program. Gandhi authorised the development of nuclear weapons in 1967, in response to the "Test No. 6" by People's Republic of China. Gandhi saw this test as Chinese nuclear intimidation, therefore, Gandhi promoted the views of Nehru to establish India's stability and security interests as independent from those of the nuclear superpowers.
The program became fully mature in 1974, when Dr. Raja Ramanna reported to Gandhi that India had the ability to test its first nuclear weapon. Gandhi gave verbal authorisation of this test, and preparations were made in a long-constructed army base, the Indian Army Pokhran Test Range. In 1974, India successfully conducted an underground nuclear test, unofficially code named as ""Smiling Buddha"", near the desert village of Pokhran in Rajasthan. As the world was quiet by this test, a vehement protest came forward from Pakistan. Great ire was raised in Pakistan and its Prime Minister, Zulfikar Ali Bhutto, described this test as ""Indian hegemony"" to intimidate Pakistan. In response to this Bhutto launched a massive campaign all over the Pakistan to make Pakistan a nuclear power as well to defend itself against Indian aggression. In these campaigns Bhutto asked the nation to get united and great slogans were raised such as "hum ghaas kha lay gay magar nuclear power ban k rhe gay (we would eat grass but would surely achieve nuclear power)". Bhutto was right in his fear of Indian policy towards Pakistan as he has seen the Partition of Pakistan with his own eyes in the hands of Indhira. Gandhi directed a letter to Bhutto and, later to the world, describing the test for peaceful purposes and India's commitment to develop its programme for industrial and scientific use.
Family and personal life.
A member of the Nehru-Gandhi family, she was married to Feroze Gandhi at the age of 25, in 1942. Their marriage lasted for 18 years, until Feroze died after a heart attack in 1960. They had two sons - Rajiv (b. 1944) and Sanjay (b. 1946). Her younger son Sanjay had initially been her chosen heir; but after his death in a flying accident in June 1980, Gandhi persuaded her reluctant elder son Rajiv to quit his job as a pilot and enter politics in February 1981. Rajiv took office as prime minister following his mother's assassination in 1984; he served until December 1989. Rajiv Gandhi himself was assassinated by a suicide bomber working on behalf of Tamil tigers on May 21, 1991.
Gandhi's yoga guru, Dhirendra Brahmachari, helped her in making certain decisions and also executed certain top level political tasks on her behalf, especially from 1975 to 1977 when Gandhi declared a state of emergency and suspended civil liberties."
Awards.
After leading India to victory against Pakistan in the Bangladesh liberation war in 1971, President V. V. Giri awarded Mrs. Gandhi India's highest civilian honour, the Bharat Ratna., In 2011, the Bangladesh Freedom Honour (Bangladesh Swadhinata Sammanona ), Bangladesh's highest civilian award, was posthumously conferred on Indira Gandhi for her outstanding contributions to Bangladesh's Liberation War.
Legacy.
Indira Gandhi is associated with fostering a culture of nepotism in Indian politics and in India's institutions. She is also almost singularly associated with the period of Emergency rule and the dark period in Indian Democracy that it entailed, the period of conflict with Khalistan militants in the western state of Punjab, and being the face of a progressive Indian electorate owing to her being the first woman elected to hold the office of the Prime Minister of India.
The Indira Awaas Yojana, a central government low-cost housing programme for the rural poor, is named after her. The international airport at New Delhi is named Indira Gandhi International Airport in her honour. The Indira Gandhi National Open University, the largest university in the world, is also named after her. Indian National Congress established the annual Indira Gandhi Award for National Integration in 1985, given in her memory on her death anniversary. The Indira Gandhi Memorial Trust also constituted the annual Indira Gandhi Prize.

</doc>
<doc id="15180" url="https://en.wikipedia.org/wiki?curid=15180" title="Intergovernmentalism">
Intergovernmentalism

Intergovernmentalism is a term in political science with two meanings. The first refers to a theory of regional integration originally proposed by Stanley Hoffmann; the second treats states and the national government as the primary factors for integration.
Definition.
Intergovernmentalism treats states, and national governments in particular, as the primary actors in the integration process. Intergovernmentalist approaches claim to be able to explain both periods of radical change in the European Union because of converging governmental preferences and periods of inertia because of diverging national interests. Intergovernmentalism is distinguishable from realism and neorealism because of its recognition of the significance of institutionalisation in international politics and the impact of domestic politics upon governmental preferences.
Regional integration.
European Integration.
The best-known example of regional integration is the European Union (EU), an economic and political intergovernmental organisation of 28 member states, all in Europe. The EU operates through a system of supranational independent institutions and intergovernmental negotiated decisions by the member states. Institutions of the EU include the European Commission, the Council of the European Union, the European Council, the Court of Justice of the European Union, the European Central Bank, the Court of Auditors, and the European Parliament. The European Parliament is elected every five years by EU citizens. The EU's "de facto" capital is Brussels.
The EU has developed a single market through a standardised system of laws that apply in all member states. Within the Schengen Area (which includes 22 EU and 4 non-EU European states) passport controls have been abolished. EU policies favour the free movement of people, goods, services, and capital within its boundaries,</ref> enact legislation in justice and home affairs, and maintain common policies on trade, agriculture, fisheries and regional development. 
A monetary union, the eurozone, was established in 1999 and is composed of 17 member states. Through the Common Foreign and Security Policy the EU has developed a role in external relations and defence. Permanent diplomatic missions have been established around the world. The EU is represented at the United Nations, the World Trade Organisation, the G8 and the G-20.
Intergovernmentalism represents a way for limiting the conferral of powers upon supranational institutions, halting the emergence of common policies. in the current institutional system of the EU, the European Council and the Council play the role of the institutions which have the last word about decisions and policies of the EU, institutionalizing a de facto intergovernmental control over the EU as a whole, with the possibility to give more power to a small group of states. This extreme consequence can create the condition of supremacy of someone over someone else violating the principle of a "Union of Equals".
African Integration.
The African Union (AU, or, in its other official languages, UA) is a continental intergovernmental union, similar but less integrated to the EU, consisting of 54 African states. The only all-African state not in the AU is Morocco. The AU was established on 26 May 2001 in Addis Ababa, Ethiopia, and launched on 9 July 2002 in South Africa to replace the Organisation of African Unity (OAU). The most important decisions of the AU are made by the Assembly of the African Union, a semi-annual meeting of the heads of state and government of its member states. The AU's secretariat, the African Union Commission, is based in Addis Ababa, Ethiopia.
Overview.
The objectives of the AU are:
The African Union is made up of both political and administrative bodies. The highest decision-making organ is the Assembly of the African Union, made up of all the heads of state or government of member states of the AU. The Assembly is chaired by Hailemariam Desalegn, Prime Minister of Ethiopia, elected at the 20th ordinary meeting of the Assembly in January 2013. The AU also has a representative body, the Pan African Parliament, which consists of 265 members elected by the national parliaments of the AU member states. Its president is Bethel Nnaemeka Amadi.
Other political institutions of the AU include:

</doc>
<doc id="15181" url="https://en.wikipedia.org/wiki?curid=15181" title="Individualism">
Individualism

Individualism is the moral stance, political philosophy, ideology, or social outlook that emphasizes the moral worth of the individual. Individualists promote the exercise of one's goals and desires and so value independence and self-reliance and advocate that interests of the individual should achieve precedence over the state or a social group, while opposing external interference upon one's own interests by society or institutions such as the government. Individualism is often contrasted with totalitarianism or collectivism.
Individualism makes the individual its focus and so starts "with the fundamental premise that the human individual is of primary importance in the struggle for liberation." Classical Liberalism, existentialism, and anarchism are examples of movements that take the human individual as a central unit of analysis. Individualism thus involves "the right of the individual to freedom and self-realization".
It has also been used as a term denoting "The quality of being an individual; individuality" related to possessing "An individual characteristic; a quirk." Individualism is thus also associated with artistic and bohemian interests and lifestyles where there is a tendency towards self-creation and experimentation as opposed to tradition or popular mass opinions and behaviors as so also with humanist philosophical positions and ethics.
Etymology.
In the English language, the word "individualism" was first introduced, as a pejorative, by the Owenites in the late 1830s, although it is unclear if they were influenced by Saint-Simonianism or came up with it independently. A more positive use of the term in Britain came to be used with the writings of James Elishama Smith, who was a millenarian and a Christian Israelite. Although an early Owenite socialist, he eventually rejected its collective idea of property, and found in individualism a "universalism" that allowed for the development of the "original genius." Without individualism, Smith argued, individuals cannot amass property to increase one's happiness. William Maccall, another Unitarian preacher, and probably an acquaintance of Smith, came somewhat later, although influenced by John Stuart Mill, Thomas Carlyle, and German Romanticism, to the same positive conclusions, in his 1847 work "Elements of Individualism".
The individual.
An individual is a person or any specific object in a collection. In the 15th century and earlier, and also today within the fields of statistics and metaphysics, individual means "indivisible", typically describing any numerically singular thing, but sometimes meaning "a person." (q.v. "The problem of proper names"). From the 17th century on, individual indicates separateness, as in individualism. Individuality is the state or quality of being an individual; a person separate from other persons and possessing his or her own needs, goals, and desires.
Individualism and society.
Individualism holds that a person taking part in society attempts to learn and discover what his or her own interests are on a personal basis, without a presumed following of the interests of a societal structure (an individualist need not be an egoist). The individualist does not follow one particular philosophy, rather creates an amalgamation of elements of many, based on personal interests in particular aspects that she/he finds of use. On a societal level, the individualist participates on a personally structured political and moral ground. Independent thinking and opinion is a common trait of an individualist. Jean-Jacques Rousseau, claims that his concept of "general will" in the "social contract" is not the simple collection of individual wills and that it furthers the interests of the individual (the constraint of law itself would be beneficial for the individual, as the lack of respect for the law necessarily entails, in Rousseau's eyes, a form of ignorance and submission to one's passions instead of the preferred autonomy of reason).
Societies and groups can differ in the extent to which they are based upon predominantly "self-regarding" (individualistic, and arguably self-interested) rather than "other-regarding" (group-oriented, and group, or society-minded) behaviour. Ruth Benedict made a distinction, relevant in this context, between "guilt" societies (e.g., medieval Europe) with an "internal reference standard", and "shame" societies (e.g., Japan, "bringing shame upon one's ancestors") with an "external reference standard", where people look to their peers for feedback on whether an action is "acceptable" or not (also known as "group-think").
Individualism is often contrasted either with totalitarianism or with collectivism, but in fact there is a spectrum of behaviors at the societal level ranging from highly individualistic societies through mixed societies to collectivist.
Individuation theories.
The principle of individuation
, or "", describes the manner in which a thing is identified as distinguished from other things. For Carl Jung, individuation is a process of transformation, whereby the personal and collective unconscious is brought into consciousness (by means of dreams, active imagination or free association to take some examples) to be assimilated into the whole personality. It is a completely natural process necessary for the integration of the psyche to take place. Jung considered individuation to be the central process of human development. In "L'individuation psychique et collective", Gilbert Simondon developed a theory of individual and collective individuation in which the individual subject is considered as an effect of individuation rather than a cause. Thus, the individual atom is replaced by a never-ending ontological process of individuation. Individuation is an always incomplete process, always leaving a "pre-individual" left-over, itself making possible future individuations. The philosophy of Bernard Stiegler draws upon and modifies the work of Gilbert Simondon on individuation and also upon similar ideas in Friedrich Nietzsche and Sigmund Freud. For Stiegler "the "I", as a psychic individual, can only be thought in relationship to "we", which is a collective individual. The "I" is constituted in adopting a collective tradition, which it inherits and in which a plurality of "I" 's acknowledge each other's existence."
Methodological individualism.
Methodological individualism is the view that phenomena can only be understood by examining how they result from the motivations and actions of individual agents. In economics, people's behavior is explained in terms of rational choices, as constrained by prices and incomes. The economist accepts individuals' preferences as givens. Becker and Stigler provide a forceful statement of this view:
Political individualism.
Individualists are chiefly concerned with protecting individual autonomy against obligations imposed by social institutions (such as the state or religious morality). For L. Susan Brown "Liberalism and anarchism are two political philosophies that are fundamentally concerned with individual freedom yet differ from one another in very distinct ways. Anarchism shares with liberalism a radical commitment to individual freedom while rejecting liberalism's competitive property relations."
Civil libertarianism is a strain of political thought that supports civil liberties, or which emphasizes the supremacy of individual rights and personal freedoms over and against any kind of authority (such as a state, a corporation, social norms imposed through peer pressure, etc.). Civil libertarianism is not a complete ideology; rather, it is a collection of views on the specific issues of civil liberties and civil rights. Because of this, a civil libertarian outlook is compatible with many other political philosophies, and civil libertarianism is found on both the right and left in modern politics. For scholar Ellen Meiksins Wood "there are doctrines of individualism that are opposed to Lockean individualism(...)and non-lockean individualism may encompass socialism".
Liberalism.
Liberalism (from the Latin "liberalis", "of freedom; worthy of a free man, gentlemanlike, courteous, generous") is the belief in the importance of individual freedom. This belief is widely accepted in the United States, Europe, Australia and other Western nations, and was recognized as an important value by many Western philosophers throughout history, in particular since the Enlightenment. It is often rejected by collectivist, Islamic, or confucian societies in Asia or the Middle East (though Taoists were and are known to be individualists). The Roman Emperor Marcus Aurelius wrote praising "the idea of a polity administered with regard to equal rights and equal freedom of speech, and the idea of a kingly government which respects most of all the freedom of the governed".
Modern liberalism has its roots in the Age of Enlightenment and rejects many foundational assumptions that dominated most earlier theories of government, such as the Divine Right of Kings, hereditary status, and established religion. John Locke is often credited with the philosophical foundations of classical liberalism. He wrote "no one ought to harm another in his life, health, liberty, or possessions."
In the 17th century, liberal ideas began to influence governments in Europe, in nations such as The Netherlands, Switzerland, England and Poland, but they were strongly opposed, often by armed might, by those who favored absolute monarchy and established religion. In the 18th century, in America, the first modern liberal state was founded, without a monarch or a hereditary aristocracy. The American "Declaration of Independence" includes the words (which echo Locke) "all men are created equal; that they are endowed by their Creator with certain unalienable rights; that among these are life, liberty, and the pursuit of happiness; that to insure these rights, governments are instituted among men, deriving their just powers from the consent of the governed."
Liberalism comes in many forms. According to John N. Gray, the essence of liberalism is toleration of different beliefs and of different ideas as to what constitutes a good life.
Anarchism.
Anarchism is a set of political philosophies that hold the state to be undesirable, unnecessary, or harmful, and often advocate stateless societies. While anti-statism is central, some argue that anarchism entails opposing authority or hierarchical organisation in the conduct of human relations, including, but not limited to, the state system.
For influential Italian anarchist Errico Malatesta "All anarchists, whatever tendency they belong to, are individualists in some way or other. But the opposite is not true; not by any means. The individualists are thus divided into two distinct categories: one which claims the right to full development for all human individuality, their own and that of others; the other which only thinks about its own individuality and has absolutely no hesitation in sacrificing the individuality of others. The Tsar of all the Russias belongs to the latter category of individualists. We belong to the former."
Individualist anarchism.
Individualist anarchism refers to several traditions of thought within the anarchist movement that emphasize the individual and their will over any kinds of external determinants such as groups, society, traditions, and ideological systems. Individualist anarchism is not a single philosophy but refers to a group of individualistic philosophies that sometimes are in conflict.
In 1793, William Godwin, who has often been cited as the first anarchist, wrote "Political Justice", which some consider to be the first expression of anarchism.
Godwin, a philosophical anarchist, from a rationalist and utilitarian basis opposed revolutionary action and saw a minimal state as a present "necessary evil" that would become increasingly irrelevant and powerless by the gradual spread of knowledge.
Godwin advocated individualism, proposing that all cooperation in labour be eliminated on the premise that this would be most conducive with the general good.
An influential form of individualist anarchism, called "egoism," or egoist anarchism, was expounded by one of the earliest and best-known proponents of individualist anarchism, the German Max Stirner. Stirner's "The Ego and Its Own", published in 1844, is a founding text of the philosophy. According to Stirner, the only limitation on the rights of the individual is their power to obtain what they desire, without regard for God, state, or morality. To Stirner, rights were "spooks" in the mind, and he held that society does not exist but "the individuals are its reality". Stirner advocated self-assertion and foresaw unions of egoists, non-systematic associations continually renewed by all parties' support through an act of will, which Stirner proposed as a form of organization in place of the state. Egoist anarchists claim that egoism will foster genuine and spontaneous union between individuals. "Egoism" has inspired many interpretations of Stirner's philosophy. It was re-discovered and promoted by German philosophical anarchist and LGBT activist John Henry Mackay.
Josiah Warren is widely regarded as the first American anarchist, and the four-page weekly paper he edited during 1833, "The Peaceful Revolutionist", was the first anarchist periodical published. For American anarchist historian Eunice Minette Schuster "It is apparent...that Proudhonian Anarchism was to be found in the United States at least as early as 1848 and that it was not conscious of its affinity to the Individualist Anarchism of Josiah Warren and Stephen Pearl Andrews...William B. Greene presented this Proudhonian Mutualism in its purest and most systematic form.". Henry David Thoreau (1817–1862) was an important early influence in individualist anarchist thought in the United States and Europe. Thoreau was an American author, poet, naturalist, tax resister, development critic, surveyor, historian, philosopher, and leading transcendentalist. He is best known for his books "Walden", a reflection upon simple living in natural surroundings, and his essay, "Civil Disobedience", an argument for individual resistance to civil government in moral opposition to an unjust state. Later Benjamin Tucker fused Stirner's egoism with the economics of Warren and Proudhon in his eclectic influential publication "Liberty".
From these early influences individualist anarchism in different countries attracted a small but diverse following of bohemian artists and intellectuals, free love and birth control advocates (see Anarchism and issues related to love and sex), individualist naturists nudists (see anarcho-naturism), freethought and anti-clerical activists as well as young anarchist outlaws in what came to be known as illegalism and individual reclamation (see European individualist anarchism and individualist anarchism in France). These authors and activists included Oscar Wilde, Emile Armand, Han Ryner, Henri Zisly, Renzo Novatore, Miguel Gimenez Igualada, Adolf Brand and Lev Chernyi among others. In his important essay "The Soul of Man under Socialism" from 1891 Oscar Wilde defended socialism as the way to guarantee individualism and so he saw that "With the abolition of private property, then, we shall have true, beautiful, healthy Individualism. Nobody will waste his life in accumulating things, and the symbols for things. One will live. To live is the rarest thing in the world. Most people exist, that is all." For anarchist historian George Woodcock "Wilde's aim in "The Soul of Man under Socialism" is to seek the society most favorable to the artist ... for Wilde art is the supreme end, containing within itself enlightenment and regeneration, to which all else in society must be subordinated ... Wilde represents the anarchist as aesthete." Woodcock finds that "The most ambitious contribution to literary anarchism during the 1890s was undoubtedly Oscar Wilde "The Soul of Man under Socialism"" and finds that it is influenced mainly by the thought of William Godwin.
Philosophical individualism.
Ethical egoism.
Ethical egoism (also called simply egoism) is the normative ethical position that moral agents ought to do what is in their own self-interest. It differs from psychological egoism, which claims that people "do" only act in their self-interest. Ethical egoism also differs from rational egoism, which holds merely that it is rational to act in one's self-interest. However, these doctrines may occasionally be combined with ethical egoism.
Ethical egoism contrasts with ethical altruism, which holds that moral agents have an obligation to help and serve others. Egoism and altruism both contrast with ethical utilitarianism, which holds that a moral agent should treat one's self (also known as the subject) with no higher regard than one has for others (as egoism does, by elevating self-interests and "the self" to a status not granted to others), but that one also should not (as altruism does) sacrifice one's own interests to help others' interests, so long as one's own interests (i.e. one's own desires or well-being) are substantially-equivalent to the others' interests and well-being. Egoism, utilitarianism, and altruism are all forms of consequentialism, but egoism and altruism contrast with utilitarianism, in that egoism and altruism are both agent-focused forms of consequentialism (i.e. subject-focused or subjective), but utilitarianism is called agent-neutral (i.e. objective and impartial) as it does not treat the subject's (i.e. the self's, i.e. the moral "agent's") own interests as being more or less important than if the same interests, desires, or well-being were anyone else's.
Ethical egoism does not, however, require moral agents to harm the interests and well-being of others when making moral deliberation; e.g. what is in an agent's self-interest may be incidentally detrimental, beneficial, or neutral in its effect on others. Individualism allows for others' interest and well-being to be disregarded or not, as long as what is chosen is efficacious in satisfying the self-interest of the agent. Nor does ethical egoism necessarily entail that, in pursuing self-interest, one ought always to do what one wants to do; e.g. in the long term, the fulfilment of short-term desires may prove detrimental to the self. Fleeting pleasance, then, takes a back seat to protracted eudaemonia. In the words of James Rachels, "Ethical egoism [...] endorses selfishness, but it doesn't endorse foolishness."
Ethical egoism is sometimes the philosophical basis for support of libertarianism or individualist anarchism as in Max Stirner, although these can also be based on altruistic motivations. These are political positions based partly on a belief that individuals should not coercively prevent others from exercising freedom of action.
Egoist anarchism.
Egoist anarchism is a school of anarchist thought that originated in the philosophy of Max Stirner, a nineteenth-century Hegelian philosopher whose "name appears with familiar regularity in historically orientated surveys of anarchist thought as one of the earliest and best-known exponents of individualist anarchism." According to Stirner, the only limitation on the rights of the individual is their power to obtain what they desire, without regard for God, state, or morality. Stirner advocated self-assertion and foresaw unions of egoists, non-systematic associations continually renewed by all parties' support through an act of will, which Stirner proposed as a form of organisation in place of the state. Egoist anarchists argue that egoism will foster genuine and spontaneous union between individuals. "Egoism" has inspired many interpretations of Stirner's philosophy but within anarchism it has also gone beyond Stirner. It was re-discovered and promoted by German philosophical anarchist and LGBT activist John Henry Mackay. John Beverley Robinson wrote an essay called "Egoism" in which he states that "Modern egoism, as propounded by Stirner and Nietzsche, and expounded by Ibsen, Shaw and others, is all these; but it is more. It is the realization by the individual that they are an individual; that, as far as they are concerned, they are the only individual." Nietzsche (see Anarchism and Friedrich Nietzsche) and Stirner were frequently compared by French "literary anarchists" and anarchist interpretations of Nietzschean ideas appear to have also been influential in the United States. Anarchists who adhered to egoism include Benjamin Tucker, Émile Armand, John Beverley Robinson, Adolf Brand, Steven T. Byington, Renzo Novatore, James L. Walker, Enrico Arrigoni, Biofilo Panclasta, Jun Tsuji, André Arru and contemporary ones such as Hakim Bey, Bob Black and Wolfi Landstreicher.
Existentialism.
Existentialism is a term applied to the work of a number of 19th- and 20th-century philosophers who, despite profound doctrinal differences, generally held that the focus of philosophical thought should be to deal with the conditions of existence of the individual person and his or her emotions, actions, responsibilities, and thoughts. The early 19th century philosopher Søren Kierkegaard, posthumously regarded as the father of existentialism, maintained that the individual solely has the responsibilities of giving one's own life meaning and living that life passionately and sincerely, in spite of many existential obstacles and distractions including despair, angst, absurdity, alienation, and boredom.
Subsequent existential philosophers retain the emphasis on the individual, but differ, in varying degrees, on how one achieves and what constitutes a fulfilling life, what obstacles must be overcome, and what external and internal factors are involved, including the potential consequences of the existence or non-existence of God. Many existentialists have also regarded traditional systematic or academic philosophy, in both style and content, as too abstract and remote from concrete human experience. Existentialism became fashionable in the post-World War years as a way to reassert the importance of human individuality and freedom.
Freethought.
Freethought holds that individuals should not accept ideas proposed as truth without recourse to knowledge and reason. Thus, freethinkers strive to build their opinions on the basis of facts, scientific inquiry, and logical principles, independent of any logical fallacies or intellectually limiting effects of authority, confirmation bias, cognitive bias, conventional wisdom, popular culture, prejudice, sectarianism, tradition, urban legend, and all other dogmas. Regarding religion, freethinkers hold that there is insufficient evidence to scientifically validate the existence of supernatural phenomena.
Humanism.
Humanism is a perspective common to a wide range of ethical stances that attaches importance to human dignity, concerns, and capabilities, particularly rationality. Although the word has many senses, its meaning comes into focus when contrasted to the supernatural or to appeals to authority. Since the 19th century, humanism has been associated with an anti-clericalism inherited from the 18th-century Enlightenment "philosophes". 21st century Humanism tends to strongly endorse human rights, including reproductive rights, gender equality, social justice, and the separation of church and state. The term covers organized non-theistic religions, secular humanism, and a humanistic life stance.
Hedonism.
Philosophical hedonism is a meta-ethical theory of value which argues that pleasure is the only intrinsic good and pain is the only intrinsic bad. The basic idea behind hedonistic thought is that pleasure (an umbrella term for all inherently likable emotions) is the only thing that is good in and of itself or by its very nature. The normative implications of this are evaluating character or behavior as morally good to the extent that one is concerned with pleasure/pain qua pleasure/pain or an action leads to a greater balance of pleasure over pain than any other would.
Libertinism.
A libertine is one devoid of most moral restraints, which are seen as unnecessary or undesirable, especially one who ignores or even spurns accepted morals and forms of behaviour sanctified by the larger society. Libertines place value on physical pleasures, meaning those experienced through the senses. As a philosophy, libertinism gained new-found adherents in the 17th, 18th, and 19th centuries, particularly in France and Great Britain. Notable among these were John Wilmot, 2nd Earl of Rochester, and the Marquis de Sade. During the Baroque era in France, there existed a freethinking circle of philosophers and intellectuals who were collectively known as "libertinage érudit" and which included Gabriel Naudé, Élie Diodati and François de La Mothe Le Vayer. The critic Vivian de Sola Pinto linked John Wilmot, 2nd Earl of Rochester's libertinism to Hobbesian materialism.
Objectivism.
Objectivism is a system of philosophy created by philosopher and novelist Ayn Rand (1905–1982) that holds: reality exists independent of consciousness; human beings gain knowledge rationally from perception through the process of concept formation and inductive and deductive logic; the moral purpose of one's life is the pursuit of one's own happiness or rational self-interest. Rand thinks the only social system consistent with this morality is full respect for individual rights, embodied in pure "laissez faire" capitalism; and the role of art in human life is to transform man's widest metaphysical ideas, by selective reproduction of reality, into a physical form—a work of art—that he can comprehend and to which he can respond emotionally. Objectivism celebrates man as his own hero, "with his own happiness as the moral purpose of his life, with productive achievement as his noblest activity, and reason as his only absolute."
Philosophical anarchism.
Philosophical anarchism is an anarchist school of thought which contends that the State lacks moral legitimacy and - in contrast to revolutionary anarchism - does not advocate violent revolution to eliminate it but advocate peaceful evolution to superate it. Though philosophical anarchism does not necessarily imply any action or desire for the elimination of the State, philosophical anarchists do not believe that they have an obligation or duty to obey the State, or conversely, that the State has a right to command.
Philosophical anarchism is a component especially of individualist anarchism. Philosophical anarchists of historical note include Mohandas Gandhi, William Godwin, Pierre-Joseph Proudhon, Max Stirner, Benjamin Tucker, and Henry David Thoreau. Contemporary philosophical anarchists include A. John Simmons and Robert Paul Wolff.
Subjectivism.
Subjectivism is a philosophical tenet that accords primacy to subjective experience as fundamental of all measure and law. In extreme forms like Solipsism, it may hold that the nature and existence of every object depends solely on someone's subjective awareness of it. For example, Wittgenstein wrote in Tractatus Logico-Philosophicus: "The subject doesn't belong to the world, but it is a limit of the world" (proposition 5.632). Metaphysical subjectivism is the theory that reality is what we perceive to be real, and that there is no underlying true reality that exists independently of perception. One can also hold that it is consciousness rather than perception that is reality (subjective idealism). In probability, a subjectivism stands for the belief that probabilities are simply degrees-of-belief by rational agents in a certain proposition, and which have no objective reality in and of themselves.
Ethical subjectivism stands in opposition to moral realism, which claims that moral propositions refer to objective facts, independent of human opinion; to error theory, which denies that any moral propositions are true in any sense; and to non-cognitivism, which denies that moral sentences express propositions at all. The most common forms of ethical subjectivism are also forms of moral relativism, with moral standards held to be relative to each culture or society (c.f. cultural relativism), or even to every individual. The latter view, as put forward by Protagoras, holds that there are as many distinct scales of good and evil as there are subjects in the world. Moral subjectivism is that species of moral relativism that relativizes moral value to the individual subject.
Horst Matthai Quelle was a Spanish language German anarchist philosopher influenced by Max Stirner. He argued that since the individual gives form to the world, he is those objects, the others and the whole universe. One of his main views was a "theory of infinite worlds" which for him was developed by pre-socratic philosophers.
Solipsism.
Solipsism is the philosophical idea that only one's own mind is sure to exist. The term comes from Latin "solus" (alone) and "ipse" (self). Solipsism as an epistemological position holds that knowledge of anything outside one's own mind is unsure. The external world and other minds cannot be known, and might not exist outside the mind. As a metaphysical position, solipsism goes further to the conclusion that the world and other minds do not exist. As such it is the only epistemological position that, by its own postulate, is both irrefutable and yet indefensible in the same manner. Although the number of individuals sincerely espousing solipsism has been small, it is not uncommon for one philosopher to accuse another's arguments of entailing solipsism as an unwanted consequence, in a kind of reductio ad absurdum. In the history of philosophy, solipsism has served as a skeptical hypothesis.
Economic individualism.
The doctrine of economic individualism holds that each individual should be allowed autonomy in making his or her own economic decisions as opposed to those decisions being made by the state, the community, the corporation etc. for him or her.
Liberalism.
Classical liberalism is a political ideology that developed in the 19th century in England, Western Europe, and the Americas. It followed earlier forms of liberalism in its commitment to personal freedom and popular government, but differed from earlier forms of liberalism in its commitment to free markets and classical economics. Notable classical liberals in the 19th century include Jean-Baptiste Say, Thomas Malthus, and David Ricardo. Classical liberalism was revived in the 20th century by Ludwig von Mises and Friedrich Hayek, and further developed by Milton Friedman, Robert Nozick, Loren Lomasky, and Jan Narveson. The phrase "classical liberalism" is also sometimes used to refer to all forms of liberalism before the 20th century.
Individualist anarchism and economics.
In regards to economic questions within individualist anarchism there are adherents to mutualism (Pierre Joseph Proudhon, Emile Armand, early Benjamin Tucker); natural rights positions (early Benjamin Tucker, Lysander Spooner, Josiah Warren); and egoistic disrespect for "ghosts" such as private property and markets (Max Stirner, John Henry Mackay, Lev Chernyi, later Benjamin Tucker, Renzo Novatore, illegalism). Contemporary individualist anarchist Kevin Carson characterizes American individualist anarchism saying that "Unlike the rest of the socialist movement, the individualist anarchists believed that the natural wage of labor in a free market was its product, and that economic exploitation could only take place when capitalists and landlords harnessed the power of the state in their interests. Thus, individualist anarchism was an alternative both to the increasing statism of the mainstream socialist movement, and to a classical liberal movement that was moving toward a mere apologetic for the power of big business." 
Mutualism.
Mutualism is an anarchist school of thought which can be traced to the writings of Pierre-Joseph Proudhon, who envisioned a society where each person might possess a means of production, either individually or collectively, with trade representing equivalent amounts of labor in the free market. Integral to the scheme was the establishment of a mutual-credit bank which would lend to producers at a minimal interest rate only high enough to cover the costs of administration. Mutualism is based on a labor theory of value which holds that when labor or its product is sold, in exchange, it ought to receive goods or services embodying "the amount of labor necessary to produce an article of exactly similar and equal utility". Receiving anything less would be considered exploitation, theft of labor, or usury.
Libertarian socialism.
Libertarian socialism (sometimes called social anarchism, left-libertarianism and socialist libertarianism) is a group of political philosophies within the socialist movement that reject the view of socialism as state ownership or command of the means of production within a more general criticism of the state form itself as well as of wage labour relationships within the workplace. Instead it emphasizes workers' self-management of the workplace and decentralized structures of political government asserting that a society based on freedom and equality can be achieved through abolishing authoritarian institutions that control certain means of production and subordinate the majority to an owning class or political and economic elite. Libertarian socialists generally place their hopes in decentralized means of direct democracy and federal or confederal associations such as libertarian municipalism, citizens' assemblies, trade unions, and workers' councils. All of this is generally done within a general call for libertarian and voluntary human relationships through the identification, criticism, and practical dismantling of illegitimate authority in all aspects of life.
Past and present political philosophies and movements commonly described as libertarian socialist include anarchism (especially anarchist communism, anarchist collectivism, anarcho-syndicalism, and mutualism) as well as autonomism, communalism, participism, revolutionary syndicalism, and libertarian Marxist philosophies such as council communism and Luxemburgism,; as well as some versions of "utopian socialism" and individualist anarchism.
Left-libertarianism.
Left-libertarianism (or left-wing libertarianism) names several related but distinct approaches to politics, society, culture, and political and social theory, which stress both individual freedom and social justice. Unlike right-libertarians, they believe that neither claiming nor mixing one's labor with natural resources is enough to generate full private property rights, and maintain that natural resources (land, oil, gold, trees) ought to be held in some egalitarian manner, either unowned or owned collectively. Those left-libertarians who support private property do so under the condition that recompense is offered to the local community.
Left-libertarianism can refer generally to three related and overlapping schools of thought: 
Right-libertarianism.
Right-libertarianism or right libertarianism is a phrase used by some to describe either non-collectivist forms of libertarianism or a variety of different libertarian views some label "right" of mainstream libertarianism including "libertarian conservatism".
Stanford Encyclopedia of Philosophy calls it "right libertarianism" but states: "Libertarianism is often thought of as 'right-wing' doctrine. This, however, is mistaken for at least two reasons. First, on social—rather than economic—issues, libertarianism tends to be 'left-wing'. It opposes laws that restrict consensual and private sexual relationships between adults (e.g., gay sex, non-marital sex, and deviant sex), laws that restrict drug use, laws that impose religious views or practices on individuals, and compulsory military service. Second, in addition to the better-known version of libertarianism—right-libertarianism—there is also a version known as 'left-libertarianism'. Both endorse full self-ownership, but they differ with respect to the powers agents have to appropriate unappropriated natural resources (land, air, water, etc.)."
As creative independent lifestyle.
The anarchist writer and bohemian Oscar Wilde wrote in his famous essay "The Soul of Man under Socialism" that "Art is individualism, and individualism is a disturbing and disintegrating force. There lies its immense value. For what it seeks is to disturb monotony of type, slavery of custom, tyranny of habit, and the reduction of man to the level of a machine." For anarchist historian George Woodcock "Wilde's aim in "The Soul of Man under Socialism" is to seek the society most favorable to the artist...for Wilde art is the supreme end, containing within itself enlightenment and regeneration, to which all else in society must be subordinated...Wilde represents the anarchist as aesthete." The word individualism in this way has been used to denote a personality with a strong tendency towards self-creation and experimentation as opposed to tradition or popular mass opinions and behaviors
Anarchist writer Murray Bookchin describes a lot of individualist anarchism as people who "expressed their opposition in uniquely personal forms, especially in fiery tracts, outrageous behavior, and aberrant lifestyles in the cultural ghettos of fin de sicle New York, Paris, and London. As a credo, individualist anarchism remained largely a bohemian lifestyle, most conspicuous in its demands for sexual freedom ('free love') and enamored of innovations in art, behavior, and clothing."
In relation to this view of individuality, French Individualist anarchist Emile Armand advocates egoistical denial of social conventions and dogmas to live in accord to one's own ways and desires in daily life since he emphasized anarchism as a way of life and practice. In this way he manifests "So the anarchist individualist tends to reproduce himself, to perpetuate his spirit in other individuals who will share his views and who will make it possible for a state of affairs to be established from which authoritarianism has been banished. It is this desire, this will, not only to live, but also to reproduce oneself, which we shall call "activity".
In the book "Imperfect garden : the legacy of humanism", humanist philosopher Tzvetan Todorov identifies individualism as an important current of socio-political thought within modernity and as examples of it he mentions Michel de Montaigne, François de La Rochefoucauld, Marquis de Sade, and Charles Baudelaire In La Rochefoucauld, he identifies a tendency similar to stoicism in which "the honest person works his being in the manner of an sculptor who searches the liberation of the forms which are inside a block of marble, to extract the truth of that matter." In Baudelaire he finds the dandy trait in which one searches to cultivate "the idea of beauty within oneself, of satisfying one´s passions of feeling and thinking."
The Russian-American poet Joseph Brodsky once manifested that "The surest defense against Evil is extreme individualism, originality of thinking, whimsicality, even—if you will—eccentricity. That is, something that can't be feigned, faked, imitated; something even a seasoned imposter couldn't be happy with." Ralph Waldo Emerson famously declared", "Whoso would be a man must be a nonconformist"—a point of view developed at length in both the life and work of (Henry David) Thoreau. Equally memorable and influential on Walt Whitman is Emerson's idea that "a foolish consistency is the hobgoblin of small minds, adored by little statesmen and philosophers and divines."...Emerson opposes on principle the reliance on social structures (civil, religious) precisely because through them the individual approaches the divine second hand, mediated by the once original experience of a genius from another age: "An institution," as he explains, "is the lengthened shadow of one man." To achieve this original relation one must "Insist on one's self; never imitate" for if the relationship is secondary the connection is lost."

</doc>
<doc id="15187" url="https://en.wikipedia.org/wiki?curid=15187" title="In vivo">
In vivo

Studies that are in vivo (Latin for "within the living"; often not italicized in English) are those in which the effects of various biological entities are tested on whole, living organisms usually animals including humans, and plants as opposed to a partial or dead organism, or those done "in vitro" ("within the glass"), i.e., in a laboratory environment using test tubes, petri dishes etc. Examples of investigations "in vivo" include: the pathogenesis of disease by comparing the effects of bacterial infection with the effects of purified bacterial toxins; the development of antibiotics, antiviral drugs, and new drugs generally; and new surgical procedures. Consequently, animal testing and clinical trials are major elements of "in vivo" research. "In vivo" testing is often employed over "in vitro" because it is better suited for observing the overall effects of an experiment on a living subject. In drug discovery, for example, verification of efficacy "in vivo" is crucial, because "in vitro" assays can sometimes yield misleading results with drug candidate molecules that are irrelevant "in vivo" (e.g., because such molecules cannot reach their site of "in vivo" action, for example as a result of rapid catabolism in the liver).
The English microbiologist Professor Harry Smith and his colleagues in the mid-1950s showed the importance of "in vivo" studies. They found that sterile filtrates of serum from animals infected with Bacillus anthracis were lethal for other animals, whereas extracts of culture fluid from the same organism grown "in vitro" were not. This discovery of anthrax toxin through the use of "in vivo" experiments had a major impact on studies of the pathogenesis of infectious disease.
The maxim "in vivo veritas" ("in a living thing is truth") is used to describe this type of testing and is a play on "in vino veritas", ("in wine is truth") a well-known proverb.
"In vivo" vs. "ex vivo" research.
In microbiology "in vivo" is often used to refer to experimentation done in live isolated cells rather than in a whole organism, for example, cultured cells derived from biopsies. In this situation, the more specific term is "ex vivo". Once cells are disrupted and individual parts are tested or analyzed, this is known as "in vitro".
Methods of use.
According to Christopher Lipinski and Andrew Hopkins, "Whether the aim is to discover drugs or to gain knowledge of biological systems, the nature and properties of a chemical tool cannot be considered independently of the system it is to be tested in. Compounds that bind to isolated recombinant proteins are one thing; chemical tools that can perturb cell function another; and pharmacological agents that can be tolerated by a live organism and perturb its systems are yet another. If it were simple to ascertain the properties required to develop a lead discovered "in vitro" to one that is active "in vivo", drug discovery would be as reliable as drug manufacturing."

</doc>
<doc id="15188" url="https://en.wikipedia.org/wiki?curid=15188" title="In vitro">
In vitro

In vitro studies are performed with microorganisms, cells or biological molecules outside their normal biological context. Colloquially called "test tube experiments", these studies in biology and its sub-disciplines have traditionally been done in test-tubes, flasks, petri dishes "etc" and since the onset of molecular biology involve techniques such as the so-called omics. Studies that are conducted using components of an organism that have been isolated from their usual biological surroundings permit a more detailed or more convenient analysis than can be done with whole organisms. In contrast, "in vivo" studies are those conducted in animals including humans, and whole plants.
Definition.
In vitro (; often not italicized in English) studies are conducted using components of an organism that have been isolated from their usual biological surroundings, such as microorganisms, cells or biological molecules. For example, microrganisms or cells can be studied in artificial culture medium, proteins can be examined in solutions. Colloquially called "test tube experiments", these studies in biology, medicine and its sub-disciplines are traditionally done in test-tubes, flasks, petri dishes "etc". They now involve the full range of techniques used in molecular biology such as the so-called omics. 
In contrast, studies conducted in living beings (microorganisms, animals, humans, or whole plants) are called "in vivo" .
Examples.
Examples of "in vitro" studies include: the isolation, growth and identification of cells derived from multicellular organisms in (cell culture or tissue culture); subcellular components (e.g. mitochondria or ribosomes); cellular or subcellular extracts (e.g. wheat germ or reticulocyte extracts); purified molecules like proteins, DNA, or RNA); and the commercial production of antibiotics and other pharmaceutical products. Viruses, which only replicate in living cells, are studied in the laboratory in cell or tissue culture, and many animal virologists refer to such work as being "in vitro" to distinguish it from "in vivo" work on whole animals.
Advantages.
In vitro studies permit a species-specific, simpler, more convenient and more detailed analysis than can be done with the whole organism. Just as studies in whole animals more and more replace human trials, so are in vitro studies replacing studies in whole animals.
Simplicity.
Living organisms are extremely complex functional systems that are made up of, at a minimum, many tens of thousands of genes, protein molecules, RNA molecules, small organic compounds, inorganic ions and complexes in an environment that is spatially organized by membranes and, in the case of multicellular organisms, organ systems. These myriad components interact with each other and with their environment in a way that processes food, removes waste, moves components to the correct location, and is responsive to signalling molecules, other organisms, light, sound, heat, taste, touch, and balance.
This complexity makes it difficult to identify the interactions between its individual components and to explore their basic biological functions. "In vitro" work simplifies the system under study, so the investigator can focus on a small number of components.
For example, the identity of proteins of the immune system (e.g. antibodies), and the mechanism by which they recognize and bind to foreign antigens would remain very obscure if not for the extensive use of "in vitro" work to isolate the proteins, identify the cells and genes that produce them, study the physical properties of their interaction with antigens, and identify how those interactions lead to cellular signals that activate other components of the immune system.
Species specificity.
Another advantage of "in vitro" methods is that human cells can be studied without "extrapolation" from an experimental animal's cellular response.
Convenience, automation.
"In vitro" methods can be miniaturized and automated, yielding high-throughput screening methods for testing molecules in pharmacology or toxicology 
Disadvantages.
The primary disadvantage of "in vitro" experimental studies is that it is challenging to extrapolate from the results of "in vitro" work back to the biology of the intact organism. Investigators doing "in vitro" work must be careful to avoid over-interpretation of their results, which can lead to erroneous conclusions about organismal and systems biology.
For example, scientists developing a new viral drug to treat an infection with a pathogenic virus (e.g. HIV-1) may find that a candidate drug functions to prevent viral replication in an "in vitro" setting (typically cell culture). However, before this drug is used in the clinic, it must progress through a series of "in vivo" trials to determine if it is safe and effective in intact organisms (typically small animals, primates and humans in succession). Typically, most candidate drugs that are effective "in vitro" prove to be ineffective "in vivo" because of issues associated with delivery of the drug to the affected tissues, toxicity towards essential parts of the organism that were not represented in the initial "in vitro" studies, or other issues.
"In vitro" to "in vivo" extrapolation (IVIVE).
Results obtained from "in vitro" experiments cannot usually be transposed as is to predict the reaction of an entire organism "in vivo". Building a consistent and reliable extrapolation procedure from "in vitro" results to "in vitro" is therefore extremely important. Two solutions have been commonly accepted:
The two approaches are not incompatible: better "in vitro" systems will provide better data to mathematical models. On the other hand, increasingly sophisticated "in vitro" experiments collect increasingly numerous, complex, and challenging data to integrate: Mathematical models, such as systems biology models are much needed here.
Extrapolating in pharmacology.
In pharmacology, IVIVE can be used to approximate pharmacokinetics (PK) or pharmacodynamics (PD).
Since the timing and intensity of effects on a given target depend on the concentration time course of candidate drug (parent molecule or metabolites) at that target site, in vivo tissue and organ sensitivities can be completely different or even inverse of those observed on cells cultured and exposed "in vitro".
That indicates that extrapolating effects observed in vitro needs a quantitative model of in vivo PK. It is generally accepted that physiologically based PK (PBPK) models are central to the extrapolations.
In the case of early effects or those without inter-cellular communications, it is assumed that the same cellular exposure concentration cause the same effects, both qualitatively and quantitatively, "in vitro" and "in vivo". In these conditions, it is enough to (1) develop a simple PD model of the dose–response relationship observed "in vitro" and (2) transpose it without changes to predict "in vivo" effects.

</doc>
<doc id="15189" url="https://en.wikipedia.org/wiki?curid=15189" title="IEEE 754-1985">
IEEE 754-1985

IEEE 754-1985 was an industry standard for representing floating-point numbers in computers, officially adopted in 1985 and superseded in 2008 by the current revision. During its 23 years, it was the most widely used format for floating-point computation. It was implemented in software, in the form of floating-point libraries, and in hardware, in the instructions of many CPUs and FPUs. The first integrated circuit to implement the draft of what was to become IEEE 754-1985 was the Intel 8087.
IEEE 754-1985 represents numbers in binary, providing definitions for four levels of precision, of which the two most commonly used are:
The standard also defines representations for positive and negative infinity, a "negative zero", five exceptions to handle invalid results like division by zero, special values called NaNs for representing those exceptions, denormal numbers to represent numbers smaller than shown above, and four rounding modes.
Representation of numbers.
Floating-point numbers in IEEE 754 format consist of three fields: a sign bit, a biased exponent, and a fraction. The following example illustrates the meaning of each.
The decimal number 0.1562510 represented in binary is 0.001012 (that is, 1/8 + 1/32). (Subscripts indicate the number base.) Analogous to scientific notation, where numbers are written to have a single non-zero digit to the left of the decimal point, we rewrite this number so it has a single 1 bit to the left of the "binary point". We simply multiply by the appropriate power of 2 to compensate for shifting the bits left by three positions:
Now we can read off the fraction and the exponent: the fraction is .012 and the exponent is −3.
As illustrated in the pictures, the three fields in the IEEE 754 representation of this number are:
IEEE 754 adds a bias to the exponent so that numbers can in many cases be compared conveniently by the same hardware that compares signed 2's-complement integers. Using a biased exponent, the lesser of two positive floating-point numbers will come out "less than" the greater following the same ordering as for sign and magnitude integers. If two floating-point numbers have different signs, the sign-and-magnitude comparison also works with biased exponents. However, if both biased-exponent floating-point numbers are negative, then the ordering must be reversed. If the exponent were represented as, say, a 2's-complement number, comparison to see which of two numbers is greater would not be as convenient.
The leading 1 bit is omitted since all numbers except zero start with a leading 1; the leading 1 is implicit and doesn't actually need to be stored which gives an extra bit of precision for "free."
Zero.
The number zero is represented specially:
Denormalized numbers.
The number representations described above are called "normalized," meaning that the implicit leading binary digit is a 1. To reduce the loss of precision when an underflow occurs, IEEE 754 includes the ability to represent fractions smaller than are possible in the normalized representation, by making the implicit leading digit a 0. Such numbers are called denormal. They don't include as many significant digits as a normalized number, but they enable a gradual loss of precision when the result of an arithmetic operation is not exactly zero but is too close to zero to be represented by a normalized number.
A denormal number is represented with a biased exponent of all 0 bits, which represents an exponent of −126 in single precision (not −127), or −1022 in double precision (not −1023). In contrast, the smallest biased exponent representing a normal number is 1 (see examples below).
Representation of non-numbers.
The biased-exponent field is filled with all 1 bits to indicate either infinity or an invalid result of a computation.
Positive and negative infinity.
Positive and negative infinity are represented thus:
NaN.
Some operations of floating-point arithmetic are invalid, such as dividing by zero or taking the square root of a negative number. The act of reaching an invalid result is called a floating-point "exception." An exceptional result is represented by a special code called a NaN, for "Not a Number". All NaNs in IEEE 754-1985 have this format:
Range and precision.
Precision is defined as the minimum difference between two successive mantissa representations; thus it is a function only in the mantissa; while the gap is defined as the difference between two successive numbers.
Single precision.
Single-precision numbers occupy 32 bits. In single precision:
Some example range and gap values for given exponents in single precision:
As an example, 16,777,217 can not be encoded as a 32-bit float as it will be rounded to 16,777,216. This shows why floating point arithmetic is unsuitable for accounting software. However, all integers within the representable range that are a power of 2 can be stored in a 32-bit float without rounding.
Double precision.
Double-precision numbers occupy 64 bits. In double precision:
Some example range and gap values for given exponents in double precision:
Extended formats.
The standard also recommends extended format(s) to be used to perform internal computations at a higher precision than that required for the final result, to minimise round-off errors: the standard only specifies minimum precision and exponent requirements for such formats. The x87 80-bit extended format is the most commonly implemented extended format that meets these requirements.
Examples.
Here are some examples of single-precision IEEE 754 representations:
Comparing floating-point numbers.
Every possible bit combination is either a NaN or a number with a unique value in the affinely extended real number system with its associated order, except for the two bit combinations negative zero and positive zero, which sometimes require special attention (see below). The binary representation has the special property that, excluding NaNs, any two numbers can be compared as sign and magnitude integers (endianness issues apply). When comparing as 2%27s-complement integers: If the sign bits differ, the negative number precedes the positive number, so 2's complement gives the correct result (except that negative zero and positive zero should be considered equal). If both values are positive, the 2's complement comparison again gives the correct result. Otherwise (two negative numbers), the correct FP ordering is the opposite of the 2's complement ordering.
Rounding errors inherent in floating point calculations often make comparison of results for exact equality not useful. Choosing an acceptable range is a complex topic.
Although negative zero and positive zero are generally considered equal for comparison purposes, some programming language relational operators and similar constructs might or do treat them as distinct. According to the Java Language Specification, comparison and equality operators treat them as equal, but Math.min() and Math.max() distinguish them (officially starting with Java version 1.1 but actually with 1.1.1), as do the comparison methods equals(), compareTo() and even compare() of classes Float and Double.
Rounding floating-point numbers.
The IEEE standard has four different rounding modes; the first is the default; the others are called "directed roundings".
Extending the real numbers.
The IEEE standard employs (and extends) the affinely extended real number system, with separate positive and negative infinities. During drafting, there was a proposal for the standard to incorporate the projectively extended real number system, with a single unsigned infinity, by providing programmers with a mode selection option. In the interest of reducing the complexity of the final standard, the projective mode was dropped, however. The Intel 8087 and Intel 80287 floating point co-processors both support this projective mode.
Functions and predicates.
Standard operations.
The following functions must be provided:
History.
In 1976 Intel began planning to produce a floating point coprocessor. John Palmer, the manager of the effort, persuaded them that they should try to develop a standard for all their floating point operations. William Kahan was hired as a consultant; he had helped improve the accuracy of Hewlett-Packard's calculators. Kahan initially recommended that the floating point base be decimal but the hardware design of the coprocessor was too far advanced to make that change.
The work within Intel worried other vendors, who set up a standardization effort to ensure a 'level playing field'. Kahan attended the second IEEE 754 standards working group meeting, held in November 1977. Here, he received permission from Intel to put forward a draft proposal based on the standard arithmetic part of their design for a coprocessor. The arguments over gradual underflow lasted until 1981 when an expert commissioned by DEC to assess it sided against the dissenters.
Even before it was approved, the draft standard had been implemented by a number of manufacturers. The Intel 8087, which was announced in 1980, was the first chip to implement the draft standard.

</doc>
<doc id="15190" url="https://en.wikipedia.org/wiki?curid=15190" title="Intel 80186">
Intel 80186

The Intel 80186, also known as the iAPX 186, or just 186, is a microprocessor and microcontroller introduced in 1982. It was based on the Intel 8086 and, like it, had a 16-bit external data bus multiplexed with a 20-bit address bus. It was also available as the 80188, with an 8-bit external data bus.
Description.
Features and performance.
The 80186 series was generally intended for embedded systems, as microcontrollers with external memory. Therefore, to reduce the number of integrated circuits required, it included features such as clock generator, interrupt controller, timers, wait state generator, DMA channels, and external chip select lines.
The initial clock rate of the 80186 was 6 MHz, but due to more hardware available for the microcode to use, especially for address calculation, many individual instructions ran faster than on an 8086 at the same clock frequency. For instance, the common "register+immediate" addressing mode was significantly faster than on the 8086, especially when a memory location was both (one of the) operand(s) and the destination. Multiply and divide also showed great improvement being several times as fast as on the original 8086 and multi-bit shifts were done almost four times as quickly as in the 8086.
A few new instructions were introduced with the 80186 (referred to as the 8086-2 instruction set in some datasheets): enter/leave (replacing several instructions when handling stack frames), pusha/popa (push/pop all general registers), bound (check array index against bounds), and ins/outs (input/output of string). A useful "immediate" mode was added for the push, imul, and multi-bit shift instructions. These instructions were also included in the contemporary 80286 and in successor chips.
The (redesigned) CMOS version, 80C186, introduced DRAM refresh, a power-save mode, and a direct interface to the 8087 or 80187 floating point numeric coprocessor.
Uses.
In personal computers.
The 80186 would have been a natural successor to the 8086 in personal computers. However, because its integrated hardware was incompatible with the hardware used in the original IBM PC, the 80286 was used as the successor instead in the IBM PC/AT.
Few personal computers used the 80186, with some notable exceptions: the Australian Dulmont Magnum laptop, one of the first laptops; the Wang Office Assistant, marketed as a PC-like stand-alone word processor; the Mindset; the Siemens (not 100% IBM PC-compatible but using MS-DOS 2.11); the Compis (a Swedish school computer); the French SMT-Goupil G4; the RM Nimbus (a British school computer); the Unisys ICON (a Canadian school computer); ORB Computer by ABS; the HP 100LX, HP 200LX, HP 1000CX, and HP OmniGo 700LX; the Tandy 2000 desktop (a somewhat PC-compatible workstation with sharp graphics for its day); the Telex 1260 (a desktop PC-XT compatible); the ; the Nokia MikroMikko 2. Acorn created a plug-in for the BBC Master range of computers containing an 80186-10 with 512 KB of RAM, the BBC Master 512 system.
In addition to the above examples of stand-alone implementations of the 80186 for personal computers, there was at least one example of an "add-in"accelerator card implementation: the Orchid Technology PC Turbo 186, released in 1985. It was intended for use with the original 8088-based IBM PC (Model 5150).
Other devices.
The Intel 80186 was intended to be embedded in electronic devices that are not primarily computers. For example, its offshoot, Intel 80188 was embedded inside the Intel 14.4EX modem released in 1991. The 16 MHz processor was used to perform complex algorithms needed for forward error correction, Trellis coded modulation, and echo cancellation in the modem.
In May 2006, Intel announced that production of the 186 would cease at the end of September 2007. Pin- and instruction-compatible replacements might still be manufactured by various third party sources.

</doc>
<doc id="15191" url="https://en.wikipedia.org/wiki?curid=15191" title="Inquisition">
Inquisition

The Inquisition is a group of institutions within the judicial system of the Catholic Church whose aim was to combat heresy. It started in 12th-century France to combat religious sectarianism, in particular the Cathars and the Waldensians. Other groups which were investigated later include the Spiritual Franciscans, the Husites (followers of Jan Hus) and Beguines. Beginning in the 1250s, inquisitors were generally chosen from members of the Dominican Order, to replace the earlier practice of using local clergy as judges. The term Medieval Inquisition covers these courts up through the 14th century.
In the Late Middle Ages and early Renaissance, the concept and scope of the Inquisition was significantly expanded in response to the Protestant Reformation and the Catholic Counter-Reformation. Its geographic scope was expanded to other European countries, resulting in the Spanish Inquisition and Portuguese Inquisition. Those two kingdoms in particular operated inquisitorial courts throughout their respective empires (Spanish and Portuguese) in the Americas (resulting in the Peruvian Inquisition and Mexican Inquisition), Asia, and Africa. One particular focus of the Spanish and Portuguese inquisitions was the issue of Jewish anusim and Muslim converts to Catholicism, partly because these minority groups were more numerous in Spain and Portugal than in many other parts of Europe, and partly because they were often considered suspect due to the assumption that they had secretly reverted to their previous religions.
Except within the Papal States, the institution of the Inquisition was abolished in the early 19th century, after the Napoleonic Wars in Europe and after the Spanish American wars of independence in the Americas. The institution survived as part of the Roman Curia, but in 1904 was given the new name of "Supreme Sacred Congregation of the Holy Office". In 1965 it became the Congregation for the Doctrine of the Faith.
Definition and purpose.
The term "Inquisition" comes from Medieval Latin "inquisitio", which referred to any court process that was based on Roman law, which had gradually come back into usage in the late medieval period. Today, the English term "Inquisition" can apply to any one of several institutions which worked against heretics (or other offenders against canon law) within the judicial system of the Roman Catholic Church. Although the term "Inquisition" is usually applied to ecclesiastical courts of the Catholic Church, nonetheless it has several different usages:
"Inquisition, as a church-court, had no jurisdiction over Moors and Jews as such." Generally, the Inquisition was concerned only with the heretical behaviour of Catholic adherents or converts.
"The overwhelming majority of sentences seem to have consisted of penances like wearing a cross sewn on one's clothes, going on pilgrimage, etc." When a suspect was convicted of unrepentant heresy, the inquisitorial tribunal was required by law to hand the person over to the secular authorities for final sentencing, at which point a magistrate would determine the penalty, which was usually burning at the stake although the penalty varied based on local law. The laws were inclusive of proscriptions against certain religious crimes (heresy, etc.), and the punishments included death by burning, although imprisonment for life or banishment would usually be used. Thus the inquisitors generally knew what would be the fate of anyone so remanded, and cannot be considered to have divorced the means of determining guilt from its effects.
The 1578 handbook for inquisitors spelled out the purpose of inquisitorial penalties: "... quoniam punitio non refertur primo & per se in correctionem & bonum eius qui punitur, sed in bonum publicum ut alij terreantur, & a malis committendis avocentur." Translation from the Latin: "... for punishment does not take place primarily and per se for the correction and good of the person punished, but for the public good in order that others may become terrified and weaned away from the evils they would commit."
Historical background.
Before 1100, the Catholic Church had already suppressed what they believed to be heresy, usually through a system of ecclesiastical proscription or imprisonment, but without using torture
and seldom resorting to executions. Such punishments had a number of ecclesiastical opponents, although some countries punished heresy with the death penalty.
In the 12th century, to counter the spread of Catharism, prosecution of heretics became more frequent. The Church charged councils composed of bishops and archbishops with establishing inquisitions (see Episcopal Inquisition). The first Inquisition was temporarily established in Languedoc (south of France) in 1184. The murder in 1208 of Pope Innocent's papal legate Pierre de Castelnau sparked the Albigensian Crusade (1209–1229). The Inquisition was permanently established in 1229. It was centered under the Dominicans in Rome and later at Carcassonne in Languedoc.
Medieval Inquisition.
Historians use the term "Medieval Inquisition" to describe the various inquisitions that started around 1184, including the Episcopal Inquisition (1184–1230s) and later the Papal Inquisition (1230s). These inquisitions responded to large popular movements throughout Europe considered apostate or heretical to Christianity, in particular the Cathars in southern France and the Waldensians in both southern France and northern Italy. Other Inquisitions followed after these first inquisition movements. The legal basis for some inquisitorial activity came from Pope Innocent IV's papal bull "Ad extirpanda" of 1252, which explicitly authorized (and defined the appropriate circumstances for) the use of torture by the Inquisition for eliciting confessions from heretics. By 1256 inquisitors were given absolution if they used instruments of torture.
In the 13th century, Pope Gregory IX (reigned 1227–1241) assigned the duty of carrying out inquisitions to the Dominican Order and Franciscan Order. Most inquisitors were friars who taught theology and/or law in the universities. They used inquisitorial procedures, a common legal practice adapted from the earlier Ancient Roman court procedures. They judged heresy along with bishops and groups of "assessors" (clergy serving in a role that was roughly analogous to a jury or legal advisers), using the local authorities to establish a tribunal and to prosecute heretics. After 1200, a Grand Inquisitor headed each Inquisition. Grand Inquisitions persisted until the mid 19th century.
Early Modern European history.
With the sharpening of debate and of conflict between the Protestant Reformation and the Catholic Counter-Reformation, Protestant societies came to see/use the Inquisition as a terrifying "Other", while staunch Catholics regarded the Holy Office as a necessary bulwark against the spread of reprehensible heresies.
Witch-trials.
While belief in and persecution of witches were widespread in pre-Christian Europe, and reflected in Germanic law, the influence of the Church in the early medieval era resulted in the revocation of these laws in many places, bringing an end to traditional pagan witch hunts. Throughout the medieval era mainstream Christian teaching had denied the existence of witches and witchcraft, condemning it as pagan superstition. However, Christian influence on popular beliefs in witches and "maleficium" (harm committed by magic), failed to entirely eradicate folk belief in witches.
The fierce denunciation and persecution of supposed sorceresses which characterized the cruel witchhunts of a later age, were not generally found in the first thirteen hundred years of the Christian era. The medieval Church distinguished between "white" and "black" magic. Local folk practice often mixed chants, incantations, and prayers to the appropriate patron saint to ward off storms, to protect cattle, or ensure a good harvest. Bonfires on Midsummer’s Eve were intended to deflect natural catastrophes or the influence of fairies, ghosts, and witches. Plants, often harvested under particular conditions, were deemed effective in healing.
Black magic was that which was used for a malevolent purpose. This was generally dealt with through confession, repentance, and charitable work assigned as penance. Early Irish canons treated sorcery as a crime to be visited with excommunication until adequate penance had been performed. In 1258 Pope Alexander IV ruled that inquisitors should limit their involvement to those cases in which there was some clear presumption of heretical belief.
The prosecution of witchcraft generally became more prominent throughout the late medieval and Renaissance era, perhaps driven partly by the upheavals of the era - the Black Death, Hundred Years War, and a gradual cooling of the climate which modern scientists call the Little Ice Age (between about the 15th and 19th centuries). Witches were sometimes blamed.
Dominican priest Heinrich Kramer was assistant to the Archbishop of Salzburg, and by 1474 was appointed Inquisitor for the Tyrol, Salzburg, Bohemia and Moravia. In 1484 Kramer requested Pope Innocent VIII clarify his authority to prosecute witchcraft in Germany, where he had been refused assistance by the local ecclesiastical authorities, who maintained that as the letter of deputation did not specifically mention where the inquisitors may operate, they could not legally exercise their functions in their areas. 
The bull, "Summis desiderantes affectibus" sought to remedy this jurisdictional dispute by specifically identifying the dioceses of Mainz, Köln, Trier, Salzburg, and Bremen. Innocent's Bull enacted nothing new. Its direct purport was to ratify the powers already conferred upon Henry to deal with witchcraft as well as heresy, and it called upon the Bishop of Strasburg to lend the inquisitors all possible support. Some scholars view the bull as "clearly political", motivated by jurisdictional disputes between the local German Catholic priests and clerics from the Office of the Inquisition who answered more directly to the pope. The bull failed to ensure that Kramer obtained the support he had hoped for, causing him to retire and to compile his views on witchcraft in his book "Malleus Maleficarum", which was written in 1486. In the book, Kramer stated his view that witchcraft was to blame for bad weather; it is also noted for its animus against women. In 1538 the Spanish Inquisition cautioned its members not to believe everything the Malleus said, even when it presented apparently firm evidence. 
Spanish Inquisition.
[[File:Pedro Berruguete Saint Dominic Presiding over an Auto-da-fe 1495.jpg|right|thumb| Pedro Berruguete, "Saint Dominic Guzmán presiding over an Auto da fe" (c. 1495). Many artistic representations depict torture and burning at the stake as occurring during the "auto-da-fé" (Portuguese for "Act of Faith").]]
Portugal and Spain in the late Middle Ages consisted largely of multicultural territories of Muslim and Jewish influence, reconquered from Islamic control, and the new Christian authorities could not assume that all their subjects would suddenly become and remain orthodox Roman Catholics. So the Inquisition in Iberia, in the lands of the Reconquista counties and kingdoms like Leon, Castile and Aragon, had a special socio-political basis as well as more fundamental religious motives.
In some parts of Spain towards the end of the 14th century, there was a wave of violent anti-Judaism, encouraged by the preaching of Ferrand Martinez, Archdeacon of Ecija. In the pogroms of June 1391 in Seville, hundreds of Jews were killed, and the synagogue was completely destroyed. The number of people killed was also high in other cities, such as Córdoba, Valencia and Barcelona.
One of the consequences of these pogroms was the mass conversion of thousands of surviving Jews. Forced baptism was contrary to the law of the Catholic Church, and theoretically anybody who had been forcibly baptized could legally return to Judaism. However, this was very narrowly interpreted. Legal definitions of the time theoretically acknowledged that a forced baptism was not a valid sacrament, but confined this to cases where it was literally administered by physical force. A person who had consented to baptism under threat of death or serious injury was still regarded as a voluntary convert, and accordingly forbidden to revert to Judaism. After the public violence, many of the converted "felt it safer to remain in their new religion." Thus, after 1391, a new social group appeared and were referred to as "conversos" or "New Christians".
King Ferdinand II of Aragon and Queen Isabella I of Castile established the Spanish Inquisition in 1478. In contrast to the previous inquisitions, it operated completely under royal Christian authority, though staffed by clergy and orders, and independently of the Holy See. It operated in Spain and in all Spanish colonies and territories, which included the Canary Islands, the Spanish Netherlands, the Kingdom of Naples, and all Spanish possessions in North, Central, and South America. It primarily targeted forced converts from Islam (Moriscos, Conversos and "secret Moors") and from Judaism (Conversos, Crypto-Jews and Marranos) — both groups still resided in Spain after the end of the Islamic control of Spain — who came under suspicion of either continuing to adhere to their old religion or of having fallen back into it.
In 1492 all Jews who had not converted were expelled from Spain; those who converted became subject to the Inquisition. (Jews were not heretics, but "Catholics" who practised the Jewish faith were regarded as heretics.)
Inquisition in the Spanish overseas empire.
In the Americas, King Philip II set up three tribunals (each formally titled "Tribunal del Santo Oficio de la Inquisición") in 1569, one in Mexico, Cartagena de Indias (in modern-day Colombia) and Peru. The Mexican office administered Mexico (central and southeastern Mexico), Nueva Galicia (northern and western Mexico), the Audiencias of Guatemala (Guatemala, Chiapas, El Salvador, Honduras, Nicaragua, Costa Rica), and the Spanish East Indies. The Peruvian Inquisition, based in Lima, administered all the Spanish territories in South America and Panama.
Portuguese Inquisition.
The Portuguese Inquisition formally started in Portugal in 1536 at the request of the King of Portugal, João III. Manuel I had asked Pope Leo X for the installation of the Inquisition in 1515, but only after his death (1521) did Pope Paul III acquiesce. At its head stood a "Grande Inquisidor", or General Inquisitor, named by the Pope but selected by the Crown, and always from within the royal family. The Portuguese Inquisition principally targeted the Sephardic Jews, whom the state forced to convert to Christianity. Spain had expelled its Sephardic population in 1492; after 1492 many of these Spanish Jews left Spain for Portugal, but eventually were targeted there as well.
The Portuguese Inquisition held its first "auto-da-fé" in 1540. The Portuguese inquisitors mostly targeted the Jewish New Christians (i.e. "conversos" or "marranos"). The Portuguese Inquisition expanded its scope of operations from Portugal to Portugal's colonial possessions, including Brazil, Cape Verde, and Goa, where it continued as a religious court, investigating and trying cases of breaches of the tenets of orthodox Roman Catholicism until 1821. King João III (reigned 1521–57) extended the activity of the courts to cover censorship, divination, witchcraft and bigamy. Originally oriented for a religious action, the Inquisition exerted an influence over almost every aspect of Portuguese society: political, cultural and social.
The Goa Inquisition, an inquisition largely aimed at Catholic converts from Hinduism or Islam who were thought to have returned to their original ways, started in Goa in 1560. In addition, the Inquisition prosecuted non-converts who broke prohibitions against the observance of Hindu or Muslim rites or interfered with Portuguese attempts to convert non-Christians to Catholicism. Aleixo Dias Falcão and Francisco Marques set it up in the palace of the Sabaio Adil Khan.
According to Henry Charles Lea, between 1540 and 1794, tribunals in Lisbon, Porto, Coimbra and Évora resulted in the burning of 1,175 persons, the burning of another 633 in effigy, and the penancing of 29,590. But documentation of 15 out of 689 autos-da-fé has disappeared, so these numbers may slightly understate the activity.
Roman Inquisition.
With the Protestant Reformation, Catholic authorities became much more ready to suspect heresy in any new ideas,
including those of Renaissance humanism, previously strongly supported by many at the top of the Church hierarchy. The extirpation of heretics became a much broader and more complex enterprise, complicated by the politics of territorial Protestant powers, especially in northern Europe. The Catholic Church could no longer exercise direct influence in the politics and justice-systems of lands which officially adopted Protestantism. Thus war (the French Wars of Religion, the Thirty Years War), massacre (the St. Bartholomew's Day massacre) and the missional and propaganda work (by the Sacra congregatio de propaganda fide) of the Counter-Reformation came to play larger roles in these circumstances, and the Roman law type of a "judicial" approach to heresy represented by the Inquisition became less important overall.
In 1542 Pope Paul III established the Congregation of the Holy Office of the Inquisition as a permanent congregation staffed with cardinals and other officials. It had the tasks of maintaining and defending the integrity of the faith and of examining and proscribing errors and false doctrines; it thus became the supervisory body of local Inquisitions. Arguably the most famous case tried by the Roman Inquisition involved Galileo Galilei in 1633.
The penances and sentences for those who confessed or were found guilty were pronounced together in a public ceremony at the end of all the processes. This was the "sermo generalis" or "auto-da-fé".
Penances (not matters for the civil authorities) might consist of a pilgrimage, a public scourging, a fine, or the wearing of a cross. The wearing of two tongues of red or other brightly colored cloth, sewn onto an outer garment in an "X" pattern, marked those who were under investigation. The penalties in serious cases were confiscation of property to the inquisition or imprisonment. This led to the possibility of false charges over confiscation with those over a certain income, particularly rich maranos. Following the French invasion of 1798, the new authorities sent 3,000 chests containing over 100,000 Inquisition documents to France from Rome.
Ending of the Inquisition in the 19th and 20th centuries.
The wars of independence of the former Spanish colonies in the Americas concluded with the abolition of the Inquisition in every quarter of Hispanic America between 1813 and 1825.
In Portugal, in the wake of the Liberal Revolution of 1820, the "General Extraordinary and Constituent Courts of the Portuguese Nation" abolished the Portuguese inquisition in 1821.
The last execution of the Inquisition was in Spain in 1826. This was the execution by garroting of the school teacher Cayetano Ripoll for purportedly teaching Deism in his school. In Spain the practices of the Inquisition were finally outlawed in 1834.
In Italy, after the restoration of the Pope as the ruler of the Papal States in 1814, the activity of the Papal States Inquisition continued on until the mid-19th century, notably in the well-publicised Mortara Affair (1858–1870). In 1908 the name of the Congregation became "The Sacred Congregation of the Holy Office", which in 1965 further changed to "Congregation for the Doctrine of the Faith", as retained to .
Statistics.
Beginning in the 19th century, historians have gradually compiled statistics drawn from the surviving court records, from which estimates have been calculated by adjusting the recorded number of convictions by the average rate of document loss for each time period. Gustav Henningsen and Jaime Contreras studied the records of the Spanish Inquisition, which list 44,674 cases of which 826 resulted in executions "in person" and 778 "in effigy" (i.e. a straw dummy was burned in place of the person). William Monter estimated there were 1000 executions between 1530–1630 and 250 between 1630–1730. Jean-Pierre Dedieu studied the records of Toledo's tribunal, which put 12,000 people on trial. For the period prior to 1530, Henry Kamen estimated there were about 2,000 executions in all of Spain's tribunals. Italian Renaissance history professor and Inquisition expert Carlo Ginzburg had his doubts about using statistics to reach a judgment about the period. “In many cases, we don’t have the evidence, the evidence has been lost,” said Ginzburg.
Appearance in popular media.
The 1982 novel Baltasar and Blimunda by José Saramago, portrays how the Portuguese Inquisition impacts the fortunes of the title characters as well as several others from history, including the priest and aviation pioneer Bartolomeu de Gusmão.
The 1981 comedy film History of the World, Part I, produced and directed by Mel Brooks, features a segment on the Spanish Inquisition.
"Inquisitio" is a French television series set in the Middle Ages.
References.
Notes
Bibliography

</doc>
<doc id="15192" url="https://en.wikipedia.org/wiki?curid=15192" title="Isaac">
Isaac

Isaac(; , ISO 259-3 "Yiçḥaq", " will laugh"; "Isaak" or "") as described in the Hebrew Bible and the Qur'an, was the second son of Abraham, the only son Abraham had with his wife Sarah, and the father of Jacob and Esau. According to the Book of Genesis, Abraham was 100 years old when Isaac was born, and Sarah was past 90.
According to the Genesis narrative, Abraham brought Isaac to Mount Moriah, where, at God's command, Abraham built a sacrificial altar to sacrifice Isaac. This event served as a test of Abraham's faith. At the last moment an angel stopped him.
Isaac was one of the three patriarchs of the Israelites. Isaac was the only biblical patriarch whose name was not changed, and the only one who did not move out of Canaan. Compared to Abraham and Jacob, the Bible relates fewer incidents of Isaac's life. He died when he was 180 years old, making him the longest-lived of the three.
Etymology.
The anglicized name Isaac is a transliteration of the Hebrew term "Yiṣḥāq" which literally means "He laughs/will laugh." Ugaritic texts dating from the 13th century BCE refer to the benevolent smile of the Canaanite deity El. Genesis, however, ascribes the laughter to Isaac's parents, Abraham and Sarah, rather than El. According to the biblical narrative, Abraham fell on his face and laughed when Elohim imparted the news of their son's eventual birth. He laughed because Sarah was past the age of childbearing; both she and Abraham were advanced in age. Later, when Sarah overheard three messengers of the Lord renew the promise, she laughed inwardly for the same reason. Sarah denied laughing when Elohim questioned Abraham about it.
Genesis narrative.
Birth.
It was prophesied to the patriarch Abraham that he would have a son and that his name should be Isaac. When Abraham became one hundred years old, this son was born to him by his first wife Sarah. Though this was Abraham's second son it was Sarah’s first and only child.
On the eighth day from his birth, Isaac was circumcised, as was necessary for all males of Abraham's household, in order to be in compliance with Yahweh's covenant.
After Isaac had been weaned, Sarah saw Ishmael mocking, and urged her husband to cast out Hagar the bondservant and her son, so that Isaac would be Abraham's sole heir. Abraham was hesitant, but at God's order he listened to his wife's request.
Binding.
At some point in Isaac's youth, his father Abraham brought him to Mount Moriah. At God's command, Abraham was to build a sacrificial altar and sacrifice his son Isaac upon it. After he had bound his son to the altar and drawn his knife to kill him, at the very last moment an angel of God prevented Abraham from proceeding. Rather, he was directed to sacrifice instead a nearby ram that was stuck in thickets. This event served as a test of Abraham's faith in God, not as an actual human sacrifice.
Family life.
When Isaac was 40, Abraham sent Eliezer, his steward, into Mesopotamia to find a wife for Isaac, from his nephew Bethuel's family. Eliezer chose the Aramean Rebekah for Isaac. After many years of marriage to Isaac, Rebekah had still not given birth to a child and was believed to be barren. Isaac prayed for her and she conceived. Rebekah gave birth to twin boys, Esau and Jacob. Isaac was 60 years old when his two sons were born. Isaac favored Esau, and Rebekah favored Jacob.
Isaac is unique among the patriarchs for remaining faithful to his wife, and for not having concubines.
Migration.
At the age of 75, Isaac moved to "Beer-lahai-roi" after his father died. When the land experienced famine, he removed to the Philistine land of Gerar where his father once lived. This land was still under the control of King Abimelech as it was in the days of Abraham. Like his father, Isaac also deceived Abimelech about his wife and also got into the well business. He had gone back to all of the wells that his father dug and saw that they were all stopped up with earth. The Philistines did this after Abraham died. So, Isaac unearthed them and began to dig for more wells all the way to Beersheba, where he made a pact with Abimelech, just like in the day of his father.
Birthright.
Isaac grew old and became blind. He called his son Esau and directed him to procure some venison for him, in order to receive Isaac's blessing. While Esau was hunting, Jacob, after listening to his mother's advice, deceived his blind father by misrepresenting himself as Esau and thereby obtained his father's blessing, such that Jacob became Isaac's primary heir and Esau was left in an inferior position. According to Genesis 25:29-34, Esau had previously sold his birthright to Jacob for "bread and stew of lentils". Thereafter, Isaac sent Jacob into Mesopotamia to take a wife of his mother's brother's house. After 20 years working for his uncle Laban, Jacob returned home. He reconciled with his twin brother Esau, then he and Esau buried their father, Isaac, in Hebron after he died at the age of 180.
Burial site.
According to local tradition, the graves of Isaac and Rebekah, along with the graves of Abraham and Sarah and Jacob and Leah, are in the Cave of the Patriarchs.
Jewish views.
In rabbinical tradition, the age of Isaac at the time of binding is taken to be 37 which contrasts with common portrayals of Isaac as a child. The rabbis also thought that the reason for the death of Sarah was the news of the intended sacrifice of Isaac. The sacrifice of Isaac is cited in appeals for the mercy of God in later Jewish traditions. The post-biblical Jewish interpretations often elaborate the role of Isaac beyond the biblical description and primarily focus on Abraham's intended sacrifice of Isaac, called the "aqedah" ("binding"). According to a version of these interpretations, Isaac died in the sacrifice and was revived. According to many accounts of Aggadah, unlike the Bible, it is Satan who is testing Isaac as an agent of God. Isaac's willingness to follow God's command at the cost of his death has been a model for many Jews who preferred martyrdom to violation of the Jewish law.
According to the Jewish tradition, Isaac instituted the afternoon prayer. This tradition is based on Genesis chapter 24, verse 63 ("Isaac went out to meditate in the field at the eventide").
Isaac was the only patriarch who stayed in Canaan during his whole life and though once he tried to leave, God told him not to do so. Rabbinic tradition gave the explanation that Isaac was almost sacrificed and anything dedicated as a sacrifice may not leave the Land of Israel. Isaac was the oldest of the biblical patriarchs at the time of his death, and the only patriarch whose name was not changed.
Rabbinic literature also linked Isaac's blindness in old age, as stated in the Bible, to the sacrificial binding: Isaac's eyes went blind because the tears of angels present at the time of his sacrifice fell on Isaac's eyes.
Christian views.
The early Christian church continued and developed the New Testament theme of Isaac as a type of Christ and the Church being both "the son of the promise" and the "father of the faithful". Tertullian draws a parallel between Isaac's bearing the wood for the sacrificial fire with Christ's carrying his cross. and there was a general agreement that, while all the sacrifices of the Old Law were anticipations of that on Calvary, the sacrifice of Isaac was so "in a pre-eminent way".
The Eastern Orthodox Church and the Roman Catholic Church consider Isaac as a Saint along with other biblical patriarchs. Along with those of other patriarchs and the Old Testament Righteous, his feast day is celebrated in the Eastern Orthodox Church and the Byzantine rite of the Catholic Church on the Second Sunday before Christmas(December 11–17), under the title "the Sunday of the Forefathers". 
New Testament.
The New Testament states Isaac was "offered up" by Abraham his father, and that Isaac blessed his sons. Paul contrasted Isaac, symbolizing Christian liberty, with the rejected older son Ishmael, symbolizing slavery; Hagar is associated with the Sinai covenant, while Sarah is associated with the covenant of grace, into which her son Isaac enters. The Epistle of James chapter 2, verses 21-24, states that the sacrifice of Isaac shows that justification (in the Johannine sense) requires both faith and works.
In the Epistle to the Hebrews, Abraham's willingness to follow God's command to sacrifice Isaac is used as an example of faith as is Isaac's action in blessing Jacob and Esau with reference to the future promised by God to Abraham In verse 19, the author views the release of Isaac from sacrifice as analogous to the resurrection of Jesus, the idea of the sacrifice of Isaac being a prefigure of the sacrifice of Jesus on the cross.
Islamic views.
Isaac (‎ "") is revered by Muslims to be a prophet of Islam. Islam considers Isaac as a prophet of Islam, and describes him as the father of the Israelites and a righteous servant of God.
Isaac, along with Ishmael, is highly important for Muslims for continuing to preach the message of monotheism after his father Abraham. Among Isaac's children was the follow-up Israelite patriarch Jacob, who too is venerated an Islamic prophet.
Isaac is mentioned fifteen times by name in the Qur'an, often with his father and his son, Jacob. The Qur'an states that Abraham received "good tidings of Isaac, a prophet, of the righteous", and that God blessed them both (37: 12). In a fuller description, when angels came to Abraham to tell him of the future punishment to be imposed on Sodom and Gomorrah, his wife, Sarah, "laughed, and We gave her good tidings of Isaac, and after Isaac of (a grandson) Jacob" (11: 71-74); and it is further explained that this event will take place despite Abraham and Sarah's old age. Several verses speak of Isaac as a "gift" to Abraham (6: 84; 14: 49-50), and 24: 26-27 adds that God made "prophethood and the Book to be among his offspring", which has been interpreted to refer to Abraham's two prophetic sons, his prophetic grandson Jacob, and his prophetic great-grandson Joseph. In the Qur'an, it later narrates that Abraham also praised God for giving him Ishmael and Isaac in his old age (14: 39-41).
Elsewhere in the Qur'an, Isaac is mentioned in lists: Joseph follows the religion of his forefathers Abraham, Isaac and Jacob (12: 38) and speaks of God's favor to them (12: 6); Jacob's sons all testify their faith and promise to worship the God that their forefathers, "Abraham, Ishmael and Isaac", worshiped (2: 127); and the Qur'an commands Muslims to believe in the revelations that were given to "Abraham, Ishmael, Isaac, Jacob and the Patriarchs" (2: 136; 3: 84). In the Qur'an's narrative of Abraham's near-sacrifice of his son (37: 102), the name of the son is not mentioned and debate has continued over the son's identity, though many feel that the identity is the least important element in a story which is given to show the courage that one develops through faith.
Qur'an.
The Qur'an mentions Isaac as a prophet and a righteous man of God. Isaac and Jacob are mentioned as being bestowed upon Abraham as gifts of God, who then worshipped God only and were righteous leaders in the way of God:
Academic.
Some scholars have described Isaac as "a legendary figure" while others view him "as a figure representing tribal history, or "as a seminomadic leader." The stories of Isaac, like other patriarchal stories of Genesis, are generally believed to have "their origin in folk memories and oral traditions of the early Hebrew pastoralist experience." "The Cambridge Companion to the Bible" makes the following comment on the biblical stories of the patriarchs:
According to Martin Noth, a scholar of the Hebrew Bible, the narratives of Isaac date back to an older cultural stage than that of the West-Jordanian Jacob. At that era, the Israelite tribes were not yet sedentary. In the course of looking for grazing areas, they had come in contact in southern Philistia with the inhabitants of the settled countryside. The biblical historian, A. Jopsen, believes in the connection between the Isaac traditions and the north, and in support of this theory adduces Amos 7:9 ("the high places of Isaac").
Albrecht Alt and Martin Noth hold that, "The figure of Isaac was enhanced when the theme of promise, previously bound to the cults of the 'God the Fathers' was incorporated into the Israelite creed during the southern-Palestinian stage of the growth of the Pentateuch tradition." According to Martin Noth, at the Southern Palestinian stage of the growth of the Pentateuch tradition, Isaac became established as one of the biblical patriarchs, but his traditions were receded in the favor of Abraham.
Documentary hypothesis.
Form critics variously assign passages like Genesis chapter 26, verses 6-11, to the Jahwist source, and Genesis chapter 20 verses 1-7, chapter 21, verse 1 to chapter 22, verse 14 and chapter 22, verse 19 to the Elohist. According to the compilation hypothesis, the formulaic use of the word "toledoth" (generations) indicates that Genesis chapter 11, verse 27 to chapter 25, verse 19 is Isaac's record through Abraham's death (with Ishmael's record appended), and Genesis chapter 25, verse 19 to chapter 37, verse 2 is Jacob's record through Isaac's death (with Esau's records appended).
In art.
The earliest Christian portrayal of Isaac is found in the Roman catacomb frescoes. Excluding the fragments, Alison Moore Smith classifies these artistic works in three categories:

</doc>
<doc id="15193" url="https://en.wikipedia.org/wiki?curid=15193" title="Italian Football League">
Italian Football League

Italian Football League (IFL) is an American football league in Italy.
History.
The league was founded in 2008, taking over previous league's significance (National Football League Italy). The league was born as a result of the escape of several of the best clubs of the old championship organized by the Italian federation, such as Milano Rhinos, Parma Panthers, Bologna Doves and Bolzano Giants. However some of the historic Italian clubs have not joined the new league and continue to participate in different tournaments organized by other federations. This is the case of Legnano Frogs, Torino Giaguari, etc.
In the following years a lot of teams moved to the FIDAF (the federation where the IFL belongs to) and most of the biggest teams are now part of the IFL that is the First Division or in the other 2 divisions.
IFL teams.
† defunct
♦ due to league expansion the Napoli team can play the 2015 IFL season and is not relegated to the second division
‡ Roma Grizzlies won the second division championship and earned the right to play the 2015 IFL season
In popular culture.
The IFL is featured in John Grisham's 2007 novel "Playing for Pizza," about an American quarterback playing for the Parma Panthers.

</doc>
<doc id="15195" url="https://en.wikipedia.org/wiki?curid=15195" title="Iduna">
Iduna

Iduna can mean several things:

</doc>
<doc id="15198" url="https://en.wikipedia.org/wiki?curid=15198" title="Indic">
Indic

Indic may refer to:

</doc>
<doc id="15199" url="https://en.wikipedia.org/wiki?curid=15199" title="Papua (province)">
Papua (province)

Papua Province () is the largest and easternmost province of Indonesia. It lies in West Papua region, which comprises the Indonesian western half of the island of New Guinea and nearby islands. Papua is bordered by the nation of Papua New Guinea to the east, and by West Papua province to the west. Its capital is Jayapura. It was formerly called Irian Jaya (before that West Irian or Irian Barat) and comprised all of Indonesian New Guinea. In 2002 the current name was adopted and in 2003 West Papua province was created within West Papua region from western parts of Papua province.
Naming.
"Papua" is the official Indonesian and internationally recognised name for the province.
During the Dutch colonial era the region was known as part of "Dutch New Guinea" or "Netherlands New Guinea". Since its annexation in 1969, it became known as "West Irian" or "Irian Barat" until 1973, and thereafter renamed "Irian Jaya" (roughly translated, "Glorious Irian") by the Suharto administration. This was the official name until the name "Papua" was adopted in 2002. Today, the indigenous inhabitants of this province prefer to call themselves Papuans.
The name "West Papua" was adopted in 1961 by the New Guinea Council until the United Nations Temporary Executive Authority (UNTEA) transferred administration to the Republic of Indonesia in 1963. "West Papua" has since been used by Papuans as a self-identifying term, especially by those demanding self-determination, and usually refers to the whole of the Indonesian portion of New Guinea. The other Indonesian province that shares New Guinea, West Irian Jaya, has been officially renamed as "West Papua", or "Papua Barat".
Within Indonesia and West Papua itself, 'Papua' usually refers to the entire western half of New Guinea despite its division into separate provinces. Western New Guinea is generally referred to as 'West Papua' internationally – especially among networks of international solidarity with the West Papuan independence movement.
Government.
The province of Papua is governed by a directly elected governor (currently Lukas Enembe) and a regional legislature, DPRP (Dewan Perwakilan Rakyat Papua). A government organisation that only exists in Papua is the MRP (Majelis Rakyat Papua / Papuan People's Council), which was formed by the Indonesian Government in 2005 as a coalition of Papuan tribal chiefs, tasked with arbitration and speaking on behalf of Papuan tribal customs.
Indonesian sovereignty over Papua dates back to 1969, when Indonesia conducted a referendum (referred to by the Indonesian government as the Act of Free Choice) on the self-determination of the peoples of Papua under an agreement with the United Nations to respect any result. Instead of conducting a democratic referendum amongst the general population, Indonesian security forces forcibly coerced a small number of tribal elders to vote to join Indonesia; some elders were not even made aware that a referendum was to be conducted beforehand. Nevertheless, the agreement with the UN was nominally upheld, and was recognised by the international community in spite of protests. This intensified the independence movement among indigenous West Papuans, deepening the Papua conflict, which began when the Dutch withdrew from the East Indies in 1963. The conflict has continued to the present, with Indonesian security forces being accused of numerous human rights abuses in their suppression of the independence movement. The Indonesian government maintains tight control over the region, barring foreign journalists or rights monitors from entering; those who do must do so covertly.
In 1999 it was proposed to split the province into three government-controlled sectors, sparking Papuan protests. In January 2003 President Megawati Sukarnoputri signed an order dividing Papua into three provinces: Central Irian Jaya (Irian Jaya Tengah), Papua (or East Irian Jaya, Irian Jaya Timur), and West Papua (Irian Jaya Barat). The formality of installing a local government for Jaraka in Irian Jaya Barat (West) took place in February 2003 and a governor was appointed in November; a government for Irian Jaya Tengah (Central Irian Jaya) was delayed from August 2003 due to violent local protests. The creation of this separate Central Irian Jaya Province was blocked by Indonesian courts, who declared it to be unconstitutional and in contravention of the Papua's special autonomy agreement. The previous division into two provinces was allowed to stand as an established fact.
Administrative divisions.
As of 2010 (following the separation of West Papua Province in 2003), the residual Papua Province consisted of 28 regencies ("kabupaten") and one autonomous city ("kota"); these regencies and the city are together subdivided into 385 districts ("kecamatan"), and thence into "villages" ("kelurahan" and "desa"). In Papua, as well as in the province of West Papua, "kelurahan" are commonly referred to as "distrik".
The regencies ("kabupaten") and the city ("kota") are listed below with their areas and their populations at the 2010 Census and according to the latest official (January 2014) Estimates.
In 2000 the present area of Papua Province consisted of nine regencies: Biak Numfor, Jayapura, Jayawijaya, Merauke, Mimika, Nabire, Paniai, Puncak Jaya, and Yapen Waropen. On 12 November 2002, Keerom and Sarmi Regencies were split from Jayapura Regency; Bintang Mountains (Pegunungan Bintan), Tolikara and Yahukimo Regencies were split from Jayawijaya Regency; Asmat, Boven Digoel and Mappi Regencies were split from Merauke Regency; and Yapen Waropen Regency was split into Yapen Islands Regency (Kepulauan Yapen) and Waropen Regency. Supiori Regency was split from Biak Numfor Regency on 8 January 2004.
Mamberamo Raya Regency was formed from parts of Sarmi Regency and Waropen Regency on 15 March 2007 under Law No. 19/2007. The Dogiyai Regency was similarly formed from the southern districts of Nabire Regency in 2007, and five other new regencies were created on 4 January 2008 by Home Affairs Minister Mardiyanto who also installed five temporary regents. These five new regencies were:
Intan Jaya Regency and Deiyai Regency were formed during 2008 from parts of Paniai Regency.
Proposed new regencies, cities and provinces.
On 25 October 2013 the Indonesian House of Representatives (DPR) began reviewing draft laws on the establishment of 57 prospective regencies/cities (and 8 new provinces). This included two new provinces to be formed from parts of the existing Papua Province (and one new province from West Papua Province), as well as the creation of seventeen new regencies and two new cities (independent municipalities). The new regencies will be Gili Menawa, Moyo, Balin Senter, Bogaga, Puncak Trikora, Muara Digul, Admi Korbay, Katengban, Okika, Northwest Yapen, East Yapen, Numfor Island, Yalimek, Mambera Hulu, Southwest Yahukimo, East Yahukimo, North Yahukimo and Ghondumi Sisare, and the new cities will be the municipalities of Merauke and Lembah Baliem (Baliem Valley).
The two new provinces from parts of the existing Papua province have recently been approved by Indonesia's House of Representatives: South Papua and Central Papua. Another new province, to be split from West Papua will be Southwest Papua.
The proposed South Papua ("Papua Selatan") Province would cover an area of 119,749 square km, which is rich in natural resources. It will encompass four existing regencies – Asmat, Boven Digoel, Mappi, and Merauke (including Merauke City, scheduled to become a municipality) and will thus equate closely to the "original" Merauke Regency prior to the splitting of that entity in 2002. Within the existing regencies, new regencies to be added are those of Moyo (from Boven Digoel Regency), Muara Digul and Admi Korbay (both from Mappi Regency).
According to a 20 January 2012 report in the "Cenderawasih Pos Jakarta", the central government is moving forward with the creation of "Central Papua". At that time it was envisaged that the new province would comprise ten existing regencies: Supiori, Biak Numfor, Yapen Islands, Waropen, Nabire, Dogiyai, Deiyai, Intan Jaya, Paniai and Mimika.
The new Central Papua Province, and the residual Papua Province, would together include the new regencies of Gili Menawa (from Jayapura Regency), Balin Senter (from Lanny Jaya Regency and Tolikara Regency), Boboga (from Tolikara Regency), Puncak Trikora (from Lanny Jaya Regency), Katengban (from Bintang Mountains Regency), Okika (from Jayawijaya Regency), Northwest Yapen and East Yapen (both from Yapen Islands Regency), Numfor Island (from Biak Numfor Regency), Yalimek, Ser Yahukimo Western Mountains, Mambera Hulu, Southwest Yahukimo, East Yahukimo and North Yakuhimo (all six from Yahukimo Regency) and Ghondumi Sisare (from Waropen Regency), and a new municipality of Lembah Baliem (Baliem Valley) created from Jayawijaya Regency.
Jayapura City.
The city of Jayapura also has the status of an autonomous city, equal to a regency. It was founded on 7 March 1910 as "Hollandia" and is the capital. Since Indonesian administration the name of the city has been changed to Kotabaru, then to Sukarnopura before its current name, Jayapura. Jayapura is also the largest city of Papua Province, with a small but active tourism industry. It is built on a slope overlooking the bay. Cenderawasih University ("UNCEN") campus at Abepura houses the University Museum where some of the Asmat artifacts collected by Michael Rockefeller is stored. Both Tanjung Ria beach, near the market at Hamadi – site of 22 April 1944 Allied invasion during World War II – and the site of General Douglas MacArthur's World War II headquarters at Ifar Gunung have monuments commemorating the events.
Geography.
A central east–west mountain range dominates the geography of the island of New Guinea, over in total length. The western section is around long and across. The province contains the highest mountains between the Himalayas and the Andes, rising up to high, and ensuring a steady supply of rain from the tropical atmosphere. The tree line is around elevation and the tallest peaks contain permanent equatorial glaciers, increasingly melting due to a changing climate. Various other smaller mountain ranges occur both north and west of the central ranges. Except in high elevations, most areas possess a hot humid climate throughout the year, with some seasonal variation associated with the northeast monsoon season.
The southern and northern lowlands stretch for hundreds of kilometres and include lowland rainforests, extensive wetlands, savanna grasslands, and expanses of mangrove forest. The southern lowlands are the site of Lorentz National Park, also a UNESCO World Heritage Site.
The province's largest river is the Mamberamo located in the northern part of the province. The result is a large area of lakes and rivers known as the Lakes Plains region. The Baliem Valley, home of the Dani people, is a tableland above sea level in the midst of the central mountain range. Puncak Jaya, also known by its Dutch colonial name, "Carstensz Pyramid", is a limestone mountain peak above sea level. It is the highest peak of Oceania.
Ethnic groups.
The following are some of the most well-known ethnic groups of Papua:
The Yei (pronounced Yay) are sometimes known as the Jei, Je, Yei-nan people.
There are approximately 2,500 speakers of the Yei language. 40% Ethno Religionists- animistic tribal religion 60% Catholics and other Christians (blended with animistic beliefs & customs):
The Yei language is believed to have two dialects observed by a Wycliffe, SIL language survey in 2001. At home the Yei people speak their own language but use Indonesian for trade, wider communication and at school. Most Yei are literate in Indonesian.
There are elementary schools in each village. About 10–30% of children continue in middle school. Very few go to high school. 
The nearest high school is in Merauke city.
They live primarily by hunting, fishing, and gardening short and long term crops in the lowlands. The Yei diet mainly consists of rice, vegetables, fish and roasted sago.
With their land at an altitude of less than 100 meters above sea level, the Yei people can best be accessed by vehicle on the road from Merauke or by motorized canoe up the Maro River. There is no airstrip or airplane access other than float plane which is currently available from Merauke through MAF by about a 15-minute flight to Toray.
The Poo and Bupul villages have a clinic but people still use traditional medicines.
There is very little infrastructure in the area: no telephones or toilets. At night electricity is run from a generator. There are single side-band radios (SSBs) in Bupul, Tanas, Poo, and Erambu villages, mainly used by the police and military force. Most villages get their drinking water from the Maro River, but some get it from wells or by collecting rain.
Demographics.
The population of Papua province has a fertility rate of 2.9 children per woman The population grew from the 1.9 million recorded in the 2000 Indonesia Census, to 2.9 million as recorded by the 2010 Census, and is officially estimated to be at about 3.5 million in 2014. Since the early 1990s Papua has had the highest population growth rate of all Indonesian provinces at over 3% annually. This is partly a result of birth rates, but mainly due to migration from other parts of Indonesia. While indigenous Papuans formed the near-totality of the population in 1961, they are now roughly 50% of the population, the other half being composed of non-Papuan migrants coming from other parts of Indonesia. An overwhelming percentage of these migrants came as part of a government-sponsored transmigration program.
According to the 2010 census, 83.15% of the Papuans identified themselves as Christian with 65.48% being Protestant and 17.67% being Roman Catholic. 15.89% of the population was Muslim and less than 1% were Buddhist or Hindu. There is also substantial practice of animism by Papuans.
The densest population center, other than the large coastal cities that house Indonesian bureaucratic and commercial apparatus, is located in and around the town of Wamena in the Baliem Valley of the Central Highlands.
Economy.
In 2011, Papuan caretaker governor Syamsul Arief Rivai claimed Papua's forests cover 42 million hectares with an estimated worth of Rp.700 trillion ($78 billion) and that if the forests were managed properly and sustainably, they could produce over 500 million cubic meters of logs per annum.
The Grasberg Mine, the world's largest gold mine and third largest copper mine, is located in the highlands near Puncak Jaya, the highest mountain in Papua.
Ecology.
The island has an estimated 16,000 species of plant, 124 genera of which are endemic. Papua's known forest fauna includes; marsupials (including possums, wallabies, tree-kangaroos, cuscuses); other mammals (including the endangered long-beaked echidna); bird species such as birds-of-paradise, cassowaries, parrots, and cockatoos; the world's longest lizards (Papua monitor); and the world's largest butterflies.
The waterways and wetlands of Papua are also home to salt and freshwater crocodile, tree monitors, flying foxes, osprey, bats and other animals; while the equatorial glacier fields remain largely unexplored.
Protected areas within Papua province include the World Heritage Lorentz National Park, and the Wasur National Park, a Ramsar wetland of international importance.
In February 2006, a team of scientists exploring the Foja Mountains, Sarmi, discovered new species of birds, butterflies, amphibians, and plants, including possibly the largest-flowered species of rhododendron.
Ecological threats include logging-induced deforestation, forest conversion for plantation agriculture (including oil palm), smallholder agricultural conversion, the introduction and potential spread of alien species such as the crab-eating macaque which preys on and competes with indigenous species, the illegal species trade, and water pollution from oil and mining operations.

</doc>
<doc id="15200" url="https://en.wikipedia.org/wiki?curid=15200" title="IMF (disambiguation)">
IMF (disambiguation)

The International Monetary Fund (IMF) is an international organization.
IMF may also refer to:

</doc>
<doc id="15201" url="https://en.wikipedia.org/wiki?curid=15201" title="Interdisciplinarity">
Interdisciplinarity

Interdisciplinarity involves the combining of two or more academic disciplines into one activity (e.g., a research project). It is about creating something new by crossing boundaries, and thinking across them. It is related to an interdiscipline or an interdisciplinary field, which is an organizational unit that crosses traditional boundaries between academic disciplines or schools of thought, as new needs and professions emerge.
The term "interdisciplinary" is applied within education and training pedagogies to describe studies that use methods and insights of several established disciplines or traditional fields of study. Interdisciplinarity involves researchers, students, and teachers in the goals of connecting and integrating several academic schools of thought, professions, or technologies—along with their specific perspectives—in the pursuit of a common task. The epidemiology of AIDS or global warming require understanding of diverse disciplines to solve complex problems. "Interdisciplinary" may be applied where the subject is felt to have been neglected or even misrepresented in the traditional disciplinary structure of research institutions, for example, women's studies or ethnic area studies. Interdisciplinarity can likewise be applied to complex subjects that can only be understood by combining the perspectives of two or more fields.
The adjective "interdisciplinary" is most often used in educational circles when researchers from two or more disciplines pool their approaches and modify them so that they are better suited to the problem at hand, including the case of the team-taught course where students are required to understand a given subject in terms of multiple traditional disciplines. For example, the subject of land use may appear differently when examined by different disciplines, for instance, biology, chemistry, economics, geography, and politics.
Development.
Although interdisciplinary and interdisciplinarity are frequently viewed as twentieth century terms, the concept has historical antecedents, most notably Greek philosophy. Julie Thompson Klein attests that "the roots of the concepts lie in a number of ideas that resonate through modern discourse—the ideas of a unified science, general knowledge, synthesis and the integration of knowledge," while Giles Gunn says that Greek historians and dramatists took elements from other realms of knowledge (such as medicine or philosophy) to further understand their own material. Actually any broadminded humanist project involves interdisciplinarity, and history shows a crowd of cases, as seventeenth-century Leibniz's task to create a system of universal justice, which required linguistics, economics, management, ethics, law philosophy, politics... even sinology.
Interdisciplinary programs sometimes arise from a shared conviction that the traditional disciplines are unable or unwilling to address an important problem. For example, social science disciplines such as anthropology and sociology paid little attention to the social analysis of technology throughout most of the twentieth century. As a result, many social scientists with interests in technology have joined science and technology studies programs, which are typically staffed by scholars drawn from numerous disciplines. They may also arise from new research developments, such as nanotechnology, which cannot be addressed without combining the approaches of two or more disciplines. Examples include quantum information processing, an amalgamation of quantum physics and computer science, and bioinformatics, combining molecular biology with computer science. Sustainable Development as a research area deals with problems requiring analysis and synthesis across economic, social and environmental spheres; often an integration of multiple social and natural science disciplines. Interdisciplinary research is also key to the study of health sciences, for example in studying optimal solutions to diseases. Some institutions of higher education offer accredited degree programs in Interdisciplinary Studies.
At another level interdisciplinarity is seen as a remedy to the harmful effects of excessive specialization. On some views, however, interdisciplinarity is entirely indebted to those who specialize in one field of study—that is, without specialists, interdisciplinarians would have no information and no leading experts to consult. Others place the focus of interdisciplinarity on the need to transcend disciplines, viewing excessive specialization as problematic both epistemologically and politically. When interdisciplinary collaboration or research results in new solutions to problems, much information is given back to the various disciplines involved. Therefore, both disciplinarians and interdisciplinarians may be seen in complementary relation to one another.
Barriers.
Because most participants in interdisciplinary ventures were trained in traditional disciplines, they must learn to appreciate differing of perspectives and methods. For example, a discipline that places more emphasis on quantitative "rigor" may produce practitioners who think of themselves (and their discipline) as "more scientific" than others; in turn, colleagues in "softer" disciplines may associate quantitative approaches with an inability to grasp the broader dimensions of a problem. An interdisciplinary program may not succeed if its members remain stuck in their disciplines (and in disciplinary attitudes). On the other hand, and from the disciplinary perspective, much interdisciplinary work may be seen as "soft," lacking in rigor, or ideologically motivated; these beliefs place barriers in the career paths of those who choose interdisciplinary work. For example, interdisciplinary grant applications are often refereed by peer reviewers drawn from established disciplines; not surprisingly, interdisciplinary researchers may experience difficulty getting funding for their research. In addition, untenured researchers know that, when they seek promotion and tenure, it is likely that some of the evaluators will lack commitment to interdisciplinarity. They may fear that making a commitment to interdisciplinary research will increase the risk of being denied tenure.
Interdisciplinary programs may fail if they are not given sufficient autonomy. For example, interdisciplinary faculty are usually recruited to a joint appointment, with responsibilities in both an interdisciplinary program (such as women's studies) and a traditional discipline (such as history). If the traditional discipline makes the tenure decisions, new interdisciplinary faculty will be hesitant to commit themselves fully to interdisciplinary work. Other barriers include the generally disciplinary orientation of most scholarly journals, leading to the perception, if not the fact, that interdisciplinary research is hard to publish. In addition, since traditional budgetary practices at most universities channel resources through the disciplines, it becomes difficult to account for a given scholar or teacher's salary and time. During periods of budgetary contraction, the natural tendency to serve the primary constituency (i.e., students majoring in the traditional discipline) makes resources scarce for teaching and research comparatively far from the center of the discipline as traditionally understood. For these same reasons, the introduction of new interdisciplinary programs is often resisted because it is perceived as a competition for diminishing funds.
Due to these and other barriers, interdisciplinary research areas are strongly motivated to become disciplines themselves. If they succeed, they can establish their own research funding programs and make their own tenure and promotion decisions. In so doing, they lower the risk of entry. Examples of former interdisciplinary research areas that have become disciplines include neuroscience, cybernetics, biochemistry and biomedical engineering. These new fields are occasionally referred to as "interdisciplines." On the other hand, even though interdisciplinary activities are now a focus of attention for institutions promoting learning and teaching, as well as organizational and social entities concerned with education, they are practically facing complex barriers, serious challenges and criticism. The most important obstacles and challenges faced by interdisciplinary activities in the past two decades can be divided into "professional", "organizational," and "cultural" obstacles.
Interdisciplinary studies and studies of interdisciplinarity.
An initial distinction should be made between interdisciplinary studies, which can be found spread across the academy today, and the study of interdisciplinarity, which involves a much smaller group of researchers. The former is instantiated in thousands of research centers across the US and the world. The latter has one US organization, the Association for Interdisciplinary Studies (founded in 1979), two international organizations, the International Network of Inter- and Transdisciplinarity (founded in 2010) and the Philosophy of/as Interdisciplinarity Network (founded in 2009), and one research institute devoted to the theory and practice of interdisciplinarity, the Center for the Study of Interdisciplinarity at the University of North Texas (founded in 2008).
An interdisciplinary study is an academic program or process seeking to synthesize broad perspectives, knowledge, skills, interconnections, and epistemology in an educational setting. Interdisciplinary programs may be founded in order to facilitate the study of subjects which have some coherence, but which cannot be adequately understood from a single disciplinary perspective (for example, women's studies or medieval studies). More rarely, and at a more advanced level, interdisciplinarity may itself become the focus of study, in a critique of institutionalized disciplines' ways of segmenting knowledge.
In contrast, studies of interdisciplinarity raise to self-consciousness questions about how interdisciplinarity works, the nature and history of disciplinarity, and the future of knowledge in post-industrial society. Researchers at the Center for the Study of Interdisciplinarity have made the distinction between philosophy 'of' and 'as' interdisciplinarity, the former identifying a new, discrete area within philosophy that raises epistemological and metaphysical questions about the status of interdisciplinary thinking, with the latter pointing toward a philosophical practice that is sometimes called 'field philosophy'.
Perhaps the most common complaint regarding interdisciplinary programs, by supporters and detractors alike, is the lack of synthesis—that is, students are provided with multiple disciplinary perspectives, but are not given effective guidance in resolving the conflicts and achieving a coherent view of the subject. Others have argued that the very idea of synthesis or integration of disciplines presupposes questionable politico-epistemic commitments. Critics of interdisciplinary programs feel that the ambition is simply unrealistic, given the knowledge and intellectual maturity of all but the exceptional undergraduate; some defenders concede the difficulty, but insist that cultivating interdisciplinarity as a habit of mind, even at that level, is both possible and essential to the education of informed and engaged citizens and leaders capable of analyzing, evaluating, and synthesizing information from multiple sources in order to render reasoned decisions.
While much has been written on the philosophy and promise of interdisciplinarity in academic programs and professional practice, social scientists are increasingly interrogating academic discourses on interdisciplinarity, as well as how interdisciplinarity actually works – and does not – in practice. Some have shown, for example, that some interdisciplinary enterprises that aim to serve society can produce deleterious outcomes for which no one can be held to account.
Politics of interdisciplinary studies.
Since 1998, there has been an ascendancy in the value of the concept and practice of interdisciplinary research and teaching and a growth in the number of bachelor's degrees awarded at U.S. universities classified as multi- or interdisciplinary studies. The number of interdisciplinary bachelor's degrees awarded annually rose from 7,000 in 1973 to 30,000 a year by 2005 according to data from the National Center of Educational Statistics (NECS). In addition, educational leaders from the Boyer Commission to Carnegie's President Vartan Gregorian to Alan I. Leshner, CEO of the American Association for the Advancement of Science have advocated for interdisciplinary rather than disciplinary approaches to problem solving in the 21st century. This has been echoed by federal funding agencies, particularly the NIH under the Direction of Elias Zerhouni, who have advocated that grant proposals be framed more as interdisciplinary collaborative projects than single researcher, single discipline ones. At the same time, many thriving longstanding bachelors in interdisciplinary studies programs in existence for 30 or more years, have been closed down, in spite of healthy enrollment. Examples include Arizona International (formerly part of the University of Arizona), the School of Interdisciplinary Studies at Miami University, and the Department of Interdisciplinary Studies at Wayne State University; others such as the Department of Interdisciplinary Studies at Appalachian State University, and George Mason University's New Century College, have been cut back. Stuart Henry has seen this trend as part of the hegemony of the disciplines in their attempt to recolonize the experimental knowledge production of otherwise marginalized fields of inquiry. This is due to threat perceptions seemingly based on the ascendancy of interdisciplinary studies against traditional academia.
Historical examples.
There are many examples of when a particular idea, almost on the same period, arises in different disciplines. One case is the shift from the approach of focusing on "specialized segments of attention" (adopting one particular perspective), to the idea of "instant sensory awareness of the whole", an attention to the "total field", a "sense of the whole pattern, of form and function as a unity", an "integral idea of structure and configuration". This has happened in painting (with cubism), physics, poetry, communication and educational theory. According to Marshall McLuhan, this paradigm shift was due to the passage from an era shaped by mechanization, which brought sequentiality, to the era shaped by the instant speed of electricity, which brought simultaneity.
Efforts to simplify and defend the concept of interdisciplinary.
An article from 1997 posted online by University College, London, attempts to provide a simple, common-sense, definition of interdisciplinarity, bypassing the difficulties of defining that concept and obviating the need for such related concepts as transdisciplinarity, pluridisciplinarity, and multidisciplinarity:"To begin with, a discipline can be conveniently defined as any comparatively self-contained and isolated domain of human experience which possesses its own community of experts. Interdisciplinarity is best seen as bringing together distinctive components of two or more disciplines. In academic discourse, interdisciplinarity typically applies to four realms: knowledge, research, education, and theory. Interdisciplinary knowledge involves familiarity with components of two or more disciplines. Interdisciplinary research combines components of two or more disciplines in the search or creation of new knowledge, operations, or artistic expressions. Interdisciplinary education merges components of two or more disciplines in a single program of instruction. Interdisciplinary theory takes interdisciplinary knowledge, research, or education as its main objects of study."In turn, interdisciplinary knowledge and research are important because:
Quotations.
"The modern mind divides, specializes, thinks in categories: the Greek instinct was the opposite, to take the widest view, to see things as an organic whole ... It was arete that the games were designed to test the arete of the whole man, not a merely specialized skill ... The great event was the pentathalon, if you won this, you were a man. Needless to say, the Marathon race was never heard of until modem times: the Greeks would have regarded it as a monstrosity.""Previously, men could be divided simply into the learned and the ignorant, those more or less the one, and those more or less the other. But your specialist cannot be brought in under either of these two categories. He is not learned, for he is formally ignorant of all that does not enter into his specialty; but neither is he ignorant, because he is 'a scientist,' and 'knows' very well his own tiny portion of the universe. We shall have to say that he is a learned ignoramus, which is a very serious matter, as it implies that he is a person who is ignorant, not in the fashion of the ignorant man, but with all the petulance of one who is learned in his own special line."

</doc>
<doc id="15205" url="https://en.wikipedia.org/wiki?curid=15205" title="Insertion sort">
Insertion sort

Insertion sort is a simple sorting algorithm that builds the final sorted array (or list) one item at a time. It is much less efficient on large lists than more advanced algorithms such as quicksort, heapsort, or merge sort. However, insertion sort provides several advantages:
When people manually sort cards in a bridge hand, most use a method that is similar to insertion sort.
Algorithm for insertion sort.
Insertion sort iterates, consuming one input element each repetition, and growing a sorted output list. Each iteration, insertion sort removes one element from the input data, finds the location it belongs within the sorted list, and inserts it there. It repeats until no input elements remain.
Sorting is typically done in-place, by iterating up the array, growing the sorted list behind it. At each array-position, it checks the value there against the largest value in the sorted list (which happens to be next to it, in the previous array-position checked). If larger, it leaves the element in place and moves to the next. If smaller, it finds the correct position within the sorted list, shifts all the larger values up to make a space, and inserts into that correct position.
The resulting array after "k" iterations has the property where the first "k" + 1 entries are sorted ("+1" because the first entry is skipped). In each iteration the first remaining entry of the input is removed, and inserted into the result at the correct position, thus extending the result:
becomes
with each element greater than "x" copied to the right as it is compared against "x".
The most common variant of insertion sort, which operates on arrays, can be described as follows:
Pseudocode of the complete algorithm follows, where the arrays are zero-based:
The outer loop runs over all the elements except the first one, because the single-element prefix codice_1 entries are sorted is true from the start. The inner loop moves element codice_2 to its correct place so that after the loop, the first codice_3 elements are sorted.
After expanding the "swap" operation in-place as codice_4 (where codice_5 is a temporary variable), a slightly faster version can be produced that moves codice_6 to its position in one go and only performs one assignment in the inner loop body:
The new inner loop shifts elements to the right to clear a spot for codice_7.
Note that although the common practice is to implement in-place, which requires checking the elements in-order, the order of checking (and removing) input elements is actually arbitrary. The choice can be made using almost any pattern, as long as all input elements are eventually checked (and removed from the input).
Best, worst, and average cases.
The best case input is an array that is already sorted. In this case insertion sort has a linear running time (i.e., O("n")). During each iteration, the first remaining element of the input is only compared with the right-most element of the sorted subsection of the array.
The simplest worst case input is an array sorted in reverse order. The set of all worst case inputs consists of all arrays where each element is the smallest or second-smallest of the elements before it. In these cases every iteration of the inner loop will scan and shift the entire sorted subsection of the array before inserting the next element. This gives insertion sort a quadratic running time (i.e., O("n"2)).
The average case is also quadratic, which makes insertion sort impractical for sorting large arrays. However, insertion sort is one of the fastest algorithms for sorting very small arrays, even faster than quicksort; indeed, good quicksort implementations use insertion sort for arrays smaller than a certain threshold, also when arising as subproblems; the exact threshold must be determined experimentally and depends on the machine, but is commonly around ten.
Example:
The following table shows the steps for sorting the sequence {3, 7, 4, 9, 5, 2, 6, 1}. In each step, the key under consideration is underlined. The key that was moved (or left in place because it was biggest yet considered) in the previous step is shown in bold.
3 7 4 9 5 2 6 1
3 7 4 9 5 2 6 1
3 7 4 9 5 2 6 1
3 4 7 9 5 2 6 1
3 4 7 9 5 2 6 1
3 4 5 7 9 2 6 1
2 3 4 5 7 9 6 1
2 3 4 5 6 7 9 1
1 2 3 4 5 6 7 9
Relation to other sorting algorithms.
Insertion sort is very similar to selection sort. As in selection sort, after "k" passes through the array, the first "k" elements are in sorted order. For selection sort these are the "k" smallest elements, while in insertion sort they are whatever the first "k" elements were in the unsorted array. Insertion sort's advantage is that it only scans as many elements as needed to determine the correct location of the "k"+1st element, while selection sort must scan all remaining elements to find the absolute smallest element.
Assuming the "k"+1st element's rank is random, insertion sort will on average require shifting half of the previous "k" elements, while selection sort always requires scanning all unplaced elements. So for unsorted input, insertion sort will usually perform about half as many comparisons as selection sort. If the input array is reverse-sorted, insertion sort performs as many comparisons as selection sort. If the input array is already sorted, insertion sort performs as few as "n"-1 comparisons, thus making insertion sort more efficient when given sorted or "nearly sorted" arrays.
While insertion sort typically makes fewer comparisons than selection sort, it requires more writes because the inner loop can require shifting large sections of the sorted portion of the array. In general, insertion sort will write to the array O("n"2) times, whereas selection sort will write only O() times. For this reason selection sort may be preferable in cases where writing to memory is significantly more expensive than reading, such as with EEPROM or flash memory.
Some divide-and-conquer algorithms such as quicksort and mergesort sort by recursively dividing the list into smaller sublists which are then sorted. A useful optimization in practice for these algorithms is to use insertion sort for sorting small sublists, where insertion sort outperforms these more complex algorithms. The size of list for which insertion sort has the advantage varies by environment and implementation, but is typically between eight and twenty elements. A variant of this scheme runs quicksort with a constant cutoff , then runs a single insertion sort on the final array:
This preserves the expected time complexity of standard quicksort, because after running the codice_8 procedure, the array codice_6 will be partially sorted in the sense that each element is at most positions away from its final, sorted position. On such a partially sorted array, insertion sort will run at most iterations of its inner loop, which is run times, so it has linear time complexity.
Variants.
D.L. Shell made substantial improvements to the algorithm; the modified version is called Shell sort. The sorting algorithm compares elements separated by a distance that decreases on each pass. Shell sort has distinctly improved running times in practical work, with two simple variants requiring O("n"3/2) and O("n"4/3) running time.
If the cost of comparisons exceeds the cost of swaps, as is the case for example with string keys stored by reference or with human interaction (such as choosing one of a pair displayed side-by-side), then using "binary insertion sort" may yield better performance. Binary insertion sort employs a binary search to determine the correct location to insert new elements, and therefore performs ⌈log2("n")⌉ comparisons in the worst case, which is O("n" log "n"). The algorithm as a whole still has a running time of O("n"2) on average because of the series of swaps required for each insertion.
The number of swaps can be reduced by calculating the position of multiple elements before moving them. For example, if the target position of two elements is calculated before they are moved into the right position, the number of swaps can be reduced by about 25% for random data. In the extreme case, this variant works similar to merge sort.
A variant named "binary merge sort" uses a "binary insertion sort" to sort groups of 32 elements, followed by a final sort using merge sort. It combines the speed of insertion sort on small data sets with the speed of merge sort on large data sets.
To avoid having to make a series of swaps for each insertion, the input could be stored in a linked list, which allows elements to be spliced into or out of the list in constant-time when the position in the list is known. However, searching a linked list requires sequentially following the links to the desired position: a linked list does not have random access, so it cannot use a faster method such as binary search. Therefore, the running time required for searching is O("n") and the time for sorting is O("n"2). If a more sophisticated data structure (e.g., heap or binary tree) is used, the time required for searching and insertion can be reduced significantly; this is the essence of heap sort and binary tree sort.
In 2006 Bender, Martin Farach-Colton, and Mosteiro published a new variant of insertion sort called "library sort" or "gapped insertion sort" that leaves a small number of unused spaces (i.e., "gaps") spread throughout the array. The benefit is that insertions need only shift elements over until a gap is reached. The authors show that this sorting algorithm runs with high probability in O("n" log "n") time.
If a skip list is used, the insertion time is brought down to O(log "n"), and swaps are not needed because the skip list is implemented on a linked list structure. The final running time for insertion would be O("n" log "n").
"List insertion sort" is a variant of insertion sort. It reduces the number of movements.
List insertion sort code in C.
If the items are stored in a linked list, then the list can be sorted with O(1) additional space. The algorithm starts with an initially empty (and therefore trivially sorted) list. The input items are taken off the list one at a time, and then inserted in the proper place in the sorted list. When the input list is empty, the sorted list has the desired result.
The algorithm below uses a trailing pointer for the insertion into the sorted list. A simpler recursive method rebuilds the list each time (rather than splicing) and can use O("n") stack space.

</doc>
<doc id="15207" url="https://en.wikipedia.org/wiki?curid=15207" title="Ig Nobel Prize">
Ig Nobel Prize

The Ig Nobel Prizes is a parody of the Nobel Prizes and is given out in early October each year for ten unusual or trivial achievements in scientific research.
The stated aim of the prizes is to "honor achievements that first make people laugh, and then make them think". The awards are sometimes veiled criticism (or gentle satire), but are also used to point out that even the most absurd-sounding avenues of research can yield useful knowledge. Organized by the scientific humor magazine "Annals of Improbable Research" ("AIR"), they are presented by a group that includes Nobel laureates at a ceremony at Harvard University's Sanders Theater, and they are followed by a set of public lectures by the winners at the Massachusetts Institute of Technology.
The name is a play on the words "ignoble" ("characterized by baseness, lowness, or meanness") and the Nobel Prize. The pronunciation used during the ceremony is , not like the word "ignoble".
History.
The first Ig Nobels were created in 1991 by Marc Abrahams, editor and co-founder of the "Annals of Improbable Research", and the master of ceremonies at all subsequent awards ceremonies. Awards were presented at that time for discoveries "that cannot, or should not, be reproduced". Ten prizes are awarded each year in many categories, including the Nobel Prize categories of physics, chemistry, physiology/medicine, literature, and peace, but also other categories such as public health, engineering, biology, and interdisciplinary research. The Ig Nobel Prizes recognize genuine achievements, with the exception of three prizes awarded in the first year to fictitious scientists Josiah S. Carberry, Paul DeFanti, and Thomas Kyle.
The awards are sometimes veiled criticism (or gentle satire), as in the two awards given for homeopathy research, prizes in "science education" to the Kansas and Colorado state boards of education for their stance regarding the teaching of evolution, and the prize awarded to "Social Text" after the Sokal affair. Most often, however, they draw attention to scientific articles that have some humorous or unexpected aspect. Examples range from the discovery that the presence of humans tends to sexually arouse ostriches, to the statement that black holes fulfill all the technical requirements to be the location of Hell, to research on the "five-second rule", a tongue-in-cheek belief that food dropped on the floor will not become contaminated if it is picked up within five seconds.
In 2010, Sir Andre Geim was awarded a Nobel Prize in physics for his work with graphene, thus becoming the first person to have received both a Nobel Prize and an individual Ig Nobel prize.
Ceremony.
The prizes are presented by genuine Nobel laureates, originally at a ceremony in a lecture hall at MIT but now in Sanders Theater at Harvard University. It contains a number of running jokes, including Miss Sweetie Poo, a little girl who repeatedly cries out, "Please stop: I'm bored", in a high-pitched voice if speakers go on too long. The awards ceremony is traditionally closed with the words: "If you didn't win a prize—and especially if you did—better luck next year!"
The ceremony is co-sponsored by the Harvard Computer Society, the Harvard–Radcliffe Science Fiction Association and the Harvard–Radcliffe Society of Physics Students.
Throwing paper planes onto the stage is a long-standing tradition at the Ig Nobels. In past years, physics professor Roy J. Glauber swept the stage clean of the airplanes as the official "Keeper of the Broom" for years. Glauber could not attend the 2005 awards because he was traveling to Stockholm to claim a genuine Nobel Prize in Physics.
The "Parade of Ignitaries" brings various supporting groups into the hall. At the 1997 ceremonies, a team of "cryogenic sex researchers" distributed a pamphlet titled "Safe Sex at Four Kelvin". Delegates from the Museum of Bad Art are often on hand to display some pieces from their collection too.
Outreach.
The ceremony is recorded and broadcast on National Public Radio and is shown live over the Internet. The recording is broadcast every year, on the Friday after U.S. Thanksgiving, on the public radio program "Science Friday". In recognition of this, the audience chants the first name of the radio show's host, Ira Flatow.
Two books have been published with write-ups on some of the winners: "The Ig Nobel Prize" (2002, US paperback ISBN 0-452-28573-9, UK paperback ISBN 0-7528-4261-7) and "The Ig Nobel Prize 2" (2005, US hardcover ISBN 0-525-94912-7, UK hardcover ISBN 0-7528-6461-0), which was later retitled "The Man Who Tried to Clone Himself" (ISBN 0-452-28772-3).
An Ig Nobel Tour has been an annual part of National Science week in the United Kingdom since 2003. The tour has also traveled to Australia several times, Aarhus University in Denmark in April 2009, Italy and The Netherlands.
Reception.
A September 2009 article in "The National" titled "A noble side to Ig Nobels" says that, although the Ig Nobel Awards are veiled criticism of trivial research, history shows that trivial research sometimes leads to important breakthroughs. For instance, in 2006, a study showing that one of the malaria mosquitoes ("Anopheles gambiae") is attracted equally to the smell of Limburger cheese and the smell of human feet earned the Ig Nobel Prize in the area of biology. However as a direct result of these findings, traps baited with this cheese have been placed in strategic locations in some parts of Africa to combat the epidemic of malaria.

</doc>
<doc id="15208" url="https://en.wikipedia.org/wiki?curid=15208" title="Isaac Albéniz">
Isaac Albéniz

Isaac Manuel Francisco Albéniz y Pascual (; 29 May 186018 May 1909) was a Spanish pianist and composer best known for his piano works based on folk music idioms. Transcriptions of many of his pieces, such as "Asturias (Leyenda)", "Granada", "Sevilla", "Cadiz", "Córdoba", "Cataluña", and the "Tango in D", are important pieces for classical guitar, though he never composed for the guitar. The personal papers of Albéniz are preserved, among other institutions, in the "Biblioteca de Catalunya".
Life.
Born in Camprodon, province of Girona, to Ángel Albéniz (a customs official) and his wife, Dolors Pascual, Albéniz was a child prodigy who first performed at the age of four. At age seven, after apparently taking lessons from Antoine François Marmontel, he passed the entrance examination for piano at the Conservatoire de Paris, but he was refused admission because he was believed to be too young. By the time he had reached 12, he had made many attempts to run away from home.
His concert career began at the age of nine when his father toured both Isaac and his sister, Clementina, throughout northern Spain. A popular myth is that at the age of twelve Albéniz stowed away in a ship bound for Buenos Aires. He then found himself in Cuba, then to the United States, giving concerts in New York and San Francisco and then travelled to Liverpool, London and Leipzig, Germany. By age 15, he had already given concerts worldwide. This over-dramatized story is not entirely false. Albéniz did travel the world as a performer; however, he was accompanied by his "father", who as a customs agent was required to travel frequently. This can be attested by comparing Isaac's concert dates with his father's travel itinerary.
In 1876, after a short stay at the Leipzig Conservatory, he went to study at the Royal Conservatory of Brussels after King Alfonso's personal secretary, Guillermo Morphy, obtained him a royal grant. Count Morphy thought highly of Albéniz, who would later dedicate "Sevilla" to Morphy's wife when it premiered in Paris in January 1886.
In 1880 Albéniz went to Budapest, Hungary to study with Franz Liszt, only to find out that Liszt was in Weimar, Germany.
In 1883 he met the teacher and composer Felip Pedrell, who inspired him to write Spanish music such as the "Chants d'Espagne". The first movement (Prelude) of that suite, later retitled after the composer's death as "Asturias (Leyenda)", is probably most famous today as part of the classical guitar repertoire, even though it was originally composed for piano. (Many of Albéniz's other compositions were also transcribed for guitar, notably by Francisco Tárrega.) At the 1888 Barcelona Universal Exposition, the piano manufacturer Érard sponsored a series of 20 concerts featuring Albéniz's music.
The apex of Albéniz's concert career is considered to be 1889 to 1892 when he had concert tours throughout Europe. During the 1890s Albéniz lived in London and Paris. For London he wrote some musical comedies which brought him to the attention of the wealthy Francis Money-Coutts, 5th Baron Latymer. Money-Coutts commissioned and provided him with librettos for the opera "Henry Clifford" and for a projected trilogy of Arthurian operas. The first of these, "Merlin" (18981902), was thought to have been lost but has recently been reconstructed and performed. Albéniz never completed "Lancelot" (only the first act is finished, as a vocal and piano score), and he never began "Guinevere", the final part.
In 1900 he started to suffer from Bright's disease and returned to writing piano music. Between 1905 and 1908 he composed his final masterpiece, "Iberia" (1908), a suite of twelve piano "impressions".
In 1883 the composer married his student Rosina Jordana. They had three children: Blanca (who died in 1886), Laura (a painter), and Alfonso (who played for Real Madrid in the early 1900s before embarking on a career as a diplomat). Two other children died in infancy.
Albéniz died from his kidney disease on 18May1909 at age48 in Cambo-les-Bains, in Labourd, south-western France. Only a few weeks before his death, the government of France awarded Albéniz its highest honor, the Grand-Croix de la Légion d'honneur. He is buried at the Montjuïc Cemetery, Barcelona.
Work.
Early works.
Albéniz's early works were mostly "salon style" music. Albéniz's first published composition, "Marcha Militar", appeared in 1868. A number of works written before this are now lost. He continued composing in traditional styles ranging from Jean-Philippe Rameau, Johann Sebastian Bach, Ludwig van Beethoven, Frédéric Chopin and Franz Liszt until the mid-1880s. He also wrote at least five zarzuelas, of which all but two are now lost.
Middle period – Spanish influences.
During the late 1880s, the strong influence of Spanish style is evident in Albéniz's music. In 1883 Albéniz met the teacher and composer Felipe Pedrell. Pedrell was a leading figure in the development of nationalist Spanish music. In his book "The Music of Spain", Gilbert Chase describes Pedrell's influence on Albéniz: "What Albéniz derived from Pedrell was above all a spiritual orientation, the realization of the wonderful values inherent in Spanish music." Felipe Pedrell inspired Albéniz to write Spanish music such as the "Suite española", Op. 47, noted for its delicate, intricate melody and abrupt dynamic changes.
In addition to the Spanish spirit infused in Albéniz's music, he incorporated other qualities as well. In her biography of Albéniz, Pola Baytelman discerns four characteristics of the music from the middle period as follows:
1. The dance rhythms of Spain, of which there are a wide variety. 2. The use of cante jondo, which means deep or profound singing. It is the most serious and moving variety of flamenco or Spanish gypsy song, often dealing with themes of death, anguish, or religion. 3. The use of exotic scales also associated with flamenco music. The Phrygian mode is the most prominent in Albéniz's music, although he also used the Aeolian and Mixolydian modes as well as the whole-tone scale. 4. The transfer of guitar idioms into piano writing. 
Following his marriage, Albéniz settled in Madrid, Spain and produced a substantial quantity of music in a relatively short period. By 1886 he had written over 50piano pieces. The Albéniz biographer Walter A. Clark says that pieces from this period received enthusiastic reception in the composer's many concerts. Chase describes music from this period,
Taking the guitar as his instrumental model, and drawing his inspiration largely from the peculiar traits of Andalusian folk musicbut without using actual folk themesAlbéniz achieves a stylization of Spanish traditional idioms that while thoroughly artistic, gives a captivating impression of spontaneous improvisation... "Córdoba" is the piece that best represents the style of Albéniz in this period, with its hauntingly beautiful melody, set against the acrid dissonances of the plucked accompaniment imitating the notes of the Moorish guslas. Here is the heady scent of jasmines amid the swaying palm trees, the dream fantasy of an Andalusian "Arabian Nights" in which Albéniz loved to let his imagination dwell.
Later period.
While Albéniz's crowning achievement, "Iberia", was written in the last years of his life in France, many of its preceding works are well-known and of great interest. The five pieces in "Chants d'Espagne", ("Songs of Spain", published in 1892) are a solid example of the compositional ideas he was exploring in the "middle period" of his life. The suite shows what Albéniz biographer Walter Aaron Clark describes as the "first flowering of his unique creative genius", and the beginnings of compositional exploration that became the hallmark of his later works. This period also includes his operatic works"Merlin, Henry Clifford", and "Pepita Jiménez". His orchestral works of this period include "Spanish Rhapsody" (1887) and "Catalonia" (1899), dedicated to Ramon Casas, who had painted his full-length portrait in 1894.
Albéniz on his own music.
Perhaps the best source on the works is Albéniz himself. He is quoted as commenting on his earlier period works as:
There are among them a few things that are not completely worthless. The music is a bit infantile, plain, spirited; but in the end, the people, our Spanish people, are something of all that. I believe that the people are right when they continue to be moved by "Córdoba, Mallorca", by the copla of the "Sevillanas", by the "Serenata", and "Granada". In all of them I now note that there is less musical science, less of the grand idea, but more color, sunlight, flavor of olives. That music of youth, with its little sins and absurdities that almost point out the sentimental affectation ... appears to me like the carvings in the Alhambra, those peculiar arabesques that say nothing with their turns and shapes, but which are like the air, like the sun, like the blackbirds or like the nightingales of its gardens. They are more valuable than all else of Moorish Spain, which though we may not like it, is the true Spain.
Impact.
Albéniz's influence on the future of Spanish music was profound. His activities as conductor, performer and composer significantly raised the profile of Spanish music abroad and encouraged Spanish music and musicians in his own country.
Albéniz's works have become an important part of the repertoire of the classical guitar, many of which have been transcribed by Miguel Llobet and others. "Asturias (Leyenda)" in particular is heard most often on the guitar, as are "Granada", "Sevilla", "Cadiz", "Cataluña", "Córdoba" and the "Tango in D". Gordon Crosskey and Cuban-born guitarist Manuel Barrueco have both made solo guitar arrangements of six of the eight-movement "Suite española". Selections from "Iberia" have rarely been attempted on solo guitar but have been very effectively performed by guitar ensembles, such as the performance by John Williams and Julian Bream of "Iberia's " opening "Evocation". The Doors incorporated "Asturias" into their song "Spanish Caravan"; also, Iron Maiden's "To Tame a Land" uses the introduction of the piece for the song bridge. More recently, a guitar version of "Granada" functions as something of a love theme in Woody Allen's 2008 film "Vicky Cristina Barcelona". The 2008 horror film "Mirrors" incorporates the theme from "Asturias" into its score.
In 1997 the "Fundación Isaac Albéniz" was founded to promote Spanish music and musicians and to act as a research centre for Albéniz and Spanish music in general.
In film.
A film about his life entitled "Albéniz" was made in 1947. It was produced in Argentina.
List of selected works.
Other works.
Symphonic versions of "Iberia" have been arranged by Enrique Fernández Arbós, Carlos Surinach, and Peter Breiner. There is a piano and orchestra version of "Rapsodia española" by Cristóbal Halffter.
References and sources.
References
Sources

</doc>
<doc id="15210" url="https://en.wikipedia.org/wiki?curid=15210" title="ITU-R">
ITU-R

The ITU Radiocommunication Sector (ITU-R) is one of the three sectors (divisions or units) of the International Telecommunication Union (ITU) and is responsible for radio communication. 
Its role is to manage the international radio-frequency spectrum and satellite orbit resources and to develop standards for radiocommunication systems with the objective of ensuring the effective use of the spectrum.
ITU is required, according to its Constitution, to allocate spectrum and register frequency allocation, orbital positions and other parameters of satellites, “in order to avoid harmful interference between radio stations of different countries”. The international spectrum management system is therefore based on regulatory procedures for frequency coordination, notification and registration.
ITU-R has a permanent secretariat, the Radiocommunication Bureau, based at the ITU HQ in Geneva, Switzerland. The elected Director of the Bureau is Mr. François Rancy of France. First elected by the ITU Membership to the Directorship in 2010.
History.
The CCIR—Comité consultatif international pour la radio, Consultative Committee on International Radio or International Radio Consultative Committee—was founded in 1927.
In 1932 the CCIR and several other organizations (including the original ITU, which had been founded as the International Telegraph Union in 1865) merged to form what would in 1934 become known as the International Telecommunication Union. In 1992, the CCIR became the ITU-R.

</doc>
