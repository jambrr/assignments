<doc id="15352" url="https://en.wikipedia.org/wiki?curid=15352" title="Identical particles">
Identical particles

Identical particles, also called indistinguishable or indiscernible particles, are particles that cannot be distinguished from one another, even in principle. Species of identical particles include, but are not limited to elementary particles such as electrons, composite subatomic particles such as atomic nuclei, as well as atoms and molecules. Quasiparticles also behave in this way. Although all known indistinguishable particles are "tiny", there is no exhaustive list of all possible sorts of particles nor a clear-cut limit of applicability; see particle statistics #Quantum statistics for detailed explication.
There are two main categories of identical particles: bosons, which can share quantum states, and fermions, which do not share quantum states due to the Pauli exclusion principle. Examples of bosons are photons, gluons, phonons, helium-4 nuclei and all mesons. Examples of fermions are electrons, neutrinos, quarks, protons, neutrons, and helium-3 nuclei.
The fact that particles can be identical has important consequences in statistical mechanics. Calculations in statistical mechanics rely on probabilistic arguments, which are sensitive to whether or not the objects being studied are identical. As a result, identical particles exhibit markedly different statistical behavior from distinguishable particles. For example, the indistinguishability of particles has been proposed as a solution to Gibbs' mixing paradox.
Distinguishing between particles.
There are two methods for distinguishing between particles. The first method relies on differences in the intrinsic physical properties of the particles, such as mass, electric charge, and spin. If differences exist, it is possible to distinguish between the particles by measuring the relevant properties. However, it is an empirical fact that microscopic particles of the same species have completely equivalent physical properties. For instance, every electron in the universe has exactly the same electric charge; this is why it is possible to speak of such a thing as "the charge of the electron".
Even if the particles have equivalent physical properties, there remains a second method for distinguishing between particles, which is to track the trajectory of each particle. As long as the position of each particle can be measured with infinite precision (even when the particles collide), then there would be no ambiguity about which particle is which.
The problem with the second approach is that it contradicts the principles of quantum mechanics. According to quantum theory, the particles do not possess definite positions during the periods between measurements. Instead, they are governed by wavefunctions that give the probability of finding a particle at each position. As time passes, the wavefunctions tend to spread out and overlap. Once this happens, it becomes impossible to determine, in a subsequent measurement, which of the particle positions correspond to those measured earlier. The particles are then said to be indistinguishable.
Quantum mechanical description of identical particles.
Symmetrical and antisymmetrical states.
What follows is an example to make the above discussion concrete, using the formalism developed in the article on the mathematical formulation of quantum mechanics.
Let "n" denote a complete set of (discrete) quantum numbers for specifying single-particle states (for example, for the particle in a box problem, take "n" to be the quantized wave vector of the wavefunction.) For simplicity, consider a system composed of two identical particles. Suppose that one particle is in the state "n"1, and another is in the state "n"2. What is the quantum state of the system? Intuitively, it should be
which is simply the canonical way of constructing a basis for a tensor product space formula_2 of the combined system from the individual spaces. However, this expression implies the ability to identify the particle with "n"1 as "particle 1" and the particle with "n"2 as "particle 2". If the particles are indistinguishable, this is impossible by definition; either particle can be in either state. It turns out that we must have: 
To see this, imagine a two identical particle system. Suppose it is known that one of the particles is in state formula_4 and the other is in state formula_5. Prior to the measurement, there is no way to know if particle 1 is in state formula_4 and particle 2 is in state formula_5, or the other way around because the particles are indistinguishable. So, there are equal probabilities for each of the states to occur - meaning that the system is in superposition of both states prior to the measurement. 
States where this is a sum are known as symmetric; states involving the difference are called antisymmetric. More completely, symmetric states have the form
while antisymmetric states have the form
Note that if "n"1 and "n"2 are the same, the antisymmetric expression gives zero, which cannot be a state vector as it cannot be normalized. In other words, in an antisymmetric state two identical particles cannot occupy the same single-particle states. This is known as the Pauli exclusion principle, and it is the fundamental reason behind the chemical properties of atoms and the stability of matter.
Exchange symmetry.
The importance of symmetric and antisymmetric states is ultimately based on empirical evidence. It appears to be a fact of nature that identical particles do not occupy states of a mixed symmetry, such as
There is actually an exception to this rule, which will be discussed later. On the other hand, it can be shown that the symmetric and antisymmetric states are in a sense special, by examining a particular symmetry of the multiple-particle states known as exchange symmetry.
Define a linear operator "P", called the exchange operator. When it acts on a tensor product of two state vectors, it exchanges the values of the state vectors:
"P" is both Hermitian and unitary. Because it is unitary, it can be regarded as a symmetry operator. This symmetry may be described as the symmetry under the exchange of labels attached to the particles (i.e., to the single-particle Hilbert spaces).
Clearly, formula_12 (the identity operator), so the eigenvalues of "P" are +1 and −1. The corresponding eigenvectors are the symmetric and antisymmetric states:
In other words, symmetric and antisymmetric states are essentially unchanged under the exchange of particle labels: they are only multiplied by a factor of +1 or −1, rather than being "rotated" somewhere else in the Hilbert space. This indicates that the particle labels have no physical meaning, in agreement with the earlier discussion on indistinguishability.
It will be recalled that "P" is Hermitian. As a result, it can be regarded as an observable of the system, which means that, in principle, a measurement can be performed to find out if a state is symmetric or antisymmetric. Furthermore, the equivalence of the particles indicates that the Hamiltonian can be written in a symmetrical form, such as
It is possible to show that such Hamiltonians satisfy the commutation relation
According to the Heisenberg equation, this means that the value of "P" is a constant of motion. If the quantum state is initially symmetric (antisymmetric), it will remain symmetric (antisymmetric) as the system evolves. Mathematically, this says that the state vector is confined to one of the two eigenspaces of "P", and is not allowed to range over the entire Hilbert space. Thus, that eigenspace might as well be treated as the actual Hilbert space of the system. This is the idea behind the definition of Fock space.
Fermions and bosons.
The choice of symmetry or antisymmetry is determined by the species of particle. For example, we must always use symmetric states when describing photons or helium-4 atoms, and antisymmetric states when describing electrons or protons.
Particles which exhibit symmetric states are called bosons. As we will see, the nature of symmetric states has important consequences for the statistical properties of systems composed of many identical bosons. These statistical properties are described as Bose–Einstein statistics.
Particles which exhibit antisymmetric states are called fermions. As we have seen, antisymmetry gives rise to the Pauli exclusion principle, which forbids identical fermions from sharing the same quantum state. Systems of many identical fermions are described by Fermi–Dirac statistics.
Parastatistics are also possible.
In certain two-dimensional systems, mixed symmetry can occur. These exotic particles are known as anyons, and they obey fractional statistics. Experimental evidence for the existence of anyons exists in the fractional quantum Hall effect, a phenomenon observed in the two-dimensional electron gases that form the inversion layer of MOSFETs. There is another type of statistic, known as braid statistics, which are associated with particles known as plektons.
The spin-statistics theorem relates the exchange symmetry of identical particles to their spin. It states that bosons have integer spin, and fermions have half-integer spin. Anyons possess fractional spin.
"N" particles.
The above discussion generalizes readily to the case of "N" particles. Suppose we have "N" particles with quantum numbers "n"1, "n"2, ..., nN. If the particles are bosons, they occupy a totally symmetric state, which is symmetric under the exchange of "any two" particle labels:
Here, the sum is taken over all different states under permutations "p" acting on "N" elements. The square root left to the sum is a normalizing constant. The quantity "mn" stands for the number of times each of the single-particle states "n" appears in the "N"-particle state. Note that "∑n mn = N".
In the same vein, fermions occupy totally antisymmetric states:
Here, is the sign of each permutation (i.e.formula_19 if formula_20 is composed of an even number of transpositions, and formula_21 if odd). Note that there is no formula_22 term, because each single-particle state can appear only once in a fermionic state. Otherwise the sum would again be zero due to the antisymmetry, thus representing a physically impossible state. This is the Pauli exclusion principle for many particles.
These states have been normalized so that
Measurements of identical particles.
Suppose we have a system of "N" bosons (fermions) in the symmetric (antisymmetric) state
and we perform a measurement of some other set of discrete observables, "m". In general, this would yield some result "m1" for one particle, "m2" for another particle, and so forth. If the particles are bosons (fermions), the state after the measurement must remain symmetric (antisymmetric), i.e.
The probability of obtaining a particular result for the "m" measurement is
We can show that 
which verifies that the total probability is 1. Note that we have to restrict the sum to "ordered" values of "m1", ..., "mN" to ensure that we do not count each multi-particle state more than once.
Wavefunction representation.
So far, we have worked with discrete observables. We will now extend the discussion to continuous observables, such as the position "x".
Recall that an eigenstate of a continuous observable represents an infinitesimal "range" of values of the observable, not a single value as with discrete observables. For instance, if a particle is in a state |"ψ"⟩, the probability of finding it in a region of volume "d"3"x" surrounding some position "x" is
As a result, the continuous eigenstates |"x"⟩ are normalized to the delta function instead of unity:
We can construct symmetric and antisymmetric multi-particle states out of continuous eigenstates in the same way as before. However, it is customary to use a different normalizing constant:
We can then write a many-body wavefunction,
where the single-particle wavefunctions are defined, as usual, by
The most important property of these wavefunctions is that exchanging any two of the coordinate variables changes the wavefunction by only a plus or minus sign. This is the manifestation of symmetry and antisymmetry in the wavefunction representation:
The many-body wavefunction has the following significance: if the system is initially in a state with quantum numbers "n"1, ..., nN, and we perform a position measurement, the probability of finding particles in infinitesimal volumes near "x"1, "x"2, ..., "x"N is
The factor of "N"! comes from our normalizing constant, which has been chosen so that, by analogy with single-particle wavefunctions,
Because each integral runs over all possible values of "x", each multi-particle state appears "N"! times in the integral. In other words, the probability associated with each event is evenly distributed across "N"! equivalent points in the integral space. Because it is usually more convenient to work with unrestricted integrals than restricted ones, we have chosen our normalizing constant to reflect this.
Finally, it is interesting to note that antisymmetric wavefunction can be written as the determinant of a matrix, known as a Slater determinant:
Statistical properties.
Statistical effects of indistinguishability.
The indistinguishability of particles has a profound effect on their statistical properties. To illustrate this, let us consider a system of "N" distinguishable, non-interacting particles. Once again, let "n""j" denote the state (i.e. quantum numbers) of particle "j". If the particles have the same physical properties, the "n""j"'s run over the same range of values. Let "ε"("n") denote the energy of a particle in state "n". As the particles do not interact, the total energy of the system is the sum of the single-particle energies. The partition function of the system is
where "k" is Boltzmann's constant and "T" is the temperature. We can factor this expression to obtain
where
If the particles are identical, this equation is incorrect. Consider a state of the system, described by the single particle states ["n"1, ..., "n""N"]. In the equation for "Z", every possible permutation of the "n"'s occurs once in the sum, even though each of these permutations is describing the same multi-particle state. We have thus over-counted the actual number of states.
If we neglect the possibility of overlapping states, which is valid if the temperature is high, then the number of times we count each state is approximately "N"!. The correct partition function is
Note that this "high temperature" approximation does not distinguish between fermions and bosons.
The discrepancy in the partition functions of distinguishable and indistinguishable particles was known as far back as the 19th century, before the advent of quantum mechanics. It leads to a difficulty known as the Gibbs paradox. Gibbs showed that if we use the equation "Z" = "ξ""N", the entropy of a classical ideal gas is
where "V" is the volume of the gas and "f" is some function of "T" alone. The problem with this result is that "S" is not extensive – if we double "N" and "V", "S" does not double accordingly. Such a system does not obey the postulates of thermodynamics.
Gibbs also showed that using "Z" = "ξ""N"/"N"! alters the result to
which is perfectly extensive. However, the reason for this correction to the partition function remained obscure until the discovery of quantum mechanics.
Statistical properties of bosons and fermions.
There are important differences between the statistical behavior of bosons and fermions, which are described by Bose–Einstein statistics and Fermi–Dirac statistics respectively. Roughly speaking, bosons have a tendency to clump into the same quantum state, which underlies phenomena such as the laser, Bose–Einstein condensation, and superfluidity. Fermions, on the other hand, are forbidden from sharing quantum states, giving rise to systems such as the Fermi gas. This is known as the Pauli Exclusion Principle, and is responsible for much of chemistry, since the electrons in an atom (fermions) successively fill the many states within shells rather than all lying in the same lowest energy state.
We can illustrate the differences between the statistical behavior of fermions, bosons, and distinguishable particles using a system of two particles. Let us call the particles A and B. Each particle can exist in two possible states, labelled formula_46 and formula_47, which have the same energy.
We let the composite system evolve in time, interacting with a noisy environment. Because the formula_46 and formula_47 states are energetically equivalent, neither state is favored, so this process has the effect of randomizing the states. (This is discussed in the article on quantum entanglement.) After some time, the composite system will have an equal probability of occupying each of the states available to it. We then measure the particle states.
If A and B are distinguishable particles, then the composite system has four distinct states: formula_50, formula_51, formula_52, and formula_53. The probability of obtaining two particles in the formula_46 state is 0.25; the probability of obtaining two particles in the formula_47 state is 0.25; and the probability of obtaining one particle in the formula_46 state and the other in the formula_47 state is 0.5.
If A and B are identical bosons, then the composite system has only three distinct states: formula_50, formula_51, and formula_60. When we perform the experiment, the probability of obtaining two particles in the formula_46 state is now 0.33; the probability of obtaining two particles in the formula_47 state is 0.33; and the probability of obtaining one particle in the formula_46 state and the other in the formula_47 state is 0.33. Note that the probability of finding particles in the same state is relatively larger than in the distinguishable case. This demonstrates the tendency of bosons to "clump."
If A and B are identical fermions, there is only one state available to the composite system: the totally antisymmetric state formula_65. When we perform the experiment, we inevitably find that one particle is in the formula_46 state and the other is in the formula_47 state.
The results are summarized in Table 1:
As can be seen, even a system of two particles exhibits different statistical behaviors between distinguishable particles, bosons, and fermions. In the articles on Fermi–Dirac statistics and Bose–Einstein statistics, these principles are extended to large number of particles, with qualitatively similar results.
The homotopy class.
To understand why particle statistics work the way that they do, note first that particles are point-localized excitations and that particles that are spacelike separated do not interact. In a flat "d"-dimensional space "M", at any given time, the configuration of two identical particles can be specified as an element of "M" × "M". If there is no overlap between the particles, so that they do not interact directly, then their locations must belong to the space the subspace with coincident points removed. The element describes the configuration with particle I at x and particle II at y, while describes the interchanged configuration. With identical particles, the state described by ought to be indistinguishable from the state described by . Now consider the homotopy class of continuous paths from to , within the space . If "M" is R"d" where , then this homotopy class only has one element. If "M" is R2, then this homotopy class has countably many elements (i.e. a counterclockwise interchange by half a turn, a counterclockwise interchange by one and a half turns, two and a half turns, etc., a clockwise interchange by half a turn, etc.). In particular, a counterclockwise interchange by half a turn is "not" homotopic to a clockwise interchange by half a turn. Lastly, if "M" is R, then this homotopy class is empty.
Suppose first that . The universal covering space of which is none other than itself, only has two points which are physically indistinguishable from , namely itself and . So, the only permissible interchange is to swap both particles. This interchange is an involution, so its only effect is to multiply the phase by a square root of 1. If the root is +1, then the points have Bose statistics, and if the root is −1, the points have Fermi statistics.
In the case "M" = R2, the universal covering space of has infinitely many points that are physically indistinguishable from . This is described by the infinite cyclic group generated by making a counterclockwise half-turn interchange. Unlike the previous case, performing this interchange twice in a row does not recover the original state; so such an interchange can generically result in a multiplication by exp("iθ") for any real "θ" (by unitarity, the absolute value of the multiplication must be 1). This is called anyonic statistics. In fact, even with two "distinguishable" particles, even though is now physically distinguishable from , the universal covering space still contains infinitely many points which are physically indistinguishable from the original point, now generated by a counterclockwise rotation by one full turn. This generator, then, results in a multiplication by exp("iφ"). This phase factor here is called the mutual statistics.
Finally, in the case "M" = R, the space is not connected, so even if particle I and particle II are identical, they can still be distinguished via labels such as "the particle on the left" and "the particle on the right". There is no interchange symmetry here.

</doc>
<doc id="15354" url="https://en.wikipedia.org/wiki?curid=15354" title="Interstitial cystitis">
Interstitial cystitis

Interstitial cystitis (IC), also known as bladder pain syndrome (BPS) or by several similar names, is a chronic inflammatory condition of the submucosal and muscular layers of the bladder. IC/BPS may be associated with urinary urgency, urinary frequency, waking at night to urinate, and sterile urine cultures. IC/BPS can result in a quality of life comparable to that of a person with rheumatoid arthritis, chronic cancer pain, or on kidney dialysis.
The cause of IC/BPS is currently unknown and the condition is regarded as a diagnosis of exclusion. Those with interstitial cystitis may have symptoms that overlap with other urinary bladder disorders such as: urinary tract infection (UTI), overactive bladder, urethritis, urethral syndrome, and prostatitis.
Signs and symptoms.
The most common symptoms of IC/BPS are suprapubic pain, urinary frequency, painful sexual intercourse, and waking up from sleep to urinate.
In general, symptoms may include painful urination described as a burning sensation in the urethra during urination, pelvic pain that is worsened with the consumption of certain foods or drinks, urinary urgency, and pressure in the bladder or pelvis. Other frequently described symptoms are urinary hesitancy (needing to wait for the urinary stream to begin, often caused by pelvic floor dysfunction and tension), and discomfort and difficulty driving, working, exercise work out or traveling. Pelvic pain experienced by those with IC typically worsens with filling of the urinary bladder and may improve with urination.
During cystoscopy, 5–10% of people with IC are found to have Hunner's ulcers. A person with IC may have discomfort only in the urethra, while another might struggle with pain in the entire pelvis. Interstitial cystitis symptoms usually fall into one of two patterns: significant suprapubic pain with little frequency or a lesser amount of suprapubic pain but with increased urinary frequency.
Association with other conditions.
Some people with IC/BPS have been diagnosed with other conditions such as irritable bowel syndrome (IBS), fibromyalgia, chronic fatigue syndrome, allergies, Sjogren's syndrome, which raises the possibility that interstitial cystitis may be caused by mechanisms that cause these other conditions. There is also some evidence of an association between urologic pain syndromes, such as IC/BPS and CP/CPPS, with non-celiac gluten sensitivity in some patients.
In addition, men with IC/PBS are frequently diagnosed as having chronic nonbacterial prostatitis, and there is an extensive overlap of symptoms and treatment between the two conditions, leading researchers to posit that the conditions may share the same etiology and pathology.
Causes.
The cause of IC/BPS is currently unknown. However, several explanations have been proposed and include the following: autoimmune theory, nerve theory, mast cell theory, leaky lining theory, infection theory, and a theory of production of a toxic substance in the urine. Other suggested etiological causes are neurologic, allergic, genetic, and stress-psychological. In addition, recent research shows that those with IC may have a substance in the urine that inhibits the growth of cells in the bladder epithelium. An infection may then predispose those people to develop IC. Current evidence from clinical and laboratory studies confirms that mast cells play a central role in IC/PBS possibly due to their ability to release histamine and cause pain, swelling, scarring, and interfere with healing. Research has shown a proliferation of nerve fibers is present in the bladders of people with IC which is absent in the bladders of people who have not been diagnosed with IC.
Regardless of the origin, the majority of people with IC/BPS struggle with a damaged urothelium, or bladder lining. When the surface glycosaminoglycan (GAG) layer is damaged (via a urinary tract infection (UTI), excessive consumption of coffee or sodas, traumatic injury, etc.), urinary chemicals can "leak" into surrounding tissues, causing pain, inflammation, and urinary symptoms. Oral medications like pentosan polysulfate and medications placed directly into the bladder via a catheter sometimes work to repair and rebuild this damaged/wounded lining, allowing for a reduction in symptoms. Most literature supports the belief that IC's symptoms are associated with a defect in the bladder epithelium lining, allowing irritating substances in the urine to penetrate into the bladder—essentially, a breakdown of the bladder lining (also known as the adherence theory). Deficiency in this glycosaminoglycan layer on the surface of the bladder results in increased permeability of the underlying submucosal tissues.
GP51 has been identified as a possible urinary biomarker for IC with significant variations in GP51 levels in those with IC when compared to individuals without interstitial cystitis.
Numerous studies have noted the link between IC, anxiety, stress, hyper-responsiveness, and panic. Another proposed etiology for interstitial cystitis is that the body's immune system attacks the bladder. Biopsies on the bladder walls of people with IC usually contain mast cells. Mast cells containing histamine packets gather when an allergic reaction is occurring. The body identifies the bladder wall as a foreign agent, and the histamine packets burst open and attack. The body attacks itself, which is the basis of autoimmune disorders. Additionally, IC may be triggered by an unknown toxin or stimulus which causes nerves in the bladder wall to fire uncontrollably. When they fire, they release substances called neuropeptides that induce a cascade of reactions that cause pain in the bladder wall.
Genes.
Some genetic subtypes, in some people, have been linked to the disorder.
Diagnosis.
A diagnosis of IC/BPS is one of exclusion, as well as a review of clinical symptoms. The AUA Guidelines recommend starting with a careful patient history, physical examination and laboratory tests to assess and document symptoms of IC, as well as other potential disorders.
The KCl test, also known as the "potassium sensitivity test", is no longer recommended. The test uses a mild potassium solution to evaluate the integrity of the bladder wall. Though the latter is not specific for IC/BPS, it has been determined to be helpful in predicting the use of compounds, such as pentosan polysulphate, which are designed to help repair the GAG layer.
For complicated cases, the use of hydrodistention with cystoscopy may be helpful. Researchers, however, determined that this visual examination of the bladder wall after stretching the bladder was not specific for IC/BPS and that the test, itself, can contribute to the development of small glomerulations (petechial hemorrhages) often found in IC/BPS. Thus, a diagnosis of IC/BPS is one of exclusion, as well as a review of clinical symptoms.
In 2006, the ESSIC society proposed more rigorous and demanding diagnostic methods with specific classification criteria so that it cannot be confused with other, similar conditions. Specifically, they require that a patient must have pain associated with the bladder, accompanied by one other urinary symptom. Thus, a patient with just frequency or urgency would be excluded from a diagnosis. Secondly, they strongly encourage the exclusion of confusable diseases through an extensive and expensive series of tests including (A) a medical history and physical exam, (B) a dipstick urinalysis, various urine cultures, and a serum PSA in men over 40, (C) flowmetry and post-void residual urine volume by ultrasound scanning and (D) cystoscopy. A diagnosis of IC/BPS would be confirmed with a hydrodistention during cystoscopy with biopsy.
They also propose a ranking system based upon the physical findings in the bladder. Patients would receive a numeric and letter based score based upon the severity of their disease as found during the hydrodistention. A score of 1–3 would relate to the severity of the disease and a rating of A–C represents biopsy findings. Thus, a patient with 1A would have very mild symptoms and disease while a patient with 3C would have the worst possible symptoms. Widely recognized scoring systems such as the O'Leary Sant symptom and problem score have emerged to evaluate the severity of IC symptoms such as pain and urinary symptoms.
Differential diagnosis.
The symptoms of IC/BPS are often misdiagnosed as a urinary tract infection. However, IC/BPS has not been shown to be caused by a bacterial infection and antibiotics are an ineffective treatment. The symptoms of IC/BPS may also initially be attributed to prostatitis and epididymitis (in men) and endometriosis and uterine fibroids (in women).
Treatment.
In 2011, the American Urological Association released consensus-based guideline for the diagnosis and treatment of IC.
They include treatments ranging from conservative to more invasive: 
The AUA guidelines also listed several discontinued treatments, including: long-term oral antibiotics, intravesical bacillus Calmette Guerin, intravesical resiniferatoxin), high-pressure and long-duration hydrodistention, and systemic glucocorticoids.
Bladder distension.
Bladder distension while under general anesthesia, also known as hydrodistention (a procedure which stretches the bladder capacity), has shown some success in reducing urinary frequency and giving short-term pain relief to those with IC. However, it is unknown exactly how this procedure causes pain relief. Recent studies show pressure on pelvic trigger points can relieve symptoms. The relief achieved by bladder distensions is only temporary (weeks or months), so is not viable as a long-term treatment for IC/BPS. The proportion of IC/BPS patients who experience relief from hydrodistention is currently unknown and evidence for this modality is limited by a lack of properly controlled studies. Bladder rupture and sepsis may be associated with prolonged, high-pressure hydrodistention.
Bladder instillations.
Bladder instillation of medication is one of the main forms of treatment of interstitial cystitis, but evidence for its effectiveness is currently limited. Advantages of this treatment approach include direct contact of the medication with the bladder and low systemic side effects due to poor absorption of the medication. Single medications or a mixture of medications are commonly used in bladder instillation preparations. DMSO is the only approved bladder instillation for IC/BPS yet it is much less frequently used in urology clinics.
Research studies presented at recent conferences of the American Urological Association by C. Subah Packer have demonstrated the FDA-approved dose of a 50% solution of DMSO had the potential to create irreversible muscle contraction. However, a lesser solution of 25% was found to be reversible. Long-term use of DMSO is questionable, as its mechanism of action is not fully understood though DMSO is thought to inhibit mast cells and may have anti-inflammatory, muscle-relaxing, and analgesic effects. Other agents used for bladder instillations to treat interstitial cystitis include: heparin, lidocaine, chondroitin sulfate, hyaluronic acid, pentosan polysulfate, oxybutynin, and botulinum toxin A. Preliminary evidence suggests these agents are efficacious in reducing symptoms of interstitial cystitis, but further study with larger, randomized, controlled clinical trials is needed.
Diet.
Diet modification is often recommended as a first-line method of self-treatment for interstitial cystitis, though rigorous controlled studies examining the impact diet has on interstitial cystitis signs and symptoms are currently lacking. Individuals with interstitial cystitis often experience an increase in symptoms when they consume certain foods and beverages. Avoidance of these potential trigger foods and beverages such as caffeine-containing beverages including coffee, tea, and soda, alcoholic beverages, chocolate, citrus fruits, hot peppers, and artificial sweeteners may be helpful in alleviating symptoms. Diet triggers vary between individuals with IC; the best way for a person to discover his or her own triggers is to use an elimination diet. Sensitivity to trigger foods may be reduced if calcium glycerophosphate and/or sodium bicarbonate is consumed. The foundation of therapy is a modification of diet to help patients avoid those foods which can further irritate the damaged bladder wall.
The mechanism by which dietary modification benefits people with IC is unclear. Integration of neural signals from pelvic organs may mediate the effects of diet on symptoms of IC.
Medications.
The antihistamine hydroxyzine failed to demonstrate superiority over placebo in treatment of IC patients in a randomized, controlled, clinical trial. 
Amitriptyline has been shown to be effective in reducing symptoms such as chronic pelvic pain and nocturia in many patients with IC/BPS with a median dose of 75 mg daily. In one study, the antidepressant duloxetine was found to be ineffective as a treatment, although a patent exists for use of duloxetine in the context of IC, and is known to relieve neuropathic pain. The calcineurin inhibitor cyclosporine A has been studied as a treatment for interstitial cystitis due to its immunosuppressive properties. A prospective randomized study found cyclosporine A to be more effective at treating IC symptoms than pentosan polysulfate, but also had more adverse effects.
Oral pentosan polysulfate is believed to repair the protective glycosaminoglycan coating of the bladder, but studies have encountered mixed results when attempting to determine if the effect is statistically significant compared to placebo.
Pain control therapies.
Acupuncture may help pain associated with IC/BPS as part of other treatments. Despite a scarcity of controlled studies on alternative medicine and IC/BPS, "rather good results have been obtained" when acupuncture is combined with other treatments.
Biofeedback, a relaxation technique aimed at helping people control functions of the autonomic nervous system, has shown some benefit in controlling pain associated with IC/BPS as part of a multimodal approach that may also include medication or hydrodistention of the bladder.
Pelvic floor treatments.
Urologic pelvic pain syndromes, such as IC/BPS and CP/CPPS, are characterized by pelvic muscle tenderness, and symptoms may be reduced with pelvic myofascial physical therapy.
This may leave the pelvic area in a sensitized condition, resulting in a loop of muscle tension and heightened neurological feedback (neural wind-up), a form of myofascial pain syndrome. Current protocols, such as the Wise–Anderson Protocol, largely focus on stretches to release overtensed muscles in the pelvic or anal area (commonly referred to as trigger points), physical therapy to the area, and progressive relaxation therapy to reduce causative stress.
Pelvic floor dysfunction is a fairly new area of specialty for physical therapists worldwide. The goal of therapy is to relax and lengthen the pelvic floor muscles, rather than to tighten and/or strengthen them as is the goal of therapy for patients with urinary incontinence. Thus, traditional exercises such as Kegel exercises, which are used to strengthen pelvic muscles, can provoke pain and additional muscle tension. A specially trained physical therapist can provide direct, hands on evaluation of the muscles, both externally and internally.
Surgery.
Surgery is rarely used for IC/BPS. Surgical intervention is very unpredictable, and is considered a treatment of last resort for severe refractory cases of interstitial cystitis. Some patients who opt for surgical intervention continue to experience pain after surgery. Typical surgical interventions for refractory cases of IC/BPS include: bladder augmentation, urinary diversion, transurethral fulguration and resection of ulcers, and bladder removal (cystectomy).
Neuromodulation can be successful in treating IC/BPS symptoms, including pain. One electronic pain-killing option is TENS. Percutaneous tibial nerve stimulation stimulators have also been used, with varying degrees of success. Percutaneous sacral nerve root stimulation was able to produce statistically significant improvements in several parameters, including pain.
Prognosis.
IC/BPS has a profound impact on quality of life. A 2007 Finnish epidemiologic study showed that two-thirds of women at moderate to high risk of having interstitial cystitis reported impairment in their quality of life and 35% of IC patients reported an impact on their sexual life. A 2012 survey showed that among a group of adult women with symptoms of interstitial cystitis, 11% reported suicidal thoughts in the past two weeks. Other research has shown that the impact of IC/BPS on quality of life is severe and may be comparable to the quality of life experienced in endstage renal disease or rheumatoid arthritis.
International recognition of interstitial cystitis has grown and international urology conferences to address the heterogeneity in diagnostic criteria have recently been held. IC/PBS is now recognized with an official disability code in the United States of America.
Epidemiology.
IC/BPS affects men and women of all cultures, socioeconomic backgrounds, and ages. Although the disease was previously believed to be a condition of menopausal women, growing numbers of men and women are being diagnosed in their twenties and younger. IC/BPS is not a rare condition. Early research suggested that IC/BPS prevalence ranged from 1 in 100,000 to 5.1 in 1,000 of the general population. In recent years, the scientific community has achieved a much deeper understanding of the epidemiology of interstitial cystitis. Recent studies have revealed that between 2.7 and 6.53 million women in the USA have symptoms of IC and up to 12% of women may have early symptoms of IC/BPS. Further study has estimated that the condition is far more prevalent in men than previously thought ranging from 1.8 to 4.2 million men having symptoms of interstitial cystitis.
The condition is officially recognized as a disability in the United States.
History.
Philadelphia surgeon Joseph Parrish published the earliest record of interstitial cystitis in 1836 describing three cases of severe lower urinary tract symptoms without the presence of a bladder stone. The term "interstitial cystitis" was coined by Dr. Alexander Skene in 1887 to describe the disease. In 2002, the United States amended the Social Security Act to include interstitial cystitis as a disability. The first guideline for diagnosis and treatment of interstitial cystitis is released by a Japanese research team in 2009. The American Urological Association released the first American clinical practice guideline for diagnosing and treating IC/BPS in 2011.
Names.
Originally called "interstitial cystitis", this disorder was renamed to "interstitial cystitis/bladder pain syndrome" (IC/BPS) in the 2002–2010 timeframe. In 2007, the National Institute of Diabetes and Digestive and Kidney Diseases (NIDDK) began using the umbrella term urologic chronic pelvic pain syndrome (UCPPS) to refer to pelvic pain syndromes associated with the bladder (e.g., interstitial cystitis/bladder pain syndrome) and with the prostate gland or pelvis (e.g., chronic prostatitis/chronic pelvic pain syndrome).
In 2008, terms currently in use in addition to IC/BPS include "painful bladder syndrome", "bladder pain syndrome" and "hypersensitive bladder syndrome", alone and in a variety of combinations. These different terms are being used in different parts of the world. The term "interstitial cystitis" is the primary term used in ICD-10 and MeSH. Grover et al said, "The International Continence Society named the disease interstitial cystitis/painful bladder syndrome (IC/PBS) in 2002 et al. 2002, while the Multinational Interstitial Cystitis Association have labeled it as painful bladder syndrome/interstitial cystitis (PBS/IC) et al. 2005. Recently, the European Society for the study of Interstitial Cystitis (ESSIC) proposed the moniker, ‘bladder pain syndrome’ (BPS) de Merwe et al. 2008."

</doc>
<doc id="15355" url="https://en.wikipedia.org/wiki?curid=15355" title="ICI">
ICI

ICI or Ici may refer to:

</doc>
<doc id="15356" url="https://en.wikipedia.org/wiki?curid=15356" title="Imperial Chemical Industries">
Imperial Chemical Industries

Imperial Chemical Industries (ICI) was a British chemical company and was, for much of its history, the largest manufacturer in Britain.
It was formed by the merger of leading British chemical companies in 1926.
Its headquarters were at Millbank in London, and it was a constituent of the FT 30 and later the FTSE 100 Indexes.
ICI made paints and speciality products, including food ingredients, speciality polymers, electronic materials, fragrances and flavourings.
It was acquired by AkzoNobel in 2008,
who immediately sold parts of ICI to Henkel, and integrated ICI's remaining operations within its existing organisation.
History.
Development of the business (1926–44).
The company was founded in December 1926 from the merger of four companies: Brunner Mond, Nobel Explosives, the United Alkali Company, and British Dyestuffs Corporation.
It established its head office at Millbank in London in 1928.
Competing with DuPont and IG Farben, the new company produced chemicals, explosives, fertilisers, insecticides, dyestuffs, non-ferrous metals, and paints.
In its first year turnover was £27 million.
In the 1920s and 30s, the company played a key role in the development of new chemical products, including the dyestuff phthalocyanine (1929), the acrylic plastic Perspex (1932), Dulux paints (1932, co-developed with DuPont), polyethylene (1937), and polyethylene terephthalate fibre known as Terylene (1941).
In 1940, ICI started British Nylon Spinners as a joint venture with Courtaulds.
ICI also owned the Sunbeam motorcycle business, which had come with Nobel Industries, and continued to build motorcycles until 1937.
During Second World War, ICI was involved with the United Kingdom's nuclear weapons programme codenamed Tube Alloys.
Postwar innovation (1945–90).
In the 1940s and 50s, the company established its pharmaceutical business and developed a number of key products, including Paludrine (1940s, an anti-malarial drug), halothane (1951, an anaesthetic agent), Inderal (1965, a beta-blocker), tamoxifen (1978, a frequently used drug for breast cancer),
and PEEK (1979, a high performance thermoplastic). ICI formed ICI Pharmaceuticals in 1957. 
ICI developed a fabric in the 1950s known as Crimplene, a thick polyester yarn used to make a fabric of the same name.
The resulting cloth is heavy and wrinkle-resistant, and retains its shape well.
The California-based fashion designer Edith Flagg was the first to import this fabric from Britain to the USA.
During the first two years, ICI gave Flagg a large advertising budget to popularise the fabric across America.
In 1960, Paul Chambers became the first chairman appointed from outside the company.
Chambers employed the consultancy firm McKinsey to help with reorganising the company.
His eight-year tenure saw export sales double, but his reputation was severely damaged by a failed takeover bid for Courtaulds in 1961–62. In 1962, ICI developed the controversial herbicide, paraquat. 
ICI was confronted with the nationalisation of its operations in Burma on 1 August 1962 as a consequence of the military coup.
In 1964, ICI acquired British Nylon Spinners (BNS), the company it had jointly set up in 1940 with Courtaulds.
ICI surrendered its 37.5 per cent holding in Courtaulds and paid Courtaulds £2 million a year for five years, "to take account of the future development expenditure of Courtaulds in the nylon field."
In return, Courtaulds transferred to ICI their 50 per cent holding in BNS.
BNS was absorbed into ICI's existing polyester operation, ICI Fibres.
The acquisition included BNS production plants in Pontypool, Gloucester and Doncaster, together with research and development in Pontypool.
Early pesticide development included Gramoxone (1962, a herbicide), the insecticides pirimiphos-methyl in 1967 and pirimicarb in 1970, brodifacoum (a rodenticide) was developed in 1974; in the late 1970s, ICI was involved in the early development of synthetic pyrethroid insecticides such as lambda-cyhalothrin.
Peter Allen was appointed chairman between 1968 and 1971.
He presided over the purchase of Viyella.
Profits shrank under his tenure.
Jack Callard was appointed chairman from 1971 to 1975.
He almost doubled company profits between 1972 and 1974, and made ICI Britain's largest exporter.
In 1971, the company acquired Atlas Chemical Industries Inc., a major American competitor.
In 1977, Imperial Metal Industries was divested as an independent quoted company.
From 1982 to 1987, the company was led by the charismatic John Harvey-Jones.
Under his leadership, the company acquired the Beatrice Chemical Division in 1985 and Glidden Coatings & Resins, a leading paints business, in 1986.
Reorganisation of the business (1991–2007).
In 1991, ICI sold the agricultural and merchandising operations of BritAg and Scottish Agricultural Industries to Norsk Hydro,
and fought off a hostile takeover bid from Hanson, who had acquired 2.8 percent of the company.
However, the move pressured the company to split-off its pharmaceuticals arm.
It also divested its soda ash products arm to Brunner Mond, ending an association with the trade that had existed since the company's inception, one that had been inherited from the original Brunner, Mond & Co. Ltd.
In 1992, the company sold its nylon business to DuPont.
In 1993, the company de-merged its pharmaceutical bio-science businesses: pharmaceuticals, agrochemicals, specialities, seeds and biological products were all transferred into a new and independent company called Zeneca Group, which subsequently merged with Astra AB to form AstraZeneca.
Charles Miller Smith was appointed CEO in 1994, one of the few times that someone from outside ICI had been appointed to lead the company, Smith having previously been a director at Unilever.
Shortly afterwards, the company acquired a number of former Unilever businesses in an attempt to move away from its historical reliance on commodity chemicals. 
In 1997, ICI acquired National Starch & Chemical, Quest, Unichema, and Crosfield, the speciality chemicals businesses of Unilever for $8 billion.
This step was part of a strategy to move away from cyclical bulk chemicals and to progress up the value chain to become a higher growth, higher margin business.
Later that year it went on to buy Rutz & Huber, a Swiss paints business.
Having taken on some £4 billion of debt to finance these acquisitions, the company had to sell off its commodity chemicals businesses:
Having sold much of its historically profitable commodities businesses, and many of the new speciality businesses which it had failed to integrate, the company consisted mainly of the Dulux paints business, which quickly found itself the subject of a takeover by AkzoNobel.
Takeover by AkzoNobel.
Dutch firm AkzoNobel (owner of Crown Berger paints) bid £7.2 billion (€10.66 billion or $14.5 billion) for ICI in June 2007.
An area of concern about a potential deal was ICI's British pension fund, which had future liabilities of more than £9 billion at the time.
Regulatory issues in the UK and other markets where Dulux and Crown Paints brands each have significant market share were also a cause for concern for the boards of ICI and AkzoNobel.
In the UK, any combined operation without divestments would have seen AkzoNobel have a 54 per cent market share in the paint market.
The initial bid was rejected by the ICI board and the majority of shareholders.
However, a subsequent bid for £8 billion (€11.82 billion) was accepted by ICI in August 2007, pending approval by regulators.
At 8a.m. on 2 January 2008, completion of the takeover of ICI plc by AkzoNobel was announced.
Shareholders of ICI received either £6.70 in cash or AkzoNobel loan notes to the value of £6.70 per one nominal ICI share.
The adhesives business of ICI was transferred to Henkel as a result of the deal,
while AkzoNobel agreed to sell its Crown Paints subsidiary to satisfy the concerns of the European Commissioner for Competition.
The areas of concern regarding the ICI UK pension scheme were addressed by ICI and AkzoNobel.
Operations.
ICI operated a number of chemical sites around the world.
In the UK, the main plants were as follows:
Argentina.
ICI subsidiary, called Duperial from 1928 to 1995, when it was renamed to ICI.
Established in the city of San Lorenzo, Santa Fe.
Operating an integrated production site with commercial offices in Buenos Aires.
Since 2009, called AkzoNobel Functional Chemicals S.A., and makes sulfuric acid with ISO certification.
Australia.
The subsidiary ICI Australia Ltd established the Dry Creek Saltfields at Dry Creek north of Adelaide, South Australia, in 1940, with an associated soda ash plant at nearby Osborne.
In 1989, these operations were sold to Penrice Soda Products.
An ICI plant was built at Botany Bay in New South Wales in the 1940s and was sold to Orica in 1997.
The plant once manufactured paints, plastics and industrial chemicals such as solvents.
It was responsible for the Botany Bay Groundwater Plume contamination of a local aquifer.

</doc>
<doc id="15357" url="https://en.wikipedia.org/wiki?curid=15357" title="Imperial Airways">
Imperial Airways

Imperial Airways was the early British commercial long-range air transport company, operating from 1924 to 1939 and serving parts of Europe but principally the British Empire routes to South Africa, India and the Far East, including Malaya and Hong Kong. There were local partnership companies; Qantas (Queensland and Northern Territory Aerial Services Ltd) in Australia and TEAL (Tasman Empire Airways Ltd) in New Zealand.
Imperial Airways was merged into the British Overseas Airways Corporation (BOAC) in 1939, which in turn merged with the British European Airways Corporation to form British Airways.
Background.
The establishment of Imperial Airways occurred in the context of facilitating overseas settlement by making travel to and from the colonies quicker, and that flight would also speed up colonial government and trade that was until then dependent upon ships. The launch of the airline followed a burst of air route surveying in the British Empire after the First World War, and after some experimental (and often dangerous) long-distance flying to the margins of Empire.
Formation.
Imperial Airways was created against a background of stiff competition from French and German airlines that enjoyed heavy government subsidies and following the advice of the government's Hambling Committee (formally known as the C.A.T Subsidies Committee). The committee produced a report on 15 February 1923 recommending that four of the largest existing airlines, The Instone Air Line Company, owned by shipping magnate Samuel Instone, Noel Pemberton Billing's British Marine Air Navigation (part of the Supermarine flying-boat company), the Daimler Airway, under the management of George Edward Woods and Handley Page Transport Co Ltd., should be merged.
It was hoped that this would create a company which could compete against French and German competition and would be strong enough to develop Britain's external air services while minimizing government subsidies for duplicated services. With this in view, a £1m subsidy over ten years was offered to encourage the merger. Agreement was made between the President of the Air Council and the British, Foreign and Colonial Corporation on 3 December 1923 for the company, under the title of the 'Imperial Air Transport Company' to acquire existing air transport services in the UK. The agreement set out the government subsidies for the new company: £137,000 in the first year diminishing to £32,000 in the tenth year as well as minimum mileages to be achieved and penalties if these weren't met.
Imperial Airways Limited was formed on 31 March 1924 with equipment from each contributing concern: British Marine Air Navigation Company Ltd, the Daimler Airway, Handley Page Transport Ltd and the Instone Air Line Ltd. The government had appointed two directors Hambling (who was also President of the Institute of Bankers) and Major J. W. Hills a former Treasury Financial Secretary.
The land operations were based at Croydon Airport to the south of London. IAL immediately discontinued its predecessors' service to points north of London, the airline being focused on international and imperial service rather than domestic. Thereafter the only IAL aircraft operating 'North of Watford' were charter flights.
Industrial troubles with the pilots delayed the start of services until 26 April 1924, when a daily London–Paris route was opened with a de Havilland DH.34. Thereafter the task of expanding the routes between England and the Continent began, with Southampton–Guernsey on 1 May 1924, London-Brussels–Cologne on 3 May, London–Amsterdam on 2 June 1924, and a summer service from London–Paris–Basle–Zürich on 17 June 1924. The first new airliner ordered by Imperial Airways, was the Handley Page W8f "City of Washington", delivered on 3 November 1924. In the first year of operation the company carried 11,395 passengers and 212,380 letters. In April 1925, the film "The Lost World" became the first film to be screened for passengers on a scheduled airliner flight when it was shown on the London-Paris route.
Empire services.
Route proving.
Between 16 November 1925 and 13 March 1926, Alan Cobham made an Imperial Airways’ route survey flight from the UK to Cape Town and back in the Armstrong Siddeley Jaguar–powered de Havilland DH.50J floatplane "G-EBFO". The outward route was London–Paris–Marseille–Pisa–Taranto–Athens–Sollum–Cairo–Luxor–Assuan–Wadi Halfa–Atbara–Khartoum–Malakal–Mongalla–Jinja–Kisumu–Tabora–Abercorn–Ndola–Broken Hill–Livingstone–Bulawayo–Pretoria–Johannesburg–Kimberley–Blomfontein–Cape Town. On his return Cobham was awarded the Air Force Cross for his services to aviation.
On 30 June 1926, Alan Cobham took off from the River Medway at Rochester in "G-EBFO" to make an Imperial Airways route survey for a service to Melbourne, arriving on 15 August 1926. He left Melbourne on 29 August 1926, and, after completing in 320 hours flying time over 78 days, he alighted on the Thames at Westminster on 1 October 1026. Cobham was met by the Secretary of State for Air, Sir Samuel Hoare, and was subsequently knighted by HM King George V.
On 27 December 1926, Imperial Airways de Havilland DH.66 Hercules "G-EBMX City of Delhi" left Croydon for a survey flight to India. The flight reached Karachi on 6 January 1927 and Delhi on 8 January 1927. The aircraft was named by Lady Irwin, wife of the Viceroy, on 10 January 1927. The return flight left on 1 February 1927 and arrived at Heliopolis, Cairo on 7 February 1927. The flying time from Croydon to Delhi was 62 hours 27 minutes and Delhi to Heliopolis 32 hours 50 minutes.
The Eastern Route.
Regular services on the Cairo to Basra route began on 12 January 1927 using DH.66 aircraft, replacing the previous RAF mail flight. Following 2 years of negotiations with the Persian authorities regarding overflight rights, a London to Karachi service started on 30 March 1929, taking 7 days and consisting of a flight from London to Basle, a train to Genoa and a Short S.8 Calcutta flying boats to Alexandria, a train to Cairo and finally a DH.66 flight to Karachi. The route was extended as far as Delhi on 29 December 1929. The route across Europe and the Mediterranean changed many times over the next few years but almost always involved a rail journey.
In April 1931 an experimental London-Australia air mail flight took place; the mail was transferred at the Dutch East Indies, and took 26 days in total to reach Sydney. For the passenger flight leaving London on 1 October 1932, the Eastern route was switched from the Persian to the Arabian side of the Persian Gulf, and Handley Page HP 42 airliners were introduced on the Cairo to Karachi sector. The move saw the establishment of an airport and rest house, Al Mahatta Fort, in the Trucial State of Sharjah now part of the United Arab Emirates.
On 29 May 1933 an England to Australia survey flight took off, operated by Imperial Airways Armstrong Whitworth Atalanta G-ABTL "Astraea". Major H G Brackley, Imperial Airways’ Air Superintendent, was in charge of the flight. "Astraea" flew Croydon-Paris-Lyons-Rome-Brindidsi-Athens-Alexandria-Cairo where it followed the normal route to Karachi then onwards to Jodhpur-Delhi-Calcutta-Akyab-Rangoon-Bangkok-Prachuab-Alor Star-Singapore-Palembang-Batavia-Sourabaya-Bima-Koepang-Bathurst Island-Darwin-Newcastle Waters-Camooweal-Cloncurry-Longreach-Roma-Toowoomba reaching Eagle Farm, Brisbane on 23 June. Sydney was visited on 26 June, Canberra on 28 June and Melbourne on 29 June.
There followed a rapid eastern extension. The first London to Calcutta service departed on 1 July 1933, the first London to Rangoon service on 23 September 1933, the first London to Singapore service on 9 December 1933, and the first London to Brisbane service on 8 December 1934, with QANTAS responsible for the Singapore to Brisbane sector. (The 1934 start was for mail; passenger flights to Brisbane began the following April.) The first London to Hong Kong passengers departed London on 14 March 1936 following the establishment of a branch from Penang to Hong Kong.
The Africa Route.
On 28 February 1931 a weekly service began between London and Mwanza on Lake Victoria in Tanganyika as part of the proposed route to Cape Town. On 9 December 1931 the Imperial Airways’ service for Central Africa was extended experimentally to Cape Town for the carriage of Christmas mail. The aircraft used on the last sector, DH66 G-AARY "City of Karachi" arrived in Cape Town on 21 December 1931. On 20 January 1932 a mail-only route to London to Cape Town was opened. On 27 April this route was opened to passengers and took 10 days. In early 1933 Atalantas replaced the DH.66s on the Kisumu to Cape Town sector of the London to Cape Town route. On 9 February 1936 the trans-Africa route was opened by Imperial Airways between Khartoum and Kano in Nigeria. This route was extended to Lagos on 15 October 1936.
Short Empire Flying Boats.
In 1937 with the introduction of Short Empire flying boats built at Short Brothers, Imperial Airways could offer a through-service from Southampton to the Empire. The journey to the Cape was via Marseille, Rome, Brindisi, Athens, Alexandria, Khartoum, Port Bell, Kisumu and onwards by land-based craft to Nairobi, Mbeya and eventually Cape Town. Survey flights were also made across the Atlantic and to New Zealand. By mid-1937 Imperial had completed its thousandth service to the Empire. Starting in 1938 Empire flying boats also flew between Britain and Australia via India and the Middle East.
In March 1939 three Shorts a week left Southampton for Australia, reaching Sydney after ten days of flying and nine overnight stops. Three more left for South Africa, taking six flying days to Durban.
Passengers.
Imperial's aircraft were small, most seating fewer than twenty passengers; about 50,000 passengers used Imperial Airways in the 1930s. Most passengers on intercontinental routes or on services within and between British colonies were men doing colonial administration, business or research. To begin with only the wealthy could afford to fly, but passenger lists gradually diversified. Travel experiences related to flying low and slow, and were reported enthusiastically in newspapers, magazines and books. There was opportunity for sightseeing from the air and at stops.
Crews.
Imperial Airways stationed its all-male flight deck crew, cabin crew and ground crew along the length of its routes. Specialist engineers and inspectors – and ground crew on rotation or leave – travelled on the airline without generating any seat revenue. Several air crew lost their lives in accidents. At the end of the 1930s crew numbers approximated 3,000. All crew were expected to be ambassadors for Britain and the British Empire.
Air Mail.
In 1934 the Government began negotiations with Imperial Airways to establish a service (Empire Air Mail Scheme) to carry mail by air on routes served by the airline. Indirectly these negotiations led to the dismissal in 1936 of Sir Christopher Bullock, the Permanent Under-Secretary at the Air Ministry, who was found by a Board of Inquiry to have abused his position in seeking a position on the board of the company while these negotiations were in train. The Government, including the Prime Minister, regretted the decision to dismiss him, later finding that, in fact, no corruption was alleged and sought Bullock's reinstatement which he declined.
The Empire Air Mail Programme started in July 1937, delivering anywhere for 1½ d./oz. By mid-1938 a hundred tons of mail had been delivered to India and a similar amount to Africa. In the same year, construction was started on the Empire Terminal in Victoria, London, designed by A. Lakeman and with a statue by Eric Broadbent, "Speed Wings Over the World" gracing the portal above the main entrance. From the terminal there were train connections to Imperial's flying boats at Southampton and coaches to its landplane base at Croydon Airport. The terminal operated as recently as 1980.
To help promote use of the Air Mail service, in June and July 1939, Imperial Airways participated with Pan American Airways in providing a special "around the world" service; Imperial carried the souvenir mail from Foynes, Ireland, to Hong Kong, out of the eastbound New York to New York route. Pan American provided service from New York to Foynes (departing 24 June, via the first flight of Northern FAM 18) and Hong Kong to San Francisco (via FAM 14), and United Airlines carried it on the final leg from San Francisco to New York, arriving on 28 July.
Captain H.W.C. Alger was the pilot for the inaugural air mail flight carrying mail from England to Australia for the first time on the Short Empire flyingboat "Castor" for Imperial Airways' Empires Air Routes, in 1937.
In November 2016, 80 years later, the Crete2Cape Vintage Air Rally will be flying this old route with fifteen vintage aeroplanes - a celebration of the skill and determination of these early aviators.
Aircraft.
Imperial Airways operated many types of aircraft from its formation on 1 April 1924 until 1 April 1940 when all aircraft still in service were transferred to BOAC.

</doc>
<doc id="15358" url="https://en.wikipedia.org/wiki?curid=15358" title="Insanity defense">
Insanity defense

The insanity defense, also known as the mental disorder defense, is a defense by excuse in criminal trials arguing that the defendant is not responsible for their actions due to an episodic or persistent psychiatric illness. Exemption from full criminal punishment on such grounds dates back to at least the Code of Hammurabi. Legal definitions of insanity or mental disorder are varied, and include the "M'Naghten Rules", the "Durham rule", the American Legal Institute definition, and other provisions, often relating to a lack of "mens rea" ("guilty mind"). In the criminal laws of Australia and Canada, statutory legislation enshrines the "M'Naghten Rules", with the terms defence of mental disorder, defence of mental illness or not criminally responsible by reason of mental disorder employed.
In the United Kingdom, Ireland, and the United States, use of the defense is rare; however, since the Criminal Procedure (Insanity and Unfitness to Plead) Act 1991, insanity pleas have steadily increased in the UK. Mitigating factors, including things not eligible for the insanity defense such as intoxication (or, more frequently, diminished capacity), may lead to reduced charges or reduced sentences.
The defense is based on evaluations by forensic mental health professionals with the appropriate test according to the jurisdiction. Their testimony guides the jury, but they are not allowed to testify to the accused's criminal responsibility, as this is a matter for the jury to decide. Similarly, mental health practitioners are restrained from making a judgment on the issue of whether the defendant is or is not insane or what is known as the "ultimate issue".
Some jurisdictions require the evaluation to address the defendant's ability to control their behavior at the time of the offense (the volitional limb). A defendant claiming the defense is pleading "not guilty by reason of insanity" (NGRI) or "guilty but insane or mentally ill" in some jurisdictions which, if successful, may result in the defendant being committed to a psychiatric facility for an indeterminate period.
Mitigating factors and diminished capacity.
The United States Supreme Court (in "Penry v. Lynaugh") and the United States Court of Appeals for the Fifth Circuit (in "Bigby v. Dretke") have been clear in their decisions that jury instructions in death penalty cases that do not ask about mitigating factors regarding the defendant's mental health violate the defendant's Eighth Amendment rights, saying that the jury is to be instructed to consider mitigating factors when answering unrelated questions. This ruling suggests specific explanations to the jury are necessary to weigh mitigating factors.
Diminished responsibility or diminished capacity can be employed as a mitigating factor or partial defense to crimes and, in the United States, is applicable to more circumstances than the insanity defense. The Homicide Act 1957 is the statutory basis for the defense of diminished responsibility in England & Wales, whereas in Scotland it is a product of case law. The number of findings of diminished responsibility has been matched by a fall in unfitness to plead and insanity findings (Walker, 1968). A plea of diminished capacity is different from a plea of insanity in that "reason of insanity" is a full defense while "diminished capacity" is merely a plea to a lesser crime.
Non compos mentis.
"Non compos mentis" (Latin) is a legal term meaning "not of sound mind". "Non compos mentis" derives from the Latin "non" meaning "not", "compos" meaning "having command" or "composed", and "mentis" (genitive singular of "mens"), meaning "of mind". It is the direct opposite of "Compos mentis" (of a sound mind).
Although typically used in law, this term can also be used metaphorically or figuratively; e.g. when one is in a confused state, intoxicated, or not of sound mind. The term may be applied when a determination of competency needs to be made by a physician for purposes of obtaining informed consent for treatments and, if necessary, assigning a surrogate to make health care decisions. While the proper sphere for this determination is in a court of law, this is practically, and most frequently, made by physicians in the clinical setting.
In English law, the rule of "non compos mentis" was most commonly used when the defendant invoked religious or magical explanations for behaviour.
Withdrawal or refusal of defense.
Several cases have ruled that persons found not guilty by reason of insanity may not withdraw the defense in a habeas petition to pursue an alternative, although there have been exceptions in other rulings. In State v. Connelly, 700 A.2d 694 (Conn. App. Ct. 1997), the petitioner who had originally been found not guilty by reason of insanity and committed for ten years to the jurisdiction of a Psychiatric Security Review Board, filed a pro se writ of "habeas corpus" and the court vacated his insanity acquittal. He was granted a new trial and found guilty of the original charges, receiving a prison sentence of 40 years.
In the landmark case of "Frendak v. United States" in 1979, the court ruled that the insanity defense cannot be imposed upon an unwilling defendant if an intelligent defendant voluntarily wishes to forego the defense.
Psychiatric treatments.
Those found to have been not guilty by reason of mental disorder or insanity are generally then required to undergo psychiatric treatment in a mental institution, except in the case of temporary insanity (see below). In England and Wales, under the Criminal Procedure (Insanity and Unfitness to Plead) Act of 1991 (amended by the Domestic Violence, Crime and Victims Act, 2004 to remove the option of a guardianship order), the court can mandate a hospital order, a restriction order (where release from hospital requires the permission of the Home Secretary), a "supervision and treatment" order, or an absolute discharge. Unlike defendants who are found guilty of a crime, they are not institutionalized for a fixed period, but rather held in the institution until they are determined not to be a threat. Authorities making this decision tend to be cautious, and as a result, defendants can often be institutionalized for longer than they would have been incarcerated in prison.
In "Foucha v. Louisiana" (1992) the Supreme Court of the United States ruled that a person could not be held "indefinitely".
So far, in the United States, those acquitted of a federal offense by reason of insanity have not been able to challenge their psychiatric confinement through a writ of habeas corpus or other remedies. In "Archuleta v. Hedrick", 365 F.3d 644 (8th Cir. 2004), the U.S. Court of Appeals for the Eighth Circuit the court ruled persons found not guilty by reason of insanity and later want to challenge their confinement may not attack their initial successful insanity defense:
Incompetency and mental illness.
An important distinction to be made is the difference between competency and criminal responsibility.
Competency largely deals with the defendant's present condition, while criminal responsibility addresses the condition at the time the crime was committed.
In the United States, a trial in which the insanity defense is invoked typically involves the testimony of psychiatrists or psychologists who will, as expert witnesses, present opinions on the defendant's state of mind at the time of the offense.
Therefore, a person whose mental disorder is not in dispute is determined to be sane if the court decides that despite a "mental illness" the defendant was responsible for the acts committed and will be treated in court as a normal defendant. If the person has a mental illness and it is determined that the mental illness interfered with the person's ability to determine right from wrong (and other associated criteria a jurisdiction may have) and if the person is willing to plead guilty or is proven guilty in a court of law, some jurisdictions have an alternative option known as either a Guilty but Mentally Ill (GBMI) or a Guilty but Insane verdict. The GBMI verdict is available as an alternative to, rather than in lieu of, a "not guilty by reason of insanity" verdict. Michigan (1975) was the first state to create a GBMI verdict, after two prisoners released after being found NGRI committed violent crimes within a year of release, one raping two women and the other killing his wife.
Temporary insanity.
The notion of temporary insanity argues that a defendant "was" insane, but is now sane, and is commonly used as a defense for crimes of passion. This defense was first used by U.S. Congressman Daniel Sickles of New York in 1859 after he had killed his wife's lover, Philip Barton Key, but was most used during the 1940s and 1950s. Another case around that time was that of Charles J. Guiteau, who assassinated President James Garfield in 1881. Guiteau's Case, 10 F.161 (1882).
History of the insanity defense.
The concept of defense by insanity has existed since ancient Greece and Rome. However, in colonial America a delusional Dorothy Talbye was hanged in 1638 for murdering her daughter, as at the time Massachusetts's common law made no distinction between insanity (or mental illness) and criminal behavior. Edward II, under English Common law, declared that a person was insane if their mental capacity was no more than that of a "wild beast" (in the sense of a dumb animal, rather than being frenzied). The first complete transcript of an insanity trial dates to 1724. It is likely that the insane, like those under 14, were spared trial by ordeal. When trial by jury replaced this, the jury members were expected to find the insane guilty but then refer the case to the King for a Royal Pardon. From 1500 onwards, juries could acquit the insane, and detention required a separate civil procedure (Walker, 1985). The Criminal Lunatics Act 1800, passed with retrospective effect following the acquittal of James Hadfield, mandated detention at the regent's pleasure (indefinitely) even for those who, although insane at the time of the offence, were now sane.
The M'Naghten Rules of 1843 were not a codification or definition of insanity but rather the responses of a panel of judges to hypothetical questions posed by Parliament in the wake of Daniel M'Naghten's acquittal for the homicide of Edward Drummond, whom he mistook for British Prime Minister Robert Peel. The rules define the defense as "at the time of committing the act the party accused was labouring under such a defect of reason, from disease of the mind, as not to know the nature and quality of the act he was doing, or as not to know that what he was doing was wrong." The key is that the defendant could not appreciate the nature of his actions during the commission of the crime. M'Naghten's Case, 8 Eng. Rep. 718 (1843).
In "Ford v. Wainwright" 477 U.S. 399 (1986), the US Supreme Court upheld the common law rule that the insane cannot be executed. It further stated that a person under the death penalty is entitled to a competency evaluation and to an evidentiary hearing in court on the question of his competency to be executed.
In "Wainwright v. Greenfield", the Court ruled that it was fundamentally unfair for the prosecutor to comment during the court proceedings on the petitioner's silence invoked as a result of a Miranda warning. The prosecutor had argued that the respondent's silence after receiving Miranda warnings was evidence of his sanity.
Controversy over the insanity defense.
The insanity plea is used in the U.S Criminal Justice System in less than 1% of all criminal cases. Little is known about the criminal justice system and the mentally ill:
Some U.S. states have begun to ban the use of the insanity defense, and in 1994 the Supreme Court denied a petition of certiorari seeking review of a Montana Supreme Court case that upheld Montana's abolition of the defense. Idaho, Kansas, and Utah have also banned the defense. However, a mentally ill defendant/patient can be found unfit to stand trial in these states. In 2001, the Nevada Supreme Court found that their state's abolition of the defense was unconstitutional as a violation of Federal due process. In 2006, the Supreme Court decided "Clark v. Arizona" upheld Arizona's limitations on the insanity defense. In that same ruling, the Court noted "We have never held that the Constitution mandates an insanity defense, nor have we held that the Constitution does not so require."
The insanity defense is also complicated because of the underlying differences in philosophy between psychiatrists/psychologists and legal professionals. In the United States, a psychiatrist, psychologist or other mental health professional is often consulted as an expert witness in insanity cases, but the ultimate "legal" judgment of the defendant's sanity is determined by a jury, not by a psychologist. In other words, psychologists provide testimony and professional opinion but are not ultimately responsible for answering legal questions.
United States law.
In the United States, variances in the insanity defense between states, and in the federal court system, are attributable to differences with respect to three key issues: (1) Whether to provide the insanity defense, (2) how to define "insanity," and (3) the burden of proof.
Federal and state availability.
In the United States, a criminal defendant may plead insanity in federal court, and in the state courts of every state except for Idaho, Kansas, Montana, and Utah.
Definition.
Each state and the federal court system currently uses one of the following "tests" to define insanity for purposes of the insanity defense:
M'Naghten test.
The guidelines for the "M'Naghten Rules" (1843) 10 C & F 200, state, "inter alia", and evaluating the criminal responsibility for defendants claiming to be insane were settled in the British courts in the case of Daniel M'Naghten in 1843. M'Naghten was a Scottish woodcutter who killed the secretary to the prime minister, Edward Drummond, in a botched attempt to assassinate the prime minister himself. M'Naghten apparently believed that the prime minister was the architect of the myriad of personal and financial misfortunes that had befallen him. During his trial, nine witnesses testified to the fact that he was insane, and the jury acquitted him, finding him "not guilty by reason of insanity."
The House of Lords asked the judges of the common law courts to answer five questions on insanity as a criminal defence, and the formulation that emerged from their review—that a defendant should not be held responsible for his actions only if, as a result of his mental disease or defect, he (i) did not know that his act would be wrong; or (ii) did not understand the nature and quality of his actions—became the basis of the law governing legal responsibility in cases of insanity in England. Under the rules, loss of control because of mental illness was no defense. The M'Naghten rule was embraced with almost no modification by American courts and legislatures for more than 100 years, until the mid-20th century. M'Naghten's Case, 8 Eng. Rep. 718 (1843).
"Durham"/New Hampshire test.
The strict M'Naghten standard for the insanity defense was used until the 1950s and the "Durham v. United States" case. In the Durham case, the court ruled that a defendant is entitled to acquittal if the crime was the "product of" his mental illness (i.e., crime would not have been committed but for the disease). The test, also called the Product Test, is broader than either the M'Naghten test or the irresistible impulse test. The test has much more lenient guideline for the insanity defense, but it addressed the issue of convicting mentally ill defendants, which was allowed under the M'Naghten Rule. M'Naghten's Case, 8 Eng.Rep. 718 (1843). However, the Durham standard drew much criticism because of its expansive definition of legal insanity.
Model Penal Code test.
The Model Penal Code, published by the American Law Institute, provides a standard for legal insanity that serves as a compromise between the strict M'Naghten Rule, the lenient Durham ruling, and the irresistible impulse test. Under the MPC standard, which represents the modern trend, a defendant is not responsible for criminal conduct "if at the time of such conduct as a result of mental disease or defect he lacks "substantial capacity" either to appreciate the criminality of his conduct or to conform his conduct to the requirements of the law." The test thus takes into account both the cognitive and volitional capacity of insanity.
Insanity Defense Reform Act of 1984.
After the perpetrator of President Reagan's assassination attempt was found not guilty by reason of insanity, Congress passed the Insanity Defense Reform Act of 1984. Under this act, the burden of proof was shifted from the prosecution to the defense and the standard of evidence in federal trials was increased from a preponderance of evidence to clear and convincing evidence. The ALI test was discarded in favor of a new test that more closely resembled M'Naghten's. Under this new test only perpetrators suffering from severe mental illnesses at the time of the crime could successfully employ the insanity defense. The defendant's ability to control himself or herself was no longer a consideration.
The Act also curbed the scope of expert psychiatric testimony and adopted stricter procedures regarding the hospitalization and release of those who found not guilty by reason of insanity.
Additional reforms have taken place besides the major Insanity Defense Reform Act, including the addition of the GBMI (Guilty but Mentally Ill) option to trial, changes in the burden and/or standard of proof in a trial, changes in the test of insanity or in the entering of the plea all together, various alterations in the trial procedures, and changes in commitment and release procedures after the trial has been complete.
Burden of proof.
In a majority of states, the burden is placed on the defendant, who must prove insanity by a preponderance of the evidence.
In a minority of states, the burden is placed on the prosecution, who must prove sanity beyond a reasonable doubt.
In federal court, and in Arizona, the burden is placed on the defendant, who must prove insanity by clear and convincing evidence. See 18 U.S.C.S. Sec. 17(b); see also A.R.S. Sec. 13-502(C).
Australian law.
In Australia there are nine law units. All may have varying rules (see [http://www.aija.org.au/ac03/papers/DavidGrace.rtf]). In South Australia, the Criminal Law Consolidation Act 1935 (SA) provides that:
269C—Mental competence
A person is mentally incompetent to commit an offence if, at the time of the conduct alleged to give rise to the offence, the person is suffering from a mental impairment and, in consequence of the mental impairment—
269H—Mental unfitness to stand trial
A person is mentally unfit to stand trial on a charge of an offence if the person's mental processes are so disordered or impaired that the person is—
In Victoria the current defence of mental impairment was introduced in the Crimes (Mental Impairment and Unfitness to be Tried) Act 1997 which replaced the common law defence of insanity and indefinite detention at the governor's pleasure with the following:
These requirements are almost identical to the M'Naghten Rules, substituting "mental impairment" for "disease of the mind".
In New South Wales, the defence has been renamed the 'Defence of Mental Illness' in Pt4 of the Mental Health (Forensic Provisions) Act 1990. However, definitions of the defence are derived from M'Naghten's case and have not been codified. Whether a particular condition amounts to a disease of the mind is not a medical but a legal question to be decided in accordance with the ordinary rules of interpretation. This defence is an exception to the Woolmington v DPP (1935) 'golden thread', as the party raising the issue of the defence of mental illness bears the burden of proving this defence on the balance of probabilities See [http://www.austlii.edu.au/au/cases/cth/HCA/1933/1.html "R v Porter". Generally, the defence will raise the issue of insanity. However, the prosecution can raise it in exceptional circumstances: "R v Ayoub (1984)." 
Australian cases have further qualified an explained the "M'Naghten Rules". The NSW Supreme Court in held there are two limbs to the "M'Naghten Rules", that the accused did not know what he was doing, or that the accused did not appreciate that what he was doing was morally wrong, in both cases the accused must be operating under a 'defect of reason, from a disease of the mind'. The High Court in "R v Porter" stated that the condition of the accused’s mind is relevant only at the time of the actus reus. In "Woodbridge v The Queen" the court stated that a symptom indicating a disease of the mind must be prone to recur and be the result of an underlying pathological infirmity. A ‘defect of reason’ is the inability to think rationally and pertains to incapacity to reason, rather than having unsound ideas or difficulty with such a task. Examples of disease of the mind include Arteriosclerosis (considered so because the hardening of the arteries affects the mind. See " See also.
Canadian law.
Criminal Code provisions.
The defence of mental disorder is codified in section 16 of the Criminal Code which states, in part:
To establish a claim of mental disorder the party raising the issue must show on a balance of probabilities first that the person who committed the act was suffering from a "disease of the mind", and second, that at the time of the offence they were either 1) unable to appreciate the "nature and quality" of the act, or 2) did not know it was "wrong".
The meaning of the word "wrong" was determined in the Supreme Court case of "R. v. Chaulk" which held that "wrong" was NOT restricted to "legally wrong" but to "morally wrong" as well.
Post-verdict conditions.
The current legislative scheme was created by the Parliament of Canada after the previous scheme was found unconstitutional by the Supreme Court of Canada in "R. v. Swain". The new provisions also replaced the old insanity defence with the current mental disorder defence.
Once a person is found not criminally responsible ("NCR"), he or she will have a hearing by a Review Board within 45 days (90 days if the court extends the delay). A Review Board is established under Part XX.1 of the Criminal Code and is composed of at least three members, a person who is a judge or eligible to be a judge, a psychiatrist and another expert in a relevant field, such as social work, criminology or psychology. Parties at a Review Board hearing are usually the accused, the Crown and the hospital responsible for the supervision or assessment of the accused. A Review Board is responsible for both accused persons found NCR or accused persons found unfit to stand trial on account of mental disorder. A Review Board dealing with an NCR offender must consider two questions: whether the accused is a "significant threat to the safety of the public" and, if so, what the "least onerous and least restrictive" restrictions on the liberty of the accused should be in order to mitigate such a threat. Proceedings before a Review Board are inquisitorial rather than adversarial. Often the Review Board will be active in conducting an inquiry. Where the Review Board is unable to conclude that the accused is a significant threat to the safety of the public, the review board must grant the accused an absolute discharge, an order essentially terminating the jurisdiction of the criminal law over the accused. Otherwise the Review Board must order that the accused be either discharged subject to conditions or detained in a hospital, both subject to conditions. The conditions imposed must be the least onerous and least restrictive necessary to mitigate any danger the accused may pose to others.
Since the Review Board is empowered under criminal law powers under s. 91(27) of the Constitution Act, 1867 the sole justification for its jurisdiction is public safety. Therefore, the nature of the inquiry is the danger the accused may pose to the public safety rather than whether the accused is "cured." For instance, many "sick" accused persons are discharged absolutely on the basis that they are not a danger to the public while many "sane" accused are detained on the basis that they are dangerous. Moreover, the notion of "significant threat to the safety of the public" is a "criminal threat." This means that the Review Board must find that the threat posed by the accused is of a criminal nature.
While proceedings before a Review Board are less formal than in court, there are many procedural safe-guards available to the accused given the potential indefinite nature of Part XX.1. Any party may appeal against the decision of a Review Board.
In 1992 when the new mental disorder provisions were enacted, Parliament included "capping" provisions which were to be enacted at a later date. These capping provisions limited the jurisdiction of a Review Board over an accused based on the maximum potential sentence had the accused been convicted (e.g. there would be a cap of 5 years if the maximum penalty for the index offence is 5 years). However, these provisions were never proclaimed into force and were subsequently repealed.
A Review Board must hold a hearing every 12 months (unless extended to 24 months) until the accused is discharged absolutely.
Accused unfit to stand trial.
The issue of mental disorder may also come into play before a trial even begins, if the accused's mental state prevents the accused from being able to appreciate the nature of a trial and to conduct a defence.
An accused who is found to be unfit to stand trial is subject to the jurisdiction a Review Board. While the considerations are essentially the same, there are a few provisions which apply only to unfit accused. A Review Board must determine whether the accused is fit to stand trial. Regardless of the determination, the Review Board must then determine what conditions should be imposed on the accused, considering both the protection of the public and the maintenance of the fitness of the accused (or conditions which would render the accused fit). Previously an absolute discharge was unavailable to an unfit accused. However, in R. v. Demers, the Supreme Court of Canada struck down the provision restricting the availability of an absolute discharge to an accused person who is deemed both "permanently unfit" and not a significant threat to the safety of the public. Presently a Review Board may recommend a judicial stay of proceedings in the event that it finds the accused both "permanently unfit" and non-dangerous. The decision is left to the court having jurisdiction over the accused.
An additional requirement for an unfit accused is the holding of a "prima facie case" hearing every two years. The Crown must demonstrate to the court having jurisdiction over the accused that it still has sufficient evidence to try the accused. If the Crown fails to meet this burden then the accused is discharged and proceedings are terminated. The nature of the hearing is virtually identical to that of a preliminary hearing.
Scottish law.
The Scottish Law Commission, in its Discussion Paper No 122 on Insanity and Diminished Responsibility (2003), pp. 16/18 confirms that the law has not substantially changed from the position stated in Hume's Commentaries:
The phrase "absolute alienation of reason" is still regarded as at the core of the defense in the modern law (see "HM Advocate v Kidd" (1960) JC 61 and "Brennan v HM Advocate" (1977)
Nordic countries.
In the Nordic countries, insanity is not a defense; instead, it is the responsibility of the court system as such to consider whether the accused may have been psychotic or suffering from other severe mental defects when perpetrating the criminal act. This explains why, in Norway, the court considered the sanity of Anders Behring Breivik, even if he himself declared to be sane.
Rules differ between Nordic countries.
In Sweden, psychotic perpetrators are seen as accountable, but the sanction is to forensic mental care.
In Denmark and Norway, psychotic perpetrators are declared guilty, but not punished. Instead of prison, they are sentenced to mandatory treatment. Still, important differences exist between Norway and Denmark.
In Norway, §44 of the penal code states specifically that "a person who at the time of the crime was insane or unconscious is not punished".
In Denmark, §16 of the penal code states that "Persons, who, at the time of the act, were irresponsible owing to mental illness or similar conditions or
to a pronounced mental deficiency, are not punishable". This means that in Denmark, 'insanity' is a legal term rather than a medical term and that the court retains the authority to decide whether an accused person is irresponsible or not.
In Finland, punishments can only be administered if the accused is "compos mentis", of sound mind; not if the accused is insane ("syyntakeeton", literally "unable to guarantee the responsibility of guilt"). Thus, an insane defendant may be found guilty based on the facts and his actions just as a sane defendant, but the insanity will only affect the punishment. The definition of insanity is similar to the M'Naught criterion above: "the accused is insane, if during the act, due to a mental illness, profound mental retardation or a severe disruption of mental health or consciousness, he cannot understand the actual nature of his act or its illegality, or that his ability to control his behavior is critically weakened". If an accused is suspected to be insane, the court must consult the National Institute for Health and Welfare (THL), which is obliged to place the accused in involuntary commitment if he is found insane. The offender receives no judicial punishment; he becomes a patient under the jurisdiction of THL, and must be released immediately once the conditions of involuntary commitment are no longer fulfilled. Diminished responsibility is also available, resulting in lighter sentences.
Usage and success rate.
This increased coverage gives the impression that the defense is widely used, but this is not the case. According to an eight-state study, the insanity defense is used in less than 1% of all court cases and, when used, has only a 26% success rate. Of those cases that were successful, 90% of the defendants had been previously diagnosed with mental illness.

</doc>
<doc id="15361" url="https://en.wikipedia.org/wiki?curid=15361" title="Ice age">
Ice age

An ice age is a period of long-term reduction in the temperature of Earth's surface and atmosphere, resulting in the presence or expansion of continental and polar ice sheets and alpine glaciers. Within a long-term ice age, individual pulses of cold climate are termed "glacial periods" (or alternatively "glacials" or "glaciations" or colloquially as "ice age"), and intermittent warm periods are called "interglacials". Glaciologically, "ice age" implies the presence of extensive ice sheets in the northern and southern hemispheres. By this definition, we are in an interglacial period—the Holocene—of the ice age that began 2.6 million years ago at the start of the Pleistocene epoch, because the Greenland, Arctic, and Antarctic ice sheets still exist.
Origin of ice age theory.
In 1742 Pierre Martel (1706–1767), an engineer and geographer living in Geneva, visited the valley of Chamonix in the Alps of Savoy. Two years later he published an account of his journey. He reported that the inhabitants of that valley attributed the dispersal of erratic boulders to the glaciers, saying that they had once extended much farther. Later similar explanations were reported from other regions of the Alps. In 1815 the carpenter and chamois hunter Jean-Pierre Perraudin (1767–1858) explained erratic boulders in the Val de Bagnes in the Swiss canton of Valais as being due to glaciers previously extending further. An unknown woodcutter from Meiringen in the Bernese Oberland advocated a similar idea in a discussion with the Swiss-German geologist Jean de Charpentier (1786–1855) in 1834. Comparable explanations are also known from the Val de Ferret in the Valais and the Seeland in western Switzerland and in Goethe's scientific work. Such explanations could also be found in other parts of the world. When the Bavarian naturalist Ernst von Bibra (1806–1878) visited the Chilean Andes in 1849–1850, the natives attributed fossil moraines to the former action of glaciers.
Meanwhile, European scholars had begun to wonder what had caused the dispersal of erratic material. From the middle of the 18th century, some discussed ice as a means of transport. The Swedish mining expert Daniel Tilas (1712–1772) was, in 1742, the first person to suggest drifting sea ice in order to explain the presence of erratic boulders in the Scandinavian and Baltic regions. In 1795, the Scottish philosopher and gentleman naturalist, James Hutton (1726–1797), explained erratic boulders in the Alps by the action of glaciers. Two decades later, in 1818, the Swedish botanist Göran Wahlenberg (1780–1851) published his theory of a glaciation of the Scandinavian peninsula. He regarded glaciation as a regional phenomenon. 
Only a few years later, the Danish-Norwegian geologist Jens Esmark (1762–1839) argued a sequence of worldwide ice ages. In a paper published in 1824, Esmark proposed changes in climate as the cause of those glaciations. He attempted to show that they originated from changes in Earth's orbit. During the following years, Esmark's ideas were discussed and taken over in parts by Swedish, Scottish and German scientists. At the University of Edinburgh Robert Jameson (1774–1854) seemed to be relatively open to Esmark's ideas, as reviewed by Norwegian professor of glaciology Bjørn G. Andersen (1992). Jameson's remarks about ancient glaciers in Scotland were most probably prompted by Esmark.</ref> In Germany, Albrecht Reinhard Bernhardi (1797–1849), a geologist and professor of forestry at an academy in Dreissigacker, since incorporated in the southern Thuringian city of Meiningen, adopted Esmark's theory. In a paper published in 1832, Bernhardi speculated about former polar ice caps reaching as far as the temperate zones of the globe.
In 1829, independently of these debates, the Swiss civil engineer Ignaz Venetz (1788–1859) explained the dispersal of erratic boulders in the Alps, the nearby Jura Mountains, and the North German Plain as being due to huge glaciers. When he read his paper before the Schweizerische Naturforschende Gesellschaft, most scientists remained sceptical. Finally, Venetz convinced his friend Jean de Charpentier. De Charpentier transformed Venetz's idea into a theory with a glaciation limited to the Alps. His thoughts resembled Wahlenberg's theory. In fact, both men shared the same volcanistic, or in de Charpentier's case rather plutonistic assumptions, about the earth's history. In 1834, de Charpentier presented his paper before the Schweizerische Naturforschende Gesellschaft. In the meantime, the German botanist Karl Friedrich Schimper (1803–1867) was studying mosses which were growing on erratic boulders in the alpine upland of Bavaria. He began to wonder where such masses of stone had come from. During the summer of 1835 he made some excursions to the Bavarian Alps. Schimper came to the conclusion that ice must have been the means of transport for the boulders in the alpine upland. In the winter of 1835 to 1836 he held some lectures in Munich. Schimper then assumed that there must have been global times of obliteration ("Verödungszeiten") with a cold climate and frozen water. Schimper spent the summer months of 1836 at Devens, near Bex, in the Swiss Alps with his former university friend Louis Agassiz (1801–1873) and Jean de Charpentier. Schimper, de Charpentier and possibly Venetz convinced Agassiz that there had been a time of glaciation. During Winter 1836/7 Agassiz and Schimper developed the theory of a sequence of glaciations. They mainly drew upon the preceding works of Venetz, de Charpentier and on their own fieldwork. Agassiz appears to have been already familiar with Bernhardi's paper at that time. At the beginning of 1837, Schimper coined the term "ice age" (""Eiszeit"") for the period of the glaciers. In July 1837 Agassiz presented their synthesis before the annual meeting of the Schweizerische Naturforschende Gesellschaft at Neuchâtel. The audience was very critical and some opposed to the new theory because it contradicted the established opinions on climatic history. Most contemporary scientists thought that the earth had been gradually cooling down since its birth as a molten globe.
In order to overcome this rejection, Agassiz embarked on geological fieldwork. He published his book "Study on Glaciers" ("Études sur les glaciers") in 1840. De Charpentier was put out by this, as he had also been preparing a book about the glaciation of the Alps. De Charpentier felt that Agassiz should have given him precedence as it was he who had introduced Agassiz to in-depth glacial research. Besides that, Agassiz had, as a result of personal quarrels, omitted any mention of Schimper in his book.
All together, it took several decades until the ice age theory was fully accepted by scientists. This happened on an international scale in the second half of the 1870s following the work of James Croll, including the publication of "Climate and Time, in Their Geological Relations" in 1875, which provided a credible explanation for the causes of ice ages.
Evidence for ice ages.
There are three main types of evidence for ice ages: geological, chemical, and paleontological.
Geological evidence for ice ages comes in various forms, including rock scouring and scratching, glacial moraines, drumlins, valley cutting, and the deposition of till or tillites and glacial erratics. Successive glaciations tend to distort and erase the geological evidence, making it difficult to interpret. Furthermore, this evidence was difficult to date exactly; early theories assumed that the glacials were short compared to the long interglacials. The advent of sediment and ice cores revealed the true situation: glacials are long, interglacials short. It took some time for the current theory to be worked out.
The chemical evidence mainly consists of variations in the ratios of isotopes in fossils present in sediments and sedimentary rocks and ocean sediment cores. For the most recent glacial periods ice cores provide climate proxies from their ice, and atmospheric samples from included bubbles of air. Because water containing heavier isotopes has a higher heat of evaporation, its proportion decreases with colder conditions. This allows a temperature record to be constructed. However, this evidence can be confounded by other factors recorded by isotope ratios.
The paleontological evidence consists of changes in the geographical distribution of fossils. During a glacial period cold-adapted organisms spread into lower latitudes, and organisms that prefer warmer conditions become extinct or are squeezed into lower latitudes. This evidence is also difficult to interpret because it requires (1) sequences of sediments covering a long period of time, over a wide range of latitudes and which are easily correlated; (2) ancient organisms which survive for several million years without change and whose temperature preferences are easily diagnosed; and (3) the finding of the relevant fossils.
Despite the difficulties, analysis of ice core and ocean sediment cores has shown periods of glacials and interglacials over the past few million years. These also confirm the linkage between ice ages and continental crust phenomena such as glacial moraines, drumlins, and glacial erratics. Hence the continental crust phenomena are accepted as good evidence of earlier ice ages when they are found in layers created much earlier than the time range for which ice cores and ocean sediment cores are available.
Major ice ages.
There have been at least five major ice ages in the earth's past (the Huronian, Cryogenian, Andean-Saharan, Karoo Ice Age and the Quaternary glaciation). Outside these ages, the Earth seems to have been ice-free even in high latitudes.
Rocks from the earliest well established ice age, called the Huronian, formed around 2.4 to 2.1 Ga (billion years) ago during the early Proterozoic Eon. Several hundreds of km of the Huronian Supergroup are exposed 10–100 km north of the north shore of Lake Huron extending from near Sault Ste. Marie to Sudbury, northeast of Lake Huron, with giant layers of now-lithified till beds, dropstones, varves, outwash, and scoured basement rocks. Correlative Huronian deposits have been found near Marquette, Michigan, and correlation has been made with Paleoproterozoic glacial deposits from Western Australia.
The next well-documented ice age, and probably the most severe of the last billion years, occurred from 850 to 630 million years ago (the Cryogenian period) and may have produced a Snowball Earth in which glacial ice sheets reached the equator, possibly being ended by the accumulation of greenhouse gases such as produced by volcanoes. "The presence of ice on the continents and pack ice on the oceans would inhibit both silicate weathering and photosynthesis, which are the two major sinks for at present." It has been suggested that the end of this ice age was responsible for the subsequent Ediacaran and Cambrian explosion, though this model is recent and controversial.
The Andean-Saharan occurred from 460 to 420 million years ago, during the Late Ordovician and the Silurian period.
The evolution of land plants at the onset of the Devonian period caused a long term increase in planetary oxygen levels and reduction of levels, which resulted in the Karoo Ice Age. It is named after the glacial tills found in the Karoo region of South Africa, where evidence for this ice age was first clearly identified. There were extensive polar ice caps at intervals from 360 to 260 million years ago in South Africa during the Carboniferous and early Permian Periods. Correlatives are known from Argentina, also in the center of the ancient supercontinent Gondwanaland.
The current ice age, the Pliocene-Quaternary glaciation, started about 2.58 million years ago during the late Pliocene, when the spread of ice sheets in the Northern Hemisphere began. Since then, the world has seen cycles of glaciation with ice sheets advancing and retreating on 40,000- and 100,000-year time scales called glacial periods, glacials or glacial advances, and interglacial periods, interglacials or glacial retreats. The earth is currently in an interglacial, and the last glacial period ended about 10,000 years ago. All that remains of the continental ice sheets are the Greenland and Antarctic ice sheets and smaller glaciers such as on Baffin Island.
Ice ages can be further divided by location and time; for example, the names "Riss" (180,000–130,000 years bp) and "Würm" (70,000–10,000 years bp) refer specifically to glaciation in the Alpine region. The maximum extent of the ice is not maintained for the full interval. The scouring action of each glaciation tends to remove most of the evidence of prior ice sheets almost completely, except in regions where the later sheet does not achieve full coverage.
Glacials and interglacials.
Within the ice ages (or at least within the current one), more temperate and more severe periods occur. The colder periods are called "glacial periods", the warmer periods "interglacials", such as the Eemian Stage.
Glacials are characterized by cooler and drier climates over most of the earth and large land and sea ice masses extending outward from the poles. Mountain glaciers in otherwise unglaciated areas extend to lower elevations due to a lower snow line. Sea levels drop due to the removal of large volumes of water above sea level in the icecaps. There is evidence that ocean circulation patterns are disrupted by glaciations. Since the earth has significant continental glaciation in the Arctic and Antarctic, we are currently in a glacial minimum of a glaciation. Such a period between glacial maxima is known as an "interglacial". The glacials and interglacials also coincided with changes in Earth's orbit called Milankovitch cycles.
The earth has been in an interglacial period known as the Holocene for more than 11,000 years. It was conventional wisdom that the typical interglacial period lasts about 12,000 years, but this has been called into question recently. For example, an article in "Nature" argues that the current interglacial might be most analogous to a previous interglacial that lasted 28,000 years. Predicted changes in orbital forcing suggest that the next glacial period would begin at least 50,000 years from now, even in absence of human-made global warming (see Milankovitch cycles). Moreover, anthropogenic forcing from increased greenhouse gases might outweigh orbital forcing for as long as intensive use of fossil fuels continues.
Positive and negative feedback in glacial periods.
Each glacial period is subject to positive feedback which makes it more severe, and negative feedback which mitigates and (in all cases so far) eventually ends it.
Positive feedback processes.
Ice and snow increase Earth's albedo, i.e. they make it reflect more of the sun's energy and absorb less. Hence, when the air temperature decreases, ice and snow fields grow, and this continues until competition with a negative feedback mechanism forces the system to an equilibrium. Also, the reduction in forests caused by the ice's expansion increases albedo.
Another theory proposed by Ewing and Donn in 1956 hypothesized that an ice-free Arctic Ocean leads to increased snowfall at high latitudes. When low-temperature ice covers the Arctic Ocean there is little evaporation or sublimation and the polar regions are quite dry in terms of precipitation, comparable to the amount found in mid-latitude deserts. This low precipitation allows high-latitude snowfalls to melt during the summer. An ice-free Arctic Ocean absorbs solar radiation during the long summer days, and evaporates more water into the Arctic atmosphere. With higher precipitation, portions of this snow may not melt during the summer and so glacial ice can form at lower altitudes "and" more southerly latitudes, reducing the temperatures over land by increased albedo as noted above. Furthermore, under this hypothesis the lack of oceanic pack ice allows increased exchange of waters between the Arctic and the North Atlantic Oceans, warming the Arctic and cooling the North Atlantic. (Current projected consequences of global warming include a largely ice-free Arctic Ocean within 5–20 years, see Arctic shrinkage.) Additional fresh water flowing into the North Atlantic during a warming cycle may also reduce the global ocean water circulation. Such a reduction (by reducing the effects of the Gulf Stream) would have a cooling effect on northern Europe, which in turn would lead to increased low-latitude snow retention during the summer. It has also been suggested that during an extensive glacial, glaciers may move through the Gulf of Saint Lawrence, extending into the North Atlantic ocean far enough to block the Gulf Stream.
Negative feedback processes.
Ice sheets that form during glaciations cause erosion of the land beneath them. After some time, this will reduce land above sea level and thus diminish the amount of space on which ice sheets can form. This mitigates the albedo feedback, as does the lowering in sea level that accompanies the formation of ice sheets.
Another factor is the increased aridity occurring with glacial maxima, which reduces the precipitation available to maintain glaciation. The glacial retreat induced by this or any other process can be amplified by similar inverse positive feedbacks as for glacial advances.
According to research published in "Nature Geoscience", human emissions of carbon dioxide will defer the next ice age. Researchers used data on Earth's orbit to find the historical warm interglacial period that looks most like the current one and from this have predicted that the next ice age would usually begin within 1,500 years. They go on to say that emissions have been so high that it will not.
Causes of ice ages.
The causes of ice ages are not fully understood for either the large-scale ice age periods or the smaller ebb and flow of glacial–interglacial periods within an ice age. The consensus is that several factors are important: atmospheric composition, such as the concentrations of carbon dioxide and methane (the specific levels of the previously mentioned gases are now able to be seen with the new ice core samples from EPICA Dome C in Antarctica over the past 800,000 years) changes in the earth's orbit around the Sun known as Milankovitch cycles, the motion of tectonic plates resulting in changes in the relative location and amount of continental and oceanic crust on the earth's surface, which affect wind and ocean currents, variations in solar output, the orbital dynamics of the Earth–Moon system, the impact of relatively large meteorites and volcanism including eruptions of supervolcanoes.
Some of these factors influence each other. For example, changes in Earth's atmospheric composition (especially the concentrations of greenhouse gases) may alter the climate, while climate change itself can change the atmospheric composition (for example by changing the rate at which weathering removes ).
Maureen Raymo, William Ruddiman and others propose that the Tibetan and Colorado Plateaus are immense "scrubbers" with a capacity to remove enough from the global atmosphere to be a significant causal factor of the 40 million year Cenozoic Cooling trend. They further claim that approximately half of their uplift (and "scrubbing" capacity) occurred in the past 10 million years.
Changes in Earth's atmosphere.
There is considerable evidence that over the very recent period of the last 100–1000 years, the sharp increases in human activity, especially the burning of fossil fuels, has caused the parallel sharp and accelerating increase in atmospheric greenhouse gases which trap the sun's heat. The consensus theory of the scientific community is that the resulting greenhouse effect is a principal cause of the increase in global warming which has occurred over the same period, and a chief contributor to the accelerated melting of the remaining glaciers and polar ice. A 2012 investigation finds that dinosaurs released methane through digestion in a similar amount to humanity's current methane release, which "could have been a key factor" to the very warm climate 150 million years ago.
There is evidence that greenhouse gas levels fell at the start of ice ages and rose during the retreat of the ice sheets, but it is difficult to establish cause and effect (see the notes above on the role of weathering). Greenhouse gas levels may also have been affected by other factors which have been proposed as causes of ice ages, such as the movement of continents and volcanism.
The Snowball Earth hypothesis maintains that the severe freezing in the late Proterozoic was ended by an increase in levels in the atmosphere, and some supporters of Snowball Earth argue that it was caused by a reduction in atmospheric . The hypothesis also warns of future Snowball Earths.
In 2009, further evidence was provided that changes in solar insolation provide the initial trigger for the earth to warm after an Ice Age, with secondary factors like increases in greenhouse gases accounting for the magnitude of the change.
William Ruddiman has proposed the early anthropocene hypothesis, according to which the anthropocene era, as some people call the most recent period in the earth's history when the activities of the human species first began to have a significant global impact on the earth's climate and ecosystems, did not begin in the 18th century with the advent of the Industrial Era, but dates back to 8,000 years ago, due to intense farming activities of our early agrarian ancestors. It was at that time that atmospheric greenhouse gas concentrations stopped following the periodic pattern of the Milankovitch cycles. In his overdue-glaciation hypothesis Ruddiman states that an incipient glacial would probably have begun several thousand years ago, but the arrival of that scheduled glacial was forestalled by the activities of early farmers.
At a meeting of the American Geophysical Union (December 17, 2008), scientists detailed evidence in support of the controversial idea that the introduction of large-scale rice agriculture in Asia, coupled with extensive deforestation in Europe began to alter world climate by pumping significant amounts of greenhouse gases into the atmosphere over the last 1,000 years. In turn, a warmer atmosphere heated the oceans making them much less efficient storehouses of carbon dioxide and reinforcing global warming, possibly forestalling the onset of a new glacial age.
Position of the continents.
The geological record appears to show that ice ages start when the continents are in positions which block or reduce the flow of warm water from the equator to the poles and thus allow ice sheets to form. The ice sheets increase Earth's reflectivity and thus reduce the absorption of solar radiation. With less radiation absorbed the atmosphere cools; the cooling allows the ice sheets to grow, which further increases reflectivity in a positive feedback loop. The ice age continues until the reduction in weathering causes an increase in the greenhouse effect.
There are three known configurations of the continents which block or reduce the flow of warm water from the equator to the poles:
Since today's Earth has a continent over the South Pole and an almost land-locked ocean over the North Pole, geologists believe that Earth will continue to experience glacial periods in the geologically near future.
Some scientists believe that the Himalayas are a major factor in the current ice age, because these mountains have increased Earth's total rainfall and therefore the rate at which carbon dioxide is washed out of the atmosphere, decreasing the greenhouse effect. The Himalayas' formation started about 70 million years ago when the Indo-Australian Plate collided with the Eurasian Plate, and the Himalayas are still rising by about 5 mm per year because the Indo-Australian plate is still moving at 67 mm/year. The history of the Himalayas broadly fits the long-term decrease in Earth's average temperature since the mid-Eocene, 40 million years ago.
Fluctuations in ocean currents.
Another important contribution to ancient climate regimes is the variation of ocean currents, which are modified by continent position, sea levels and salinity, as well as other factors. They have the ability to cool (e.g. aiding the creation of Antarctic ice) and the ability to warm (e.g. giving the British Isles a temperate as opposed to a boreal climate). The closing of the Isthmus of Panama about 3 million years ago may have ushered in the present period of strong glaciation over North America by ending the exchange of water between the tropical Atlantic and Pacific Oceans.
Analyses suggest that ocean current fluctuations can adequately account for recent glacial oscillations. During the last glacial period the sea-level has fluctuated 20–30 m as water was sequestered, primarily in the Northern Hemisphere ice sheets. When ice collected and the sea level dropped sufficiently, flow through the Bering Strait (the narrow strait between Siberia and Alaska is ~50 m deep today) was reduced, resulting in increased flow from the North Atlantic. This realigned the thermohaline circulation in the Atlantic, increasing heat transport into the Arctic, which melted the polar ice accumulation and reduced other continental ice sheets. The release of water raised sea levels again, restoring the ingress of colder water from the Pacific with an accompanying shift to northern hemisphere ice accumulation.
Uplift of the Tibetan plateau and surrounding mountain areas above the snowline.
Matthias Kuhle's geological theory of Ice Age development was suggested by the existence of an ice sheet covering the Tibetan Plateau during the Ice Ages (Last Glacial Maximum?). According to Kuhle, the plate-tectonic uplift of Tibet past the snow-line has led to a surface of c. 2,400,000 square kilometres (930,000 sq mi) changing from bare land to ice with a 70% greater albedo. The reflection of energy into space resulted in a global cooling, triggering the Pleistocene Ice Age. Because this highland is at a subtropical latitude, with 4 to 5 times the insolation of high-latitude areas, what would be Earth's strongest heating surface has turned into a cooling surface.
Kuhle explains the interglacial periods by the 100,000-year cycle of radiation changes due to variations in Earth's orbit. This comparatively insignificant warming, when combined with the lowering of the Nordic inland ice areas and Tibet due to the weight of the superimposed ice-load, has led to the repeated complete thawing of the inland ice areas.
Variations in Earth's orbit (Milankovitch cycles).
The Milankovitch cycles are a set of cyclic variations in characteristics of the Earth's orbit around the Sun. Each cycle has a different length, so at some times their effects reinforce each other and at other times they (partially) cancel each other.
There is strong evidence that the Milankovitch cycles affect the occurrence of glacial and interglacial periods within an ice age. The present ice age is the most studied and best understood, particularly the last 400,000 years, since this is the period covered by ice cores that record atmospheric composition and proxies for temperature and ice volume. Within this period, the match of glacial/interglacial frequencies to the Milanković orbital forcing periods is so close that orbital forcing is generally accepted. The combined effects of the changing distance to the Sun, the precession of the Earth's axis, and the changing tilt of the Earth's axis redistribute the sunlight received by the Earth. Of particular importance are changes in the tilt of the Earth's axis, which affect the intensity of seasons. For example, the amount of solar influx in July at 65 degrees north latitude varies by as much as 22% (from 450 W/m² to 550 W/m²). It is widely believed that ice sheets advance when summers become too cool to melt all of the accumulated snowfall from the previous winter. Some believe that the strength of the orbital forcing is too small to trigger glaciations, but feedback mechanisms like may explain this mismatch.
While Milankovitch forcing predicts that cyclic changes in the Earth's orbital elements can be expressed in the glaciation record, additional explanations are necessary to explain which cycles are observed to be most important in the timing of glacial–interglacial periods. In particular, during the last 800,000 years, the dominant period of glacial–interglacial oscillation has been 100,000 years, which corresponds to changes in Earth's orbital eccentricity and orbital inclination. Yet this is by far the weakest of the three frequencies predicted by Milankovitch. During the period 3.0–0.8 million years ago, the dominant pattern of glaciation corresponded to the 41,000-year period of changes in Earth's obliquity (tilt of the axis). The reasons for dominance of one frequency versus another are poorly understood and an active area of current research, but the answer probably relates to some form of resonance in the Earth's climate system.
The "traditional" Milankovitch explanation struggles to explain the dominance of the 100,000-year cycle over the last 8 cycles. Richard A. Muller, Gordon J. F. MacDonald, and others have pointed out that those calculations are for a two-dimensional orbit of Earth but the three-dimensional orbit also has a 100,000-year cycle of orbital inclination. They proposed that these variations in orbital inclination lead to variations in insolation, as the Earth moves in and out of known dust bands in the solar system. Although this is a different mechanism to the traditional view, the "predicted" periods over the last 400,000 years are nearly the same. The Muller and MacDonald theory, in turn, has been challenged by Jose Antonio Rial.
Another worker, William Ruddiman, has suggested a model that explains the 100,000-year cycle by the modulating effect of eccentricity (weak 100,000-year cycle) on precession (26,000-year cycle) combined with greenhouse gas feedbacks in the 41,000- and 26,000-year cycles. Yet another theory has been advanced by Peter Huybers who argued that the 41,000-year cycle has always been dominant, but that the Earth has entered a mode of climate behavior where only the second or third cycle triggers an ice age. This would imply that the 100,000-year periodicity is really an illusion created by averaging together cycles lasting 80,000 and 120,000 years. This theory is consistent with a simple empirical multi-state model proposed by Didier Paillard. Paillard suggests that the late Pleistocene glacial cycles
can be seen as jumps between three quasi-stable climate states. The jumps are induced by the orbital forcing, while in the early Pleistocene the 41,000-year glacial cycles resulted from jumps between only two climate states. A dynamical
model explaining this behavior was proposed by Peter Ditlevsen. This is in support of the suggestion that the late Pleistocene glacial cycles are not due to the weak 100,000-year eccentricity cycle, but a non-linear response to mainly the 41,000-year obliquity cycle.
Variations in the Sun's energy output.
There are at least two types of variation in the Sun's energy output
The long-term increase in the Sun's output cannot be a cause of ice ages.
Volcanism.
Volcanic eruptions may have contributed to the inception and/or the end of ice age periods. At times during the paleoclimate, carbon dioxide levels were two or three times greater than today. Volcanoes and movements in continental plates contributed to high amounts of CO2 in the atmosphere. Carbon dioxide from volcanoes probably contributed to periods with highest overall temperatures. One suggested explanation of the Paleocene-Eocene Thermal Maximum is that undersea volcanoes released methane from clathrates and thus caused a large and rapid increase in the greenhouse effect. There appears to be no geological evidence for such eruptions at the right time, but this does not prove they did not happen.
Recent glacial and interglacial phases.
Glacial stages in North America.
The major glacial stages of the current ice age in North America are the Illinoian, Sangamonian and Wisconsin stages. The use of the Nebraskan, Afton, Kansan, and Yarmouthian (Yarmouth) stages to subdivide the ice age in North America have been discontinued by Quaternary geologists and geomorphologists. These stages have all been merged into the Pre-Illinoian Stage in the 1980s.
During the most recent North American glaciation, during the latter part of the Wisconsin Stage (26,000 to 13,300 years ago), ice sheets extended to about 45 degrees north latitude. These sheets were 3 to 4 km thick.
This Wisconsin glaciation left widespread impacts on the North American landscape. The Great Lakes and the Finger Lakes were carved by ice deepening old valleys. Most of the lakes in Minnesota and Wisconsin were gouged out by glaciers and later filled with glacial meltwaters. The old Teays River drainage system was radically altered and largely reshaped into the Ohio River drainage system. Other rivers were dammed and diverted to new channels, such as the Niagara, which formed a dramatic waterfall and gorge, when the waterflow encountered a limestone escarpment. Another similar waterfall, at the present Clark Reservation State Park near Syracuse, New York, is now dry.
The area from Long Island to Nantucket was formed from glacial till, and the plethora of lakes on the Canadian Shield in northern Canada can be almost entirely attributed to the action of the ice. As the ice retreated and the rock dust dried, winds carried the material hundreds of miles, forming beds of loess many dozens of feet thick in the Missouri Valley. Isostatic rebound continues to reshape the Great Lakes and other areas formerly under the weight of the ice sheets.
The Driftless Zone, a portion of western and southwestern Wisconsin along with parts of adjacent Minnesota, Iowa, and Illinois, was not covered by glaciers.
Last Glacial Period in the semiarid Andes around Aconcagua and Tupungato.
A specially interesting climatic change during glacial times has taken place in the semi-arid Andes. Beside the expected cooling down in comparison with the current climate, a significant precipitation change happened here. So, researches in the presently semiarid subtropic Aconcagua-massif (6,962 m) have shown an unexpectedly extensive glacial glaciation of the type "ice stream network". The connected valley glaciers exceeding 100 km in length, flowed down on the East-side of this section of the Andes at 32–34°S and 69–71°W as far as a height of 2,060 m and on the western luff-side still clearly deeper. Where current glaciers scarcely reach 10 km in length, the snowline (ELA) runs at a height of 4,600 m and at that time was lowered to 3,200 m asl, i.e. about 1,400 m. From this follows that—beside of an annual depression of temperature about c. 8.4 °C— here was an increase in precipitation. Accordingly, at glacial times the humid climatic belt that today is situated several latitude degrees further to the S, was shifted much further to the N.
Effects of glaciation.
Although the last glacial period ended more than 8,000 years ago, its effects can still be felt today. For example, the moving ice carved out the landscape in Canada (See Canadian Arctic Archipelago), Greenland, northern Eurasia and Antarctica. The erratic boulders, till, drumlins, eskers, fjords, kettle lakes, moraines, cirques, horns, etc., are typical features left behind by the glaciers.
The weight of the ice sheets was so great that they deformed the Earth's crust and mantle. After the ice sheets melted, the ice-covered land rebounded. Due to the high viscosity of the Earth's mantle, the flow of mantle rocks which controls the rebound process is very slow—at a rate of about 1 cm/year near the center of rebound area today.
During glaciation, water was taken from the oceans to form the ice at high latitudes, thus global sea level dropped by about 110 meters, exposing the continental shelves and forming land-bridges between land-masses for animals to migrate. During deglaciation, the melted ice-water returned to the oceans, causing sea level to rise. This process can cause sudden shifts in coastlines and hydration systems resulting in newly submerged lands, emerging lands, collapsed ice dams resulting in salination of lakes, new ice dams creating vast areas of freshwater, and a general alteration in regional weather patterns on a large but temporary scale. It can even cause temporary reglaciation. This type of chaotic pattern of rapidly changing land, ice, saltwater and freshwater has been proposed as the likely model for the Baltic and Scandinavian regions, as well as much of central North America at the end of the last glacial maximum, with the present-day coastlines only being achieved in the last few millennia of prehistory. Also, the effect of elevation on Scandinavia submerged a vast continental plain that had existed under much of what is now the North Sea, connecting the British Isles to Continental Europe.
The redistribution of ice-water on the surface of the Earth and the flow of mantle rocks causes changes in the gravitational field as well as changes to the distribution of the moment of inertia of the Earth. These changes to the moment of inertia result in a change in the angular velocity, axis, and wobble of the Earth's rotation.
The weight of the redistributed surface mass loaded the lithosphere, caused it to flex and also induced stress within the Earth. The presence of the glaciers generally suppressed the movement of faults below. However, during deglaciation, the faults experience accelerated slip triggering earthquakes. Earthquakes triggered near the ice margin may in turn accelerate ice calving and may account for the Heinrich events. As more ice is removed near the ice margin, more intraplate earthquakes are induced and this positive feedback may explain the fast collapse of ice sheets.
In Europe, glacial erosion and isostatic sinking from weight of ice made the Baltic Sea, which before the Ice Age was all land drained by the Eridanos River.

</doc>
<doc id="15362" url="https://en.wikipedia.org/wiki?curid=15362" title="Irving Langmuir">
Irving Langmuir

Irving Langmuir (January 31, 1881 – August 16, 1957) was an American chemist and physicist. His most noted publication was the famous 1919 article "The Arrangement of Electrons in Atoms and Molecules" in which, building on Gilbert N. Lewis's cubical atom theory and Walther Kossel's chemical bonding theory, he outlined his "concentric theory of atomic structure". Langmuir became embroiled in a priority dispute with Lewis over this work; Langmuir's presentation skills were largely responsible for the popularization of the theory, although the credit for the theory itself belongs mostly to Lewis. While at General Electric from 1909–1950, Langmuir advanced several basic fields of physics and chemistry, invented the gas-filled incandescent lamp, the hydrogen welding technique, and was awarded the 1932 Nobel Prize in Chemistry for his work in surface chemistry. The Langmuir Laboratory for Atmospheric Research near Socorro, New Mexico, was named in his honor as was the American Chemical Society journal for Surface Science, called Langmuir.
Biography.
Early years.
Irving Langmuir was born in Brooklyn, New York, on January 31, 1881. He was the third of the four children of Charles Langmuir and Sadie, Comings. During his childhood, Langmuir's parents encouraged him to carefully observe nature and to keep a detailed record of his various observations. When Irving was eleven, it was discovered that he had poor eyesight. When this problem was corrected, details that had previously eluded him were revealed, and his interest in the complications of nature was heightened.
During his childhood, Langmuir was influenced by his older brother, Arthur Langmuir. Arthur was a research chemist who encouraged Irving to be curious about nature and how things work. Arthur helped Irving set up his first chemistry lab in the corner of his bedroom, and he was content to answer the myriad of questions that Irving would pose. Langmuir's hobbies included mountaineering, skiing, piloting his own plane, and classical music. In addition to his professional interest in the politics of atomic energy, he was concerned about wilderness conservation.
Education.
Langmuir attended his early education at various schools and institutes in America and Paris (1892–1895). Langmuir graduated high school from Chestnut Hill Academy (1898), an elite private school located in the affluent Chestnut Hill area in Philadelphia. He graduated with a Bachelor of Science degree in metallurgical engineering (Met.E.) from the Columbia University School of Mines (the first mining and metallurgy school in the U.S., established,1864 and presently known as Fu Foundation School of Engineering and Applied Science) in 1903. He earned his Ph.D. degree in 1906 under Nobel laureate Walther Nernst in Göttingen, for research done using the "Nernst glower", an electric lamp invented by Nernst. His doctoral thesis was entitled “On the Partial Recombination of Dissolved Gases During Cooling.” He later did postgraduate work in chemistry. Langmuir then taught at Stevens Institute of Technology in Hoboken, New Jersey, until 1909, when he began working at the General Electric research laboratory (Schenectady, New York).
Research.
His initial contributions to science came from his study of light bulbs (a continuation of his Ph.D. work). His first major development was the improvement of the diffusion pump, which ultimately led to the invention of the high-vacuum rectifier and amplifier tubes. A year later, he and colleague Lewi Tonks discovered that the lifetime of a tungsten filament could be greatly lengthened by filling the bulb with an inert gas, such as argon, the critical factor (overlooked by other researchers) being the need for extreme cleanliness in all stages of the process. He also discovered that twisting the filament into a tight coil improved its efficiency. These were important developments in the history of the incandescent light bulb. His work in surface chemistry began at this point, when he discovered that molecular hydrogen introduced into a tungsten-filament bulb dissociated into atomic hydrogen and formed a layer one atom thick on the surface of the bulb.
His assistant in vacuum tube research was his cousin William Comings White.
As he continued to study filaments in vacuum and different gas environments, he began to study the emission of charged particles from hot filaments (thermionic emission). He was one of the first scientists to work with plasmas and was the first to call these ionized gases by that name, because they reminded him of blood plasma. Langmuir and Tonks discovered electron density waves in plasmas that are now known as Langmuir waves.
He introduced the concept of electron temperature and in 1924 invented the diagnostic method for measuring both temperature and density with an electrostatic probe, now called a Langmuir probe and commonly used in plasma physics. The current of a biased probe tip is measured as a function of bias voltage to determine the local plasma temperature and density. He also discovered atomic hydrogen, which he put to use by inventing the atomic hydrogen welding process; the first plasma weld ever made. Plasma welding has since been developed into gas tungsten arc welding.
In 1917, he published a paper on the chemistry of oil films that later became the basis for the award of the 1932 Nobel Prize in chemistry. Langmuir theorized that oils consisting of an aliphatic chain with a hydrophilic end group (perhaps an alcohol or acid) were oriented as a film one-molecule thick upon the surface of water, with the hydrophilic group down in the water and the hydrophobic chains clumped together on the surface. The thickness of the film could be easily determined from the known volume and area of the oil, which allowed investigation of the molecular configuration before spectroscopic techniques were available.
Later years.
Following World War I Langmuir contributed to atomic theory and the understanding of atomic structure by defining the modern concept of valence shells and isotopes.
Langmuir was president of the Institute of Radio Engineers in 1923.
Based on his work at General Electric, John B. Taylor developed a detector ionizing beams of alkali metals, called nowadays the Langmuir-Taylor detector.
He joined Katharine B. Blodgett to study thin films and surface adsorption. They introduced the concept of a monolayer (a layer of material one molecule thick) and the two-dimensional physics which describe such a surface. In 1932 he received the Nobel Prize in Chemistry "for his discoveries and investigations in surface chemistry."
In 1938, Langmuir's scientific interests began to turn to atmospheric science and meteorology. One of his first ventures, although tangentially related, was a refutation of the claim of entomologist Charles H. T. Townsend that the deer botfly flew at speeds in excess of 800 miles per hour. Langmuir estimated the fly's true speed at 25 miles per hour.
After observing windrows of drifting seaweed in the Sargasso Sea he discovered a wind-driven surface circulation in the sea. It is now called the Langmuir circulation.
During World War II, Langmuir worked on improving naval sonar for submarine detection, and later to develop protective smoke screens and methods for deicing aircraft wings. This research led him to theorize that the introduction of dry ice and iodide into a sufficiently moist cloud of low temperature could induce precipitation (cloud seeding); though in frequent practice, particularly in Australia and the People's Republic of China, the efficiency of this technique remains controversial today.
In 1953 Langmuir coined the term "pathological science", describing research conducted with accordance to the scientific method, but tainted by unconscious bias or subjective effects. This is in contrast to pseudoscience, which has no pretense of following the scientific method. In his original speech, he presented ESP and flying saucers as examples of pathological science; since then, the label has been applied to polywater and cold fusion.
His house in Schenectady, was designated a National Historic Landmark in 1976.
Personal life.
Langmuir was married to Marion Mersereau (1883-1971) in 1912 with whom he adopted two children: Kenneth and Barbara. After a short illness, he died in Woods Hole, Massachusetts from a heart attack on August 16, 1957. His obituary ran on the front page of "The New York Times".
On his religious views, Langmuir was an agnostic.
In fiction.
According to author Kurt Vonnegut, Langmuir was the inspiration for his fictional scientist Dr. Felix Hoenikker in the novel "Cat's Cradle". The character's invention of ice-nine eventually destroyed the world. Langmuir had worked with Vonnegut's brother, Bernard Vonnegut.

</doc>
<doc id="15365" url="https://en.wikipedia.org/wiki?curid=15365" title="International Association of Travel Agents Network">
International Association of Travel Agents Network

The International Airlines Travel Agent Network (IATAN) is an industry association in the USA designed to represent the interests of its member companies.
In addition, it (along with the IATA) is the body responsible for the standard international codes for airlines, airports, hotels, cities and car rental firms (for example, the three-digit codes that designate London Heathrow Airport as LHR). These codes provide a method to link international travel network with international suppliers.

</doc>
<doc id="15368" url="https://en.wikipedia.org/wiki?curid=15368" title="Insider trading">
Insider trading

Insider trading is the trading of a public company's stock or other securities (such as bonds or stock options) by individuals with access to nonpublic information about the company. In various countries, some kinds of trading based on insider information is illegal. This is because it is seen as unfair to other investors who do not have access to the information, as the investor with insider information could potentially make far larger profits that a typical investor could not make.
The authors of one study claim that illegal insider trading raises the cost of capital for securities issuers, thus decreasing overall economic growth. However, some economists have argued that insider trading should be allowed and could, in fact, benefit markets.
Trading by specific insiders, such as employees, is commonly permitted as long as it does not rely on material information not in the public domain. Many jurisdictions require that such trading be reported so that the transactions can be monitored. In the United States and several other jurisdictions, trading conducted by corporate officers, key employees, directors, or significant shareholders must be reported to the regulator or publicly disclosed, usually within a few business days of the trade. In these cases, insiders in the United States are required to file a Form 4 with the U.S. Securities and Exchange Commission (SEC) when buying or selling shares of their own companies.
The rules governing insider trading are complex and vary significantly from country to country. The extent of enforcement is varies from one country to another. The definition of insider in one jurisdiction can be broad, and may cover not only insiders themselves but also any persons related to them, such as brokers, associates and even family members. A person who becomes aware of non-public information and trades on that basis may be guilty.
Illegal insider trading.
Rules prohibiting or criminalizing insider trading on material non-public information exist in most jurisdictions around the world (Bhattacharya and Daouk, 2002), but the details and the efforts to enforce them vary considerably. In the United States, Sections 16(b) and 10(b) of the Securities Exchange Act of 1934 directly and indirectly address insider trading. The U.S. Congress enacted this law after the stock market crash of 1929. The United States is generally viewed as having the strictest laws against illegal insider trading, and makes the most serious efforts to enforce the laws. In the United Kingdom, the Financial Services and Markets Act, 2000 gives the UK's Financial Conduct Authority the responsibility to investigate and prosecute insider dealing, defined by The Criminal Justice Act 1993.
Definition of "insider".
In the United States, Canada, Australia and Germany, for mandatory reporting purposes, corporate insiders are defined as a company's officers, directors and any beneficial owners of more than 10% of a class of the company's equity securities. Trades made by these types of insiders in the company's own stock, based on material non-public information, are considered fraudulent since the insiders are violating the fiduciary duty that they owe to the shareholders. The corporate insider, simply by accepting employment, has undertaken a legal obligation to the shareholders to put the shareholders' interests before their own, in matters related to the corporation. When the insider buys or sells based upon company owned information, he is violating his obligation to the shareholders.
For example, illegal insider trading would occur if the chief executive officer of Company A learned (prior to a public announcement) that Company A will be taken over and then bought shares in Company A while knowing that the share price would likely rise.
In the United States and many other jurisdictions, however, "insiders" are not just limited to corporate officials and major shareholders where illegal insider trading is concerned but can include any individual who trades shares based on material non-public information in violation of some duty of trust. This duty may be imputed; for example, in many jurisdictions, in cases of where a corporate insider "tips" a friend about non-public information likely to have an effect on the company's share price, the duty the corporate insider owes the company is now imputed to the friend and the friend violates a duty to the company if the corporate insider trades on the basis of this information.
Liability for insider trading.
Liability for inside trading violations generally cannot be avoided by passing on the information in an "I scratch your back; you scratch mine" or quid pro quo arrangement if the person receiving the information knew or should have known that the information was material non-public information. In the United States, at least one court has indicated that the insider who releases the non-public information must have done so for an improper purpose. In the case of a person who receives the insider information (called the "tippee"), the tippee must also have been aware that the insider released the information for an improper purpose.
One commentator has argued that if Company A's CEO did not trade on the undisclosed takeover news, but instead passed the information on to his brother-in-law who traded on it, illegal insider trading would still have occurred (albeit by proxy by passing it on to a "non-insider" so Company A's CEO wouldn't get his hands dirty).
Misappropriation theory.
A newer view of insider trading, the misappropriation theory, is now accepted in U.S. law. It states that anyone who misappropriates information from his or her employer and trades on that information in any stock (either the employer's stock or the company's competitor stocks) may be guilty of insider trading.
For example, if a journalist who worked for Company B learned about the takeover of Company A while performing his work duties and bought stock in Company A, illegal insider trading might still have occurred. Even though the journalist did not violate a fiduciary duty to Company A's shareholders, he might have violated a fiduciary duty to Company B's shareholders (assuming the newspaper had a policy of not allowing reporters to trade on stories they were covering).
Proof of responsibility.
Proving that someone has been responsible for a trade can be difficult because traders may try to hide behind nominees, offshore companies, and other proxies. The Securities and Exchange Commission prosecutes over 50 cases each year, with many being settled administratively out of court. The SEC and several stock exchanges actively monitor trading, looking for suspicious activity. The SEC does not have criminal enforcement authority, but can refer serious matters to the U.S. Attorney's Office for further investigation and prosecution.
Trading on information in general.
Not all trading on non-public information is illegal insider trading. For example, a person in a restaurant who hears the CEO of Company A at the next table tell the CFO that the company's profits will be higher than expected and then buys the stock is not guilty of insider trading—unless he or she had some closer connection to the company or company officers. However, information about a tender offer (usually regarding a merger or acquisition) is held to a higher standard. If this type of information is obtained (directly or indirectly) and there is reason to believe it is nonpublic, there is a duty to disclose it or abstain from trading. The punishment for insider trading depends on a few different factors. There are three main factors, which can be identified. 
Tracking insider trades.
Since insiders are required to report their trades, others often track these traders, and there is a school of investing which follows the lead of insiders. Following such leads subjects the follower to the risk that an insider is making a buy specifically to increase investor confidence, or is making a sale for reasons unrelated to the health of the company (such as a desire to diversify or pay a personal expense).
Legal insider trading.
Legal trades by insiders are common, as employees of publicly traded corporations often have stock or stock options. These trades are made public in the United States through Securities and Exchange Commission filings, mainly Form 4.
SEC Rule 10b5-1 clarified that the prohibition against insider trading does not require proof that an insider actually used material nonpublic information when conducting a trade; possession of such information alone is sufficient to violate the provision, and the SEC would infer that an insider in possession of material nonpublic information used this information when conducting a trade. However, SEC Rule 10b5-1 also created for insiders an affirmative defense if the insider can demonstrate that the trades conducted on behalf of the insider were conducted as part of a pre-existing contract or written binding plan for trading in the future.
For example, if an insider expects to retire after a specific period of time and, as part of retirement planning, the insider has adopted a written binding plan to sell a specific amount of the company's stock every month for two years, and the insider later comes into possession of material nonpublic information about the company, trades based on the original plan might not constitute prohibited insider trading.
American insider trading law.
The United States has been the leading country in prohibiting insider trading made on the basis of material non-public information. Thomas Newkirk and Melissa Robertson of the U.S. Securities and Exchange Commission (SEC) summarize the development of US insider trading laws. Insider trading has a base offense level of 8, which puts it in Zone A under the U.S. Sentencing Guidelines. This means that first-time offenders are eligible to receive probation rather than incarceration.
Statutory law.
U.S. insider trading prohibitions are based on English and American common law prohibitions against fraud. In 1909, well before the Securities Exchange Act was passed, the United States Supreme Court ruled that a corporate director who bought that company's stock when he knew the stock's price was about to increase committed fraud by buying but not disclosing his inside information.
Section 15 of the Securities Act of 1933 contained prohibitions of fraud in the sale of securities which were greatly strengthened by the Securities Exchange Act of 1934.
Section 16(b) of the Securities Exchange Act of 1934 prohibits short-swing profits (from any purchases and sales within any six-month period) made by corporate directors, officers, or stockholders owning more than 10% of a firm's shares. Under Section 10(b) of the 1934 Act, SEC Rule 10b-5, prohibits fraud related to securities trading.
The Insider Trading Sanctions Act of 1984 and the Insider Trading and Securities Fraud Enforcement Act of 1988 place penalties for illegal insider trading as high as three times the amount of profit gained or loss avoided from the illegal trading.
SEC regulations.
SEC regulation FD ("Fair Disclosure") requires that if a company intentionally discloses material non-public information to one person, it must simultaneously disclose that information to the public at large. In the case of an unintentional disclosure of material non-public information to one person, the company must make a public disclosure "promptly."
Insider trading, or similar practices, are also regulated by the SEC under its rules on takeovers and tender offers under the Williams Act.
Court decisions.
Much of the development of insider trading law has resulted from court decisions.
In "SEC v. Texas Gulf Sulphur Co.", a federal circuit court stated that anyone in possession of inside information must either disclose the information or refrain from trading. Officers of the Texas Gulf Sulphur Corporation had used inside information about the discovery of the Kidd Mine to make profits by buying shares and call options on company stock.
In 1909, the Supreme Court of the United States ruled in "Strong v. Repide" that a director who expects to act in a way that affects the value of shares cannot use that knowledge to acquire shares from those who do not know of the expected action. Even though in general, ordinary relations between directors and shareholders in a business corporation are not of such a fiduciary nature as to make it the duty of a director to disclose to a shareholder the general knowledge which he may possess regarding the value of the shares of the company before he purchases any from a shareholder, yet there are cases where, by reason of the special facts, such duty exists.
In 1984, the Supreme Court of the United States ruled in the case of "Dirks v. Securities and Exchange Commission" that tippees (receivers of second-hand information) are liable if they had reason to believe that the tipper had breached a fiduciary duty in disclosing confidential information and the tipper received any personal benefit from the disclosure. In "Dirks", the "tippee" received confidential information from an insider, a former employee of a company. The reason the insider disclosed the information to the tippee, and the reason the tippee disclosed the information to third parties, was to blow the whistle on massive fraud at the company. As a result of the tippee's efforts the fraud was uncovered, and the company went into bankruptcy. But, while the tippee had given the "inside" information to clients who made profits from the information, the U.S. Supreme Court ruled that the tippee could not be held liable under the federal securities laws—for the simple reason that the insider from whom he received the information was not releasing the information for an improper purpose (a personal benefit), but rather for the purpose of exposing the fraud. The Supreme Court ruled that the tippee could not have been aiding and abetting a securities law violation committed by the insider—for the simple reason that no securities law violation had been committed by the insider.
In "Dirks", the Supreme Court also defined the concept of "constructive insiders," who are lawyers, investment bankers and others who receive confidential information from a corporation while providing services to the corporation. Constructive insiders are also liable for insider trading violations if the corporation expects the information to remain confidential, since they acquire the fiduciary duties of the true insider.
The next expansion of insider trading liability came in "SEC vs. Materia" 745 F.2d 197 (2d Cir. 1984), the case which first introduced the misappropriation theory of liability for insider trading. Materia, a financial printing firm proofreader, and clearly not an insider by any definition, was found to have determined the identity of takeover targets based on proofreading tender offer documents during his employment. After a two-week trial, the district court found him liable for insider trading, and the Second Circuit Court of Appeals affirmed holding that the theft of information from an employer, and the use of that information to purchase or sell securities in another entity, constituted a fraud in connection with the purchase or sale of a securities. The misappropriation theory of insider trading was born, and liability further expanded to encompass a larger group of outsiders.
In "United States v. Carpenter" (1986) the U.S. Supreme Court cited an earlier ruling while unanimously upholding mail and wire fraud convictions for a defendant who received his information from a journalist rather than from the company itself. The journalist R. Foster Winans was also convicted, on the grounds that he had misappropriated information belonging to his employer, the "Wall Street Journal". In that widely publicized case, Winans traded in advance of "Heard on the Street" columns appearing in the Journal.
The Court stated in "Carpenter": "It is well established, as a general proposition, that a person who acquires special knowledge or information by virtue of a confidential or fiduciary relationship with another is not free to exploit that knowledge or information for his own personal benefit but must account to his principal for any profits derived therefrom."
However, in upholding the securities fraud (insider trading) convictions, the justices were evenly split.
In 1997, the U.S. Supreme Court adopted the misappropriation theory of insider trading in "United States v. O'Hagan", 521 U.S. 642, 655 (1997). O'Hagan was a partner in a law firm representing Grand Metropolitan, while it was considering a tender offer for Pillsbury Company. O'Hagan used this inside information by buying call options on Pillsbury stock, resulting in profits of over $4.3 million. O'Hagan claimed that neither he nor his firm owed a fiduciary duty to Pillsbury, so he did not commit fraud by purchasing Pillsbury options.
The Court rejected O'Hagan's arguments and upheld his conviction.
The "misappropriation theory" holds that a person commits fraud "in connection with" a securities transaction and thereby violates 10(b) and Rule 10b-5, when he misappropriates confidential information for securities trading purposes, in breach of a duty owed to the source of the information. Under this theory, a fiduciary's undisclosed, self-serving use of a principal's information to purchase or sell securities, in breach of a duty of loyalty and confidentiality, defrauds the principal of the exclusive use of the information. In lieu of premising liability on a fiduciary relationship between company insider and purchaser or seller of the company's stock, the misappropriation theory premises liability on a fiduciary-turned-trader's deception of those who entrusted him with access to confidential information.
The Court specifically recognized that a corporation's information is its property: "A company's confidential information ... qualifies as property to which the company has a right of exclusive use. The undisclosed misappropriation of such information in violation of a fiduciary duty ... constitutes fraud akin to embezzlement – the fraudulent appropriation to one's own use of the money or goods entrusted to one's care by another."
In 2000, the SEC enacted SEC Rule 10b5-1, which defined trading "on the basis of" inside information as any time a person trades while aware of material nonpublic information. It is no longer a defense for one to say that one would have made the trade anyway. The rule also created an affirmative defense for pre-planned trades.
In 2014, in the case of "United States v. Newman", the United States Court of Appeals for the Second Circuit cited the Supreme Court's decision in "Dirks", and ruled that in order for a "tippee" (a person who has received insider information from an insider and has used that information) to be guilty of insider trading, the tippee must have been aware not only that the information was insider information, but must also have been aware that the insider released the information for an improper purpose (such as a personal benefit). The Court concluded that the insider's breach of a fiduciary duty not to release confidential information—in the absence of an improper purpose on the part of the insider—is not enough for criminal liability to be imposed on the either the insider or the tippee.
Insider trading by members of Congress.
Members of the US Congress are not exempt from the laws that ban insider trading. Because they generally do not have a confidential relationship with the source of the information they receive, however, they do not meet the usual definition of an "insider."
House of Representatives rules may however consider congressional insider trading unethical. A 2004 study found that stock sales and purchases by Senators outperformed the market by 12.3% per year. Peter Schweizer points out several examples of insider trading by members of Congress, including action taken by Spencer Bachus following a private, behind-the-doors meeting on the evening of September 18, 2008 when Hank Paulson and Ben Bernanke informed members of Congress about the imminent financial crisis, Bachus then shorted stocks the next morning and cashed in his profits within a week. Also attending the same meeting were Senator Dick Durbin and John Boehner; the same day (trade effective the next day), Durbin sold mutual-fund shares worth $42,696, and reinvested it all with Warren Buffett. Also the same day (trade effective the next day), Congressman Boehner cashed out of an equity mutual fund.
Insider trading with Congress-sourced information.
In 2014, federal prosecutors issued a subpoena to the House Ways and Means committee and Brian Sutter, staff director of its health-care sub-committee, relative to a price move in stocks just prior to the passage of a law favorable to the companies involved. An e-mail was sent out by a "Washington-based policy-research firm that predicted the change the law for its Wall Street clients. That alert, in turn, was based in part on information provided to the firm by a former congressional health-care aide turned lobbyist, according to emails reviewed by the "Street Journal" in 2013.
Security analysis and insider trading.
Security analysts gather and compile information, talk to corporate officers and other insiders, and issue recommendations to traders. Thus their activities may easily cross legal lines if they are not especially careful. The CFA Institute in its code of ethics states that analysts should make every effort to make all reports available to all the broker's clients on a timely basis. Analysts should never report material nonpublic information, except in an effort to make that information available to the general public. Nevertheless, analysts' reports may contain a variety of information that is "pieced together" without violating insider trading laws, under the Mosaic theory. This information may include non-material nonpublic information as well as material public information, which may increase in value when properly compiled and documented.
In May 2007, a bill entitled the "Stop Trading on Congressional Knowledge Act, or STOCK Act" was introduced that would hold congressional and federal employees liable for stock trades they made using information they gained through their jobs and also regulate analysts or "Political Intelligence" firms that research government activities. The 2012 STOCK Act was passed on April 4, 2012.
Arguments for legalizing insider trading.
Some economists and legal scholars (such as Henry Manne, Milton Friedman, Thomas Sowell, Daniel Fischel, and Frank H. Easterbrook) have argued that laws against insider trading should be repealed. They claim that insider trading based on material nonpublic information benefits investors, in general, by more quickly introducing new information into the market.
Friedman, laureate of the Nobel Memorial Prize in Economics, said: "You want more insider trading, not less. You want to give the people most likely to have knowledge about deficiencies of the company an incentive to make the public aware of that." Friedman did not believe that the trader should be required to make his trade known to the public, because the buying or selling pressure itself is information for the market.
Other critics argue that insider trading is a victimless act: a willing buyer and a willing seller agree to trade property which the seller rightfully owns, with no prior contract (according to this view) having been made between the parties to refrain from trading if there is asymmetric information. The Atlantic has described the process as "arguably the closest thing that modern finance has to a victimless crime".
Legalization advocates also question why "trading" where one party has more information than the other is legal in other markets, such as real estate, but not in the stock market. For example, if a geologist knows there is a high likelihood of the discovery of petroleum under Farmer Smith's land, he may be entitled to make Smith an offer for the land, and buy it, without first telling Farmer Smith of the geological data. Nevertheless, circumstances can occur when the geologist would be committing fraud if, because he owes a duty to the farmer, he did not disclose the information (for example, where the geologist had been hired by Farmer Smith to assess the geology of the farm).
Advocates of legalization make free speech arguments. Punishment for communicating about a development pertinent to the next day's stock price might seem an act of censorship. If the information being conveyed is proprietary information and the corporate insider has contracted to not expose it, he has no more right to communicate it than he would to tell others about the company's confidential new product designs, formulas, or bank account passwords.
Some authors have used these arguments to propose legalizing insider trading on negative information (but not on positive information). Since negative information is often withheld from the market, trading on such information has a higher value for the market than trading on positive information.
There are very limited laws against "insider trading" in the commodities markets if, for no other reason than that the concept of an "insider" is not immediately analogous to commodities themselves (corn, wheat, steel, etc.). However, analogous activities such as front running are illegal under US commodity and futures trading laws. For example, a commodity broker can be charged with fraud by receiving a large purchase order from a client (one likely to affect the price of that commodity) and then purchasing that commodity before executing the client's order to benefit from the anticipated price increase.
Legal differences among jurisdictions.
The US and the UK vary in the way the law is interpreted and applied with regard to insider trading.
In the UK, the relevant laws are the Criminal Justice Act 1993, Part V, Schedule 1, and the Financial Services and Markets Act 2000, which defines an offence of Market Abuse. It is also illegal to fail to trade based on inside information (whereas without the inside information the trade would have taken place). The principle is that it is illegal to trade on the basis of market-sensitive information that is not generally known. No relationship to the issuer of the security is required; all that is required is that the guilty party traded (or caused trading) whilst having inside information.
Japan enacted its first law against insider trading in 1988. Roderick Seeman said, "Even today many Japanese do not understand why this is illegal. Indeed, previously it was regarded as common sense to make a profit from your knowledge."
In accordance with EU Directives, Malta enacted the Financial Markets Abuse Act in 2002, which effectively replaced the Insider Dealing and Market Abuse Act of 1994.
The "Objectives and Principles of Securities Regulation" published by the International Organization of Securities Commissions (IOSCO) in 1998 and updated in 2003 states that the three objectives of good securities market regulation are:
The discussion of these "Core Principles" state that "investor protection" in this context means "Investors should be protected from misleading, manipulative or fraudulent practices, including insider trading, front running or trading ahead of customers and the misuse of client assets." More than 85 percent of the world's securities and commodities market regulators are members of IOSCO and have signed on to these Core Principles.
The World Bank and International Monetary Fund now use the IOSCO Core Principles in reviewing the financial health of different country's regulatory systems as part of these organization's financial sector assessment program, so laws against insider trading based on non-public information are now expected by the international community. Enforcement of insider trading laws varies widely from country to country, but the vast majority of jurisdictions now outlaw the practice, at least in principle.
Larry Harris claims that differences in the effectiveness with which countries restrict insider trading help to explain the differences in executive compensation among those countries. The US, for example, has much higher CEO salaries than do Japan or Germany, where insider trading is less effectively restrained.
By nation.
European Union.
In 2014, the European Union (EU) adopted legislation (Criminal Sanctions for Market Abuse Directive) that harmonises criminal sanctions for insider dealing. All EU Member States agreed to introduce maximum prison sentences of at least four years for serious cases of market manipulation and insider dealing, and at least two years for improper disclosure of insider information.
Norway.
In 2009, a journalist in Nettavisen (Thomas Gulbrandsen) was sentenced to 4 months in prison for insider trading.
The longest prison sentence in a Norwegian trial where the main charge was insider trading, was for 8 years (2 of which suspended) when Alain Angelil was convicted in a district court on December 9, 2011.
United Kingdom.
Although insider trading in the UK has been illegal since 1980, it proved difficult to successfully prosecute individuals accused of insider trading. There were a number of notorious cases where individuals were able to escape prosecution. Instead the UK regulators relied on a series of fines to punish market abuses.
These fines were widely perceived as an ineffective deterrent (Cole, 2007), and there was a statement of intent by the UK regulator (the Financial Services Authority) to use its powers to enforce the legislation (specifically the Financial Services and Markets Act 2000). Between 2009–2012 the FSA secured 14 convictions in relation to insider dealing.
United States.
Rajat Gupta, who had been managing partner of McKinsey & Co. and a director at Goldman Sachs Group Inc. and Procter & Gamble Co., was convicted by a federal jury in 2012 of leaking inside information to hedge fund manager Raj Rajaratnam. The case was prosecuted by the office of United States Attorney for the Southern District of New York Preet Bharara.
With the guilty plea by Perkins Hixon in 2014 for insider trading from 2010-2013 while at Evercore Partners, Bharara said in a press release that 250 defendants whom his office had charged since August 2009 had now been convicted.
On December 10, 2014, a federal appeals court overturned the insider trading convictions of two former hedge fund traders, Todd Newman and Anthony Chiasson, based on the "erroneous" instructions given to jurors by the trial judge.
Canada.
In 2008, police uncovered an insider trading conspiracy involving Bay Street and Wall Street lawyer Gil Cornblum and another lawyer, Stan Grmovsek, who were found to have gained over $10 million in illegal profits over a 14-year span. Cornblum committed suicide before criminal charges were laid. Grmovsek pleaded guilty and was sentenced to 39 months in prison. This was the longest term ever imposed for insider trading in Canada. These crimes were explored in Mark Coakley's 2011 non-fiction book, Tip and Trade.
China.
On October 1, 2015, Chinese fund manager Xu Xiang, was arrested due to insider trading.

</doc>
<doc id="15369" url="https://en.wikipedia.org/wiki?curid=15369" title="International Brigades">
International Brigades

The International Brigades () were military units, made up of volunteers from different countries, who travelled to Spain in order to fight for the Second Spanish Republic in the Spanish Civil War, between 1936 and 1939.
The number of combatant volunteers has been estimated at between 32,000–35,000, though with no more than about 20,000 active at any one time. A further 10,000 people probably participated in non-combatant roles and about 3,000–5,000 foreigners were members of CNT or POUM. They came from a claimed "53 nations" to fight against the Spanish Falangist forces led by General Francisco Franco, who was assisted by German and Italian forces.
Formation and recruitment.
Using foreign Communist Parties to recruit volunteers for Spain was first proposed in the Soviet Union in September 1936—apparently at the suggestion of Maurice Thorez—by Willi Münzenberg, chief of Comintern propaganda for Western Europe. As a security measure, non-Communist volunteers would first be interviewed by an NKVD agent.
By the end of September, the Italian and French Communist Parties had decided to set up a column. Luigi Longo, ex-leader of the Italian Communist Youth, was charged to make the necessary arrangements with the Spanish government. The Soviet Ministry of Defense also helped, since they had experience of dealing with corps of international volunteers during the Russian Civil War. The idea was initially opposed by Largo Caballero, but after the first setbacks of the war, he changed his mind, and finally agreed to the operation on 22 October. However, the Soviet Union did not withdraw from the Non-Intervention Committee, probably to avoid diplomatic conflict with France and the United Kingdom.
The main recruitment centre was in Paris, under the supervision of Soviet colonel Karol "Walter" Świerczewski. On 17 October 1936, an open letter by Joseph Stalin to José Díaz was published in "Mundo Obrero", arguing that victory for the Spanish second republic was a matter not only for Spaniards, but also for the whole of "progressive humanity"; in a matter of days, support organisations for the Spanish Republic were founded in most countries, all more or less controlled by the Comintern.
Entry to Spain was arranged for volunteers: for instance, a Yugoslav, Josip Broz, who would become famous as Marshal Josip Broz Tito, was in Paris to provide assistance, money and passports for volunteers from Eastern Europe. Volunteers were sent by train or ship from France to Spain, and sent to the base at Albacete. However, many of them also went by themselves to Spain. The volunteers were under no contract, nor defined engagement period, which would later prove a problem.
Also many Italians, Germans, and people from other countries joined the movement, with the idea that combat in Spain was a first step to restore democracy or advance a revolutionary cause in their own country. There were also many unemployed workers (especially from France), and adventurers. Finally, some 500 communists who had been exiled to Russia were sent to Spain (among them, experienced military leaders from the First World War like "Kléber" Stern, "Gomez" Zaisser, "Lukacs" Zalka and "Gal" Galicz, who would prove invaluable in combat).
The operation was met with enthusiasm by communists, but by anarchists with skepticism, at best. At first, the anarchists, who controlled the borders with France, were told to refuse communist volunteers, but reluctantly allowed their passage after protests. A group of 500 volunteers (mainly French, with a few exiled Poles and Germans) arrived in Albacete on 14 October 1936. They were met by international volunteers who had already been fighting in Spain: Germans from the Thälmann Battalion, Italians from Centuria Gastone Sozzi and French from Commune de Paris Battalion. Among them was British poet John Cornford. Men were sorted according to their experience and origin, and dispatched to units.
Albacete soon became the International Brigades headquarters and its main depot. It was run by a "troika" of Comintern heavyweights: André Marty was commander; Luigi Longo ("Gallo") was Inspector-General; and Giuseppe Di Vittorio ("Nicoletti") was chief political commissar.
The French Communist Party provided uniforms for the Brigades. They were organized into mixed brigades, the basic military unit of the Republican People's Army. Discipline was extreme. For several weeks, the Brigades were locked in their base while their strict military training was under way.
Service.
First engagements: Siege of Madrid.
The Battle of Madrid was a major success for the Republic. It staved off the prospect of a rapid defeat at the hands of Francisco Franco's forces. The role of the International Brigades in this victory was generally recognised, but was exaggerated by Comintern propaganda, so that the outside world heard only of their victories, and not those of Spanish units. So successful was such propaganda that the British Ambassador, Sir Henry Chilton, declared that there were no Spaniards in the army which had defended Madrid. The International Brigade forces that fought in Madrid arrived after other successful Republican fighting. Of the 40,000 Republican troops in the city, the foreign troops numbered less than 3,000. Even though the International Brigades did not win the battle by themselves, nor significantly change the situation, they certainly did provide an example by their determined fighting, and improved the morale of the population by demonstrating the concern of other nations in the fight. Many of the older members of the International Brigades provided valuable combat experience, having fought during the First World War (Spain remained neutral in 1914–18) and the Irish War of Independence (Some had fought in the British army while others had fought in the IRA).
One of the strategic positions in Madrid was the Casa de Campo. There the Nationalist troops were Moroccans, commanded by General José Enrique Varela. They were stopped by III and IV Brigades of the Spanish Republican Army.
On 9 November 1936, the XI International Brigade - comprising 1,900 men from the Edgar André Battalion, the Commune de Paris Battalion and the Dabrowski Battalion, together with a British machine-gun company — took up position at the Casa de Campo. In the evening, its commander, General Kléber, launched an assault on the Nationalist positions. This lasted for the whole night and part of the next morning. At the end of the fight, the Nationalist troops had been forced to retreat, abandoning all hopes of a direct assault on Madrid by Casa de Campo, while the XIth Brigade had lost a third of its personnel.
On 13 November, the 1,550-man strong XII International Brigade, made up of the Thälmann Battalion, the Garibaldi Battalion and the André Marty Battalion, deployed. Commanded by General "Lukacs", they assaulted Nationalist positions on the high ground of Cerro de los Angeles. As a result of language and communication problems, command issues, lack of rest, poor coordination with armoured units, and insufficient artillery support, the attack failed.
On November 19, the anarchist militias were forced to retreat, and Nationalist troops — Moroccans and Spanish Foreign Legionnaires, covered by the Nazi Condor Legion — captured a foothold in the University City. The 11th Brigade was sent to drive the Nationalists out of the University City. The battle was extremely bloody, a mix of artillery and aerial bombardment, with bayonet and grenade fights, room by room. Anarchist leader Buenaventura Durruti was shot there on 19 November 1936, and died the next day. The battle in the University went on until three quarters of the University City was under Nationalist control. Both sides then started setting up trenches and fortifications. It was then clear that any assault from either side would be far too costly; the nationalist leaders had to renounce the idea of a direct assault on Madrid, and prepare for a siege of the capital.
On 13 December 1936, 18,000 nationalist troops attempted an attack to close the encirclement of Madrid at Guadarrama — an engagement known as the Battle of the Corunna Road. The Republicans sent in a Soviet armoured unit, under General Dmitry Pavlov, and both XI and XII International Brigades. Violent combat followed, and they stopped the Nationalist advance.
An attack was then launched by the Republic on the Córdoba front. The battle ended in a form of stalemate; a communique was issued, saying: "During the day the advance continued without the loss of any territory." Poets Ralph Winston Fox and John Cornford were killed. Eventually, the Nationalists advanced, taking the hydroelectric station at El Campo. André Marty accused the commander of the Marseillaise Battalion, Gaston Delasalle, of espionage and treason and had him executed. (It is doubtful that Delasalle would have been a spy for Francisco Franco; he was denounced by his own second-in-command, André Heussler, who was subsequently executed for treason during World War II by the French Resistance.)
Further Nationalist attempts after Christmas to encircle Madrid met with failure, but not without extremely violent combat. On 6 January 1937, the Thälmann Battalion arrived at Las Rozas, and held its positions until it was destroyed as a fighting force. On January 9, only 10 km had been lost to the Nationalists, when the XIII International Brigade and XIV International Brigade and the 1st British Company, arrived in Madrid. Violent Republican assaults were launched in attempt to retake the land, with little success. On January 15, trenches and fortifications were built by both sides, resulting in a stalemate.
The Nationalists did not take Madrid until the very end of the war, in March 1939, when they marched in unopposed. There were some pockets of resistance during the consecutive months.
Battle of Jarama.
On 6 February 1937, following the fall of Málaga, the nationalists launched an attack on the Madrid–Andalusia road, south of Madrid. The Nationalists quickly advanced on the little town Ciempozuelos, held by the XV International Brigade, which was composed of the British Battalion (British Commonwealth and Irish), the Dimitrov Battalion (miscellaneous Balkan nationalities), the 6 Février Battalion (Belgians and French), the Canadian Mackenzie-Papineau Battalion and the Abraham Lincoln Brigade (Americans, including African-American).
An independent 80-men-strong (mainly) Irish unit, known as the Connolly Column, made up of people from both sides of the Irish border also fought. Several histories of the Irish in Spain record that they included an ex-Catholic Christian Brother and an ordained Church of Ireland (Anglican Protestant) Clergyman, fighting and dying on the same side. (These battalions were not composed entirely of one nationality or another, rather they were for the most part a mix of many)
On 11 February 1937, a Nationalist brigade launched a surprise attack on the André Marty Battalion (XIV International Brigade), stabbing its sentries and crossing the Jarama. The Garibaldi Battalion stopped the advance with heavy fire. At another point, the same tactic allowed the Nationalists to move their troops across the river.
On 12 February, the British Battalion, XV International Brigade took the brunt of the attack, remaining under heavy fire for seven hours. The position became known as "Suicide Hill". At the end of the day, only 225 of the 600 members of the British battalion remained. One company was captured by ruse, when Nationalists advanced among their ranks singing "The Internationale".
On 17 February, the Republican Army counter-attacked. On February 23 and 27, the International Brigades were engaged, but with little success. The Lincoln Battalion was put under great pressure, with no artillery support. It suffered 120 killed and 175 wounded. Amongst the dead was the Irish poet Charles Donnelly and Leo Greene.
There were heavy casualties on both sides, and although "both claimed victory ... both suffered defeats". It resulted in a stalemate, with both sides digging in, creating elaborate trench systems.
On 22 February 1937 the League of Nations Non-Intervention Committee ban on foreign volunteers went into effect.
Battle of Guadalajara.
After the failed assault on the Jarama, the Nationalists attempted another assault on Madrid, from the North-East this time. The objective was the town of Guadalajara, 50 km from Madrid. The whole Italian expeditionary corps — 35,000 men, with 80 battle tanks and 200 field artillery — was deployed, as Benito Mussolini wanted the victory to be credited to Italy. On 9 March 1937, the Italians made a breach in the Republican lines, but did not properly exploit the advance. However, the rest of the Nationalist army was advancing, and the situation appeared critical for the Republicans. A formation drawn from the best available units of the Republican army, including the XI and XII International Brigades, was quickly assembled.
At dawn on 10 March, the Nationalists closed in, and by noon, the Garibaldi Battalion counterattacked. Some confusion arose from the fact that the sides were not aware of each other's movements, and that both sides spoke Italian; this resulted in scouts from both sides exchanging information without realising they were enemies. The Republican lines advanced and made contact with XI International Brigade. Nationalist tanks were shot at and infantry patrols came into action.
On March 11, the Nationalist army broke the front of the Republican army. The Thälmann Battalion suffered heavy losses, but succeeded in holding the Trijueque–Torija road. The Garibaldi also held its positions. On March 12, Republican planes and tanks attacked. The Thälmann Battalion attacked Trijuete in a bayonet charge and re-took the town, capturing numerous prisoners.
The International Brigades also saw combat in the Battle of Teruel in January 1938. The 35th International Division suffered heavily in this battle from aerial bombardment as well as shortages of food, winter clothing and ammunition. The XIV International Brigade fought in the Battle of Ebro in July 1938, the last Republican offensive of the war.
Casualties.
Although exact figures are not available, an estimated 5,857 to 25,229 brigadiers died in Spain, of an estimated 23,670 to 59,380 who served, with estimated death rates of 16.7% to 29.2%. These high casualty rates are blamed on lack of training, poor leadership and use as shock troops.
Disbandment.
In October 1938, at the height of the Battle of the Ebro, the Non-Intervention Committee ordered the withdrawal of the International Brigades which were fighting on the Republican side. The Republican government of Juan Negrín announced the decision in the League of Nations on 21 September 1938. The disbandment was part of an ill-advised effort to get the Nationalists' foreign backers to withdraw their troops and to persuade the Western democracies such as France and Britain to end their arms embargo on the Republic.
By this time there were about an estimated 10,000 foreign volunteers still serving in Spain for the Republican side, and about 50,000 foreign conscripts for the Nationalists (excluding another 30,000 Moroccans). Perhaps half of the International Brigadists were exiles or refugees from Nazi Germany, Fascist Italy or other countries, such as Hungary, which had authoritarian right-wing governments at the time. These men could not safely return home and some were instead given honorary Spanish citizenship and integrated into Spanish units of the Popular Army. The remainder were repatriated to their own countries. The Belgian and Dutch volunteers lost their citizenship because they had served in a foreign army.
Composition.
Overview.
The first brigades were composed mostly of French, Belgian, Italian, and German volunteers, backed by a sizeable contingent of Polish miners from Northern France and Belgium. The XIth, XIIth and XIIIth were the first brigades formed. Later, the XIVth and XVth Brigades were raised, mixing experienced soldiers with new volunteers. Smaller Brigades — the 86th, 129th and 150th - were formed in late 1937 and 1938, mostly for temporary tactical reasons.
About 32,000 people volunteered to defend the Spanish Republic. Many were veterans of World War I. Their early engagements in 1936 during the Siege of Madrid amply demonstrated their military and propaganda value.
The international volunteers were mainly socialists, communists, or under communist authority, and a high proportion were Jewish. Some were involved in the fighting in Barcelona against Republican opponents of the Communists: the Workers' Party of Marxist Unification (POUM) ("Partido Obrero de Unificación Marxista", an anti-Stalinist Marxist party) and anarchists. These more libertarian groups like the POUM fought together on the front with the anarchist federations of the CNT (CNT, Confederación Nacional del Trabajo) and the FAI (FAI, Iberian Anarchist Federation) who had large support in the area of Catalonia. However, overseas volunteers from anarchist, socialist, liberal and other political positions also served with the international brigades.
To simplify communication, the battalions usually concentrated people of the same nationality or language group. The battalions were often (formally, at least) named after inspirational people or events. From Spring 1937 onwards, many battalions contained one Spanish volunteer company (about 150 men).
Later in the war, military discipline tightened and learning Spanish became mandatory. By decree of 23 September 1937, the International Brigades formally became units of the Spanish Foreign Legion. This made them subject to the Spanish Code of Military Justice. However the Spanish Foreign Legion itself sided with the Nationalists throughout the coup and the civil war.
MKVD created in 1937 ‘Control and Security Service’.
Status after the war.
Since the Civil War was eventually won by the Nationalists, the Brigadiers were initially on the "wrong side" of history, especially since most of their home countries had a right-wing government (in France, for instance, the Popular Front was not in power any more).
However, since most of these countries found themselves at war with the very powers which had been supporting the Nationalists, the Brigadists gained some prestige as the first guard of the democracies, having fought a prophetical combat. Retrospectively, it was clear that the war in Spain was as much a precursor of the Second World War as a Spanish civil war.
Some glory therefore accrued to the volunteers (a great many of the survivors also fought during World War II), but this soon faded in the fear that it would promote (by association) communism.
An exception is among groups to the left of the Communist Parties, for example anarchists. Among these groups the Brigades, or at least their leadership, are criticised for their alleged role in suppressing the Spanish Revolution. An example of a modern work which promotes this view is Ken Loach's film "Land and Freedom". A well-known contemporary account of the Spanish Civil War which also takes this view is George Orwell's book "Homage to Catalonia".
East Germany.
Germany was undivided until after the Second World War. At that time, the new German Democratic Republic began to create a national identity which was separate from and antithetical to the former Nazi Germany. The Spanish Civil War, and especially the role of the International Brigades, became a substantial part of East Germany's memorial rituals because of the substantial numbers of German communists who had served in the brigades. These showcased a commitment by many Germans to antifascism at a time when Germany and Nazism were often conflated together.
Canada.
Survivors of the Mackenzie-Papineau Battalion were often investigated by the Royal Canadian Mounted Police and denied employment when they returned to Canada. Some were prevented from serving in the military during the Second World War due to "political unreliability".
In 1995 a monument to veterans of the war was built near Ontario's provincial parliament.
On February 12, 2000, a bronze statue "The Spirit of the Republic" based on an original poster from the Spanish Republic, by sculptor Jack Harman, was placed on the grounds of the BC Legislature. And in 2001, the few remaining Canadian veterans of the Spanish Civil War dedicated a monument to Canadian members of the International Brigades in Ottawa's Green Park.
Switzerland.
In Switzerland, public sympathy was high for the Republican cause, but the federal government banned all fundraising and recruiting activities a month after the start of the war so as to preserve Swiss neutrality. Around 800 Swiss volunteers joined the International Brigades, among them a small number of women. Sixty percent of Swiss volunteers identified as communists, while the others included socialists, anarchists and antifascists.
Some 170 Swiss volunteers were killed in the war. The survivors were tried by military courts upon their return to Switzerland for violating the criminal prohibition on foreign military service. The courts pronounced 420 sentences which ranged from around two weeks to four years in prison, and often also stripped the convicts of their political rights. In the judgment of Swiss historian Mauro Cerutti, volunteers were punished more harshly in Switzerland than in any other democratic country.
Motions to pardon the Swiss brigadists on the account that they fought for a just cause have been repeatedly introduced in the Swiss federal parliament. A first such proposal was defeated in 1939 on neutrality grounds. In 2002, Parliament again rejected a pardon of the Swiss war volunteers, with a majority arguing that they did break a law that remains in effect to this day. In March 2009, Parliament adopted a third bill of pardon, retroactively rehabilitating Swiss brigadists, only a handful of whom were still alive.
United Kingdom.
On disbandment, 305 British volunteers left Spain. They arrived at Victoria Station on 7 December, to be met by a crowd of supporters including Clement Attlee, Stafford Cripps, Willie Gallacher, and Will Lawther.
United States.
In the United States, the returned volunteers were labeled "premature anti-fascists" by the FBI, denied promotion during service in the US military during World War II, and pursued by Congressional committees during the Red Scare of 1947-1957. However, threats of loss of citizenship were not carried out.
Recognition.
Spain.
On 26 January 1996, the Spanish government gave Spanish citizenship to the Brigadists. At the time, roughly 600 remained. At the end of 1938, Prime Minister Juan Negrín had promised Spanish citizenship to the Brigadists, which citizenship was of course not recognized by the Nationalists who were about to take over the entire country.
France.
In 1996, Jacques Chirac, then French President, granted the former French members of the International Brigades the legal status of former service personnel ("anciens combattants") following the request of two French communist Members of Parliament, Lefort and Asensi, both children of volunteers. Before 1996, the same request was turned down several times including by François Mitterrand, the former Socialist President.
Monuments.
There is a full list of British and Irish monuments on the International Brigade Memorial Trust's website.
Symbolism and heraldry.
The International Brigades were inheritors of a socialist aesthetic.
The flags featured the colours of the Spanish Republic: red, yellow and purple, often along with socialist symbols (red flags, hammer and sickle, fist). The emblem of the brigades themselves was the three-pointed red star, which is often featured.
Notable associated people.
Note: not all the following were International Brigade members. 

</doc>
<doc id="15373" url="https://en.wikipedia.org/wiki?curid=15373" title="Iron Duke">
Iron Duke

Iron Duke may refer to:
People:
Artistic works:
Ship names:
Locomotive names:
Miscellaneous:

</doc>
<doc id="15374" url="https://en.wikipedia.org/wiki?curid=15374" title="Food irradiation">
Food irradiation

Food irradiation is the process of exposing foodstuffs to ionizing radiation. Ionizing radiation is energy that can be transmitted without direct contact (radiation) capable of freeing electrons from their atomic bonds (ionization) in the targeted food. This treatment is used to preserve food, reduce the risk of food borne illness, prevent the spread of invasive pests, and delay or eliminate sprouting or ripening. Irradiated food does not become radioactive. The radiation can be emitted by a radioactive substance or generated electrically.
Irradiation is also used for non-food applications, such as medical devices.
Although there have been concerns about the safety of irradiated food, a large amount of independent research has confirmed it to be safe. One family of chemicals is uniquely formed by irradiation, and this product is nontoxic. When heating food, all other chemicals occur in a lower or comparable frequency. Others criticize irradiation because of confusion with radioactive contamination or because of negative impressions of the nuclear industry.
Food irradiation is permitted by over 60 countries, with about 500,000 metric tons of food annually processed worldwide. The regulations that dictate how food is to be irradiated, as well as the food allowed to be irradiated, vary greatly from country to country. In Austria, Germany, and many other countries of the European Union only dried herbs, spices, and seasonings can be processed with irradiation and only at a specific dose, while in Brazil all foods are allowed at any dose.
Uses.
Irradiation is used to reduce the pathogens in foods. Depending on the dose, some or all of the microorganisms, bacteria, and viruses present are destroyed, slowed down, or rendered incapable of reproduction. This reduces or eliminates the risk of food borne illnesses. Some foods are irradiated at sufficient doses to ensure that the product is sterilized and does not add any spoilage or pathogenic microorganisms into the final product.
Irradiation is used to delay the ripening of fruits and the sprouting of vegetables by slowing down the speed at which enzymes produced by the food can change the food. By halting or slowing down spoilage and slowing down the ripening of food, irradiation prolongs the shelf life of goods. Irradiation cannot revert spoiled or over ripened food to a fresh state. If this food was processed by irradiation, spoilage would cease and ripening would slow down, yet the irradiation would not destroy the toxins or repair the texture, color, or taste of the food.
Insect pests are sterilized using irradiation at relatively low doses of irradiation. This stops the spread of foreign invasive species across national boundaries, and allows foods to pass quickly through quarantine and avoid spoilage. Depending on the dose, some or all of the insects present are destroyed, or rendered incapable of reproduction.
Public perception and impact.
Irradiation has been approved by the FDA for over 50 years, but the only major growth area for the commercial sale of irradiated foods for human consumption is fruits and vegetables that are irradiated to kill insects for the purpose of quarantine. In the early 2000s in the US, irradiated meat was common at some grocery stores, but because of lack of consumer demand it is no longer common. Because consumer demand for irradiated food is low, reducing the spoilage between manufacture and consumer purchase and reducing the risk of food borne illness is currently not sufficient incentive for most manufactures to supplement their process with irradiation.
It is widely believed that consumer perception of foods treated with irradiation is more negative than those processed by other means, although some industry studies indicate the number of consumers concerned about the safety of irradiated food has decreased in the last 10 years to levels comparable to those of people concerned about food additives and preservatives. “These irradiated foods are not less safe than others,” Dr. Tarantino said, “and the doses are effective in reducing the level of disease-causing micro-organisms.” "People think the product is radioactive," said Harlan Clemmons, president of Sadex, a food irradiation company based in Sioux City, Iowa.
Some common concerns about food irradiation include the impact of irradiation on food chemistry, as well as the cumulative impacts of the irradiation process and the indirect effects of irradiation becoming a prevalent in the food handling process. Irradiation reduces the risk of infection and spoilage, does not make food radioactive, and the food is shown to be safe, but it does cause chemical reactions that alter the food and therefore alters the chemical makeup, nutritional content, and the sensory qualities of the food. Some of the potential secondary impacts of irradiation are hypothetical, while others are demonstrated. These effects include cumulative impacts to pathogens, people, and the environment due to the reduction of food quality, the transpiration and storage of radioactive goods, and destruction of pathogens. People are also concerned about the indirect effects of irradiation, such as the way irradiation changes the way we relate to food and how irradiation changes the food production and shipping industries. Because of these concerns and the increased cost of irradiated foods, there is not a widespread public demand for the irradiation of foods for human consumption.
Direct effects of irradiation.
The Direct effects of irradiation are immediate effects of irradiation. This includes the changes to food chemistry.
The irradiation source supplies energetic particles or waves. As these waves/particles pass through a target material they collide with other particles. Around the sites of these collisions chemical bonds are broken, creating short lived radicals (e.g. the hydroxyl radical, the hydrogen atom and solvated electrons). These radicals cause further chemical changes by bonding with and or stripping particles from nearby molecules. When collisions damage DNA or RNA, effective reproduction becomes unlikely, also when collisions occur in cells, cell division is often suppressed.
Irradiation can not make food radioactive, but it does reduce the nutritional content and change the flavor (much like cooking), produce radiolytic products, and increase the number of free radicals in the food.
Irradiation causes a multitude of chemical changes including introducing radiolytic products and free radicals. A few of these products are unique, but not considered dangerous. The scale of these chemical changes is not unique. Cooking, smoking, salting, and other less novel techniques, cause the food to be altered so drastically that its original nature is almost unrecognizable, and must be called by a different name. Storage of food also causes dramatic chemical changes, ones that eventually lead to deterioration and spoilage.
Misconceptions.
A major concern is that irradiation might cause chemical changes that are harmful to the consumer. Several national expert groups and two international expert groups evaluated the available data and concluded that any food at any dose is wholesome and safe to consume as long as it remains palatable and maintains its technical properties (e.g. feel, texture, or color).
Irradiated food does not become radioactive, just as an object exposed to light does not start producing light. Radioactivity is the ability of a substance to emit high energy particles. When particles hit the target materials they may free other highly energetic particles. This ends shortly after the end of the exposure, much like objects stop reflecting light when the source is turned off and warm objects emit heat until they cool down but do not continue to produce their own heat. To modify a material so that it keeps emitting radiation (induce radiation) the atomic cores (nucleus) of the atoms in the target material must be modified.
It is impossible for food irradiators to induce radiation into a product. Irradiators emit electrons or photons and the radiation is intrinsically radiated at precisely known strengths (wavelengths for photons, and speeds for electrons). These radiated particles at these strengths can never be strong enough to modify the nucleus of the targeted atom in the food, regardless of how many particles hit the target material, and radioactivity can not be induced without modifying the nucleus.
Food quality.
Because of the extent of the chemical reactions, changes to the foods quality after irradiation are inevitable. The nutritional content of food, as well as the sensory qualities (taste, appearance, and texture) is impacted by irradiation. Because of this food advocacy groups consider labeling irradiated food raw as misleading. However, the degradation of vitamins caused by irradiation is similar or even less than the loss caused by other food preservation processes. Other processes like chilling, freezing, drying, and heating also result in some vitamin loss.
The changes in quality and nutrition vary greatly from food to food. The changes in the flavor of fatty foods like meats, nuts and oils are sometimes noticeable, while the changes in lean products like fruits and vegetables are less so. Some studies by the irradiation industry show that for some properly treated fruits and vegetables irradiation is seen by consumers to improve the sensory qualities of the product compared to untreated fruits and vegetables.
There has been low level gamma irradiation that has been attempted on arugula, spinach, cauliflower, ash gourd, bamboo shoots, coriander, parsley, and watercress. There has been limited information, however, regarding the physical, chemical and/or bioactive properties and the shelf life on these minimally processed vegetables.
Radiolytic products and free radicals.
The formation of new, previously unknown chemical compounds (unique radiolytic products) via irradiation is a concern. Most of the substances found in irradiated food are also found in food that has been subjected to other food processing treatments, and are therefore not unique. Furthermore, the quantities in which they occur in irradiated food are lower or similar to the quantities formed in heat treatments.
When fatty acids are irradiated, a family of compounds called 2-alkylcyclobutanones (2-ACBs) are produced. These are thought to be unique radiolytic products.
The radiation doses to cause toxic changes are much higher than the doses needed to accomplish the benefits of irradiation, and taking into account the presence of 2-ACBs along with what is known of free radicals, these results lead to the conclusion that there is no significant risk from radiolytic products.
Compounds known as free radicals form when food is irradiated. Most of these are oxidizers (i.e., accept electrons) and some react very strongly. According to the free-radical theory of aging excessive amounts of these free radicals can lead to cell injury and cell death, which may contribute to many diseases. However, this generally relates to the free radicals generated in the body, not the free radicals consumed by the individual, as much of these are destroyed in the digestive process.
Cumulative impacts of irradiation.
The cumulative impacts of irradiation are the concerns and benefits of irradiation that are not directly related to the chemical changes that occur when food is irradiated, but instead are directly related to what would occur with the continued use and production of irradiated foods.
If the majority of food was irradiated at high enough levels to decrease its nutritional content significantly, there could be an increase in nutritional deficiencies due to a diet composed entirely of irradiated foods. Furthermore, for at least 3 studies on cats, the consumption of irradiated food was associated with a loss of tissue in the myelin sheath, leading to reversible paralysis. Researchers suspect that reduced levels of vitamin A and high levels of free radicals may be the cause. This effect is thought to be specific to cats and has not been reproduced in any other animal. To produce these effects the cats were fed solely on food that was irradiated at a dose at least five times higher than the maximum allowable dose.
It may seem reasonable to assume that irradiating food might lead to radiation-tolerant strains, similar to the way that strains of bacteria have developed resistance to antibiotics. Bacteria develop a resistance to antibiotics after an individual uses antibiotics repeatedly. Much like pasteurization plants products that pass through irradiation plants are processed once, and are not processed and reprocessed. Cycles of heat treatment have been shown to produce heat tolerant bacteria, yet no problems have appeared so far in pasteurization plants. Furthermore, when the irradiation dose is chosen to target a specific species of microbe, it is calibrated to doses several times the value required to target the species. This ensures that the process randomly destroys all members of a target species. Therefore, the more irradiation tolerant members of the target species are not given any evolutionary advantage. Without evolutionary advantage selection does not occur. As to the irradiation process directly producing mutations that lead to more virulent, radiation resistant, strains the European Commission's Scientific Committee on Food found that there is no evidence, on the contrary, irradiation has been found to cause loss of virulence and infectivity as mutants are usually less competitive and less adapted."
Misconceptions.
The argument is made that there is a lack of long-term studies, and therefore the safety of irradiated food is not scientifically proven in spite of the fact that hundreds of animal feeding studies of irradiated food, including multigenerational studies, have been performed since 1950. Endpoints investigated have included subchronic and chronic changes in metabolism, histopathology, and function of most systems; reproductive effects; growth; teratogenicity; and mutagenicity. A large number of studies have been performed; meta-studies have supported the safety of irradiated food.
The below experiments are cited by food irradiation opponents, but could be either not verified in later experiments, could not be clearly attributed to the radiation effect, or could be attributed to an inappropriate design of the experiment etc.
Indirect effects of irradiation.
The indirect effects of irradiation are the concerns and benefits of irradiation that are related to how making food irradiation a common process will change the world, with emphasis on the system of food production.
If irradiation was to become common in the food handling process there would be a reduction of the prevalence of foodborne illness and potentially the eradication of specific pathogens. However, multiple studies suggest that an increased rate of pathogen growth may occur when irradiated food is cross-contaminated with a pathogen, as the competing spoilage organisms are no longer present. This being said, cross contamination itself becomes less prevalent with an increase in usage of irradiated foods.
The ability to remove bacterial contamination through post-processing by irradiation may reduce the fear of mishandling food which could cultivate a cavalier attitude toward hygiene and result in contaminants other than bacteria. However, concerns that the pasteurization of milk would lead to increased contamination of milk were prevalent when mandatory pasteurization was introduced, but these fears never materialized after adoption of this law. Therefore, it is unlikely for irradiation to cause an increase of illness due to nonbacteria-based contamination.
Treatment.
Up to the point where the food is processed by irradiation, the food is processed in the same way as all other food. To treat the food, they are exposed to a radioactive source, for a set period of time to achieve a desired dose. Radiation may be emitted by a radioactive substance, or by X-ray and electron beam accelerators. Special precautions are taken to ensure the food stuffs never come in contact with the radioactive substances and that the personnel and the environment are protected from exposure radiation.
Irradiation treatments are typically classified by dose (high, medium, and low), but are sometimes classified by the effects of the treatment (radappertization, radicidation and radurization). Food irradiation is sometimes referred to as "cold pasteurization" or "electronic pasteurization" because ionizing the food does not heat the food to high temperatures during the process, and the effect is similar to heat pasteurization. The term "cold pasteurization" is controversial because the term may be used to disguise the fact the food has been irradiated and pasteurization and irradiation are fundamentally different processes.
Treatment costs vary as a function of dose and facility usage. A pallet or tote is typically exposed for several minutes to hours depending on dose. Low-dose applications such as disinfestation of fruit range between US$0.01/lbs and US$0.08/lbs while higher-dose applications can cost as much as US$0.20/lbs.
Process.
Typically, when the food is being irradiated, pallets of food are exposed to a source of radiation for a specific time. Dosimeters are embedded in the pallet (at various locations) of food to determine what dose was achieved.
Most irradiated food is processed by gamma irradiation., however the usage of electron beam and X-ray is becoming more popular as well [http://www.sciencedirect.com/science/article/pii/0969806X9390312I]. Special precautions are taken because gamma rays are continuously emitted by the radioactive material. In most designs, to nullify the effects of radiation, the radioisotope is lowered into a water-filled storage pool, which absorbs the radiation but does not become radioactive. This allows pallets of the products to be added and removed from the irradiation chamber and other maintenance to be done. Sometimes movable shields are used to reduce radiation levels in areas of the irradiation chamber instead of submerging the source. For x ray and electron irradiation these precautions are not necessary as the source of the radiation can be switched off.
For x-ray, gamma ray and electron irradiation, shielding is required when the foods are being irradiated. This is done to protect workers and the environment outside of the chamber from radiation exposure. Typically permanent or movable shields are used. In some gamma irradiators the radioactive source is under water at all times, and the hermetically sealed product is lowered into the water. The water acts as the shield in this application. Because of the lower penetration depth of electron irradiation, treatment to entire industrial pallets or totes is not possible.
Dosimetry.
The radiation absorbed dose is the amount energy absorbed per unit weight of the target material. Dose is used because, when the same substance is given the same dose, similar changes are observed in the target material. The SI unit for dose is grays (Gy or J/kg). Dosimeters are used to measure dose, and are small components that, when exposed to ionizing radiation, change measurable physical attributes to a degree that can be correlated to the dose received. Measuring dose (dosimetry) involves exposing one or more dosimeters along with the target material.
For purposes of legislation doses are divided into low (up to 1 kGy), medium (1 kGy to 10 kGy), and high-dose applications (above 10 kGy). High-dose applications are above those currently permitted in the US for commercial food items by the FDA and other regulators around the world. Though these doses are approved for non commercial applications, such as sterilizing frozen meat for NASA astronauts (doses of 44 kGy) and food for hospital patients.
Dosimetry on Minimally Processed Vegetables.
Watercress ("Nasturtium Officinale") is a rapidly growing aquatic or semi aquatic perennial plant. It contains health promoting phytochemicals endowed in therapeutic properties. Because chemical agents do not provide an efficient microbial reductions, watercress has been tested with gamma irradiation treatment in order to improve both safety and the shelf life of the product. It is traditionally used on horticultural products to prevent sprouting and post-packaging contamination, delay post-harvest ripening, maturation and senescence.
In a Food Chemistry food journal, scientists studied the suitability of gamma irradiation of 1, 2, and 5 kGy for preserving quality parameters of the fresh cut watercress at around 4 degrees Celsius for 7 days. They determined that a 2 kGy dose of irradiation was the does that contained most similar qualities to non-stored control samples, which is one of the goals of irradiation. 2 kGy preserved high levels of reducing sugars and favoured PUFA; while samples of the 5 kGy dose revealed high contents of sucrose and MUFA. Both cases the watercress samples obtained healthier fatty acids profiles. However, a 5kGy dose better preserved the antioxidant activity and total flavonoids.
Technology.
Electron irradiation uses electrons accelerated in an electric field to a velocity close to the speed of light. Electrons have a charge, and therefore do not penetrate the product beyond a few centimeters, depending on product density.
Gamma irradiation involves exposing the target material to packets of light (photons) that are highly energetic (Gamma rays). A radioactive material (radioisotopes) is used as the source for the gamma rays. Gamma irradiation is the standard because the deeper penetration of the gamma rays enables administering treatment to entire industrial pallets or totes (reducing the need for material handling) and it is significantly less expensive than using a X-ray source. Generally cobalt-60 is used as a radioactive source for gamma irradiation. Cobalt-60 is bred from cobalt-59 using neutron irradiation in specifically designed nuclear reactors. In limited applications caesium-137, a less costly alternative recovered during the processing of spent nuclear fuel, is used as a radioactive source. Insufficient quantities are available for large scale commercial use. An incident where water-soluble caesium-137 leaked into the source storage pool requiring NRC intervention has led to near elimination of this radioisotope outside of military applications.
Irradiation by X-ray is similar to irradiation by gamma rays in that less energetic packets of light (X-rays) are used. X-rays are generated by colliding accelerated electrons with a dense material (this process is known as bremsstrahlung-conversion), and therefore do not necessitate the use of radioactive materials.
X-rays ability to penetrate the target is similar to gamma irradiation. X-ray machines produce better dose uniformity than Gamma irradiation but require much more electricity as only as much as 12% of the input energy is converted into X-rays.
Cost.
The cost of food irradiation is influenced by dose requirements, the food's tolerance of radiation, handling conditions, i.e., packaging and stacking requirements, construction costs, financing arrangements, and other variables particular to the situation. Irradiation is a capital-intensive technology requiring a substantial initial investment, ranging from $1 million to $5 million. In the case of large research or contract irradiation facilities, major capital costs include a radiation source, hardware (irradiator, totes and conveyors, control systems, and other auxiliary equipment), land (1 to 1.5 acres), radiation shield, and warehouse. Operating costs include salaries (for fixed and variable labor), utilities, maintenance, taxes/insurance, cobalt-60 replenishment, general utilities, and miscellaneous operating costs.
Regulations and international standards.
The Codex Alimentarius represents the global standard for irradiation of food, in particular under the WTO-agreement. Member states are free to convert those standards into national regulations at their discretion, therefore regulations about irradiation differ from country to country.
The United Nations Food and Agricultural Organization (FAO) has passed a motion to commit member states to implement irradiation technology for their national phytosanitary programs; the General assembly of the International Atomic Energy Agency (IAEA) has urged wider use of the irradiation technology.
Labeling regulations and international standards.
The provisions of the Codex Alimentarius are that any "first generation" product must be labeled "irradiated" as any product derived directly from an irradiated raw material; for ingredients the provision is that even the last molecule of an irradiated ingredient must be listed with the ingredients even in cases where the unirradiated ingredient does not appear on the label. The RADURA-logo is optional; several countries use a graphical version that differs from the Codex-version. The suggested rules for labeling is published at CODEX-STAN – 1 (2005), and includes the usage of the Radura symbol for all products that contain irradiated foods. The Radura symbol is not a designator of quality. The amount of pathogens remaining is based upon dose and the original content and the dose applied can vary on a product by product basis.
The European Union follows the Codex's provision to label irradiated ingredients down to the last molecule of irradiated food. The European Community does not provide for the use of the Radura logo and relies exclusively on labeling by the appropriate phrases in the respective languages of the Member States. The European Union enforces its irradiation labeling laws by requiring its member countries to perform tests on a cross section of food items in the market-place and to report to the European Commission. The results are published annually in the OJ of the European Communities.
The US defines irradiated foods as foods in which the irradiation causes a material change in the food, or a material change in the consequences that may result from the use of the food. Therefore, food that is processed as an ingredient by a restaurant or food processor is exempt from the labeling requirement in the US. This definition is not consistent with the Codex Alimentarius. All irradiated foods must bear a slightly modified Radura symbol at the point of sale and use the term "irradiated" or a derivative there of, in conjunction with explicit language describing the change in the food or its conditions of use.
Food safety regulations and international standards.
In 2003, the Codex Alimentarius removed any upper dose limit for food irradiation as well as clearances for specific foods, declaring that all are safe to irradiate. Countries such as Pakistan and Brazil have adopted the Codex without any reservation or restriction. Other countries, including New Zealand, Australia, Thailand, India, and Mexico, have permitted the irradiation of fresh fruits for fruit fly quarantine purposes, amongst others.
Standards that describe calibration and operation for radiation dosimetry, as well as procedures to relate the measured dose to the effects achieved and to report and document such results, are maintained by the American Society for Testing and Materials (ASTM international) and are also available as ISO/ASTM standards.
All of the rules involved in processing food are applied to all foods before they are irradiated.
United States clearances.
In the United States, each new food is approved separately with a guideline specifying a maximum dosage; in case of quarantine applications the minimum dose is regulated. Packaging materials containing the food processed by irradiation must also undergo approval. Food irradiation in the United States is primarily regulated by the FDA since it is considered a food additive. The United States Department of Agriculture (USDA) amends these rules for use with meat, poultry, and fresh fruit.
The United States Department of Agriculture (USDA) has approved the use of low-level irradiation as an alternative treatment to pesticides for fruits and vegetables that are considered hosts to a number of insect pests, including fruit flies and seed weevils. Under bilateral agreements that allows less-developed countries to earn income through food exports agreements are made to allow them to irradiate fruits and vegetables at low doses to kill insects, so that the food can avoid quarantine.
The U.S. Food and Drug Administration (FDA) and the USDA have approved irradiation of the following foods and purposes:
European Union clearances.
European law dictates that no foods other than dried aromatic herbs, spices and vegetable seasonings are permitted for the application of irradiation. However, any Member State is permitted to maintain previous clearances that are in categories that the EC's Scientific Committee on Food (SCF) had previously approved, or add clearance granted to other Member States. Presently, Belgium, Czech Republic, France, Italy, Netherlands, Poland, and the United Kingdom) have adopted such provisions. Before individual items in an approved class can be added to the approved list, studies into the toxicology of each of such food and for each of the proposed dose ranges are requested. It also states that irradiation shall not be used "as a substitute for hygiene or health practices or good manufacturing or agricultural practice". These regulations only govern food irradiation in consumer products to allow irradiation to be used for patients requiring sterile diets.
Because of the Single Market of the EC any food, even if irradiated, must be allowed to be marketed in any other Member State even if a general ban of food irradiation prevails, under the condition that the food has been irradiated legally in the state of origin.
Furthermore, imports into the EC are possible from third countries if the irradiation facility had been inspected and approved by the EC and the treatment is legal within the EC or some Member state.
Australia ban on irradiated cat food.
Australia banned irradiated cat food after a national scare where cats suffered from paralyzation after eating a specific brand of highly irradiated catfood for an extended period of time. The suspected culprit was malnutrition from consuming food depleted of Vitamin A by the irradiation process. The incident was linked only to a single batch of one brand's product and no illness was linked to any of that brand's other irradiated batches of the same product or to any other brand of irradiated cat food. This, along with incomplete evidence indicating that the cat food was not sufficiently depleted of Vitamin A makes irradiation a less likely cause. Further research has been able to experimentally induce the paralyzation of cats by via Vitamin A deficiency by feeding highly irradiated food. For more details see the cumulative impacts of irradiation section.
Nuclear and employee safety regulations.
Interlocks and safeguards are mandated to minimize this risk. There have been radiation related accidents, deaths, and injury at such facilities, many of them caused by operators overriding the safety related interlocks. In a radiation processing facility, radiation specific concerns are supervised by special authorities, while "Ordinary" occupational safety regulations are handled much like other businesses.
The safety of irradiation facilities is regulated by the United Nations International Atomic Energy Agency and monitored by the different national Nuclear Regulatory Commissions. The regulators enforce a safety culture that mandates that all incidents that occur are documented and thoroughly analyzed to determine the cause and improvement potential. Such incidents are studied by personnel at multiple facilities, and improvements are mandated to retrofit existing facilities and future design.
In the US the Nuclear Regulatory Commission (NRC) regulates the safety of the processing facility, and the United States Department of Transportation (DOT) regulates the safe transport of the radioactive sources.
Irradiated food supply.
There are analytical methods available to detect the usage of irradiation on food items in the marketplace. This is used as a tool for government authorities to enforce existing labeling standards and to bolster consumer confidence. Phytosanitary irradiation of fruits and vegetables has been increasing globally. In 2010, 18446 tonnes of fruits and vegetables were irradiated in six countries for export quarantine control; the countries follow: Mexico (56.2%), United States (31.2%), Thailand (5.18%), Vietnam (4.63%), Australia (2.69%), and India (0.05%). The three types of fruits irradiated the most were guava (49.7%), sweet potato(29.3%) and sweet lime (3.27%).
In total, 103 000 tonnes of food products were irradiated on mainland United States in 2010. The three types of foods irradiated the most were spices (77.7%), fruits and vegetables (14.6%) and meat and poultry (7.77%). 17 953 tonnes of irradiated fruits and vegetables were exported to the mainland United States. Mexico, the United States' state of Hawaii, Thailand, Vietnam and India export irradiated produce to the mainland U.S. Mexico, followed by the United States' state of Hawaii, is the largest exporter of irradiated produce to the mainland U.S.
In total, 7 972 tonnes of food products were irradiated in European Union countries in 2012; mainly in three member state countries: Belgium (64.7%), the Netherlands (18.5%) and France (7.7%). The three types of foods irradiated the most were frog legs (36%), poultry (35%) and dried herbs and spices (15%). The European Union's official site gives information on the regulatory status of food irradiation, the quantities of foods irradiated at authorized facilities in European Union member states and the results of market surveillance where foods have been tested to see if they are irradiated. The Official Journal of the European Union publishes annual reports on food irradiation, the current report covers the period from January 1, 2012 to December 31, 2012 and compiles information from 27 member States.

</doc>
<doc id="15378" url="https://en.wikipedia.org/wiki?curid=15378" title="IUD with copper">
IUD with copper

A copper IUD (ATC ) (also intrauterine device, IUD, copper-T, or coil) is a type of intrauterine device. The copper IUD is a type of long-acting reversible contraception and is one of the most effective forms of birth control.
It is on the World Health Organization's List of Essential Medicines, the most important medication needed in a basic health system.
Medical uses.
Copper IUDs are a form of long-acting reversible contraception and are one of the most effective forms of birth control available. The type of frame and amount of copper can affect the effectiveness of different copper IUD models. The failure rates for different models vary between 0.1 and 2.2% after 1 year of use. The T-shaped models with a surface area of 380 mm² of copper have the lowest failure rates. The TCu 380A (ParaGard) has a one-year failure rate of 0.8% and a cumulative 12-year failure rate of 2.2%. Over 12 years of use, the models with less surface area of copper have higher failure rates. The TCu 220A has a 12-year failure rate of 5.8%, whereas the TCu 380A has a 12-year failure rate of 2.2%. The frameless GyneFix also has a failure rate of less than 1% per year. Worldwide, older IUD models with lower effectiveness rates are no longer produced.
Unlike other forms of reversible contraception, the typical use failure rate and the perfect use failure rate for the copper IUDs are the same because the IUD does not depend on user action. A 2008 review of the available T-shaped copper IUDs recommended that the TCu 380A and the TCu 280S be used as the first choice for copper IUDs because those two models have the lowest failure rates and the longest lifespans. The effectiveness of the copper IUD is comparable to tubal sterilization, which has a first year failure rate of 0.5%. However, the effects of the copper IUD are reversible, which can be viewed as either an advantage or a disadvantage, depending on a woman's goals for contraception.
Emergency contraception.
It was first discovered in 1976 that the copper IUD could be used as a form of emergency contraception (EC).
The copper IUD is the most effective form of emergency contraception. It is more effective than the hormonal EC pills currently available.
The pregnancy rate among those using the copper IUD for EC is 0.09%. It can be used for EC up to 5 days after the act of unprotected sex and does not decrease in effectiveness during the 5 days. 
An additional advantage of using the copper IUD for emergency contraception is that it can be used as a form of birth control for 10–12 years after insertion.
Removal and return to fertility.
Removal of the copper IUD should also be performed by a qualified medical practitioner. Fertility has been shown to return to previous levels quickly after removal of the device. One study found that the median amount of time from removal to planned pregnancy was three months for those women using the TCu 380Ag.
Adverse effects.
Expulsion: Sometimes the copper IUD can be spontaneously expelled from the uterus. Expulsion rates can range from 2.2% to 11.4% of users from the first year to the 10th year. The TCu380A may have lower rates of expulsion than other models. Unusual vaginal discharge, cramping or pain, spotting between periods, postcoital (after sex) spotting, dyspareunia, or the absence or lengthening of the strings can be signs of a possible expulsion. If expulsion occurs, the woman is not protected against pregnancy. If an IUD with copper is inserted after an expulsion has occurred, the risk of re-expulsion has been estimated in one study to be approximately one third of cases after one year.
Perforation: Very rarely, the IUD can move through the wall of the uterus. Risk of perforation is mostly determined by the skill of the practitioner performing the insertion. For experienced medical practitioners, the risk of perforation is 1 per 1,000 insertions or less. If perforation does occur it can damage the internal organs, and in some cases surgery is needed to remove the IUD.
Infection: The insertion of a copper IUD poses a transient risk of pelvic inflammatory disease (PID) in the first 21 days after insertion. However, it is a small risk and is attributable to preexisting gonorrhea or chlamydia infection at the time of insertion, and not to the IUD itself. Proper infection prevention procedures have little or no effect on the course of gonorrhea or chlamydia infections, but is important in helping protect both clients and providers from infection in general. Such infection prevention practices include washing hands and then putting on gloves, cleaning the cervix and vagina, making minimal contact with non-sterile surfaces (using a "no touch insertion technique") and, after the procedure, washing hands again and then processing instruments. The device itself carries no increased risk of PID beyond the time of insertion.
Cramping: Many women feel cramping or pain during the IUD insertion process and immediately after as a result of cervix dilation during insertion. Taking NSAIDS before the procedure can reduce discomfort, as can the use of a local anaesthetic. Misoprostol 6 to 12 hrs before insertion can help with cervical dilation. Some women may have cramps for 1 to 2 weeks following insertion. The copper IUD can also increase cramps during a woman’s period. This symptom will clear up for some women in 3 to 6 months, but may not for others.
Heavier Periods: The copper IUD increases the amount of blood flow during a woman’s menstrual periods. On average, menstrual blood loss increases by 20–50% after insertion of a copper-T IUD; increased menstrual discomfort is the most common medical reason for IUD removal. This symptom may clear up for some women after 3 to 6 months, but may not for others.
Irregular Bleeding and Spotting: For the first 3 to 6 months after insertion, the copper IUD can cause irregular periods and spotting between periods.
String problems: A small portion of men report that they can feel the strings during intercourse. In this case, strings can be trimmed. However, very short strings can prevent the woman from checking the strings for expulsion. Medical ultrasonography may be required in such cases to check the location of the IUD.
Pregnancy: Although rare, if pregnancy does occur with the copper IUD in place there can be serious side effects. The risk of ectopic pregnancy to a woman using an IUD is lower than the risk of ectopic pregnancy to a woman using no form of birth control. However, of pregnancies that do occur during IUD use, a higher than expected percentage (3–4%) are ectopic. If a pregnancy occurs with the IUD in place there is a higher risk of miscarriage or early delivery. If this occurs and the IUD strings are visible, the IUD should be removed immediately by a clinician. Although the Dalkon Shield IUD was associated with septic abortions (infections associated with miscarriage), other brands of IUD are not. IUDs are also "not" associated with birth defects.
Some barrier contraceptives protect against STIs. Hormonal contraceptives reduce the risk of developing pelvic inflammatory disease (PID), a serious complication of certain STIs. IUDs, by contrast, do "not" protect against STIs or PID.
Contraindications.
A category 3 condition indicates conditions where the theoretical or proven risks usually outweigh the advantages of inserting a copper IUD. A category 4 condition indicates conditions that represent an unacceptable health risk if a copper IUD is inserted.
Women should not use a copper IUD if they:
(Category 3) 
A full list of contraindications can be found in the WHO "Medical Eligibility Criteria for Contraceptive Use" and the CDC "United States Medical Eligibility Criteria for Contraceptive Use".
While nulliparous women (women who have never given birth) are somewhat more likely to have side effects, this is not a contraindication for IUD use. Overall, IUDs are safe and acceptable also in young nulliparous women. The same is likely the case for virgin women, unless there is a microperforate hymen that obstructs any insertion of the IUD.
Device description.
There are a number of models of the copper IUD available around the world. Most copper devices consist of a plastic core that is wrapped in a copper wire.
Many of the devices, including the TCu 380A (ParaGard), have a T-shape similar to the hormonal IUD. However, there are "frameless" copper IUDs available around the world as well. ParaGard is the only model currently available in the United States. At least three copper IUD models are available in Canada, two of which are a slimmer T-shape version used for women who have not had children. Early copper IUDs had copper around only the vertical stem, but more recent models have copper sleeves wrapped around the horizontal arms as well, increasing effectiveness.
Some newer models also contain a silver core instead of a plastic core to delay copper fragmentation as well as increase the lifespan of the device. The lifespan of the devices range from 3 years to 10 years; however, some studies have demonstrated that the TCu 380A may be effective through 12 years.
Insertion.
The copper IUD must be inserted by a qualified medical practitioner. A copper IUD can be inserted at any phase of the menstrual cycle, but the optimal time is right after the menstrual period, when the cervix is softest and the woman is least likely to be pregnant. The insertion process generally takes five minutes or less. The procedure can cause cramping or be painful for some women. 
Before placement of an IUD, a medical history and physical examination by a medical professional is useful to check for any contraindications or concerns. It is also recommended by some clinicians that patients be tested for gonorrhea and chlamydia, as these two infections increase the risk of contracting pelvic inflammatory disease shortly after insertion.
Immediately prior to insertion, the clinician will perform a pelvic exam to determine the position of the uterus.Mechanism of action Although the precise mechanism of action is not known, currently available IUCs work primarily by preventing sperm from fertilizing ova.26 IUCs are not abortifacients: they do not interrupt an implanted pregnancy.27 Pregnancy is prevented by a combination of the "foreign body effect" of the plastic or metal frame and the specific action of the medication (copper or levonorgestrel) that is released. Exposure to a foreign body causes a sterile inflammatory reaction in the intrauterine environment that is toxic to sperm and ova and impairs implantation.28,29 The production of cytotoxic peptides and activation of enzymes lead to inhibition of sperm motility, reduced sperm capacitation and survival, and increased phagocytosis of sperm.30,31 The TCu380A causes an increase in copper ions, enzymes, prostaglandins, and white blood cells (macrophages) in uterine and tubal fluids; these impair sperm function and prevent fertilization.p. 162:<br>Table 7-1. Myths and misconceptions about IUCs<br>Myth: IUCs are abortifacients. Fact: IUCs prevent fertilization and are true contraceptives.</ref>
After the pelvic exam, the vagina is held open with a speculum, the same device used during a pap smear. A tenaculum is used to steady the cervix and uterus. Uterine sounding may be used to measure the length and direction of the cervical canal and uterus in order to decrease the risk of uterine perforation. The IUD is placed using a narrow tube, which is inserted through the cervix into the uterus. Short monofilament plastic/nylon strings hang down from the uterus into the vagina. The clinician will trim the threads so that they only protrude 3 to 4 cm out of the cervix and remain in the upper vagina. The strings allow the patient or clinician to periodically check to ensure the IUD is still in place and to enable easy removal of the device.
The copper IUD can be inserted at any time in a woman's menstrual cycle as long as the woman is not pregnant. An IUD can also be inserted immediately postpartum and post-abortion as long as no infection has occurred. Breastfeeding is not a contraindication for the use of the copper IUD. The IUD can be inserted in women with HIV or AIDS as it does not increase the risk of transmission.
Although previously not recommended for nulliparous women (women who have not had children), the IUD is now recommended for most women who are past menarche (their first period), including adolescents.
After the insertion is finished, normal activities such as sex, exercise, and swimming can be performed as soon as it feels comfortable. Strenuous physical activity does not affect the position of the IUD.
Types.
Many different types of copper IUDs are currently manufactured worldwide, but availability varies by country. In the United States, only one type of copper IUD is approved for use, while in the United Kingdom, over ten varieties are available. One company, Mona Lisa N.V. offers generic versions of many existing IUDs.
Frameless IUDs.
The frameless IUD eliminates the use of the frame that gives conventional IUDs their signature T-shape. This change in design was made to reduce discomfort and expulsion associated with prior IUDs; without a solid frame, the frameless IUD should mold to the shape of the uterus. It may reduce expulsion and discontinuation rates compared to framed copper IUDs.
Gynefix is the only frameless IUD brand currently available. It consists of hollow copper tubes on a polypropylene thread. It is inserted through the cervix with a special applicator that sutures the thread to the fundus (top) of the uterus; the thread is then cut with a tail hanging outside of the cervix, similar to frame IUDs such as the ParaGard. When this tail is pulled, the suture comes undone and the device can be removed. This requires more force than removing a T-shaped IUD, but results in comparable discomfort during removal. Gynefix is not approved for use in the United States.
Mechanism of action.
The copper IUD's primary mechanism of action is to prevent fertilization.ConclusionsActive substances released from the IUD or IUS, together with products derived from the inflammatory reaction present in the luminal fluids of the genital tract, are toxic for spermatozoa and oocytes, preventing the encounter of healthy gametes and the formation of viable embryos. The current data do not indicate that embryos are formed in IUD users at a rate comparable to that of nonusers. The common belief that the usual mechanism of action of IUDs in women is destruction of embryos in the uterus is not supported by empirical evidence. The bulk of the data indicate that interference with the reproductive process after fertilization has taken place is exceptional in the presence of a T-Cu or LNG-IUD and that the usual mechanism by which they prevent pregnancy in women is by preventing fertilization.</ref>Mechanism of actionThe contraceptive action of all IUDs is mainly in the intrauterine cavity. Ovulation is not affected, and the IUD is not an abortifacient.58–60 It is currently believed that the mechanism of action for IUDs is the production of an intrauterine environment that is spermicidal.Nonmedicated IUDs depend for contraception on the general reaction of the uterus to a foreign body. It is believed that this reaction, a sterile inflammatory response, produces tissue injury of a minor degree but sufficient enough to be spermicidal. Very few, if any, sperm reach the ovum in the fallopian tube… In women using copper IUDs, sensitive assays for human chorionic gonadotropin do not find evidence of fertilization.62,63 This is consistent with the fact that the copper IUD protects against both intrauterine and ectopic pregnancies.The copper IUD releases free copper and copper salts that have both a biochemical and morphological impact on the endometrium and also produce alterations in cervical mucus and endometrial secretions... An additional spermicidal effect probably takes place in the cervical mucus.</ref> p. 259:Intrauterine devicesMechanisms of actionThe common belief that the usual mechanism of action of IUDs in women is destruction of embryos in the uterus is not supported by empirical evidence... Because concern over mechanism of action represents a barrier to acceptance of this important and highly effective method for some women and some clinicians, it is important to point out that there is no evidence to suggest that the mechanism of action of IUDs is abortifacient... the principal mechanism of action of the copper T 380A IUD is to interfere with sperm action, preventing fertilization of the ovum.</ref> Copper acts as a natural spermicide within the uterus. The presence of copper increases the levels of copper ions, prostaglandins, and white blood cells within the uterine and tubal fluids.
Although not a primary mechanism of action, some experts in human reproduction believe there is sufficient evidence to suggest that IUDs with copper can disrupt implantation,Mechanisms of actionThus, both clinical and experimental evidence suggests that IUDs can prevent and disrupt implantation. It is unlikely, however, that this is the main IUD mode of action, … The best evidence indicates that in IUD users it is unusual for embryos to reach the uterus.In conclusion, IUDs may exert their contraceptive action at different levels. Potentially, they interfere with sperm function and transport within the uterus and tubes. It is difficult to determine whether fertilization of the oocyte is impaired by these compromised sperm. There is sufficient evidence to suggest that IUDs can prevent and disrupt implantation. The extent to which this interference contributes to its contraceptive action is unknown. The data are scanty and the political consequences of resolving this issue interfere with comprehensive research.p. 205:SummaryIUDs that release copper or levonorgestrel are extremely effective contraceptives... Both copper IUDs and levonorgestrel releasing IUSs may interfere with implantation, although this may not be the primary mechanism of action. The devices also create barriers to sperm transport and fertilization, and sensitive assays detect hCG in less than 1% of cycles, indicating that significant prevention must occur before the stage of implantation.</ref> especially when used for emergency contraception.Emergency postcoital contraceptionOther methodsAnother method of emergency contraception is the insertion of a copper IUD, anytime during the preovulatory phase of the menstrual cycle and up to 5 days after ovulation. The failure rate (in a small number of studies) is very low, 0.1%.34,35 This method definitely prevents implantation, but it is not suitable for women who are not candidates for intrauterine contraception, e.g., multiple sexual partners or a rape victim. The use of a copper IUD for emergency contraception is expensive, but not if it is retained as an ongoing method of contraception.</ref>Mechanism of actionCopper-releasing IUCsWhen used as a regular or emergency method of contraception, copper-releasing IUCs act primarily to prevent fertilization. Emergency insertion of a copper IUC is significantly more effective than the use of ECPs, reducing the risk of pregnancy following unprotected intercourse by more than 99%.2,3 This very high level of effectiveness implies that emergency insertion of a copper IUC must prevent some pregnancies after fertilization.Pregnancy begins with implantation according to medical authorities such as the US FDA, the National Institutes of Health79 and the American College of Obstetricians and Gynecologists (ACOG).80</ref> Despite this, there has been no definitive evidence that IUD users have higher rates of embryonic loss than women not using contraception. Therefore, the copper IUD is considered to be a true contraceptive and not an abortifacient.
Usage.
Globally, the IUD is the most widely used method of reversible birth control. The most recent data indicates that there are 169 million IUD users around the world. This includes both the nonhormonal and hormonal IUDs. IUDs are most popular in Asia, where the prevalence is almost 30%. In Africa and Europe the prevalence is around 20%. As of 2009, levels of IUD use in the United States are estimated to be 5.5%. Data in the United States does not distinguish between hormonal and nonhormonal IUDs. In Europe, copper IUD prevalence ranges from under 5% in the United Kingdom to over 10% in Denmark in 2006.
History.
According to popular legend, Arab traders inserted small stones into the uteruses of their camels to prevent pregnancy during long desert treks. The story was originally a tall tale to entertain delegates at a scientific conference on family planning; although it was later repeated as truth, it has no known historical basis.
Precursors to IUDs were first marketed in 1902. Developed from stem pessaries (where the stem held the pessary in place over the cervix), the 'stem' on these devices actually extended into the uterus itself. Because they occupied both the vagina and the uterus, this type of stem pessary was also known as an "interuterine device". Use of "interuterine" devices was associated with high rates of infection; for this reason, they were condemned by the medical community.
The first intrauterine device (contained entirely in the uterus) was described in a German publication in 1909, although the author appears to have never marketed his product.
In 1929, Ernst Gräfenberg of Germany published a report on an IUD made of silk suture. He had found a 3% pregnancy rate among 1,100 women using his ring. In 1930, Gräfenberg reported a lower pregnancy rate of 1.6% among 600 women using an improved ring wrapped in silver wire. Unbeknownst to Gräfenberg, the silver wire was contaminated with 26% copper. Copper's role in increasing IUD efficacy would not be recognized until nearly 40 years later.
In 1934, Japanese physician Tenrei Ota developed a variation of the "Gräfenberg ring" that contained a supportive structure in the center. The addition of this central disc lowered the IUD's expulsion rate. These devices still had high rates of infection, and their use and development was further stifled by World War II politics: contraception was forbidden in both Nazi Germany and Axis-allied Japan. The Allies did not learn of the work by Gräfenberg and Ota until well after the war ended.
The first plastic IUD, the "Margulies Coil" or "Margulies Spiral", was introduced in 1958. This device was somewhat large, causing discomfort to a large proportion of women users, and had a hard plastic tail, causing discomfort to their male partners. The modern colloquialism "coil" is based on the coil-shaped design of early IUDs.
The "Lippes Loop", a slightly smaller device with a monofilament tail, was introduced in 1962 and gained in popularity over the Margulies device.
The stainless steel single-ring IUD was developed in the 1970s and widely used in China because of low manufacturing costs. The Chinese government banned production of steel IUDs in 1993 due to high failure rates (up to 10% per year).
Howard Tatum, in the USA, conceived the plastic T-shaped IUD in 1968. Shortly thereafter Jaime Zipper, in Chile, introduced the idea of adding copper to the devices to improve their contraceptive effectiveness. It was found that copper-containing devices could be made in smaller sizes without compromising effectiveness, resulting in fewer side effects such as pain and bleeding. T-shaped devices had lower rates of expulsion due to their greater similarity to the shape of the uterus.
The poorly designed Dalkon Shield plastic IUD (which had a multifilament tail) was manufactured by the A. H. Robins Company and sold by Robins in the United States for three and a half years from January 1971 through June 1974, before sales were suspended by Robins on June 28, 1974 at the request of the FDA because of safety concerns following reports of 110 septic spontaneous abortions in women with the Dalkon Shield in place, seven of whom had died. 
Robins stopped international sales of the Dalkon Shield in April 1975.
Tatum developed many different models of the copper IUD. He created the TCu220 C, which had copper collars as opposed to copper filament, which prevented metal loss and increased the lifespan of the device. Second-generation copper-T IUDs were also introduced in the 1970s. These devices had higher surface areas of copper, and for the first time consistently achieved effectiveness rates of greater than 99%. The last model Tatum developed was the TCu380A, the model that is most recommended today. In addition to T-shaped IUDs, there are also U-shaped IUDs (such as the Multiload) and 7-shaped Gravigard Copper 7 (with a mini version for nulliparous women introduced in the 1980s). More recently, a frameless IUD called Gynefix was introduced.

</doc>
<doc id="15379" url="https://en.wikipedia.org/wiki?curid=15379" title="Isle Royale National Park">
Isle Royale National Park

Isle Royale National Park is a U.S. National Park on Isle Royale and adjacent islands in Lake Superior, in the state of Michigan. Isle Royale National Park was established on April 3, 1940; designated as a National Wilderness Area in 1976; and made an International Biosphere Reserve in 1980. The park covers , with above water. At the U.S.-Canada border, it meets the borders of the Canadian Lake Superior National Marine Conservation Area.
Geography.
Isle Royale, the largest island in Lake Superior, is over in length and wide at its widest point. The park is made up of Isle Royale itself and approximately 400 smaller islands, along with any submerged lands within of the surrounding islands (16USC408g).
History.
Prehistoric.
In older times, large quantities of copper were mined on Isle Royale and the nearby Keweenaw Peninsula by the indigenous peoples. The region is scarred by ancient mine pits and trenches up to deep. Carbon-14 testing of wood remains found in sockets of copper artifacts indicates that they are at least 5700 years old.
In "Prehistoric Copper Mining in the Lake Superior Region", published in 1961, Drier and Du Temple estimated that over 1.5 billion pounds (630,400 t) of copper had been mined from the region. However, David Johnson and Susan Martin contend that their estimate was based on exaggerated and inaccurate assumptions.
19th & 20th centuries.
In the mid-1840s, a report by Douglass Houghton, Michigan's first state geologist, set off a copper boom in the state, and the first modern copper mines were opened on the island. Evidence of the earlier mining efforts was everywhere, in the form of many stone hammers, some copper artifacts, and places where copper had been partially worked out of the rock but left in place. The ancient pits and trenches led to the discovery of many of the copper deposits that were mined in the 19th century.
The island was once the site of a resort community. The fishing industry has declined considerably, but continues at Edisen Fishery. Because numerous small islands surround Isle Royale, ships were once guided through the area by lighthouses at Passage Island, Rock Harbor, Rock of Ages, and Isle Royale Lighthouse on Menagerie Island.
Within the waters of Isle Royale National Park are several shipwrecks. The area’s notoriously harsh weather, dramatic underwater topography, the island’s central location on historic shipping routes, and the cold, fresh water have resulted in largely intact, well preserved wrecks throughout the park. These were documented in the 1980s, with follow up occurring in 2009, by the National Park Service Submerged Resources Center.
Natural history.
Flora.
The predominant floral habitats of Isle Royal are within the Laurentian Mixed Forest Province. The area is a Temperate broadleaf and mixed forests biome transition zone between the true boreal forest to the north and Big Woods to the south, with characteristics of each. It has areas of both broadleaf and conifer forest cover, and bodies of water ranging from conifer bogs to swamps.
Conifers can include: Jack pines ("Pinus banksiana"); Black and White spruces ("Picea mariana" and "Picea glauca"); Balsam firs ("Abies balsamea"), and EasternRed junipers ("Juniperus virginiana").
Deciduous trees can include: Quaking aspens ("Populus tremuloides"), Bur oaks ("Quercus macrocarpa"), Paper birches ("Betula papyrifera"), American mountain ash ("Sorbus americana"), and Red maples ("Acer rubrum"), Sugar maples ("Acer saccharum"), and Mountain maples ("Acer spicatum").
Fauna.
Isle Royale National Park is known for its wolf and moose populations which are studied by scientists investigating predator-prey relationships in a closed environment. This is made easier because Isle Royale has been colonized by roughly just one third of the mainland mammal species, due to it being so remote. In addition, the environment is unique in that it is the only known place where wolves and moose coexist without the presence of bears.
There are usually around 25 wolves and 1000 moose on the island, but the numbers change greatly year to year. In rare years with very hard winters, animals can travel over the frozen lake from the Canadian mainland. To protect the wolves from canine diseases, dogs are not allowed in any part of the park, including the adjacent waters. In the 2006-2007 winter, 385 moose were counted, as well as 21 wolves, in three packs. In spring 2008, 23 wolves and approximately 650 moose were counted.
Geology.
Bedrock on Isle Royale is basalt or sandstone and conglomerates on the 1.1 billion year old Midcontinent Rift. Most of the island is covered with a thin layer of glacial material. A number of small native copper mines were active in the 1800s but mining was never really prosperous. Recent analyses by the USGS of both unmineralized basalt and copper-mineralized rock show that a small amount of naturally-occurring mercury is associated with mineralization.
Isle Royale greenstone (chlorastrolite, a form of pumpellyite) is found here, as well as on the Upper Peninsula of Michigan. It is the official Michigan state gemstone.
Recreation.
The Greenstone Ridge is a high ridge in the center of the island and carries the longest trail in the park, the Greenstone Ridge Trail, which runs from one end of the island to the other. This is generally done as a 4 or 5 day hike. A boat shuttle can carry hikers back to their starting port. In total there are of hiking trails. There are also canoe/kayak routes, many involving portages, along coastal bays and inland lakes.
Services.
The park has two developed areas:
Windigo, at the southwest end of the island (docking site for the ferries from Minnesota), with a campstore, showers, campsites, rustic camper cabins for those wanting to sleep off of the ground and a boat dock.
Rock Harbor on the south side of the northeast end (docking site for the ferries from Michigan), with a campstore, showers, restaurant, lodge, campsites, and a boat dock. Non-camping sleeping accommodations at the park are limited to the lodge at Rock Harbor and the camper cabins at Windigo.
Camping.
The park has 36 designated wilderness campgrounds. Some campgrounds in the interior are accessible only by trail or by canoe/kayak on the island lakes. Others campgrounds are accessible only by private boat. The campsites vary in capacity but typically include a few three-sided wood shelters (the fourth wall is screened) with floors and roofs, and several individual sites suitable for pitching a small tent. Some tent sites with space for groups of up to 10 are available, and are used for overflow if all the individual sites are filled.
The only amenities at the campgrounds are pit toilets, picnic tables, and fire-rings at specific areas. Campfires are not permitted at most campgrounds; gas or alcohol camp stoves are recommended. Drinking and cooking water must be drawn from local water sources (Lake Superior and inland lakes) and filtered, treated, or boiled to avoid parasites. Hunting is not permitted, but fishing is, and edible berries (blueberries, thimbleberries) may be picked from the trail.
Access.
The park is accessible by ferries, floatplanes, and passenger ships during the summer months — from Houghton and Copper Harbor in Michigan; and Grand Portage in Minnesota. Private boats travel to the island from the coasts of Michigan, Minnesota, and Ontario. Isle Royale is quite popular with day-trippers in private boats, and day trip ferry service is provided from Copper Harbor and Grand Portage to and from the park. 
Because of the difficulty of travel and the hazards of wilderness survival during the winter months, Isle Royale National Park is the only U. S. National Park to entirely close for the full winter season. Due to the distance across Lake Superior to reach the park, and the winter closing, fewer than 20,000 people a year visit Isle Royale — which is fewer than the number of visitors to the most popular National Parks in a single day. Isle Royale had 15,973 visitors in 2007, making it the least-visited national park in the contiguous United States; and the fifth-least visited in the National Park Service system.
Ships.
Scheduled ferry service operates from Grand Portage, Copper Harbor and Houghton. 
The Grand Portage ferries reach the island in 1 1/2 hours, and stay 4 hours at the island, allowing time for hiking, a guided hike or program by the park staff, and picnics. 
The "Isle Royale Queen" serves park visitors out of Copper Harbor, on the northern Upper Peninsula coast of Michigan. It arrives at Rock Harbor in the park in 3 to 3 1/2 hours, spends 3 1/2 hours before returning to Copper Harbor.
The "Sea Hunter" operates round-trips and offers day trips to the Windigo visitor center through much of the season, less frequently in early summer and autumn, it will transport kayaks and canoes for visitors wanting to explore the park from the water. It is the fastest ferry serving the island and arrives in 1 1/2 hours including some sightseeing points along the way out and back. Because of the relatively short boat ride day visitors are able to get 4 hours on the island, more than any others, and get back to the mainland earlier in the afternoon. This gives visitors on a tight schedule time to visit the Grand Portage National Monument or other attractions in the same day.
The "Ranger III" is a ship that serves park visitors from Houghton, Michigan to Rock Harbor. It is operated by the National Park Service, and said to be the largest piece of equipment in the National Park system. It carries 125 passengers, and canoes, kayaks, and even small powerboats. It is a six-hour voyage from Houghton to the park. The ship overnights at Rock Harbor before returning the next day, making two round trips each week, June to mid-September. Briefly in the 2008 season, the Ranger III carried visitors to/from Windigo. This was not continued after 4 trips, due to low interest and long crossing times.
The "Voyageur II", out of Grand Portage, crosses up to three times a week, overnighting at Rock Harbor and providing transportation between popular lakeside campgrounds. In the fall season in addition to carrying campers and hikers it provides day trip service to Windigo on weekends. The Voyageur transports kayaks and canoes for visitors wanting to explore the island from the water. The "Voyageur II" and other boat taxi services ferry hikers to points along the island, allowing a one-way hike back to Rock Harbor or Windigo. Visitors may land at Rock Harbor and depart from Windigo several days later, or vice versa. Hikers will frequently ride it in one direction to do a cross-island hike and be picked up at the other end when they finish.

</doc>
<doc id="15381" url="https://en.wikipedia.org/wiki?curid=15381" title="NATO Integrated Air Defense System">
NATO Integrated Air Defense System

NATO Integrated Air Defense System (short: NATINADS) is according to the "German army regulation" 100/900 defined as – «"A command and control compound (structure or system) of the alliance´s air defence forces, subordinated to the NATO command authorities, already in peace time as well as in crisis and war"».
Development.
Development was approved by the NATO Military Committee in December 1955. The system was to be based on four air defense regions (ADRs) coordinated by SACEUR (Supreme Allied Commander Europe). Starting from 1956 early warning coverage was extended across Western Europe using 18 radar stations. This part of the system was completed by 1962. Linked to existing national radar sites the coordinated system was called the NATO Air Defence Ground Environment (NADGE).
From 1960 NATO countries agreed to place all their air defence forces under the command of SACEUR in the event of war. These forces included command & control (C2) systems, radar installations, and Surface-to-Air (SAM) missile units as well as interceptor aircraft.
By 1972 NADGE was converted into NATINADS consisting of 84 radar sites and associated Control Reporting Centers (CRC) and in the 1980s the Airborne Early Warning / Ground Environment Integration Segment (AEGIS) upgraded the NATINADS with the possibility to integrate the AWACS radarpicture and all of its information into its visual displays. (NOTE: This AEGIS is not to be confused with the U.S.Navy AEGIS, a shipboard fire control radar and weapons system.) AEGIS processed the information through Hughes H5118ME computers, which replaced the H3118M computers installed at NADGE sites in the late 1960s and early 1970s.
NATINADS ability to handle data increased with faster clock rates. The H5118M computer had a staggering 1 megabyte of memory and could handle 1.2 million instructions per second while the former model had a memory of only 256 kilobytes and a clock speed of 150000 instructions per seconds.
NATINADS/AEGIS were complemented, in W-Germany by the German Air Defence Ground Environment (GEADGE), an updated radar network adding the southern part of W-Germany to the European system and Coastal Radar Integration System (CRIS), adding data links from Danish coastal radars.
In order to counter the hardware obsolescence, during the mid-90's NATO started the AEGIS Site Emulator (ASE) program allowing the NATINADS/AEGIS sites to replace the proprietary hardware (the 5118ME computer and the various operator consoles IDM-2, HMD-22, IDM-80) with Commercial-Off-the-Shelf servers and workstations.
In the first years 2000, the initial ASE capability was expanded with the possibility to run, thanks to the new hardware power, multiple site emulators on the same hardware, so the system was renamed into Multi-AEGIS Site Emulator (MASE). The NATO system designed to replace MASE in the near future is the Air Command and Control System (ACCS).
Because of changing politics, NATO expanding and financial crises most European (NATO) countries are trying to cut defence budgets; as a direct result lots of obsolete and outdated NATINADS facilities are phased out earlier. Currently (2013) still operational NATO radar sites in Europe are these:
Radar sensor location.
Italy.
Combined Air Operations Centre (CAOC 5) Poggio Renatico (44°47'32"N 11°29'41"E); also Aeronautica Militare Italia Gruppo Riporto e Controllo Difesa Aerea (GRCDA) - airdefense control & reporting center was a former NADGE and later NATINADS radarsite. 
Other (non-NATO) operational radar sites in Europe.
Austrian radar (GOLDHAUBE)
Swiss Air Force radar (FLORAKO)

</doc>
<doc id="15382" url="https://en.wikipedia.org/wiki?curid=15382" title="Invisible balance">
Invisible balance

The invisible balance or balance of trade on services is that part of the balance of trade that refers to services and other products that do not result in the transfer of physical objects. Examples include consulting services, shipping services, tourism, and patent license revenues. This figure is usually generated by tertiary industry. The term 'invisible balance' is especially common in the United Kingdom.
For countries that rely on service exports or on tourism, the invisible balance is particularly important. For instance the United Kingdom and Saudi Arabia receive significant international income from financial services, while Japan and Germany rely more on exports of manufactured goods.
Types of invisibles.
Invisibles are both international payments for services (as opposed to goods), as well as movements of money without exchange for goods or services. These invisibles are called 'transfer payments' or 'remittances' and may include money sent from one country to another by an individual, business, government or non-governmental organisations (NGO) – often charities.
An individual remittance may include money sent to a relative overseas. Business transfers may include profits sent by a foreign subsidiary to a parent company or money invested by a business in a foreign country. Bank loans to foreign countries are also included in this category, as are license fees paid for the use of patents and trademarks. Government transfers may involve loans made, or official aid given to, foreign countries while transfers made by NGO's include money used for charitable work in foreign country.
Balance of payments and invisibles.
In many countries a useful distinction is drawn between the balance of trade and the balance of payments. 'Balance of trade' refers to the trade of both tangible (physical) objects as well as the trade in services – collectively known as exports and imports (in other words, 'visibles plus services') – while the 'balance of payments' also includes transfers of Capital in the form of loans, investments in shares or direct investment in projects.
A nation may have a visibles balance surplus but this can be offset by a larger deficit in the invisibles balance (creating a Balance of Trade deficit overall) – if, for example, there are large payments made to foreign businesses for invisibles such as shipping or tourism. On the other hand, a Visibles Balance deficit can be offset by a strong surplus on the invisibles balance if, for example, foreign aid is being provided.
In a similar way, a nation may also have a surplus 'balance of trade' because it exports more than it imports but a negative (or deficit) 'balance of payments' because, it has a much greater shortfall in transfers of capital. And, just as easily, a deficit in the 'balance of trade' may be offset by a larger surplus in capital transfers from overseas to produce a balance of payments surplus overall.
Balance of payments problems and the invisible balance.
Problems with a country's balance of trade (or balance of payments) are often associated with an inappropriate valuation of its currency, its country's foreign exchange rate.
If a country's exchange rate is too high, its exports will become uncompetitive as buyers in foreign countries require more of their own currency to pay for them. In the meantime, it also becomes cheaper for the citizens of the country to buy goods from overseas,as opposed to buying locally produced goods), because an overvalued currency makes foreign products less expensive.
The simultaneous decline in currency inflows from decreased exports and the rise in outflows, due to increased imports, sends the balance of trade into deficit, which then needs to be paid for by a transfer of funds in some form, either invisible transfers (aid, etc.) or capital flows (loans, etc.). However, relying on funds like that to support a trade deficit, is unsustainable, and the country may eventually require its currency to be devalued.
If, on the other hand, a currency is undervalued, its exports will become cheaper and therefore more competitive internationally. At the same time, imports will also become more costly, stimulating the production of domestic substitutes to replace them. That will result in a growth of currency flowing into the country and a decline in currency flowing out of it, resulting in an improvement in the country's balance of trade.
Because a nation's exchange rate has a big impact on its 'balance of trade' and its 'balance of payments', many economists favour freely floating exchange rates over the older, fixed (or pegged) rates of foreign currency exchange. Floating exchange rates allow more regular adjustments in exchange rates to occur, allowing the greater opportunity for international payments to maintain equilibrium.

</doc>
<doc id="15387" url="https://en.wikipedia.org/wiki?curid=15387" title="Irreducible complexity">
Irreducible complexity

Irreducible complexity (IC) is a pseudoscientific argument that certain biological systems cannot evolve by successive small modifications to pre-existing functional systems through natural selection. Central to the creationist concept of intelligent design, IC is rejected by the scientific community, which regards intelligent design as pseudoscience.Mark D. Decker. College of Biological Sciences, General Biology Program, University of Minnesota Frequently Asked Questions About the Texas Science Textbook Adoption Controversy "The Discovery Institute and ID proponents have a number of goals that they hope to achieve using disingenuous and mendacious methods of marketing, publicity, and political persuasion. They do not practice real science because that takes too long, but mainly because this method requires that one have actual evidence and logical reasons for one's conclusions, and the ID proponents just don't have those. If they had such resources, they would use them, and not the disreputable methods they actually use."See also list of scientific societies explicitly rejecting intelligent design</ref> Irreducible complexity is one of two main arguments used by intelligent design proponents, the other being specified complexity.
Michael Behe, a professor of biochemistry at Lehigh University, first argued that irreducible complexity made evolution purely through natural selection of random mutations impossible. However, evolutionary biologists have demonstrated how such systems could have evolved. There are many examples documented through comparative genomics showing that complex molecular systems are formed by the addition of components as revealed by different temporal origins of their proteins.
In the 2005 "Kitzmiller v. Dover Area School District" trial, Behe gave testimony on the subject of irreducible complexity. The court found that "Professor Behe's claim for irreducible complexity has been refuted in peer-reviewed research papers and has been rejected by the scientific community at large."
Definitions.
Michael Behe defined irreducible complexity in natural selection his book "Darwin's Black Box":
A single system which is composed of several interacting parts that contribute to the basic function, and where the removal of any one of the parts causes the system to effectively cease functioning.
A second definition given by Behe (his "evolutionary definition") is as follows:
An irreducibly complex evolutionary pathway is one that contains one or more unselected steps (that is, one or more necessary-but-unselected mutations). The degree of irreducible complexity is the number of unselected steps in the pathway.
Intelligent design advocate William A. Dembski gives this definition:
A system performing a given basic function is irreducibly complex if it includes a set of well-matched, mutually interacting, nonarbitrarily individuated parts such that each part in the set is indispensable to maintaining the system's basic, and therefore original, function. The set of these indispensable parts is known as the irreducible core of the system.
History.
Forerunners.
The argument from irreducible complexity is a descendant of the teleological argument for God (the argument from design or from complexity). This states that because certain things in nature are very complicated, they must have been designed. William Paley famously argued, in his 1802 watchmaker analogy, that complexity in nature implies a God for the same reason that the existence of a watch implies the existence of a watchmaker. This argument has a long history, and can be traced back at least as far as Cicero's "De Natura Deorum" ii.34.
Up to the 18th century.
Galen (1st and 2nd centuries AD) wrote about the large number of parts of the body and their relationships, which observation was cited as evidence for creation. The idea that the interdependence between parts would have implications for the origins of living things was raised by writers starting with Pierre Gassendi in the mid-17th century and John Wilkins, who wrote (citing Galen), "Now to imagine, that all these things, according to their several kinds, could be brought into this regular frame and order, to which such an infinite number of Intentions are required, without the contrivance of some wise Agent, must needs be irrational in the highest degree." In the late 17th century, Thomas Burnet referred to "a multitude of pieces aptly joyn'd" to argue against the eternity of life. In the early 18th century, Nicolas Malebranche wrote "An organized body contains an infinity of parts that mutually depend upon one another in relation to particular ends, all of which must be actually formed in order to work as a whole," arguing in favor of preformation, rather than epigenesis, of the individual; and a similar argument about the origins of the individual was made by other 18th-century students of natural history. In his 1790 book, "The Critique of Judgment", Kant is said to argue that "we cannot conceive how a whole that comes into being only gradually from its parts can nevertheless be the cause of the properties of those parts".
19th century.
Chapter XV of Paley's "Natural Theology" discusses at length what he called "relations" of parts of living things as an indication of their design.
Georges Cuvier applied his principle of the "correlation of parts" to describe an animal from fragmentary remains. For Cuvier, this was related to another principle of his, the "conditions of existence", which excluded the possibility of transmutation of species.
While he did not originate the term, Charles Darwin identified the argument as a possible way to falsify a prediction of the theory of evolution at the outset. In "The Origin of Species", he wrote, "If it could be demonstrated that any complex organ existed, which could not possibly have been formed by numerous, successive, slight modifications, my theory would absolutely break down. But I can find out no such case." Darwin's theory of evolution challenges the teleological argument by postulating an alternative explanation to that of an intelligent designer—namely, evolution by natural selection. By showing how simple unintelligent forces can ratchet up designs of extraordinary complexity without invoking outside design, Darwin showed that an intelligent designer was not the necessary conclusion to draw from complexity in nature. The argument from irreducible complexity attempts to demonstrate that certain biological features cannot be purely the product of Darwinian evolution.
In the late 19th century, in a dispute between supporters of the adequacy of natural selection and those who held for inheritance of acquired characteristics, one of the arguments made repeatedly by Herbert Spencer, and followed by others, depended on what Spencer referred to as "co-adaptation" of "co-operative" parts, as in: "We come now to Professor Weismann's endeavour to disprove my second thesis — that it is impossible to explain by natural selection alone the co-adaptation of co-operative parts. It is thirty years since this was set forth in "The Principles of Biology." In §166, I instanced the enormous horns of the extinct Irish elk, and contended that in this and in kindred cases, where for the efficient use of some one enlarged part many other parts have to be simultaneously enlarged, it is out of the question to suppose that they can have all spontaneously varied in the required proportions." Darwin responded to Spencer's objections in chapter XXV of "The Variation of Animals and Plants under Domestication". The history of this concept in the dispute has been characterized: "An older and more religious tradition of idealist thinkers were committed to the explanation of complex adaptive contrivances by intelligent design. ... Another line of thinkers, unified by the recurrent publications of Herbert Spencer, also saw co-adaptation as a composed, irreducible whole, but sought to explain it by the inheritance of acquired characteristics."
St. George Jackson Mivart raised the objection to natural selection that "Complex and simultaneous co-ordinations … until so far developed as to effect the requisite junctions, are useless" which "amounts to the concept of "irreducible complexity" as defined by … Michael Behe".
20th century.
Hermann Muller, in the early 20th century, discussed a concept similar to irreducible complexity. However, far from seeing this as a problem for evolution, he described the "interlocking" of biological features as a consequence to be expected of evolution, which would lead to irreversibility of some evolutionary changes. He wrote, "Being thus finally woven, as it were, into the most intimate fabric of the organism, the once novel character can no longer be withdrawn with impunity, and may have become vitally necessary."
In 1974, young Earth creationist Henry M. Morris introduced a similar concept in his book "Scientific Creationism" in which he wrote; "This issue can actually be attacked quantitatively, using simple principles of mathematical probability. The problem is simply whether a complex system, in which many components function unitedly together, and in which each component is uniquely necessary to the efficient functioning of the whole, could ever arise by random processes."
A book-length study of a concept similar to irreducible complexity, explained by gradual, step-wise, non-teleological evolution, was published in 1975 by Thomas H. Frazzetta. "A complex adaptation is one constructed of "several" components that must blend together operationally to make the adaptation "work". It is analogous to a machine whose performance depends upon careful cooperation among its parts. In the case of the machine, no single part can greatly be altered without changing the performance of the entire machine." The machine that he chose as an analog is the Peaucellier–Lipkin linkage, and one biological system given extended description was the jaw apparatus of a python. The conclusion of this investigation, rather than that evolution of a complex adaptation was impossible, "awed by the adaptations of living things, to be stunned by their complexity and suitability", was "to accept the inescapable but not humiliating fact that much of mankind can be seen in a tree or a lizard."
In 1981, Ariel Roth, in defense of the creation science position in the trial "McLean v. Arkansas", said of "complex integrated structures" that "This system would not be functional until all the parts were there ... How did these parts survive during evolution ...?"
In 1985 Cairns-Smith wrote of "interlocking": "How can a complex collaboration between components evolve in small steps?" and used the analogy of the scaffolding called centering used to build an arch then removed afterwards: "Surely there was 'scaffolding'. Before the multitudinous components of present biochemistry could come to lean together "they had to lean on something else."" However, neither Muller or Cairns-Smith claimed that their ideas were evidence of something supernatural.
An essay in support of creationism published in 1994 referred to bacterial flagella as showing "multiple, integrated components", where "nothing about them works unless every one of their complexly fashioned and integrated components are in place" and asked the reader to "imagine the effects of natural selection on those organisms that fortuitously evolved the flagella ... without the concommitant control mechanisms".
An early concept of irreducibly complex systems comes from Ludwig von Bertalanffy, a 20th-century Austrian biologist. He believed that complex systems must be examined as complete, irreducible systems in order to fully understand how they work. He extended his work on biological complexity into a general theory of systems in a book titled "General Systems Theory".
After James Watson and Francis Crick published the structure of DNA in the early 1950s, General Systems Theory lost many of its adherents in the physical and biological sciences. However, Systems theory remained popular in the social sciences long after its demise in the physical and biological sciences.
Origins.
Michael Behe developed his ideas on the concept around 1992, in the early days of the 'wedge movement', and first presented his ideas about "irreducible complexity" in June 1993 when the "Johnson-Behe cadre of scholars" met at Pajaro Dunes in California.</ref> He set out his ideas in the second edition of "Of Pandas and People" published in 1993, extensively revising Chapter 6 "Biochemical Similarities" with new sections on the complex mechanism of blood clotting and on the origin of proteins.Introduction: Of Pandas and People, the foundational work of the 'Intelligent Design' movement by Nick Matzke 2004,Design on Trial in Dover, Pennsylvania by Nicholas J Matzke, NCSE Public Information Project Specialist</ref>
He first used the term "irreducible complexity" in his 1996 book "Darwin's Black Box", to refer to certain complex biochemical cellular systems. He posits that evolutionary mechanisms cannot explain the development of such "irreducibly complex" systems. Notably, Behe credits philosopher William Paley for the original concept (alone among the predecessors) and suggests that his application of the concept to biological systems is entirely original.
Intelligent design advocates argue that irreducibly complex systems must have been deliberately engineered by some form of intelligence.
In 2001, Michael Behe wrote: "is an asymmetry between my current definition of irreducible complexity and the task facing natural selection. I hope to repair this defect in future work." Behe specifically explained that the "current definition puts the focus on removing a part from an already functioning system", but the "difficult task facing Darwinian evolution, however, would not be to remove parts from sophisticated pre-existing systems; it would be to bring together components to make a new system in the first place". In the 2005 "Kitzmiller v. Dover Area School District" trial, Behe testified under oath that he "did not judge [the asymmetry serious enough to revised the book yet."
Behe additionally testified that the presence of irreducible complexity in organisms would not rule out the involvement of evolutionary mechanisms in the development of organic life. He further testified that he knew of no earlier "peer reviewed articles in scientific journals discussing the intelligent design of the blood clotting cascade," but that there were "probably a large number of peer reviewed articles in science journals that demonstrate that the blood clotting system is indeed a purposeful arrangement of parts of great complexity and sophistication." (The judge ruled that "intelligent design is not science and is essentially religious in nature".)
According to the theory of evolution, genetic variations occur without specific design or intent. The environment "selects" the variants that have the highest fitness, which are then passed on to the next generation of organisms. Change occurs by the gradual operation of natural forces over time, perhaps slowly, perhaps more quickly (see punctuated equilibrium). This process is able to adapt complex structures from simpler beginnings, or convert complex structures from one function to another (see spandrel). Most intelligent design advocates accept that evolution occurs through mutation and natural selection at the "micro level", such as changing the relative frequency of various beak lengths in finches, but assert that it cannot account for irreducible complexity, because none of the parts of an irreducible system would be functional or advantageous until the entire system is in place.
The mousetrap example.
Behe uses the mousetrap as an illustrative example of this concept. A mousetrap consists of five interacting pieces: the base, the catch, the spring, the hammer, and the hold-down bar. All of these must be in place for the mousetrap to work, as the removal of any one piece destroys the function of the mousetrap. Likewise, he asserts that biological systems require multiple parts working together in order to function. Intelligent design advocates claim that natural selection could not create from scratch those systems for which science is currently unable to find a viable evolutionary pathway of successive, slight modifications, because the selectable function is only present when all parts are assembled.
In his 2008 book "Only A Theory", biologist Kenneth R. Miller challenges Behe's claim that the mousetrap is irreducibly complex. Miller observes that various subsets of the five components can be devised to form cooperative units, ones that have different functions from the mousetrap and so, in biological terms, could form functional spandrels before being adapted to the new function of catching mice. In an example taken from his high school experience, Miller recalls that one of his classmates...struck upon the brilliant idea of using an old, broken mousetrap as a spitball catapult, and it worked brilliantly... It had worked perfectly as something other than a mousetrap... my rowdy friend had pulled a couple of parts --probably the hold-down bar and catch-- off the trap to make it easier to conceal and more effective as a catapult... the base, the spring, and the hammer. Not much of a mousetrap, but a helluva spitball launcher... I realized why [Behe's mousetrap analogy had bothered me. It was wrong. The mousetrap is not irreducibly complex after all.
Other systems identified by Miller that include mousetrap components include the following:
The point of the reduction is that - in biology - most or all of the components were already at hand, by the time it became necessary to build a mousetrap. As such, it required far fewer steps to develop a mousetrap than to design all the components from scratch.
Thus the development of the mousetrap, said to consist of five different parts which had no function on their own, has been reduced to one step: the assembly from parts that are already present, performing other functions.
The Intelligent Design argument focuses on the functionality to catch mice. It skips over the case that many, if not all, parts are already available in their own right, at the time that the need for a mousetrap arises.
Consequences of irreducible complexity.
Supporters of intelligent design argue that anything less than the complete form of such a system or organ would not work at all, or would in fact be a "detriment" to the organism, and would therefore never survive the process of natural selection. Although they accept that some complex systems and organs "can" be explained by evolution, they claim that organs and biological features which are "irreducibly complex" cannot be explained by current models, and that an intelligent designer must have created life or guided its evolution. Accordingly, the debate on irreducible complexity concerns two questions: whether irreducible complexity can be found in nature, and what significance it would have if it did exist in nature.
Behe's original examples of irreducibly complex mechanisms included the bacterial flagellum of "E. coli", the blood clotting cascade, cilia, and the adaptive immune system.
Behe argues that organs and biological features which are irreducibly complex cannot be wholly explained by current models of evolution. In explicating his definition of "irreducible complexity" he notes that:
An irreducibly complex system cannot be produced directly (that is, by continuously improving the initial function, which continues to work by the same mechanism) by slight, successive modifications of a precursor system, because any precursor to an irreducibly complex system that is missing a part is by definition nonfunctional.
Irreducible complexity is not an argument that evolution does not occur, but rather an argument that it is "incomplete". In the last chapter of "Darwin's Black Box", Behe goes on to explain his view that irreducible complexity is evidence for intelligent design. Mainstream critics, however, argue that irreducible complexity, as defined by Behe, can be generated by known evolutionary mechanisms. Behe's claim that no scientific literature adequately modeled the origins of biochemical systems through evolutionary mechanisms has been challenged by TalkOrigins. The judge in the "Dover" trial wrote "By defining irreducible complexity in the way that he has, Professor Behe attempts to exclude the phenomenon of exaptation by definitional fiat, ignoring as he does so abundant evidence which refutes his argument. Notably, the NAS has rejected Professor Behe's claim for irreducible complexity..."
Stated examples.
Behe and others have suggested a number of biological features that they believe may be irreducibly complex.
Blood clotting cascade.
The process of blood clotting or coagulation cascade in vertebrates is a complex biological pathway which is given as an example of apparent irreducible complexity.
The irreducible complexity argument assumes that the necessary parts of a system have always been necessary, and therefore could not have been added sequentially. However, in evolution, something which is at first merely advantageous can later become necessary. Natural selection can lead to complex biochemical systems being built up from simpler systems, or to existing functional systems being recombined as a new system with a different function. For example, one of the clotting factors that Behe listed as a part of the clotting cascade (Factor XII, also called Hageman factor) was later found to be absent in whales, demonstrating that it is not essential for a clotting system. Many purportedly irreducible structures can be found in other organisms as much simpler systems that utilize fewer parts. These systems, in turn, may have had even simpler precursors that are now extinct. Behe has responded to critics of his clotting cascade arguments by suggesting that homology is evidence for evolution, but not for natural selection.
The "improbability argument" also misrepresents natural selection. It is correct to say that a set of simultaneous mutations that form a complex protein structure is so unlikely as to be unfeasible, but that is not what Darwin advocated. His explanation is based on small accumulated changes that take place without a final goal. Each step must be advantageous in its own right, although biologists may not yet understand the reason behind all of them—for example, jawless fish accomplish blood clotting with just six proteins instead of the full ten.
Eye.
The eye is an example of a supposedly irreducibly complex structure, due to its many elaborate and interlocking parts, seemingly all dependent upon one another. It is frequently cited by intelligent design and creationism advocates as an example of irreducible complexity. Behe used the "development of the eye problem" as evidence for intelligent design in "Darwin's Black Box". Although Behe acknowledged that the evolution of the larger anatomical features of the eye have been well-explained, he claimed that the complexity of the minute biochemical reactions required at a molecular level for light sensitivity still defies explanation. Creationist Jonathan Sarfati has described the eye as evolutionary biologists' "greatest challenge as an example of superb 'irreducible complexity' in God's creation", specifically pointing to the supposed "vast complexity" required for transparency.
In an often misquoted passage from "On the Origin of Species", Charles Darwin appears to acknowledge the eye's development as a difficulty for his theory. However, the quote in context shows that Darwin actually had a very good understanding of the evolution of the eye (see fallacy of quoting out of context). He notes that "to suppose that the eye ... could have been formed by natural selection, seems, I freely confess, absurd in the highest possible degree". Yet this observation was merely a rhetorical device for Darwin. He goes on to explain that if gradual evolution of the eye could be shown to be possible, "the difficulty of believing that a perfect and complex eye could be formed by natural selection ... can hardly be considered real". He then proceeded to roughly map out a likely course for evolution using examples of gradually more complex eyes of various species.
Since Darwin's day, the eye's ancestry has become much better understood. Although learning about the construction of ancient eyes through fossil evidence is problematic due to the soft tissues leaving no imprint or remains, genetic and comparative anatomical evidence has increasingly supported the idea of a common ancestry for all eyes.
Current evidence does suggest possible evolutionary lineages for the origins of the anatomical features of the eye. One likely chain of development is that the eyes originated as simple patches of photoreceptor cells that could detect the presence or absence of light, but not its direction. When, via random mutation across the population, the photosensitive cells happened to have developed on a small depression, it endowed the organism with a better sense of the light's source. This small change gave the organism an advantage over those without the mutation. This genetic trait would then be "selected for" as those with the trait would have an increased chance of survival, and therefore progeny, over those without the trait. Individuals with deeper depressions would be able to discern changes in light over a wider field than those individuals with shallower depressions. As ever deeper depressions were advantageous to the organism, gradually, this depression would become a pit into which light would strike certain cells depending on its angle. The organism slowly gained increasingly precise visual information. And again, this gradual process continued as individuals having a slightly shrunken aperture of the eye had an advantage over those without the mutation as an aperture increases how collimated the light is at any one specific group of photoreceptors. As this trait developed, the eye became effectively a pinhole camera which allowed the organism to dimly make out shapes—the nautilus is a modern example of an animal with such an eye. Finally, via this same selection process, a protective layer of transparent cells over the aperture was differentiated into a crude lens, and the interior of the eye was filled with humours to assist in focusing images. In this way, eyes are recognized by modern biologists as actually a relatively unambiguous and simple structure to evolve, and many of the major developments of the eye's evolution are believed to have taken place over only a few million years, during the Cambrian explosion.
Behe maintains that the complexity of light sensitivity at the molecular level and the minute biochemical reactions required for those first "simple patches of photoreceptor" still defies explanation. Other intelligent design proponents claim that the evolution of the entire visual system would be difficult rather than the eye alone.
Flagella.
The flagella of certain bacteria constitute a molecular motor requiring the interaction of about 40 different protein parts. Behe presents this as a prime example of an irreducibly complex structure defined as "a single system composed of several well-matched, interacting parts that contribute to the basic function, wherein the removal of any one of the parts causes the system to effectively cease functioning", and argues that since "an irreducibly complex system that is missing a part is by definition nonfunctional", it could not have evolved gradually through natural selection.
Reducible complexity. In contrast to Behe's claims, many proteins can be deleted or mutated and the flagellum still works, even though sometimes at reduced efficiency. In fact, the composition of flagella is surprisingly diverse across bacteria with many proteins only found in some species but not others. Hence the flagellar apparatus is clearly very flexible in evolutionary terms and perfectly able to lose or gain protein components. Further studies have shown that, contrary to claims of "irreducible complexity", flagella and related protein transport mechanisms show evidence of evolution through Darwinian processes, providing case studies in how complex systems can evolve from simpler components.
Evolution from Type Three Secretion Systems. Scientists regard this argument as having been disproved in the light of research dating back to 1996 as well as more recent findings. The TTSS system negates Behe's claim that taking away any one of the flagellum's parts would prevent the system from functioning. On this basis, Kenneth Miller notes that, "The parts of this supposedly irreducibly complex system actually have functions of their own."
Dembski has argued that phylogenetically, the TTSS is found in a narrow range of bacteria which makes it seem to him to be a late innovation, whereas flagella are widespread throughout many bacterial groups, and he argues that it was an early innovation. Against Dembski's argument, different flagella use completely different mechanisms, and publications show a plausible path in which bacterial flagella could have evolved from a secretion system.
Response of the scientific community.
Like intelligent design, the concept it seeks to support, irreducible complexity has failed to gain any notable acceptance within the scientific community. One science writer called it a "full-blown intellectual surrender strategy".
Reducibility of "irreducible" systems.
Researchers have proposed potentially viable evolutionary pathways for allegedly irreducibly complex systems such as blood clotting, the immune system and the flagellum - the three examples Behe proposed. John H. McDonald even showed his example of a mousetrap to be reducible. If irreducible complexity is an insurmountable obstacle to evolution, it should not be possible to conceive of such pathways.
Niall Shanks and Karl H. Joplin, both of East Tennessee State University, have shown that systems satisfying Behe's characterization of irreducible biochemical complexity can arise naturally and spontaneously as the result of self-organizing chemical processes. They also assert that what evolved biochemical and molecular systems actually exhibit is "redundant complexity"—a kind of complexity that is the product of an evolved biochemical process. They claim that Behe overestimated the significance of irreducible complexity because of his simple, linear view of biochemical reactions, resulting in his taking snapshots of selective features of biological systems, structures, and processes, while ignoring the redundant complexity of the context in which those features are naturally embedded. They also criticized his over-reliance of overly simplistic metaphors, such as his mousetrap.
A computer model of the co-evolution of proteins binding to DNA in the peer-reviewed journal "Nucleic Acids Research" consisted of several parts (DNA binders and DNA binding sites) which contribute to the basic function; removal of either one leads immediately to the death of the organism. This model fits the definition of irreducible complexity exactly, yet it evolves. (The program can be run from Ev program.)
In addition, research published in the peer-reviewed journal "Nature" has shown that computer simulations of evolution demonstrate that it is possible for complex features to evolve naturally.
One can compare a mousetrap with a cat in this context. Both normally function so as to control the mouse population. The cat has many parts that can be removed leaving it still functional; for example, its tail can be bobbed, or it can lose an ear in a fight. Comparing the cat and the mousetrap, then, one sees that the mousetrap (which is not alive) offers better evidence, in terms of irreducible complexity, for intelligent design than the cat. Even looking at the mousetrap analogy, several critics have described ways in which the parts of the mousetrap could have independent uses or could develop in stages, demonstrating that it is not irreducibly complex.
Moreover, even cases where removing a certain component in an organic system will cause the system to fail do not demonstrate that the system could not have been formed in a step-by-step, evolutionary process. By analogy, stone arches are irreducibly complex—if you remove any stone the arch will collapse—yet humans build them easily enough, one stone at a time, by building over centering that is removed afterward. Similarly, naturally occurring arches of stone form by the weathering away of bits of stone from a large concretion that has formed previously.
Evolution can act to simplify as well as to complicate. This raises the possibility that seemingly irreducibly complex biological features may have been achieved with a period of increasing complexity, followed by a period of simplification.
A team led by Joseph Thornton, assistant professor of biology at the University of Oregon's Center for Ecology and Evolutionary Biology, using techniques for resurrecting ancient genes, reconstructed the evolution of an apparently irreducibly complex molecular system. The April 7, 2006 issue of "Science" published this research.
Irreducible complexity may not actually exist in nature, and the examples given by Behe and others may not in fact represent irreducible complexity, but can be explained in terms of simpler precursors. The theory of facilitated variation challenges irreducible complexity. Marc W. Kirschner, a professor and chair of Department of Systems Biology at Harvard Medical School, and John C. Gerhart, a professor in Molecular and Cell Biology, University of California, Berkeley, presented this theory in 2005. They describe how certain mutation and changes can cause apparent irreducible complexity. Thus, seemingly irreducibly complex structures are merely "very complex", or they are simply misunderstood or misrepresented.
Gradual adaptation to new functions.
The precursors of complex systems, when they are not useful in themselves, may be useful to perform other, unrelated functions. Evolutionary biologists argue that evolution often works in this kind of blind, haphazard manner in which the function of an early form is not necessarily the same as the function of the later form. The term used for this process is exaptation. The mammalian middle ear (derived from a jawbone) and the panda's thumb (derived from a wrist bone spur) provide classic examples. A 2006 article in "Nature" demonstrates intermediate states leading toward the development of the ear in a Devonian fish (about 360 million years ago). Furthermore, recent research shows that viruses play a heretofore unexpected role in evolution by mixing and matching genes from various hosts.
Arguments for irreducibility often assume that things started out the same way they ended up—as we see them now. However, that may not necessarily be the case. In the "Dover" trial an expert witness for the plaintiffs, Ken Miller, demonstrated this possibility using Behe's mousetrap analogy. By removing several parts, Miller made the object unusable as a mousetrap, but he pointed out that it was now a perfectly functional, if unstylish, tie clip.
Methods by which irreducible complexity may evolve.
Irreducible complexity can be seen as equivalent to crossing a "valley" in a fitness landscape. A number of mathematical models of evolution have explored the circumstances under which this can happen.
Falsifiability and experimental evidence.
Some critics, such as Jerry Coyne (professor of evolutionary biology at the University of Chicago) and Eugenie Scott (a physical anthropologist and former executive director of the National Center for Science Education) have argued that the concept of irreducible complexity and, more generally, intelligent design is not falsifiable and, therefore, not scientific.
Behe argues that the theory that irreducibly complex systems could not have evolved can be falsified by an experiment where such systems are evolved. For example, he posits taking bacteria with no flagellum and imposing a selective pressure for mobility. If, after a few thousand generations, the bacteria evolved the bacterial flagellum, then Behe believes that this would refute his theory.
Other critics take a different approach, pointing to experimental evidence that they believe falsifies the argument for Intelligent Design from irreducible complexity. For example, Kenneth Miller cites the lab work of Barry G. Hall on E. coli, which he asserts is evidence that "Behe is wrong".
Other evidence that irreducible complexity is not a problem for evolution comes from the field of computer science, which routinely uses computer analogues of the processes of evolution in order to automatically design complex solutions to problems. The results of such genetic algorithms are frequently irreducibly complex since the process, like evolution, both removes non-essential components over time as well as adding new components. The removal of unused components with no essential function, like the natural process where rock underneath a natural arch is removed, can produce irreducibly complex structures without requiring the intervention of a designer. Researchers applying these algorithms automatically produce human-competitive designs—but no human designer is required.
Argument from ignorance.
Intelligent design proponents attribute to an intelligent designer those biological structures they believe are irreducibly complex and therefore they say a natural explanation is insufficient to account for them. However, critics view irreducible complexity as a special case of the "complexity indicates design" claim, and thus see it as an argument from ignorance and as a God-of-the-gaps argument.
Eugenie Scott, along with Glenn Branch and other critics, has argued that many points raised by intelligent-design proponents are arguments from ignorance. Behe has been accused by critics of using an "argument by lack of imagination".
False dilemma.
Irreducible complexity is at its core an argument against evolution. If truly irreducible systems are found, the argument goes, then intelligent design must be the correct explanation for their existence. However, this conclusion is based on the assumption that current evolutionary theory and intelligent design are the only two valid models to explain life, a false dilemma.
Irreducible complexity in the Dover trial.
While testifying during the 2005 "Kitzmiller v. Dover Area School District" trial, Behe conceded that there are no peer-reviewed papers supporting his claims that complex molecular systems, like the bacterial flagellum, the blood-clotting cascade, and the immune system, were intelligently designed nor are there any peer-reviewed articles supporting his argument that certain complex molecular structures are "irreducibly complex."
In the final ruling of "Kitzmiller v. Dover Area School District", Judge Jones specifically singled out Behe and irreducible complexity:

</doc>
<doc id="15388" url="https://en.wikipedia.org/wiki?curid=15388" title="Religion in pre-Islamic Arabia">
Religion in pre-Islamic Arabia

Religion in pre-Islamic Arabia was a mix of polytheism, Christianity, Judaism, and Iranian religions. Arab polytheism, the dominant form of religion in pre-Islamic Arabia, was based on veneration of deities and other rituals. Gods and goddesses, including Hubal and the goddesses Al-lāt, Al-‘Uzzá and Manāt, were worshipped at local shrines, such as the Kaaba in Mecca. Different theories have been proposed regarding the role of Allah in Meccan religion. Many of the physical descriptions of the pre-Islamic gods are traced to idols, especially near the Kaaba, which is said to have contained up to 360 of them.
Other religions were represented to varying, lesser degrees. The influence of the adjacent Roman, Axumite and Sassanian empires resulted in Christian communities in the northwest, northeast and south of Arabia. Christianity made a lesser impact, but secured some conversions, in the remainder of the peninsula. With the exception of Nestorianism in the northeast and the Persian Gulf, the dominant form of Christianity was Monophysitism. The peninsula had been subject to Jewish migration since Roman times, which had resulted in a diaspora community supplemented by local converts. Additionally, the influence of the Sasanian Empire resulted in Iranian religions being present in the peninsula. Zoroastrianism existed in the east and south whilst there is evidence of Manichaeism or possibly Mazdakism being practised in Mecca.
Polytheism and indigenous beliefs.
Background, belief systems and sources.
Until about the 4th century A.D, almost all Arabs practised polytheistic religions. Although significant Jewish and Christian minorities developed, polytheism remained the dominant belief system in pre-Islamic Arabia. The religious beliefs and practices of the nomadic bedouin were distinct from those of the settled tribes of towns such as Mecca. Nomadic religious belief systems and practices are believed to have included fetishism, totemism and ancestor worship but were connected principally with immediate concerns and problems and did not consider larger philosophical questions such as the afterlife. Settled urban Arabs, on the other hand, are thought to have believed in a more complex pantheon of deities. While the Meccans and the other settled inhabitants of the Hejaz worshipped their gods at permanent shrines in towns and oases, the bedouin practised their religion on the move. Historians have debated whether these belief systems were derived from indigenous Semitic religious traditions or were a "degenerate" offshoot of the more sophisticated mythologies of the nearby Fertile Crescent.
The contemporary sources of information regarding the pre-Islamic pantheon include a small number of inscriptions and carvings, remnants of stone idol-worship, references in the poetry of the pre-Islamic Arab poet Zuhair and pre-Islamic personal names. Nevertheless, information is limited and while scholars believe that the dominant traditions of the pre-Islamic Arabia were polytheistic, there is little certainty about the nature of pre-Islamic polytheism and considerable debate. According to F.E. Peters, "one of the characteristics of Arab paganism as it has come down to us is the absence of a mythology, narratives that might serve to explain the origin or history of the gods."
The majority of extant information about Mecca during the rise of Islam and earlier times comes from the text of Quran itself and later Muslim sources such as the "sīra" literature dealing with the biography of the prophet Muhammad and the 8th-century Book of Idols. Alternative sources are so fragmentary and specialized that writing a convincing history of this period based on them alone is impossible. Several scholars hold that the sīra literature is not independent of Quran but has been fabricated to explain the verses of Quran. There is evidence to support the contention that some reports of the sīras are of dubious validity, but there is also evidence to support the contention that the sīra narratives originated independently of the Quran. Compounding the problem is that the earliest extant Muslim historical works, including the sīras, were composed in their definitive form more than a century after the beginning of the Islamic era. Some of these works were based on subsequently lost earlier texts, which in their turn recorded a fluid oral tradition. Scholars do not agree as to the time when such oral accounts began to be systematically collected and written down, and they differ greatly in their assessment of the historical reliability of the available texts.
Allah.
Some scholars postulate that in pre-Islamic Arabia, including in Mecca, Allah was considered to be a deity, possibly a creator god or a supreme deity in a polytheistic pantheon. The word "Allah" (from the Arabic "al-ilah" meaning "the god") may have been used as a title rather than a name. The concept of "Allah" may have been vague in the Meccan religion, and some scholars postulate based on Quranic verse that he may have had sons and daughters who were also divinities. However, according to F.E. Peters, in these verses, the Quran is actually here refuting the idea that Allah had any children and asserts that if he had any offspring then they would surely be sons.
Regional variants of the word "Allah" occur in both pagan and Christian pre-Islamic inscriptions. Muhammad's father's name was meaning "the servant of Allāh".
Mecca.
The Kaaba, whose environs were regarded as sacred ("haram"), became a national shrine under the custodianship of the Quraysh, the chief tribe of Mecca, which made the Hejaz the most important religious area in north Arabia. Its role was solidified by a confrontation with the Christian king Abraha, who controlled much of Arabia from a seat of power in Yemen in the middle of the sixth century. Abraha had recently constructed a splendid church in Sana'a, and he wanted to make that city a major center of pilgrimage, but Mecca's Kaaba presented a challenge to his plan. Abraha found a pretext, presented by different sources alternatively as pollution of the church by a tribe allied to the Meccans or as an attack on Abraha's grandson in Najran by a Meccan party. The defeat of the army he assembled to conquer Mecca is recounted with miraculous details by the Islamic tradition and is also alluded to in the Quran and pre-Islamic poetry. After the battle, which probably occurred around 565, the Quraysh became a dominant force in western Arabia, receiving the title "God's people" ("ahl Allah") according to Islamic sources, and formed the cult association of "ḥums", which tied members of many tribes in western Arabia to the Kaaba.
According to tradition, the Kaaba was a cube-like, originally roofless structure housing a black stone venerated as a fetish. The sanctuary was dedicated to Hubal (), who, according to some sources, was worshiped as the greatest of the 360 idols the Kaaba contained, which probably represented the days of the year. Ibn Ishaq and Ibn Al-Kalbi both report that the human-shaped idol of Hubal made of precious stone came into possession of the Quraysh with its right hand broken off and that the Quraysh made a hand of gold to replace it. A soothsayer performed divination in the shrine by drawing ritual arrows, and vows and sacrifices were made to assure success. Marshall Hodgson argues that relations with deities and fetishes in pre-Islamic Mecca were maintained chiefly on the basis of bargaining, where favors were expected in return for offerings. A deity's or oracle's failure to provide the desired response was sometimes met with anger.
The three chief goddesses of Meccan religion were Al-lāt, Al-‘Uzzá, and Manāt. Some scholars postulate based on the verses of Quran that they might have been considered to be the daughters of Allah. However F.E. Peters disputes this saying that these verses are actually refuting and if had children they would be sons and not daughters. Also, he interprets that the Quran argues that these three goddesses and others were angels whose identities have been corrupted into female goddesses.
Allāt () or Al-lāt was worshipped throughout the ancient Near East with various associations. Herodotus identifies "Alilat" () as the Arabic name for Aphrodite (and, in another passage, for Urania), which is strong evidence for worship of Allāt in Arabia at that early date. According to the Book of Idols, her idol and shrine stood in Ta'if. Al-‘Uzzá () "The Mightiest" was a fertility goddess or possibly a goddess of love. Her principal shrine was in Nakhla, a day's journey from Mecca. Manāt () was the goddess of fate. According to the Book of Idols, an idol of Manāt was erected on the seashore between Medina and Mecca. Inhabitants of several areas venerated Manāt, performing sacrifices before her idol, and pilgrimages of some were not considered completed until they visited Manāt and shaved their heads.
Manaf () was another Meccan god who is thought by some scholars to be a sun-god. His idol was caressed by women. Menstruating women were forbidden from coming near his idol. The Meccans were accustomed to name their children "Abd Manaf". Muhammad's great-great-grandfather's name was Abd Manaf which means "slave of Manaf".
The pantheon of the Quraysh was not identical with that of the tribes who entered into various cult and commercial associations with them, especially that of the "hums". Christian Julien Robin argues that the former was composed principally of idols that were in the sanctuary of Mecca, including Hubal and Manaf, while the pantheon of the associations was superimposed on it, and its principal deities included the three goddesses, who had neither idols nor a shrine in that city.
Different theories have been proposed regarding the role of Allah in Meccan religion. According to one hypothesis, which goes back to Julius Wellhausen, Allah (the supreme deity of the tribal federation around Quraysh) was a designation that consecrated the superiority of Hubal (the supreme deity of Quraysh) over the other gods. However, there is also evidence that Allah and Hubal were two distinct deities. According to that hypothesis, the Kaaba was first consecrated to a supreme deity named Allah and then hosted the pantheon of Quraysh after their conquest of Mecca, about a century before the time of Muhammad. Some inscriptions seem to indicate the use of Allah as a name of a polytheist deity centuries earlier, but we know nothing precise about this use. Some scholars have suggested that Allah may have represented a remote creator god who was gradually eclipsed by more particularized local deities. There is disagreement on whether Allah played a major role in the Meccan religious cult. No iconic representation of Allah is known to have existed.
The second half of the sixth century was a period of political disorder in Arabia and communication routes were no longer secure. Religious divisions were an important cause of the crisis. Judaism became the dominant religion in Yemen while Christianity took root in the Arab-Persian Gulf. In line with the broader trends of the ancient world, Arabia yearned for a more spiritual form of religion and began believing in afterlife, while the choice of religion increasingly became a personal rather than communal choice. While many were reluctant to convert to a foreign faith, those faiths provided intellectual and spiritual reference points, and the old pagan vocabulary of Arabic began to be replaced by Jewish and Christian loanwords from Aramaic everywhere, including Mecca. The distribution of pagan temples supports Gerald Hawting's argument that Arabian polytheism was marginalized in the region and already dying in Mecca on the eve of Islam. The practice of polytheistic cults was increasingly limited to the steppe and the desert, and in Yathrib, which included two tribes with polytheistic majority, the absence of a public pagan temple in the town or its immediate neighborhood indicates that polytheism was confined to the private sphere. Looking at the text of Quran itself, Hawting has also argued that the criticism of idolators and polytheists contained in Quran is in fact a hyperbolic reference to other monotheists, in particular the Arab Jews and Arab Christians, whose religious beliefs were considered imperfect. According to some traditions, the Kaaba contained no statues, but its interior was decorated with images of Mary and Jesus, of prophets, angels, and trees.
To counter the effects of anarchy, the institution of sacred months during which every act of violence was prohibited, was reestablished. During those months, it was possible to participate in pilgrimages and fairs without danger. The Quraysh upheld the principle of two annual truces, one of one month and the second of three months, which conferred a sacred character to the Meccan sanctuary. The cult association of "hums", in which individuals and groups partook in the same rites, was primarily religious, but it also had important economic consequences. Although, as Patricia Crone has shown, Mecca could not compare with the great centers of caravan trade on the eve of Islam, it was probably one of the most prosperous and secure cities of the peninsula, since, unlike many of them, it did not have surrounding walls. Pilgrimage to Mecca was a popular custom. Some Islamic rituals, including processions around the Kaaba and between the hills of al-Safa and Marwa, as well as the salutation "we are here, O Allah, we are here" repeated on approaching the Kaaba are believed to have antedated Islam. Spring water acquired a sacred character in Arabia early on and Islamic sources state that the well of Zamzam became holy long before the Islamic era.
South Arabia.
The civilizations of south Arabia had the most developed pantheon in the Arabian peninsula. Evidence from surviving inscriptions suggests that each of the southern kingdoms of Qataban, Saba, Hadhramaut, Ma'in and Himyar had its own pantheon of three to five deities, the major deity always being a god. For example, the pantheon of Saba comprised Almaqah, the major deity, together with Athtar, Haubas, Himyam, and Dhat-Badan The main god in Ma'in and Himyar was Athtar, in Qataban it was Amm, and in Hadhramaut it was Sayin. Amm was a moon god and was associated with the weather, especially lightning.
Each kingdom's central temple was the focus of worship for the main god and would be the destination for an annual pilgrimage, with regional temples dedicated to a local manifestation of the main god. Other beings worshipped included local deities or deities dedicated to specific functions as well as deified ancestors.
Other deities included:
Nabateans.
The main deity of the Nabatean culture in northern Arabia was Dushara (). He was the only god known for certain to have been worshipped throughout Nabatea and was associated with the Greek gods Zeus and Dionysus. The meaning of his name is not clear as there are no definite interpretations of it. John Healey speculated his name to mean "The lord of ." Dushara was represented in the form of a stone cube or more generally in the form of cuboid architecture which can be seen throughout the remains of the Nabateans' principal city, Petra. Warwick Ball has noted a possible connection with the Kaaba and has commented that, as a result, "the Islamic abstract concept of deity certainly owes a debt to Nabatean religion".
Al-Uzza was worshipped in Nabataea where she had been adopted alongside Dushara as the presiding goddess at Petra, the Nabataean capital, where she assumed attributes of Isis, Tyche, and Aphrodite. She was the protectress of the city and also of love and immortality. Despite the same name shared between the al-Uzza of Nabataea and that of Mecca and other places, it is unclear whether there is any continuity of worship or identity between them.
Manat was another Nabatean goddess and was identified with the Greek goddess Nemesis. She was the goddess of fate and justice. Within the Nabataean kingdom, the place she is most often mentioned is Hegra however there is no direct portrayal of her. In some of the inscriptions, she is linked with Dushara in cursing and fining those who violate the terms of use of the tombs and do not observe the rules, respectively. In two of these inscriptions she is linked with her "Qaysha" which according to various interpretations might be referring to another deity or an object.
Al-lat was another Nabatean goddess who was probably identified with Athena and Tyche. An image of her containing elements of both human and block form exists at 'Ain Shellaleh in er-Rumm along with an inscription which describes her as the goddess of Bosra. Three inscriptions mentioning her exist in Salkhad. However, her name isn't recorded anywhere in Bosra or Petra. Only a single bust of her near the Arched Gate of Petra testifies her existence in the capital. An inscription in Hegra on a tomb mentions her as cursing those who violate the terms of its use.
In the same inscription where Al-lat is mentioned, a deity named "Hubul" is also mentioned. Jane Taylor takes this deity to be a god of divination. This is the only place outside South Arabia where a name similar to that of Hubal is mentioned. Maxime Rodinson suggests that the Meccan god Hubal may have been of Nabataean origin.
Petra has many "sacred high places" which include altars that have usually been interpreted as places of human sacrifice, although, since the 1960s, an alternative theory that they are "exposure platforms" for placing the corpses of the deceased as part of a funerary ritual has been put forward. However, there is, in fact, little evidence for either proposition.
Other northern Arabian cultures.
Religious worship amongst the Qedarites, an ancient tribal confederation that was probabably subsumed into Nabatea around the 2nd century AD, was centered around a polytheistic system in which women rose to prominence. Divine images of the gods and goddesses worshipped by Qedarite Arabs, as noted in Assyrian inscriptions, included representations of Atarsamain, Nuha, Ruda, Dai, Abirillu and Atarquruma. The female guardian of these idols, usually the reigning queen, served as a priestess ("apkallatu", in Assyrian texts) who communed with the other world. Inscriptions in a North Arabian dialect in the region of Najd referring to Nuha describe emotions as a gift from him. In addition, they also refer to Ruda being responsible for all things good and bad. There is also evidence that the Qedar worshipped Al-lāt to whom the inscription on a silver bowl from a king of Qedar is dedicated. In the Babylonian Talmud, which was passed down orally for centuries before being transcribed c. 500 AD, in tractate Taanis (folio 5b), it is said that most Qedarites worshiped pagan gods .
The Midianites, a people referred to in the Book of Genesis and located in north-western Arabia, may have worshipped Yahweh. Indeed, some scholars believe that Yahweh was originally a Midianite god and that he was subsequently adopted by the Israelites. An Egyptian temple of Hathor continued to be used during the Midianite occupation of the site, although images of Hathor were defaced suggesting Midianite opposition. They transformed it into a desert tent-shrine set up with a copper sculpture of a snake.
Eastern Arabia.
The Dilmun civilization, which existed along the Gulf Coast and Bahrain until the 6th century BC, worshipped a pair of deities, Inzak and Meskilak. It is not known whether these were the only deities in the pantheon or whether there were others. The discovery of wells at the sites of a Dilmun temple and a shrine suggests that sweet water played an important part in religious practices.
In the subsequent Greco-Roman period, there is evidence that the worship of non-indigenous deities was brought to the region by merchants and visitors. These included Bel, a god popular in the Syrian city of Palmyra, the Mesopotamian deities Nabu and Shamash, the Greek gods Poseidon and Artemis as well as the west Arabian deities Kahl and Manat.
Bedouins.
The Bedouin were introduced to Meccan ritualistic practices as they frequented settled towns of the Hejaz during the four months of the "holy truce", the first three of which were devoted to religious observance, while the fourth was set aside for trade. Alan Jones infers from Bedouin poetry that the gods, even Allah, were less important to the Bedouins than Fate. They seem to have had little trust in rituals and pilgrimages as means of propitiating Fate, but had recourse to divination and soothsayers ("kahins"). The Bedouins regarded some trees, wells, caves and stones as sacred objects, either as fetishes or as means of reaching a deity. They created sanctuaries where people could worship fetishes.
The Bedouins had a code of honour which Fazlur Rahman Malik states may be regarded as their religious ethics. This code encompassed women, bravery, hospitality, honouring one's promises and pacts, and vengeance. They believed that the ghost of a slain person would cry out from the grave until their thirst for blood was quenched. Practices such as killing of infant girls were often regarded as having religious sanction. Numerous mentions of jinn in the Quran and testimony of both pre-Islamic and Islamic literature indicate that the belief in spirits was prominent in pre-Islamic Bedouin religion. However, there is evidence that the word jinn is derived from Aramaic, where it was used by Christians to designate pagan gods reduced to the status of demons, and was introduced into Arabic folklore only late in the pre-Islamic era. Julius Wellhausen has observed that such spirits were thought to inhabit desolate, dingy and dark places and that they were feared. One had to protect oneself from them, but they were not the objects of a true cult.
Bedouin religious experience also included an apparently indigenous cult of ancestors. The dead were not regarded as powerful, but rather as deprived of protection and needing charity of the living as a continuation of social obligations beyond the grave. Only certain ancestors, especially heroes from which the tribe was said to derive its name, seem to have been objects of real veneration.
Judaism.
A thriving community of Jewish tribes existed in pre-Islamic Arabia and included both sedentary and nomadic communities. Jews had migrated into Arabia from Roman times onwards. Arabian Jews spoke Arabic as well as Hebrew and Aramaic and had contact with Jewish religious centers in Babylonia and Palestine. The Yemeni Himyarites converted to Judaism in the 4th century, and some of the Kindah, a tribe in central Arabia who were their vassals, were also converted in the 4th/5th century. There is evidence that Jewish converts in the Hejaz were regarded as Jews by other Jews and non-Jews alike and have sought advice from Babylonian rabbis on matters of attire and kosher food. In at least one case, it is known that an Arab tribe agreed to adopting Judaism as a condition for settling in a town dominated by Jewish inhabitants. Some Arab women in Yathrib/Medina are said to have vowed making their child a Jew if the child survived, since they considered the Jews to be people "of knowledge and the book" ("`ilmin wa-kitābin"). Philip Hitti infers from proper names and agricultural vocabulary that the Jewish tribes of Yathrib consisted mostly of Judaized clans of Arabian and Aramaean origin.
The key role played by Jews in the trade and markets of the Hejaz meant that market day for the week was the day preceding the Jewish Sabbath. This day, which was called "aruba" in Arabic, also provided occasion for legal proceedings and entertainment, which in turn may have influenced the choice of Friday as the day of Muslim congregational prayer. Toward the end of the sixth century, the Jewish communities in the Hejaz were in a state of economic and political decline, but they continued to flourish culturally in and beyond the region. They had developed their distinctive beliefs and practices, with a pronounced mystical and eschatological dimension. In the Islamic tradition, based on a phrase in the Qur'an, Arabic Jews are said to have referred to Uzair as the son of Allah, although historical accuracy of this assertion has been disputed.
Christianity.
The main areas of Christian influence in Arabia were on the north eastern and north western borders and in what was to become Yemen in the south. The north west was under the influence of Christian missionary activity from the Roman Empire where the Ghassanids, a client kingdom of the Romans, were converted to Christianity. In the south, particularly at Najran, a centre of Christianity developed as a result of the influence of the Christian Kingdom of Axum based on the other side of the Red Sea in Ethiopia. Both the Ghassanids and the Christians in the south adopted Monophysitism. 
The third area of Christian influence was on the north eastern borders where the Lakhmids, a client tribe of the Sassanians, adopted Nestorianism, being the form of Christianity having the most influence in the Sassanian Empire. As the Persian Gulf region of Arabia increasingly fell under the influence of the Sasanians from the early third century, many of the inhabitants were exposed to Christianity following the eastward dispersal of the religion by Mesopotamian Christians. However, it was not until the fourth century that Christianity gained popularity in the region with the establishment of monasteries and a diocesan structure. In 1986, the remains of a church thought to date to the 4th century were discovered in Jubail in eastern Saudi Arabia.
Beth Qatraye which translates "region of the Qataris" in Syriac was the Christian name used for the region encompassing north-eastern Arabia. It included Bahrain, Tarout Island, Al-Khatt, Al-Hasa, and Qatar. Oman and the United Arab Emirates comprised the diocese known as Beth Mazunaye. The name was derived from 'Mazun', the Persian name for Oman and the United Arab Emirates. Sohar was the central city of the diocese.
In Nejd, in the centre of the peninsula, there is evidence of members of two tribes, Kindah and Taghlib, converting to Christianity in the 6th century. However, in the Hejaz in the west, whilst there is evidence of the presence of Christianity, it is not thought to have been significant amongst the indigenous population of the area.
Arabicized Christian names were fairly common among pre-Islamic Arabians, which has been attributed to the influence that Syrianized Christian Arabs had on bedouins of the peninsula for several centuries before the rise of Islam.
Neal Robinson, based on verses in the Quran, believes that some Arab Christians may have held unorthodox beliefs such as the worshipping of a divine triad of God the father, Jesus the Son and Mary the Mother. Furthermore, there is evidence that unorthodox groups such as the Collyridians, whose adherents worshiped Mary, were present in Arabia, and it has been proposed that the Qur'an refers to their beliefs. However, other scholars, notably Mircea Eliade, William Montgomery Watt, G.R. Hawting and Sidney H. Griffith, cast doubt on the historicity or reliability of such references in the Quran.
Iranian religions.
Iranian religions existed in pre-Islamic Arabia on account of Sasanian military presence along the Persian Gulf and South Arabia and on account of trade routes between the Hejaz and Iraq. Some Arabs in northeast of the peninsula converted to Zoroastrianism and several Zoroastrian temples were constructed in Najd. Some of the members from the tribe of Banu Tamim had converted to the religion. There is also evidence of existence of Manichaeism in Arabia as several early sources indicate a presence of "zandaqas" in Mecca, although the term could also be interpreted as referring to Mazdakism. There is evidence for the circulation of Iranian religious ideas in the form of Persian loan words in Quran such as "firdaws" (paradise).
Zorastrianism was introduced in the Eastern Arabia including modern-day Bahrain during the rule of Persian empires in the region starting from 250 B.C. The religion was mainly practiced in Bahrain by Persian settlers. Zorastrianism was also practiced in the Persian-ruled area of modern-day Oman. The religion also existed in Persian-ruled area of modern Yemen. The descendants of Abna, the Persian conquerors of Yemen were followers of Zorastrianism.

</doc>
<doc id="15392" url="https://en.wikipedia.org/wiki?curid=15392" title="Imperial Conference">
Imperial Conference

[[File:ImperialConference.jpg|thumb|right|300px|King George V (front, centre) with his prime ministers in 1926. Standing (left to right): Walter Stanley Monroe (Newfoundland), Gordon Coates (New Zealand), Stanley Bruce (Australia), 
J. B. M. Hertzog (Union of South Africa), 
W.T. Cosgrave (Irish Free State). Seated: Stanley Baldwin (United Kingdom), King George V, William Lyon Mackenzie King (Canada).]]
Imperial Conferences (Colonial Conferences before 1907) were periodic gatherings of government leaders from the self-governing colonies and dominions of the British Empire between 1887 and 1937, before the establishment of regular Meetings of Commonwealth Prime Ministers in 1944. They were held in 1887, 1894, 1897, 1902, 1907, 1911, 1921, 1923, 1926, 1930, 1932 and 1937.
All the conferences were held in London, the United Kingdom, the seat of the Empire, except for the 1894 and 1932 conferences which were held in Ottawa, the capital of the most senior dominion. The 1907 conference changed the name of the meetings to Imperial Conferences and agreed that the meetings should henceforth be regular rather than taking place while overseas statesmen were visiting London for royal occasions (e.g. jubilees and coronations).
Notable meetings.
Originally instituted to emphasise imperial unity, as time went on, the conferences became a key forum for dominion governments to assert the desire for removing the remaining vestiges of their colonial status. The conference of 1926 agreed the Balfour Declaration, which acknowledged that the dominions would henceforth rank as equals to the United Kingdom, as members of the 'British Commonwealth of Nations'.
The conference of 1930 decided to abolish the legislative supremacy of the British Parliament as it was expressed through the Colonial Laws Validity Act and other Imperial Acts. The statesmen recommended that a declaratory enactment of the Parliament, which became the Statute of Westminster 1931, be passed with the consent of the dominions, but some dominions did not ratify the statute until some years afterwards. The 1930 conference was notable, too, for the attendance of Southern Rhodesia, despite it being a self-governing colony, not a dominion.
The 1932 British Empire Economic Conference held in Ottawa discussed the Great Depression, and the governments agreed to institute 'Imperial Preference': a system of protectionist tariffs on imports from non-imperial countries.
Towards Commonwealth meetings.
As World War II drew to a close, Imperial Conferences were replaced by Commonwealth Prime Ministers' Conferences, with 17 such meetings occurring from 1944 until 1969, all but one of the meetings occurred in London. The gatherings were renamed Commonwealth Heads of Government Meetings (CHOGM) in 1971 and were henceforth held every two years with hosting duties rotating around the Commonwealth.

</doc>
<doc id="15395" url="https://en.wikipedia.org/wiki?curid=15395" title="International Refugee Organisation">
International Refugee Organisation

The International Refugee Organization (IRO) was founded on April 20, 1946 to deal with the massive refugee problem created by World War II. A Preparatory Commission began operations fourteen months previously. It was a United Nations specialized agency and took over many of the functions of the earlier United Nations Relief and Rehabilitation Administration. In 1952, its operations ceased, and it was replaced by the Office of the United Nations High Commissioner for Refugees (UNHCR).
The '"Constitution of the International Refugee Organization", adopted by the United Nations General Assembly on December 15, 1946, specified the agency's field of operations. Controversially, this defined "persons of German ethnic origin" who had been expelled, or were to be expelled from their countries of birth into the postwar Germany, as individuals who would "not be the concern of the Organization." This excluded from its purview a group that exceeded in number all the other European displaced persons put together. Also, because of disagreements between the Western allies and the Soviet Union, the IRO only worked in areas controlled by Western armies of occupation.
Twenty-six states became members of the IRO and it formally came into existence in 1948: Argentina, Australia, Belgium, Bolivia, Brazil, Canada, Republic of China, Denmark, the Dominican Republic, France, Guatemala, Honduras, Iceland, Italy, Liberia, Luxembourg, the Netherlands, New Zealand, Norway, Panama, Peru, the Philippines, Switzerland, the United Kingdom, the United States, and Venezuela. The U.S. provided about 40% of the IRO's $155 million annual budget. The total contribution by the members for the five years of operation was around $400 million. It had rehabilitated around 10 million people during this time, out of 15 million people who were stranded in Europe. The IRO's first Director General was William Hallam Tuck, succeeded by J. Donald Kingsley on July 31, 1949.

</doc>
<doc id="15396" url="https://en.wikipedia.org/wiki?curid=15396" title="IRO">
IRO

IRO may refer to:

</doc>
<doc id="15401" url="https://en.wikipedia.org/wiki?curid=15401" title="Isabella d'Este">
Isabella d'Este

Isabella d'Este (19 May 1474 – 13 February 1539) was "Marchesa" of Mantua and one of the leading women of the Italian Renaissance as a major cultural and political figure. She was a patron of the arts as well as a leader of fashion, whose innovative style of dressing was copied by women throughout Italy and at the French court. The poet Ariosto labeled her as the "liberal and magnanimous Isabella", while author Matteo Bandello described her as having been "supreme among women". Diplomat Niccolò da Correggio went even further by hailing her as "The First Lady of the world".
She served as the regent of Mantua during the absence of her husband, Francesco II Gonzaga, Marquess of Mantua and the minority of her son, Federico, Duke of Mantua. In 1500 she met King Louis XII of France in Milan on a diplomatic mission to persuade him not to send his troops against Mantua.
She was a prolific letter-writer, and maintained a lifelong correspondence with her sister-in-law Elisabetta Gonzaga. Lucrezia Borgia was another sister-in-law; she later became the mistress of Isabella's husband.
Early life.
Isabella d'Este grew up in a cultured family in the city-state of Ferrara. She received a fine classical education and, as a girl, met many famous humanist scholars and artists. 
Due to the vast amount of extant correspondence between Isabella and her family and friends, her life is unusually well-documented. She was born on Tuesday 19 May 1474 at nine o'clock in the evening in Ferrara, to Ercole I d'Este, Duke of Ferrara and Eleanor of Naples. Eleanor was the daughter of Ferdinand I, the Aragonese King of Naples, and Isabella of Clermont.
One year later on 29 June 1475 her sister Beatrice d'Este was born, and in 1476 and 1477 two brothers, Alfonso and Ippolito arrived. In 1479 and 1480 two more brothers were born; they were Ferrante and Sigismondo. Of all the children Isabella was considered to have been the favourite.
In the year of Ferrante's birth, Isabella travelled to Naples with her mother. When her mother returned to Ferrara, Isabella accompanied her, while the other children stayed behind with their grandfather for eight years. It was during the journey with her mother that Isabella acquired the art of diplomacy and statecraft.
Education.
Isabella, being naturally gifted and intellectually precocious in her youth, received an excellent education. As a child she studied Roman history, and rapidly learned to translate Greek and Latin (the former would become her favourite language) needed. Because of her outstanding intellect, she often discussed the classics and the affairs of state with ambassadors. Moreover, she was personally acquainted with the painters, musicians, writers, and scholars, who lived in and around the court. Besides her knowledge of history and languages, she could also recite Virgil and Terence by heart. Isabella was also a talented singer and musician, and was taught to play the lute by Giovanni Angelo Testagrossa In addition to all these admirable accomplishments, she also was an innovator of new dances, having been instructed in the art by Ambrogio, a Jewish dancing master.
She was described as having been physically attractive, albeit slightly plump; however, she also possessed "lively eyes" and was "of lively grace".
In 1480, at the age of six, Isabella was betrothed to Gianfrancesco, the heir to the Marquis of Mantua. Although he was not handsome, Isabella admired him for his strength and bravery; she also regarded him as a gentleman. After their first few encounters, she found that she enjoyed his company and spent the next few years getting to know him and preparing herself to be the Marchesa of Mantua. During their courtship, Isabella treasured the letters, poems, and sonnets he sent her as gifts.
Marriage.
Ten years later on 11 February 1490, at age 15, she married Francesco Gonzaga, who had by then succeeded to the marquisate. Isabella became his wife and Marchesa amid a spectacular outpouring of popular acclamation. Besides Marquis, Francesco was also Captain General of the armies of the Republic of Venice. She brought as her marriage portion the sum of 3,000 ducats besides valuable jewellery, dishes, and a silver service. Prior to the magnificent banquet which followed the wedding ceremony, Isabella rode through the main streets of Ferrara astride a horse draped in gems and gold.
As the couple had known and admired one another for many years, their mutual attraction deepened into love; marriage to Francesco allegedly caused Isabella to "bloom". At the time of her wedding, Isabella was said to have been pretty, slim, graceful and well-dressed. Her long, fine hair was dyed pale blonde, and her eyes, "brown as fir cones in autumn, scattered laughter".
Francesco, in his capacity of Captain General of the Venetian armies, was often required to go to Venice for conferences which left Isabella in Mantua on her own at "La Reggia" the ancient palace which was the family seat of the Gonzagas. She did not lack company, however, as she passed the time with her mother and sister, Beatrice; and upon meeting Elisabetta Gonzaga, her 18-year-old sister-in-law, the two women became close friends. They enjoyed reading books, playing cards, and travelling about the countryside together. Once they journeyed as far as Lake Garda during one of Francesco's absences, and later travelled to Venice. They maintained a steady correspondence until Elisabetta's death in 1526.
Almost four years after her marriage in December 1493, Isabella gave birth to her first child out of an eventual total of eight; it was a daughter, Eleonora, whom they called Leonora for short.
Children.
Together Isabella and Francesco had eight children:
Lucrezia Borgia.
A year after her marriage to Isabella's brother, Alfonso in 1502, the notorious Lucrezia Borgia became the mistress of Francesco. Isabella had given birth to a daughter, Ippolita at about the same time, and she continued to bear him children throughout Francesco and Lucrezia's long, passionate affair, which was more sexual than romantic. Lucrezia had previously made overtures of friendship to Isabella which the latter had coldly and disdainfully ignored. From the time Lucrezia had first arrived in Ferrara as Alfonso's intended bride, Isabella, despite having acted as hostess during the wedding festivities, had regarded Lucrezia as a rival, whom she sought to outdo at every opportunity. Francesco's affair with Lucrezia, whose beauty was renowned, caused Isabella much jealous suffering and emotional pain. Their liaison ended when he contracted syphilis as a result of encounters with prostitutes.
Regency.
Isabella played an important role in Mantua during the city's troubled times. When her husband was captured in 1509 and held hostage in Venice, she took control of Mantua's military forces and held off their invaders until his release in 1512. In the same year, 1512, she was the hostess at the Congress of Mantua, which was held to settle questions concerning Florence and Milan. As a ruler, she appeared to have been much more assertive and competent than her husband. When apprised of this fact upon his return, Francesco was furious and humiliated at being upstaged by his wife's superior political ability. This caused their marriage to break down irrevocably. As a result, Isabella began to travel freely and live independently from her husband until his death on 19 March 1519.
After the death of her husband, Isabella ruled Mantua as regent for her son, Federico. She began to play an increasingly important role in Italian politics, steadily advancing Mantua's position. She was instrumental in promoting Mantua to a Duchy, which was obtained by wise diplomatic use of her son's marriage contracts. She also succeeded in obtaining a cardinalate for her son Ercole. She further displayed shrewd political acumen in her negotiations with Cesare Borgia, who had dispossessed Guidobaldo da Montefeltro, duke of Urbino, the husband of her sister-in-law and good friend Elisabetta Gonzaga in 1502.
Cultural pursuits.
Isabella d’Este is famous as the most important art patron of the Renaissance; her life is documented by her correspondence, which is still archived in Mantua (c. 28,000 letters received and copies of c. 12,000 letters written).
Isabella d'Este and Leonardo’s "Mona Lisa".
Isabella d’Este has been proposed as a plausible candidate for Leonardo’s ‘Mona Lisa’ of 1502-06, which is usually considered a portrait of Lisa del Giocondo. (Lisa was the wife of a merchant in Florence and Giorgio Vasari wrote of her portrait by Leonardo – it remains open whether this is the portrait now known as the ‘Mona Lisa’.) Evidence in favor of Isabella as the subject of the famous work includes Leonardo’s drawing ‘Isabella d’Este’ from 1499 and her letters of 1501-06 requesting the promised painted portrait; further arguments are the mountains in the background and the armrest as a Renaissance symbol for a portrait of a sovereign.
Potential portrait identifications.
Despite her significant art patronage which included a number of portraits – no other person of her time was so often portrayed – there are very few surviving identified portraits of Isabella. These few identifications are known as inhomogeneous (i.e. differing eye and hair colours as well as divergent eyebrows in both Titian portraits) and there are no images of her between the ages of 26 and 54 (see picture). It is known that the elderly Isabella preferred idealized paintings and even waived sitting as a model. However it could be assumed that she still insisted on seeing her personal characteristics in the outcome.
In recent years several museums have withdrawn their few identifications of portraits as Isabella because of the risk of misidentification. The remaining three colourful portraits are still inhomogeneous (Kunsthistorisches Museum/KHM, Vienna): 
‘La Bella’ (Palazzo Pitti, Florence) has been discussed as an alternative to Titian’s 1536 portrait in Vienna, because the commission from the 60-year-old patron was for a rejuvenated portrait; if La Bella were Isabella, eye colour, hair colour, eyebrows and general appearance would homogenize in all known portraits, allowing potential links toward further identifications.
At present the 1495 medal by Gian Cristoforo Romano (several extant copies) is the only reliable identification because of the inscription created during Isabella's life time.
Diplomatic missions and her treatment of slaves.
Isabella had met the French king in Milan in 1500 on a successful diplomatic mission which she had undertaken to protect Mantua from French invasion. Louis had been impressed by her alluring personality and keen intelligence. It was while she was being entertained by Louis, whose troops occupied Milan, that she offered asylum to Milanese refugees including Cecilia Gallerani, the refined mistress of her sister Beatrice's husband, Ludovico Sforza, Duke of Milan, who had been forced to leave his duchy in the wake of French occupation. Isabella presented Cecilia to King Louis, describing her as a "lady of rare gifts and charm".
Isabella was also an extreme example of the Renaissance European tendency to treat black African slaves in her household as exotic accessories. Isabella's fascination with black child servants is extensively documented. On 1 May 1491 Isabella asked Giorgio Brognolo, her agent in Venice, to procure a young black girl ('una moreta') between the ages of one-and-a-half and four, and twice in early June reminded him of the request, emphasizing that the girl should be 'as black as possible'. Isabella’s household and financial records reflect she already had a significantly older black girl in her service when she inquired after a younger black child. Records also reflect that she obtained a little black girl from a Venetian orphanage, opened negotiations with a Venetian patrician household for the sale of a little black boy and purchased an enslaved little black girl from her sister. The commission for the purchase of a little girl "as black as possible" could be construed as a wish for maximum exoticism.
Widowhood.
"Devoted head of state".
As a widow, Isabella at the age of 45 became a "devoted head of state". Her position as a Marchesa required her serious attention, therefore she was required to study the problems faced by a ruler of a city-state. To improve the well-being of her subjects she studied architecture, agriculture, and industry, and followed the principles that Niccolò Machiavelli had set forth for rulers in his book "The Prince". In return, the people of Mantua respected and loved her.
Isabella left Mantua for Rome in 1527. She was present during the catastrophic Sack of Rome, when she converted her house into an asylum for about 2000 people fleeing the Imperial soldiers. Isabella's house was one of the very few which was not attacked, due to the fact that her son was a member of the invading army. When she left, she managed to acquire safe passage for all the refugees who had sought refuge in her home.
Later years and death.
After Rome became stabilised following the sacking, she left the city and returned to Mantua. She made it a centre of culture, started a school for girls, and turned her ducal apartments into a museum containing the finest art treasures. This was not enough to satisfy Isabella, already in her mid-60s, so she returned to political life and ruled Solarolo, in Romagna until her death on 13 February 1539.
Legacy.
During her lifetime and after her death, poets, popes, and statesmen paid tribute to Isabella. Pope Leo X invited her to treat him with "as much friendliness as you would your brother". The latter's secretary Pietro Bembo described her as "one of the wisest and most fortunate of women"; while the poet Ariosto deemed her the "liberal and magnanimous Isabella". Author Matteo Bandello wrote that she was "supreme among women", and the diplomat Niccolò da Correggio entitled her "The First Lady of the world".
Depiction in media.
The artwork "The Dinner Party" features a place setting for Isabella d'Este.
Isabella d'Este is portrayed by Belgian actress Alexandra Oppo in the TV show Borgia.
Further reading.
George, L., The Public Perception of Isabella d'Este, "Clio History Journal", 2009.

</doc>
<doc id="15402" url="https://en.wikipedia.org/wiki?curid=15402" title="International standard">
International standard

International standards are standards developed by international standards organizations. International standards are available for consideration and use worldwide. One prominent organization is the International Organization for Standardization.
Purpose.
International standards may be used either by direct application or by a process of modifying an international standard to suit local conditions. The adoption of international standards results in the creation of equivalent, national standards that are substantially the same as international standards in technical content, but may have (i) editorial differences as to appearance, use of symbols and measurement units, substitution of a point for a comma as the decimal marker, and (ii) differences resulting from conflicts in governmental regulations or industry-specific requirements caused by fundamental climatic, geographical, technological, or infrastructural factors, or the stringency of safety requirements that a given standard authority considers appropriate.
International standards are one way of overcoming technical barriers in international commerce caused by differences among technical regulations and standards developed independently and separately by each nation, national standards organization, or company. Technical barriers arise when different groups come together, each with a large user base, doing some well established thing that between them is mutually incompatible. Establishing international standards is one way of preventing or overcoming this problem.
History.
Standardization.
The implementation of standards in industry and commerce became highly important with the onset of the Industrial Revolution and the need for high-precision machine tools and interchangeable parts. Henry Maudslay developed the first industrially practical screw-cutting lathe in 1800, which allowed for the standardisation of screw thread sizes for the first time.
Maudslay's work, as well as the contributions of other engineers, accomplished a modest amount of industry standardization; some companies' in-house standards spread a bit within their industries. Joseph Whitworth's screw thread measurements were adopted as the first (unofficial) national standard by companies around the country in 1841. It came to be known as the British Standard Whitworth, and was widely adopted in other countries.
By the end of the 19th century differences in standards between companies were making trade increasingly difficult and strained. The Engineering Standards Committee was established in London in 1901 as the world's first national standards body. After the First World War, similar national bodies were established in other countries. The Deutsches Institut für Normung was set up in Germany in 1917, followed by its counterparts, the American National Standard Institute and the French Commission Permanente de Standardisation, both in 1918.
International organizations.
By the mid to late 19th century, efforts were being made to standardize electrical measurement. An important figure was R. E. B. Crompton, who became concerned by the large range of different standards and systems used by electrical engineering companies and scientists in the early 20th century. Many companies had entered the market in the 1890s and all chose their own settings for voltage, frequency, current and even the symbols used on circuit diagrams. Adjacent buildings would have totally incompatible electrical systems simply because they had been fitted out by different companies. Crompton could see the lack of efficiency in this system and began to consider proposals for an international standard for electric engineering.
In 1904, Crompton represented Britain at the Louisiana Purchase Exposition in Saint Louis as part of a delegation by the Institute of Electrical Engineers. He presented a paper on standardisation, which was so well received that he was asked to look into the formation of a commission to oversee the process. By 1906 his work was complete and he drew up a permanent constitution for the first international standards organization, the International Electrotechnical Commission. The body held its first meeting that year in London, with representatives from 14 countries. In honour of his contribution to electrical standardisation, Lord Kelvin was elected as the body's first President.
The International Federation of the National Standardizing Associations (ISA) was founded in 1926 with a broader remit to enhance international cooperation for all technical standards and specifications. The body was suspended in 1942 during World War II.
After the war, ISA was approached by the recently formed United Nations Standards Coordinating Committee (UNSCC) with a proposal to form a new global standards body. In October 1946, ISA and UNSCC delegates from 25 countries met in London and agreed to join forces to create the new International Organization for Standardization (ISO); the new organization officially began operations in February 1947.

</doc>
<doc id="15403" url="https://en.wikipedia.org/wiki?curid=15403" title="ISO 4217">
ISO 4217

ISO 4217 is a standard published by International Organization for Standardization, which delineates currency designators, country codes (alpha and numeric), and references to minor units in three tables:
The tables, history and ongoing discussion are maintained by SIX Interbank Clearing on behalf of ISO and the Swiss Association for Standardization.
The ISO 4217 code list is used in banking and business globally. In many countries the ISO codes for the more common currencies are so well known publicly that exchange rates published in newspapers or posted in banks use only these to delineate the different currencies, instead of translated currency names or ambiguous currency symbols. ISO 4217 codes are used on airline tickets and international train tickets to remove any ambiguity about the price.
Code formation.
The first two letters of the code are the two letters of ISO 3166-1 alpha-2 country codes (which are also used as the basis for national top-level domains on the Internet) and the third is usually the initial of the currency itself. So Japan's currency code is JPY—JP for Japan and Y for yen. This eliminates the problem caused by the names "dollar, franc" and "pound" being used in dozens of different countries, each having significantly differing values. Also, if a currency is revalued, the currency code's last letter is changed to distinguish it from the old currency. In some cases, the third letter is the initial for "new" in that country's language, to distinguish it from an older currency that was revalued; the code sometimes outlasts the usage of the term "new" itself (for example, the code for the Mexican peso is MXN). Other changes can be seen, however; the Russian ruble, for example, changed from RUR to RUB, where the B comes from the third letter in the word "ruble".
X currencies.
In addition to codes for most active national currencies ISO 4217 provides codes for "supranational" currencies, procedural purposes, and several things which are "similar to" currencies:
The use of an initial letter "X" for these purposes is facilitated by the ISO 3166 rule that no official country code beginning with X will ever be assigned. Because of this rule ISO 4217 can use X codes without risk of clashing with a future country code. ISO 3166 country codes beginning with "X" are used for private custom use (reserved), never for official codes. For instance, the ISO 3166-based NATO country codes (STANAG 1059, 9th edition) use "X" codes for imaginary exercise countries ranging from XXB for "Brownland" to XXR for "Redland", as well as for major commands such as XXE for SHAPE or XXS for SACLANT. Consequently, ISO 4217 can use "X" codes for non-country-specific currencies without risk of clashing with future country codes.
The inclusion of EU (denoting the European Union) in the ISO 3166-1 reserved codes list, allows the euro to be coded as EUR rather than assigned a code beginning with X even though it is a supranational currency.
Treatment of minor currency units (the "exponent").
The ISO 4217 standard includes a crude mechanism for expressing the relationship between a major currency unit and its corresponding minor currency unit. This mechanism is called the currency "exponent" and assumes a base of 10. For example, USD (the United States dollar) is equal to 100 of its minor currency unit the "cent". So the USD has exponent 2 (10 to the power 2 is 100, which is the number of cents in a dollar). The code JPY (Japanese yen) is given the exponent 0, because its minor unit, the sen, although nominally valued at 1/100 of a yen, is of such negligible value that it is no longer used. Usually, as with the USD, the minor currency unit has a value that is 1/100 of the major unit, but in some cases (including most varieties of the dinar) 1/1000 is used, and sometimes ratios apply which are not integer powers of 10. Mauritania does not use a decimal division of units, setting 1 ouguiya (UM) equal to 5 khoums, and Madagascar has 1 ariary = 5 iraimbilanja. Some currencies do not have any minor currency unit at all and these are given an exponent of 0, as with currencies whose minor units are unused due to negligible value.
Currency numbers.
There is also a three-digit code number assigned to each currency, in the same manner as there is also a three-digit code number assigned to each country as part of ISO 3166. This numeric code is usually the same as the ISO 3166-1 numeric code. For example, USD (United States dollar) has code 840 which is also the numeric code for the US (United States).
Position of ISO 4217 code in amounts.
The ISO standard does not regulate either the spacing, prefixing or suffixing in usage of currency codes. According however to the European Union's Publication Office, in English, Irish, Latvian and Maltese texts, the ISO 4217 code is to be followed by a fixed space and the amount:
In Bulgarian, Croatian, Czech, Danish, Dutch, Estonian, Finnish, French, German, Greek, Hungarian, Italian, Lithuanian, Polish, Portuguese, Romanian, Slovak, Slovene, Spanish and Swedish the order is reversed; the amount is followed by a fixed space and the ISO 4217 code:
Note that, as illustrated, the order is determined not by the currency, but by the native language of the document context.
History.
In 1973, the ISO Technical Committee 68 decided to develop codes for the representation of currencies and funds for use in any application of trade, commerce or banking. At the 17th session (February 1978), the related UN/ECE Group of Experts agreed that the three-letter alphabetic codes for International Standard ISO 4217, "Codes for the representation of currencies and funds", would be suitable for use in international trade.
Over time, new currencies are created and old currencies are discontinued. Frequently, these changes are due to the formation of new governments, treaties between countries standardizing on a shared currency, or revaluation of an existing currency due to excessive inflation. As a result, the list of codes must be updated from time to time. The ISO 4217 maintenance agency (MA), SIX Interbank Clearing, is responsible for maintaining the list of codes.
Active codes.
The following is a list of active codes of official ISO 4217 currency names.
USD/USS/USN, three currency codes belonging to the US.
The US dollar has two codes assigned: USD and USN (next day). The USS (same day) code is not in use any longer, and was removed from the list of active ISO 4217 codes in March 2014.
According to UN/CEFACT recommendation 9, paragraphs 8–9 ECE/TRADE/203, 1996, available online:
Non ISO 4217 currencies.
Currencies without ISO 4217 currency codes.
A number of currencies are not included in ISO 4217, because these currencies are: (a) minor currencies pegged 1:1 to a larger currency, even if independently regulated (b) a legal tender only issued as commemorative banknotes or coinage, or (c) a currency of an unrecognized or partially recognized state. These currencies include:
See :Category:Fixed exchange rate for a list of all currently pegged currencies.
Unofficial currency codes.
Despite the lack of an ISO code, the following non-ISO codes are sometimes used commercially.
In addition, GBX is sometimes used (for example on the London Stock Exchange) to denote Penny sterling, a subdivision of pound sterling, the currency for the United Kingdom.
On top of bitcoin listed in the table, there are countless other cryptocurrency that each have their own codes and being used on various cryptocurrency exchanges, for instance LTC for Litecoin, NMC for Namecoin and XRP for Ripples.
A study group of ISO 4217 is currently working on cryptocurrencies case
Historical currency codes.
A number of currencies were official ISO 4217 currency codes and currency names until their replacement by the euro or other currencies. The table below shows the ISO currency codes of former currencies and their common names (which do not always match the ISO 4217 names). These codes were introduced in 1989 after a request from the reinsurance sector in 1988 was accepted.

</doc>
<doc id="15406" url="https://en.wikipedia.org/wiki?curid=15406" title="Irgun">
Irgun

The Irgun (; full title: ', lit. "The National Military Organization in the Land of Israel"), was a Zionist paramilitary organization that operated in Mandate Palestine between 1931 and 1948. It was an offshoot of the older and larger Jewish paramilitary organization Haganah (Hebrew: "Defense", הגנה). When the group broke from the Haganah it became known as the "Haganah Bet" (Hebrew: literally "Defense 'B' " or "Second Defense", ), or alternatively as haHaganah haLeumit () or Hama'amad (). Irgun members were absorbed into the Israel Defense Forces at the start of the 1948 Arab–Israeli war. The Irgun is also referred to as Etzel (), an acronym of the Hebrew initials, or by the abbreviation IZL.
The Irgun policy was based on what was then called Revisionist Zionism founded by Ze'ev Jabotinsky. According to Howard Sachar, "The policy of the new organization was based squarely on Jabotinsky's teachings: every Jew had the right to enter Palestine; only active retaliation would deter the Arabs; only Jewish armed force would ensure the Jewish state".
Two of the operations for which the Irgun is best known are the bombing of the King David Hotel in Jerusalem on 22 July 1946 and the Deir Yassin massacre, carried out together with Lehi on 9 April 1948.
The Irgun has been viewed as a terrorist organization or organization which carried out terrorist acts. Specifically the organization "committed acts of terrorism and assassination against the British, whom it regarded as illegal occupiers, and it was also violently anti-Arab" according to the Encyclopædia Britannica. In particular the Irgun was described as a terrorist organization by the United Nations, British, and United States governments, and in media such as "The New York Times" newspaper, and by the Anglo-American Committee of Inquiry., the 1946 Zionist Congress and the Jewish Agency. Irgun's tactics appealed to a certain segment of the Jewish community that believed that any action taken in the cause of the creation of a Jewish state was justified, including terrorism.
The Irgun was a political predecessor to Israel's right-wing "Herut" (or "Freedom") party, which led to today's Likud party. Likud has led or been part of most Israeli governments since 1977.
Nature of the Movement.
Members of the Irgun came mostly from Betar and from the Revisionist Party both in Palestine and abroad. The Revisionist Movement made up a popular backing for the underground organization. Ze'ev Jabotinsky, founder of Revisionist Zionism, was the commander of the organization until he died. He formulated the general realm of operation, regarding "Restraint" and the end thereof, and was the inspiration for the organization overall. An additional major source of ideological inspiration was the poetry of Uri Zvi Greenberg. The symbol of the organization, with the motto רק כך (only thus), underneath a hand holding a rifle in the foreground of a map showing both Mandatory Palestine and the Emirate of Transjordan (at the time, both were administered under the terms of the British Mandate for Palestine), implying that force was the only way to "liberate the homeland".
The number of members of the Irgun varied from a few hundred to a few thousand. Most of its members were people who joined the organization's command, under which they carried out various operations and filled positions, largely in opposition to British law. Most of them were "ordinary" people, who held regular jobs, and only a few dozen worked full-time in the Irgun.
The Irgun disagreed with the policy of the Yishuv and with the World Zionist Organization, both with regard to strategy and basic ideology and with regard to PR and military tactics, such as use of armed force to accomplish the Zionist ends, operations against the Arabs during the riots, and relations with the British mandatory government. Therefore, the Irgun tended to ignore the decisions made by the Zionist leadership and the Yishuv's institutions. This fact caused the elected bodies not to recognize the independent organization, and during most of the time of its existence the organization was seen as irresponsible, and its actions thus worthy of thwarting. Therefore, the Irgun accompanied its armed operations with public relations campaigns, in order to convince the public of the Irgun's way and the problems with the official political leadership of the Yishuv. The Irgun put out numerous advertisements, an underground newspaper and even ran the first independent Hebrew radio station – Kol Zion HaLochemet.
Structure, command, and organization.
As an underground armed organization, members did not normally call it by its name, but rather used other names. In the first years of its existence it was known primarily as "Ha-Haganah Leumit"' (The National Defense), and also by names such as "Haganah Bet" ("Second Defense"), "Irgun Bet" ("Second Irgun"), the "Parallel Organization" and the "Rightwing Organization". Later on it was most widely known as המעמד (the Stand). The anthem adopted by the Irgun was "Anonymous Soldiers", written by Avraham (Yair) Stern who was at the time a commander in the Irgun. Later on Stern defected from the Irgun and founded Lehi, and the song became the anthem of the Lehi. The Irgun's new anthem then became the third verse of the "Betar Song", by Ze'ev Jabotinsky.
The Irgun gradually evolved from its humble origins into a serious and well-organized paramilitary organization. The movement developed a series of ranks and a sophisticated command structure, and came to demand serious military training and strict discipline from its members. It developed clandestine networks of hidden arms caches and weapons-production workshops, safe-houses, and training camps.
The ranks of the Irgun were (in ascending order): 
The Irgun was led by a High Command, which set policy and gave orders. Directly underneath it was a General Staff, which oversaw the activities of the Irgun. The General Staff was divided into a military and support staff. The military staff was divided into operational units that oversaw operations and support units in charge of planning, instruction, weapons caches and manufacture, and first aid. The military and support staff never met jointly and communicated through the High Command. Beneath the General Staff were six district commands: Jerusalem, Tel Aviv, Haifa-Galilee, Southern, Sharon, and Shomron, each led by a district commander. A local Irgun district unit was called a "Branch". A "brigade" in the Irgun was made up of three sections. A section was made up of two groups, at the head of each was a "Group Head", and a deputy. Eventually, various units were established, which answered to a "Center" or "Staff".
The head of the Irgun High Command was the overall commander of the organization, but the name of his rank varied. During the revolt against the British, Irgun commander Menachem Begin and the entire High Command held the rank of "Gundar Rishon". His predecessors, however, had held their own ranks. A rank of Military Commander (Seren) was awarded to the Irgun commander Yaakov Meridor and a rank of High Commander (Aluf) to David Raziel. Until his death in 1940, Jabotinsky was known as the "Military Commander of the Etzel" or the "Ha-Matzbi Ha-Elyon" ("Supreme Commander").
Under the command of Menachem Begin, the Irgun was divided into different corps:
In theory, the Irgun was supposed to have a regular combat force, a reserve, and shock units, but in practice there were not enough personnel for a reserve or a shock force.
The Irgun emphasized that its fighters be highly disciplined. Strict drill exercises were carried out at ceremonies at different times, and strict attention was given to discipline, formal ceremonies and military relationships between the various ranks. The Irgun put out professional publications on combat doctrine, weaponry, leadership, drill exercises, etc. Among these publications were three books written by David Raziel, who had studied military history, techniques, and strategy: "The Pistol" (written in collaboration with Avraham Stern), "The Theory of Training", and "Parade Ground and Field Drill". A British analysis noted that the Irgun's discipline was "as strict as any army in the world."
The Irgun operated a sophisticated recruitment and military training regime. Those wishing to join had to find and make contact with a member, meaning only those who personally knew a member or were persistent could find their way in. Once contact had been established, a meeting was set up with the three-member selection committee at a safe-house, where the recruit was interviewed in a darkened room, with the committee either positioned behind a screen, or with a flashlight shone into the recruit's eyes. The interviewers asked basic biographical questions, and then asked a series of questions designed to weed out romantics and adventurers and those who had not seriously contemplated the potential sacrifices. Those selected attended a four-month series of indoctrination seminars in groups of five to ten, where they were taught the Irgun's ideology and the code of conduct it expected of its members. These seminars also had another purpose - to weed out the impatient and those of flawed purpose who had gotten past the selection interview. Then, members were introduced to other members, were taught the locations of safe-houses, and given military training. Irgun recruits trained with firearms, hand grenades, and were taught how to conduct combined attacks on targets. Arms handling and tactics courses were given in clandestine training camps, while practice shooting took place in the desert or by the sea. Eventually, separate training camps were established for heavy-weapons training. The most rigorous course was the explosives course for bomb-makers, which lasted a year. The British authorities believed that some Irgun members enlisted in the Jewish section of the Palestine Police Force for a year as part of their training, during which they also passed intelligence. In addition to the Irgun's sophisticated training program, many Irgun members were veterans of the Haganah (including the Palmach), the British Armed Forces, and Jewish partisan groups that had waged guerrilla warfare in Nazi-occupied Europe, thus bringing significant military training and combat experience into the organization. The Irgun also operated a course for its intelligence operatives, in which recruits were taught espionage, cryptography, and analysis techniques.
Of the Irgun's members, almost all were part-time members. They were expected to maintain their civilian lives and jobs, dividing their time between their civilian lives and underground activities. There were never more than 40 full-time members, who were given a small expense stipend on which to live on. Upon joining, every member received an underground name. The Irgun's members were divided into cells, and worked with the members of their own cells. The identities of Irgun members in other cells were withheld. This ensured that an Irgun member taken prisoner could betray no more than a few comrades.
In addition to the Irgun's members in Palestine, underground Irgun cells composed of local Jews were established in Europe following World War II. An Irgun cell was also established in Shanghai, home to many European-Jewish refugees. The Irgun also set up a Swiss bank account. Eli Tavin, the former head of Irgun intelligence, was appointed commander of the Irgun abroad.
In November 1947, the Jewish insurgency came to an end as the UN approved of the partition of Palestine, and the British had announced their intention to withdraw the previous month. As the British left and the 1947-48 Civil War in Mandatory Palestine got underway, the Irgun came out of the underground and began to function more as a standing army rather an underground organization. It began openly recruiting, training, and raising funds, and established bases, including training facilities. It also introduced field communications and created a medical unit and supply service.
Until World War II the group armed itself with weapons purchased in Europe, primarily Italy and Poland, and smuggled to Palestine. The Irgun also established workshops that manufactured spare parts and attachments for the weapons. Also manufactured were land mines and simple hand grenades. Another way in which the Irgun armed itself was theft of weapons from the British Police and military.
Prior to World War II.
Founding.
The Irgun's first steps were in the aftermath of the Riots of 1929. In the Jerusalem branch of the Haganah there were feelings of disappointment and internal unrest towards the leadership of the movements and the Histadrut (at that time the organization running the Haganah). These feelings were a result of the view that the Haganah was not adequately defending Jewish interests in the region. Likewise, critics of the leadership spoke out against alleged failures in the number of weapons, readiness of the movement and its policy of restraint and not fighting back. On April 10, 1931, commanders and equipment managers announced that they refuse to return weapons to the Haganah that had been issued to them earlier, prior to the Nebi Musa holiday. These weapons were later returned by the commander of the Jerusalem branch, Avraham Tehomi, a.k.a. "Gideon". However, the commanders who decided to rebel against the leadership of the Haganah relayed a message regarding their resignations to the Vaad Leumi, and thus this schism created a new independent movement.
The leader of the new underground movement was Avraham Tehomi, alongside other founding members who were all senior commanders in the Haganah, members of Hapoel Hatzair and of the Histadrut. Also among them was Eliyahu Ben Horin, an activist in the Revisionist Party. This group was known as the "Odessan Gang", because they previously had been members of the "Haganah Ha'Atzmit" of Jewish Odessa. The new movement was named "Irgun Tsvai Leumi", ("National Military Organization") in order to emphasize its active nature in contrast to the Haganah. Moreover, the organization was founded with the desire to become a true military organization and not just a militia as the Haganah was at the time.
In the autumn of that year the Jerusalem group merged with other armed groups affiliated with Betar. The Betar groups' center of activity was in Tel Aviv, and they began their activity in 1928 with the establishment of "Officers and Instructors School of Betar". Students at this institution had broken away from the Haganah earlier, for political reasons, and the new group called itself the "National Defense", הגנה הלאומית. During the riots of 1929 Betar youth participated in the defense of Tel Aviv neighborhoods under the command of Yermiyahu Halperin, at the behest of the Tel Aviv city hall. After the riots the Tel Avivian group expanded, and was known as "The Right Wing Organization".
After the Tel Aviv expansion another branch was established in Haifa. Towards the end of 1932 the Haganah branch of Safed also defected and joined the Irgun, as well as many members of the Maccabi sports association. At that time the movement's underground newsletter, "Ha'Metsudah" (the Fortress) also began publication, expressing the active trend of the movement. The Irgun also increased its numbers by expanding draft regiments of Betar – groups of volunteers, committed to two years of security and pioneer activities. These regiments were based in places that from which stemmed new Irgun strongholds in the many places, including the settlements of Yesod HaMa'ala, Mishmar HaYarden, Rosh Pina, Metula and Nahariya in the north; in the center – Hadera, Binyamina, Herzliya, Netanya and Kfar Saba, and south of there – Rishon LeZion, Rehovot and Ness Ziona. Later on regiments were also active in the Old City of Jerusalem ("the Kotel Brigades") among others. Primary training centers were based in Ramat Gan, Qastina (by Kiryat Mal'akhi of today) and other places.
Under Tehomi's command.
In 1933 there were some signs of unrest, seen by the incitement of the local Arab leadership to act against the authorities. The strong British response put down the disturbances quickly. During that time the Irgun operated in a similar manner to the Haganah and was a guarding organization. The two organizations cooperated in ways such as coordination of posts and even intelligence sharing.
Within the Irgun, Tehomi was the first to serve as "Head of the Headquarters" or "Chief Commander". Alongside Tehomi served the senior commanders, or "Headquarters" of the movement. As the organization grew, it was divided into district commands.
In August 1933 a "Supervisory Committee" for the Irgun was established, which included representatives from most of the Zionist political parties. The members of this committee were Meir Grossman (of the Hebrew State Party), Rabbi Meir Bar-Ilan (of the Mizrachi Party, either Immanuel Neumann or Yehoshua Supersky (of the General Zionists) and Ze'ev Jabotinsky or Eliyahu Ben Horin (of Hatzohar).
In protest against, and with the aim of ending Jewish immigration to Palestine, the Great Arab Revolt of 1936–1939 broke out on April 19, 1936. The riots took the form of attacks by Arab rioters ambushing main roads, bombing of roads and settlements as well as property and agriculture vandalism. In the beginning, the Irgun and the Haganah generally maintained a policy of restraint, apart from a few instances. Some expressed resentment at this policy, leading up internal unrest in the two organizations. The Irgun tended to retaliate more often, and sometimes Irgun members patrolled areas beyond their positions in order to encounter attackers ahead of time. However, there were differences of opinion regarding what to do in the Haganah, as well. Due to the joining of many Betar Youth members, Jabotinsky (founder of Betar) had a great deal of influence over Irgun policy. Nevertheless, Jabotinsky was of the opinion that for moral reasons violent retaliation was not to be undertaken.
In November 1936 the Peel Commission was sent to inquire regarding the breakout of the riots and propose a solution to end the Revolt. In early 1937 there were still some in the Yishuv who felt the commission would recommend a partition of Mandatory Palestine (the land west of the Jordan River), thus creating a Jewish state on part of the land. The Irgun leadership, as well as the "Supervisory Committee" held similar beliefs, as did some members of the Haganah and the Jewish Agency. This belief strengthened the policy of restraint and led to the position that there was no room for defense institutions in the future Jewish state. Tehomi was quoted as saying: "We stand before great events: a Jewish state and a Jewish army. There is a need for a single military force". This position intensified the differences of opinion regarding the policy of restraint, both within the Irgun and within the political camp aligned with the organization. The leadership committee of the Irgun supported a merger with the Haganah. On April 24, 1937 a referendum was held among Irgun members regarding its continued independent existence. David Raziel and Avraham (Yair) Stern came out publicly in support for the continued existence of the Irgun:
The first split.
In April 1937 the Irgun split after the referendum. Approximately 1,500–2,000 people, about half of the Irgun's membership, including the senior command staff, regional committee members, along with most of the Irgun's weapons, returned to the Haganah, which at that time was under the Jewish Agency's leadership. The Supervisory Committee's control over the Irgun ended, and Jabotinsky assumed command. In their opinion, the removal of the Haganah from the Jewish Agency's leadership to the national institutions necessitated their return. Furthermore, they no longer saw significant ideological differences between the movements. Those who remained in the Irgun were primarily young activists, mostly laypeople, who sided with the independent existence of the Irgun. In fact, most of those who remained were originally Betar people. Moshe Rosenberg estimated that approximately 1,800 members remained. In theory, the Irgun remained an organization not aligned with a political party, but in reality the supervisory committee was disbanded and the Irgun's continued ideological path was outlined according to Ze'ev Jabotinsky's school of thought and his decisions, until the movement eventually became Revisionist Zionism's military arm. One of the major changes in policy by Jabotinsky was the end of the policy of restraint.
On April 27, 1937 the Irgun founded a new headquarters, staffed by Moshe Rosenberg at the head, Avraham (Yair) Stern as secretary, David Raziel as head of the Jerusalem branch, Hanoch Kalai as commander of Haifa and Aharon Haichman as commander of Tel Aviv. On 20 Tammuz, (June 29) the day of Theodor Herzl's death, a ceremony was held in honor of the reorganization of the underground movement. For security purposes this ceremony was held at a construction site in Tel Aviv.
Ze'ev Jabotinsky placed Col. Robert Bitker at the head of the Irgun. Bitker had previously served as Betar commissioner in China and had military experience. A few months later, probably due to total incompatibility with the position, Jabotinsky replaced Bitker with Moshe Rosenberg. When the Peel Commission report was published a few months later, the Revisionist camp decided not to accept the commission's recommendations. Moreover, the organizations of Betar, Hatzohar and the Irgun began to increase their efforts to bring Jews to the land of Israel, illegally. This Aliyah was known as the עליית אף על פי "Af Al Pi (Nevertheless) Aliyah". As opposed to this position, the Jewish Agency began acting on behalf of the Zionist interest on the political front, and continued the policy of restraint. From this point onwards the differences between the Haganah and the Irgun were much more obvious.
Illegal immigration.
According to Jabotinsky's "Evacuation Plan", which called for millions of European Jews to be brought to Palestine at once, the Irgun helped the illegal immigration of European Jews to the land of Israel. This was named by Jabotinsky the "National Sport". The most significant part of this immigration prior to World War II was carried out by the Revisionist camp, largely because the Yishuv institutions and the Jewish Agency shied away from such actions on grounds of cost and their belief that Britain would in the future allow widespread Jewish immigration.
The Irgun joined forces with Hatzohar and Betar in September 1937, when it assisted with the landing of a convoy of 54 Betar members at Tantura Beach (near Haifa.) The Irgun was responsible for discreetly bringing the Olim, or Jewish immigrants, to the beaches, and dispersing them among the various Jewish settlements. The Irgun also began participating in the organisation of the immigration enterprise and undertook the process of accompanying the ships. This began with the ship "Draga" which arrived at the coast of British Palestine in September 1938. In August of the same year, an agreement was made between Ari Jabotinsky (the son of Ze'ev Jabotinsky), the Betar representative and Hillel Kook, the Irgun representative, to coordinate the immigration (also known as Ha'apala). This agreement was also made in the "Paris Convention" in February 1939, at which Ze'ev Jabotinsky and David Raziel were present. Afterwards, the "Aliyah Center" was founded, made up of representatives of Hatzohar, Betar, and the Irgun, thereby making the Irgun a full participant in the process.
The difficult conditions on the ships demanded a high level of discipline. The people on board the ships were often split into units, led by commanders. In addition to having a daily roll call and the distribution of food and water (usually very little of either), organized talks were held to provide information regarding the actual arrival in Palestine. One of the largest ships was the "Sakaria", with 2,300 passengers, which equalled about 0.5% of the Jewish population in Palestine. The first vessel arrived on April 13, 1937, and the last on February 13, 1940. All told, about 18,000 Jews immigrated to Palestine with the help of the Revisionist organizations and private initiatives by other Revisionists. Most were not caught by the British.
End of restraint.
Irgun members continued to defend settlements, but at the same time began attacks on Arab villages, thus ending the policy of restraint. These attacks were intended to instill fear in the Arab side, in order to cause the Arabs to wish for peace and quiet. In March 1938, David Raziel wrote in the underground newspaper "By the Sword" a constitutive article for the Irgun overall, in which he coined the term "Active Defense":
The first attacks began around April 1936, and by the end of World War II, more than 250 Arabs had been killed. Examples include:
During 1936, Irgun members carried out approximately ten attacks.
Throughout 1937 the Irgun continued this line of operation.
A more complete list can be found here.
At that time, however, these acts were not yet a part of a formulated policy of the Irgun. Not all of the aforementioned operations received a commander's approval, and Jabotinsky was not in favor of such actions at the time. Jabotinsky still hoped to establish a Jewish force out in the open that would not have to operate underground. However, the failure, in its eyes, of the Peel Commission and the renewal of violence on the part of the Arabs caused the Irgun to rethink its official policy.
Increase in operations.
14 November 1937 was a watershed in Irgun activity. From that date, the Irgun increased its reprisals. Following an increase in the number of attacks aimed at Jews, including the killing of five kibbutz members near Kiryat Anavim (today kibbutz Ma'ale HaHamisha), the Irgun undertook a series of attacks in various places in Jerusalem, killing five Arabs. Operations were also undertaken in Haifa (shooting at the Arab-populated Wadi Nisnas neighborhood) and in Herzliya. The date is known as the day the policy of restraint (Havlagah) ended, or as "Black Sunday". This is when the organization fully changed its policy, with the approval of Jabotinsky and Headquarters to the policy of "active defense" in respect of Irgun actions.
The British responded with the arrest of Betar and Hatzohar members as suspected members of the Irgun. Military courts were allowed to act under "Time of Emergency Regulations" and even sentence people to death. In this manner Yehezkel Altman, a guard in a Betar battalion in the Nahalat Yizchak neighborhood of Tel Aviv, shot at an Arab bus, without his commanders' knowledge. Altman was acting in response to a shooting at Jewish vehicles on the Tel Aviv–Jerusalem road the day before. He turned himself in later and was sentenced to death, a sentence which was later commuted to a life sentence.
Despite the arrests, Irgun members continued fighting. Jabotinsky lent his moral support to these activities. In a letter to Moshe Rosenberg on 18 March 1938 he wrote:
Although the Irgun continued activities such as these, following Rosenberg's orders, they were greatly curtailed. Furthermore, in fear of the British threat of the death sentence for anyone found carrying a weapon, all operations were suspended for eight months. However, opposition to this policy gradually increased. In April, 1938, responding to the killing of six Jews, Betar members from the Rosh Pina Brigade went on a reprisal mission, without the consent of their commander, as described by historian Avi Shlaim:
Although the incident ended without casualties, the three were caught, and one of them – Shlomo Ben-Yosef was sentenced to death. Demonstrations around the country, as well as pressure from institutions and people such as Dr. Chaim Weizmann and the Chief Rabbi of Mandatory Palestine, Yitzhak HaLevi Herzog did not reduce his sentence. In Shlomo Ben-Yosef's writings in Hebrew were later found:
On 29 June 1938 he was executed, and was the first of the Olei Hagardom. The Irgun revered him after his death and many regarded him as an example.
In light of this, and due to the anger of the Irgun leadership over the decision to adopt a policy of restraint until that point, Jabotinsky relieved Rosenberg of his post and replaced him with David Raziel, who proved to be the most prominent Irgun commander until Menachem Begin. Jabotinsky simultaneously instructed the Irgun to end its policy of restraint, leading to armed offensive operations until the end of the Arab Revolt in 1939. In this time, the Irgun mounted about 40 operations against Arabs and Arab villages, for instance:
This action led the British Parliament to discuss the disturbances in Palestine. On 23 February 1939 the Secretary of State for the Colonies, Malcolm MacDonald revealed the British intention to cancel the mandate and establish a state that would preserve Arab rights. This caused a wave of riots and attacks by Arabs against Jews. The Irgun responded four days later with a series of attacks on Arab buses and other sites. The British used military force against the Arab rioters and in the latter stages of the revolt by the Arab community in Palestine, it deteriorated into a series of internal gang wars.
During the same period.
At the same time, the Irgun also established itself in Europe. The Irgun built underground cells that participated in organizing migration to Palestine. The cells were made up almost entirely of Betar members, and their primary activity was military training in preparation for emigration to Palestine. Ties formed with the Polish authorities brought about courses in which Irgun commanders were trained by Polish officers in advanced military issues such as guerrilla warfare, tactics and laying land mines. Avraham (Yair) Stern was notable among the cell organizers in Europe. In 1937 the Polish authorities began to deliver large amounts of weapons to the underground. The transfer of handguns, rifles, explosives and ammunition stopped with the outbreak of World War II. Another field in which the Irgun operated was the training of pilots, so they could serve in the Air Force in the future war for independence, in the flight school in Lod.
Towards the end of 1938 there was progress towards aligning the ideologies of the Irgun and the Haganah. Many abandoned the belief that the land would be divided and a Jewish state would soon exist. The Haganah founded פו"מ, a special operations unit, (pronounced "poom"), which carried out reprisal attacks following Arab violence. These operations continued into 1939. Furthermore, the opposition within the Yishuv to illegal immigration significantly decreased, and the Haganah began to bring Jews to Palestine using rented ships, as the Irgun had in the past.
First operations against the British.
The publishing of the MacDonald White Paper of 1939 brought with it new edicts that were intended to lead to a more equitable settlement between Jews and Arabs. However, it was considered by some Jews to have an adverse effect on the continued development of the Jewish community in Palestine. Chief among these was the prohibition on selling land to Jews, and the smaller quotas for Jewish immigration. The entire Yishuv was furious at the contents of the White Paper. There were demonstrations against the "Treacherous Paper", as it was considered that it would preclude the establishment of a Jewish homeland in Palestine.
Under the temporary command of Hanoch Kalai, the Irgun began sabotaging strategic infrastructure such as electricity facilities, radio and telephone lines. It also started publicizing its activity and its goals. This was done in street announcements, newspapers, as well as the underground radio station Kol Zion HaLochemet. On August 26, 1939, the Irgun killed Ralph Cairns, a British police officer who, as head of the Jewish Department in the Palestine Police, had tortured a number of youths who were underground members. Cairns and Ronald Barker, another British police officer, were killed by an Irgun IED.
The British increased their efforts against the Irgun. As a result, on August 31 the British police arrested members meeting in the Irgun headquarters. On the next day, September 1, 1939, World War II broke out.
During World War II.
Following the outbreak of war, Ze'ev Jabotinsky and the New Zionist Organization voiced their support for Britain and France. In mid-September 1939 Raziel was moved from his place of detention in Tzrifin. This, among other events, encouraged the Irgun to announce a cessation of its activities against the British so as not to hinder Britain's effort to fight "the Hebrew's greatest enemy in the world – German Nazism". This announcement ended with the hope that after the war a Hebrew state would be founded "within the historical borders of the liberated homeland". After this announcement Irgun, Betar and Hatzohar members, including Raziel and the Irgun leadership, were gradually released from detention. The Irgun did not rule out joining the British army and the Jewish Brigade. Irgun members did enlist in various British units. Irgun members also assisted British forces with intelligence in Romania, Bulgaria, Morocco and Tunisia. An Irgun unit also operated in Syria and Lebanon. David Raziel later died during one of these operations.
During the Holocaust, Betar members revolted numerous times against the Nazis in occupied Europe. The largest of these revolts was the Warsaw Ghetto Uprising, in which an armed underground organization fought, formed by Betar and Hatzoar and known as the "Żydowski Związek Wojskowy (ŻZW)" (Jewish Military Union). Despite its political origins, the ŻZW accepted members without regard to political affiliation, and had contacts established before the war with elements of the Polish military. Because of differences over objectives and strategy, the ŻZW was unable to form a common front with the mainstream ghetto fighters of the Żydowska Organizacja Bojowa, and fought independently under the military leadership of Paweł Frenkiel and the political leadership of Dawid Wdowiński.
There were instances of Betar members enlisted in the British military smuggling British weapons to the Irgun. [ref?]
From 1939 onwards, an Irgun delegation in the United States worked for the creation of a Jewish army made up of Jewish refugees and Jews from Palestine, to fight alongside the Allied Forces. In July 1943 the "Emergency Committee to Save the Jewish People in Europe" was formed, and worked until the end of the war to rescue the Jews of Europe from the Nazis and to garner public support for a Jewish state. However, it was not until January 1944 that US President Franklin Roosevelt established the War Refugee Board, which achieved some success in saving European Jews.
Second split.
Throughout this entire period, the British continued enforcing the White Paper's provisions, which included a ban on the sale of land, restrictions on Jewish immigration and increased vigilance against illegal immigration. Part of the reason why the British banned land sales (to anyone) was the confused state of the post Ottoman land registry; it was difficult to determine who actually owned the land that was for sale.
Within the ranks of the Irgun this created much disappointment and unrest, at the center of which was disagreement with the leadership of the New Zionist Organization, David Raziel and the Irgun Headquarters. On June 18, 1939, Avraham (Yair) Stern and others of the leadership were released from prison and a rift opened between them the Irgun and Hatzohar leadership. The controversy centred on the issues of the underground movement submitting to public political leadership and fighting the British. On his release from prison Raziel resigned from Headquarters. To his chagrin, independent operations of senior members of the Irgun were carried out and some commanders even doubted Raziel's loyalty.
In his place, Stern was elected to the leadership. In the past, Stern had founded secret Irgun cells in Poland without Jabotinsky's knowledge, in opposition to his wishes. Furthermore, Stern was in favor of removing the Irgun from the authority of the New Zionist Organization, whose leadership urged Raziel to return to the command of the Irgun. He finally consented. Jabotinsky wrote to Raziel and to Stern, and these letters were distributed to the branches of the Irgun:
Stern was sent a telegram with an order to obey Raziel, who was reappointed. However, these events did not prevent the splitting of the organization. Suspicion and distrust were rampant among the members. Out of the Irgun a new organization was created on July 17, 1940, which was first named "The National Military Organization in Israel" (as opposed to the "National Military Organization in the Land of Israel") and later on changed its name to Lehi, an acronym for Lohamei Herut Israel, "Fighters for the Freedom of Israel", (לח"י – לוחמי חירות ישראל). Jabotinsky died in New York on August 4, 1940, yet this did not prevent the Lehi split. Following Jabotinsky's death, ties were formed between the Irgun and the New Zionist Organization. These ties would last until 1944, when the Irgun declared a revolt against the British.
The primary difference between the Irgun and the newly formed organization was its intention to fight the British in Palestine, regardless of their war against Germany. Later, additional operational and ideological differences developed that contradicted some of the Irgun's guiding principles. For example, the Lehi, unlike the Irgun, supported a population exchange with local Arabs.
Change of policy.
The split damaged the Irgun both organizationally and from a morale point of view. As their spiritual leader, Jabotinsky's death also added to this feeling. Together, these factors brought about a mass abandonment by members. The British took advantage of this weakness to gather intelligence and arrest Irgun activists. The new Irgun leadership, which included Meridor, Yerachmiel Ha'Levi, Moshe Segal and others used the forced hiatus in activity to rebuild the injured organization. This period was also marked by more cooperation between the Irgun and the Jewish Agency, however David Ben-Gurion's uncompromising demand that Irgun accept the Agency's command foiled any further cooperation.
In both the Irgun and the Haganah more voices were being heard opposing any cooperation with the British. Nevertheless, an Irgun operation carried out in the service of Britain was aimed at sabotaging pro-Nazi forces in Iraq, including the assassination of Haj Amin al-Husayni. Among others, Raziel and Yaakov Meridor participated. On April 20, 1941, during a Luftwaffe air raid on RAF Hannaniya near Baghdad, David Raziel, commander of the Irgun, was killed during the operation.
In late 1943 a joint Haganah – Irgun initiative was developed, to form a single fighting body, unaligned with any political party, by the name of עם לוחם ("Fighting Nation"). The new body's first plan was to kidnap the British High Commissioner of Palestine, Sir Harold MacMichael and take him to Cyprus. However, the Haganah leaked the planned operation and it was thwarted before it got off the ground. Nevertheless, at this stage the Irgun ceased its cooperation with the British. As Eliyahu Lankin tells in his book:
The "Revolt".
In 1943 the Polish II Corps, commanded by Władysław Anders, arrived in Palestine from Iraq. The British insisted that no Jewish units of the army be created. Eventually, many of the soldiers of Jewish origin that arrived with the army were released and allowed to stay in Palestine. One of them was Menachem Begin, whose arrival in Palestine created new-found expectations within the Irgun and Betar. Begin had served as head of the Betar movement in Poland, and was a respected leader. Yaakov Meridor, then the commander of the Irgun, raised the idea of appointing Begin to the post. In late 1943, when Begin accepted the position, a new leadership was formed. Meridor became Begin's deputy, and other members of the board were Aryeh Ben Eliezer, Eliyahu Lankin, and Shlomo Lev Ami.
On February 1, 1944 the Irgun put up posters all around the country, proclaiming a revolt against the British mandatory government. The posters began by saying that all of the Zionist movements stood by the Allied Forces and over 25,000 Jews had enlisted in the British military. The hope to establish a Jewish army had died. European Jewry was trapped and was being destroyed, yet Britain, for its part, did not allow any rescue missions. This part of the document ends with the following words:
The Irgun then declared that, for its part, the ceasefire was over and they were now at war with the British. It demanded the transfer of rule to a Jewish government, to implement ten policies. Among these were the mass evacuation of Jews from Europe, the signing of treaties with any state that recognized the Jewish state's sovereignty, including Britain, granting social justice to the state's residents, and full equality to the Arab population. The proclamation ended with:
The Irgun began this campaign rather weakly. At the time of the start of the revolt, it was only about 1,000 strong, including some 200 fighters. It possessed about 4 submachine guns, 40 rifles, 60 pistols, 150 hand grenades, and 2,000 kilograms of explosive material, and it's funds were about £800.
Struggle against the British.
The Irgun began a militant operation against the symbols of government, in an attempt to harm the regime's operation as well as its reputation. The first attack was on February 12, 1944 at the government immigration offices, a symbol of the immigration laws. The attacks went smoothly and ended with no casualties—as they took place on a Saturday night, when the buildings were empty—in the three largest cities: Jerusalem, Tel Aviv, and Haifa. On February 27 the income tax offices were bombed. Parts of the same cities were blown up, also on a Saturday night; prior warnings were put up near the buildings. On March 23 the national headquarters building of the British police in the Russian Compound in Jerusalem was attacked, and part of it was blown up. These attacks in the first few months were sharply condemned by the organized leadership of the Yishuv and by the Jewish Agency, who saw them as dangerous provocations.
At the same time the Lehi also renewed its attacks against the British. The Irgun continued to attack police stations and headquarters, and Tegart Fort, a fortified police station (today the location of Latrun). One relatively complex operation was the takeover of the radio station in Ramallah, on May 17, 1944.
One symbolic act by the Irgun happened before Yom Kippur of 1944. They plastered notices around town, warning that no British officers should come to the Western Wall on Yom Kippur, and for the first time since the mandate began no British police officers were there to prevent the Jews from the traditional Shofar blowing at the end of the fast. After the fast that year the Irgun attacked four police stations in Arab settlements. In order to obtain weapons, the Irgun carried out "confiscation" operations – they robbed British armouries and smuggled stolen weapons to their own hiding places. During this phase of activity the Irgun also cut all of its official ties with the New Zionist Organization, so as not to tie their fate in the underground organization.
Begin wrote in his memoirs, "The Revolt":
Underground exiles.
In October 1944 the British began expelling hundreds of arrested Irgun and Lehi members to detention camps in Africa. 251 detainees from Latrun were flown on thirteen planes, on October 19 to a camp in Asmara, Eritrea. Eleven additional transports were made. Throughout the period of their detention, the detainees often initiated rebellions and hunger strikes. Many escape attempts were made until July 1948 when the exiles were returned to Israel. While there were numerous successful escapes from the camp itself, only nine men actually made it back all the way. One noted success was that of Yaakov Meridor, who escaped nine times before finally reaching Europe in April 1948. These tribulations were the subject of his book "Long is the Path to Freedom: Chronicles of one of the Exiles".
Hunting Season.
On November 6, 1944, Lord Moyne, British Deputy Resident Minister of State in Cairo was assassinated by Lehi members Eliyahu Hakim and Eliyahu Bet-Zuri. This act raised concerns within the Yishuv from the British regime's reaction to the underground's violent acts against them. Therefore, the Jewish Agency decided on starting a "Hunting Season", known as the "saison", (from the French "la saison de chasse").
The Irgun's recuperation was noticeable when it began to renew its cooperation with the Lehi in May 1945, when it sabotaged oil pipelines, telephone lines and railroad bridges. All in all, over 1,000 members of the Irgun and Lehi were arrested and interred in British camps during the "Saison". Eventually the Hunting Season died out, and there was even talk of cooperation with the Haganah leading to the formation of the Jewish Resistance Movement.
The Jewish Resistance Movement.
Towards the end of July 1945 the Labour party in Britain was elected to power. The Yishuv leadership had high hopes that this would change the anti-Zionist policy that the British maintained at the time. However, these hopes were quickly dashed when the government limited Jewish immigration, with the intention that the population of Mandatory Palestine (the land west of the Jordan River) would not be more than one third of the total. This, along with the stepping up of arrests and their pursuit of underground members and illegal immigration organizers led to the formation of the Jewish Resistance Movement. This body consolidated the armed resistance to the British of the Irgun, Lehi, and Haganah. For ten months the Irgun and the Lehi cooperated and they carried out nineteen attacks and defense operations. The Haganah and Palmach carried out ten such operations. The Haganah also assisted in landing 13,000 illegal immigrants.
Tension between the underground movements and the British increased with the increase in operations. On April 23, 1945 an operation undertaken by the Irgun to gain weapons from the Tegart fort at Ramat Gan resulted in a firefight. One Irgun member was killed and his body was later hanged on the fort's fence. Another fighter, Yizchak Bilu, was killed as well in a diversionary ploy – an explosive device fell out of his hand, and he leapt onto it in order to save his comrades, who were also carrying explosives. A third fighter, Dov Gruner, was caught. He stood trial and was sentenced to be death by hanging, refusing to sign a pardon request.
In 1946, British relations with the Yishuv worsened, building up to Operation Agatha of June 29. The authorities ignored the Anglo-American Committee of Inquiry's recommendation to allow 100,000 Jews into Palestine at once. As a result of the discovery of documents tying the Jewish Agency to the Jewish Resistance Movement, the Irgun was asked to speed up the plans for the King David Hotel bombing of July 22. The hotel was where the documents were located, the base for the British Secretariat, the military command and a branch of the Criminal Investigation Division of the police. The Irgun later claimed to have sent a warning that was ignored. 91 people were killed in the attack where a 350 kg bomb was placed in the basement of the hotel and caused a large section of it to collapse. Only 13 were British soldiers.
Further struggle against the British.
The King David Hotel bombing and the arrest of Jewish Agency and other Yishuv leaders as part of Operation Agatha caused the Haganah to cease their armed activity against the British. Yishuv and Jewish Agency leaders were released from prison. From then until the end of the British mandate, resistance activities were led by the Irgun and Lehi. In early September 1946 the Irgun renewed its attacks against civil structures, railroads, communication lines and bridges. One operation was the attack on the train station in Jerusalem, in which Meir Feinstein was arrested and later committed suicide awaiting execution. According to the Irgun these sort of armed attacks were legitimate, since the trains primarily served the British, for redeployment of their forces. The Irgun also publicized leaflets, in three languages, not to use specific trains in danger of being attacked. For a while the British stopped train traffic at night. The Irgun also carried out repeated attacks against military and police traffic using disguised, electronically-detonated roadside mines which could be detonated by an operator hiding nearby as a vehicle passed, carried out arms raids against military bases and police stations (often disguised as British soldiers), launched bombing, shooting, and mortar attacks against military and police installations and checkpoints, and robbed banks to gain funds as a result of losing access to Haganah funding following the collapse of the Jewish Resistance Movement.
On October 31, 1946, in response to the British barring entry of Jews from Palestine, the Irgun blew up the British embassy in Rome, a center of British efforts to monitor and stop Jewish immigration. The Irgun also carried out a few other operations in Europe: a British troop train was derailed and an attempt against another troop train failed. An attack on a British officers club in Vienna took place in 1947, and an attack on another British officer's club in Vienna and a sergeant's club in Germany took place in 1948.
In December 1946 a sentence of 18 years and 18 beatings was handed down to a young Irgun member. The Irgun made good on a threat they made and after the detainee was whipped, Irgun members kidnapped British officers and beat them in public. The operation, known as the "Night of the Beatings" brought an end to British punitive beatings. The British, taking these acts seriously, moved many British families in Palestine into the confines of military bases, and some moved home.
On February 14, 1947, Ernest Bevin announced that the Jews and Arabs would not be able to agree on any British proposed solution for the land, and therefore the issue must be brought to the United Nations (UN) for a final decision. The Yishuv thought of the idea to transfer the issue to the UN as a British attempt to achieve delay while a UN inquiry commission would be established, and its ideas discussed, and all the while the Yishuv would weaken. Foundation for Immigration B increased the number of ships bringing in Jewish refugees. The British still strictly enforced the policy of limited Jewish immigration and illegal immigrants were placed in detention camps in Cyprus, which increased the anger of the Jewish community towards the mandate government.
The Irgun stepped up its activity and from February 19 until March 3 it attacked 18 British military camps, convoy routes, vehicles, and other facilities. The most notable of these attacks was the bombing of a British officer's club located in Goldschmidt House in Jerusalem, which was in a heavily guarded security zone. Covered by machine-gun fire, an Irgun assault team in a truck penetrated the security zone and lobbed explosives into the building. Thirteen people, including two officers, were killed. As a result, martial law was imposed over much of the country, enforced by approximately 20,000 British soldiers. Despite this, attacks continued throughout the martial law period. The most notable one was an Irgun attack against the Royal Army Pay Corps base at the Schneller Orphanage, in which a British soldier was killed.
Throughout its struggle against the British, the Irgun sought to publicize its cause around the world. By humiliating the British, it attempted to focus global attention on Palestine, hoping that any British overreaction would be widely reported, and thus result in more political pressure against the British. Begin described this strategy as turning Palestine into a "glass house". The Irgun also re-established many representative offices internationally, and by 1948 operated in 23 states. In these countries the Irgun sometimes acted against the local British representatives or led public relations campaigns against Britain. According to Bruce Hoffman: ""In an era long before the advent of 24/7 global news coverage and instantaneous satellite-transmitted broadcasts, the Irgun deliberately attempted to appeal to a worldwide audience far beyond the immediate confines of its local struggle, and beyond even the ruling regime's own homeland"."
The Acre Prison break.
On April 16, 1947, Dov Gruner, Yehiel Dresner, Eliezer Kashani, and Mordechai El'kachi were hanged, while singing Hatikvah. On April 21 Meir Feinstein and Lehi member Moshe Barazani blew themselves up, using an improvised explosive device (IED), hours before their scheduled hanging. And on May 4 one of the Irgun's largest operations took place – the raid of the prison in the citadel in Acre. The operation was carried out by 23 men, commanded by Dov Cohen – AKA "Shimshon", along with the help of the Irgun and Lehi prisoners inside the prison. The raid allowed 41 underground members to escape, although some were caught outside of the prison, and some were killed in the escape. Along with the underground movement members, other criminals – including 214 Arabs – also escaped. Five of the attackers were caught and three of them – Avshalom Haviv, Meir Nakar, and Yaakov Weiss, were sentenced to death.
The Sergeants affair.
After the death sentences of the three were confirmed, the Irgun tried to save them by kidnapping hostages — British sergeants Clifford Martin and Mervyn Paice — in the streets of Netanya. British forces closed off and combed the area in search of the two, but did not find them. On July 29, 1947, in the afternoon, Meir Nakar, Avshalom Haviv, and Yaakov Weiss were executed. Approximately thirteen hours later the hostages were hanged in retaliation by the Irgun and their bodies, booby-trapped with an explosive, afterwards strung up from trees in woodlands south of Netanya. This action caused an outcry in Britain and was condemned both there and by Jewish leaders in Palestine.
This episode has been given as a major influence on the British decision to terminate the Mandate and leave Palestine. The United Nations Special Committee on Palestine (UNSCOP) was also influenced by this and other actions. At the same time another incident was developing – the events of the ship "Exodus 1947". The 4,500 Holocaust survivors on board were not allowed to enter Palestine. UNSCOP also covered the events. Some of its members were even present at Haifa port when the putative immigrants were forcefully removed from their ship (later found to have been rigged with an IED by some of its passengers) onto the deportation ships, and later commented that this strong image helped them press for an immediate solution for Jewish immigration and the question of Palestine.
Two weeks later, the House of Commons convened for a special debate on events in Palestine, and concluded that their soldiers should be withdrawn as soon as possible.
The 1948 Palestine War.
UNSCOP's conclusion was a unanimous decision to end the British mandate and majority opinion to divide the Mandatory Palestine (the land west of the Jordan River) between a Jewish state and an Arab state. During the UN's deliberations regarding the committee's recommendations the Irgun avoided initiating any attacks, so as not to influence the UN negatively on the idea of a Jewish state. On November 29 the UN General Assembly voted in favor of ending the mandate and establishing two states on the land. That very same day the Irgun and the Lehi renewed their attacks on British targets. The next day the local Arabs began attacking the Jewish community, thus beginning the first stage of the 1948 Palestine War. The first attacks on Jews were in Jewish neighborhoods of Jerusalem, in and around Jaffa, Bat Yam, Holon, and the Ha'Tikvah neighborhood in Tel Aviv.
In the autumn of 1947 the Irgun membership was approximately 4,000 people. The goal of the organization at that point was the conquest of the land between the Jordan River and the Mediterranean Sea for the sake of the future Jewish state and preventing the Arab Legion from driving out the Jewish community. The Irgun became almost an overt organization, establishing military bases in Ramat Gan and Petah Tikva. It began recruiting openly, thus significantly increasing in size. During the war the Irgun fought alongside the Lehi and the Haganah in the front against the Arab attacks. At first the Haganah maintained a defensive policy, as it had until then, but after the Convoy of 35 incident it completely abandoned its policy of restraint: "Distinguishing between individuals is no longer possible, for now – it is a war, and the even the innocent shall not be absolved."
The Irgun also began carrying out reprisal missions, as it had under David Raziel's command. At the same time though, it published announcements calling on the Arabs to lay down their weapons and maintain a ceasefire:
However the mutual attacks continued. The Irgun attacked the Arab villages of Tira near Haifa, Yehudiya ('Abassiya) in the center, and Shuafat by Jerusalem. The Irgun also attacked in the Wadi Rushmiya neighborhood in Haifa and Abu Kabir in Jaffa. On December 29 Irgun units arrived by boat to the Jaffa shore and a gunfight between them and Arab gangs ensued. The following day a bomb was thrown from a speeding Irgun car at a group of Arab men waiting to be hired for the day at the Haifa oil refinery, resulting in seven Arabs killed, and dozens injured. In response, some Arab workers attacked Jews in the area, killing 41. This sparked a Haganah response in Balad al-Sheykh, which resulted in the deaths of 60 civilians. The Irgun's goal in the fighting was to move the battles from Jewish populated areas to Arab populated areas. On January 1, 1948 the Irgun attacked again in Jaffa, its men entering the city dressed as British troops; later in the month it attacked in Beit Nabala, a base for many Arab fighters. On 5 January 1948 the Irgun detonated a lorry bomb outside Jaffa's Ottoman built Town Hall, killing 14 and injuring 19. In Jerusalem, two days later, Irgun members in a stolen police van rolled a barrel bomb into a large group of civilians who were waiting for a bus by the Jaffa Gate, killing around sixteen. In the pursuit that followed three of the attackers were killed and two taken prisoner.
On 6 April 1948, the Irgun raided the British Army camp at Pardes Hanna killing six British soldiers and their commanding officer.
The Deir Yassin massacre was carried out in a village west of Jerusalem that had signed a non-belligerency pact with its Jewish neighbors and the Haganah, and repeatedly had barred entry to foreign irregulars. On 9 April approximately 120 Irgun and Lehi members began an operation to capture the village. During the operation, the villagers fiercely resisted the attack, and a battle broke out. In the end, the Irgun and Lehi forces advanced gradually through house-to-house fighting. The village was only taken after the Irgun began systematically dynamiting houses, and after a Palmach unit intervened and employed mortar fire to silence the villagers' sniper positions. The operation resulted in five Jewish fighters dead and 40 injured. Some 100 to 120 villagers were also killed.
There are allegations that Irgun and Lehi forces committed war crimes during and after the capture of the village. These allegations include reports that fleeing individuals and families were fired at, and prisoners of war were killed after their capture. A Haganah report writes:
Some say that this incident was an event that accelerated the Arab exodus from Palestine.
The Irgun cooperated with the Haganah in the conquest of Haifa. At the regional commander's request, on April 21 the Irgun took over an Arab post above Hadar Ha'Carmel as well as the Arab neighborhood of Wadi Nisnas, adjacent to the Lower City.
The Irgun acted independently in the conquest of Jaffa (part of the proposed Arab State according to the UN Partition Plan). On April 25 Irgun units, about 600 strong, left the Irgun base in Ramat Gan towards Arab Jaffa. Difficult battles ensued, and the Irgun faced resistance from the Arabs as well as the British. Under the command of Amichai "Gidi" Paglin, the Irgun's chief operations officer, the Irgun captured the neighborhood of Manshiya, which threatened the city of Tel Aviv. Afterwards the force continued to the sea, towards the area of the port, and using mortars, shelled the southern neighborhoods.
Integration with the IDF and the Altalena Affair.
On May 14, 1948 the establishment of the State of Israel was proclaimed. The declaration of independence was followed by the establishment of the Israel Defense Forces (IDF), and the process of absorbing all military organizations into the IDF started. On June 1, an agreement had been signed Between Menachem Begin and Yisrael Galili for the absorption of the Irgun into the IDF. One of the clauses stated that the Irgun had to stop smuggling arms. Meanwhile, in France, Irgun representatives purchased a ship, renamed "Altalena" (a pseudonym of Ze'ev Jabotinsky), and weapons. The ship sailed on June 11 and arrived at the Israeli coast on June 20 in violation of the four-week ceasefire agreement in the ongoing war with the neighbouring Arab states and the United Nations Security Council Resolution 50.
When the ship arrived the Israeli government, headed by Ben-Gurion, was adamant in its demand that the Irgun surrender and hand over all of the weapons. Ben-Gurion said: "We must decide whether to hand over power to Begin or to order him to cease his activities. If he does not do so, we will open fire! Otherwise, we must decide to disperse our own army."
There were two confrontations between the newly formed IDF and the Irgun: when "Altalena" reached Kfar Vitkin in the late afternoon of Sunday, June 20 many Irgun militants, including Begin, waited on the shore. A clash with the Alexandroni Brigade, commanded by Dan Even (Epstein), occurred. Fighting ensued and there were a number of casualties on both sides. The clash ended in a ceasefire and the transfer of the weapons on shore to the local IDF commander, and with the ship, now reinforced with local Irgun members, including Begin, sailing to Tel Aviv, where the Irgun had more supporters.
Many Irgun members, who joined the IDF earlier that month, left their bases and concentrated on the Tel Aviv beach. A confrontation between them and the IDF units started. In response, Ben-Gurion ordered Yigael Yadin (acting Chief of Staff) to concentrate large forces on the Tel Aviv beach and to take the ship by force. Heavy guns were transferred to the area and at four in the afternoon, Ben-Gurion ordered the shelling of the "Altalena". One of the shells hit the ship, which began to burn.
Sixteen Irgun fighters were killed in the confrontation with the army; six were killed in the Kfar Vitkin area and ten on Tel Aviv beach. Three IDF soldiers were killed: two at Kfar Vitkin and one in Tel Aviv.
After the shelling of the "Altalena", more than 200 Irgun fighters were arrested. Most of them were freed several weeks later. The Irgun militants were then fully integrated with the IDF and not kept in separate units.
The initial agreement for the integration of the Irgun into the IDF did not include Jerusalem, where a small remnant of the Irgun called the "Jerusalem Battalion", numbering around 400 fighters, and Lehi, continued to operate independently of the government. Following the assassination of UN Envoy for Peace Folke Bernadotte by Lehi in September 1948, the Israeli government determined to immediately dismantle the underground organizations. An ultimatum was issued to the Irgun to liquidate itself and integrate into the IDF or be destroyed. The Irgun accepted the ultimatum, and shortly afterward, it's fighters began enlisting in the IDF and turning over their arms.
Criticism.
References to the Irgun as a terrorist organization came from sources including the Anglo-American Committee of Inquiry, newspapers and a number of prominent world and Jewish figures.
Leaders within the mainstream Jewish organizations, the Jewish Agency, Haganah and Histadrut, as well as the British authorities, routinely condemned Irgun operations as terrorism and branded it an illegal organization as a result of the group's attacks on civilian targets. However, privately at least the Haganah kept a dialogue with the dissident groups.
Ironically, in early 1947, "the British army in Mandate Palestine banned the use of the term 'terrorist' to refer to the Irgun zvai Leumi ... because it implied that British forces had reason to be terrified."
Irgun attacks prompted a formal declaration from the World Zionist Congress in 1946, which strongly condemned "the shedding of innocent blood as a means of political warfare."
The Israeli government, in September 1948, acting in response to the assassination of Count Folke Bernadotte, outlawed the Irgun and Lehi groups, declaring them terrorist organizations under the Prevention of Terrorism Ordinance.
In 1948, "The New York Times" published a letter signed by a number of prominent Jewish figures including Hannah Arendt, Albert Einstein, Sidney Hook, and Rabbi Jessurun Cardozo, which described Irgun as "a terrorist, right-wing, chauvinist organization in Palestine". The letter went on to state that Irgun and the Stern gang "inaugurated a reign of terror in the Palestine Jewish community. Teachers were beaten up for speaking against them, adults were shot for not letting their children join them. By gangster methods, beatings, window-smashing, and widespread robberies, the terrorists intimidated the population and exacted a heavy tribute."
Soon after World War II, Winston Churchill said "we should never have stopped immigration before the war", but that the Irgun were "the vilest gangsters" and that he would "never forgive the Irgun terrorists."
A US military intelligence report, dated January 1948, described Irgun recruiting tactics amongst Displaced Persons (DP) in the camps across Germany:
Clare Hollingworth, the "Daily Telegraph" and "The Scotsman" correspondent in Jerusalem during 1948 wrote several outspoken reports after spending several weeks in West Jerusalem:
In 2006, Simon McDonald, the British ambassador in Tel Aviv, and John Jenkins, the Consul-General in Jerusalem, wrote in response to a pro-Irgun commemoration of the King David Hotel bombing: "We do not think that it is right for an act of terrorism, which led to the loss of many lives, to be commemorated." They also called for the removal of plaques at the site which presented as a fact that the deaths were due to the British ignoring warning calls. The plaques, in their original version, read:
Warning phone calls had been made urging the hotel's occupants to leave immediately. For reasons known only to the British the hotel was not evacuated and after 25 minutes the bombs exploded, and to the Irgun's regret and dismay 91 persons were killed.
McDonald and Jenkins said that no such warning calls were made, adding that even if they had, "this does not absolve those who planted the bomb from responsibility for the deaths."
"Ha'aretz" columnist and Israeli historian Tom Segev wrote of the Irgun: "In the second half of 1940, a few members of the Irgun Zvai Leumi (National Military Organization) – the anti-British terrorist group sponsored by the Revisionists and known by its acronym Etzel, and to the British simply as the Irgun – made contact with representatives of Fascist Italy, offering to cooperate against the British."
Alan Dershowitz wrote in his book "The Case for Israel" that unlike the Haganah, the policy of the Irgun had been to encourage the flight of local Arabs.

</doc>
<doc id="15408" url="https://en.wikipedia.org/wiki?curid=15408" title="Isoroku Yamamoto">
Isoroku Yamamoto

Yamamoto held several important posts in the Imperial Japanese Navy, and undertook many of its changes and reorganizations, especially its development of naval aviation. He was the commander-in-chief during the decisive early years of the Pacific War and so was responsible for major battles such as Pearl Harbor and Midway. He died when American codebreakers identified his flight plans and his plane was shot down. His death was a major blow to Japanese military morale during World War II.
Family background.
Yamamoto was born in Nagaoka, Niigata. His father was Sadayoshi Takano (高野 貞吉), an intermediate-rank "samurai" of the Nagaoka Domain. "Isoroku" is an old Japanese term meaning "56"; the name referred to his father's age at Isoroku's birth.
In 1916, Isoroku was adopted into the Yamamoto family (another family of former Nagaoka samurai) and took the Yamamoto name. It was a common practice for samurai families lacking sons to adopt suitable young men in this fashion to carry on the family name, the rank and the income that comes with it. In 1918 Isoroku married Reiko Mihashi, with whom he had two sons and two daughters.
Early career.
After graduating from the Imperial Japanese Naval Academy in 1904, Yamamoto served on the armored cruiser during the Russo-Japanese War. He was wounded at the Battle of Tsushima, losing two fingers (the index and middle fingers) on his left hand, as the cruiser was hit repeatedly by the Russian battleline. He returned to the Naval Staff College in 1914, emerging as a Lieutenant Commander in 1916.
1920s and 1930s.
Yamamoto was part of the Japanese Navy establishment, who were rivals of the more aggressive Army establishment, especially the officers of the Kwantung Army. As such he promoted a policy of a strong fleet to project force through gunboat diplomacy, rather than a fleet used primarily for transport of invasion land forces, as some of his political opponents in the army wanted. This stance led him to oppose the invasion of China. He also opposed war against the United States partly because of his studies at Harvard University (1919–1921) and his two postings as a naval attaché in Washington, D.C. He learned to speak fluent English as a result.
Yamamoto traveled extensively in the United States during his tour of duty there, where he studied American customs and business practices.
He was promoted to Captain in 1923. On April 13, 1924 at the rank of captain, he was part of the Japanese delegation visiting the U.S. Naval War College. Later that year, he changed his specialty from gunnery to naval aviation. His first command was the cruiser in 1928, followed by the aircraft carrier .
He participated in the second London Naval Conference of 1930 as a Rear Admiral and the 1934 London Naval Conference as a Vice Admiral, as the growing military influence on the government at the time deemed that a career military specialist needed to accompany the diplomats to the arms limitations talks. Yamamoto was a strong proponent of naval aviation, and served as head of the Aeronautics Department before accepting a post as commander of the First Carrier Division. Yamamoto personally opposed the invasion of Manchuria in 1931, the subsequent land war with China (1937), and the 1940 Tripartite Pact with Nazi Germany and Fascist Italy. As Deputy Navy Minister, he apologized to United States Ambassador Joseph C. Grew for the bombing of the gunboat in December 1937. These issues made him a target of assassination threats by pro-war militarists.
Throughout 1938, many young army and naval officers began to speak publicly against Yamamoto and certain other Japanese admirals such as Mitsumasa Yonai and Shigeyoshi Inoue for their strong opposition towards a Tripartite pact with Nazi Germany for reportedly being against "Japan's natural interests." Yamamoto himself received a steady stream of hate mail and death threats from Japanese nationalists but his reaction to the prospect of death by assassination was passive and accepting. The Admiral wrote:
The Japanese army, annoyed at Yamamoto's unflinching opposition to a Rome-Berlin-Tokyo treaty, dispatched military police to "guard" Yamamoto; this was an attempt by the Army to keep an eye on him. He was later reassigned from the Navy Ministry to sea as the Commander-in-Chief of the Combined Fleet on (August 30, 1939). This was done as one of the last acts of the then-acting Navy Minister Mitsumasa Yonai, under Baron Hiranuma's short-lived administration partly to make it harder for assassins to target Yamamoto; Yonai was certain that if Yamamoto remained ashore, he would be killed before the year (1939) ended.
1940–1941.
Yamamoto was promoted to Admiral on November 15, 1940. This, in spite of the fact that when Hideki Tōjō was appointed Prime Minister on October 18, 1941, many political observers thought that Yamamoto's career was essentially over. Tōjō had been Yamamoto's old opponent from the time when the latter served as Japan's deputy navy minister and Tōjō was the prime mover behind Japan's takeover of Manchuria. It was believed that Yamamoto would be appointed to command the Yokosuka Naval Base, "a nice safe demotion with a big house and no power at all." After the new Japanese cabinet was announced, however, Yamamoto found himself left alone in his position despite his open conflicts with Tōjō and other members of the army's oligarchy who favored war with the European powers and America. Two of the main reasons for Yamamoto's political survival were his immense popularity within the fleet, where he commanded the respect of his men and officers, and his close relations with the imperial family. He also had the acceptance by Japan's naval hierarchy:
Consequently, Yamamoto stayed in his post. With Tōjō now in charge of Japan's highest political office, it became clear the army would lead the navy into a war about which Yamamoto had serious reservations. He wrote to an ultranationalist:
This quote was spread by the militarists, minus the last sentence, where it was interpreted in America as a boast that Japan would conquer the entire continental United States. The omitted sentence showed Yamamoto's counsel of caution towards a war that could cost Japan dearly. Nevertheless, Yamamoto accepted the reality of impending war and planned for a quick victory by destroying the US fleet at Pearl Harbor in a preventive strike while simultaneously thrusting into the oil and rubber resource rich areas of Southeast Asia, especially the Dutch East Indies, Borneo and Malaya. In naval matters, Yamamoto opposed the building of the super-battleships and as an unwise investment of resources.
Yamamoto was responsible for a number of innovations in Japanese naval aviation. Although remembered for his association with aircraft carriers due to Pearl Harbor and Midway, Yamamoto did more to influence the development of land-based naval aviation, particularly the Mitsubishi G3M and G4M medium bombers. His demand for great range and the ability to carry a torpedo was intended to conform to Japanese conceptions of attriting the American fleet as it advanced across the Pacific in war. The planes did achieve long range, but long-range fighter escorts were not available. These planes were lightly constructed and when fully fueled, they were especially vulnerable to enemy fire. This earned the G4M the sardonic nickname "the Flying Cigarette Lighter." Yamamoto would eventually die in one of these aircraft.
The range of the G3M and G4M contributed to a demand for great range in a fighter aircraft. This partly drove the requirements for the A6M Zero which was as noteworthy for its range as for its maneuverability. Both qualities were again purchased at the expense of light construction and flammability that later contributed to the A6M's high casualty rates as the war progressed.
As Japan moved toward war during 1940, Yamamoto gradually moved toward strategic as well as tactical innovation, again with mixed results. Prompted by talented young officers such as Lieutenant Commander Minoru Genda, Yamamoto approved the reorganization of Japanese carrier forces into the First Air Fleet, a consolidated striking force that gathered Japan's six largest carriers into one unit. This innovation gave great striking capacity, but also concentrated the vulnerable carriers into a compact target; both boon and bane would be realized in war. Yamamoto also oversaw the organization of a similar large land-based organization in the 11th Air Fleet, which would later use the G3M and G4M to neutralize American air forces in the Philippines and sink the British Force "Z".
In January 1941, Yamamoto went even further and proposed a radical revision of Japanese naval strategy. For two decades, in keeping with the doctrine of Captain Alfred T. Mahan, the Naval General Staff had planned in terms of Japanese light surface forces, submarines and land-based air units whittling down the American Fleet as it advanced across the Pacific until the Japanese Navy engaged it in a climactic "Decisive Battle" in the northern Philippine Sea (between the Ryukyu Islands and the Marianas Islands), with battleships meeting in the traditional exchange between battle lines.
Correctly pointing out this plan had never worked even in Japanese war games, and painfully aware of American strategic advantages in military productive capacity, Yamamoto proposed instead to seek a decision with the Americans by first reducing their forces with a preventive strike, and following it with a "Decisive Battle" fought offensively, rather than defensively. Yamamoto hoped, but probably did not believe, if the Americans could be dealt such terrific blows early in the war, they might be willing to negotiate an end to the conflict. As it turned out, however, the note officially breaking diplomatic relations with the United States was delivered late, and he correctly perceived the Americans would be resolved upon revenge and unwilling to negotiate. At the end of the attack upon Pearl Harbor, upon hearing of the mis-timing of the communique breaking diplomatic relations with the United States earlier that day, it is reputed Yamamoto said, "I fear all we have done today is to awaken a great, sleeping giant and fill him with a terrible resolve."; however, there is no documented evidence the statement was made.
The Naval General Staff proved reluctant to go along and Yamamoto was eventually driven to capitalize on his popularity in the fleet by threatening to resign to get his way. Admiral Osami Nagano and the Naval General Staff eventually caved in to this pressure, but only insofar as approving the attack on Pearl Harbor.
The First Air Fleet commenced preparations for the Pearl Harbor Raid, solving a number of technical problems along the way, including how to launch torpedoes in the shallow water of Pearl Harbor and how to craft armor-piercing bombs by machining down battleship gun projectiles.
Attack on Pearl Harbor.
As Yamamoto had planned, the First Air Fleet of six carriers commenced hostilities against the Americans on December 7, 1941, launching 353 aircraft against Pearl Harbor in two waves. The attack was a complete success according to the parameters of the mission which sought to sink at least four American battleships and prevent the U.S. Fleet from interfering in Japan's southward advance for at least six months. American aircraft carriers were also considered a choice target, but these were not in port at the time of the attack.
In the end, five American battleships were sunk, three were damaged, and eleven other cruisers, destroyers and auxiliaries were sunk or seriously damaged. The Japanese lost only 29 aircraft, while 74 were damaged from anti-aircraft fire from the ground. The damaged aircraft were disproportionately dive and torpedo bombers, seriously impacting available firepower to exploit the first two waves' success, so the commander of the First Air Fleet, Naval Lieutenant-General Chuichi Nagumo, withdrew. Yamamoto later lamented Nagumo's failure to seize the initiative to seek out and destroy the American carriers, absent from the harbor, or further bombard various strategically important facilities on Oahu. Nagumo had absolutely no idea where the American carriers might be, and remaining on station while his forces cast about looking for them ran the risk of his own forces being found first and attacked while his aircraft were absent searching. In any case, insufficient daylight remained after recovering the aircraft from the first two waves for the carriers to launch and recover a third before dark, and Nagumo's escorting destroyers lacked the fuel capacity for him to loiter long. Much has been made of Yamamoto's hindsight, but (in keeping with Japanese military tradition not to criticize the commander on the spot) he did not punish Nagumo in any way for his withdrawal.
On the strategic level, the attack was a disaster for Japan, rousing American passions for revenge due to it being a "sneak attack". The shock of the attack coming in an unexpected place, with such devastating results and without the expected "fair play" of a declaration of war galvanized the American public's determination to avenge the attack. When asked by Prime Minister Fumimaro Konoe in mid-1941 concerning the outcome of a possible war with the United States, Yamamoto made a well-known and prophetic statement: If ordered to fight, "I shall run wild considerably for the first six months or a year, but I have utterly no confidence for the second and third years." His prediction would be vindicated as Japan easily conquered territories and islands for the first six months of the war until it suffered a shattering defeat at the Battle of Midway on June 4–7, 1942, which ultimately tilted the balance of power in the Pacific towards the U.S.
As a strategic blow intended to prevent American interference in the Dutch East Indies for six months, the Pearl Harbor attack was a success, but unbeknownst to Yamamoto, it was a pointless one. The U.S. Navy had abandoned any intention of attempting to charge across the Pacific towards the Philippines at the outset of war in 1935 (in keeping with the evolution of War Plan Orange). In 1937, the U.S. Navy had further determined even fully manning the fleet to wartime levels could not be accomplished in less than six months, and myriad other logistic assets needed to execute a trans-Pacific movement simply did not exist and would require two years to construct after the onset of war. In 1940, U.S. Chief of Naval Operations, Admiral Harold Stark had penned a Plan Dog memorandum, which emphasized a defensive war in the Pacific while the U.S. concentrated on defeating Nazi Germany first, and consigned Admiral Husband Kimmel's Pacific Fleet to merely keeping the Imperial Japanese Navy (IJN) out of the eastern Pacific and away from the shipping lanes to Australia. Moreover, it is in question whether the U.S. would have gone to war at all had Japan only attacked British and Dutch possessions in the Far East.
December 1941 to May 1942.
With the American Fleet largely neutralized at Pearl Harbor, Yamamoto's Combined Fleet turned to the task of executing the larger Japanese war plan devised by the Imperial Japanese Army and Navy General Staff. The First Air Fleet proceeded to make a circuit of the Pacific, striking American, Australian, Dutch and British installations from Wake Island to Australia to Ceylon in the Indian Ocean. The 11th Air Fleet caught the American 5th Air Force on the ground in the Philippines hours after Pearl Harbor, and then proceeded to sink the British Force "Z" (battleship and battlecruiser ) underway at sea.
Under Yamamoto's able subordinates, Naval Lieutenant-Generals Jisaburō Ozawa, Nobutake Kondō and Ibō Takahashi, the Japanese swept the inadequate remaining American, British, Dutch and Australian naval assets from the Dutch East Indies in a series of amphibious landings and surface naval battles that culminated in the Battle of the Java Sea on February 27, 1942. Along with the occupation of the Dutch East Indies came the fall of Singapore (February 15, 1942), and the eventual reduction of the remaining American-Filipino defensive positions in the Philippines on the Bataan peninsula (April 9, 1942) and Corregidor island (May 6, 1942). The Japanese had secured their oil- and rubber-rich "Southern Resources Area".
By late March, having achieved their initial aims with surprising speed and little loss (albeit against enemies ill-prepared to resist them), the Japanese paused to consider their next moves. Yamamoto and a few Japanese military leaders and officials waited, hoping that the United States or Great Britain would negotiate for an armistice or a peace treaty to end the war in their favour. But when the British, as well as the Americans, expressed no interest in negotiating with Japan for any cease fire, the Japanese thoughts turned to securing their newly seized territory and acquiring more with an eye toward attempting to force one or more of their enemies out of the war.
Competing plans were developed at this stage, including thrusts to the west against India, the south against Australia and the east against the United States. Yamamoto was involved in this debate, supporting different plans at different times with varying degrees of enthusiasm and for varying purposes, including "horse-trading" for support of his own objectives.
Plans included ideas as ambitious as invading India or Australia, or seizing Hawaii. These grandiose ventures were inevitably set aside as the army could not spare enough troops from China for the first two (which would require a minimum of 250,000 men), nor shipping to support the latter two. (Shipping was allocated separately to IJN & IJA, and jealously guarded.) Instead, the Imperial General Staff supported an army thrust into Burma in hopes of linking up with Indian Nationalists revolting against British rule, and attacks in New Guinea and the Solomon Islands designed to imperil Australia's sea line of communication with the United States. Yamamoto agitated for an offensive decisive battle in the east to finish off the American fleet, but the more conservative Naval General Staff officers were unwilling to risk it.
On April 18, in the midst of these debates, the Doolittle Raid struck Tokyo and the surrounding areas, galvanizing the threat posed by the American aircraft carriers in the minds of staff officers, and giving Yamamoto an event he could exploit to get his way as further debate over military strategy came to a quick end. The Naval General Staff agreed to Yamamoto's Midway (MI) Operation, subsequent to the first phase of the operations against Australia's link with America, and concurrent with their own plan to seize positions in the Aleutian Islands.
Yamamoto rushed planning for the Midway and Aleutians missions, while dispatching a force under Naval Major-General Takeo Takagi, including the Fifth Carrier Division (the large, new carriers and ), to support the effort to seize the islands of Tulagi and Guadalcanal for seaplane and aeroplane bases, and the town of Port Moresby on Papua New Guinea's south coast facing Australia.
The Port Moresby (MO) Operation proved an unwelcome setback. Although Tulagi and Guadalcanal were taken, the Port Moresby invasion fleet was compelled to turn back when Takagi clashed with an American carrier task force in the Battle of the Coral Sea in early May. Although the Japanese sank the American carrier and damaged the , the Americans damaged the carrier "Shōkaku" so badly that she required dockyard repairs, and the Japanese lost the light carrier . Just as importantly, Japanese operational mishaps and American fighters and anti-aircraft fire devastated the dive bomber and torpedo plane formations of both "Shōkaku"s and "Zuikaku"s air groups. These losses sidelined "Zuikaku" while she awaited replacement aircraft and aircrews, and saw to tactical integration and training. These two ships would be sorely missed a month later at Midway.
Battle of Midway, June 1942.
Yamamoto's plan for Midway Island was an extension of his efforts to knock the U.S. Pacific Fleet out of action long enough for Japan to fortify her defensive perimeter in the Pacific island chains. Yamamoto felt it necessary to seek an early, offensive decisive battle.
This plan was long believed to have been to draw American attention—and possibly carrier forces—north from Pearl Harbor by sending his Fifth Fleet (two light carriers, five cruisers, 13 destroyers, and four transports) against the Aleutians, raiding Dutch Harbor on Unalaska Island and invading the more distant islands of Kiska and Attu. 
While Fifth Fleet attacked the Aleutians, First Mobile Force (4 carriers, 2 battleships, 3 cruisers, and 12 destroyers) would raid Midway and destroy its air force. Once this was neutralized, Second Fleet (1 light carrier, 2 battleships, 10 cruisers, 21 destroyers, and 11 transports) would land 5,000 troops to seize the atoll from the American Marines.
The seizure of Midway was expected to draw the American carriers west into a trap where the First Mobile Force would engage and destroy them. Afterward, First Fleet (1 light carrier, 7 battleships, 3 cruisers and 13 destroyers), in conjunction with elements of Second Fleet, would mop up remaining American surface forces and complete the destruction of the Pacific Fleet.
To guard against mischance, Yamamoto initiated two security measures. The first was an aerial reconnaissance mission (Operation K) over Pearl Harbor to ascertain if the American carriers were there. The second was a picket line of submarines to detect the movement of the American carriers toward Midway in time for First Mobile Force, First Fleet, and Second Fleet to combine against it. In the event, the first was aborted and the second delayed until after American carriers had sortied.
The plan was a compromise and hastily prepared (apparently so it could be launched in time for the anniversary of Tsushima), but appeared well thought out, well organized, and finely timed when viewed from a Japanese viewpoint. Against four carriers, two light carriers, 11 battleships, 16 cruisers and 46 destroyers likely to be in the area of the main battle the Americans could field only three carriers, eight cruisers, and 15 destroyers. The disparity appeared crushing. Only in numbers of carrier decks, available aircraft, and submarines was there near parity between the two sides. Despite various frictions developed in the execution, it appeared—barring something extraordinary—Yamamoto held all the cards.
The Americans were able to learn of the Japanese plans thanks to code breaking of Japanese naval code D (known to the U.S. as JN-25). As a result, Admiral Chester Nimitz, the Pacific Fleet commander, was able to circumvent both of Yamamoto's security measures and position his outnumbered forces in the exact position to conduct a devastating ambush. By Nimitz's calculation, his three available carrier decks, plus Midway, gave him rough parity with Nagumo's First Mobile Force.
Following a nuisance raid by Japanese flying boats in May, Nimitz dispatched a minesweeper to guard the intended refueling point for Operation K near French Frigate Shoals, causing the reconnaissance mission to be aborted and leaving Yamamoto ignorant of whether Pacific Fleet carriers were still at Pearl Harbor. (It remains unclear why Yamamoto permitted the earlier attack, and why his submarines did not sortie sooner, as reconnaissance was essential to the success of MI.) He also dispatched his carriers toward Midway early, and they passed the intended picket line force of submarines "en route" to their station, negating Yamamoto's back-up security measure. Nimitz's carriers positioned themselves to ambush the "Kido Butai" (Striking Force) when it struck Midway. A token cruiser and destroyer force was sent toward the Aleutians, but otherwise Nimitz ignored them. On June 4, 1942, days before Yamamoto expected them to interfere in the Midway operation, American carrier-based aircraft destroyed the four carriers of the "Kido Butai", catching the Japanese carriers at an especially vulnerable moment.
With his air power destroyed and his forces not yet concentrated for a fleet battle, Yamamoto attempted to maneuver his remaining forces, still strong on paper, to trap the American forces. He was unable to do so because his initial dispositions had placed his surface combatants too far from Midway, and because Admiral Raymond Spruance prudently withdrew to the east in a position to further defend Midway Island, believing (based on a mistaken submarine report) the Japanese still intended to invade. Not knowing several battleships, including the powerful , were on the Japanese order of battle, he did not comprehend the severe risk of a night surface battle, in which his carriers and cruisers would be at a disadvantage. However, his move to the east did avoid the possibility of such a battle taking place. Correctly perceiving he had lost and could not bring surface forces into action, Yamamoto aborted the invasion of Midway and withdrew. The defeat marked the high tide of Japanese expansion.
Yamamoto's plan for MI has been the subject of much criticism. Many commentators state it violated the principle of concentration of force, and was overly complex. Others point out similarly complex Allied operations (such as Operation MB8) that were successful, and note the extent to which the American intelligence "coup" derailed the operation before it began. Had Yamamoto's dispositions not denied Nagumo adequate pre-attack reconnaissance assets, both the American cryptanalytic success and the unexpected appearance of Fletcher's carriers would have been irrelevant.
Actions after Midway.
The Battle of Midway solidly checked Japanese momentum, but the IJN was still a powerful force and capable of regaining the initiative. They planned to resume the thrust with "Operation FS" aimed at eventually taking Samoa and Fiji to cut the American life-line to Australia. This was expected to short-circuit the threat posed by General Douglas MacArthur and his American and Australian forces in New Guinea. To this end, development of the airfield on Guadalcanal continued and attracted the baleful eye of Yamamoto's opposite number, Admiral Ernest King.
To prevent the Japanese from regaining the initiative, King ramrodded the idea of an immediate American counterattack through the Joint Chiefs of Staff. This precipitated the American invasion of Guadalcanal and beat the Japanese to the punch, with Marines landing on the island in August 1942 and starting a bitter struggle that lasted until February 1943 and commenced a battle of attrition Japan could ill afford.
Yamamoto remained in command as Commander-in-Chief, retained at least partly to avoid diminishing the morale of the Combined Fleet. However, he had lost face as a result of the Midway defeat and the Naval General Staff were disinclined to indulge in further gambles. This reduced Yamamoto to pursuing the classic defensive Decisive Battle strategy he had attempted to overturn.
The naval and land battles at Guadalcanal caught the Japanese over-extended and attempting to support fighting in New Guinea while guarding the Central Pacific and preparing to conduct Operation FS. The FS operation was abandoned and the Japanese attempted to fight in both New Guinea and Guadalcanal at the same time. Already stretched thin, they suffered repeated setbacks due to a lack of shipping, a lack of troops, and a disastrous inability to coordinate Army and Navy activities.
Yamamoto committed Combined Fleet units to a series of small attrition actions across the south and central Pacific that stung the Americans, but suffered losses he could ill afford in return. Three major efforts to carry the island precipitated a pair of carrier battles that Yamamoto commanded personally at the Eastern Solomons and Santa Cruz Islands in September and October, and finally a wild pair of surface engagements in November, all timed to coincide with Japanese Army pushes. The timing of each major battle was successively derailed when the army could not hold up its end of the operation. Yamamoto's naval forces won a few victories and inflicted considerable losses and damage to the U.S. Fleet in several naval battles around Guadalcanal which included the battles of Savo Island, Cape Esperance, and Tassafaronga, but he could never draw the Americans into a decisive fleet action. As a result, the Japanese Navy's strength began to bleed off.
There were severe losses of carrier dive-bomber and torpedo-bomber crews in the carrier battles, emasculating the already depleted carrier air groups. Japan could not hope to match the United States in quantities of well-trained replacement pilots, and the quality of both Japanese land-based and naval aviation began declining. Particularly harmful, however, were losses of numerous destroyers in the unsuccessful Tokyo Express supply runs. The IJN already faced a shortage of such ships, and these losses further exacerbated Japan's already weakened commerce defense. With Guadalcanal lost in February 1943, there was no further attempt by the Japanese navy to seek a major battle in the Solomon Islands against the U.S. fleet, although smaller attrition battles continued. Yamamoto shifted the load of the air battle away from the depleted carrier air wings to land-based naval air forces.
Death.
To boost morale following the defeat at Guadalcanal, Yamamoto decided to make an inspection tour throughout the South Pacific. On April 14, 1943, the US naval intelligence effort, code-named "Magic", intercepted and decrypted a message containing specific details regarding Yamamoto's tour, including arrival and departure times and locations, as well as the number and types of aircraft that would transport and accompany him on the journey. Yamamoto, the itinerary revealed, would be flying from Rabaul to Balalae Airfield, on an island near Bougainville in the Solomon Islands, on the morning of April 18, 1943.
U.S. President Franklin D. Roosevelt ordered Secretary of the Navy Frank Knox to "Get Yamamoto." Knox instructed Chief of Naval Operations Admiral Ernest J. King of Roosevelt's wishes. Admiral King telephoned Admiral Chester W. Nimitz at Pearl Harbor. This mission would be Top Secret and Urgent. Admiral Nimitz consulted Admiral William F. Halsey, Jr., Commander, South Pacific, then authorized a mission on April 17 to intercept Yamamoto's flight "en route" and shoot it down. A squadron of USAAF Lockheed P-38 Lightning aircraft were assigned the task as only they possessed the range to intercept and engage. Select pilots from three units were informed that they were intercepting an "important high officer" with no specific name given.
On the morning of April 18, despite urgings by local commanders to cancel the trip for fear of ambush, Yamamoto's two Mitsubishi G4M bombers, used as fast transport aircraft without bombs, left Rabaul as scheduled for the trip. Sixteen Lightnings intercepted the flight over Bougainville and a dogfight ensued between them and the six escorting Mitsubishi A6M Zeroes. First Lieutenant Rex T. Barber engaged the first of the two Japanese transports which turned out to be "T1-323" (Yamamoto's aircraft). He targeted the aircraft with gunfire until it began to spew smoke from its left engine. Barber turned away to attack the other transport as Yamamoto's plane crashed into the jungle.
The crash site and body of Yamamoto were found the next day in the jungle north of Buin, Papua New Guinea, by a Japanese search and rescue party, led by army engineer, Lieutenant Hamasuna. According to Hamasuna, Yamamoto had been thrown clear of the plane's wreckage, his white-gloved hand grasping the hilt of his katana, still upright in his seat under a tree. Hamasuna said Yamamoto was instantly recognizable, head dipped down as if deep in thought. A post-mortem of the body disclosed that Yamamoto had received two 0.50-caliber bullet wounds, one to the back of his left shoulder and another to his left lower jaw that exited above his right eye. The Japanese navy doctor examining the body determined that the head wound killed Yamamoto. The more violent details of Yamamoto's death were hidden from the Japanese public; the medical report was whitewashed, changed "on orders from above", according to biographer Hiroyuki Agawa.
His staff cremated his remains at Buin, and the ashes were returned to Tokyo aboard the battleship , Yamamoto's last flagship. Yamamoto was given a full state funeral on June 5, 1943, where he received, posthumously, the title of Marshal and was awarded the Order of the Chrysanthemum (1st Class). He was also awarded Nazi Germany's Knight's Cross of the Iron Cross with Oak Leaves and Swords. Part of his ashes were buried in the public Tama Cemetery, Tokyo (多摩霊園), and the remainder at his ancestral burial grounds at the temple of Chuko-ji in Nagaoka City. He was succeeded as commander-in-chief of the Combined Fleet by Admiral Mineichi Koga.
The wreck of the aircraft that carried Yamamoto remains as a tourist attraction in the Bougainville jungle near Moila Point, a few kilometers off the Panguna-Buin road. Signposts can be found near the village of Aku, 24 km outside Buin. A path to the wreck has been cut through the jungle, an hour's walk from the road. Other artifacts from the crash site, including the outer wing panel and the Admiral's seat, are at the Isoroku Yamamoto Memorial Hall and Museum in Nagaoka, Niigata, Japan.
Personal life.
Yamamoto practiced calligraphy. He and his wife, Reiko, had four children: two sons and two daughters. Yamamoto was an avid gambler, enjoying "Go", "shogi", billiards, bridge, mah jong, poker, and other games that tested his wits and sharpened his mind. He frequently made jokes about moving to Monaco and starting his own casino. He enjoyed the company of "geisha", and his wife Reiko revealed to the Japanese public in 1954 that Yamamoto was closer to his favorite "geisha" Kawai Chiyoko than to her, which stirred some controversy. After his death, his funeral procession passed by Kawai's quarters on the way to the cemetery.
Media portrayals.
Since the end of the Second World War, a number of Japanese and American films have depicted the character of Isoroku Yamamoto.
One of the most notable films is the 1970 movie "Tora! Tora! Tora!", which stars Japanese actor Sô Yamamura as Yamamoto, who states after the attack on Pearl Harbor:
The first film to feature Yamamoto was Toho's 1953 film "Taiheiyô no washi", (later released in the United States as "Eagle of the Pacific"), in which Yamamoto was portrayed by Denjirô Ôkôchi.
The 1960 film "The Gallant Hours" depicts the battle of wits between Vice-Admiral William Halsey, Jr. and Yamamoto from the start of the Guadalcanal Campaign in August 1942 to Yamamoto's death in April 1943. The film, however, portrays Yamamoto's death as occurring in November 1942, the day after the Naval Battle of Guadalcanal, and the P-38 aircraft that killed him as coming from Guadalcanal.
In Daiei Studios's 1969 film "Aa, kaigun" (later released in the United States as "Gateway to Glory"), Yamamoto was portrayed by Shôgo Shimada.
Award-winning Japanese actor Toshiro Mifune (star of "The Seven Samurai") portrayed Yamamoto in three films:
In Shūe Matsubayashi's 1981 film "Rengō kantai" (lit. "Combined Fleet", later released in the United States as "The Imperial Navy"), Yamamoto was portrayed by Keiju Kobayashi.
In the 2001 film "Pearl Harbor", Yamamoto was portrayed by Oscar-nominated Japanese-born American actor Mako Iwamatsu. Like "Tora! Tora! Tora!", this film also features the sleeping giant quote.
In Toei's 2011 war film "", Yamamoto was portrayed by Kōji Yakusho.
Alternate History.
In the 1993 OVA series "Konpeki no Kantai" (lit. "Deep Blue Fleet"), instead of dying in the plane crash, Yamamoto blacks out and suddenly wakes up as his younger self, Isoroku Takano, after the Battle of Tsushima in 1905. His memory from the original timeline intact, Yamamoto uses his knowledge of the future to help Japan become a stronger military power, and eventually launching a "coup d'état" against Hideki Tōjō's government. In the subsequent Pacific war, Japan's technologically advanced navy decisively defeats the United States, and grants all of the former European and American colonies in Asia full independence. Later on, Yamamoto convinces Japan to join forces with the United States and Britain to defeat Nazi Germany.
In the 2004 anime series "Zipang", Yamamoto (who is voiced by Bunmei Tobayama) works to develop the uneasy partnership with the crew of the JMSDF "Mirai", which has been transported back sixty years through time to the year 1942.
In the Axis of Time trilogy by author John Birmingham, which depicts an alternate history of World War II, after a naval task force from the year 2021 is accidentally transported back through time to 1942, Yamamoto assumes a leadership role in the dramatic alteration of Japan's war strategy.
In Douglas Niles' 2007 book "MacArthur's War: A Novel of the Invasion of Japan" (written with Michael Dobson), which focuses on General Douglas MacArthur and an alternate history of the Pacific War (following a considerably different outcome of the Battle of Midway), Yamamoto is portrayed sympathetically, with much of the action in the Japanese government seen through his eyes, though he could not change the major decisions of Japan in World War II.
In Robert Conroy's 2011 book "Rising Sun", Yamamoto directs the IJN to launch a series of attacks on the American West Coast, in the hope that the United States can be convinced to sue for peace and securing Japan's place as a world power; but he cannot escape his lingering fear that the war will ultimately doom Japan.
In Neal Stephenson's 1999 book "Cryptonomicon", Yamamoto's final moments are depicted as him realising that Japan's cryptographic codes have been broken, and that he must inform them, right up until he and his chair hit the tree.

</doc>
<doc id="15412" url="https://en.wikipedia.org/wiki?curid=15412" title="Infrared spectroscopy">
Infrared spectroscopy

Infrared spectroscopy (IR spectroscopy or Vibrational Spectroscopy) is the spectroscopy that deals with the infrared region of the electromagnetic spectrum, that is light with a longer wavelength and lower frequency than visible light. It covers a range of techniques, mostly based on absorption spectroscopy. As with all spectroscopic techniques, it can be used to identify and study chemicals. For a given sample which may be solid, liquid, or gaseous, the method or technique of infrared spectroscopy uses an instrument called an infrared spectrometer (or spectrophotometer) to produce an infrared spectrum. A basic IR spectrum is essentially a graph of infrared light absorbance (or transmittance) on the vertical axis vs. frequency or wavelength on the horizontal axis. Typical units of frequency used in IR spectra are reciprocal centimeters (sometimes called wave numbers), with the symbol cm−1. Units of IR wavelength are commonly given in micrometers (formerly called "microns"), symbol μm, which are related to wave numbers in a reciprocal way. A common laboratory instrument that uses this technique is a Fourier transform infrared (FTIR) spectrometer. Two-dimensional IR is also possible as discussed below.
The infrared portion of the electromagnetic spectrum is usually divided into three regions; the near-, mid- and far- infrared, named for their relation to the visible spectrum. The higher-energy near-IR, approximately 14000–4000 cm−1 (0.8–2.5 μm wavelength) can excite overtone or harmonic vibrations. The mid-infrared, approximately 4000–400 cm−1 (2.5–25 μm) may be used to study the fundamental vibrations and associated rotational-vibrational structure. The far-infrared, approximately 400–10 cm−1 (25–1000 μm), lying adjacent to the microwave region, has low energy and may be used for rotational spectroscopy. The names and classifications of these subregions are conventions, and are only loosely based on the relative molecular or electromagnetic properties.
Theory.
Infrared spectroscopy exploits the fact that molecules absorb specific frequencies that are characteristic of their structure. These absorptions are resonant frequencies, i.e. the frequency of the absorbed radiation matches the transition energy of the bond or group that vibrates. The energies are determined by the shape of the molecular potential energy surfaces, the masses of the atoms, and the associated vibronic coupling.
Number of vibrational modes.
In order for a vibrational mode in a molecule to be "IR active", it must be associated with changes in the dipole. A permanent dipole is not necessary, as the rule requires only a change in dipole moment.
A molecule can vibrate in many ways, and each way is called a "vibrational mode." For molecules with N number of atoms in them, linear molecules have 3N – 5 degrees of vibrational modes, whereas nonlinear molecules have 3N – 6 degrees of vibrational modes (also called vibrational degrees of freedom). As an example H2O, a non-linear molecule, will have 3 × 3 – 6 = 3 degrees of vibrational freedom, or modes.
Simple diatomic molecules have only one bond and only one vibrational band. If the molecule is symmetrical, e.g. N2, the band is not observed in the IR spectrum, but only in the Raman spectrum. Asymmetrical diatomic molecules, e.g. CO, absorb in the IR spectrum. More complex molecules have many bonds, and their vibrational spectra are correspondingly more complex, i.e. big molecules have many peaks in their IR spectra.
The atoms in a CH2X2 group, commonly found in organic compounds and where X can represent any other atom, can vibrate in nine different ways. Six of these vibrations involve only the CH2 portion: symmetric and antisymmetric stretching, scissoring, rocking, wagging and twisting, as shown below. Structures that do not have the two additional X groups attached have fewer modes because some modes are defined by specific relationships to those other attached groups. For example, in water, the rocking, wagging, and twisting modes do not exist because these types of motions of the H represent simple rotation of the whole molecule rather than vibrations within it.
These figures do not represent the "recoil" of the C atoms, which, though necessarily present to balance the overall movements of the molecule, are much smaller than the movements of the lighter H atoms.
Special effects.
The simplest and most important IR bands arise from the "normal modes," the simplest distortions of the molecule. In some cases, "overtone bands" are observed. These bands arise from the absorption of a photon that leads to a doubly excited vibrational state. Such bands appear at approximately twice the energy of the normal mode. Some vibrations, so-called 'combination modes," involve more than one normal mode. The phenomenon of Fermi resonance can arise when two modes are similar in energy; Fermi resonance results in an unexpected shift in energy and intensity of the bands etc.
Practical IR spectroscopy.
The infrared spectrum of a sample is recorded by passing a beam of infrared light through the sample. When the frequency of the IR is the same as the vibrational frequency of a bond or collection of bonds, absorption occurs. Examination of the transmitted light reveals how much energy was absorbed at each frequency (or wavelength). This measurement can be achieved by scanning the wavelength range using a monochromator. Alternatively, the entire wavelength range is measured using a Fourier transform instrument and then a transmittance or absorbance spectrum is generated using a dedicated procedure.
This technique is commonly used for analyzing samples with covalent bonds. Simple spectra are obtained from samples with few IR active bonds and high levels of purity. More complex molecular structures lead to more absorption bands and more complex spectra.
Sample preparation.
Gaseous samples require a sample cell with a long pathlength to compensate for the diluteness. The pathlength of the sample cell depends on the concentration of the compound of interest. A simple glass tube with length of 5 to 10 cm equipped with infrared-transparent windows at the both ends of the tube can be used for concentrations down to several hundred ppm. Sample gas concentrations well below ppm can be measured with a White's cell in which the infrared light is guided with mirrors to travel through the gas. White's cells are available with optical pathlength starting from 0.5 m up to hundred meters.
Liquid samples can be sandwiched between two plates of a salt (commonly sodium chloride, or common salt, although a number of other salts such as potassium bromide or calcium fluoride are also used).
The plates are transparent to the infrared light and do not introduce any lines onto the spectra.
Solid samples can be prepared in a variety of ways. One common method is to crush the sample with an oily mulling agent (usually mineral oil Nujol). A thin film of the mull is applied onto salt plates and measured. The second method is to grind a quantity of the sample with a specially purified salt (usually potassium bromide) finely (to remove scattering effects from large crystals). This powder mixture is then pressed in a mechanical press to form a translucent pellet through which the beam of the spectrometer can pass. A third technique is the "cast film" technique, which is used mainly for polymeric materials. The sample is first dissolved in a suitable, non hygroscopic solvent. A drop of this solution is deposited on surface of KBr or NaCl cell. The solution is then evaporated to dryness and the film formed on the cell is analysed directly. Care is important to ensure that the film is not too thick otherwise light cannot pass through. This technique is suitable for qualitative analysis. The final method is to use microtomy to cut a thin (20–100 µm) film from a solid sample. This is one of the most important ways of analysing failed plastic products for example because the integrity of the solid is preserved.
In photoacoustic spectroscopy the need for sample treatment is minimal. The sample, liquid or solid, is placed into the sample cup which is inserted into the photoacoustic cell which is then sealed for the measurement. The sample may be one solid piece, powder or basically in any form for the measurement. For example, a piece of rock can be inserted into the sample cup and the spectrum measured from it.
Comparing to a reference.
To take the infrared spectrum of a sample, it is necessary to measure both the sample and a "reference" (or "control"). This is because each measurement is affected by not only the light-absorption properties of the sample, but also the properties of the instrument (for example, what light source is used, what infrared detector is used, etc.). The reference measurement makes it possible to eliminate the instrument influence. Mathematically, the sample transmission spectrum is divided by the reference transmission spectrum.
The appropriate "reference" depends on the measurement and its goal. The simplest reference measurement is to simply remove the sample (replacing it by air). However, sometimes a different reference is more useful. For example, if the sample is a dilute solute dissolved in water in a beaker, then a good reference measurement might be to measure pure water in the same beaker. Then the reference measurement would cancel out not only all the instrumental properties (like what light source is used), but also the light-absorbing and light-reflecting properties of the water and beaker, and the final result would just show the properties of the solute (at least approximately).
A common way to compare to a reference is sequentially: first measure the reference, then replace the reference by the sample and measure the sample. This technique is not perfectly reliable; if the infrared lamp is a bit brighter during the reference measurement, then a bit dimmer during the sample measurement, the measurement will be distorted. More elaborate methods, such as a "two-beam" setup (see figure), can correct for these types of effects to give very accurate results. The Standard addition method can be used to statistically cancel these errors.
FTIR.
Fourier transform infrared (FTIR) spectroscopy is a measurement technique that allows one to record infrared spectra. Infrared light is guided through an interferometer and then through the sample (or vice versa). A moving mirror inside the apparatus alters the distribution of infrared light that passes through the interferometer. The signal directly recorded, called an "interferogram", represents light output as a function of mirror position. A data-processing technique called Fourier transform turns this raw data into the desired result (the sample's spectrum): Light output as a function of infrared wavelength (or equivalently, wavenumber). As described above, the sample's spectrum is always compared to a reference.
An alternate method for acquiring spectra is the "dispersive" or "scanning monochromator" method. In this approach, the sample is irradiated sequentially with various single wavelengths. The dispersive method is more common in UV-Vis spectroscopy, but is less practical in the infrared than the FTIR method. One reason that FTIR is favored is called "Fellgett's advantage" or the "multiplex advantage": The information at all frequencies is collected simultaneously, improving both speed and signal-to-noise ratio. Another is called "Jacquinot's Throughput Advantage": A dispersive measurement requires detecting much lower light levels than an FTIR measurement. There are other advantages, as well as some disadvantages, but virtually all modern infrared spectrometers are FTIR instruments.
Absorption bands.
IR spectroscopy is often used to identify structures because functional groups give rise to characteristic bands both in terms of intensity and position (frequency). The positions of these bands is summarized in correlation tables as shown below.
Wavenumbers listed in cm−1.
Badger's rule.
For many kinds of samples, the assignments are known, i.e. which bond deformation(s) are associated with which frequency. In such cases further information can be gleaned about the strength on a bond, relying on the empirical guideline called Badger's Rule. Originally published by Richard Badger in 1934, this rule states that the strength of a bond correlates with the frequency of its vibrational mode. That is, increase in bond strength leads to corresponding frequency increase and vice versa.
Uses and applications.
Infrared spectroscopy is a simple and reliable technique widely used in both organic and inorganic chemistry, in research and industry. It is used in quality control, dynamic measurement, and monitoring applications such as the long-term unattended measurement of CO2 concentrations in greenhouses and growth chambers by infrared gas analyzers.
It is also used in forensic analysis in both criminal and civil cases, for example in identifying polymer degradation. It can be used in determining the blood alcohol content of a suspected drunk driver.
IR-spectroscopy has been successfully used in analysis and identification of pigments in paintings and other art objects such as illuminated manuscripts.
A useful way of analyzing solid samples without the need for cutting samples uses ATR or attenuated total reflectance spectroscopy. Using this approach, samples are pressed against the face of a single crystal. The infrared radiation passes through the crystal and only interacts with the sample at the interface between the two materials.
With increasing technology in computer filtering and manipulation of the results, samples in solution can now be measured accurately (water produces a broad absorbance across the range of interest, and thus renders the spectra unreadable without this computer treatment).
Some instruments will also automatically tell you what substance is being measured from a store of thousands of reference spectra held in storage.
Infrared spectroscopy is also useful in measuring the degree of polymerization in polymer manufacture. Changes in the character or quantity of a particular bond are assessed by measuring at a specific frequency over time. Modern research instruments can take infrared measurements across the range of interest as frequently as 32 times a second. This can be done whilst simultaneous measurements are made using other techniques. This makes the observations of chemical reactions and processes quicker and more accurate.
Infrared spectroscopy has also been successfully utilized in the field of semiconductor microelectronics: for example, infrared spectroscopy can be applied to semiconductors like silicon, gallium arsenide, gallium nitride, zinc selenide, amorphous silicon, silicon nitride, etc.
Another important application of Infrared Spectroscopy is in the food industry to measure the concentration of various compounds in different food products
The instruments are now small, and can be transported, even for use in field trials.
Infrared Spectroscopy is also used in gas leak detection devices such as the DP-IR and EyeCGAs. These devices detect hydrocarbon gas leaks in the transportation of natural gas and crude oil.
In February 2014, NASA announced a greatly upgraded database, based on IR spectroscopy, for tracking polycyclic aromatic hydrocarbons (PAHs) in the universe. According to scientists, more than 20% of the carbon in the universe may be associated with PAHs, possible starting materials for the formation of life. PAHs seem to have been formed shortly after the Big Bang, are widespread throughout the universe, and are associated with new stars and exoplanets.
Isotope effects.
The different isotopes in a particular species may exhibit different fine details in infrared spectroscopy. For example, the O–O stretching frequency (in reciprocal centimeters) of oxyhemocyanin is experimentally determined to be 832 and 788 cm−1 for ν(16O–16O) and ν(18O–18O), respectively.
By considering the O–O bond as a spring, the wavenumber of absorbance, ν can be calculated:
where "k" is the spring constant for the bond, "c" is the speed of light, and "μ" is the reduced mass of the A–B system:
(formula_3 is the mass of atom formula_4).
The reduced masses for 16O–16O and 18O–18O can be approximated as 8 and 9 respectively. Thus
Where formula_6 is the wavenumber; = frequency/(speed of light)
The effect of isotopes, both on the vibration and the decay dynamics, has been found to be stronger than previously thought. In some systems, such as silicon and germanium, the decay of the anti-symmetric stretch mode of interstitial oxygen involves the symmetric stretch mode with a strong isotope dependence. For example, it was shown that for a natural silicon sample, the lifetime of the anti-symmetric vibration is 11.4 ps. When the isotope of one of the silicon atoms is increased to 29Si, the lifetime increases to 19 ps. In similar manner, when the silicon atom is changed to 30Si, the lifetime becomes 27 ps.
Two-dimensional IR.
Two-dimensional infrared correlation spectroscopy analysis combines multiple samples of infrared spectra to reveal more complex properties. By extending the spectral information of a perturbed sample, spectral analysis is simplified and resolution is enhanced. The 2D synchronous and 2D asynchronous spectra represent a graphical overview of the spectral changes due to a perturbation (such as a changing concentration or changing temperature) as well as the relationship between the spectral changes at two different wavenumbers.
Nonlinear two-dimensional infrared spectroscopy is the infrared version of correlation spectroscopy. Nonlinear two-dimensional infrared spectroscopy is a technique that has become available with the development of femtosecond infrared laser pulses. In this experiment, first a set of pump pulses is applied to the sample. This is followed by a waiting time during which the system is allowed to relax. The typical waiting time lasts from zero to several picoseconds, and the duration can be controlled with a resolution of tens of femtoseconds. A probe pulse is then applied, resulting in the emission of a signal from the sample. The nonlinear two-dimensional infrared spectrum is a two-dimensional correlation plot of the frequency ω1 that was excited by the initial pump pulses and the frequency ω3 excited by the probe pulse after the waiting time. This allows the observation of coupling between different vibrational modes; because of its extremely fine time resolution, it can be used to monitor molecular dynamics on a picosecond timescale. It is still a largely unexplored technique and is becoming increasingly popular for fundamental research.
As with two-dimensional nuclear magnetic resonance (2DNMR) spectroscopy, this technique spreads the spectrum in two dimensions and allows for the observation of cross peaks that contain information on the coupling between different modes. In contrast to 2DNMR, nonlinear two-dimensional infrared spectroscopy also involves the excitation to overtones. These excitations result in excited state absorption peaks located below the diagonal and cross peaks. In 2DNMR, two distinct techniques, COSY and NOESY, are frequently used. The cross peaks in the first are related to the scalar coupling, while in the latter they are related to the spin transfer between different nuclei. In nonlinear two-dimensional infrared spectroscopy, analogs have been drawn to these 2DNMR techniques. Nonlinear two-dimensional infrared spectroscopy with zero waiting time corresponds to COSY, and nonlinear two-dimensional infrared spectroscopy with finite waiting time allowing vibrational population transfer corresponds to NOESY. The COSY variant of nonlinear two-dimensional infrared spectroscopy has been used for determination of the secondary structure content of proteins.

</doc>
