<doc id="15414" url="https://en.wikipedia.org/wiki?curid=15414" title="Irenaeus">
Irenaeus

Irenaeus (; Greek: ) (early 2nd century – c. AD 202), also referred to as Saint Irenaeus, was Bishop of Lugdunum in Gaul, then a part of the Roman Empire (now Lyon, France). He was an early Church Father and apologist, and his writings were formative in the early development of Christian theology. A resident of Smyrna, he had listened to the preaching of St. Polycarp. Polycarp is traditionally considered a disciple of John the Evangelist.
Irenaeus' best-known book, "Adversus Haereses" or "Against Heresies" (c. 180), is a detailed attack on Gnosticism, which was then a serious threat to the Church, and especially on the system of the Gnostic Valentinus. As one of the first great Christian theologians, he emphasized the traditional elements in the Church, especially the episcopate, Scripture, and tradition. Against the Gnostics, who said that they possessed a secret oral tradition from Jesus himself, Irenaeus maintained that the bishops in different cities are known as far back as the Apostles and that the bishops provided the only safe guide to the interpretation of Scripture. His polemical work is credited for laying out the "orthodoxies of the Christian church, its faith, its preaching and the books that it held as sacred authority." His writings, with those of Clement and Ignatius, are taken as among the earliest signs of the developing doctrine of the primacy of the Roman see. Irenaeus is the earliest witness to recognition of the canonical character of all four gospels
Irenaeus is recognized as a saint in both Roman Catholicism and Eastern Orthodoxy. His feast day is on June 28 in the General Roman Calendar, where it was inserted for the first time in 1920; in 1960 the Catholic Church transferred it to July 3, leaving June 28 for the Vigil of the Feast of Saints Peter and Paul, but in 1969 it was returned to June 28, the day of his death. The Lutheran Church commemorates Irenaeus on that same date for his life of exemplary Christian witness. In the Orthodox Church his feast day is 23 August.
Biography.
During the persecution of Marcus Aurelius, the Roman Emperor from 161–180, Irenaeus was a priest of the Church of Lyon. The clergy of that city, many of whom were suffering imprisonment for the faith, sent him in 177 to Rome with a letter to Pope Eleuterus concerning the heresy Montanism, and that occasion bore emphatic testimony to his merits. While Irenaeus was in Rome, a massacre took place in Lyon. Returning to Gaul, Irenaeus succeeded the martyr Saint Pothinus and became the second Bishop of Lyon.
During the religious peace which followed the persecution of Marcus Aurelius, the new bishop divided his activities between the duties of a pastor and of a missionary (as to which we have but brief data, late and not very certain). Almost all his writings were directed against Gnosticism. The most famous of these writings is "Adversus haereses" ("Against Heresies"). Irenaeus alludes to coming across Gnostic writings, and holding conversations with Gnostics, and this may have taken place in Asia Minor or in Rome. However, it also appears that Gnosticism was present near Lyon: he writes that there were followers of 'Magus the Magician' living and teaching in the Rhone valley.
Little is known about the career of Irenaeus after he became bishop. The last action reported of him (by Eusebius, 150 years later) is that in 190 or 191, he exerted influence on Pope Victor I not to excommunicate the Christian communities of Asia Minor which persevered in the practice of the Quartodeciman celebration of Easter.
Nothing is known of the date of his death, which must have occurred at the end of the 2nd or the beginning of the 3rd century. A few within the Roman Catholic Church and Orthodox Church celebrate him as a martyr. He was buried under the Church of Saint John in Lyon, which was later renamed St Irenaeus in his honour. The tomb and his remains were utterly destroyed in 1562 by the Huguenots.
Writings.
Irenaeus wrote a number of books, but the most important that survives is the "Against Heresies" (or, in its Latin title, "Adversus Haereses"). In Book I, Irenaeus talks about the Valentinian Gnostics and their predecessors, who go as far back as the magician Simon Magus. In Book II he attempts to provide proof that Valentinianism contains no merit in terms of its doctrines. In Book III Irenaeus purports to show that these doctrines are false, by providing counter-evidence gleaned from the Gospels. Book IV consists of Jesus' sayings, and here Irenaeus also stresses the unity of the Old Testament and the Gospel. In the final volume, Book V, Irenaeus focuses on more sayings of Jesus plus the letters of Paul the Apostle.
Until the discovery of the Library of Nag Hammadi in 1945, "Against Heresies" was the best-surviving description of Gnosticism. According to some biblical scholars, the findings at Nag Hammadi have shown Irenaeus' description of Gnosticism to be largely inaccurate and polemic in nature. Though correct in some details about the belief systems of various groups, Irenaeus' main purpose was to warn Christians against Gnosticism, rather than catalog those beliefs. He described Gnostic groups as sexual libertines, for example, when some of their own writings advocated chastity more strongly than did orthodox texts—yet the gnostic texts cannot be taken as guides to their actual practices, about which almost nothing is reliably known today. In any case the gnostics were not a single group, but a wide array of sects. Some groups were indeed libertine because they considered bodily existence meaningless; others praise chastity, and strongly prohibited any sexual activity, even within marriage. Rodney Stark asserts that it is the same Nag Hammadi library that proves Irenaeus right.
Irenaeus also wrote "The Demonstration of the Apostolic Preaching" (also known as "Proof of the Apostolic Preaching"), an Armenian copy of which was discovered in 1904. This work seems to have been an instruction for recent Christian converts.
Eusebius attests to other works by Irenaeus, today lost, including "On the Ogdoad," an untitled letter to Blastus regarding schism, "On the Subject of Knowledge", "On the Monarchy" or "How God is not the Cause of Evil".
Irenaeus exercised wide influence on the generation which followed. Both Hippolytus and Tertullian freely drew on his writings. However, none of his works aside from "Against Heresies" and "The Demonstration of the Apostolic Preaching" survive today, perhaps because his literal hope of an earthly millennium may have made him uncongenial reading in the Greek East. Even though no complete version of "Against Heresies" in its original Greek exists, we possess the full ancient Latin version, probably of the third century, as well as thirty-three fragments of a Syrian version and a complete Armenian version of books 4 and 5.
Irenaeus' works were first translated into English by John Keble and published in 1872 as part of the Library of the Fathers series.
Scripture.
Irenaeus pointed to Scripture as a proof of orthodox Christianity against heresies, classifying as Scripture not only the Old Testament but most of the books now known as the New Testament, while excluding many works, a large number by Gnostics, that flourished in the 2nd century and claimed scriptural authority. Oftentimes, Irenaeus, as a student of Polycarp, who was a direct disciple of the Apostle John, believed that he was interpreting scriptures in the same hermeneutic as the Apostles. This connection to Christ was important to Irenaeus because both he and the Gnostics based their arguments on Scripture. Irenaeus argued that since he could trace his authority to Christ and the Gnostics could not, his interpretation of Scripture was correct. He also used "the Rule of Faith", a "proto-creed" with similarities to the Apostles' Creed, as a hermeneutical key to argue that his interpretation of Scripture was correct.
Before Irenaeus, Christians differed as to which gospel they preferred. The Christians of Asia Minor preferred the Gospel of John. The Gospel of Matthew was the most popular overall. Irenaeus asserted that four Gospels, Matthew, Mark, Luke, and John, were canonical scripture. Thus Irenaeus provides the earliest witness to the assertion of the four canonical Gospels, possibly in reaction to Marcion's edited version of the Gospel of Luke, which Marcion asserted was the one and only true gospel.
Based on the arguments Irenaeus made in support of only four authentic gospels, some interpreters deduce that the "fourfold Gospel" must have still been a novelty in Irenaeus' time. "Against Heresies" 3.11.7 acknowledges that many heterodox Christians use only one gospel while 3.11.9 acknowledges that some use more than four. The success of Tatian's Diatessaron in about the same time period is "... a powerful indication that the fourfold Gospel contemporaneously sponsored by Irenaeus was not broadly, let alone universally, recognized."
Irenaeus is also the earliest attestation that the Gospel of John was written by John the Apostle, and that the Gospel of Luke was written by Luke, the companion of Paul.
The apologist and ascetic Tatian had previously harmonized the four gospels into a single narrative, the "Diatesseron" ("c" 150–160).
Scholars contend that Irenaeus quotes from 21 of the 27 New Testament Texts:
Matthew ("Book 3, Chapter 16")
Mark ("Book 3, Chapter 10")
Luke ("Book 3, Chapter 14")
John ("Book 3, Chapter 11")
Acts of the Apostles ("Book 3, Chapter 14")
Romans ("Book 3, Chapter 16")
1 Corinthians ("Book 1, Chapter 3")
2 Corinthians ("Book 3, Chapter 7")
Galatians ("Book 3, Chapter 22")
Ephesians ("Book 5, Chapter 2")
Philippians ("Book 4, Chapter 18")
Colossians ("Book 1, Chapter 3")
1 Thessalonians ("Book 5, Chapter 6")
2 Thessalonians ("Book 5, Chapter 25")
1 Timothy ("Book 1, Preface")
2 Timothy ("Book 3, Chapter 14")
Titus ("Book 3, Chapter 3")
1 Peter ("Book 4, Chapter 9")
1 John ("Book 3, Chapter 16")
2 John ("Book 1, Chapter 16")
Revelation to John ("Book 4, Chapter 20")
He may refer to Hebrews ("Book 2, Chapter 30") and James ("Book 4, Chapter 16") and maybe even 2 Peter ("Book 5, Chapter 28") but does not cite Philemon, 3 John or Jude.
Irenaeus cited the New Testament approximately 1000 times. About one third of his citations are made to Paul's letters. Irenaeus considered all 13 letters belonging to the Pauline corpus to have been written by Paul himself.
Apostolic authority.
Irenaeus is also known as one of the first theologians to use the principle of apostolic succession to refute his opponents.
In his writing against the Gnostics, who claimed to possess a secret oral tradition from Jesus himself, Irenaeus maintained that the bishops in different cities are known as far back as the Apostles and that the bishops provided the only safe guide to the interpretation of Scripture. In a passage that became a "locus classicus" of Catholic-Protestant polemics, he cited the Roman church as an example of the unbroken chain of authority which text Western polemics would use to assert the primacy of Rome over Eastern churches by virtue of its "preeminent authority".
With the lists of bishops to which Irenaeus referred, the doctrine of the apostolic succession, firmly established in the Church at this time, of the bishops could be linked. This succession was important to establish a chain of custody for orthodoxy. He felt it important, however, to also speak of a succession of elders (presbyters).
Irenaeus' point when refuting the Gnostics was that all of the Apostolic churches had preserved the same traditions and teachings in many independent streams. It was the unanimous agreement between these many independent streams of transmission that proved the orthodox Faith, current in those churches, to be true. 
Irenaeus' theology and contrast with Gnosticism.
The central point of Irenaeus' theology is the unity and the goodness of God, in opposition to the Gnostics' theory of God; a number of divine emanations (Aeons) along with a distinction between the Monad and the Demiurge. Irenaeus uses the Logos theology he inherited from Justin Martyr. Irenaeus was a student of Polycarp, who was said to have been tutored by John the Apostle. (John had used Logos terminology in the Gospel of John and the letter of 1 John). Irenaeus prefers to speak of the Son and the Spirit as the "hands of God".
The Unity of Salvation History.
Irenaeus' emphasis on the unity of God is reflected in his corresponding emphasis on the unity of salvation history. Irenaeus repeatedly insists that God began the world and has been overseeing it ever since this creative act; everything that has happened is part of his plan for humanity. The essence of this plan is a process of maturation: Irenaeus believes that humanity was created immature, and God intended his creatures to take a long time to grow into or assume the divine likeness.
Everything that has happened since has therefore been planned by God to help humanity overcome this initial mishap and achieve spiritual maturity. The world has been intentionally designed by God as a difficult place, where human beings are forced to make moral decisions, as only in this way can they mature as moral agents. Irenaeus likens death to the big fish that swallowed Jonah: it was only in the depths of the whale's belly that Jonah could turn to God and act according to the divine will. Similarly, death and suffering appear as evils, but without them we could never come to know God.
According to Irenaeus, the high point in salvation history is the advent of Jesus. For Irenaeus, the Incarnation of Christ was intended by God before He determined that humanity would be created. Irenaeus develops this idea based on Rom. 5:14, saying "Forinasmuch as He had a pre-existence as a saving Being, it was necessary that what might be saved should also be called into existence, in order that the Being who saves should not exist in vain." Some theologians maintain that Irenaeus believed that Incarnation would have occurred even if humanity had never sinned; but the fact that they did sin determined his role as the savior.
Irenaeus sees Christ as the new Adam, who systematically "undoes" what Adam did: thus, where Adam was disobedient concerning God's edict concerning the fruit of the Tree of Knowledge of Good and Evil, Christ was obedient even to death on the wood of a tree. Irenaeus is the first to draw comparisons between Eve and Mary, contrasting the faithlessness of the former with the faithfulness of the latter. In addition to reversing the wrongs done by Adam, Irenaeus thinks of Christ as "recapitulating" or "summing up" human life.
Irenaeus conceives of our salvation as essentially coming about through the incarnation of God as a man. He characterizes the penalty for sin as death and corruption. God, however, is immortal and incorruptible, and simply by becoming united to human nature in Christ he conveys those qualities to us: they spread, as it were, like a benign infection. Irenaeus emphasizes that salvation occurs through Christ's His Incarnation, which bestows incorruptibility on humanity, rather than emphasizing His Redemptive death in the crucifixion, although the latter event is an integral part of the former.
Christ's Life.
Part of the process of recapitulation is for Christ to go through every stage of human life, from infancy to old age, and simply by living it, sanctify it with his divinity. Although it is sometimes claimed that Irenaeus believed Christ did not die until he was older than is conventionally portrayed, the bishop of Lyons simply pointed out that because Jesus turned the permissible age for becoming a rabbi (30 years old and above), he recapitulated and sanctified the period between 30 and 50 years old, as per the Jewish custom of periodization of human life, and so touches the beginning of old age when one becomes 50 years old. (see Adversus Haereses, book II, chapter 22).
In the passage of "Adversus Haereses" under consideration, Irenaeus is clear that after receiving baptism at the age of thirty, citing Luke 3:23, Gnostics then falsely assert that "He preached only one year reckoning from His baptism," and also, "On completing His thirtieth year He [Jesus suffered, being in fact still a young man, and who had by no means attained to advanced age." Irenaeus argues against the Gnostics by using scripture to show that Jesus lives at least several years after his baptism by referencing 3 distinctly separate visits to Jerusalem. The first is when Jesus makes wine out of water, He goes up to the Paschal feast-day, after which He withdraws and is found in Samaria. The second is when Jesus goes up to Jerusalem for Passover and cures the paralytic, after which He withdraws over the sea of Tiberias. The third mention is when He travels to Jerusalem, eats the Passover, and suffers on the following day.
Irenaeus quotes scripture, which we reference as John 8:57, to suggest that Jesus ministers while in his 40's. In this passage, Jesus' opponents want to argue that Jesus has not seen Abraham, because Jesus is too young. Jesus' opponents argue that Jesus is not yet 50 years old. Irenaeus argues that if Jesus was in his thirties, his opponents would've argued that He's not yet 40 years, since that would make Him even younger. Irenaeus' argument is that they would not weaken their own argument by adding years to Jesus' age. Irenaeus also writes that "The Elders witness to this, who in Asia conferred with John the Lord's disciple, to the effect that John had delivered these things unto them: for he abode with them until the times of Trajan. And some of them saw not only John, but others also of the Apostles, and had this same account from them, and witness to the aforesaid relation."
In Demonstration (74) Irenaeus notes "For Pontius Pilate was governor of Judæa, and he had at that time resentful enmity against Herod the king of the Jews. But then, when Christ was brought to him bound, Pilate sent Him to Herod, giving command to enquire of him, that he might know of a certainty what he should desire concerning Him; making Christ a convenient occasion of reconciliation with the king." Pilate was the prefect of the Roman province of Judaea from AD 26–36. He served under Emperor Tiberius Claudius Nero. Herod Antipas was tetrarch of Galilee and Perea, a client state of the Roman Empire. He ruled from 4 BC to 39 AD. In refuting Gnostic claims that Jesus preached for only one year after his baptism, Irenaeus used the "recapitulation" approach to demonstrate that by living beyond the age of thirty Christ sanctified even old age. 
Irenaeus' use of Paul's Epistles.
Many aspects of Irenaeus' presentation of salvation history depend on Paul's Epistles.
Irenaeus’ conception of salvation relies heavily on the understanding found in Paul’s letters. Irenaeus first brings up the theme of victory over sin and evil that is afforded by Jesus’s death. God’s intervention has saved humanity from the Fall of Adam and the wickedness of Satan. Human nature has become joined with God’s in the person of Jesus, thus allowing human nature to have victory over sin. Paul writes on the same theme, that Christ has come so that a new order is formed, and being under the Law, is being under the sin of Adam Rom. 6:14, Gal. 5:18.
Reconciliation is also a theme of Paul’s that Irenaeus stresses in his teachings on Salvation. Irenaeus believes Jesus coming in flesh and blood sanctified humanity so that it might again reflect the perfection associated with the likeness of the Divine. This perfection leads to a new life, in the lineage of God, which is forever striving for eternal life and unity with the Father. This is a carryover from Paul, who attributes this reconciliation to the actions of Christ: “For since death came through a human being, the resurrection of the dead has also come through a human being; for as all die in Adam, so all will be made alive in Christ” 1 Cor. 15:21-2.
A third theme in both Paul’s and Irenaeus’s conceptions of salvation is the sacrifice of Christ being necessary for the new life given to humanity in the triumph over evil. It is in this obedient sacrifice that Jesus is victor and reconciler, thus erasing the marks that Adam left on human nature. To argue against the Gnostics on this point, Irenaeus uses Colossians Col. 2:13-4 in showing that the debt which came by a tree has been paid for us in another tree. Furthermore, the first chapter of Ephesians is picked up in Irenaeus's discussion of the topic when he asserts, “By His own blood He redeemed us, as also His apostle declares, and ‘In whom we have redemption through His blood, even the remission of sins.’"
Irenaeus does not simply parrot back the message of Paul in his understanding of salvation. One of the major changes that Irenaeus makes is when the Parousia will occur. Paul states that he believes that it was going to happen soon, probably in his own life time 1 Thess. 4:15 1 Cor. 15:51-2. However, the end times does not happen immediately and Christians begin to worry and have doubts about the faith. For Irenaeus, sin is seen as haste, just as Adam and Eve quickly ate from the tree of knowledge as they pleased. On the other hand, redemption restored to humanity through the Christ's submission to God’s will. Thus, the salvation of man will also be restored to the original trajectory controlled by God forfeited in humanity's sinful in haste. This rather slower version of salvation is not something that Irenaeus received from Paul, but was a necessary construct given the delay of the second coming of Jesus.
Christ as the New Adam.
To counter his Gnostic opponents, Irenaeus significantly develops Paul's presentation of Christ as the Last Adam.
Irenaeus' presentation of Christ as the New Adam is based on Paul's Christ-Adam parallel in Romans 5:12-21. Irenaeus uses this parallel to demonstrate that Christ truly took human flesh. Irenaeus consideres it important to emphasize this point because he understands the failure to recognize Christ's full humanity the bond linking the various strains of Gnosticism together, as seen in his statement that "according to the opinion of no one of the heretics was the Word of God made flesh." Irenaeus believes that unless the Word became flesh, humans were not fully redeemed. He explains that by becoming man, Christ restored humanity to being in the image and likeness of God, which they had lost in the Fall of man Just as Adam was the original head of humanity through whom all sinned, Christ is the new head of humanity who fulfills Adam's role in the Economy of Salvation. Irenaeus calls this process of restoring humanity recapitulation.
For Irenaeus, Paul's presentation of the Old Law (the Mosaic covenant) in this passage indicates that the Old Law revealed humanity's sinfulness but could not save them. He explains that "For as the law was spiritual, it merely made sin to stand out in relief, but did not destroy it. For sin had no dominion over the spirit, but over man." Since humans have a physical nature, they cannot be saved by a spiritual law. Instead, they need a human Savior. This is why it was necessary for Christ to take human flesh. Irenaeus summarizes how Christ's taking human flesh saves humanity with a statement that closely resembles Romans 5:19, "For as by the disobedience of the one man who was originally moulded from virgin soil, the many were made sinners, and forfeited life; so was it necessary that, by the obedience of one man, who was originally born from a virgin, many should be justified and receive salvation." The physical creation of Adam and Christ is emphasized by Irenaeus to demonstrate how the Incarnation saves humanity's physical nature.
Irenaeus emphasizes the importance of Christ's reversal of Adams's action. Through His obedience, Christ undoes Adam's disobedience. Irenaeus presents the Passion as the climax of Christ's obedience, emphasizing how this obedience on the tree of the Cross Phil. 2:8 undoes the disobedience that occurred through a tree Gen. 3:17. 
Irenaeus' interpretation of Paul's discussion of Christ as the New Adam is significant because it helped develop the Recapitulation theory of atonement. Irenaeus emphasizes that it is through Christ's reversal of Adam's action that humanity is saved, rather than considering the Redemption to occur in a cultic or juridical way.
Valentinian Gnosticism.
Valentinian Gnosticism was one of the major forms of Gnosticism that Irenaeus opposed.
According to the Gnostic view of Salvation, creation was perfect to begin with; it did not need time to grow and mature. For the Valentinians, the material world is the result of the loss of perfection which resulted from Sophia's desire to understand the Forefather. Therefore, one is ultimately redeemed, through secret knowledge, to enter the pleroma of which the Achamoth originally fell.
According to the Valentinian Gnostics, there are three classes of human beings. They are the material, who cannot attain salvation; the psychic, who are strengthened by works and faith (they are part of the church); and the spiritual, who cannot decay or be harmed by material actions. 
Essentially, ordinary humans—those who have faith but do not possess the special knowledge—will not attain salvation. Spirituals, on the other hand—those who obtain this great gift—are the only class that will eventually attain salvation.
In his article entitled ""The Demiurge,"" J.P. Arendzen sums up the Valentinian view of the salvation of man. He writes, "The first, or carnal men, will return to the grossness of matter and finally be consumed by fire; the second, or psychic men, together with the Demiurge as their master, will enter a middle state, neither heaven (pleroma) nor hell (whyle); the purely spiritual men will be completely freed from the influence of the Demiurge and together with the Saviour and Achamoth, his spouse, will enter the pleroma divested of body (húle) and soul (psuché)."
In this understanding of salvation, the purpose of the Incarnation was to redeem the Spirituals from their material bodies. By taking a material body, the Son becomes the Savior and facilitates this entrance into the pleroma by making it possible for the Spirituals to receive his spiritual body. However, in becoming a body and soul, the Son Himself becomes one of those needing redemption. Therefore, the Word descends onto the Savior at His Baptism in the Jordan, which liberates the Son from his corruptible body and soul. His redemption from the body and soul is then applied to the Spirituals. In response to this Gnostic view of Christ, Irenaeus emphasized that the Word became flesh and developed a soteriology that emphasized the significance of Christ's material Body in saving humanity, as discussed in the sections above.
In his criticism of Gnosticism, Irenaeus made reference to a Gnostic gospel which portrayed Judas in a positive light, as having acted in accordance with Jesus' instructions. The recently discovered Gospel of Judas dates close to the period when Irenaeus lived (late 2nd century), and scholars typically regard this work as one of many Gnostic texts, showing one of many varieties of Gnostic beliefs of the period.
Irenaeus' Mariology.
Irenaeus of Lyons is perhaps the earliest of the Church Fathers to develop a thorough mariology. It is certain that, while still very young, Irenaeus had seen and heard Bishop Polycarp (d. 155) at Smyrna. Irenaeus sets out a forthright account of Mary's role in the economy of salvation, presenting Mary as New Eve whose obedience in the Annunciation counters Eve's disobedience. He states, "even though Eve had Adam for a husband, she was still a virgin... By disobeying, Eve became the cause of death for herself and for the whole human race. In the same way Mary, though she had a husband, was still a virgin, and by obeying, she became the cause of salvation for herself and for the whole human race.
This presentation of Mary as the New Eve is an extension of Irenaeus' Adam-Christ typology. Just as Christ undoes Adam's disobedience, Mary undoes Eve's disobedience. His emphasis on the role of Mary helps Irenaeus counter Christologies along the lines of Docetism and Adoptionism. His emphasis on Mary's role in the economy of salvation further demonstrates how God transforms the material world through the Incarnation, which was an important part of Irenaeus' conflict with the Gnostics.
Like Ireneaus, Tertullian describes how Christ's Virgin birth parallels Adam's creation from virgin earth. Tertullian also discusses how it was necessary for God to be born of a Virgin so that what was lost through a woman would be saved through a woman. This indicates that the concept of Mary as the New Eve was known in both the Eastern and Western Church during the second and third centuries.
Pope Pius IX made reference to this theme of Irenaeus, in the 1854 apostolic constitution "Ineffabilis Deus", which defined the dogma of the Immaculate Conception.
Prophetic exegesis.
The first four books of "Against Heresies" constitute a minute analysis and refutation of the Gnostic doctrines. The fifth is a statement of positive belief contrasting the constantly shifting and contradictory Gnostic opinions with the steadfast faith of the church. He appeals to the prophecies to demonstrate the truthfulness of Christianity.
Rome and Ten Horns.
Irenaeus shows the close relationship between the predicted events of Daniel 2 and 7. Rome, the fourth prophetic kingdom, would end in a tenfold partition. The ten divisions of the empire are the "ten horns" of Daniel 7 and the "ten horns" in Revelation 17. A "little horn," which is to supplant three of Rome's ten divisions, is also the still future "eighth" in Revelation. Irenaeus climaxes with the destruction of all kingdoms at the Second Advent, when Christ, the prophesied "stone," cut out of the mountain without hands, smites the image after Rome's division.
Antichrist.
Irenaeus identified the Antichrist, another name of the apostate Man of Sin, with Daniel's Little Horn and John's Beast of Revelation 13. He sought to apply other expressions to the Antichrist, such as "the abomination of desolation," mentioned by Christ (Matt. 24:15) and the "king of a most fierce countenance," in Gabriel's explanation of the Little Horn of Daniel 8. But he is not very clear how "the sacrifice and the libation shall be taken away" during the "half-week," or three and one-half years of the Antichrist's reign.
Under the notion that the Antichrist, as a single individual, might be of Jewish origin, he fancies that the mention of "Dan," in Jeremiah 8:16, and the omission of that name from those tribes listed in Revelation 7, might indicate the Antichrist's tribe. This surmise became the foundation of a series of subsequent interpretations by others.
Time, Times and Half a Time.
Like the other early church fathers, Irenaeus interpreted the three and one-half "times" of the Little Horn of Daniel 7 as three and one-half literal years. Antichrist's three and a half years of sitting in the temple are placed immediately before the Second Coming of Christ.
They are identified as the second half of the "one week" of Daniel 9. Irenaeus says nothing of the seventy weeks; we do not know whether he placed the "one week" at the end of the seventy or whether he had a gap.
666.
Irenaeus is the first of the church fathers to consider the mystic number 666. While Irenaeus did propose some solutions of this numerical riddle, his interpretation was quite reserved. Thus, he cautiously states:
Although Irenaeus did speculate upon three names to symbolize this mystical number, namely Euanthas, Teitan, and Lateinos, nevertheless he was content to believe that the Antichrist would arise some time in the future after the fall of Rome and then the meaning of the number would be revealed.
Millennium.
Irenaeus declares that the Antichrist's future three-and-a-half-year reign, when he sits in the temple at Jerusalem, will be terminated by the second advent, with the resurrection of the just, the destruction of the wicked, and the millennial reign of the righteous. The general resurrection and the judgment follow the descent of the New Jerusalem at the end of the millennial kingdom.
Irenaeus calls those "heretics" who maintain that the saved are immediately glorified in the kingdom to come after death, before their resurrection. He avers that the millennial kingdom and the resurrection are actualities, not allegories, the first resurrection introducing this promised kingdom in which the risen saints are described as ruling over the renewed earth during the millennium, between the two resurrections.
Irenaeus held to the old Jewish tradition that the first six days of creation week were typical of the first six thousand years of human history, with Antichrist manifesting himself in the sixth period. And he expected the millennial kingdom to begin with the second coming of Christ to destroy the wicked and inaugurate, for the righteous, the reign of the kingdom of God during the seventh thousand years, the millennial Sabbath, as signified by the Sabbath of creation week.
In common with many of the fathers, Irenaeus did not distinguish between the new earth re-created in its eternal state—the thousand years of Revelation 20—when the saints are with Christ after His second advent, and the Jewish traditions of the Messianic kingdom. Hence, he applies Biblical and traditional ideas to his descriptions of this earth during the millennium, throughout the closing chapters of Book 5. This conception of the reign of resurrected and translated saints with Christ on this earth during the millennium-popularly known as chiliasm—was the increasingly prevailing belief of this time. Incipient distortions due to the admixture of current traditions, which figure in the extreme forms of chiliasm, caused a reaction against the earlier interpretations of Bible prophecies.
Irenaeus was not looking for a Jewish kingdom. He interpreted Israel as the Christian church, the spiritual seed of Abraham.
At times his expressions are highly fanciful. He tells, for instance, of a prodigious fertility of this earth during the millennium, after the resurrection of the righteous, "when also the creation, having been renovated and set free, shall fructify with an abundance of all kinds of food." In this connection, he attributes to Christ the saying about the vine with ten thousand branches, and the ear of wheat with ten thousand grains, and so forth, which he quotes from Papias of Hierapolis.
Exegesis.
Irenaeus' exegesis does not give complete coverage. On the seals, for example, he merely alludes to Christ as the rider on the white horse. He stresses five factors with greater clarity and emphasis than Justin:

</doc>
<doc id="15416" url="https://en.wikipedia.org/wiki?curid=15416" title="Involuntary commitment">
Involuntary commitment

Involuntary commitment or civil commitment (also known as sectioning in some jurisdictions) is a legal process through which an individual with symptoms of severe mental illness is court-ordered into treatment in a psychiatric hospital (inpatient) or in the community (outpatient).
Criteria for civil commitment are established by laws, which vary between nations. Commitment proceedings often follow a period of emergency hospitalization, during which an individual with acute psychiatric symptoms is confined for a relatively short duration (e.g. 72 hours) in a treatment facility for evaluation and stabilization by mental health professionals—who may then determine whether further civil commitment is appropriate or necessary. If civil commitment proceedings follow, then the evaluation is presented in a formal court hearing where testimony and other evidence may also be submitted. The subject of the hearing is typically entitled to legal counsel and may challenge a commitment order through habeas corpus rules.
Historically, until the first third of the twentieth century or later in most jurisdictions, all committals to public psychiatric facilities and most committals to private ones were involuntary. Since then, there have been alternating trends towards the abolition or substantial reduction of involuntary commitment, a trend known as "deinstitutionalisation."
Purpose.
In most jurisdictions, involuntary commitment is specifically applied to individuals found to be suffering from a mental illness that impairs their reasoning ability to such an extent that the laws, state, or courts find that decisions must or should be made for them under a legal framework. (In some jurisdictions, this is a distinct proceeding from being "found incompetent.")
Involuntary commitment is used to some degree for each of the following headings although different jurisdictions have different criteria. Some jurisdictions limit court-ordered treatment to individuals who meet statutory criteria for presenting a danger "to self or others." Other jurisdictions have broader criteria.
First aid.
Training is gradually becoming available in mental health first aid to equip community members such as teachers, school administrators, police officers, and medical workers in recognizing and managing situations where evaluations of behavior might be appropriate. The extension of first aid training to cover mental health problems and crises is a quite recent development. A mental health first aid training course was developed in Australia in 2001 and has been found to improve assistance provided to persons with a mental illness or in a mental health crisis. This form of training has now spread to a number of other countries (Canada, Finland, Hong Kong, Ireland, Singapore, Scotland, England, Wales, and the United States). Mental health triage may be used in an emergency room to evaluate the degree of risk and prioritize treatment.
Observation.
Observation is sometimes used to determine whether a person warrants involuntary commitment. It is not always clear on a relatively brief examination whether a person is psychotic or otherwise warrants commitment.
Containment of danger.
Austria, Belgium, Germany, Israel, the Netherlands, Northern Ireland, Russia, Taiwan, Ontario (Canada), and the United States have adopted commitment criteria based on the presumed danger of the defendant to self or to others. People with suicidal thoughts may act on these impulses and harm or kill themselves. People with psychosis are occasionally driven by their delusions or hallucinations to harm themselves or others. People with certain types of personality disorders can occasionally present a danger to themselves or others.
This concern has found expression in the standards for involuntary commitment in every U.S. state and in other countries as the "danger to self or others" standard, sometimes supplemented by the requirement that the danger be "imminent." In some jurisdictions, the "danger to self or others" standard has been broadened in recent years to include need-for-treatment criteria such as "gravely disabled."
Deinstitutionalization.
Starting in the 1960s, there has been a worldwide trend toward moving psychiatric patients from hospital settings to less restricting settings in the community, a shift known as "deinstitutionalization." Because the shift was typically not accompanied by a commensurate development of community-based services, critics say that deinstitutionalization has led to large numbers of people who would once have been inpatients being incarcerated in jails and prisons or becoming homeless. These scenarios occurred when outpatient services were not available or patients chose not to adhere to treatment outside the hospital. In some jurisdictions, laws authorizing court-ordered outpatient treatment have been passed in an effort to compel individuals with chronic, untreated severe mental illness to accept treatment while living outside the hospital (e.g. see Laura's Law, Kendra's Law).
Since the late 1960s the Italian physician Giorgio Antonucci questioned the basis themselves of psychiatry through the dismantling of the psychiatric hospitals "Osservanza" and "Luigi Lolli" and the liberation – and restitution to life – of the people there secluded.
Before the 1960s deinstitutionalization there were earlier efforts to free psychiatric patients. Doctor Philippe Pinel (1745–1826) ordered the removal of chains from patients.
There was a study of 269 patients from Vermont State Hospital done by Courtenay M. Harding, PhD and associates, about two-thirds of the ex-patients did well after deinstitutionalization.
Around the world.
United Nations.
United Nations General Assembly (resolution 46/119 of 1991), "Principles for the Protection of Persons with Mental Illness and the Improvement of Mental Health Care" is a non-binding resolution advocating certain broadly drawn procedures for the carrying out of involuntary commitment. These principles have been used in many countries where local laws have been revised or new ones implemented. The UN runs programs in some countries to assist in this process.
Politically motivated abuses.
At certain places and times, the practice of involuntary commitment has been used for the suppression of dissent, or in a punitive way.
In the former Soviet Union, psychiatric hospitals were used as prisons to isolate political prisoners from the rest of society. British playwright Tom Stoppard wrote "Every Good Boy Deserves Favour" about the relationship between a patient and his doctor in one of these hospitals. Stoppard was inspired by a meeting with a Russian exile.
In 1927, after the execution of Sacco and Vanzetti in the United States, a demonstrator named Aurora D'Angelo was sent to a mental health facility for psychiatric evaluation after she participated in a rally in support of the anarchists.

</doc>
<doc id="15417" url="https://en.wikipedia.org/wiki?curid=15417" title="Intermolecular force">
Intermolecular force

'Intermolecular forces" (IMFs) are forces of attraction or repulsion which act between neighboring particles (atoms, molecules, or ions). They are weak compared to the intramolecular forces, the forces which keep a molecule together. For example the covalent bond, involving the sharing of electron pairs between atoms is much stronger than the forces present between the neighboring molecules. They are an essential part of force fields frequently used in molecular mechanics.
The investigation of intermolecular forces starts from macroscopic observations which point out the existence and action of forces at a molecular level. These observations include non-ideal-gas thermodynamic behavior reflected by virial coefficients, vapor pressure, viscosity, superficial tension and absorption data.
The first reference to the nature of microscopic forces is found in Alexis Clairaut's work "Theorie de la Figure de la Terre". Other scientists who have contributed to the investigation of microscopic forces include: Laplace, Gauss, Maxwell and Boltzmann.
Attractive intermolecular forces are considered by the following types:
Information on intermolecular force is obtained by macroscopic measurements of properties like viscosity, PVT data. The link to microscopic aspects is given by virial coefficients and Lennard-Jones potentials.
Dipole-dipole interactions.
Dipole-dipole interactions are electrostatic interactions between permanent dipoles in molecules. These interactions tend to align the molecules to increase attraction (reducing potential energy). An example of a dipole-dipole interaction can be seen in hydrogen chloride (HCl): the positive end of a polar molecule will attract the negative end of the other molecule and influence its position. Polar molecules have a net attraction between them. Examples of polar molecules include hydrogen chloride (HCl) and chloroform (CHCl3).
Often molecules contain dipolar groups, but have no overall dipole moment. This occurs if there is symmetry within the molecule that causes the dipoles to cancel each other out. This occurs in molecules such as tetrachloromethane and carbon dioxide. Note that the dipole-dipole interaction between two individual atoms is usually zero, since atoms rarely carry a permanent dipole.
Ion-dipole and ion-induced dipole forces.
Ion-dipole and ion-induced dipole forces are similar to dipole-dipole and induced-dipole interactions but involve ions, instead of only polar and non-polar molecules. Ion-dipole and ion-induced dipole forces are stronger than dipole-dipole interactions because the charge of any ion is much greater than the charge of a dipole moment. Ion-dipole bonding is stronger than hydrogen bonding.
An ion-dipole force consists of an ion and a polar molecule interacting. They align so that the positive and negative groups are next to one another, allowing for maximum attraction.
An ion-induced dipole force consists of an ion and a non-polar molecule interacting. Like a dipole-induced dipole force, the charge of the ion causes distortion of the electron cloud on the non-polar molecule.
Hydrogen bonding.
A hydrogen bond is the attraction between the lone pair of an electronegative atom and a hydrogen atom that is bonded to either nitrogen, oxygen, or fluorine. The hydrogen bond is often described as a strong electrostatic dipole-dipole interaction. However, it also has some features of covalent bonding: it is directional, stronger than a van der Waals interaction, produces interatomic distances shorter than the sum of van der Waals radii, and usually involves a limited number of interaction partners, which can be interpreted as a kind of valence.
Intermolecular hydrogen bonding is responsible for the high boiling point of water (100 °C) compared to the other group 16 hydrides, which have no hydrogen bonds. Intramolecular hydrogen oxygen bonding is partly responsible for the secondary, tertiary, and quaternary structures of proteins and nucleic acids. It also plays an important role in the structure of polymers, both synthetic and natural.
Van der Waals forces.
The van der Waals forces arise from interaction between uncharged atoms or molecules, leading not only to such phenomena as the cohesion of condensed phases and physical adsorption of gases, but also to a universal force of attraction between macroscopic bodies.
Keesom (permanent-permanent dipoles) interaction.
The first contribution to van der Waals forces is due to electrostatic interactions between charges (in molecular ions), dipoles (for polar molecules), quadrupoles (all molecules with symmetry lower than cubic), and permanent multipoles. It is referred to as Keesom interactions(named after Willem Hendrik Keesom). These forces originate from the attraction between permanent dipoles (dipolar molecules) and are temperature dependent.
They consist of attractive interactions between dipoles that are ensemble averaged over different rotational orientations of the dipoles. It is assumed that the molecules are constantly rotating and never get locked into place. This is a good assumption, but at some point molecules do get locked into place. The energy of a Keesom interaction depends on the inverse sixth power of the distance, unlike the interaction energy of two spatially fixed dipoles, which depends on the inverse third power of the distance. The Keesom interaction can only occur among molecules that possess permanent dipole moments a.k.a. two polar molecules. Also Keesom interactions are very weak van der Waals interactions and do not occur in aqueous solutions that contain electrolytes. The angle averaged interaction is given by the following equation:
formula_1
Where m = charge per length, formula_2 = permitivity of free space, formula_3 = dielectric constant of surrounding material, T = temperature, formula_4 = Boltzmann constant, and r = distance between molecules.
Debye (permanent-induced dipoles) force.
The second contribution is the induction (also known as polarization) or Debye force, arising from interactions between rotating permanent dipoles and from the polarizability of atoms and molecules (induced dipoles). These induced dipoles occur when one molecule with a permanent dipole repels another molecule’s electrons. A molecule with permanent dipole can induce a dipole in a similar neighboring molecule and cause mutual attraction. Debye forces cannot occur between atoms. The forces between induced and permanent dipoles are not as temperature dependent as Keesom interactions because the induced dipole is free to shift and rotate around the non-polar molecule. The Debye induction effects and Keesom orientation effects are referred to as polar interactions.
The induced dipole forces appear from the induction (also known as polarization), which is the attractive interaction between a permanent multipole on one molecule with an induced (by the former di/multi-pole) multipole on another. This interaction is called the Debye force, named after Peter J.W. Debye.
One example of an induction-interaction between permanent dipole and induced dipole is the interaction between HCl and Ar. In this system, Ar experiences a dipole as its electrons are attracted (to the H side of HCl) or repelled (from the Cl side) by HCl. The angle averaged interaction is given by the following equation.
formula_5
Where formula_6= polarizability
This kind of interaction can be expected between any polar molecule and non-polar/symmetrical molecule. The induction-interaction force is far weaker than dipole-dipole interaction, but stronger than the London dispersion force.
London dispersion force (dipole-induced dipoles interaction).
The third and dominant contribution is the dispersion or London force (fluctuating dipole-induced dipole), which arises due to the non-zero instantaneous dipole moments of all atoms and molecules. Such polarization can be induced either by a polar molecule or by the repulsion of negatively charged electron clouds in non-polar molecules. Thus, London interactions are caused by random fluctuations of electron density in an electron cloud. An atom with a large number of electrons will have a greater associated London force than an atom with fewer electrons. The dispersion (London) force is the most important component because all materials are polarizable, whereas Keesom and Debye forces require permanent dipoles. The London interaction is universal and is present in atom-atom interactions as well. For various reasons, London interactions (dispersion) have been considered relevant for interactions between macroscopic bodies in condensed systems. Hamaker developed the theory of van der Waals between macroscopic bodies in 1937 and showed that the additivity of these interactions renders them considerably more long-range.
Relative strength of forces.
Note: this comparison is only approximate – the actual relative strengths will vary depending on the molecules involved. Ionic and covalent bonding will always be stronger than intermolecular forces in any given substance.
Effect on the behavior of gases.
Intermolecular forces are repulsive at short distances and attractive at long distances (see the Lennard-Jones potential). In a gas, the repulsive force chiefly has the effect of keeping two molecules from occupying the same volume. This gives a real gas a tendency to occupy a larger volume than an ideal gas at the same temperature and pressure. The attractive force draws molecules closer together and gives a real gas a tendency to occupy a smaller volume than an ideal gas. Which interaction is more important depends on temperature and pressure (see compressibility factor).
In a gas, the distances between molecules are generally large, so intermolecular forces have only a small effect. The attractive force is not overcome by the repulsive force, but by the thermal energy of the molecules. Temperature is the measure of thermal energy, so increasing temperature reduces the influence of the attractive force. In contrast, the influence of the repulsive force is essentially unaffected by temperature.
When a gas is compressed to increase its density, the influence of the attractive force increases. If the gas is made sufficiently dense, the attractions can become large enough to overcome the tendency of thermal motion to cause the molecules to spread out. Then the gas can condense to form a solid or liquid (i.e., a condensed phase). Lower temperature favors the formation of a condensed phase. In a condensed phase, there is very nearly a balance between the attractive and repulsive forces.
Quantum mechanical theories.
Intermolecular forces observed between atoms and molecules can be described phenomenologically as occurring between permanent and instantaneous dipoles, as outlined above. Alternatively, one may seek a fundamental, unifying theory that is able to explain the various types of interactions such as hydrogen bonding, van der Waals forces and dipole-dipole interactions. Typically, this is done by applying the ideas of quantum mechanics to molecules, and Rayleigh–Schrödinger perturbation theory has been especially effective in this regard. When applied to existing quantum chemistry methods, such a quantum mechanical explanation of intermolecular interactions, this provides an array of approximate methods that can be used to analyze intermolecular interactions.

</doc>
<doc id="15420" url="https://en.wikipedia.org/wiki?curid=15420" title="IRQ">
IRQ

IRQ may refer to:

</doc>
<doc id="15422" url="https://en.wikipedia.org/wiki?curid=15422" title="List of Internet top-level domains">
List of Internet top-level domains

This list of Internet top-level domain extensions contains all top-level domains, which are those domains in the DNS root zone of the Domain Name System of the Internet.
The official list of all top-level domains is maintained by the Internet Assigned Numbers Authority (IANA). IANA also oversees the approval process for new proposed top-level domains. , the root domain contains 1205 top-level domains, while a few have been retired and are no longer functional.
Types.
As of 2015, IANA distinguishes the following groups of top-level domains:
Original top-level domains.
Seven generic top-level domains were created early in the development of the Internet, and pre-date the creation of ICANN in 1998.
Internationalized country code top-level domains.
Source:
Internationalized generic top-level domains.
All of these TLDs are internationalized domain names (IDN) and support second-level IDNs.
Test TLDs.
ICANN created a set of top-level Internationalized domain names in October 2007 for the purpose of testing the use of IDNA in the root zone and within those domains. These testing domains were abolished on 31 October 2013. Each of these TLDs encoded a word meaning "test" in the respective language.
Each of these domains contained only one site with the word "example" encoded in the respective script and language. These "example.test" sites were test wikis used by ICANN.

</doc>
<doc id="15428" url="https://en.wikipedia.org/wiki?curid=15428" title="Idealism">
Idealism

In philosophy, idealism is the group of philosophies which assert that reality, or reality as we can know it, is fundamentally mental, mentally constructed, or otherwise immaterial. Epistemologically, idealism manifests as a skepticism about the possibility of knowing any mind-independent thing. In a sociological sense, idealism emphasizes how human ideas—especially beliefs and values—shape society. As an ontological doctrine, idealism goes further, asserting that all entities are composed of mind or spirit. Idealism thus rejects physicalist and dualist theories that fail to ascribe priority to the mind.
The earliest extant arguments that the world of experience is grounded in the mental derive from India and Greece. The Hindu idealists in India and the Greek Neoplatonists gave panentheistic arguments for an all-pervading consciousness as the ground or true nature of reality. In contrast, the Yogācāra school, which arose within Mahayana Buddhism in India in the 4th century CE, based its "mind-only" idealism to a greater extent on phenomenological analyses of personal experience. This turn toward the subjective anticipated empiricists such as George Berkeley, who revived idealism in 18th-century Europe by employing skeptical arguments against materialism.
Beginning with Immanuel Kant, German idealists such as G. W. F. Hegel, Johann Gottlieb Fichte, Friedrich Wilhelm Joseph Schelling, and Arthur Schopenhauer dominated 19th-century philosophy. This tradition, which emphasized the mental or "ideal" character of all phenomena, gave birth to idealistic and subjectivist schools ranging from British idealism to phenomenalism to existentialism. The historical influence of this branch of idealism remains central even to the schools that rejected its metaphysical assumptions, such as Marxism, pragmatism and positivism.
Definitions.
"Idealism" is a term with several related meanings. It comes via "idea" from the Greek "idein" (ἰδεῖν), meaning "to see". The term entered the English language by 1743.
Any philosophy that assigns crucial importance to the ideal or spiritual realm in its account of human existence may be termed "idealist". Metaphysical idealism is an ontological doctrine that holds that reality itself is incorporeal or experiential at its core. Beyond this, idealists disagree on which aspects of the mental are more basic. Platonic idealism affirms that abstractions are more basic to reality than the things we perceive, while subjective idealists and phenomenalists tend to privilege sensory experience over abstract reasoning. Epistemological idealism is the view that reality can only be known through ideas, that only psychological experience can be apprehended by the mind.
Subjective idealists like George Berkeley are anti-realists in terms of a mind-independent world, whereas transcendental idealists like Immanuel Kant are strong skeptics of such a world, affirming epistemological and not metaphysical idealism. Thus Kant defines "idealism" as "the assertion that we can never be certain whether all of our putative outer experience is not mere imagining". He claimed that, according to "idealism", "the reality of external objects does not admit of strict proof. On the contrary, however, the reality of the object of our internal sense (of myself and state) is clear immediately through consciousness." However, not all idealists restrict the real or the knowable to our immediate subjective experience. Objective idealists make claims about a transempirical world, but simply deny that this world is essentially divorced from or ontologically prior to the mental. Thus Plato and Gottfried Leibniz affirm an objective and knowable reality transcending our subjective awareness—a rejection of epistemological idealism—but propose that this reality is grounded in ideal entities, a form of metaphysical idealism. Nor do all metaphysical idealists agree on the nature of the ideal; for Plato, the fundamental entities were non-mental abstract forms, while for Leibniz they were proto-mental and concrete monads.
As a rule, transcendental idealists like Kant affirm idealism's epistemic side without committing themselves to whether reality is "ultimately" mental; objective idealists like Plato affirm reality's metaphysical basis in the mental or abstract without restricting their epistemology to ordinary experience; and subjective idealists like Berkeley affirm both metaphysical and epistemological idealism.
Classical idealism.
Monistic idealism holds that consciousness, not matter, is the ground of all being. It is monist because it holds that there is only one type of thing in the universe and idealist because it holds that one thing to be consciousness.
Anaxagoras (480 BC) was known as ""Nous"" ("Mind") because he taught that "all things" were created by Mind, that Mind held the cosmos together and gave human beings a connection to the cosmos or a pathway to the divine.
Many religious philosophies are specifically idealist. The belief that beings with knowledge (God/s, angels & spirits) preceded insentient matter seems to suggest that an experiencing subject is a necessary reality. Hindu idealism is central to Vedanta philosophy and to such schools as Kashmir Shaivism. Proponents include P.R. Sarkar and his disciple Sohail Inayatullah.
Christian theologians have held idealist views, often based on Neoplatonism, despite the influence of Aristotelian scholasticism from the 12th century onward. Later western theistic idealism such as that of Hermann Lotze offers a theory of the "world ground" in which all things find their unity: it has been widely accepted by Protestant theologians. Several modern religious movements, for example the organizations within the New Thought Movement and the Unity Church, may be said to have a particularly idealist orientation. The theology of Christian Science includes a form of idealism: it teaches that all that truly exists is God and God's ideas; that the world as it appears to the senses is a distortion of the underlying spiritual reality, a distortion that may be corrected (both conceptually and in terms of human experience) through a reorientation (spiritualization) of thought.
Wang Yangming, a Ming Chinese neo-Confucian philosopher, official, educationist, calligraphist and general, held that objects do not exist entirely apart from the mind because the mind shapes them. It is not the world that shapes the mind but the mind that gives reason to the world, so the mind alone is the source of all reason, having an inner light, an innate moral goodness and understanding of what is good.
The consciousness-only approach of the Yogācāra school of Mahayana Buddhism is not true metaphysical idealism as Yogācāra thinkers did not focus on consciousness to assert it as ultimately real, it is only conventionally real since it arises from moment to moment due to fluctuating causes and conditions and is significant because it is the cause of karma and hence suffering.
Platonism and neoplatonism.
Plato's theory of forms or "ideas" describes ideal forms (for example the platonic solids in geometry or abstracts like Goodness and Justice), as universals existing independently of any particular instance. Arne Grøn calls this doctrine "the classic example of a metaphysical idealism as a "transcendent" idealism", while Simone Klein calls Plato "the earliest representative of metaphysical objective idealism". Nevertheless, Plato holds that matter is real, though transitory and imperfect, and is perceived by our body and its senses and given existence by the eternal ideas that are perceived directly by our rational soul. Plato was therefore a metaphysical and epistemological dualist, an outlook that modern idealism has striven to avoid: Plato's thought cannot therefore be counted as idealist in the modern sense, although quantum physics' assertion that man's consciousness is an immutable and primary requisite for not merely perceiving but shaping matter, and thus his reality, would give more credence to Plato's dualist position.
With the neoplatonist Plotinus, wrote Nathaniel Alfred Boll; "there even appears, probably for the first time in Western philosophy, "idealism" that had long been current in the East even at that time, for it taught... that the soul has made the world by stepping from eternity into time...". Similarly, in regard to passages from the Enneads, "The only space or place of the world is the soul" and "Time must not be assumed to exist outside the soul", Ludwig Noiré wrote: "For the first time in Western philosophy we find idealism proper in Plotinus, However, Plotinus does not address whether we know external objects, unlike Schopenhauer and other modern philosophers.
Subjective idealism.
Subjective Idealism (immaterialism or phenomenalism) describes a relationship between experience and the world in which objects are no more than collections or "bundles" of sense data in the perceiver. Proponents include Berkeley, Bishop of Cloyne, an Anglo-Irish philosopher who advanced a theory he called immaterialism, later referred to as "subjective idealism", contending that individuals can only know sensations and ideas of objects directly, not abstractions such as "matter", and that ideas also depend upon being perceived for their very existence - "esse est percipi"; "to be is to be perceived".
Arthur Collier published similar assertions though there seems to have been no influence between the two contemporary writers. The only knowable reality is the represented image of an external object. Matter as a cause of that image, is unthinkable and therefore nothing to us. An external world as absolute matter unrelated to an observer does not exist as far as we are concerned. The universe cannot exist as it appears if there is no perceiving mind. Collier was influenced by "An Essay Towards the Theory of the Ideal or Intelligible World" by "Cambridge Platonist" John Norris (1701).
Bertrand Russell's popular book "The Problems of Philosophy" highlights Berkeley's tautological premise for advancing idealism;
The Australian philosopher David Stove harshly criticized philosophical idealism, arguing that it rests on what he called "the worst argument in the world". Stove claims that Berkeley tried to derive a non-tautological conclusion from tautological reasoning. He argued that in Berkeley's case the fallacy is not obvious and this is because one premise is ambiguous between one meaning which is tautological and another which, Stove argues, is logically equivalent to the conclusion.
Alan Musgrave argues that conceptual idealists compound their mistakes with use/mention confusions;
and proliferation of hyphenated entities such as "thing-in-itself" (Immanuel Kant), "things-as-interacted-by-us" (Arthur Fine), "table-of-commonsense" and "table-of-physics" (Sir Arthur Eddington) which are "warning signs" for conceptual idealism according to Musgrave because they allegedly do not exist but only highlight the numerous ways in which people come to know the world. This argument does not take into account the issues pertaining to hermeneutics, especially at the backdrop of analytic philosophy. Musgrave criticized Richard Rorty and Postmodernist philosophy in general for confusion of use and mention.
A. A. Luce and John Foster are other subjectivists. Luce, in "Sense without Matter" (1954), attempts to bring Berkeley up to date by modernizing his vocabulary and putting the issues he faced in modern terms, and treats the Biblical account of matter and the psychology of perception and nature. Foster's "The Case for Idealism" argues that the physical world is the logical creation of natural, non-logical constraints on human sense-experience. Foster's latest defense of his views is in his book "A World for Us: The Case for Phenomenalistic Idealism".
Paul Brunton, a British philosopher, mystic, traveler, and guru, taught a type of idealism called "mentalism", similar to that of Bishop Berkeley, proposing a master world-image, projected or manifested by a world-mind, and an infinite number of individual minds participating. A tree does not cease to exist if nobody sees it because the world-mind is projecting the idea of the tree to all minds.
John Searle, criticizing some versions of idealism, summarizes two important arguments for subjective idealism. The first is based on our perception of reality:
therefore;
Whilst agreeing with (2) Searle argues that (1) is false and points out that (3) does not follow from (1) and (2). The second argument runs as follows;
Searle contends that "Conclusion 2" does not follow from the premises.
Epistemological idealism is a subjectivist position in epistemology that holds that what one knows about an object exists only in one's mind. Proponents include Brand Blanshard.
Transcendental idealism.
Transcendental idealism, founded by Immanuel Kant in the eighteenth century, maintains that the mind shapes the world we perceive into the form of space-and-time.
The 2nd edition (1787) contained a "Refutation of Idealism" to distinguish his transcendental idealism from Descartes's Sceptical Idealism and Berkeley's anti-realist strain of Subjective Idealism. The section "Paralogisms of Pure Reason" is an implicit critique of Descartes' idealism. Kant says that it is not possible to infer the 'I' as an object (Descartes' "cogito ergo sum") purely from "the spontaneity of thought". Kant focused on ideas drawn from British philosophers such as Locke, Berkeley and Hume but distinguished his transcendental or critical idealism from previous varieties;
Kant distinguished between things as they appear to an observer and things in themselves, "that is, things considered without regard to whether and how they may be given to us". We cannot approach the "noumenon", the "thing in Itself" () without our own mental world. He added that the mind is not a blank slate, "tabula rasa" but rather comes equipped with categories for organising our sense impressions.
In the first volume of his "Parerga and Paralipomena", Schopenhauer wrote his "Sketch of a History of the Doctrine of the Ideal and the Real". He defined the ideal as being mental pictures that constitute subjective knowledge. The ideal, for him, is what can be attributed to our own minds. The images in our head are what comprise the ideal. Schopenhauer emphasized that we are restricted to our own consciousness. The world that appears is only a representation or mental picture of objects. We directly and immediately know only representations. All objects that are external to the mind are known indirectly through the mediation of our mind. He offered a history of the concept of the "ideal" as "ideational" or "existing in the mind as an image".
Charles Bernard Renouvier was the first Frenchman after Nicolas Malebranche to formulate a complete idealistic system, and had a vast influence on the development of French thought. His system is based on Immanuel Kant's, as his chosen term "néo-criticisme" indicates; but it is a transformation rather than a continuation of Kantianism.
Friedrich Nietzsche argued that Kant commits an agnostic tautology and does not offer a satisfactory answer as to the "source" of a philosophical right to such-or-other metaphysical claims; he ridicules his pride in tackling "the most difficult thing that could ever be undertaken on behalf of metaphysics." The famous "thing-in-itself" was called a product of philosophical habit, which seeks to introduce a grammatical subject: because wherever there is cognition, there must be a "thing" that is cognized and allegedly it must be added to ontology as a being (whereas, to Nietzsche, only the world as ever changing appearances can be assumed). Yet he attacks the idealism of Schopenhauer and Descartes with an argument similar to Kant's critique of the latter "(see above)".
Objective idealism.
Objective idealism asserts that the reality of experiencing combines and transcends the realities of the object experienced and of the mind of the observer. Proponents include Thomas Hill Green, Josiah Royce, Benedetto Croce and Charles Sanders Peirce.
Absolute idealism.
Schelling (1775–1854) claimed that the Fichte's "I" needs the Not-I, because there is no subject without object, and vice versa. So there is no difference between the subjective and the objective, that is, the ideal and the real. This is Schelling's "absolute identity": the ideas or mental images in the mind are identical to the extended objects which are external to the mind.
Absolute idealism is G. W. F. Hegel's account of how existence is comprehensible as an all-inclusive whole. Hegel called his philosophy "absolute" idealism in contrast to the "subjective idealism" of Berkeley and the "transcendental idealism" of Kant and Fichte, which were not based on a critique of the finite and a dialectical philosophy of history as Hegel's idealism was. The exercise of reason and intellect enables the philosopher to know ultimate historical reality, the phenomenological constitution of self-determination, the dialectical development of self-awareness and personality in the realm of History.
In his "Science of Logic" (1812–1814) Hegel argues that finite qualities are not fully "real" because they depend on other finite qualities to determine them. Qualitative "infinity", on the other hand, would be more self-determining and hence more fully real. Similarly finite natural things are less "real"—because they are less self-determining—than spiritual things like morally responsible people, ethical communities and God. So any doctrine, such as materialism, that asserts that finite qualities or natural objects are fully real is mistaken.
Hegel certainly intends to preserve what he takes to be true of German idealism, in particular Kant's insistence that ethical reason can and does go beyond finite inclinations. For Hegel there must be some identity of thought and being for the "subject" (any human observer)) to be able to know any observed "object" (any external entity, possibly even another human) at all. Under Hegel's concept of "subject-object identity," subject and object both have Spirit (Hegel's ersatz, redefined, nonsupernatural "God") as their "conceptual" (not metaphysical) inner reality—and in that sense are identical. But until Spirit's "self-realization" occurs and Spirit graduates from Spirit to "Absolute" Spirit status, subject (a human mind) mistakenly thinks every "object" it observes is something "alien," meaning something separate or apart from "subject." In Hegel's words, "The object is revealed to it "subject" by something alien, and it does not recognize itself." Self-realization occurs when Hegel (part of Spirit's nonsupernatural Mind, which is the collective mind of all humans) arrives on the scene and realizes that every "object" is "himself", because both subject and object are essentially Spirit. When self-realization occurs and Spirit becomes "Absolute" Spirit, the "finite" (man, human) becomes the "infinite" ("God," divine), replacing the imaginary or "picture-thinking" supernatural God of theism: man becomes God. Tucker puts it this way: "Hegelianism . . . is a religion of self-worship whose fundamental theme is given in Hegel's image of the man who aspires to be God himself, who demands 'something more, namely infinity.'" The picture Hegel presents is "a picture of a self-glorifying humanity striving compulsively, and at the end successfully, to rise to divinity."
Kierkegaard criticised Hegel's idealist philosophy in several of his works, particularly his claim to a comprehensive system that could explain the whole of reality. Where Hegel argues that an ultimate understanding of the logical structure of the world is an understanding of the logical structure of God's mind, Kierkegaard asserting that for God reality can be a system but it cannot be so for any human individual because both reality and humans are incomplete and all philosophical systems imply completeness. A logical system is possible but an existential system is not. "What is rational is actual; and what is actual is rational". Hegel's absolute idealism blurs the distinction between existence and thought: our mortal nature places limits on our understanding of reality;
So-called systems have often been characterized and challenged in the assertion that they abrogate the distinction between good and evil, and destroy freedom. Perhaps one would express oneself quite as definitely, if one said that every such system fantastically dissipates the concept existence. ... Being an individual man is a thing that has been abolished, and every speculative philosopher confuses himself with humanity at large; whereby he becomes something infinitely great, and at the same time nothing at all.
A major concern of Hegel's "Phenomenology of Spirit" (1807) and of the philosophy of Spirit that he lays out in his "Encyclopedia of the Philosophical Sciences" (1817–1830) is the interrelation between individual humans, which he conceives in terms of "mutual recognition." However, what Climacus means by the aforementioned statement, is that Hegel, in the "Philosophy of Right", believed the best solution was to surrender one's individuality to the customs of the State, identifying right and wrong in view of the prevailing bourgeois morality. Individual human will ought, at the State's highest level of development, to properly coincide with the will of the State. Climacus rejects Hegel's suppression of individuality by pointing out it is impossible to create a valid set of rules or system in any society which can adequately describe existence for any one individual. Submitting one's will to the State denies personal freedom, choice, and responsibility.
In addition, Hegel does believe we can know the structure of God's mind, or ultimate reality. Hegel agrees with Kierkegaard that both reality and humans are incomplete, inasmuch as we are in time, and reality develops through time. But the relation between time and eternity is outside time and this is the "logical structure" that Hegel thinks we can know. Kierkegaard disputes this assertion, because it eliminates the clear distinction between ontology and epistemology. Existence and thought are not identical and one cannot possibly think existence. Thought is always a form of abstraction, and thus not only is pure existence impossible to think, but all forms in existence are unthinkable; thought depends on language, which merely abstracts from experience, thus separating us from lived experience and the living essence of all beings. In addition, because we are finite beings, we cannot possibly know or understand anything that is universal or infinite such as God, so we cannot know God exists, since that which transcends time simultaneously transcends human understanding.
Bradley saw reality as a monistic whole apprehended through "feeling", a state in which there is no distinction between the perception and the thing perceived. Like Berkeley, Bradley thought that nothing can be known to exist unless it is known by a mind.
Bradley was the apparent target of G. E. Moore's radical rejection of idealism. Moore claimed that Bradley did not understand the statement that something is real. We know for certain, through common sense and prephilosophical beliefs, that some things are real, whether they are objects of thought or not, according to Moore. The 1903 article "The Refutation of Idealism" is one of the first demonstrations of Moore's commitment to analysis. He examines each of the three terms in the Berkeleian aphorism "esse est percipi", "to be is to be perceived", finding that it must mean that the object and the subject are "necessarily" connected so that "yellow" and "the sensation of yellow" are identical - "to be yellow" is "to be experienced as yellow". But it also seems there is a difference between "yellow" and "the sensation of yellow" and "that "esse" is held to be "percipi", solely because what is experienced is held to be identical with the experience of it". Though far from a complete refutation, this was the first strong statement by analytic philosophy against its idealist predecessors, or at any rate against the type of idealism represented by Berkeley. This argument did not show that the GEM (in post–Stove vernacular, see below) is logically invalid.
Actual idealism.
Actual Idealism is a form of idealism developed by Giovanni Gentile that grew into a "grounded" idealism contrasting Kant and Hegel.
Pluralistic idealism.
Pluralistic idealism such as that of Gottfried Leibniz takes the view that there are many individual minds that together underlie the existence of the observed world and make possible the existence of the physical universe. Unlike absolute idealism, pluralistic idealism does not assume the existence of a single ultimate mental reality or "Absolute". Leibniz' form of idealism, known as Panpsychism, views "monads" as the true atoms of the universe and as entities having perception. The monads are "substantial forms of being",elemental, individual, subject to their own laws, non-interacting, each reflecting the entire universe. Monads are centers of force, which is substance while space, matter and motion are phenomenal and their form and existence is dependent on the simple and immaterial monads. There is a pre-established harmony established by God, the central monad, between the world in the minds of the monads and the external world of objects. Leibniz's cosmology embraced traditional Christian Theism. The English psychologist and philosopher James Ward inspired by Leibniz had also defended a form of pluralistic idealism. According to Ward the universe is composed of "psychic monads" of different levels, interacting for mutual self- betterment.
Personalism is the view that the minds that underlie reality are the minds of persons. Borden Parker Bowne, a philosopher at Boston University, a founder and popularizer of personal idealism, presented it as a substantive reality of persons, the only reality, as known directly in self-consciousness. Reality is a society of interacting persons dependent on the Supreme Person of God. Other proponents include George Holmes Howison and J. M. E. McTaggart.
Howison's personal idealism was also called "California Personalism" by others to distinguish it from the "Boston Personalism" which was of Bowne. Howison maintained that both impersonal, monistic idealism and materialism run contrary to the experience of moral freedom. To deny freedom to pursue truth, beauty, and "benignant love" is to undermine every profound human venture, including science, morality, and philosophy. Personalistic idealists Borden Parker Bowne and Edgar S. Brightman and realistic personal theist Saint Thomas Aquinas address a core issue, namely that of dependence upon an infinite personal God.
Howison, in his book "The Limits of Evolution and Other Essays Illustrating the Metaphysical Theory of Personal Idealism", created a democratic notion of personal idealism that extended all the way to God, who was no more the ultimate monarch but the ultimate democrat in eternal relation to other eternal persons. J. M. E. McTaggart's idealist atheism and Thomas Davidson's Apeirionism resemble Howisons personal idealism.
J. M. E. McTaggart of Cambridge University, argued that minds alone exist and only relate to each other through love. Space, time and material objects are unreal. In "The Unreality of Time" he argued that time is an illusion because it is impossible to produce a coherent account of a sequence of events. "The Nature of Existence" (1927) contained his arguments that space, time, and matter cannot possibly be real. In his "Studies in Hegelian Cosmology" (Cambridge, 1901, p196) he declared that metaphysics are not relevant to social and political action. McTaggart "thought that Hegel was wrong in supposing that metaphysics could show that the state is more than a means to the good of the individuals who compose it". For McTaggart "philosophy can give us very little, if any, guidance in action... Why should a Hegelian citizen be surprised that his belief as to the organic nature of the Absolute does not help him in deciding how to vote? Would a Hegelian engineer be reasonable in expecting that his belief that all matter is spirit should help him in planning a bridge?
Thomas Davidson taught a philosophy called "apeirotheism", a "form of pluralistic idealism...coupled with a stern ethical rigorism" which he defined as "a theory of Gods infinite in number." The theory was indebted to Aristotle's pluralism and his concepts of Soul, the rational, living aspect of a living substance which cannot exist apart from the body because it is not a substance but an essence, and "nous", rational thought, reflection and understanding. Although a perennial source of controversy, Aristotle arguably views the latter as both eternal and immaterial in nature, as exemplified in his theology of unmoved movers. Identifying Aristotle's God with rational thought, Davidson argued, contrary to Aristotle, that just as the soul cannot exist apart from the body, God cannot exist apart from the world.
Idealist notions took a strong hold among physicists of the early 20th century confronted with the paradoxes of quantum physics and the theory of relativity. In "The Grammar of Science", Preface to the 2nd Edition, 1900, Karl Pearson wrote, "There are many signs that a sound idealism is surely replacing, as a basis for natural philosophy, the crude materialism of the older physicists." This book influenced Einstein's regard for the importance of the observer in scientific measurements. In § 5 of that book, Pearson asserted that "...science is in reality a classification and analysis of the contents of the mind..." Also, "...the field of science is much more consciousness than an external world."
Sir Arthur Eddington, a British astrophysicist of the early 20th century, wrote in his book "The Nature of the Physical World"; "The stuff of the world is mind-stuff";
"The mind-stuff of the world is, of course, something more general than our individual conscious minds... The mind-stuff is not spread in space and time; these are part of the cyclic scheme ultimately derived out of it... It is necessary to keep reminding ourselves that all knowledge of our environment from which the world of physics is constructed, has entered in the form of messages transmitted along the nerves to the seat of consciousness... Consciousness is not sharply defined, but fades into subconsciousness; and beyond that we must postulate something indefinite but yet continuous with our mental nature... It is difficult for the matter-of-fact physicist to accept the view that the substratum of everything is of mental character. But no one can deny that mind is the first and most direct thing in our experience, and all else is remote inference."
Ian Barbour in his book "Issues in Science and Religion" (1966), p. 133, cites Arthur Eddington's "The Nature of the Physical World" (1928) for a text that argues The Heisenberg Uncertainty Principles provides a scientific basis for "the defense of the idea of human freedom" and his "Science and the Unseen World" (1929) for support of philosophical idealism "the thesis that reality is basically mental".
Sir James Jeans wrote; "The stream of knowledge is heading towards a non-mechanical reality; the Universe begins to look more like a great thought than like a great machine. Mind no longer appears to be an accidental intruder into the realm of matter... we ought rather hail it as the creator and governor of the realm of matter."
Jeans, in an interview published in The Observer (London), when asked the question: "Do you believe that life on this planet is the result of some sort of accident, or do you believe that it is a part of some great scheme?" replied:
"I incline to the idealistic theory that consciousness is fundamental, and that the material universe is derivative from consciousness, not consciousness from the material universe... In general the universe seems to me to be nearer to a great thought than to a great machine. It may well be, it seems to me, that each individual consciousness ought to be compared to a brain-cell in a universal mind.
Addressing the British Association in 1934, Jeans said: "What remains is in any case very different from the full-blooded matter and the forbidding materialism of the Victorian scientist. His objective and material universe is proved to consist of little more than constructs of our own minds. To this extent, then, modern physics has moved in the direction of philosophic idealism. Mind and matter, if not proved to be of similar nature, are at least found to be ingredients of one single system. There is no longer room for the kind of dualism which has haunted philosophy since the days of Descartes." 
In "The Universe Around Us", Jeans writes: "Finite picture whose dimensions are a certain amount of space and a certain amount of time; the protons and electrons are the streaks of paint which define the picture against its space-time background. Traveling as far back in time as we can, brings us not to the creation of the picture, but to its edge; the creation of the picture lies as much outside the picture as the artist is outside his canvas. On this view, discussing the creation of the universe in terms of time and space is like trying to discover the artist and the action of painting, by going to the edge of the canvas. This brings us very near to those philosophical systems which regard the universe as a thought in the mind of its Creator, thereby reducing all discussion of material creation to futility." 
The chemist Ernest Lester Smith wrote a book "Intelligence Came First" (1975) in which he claimed that consciousness is a fact of nature and that the cosmos is grounded in and pervaded by mind and intelligence.
Bernard d'Espagnat, a French theoretical physicist best known for his work on the nature of reality, wrote a paper titled "The Quantum Theory and Reality." According to the paper: "The doctrine that the world is made up of objects whose existence is independent of human consciousness turns out to be in conflict with quantum mechanics and with facts established by experiment." 
In an article in the Guardian titled "Quantum weirdness: What We Call 'Reality' is Just a State of Mind", d'Espagnat wrote: "What quantum mechanics tells us, I believe, is surprising to say the least. It tells us that the basic components of objects – the particles, electrons, quarks etc. – cannot be thought of as "self-existent".
He further writes that his research in quantum physics has led him to conclude that an "ultimate reality" exists, which is not embedded in space or time.
References.
Further reading

</doc>
<doc id="15430" url="https://en.wikipedia.org/wiki?curid=15430" title="Inheritance">
Inheritance

Inheritance is the practice of passing on property, titles, debts, rights and obligations upon the death of an individual. It has long played an important role in human societies. The rules of inheritance differ between societies and have changed over time.
Terminology.
In law, an "heir" is a person who is entitled to receive a share of the deceased's (the person who died) property, subject to the rules of inheritance in the jurisdiction where the deceased (decedent) died or owned property at the time of death. A person does not become an heir before the death of the deceased, since the exact identity of the persons entitled to inherit is determined only then. Members of ruling noble or royal houses expected to become heirs are called heirs apparent if first in line and incapable of being displaced from inheriting by another claim; otherwise, they are heirs presumptive. There is a further concept of joint inheritance, pending renunciation by all but one, which is called coparceny.
In modern law, the terms "inheritance" and "heir" refer exclusively to succession to property by descent from a deceased dying intestate. Takers in property succeeded to under a will are termed generally "beneficiaries," and specifically "devisees" for real property, "bequestees" for personal property, or "legatees" for money.
History.
Detailed anthropological and sociological studies have been made about customs of patrilineal inheritance, where only male children can inherit. Some cultures also employ matrilineal succession, where property can only pass along the female line, most commonly going to the sister's sons of the decedent; but also, in some societies, from the mother to her daughters. Some ancient societies and most modern states employ egalitarian inheritance, without discrimination based on gender and/or birth order.
Islamic laws of inheritance.
The Quran introduced a number of different rights and restrictions on matters of inheritance, including general improvements to the treatment of women and family life compared to the pre-Islamic societies that existed in the Arabian Peninsula at the time. The Quran also presented efforts to fix the laws of inheritance, and thus forming a complete legal system. This development was in contrast to pre-Islamic societies where rules of inheritance varied considerably. Furthermore, the Quran introduced additional heirs that were not entitled inheritance in pre-Islamic times, mentioning nine relatives specifically of which six were female and three were male. In addition to the above changes, the Quran imposed restrictions on testamentary powers of a Muslim in disposing his or her property. In their will, a Muslim can only give out a maximum of one third of their property.
The Quran contains only three verses that give specific details of inheritance and shares, in addition to few other verses dealing with testamentary. But this information was used as a starting point by Muslim jurists who expounded the laws of inheritance even further using Hadith, as well as methods of juristic reasoning like Qiyas. Nowadays, inheritance is considered an integral part of Sharia law and its application for Muslims is mandatory, though many peoples (see Historical inheritance systems), despite being Muslim, have other inheritance customs.
Jewish laws of inheritance.
The inheritance is patrilineal. The father —that is, the owner of the land— bequeaths only to his male descendants, so the Promised Land passes from one Jewish father to his sons.
If there were no living sons and no descendants of any previously living sons, daughters could inherit. In , the daughters of Zelophehad (Mahlah, Noa, Hoglah, Milcah, and Tirzah) of the tribe of Manasseh come to Moses and ask for their father's inheritance, as they have no brothers. The order of inheritance is set out in : a man's sons inherit first, daughters if no sons, brothers if he has no children, and so on.
Later, in , some of the heads of the families of the tribe of Manasseh come to Moses and point out that, if a daughter inherits and then marries a man not from her paternal tribe, her land will pass from her birth-tribe's inheritance into her marriage-tribe's. So a further rule is laid down: if a daughter inherits land, she must marry someone within her father's tribe. (The daughters of Zelophehad marry the sons' of their father's brothers. There is "no" indication that this was not their choice.)
The tractate Baba Bathra, written during late Antiquity in Babylon, deals extensively with issues of property ownership and inheritance according to Jewish Law. Other works of Rabbinical Law, such as the Hilkhot naḥalot : mi-sefer Mishneh Torah leha-Rambam, and the Sefer ha-yerushot: ʻim yeter ha-mikhtavim be-divre ha-halakhah be-ʻAravit uve-ʻIvrit uve-Aramit also deal with inheritance issues. The first, often abbreviated to Mishneh Torah, was written by Maimonides and was very important in Jewish tradition.
All these sources agree that the firstborn son is entitled to a double portion of his father's estate: . This means that, for example, if a father left five sons, the firstborn receives a third of the estate and each of the other four receives a sixth. If he left nine sons, the firstborn receives a fifth and each of the other eight receive a tenth. If the eldest surviving son is not the firstborn son, he is not entitled to the double portion.
Philo of Alexandria and Josephus also comment on the Jewish laws of inheritance, praising them above other law codes of their time. They also agreed that the firstborn son must receive a double portion of his father's estate.
Inheritance inequality.
The distribution of the inherited wealth is often unequal. The majority might receive little while only a small number inherit a larger amount, with the lesser amount given to daughter in the family. The amount of inheritance is often far less than the value of a business initially given to the son, especially when a son takes over a thriving multimillion-dollar business, yet the daughter is given the balance of the actual inheritance amounting to far less than the value of business that was initially given to the son. This is especially seen in old world cultures, but continues in many families to this day.
Arguments for eliminating the disparagement of inheritance inequality include the right to property and the merit of individual allocation of capital over government wealth confiscation and redistribution, but this does not resolve the problem of unequal inheritance. In terms of inheritance inequality, some economists and sociologists focus on the inter generational transmission of income or wealth which is said to have a direct impact on one's mobility (or immobility) and class position in society. Nations differ on the political structure and policy options that govern the transfer of wealth.
According to the American federal government statistics compiled by Mark Zandi, currently of "Moody's Economy.com", back in 1985, the average US inheritance was $39,000. In subsequent years, the overall amount of total annual inheritance was more than doubled, reaching nearly $200 billion. By 2050, there is an estimated $25 trillion average inheritance transmitted across generations. Some researchers have attributed this rise to the baby boomer generation. Historically, the baby boomers were the largest influx of children conceived after WW2. For this reason, Thomas Shapiro suggests that this generation "is in the midst of benefiting from the greatest inheritance of wealth in history."
Inherited wealth may help explain why many Americans who have become rich may have had a "substantial head start". In September 2012, according to the Institute for Policy Studies, "over 60 percent" of the Forbes richest 400 Americans "grew up in substantial privilege".
Inheritance and social stratification.
Inheritance has a significant effect on stratification. Inheritance is an integral component of family, economic, and legal institutions, and a basic mechanism of class stratification. It also affects the distribution of wealth at the societal level. The total cumulative effect of inheritance on stratification outcomes takes three forms. The first form of inheritance is the inheritance of cultural capital (i.e. linguistic styles, higher status social circles, and aesthetic preferences). The second form of inheritance is through familial interventions in the form of "inter vivos" transfers (i.e. gifts between the living), especially at crucial junctures in the life courses. Examples include during a child's milestone stages, such as going to college, getting married, getting a job, and purchasing a home. The third form of inheritance is the transfers of bulk estates at the time of death of the testators, thus resulting in significant economic advantage accruing to children during their adult years. The origin of the stability of inequalities is material (personal possessions one is able to obtain) and is also cultural, rooted either in varying child-rearing practices that are geared to socialization according to social class and economic position. Child-rearing practices among those who inherit wealth may center around favoring some groups at the expense of others at the bottom of the social hierarchy.
Sociological and economic effects of inheritance inequality.
The degree to which economic status and inheritance is transmitted across generations determines one's life chances in society. Although many have linked one's social origins and educational attainment to life chances and opportunities, education cannot serve as the most influential predictor of economic mobility. In fact, children of well-off parents generally receive better schooling and benefit from material, cultural, and genetic inheritances. Likewise, schooling attainment is often persistent across generations and families with higher amounts of inheritance are able to acquire and transmit higher amounts of human capital. Lower amounts of human capital and inheritance can perpetuate inequality in the housing market and higher education. Research reveals that inheritance plays an important role in the accumulation of housing wealth. Those who receive an inheritance are more likely to own a home than those who do not regardless of the size of the inheritance.
Often, minorities and individuals from socially disadvantaged backgrounds receive less inheritance and wealth. As a result, mixed races might be excluded in inheritance privilege and are more likely to rent homes or live in poorer neighborhoods, as well as achieve lower educational attainment compared with whites in America. Individuals with a substantial amount of wealth and inheritance often intermarry with others of the same social class to protect their wealth and ensure the continuous transmission of inheritance across generations; thus perpetuating a cycle of privilege.
Nations with the highest income and wealth inequalities often have the highest rates of homicide and disease (such as obesity, diabetes, and hypertension). A New York Times article reveals that the U.S. is the world's wealthiest nation, but "ranks twenty-ninth in life expectancy, right behind Jordan and Bosnia." This has been regarded as highly attributed to the significant gap of inheritance inequality in the country, although there are clearly other factors such as the healthcare system.
When social and economic inequalities centered on inheritance are perpetuated by major social institutions such as family, education, religion, etc., these differing life opportunities are transmitted from each generation. As a result, this inequality becomes part of the overall social structure.
Taxation.
Many states have inheritance taxes or death duties, under which a portion of any estate goes to the government.

</doc>
<doc id="15432" url="https://en.wikipedia.org/wiki?curid=15432" title="ISO 6166">
ISO 6166

ISO 6166 defines the structure of an International Securities Identifying Number (ISIN). An ISIN uniquely identifies a fungible security. Securities with which ISINs can be used are equities, debts, ETFs, options, derivatives, and futures.
ISINs consist of two alphabetic characters, which are the ISO 3166-1 alpha-2 code for the issuing country, nine alpha-numeric digits (the National Securities Identifying Number, or NSIN, which identifies the security), and one numeric check digit. The NSIN is issued by a national numbering agency (NNA) for that country. Regional substitute NNAs have been allocated the task of functioning as NNAs in those countries where NNAs have not yet been established.
NNAs cooperate through the Association of National Numbering Agencies (ANNA). ANNA also functions as the ISO 6166 Maintenance Agency (MA).

</doc>
<doc id="15435" url="https://en.wikipedia.org/wiki?curid=15435" title="Ignatius of Antioch">
Ignatius of Antioch

Ignatius of Antioch (, "Ignátios Antiokheías") (c.35  – c.108), also known as Ignatius Theophorus (, "Ignátios ho Theophóros", lit. "the God-bearing"), Ignatius Nurono (lit. "The fire-bearer") was an Apostolic Father, student of the Apostle John, and the third bishop of Antioch. En route to Rome, where he met his martyrdom by being fed to wild beasts, he wrote a series of letters which have been preserved as an example of very early Christian theology. Important topics addressed in these letters include ecclesiology, the sacraments, and the role of bishops.
Life.
Ignatius converted to Christianity at a young age. Later in his life he was chosen to serve as a Bishop of Antioch, succeeding Saint Peter and St. Evodius (who died around AD 67). The 4th-century Church historian Eusebius records that Ignatius succeeded Evodius. Making his apostolic succession even more immediate, Theodoret of Cyrrhus reported that St. Peter himself left directions that Ignatius be appointed to the episcopal see of Antioch. Ignatius called himself "Theophorus" (God Bearer). A tradition arose that he was one of the children whom Jesus took in his arms and blessed. Tradition also identifies Ignatius, along with his friend Polycarp, as disciples of John the Apostle.
Epistles attributed to Ignatius report his arrest by the authorities and travel to Rome:
The ship made a number of stops along the shores of Asia Minor on the way to Rome. At Smyrna he met Bishop Polycarp. Along the route he wrote six letters to the churches in the region and one to a fellow bishop. He was sentenced to die at the Colosseum. In his "Chronicle", Eusebius gives the date of Ignatius's death as AA 2124 (2124 years after Abraham), which would amount to the 11th year of Trajan's reign; i.e., AD 108.
After Ignatius' martyrdom in the Colosseum his remains were carried back to Antioch by his companions and were interred outside the city gates. The reputed remains of Ignatius were moved by the Emperor Theodosius II to the Tychaeum, or Temple of Tyche, which had been converted into a church dedicated to Ignatius. In 637 the relics were transferred to the Basilica di San Clemente in Rome.
Veneration.
Ignatius' feast day was kept in his own Antioch on 17 October, the day on which he is now celebrated in the Catholic Church and generally in western Christianity, although from the 12th century until 1969 it was put at 1 February in the General Roman Calendar.
In the Eastern Orthodox Church it is observed on 20 December. The Synaxarium of the Coptic Orthodox Church of Alexandria places it on the 24th of the Coptic Month of Koiak, corresponding in three years out of every four to 20 December in the Julian Calendar, which currently falls on 2 January of the Gregorian Calendar.
Letters.
The following seven letters preserved under the name of Ignatius are generally considered authentic as they were mentioned by the historian Eusebius in the first half of the fourth century.
Seven Authentic Letters:
Writing in 1886, Dr. William P. Killen regarded all the Ignatian epistles, beginning with that to the Romans, as having been pseudepigraphically composed in the early 3rd century. His reasons included their episcopal emphasis, which is otherwise unknown before the reign of Callistus, the Bishop of Rome around 220. Most scholars, however, accept at least the two Ignatian epistles which were referenced by Origen, and believe that by the 5th century, this collection had been enlarged by spurious letters. The original text of six of the seven authentic letters are found in the Codex Mediceo Laurentianus written in Greek in the 11th century (which also contains the pseudepigraphical letters of the Long Recension, except that to the Philippians), while the letter to the Romans is found in the Codex Colbertinus. Some of the original letters were, at one point, believed to had been changed with interpolations. The oldest is known as the "Long Recension" which dates from the latter part of the fourth century. These were created to posthumously enlist Ignatius as an unwitting witness in theological disputes of that age, but that position was vigorously combated by several British and German critics, including the Catholics Denzinger and Hefele, who defended the genuineness of the entire seven epistles. At the same time, the purported eye-witness account of his martyrdom is also thought to be a forgery from around the same time. A detailed but spurious account of Ignatius' arrest and his travails and martyrdom is the material of the "Martyrium Ignatii" which is presented as being an eyewitness account for the church of Antioch, and attributed to Ignatius' companions, Philo of Cilicia, deacon at Tarsus, and Rheus Agathopus, a Syrian.
Although James Ussher regarded it as genuine, if there is any genuine nucleus of the "Martyrium", it has been so greatly expanded with interpolations that no part of it is without questions. Its most reliable manuscript is the 10th-century "Codex Colbertinus" (Paris), in which the "Martyrium" closes the collection. The "Martyrium" presents the confrontation of the bishop Ignatius with Trajan at Antioch, a familiar trope of "Acta" of the martyrs, and many details of the long, partly overland voyage to Rome. The Synaxarium of the Coptic Orthodox Church of Alexandria says that he was thrown to the wild beasts that devoured him and rent him to pieces.
Ignatius's letters proved to be important testimony to the development of Christian theology, since the number of extant writings from this period of Church history is very small. They bear signs of being written in great haste and without a proper plan, such as run-on sentences and an unsystematic succession of thought.
Ignatius modeled his writings after Paul, Peter, and John, and even quoted or paraphrased their own works freely, such as when he quoted 1 Cor 1:18, in his letter to the Ephesians: "Let my spirit be counted as nothing for the sake of the cross, which is a stumbling-block to those that do not believe, but to us salvation and life eternal." - "Letter to the Ephesians" 18, Roberts and Donaldson translation
Ignatius is the earliest known Christian writer to emphasize loyalty to a single bishop in each city (or diocese) who is assisted by both presbyters (possibly elders or priests) and deacons. Earlier writings only mention "either" bishops "or" presbyters, and give the impression that there was usually more than one bishop per congregation.
For instance, his writings on bishops, presbyters and deacons: 
Ignatius is known to have taught the deity of Christ: 
Though this is less clear in the interpolated text of the 4th Century Long Recension: 
He stressed the value of the Eucharist, calling it a "medicine of immortality" ("Ignatius to the Ephesians" 20:2). The very strong desire for bloody martyrdom in the arena, which Ignatius expresses rather graphically in places, may seem quite odd to the modern reader. An examination of his theology of soteriology shows that he regarded salvation as one being free from the powerful fear of death and thus to bravely face martyrdom.
Ignatius is claimed to be the first known Christian writer to argue in favor of Christianity's replacement of the Sabbath with the Lord's Day: 
He is also responsible for the first known use of the Greek word "katholikos" (καθολικός), meaning "universal", "complete" and "whole" to describe the church, writing: 
It is from the word "katholikos" ("according to the whole") that the word "catholic" comes. When Ignatius wrote the Letter to the Smyrnaeans in about the year 107 and used the word "catholic", he used it as if it were a word already in use to describe the Church. This has led many scholars to conclude that the appellation "Catholic Church" with its ecclesial connotation may have been in use as early as the last quarter of the 1st century. On the Eucharist, he wrote in his letter to the Smyrnaeans: 
In his letter was addressed to the Christians of Rome, he entreats to do nothing to prevent his martyrdom.
Letters of Pseudo-Ignatius.
Epistles attributed to Saint Ignatius but of spurious origin include

</doc>
<doc id="15437" url="https://en.wikipedia.org/wiki?curid=15437" title="ITU prefix">
ITU prefix

The International Telecommunication Union (ITU) allocates call sign prefixes for radio and television stations of all types. They also form the basis for, but do not exactly match, aircraft registration identifiers. These prefixes are agreed upon internationally, and are a form of country code. A call sign can be any number of letters and numerals but each country must only use call signs that begin with the characters allocated for use in that country.
A few countries do not fully comply with these rules. Australian broadcast stations officially have—but do not use—the VL prefix, and Canada uses Chile's CB for its own Canadian Broadcasting Corporation stations. This is through a special agreement with the government of Chile, which is officially assigned the CB prefix.
With regard to the second and/or third letters in the prefixes in the list below, if the country in question is allocated all callsigns with A to Z in that position, then that country can also use call signs with the digits 0 to 9 in that position. For example, the United States is assigned KA–KZ, and therefore can also use prefixes like KW0 or K1.
Many large countries in turn have internal rules on how and where specific subsets of their callsigns can be used (such as Mexico's XE for AM and XH for FM radio and television broadcasting), which are not covered here.
Unallocated and unavailable call sign prefixes.
Unallocated: The following call sign prefixes are available for future allocation by the ITU. ("x" represents any letter; "n" represents any digit from 2–9.)
Unavailable: Under present ITU guidelines the following call sign prefixes shall not be allocated . They are sometimes used unofficially - such as amateur radio operators operating in a disputed territory or in a nation state that has no official prefix (e.g. S0 in Western Sahara or station 1A0 at Knights of Malta headquarters in Rome). ("x" represents any letter; "n" represents any digit from 2–9.)

</doc>
<doc id="15440" url="https://en.wikipedia.org/wiki?curid=15440" title="IBM PC keyboard">
IBM PC keyboard

The keyboards for IBM PC compatible computers are standardized. However, during the more than 30 years of PC architecture being constantly updated, multiple types of keyboard layout variations have been developed.
A well-known class of IBM PC keyboards is the Model M. Introduced in 1986 and manufactured by IBM, Lexmark, Maxi-Switch and Unicomp, the vast majority of Model M keyboards feature a buckling spring key design and many have fully swappable keycaps.
Keyboard layouts.
The PC keyboard changed over the years, often at the launch of new IBM PC versions.
Common additions to the standard layouts include additional power management keys, volume controls, media player controls, and miscellaneous user-configurable shortcuts for email client, World Wide Web browser, etc.
The IBM PC layout, particularly the Model M, has been extremely influential, and today most keyboards use some variant of it. This has caused problems for applications developed with alternative layouts, which require keys that are in awkward positions on the Model M layout – often requiring the pinkie to operate – and thus require remapping for comfortable use. One notable example is the Escape key, used by the vi editor: on the ADM-3A terminal this was located where the Tab key is on the IBM PC, but on the IBM PC the Escape key is in the corner; this is typically solved by remapping Caps Lock to Escape. Another example is the Emacs editor, which makes extensive use of modifier keys, and uses the Control key more than the Meta key (IBM PC instead has the Alt key) – these date to the Knight keyboard, which had the Control key on the "inside" of the Meta key, opposite to the Model M, where it is on the "outside" of the Alt key; and to the space-cadet keyboard, where the four bucky bit keys (Control, Meta, Super, Hyper) are in a row, allowing easy chording to press several, unlike on the Model M layout. This results in the "Emacs pinky" problem.
Reception.
Although "PC Magazine" praised most aspects of the 1981 IBM PC keyboard's hardware design, it questioned "how IBM, that ultimate pro of keyboard manufacture, could put the left-hand SHIFT key at the awkward reach they did". The magazine reported in 1982 that it received more letters to its "Wish List" column asking for the ability to determine the status of the three lock keys than on any other topic. "BYTE" columnist Jerry Pournelle praised the keyboard's feel as "excellent" but complained that the Shift and other keys' locations were "enough to make a saint weep", and denounced the trend of PC compatible computers to emulate the layout but not the feel. He reported that the layout "nearly drove" science-fiction editor Jim Baen "crazy", and that "many of authors refused to work with that keyboard" so could not submit manuscripts in a compatible format. "BYTE"'s review was more sanguine. It praised the keyboard as "bar none, the best ... on any microcomputer" and described the unusual Shift key locations as "minor [problems compared to some of the gigantic mistakes made on almost every other microcomputer keyboard".
"I wasn't thrilled with the placement of left Shift and Return keys, either", IBM's Don Estridge stated in 1983. He defended the layout, however, stating that "every place you pick to put them is not a good place for somebody ... there's no consensus", and claimed that "if we were to change it now we would be in hot water".
Standard key meanings.
The PC keyboard with its various keys has a long history of evolution reaching back to teletypewriters. In addition to the 'old' standard keys, the PC keyboard has accumulated several special keys over the years. Some of the additions have been inspired by the opportunity or requirement for improving user productivity with general office application software, while other slightly more general keyboard additions have become the factory standards after being introduced by certain operating system or GUI software vendors such as Microsoft.

</doc>
<doc id="15441" url="https://en.wikipedia.org/wiki?curid=15441" title="Italian battleship Giulio Cesare">
Italian battleship Giulio Cesare

Giulio Cesare was one of three dreadnought battleships built for the Royal Italian Navy ("Regia Marina") in the 1910s. She served in both World Wars, although she was little used and saw no combat during the former. The ship supported operations during the Corfu Incident in 1923 and spent much of the rest of the decade in reserve. She was rebuilt between 1933 and 1937 with more powerful guns, additional armor and considerably more speed than before.
Both "Giulio Cesare" and her sister ship, , participated in the Battle of Calabria in July 1940, when the former was lightly damaged. They were both present when British torpedo bombers attacked the fleet at Taranto in November 1940, but "Giulio Cesare" was not damaged. She escorted several convoys to North Africa and participated in the Battle of Cape Spartivento in late 1940 and the First Battle of Sirte in late 1941. She was designated as a training ship in early 1942, and escaped to Malta after Italy surrendered. The ship was transferred to the Soviet Union in 1949 and renamed Novorossiysk. The Soviets also used her for training until she was sunk when an old German mine exploded in 1955. She was salvaged the following year and later scrapped.
Description.
Named after Julius Caesar, "Giulio Cesare" was long at the waterline, and overall. The ship had a beam of , and a draft of . She displaced at normal load, and at deep load. She had a crew of 31 officers and 969 enlisted men. The ship's machinery consisted of four Parsons steam turbines, each driving one propeller shaft. Steam for the turbines was provided by 24 Babcock & Wilcox boilers, half of which burned fuel oil and the other half burning both oil and coal. Designed to reach a maximum speed of from , "Giulio Cesare" failed to reach this goal on her sea trials, despite generally exceeding the rated power of her turbines. The ship only made a maximum speed of using . She had a cruising radius of at .
The ship was armed with a main battery of thirteen guns in three triple-gun turret and two twin-gun turrets, designated 'A', 'B', 'Q', 'X', and 'Y' from front to rear. The secondary battery comprised eighteen guns, all mounted in casemates in the sides of the hull. "Giulio Cesare" was also armed with fourteen guns. As was customary for capital ships of the period, she was equipped with three submerged torpedo tubes. She was protected with Krupp cemented steel manufactured by Terni. The belt armor was thick and the main deck was thick. The conning tower and main battery turrets were protected with worth of armor plating.
Modifications and reconstruction.
Shortly after the end of World War I, the number of 50-caliber 76 mm guns was reduced to 13, all mounted on the turret tops, and six new 40-caliber 76 mm anti-aircraft (AA) guns were installed abreast the aft funnel. In addition two license-built 2-pounder AA guns were mounted on the forecastle deck. In 1925–26 the foremast was replaced by a four-legged mast, which was moved forward of the funnels, the rangefinders were upgraded, and the ship was equipped to handle a Macchi M.18 seaplane mounted on the center turret. Around that same time, either one or both of the ships was equipped with a fixed aircraft catapult on the port side of the forecastle.
"Giulio Cesare" began an extensive reconstruction in October 1933 at the Cantieri del Tirreno shipyard in Genoa that lasted until October 1937. A new bow section was grafted over the existing bow which increased her length by to and her beam increased to . The ship's draft at deep load increased to . All of the changes made increased her displacement to at standard load and at deep load. The ship's crew increased to 1,260 officers and enlisted men. Two of the propeller shafts were removed and the existing turbines were replaced by two Belluzzo geared steam turbines rated at . The boilers were replaced by eight Yarrow boilers. On her sea trials in December 1936, before her reconstruction was fully completed, "Giulio Cesare" reached a speed of from . In service her maximum speed was about and she had a range of at a speed of .
The main guns were bored out to and the center turret and the torpedo tubes were removed. All of the existing secondary armament and AA guns were replaced by a dozen 120 mm guns in six twin-gun turrets and eight AA guns in twin turrets. In addition the ship was fitted with a dozen Breda Cannone-Mitragliera da 37/54 (Breda) light AA guns in six twin-gun mounts and twelve Breda M31 anti-aircraft machine guns, also in twin mounts. In 1940 the 13.2 mm machine guns were replaced by AA guns in twin mounts. "Giulio Cesare" received two more twin mounts as well as four additional 37 mm guns in twin mounts on the forecastle between the two turrets in 1941. The tetrapodal mast was replaced with a new forward conning tower, protected with thick armor. Atop the conning tower there was a fire-control director fitted with two large stereo-rangefinders, with a base length of .
The deck armor was increased during the reconstruction to a total of over the engine and boiler rooms and over the magazines, although its distribution over three decks, each with multiple layers, meant that it was considerably less effective than a single plate of the same thickness. The armor protecting the barbettes was reinforced with plates. All this armor weighed a total of . The existing underwater protection was replaced by the Pugliese torpedo defense system that consisted of a large cylinder surrounded by fuel oil or water that was intended to absorb the blast of a torpedo warhead. It lacked, however, enough depth to be fully effective against contemporary torpedoes. A major problem of the reconstruction was that the ship's increased draft meant that their waterline armor belt was almost completely submerged with any significant load.
Construction and service.
"Giulio Cesare", named after Julius Caesar, was laid down at the Gio. Ansaldo & C. shipyard in Genoa on 24 June 1910 and launched on 15 October 1911. She was completed on 14 May 1914 and served as a flagship in the southern Adriatic Sea during World War I. She saw no action, however, and spent little time at sea. Admiral Paolo Thaon di Revel, the Italian naval chief of staff, believed that Austro-Hungarian submarines and minelayers could operate too effectively in the narrow waters of the Adriatic. The threat from these underwater weapons to his capital ships was too serious for him to use the fleet in an active way. Instead, Revel decided to implement a blockade at the relatively safer southern end of the Adriatic with the battle fleet, while smaller vessels, such as the MAS torpedo boats, conducted raids on Austro-Hungarian ships and installations. Meanwhile, Revel's battleships would be preserved to confront the Austro-Hungarian battle fleet in the event that it sought a decisive engagement.
"Giulio Cesare" made port visits in the Levant in 1919 and 1920. Both "Giulio Cesare" and "Conte di Cavour" supported Italian operations on Corfu in 1923 after an Italian general and his staff were murdered on Corfu; Benito Mussolini was not satisfied with the Greek government's response so he ordered Italian troops to occupy the island. "Cesare" became a gunnery training ship in 1928, after having been in reserve since 1926. She was reconstructed at Cantieri del Tirreno, Genoa, between 1933 and 1937. Both ships participated in a naval review by Adolf Hitler in the Bay of Naples in May 1938 and covered the invasion of Albania in May 1939.
World War II.
Early in World War II, the ship took part in the Battle of Calabria (also known as the Battle of Punto Stilo), together with "Conte di Cavour", on 9 July 1940, as part of the 1st Battle Squadron, commanded by Admiral Inigo Campioni, during which she engaged major elements of the British Mediterranean Fleet. The British were escorting a convoy from Malta to Alexandria, while the Italians had finished escorting another from Naples to Benghazi, Libya. Admiral Andrew Cunningham, commander of the Mediterranean Fleet, attempted to interpose his ships between the Italians and their base at Taranto. Crew on the fleets spotted each other in the middle of the afternoon and the battleships opened fire at 15:53 at a range of nearly . The two leading British battleships, and , replied a minute later. Three minutes after she opened fire, shells from "Giulio Cesare" began to straddle "Warspite" which made a small turn and increased speed, to throw off the Italian ship's aim, at 16:00. At that same time, a shell from "Warspite" struck "Giulio Cesare" at a distance of about . The shell pierced the rear funnel and detonated inside it, blowing out a hole nearly across. Fragments started several fires and their smoke was drawn into the boiler rooms, forcing four boilers off-line as their operators could not breathe. This reduced the ship's speed to . Uncertain how severe the damage was, Campioni ordered his battleships to turn away in the face of superior British numbers and they successfully disengaged. Repairs to "Giulio Cesare" were completed by the end of August and both ships unsuccessfully attempted to intercept British convoys to Malta in August and September.
On the night of 11 November 1940, "Giulio Cesare" and the other Italian battleships were at anchor in Taranto harbor when they were attacked by 21 Fairey Swordfish torpedo bombers from the British aircraft carrier , along with several other warships. One torpedo sank Conte di Cavour in shallow water, but "Giulio Cesare" was not hit during the attack. She participated in the Battle of Cape Spartivento on 27 November 1940, but never got close enough to any British ships to fire at them. The ship was damaged in January 1941 by splinters from a near miss during an air raid on Naples by Vickers Wellington bombers of the Royal Air Force; repairs at Genoa were completed in early February. On 8 February, she sailed from to the Straits of Bonifacio to intercept what the Italians thought was a Malta convoy, but was actually a raid on Genoa. She failed to make contact with any British forces. She participated in the First Battle of Sirte on 17 December 1941, providing distant cover for a convoy bound for Libya, again never firing her main armament. She also provided distant cover for another convoy to North Africa in early January 1942. "Giulio Cesare" was reduced to a training ship afterwards at Taranto and later Pola. The unsuccessfully attacked the ship in the Gulf of Taranto in early March 1944. After the Italian surrender on 9 September 1943, she steamed to Taranto, putting down a mutiny and enduring an ineffective attack by five German aircraft en route. She then sailed for Malta where she arrived on 12 September to be interned. The ship remained there until 17 June 1944 when she returned to Taranto where she remained for the next four years.
Soviet service.
After the war, "Giulio Cesare" was allocated to the Soviet Union as part of the war reparations and she was moved to Augusta, Sicily on 9 December 1948 where an unsuccessful attempt was made to sabotage the ship. The ship was stricken from the naval register on 15 December and turned over to the Soviets on 6 February 1949 under the temporary name of "Z11" in Vlorë, Albania. She was renamed by them as "Novorossiysk", after the Soviet city on the Black Sea. The Soviets used her as a training ship when she was not undergoing one of her eight refits in their hands. In 1953, all remaining Italian light AA guns were replaced by eighteen 37 mm 70-K AA guns in six twin mounts and six singles. They also replaced her fire-control systems and added radars, although the exact changes are unknown. The Soviets intended to rearm her with their own 305 mm guns, but this was forestalled by her loss. While at anchor in Sevastopol on the night of 28/29 October 1955, she most likely detonated a large German mine left over from World War II. The explosion blew a hole completely through the ship, making a hole in the forecastle forward of 'A' turret. The flooding could not be controlled and she later capsized with the loss of 608 men, including men sent from other ships to assist.
The cause of the explosion is still unclear. The officially named cause, regarded as the most probable, was a magnetic RMH or LMB bottom mine, laid by the Germans during World War II and triggered by the dragging of the battleship's anchor chain before mooring for the last time. Subsequent searches located 32 mines of these types, some of them within of the explosion. The damage was consistent with an explosion of of TNT and more than one mine may have detonated. Nonetheless, other explanations for the ship's loss have been proposed and the most popular of these is that she was sunk by Italian frogmen of the wartime special operations unit "Decima Flottiglia MAS" who — more than ten years after the cessation of hostilities — were either avenging the transfer of the former Italian battleship to the USSR or sinking it on behalf of NATO. "Novorossiysk" was stricken from the naval register on 24 February 1956, salvaged on 4 May 1957, and subsequently scrapped.

</doc>
<doc id="15442" url="https://en.wikipedia.org/wiki?curid=15442" title="INS Vikrant (R11)">
INS Vikrant (R11)

INS "Vikrant" (Hindi : भा नौ पो विक्रान्त; , for "courageous") was a of the Indian Navy. She played a key role in enforcing the naval blockade on East Pakistan during the Indo-Pakistan War of 1971.
The ship was built under the name Hercules for the British Royal Navy during World War II, but construction was put on hold after the war's end, and she never entered British service. India purchased the incomplete carrier from the United Kingdom in 1957, and construction was completed in 1961. INS "Vikrant" was commissioned as the first aircraft carrier of the Indian Navy. After years of distinguished service, she was decommissioned in January 1997.
From 1997 to 2012, she was preserved as a museum ship in Cuffe Parade, Mumbai, until it was closed in 2012 due to safety concerns. At the end of January 2014, "Vikrant" was sold through an online auction to a Darukhana ship-breaker, where she underwent preparations to be broken up. Although a public-interest litigation was filed and heard by the Supreme Court of India challenging "Vikrant" 's sale and scrapping, on 14 August 2014, the Supreme Court rejected the PIL and cleared the way for the warship to be scrapped. Vikrant remained beached off Darukhana in Mumbai Port while awaiting the final clearances of the Mumbai Port Trust. On 12 November 2014, the Supreme Court gave its final approval for the carrier to be scrapped with the scrapping of "Vikrant" commencing on 22 November 2014.
History.
"Vikrant" was ordered as "Hercules" by the Royal Navy. She was laid down on 12 November 1943 by Vickers-Armstrong on the River Tyne. She was launched on 22 September 1945. However, with the end of World War II, her construction was suspended in May 1946 and she was laid up for possible future use.
In January 1957 she was sold to India. She was towed to Belfast to complete her construction and for modifications by Harland and Wolff. A number of improvements to the original design were ordered by the Indian Navy, including an angled deck, steam catapults and a modified island.
"Vikrant" was commissioned into the Indian Navy by then Indian High Commissioner to the United Kingdom, Vijayalakshmi Pandit on 4 March 1961 in Belfast. The name "Vikrant" was taken from Sanskrit "vikrānta" meaning "stepping beyond", i.e. "courageous" or "bold". Captain Pritam Singh was the first commanding officer of the carrier.
"Vikrant"s initial air wing consisted of British Hawker Sea Hawk fighter-bombers and a French Alize anti-submarine aircraft. On 18 May 1961, the first jet landed on her deck piloted by Lieutenant (later Admiral) Radhakrishna Hariram Tahiliani. She formally joined the Indian Navy's Fleet in Bombay on 3 November 1961, when she was received at Ballard Pier by Prime Minister Jawaharlal Nehru.
During the Indo-Pakistan War of 1965, Pakistan reported that it had sunk "Vikrant". However, at the time the ship was in dry dock undergoing modifications.
In June 1970, "Vikrant" was at the Naval Dockyard for repairs due to a crack in a water drum of one of the boilers powering her steam catapult. Unable to procure a replacement drum from the United Kingdom due to an embargo, Admiral Sardarilal Mathradas Nanda ordered the routing of steam from her forward machinery to the steam catapult to bypass the damaged boiler. This repair enabled her to launch both the Sea Hawks as well as the Breguet Alizé, although she lost some cruising power. In March 1971, she was put through trials to test the fix. These modifications turned out to be valuable, enabling "Vikrant" to enter combat against East Pakistan in the Indo-Pakistani War of 1971 despite the cracked boiler.
Stationed off the Andaman & Nicobar Islands along with frigates, and , "Vikrant" redeployed towards Chittagong at the outbreak of hostilities. Based on naval intelligence reports that the Pakistan Navy intended to break through the Indian Naval blockade using camouflaged merchant ships, the Sea Hawks struck shipping in the Chittagong and Cox's Bazar harbours, sinking or incapacitating most ships there. On the morning of 4 December 1971, "Vikrant"s eight Sea Hawk aircraft launched an air raid on Cox's Bazar from offshore. On the evening of 4 December, the air group struck Chittagong Harbour. Later strikes targeted Khulna and Port of Mongla. A Press Trust of India report of 4 December read, "Chittagong harbour ablaze as ships and aircraft of the Eastern Naval Fleet bombed and rocketed. Not a single vessel can be put to sea from Chittagong." Air strikes continued until 10 December 1971 with not a single Sea Hawk lost.
The Pakistan Navy deployed the submarine to specifically target and sink "Vikrant". However, "Ghazi" sank off Visakhapatnam harbour, probably due to depth charge attacks by . During the war, the crew of "Vikrant" earned two Mahavir Chakras and 12 Vir Chakras.
Air Arm.
"Vikrant" had four squadrons on board :
Squadron insignia
Subsequent service.
"Vikrant" was given an extensive refit, including new engines and modernization between 1979 and 3 January 1982. Between December 1982 and February 1983 she was refitted again to enable her to operate BAe Sea Harriers which replaced the Sea Hawk. After the retirement of the Breguet Alizé from carrier service in 1989, she received a 'ski jump' for more efficient use of her Sea Harriers.
"Vikrant" was India's only carrier for over twenty years, but by the early 1990s she was effectively out of service due to her poor condition. Even following major overhauls she was rarely put to sea. She was formally decommissioned on 31 January 1997.
Museum ship.
Following her decommissioning, "Vikrant" was marked for preservation as a museum ship in Mumbai, although a lack of funding prevented progress on the ship's conversion for this role. Similarly, speculation that the ship would be made into a training ship in 2006 came to nothing. From 2001, "Vikrant" was made open to the public by the Indian Navy for short periods, but as of April 2010, the Government of Maharashtra was unable to find an industrial partner to operate the museum on a permanent, long-term basis. In 2012, the museum was closed after "Vikrant" was deemed unsafe.
"Vikrant" was the only World War II-era British-built aircraft carrier to be preserved as a museum.
Auction and scrapping.
In August 2013, Vice-Admiral Shekhar Sinha, chief of the Western Naval Command, said the Ministry of Defence would scrap "Vikrant" as he had become "very difficult to maintain," and as no private bidders had offered to fund the museum's operations. On 3 December 2013 the Indian government decided to auction the ship, due to maintenance difficulties. The Bombay High Court dismissed a public-interest litigation filed by Kiran Paigankar, founder of the "Save Vikrant Committee," stating the vessel's dilapidated condition did not warrant her preservation, nor were the necessary funds or government support available.
At the end of January 2014, "Vikrant" was sold through an online auction to a Darukhana ship-breaker for Rs.60 crores (Indian Express-21-Nov-2014: http://indianexpress.com/article/india/india-others/dismantling-of-iconic-warship-ins-vikrant-begins/ ). Although a public-interest litigation was filed and heard by the Supreme Court of India challenging "Vikrant" 's sale and scrapping, on 14 August 2014, the Supreme Court rejected the PIL and cleared the way for the warship to be scrapped. Vikrant remained beached off Darukhana in Mumbai Port while awaiting the final clearances of the Mumbai Port Trust. On 12 November 2014, the Supreme Court gave its final approval for the carrier to be scrapped with the scrapping of "Vikrant" commencing on 22 November 2014.
In February 2016, Indian auto manufacturer Bajaj unveiled a new bike made with metal from Vikrant's scrap and named it Bajaj V in honor of "Vikrant".
In popular culture.
The decommissioned ship while it was moored near Darukhana in Mumbai was featured prominently in the first half of the film ABCD 2 as a backdrop. The ship was shown parked behind the practice area of the dance troupe. The ship was moored at the place till a decision was made against its scrapping. A few months after the shoot the ship was ultimately scrapped. 

</doc>
<doc id="15443" url="https://en.wikipedia.org/wiki?curid=15443" title="Western imperialism in Asia">
Western imperialism in Asia

Western imperialism in Asia as presented in this article pertains to Western European entry into what was first called the East Indies. This was sparked early in the 15th century by the search for trade routes to China that led directly to the Age of Discovery, and the introduction of early modern warfare into what was then called the Far East. By the early 16th century the Age of Sail greatly expanded Western European influence and development of the Spice Trade under colonialism. There has been a presence of Western European colonial empires and imperialism in Asia throughout six centuries of colonialism, formally ending with the independence of the Portuguese Empire's last colony East Timor in 2002. The empires introduced Western concepts of nation and the multinational state. This article attempts to outline the consequent development of the Western concept of the nation state.
The thrust of European political power, commerce, and culture in Asia gave rise to growing trade in commodities—a key development in the rise of today's modern world free market economy. In the 16th century, the Portuguese broke the (overland) monopoly of the Arabs and Italians of trade between Asia and Europe by the discovery of the sea route to India around the Cape of Good Hope. With the ensuing rise of the rival Dutch East India Company, Portuguese influence in Asia was gradually eclipsed. Dutch forces first established independent bases in the East (most significantly Batavia, the heavily fortified headquarters of the Dutch East India Company) and then between 1640 and 1660 wrestled Malacca, Ceylon, some southern Indian ports, and the lucrative Japan trade from the Portuguese. Later, the English and the French established settlements in India and established a trade with China and their own acquisitions would gradually surpass those of the Dutch. Following the end of the Seven Years' War in 1763, the British eliminated French influence in India and established the British East India Company as the most important political force on the Indian Subcontinent.
Before the Industrial Revolution in the mid-to-late 19th century, demand for oriental goods such as (porcelain, silk, spices and tea) remained the driving force behind European imperialism, and (with the important exception of British East India Company rule in India) the European stake in Asia remained confined largely to trading stations and strategic outposts necessary to protect trade. Industrialisation, however, dramatically increased European demand for Asian raw materials; and the severe Long Depression of the 1870s provoked a scramble for new markets for European industrial products and financial services in Africa, the Americas, Eastern Europe, and especially in Asia. This scramble coincided with a new era in global colonial expansion known as "the New Imperialism," which saw a shift in focus from trade and indirect rule to formal colonial control of vast overseas territories ruled as political extensions of their mother countries. Between the 1870s and the beginning of World War I in 1914, the United Kingdom, France, and the Netherlands—the established colonial powers in Asia—added to their empires vast expanses of territory in the Middle East, the Indian Subcontinent, and South East Asia. In the same period, the Empire of Japan, following the Meiji Restoration; the German Empire, following the end of the Franco-Prussian War in 1871; Tsarist Russia; and the United States, following the Spanish–American War in 1898, quickly emerged as new imperial powers in East Asia and in the Pacific Ocean area.
In Asia, World War I and World War II were played out as struggles among several key imperial powers—conflicts involving the European powers along with Russia and the rising American and Japanese powers. None of the colonial powers, however, possessed the resources to withstand the strains of both world wars and maintain their direct rule in Asia. Although nationalist movements throughout the colonial world led to the political independence of nearly all of the Asia's remaining colonies, decolonisation was intercepted by the Cold War; and South East Asia, South Asia, the Middle East, and East Asia remained embedded in a world economic, financial, and military system in which the great powers compete to extend their influence. However, the rapid post-war economic development of the East Asian Tigers, India, the People's Republic of China, along with the collapse of the Soviet Union, have loosened European and American influence in Asia, generating speculation today about emergence of modern India and China as potential superpowers.
Early European exploration of Asia.
European exploration of Asia started in ancient Roman times. Knowledge of lands as distant as China were held by the Romans. Trade with India through the Roman Egyptian Red Sea ports was significant in the first centuries of the Common Era.
Medieval European exploration of Asia.
In the 13th and 14th centuries, a number of Europeans, many of them Christian missionaries, had sought to penetrate into China. The most famous of these travelers was Marco Polo. But these journeys had little permanent effect on East-West trade because of a series of political developments in Asia in the last decades of the 14th century, which put an end to further European exploration of Asia. The Yuan dynasty in China, which had been receptive to European missionaries and merchants, was overthrown, and the new Ming rulers were found to be unreceptive of religious proselytism. Meanwhile, the Turks consolidated control over the eastern Mediterranean, closing off key overland trade routes. Thus, until the 15th century, only minor trade and cultural exchanges between Europe and Asia continued at certain terminals controlled by Muslim traders.
Oceanic voyages to Asia.
Western European rulers determined to find new trade routes of their own. The Portuguese spearheaded the drive to find oceanic routes that would provide cheaper and easier access to South and East Asian goods. This chartering of oceanic routes between East and West began with the unprecedented voyages of Portuguese and Spanish sea captains. Their voyages were influenced by medieval European adventurers, who had journeyed overland to the Far East and contributed to geographical knowledge of parts of Asia upon their return.
In 1488, Bartolomeu Dias rounded the southern tip of Africa under the sponsorship of Portugal's John II, from which point he noticed that the coast swung northeast (Cape of Good Hope). While Dias' crew forced him to turn back, by 1497, Portuguese navigator Vasco da Gama made the first open voyage from Europe to India. In 1520, Ferdinand Magellan, a Portuguese navigator in the service of Spain, found a sea route into the Pacific Ocean.
Portuguese and Spanish trade and colonization in Asia.
Portuguese monopoly over trade in the Indian Ocean and Asia.
Early in the 16th century Afonso de Albuquerque (left) emerged as the Portuguese colonial viceroy most instrumental in consolidating Portugal's holdings in Africa and in Asia. He understood that Portugal could wrest commercial supremacy from the Arabs only by force, and therefore devised a plan to establish forts at strategic sites which would dominate the trade routes and also protect Portuguese interests on land. In 1510, he conquered Goa in India, which enabled him to gradually consolidate control of most of the commercial traffic between Europe and Asia, largely through trade; Europeans started to carry on trade from forts, acting as foreign merchants rather than as settlers. In contrast, early European expansion in the "West Indies", (later known to Europeans as a separate continent from Asia that they would call the "Americas") following the 1492 voyage of Christopher Columbus, involved heavy settlement in colonies that were treated as political extensions of the mother countries.
Lured by the potential of high profits from another expedition, the Portuguese established a permanent base south of the Indian trade port of Calicut in the early 15th century. In 1510, the Portuguese seized Goa on the coast of India, which Portugal held until 1961. The Portuguese soon acquired a monopoly over trade in the Indian Ocean.
Portuguese viceroy Afonso de Albuquerque (1509–1515) resolved to consolidate Portuguese holdings in Africa and Asia, and secure control of trade with the East Indies and China. His first objective was Malacca, which controlled the narrow strait through which most Far Eastern trade moved. Captured in 1511, Malacca became the springboard for further eastward penetration; several years later the first trading posts were established in the Moluccas, or "Spice Islands," which was the source for some of the world's most hotly demanded spices. By 1516, the first Portuguese ships had reached Canton on the southern coasts of China.
In 1513, after the failed attempt to conquer Aden, Albuquerque entered with an armada, for the first time for Europeans by the ocean via, on the Red Sea; and in 1515, Albuquerque consolidated the Portuguese hegemony in the Persian Gulf gates, already begun by him in 1507, with the domain of Muscat and Ormuz.
The Portuguese conquest of Malacca triggered the Malayan–Portuguese war. In 1521, Ming dynasty China defeated the Portuguese at the Battle of Tunmen and then defeated the Portuguese again at the Battle of Xicaowan. The Portuguese tried to establish trade with China by illegally smuggling with the pirates on the offshore islands off the coast of Zhejiang and Fujian, but they were driven away by the Ming navy in the 1530s-1540's.
In 1557, China decided to lease Macau to the Portuguese as a place where they could dry goods they transported on their ships, which they held until 1999. The Portuguese, based at Goa and Malacca, had now established a lucrative maritime empire in the Indian Ocean meant to monopolise the spice trade. The Portuguese also began a channel of trade with the Japanese, becoming the first recorded Westerners to have visited Japan. This contact introduced Christianity and fire-arms into Japan.
The energies of Spain, the other major colonial power of the 16th century, were largely concentrated on the Americas, not South and East Asia. But the Spanish did establish a footing in the Far East in the Philippines. After 1565, cargoes of Chinese goods were transported from the Philippines to Mexico and from there to Spain. By this long route, Spain reaped some of the profits of Far Eastern commerce. Spanish officials converted the islands to Christianity and established some settlements, permanently establishing the Philippines as the area of East Asia most oriented toward the West in terms of culture and commerce. The Moro Muslims fought against the Spanish for over three centuries in the Spanish–Moro conflict.
Decline of Portugal's Asian empire since the 17th century.
The lucrative trade was vastly expanded when the Portuguese began to export slaves from Africa in 1541; however, over time, the rise of the slave trade left Portugal over-extended, and vulnerable to competition from other Western European powers. Envious of Portugal's control of trade routes, other Western European nations—mainly the Netherlands, France, and England—began to send in rival expeditions to Asia. In 1642, the Dutch drove the Portuguese out of the Gold Coast in Africa, the source of the bulk of Portuguese slave labourers, leaving this rich slaving area to other Europeans, especially the Dutch and the English.
Rival European powers began to make inroads in Asia as the Portuguese and Spanish trade in the Indian Ocean declined primarily because they had become hugely over-stretched financially due to the limitations on their investment capacity and contemporary naval technology. Both of these factors worked in tandem, making control over Indian Ocean trade extremely expensive.
The existing Portuguese interests in Asia proved sufficient to finance further colonial expansion and entrenchment in areas regarded as of greater strategic importance in Africa and Brazil. Portuguese maritime supremacy was lost to the Dutch in the 17th century, and with this came serious challenges for the Portuguese. However, they still clung to Macau, and settled a new colony on the island of Timor. It was as recent as the 1960s and 1970s that the Portuguese began to relinquish their colonies in Asia. Goa was invaded by India in 1961 and became an Indian state in 1987; Portuguese Timor was abandoned in 1975 and was then invaded by Indonesia. It became an independent country in 2002; and Macau was handed back to the Chinese as per a treaty in 1999.
Holy wars.
The arrival of the Portuguese and Spanish and their holy wars against Muslim states in the Malayan–Portuguese war Spanish–Moro conflict and Castilian War inflamed religious tensions and turned Southeast Asia into an arena of conflict between Muslims and Christians. The Brunei Sultanate's capital at Kota Batu was assaulted by Governor Sande who led the 1578 Spanish attack.
The word "savages" in Spanish, cafres, was from the word "infidel" in Arabic - Kafir, and was used by the Spanish to refer to their own "christian savages" who were arrested in Brunei. It was said "Castilians are kafir, men who have no souls, who are condemned by fire when they die, and that too because they eat pork." by the Brunei Sultan after the term "accursed doctrine" was used to attack Islam by the Spaniards which fed into hatred between Muslims and Christians sparked by their 1571 war against Brunei. The Sultan's words were in response to insults coming from the Spanish at Manila in 1578, other Muslims from Champa, Java, Borneo, Luzon, Pahang, Demak, Aceh, and the Malays echoed the rhetoric of holy war against the Spanish and Iberian Portuguese, calling them kafir enemies which was a contrast to their earlier nuanced views of the Portuguese in the Hikayat Tanah Hitu and Sejarah Melayu. The war by Spain against Brunei was defended in an apologia written by Doctor De Sande. The British eventually partitioned and took over Brunei while Sulu was attacked by the British, Americans, and Spanish which caused its breakdown and downfall after both of them thrived from 1500-1900 for four centuries. Dar al-Islam was seen as under invasion by "kafirs" by the Atjehnese led by Zayn al-din and by Muslims in the Philippines as they saw the Spanish invasion, since the Spanish brought the idea of a crusader holy war against Muslim Moros just as the Portuguese did in Indonesia and India against what they called "Moors"in their political and commercial conquests which they saw through the lens of religion in the 16th century.
In 1578 an attack was launched by the Spanish against Jolo, and in 1875 it was destroyed at their hands, and once again in 1974 it was destroyed by the Philippines. The Spanish first set foot on Borneo in Brunei.
The Spanish war against Brunei failed to conquer Brunei but it totally cut off the Philippines from Brunei's influence, the Spanish then started colonizing Mindanao and building fortresses. In response, the Bisayas, where Spanish forces were stationed, were subjected to retaliatory attacks by the Magindanao in 1599-1600 due to the Spanish attacks on Mindanao.
The Brunei royal family was related to the Muslim Rajahs who in ruled the principality in 1570 of Manila (Kingdom of Maynila) and this was what the Spaniards came across on their initial arrival to Manila, Spain uprooted Islam out of areas where it was shallow after they began to force Christianity on the Philippines in their conquests after 1521 while Islam was already widespread in the 16th century Philippines. In the Philippines in the Cebu islands the natives killed the Spanish fleet leader Magellan. Borneo's western coastal areas at Landak, Sukadana, and Sambas saw the growth of Muslim states in the sixteenth century, in the 15th century at Nanking, the capital of China, the death and burial of the Borneo Bruneian king Maharaja Kama took place upon his visit to China with Zheng He's fleet.
Dutch trade and colonization in Asia.
Rise of Dutch control over Asian trade in the 17th century.
Portuguese decline in Asia was accelerated by the attacks on their commercial empire by the Dutch and the English, which began a global struggle over empire in Asia that lasted until the end of the Seven Years' War in 1763. The Netherlands revolt against Spanish rule facilitated Dutch encroachment of the Portuguese monopoly over South and East Asian trade. The Dutch looked on Spain's trade and colonies as potential spoils in war. When the two crowns of the Iberian peninsula were joined in 1581, the Dutch felt free to attack Portuguese territories in Asia.
By the 1590s, a number of Dutch companies were formed to finance trading expeditions in Asia. Because competition lowered their profits, and because of the doctrines of mercantilism, in 1602 the companies united into a cartel and formed the Dutch East India Company, and received from the government the right to trade and colonise territory in the area stretching from the Cape of Good Hope eastward to the Strait of Magellan.
In 1605, armed Dutch merchants captured the Portuguese fort at Amboyna in the Moluccas, which was developed into the first secure base of the company. Over time, the Dutch gradually consolidated control over the great trading ports of the East Indies. Control over the East Indies trading ports allowed the company to monopolise the world spice trade for decades. Their monopoly over the spice trade became complete after they drove the Portuguese from Malacca in 1641 and Ceylon in 1658.
Dutch East India Company colonies or outposts were later established in Atjeh (Aceh), 1667; Macassar, 1669; and Bantam, 1682. The company established its headquarters at Batavia (today Jakarta) on the island of Java. Outside the East Indies, the Dutch East India Company colonies or outposts were also established in Persia (Iran), Bengal (now Bangladesh and part of India), Mauritius (1638-1658/1664-1710), Siam (now Thailand), Guangzhou (Canton, China), Taiwan (1624–1662), and southern India (1616–1795).
Ming dynasty China defeated the Dutch East India Company in the Sino-Dutch conflicts. The Chinese first defeated and drove the Dutch out of the Pescadores in 1624. The Ming navy under Zheng Zhilong defeated the Dutch East India Company's fleet at the 1633 Battle of Liaoluo Bay. In 1662, Zheng Zhilong's son Zheng Chenggong (also known as Koxinga) expelled the Dutch from Taiwan after defeating them in the Siege of Fort Zeelandia. ("see" History of Taiwan) Further, the Dutch East India Company trade post on Dejima (1641–1857), an artificial island off the coast of Nagasaki, was for a long time the only place where Europeans could trade with Japan.
The Vietnamese Nguyễn lords defeated the Dutch in a naval battle in 1643.
The Cambodians defeated the Dutch in the Cambodian–Dutch War in 1644.
In 1652, Jan van Riebeeck established an outpost at the Cape of Good Hope (the southwestern tip of Africa, currently in South Africa) to restock company ships on their journey to East Asia. This post later became a fully-fledged colony, the Cape Colony (1652–1806). As Cape Colony attracted increasing Dutch and European settlement, the Dutch founded the city of Kaapstad (Cape Town).
By 1669, the Dutch East India Company was the richest private company in history, with a huge fleet of merchant ships and warships, tens of thousands of employees, a private army consisting of thousands of soldiers, and a reputation on the part of its stockholders for high dividend payments.
Dutch New Imperialism in Asia.
The company was in almost constant conflict with the English; relations were particularly tense following the Amboyna Massacre in 1623. During the 18th century, Dutch East India Company possessions were increasingly focused on the East Indies. After the fourth war between the United Provinces and England (1780–1784), the company suffered increasing financial difficulties. In 1799, the company was dissolved, commencing official colonisation of the East Indies. During the era of New Imperialism the territorial claims of the Dutch East India Company (VOC) expanded into a fully fledged colony named the Dutch East Indies. Partly driven by re-newed colonial aspirations of fellow European nation states the Dutch strived to establish unchallenged control of the archipelago now known as Indonesia.
Six years into formal colonisation of the East Indies, in Europe the Dutch Republic was occupied by the French forces of Napoleon. The Dutch government went into exile in England and formally ceded its colonial possessions to Great Britain. The pro-French Governor General of Java Jan Willem Janssens, resisted a British invasion force in 1811 until forced to surrender. British Governor Raffles, who the later founded the city of Singapore, ruled the colony the following 10 years of the British interregnum (1806–1816).
After the defeat of Napoleon and the Anglo-Dutch Treaty of 1814 colonial government of the East Indies was ceded back to the Dutch in 1817. The loss of South Africa and the continued scramble for Africa stimulated the Dutch to secure unchallenged dominion over its colony in the East Indies. The Dutch started to consolidate its power base through extensive military campaigns and elaborate diplomatic alliances with indigenous rulers ensuring the Dutch tricolor was firmly planted in all corners of the Archipelago. These military campaigns included: the Padri War (1821–1837), the Java War (1825–1830) and the Aceh War (1873–1904). This raised the need for a considerable military buildup of the colonial army (KNIL). From all over Europe soldiers were recruited to join the KNIL.
The Dutch concentrated their colonial enterprise in the Dutch East Indies (Indonesia) throughout the 19th century. The Dutch lost control over the East Indies to the Japanese during much of World War II. Following the war, the Dutch fought Indonesian independence forces after Japan surrendered to the Allies in 1945. In 1949 most of what was known as the Dutch East Indies was ceded to the independent Republic of Indonesia. In 1962 also Dutch New Guinea was annexed by Indonesia de facto ending Dutch imperialism in Asia.
British in India.
Portuguese, French, and British competition in India (1600–1763).
The English sought to stake out claims in India at the expense of the Portuguese dating back to the Elizabethan era. In 1600, Queen Elizabeth I incorporated the English East India Company (later the British East India Company), granting it a monopoly of trade from the Cape of Good Hope eastward to the Strait of Magellan. In 1639 it acquired Madras on the east coast of India, where it quickly surpassed Portuguese Goa as the principal European trading centre on the Indian Subcontinent.
Through bribes, diplomacy, and manipulation of weak native rulers, the company prospered in India, where it became the most powerful political force, and outrivaled its Portuguese and French competitors. For more than one hundred years, English and French trading companies had fought one another for supremacy, and, by the middle of the 18th century, competition between the British and the French had heated up. French defeat by the British under the command of Robert Clive during the Seven Years' War (1756–1763) marked the end of the French stake in India.
Collapse of Mughal India.
The British East India Company, although still in direct competition with French and Dutch interests until 1763, was able to extend its control over almost the whole of India in the century following the subjugation of Bengal at the 1757 Battle of Plassey. The British East India Company made great advances at the expense of a Mughal dynasty.
The reign of Aurangzeb had marked the height of Mughal power, By 1690. Mughal territorial expansion reached its greatest extent, Aurangzeb's Empire encompassed the entire Indian Subcontinent. But this period of power was followed by one of decline. Fifty years after the death of Aurangzeb, the great Mughal empire had crumbled. Meanwhile, marauding warlords, nobles, and others bent on gaining power left the Subcontinent increasingly anarchic. Although the Mughals kept the imperial title until 1858, the central government had collapsed, creating a power vacuum.
From Company to Crown.
Aside from defeating the French during the Seven Years' War, Robert Clive, the leader of the Company in India, defeated a key Indian ruler of Bengal at the decisive Battle of Plassey (1757), a victory that ushered in the beginning of a new period in Indian history, that of informal British rule. While still nominally the sovereign, the Mughal Indian emperor became more and more of a puppet ruler, and anarchy spread until the company stepped into the role of policeman of India. The transition to formal imperialism, characterised by Queen Victoria being crowned "Empress of India" in the 1870s was a gradual process. The first step toward cementing formal British control extended back to the late 18th century. The British Parliament, disturbed by the idea that a great business concern, interested primarily in profit, was controlling the destinies of millions of people, passed acts in 1773 and 1784 that gave itself the power to control company policies and to appoint the highest company official in India, the Governor-General. (This system of dual control lasted until 1858.) By 1818 the East India Company was master of all of India. Some local rulers were forced to accept its overlordship; others were deprived of their territories. Some portions of India were administered by the British directly; in others native dynasties were retained under British supervision.
Until 1858, however, much of India was still officially the dominion of the Mughal emperor. Anger among some social groups, however, was seething under the governor-generalship of James Dalhousie (1847–1856), who annexed the Punjab (1849) after victory in the Second Sikh War, annexed seven princely states on the basis of lapse, annexed the key state of Oudh on the basis of misgovernment, and upset cultural sensibilities by banning Hindu practices such as sati.
The 1857 Sepoy Rebellion, or Indian Mutiny, an uprising initiated by Indian troops, called sepoys, who formed the bulk of the Company's armed forces, was the key turning point. Rumour had spread among them that their bullet cartridges were lubricated with pig and cow fat. The cartridges had to be bit open, so this upset the Hindu and Muslim soldiers. The Hindu religion held cows sacred, and for Muslims pork was considered haraam. In one camp, 85 out of 90 sepoys would not accept the cartridges from their garrison officer. The British harshly punished those who would not by jailing them. The Indian people were outraged, and on May 10, 1857, sepoys marched to Delhi, and, with the help of soldiers stationed there, captured it. Fortunately for the British, many areas remained loyal and quiescent, allowing the revolt to be crushed after fierce fighting. One important consequence of the revolt was the final collapse of the Mughal dynasty. The mutiny also ended the system of dual control under which the British government and the British East India Company shared authority. The government relieved the company of its political responsibilities, and in 1858, after 258 years of existence, the company relinquished its role. Trained civil servants were recruited from graduates of British universities, and these men set out to rule India. Lord Canning (created earl in 1859), appointed Governor-General of India in 1856, became known as "Clemency Canning" as a term of derision for his efforts to restrain revenge against the Indians during the Indian Mutiny. When the Government of India was transferred from the Company to the Crown, Canning became the first viceroy of India.
The Company initiated the first of the Anglo-Burmese wars in 1824, which led to total annexation of Burma by the Crown in 1885. The British ruled Burma as a province of British India until 1937, then administered her separately under the Burma Office except during the Japanese occupation of Burma, 1942–1945, until granted independence on 4 January 1948. (Unlike India, Burma opted not to join the Commonwealth of Nations.)
Rise of Indian nationalism.
The denial of equal status to Indians was the immediate stimulus for the formation in 1885 of the Indian National Congress, initially loyal to the Empire but committed from 1905 to increased self-government and by 1930 to outright independence. The "Home charges", payments transferred from India for administrative costs, were a lasting source of nationalist grievance, though the flow declined in relative importance over the decades to independence in 1947.
Although majority Hindu and minority Muslim political leaders were able to collaborate closely in their criticism of British policy into the 1920s, British support for a distinct Muslim political organisation, the Muslim League from 1906 and insistence from the 1920s on separate electorates for religious minorities, is seen by many in India as having contributed to Hindu-Muslim discord and the country's eventual Partition.
France in Indochina.
France, which had lost its empire to the British by the end of the 18th century, had little geographical or commercial basis for expansion in Southeast Asia. After the 1850s, French imperialism was initially impelled by a nationalistic need to rival the United Kingdom and was supported intellectually by the notion that French culture was superior to that of the people of Annam, and its "mission civilisatrice"—or its "civilizing mission" of the Annamese through their assimilation to French culture and the Catholic religion. The pretext for French expansionism in Indochina was the protection of French religious missions in the area, coupled with a desire to find a southern route to China through Tonkin, the European name for a region of northern Vietnam.
French religious and commercial interests were established in Indochina as early as the 17th century, but no concerted effort at stabilizing the French position was possible in the face of British strength in the Indian Ocean and French defeat in Europe at the beginning of the 19th century. A mid-19th century religious revival under the Second Empire provided the atmosphere within which interest in Indochina grew. Anti-Christian persecutions in the Far East provided the pretext for the bombardment of Tourane (Da Nang) in 1847, and invasion and occupation of Da Nang in 1857 and Saigon in 1858. Under Napoleon III, France decided that French trade with China would be surpassed by the British, and accordingly the French joined the British against China in the Second Opium War from 1857 to 1860, and occupied parts of Vietnam as its gateway to China.
By the Treaty of Saigon in 1862, on June 5, the Vietnamese emperor ceded France three provinces of southern Vietnam to form the French colony of Cochinchina; France also secured trade and religious privileges in the rest of Vietnam and a protectorate over Vietnam's foreign relations. Gradually French power spread through exploration, the establishment of protectorates, and outright annexations. Their seizure of Hanoi in 1882 led directly to war with China (1883–1885), and the French victory confirmed French supremacy in the region. France governed Cochinchina as a direct colony, and central and northern Vietnam under the protectorates of Annam and Tonkin, and Cambodia as protectorates in one degree or another. Laos too was soon brought under French "protection."
By the beginning of the 20th century, France had created an empire in Indochina nearly 50 percent larger than the mother country. A Governor-General in Hanoi ruled Cochinchina directly and the other regions through a system of residents. Theoretically, the French maintained the precolonial rulers and administrative structures in Annam, Tonkin, Cochinchina, Cambodia, and Laos, but in fact the governor-generalship was a centralised fiscal and administrative regime ruling the entire region. Although the surviving native institutions were preserved in order to make French rule more acceptable, they were almost completely deprived of any independence of action. The ethnocentric French colonial administrators sought to assimilate the upper classes into France's "superior culture." While the French improved public services and provided commercial stability, the native standard of living declined and precolonial social structures eroded. Indochina, which had a population of over eighteen million in 1914, was important to France for its tin, pepper, coal, cotton, and rice. It is still a matter of debate, however, whether the colony was commercially profitable.
Russia and "The Great Game".
Tsarist Russia is not often regarded as a colonial power such as the United Kingdom or France because of the manner of Russian expansions: unlike the United Kingdom, which expanded overseas, the Russian empire grew from the centre outward by a process of accretion, like the United States. In the 19th century, Russian expansion took the form of a struggle of an effectively landlocked country for access to a warm water port.
Qing China defeated Russia in the Sino-Russian border conflicts.
While the British were consolidating their hold on India, Russian expansion had moved steadily eastward to the Pacific, then toward the Middle East. In the early 19th century it succeeded in conquering the South Caucasus and Dagestan from Qajar Persia following the Russo-Persian War (1804–13), the Russo-Persian War (1826–28) and the out coming treaties of Gulistan and Turkmenchay, giving Russia direct borders with both Persia's as well as Ottoman Turkey's heartlands. Later, they eventually reached the frontiers of Afghanistan as well (which had the largest foreign border adjacent to British holdings in India). In response to Russian expansion, the defense of India's land frontiers and the control of all sea approaches to the Subcontinent via the Suez Canal, the Red Sea, and the Persian Gulf became preoccupations of British foreign policy in the 19th century.
Anglo-Russian rivalry in the Middle East and Central Asia led to a brief confrontation over Afghanistan in the 1870s. In Persia (Iran), both nations set up banks to extend their economic influence. The United Kingdom went so far as to invade Tibet, a land subordinate to the Chinese empire, in 1904, but withdrew when it became clear that Russian influence was insignificant and when Chinese resistance proved tougher than expected.
In 1907, the United Kingdom and Russia signed an agreement which — on the surface —ended their rivalry in Central Asia. ("see" Anglo-Russian Entente) As part of the entente, Russia agreed to deal with the sovereign of Afghanistan only through British intermediaries. In turn, the United Kingdom would not annex or occupy Afghanistan. Chinese suzerainty over Tibet also was recognised by both Russia and the United Kingdom, since nominal control by a weak China was preferable to control by either power. Persia was divided into Russian and British spheres of influence and an intervening "neutral" zone. The United Kingdom and Russia chose to reach these uneasy compromises because of growing concern on the part of both powers over German expansion in strategic areas of China and Africa.
Following the entente, Russia increasingly intervened in Persian domestic politics and suppressed nationalist movements that threatened both St. Petersburg and London. After the Russian Revolution, Russia gave up its claim to a sphere of influence, though Soviet involvement persisted alongside the United Kingdom's until the 1940s.
In the Middle East, in Persia (Iran) and the Ottoman Empire, a German company built a railroad from Constantinople to Baghdad and the Persian Gulf in the latter, while it built a railroad from the north of the country to the south, connecting the Caucasus with the Persian Gulf in the former. Germany wanted to gain economic influence in the region and then, perhaps, move on to India. This was met with bitter resistance by the United Kingdom, Russia, and France who divided the region among themselves.
European intrusions into China.
The 16th century brought many Jesuit missionaries to China, such as Matteo Ricci, who established missions where Western science was introduced, and where Europeans gathered knowledge of Chinese society, history, culture, and science. During the 18th century, merchants from Western Europe came to China in increasing numbers. However, merchants were confined to Guangzhou and the Portuguese colony of Macau, as they had been since the 16th century. European traders were increasingly irritated by what they saw as the relatively high customs duties they had to pay and by the attempts to curb the growing import trade in opium. By 1800, its importation was forbidden by the imperial government. However, the opium trade continued to boom.
Early in the 19th century, serious internal weaknesses developed in the Qing dynasty that left China vulnerable to Western, Meiji period Japanese, and Russian imperialism. In 1839, China found itself fighting the First Opium War with Britain. China was defeated, and in 1842, signed the provisions of the Treaty of Nanjing which were first of the unequal treaties signed during the Qing Dynasty. Hong Kong Island was ceded to Britain, and certain ports, including Shanghai and Guangzhou, were opened to British trade and residence. In 1856, the Second Opium War broke out. The Chinese were again defeated, and now forced to the terms of the 1858 Treaty of Tientsin. The treaty opened new ports to trade and allowed foreigners to travel in the interior. In addition, Christians gained the right to propagate their religion. The United States Treaty of Wanghia and Russia later obtained the same prerogatives in separate treaties.
Toward the end of the 19th century, China appeared on the way to territorial dismemberment and economic vassalage—the fate of India's rulers that played out much earlier. Several provisions of these treaties caused long-standing bitterness and humiliation among the Chinese: extraterritoriality (meaning that in a dispute with a Chinese person, a Westerner had the right to be tried in a court under the laws of his own country), customs regulation, and the right to station foreign warships in Chinese waters, including its navigable rivers.
Jane E. Elliott criticized the allegation that China refused to modernize or was unable to defeat Western armies as simplistic, noting that China embarked on a massive military modernization in the late 1800s after several defeats, buying weapons from Western countries and manufacturing their own at arsenals, such as the Hanyang Arsenal during the Boxer Rebellion. In addition, Elliott questioned the claim that Chinese society was traumatized by the Western victories, as many Chinese peasants (90% of the population at that time) living outside the concessions continued about their daily lives, uninterrupted and without any feeling of "humiliation".
Historians have judged the Qing dynasty's vulnerability and weakness to foreign imperialism in the 19th century to be based mainly on its maritime naval weakness while it achieved military success against westerners on land, the historian Edward L. Dreyer said that "China’s nineteenth-century humiliations were strongly related to her weakness and failure at sea. At the start of the Opium War, China had no unified navy and no sense of how vulnerable she was to attack from the sea; British forces sailed and steamed wherever they wanted to go...In the Arrow War (1856-60), the Chinese had no way to prevent the Anglo-French expedition of 1860 from sailing into the Gulf of Zhili and landing as near as possible to Beijing. Meanwhile, new but not exactly modern Chinese armies suppressed the midcentury rebellions, bluffed Russia into a peaceful settlement of disputed frontiers in Central Asia, and defeated the French forces on land in the Sino-French War (1884-85). But the defeat of the fleet, and the resulting threat to steamship traffic to Taiwan, forced China to conclude peace on unfavorable terms."
During the Sino-French War, Chinese forces defeated the French at the Battle of Cầu Giấy (Paper Bridge), Bắc Lệ ambush, Battle of Phu Lam Tao, Battle of Zhenhai, the Battle of Tamsui in the Keelung Campaign and in the last battle which ended the war, the Battle of Bang Bo (Zhennan Pass), which triggered the French Retreat from Lạng Sơn and resulted in the collapse of the French Jules Ferry government in the Tonkin Affair.
The Qing dynasty forced Russia to hand over disputed territory in Ili in the Treaty of Saint Petersburg (1881), in what was widely seen by the west as a diplomatic victory for the Qing. Russia acknowledged that Qing China potentially posed a serious military threat. Mass media in the west during this era portrayed China as a rising military power due to its modernization programs and as a major threat to the western world, invoking fears that China would successfully conquer western colonies like Australia.
The British observer Demetrius Charles de Kavanagh Boulger suggested a British-Chinese alliance to check Russian expansion in Central Asia.
During the Ili crisis when Qing China threatened to go to war against Russia over the Russian occupation of Ili, the British officer Charles George Gordon was sent to China by Britain to advise China on military options against Russia should a potential war break out between China and Russia.
The Russians observed the Chinese building up their arsenal of modern weapons during the Ili crisis, the Chinese bought thousands of rifles from Germany. In 1880 massive amounts of military equipment and rifles were shipped via boats to China from Antwerp as China purchased torpedoes, artillery, and 260,260 modern rifles from Europe.
The Russian military observer D. V. Putiatia visited China in 1888 and found that in Northeastern China (Manchuria) along the Chinese-Russian border,the Chinese soldiers were potentially able to become adept at "European tactics" under certain circumstances, and the Chinese soldiers were armed with modern weapons like Krupp artillery, Winchester carbines, and Mauser rifles.
Compared to Russian controlled areas, more benefits were given to the Muslim Kirghiz on the Chinese controlled areas. Russian settlers fought against the Muslim nomadic Kirghiz, which led the Russians to believe that the Kirghiz would be a liability in any conflict against China. The Muslim Kirghiz were sure that in an upcoming war, that China would defeat Russia.
Russian sinologists, the Russian media, threat of internal rebellion, the pariah status inflicted by the Congress of Berlin, the negative state of the Russian economy all led Russia to concede and negotiate with China in St Petersburg, and return most of Ili to China.
The rise of Japan since the Meiji Restoration as an imperial power led to further subjugation of China. In a dispute over China's longstanding claim of suzerainty in Korea, war broke out between China and Japan, resulting in humiliating defeat for the Chinese. By the Treaty of Shimonoseki (1895), China was forced to recognize effective Japanese rule of Korea and Taiwan was ceded to Japan until its recovery in 1945 at the end of the WWII by the Republic of China.
China's defeat at the hands of Japan was another trigger for future aggressive actions by Western powers. In 1897, Germany demanded and was given a set of exclusive mining and railroad rights in Shandong province. Russia obtained access to Dairen and Port Arthur and the right to build a railroad across Manchuria, thereby achieving complete domination over a large portion of northwestern China. The United Kingdom and France also received a number of concessions. At this time, much of China was divided up into "spheres of influence": Germany dominated Jiaozhou (Kiaochow) Bay, Shandong, and the Huang He (Hwang-Ho) valley; Russia dominated the Liaodong Peninsula and Manchuria; the United Kingdom dominated Weihaiwei and the Yangtze Valley; and France dominated the Guangzhou Bay and several other southern provinces.
China continued to be divided up into these spheres until the United States, which had no sphere of influence, grew alarmed at the possibility of its businessmen being excluded from Chinese markets. In 1899, Secretary of State John Hay asked the major powers to agree to a policy of equal trading privileges. In 1900, several powers agreed to the U.S.-backed scheme, giving rise to the "Open Door" policy, denoting freedom of commercial access and non-annexation of Chinese territory. In any event, it was in the European powers' interest to have a weak but independent Chinese government. The privileges of the Europeans in China were guaranteed in the form of treaties with the Qing government. In the event that the Qing government totally collapsed, each power risked losing the privileges that it already had negotiated.
The erosion of Chinese sovereignty and seizures of land from Chinese by foreigners contributed to a spectacular anti-foreign outbreak in June 1900, when the "Boxers" (properly the society of the "righteous and harmonious fists") attacked foreigners around Beijing. The Imperial Court was divided into anti-foreign and pro-foreign factions, with the pro-foreign faction led by Ronglu and Prince Qing hampering any military effort by the anti-foreign faction lead by Prince Duan and Dong Fuxiang. The Qing Empress Dowager ordered all diplomatic ties to be cut off and all foreigners to leave the legations in Beijing to go to Tianjin. The foreigners refused to leave. Fueled by entirely false reports that the foreigners in the legations were massacred, the Eight-Nation Alliance decided to launch an expedition on Beijing to reach the legations but they underestimated the Qing military. The Qing and Boxers defeated the foreigners at the Seymour Expedition, forcing them to turn back at the Battle of Langfang. In response to the foreign attack on Dagu Forts the Qing responded by declaring war against the foreigners. the Qing forces and foreigners fought a fierce battle at the Battle of Tientsin before the foreigners could launch a second expedition. On their second try Gaselee Expedition, with a much larger force, the foreigners managed to reach Beijing and fight the Battle of Peking (1900). British and French forces looted, plundered and burned the Old Summer Palace to the ground for the second time (the first time being in 1860, following the Second Opium War). German forces were particularly severe in exacting revenge for the killing of their ambassador due to the orders of Kaiser Wilhelm II, who held anti-Asian sentiments, while Russia tightened its hold on Manchuria in the northeast until its crushing defeat by Japan in the war of 1904–1905. The Qing court evacuated to Xi'an and threatened to continue the war against foreigners, until the foreigners tempered their demands in the Boxer Protocol, promising that China would not have to give up any land and gave up the demands for the execution of Dong Fuxiang and Prince Duan.
The correspondent Douglas Story observed Chinese troops in 1907 and praised their abilities and military skill.
Extraterritorial jurisdiction was abandoned by the United Kingdom and the United States in 1943. Chiang Kai-shek forced the French to hand over all their concessions back to China control after World War II. Foreign political control over leased parts of China ended with the incorporation of Hong Kong and the small Portuguese territory of Macau into the People's Republic of China in 1997 and 1999 respectively.
U.S. imperialism in Asia.
Some Americans advocated for the annexation of Taiwan from China. Aboriginals on Taiwan often attacked and massacred shipwrecked western sailors. In 1867, during the Rover incident, Taiwanese aborigines attacked shipwrecked American sailors, killing the entire crew. They subsequently defeated a retaliatory expedition by the American military and killed another American during the battle.
As the United States emerged as a new imperial power in the Pacific and Asia, one of the two oldest Western imperialist powers in the regions, Spain, was finding it increasingly difficult to maintain control of territories it had held in the regions since the 16th century. In 1896, a widespread revolt against Spanish rule broke out in the Philippines. Meanwhile, the recent string of U.S. territorial gains in the Pacific posed an even greater threat to Spain's remaining colonial holdings.
As the U.S. continued to expand its economic and military power in the Pacific, it declared war against Spain in 1898. During the Spanish–American War, U.S. Admiral Dewey destroyed the Spanish fleet at Manila and U.S. troops landed in the Philippines. Spain later agreed by treaty to cede the Philippines in Asia and Guam in the Pacific. In the Caribbean, Spain ceded Puerto Rico to the U.S. The war also marked the end of Spanish rule in Cuba, which was to be granted nominal independence but remained heavily influenced by the U.S. government and U.S. business interests. One year following its treaty with Spain, the U.S. occupied the small Pacific outpost of Wake Island.
The Filipinos, who assisted U.S. troops in fighting the Spanish, wished to establish an independent state and, on June 12, 1898, declared independence from Spain. In 1899, fighting between the Filipino nationalists and the U.S. broke out; it took the U.S. almost fifteen years to fully subdue the insurgency. The U.S. sent 70,000 troops and suffered thousands of casualties. The Filipinos insurgents, however, suffered considerably higher casualties than the Americans. Most casualties in the war were civilians dying primarily from disease.
U.S. attacks into the countryside often included scorched earth campaigns where entire villages were burned and destroyed, and concentrated civilians into camps known as "protected zones." Most of these civilian casualties resulted from disease and famine. Reports of the execution of U.S. soldiers taken prisoner by the Filipinos led to disproportionate reprisals by American forces.
The Moro Muslims fought against the Americans in the Moro Rebellion.
In 1914, Dean C. Worcester, U.S. Secretary of the Interior for the Philippines (1901–1913) described "the regime of civilisation and improvement which started with American occupation and resulted in developing naked savages into cultivated and educated men." Nevertheless, some Americans, such as Mark Twain, deeply opposed American involvement/imperialism in the Philippines, leading to the abandonment of attempts to construct a permanent U.S. naval base and using it as an entry point to the Chinese market. In 1916, Congress guaranteed the independence of the Philippines by 1945.
World War I: Changes in Imperialism.
World War I brought about the fall of several empires in Europe. This had repercussions around the world. The defeated Central Powers included Germany and the Turkish Ottoman Empire. Germany lost all of its colonies in Asia. German New Guinea, a part of Papua New Guinea, became administered by Australia. German possessions and concessions in China, including Qingdao, became the subject of a controversy during the Paris Peace Conference when the Beiyang government in China agreed to cede these interests to Japan, to the anger of many Chinese people. Although the Chinese diplomats refused to sign the agreement, these interests were ceded to Japan with the support of the United States and the United Kingdom.
Turkey gave up her provinces; Syria, Palestine, and Mesopotamia (now Iraq) came under French and British control as League of Nations Mandates. The discovery of petroleum first in Iran and then in the Arab lands in the interbellum provided a new focus for activity on the part of the United Kingdom, France, and the United States.
Japan.
In 1641, all Westerners were thrown out of Japan. For the next two centuries, Japan was free from Western influence, except for at the port of Nagasaki, which Japan allowed Dutch merchant vessels to enter on a limited basis.
Japan's freedom from Western penetration ended on 8 July 1853, when Commodore Matthew Perry of the U.S. Navy sailed a squadron of black-hulled warships into Edo (modern Tokyo) harbor. The Japanese told Perry to sail to Nagasaki but he refused. Perry sought to present a letter from U.S. President Millard Fillmore to the emperor which demanded concessions from Japan. Japanese authorities responded by stating that they could not present the letter directly to the emperor, but scheduled a meeting on July 14 with a representative of the emperor. On 14 July, the squadron sailed towards the shore, giving a demonstration of their cannon's firepower thirteen times. Perry landed with a large detachment of Marines and presented the emperor's representative with Fillmore's letter. Perry said he would return, and did so, this time with even more war ships. The U.S. show of force led to Japan's concession to the Convention of Kanagawa on 31 March 1854. This treaty conferred extraterritoriality on American nationals, as well as, opening up further treaty ports beyond Nagasaki. This treaty was followed up by similar treaties with the United Kingdom, the Netherlands, Russia and France. These events made Japanese authorities aware that the country was lacking technologically and needed the strength of industrialism in order to keep their power. This realisation eventually led to a civil war and political reform known the Meiji Restoration.
The Meiji Restoration of 1868 led to administrative overhaul, deflation and subsequent rapid economic development. Japan had limited natural resources of her own and sought both overseas markets and sources of raw materials, fuelling a drive for imperial conquest which began with the defeat of China in 1895.
Taiwan, ceded by Qing Dynasty China, became the first Japanese colony. In 1899, Japan won agreements from the great powers' to abandon extraterritoriality for their citizens, and an alliance with the United Kingdom established it in 1902 as an international power. Its spectacular defeat of Russia's navy in 1905 gave it the southern half of the island of Sakhalin; exclusive Japanese influence over Korea (propinquity); the former Russian lease of the Liaodong Peninsula with Port Arthur (Lüshunkou); and extensive rights in Manchuria (see the Russo-Japanese War).
The Empire of Japan and the Joseon Dynasty in Korea formed bilateral diplomatic relations in 1876. China lost its suzerainty of Korea after defeat in the Sino-Japanese War in 1894. Russia also lost influence on the Korean peninsula with the Treaty of Portsmouth as a result of the Russo-Japanese war in 1904. The Joseon Dynasty became increasingly dependent on Japan. Korea became a protectorate of Japan with the Japan–Korea Treaty of 1905. Korea was then "de jure" annexed to Japan with the Japan–Korea Treaty of 1910.
Japan was now one of the most powerful forces in the Far East, and in 1914, it entered World War I on the side of the Allies, seizing German-occupied Kiaochow and subsequently demanding Chinese acceptance of Japanese political influence and territorial acquisitions (Twenty-One Demands, 1915). Mass protests in Peking in 1919 coupled with Allied (and particularly U.S.) opinion led to Japan's abandonment of most of the demands and Joseon's 1922 return to China. Japan received the German territory from the Treaty of Versailles, 1919, sparking widespread Chinese nationalism.
Tensions with China increased over the 1920s, and in 1931 Japanese army units based in Manchuria seized control of the region without direction from Tokyo. Intermittent conflict with China led to full-scale war in mid-1937, drawing Japan toward an overambitious bid for Asian hegemony (Greater East Asia Co-Prosperity Sphere), which ultimately led to defeat and the loss of all its overseas territories after World War II (see Japanese expansionism and Japanese nationalism).
Post World War II era.
Decolonisation and the rise of nationalism in Asia.
In the aftermath of World War II, European colonies, controlling more than one billion people throughout the world, still ruled most of the Middle East, South East Asia, and the Indian Subcontinent. However, the image of European pre-eminence was shattered by the wartime Japanese occupations of large portions of British, French, and Dutch territories in the Pacific. The destabilisation of European rule led to the rapid growth of nationalist movements in Asia—especially in Indonesia, Malaya, Burma, and French Indochina.
The war, however, only accelerated forces already in existence undermining Western imperialism in Asia. Throughout the colonial world, the processes of urbanisation and capitalist investment created professional merchant classes that emerged as new Westernised elites. While imbued with Western political and economic ideas, these classes increasingly grew to resent their unequal status under European rule.
British in India and the Middle East.
In India, the westward movement of Japanese forces towards Bengal during World War II had led to major concessions on the part of British authorities to Indian nationalist leaders. In 1947, the United Kingdom, devastated by war and embroiled in economic crisis at home, granted British India its independence as two nations: India and Pakistan. The following year independence was granted to Burma and Ceylon. In the Middle East, the United Kingdom granted independence to Jordan in 1946 and two years later ended its mandate of Palestine.
Dutch East Indies.
Following the end of the war, nationalists in Indonesia demanded complete independence from the Netherlands. A brutal conflict ensued, and finally, in 1949, through United Nations mediation, the Dutch East Indies achieved independence, becoming the new nation of Indonesia. Dutch imperialism moulded this new multi-ethnic state comprising roughly 3,000 islands of the Indonesian archipelago with a population at the time of over 100 million.
The end of Dutch rule opened up latent tensions between the roughly 300 distinct ethnic groups of the islands, with the major ethnic fault line being between the Javanese and the non-Javanese.
Netherlands New Guinea was under the Dutch administration until 1962 (see also West New Guinea dispute).
United States in Asia.
In the Philippines, the U.S. remained committed to its previous pledges to grant the islands their independence, and the Philippines became the first of the Western-controlled Asian colonies to be granted independence post-World War II. However, the Philippines remained under pressure to adopt a political and economic system similar to their old imperial master.
This aim was greatly complicated by the rise of new political forces. During the war, the "Hukbalahap" (People's Army), which had strong ties to the Communist Party of the Philippines (PKP), fought against the Japanese occupation of the Philippines and won strong popularity among many sectors of the Filipino working class and peasantry. In 1946, the PKP participated in elections as part of the Democratic Alliance. However, with the onset of the Cold War, its growing political strength drew a reaction from the ruling government and the United States, resulting in the repression of the PKP and its associated organisations. In 1948, the PKP began organizing an armed struggle against the government and continued U.S. military presence. In 1950, the PKP created the People's Liberation Army ("Hukbong Mapagpalaya ng Bayan"), which mobilised thousands of troops throughout the islands. The insurgency lasted until 1956, when the PKP gave up armed struggle.
In 1968, the PKP underwent a split, and in 1969 the Maoist faction of the PKP created the New People's Army. Maoist rebels re-launched an armed struggle against the government and the U.S. military presence in the Philippines, which continues to this day.
France in Indochina.
Post-war resistance to French rule.
France remained determined to retain its control of Indochina. However, in Hanoi, in 1945, a broad front of nationalists and communists led by Ho Chi Minh declared an independent Republic of Vietnam, commonly referred to as the Viet Minh regime by Western outsiders. France, seeking to regain control of Vietnam, countered with a vague offer of self-government under French rule. France's offers were unacceptable to Vietnamese nationalists; and in December 1946 the Việt Minh launched a rebellion against the French authority governing the colonies of French Indochina. The first few years of the war involved a low-level rural insurgency against French authority. However, after the Chinese communists reached the Northern border of Vietnam in 1949, the conflict turned into a conventional war between two armies equipped with modern weapons supplied by the United States and the Soviet Union. Meanwhile, the France granted the State of Vietnam based in Saigon independence in 1949 whilst Laos and Cambodia received independence in 1953. The US recognized the regime in Saigon, and provided the French military effort with military aid.
Meanwhile, in Vietnam, the French war against the Viet Minh continued for nearly eight years. The French were gradually worn down by guerrilla and jungle fighting. The turning point for France occurred at Dien Bien Phu in 1954, which resulted in the surrender of ten thousand French troops. Paris was forced to accept a political settlement that year at the Geneva Conference, which led to a precarious set of agreements regarding the future political status of Laos, Cambodia, and Vietnam.

</doc>
<doc id="15445" url="https://en.wikipedia.org/wiki?curid=15445" title="Entropy (information theory)">
Entropy (information theory)

In information theory, systems are modeled by a transmitter, channel, and receiver. The transmitter produces messages that are sent through the channel. The channel modifies the message in some way. The receiver attempts to infer which message was sent. In this context, entropy (more specifically, Shannon entropy) is the expected value (average) of the information contained in each message. 'Messages' can be modeled by any flow of information.
In a more technical sense, there are reasons (explained below) to define information as the negative of the logarithm of the probability distribution. The probability distribution of the events, coupled with the information amount of every event, forms a random variable whose expected value is the average amount of information, or entropy, generated by this distribution. Units of entropy are the shannon, nat, or hartley, depending on the base of the logarithm used to define it, though the shannon is commonly referred to as a bit.
The logarithm of the probability distribution is useful as a measure of entropy because it is additive for independent sources. For instance, the entropy of a coin toss is 1 shannon, whereas of tosses it is shannons. Generally, you need bits to represent a variable that can take one of values if is a power of 2. If these values are equally probable, the entropy (in shannons) is equal to the number of bits. Equality between number of bits and shannons holds only while all outcomes are equally probable. If one of the events is more probable than others, observation of that event is less informative. Conversely, rarer events provide more information when observed. Since observation of less probable events occurs more rarely, the net effect is that the entropy (thought of as average information) received from non-uniformly distributed data is less than . Entropy is zero when one outcome is certain. Shannon entropy quantifies all these considerations exactly when a probability distribution of the source is known. The "meaning" of the events observed (the meaning of "messages") does not matter in the definition of entropy. Entropy only takes into account the probability of observing a specific event, so the information it encapsulates is information about the underlying probability distribution, not the meaning of the events themselves.
Generally, "entropy" refers to disorder or uncertainty. Shannon entropy was introduced by Claude E. Shannon in his 1948 paper "A Mathematical Theory of Communication". Shannon entropy provides an absolute limit on the best possible average length of lossless encoding or compression of an information source. Rényi entropy generalizes Shannon entropy.
Introduction.
Entropy is a measure of "unpredictability" of "information content". To get an informal, intuitive understanding of the connection between these three English terms, consider the example of a poll on some political issue. Usually, such polls happen because the outcome of the poll isn't already known. In other words, the outcome of the poll is relatively "unpredictable", and actually performing the poll and learning the results gives some new "information"; these are just different ways of saying that the "entropy" of the poll results is large. Now, consider the case that the same poll is performed a second time shortly after the first poll. Since the result of the first poll is already known, the outcome of the second poll can be predicted well and the results should not contain much new information; in this case the entropy of the second poll result is small relative to the first.
Now consider the example of a coin toss. When the coin is fair, that is, when the probability of heads is the same as the probability of tails, then the entropy of the coin toss is as high as it could be. This is because there is no way to predict the outcome of the coin toss ahead of time—the best we can do is predict that the coin will come up heads, and our prediction will be correct with probability 1/2. Such a coin toss has one bit of entropy since there are two possible outcomes that occur with equal probability, and learning the actual outcome contains one bit of information. Contrarily, a coin toss with a coin that has two heads and no tails has zero entropy since the coin will always come up heads, and the outcome can be predicted perfectly. Analogously, one binary bit has a formula_1 Shannon or bit entropy because it can have one of two values (1 and 0). Similarly, one trit contains formula_2 (about 1.58496) bits of information because it can have one of three values.
English text has fairly low entropy. In other words, it is fairly predictable. Even if we don't know exactly what is going to come next, we can be fairly certain that, for example, there will be many more e's than z's, that the combination 'qu' will be much more common than any other combination with a 'q' in it, and that the combination 'th' will be more common than 'z', 'q', or 'qu'. After the first few letters one can often guess the rest of the word. English text has between 0.6 and 1.3 bits of entropy for each character of message.
The Chinese version of Wikipedia points out that Chinese characters have a much higher entropy than English. Each character of Chinese has about -log2(1/2500)=11.3 bits, almost three times higher than English. However, the discussion could be much more sophisticated than this simple calculation because in English the usage of words, not only characters, and redundancy factors could be considered.
If a compression scheme is lossless—that is, you can always recover the entire original message by decompressing—then a compressed message has the same quantity of information as the original, but communicated in fewer characters. That is, it has more information, or a higher entropy, per character. This means a compressed message has less redundancy. Roughly speaking, Shannon's source coding theorem says that a lossless compression scheme cannot compress messages, on average, to have "more" than one bit of information per bit of message, but that any value "less" than one bit of information per bit of message can be attained by employing a suitable coding scheme. The entropy of a message per bit multiplied by the length of that message is a measure of how much total information the message contains.
Shannon's theorem also implies that no lossless compression scheme can shorten "all" messages. If some messages come out shorter, at least one must come out longer due to the pigeonhole principle. In practical use, this is generally not a problem, because we are usually only interested in compressing certain types of messages, for example English documents as opposed to gibberish text, or digital photographs rather than noise, and it is unimportant if a compression algorithm makes some unlikely or uninteresting sequences larger. However, the problem can still arise even in everyday use when applying a compression algorithm to already compressed data: for example, making a ZIP file of music, pictures or videos that are already in a compressed format such as FLAC, MP3, WebM, MP4, PNG or JPEG will generally result in a ZIP file that is slightly "larger" than the source file(s).
Definition.
Named after Boltzmann's Η-theorem, Shannon defined the entropy Η (Greek letter Eta) of a discrete random variable with possible values } and probability mass function as:
Here E is the expected value operator, and I is the information content of .
The entropy can explicitly be written as
where is the base of the logarithm used. Common values of are 2, Euler's number , and 10, and the unit of entropy is shannon for , nat for , and hartley for . When , the units of entropy are also commonly referred to as bits.
In the case of for some , the value of the corresponding summand is taken to be 0, which is consistent with the limit:
When the distribution is continuous rather than discrete, the sum is replaced with an integral as
where represents a probability density function.
One may also define the conditional entropy of two events and taking values and respectively, as
where is the probability that and . This quantity should be understood as the amount of randomness in the random variable given the event .
Example.
Consider tossing a coin with known, not necessarily fair, probabilities of coming up heads or tails; this is known as the Bernoulli process.
The entropy of the unknown result of the next toss of the coin is maximized if the coin is fair (that is, if heads and tails both have equal probability 1/2). This is the situation of maximum uncertainty as it is most difficult to predict the outcome of the next toss; the result of each toss of the coin delivers one full bit of information.
However, if we know the coin is not fair, but comes up heads or tails with probabilities and , where , then there is less uncertainty. Every time it is tossed, one side is more likely to come up than the other. The reduced uncertainty is quantified in a lower entropy: on average each toss of the coin delivers less than one full bit of information.
The extreme case is that of a double-headed coin that never comes up tails, or a double-tailed coin that never results in a head. Then there is no uncertainty. The entropy is zero: each toss of the coin delivers no new information as the outcome of each coin toss is always certain. In this respect, entropy can be normalized by dividing it by information length. This ratio is called metric entropy and is a measure of the randomness of the information.
Rationale.
To understand the meaning of , at first, try to define an information function, , in terms of an event with probability . How much information is acquired due to the observation of event ? Shannon's solution follows from the fundamental properties of information:
The last is a crucial property. It states that joint probability communicates as much information as two individual events separately. Particularly, if the first event can yield one of equiprobable outcomes and another has one of equiprobable outcomes then there are possible outcomes of the joint event. This means that if bits are needed to encode the first value and to encode the second, one needs to encode both. Shannon discovered that the proper choice of function to quantify information, preserving this additivity, is logarithmic, i.e.,
The base of the logarithm can be any fixed real number greater than 1. The different units of information (bits for , trits for , nats for the natural logarithm and so on) are just constant multiples of each other. (In contrast, the entropy would be negative if the base of the logarithm were less than 1.) For instance, in case of a fair coin toss, heads provides bit of information, which is approximately 0.693 nats or 0.631 trits. Because of additivity, tosses provide bits of information, which is approximately nats or trits.
Now, suppose we have a distribution where event can happen with probability . Suppose we have sampled it times and outcome was, accordingly, seen times. The total amount of information we have received is 
The average amount of information that we receive with every event is therefore
Aspects.
Relationship to thermodynamic entropy.
The inspiration for adopting the word "entropy" in information theory came from the close resemblance between Shannon's formula and very similar known formulae from statistical mechanics.
In statistical thermodynamics the most general formula for the thermodynamic entropy of a thermodynamic system is the Gibbs entropy,
where is the Boltzmann constant, and is the probability of a microstate. The Gibbs entropy was defined by J. Willard Gibbs in 1878 after earlier work by Boltzmann (1872).
The Gibbs entropy translates over almost unchanged into the world of quantum physics to give the von Neumann entropy, introduced by John von Neumann in 1927,
where ρ is the density matrix of the quantum mechanical system and Tr is the trace.
At an everyday practical level the links between information entropy and thermodynamic entropy are not evident. Physicists and chemists are apt to be more interested in "changes" in entropy as a system spontaneously evolves away from its initial conditions, in accordance with the second law of thermodynamics, rather than an unchanging probability distribution. And, as the minuteness of Boltzmann's constant indicates, the changes in for even tiny amounts of substances in chemical and physical processes represent amounts of entropy that are extremely large compared to anything in data compression or signal processing. Furthermore, in classical thermodynamics the entropy is defined in terms of macroscopic measurements and makes no reference to any probability distribution, which is central to the definition of information entropy.
The connection between thermodynamics and what is now known as information theory was first made by Ludwig Boltzmann and expressed by his famous equation:
where "S" is the thermodynamic entropy of a particular macrostate (defined by thermodynamic parameters such as temperature, volume, energy, etc.), "W" is the number of microstates (various combinations of particles in various energy states) that can yield the given macrostate, and "kB" is Boltzmann's constant. It is assumed that each microstate is equally likely, so that the probability of a given microstate is "pi = 1/W". When these probabilities are substituted into the above expression for the Gibbs entropy (or equivalently "kB" times the Shannon entropy), Boltzmann's equation results. In information theoretic terms, the information entropy of a system is the amount of "missing" information needed to determine a microstate, given the macrostate.
In the view of Jaynes (1957), thermodynamic entropy, as explained by statistical mechanics, should be seen as an "application" of Shannon's information theory: the thermodynamic entropy is interpreted as being proportional to the amount of further Shannon information needed to define the detailed microscopic state of the system, that remains uncommunicated by a description solely in terms of the macroscopic variables of classical thermodynamics, with the constant of proportionality being just the Boltzmann constant. For example, adding heat to a system increases its thermodynamic entropy because it increases the number of possible microscopic states of the system that are consistent with the measurable values of its macroscopic variables, thus making any complete state description longer. (See article: "maximum entropy thermodynamics"). Maxwell's demon can (hypothetically) reduce the thermodynamic entropy of a system by using information about the states of individual molecules; but, as Landauer (from 1961) and co-workers have shown, to function the demon himself must increase thermodynamic entropy in the process, by at least the amount of Shannon information he proposes to first acquire and store; and so the total thermodynamic entropy does not decrease (which resolves the paradox). Landauer's principle imposes a lower bound on the amount of heat a computer must generate to process a given amount of information, though modern computers are far less efficient.
Entropy as information content.
Entropy is defined in the context of a probabilistic model. Independent fair coin flips have an entropy of 1 bit per flip. A source that always generates a long string of B's has an entropy of 0, since the next character will always be a 'B'.
The entropy rate of a data source means the average number of bits per symbol needed to encode it. Shannon's experiments with human predictors show an information rate between 0.6 and 1.3 bits per character in English; the PPM compression algorithm can achieve a compression ratio of 1.5 bits per character in English text.
From the preceding example, note the following points:
Shannon's definition of entropy, when applied to an information source, can determine the minimum channel capacity required to reliably transmit the source as encoded binary digits (see caveat below in italics). The formula can be derived by calculating the mathematical expectation of the "amount of information" contained in a digit from the information source. "See also" Shannon-Hartley theorem.
Shannon's entropy measures the information contained in a message as opposed to the portion of the message that is determined (or predictable). "Examples of the latter include redundancy in language structure or statistical properties relating to the occurrence frequencies of letter or word pairs, triplets etc." See Markov chain.
Entropy as a measure of diversity.
Entropy is one of several ways to measure diversity. Specifically, Shannon entropy is the logarithm of , the true diversity index with parameter equal to 1.
Data compression.
Entropy effectively bounds the performance of the strongest lossless compression possible, which can be realized in theory by using the typical set or in practice using Huffman, Lempel–Ziv or arithmetic coding. The performance of existing data compression algorithms is often used as a rough estimate of the entropy of a block of data. See also Kolmogorov complexity. In practice, compression algorithms deliberately include some judicious redundancy in the form of checksums to protect against errors.
World's technological capacity to store and communicate information.
A 2011 study in "Science" estimates the world's technological capacity to store and communicate optimally compressed information normalized on the most effective compression algorithms available in the year 2007, therefore estimating the entropy of the technologically available sources. 
The authors estimate humankind technological capacity to store information (fully entropically compressed) in 1986 and again in 2007. They break the information into three categories—to store information on a medium, to receive information through a one-way broadcast networks, or to exchange information through two-way telecommunication networks.
Limitations of entropy as information content.
There are a number of entropy-related concepts that mathematically quantify information content in some way:
(The "rate of self-information" can also be defined for a particular sequence of messages or symbols generated by a given stochastic process: this will always be equal to the entropy rate in the case of a stationary process.) Other quantities of information are also used to compare or relate different sources of information.
It is important not to confuse the above concepts. Often it is only clear from context which one is meant. For example, when someone says that the "entropy" of the English language is about 1 bit per character, they are actually modeling the English language as a stochastic process and talking about its entropy "rate". Shannon himself used the term in this way.
Although entropy is often used as a characterization of the information content of a data source, this information content is not absolute: it depends crucially on the probabilistic model. A source that always generates the same symbol has an entropy rate of 0, but the definition of what a symbol is depends on the alphabet. Consider a source that produces the string ABABABABAB… in which A is always followed by B and vice versa. If the probabilistic model considers individual letters as independent, the entropy rate of the sequence is 1 bit per character. But if the sequence is considered as "AB AB AB AB AB …" with symbols as two-character blocks, then the entropy rate is 0 bits per character.
However, if we use very large blocks, then the estimate of per-character entropy rate may become artificially low. This is because in reality, the probability distribution of the sequence is not knowable exactly; it is only an estimate. For example, suppose one considers the text of every book ever published as a sequence, with each symbol being the text of a complete book. If there are published books, and each book is only published once, the estimate of the probability of each book is , and the entropy (in bits) is . As a practical code, this corresponds to assigning each book a unique identifier and using it in place of the text of the book whenever one wants to refer to the book. This is enormously useful for talking about books, but it is not so useful for characterizing the information content of an individual book, or of language in general: it is not possible to reconstruct the book from its identifier without knowing the probability distribution, that is, the complete text of all the books. The key idea is that the complexity of the probabilistic model must be considered. Kolmogorov complexity is a theoretical generalization of this idea that allows the consideration of the information content of a sequence independent of any particular probability model; it considers the shortest program for a universal computer that outputs the sequence. A code that achieves the entropy rate of a sequence for a given model, plus the codebook (i.e. the probabilistic model), is one such program, but it may not be the shortest.
For example, the Fibonacci sequence is 1, 1, 2, 3, 5, 8, 13, …. Treating the sequence as a message and each number as a symbol, there are almost as many symbols as there are characters in the message, giving an entropy of approximately . So the first 128 symbols of the Fibonacci sequence has an entropy of approximately 7 bits/symbol. However, the sequence can be expressed using a formula [ for , , ] and this formula has a much lower entropy and applies to any length of the Fibonacci sequence.
Data as a Markov process.
A common way to define entropy for text is based on the Markov model of text. For an order-0 source (each character is selected independent of the last characters), the binary entropy is:
where is the probability of . For a first-order Markov source (one in which the probability of selecting a character is dependent only on the immediately preceding character), the entropy rate is:
where is a state (certain preceding characters) and formula_16 is the probability of given as the previous character.
For a second order Markov source, the entropy rate is
-ary entropy.
In general the -ary entropy of a source formula_18 with source alphabet } and discrete probability distribution } where is the probability of (say is defined by:
Note: the in "-ary entropy" is the number of different symbols of the "ideal alphabet" used as a standard yardstick to measure source alphabets. In information theory, two symbols are necessary and sufficient for an alphabet to encode information. Therefore, the default is to let ("binary entropy"). Thus, the entropy of the source alphabet, with its given empiric probability distribution, is a number equal to the number (possibly fractional) of symbols of the "ideal alphabet", with an optimal probability distribution, necessary to encode for each symbol of the source alphabet. Also note that "optimal probability distribution" here means a uniform distribution: a source alphabet with symbols has the highest possible entropy (for an alphabet with symbols) when the probability distribution of the alphabet is uniform. This optimal entropy turns out to be .
Efficiency.
A source alphabet with non-uniform distribution will have less entropy than if those symbols had uniform distribution (i.e. the "optimized alphabet"). This deficiency in entropy can be expressed as a ratio called efficiency:
Efficiency has utility in quantifying the effective use of a communications channel. This formulation is also referred to as the normalized entropy, as the entropy is divided by the maximum entropy formula_21.
Characterization.
Shannon entropy is characterized by a small number of criteria, listed below. Any definition of entropy satisfying these assumptions has the form
where is a constant corresponding to a choice of measurement units.
In the following, and .
Continuity.
The measure should be continuous, so that changing the values of the probabilities by a very small amount should only change the entropy by a small amount.
Symmetry.
The measure should be unchanged if the outcomes are re-ordered.
Maximum.
The measure should be maximal if all the outcomes are equally likely (uncertainty is highest when all possible events are equiprobable).
For equiprobable events the entropy should increase with the number of outcomes.
Additivity.
The amount of entropy should be independent of how the process is regarded as being divided into parts.
This last functional relationship characterizes the entropy of a system with sub-systems. It demands that the entropy of a system can be calculated from the entropies of its sub-systems if the interactions between the sub-systems are known.
Given an ensemble of uniformly distributed elements that are divided into boxes (sub-systems) with elements each, the entropy of the whole ensemble should be equal to the sum of the entropy of the system of boxes and the individual entropies of the boxes, each weighted with the probability of being in that particular box.
For positive integers where ,
Choosing , this implies that the entropy of a certain outcome is zero: . This implies that the efficiency of a source alphabet with symbols can be defined simply as being equal to its -ary entropy. See also Redundancy (information theory).
Further properties.
The Shannon entropy satisfies the following properties, for some of which it is useful to interpret entropy as the amount of information learned (or uncertainty eliminated) by revealing the value of a random variable :
Proving this mathematically follows easily from the previous two properties of entropy.
Extending discrete entropy to the continuous case.
Differential entropy.
The Shannon entropy is restricted to random variables taking discrete values. The corresponding formula for a continuous random variable with probability density function with finite or infinite support formula_33 on the real line is defined by analogy, using the above form of the entropy as an expectation:
This formula is usually referred to as the continuous entropy, or differential entropy. A precursor of the continuous entropy is the expression for the functional in the Η-theorem of Boltzmann.
Although the analogy between both functions is suggestive, the following question must be set: is the differential entropy a valid extension of the Shannon discrete entropy? Differential entropy lacks a number of properties that the Shannon discrete entropy has – it can even be negative – and thus corrections have been suggested, notably limiting density of discrete points.
To answer this question, we must establish a connection between the two functions:
We wish to obtain a generally finite measure as the bin size goes to zero. In the discrete case, the bin size is the (implicit) width of each of the (finite or infinite) bins whose probabilities are denoted by . As we generalize to the continuous domain, we must make this width explicit.
To do this, start with a continuous function discretized into bins of size formula_35.
By the mean-value theorem there exists a value in each bin such that
and thus the integral of the function can be approximated (in the Riemannian sense) by
where this limit and "bin size goes to zero" are equivalent.
We will denote
and expanding the logarithm, we have
As Δ → 0, we have
But note that as , therefore we need a special definition of the differential or continuous entropy:
which is, as said before, referred to as the differential entropy. This means that the differential entropy "is not" a limit of the Shannon entropy for . Rather, it differs from the limit of the Shannon entropy by an infinite offset (see also the article on information dimension)
Limiting Density of Discrete Points.
It turns out as a result that, unlike the Shannon entropy, the differential entropy is "not" in general a good measure of uncertainty or information. For example, the differential entropy can be negative; also it is not invariant under continuous co-ordinate transformations. This problem may be illustrated by a change of units when "x" is a dimensioned variable. "f(x)" will then have the units of "1/x". The argument of the logarithm must be dimensionless, otherwise it is improper, so that the differential entropy as given above will be improper. If "Δ" is some "standard" value of "x" (i.e. "bin size") and therefore has the same units, then a modified differential entropy may be written in proper form as:
and the result will be the same for any choice of units for "x". In fact, the limit of discrete entropy as formula_43 would also include a term of formula_44, which would in general be infinite. This is expected, continuous variables would typically have infinite entropy when discretized. The limiting density of discrete points is really a measure of how much easier a distribution is to describe than a distribution that is uniform over its quantization scheme. 
Relative entropy.
Another useful measure of entropy that works equally well in the discrete and the continuous case is the relative entropy of a distribution. It is defined as the Kullback–Leibler divergence from the distribution to a reference measure as follows. Assume that a probability distribution is absolutely continuous with respect to a measure , i.e. is of the form for some non-negative -integrable function with -integral 1, then the relative entropy can be defined as
In this form the relative entropy generalises (up to change in sign) both the discrete entropy, where the measure is the counting measure, and the differential entropy, where the measure is the Lebesgue measure. If the measure is itself a probability distribution, the relative entropy is non-negative, and zero if as measures. It is defined for any measure space, hence coordinate independent and invariant under co-ordinate reparameterizations if one properly takes into account the transformation of the measure . The relative entropy, and implicitly entropy and differential entropy, do depend on the "reference" measure .
Use in combinatorics.
Entropy has become a useful quantity in combinatorics.
Loomis-Whitney inequality.
A simple example of this is an alternate proof of the Loomis-Whitney inequality: for every subset , we have
where is the orthogonal projection in the -th coordinate:
The proof follows as a simple corollary of Shearer's inequality: if are random variables and are subsets of } such that every integer between 1 and lies in exactly of these subsets, then
where formula_49 is the Cartesian product of random variables with indexes in (so the dimension of this vector is equal to the size of ).
We sketch how Loomis-Whitney follows from this: Indeed, let be a uniformly distributed random variable with values in and so that each point in occurs with equal probability. Then (by the further properties of entropy mentioned above) , where denotes the cardinality of . Let }. The range of formula_50 is contained in and hence formula_51. Now use this to bound the right side of Shearer's inequality and exponentiate the opposite sides of the resulting inequality you obtain.
Approximation to binomial coefficient.
For integers let . Then
where 
Here is a sketch proof. Note that formula_54 is one term of the expression
Rearranging gives the upper bound. For the lower bound one first shows, using some algebra, that it is the largest term in the summation. But then,
since there are terms in the summation. Rearranging gives the lower bound.
A nice interpretation of this is that the number of binary strings of length with exactly many 1's is approximately formula_57.

</doc>
<doc id="15446" url="https://en.wikipedia.org/wiki?curid=15446" title="Ithaca College">
Ithaca College

Ithaca College is a coeducational, nonsectarian private college located on the South Hill of Ithaca, New York, United States. The school was founded by William Egbert in 1892 as a conservatory of music. The college has a strong liberal arts core, but also offers several pre-professional programs and some graduate programs. The college is also known internationally for its communications program, the Roy H. Park School of Communications, which was most recently ranked as a top school for journalism, film and media. The college is set against the backdrop of Cayuga Lake, the city of Ithaca, and several waterfalls and gorges. The college is perhaps best known for its large list of alumni who play or have played substantial roles in the worlds of media and entertainment.
Ithaca College has been ranked among the top ten master's universities in the North by "U.S. News & World Report" every year since 1996. For the 2016 rankings, the college was ranked ninth in this category. Ithaca College is also consistently named among the best colleges in the nation by "Princeton Review", with the 2013 guide also ranking the college No.1 for radio and No. 7 for theater.
History.
Beginnings.
Ithaca College was founded as the Ithaca Conservatory of Music in 1892 when a local violin teacher, William Grant Egbert, rented four rooms and arranged for the instruction of eight students. For nearly seven decades the institution flourished in the city of Ithaca, adding to its music curriculum the study of elocution, dance, physical education, speech correction, radio, business, and the liberal arts. In 1931 the conservatory was chartered as a private college. The college was originally housed in the Boardman House, that later became the Ithaca College Museum of Art, and it was listed on the National Register of Historic Places in 1971.
Modern era.
By 1960, some 2,000 students were in attendance. A modern campus was built on South Hill in the sixties, and students were shuttled between the old and new during the construction. The hillside campus continued to grow in the ensuing 30 years to accommodate more than 6,000 students.
As the campus expanded, the college also began to expand its curriculum. By the 1990s, some 2,000 courses in more than 100 programs of study were available in the college's five schools. The school attracts a multicultural student body with representatives from almost every state and from 78 foreign countries.
Campus.
Ithaca College's current campus was built in the 1960s on South Hill. In 1968 the College's final academic department moved to the South Hill campus from downtown, making the move complete.
Satellite campuses.
Besides its Ithaca campus, Ithaca College has also operated satellite campuses in other cities. The Ithaca College London Center has been in existence since 1972. Ithaca runs the Ithaca College Los Angeles Program at the James B. Pendleton Center. Additionally, there is an Ithaca College Washington Semester Program, and a recently launched Ithaca College New York City Center.
Former programs include the Ithaca College Antigua Program and the Ithaca College Walkabout Down Under Program in Australia.
Ithaca College also operates direct enrollment exchange programs with several universities, including Griffith University, La Trobe University, Murdoch University, and University of Tasmania (Australia); Chengdu Sport University and Beijing Sport University (China); University of Hong Kong; Masaryk University (Czech Republic); Akita International University and University of Tsukuba (Japan); Hanyang University (Korea); Nanyang Technological University (Singapore); University of Valencia (Spain); and Jönköping University (Sweden).
Academics.
The college offers a curriculum with more than 100 degree programs in its five schools.
Until recently, several cross-disciplinary degree programs along with the Center for the Study of Culture, Race, and Ethnicity were housed in the Division of Interdisciplinary and International Studies; however, starting spring 2011, the division was eliminated and its programs, centers and institutes were absorbed within other schools.
As of the 2012-2013 academic year, Television-Radio and Business Administration were the two most popular majors, while the School of Humanities & Sciences had the most students overall.
Student life.
Media and publications.
With its top-ranked Roy H. Park School of Communications, Ithaca College is well known for its several prominent student-run media vehicles, including:
Greek life.
Historically, various independent and national fraternities and sororities had active chapters at Ithaca College. However, due to a series of highly publicized hazing incidents in the 1980s, including one that was responsible for the death of a student, the College administration removed all but five Greek letter organizations from campus, and adopted a non-expansion policy, prohibiting any new Greek houses from affiliating with the College. As of 2014, three recognized Greek organizations remain on campus, all of which are music-oriented:
A fourth house, performing arts fraternity Kappa Gamma Psi (Iota Chapter) became inactive in 2008. Although there are potentially plans to reactivate the chapter, it is unclear whether this will be permitted or not due to the college's non-expansionist policy.
However, there are various Greek letter organizations at Ithaca College that are unaffiliated with the school, and therefore not subject to the same housing privileges or rules that contribute to the safety of their members such as non-hazing and non-drinking policies. Additionally, while not particularly common, Ithaca College students may rush for Greek houses affiliated with Cornell University, subject to the rules of each individual fraternity or sorority. Some Cornell affiliated Greek organizations actively recruit Ithaca College students.
There are a few unaffiliated fraternities that many Ithaca College students join - ΔKE (Delta Kappa Epsilon), AEΠ (Alpha Epsilon Pi), and KΣ (Kappa Sigma).
There is one unaffiliated sorority - ΓΔΠ (Gamma Delta Pi).
Athletics.
The Ithaca athletics nickname "Bombers" is unique in NCAA athletics, and the origins of the nickname are obscure. Ithaca College's sports teams were originally named the Cayugas, but the name was changed to the Bombers sometime in the 1930s. Some other names that have been used for Ithaca College's teams include: Blue Team, Blues, Blue and Gold, Collegians, and the Seneca Streeters. Several possibilities for the change to the "Bombers" have been posited. The most common explanation is that the school's baseball uniforms—white with navy blue pinstripes and an interlocking "IC" on the left chest—bear a striking resemblance to the distinctive home uniforms of the New York Yankees, who are known as the Bronx Bombers. It may also have referred to the Ithaca basketball team of that era and its propensity for half-court "bombs". Grumman Aircraft also manufactured airplanes including bombers in Ithaca for many years. The first “Bombers” reference on record was in the December 17, 1938 issue of the "Rochester Times-Union" in a men’s basketball article.
The name has at times sparked controversy for its perceived martial connotations. It is an occasional source of umbrage from Ithaca's prominent pacifist community, but the athletics department has consistently stated it has no interest in changing the name. The athletics logo has in the past incorporated World War II era fighter planes, but currently does not, and the school does not currently have a physical mascot to personify the name. In 2010 the school launched a contest to choose one. It received over 250 suggestions and narrowed the field down to three: a phoenix, a flying squirrel, and a Lake Beast. In June 2011, President Rochon announced that the school would discontinue the search due to opposition in the alumni community.
Ithaca is a member of the NCAA's Division III, the Empire Eight Conference, and the Eastern College Athletic Conference. Ithaca has one of Division III's strongest athletic programs. The Bombers have won a total of 15 national titles in seven team sports and five individual sports.
Ithaca College recently remodeled the Hill Center in 2013. The building features hardwood floors (Ben Light Gymnasium) as well as coaches offices. The building is home to Ithaca's men's and women's basketball teams, women's volleyball team, wrestling, and gymnastics. Ithaca also opened the Athletics & Events Center in 2011, a $65.5 million facility funded by donors. The facility is mainly used by the school's varsity athletes. It has a 47,000 square foot, 9-lane 50 meter Olympic-size pool. The building also has Glazer Arena, a 130,000 square foot event space. It is a track and field center that doubles as a practice facility for lacrosse, field hockey, soccer, baseball, tennis, and football. The facility was designed by the architectural firm Moody-Nolan and began construction in June 2009.
Coached by Jim Butterfield for 27 years, the football team has won three NCAA Division III National Football Championships in 1979, 1988 and 1991 (a total surpassed only by Augustana, Mount Union and Wisconsin-Whitewater). Bomber football teams made a record seven appearances in the Division III national championship game, the Amos Alonzo Stagg Bowl, which has since been surpassed by Mount Union in 2003. The Bombers play the SUNY Cortland Red Dragons for the Cortaca Jug, which was added in 1959 to an already competitive rivalry. The matchup is one of the most prominent in Division III college football. The game alternates locations between Ithaca and Cortland. Cortland has won the Cortaca Jug the past 6 years.
Most recently, the women's crew won back-to-back NCAA Division III championships in 2004 and 2005.
The men's crew saw much success in 2008, receiving 4 medals at the New York State Collegiate Championships.
Women's soccer has won two national championships in Division III and is consistently ranked in the top 20 nationally.
The Men's Wrestling team won NCAA Division III National Championships in 1989, 1990 and 1994.
Women's field hockey won the 1982 NCAA Division III Field Hockey Championship.
This past year in 2013, Paula Miller, head of Woman's Swimming team completed her 30th year as head coach of the Ithaca Bombers. She has led the team to many victories. The past four years the Bombers have been undefeated throughout their season defeating tough competition. Ithaca has finished first or second at 25 of the past 29 state meets. The Bombers have also won the Empire 8 crown in each of the past nine seasons.
The 2013-2014 season ended with regaining the NCAA Division III Championship trophy.
During the 2015-2016 season the Bombers Swimming and Diving team held the UNYSCSA Empire 8 state champion meet in the Athletic and Events center at Ithaca College. The Mens Swimming and Diving team scored 616.5 points finishing 4th in states under coach Kevin Markwardt. The men's team was lead by caption Addison Hebert who was injured the first day of the meet and was able to overcome it by the last day helping the rest of the bombers get 3rd place in the 400 Freestyle Relay by .01 seconds. The Girls Swimming and Diving team scored 1227 points winning states under Paula Miller. The bombers are bringing two women divers to South Carolina where they will be competing in nationals in March.
Ithaca is also home to more than 60 club sports, many of which compete regularly against other colleges in leagues and tournaments.
"Ithaca Forever".
"Ithaca Forever" is the official alma mater or school song of Ithaca College.
Intramurals.
Along with Intercollegiate athletics, Ithaca College has a rather large Intramural sport program. This extracurricular program serves approximately 25% of the undergraduate population yearly. Fourteen traditional team activities are offered throughout the year and include: basketball, flag football, kickball, soccer, softball, ultimate frisbee, ski racing, and volleyball.
For most activities divisions are offered for men’s, women’s, and co-recreational teams. Throughout the year usually two or more activities run concurrently and participants are able to play on a single sex team and co-recreational team for each activity.
The most popular activities recently have been 5-on-5 basketball with over forty teams entered for the past three years, for the past two years there have been over thirty indoor flag football teams and teams have been turned away.
During 08-09 new records were established for total teams in both 4-person and 6-person volleyball, 3-on-3 basketball, and co-recreational indoor soccer. During the 08-09 year there were 1,559 intramural participants and over 500 female participants. It was estimated that the 2009–10 year and the 2010–11 have even more participants in intramural sports.
Sustainability.
Ithaca's School of Business was the first college or university business school in the world to achieve LEED Platinum Certification alongside Yale University, which had the second. Ithaca's Peggy Ryan Williams Center is also LEED Platinum certified. It makes extensive use of day light in occupied spaces. There are sensors that regulate lighting and ventilation based on occupancy and natural light. Over 50% of the building energy comes from renewable sources such as wind power. The college also has a LEED Gold Certified building, the Athletics & Events Center. The College composts its dining hall waste, runs a "Take It or Leave It" Green move-out program, and offers a sustainable living option. It also operates an office supply collection and reuse program, as well as a sustainability education program during new student orientation. Ithaca received a B- grade on the Sustainable Endowments Institute's 2009 College Sustainability Report Card and an A- for 2010.
Environmental record.
Commitments to action on climate change.
In Spring 2007, then-President Peggy R. Williams signed the American College and University President's Climate Commitment (ACUPCC), pledging Ithaca College to the task of developing a strategy and long-range plan to achieve "carbon neutrality" at some point in the future. In 2009 the Ithaca College Board of Trustees approved the Ithaca College Climate Action Plan, which calls for 100% carbon neutrality by 2050. In 2009, the Ithaca College Board of Trustees approved the Ithaca College Climate Action Plan, which calls for 100% carbon neutrality by 2050 and offers a 40-year action plan to work toward that ambitious goal.
Energy profile.
The college purchases 14 percent of its electricity from renewable sources and offsets 3 percent of its energy use with renewable energy credits.
Energy investments.
The college aims to optimize investment returns and does not invest the endowment in on-campus sustainability projects, renewable energy funds, or community development loan funds. The college's investment policy reserves the right of the investment committee to restrict investments for any reason, which could include environmental and sustainability factors.
Community impact.
While the Ithaca College Natural Lands has issued a statement that Ithaca College should join efforts calling for a moratorium on horizontal drilling and high volume (“slick water”) hydraulic fracturing, or fracking, the college as a whole has refused to issue a statement regarding the issue.
Leadership.
Current president.
Ithaca's current president is Thomas Rochon. Thomas Rochon was named the eighth president of Ithaca College on April 11, 2008. Rochon took over as president of the college following Peggy Williams, who had announced on July 12, 2007, that she would retire from the presidency post effective May 31, 2009, following a one-year sabbatical.
During the Fall 2015 semester multiple protests focusing on campus climate and Rochon's leadership were lead by students and faculty. After multiple racially charged events including student house party themes, and racially tinged comments at administration lead programs students, faculty and staff all decided to hold votes of "no confidence" in Rochon. Students voted "no confidence" by a count of 72% no confidence, 27% confidence, and 1% abstaining. The Faculty Voted 77.8% no confidence to 22.2% confidence.
On January 14, 2016, Tom Rochon announced his retirement effective July 1, 2017.
Alumni.
Ithaca College has 49,570 alumni in the United States. There are alumni clubs for Boston, Chicago, Connecticut, Los Angeles, Metro New York, National Capital, N. & S. Carolina, Philadelphia, Rochester (NY), San Diego, and Southern Florida. Alumni events are hosted in cooperation with the specific clubs and also through a program called "IC on the Road".
Following is a brief list of noteworthy Ithaca College alumni. 
For a more extensive list, see main entry List of Ithaca College alumni.
Faculty.
Following is a brief list of current and former noteworthy Ithaca College faculty.

</doc>
<doc id="15447" url="https://en.wikipedia.org/wiki?curid=15447" title="Differential psychology">
Differential psychology

Differential psychology studies the ways in which individuals differ in their behavior and the processes that underlie it. This is distinguished from other aspects of psychology in that although psychology is ostensibly a study of individuals, modern psychologists often study groups or biological underpinnings of cognition. 
For example, in evaluating the effectiveness of a new therapy, the mean performance of the therapy in one treatment group might be compared to the mean effectiveness of a placebo (or a well-known therapy) in a second, control group. In this context, differences between individuals in their reaction to the experimental and control manipulations are actually treated as errors rather than as interesting phenomena to study. 
This approach is because psychological research depends upon statistical controls that are only defined upon groups of people. Individual difference psychologists usually express their interest in individuals while studying groups by seeking dimensions shared by all individuals but upon which individuals differ.
Importance of individual differences.
Individual differences are essential whenever we wish to explain how 
individuals differ in their behavior. In any study, significant 
variation exists between individuals. Reaction time, preferences, 
values, and health-linked behaviors are just a few examples. Individual 
differences in factors such as personality, intelligence, 
memory, or physical factors such as body size, sex, age, and other 
factors can be studied and used in understanding this large source of 
variance. Importantly, individuals can also differ not only in their 
current state, but in the magnitude or even direction of response to a 
given stimulus. Such phenomena, often 
explained in terms of inverted-U response curves, 
place differential psychology at an important location in such 
endeavours as personalized medicine, in which diagnoses are 
customised for an individual's response profile.
Areas of study.
Individual differences research typically includes personality, motivation, intelligence, ability, IQ, interests, values, self-concept, self-efficacy, and self-esteem (to name just a few). There are few remaining "differential psychology" programs in the United States, although research in this area is very active. Current researchers are found in a variety of applied and experimental programs, including educational psychology, Industrial and organizational psychology, personality psychology, social psychology, behavioral genetics, and developmental psychology programs, in the neo-Piagetian theories of cognitive development in particular.

</doc>
<doc id="15450" url="https://en.wikipedia.org/wiki?curid=15450" title="Industrial and organizational psychology">
Industrial and organizational psychology

Industrial and organizational psychology (also known as I–O psychology, occupational psychology, work psychology, WO psychology, IWO psychology and business psychology) is the scientific study of human behavior in the workplace and applies psychological theories and principles to organizations and individuals in their workplace.
I-O psychologists are trained in the scientist–practitioner model. I-O psychologists contribute to an organization's success by improving the performance, motivation, job satisfaction, occupational safety and health as well as the overall health and well-being of its employees. An I–O psychologist conducts research on employee behaviors and attitudes, and how these can be improved through hiring practices, training programs, feedback, and management systems. I–O psychologists also help organizations and their employees transition during periods of change and organization development.
I-O psychology is one of the 14 recognized specialties and proficiencies in professional psychology in the United States and is represented by Division 14 of the American Psychological Association (APA), known formally as the Society for Industrial and Organizational Psychology (SIOP).
In the UK, industrial and organizational psychologists are referred to as occupational psychologists and one of 7 'protected titles' and specializations in psychology regulated by the Health and Care Professions Council.
In Australia, the title organizational psychologist is also protected by law and is regulated by the Australian Health Practitioner Regulation Agency (AHPRA). Organizational psychology is one of nine areas of specialist endorsement for psychology practice in Australia. Graduate programs at both the Masters and Doctorate level are offered worldwide.
In the UK graduate degrees are accredited by the British Psychological Society and required as part of the process to become an occupational psychologist.
In Europe someone with a specialist EuroPsy Certificate in Work and Organisational Psychology is a fully qualified psychologist and an expert in the work psychology field with further advanced education and training. Industrial and organizational psychologists reaching the EuroPsy standard are recorded in the Register of European Psychologists and industrial and organizational psychology is one of the three main psychology specializations in Europe.
Historical overview.
The historical development of I–O psychology had parallel developments in the United States and other countries, such as the UK, Australia, Germany, the Netherlands, and eastern European countries such as Romania. However, many foreign countries do not have a published English language account of their development of I–O psychology. The roots of I-O psychology trace back nearly to the beginning of psychology as a science, when Wilhelm Wundt founded one of the first psychological laboratories in 1876 in Leipzig, Germany. In the mid 1880s, Wundt trained two psychologists who had a major influence on the eventual emergence of I–O Psychology: Hugo Münsterberg and James McKeen Cattell. Instead of viewing differences as “errors”, Cattell was one of the first to recognize the importance of these differences among individuals as a way of predicting and better understanding their behavior. Walter Dill Scott, who was a contemporary of Cattell, was elected President of the American Psychological Association (APA) in 1919, was arguably the most prominent I–O psychologist of his time. Scott, along with Walter Van Dyke Bingham worked at the Carnegie Institute of Technology, developing methods for selecting and training sales personnel
The "industrial" side of I–O psychology has its historical origins in research on individual differences, assessment, and the prediction of work performance. This branch crystallized during World War I, in response to the need to rapidly assign new troops to duty stations. Scott and Bingham volunteered to help with the testing and placement of more than a million army recruits. In 1917, together, along with other prominent psychologists, adapted a well-known intelligence test, (the Stanford-Binet test, designed for testing one individual at a time) to make it suitable for mass group testing. This new test form was called the Army Alpha. After the War, the growing industrial base in the US added impetus to I–O psychology. The private industry set out to emulate the successful testing of army personnel, and mental ability testing soon became a commonplace in the work setting. Industrial psychology began to gain prominence when Elton Mayo arrived in the United States in 1924. Mayo was fascinated by not the efficiency of workers, but their emotions and how work may cause workers to act in particular pathological ways. These observations of workers’ thoughts and emotions were studied to see how prone employees would be to resist management attempts to increase productivity and how sympathetic to labor unions they would become. These studies are known as Hawthorne studies. The results of these studies ushered in a radically new movement known as the Human Relations Movement. This movement was interested in the more complicated theories of motivation, the emotional world of the worker, job satisfaction, and interviews with workers.
World War II brought in new problems that led to I–O Psychology's continued development. The war brought renewed interest in ability testing (to accurately place recruits in these new technologically advanced military jobs), the introduction of the assessment center, concern with morale and fatigue of war industry workers, and military intelligence. Post-Second World War years were a boom time for industry with many jobs to be filled and applicants to be tested. Interestingly, however, when the war ended and the soldiers came back to work, there was an increasing trend towards labor unrest with rising numbers of authorized and unauthorized work stoppages staged by unions and workers. This caused management to grow concern about work productivity and worker attitude surveys became of much interest in the field. Following Industrial Organizational Psychology's admission into Division 14 of the American Psychological Association, there continued to be an influx of new tests for selection, productivity, and workforce stability. This influx continued unabated until the passage of the Civil Rights Act of 1964. Section, Title VII dealt with employment discrimination and required employers to justify and show relevance for the use of tests for selection.
The mid-1960s seemed to mark a line of demarcation between "classic" and "modern" thinking. During this period, the name changed from just industrial psychology to industrial and organizational psychology. The earlier periods addressed work behavior from the individual perspective, examining performance and attitudes of individual workers. Although this was a valuable approach, it became clear that there were other, broader influences not only on individual, but also on group behavior in the work place. Thus, in 1973, "organizational" was added to the name to emphasize the fact that when an individual joins an organization (e.g., the organization that hired him or her), he or she will be exposed to a common goal and a common set of operating procedures.
In the 1970s in the United Kingdom, references to occupational psychology became more common than I-O psychology. Rigor and methods of psychology are applied to issues of critical relevance to business, including talent management, coaching, assessment, selection, training, organizational development, performance, well-being and work-life balance. During the 1990s references to "business psychology" became increasingly common. Business psychology is defined as the study and practice of improving working life. It combines an understanding of the science of human behavior with experience of the world of work to attain effective and sustainable performance for both individuals and organizations.
Research methods.
As described above, I–O psychologists are trained in the scientist–practitioner model. I–O psychologists rely on a variety of methods to conduct organizational research. Study designs employed by I–O psychologists include surveys, experiments, quasi-experiments, and observational studies. I–O psychologists rely on diverse data sources including human judgments, historical databases, objective measures of work performance (e.g., sales volume), and questionnaires and surveys.
I–O researchers employ both quantitative and qualitative research methods. Quantitative methods used in I–O psychology include both descriptive statistics and inferential statistics (e.g., correlation, multiple regression, and analysis of variance). More advanced statistical methods employed by some I–O psychologists include logistic regression, multivariate analysis of variance, structural equation modeling, and hierarchical linear modeling (HLM; also known as multilevel modeling). HLM is particularly applicable to research on team- and organization-level effects on individuals. I–O psychologists also employ psychometric methods including methods associated with classical test theory (CTT), generalizability theory, and item response theory (IRT). In the 1990s, a growing body of empirical research in I–O psychology was influential in the application of meta-analysis, particularly in the area of the stability of research findings across contexts. The most well-known meta-analytic approaches are those associated with Hunter & Schmidt, Rosenthal, and Hedges & Olkin. With the help of meta-analysis, Hunter & Schmidt advanced the idea of validity generalization, which suggests that some performance predictors, specifically cognitive ability tests (see especially Hunter J.E. (1986). Cognitive ability, cognitive aptitude, job knowledge, and job performance. "Journal of Vocational Behavior", 29, 340–62.</ref> and Hunter & Schmidt [1996) have a relatively stable and positive relation to job performance across all jobs. Although not unchallenged, validity generalization has broad acceptance with regard to many selection instruments (e.g. cognitive ability tests, job knowledge tests, work samples, and structured interviews) across a broad range of jobs.
Qualitative methods employed in I–O psychology include content analysis, focus groups, interviews, case studies, and several other observational techniques. I–O research on organizational culture research has employed ethnographic techniques and participant observation to collect data. One well-known qualitative technique employed in I–O psychology is John Flanagan's Critical Incident Technique, which requires "qualified observers" (e.g., pilots in studies of aviation, construction workers in studies of construction projects) to describe a work situation that resulted in a good or bad outcome. Objectivity is ensured when multiple observers identify the same incidents. The observers are also asked to provide information about what the actor in the situation could have done differently to influence the outcome. This technique is then used to describe the critical elements of performance in certain jobs and how worker behavior relates to outcomes. Most notably, this technique has been employed to improve performance among aircraft crews and surgical teams, literally saving thousands of lives since its introduction. An application of the technique in research on coping with job stress comes from O'Driscoll & Cooper. The resistance to qualitative research resulted from viewing it too excessively subjective. This concern, however, is misplaced due to all methods of research, either qualitative or quantitative, ultimately requiring some sort of interpretation. When a researcher is developing and researching a phenomenon, all information available should be used, regardless of its form. The key is triangulation, which is an approach looking for converging information from different sources to develop that theory.
I–O psychologists sometimes use quantitative and qualitative methods in concert. The two are not mutually exclusive. For example, when constructing behaviorally-anchored rating scales (BARS), a job analyst may use qualitative methods, such as critical incidents interviews and focus groups to collect data bearing on performance. Then the analyst would have SMEs rate those examples on a Likert scale and compute inter-rater agreement statistics to judge the adequacy of each item. Each potential item would additionally be correlated with an external criterion in order to evaluate its usefulness if it were to be selected to be included in a BARS metric. As a simpler example, consider an extended observation of a worker, which might include videotaped episodes of performance – a qualitative measure. The qualitative video could easily be used to develop a frequency count of a particular behavior – a quantitative measure.
Topics.
Job analysis.
Job analysis has a few different methods but it primarily involves the systematic collection of information about a job. The task-oriented job analysis, involves an examination of the duties, tasks, and/or competencies required by a job, whereas a worker-oriented job analysis, involves an examination of the knowledge, skills, abilities, and other characteristics (KSAOs) required to successfully perform the work. Job analysis information is used for many purposes, including the creation of job-relevant selection procedures, performance appraisals and criteria, or training programs. Position analysis questionnaire is a particular analysis that is used to determine an individual's job characteristics and relates them to human characteristics.
Personnel recruitment and selection.
I–O psychologists typically work with HR specialists to design (a) recruitment processes and (b) personnel selection systems. Personnel recruitment is the process of identifying qualified candidates in the workforce and getting them to apply for jobs within an organization. Personnel recruitment processes include developing job announcements, placing ads, defining key qualifications for applicants, and screening out unqualified applicants.
Personnel selection is the systematic process of hiring and promoting personnel. Personnel selection systems employ evidence-based practices to determine the most qualified candidates. Personnel selection involves both the newly hired and individuals who can be promoted from within the organization. Common selection tools include ability tests (e.g., cognitive, physical, or psycho-motor), knowledge tests, personality tests, structured interviews, the systematic collection of biographical data, and work samples. I–O psychologists must evaluate evidence regarding the extent to which selection tools predict job performance, evidence that bears on the validity of selection tools.
Personnel selection procedures are usually validated, i.e., shown to be job relevant, using one or more of the following types of validity: content validity, construct validity, and/or criterion-related validity. I–O psychologists adhere to professional standards, such as the Society for Industrial and Organizational Psychology's (SIOP) "Principles for Validation and Use of Personnel Selection Procedures" and the Standards for Educational and Psychological Testing. The Equal Employment Opportunity Commission's "Uniform Guidelines" are also influential in guiding personnel selection although they have been criticized as outdated when compared to the current state of knowledge in I–O psychology.
I–O psychologists not only help in the selection and assessment of personnel for jobs, but also assist in the selection of students for admission to colleges, universities, and graduate and professional schools as well as the assessment of student achievement, student aptitude, and the performance of teachers and K–12 schools. Increasingly, I–O psychologists are working for educational assessment and testing organizations and divisions.
A meta-analysis of selection methods in personnel psychology found that general mental ability was the best overall predictor of job performance and training performance.
Performance appraisal/management.
Performance appraisal or performance evaluation is the process of measuring an individual's or a group's work behaviors and outcomes against the expectations of the job. Performance appraisal is frequently used in promotion and compensation decisions, to help design and validate personnel selection procedures, and for performance management. Performance management is the process of providing performance feedback relative to expectations and improvement information (e.g., coaching, mentoring). Performance management may also include documenting and tracking performance information for organization-level evaluation purposes.
An I–O psychologist would typically use information from the job analysis to determine a job's performance dimensions, and then construct a rating scale to describe each level of performance for the job. Often, the I–O psychologist would be responsible for training organizational personnel how to use the performance appraisal instrument, including ways to minimize bias when using the rating scale, and how to provide effective performance feedback. Additionally, the I–O psychologist may consult with the organization on ways to use the performance appraisal information for broader performance management initiatives.
Individual assessment and psychometrics.
Individual assessment involves the measurement of individual differences. I–O psychologists perform individual assessments in order to evaluate differences among candidates for employment as well as differences among employees. The constructs measured pertain to job performance. With candidates for employment, individual assessment is often part of the personnel selection process. These assessments can include written tests, aptitude tests, physical tests, psycho-motor tests, personality tests, integrity and reliability tests, work samples, simulation and assessment centres.
Psychometrics is the science of measuring psychological variables, such as knowledge, skills, and abilities. I–O psychologists are generally well-trained in psychometric psychology.
Occupational health and wellbeing.
I/O psychologists and researchers are also concerned with occupational health and wellbeing. Researchers have examined the effect of physical exercise, and staying vigorous at work. Sonnentag and Niessen (2008) found that staying vigorous during working hours is important for work-related behaviour, subjective well-being, and for effective functioning in the family domain. Individuals high on their general level of vigour at work, benefited most from recovery experienced over the course of several days. A 2010 study found positive relationships between job satisfaction and life satisfaction, happiness, positive affect, and the absence of negative affect and feelings of positive wellbeing. Other researchers have looked at the negative health impacts of mature-aged unemployment. Another recent study conducted by Potocnik & Sonnentag (2013) examined the impact of engaging in seven types of activities on depression and quality of life in older workers over a period of 2 years, using a sample from the Survey of Health, Ageing and Retirement in Europe. Results indicated that I/O psychologists should make attempts to reduce physical demands over older employees at work, to help improve their health and well-being. Practitioners should also design intervention programmes and preventive measures that focus on how to stimulate older employees' engagement in community activities. I/O research has also examined effects of job mobility and negative health effects, including burnout in workers.
Workplace bullying, aggression and violence.
I/O psychology and I/O psychologists are also concerned with the related topics of workplace bullying, aggression and violence. This 2010 study investigated the impact of the larger organizational context on bullying as well as the group-level processes that impact on the incidence, and maintenance of bullying behaviour. The impact of engaging in certain thought patterns after exposure to workplace violence has also been examined. This 2011 research examines the detrimental effect that interpersonal aggressive behaviours may have on dimensions of team effectiveness particularly team performance and team viability.
Remuneration and compensation.
Compensation includes wages or salary, bonuses, pension/retirement contributions, and perquisites that can be converted to cash or replace living expenses. I–O psychologists may be asked to conduct a job evaluation for the purpose of determining compensation levels and ranges. I–O psychologists may also serve as expert witnesses in pay discrimination cases when disparities in pay for similar work are alleged.
Training and training evaluation.
Training is the systematic acquisition of skills, concepts, or attitudes that results in improved performance in another environment. Most people hired for a job are not already versed in all the tasks required to perform the job effectively. Evidence indicates that training is effective and that these training expenditures are paying off in terms of higher net sales and gross profitability per employee. Training can be beneficial for the organization and for employees in terms of increasing their value to their organization as well as their employability in the broader marketplace. Many organizations are using training and development as a way to attract and retain their most successful employees.
Similar to performance management (see above), an I–O psychologist would employ a job analysis in concert with principles of instructional design to create an effective training program. A training program is likely to include a summative evaluation at its conclusion in order to ensure that trainees have met the training objectives and can perform the target work tasks at an acceptable level. Training programs often include formative evaluations to assess the impact of the training as the training proceeds. Formative evaluations can be used to locate problems in training procedures and help I–O psychologists make corrective adjustments while the training is ongoing.
The basic foundation for training programs is learning. Learning outcomes can be organized into three broad categories: cognitive, skill-based, and affective outcomes. Cognitive is a type of learning outcome that includes declarative knowledge or the knowledge of rules, facts, and principles. An example is police officers acquire declarative knowledge about laws and court procedures. Skill-based is a learning outcome that concerns procedural knowledge and the development of motor and technical skills. An example is motor skills that involve the coordination of physical movements such as using a special tool or flying a certain aircraft, whereas technical skills might include understanding a certain software program, or exhibiting effective customer relations behaviors. Affective is a type of learning outcome that includes attitudes or beliefs that predispose a person to behave in a certain way. Attitudes can be developed or changed through training programs. Examples of these attitudes are organizational commitment and appreciation of diversity.
Before training design issues are considered, a careful needs analysis is required to develop a systematic understanding of where training is needed, what needs to be taught or trained, and who will be trained. Training needs analysis typically involves a three-step process that includes organizational analysis, task analysis and person analysis. Organizational analysis examines organizational goals, available resources, and the organizational environment to determine where training should be directed. This analysis identifies the training needs of different departments or subunits and systematically assessing manager, peer, and technological support for transfer of training. Organizational analysis also takes into account the climate of the organization and its subunits. For example, if a climate for safety is emphasized throughout the organization or in particular parts of the organization (e.g., production), then training needs will likely reflect this emphasis. Task analysis uses the results from job analysis on determining what is needed for successful job performance and then determines what the content of training should be. Task analysis can consist of developing task statements, determining homogeneous task clusters, and identifying KSAOs (knowledge, skills, abilities, other characteristics) required for the job. With organizations increasingly trying to identify "core competencies" that are required for all jobs, task analysis can also include an assessment of competencies. Person analysis identifies which individuals within an organization should receive training and what kind of instruction they need. Employee needs can be assessed using a variety of methods that identify weaknesses that training and development can address. The needs analysis makes it possible to identify the training program's objectives, which in turn, represents the information for both the trainer and trainee about what is to be learned for the benefit of the organization.
Therefore, with any training program it is key to establish specify training objectives.
Schultz & Schultz (2010) states that need assessment is an analysis of corporate and individual goals undertaken before designing a training program. Examples of need assessment are based on organizational, task, and work analysis is conducted using job analysis critical incidents, performance appraisal, and self-assessment techniques.
But with any training there are always challenges that one faces. Challenges which I–O psychologists face:
Motivation in the workplace.
Work motivation "is a set of energetic forces that originate both within as well as beyond an individual's being, to initiate work-related behavior, and to determine its form, direction, intensity, and duration" Understanding what motivates an organization's employees is central to the study of I–O psychology. Motivation is a person's internal disposition to be concerned with and approach positive incentives and avoid negative incentives. To further this, an "incentive" is the anticipated reward or aversive event available in the environment. While motivation can often be used as a tool to help predict behavior, it varies greatly among individuals and must often be combined with ability and environmental factors to actually influence behavior and performance. Because of motivation's role in influencing workplace behavior and performance, it is key for organizations to understand and to structure the work environment to encourage productive behaviors and discourage those that are unproductive.
There is general consensus that motivation involves three psychological processes: arousal, direction, and intensity. Arousal is what initiates action. It is fueled by a person's need or desire for something that is missing from their lives at a given moment, either totally or partially. Direction refers to the path employees take in accomplishing the goals they set for themselves. Finally, intensity is the vigor and amount of energy employees put into this goal-directed work performance. The level of intensity is based on the importance and difficulty of the goal. These psychological processes result in four outcomes. First, motivation serves to direct attention, focusing on particular issues, people, tasks, etc. It also serves to stimulate an employee to put forth effort. Next, motivation results in persistence, preventing one from deviating from the goal-seeking behavior. Finally, motivation results in task strategies, which as defined by Mitchell & Daniels, are "patterns of behavior produced to reach a particular goal."
Occupational stress.
I/O psychologists are involved in the research and the practice of occupational stress and design of individual and organizational interventions to manage and reduce the stress levels and increase productivity, performance, health and wellbeing. Occupational stress is concerned with physical and psychosocial working conditions (termed stressors) that can elicit negative responses (termed strains) from employees. Occupational stress can have implications for organizational performance because of the emotions job stress evokes. For example, a job stressor such as conflict with a supervisor can precipitate anger that in turn motivates counterproductive workplace behaviors. Job-related hindrance stressors are directly (and challenge stressors inversely) related to turnover and turnover intentions. I/O research has examined the relations among work stressors and workplace aggression, withdrawal, theft, and substance abuse, strategies that individuals use to cope with work stress and prevent occupational burnout, and the relation of work stress to depressive symptoms.
A number of models have been developed to explain the job stress process. Examples of models that have influenced research include the person-environment fit model and the demand-control model. Research has also examined the interaction among personality variables and stressors and their effects on employee strains. I/O psychology is also concerned with the physical health outcomes caused by occupational stress. For instance, researchers at the institute of work psychology (IWP) examined the mediating role of psychological strain in relation to musculoskeletal disorders.
Research has also examined occupational stress in specific occupations. For example, there has been research on job stress in police, teachers, general practitioners, and dentists. Another concern has been the relation of occupational stress to family life. Other research has examined gender differences in leadership style and job stress and strain in the context of male- and female-dominated industries, burnout in the human services and other occupations, and unemployment-related distress. I/O psychology is also concerned with the relation of occupational stress to career advancement.
Occupational health and safety.
Occupational health and safety is concerned with how the work environment contributes to illness and injury of workers. Of particular importance are psychosocial hazards or risk factors that include fatigue, workplace violence, workplace bullying. Other factors important to employee health and well-being include work schedules (e.g., night shifts), work/family conflict, and burnout. Tools have been developed by I/O researchers and psychologists to measure these psychosocial risk factors in the workplace and "stress audits" can be used to help organizations remain compliant with various occupational health and safety regulations around the world.
Another area of concern is the high rate of occupational fatalities and injuries due to accidents. There is also research interest in how psychosocial hazards affect physical ailments like musculoskeletal disorder. A contributing psychosocial factor to accidents is safety climate, that concerns organizational policies and practices concerning safe behavior at work. A related concept that has to do with psychological well-being as opposed to accidents is psychosocial safety climate (PSC). PSC refers to policies, practices, and procedures for the protection of worker psychological health and safety. Safety leadership is another area of occupational health and safety I/O psychology is concerned with, where specific leadership styles affect safety compliance and safety participation.
Organizational culture.
Organizational culture can be described as a set of assumptions shared by the individuals in an organization that directs interpretation and action by defining appropriate behavior for various situations. There are three levels of organizational culture: artifacts, shared values, and basic beliefs and assumptions. Artifacts comprise the physical components of the organization that relay cultural meaning. Shared values are individuals' preferences regarding certain aspects of the organization's culture (e.g., loyalty, customer service). Basic beliefs and assumptions include individuals' impressions about the trustworthiness and supportiveness of an organization, and are often deeply ingrained within the organization's culture.
In addition to an overall culture, organizations also have subcultures. Examples of subcultures include corporate culture, departmental culture, local culture, and issue-related culture. While there is no single "type" of organizational culture, some researchers have developed models to describe different organizational cultures.
Organizational culture has been shown to have an impact on important organizational outcomes such as performance, attraction, recruitment, retention, employee satisfaction, and employee well-being. Also, organizations with an adaptive culture tend to perform better than organizations with an maladaptive culture.
Group behavior.
Group behavior is the interaction between individuals of a collective and the processes such as opinions, attitudes, growth, feedback loops, and adaptations that occur and change as a result of this interaction. The interactions serve to fulfill some need satisfaction of an individual who is part of the collective and helps to provide a basis for his interaction with specific members of the group.
A specific area of research in group behavior is the dynamics of teams. Team effectiveness refers to the system of getting people in a company or institution to work together effectively. The idea behind team effectiveness is that a group of people working together can achieve much more than if the individuals of the team were working on their own.
Team effectiveness.
Organizations support the use of teams, because teams can accomplish a much greater amount of work in a short period of time than can be accomplished by an individual contributor, and because the collective results of a group of contributors can produce higher quality deliverables. Five elements that are contributors to team effectiveness include: 
I/O research has looked at the negative impacts of workplace aggression on team performance and particularly team effectiveness as was evidenced in a recent study by Aube and Rousseau.
Team composition.
The composition of teams is initially decided during the selection of individual contributors that are to be assigned to specific teams and has a direct bearing on the resulting effectiveness of those teams. Aspects of team composition that should be considered during the team selection process include team member: knowledge, skills and abilities (KSAs), personalities, and attitudes.
As previously stated, one of the reasons organizations support the use of teams is the expectation of the delivery of higher quality results. To achieve these types of results, highly skilled members are more effective than teams built around those with lesser skills, and teams that include a diversity of skills have improved team performance (Guzzo & Shea, 1992). Additionally, increased average cognitive ability of team members has been shown to consistently correlate to increased work group effectiveness (Sundstrom et al., 2000). Therefore, organizations should seek to assign teams with team members that have a mix of KSAs. Teams that are composed of members that have the same KSAs may prove to be ineffective in meeting the team goals, no matter how talented the individual members are.
The personalities and attitudes of the individuals that are selected as team members are other aspects that should be taken into consideration when composing teams, since these individual traits have been found to be good indicators of team effectiveness. For example, a positive relationship between the team-level traits of agreeableness and conscientiousness and the team performance has been shown to exist (Van Vianen & De Dreu, 2001). Differing personalities of individual team members can affect the team climate in a negative way as members may clash and reduce team performance (Barrick, et al., 1998).
Task design.
A fundamental question in team task design is whether or not a task is even appropriate for a team. Those tasks that require predominantly independent work are best left to individuals, and team tasks should include those tasks that consist primarily of interdependent work. When a given task is appropriate for a team, task design can play a key role in team effectiveness (Sundstrom, et al., 2000).
The Job Characteristics Theory of motivation identifies core job dimensions that provide motivation for individuals and include: skill variety, task identity, task significance, autonomy and feedback (Hackman & Oldham, 1980). These dimensions map well to the team environment. Individual contributors that perform team tasks that are challenging, interesting, and engaging are more likely to be motivated to exert greater effort and perform better than those team members that are working on those tasks that do not have these characteristics.
Interrelated to the design of various tasks is the implementation method for the tasks themselves. For example, certain team members may find it challenging to cross train with other team members that have subject matter expertise in areas in which they are not familiar. In utilizing this approach, greater motivation is likely to result for both parties as the expert becomes the mentor and trainer and the cross-training team member finds learning new tasks to be an interesting change of pace. Such expansions of team task assignments can make teams more effective and require teams to spend greater amounts of time discussing and planning strategies and approaches for completing assigned tasks (Hackman, et al., 1976).
Organizational resources.
Organizational support systems impact the effectiveness of teams (Sundstrum, et al., 1990) and provide resources for teams operating in the multi-team environment. In this case, the provided resources include various resource types that teams require to be effective. During the chartering of new teams, organizational enabling resources are first identified. Examples of enabling resources include facilities, equipment, information, training and leadership. Also identified during team chartering are team-specific resources (e.g., budgetary resources, human resources). Team-specific human resources represent the individual contributors that are selected for each team as team members. Intra-team processes (e.g., task design, task assignment) are sufficient for effective utilization of these team-specific resources.
Teams also function in multi-team environments that are dynamic in nature and require teams to respond to shifting organizational contingencies (Salas, et al., 2004). In regards to resources, such contingencies include the constraints imposed by organizational resources that are not specifically earmarked for the exclusive use of certain teams. These types of resources are scarce in nature and must be shared by multiple teams. Examples of these scarce resources include subject matter experts, simulation and testing facilities, and limited amounts of time for the completion of multi-team goals. For these types of shared resources inter-team management processes (e.g.: constraint resource scheduling) must be provided to enable effective multi-team utilization.
Team rewards.
Organizational reward systems are a driver for strengthening and enhancing individual team member efforts that contribute towards reaching collective team goals (Luthans & Kreitner, 1985). In other words, rewards that are given to individual team members should be contingent upon the performance of the entire team (Sundstrom, et al., 1990).
Several design elements of organizational reward systems are needed to meet this objective. The first element for reward systems design is the concept that for a collective assessment to be appropriate for individual team members, the group's tasks must be highly interdependent. If this is not the case, individual assessment is more appropriate than team assessment (Wageman & Baker, 1997). A second design element is the compatibility between individual-level reward systems and team-level reward systems (DeMatteo, Eby, & Sundstrom, 1998). For example, it would be an unfair situation to reward the entire team for a job well done if only one team member did the great majority of the work. That team member would most likely view teams and teamwork in a negative fashion and not want to participate in a team setting in the future. A final design element is the creation of an organizational culture that supports and rewards employees who believe in the value of teamwork and who maintain a positive mental attitude towards team-based rewards (Haines and Taggar, 2006).
Team goals.
Goals for individual contributors have been shown to be motivating when they contain three elements: (1) difficulty, (2) acceptance, and (3) specificity (Lock & Latham, 1990). In the team setting, goal difficulty is related to group belief that the team can accomplish the tasks required to meet the assigned goal (Whitney, 1994). This belief (collective efficacy) is somewhat counterintuitive, but rests on team member perception that they now view themselves as more competent than others in the organization who were not chosen to complete such difficult goals. This in turn, can lead to higher levels of performance. Goal acceptance and specificity is also applicable to the team setting. When team members individually and collectively commit to team goals, team effectiveness is increased and is a function of increased supportive team behaviors (Aube & Rousseau, 2005).
As related to the team setting, it is also important to be aware of the interplay between the goals of individual contributors that participate on teams and the goals of the teams themselves. The selection of team goals must be done in coordination with the selection of goals for individuals. Individual goals must be in line with team goals (or not exist at all) to be effective (Mitchell & Silver, 1990). For example, a professional ball player that does well in his/her sport is rewarded individually for excellent performance. This individual performance generally contributes to improved team performance which can, in turn, lead to team recognition, such as a league championship.
Job satisfaction and commitment.
Job satisfaction reflects an employee's overall assessment of their job, particularly their emotions, behaviors, and attitudes about their work experience. It is one of the most heavily researched topics in industrial–organizational psychology with several thousand published studies. Job satisfaction has theoretical and practical utility for the field of psychology and has been linked to important job outcomes including attitudinal variables, absenteeism, employee turnover, and job performance. For instance, job satisfaction is strongly correlated with attitudinal variables such as job involvement, organizational commitment, job tensions, frustration, and feelings of anxiety. A 2010 meta-analyses found positive relationships between job satisfaction and life satisfaction, happiness, positive affect, and the absence of negative affect. Job satisfaction also has a weak correlation with employee's absentee behaviors and turnover from an organization with employees more likely to miss work or find other jobs if they are not satisfied. Finally, research has found that although a positive relationship exists between job satisfaction and performance, it is moderated by the use of rewards at an organization and the strength of employee's attitudes about their job.
Productive behavior.
Productive behavior is defined as employee behavior that contributes positively to the goals and objectives of an organization. When an employee begins a new job, there is a transition period during which he or she is not contributing positively to the organization. To successfully transition from being an outsider to a full-fledged member of an organization, an employee typically needs job-related training as well as more general information about the culture of the organization. In financial terms, productive behavior represents the point at which an organization begins to achieve some return on the investment it has made in a new employee. Industrial–organizational psychologists are typically more focused on productive behavior rather than simple job or task performance because of the ability to account for extra-role performance in addition to in-role performance. While in-role performance tells managers or researchers how well the employee performs the required technical aspects of the job, extra-role performance includes behaviors not necessarily required as part of the job but still contribute to organizational effectiveness. By taking both in-role and extra-role performance into account, industrial–organizational psychologists are able to assess employees' effectiveness (how well they do what they were hired to do), efficiency (their relative outputs to relative inputs), and their productivity (how much they help the organization reach its goals). Jex & Britt outline three different forms of productive behavior that industrial–organizational psychologists frequently evaluate in organizations: job performance; organizational citizenship behavior; and innovation.
Job performance.
Job performance represents behaviors employees engage in while at work which contribute to organizational goals. These behaviors are formally evaluated by an organization as part of an employee's responsibilities. In order to understand and ultimately predict job performance, it is important to be precise when defining the term. Job performance is about behaviors that are within the control of the employee and not about results (effectiveness), the costs involved in achieving results (productivity), the results that can be achieved in a period of time (efficiency), or the value an organization places on a given level of performance, effectiveness, productivity or efficiency (utility).
To model job performance, researchers have attempted to define a set of dimensions that are common to all jobs. Using a common set of dimensions provides a consistent basis for assessing performance and enables the comparison of performance across jobs. Performance is commonly broken into two major categories: in-role (technical aspects of a job) and extra-role (non-technical abilities such as communication skills and being a good team member). While this distinction in behavior has been challenged it is commonly made by both employees and management. A model of performance by Campbell breaks performance into in-role and extra-role categories. Campbell labeled job-specific task proficiency and non-job-specific task proficiency as in-role dimensions, while written and oral communication, demonstrating effort, maintaining personal discipline, facilitating peer and team performance, supervision and leadership and management and administration are labeled as extra-role dimensions. Murphy's model of job performance also broke job performance into in-role and extra-role categories. However, task-orientated behaviors composed the in-role category and the extra-role category included interpersonally-oriented behaviors, down-time behaviors and destructive and hazardous behaviors. However, it has been challenged as to whether the measurement of job performance is usually done through pencil/paper tests, job skills tests, on-site hands-on tests, off-site hands-on tests, high-fidelity simulations, symbolic simulations, task ratings and global ratings. These various tools are often used to evaluate performance on specific tasks and overall job performance. Van Dyne and LePine developed a measurement model in which overall job performance was evaluated using Campbell's in-role and extra-role categories. Here, in-role performance was reflected through how well "employees met their performance expectations and performed well at the tasks that made up the employees' job." Dimensions regarding how well the employee assists others with their work for the benefit of the group, if the employee voices new ideas for projects or changes to procedure and whether the employee attends functions that help the group composed the extra-role category.
To assess job performance, reliable and valid measures must be established. While there are many sources of error with performance ratings, error can be reduced through rater training and through the use of behaviorally-anchored rating scales. Such scales can be used to clearly define the behaviors that constitute poor, average, and superior performance. Additional factors that complicate the measurement of job performance include the instability of job performance over time due to forces such as changing performance criteria, the structure of the job itself and the restriction of variation in individual performance by organizational forces. These factors include errors in job measurement techniques, acceptance and the justification of poor performance and lack of importance of individual performance.
The determinants of job performance consist of factors having to do with the individual worker as well as environmental factors in the workplace. According to Campbell's Model of The Determinants of Job Performance, job performance is a result of the interaction between declarative knowledge (knowledge of facts or things), procedural knowledge (knowledge of what needs to be done and how to do it), and motivation (reflective of an employee's choices regarding whether to expend effort, the level of effort to expend, and whether to persist with the level of effort chosen). The interplay between these factors show that an employee may, for example, have a low level of declarative knowledge, but may still have a high level of performance if the employee has high levels of procedural knowledge and motivation.
Regardless of the job, three determinants stand out as predictors of performance: (1) general mental ability (especially for jobs higher in complexity); (2) job experience (although there is a law of diminishing returns); and (3) the personality trait of conscientiousness (people who are dependable and achievement-oriented, who plan well). These determinants appear to influence performance largely through the acquisition and usage of job knowledge and the motivation to do well. Further, an expanding area of research in job performance determinants includes emotional intelligence.
Organizational citizenship behavior.
Organizational citizenship behaviors ("OCBs") are another form of productive behavior, having been shown to be beneficial to both organization and team effectiveness. Dennis Organ is often thought of as the father of OCB research and defines OCBs as "individual behavior that is discretionary, not directly or explicitly recognized by the formal reward system, and that in the aggregate promotes the effective functioning of the organization." Behaviors that qualify as OCBs can fall into one of the following five categories: altruism, courtesy, sportsmanship, conscientiousness, and civic virtue.
Researchers have adapted, elaborated, or otherwise changed Organ's (1988) five OCB categories, but they remain popular today. The categories and their descriptions are as follows:
OCBs are also categorized using other methods. For example, Williams and Anderson categorize OCBs by their intended target, separating them into those targeted at individuals ("OCBIs"), supervisors ("OCBSs"), and those targeted at the organization as a whole ("OCBOs"). Additionally, Vigoda-Gadot uses a sub-category of OCBs called CCBs, or "compulsory OCBs" which is used to describe OCBs that are done under the influence of coercive persuasion or peer pressure rather than out of good will. This theory stems from debates concerning the reasons for conducting OCBs and whether or not they are truly voluntary in nature.
Jex & Britt offer three explanations as to why employees engage in organizational citizenship behavior. One relates to positive affect; for example, an overall positive mood tends to change the frequency of helping behavior to a higher rate. This theory stems from a history of numerous studies indicating that positive mood increases the frequency of helping and prosocial behaviors.
A second explanation, which stems from equity theory, is that employees reciprocate fair treatment that they received from the organization. Equity theory researchers found that certain forms of fairness or justice predict OCB better than others. For example, Jex & Britt mention research that indicates that interactional justice is a better predictor than procedural justice, which is in turn a better predictor than distributive justice.
A third explanation Jex & Britt offer is that, on the one hand, some employees hold personal values that tend to skew their behavior positively to participate in organizational citizenship activities. On the other hand, Jex & Britt's interpretation of research results suggest that other employees will tend to perform organizational citizenship behavior merely to influence how they are viewed within the organization, not because it reflects their personally held values. While these behaviors are not formally part of the job description, performing them can certainly influence performance appraisals. In contrast to this view, some I–O psychologists believe that employees engage in OCBs as a form of "impression management," a term coined by Erving Goffman in his 1959 book "The Presentation of Self in Everyday Life". Goffman defines impression management as "the way in which the individual ... presents himself and his activity to others, the ways in which he guides and controls the impression they form of him, and the kinds of things he may and may not do while sustaining his performance before them." Researchers such as Bolino have hypothesized that the act of performing OCBs is not done out of goodwill, positive affect, etc., but instead as a way of being noticed by superiors and looking good in the eyes of others. The key difference between this view and those mentioned by Jex & Britt is that the intended beneficiary of the behavior is the individual who engages in it, rather than another individual, the organization, or the supervisor.
With this research on why employees engage in OCBs comes the debate among I–O psychologists about the voluntary or involuntary nature of engaging in OCBs. Many researchers, including the "father of OCB research," Dennis Organ have consistently portrayed OCBs as voluntary behaviors done at the discretion of the individual. However, more recently researchers have brought attention to potential underlying causes of OCBs, including social pressure, coercion, and other external forces. For example, Eran Vigoda-Gadot suggests that some, but not all, OCBs may be performed voluntarily out of goodwill, but many may be more involuntary in nature and "may arise from coercive managerial strategies or coercive social pressure by powerful peers." As mentioned previously, Vigoda-Gadot categorizes these behaviors in a separate category of OCBs as "compulsory OCBs" or CCBs, which he suggests are a form of "abusive supervision" and will result in poorer organizational performance, similar to what has been seen in other research on abusive supervision and coercive persuasion.
Innovation.
Industrial and Organizational Psychologists consider innovation, more often than not, a variable of less importance and often a counter-productive one to include in conducting job performance appraisals when irrelevant to the major job functions for which a given job exists. Nonetheless, Industrial and Organizational Psychologists see the value of that variable where its consideration would, were its reliability and validity questioned, achieve a statistically significant probability that its results are not due to chance, and that it can be replicated reliably with a statistically significant ratio of reliability, and that were a court to raise a question on its reliability and validity testing, the Industrial and Organizational Psychologist behind its use would be able to defend it before a court of justice with the belief that it will stand before such a court as reliable, and valid.
With the above in mind, innovation is often considered a form of productive behavior that employees exhibit when they come up with novel ideas that further the goals of the organization. This section will discuss three topics of interest: research on innovation; characteristics of an individual that may predict innovation; and how organizations may be structured to promote innovation. According to Jex & Britt, individual and organization research can be divided into four unique research focuses. 
As indicated above, the first focus looks specifically to find certain attributes of an individual that may lead to innovation, therefore, one must ask, "Are there quantifiable predictors that an individual will be innovative?" Research indicates if various skills, knowledge, and abilities are present then an individual will be more apt to innovation. These qualities are generally linked to creativity. A brief overview of these characteristics are listed below.
In addition to the role and characteristics of the individual, one must consider what it is that may be done on an organizational level to develop and reward innovation. A study by Damanpour identified four specific characteristics that may predict innovation within an organization. They are the following ones:
Additionally, organizations could use and institutionalize many participatory system-processes, which could breed innovation in the workplace. Some of these items include providing creativity training, having leaders encourage and model innovation, allowing employees to question current procedures and rules, seeing that the implementation of innovations had real consequences, documenting innovations in a professional manner, allowing employees to have autonomy and freedom in their job roles, reducing the number of obstacles that may be in the way of innovation, and giving employees access to resources (whether these are monetary, informational, or access to key people inside or outside of the organization).
According to the American Productivity & Quality Center ("APQC") there are basic principles an organization can develop to encourage and reward innovation.
innovation.
In discussing innovation for a Best-Practice report, APQC Knowledge Management expert, Kimberly Lopez, stated, "It requires a blending of creativity within business processes to ensure good ideas become of value to the company ... Supporting a creative environment requires innovation to be recognized, nurtured, and rewarded."
Counterproductive work behavior.
Counterproductive work behavior (CWB) can be defined as employee behavior that goes against the goals of an organization. These behaviors can be intentional or unintentional and result from a wide range of underlying causes and motivations. Some CWBs have instrumental motivations (e.g., theft). It has been proposed that a person-by-environment interaction can be utilized to explain a variety of counterproductive behaviors (Fox and Spector, 1999). For instance, an employee who sabotages another employee's work may do so because of lax supervision (environment) and underlying psychopathology (person) that work in concert to result in the counterproductive behavior. There is evidence that an emotional response (e.g., anger) to job stress (e.g., unfair treatment) can motivate CWBs.
The forms of counterproductive behavior with the most empirical examination are ineffective job performance, absenteeism, job turnover, and accidents. Less common but potentially more detrimental forms of counterproductive behavior have also been investigated including violence and sexual harassment.
Leadership.
In I–O psychology, leadership can be defined as a process of influencing others to agree on a shared purpose, and to work towards shared objectives. A distinction should be made between leadership and management. Managers process administrative tasks and organize work environments. Although leaders may be required to undertake managerial duties as well, leaders typically focus on inspiring followers and creating a shared organizational culture and values. Managers deal with complexity, while leaders deal with initiating and adapting to change. Managers undertake the tasks of planning, budgeting, organizing, staffing, controlling and problem solving. In contrast, leaders undertake the tasks of setting a direction or vision, aligning people to shared goals, communicating, and motivating.
Approaches to studying leadership in I–O psychology can be broadly classified into three categories: Leader-focused approaches, Contingency-focused approaches, and Follower-focused approaches.
Leader-focused approaches.
Leader-focused approaches look to organizational leaders to determine the characteristics of effective leadership. According to the trait approach, more effective leaders possess certain traits that less effective leaders lack. More recently, this approach is being used to predict leader emergence. The following traits have been identified as those that predict leader emergence when there is no formal leader: high intelligence, high needs for dominance, high self-motivation, and socially perceptive. Another leader-focused approached is the behavioral approach which focuses on the behaviors that distinguish effective from ineffective leaders. There are two categories of leadership behaviors: (1) consideration; and (2) initiating structure. Behaviors associated with the category of consideration include showing subordinates they are valued and that the leader cares about them. An example of a consideration behavior is showing compassion when problems arise in or out of the office. Behaviors associated with the category of initiating structure include facilitating the task performance of groups. One example of an initiating structure behavior is meeting one-on-one with subordinates to explain expectations and goals. The final leader-focused approach is power and influence. To be most effective a leader should be able to influence others to behave in ways that are in line with the organization's mission and goals. How influential a leader can be depends on their social power or their potential to influence their subordinates. There are six bases of power: coercive power, reward power, legitimate power, expert power, referent power, and informational power. A leader can use several different tactics to influence others within an organization. These common tactics include: rational persuasion, inspirational appeal, consultation, ingratiation, exchange, personal appeal, coalition, legitimating, and pressure.
Contingency-focused approaches.
Of the 3 approaches to leadership, contingency-focused approaches have been the most prevalent over the past 30 years. Contingency-focused theories base a leader's effectiveness on their ability to assess a situation and adapt their behavior accordingly. These theories assume that an effective leader can accurately "read" a situation and skillfully employ a leadership style that meets the needs of the individuals involved and the task at hand. A brief introduction to the most prominent contingency-focused theories will follow.
Fiedler's Contingency Theory holds that a leader's effectiveness depends on the interaction between their characteristics and the characteristics of the situation. Path–Goal Theory asserts that the role of the leader is to help his or her subordinates achieve their goals. To effectively do this, leaders must skillfully select from four different leadership styles to meet the situational factors. The situational factors are a product of the characteristics of subordinates and the characteristics of the environment. The Leader-Member Exchange (LMX) Model focuses on how leader–subordinate relationships develop. Generally speaking, when a subordinate performs well or when there are positive exchanges between a leader and a subordinate, their relationship is strengthened, performance and job satisfaction are enhanced, and the subordinate will feel more commitment to the leader and the organization as a whole. Vroom-Yetton-Jago Model focuses on decision making with respect to a "feasibility set" which is composed of the situational attributes.
In addition to the contingency-focused approaches mentioned, there has been a high degree of interest paid to three novel approaches that have recently emerged. The first is transformational leadership, which posits that there are certain leadership traits that inspire subordinates to perform beyond their capabilities. The second is transactional leadership, which is most concerned with keeping subordinates in-line with deadlines and organizational policy. This type of leader fills more of a managerial role and lacks qualities necessary to inspire subordinates and induce meaningful change. And the third is authentic leadership which is centered around empathy and a leader's values or character. If the leader understands their followers, they can inspire subordinates by cultivating a personal connection and leading them to share in the vision and goals of the team. Although there has been a limited amount of research conducted on these theories, they are sure to receive continued attention as the field of I–O psychology matures.
Follower-focused approaches.
Follower-focused approaches look at the processes by which leaders motivate followers, and lead teams to achieve shared goals. Understandably, the area of leadership motivation draws heavily from the abundant research literature in the domain of motivation in I–O psychology. Because leaders are held responsible for their followers' ability to achieve the organization's goals, their ability to motivate their followers is a critical factor of leadership effectiveness. Similarly, the area of team leadership draws heavily from the research in teams and team effectiveness in I–O psychology. Because organizational employees are frequently structured in the form of teams, leaders need to be aware of the potential benefits and pitfalls of working in teams, how teams develop, how to satisfy team members' needs, and ultimately how to bring about team effectiveness and performance. An emerging area of research in the area of team leadership is in leading virtual teams, where people in the team are geographically-distributed across various distances and sometimes even countries. While technological advances have enabled the leadership process to take place in such virtual contexts, they present new challenges for leaders as well, such as the need to use technology to build relationships with followers, and influencing followers when faced with limited (or no) face-to-face interaction.
Organizational change/development.
Organizational development.
Industrial-organizational psychologists have displayed a great deal of consideration for the problems of total organizational change and systematic ways to bring about planned change. This effort, called organizational development (OD), involves techniques such as:
Within the survey feedback technique, surveys after being answered by employees periodically, are assessed for their emotions and attitudes which are then communicated to various members within the organization. The team building technique was created due to realization that most tasks within the organization are completed by small groups and/or teams. In order to further enhance a team's or group's morale and problem-solving skills, OD consultants (called change agents) help the groups to build their self-confidence, group cohesiveness, and working effectiveness. A change agent's impartiality, gives the managers within the organization a new outlook of the organization's structure, functions, and culture. A change agent's first task is diagnosis, where questionnaires and interviews are used to assess the problems and needs of the organization. Once analyzed, the strengths and weaknesses of the organization are presented and used to create strategies for solving problems and coping with future changes.
Flexibility and adaptability are some strengths of the OD process, as it possesses the ability to conform to the needs of the situation. Regardless of the specific techniques applied, the OD process helps to free the typical bureaucratic organization from its rigidity and formality, hereby allowing more responsiveness and open participation. Public and private organizations both have employed OD techniques, despite their varied results in research conducted. However, the use of the techniques are justified by the significant increases in productivity that was proven by various studies.
Relation to organizational behavior.
The i/o psychology and organizational behavior have manifested some overlap. The overlap has led to some confusion regarding how the two disciplines differ.
Training and outlook.
Graduate programs.
Schultz and Schultz (2010) states that modern I–O Psychology is a complex and intricate position. It requires intense university training, and hands on experience. Individuals who choose I–O psychology as a profession should also be aware that they will be constantly studying to learn about new developments that may emerge. The minimum requirement for working as an I–O psychologist is a master's degree. Normally, this degree requires 42 semester hours and takes about 2–3 years to complete. Most master's degree students work, either full-time or part-time, while studying to become an I–O psychologist. Of all the degrees granted in I–O psychology, each year approximately two thirds are at the master's level.
A comprehensive list of US and Canadian master's and doctoral programs can be found at the web site of the Society for Industrial and Organizational Psychology (SIOP). Some helpful ways to learn about graduate programs include visiting the web sites on the SIOP list and speaking to I–O faculty at the institutions listed. Admission into I–O psychology PhD programs is highly competitive given that many programs accept a small number of applicants every year.
There are graduate degree programs in I–O psychology outside of the US and Canada. The SIOP web site also provides a comprehensive list of I–O programs in many other countries.
Job outlook.
According to the United States Department of Labor's Bureau of Labor Statistics, I-O psychology is the fastest growing occupation in the United States, based on projections between 2012 and 2022.
According to recent salary and employment surveys conducted by SIOP, the median salary for a PhD in I–O psychology was $98,000; for a master's level I–O psychologist was $72,000. The highest paid PhD I–O psychologists in private industry worked in pharmaceuticals and averaged approximately $151,000 per year; the median salary for self-employed consultants was $150,000; those employed in retail, energy, and manufacturing followed closely behind, averaging approximately $133,000. The lowest earners were found in state and local government positions, averaging approximately $77,000. I–O psychologists whose primary responsibility is teaching at private and public colleges and universities often earn additional income from consulting with government and industry.
Pros and cons of an industrial and organizational psychology career.
Pros of a Career in I–O Psychology:
Cons of a Career in I–O Psychology:
Ethics.
In the consulting field, it is important for the consultant to maintain high ethical standards in all aspects of relationships: consultant to client, consultant to consultant, and client to consultant. After all, all decisions made and actions taken by the consultant will reflect what kind of consultant he or she is. Although ethical situations can be more intricate in the business world, American Psychology Association (APA)’s Ethical Principles of Psychologists and Code of Conduct can be applied to I–O consultants as well. For example, the consultant should only accept projects for which he or she is qualified; the consultant should also avoid all conflicts of interest and being in multiple relationships with those he or she is working with. On the other hand, some might disagree that it is the consultant’s responsibility to actively promote the application of moral and ethical standards in the consultation and examine ethical issues in organizational decisions and policies. It is an ongoing controversial issue in the consulting field. In addition, as more and more organizations are becoming global, it is imperative for consultants working abroad to quickly become aware of rules, regulations, and cultures of the organizations and countries they are in as well as not to ignore ethical standards and codes just because they are abroad.
Industrial/organizational consultancy.
Definition.
An industrial/organizational (I–O) consultant helps clients and organizations improve productivity and create an optimal working environment through human capital consulting and strategies. Areas of consulting include but are not limited to selection and recruiting, training, leadership, and development, compensation and benefits, employee relations, performance management, succession planning, and executive coaching.
Types.
Consultants can be categorized as internal or external to an organization. An internal consultant is someone who is working specifically for an organization that he or she is a part of whereas an external consultant can be either a sole proprietor or an employee of a consulting firm who is hired by another organization on a project basis or for a certain period of time. There are different types of I–O consultants:
Services offered.
Kurpius (1978; as cited in Hedge & Borman, 2009) gave four general types of consultation:
Consultants offer these consulting services to all kinds of organizations, such as profit and nonprofit sectors, public and private sectors, and a government organization.
Pros and cons.
Like any other careers, there are many benefits and downsides of consulting. Some advantages are substantial material rewards, trust and respect from clients, and personal satisfaction. Some disadvantages are traveling (the number one complaint of all I/O consultants), uncertainty in business especially for external consultants, and marginality which is not belonging to any group or organization that the consultant works for.
Competencies.
There are many different sets of competencies for different specializations within I–O psychology and I–O psychologists are versatile behavioral scientists. For example, an I–O psychologist specializing in selection and recruiting should have expertise in finding the best talent for the organization and getting everyone on board while he or she might not need to know much about executive coaching. Some consultants tend to specialize in specific areas of consulting whereas others tend to generalize their areas of expertise. However, Cummings and Worley (2009) claimed that there are basic skills and knowledge, which most consultants agree, needed to be effective consultants: 
Stages.
Block (2011) identified the following five stages of consulting.
Entry and contracting.
This stage is where the consultant makes the initial contact with the client about the project, and it includes setting up the first meeting, exploring more about the project and the client, roles, responsibilities, and expectations about the consultant, the client, and the project, and whether the consultant’s expertise and experience fit with what the client wants out of the project. This is the most important part of the consulting, and most consultants agree that most mistakes in the project can essentially be traced back to the faulty contracting stage.
Discovery and diagnosis.
This stage is where the consultant makes his or her own judgment about the problem identified by the client and about the project. Sometimes, the problem presented by the client is not the actual problem but a symptom of a true cause. Then, the consultant collects more information about the situation.
Analysis and planning.
This stage is where the consultant analyzes the data and presents the results to the client. The consultant needs to reduce a large amount of data into a manageable size and present them to the client in a clear and simple way. After presenting the results, the consultant helps the client make plans and goals for actions to be taken as a next step to solve the identified problem.
Engagement and implementation.
This stage sometimes falls entirely on the client or the organization, and the consultant’s job might be completed at the end of third stage. However, it is important for the consultant to be present at the fourth stage since without implementing the changes suggested by the consultant, the problem is not likely to be solved. Moreover, despite how good the consultant’s advice might be, employees are actually the ones who need to live the changes. So, in this fourth stage, the consultant needs to get everyone on board with the changes and help implement the changes.
Extension or termination.
This final stage is where the consultant and the client evaluate the project, and it is usually the most neglected yet important stage. Then, the project is completed or extended depending on the client’s needs.
Future trends.
Teachout and Vequist (2008) identified driving forces affecting future trends in the business consulting: 
They also discussed three trends in the field as a result of these forces – people, process, and technology.
Human capital or people.
In terms of human capital or people consulting, there are major forces for future trends:
As a result, trends, such as major talent management, selection and recruiting, workplace education and training, and planning for next generation, have emerged. In addition, change management also becomes important in organizations in order to innovate and implement new technology, tools, and systems to cope with changes in the business.
Process.
In terms of process consulting, because of an increase in competition, it becomes important to identify and improve key processes that meet customer values and demands as well as that are faster and cheaper.
Technology.
In terms of technology consulting, there is an increased need to automate processes or data so that employees can focus on actually doing work rather than doing the manual labor. The consultant can add value to these technologies by providing training, communication plans, and change management as well as to incorporate these technologies into organizational culture. So, regardless of how advanced technology is, consultants are still needed to make sure that these advanced technologies have positive effects on employees and organizations in both technical and social aspects.
Aside from technology consulting, there is a future trend for the interaction that comes with technology. This includes human-technology interaction, technology-technology interaction, and human-human interaction through technology. Due to evolving technologies, communication and relationships in the workplace are dramatically changing. Technology consultants help organizations cope with the interjection of technology in the work place. However, their job descriptions will eventually expand to include proper technology communication styles and when technology does or does not have a place in an interaction. This delicate subject alters the meanings and interpretations behind social interactions and creating concise guidelines to technological interactions is essential.

</doc>
<doc id="15451" url="https://en.wikipedia.org/wiki?curid=15451" title="International Council of Unitarians and Universalists">
International Council of Unitarians and Universalists

The International Council of Unitarians and Universalists (ICUU) is an umbrella organization founded in 1995 bringing together many Unitarian, Universalist, and Unitarian Universalist organizations. 
The size of the affiliated organizations varies widely. Some groups represent only a few hundred people; while the largest, the Unitarian Universalist Association, has over 160,000 members and is larger than all the other groups put together.
History.
The original initiative for its establishment was contained in a resolution of the General Assembly of Unitarian and Free Christian Churches (British Unitarians) in 1987. This led to the establishment of the Advocates for the Establishment of an International Organization of Unitarians (AEIOU), which worked towards creating the council. However, the General Assembly resolution provided no funding.
The Unitarian Universalist Association (UUA) became particularly interested in the establishment of a council when it had to deal with an increasing number of applications for membership from congregations outside North America. It had already granted membership to congregations in Adelaide, Auckland, the Philippines and Pakistan, and congregations in Sydney, Russia and Spain had applied for membership. Rather than admit congregations from all over the world, the UUA hoped that they would join a world council instead. The UUA thus became willing to provide funding for the council's establishment.
As a result, the council was finally established at a meeting in Essex, Massachusetts, United States on 23–26 March 1995. The Rev. David Usher, a British Unitarian minister of Australian origin who had proposed the original motion eight years previously, became the ICUU's first President.
Principles and purposes.
The Preamble to the Constitution of the International Council of Unitarians and Universalists reads:
We, the member groups of the International Council of Unitarians and Universalists, affirming our belief in religious community based on:
declare our purposes to be:
Members.
Reorganizing.
Spain and the Polish Unitarians have reported a need for a period of reorganization, and that at this time they are unable to maintain the level of activity needed to be full Council members, be it moved that membership of these groups be suspended. This action is taken with regret and the ICUU looks forward to welcoming Spain and Poland back into membership at the earliest possible date.
The Unitarian Universalists of Russia were a founding member of the ICUU. Its membership in the Council was officially dropped in 2007 because of persistent lack of activity.
Provisional members.
Churches and religious associations which have expressed their will to become members of the Council may be admitted as "Provisional Members" for a period of time (generally two or four years), until the Council decides that they have shown their organizational stability, affinity with the ICUU principles and commitment to deserve becoming Full Members of the Council. Provisional Members are invited to Council meetings through a delegate but cannot vote.
The following organizations have been accepted by the ICUU Executive Committee but they have not yet been ratified by the Council Meeting:
Emerging groups.
According to the Bylaws of the ICUU, Emerging Groups are ""applicants that are deemed to be reasonable prospects for membership, but do not fulfil the conditions of either Provisional membership or Full Membership"". These groups may be designated as Emerging Groups by the Executive Committee upon its sole discretion. Emerging Groups may be invited as observers to General Meetings.
The current list of Emerging Groups after the last meeting of the Executive Committee (London, 22–25 November 2008) is as follows:
Associates.
Organizations with beliefs and purposes closely akin to those of ICUU but which by nature of their constitution are not eligible for full membership or which do not wish to become full members now or in the foreseeable future, may become Associates of the ICUU. The application must be approved by the ICUU Council Meeting.
The current list of Associates is as follows:
The following organizations have been accepted by the ICUU Executive Committee but they have not yet been ratified by the Council Meeting:

</doc>
<doc id="15454" url="https://en.wikipedia.org/wiki?curid=15454" title="Itanium">
Itanium

Itanium ( ) is a family of 64-bit Intel microprocessors that implement the Intel Itanium architecture (formerly called IA-64). Intel markets the processors for enterprise servers and high-performance computing systems. The Itanium architecture originated at Hewlett-Packard (HP), and was later jointly developed by HP and Intel.
Itanium-based systems have been produced by HP (the HP Integrity Servers line) and several other manufacturers. , Itanium was the fourth-most deployed microprocessor architecture for enterprise-class systems, behind x86-64, Power Architecture, and SPARC.
The processor was released on November 8, 2012. While Intel said in April 2015 that it continued to work on Poulson's successor, Kittson, Poulson was the most recent processor available, Hewlett-Packard was the only customer; even HP had introduced Xeon-based machines, and it appeared that Kittson would be the last Itanium.
Market reception.
High-end server market.
When first released in 2001, Itanium's performance was disappointing compared to better-established RISC and CISC processors. Emulation to run existing x86 applications and operating systems was particularly poor, with one benchmark in 2001 reporting that it was equivalent at best to a 100 MHz Pentium in this mode (1.1 GHz Pentiums were on the market at that time).
Itanium failed to make significant inroads against IA-32 or RISC, and suffered further following the arrival of x86-64 systems which offered greater compatibility with older x86 applications.
In a 2009 article on the history of the processor — "How the Itanium Killed the Computer Industry" — journalist John C. Dvorak reported "This continues to be one of the great fiascos of the last 50 years" . Tech columnist Ashlee Vance commented that the delays and underperformance "turned the product into a joke in the chip industry." In an interview, Donald Knuth said "The Itanium approach...was supposed to be so terrific—until it turned out that the wished-for compilers were basically impossible to write."
Both Red Hat and Microsoft announced plans to drop Itanium support in their operating systems due to lack of market interest; however, other Linux distributions such as Gentoo and Debian remain available for Itanium. On March 22, 2011, Oracle announced discontinuation of development on Itanium, although its technical support for its existing products would continue. In October 2013, Oracle committed to release Oracle Database 12.1.0.1.0 on HP-UX Itanium 11.31 by early 2014.
A former Intel official reported that the Itanium business had become profitable for Intel in late 2009. By 2009, the chip was almost entirely deployed on servers made by HP, which had over 95% of the Itanium server market share, making the main operating system for Itanium HP-UX. On March 22, 2011, Intel reaffirmed its commitment to Itanium with multiple generations of chips in development and on schedule.
Other markets.
Although Itanium did attain limited success in the niche market of high-end computing, Intel had originally hoped it would find broader acceptance as a replacement for the original x86 architecture.
AMD chose a different direction, designing the less radical x86-64, a 64-bit extension to the existing x86 architecture, which Microsoft then supported, forcing Intel to introduce the same extensions in its own x86-based processors. These designs can run existing 32-bit applications at native hardware speed, while offering support for 64-bit memory addressing and other enhancements to new applications. This architecture has now become the predominant 64-bit architecture in the desktop and portable market. Although some Itanium-based workstations were initially introduced by companies such as SGI, they are no longer available.
History.
Development: 1989–2000.
In 1989, HP determined that Reduced Instruction Set Computing (RISC) architectures were approaching a processing limit at one instruction per cycle. HP researchers investigated a new architecture, later named Explicitly Parallel Instruction Computing (EPIC), that allows the processor to execute multiple instructions in each clock cycle. EPIC implements a form of very long instruction word (VLIW) architecture, in which a single instruction word contains multiple instructions. With EPIC, the compiler determines in advance which instructions can be executed at the same time, so the microprocessor simply executes the instructions and does not need elaborate mechanisms to determine which instructions to execute in parallel.
The goal of this approach is twofold: to enable deeper inspection of the code at compile time to identify additional opportunities for parallel execution, and to simplify processor design and reduce energy consumption by eliminating the need for runtime scheduling circuitry.
HP believed that it was no longer cost-effective for individual enterprise systems companies such as itself to develop proprietary microprocessors, so it partnered with Intel in 1994 to develop the IA-64 architecture, derived from EPIC. Intel was willing to undertake a very large development effort on IA-64 in the expectation that the resulting microprocessor would be used by the majority of enterprise systems manufacturers. HP and Intel initiated a large joint development effort with a goal of delivering the first product, Merced, in 1998.
During development, Intel, HP, and industry analysts predicted that IA-64 would dominate in servers, workstations, and high-end desktops, and eventually supplant RISC and Complex Instruction Set Computing (CISC) architectures for all general-purpose applications.
Compaq and Silicon Graphics decided to abandon further development of the Alpha and MIPS architectures respectively in favor of migrating to IA-64.
Several groups developed operating systems for the architecture, including Microsoft Windows, OpenVMS, Linux, and UNIX variants such as HP-UX, Solaris,
Tru64 UNIX, and Monterey/64
(the last three were canceled before reaching the market). By 1997, it was apparent that the IA-64 architecture and the compiler were much more difficult to implement than originally thought, and the delivery of Merced began slipping.
Technical difficulties included the very high transistor counts needed to support the wide instruction words and the large caches. There were also structural problems within the project, as the two parts of the joint team used different methodologies and had slightly different priorities. Since Merced was the first EPIC processor, the development effort encountered more unanticipated problems than the team was accustomed to. In addition, the EPIC concept depends on compiler capabilities that had never been implemented before, so more research was needed.
Intel announced the official name of the processor, "Itanium", on October 4, 1999.
Within hours, the name "Itanic" had been coined on a Usenet newsgroup, a reference to "Titanic", the "unsinkable" ocean liner that sank in 1912.
"Itanic" has since often been used by "The Register",
and others,
to imply that the multibillion-dollar investment in Itanium—and the early hype associated with it—would be followed by its relatively quick demise.
Itanium (Merced): 2001.
By the time Itanium was released in June 2001, its performance was not superior to competing RISC and CISC processors.
Itanium competed at the low-end (primarily 4-CPU and smaller systems) with servers based on x86 processors, and at the high end with IBM's POWER architecture and Sun Microsystems' SPARC architecture. Intel repositioned Itanium to focus on high-end business and HPC computing, attempting to duplicate x86's successful "horizontal" market (i.e., single architecture, multiple systems vendors). The success of this initial processor version was limited to replacing PA-RISC in HP systems, Alpha in Compaq systems and MIPS in SGI systems, though IBM also delivered a supercomputer based on this processor.
POWER and SPARC remained strong, while the 32-bit x86 architecture continued to grow into the enterprise space, building on economies of scale fueled by its enormous installed base.
Only a few thousand systems using the original "Merced" Itanium processor were sold, due to relatively poor performance, high cost and limited software availability.
Recognizing that the lack of software could be a serious problem for the future, Intel made thousands of these early systems available to independent software vendors (ISVs) to stimulate development. HP and Intel brought the next-generation Itanium 2 processor to market a year later.
Itanium 2: 2002–2010.
The Itanium 2 processor was released in 2002, and was marketed for enterprise servers rather than for the whole gamut of high-end computing. The first Itanium 2, code-named "McKinley", was jointly developed by HP and Intel. It relieved many of the performance problems of the original Itanium processor, which were mostly caused by an inefficient memory subsystem. "McKinley" contained 221 million transistors (of which 25 million were for logic), measured 19.5 mm by 21.6 mm (421 mm2) and was fabricated in a 180 nm, bulk CMOS process with six layers of aluminium metallization.
In 2003, AMD released the Opteron, which implemented its own 64-bit architecture (AMD64). Opteron gained rapid acceptance in the enterprise server space because it provided an easy upgrade from x86. Intel responded by implementing x86-64 in its Xeon microprocessors in 2004.
Intel released a new Itanium 2 family member, codenamed "Madison", in 2003. Madison used a 130 nm process and was the basis of all new Itanium processors until Montecito was released in June 2006.
In March 2005, Intel announced that it was working on a new Itanium processor, codenamed "Tukwila", to be released in 2007. Tukwila would have four processor cores and would replace the Itanium bus with a new Common System Interface, which would also be used by a new Xeon processor.
Later that year, Intel revised Tukwila's delivery date to late 2008.
In November 2005, the major Itanium server manufacturers joined with Intel and a number of software vendors to form the Itanium Solutions Alliance to promote the architecture and accelerate software porting.
The Alliance announced that its members would invest $10 billion in Itanium solutions by the end of the decade.
In 2006, Intel delivered "Montecito" (marketed as the Itanium 2 9000 series), a dual-core processor that roughly doubled performance and decreased energy consumption by about 20 percent.
Intel released the Itanium 2 9100 series, codenamed "Montvale", in November 2007.
In May 2009, the schedule for Tukwila, its follow-on, was revised again, with release to OEMs planned for the first quarter of 2010.
Itanium 9300 (Tukwila): 2010.
The Itanium 9300 series processor, codenamed "Tukwila", was released on February 8, 2010, with greater performance and memory capacity.
The device uses a 65 nm process, includes two to four cores, up to 24 MB on-die caches, Hyper-Threading technology and integrated memory controllers. It implements double-device data correction, which helps to fix memory errors. Tukwila also implements Intel QuickPath Interconnect (QPI) to replace the Itanium bus-based architecture. It has a peak interprocessor bandwidth of 96 GB/s and a peak memory bandwidth of 34 GB/s. With QuickPath, the processor has integrated memory controllers and interfaces the memory directly, using QPI interfaces to directly connect to other processors and I/O hubs. QuickPath is also used on Intel processors using the "Nehalem" microarchitecture, making it probable that Tukwila and Nehalem will be able to use the same chipsets.
Tukwila incorporates four memory controllers, each of which supports multiple DDR3 DIMMs via a separate memory controller,
much like the Nehalem-based Xeon processor code-named "Beckton".
Itanium 9500 (Poulson): 2012.
The Itanium 9500 series processor, codenamed "Poulson", is the follow-on processor to Tukwila and was released on November 8, 2012.
According to Intel, it skips the 45 nm process technology and uses a 32 nm process technology; it features eight cores, has a 12-wide issue architecture, multithreading enhancements, and new instructions to take advantage of parallelism, especially in virtualization.
The Poulson L3 cache size is 32 MB. L2 cache size is 6 MB, 512 I KB, 256 D KB per core. Die size is 544 mm², less than its predecessor Tukwila (698.75 mm²).
At ISSCC 2011, Intel presented a paper called, "A 32nm 3.1 Billion Transistor 12-Wide-Issue Itanium Processor for Mission Critical Servers."
Given Intel's history of disclosing details about Itanium microprocessors at ISSCC, this paper most likely refers to Poulson. Analyst David Kanter speculates that Poulson will use a new microarchitecture, with a more advanced form of multi-threading that uses as many as two threads, to improve performance for single threaded and multi-threaded workloads.
Some new information was released at Hot Chips conference.
New information presents improvements in multithreading, resilency improvements (Intel Instruction Replay RAS) and few new instructions (thread priority, integer instruction, cache prefetching, data access hints).
In Intel's Product Change Notification (PCN) 111456-01, it listed 4 models of Itanium 9500 series CPU, which was later removed in a revised document. The parts were later listed in Intel's Material Declaration Data Sheets (MDDS) database. Intel later posted Itanium 9500 reference manual.
The models are:
Kittson.
Rumours of a successor to Poulson coded-name Kittson began to circulate in 2012–2013, at first associated with a forthcoming 22 nm shrink, later walked-back in the face of declining Itanium sales to a less-ambitious 32 nm node. Intel has never confirmed the formal specifications for Kittson, but has confirmed that it continues to work on the project as recently as April 2015. Meanwhile, the aggressively multicore Xeon E7 platform has begun to displace Itanium-based solutions in the Intel roadmap.
Market share.
In comparison with its Xeon family of server processors, Itanium has never been a high-volume product for Intel. Intel does not release production numbers. One industry analyst estimated that the production rate was 200,000 processors per year in 2007.
According to Gartner Inc., the total number of Itanium servers (not processors) sold by all vendors in 2007, was about 55,000. (It is unclear whether clustered servers counted as a single server or not.) This compares with 417,000 RISC servers (spread across all RISC vendors) and 8.4 million x86 servers. IDC reports that a total of 184,000 Itanium-based systems were sold from 2001 through 2007. For the combined POWER/SPARC/Itanium systems market, IDC reports that POWER captured 42% of revenue and SPARC captured 32%, while Itanium-based system revenue reached 26% in the second quarter of 2008.
According to an IDC analyst, in 2007, HP accounted for perhaps 80% of Itanium systems revenue.
According to Gartner, in 2008, HP accounted for 95% of Itanium sales. HP's Itanium system sales were at an annual rate of $4.4Bn at the end of 2008, and declined to $3.5Bn by the end of 2009,
compared to a 35% decline in UNIX system revenue for Sun and an 11% drop for IBM, with an x86-64 server revenue increase of 14% during this period.
In December 2012, IDC released a research report stating that Itanium server shipments would remain flat through 2016, with annual shipment of 26,000 systems (a decline of over 50% compared to shipments in 2008).
Hardware support.
Systems.
By 2006 HP manufactured at least 80% of all Itanium systems, and sold 7,200 in the first quarter of 2006.
The bulk of systems sold were enterprise servers and machines for large-scale technical computing, with an average selling price per system in excess of US$200,000. A typical system uses eight or more Itanium processors.
By 2012 only a few manufacturers offered Itanium systems, including HP, Bull, NEC, Inspur and Huawei. In addition, Intel offered a chassis that could be used by system integrators to build Itanium systems.
By 2015 only HP supplied Itanium-based systems.
Chipsets.
The Itanium bus interfaces to the rest of the system via a chipset. Enterprise server manufacturers differentiate their systems by designing and developing chipsets that interface the processor to memory, interconnections, and peripheral controllers. The chipset is the heart of the system-level architecture for each system design. Development of a chipset costs tens of millions of dollars and represents a major commitment to the use of the Itanium. IBM created a chipset in 2003, and Intel in 2002, but neither of them developed chipsets to support newer technologies such as DDR2 or PCI Express.
Currently chipsets for Itanium supporting such technologies are manufactured by HP, Fujitsu, SGI, NEC, and Hitachi.
The "Tukwila" Itanium processor model had been designed to share a common chipset with the Intel Xeon processor EX (Intel's Xeon processor designed for four processor and larger servers). The goal was to streamline system development and reduce costs for server OEMs, many of which develop both Itanium- and Xeon-based servers. However, in 2013 this goal was pushed back to "evaluated for future implementation opportunities".
Software support.
Itanium is or was supported by the following operating systems:
Microsoft announced that Windows Server 2008 R2 would be the last version of Windows Server to support the Itanium, and that it would also discontinue development of the Itanium versions of Visual Studio and SQL Server.
Likewise, Red Hat Enterprise Linux 5 (first released in March 2007) was the last Itanium edition of Red Hat Enterprise Linux
and Canonical's Ubuntu 10.04 LTS (released in April 2010) was the last supported Ubuntu release on Itanium.
HP will not be supporting or certifying Linux on Itanium 9300 (Tukwila) servers.
In late September 2012, NEC announced a return from IA64 to the previous NOAH line of proprietary mainframe processors, now produced in a quad-core variant on 40 nm, called NOAH-6.
Oracle Corporation announced in March 2011, that it would drop development of application software for Itanium platforms, with the explanation that "Intel management made it clear that their strategic focus is on their x86 microprocessor and that Itanium was nearing the end of its life." However, a California state judge ruled that Oracle will have to continue supporting and releasing new versions of its software designed for Intel Itanium-based servers sold by Hewlett-Packard, after a settlement and release agreement between HP, Oracle and Mark Hurd had revealed that Oracle must continue to offer its product suite on HP's Itanium-based server platforms and does not confer on Oracle the discretion to decide whether to do so or not. Oracle's obligation to continue to offer its products on HP's Itanium-based server platforms lasts until such time as HP discontinues the sales of its Itanium-based servers. Oracle was ordered to port its products to HP's Itanium-based servers without charge to HP.
HP sells a virtualization technology for Itanium called Integrity Virtual Machines.
To allow more software to run on the Itanium, Intel supported the development of compilers optimized for the platform, especially its own suite of compilers.
Starting in November 2010, with the introduction of new product suites, the Intel Itanium Compilers were no longer bundled with the Intel x86 compilers in a single product. Intel offers Itanium tools and Intel x86 tools, including compilers, independently in different product bundles.
GCC,
Open64 and Microsoft Visual Studio 2005 (and later)
are also able to produce machine code for Itanium. According to the Itanium Solutions Alliance over 13,000 applications were available for Itanium-based systems in early 2008,
though Sun has contested Itanium application counts in the past.
The ISA also supported Gelato, an Itanium HPC user group and developer community that ported and supported open source software for Itanium.
Emulation.
Emulation is a technique that allows a computer to execute binary code that was compiled for a different type of computer. Before IBM's acquisition of QuickTransit in 2009, application binary software for IRIX/MIPS and Solaris/SPARC could run via type of emulation called "dynamic binary translation" on Linux/Itanium. Similarly, HP implemented a method to execute PA-RISC/HP-UX on the Itanium/HP-UX via emulation, to simplify migration of its PA-RISC customers to the radically different Itanium instruction set. Itanium processors can also run the mainframe environment GCOS from Groupe Bull and several x86 operating systems via instruction set simulators.
Competition.
Itanium is aimed at the enterprise server and high-performance computing (HPC) markets. Other enterprise- and HPC-focused processor lines include Oracle Corporation's SPARC M7, Fujitsu's SPARC64 X+ and IBM's POWER8. Measured by quantity sold, Itanium's most serious competition comes from x86-64 processors including Intel's own Xeon line and AMD's Opteron line. Since 2009, most servers were being shipped with x86-64 processors.
In 2005, Itanium systems accounted for about 14% of HPC systems revenue, but the percentage has declined as the industry shifts to x86-64 clusters for this application.
An October 2008 paper by Gartner, on the Tukwila processor stated that "...the future roadmap for Itanium looks as strong as that of any RISC peer like Power or SPARC."
Supercomputers and high-performance computing.
An Itanium-based computer first appeared on the list of the TOP500 supercomputers in November 2001. The best position ever achieved by an "Itanium 2" based system in the list was #2, achieved in June 2004, when Thunder (LLNL) entered the list with an Rmax of 19.94 Teraflops. In November 2004, Columbia entered the list at #2 with 51.8 Teraflops, and there was at least one Itanium-based computer in the top 10 from then until June 2007. The peak number of Itanium-based machines on the list occurred in the November 2004 list, at 84 systems (16.8%); by June 2012, this had dropped to one system (0.2%), and no Itanium system remained on the list in November 2012.
Processors.
Released processors.
The Itanium processors show a progression in capability. Merced was a proof of concept. McKinley dramatically improved the memory hierarchy and allowed Itanium to become reasonably competitive. Madison, with the shift to a 130 nm process, allowed for enough cache space to overcome the major performance bottlenecks. Montecito, with a 90 nm process, allowed for a dual-core implementation and a major improvement in performance per watt. Montvale added three new features: core-level lockstep, demand-based switching and front-side bus frequency of up to 667 MHz.
Future processors.
During the HP vs. Oracle support lawsuit, court documents unsealed by Santa Clara County Court judge revealed in 2008, Hewlett-Packard had paid Intel Corp. around $440 million to keep producing and updating Itanium microprocessors from 2009 to 2014. In 2010, the two companies signed another $250 million deal, which obliged Intel to continue making Itanium central processing units for HP's machines until 2017. Under the terms of the agreements, HP has to pay for chips it gets from Intel, while Intel launches Tukwila, Poulson, Kittson and Kittson+ chips in a bid to gradually boost performance of the platform.
Kittson.
"Kittson" is planned to follow Poulson. Kittson, like Poulson, will be manufactured using Intel's 32 nm process. Few other details are known beyond the existence of the codename and the binary and socket compatibility with Poulson and Tukwila, though moving to a common socket with x86 Xeon "will be evaluated for future implementation opportunities" after Kittson.

</doc>
<doc id="15459" url="https://en.wikipedia.org/wiki?curid=15459" title="International Statistical Classification of Diseases and Related Health Problems">
International Statistical Classification of Diseases and Related Health Problems

The International Statistical Classification of Diseases and Related Health Problems, usually called by the short-form name International Classification of Diseases (ICD), is the international "standard diagnostic tool for epidemiology, health management and clinical purposes". The ICD is maintained by the World Health Organization (WHO), the directing and coordinating authority for health within the United Nations System. The ICD is designed as a health care classification system, providing a system of diagnostic codes for classifying diseases, including nuanced classifications of a wide variety of signs, symptoms, abnormal findings, complaints, social circumstances, and external causes of injury or disease. This system is designed to map health conditions to corresponding generic categories together with specific variations, assigning for these a designated code, up to six characters long. Thus, major categories are designed to include a set of similar diseases.
The ICD is published by the WHO and used worldwide for morbidity and mortality statistics, reimbursement systems, and automated decision support in health care. This system is designed to promote international comparability in the collection, processing, classification, and presentation of these statistics. As in the case of the analogous (but limited to mental and behavioral disorders) "Diagnostic and Statistical Manual of Mental Disorders" (DSM, currently in version 5), the ICD is a major project to statistically classify health disorders, and provide diagnostic assistance. The ICD is a core statistically based classificatory diagnostic system for health care related issues of the WHO Family of International Classifications (WHO-FIC).
The ICD is revised periodically and is currently in its tenth revision. The ICD-10, as it is therefore known, was developed in 1992 to track health statistics. ICD-11 was planned for 2017, but has been pushed back to 2018. , development plans included using Web 2.0 principles to support detailed revision. Annual minor updates and triennial major updates are published by the WHO. The ICD is part of a "family" of guides that can be used to complement each other, including also the International Classification of Functioning, Disability and Health which focuses on the domains of functioning (disability) associated with health conditions, from both medical and social perspectives.
Historical synopsis.
In 1860, during the international statistical congress held in London, Florence Nightingale made a proposal that was to result in the development of the first model of systemic collection of hospital data. 
In 1893, a French physician, Jacques Bertillon, introduced the "Bertillon Classification of Causes of Death" at a congress of the International Statistical Institute in Chicago. A number of countries and cities adopted Bertillon's system, which was based on the principle of
distinguishing between general diseases and those localized to a particular organ or anatomical site, as used by the City of Paris for classifying deaths. Subsequent revisions represented a synthesis of English, German, and Swiss classifications, expanding from the original 44 titles to 161 titles. In 1898, the American Public Health Association (APHA) recommended that the registrars of Canada, Mexico, and the United States also adopt it. The APHA also recommended revising the system every ten years to ensure the system remained current with medical practice advances. As a result, the first international conference to revise the International Classification of Causes of Death took place in 1900, with revisions occurring every ten years thereafter. At that time, the classification system was contained in one book, which included an Alphabetic Index as well as a Tabular List. The book was small compared with current coding texts.
The revisions that followed contained minor changes, until the sixth revision of the classification system. With the sixth revision, the classification system expanded to two volumes. The sixth revision included morbidity and mortality conditions, and its title was modified to reflect the changes: International Statistical Classification of Diseases, Injuries and Causes of Death (ICD). Prior to the sixth revision, responsibility for ICD revisions fell to the Mixed Commission, a group composed of representatives from the International Statistical Institute and the Health Organization of the League of Nations. In 1948, the WHO assumed responsibility for preparing and publishing the revisions to the ICD every ten years. WHO sponsored the seventh and eighth revisions in 1957 and 1968, respectively. It later become clear that the established ten year interval between revisions was too short.
The ICD is currently the most widely used statistical classification system for diseases in the world. International health statistics using this system are available at the Global Health Observatory (GHO).
In addition, some countries—including Australia, Canada, and the United States—have developed their own adaptations of ICD, with more procedure codes for classification of operative or diagnostic procedures.
Versions of ICD.
ICD-6.
The ICD-6, published in 1949, was the first to be shaped to become suitable for morbidity reporting. Accordingly, the name changed from International List of Causes of Death to International Statistical Classification of Diseases. The combined code section for injuries and their associated accidents was split into two, a chapter for injuries, and a chapter for their external causes. With use for morbidity there was a need for coding mental conditions, and for the first time a section on mental disorders was added.
ICD-7.
The International Conference for the Seventh Revision of the International Classification of Diseases was held in Paris under the auspices of WHO in February 1955. In accordance with a recommendation of the WHO Expert Committee on Health Statistics, this revision was limited to essential changes and amendments of errors and inconsistencies.
ICD-8a.
The Eighth Revision Conference convened by WHO met in Geneva, from 6 to 12 July 1965. This revision was more radical than the Seventh but left unchanged the basic structure of the Classification and the general philosophy of classifying diseases, whenever possible, according to their etiology rather than a particular manifestation.
During the years that the Seventh and Eighth Revisions of the ICD were in force, the use of the ICD for indexing hospital medical records increased rapidly and some countries prepared national adaptations which provided the additional detail needed for this application of the ICD. 
In the USA, a group of consultants was asked to study the 8th revision of ICD (ICD-8a) for its applicability to various users in the United States. This group recommended that further detail be provided for coding hospital and morbidity data. The American Hospital Association's "Advisory Committee to the Central Office on ICDA" developed the needed adaptation proposals, resulting in the publication of the International Classification of Diseases, Adapted (ICDA). In 1968, the United States Public Health Service published the International Classification of Diseases, Adapted, 8th Revision for use in the United States (ICDA-8a). Beginning in 1968, ICDA-8a served as the basis for coding diagnostic data for both official morbidity mortality statistics in the United States.
ICD-9.
The International Conference for the Ninth Revision of the International Classification of Diseases, convened by WHO, met in Geneva from 30 September to 6 October 1975. In the discussions leading up to the conference, it had originally been intended that there should be little change other than updating of the classification. This was mainly because of the expense of adapting data processing systems each time the classification was revised.
There had been an enormous growth of interest in the ICD and ways had to be found of responding to this, partly by modifying the classification itself and partly by introducing special coding provisions. A number of representations were made by specialist bodies which had become interested in using the ICD for their own statistics. Some subject areas in the classification were regarded as inappropriately arranged and there was considerable pressure for more detail and for adaptation of the classification to make it more relevant for the evaluation of medical care, by classifying conditions to the chapters concerned with the part of the body affected rather than to those dealing with the underlying generalized disease.
At the other end of the scale, there were representations from countries and areas where a detailed and sophisticated classification was irrelevant, but which nevertheless needed a classification based on the ICD in order to assess their progress in health care and in the control of disease. A field test with a bi-axial classification approach—one axis (criterion) for anatomy, with another for etiology—showed the impracticability of such approach for routine use.
The final proposals presented to and accepted by the Conference in 1978 retained the basic structure of the ICD, although with much additional detail at the level of the four digit subcategories, and some optional five digit subdivisions. For the benefit of users not requiring such detail, care was taken to ensure that the categories at the three digit level were appropriate.
For the benefit of users wishing to produce statistics and indexes oriented towards medical care, the Ninth Revision included an optional alternative method of classifying diagnostic statements, including information about both an underlying general disease and a manifestation in a particular organ or site. This system became known as the dagger and asterisk system and is retained in the Tenth Revision. A number of other technical innovations were included in the Ninth Revision, aimed at increasing its flexibility for use in a variety of situations.
It was eventually replaced by ICD-10, the version currently in use by the WHO and most countries. Given the widespread expansion in the tenth revision, it is not possible to convert ICD-9 data sets directly into ICD-10 data sets, although some tools are available to help guide users.
Publication of ICD-9 without IP restrictions in a world with evolving electronic data systems led to a range of products based on ICD-9, such as MeDRA or the Read directory.
ICPM.
When ICD-9 was published by the World Health Organization (WHO), the International Classification of Procedures in Medicine (ICPM) was also developed (1975) and published (1978). The ICPM surgical procedures fascicle was originally created by the United States, based on its adaptations of ICD (called ICDA), which had contained a procedure classification since 1962. ICPM is published separately from the ICD disease classification as a series of supplementary documents called fascicles (bundles or groups of items). Each fascicle contains a classification of modes of laboratory, radiology, surgery, therapy, and other diagnostic procedures. Many countries have adapted and translated the ICPM in parts or as a whole and are using it with amendments since then.
ICD-9-CM.
"International Classification of Diseases, Clinical Modification" (ICD-9-CM) is an adaption created by the U.S. National Center for Health Statistics (NCHS) and used in assigning diagnostic and procedure codes associated with inpatient, outpatient, and physician office utilization in the United States. The ICD-9-CM is based on the ICD-9 but provides for additional morbidity detail. It is updated annually on October 1.
It consists of two or three volumes: 
The NCHS and the Centers for Medicare and Medicaid Services are the U.S. governmental agencies responsible for overseeing all changes and modifications to the ICD-9-CM.
ICD-10.
Work on ICD-10 began in 1983, and the new revision was endorsed by the Forty-third World Health Assembly in May 1990. The latest version came into use in WHO Member States starting in 1994. The classification system allows more than 155,000 different codes and permits tracking of many new diagnoses and procedures, a significant expansion on the 17,000 codes available in ICD-9.
Adoption was relatively swift in most of the world. Several materials are made available online by WHO to facilitate its use, including a manual, training guidelines, a browser, and files for download. Some countries have adapted the international standard, such as the "ICD-10-AM" published in Australia in 1998 (also used in New Zealand), and the "ICD-10-CA" introduced in Canada in 2000.
ICD-10-CM.
Adoption of ICD-10-CM was slow in the United States. Since 1979, the USA had required ICD-9-CM codes for Medicare and Medicaid claims, and most of the rest of the American medical industry followed suit.
On 1 January 1999 the ICD-10 (without clinical extensions) was adopted for reporting mortality, but ICD-9-CM was still used for morbidity. Meanwhile, NCHS received permission from the WHO to create a clinical modification of the ICD-10, and has production of all these systems:
On August 21, 2008, the US Department of Health and Human Services (HHS) proposed new code sets to be used for reporting diagnoses and procedures on health care transactions. Under the proposal, the ICD-9-CM code sets would be replaced with the ICD-10-CM code sets, effective October 1, 2013. On April 17, 2012 the Department of Health and Human Services (HHS) published a proposed rule that would delay, from October 1, 2013 to October 1, 2014,the compliance date for the ICD-10-CM and PCS. Once again, Congress delayed implementation date to October 1, 2015, after it was inserted into "Doc Fix" Bill without debate over objections of many.
Revisions to ICD-10-CM Include:
ICD-10-CA.
ICD-10-CA is a clinical modification of ICD-10 developed by the Canadian Institute for Health Information for morbidity classification in Canada. ICD-10-CA applies beyond acute hospital care, and includes conditions and situations that are not diseases but represent risk factors to health, such as occupational and environmental factors, lifestyle and psycho-social circumstances.
ICD-11.
The World Health Organization is currently revising the International Classification of Diseases (ICD) towards the ICD-11. The development is taking place on an internet-based workspace, called iCAT (Collaborative Authoring Tool) Platform, somewhat similar to a wiki – yet it requires more structure and peer review process. The WHO collaborates through this platform with all interested parties.
The final draft of the ICD-11 system is expected to be submitted to WHO's World Health Assembly (WHA) for official endorsement by 2017. The draft review was completed in April 2015 A final version for approval at the WHA is expected in 2018.
In ICD-11 each disease entity will have definitions that give key descriptions and guidance on what the meaning of the entity/category is in human readable terms - to guide users. This is an advancement over ICD-10, which had only title headings. The Definitions have a standard structure according to a template with standard definition templates and further features exemplified in a “Content Model”. The Content Model is a structured framework that captures the knowledge that underpins the definition of an ICD entity. The Content Model therefore allows computerization (with links to ontologies and SNOMED CT). Each ICD entity can be seen from different dimensions or “parameters”. For example, there are currently 13 defined main parameters in the Content Model (see below) to describe a category in ICD. 
ICD exists in 41 Languages in electronic versions and its expression in multiple languages will be systematically pursued in ICD11.
Usage and current topics.
History and usage in the United States.
In the United States, the U.S. Public Health Service published "The International Classification of Diseases, Adapted for Indexing of Hospital Records and Operation Classification (ICDA)," completed in 1962 and expanding the ICD-7 in a number of areas to more completely meet the indexing needs of hospitals. The U.S. Public Health Service later published the "Eighth Revision, International Classification of Diseases, Adapted for Use in the United States," commonly referred to as ICDA-8, for official national morbidity and mortality statistics. This was followed by the "ICD, 9th Revision, Clinical Modification", known as ICD-9-CM, published by the U.S. Department of Health and Human Services and used by hospitals and other healthcare facilities to better describe the clinical picture of the patient. The diagnosis component of ICD-9-CM is completely consistent with ICD-9 codes, and remains the data standard for reporting morbidity. National adaptations of the ICD-10 progressed to incorporate both clinical code (ICD-10-CM) and procedure code (ICD-10-PCS) with the revisions completed in 2003. In 2009, the U.S. Centers for Medicare and Medicaid Services announced that it would begin using ICD-10 on April 1, 2010, with full compliance by all involved parties by 2013.
The years for which causes of death in the United States have been classified by each revision as follows:
Mental and behavioral disorders.
The ICD includes a section classifying mental and behavioral disorders (Chapter V). This has developed alongside the American Psychiatric Association's "Diagnostic and Statistical Manual of Mental Disorders" (DSM) and the two manuals seek to use the same codes. There are significant differences, however, such as the ICD including personality disorders in the same way as other mental disorders. The WHO is revising their classifications in these sections as part the development of the ICD-11 (scheduled for 2017), and an "International Advisory Group" has been established to guide this. An international survey of psychiatrists in 66 countries comparing use of the ICD-10 and DSM-IV found that the former was more often used for clinical diagnosis while the latter was more valued for research. The ICD is actually the official system for the US, although many mental health professionals do not realize this due to the dominance of the DSM. A psychologist has stated: "Serious problems with the clinical utility of both the ICD and the DSM are widely acknowledged."
External links.
Note: since adoption of ICD-10 CM in the USA, several online tools have been mushrooming. They all refer to that particular modification and thus are not linked here.

</doc>
<doc id="15462" url="https://en.wikipedia.org/wiki?curid=15462" title="Integral domain">
Integral domain

In mathematics, and specifically in abstract algebra, an integral domain is a nonzero commutative ring in which the product of any two nonzero elements is nonzero. Integral domains are generalizations of the ring of integers and provide a natural setting for studying divisibility.
In an integral domain the cancellation property holds for multiplication by a nonzero element "a", that is, if , an equality implies .
"Integral domain" is defined almost universally as above, but there is some variation. This article follows the convention that rings have a multiplicative identity, generally denoted 1, but some authors do not follow this, by not requiring integral domains to have a multiplicative identity. Noncommutative integral domains are sometimes admitted. This article, however, follows the much more usual convention of reserving the term "integral domain" for the commutative case and using "domain" for the general case including noncommutative rings.
Some sources, notably Lang, use the term entire ring for integral domain.
Some specific kinds of integral domains are given with the following chain of class inclusions:
Definitions.
There are a number of equivalent definitions of integral domain:
Non-examples.
The following rings are "not" integral domains.
Divisibility, prime elements, and irreducible elements.
In this section, "R" is an integral domain.
Given elements "a" and "b" of "R", we say that "a" divides "b", or that "a" is a divisor of "b", or that "b" is a multiple of "a", if there exists an element "x" in "R" such that "ax" = "b".
The elements that divide 1 are called the units of "R"; these are precisely the invertible elements in "R". Units divide all other elements.
If "a" divides "b" and "b" divides "a", then we say "a" and "b" are associated elements or associates. Equivalently, "a" and "b" are associates if "a"="ub" for some unit "u".
If "q" is a nonzero non-unit, we say that "q" is an irreducible element if "q" cannot be written as a product of two non-units.
If "p" is a nonzero non-unit, we say that "p" is a prime element if, whenever "p" divides a product "ab", then "p" divides "a" or "p" divides "b". Equivalently, an element "p" is prime if and only if the principal ideal ("p") is a nonzero prime ideal. The notion of prime element generalizes the ordinary definition of prime number in the ring Z, except that it allows for negative prime elements.
Every prime element is irreducible. The converse is not true in general: for example, in the quadratic integer ring formula_7 the element 3 is irreducible (if it factored nontrivially, the factors would each have to have norm 3, but there are no norm 3 elements since formula_8 has no integer solutions), but not prime (since 3 divides formula_9 without dividing either factor). In a unique factorization domain (or more generally, a GCD domain), an irreducible element is a prime element.
While unique factorization does not hold in formula_7, there is unique factorization of ideals. See Lasker–Noether theorem.
Field of fractions.
The field of fractions "K" of an integral domain "R" is the set of fractions "a"/"b" with "a" and "b" in "R" and "b" ≠ 0 modulo an appropriate equivalence relation, equipped with the usual addition and multiplication operations. It is "the smallest field containing "R"" in the sense that there is an injective ring homomorphism such that any injective ring homomorphism from "R" to a field factors through "K".
The field of fractions of the ring of integers Z is the field of rational numbers Q. The field of fractions of a field is isomorphic to the field itself.
Algebraic geometry.
Integral domains are characterized by the condition that they are reduced (that is "x"2 = 0 implies "x" = 0) and irreducible (that is there is only one minimal prime ideal). The former condition ensures that the nilradical of the ring is zero, so that the intersection of all the ring's minimal primes is zero. The latter condition is that the ring have only one minimal prime. It follows that the unique minimal prime ideal of a reduced and irreducible ring is the zero ideal, so such rings are integral domains. The converse is clear: an integral domain has no nonzero nilpotent elements, and the zero ideal is the unique minimal prime ideal.
This translates, in algebraic geometry, into the fact that the coordinate ring of an affine algebraic set is an integral domain if and only if the algebraic set is an algebraic variety.
More generally, a commutative ring is an integral domain if and only if its spectrum is an integral affine scheme.
Characteristic and homomorphisms.
The characteristic of an integral domain is either 0 or a prime number.
If "R" is an integral domain of prime characteristic "p", then the Frobenius endomorphism "f"("x") = "x" "p" is injective.

</doc>
<doc id="15466" url="https://en.wikipedia.org/wiki?curid=15466" title="Infundibulum">
Infundibulum

An infundibulum (Latin for "funnel"; plural, "infundibula") is a funnel-shaped cavity or organ.

</doc>
<doc id="15467" url="https://en.wikipedia.org/wiki?curid=15467" title="Interrupt latency">
Interrupt latency

In computing, interrupt latency is the time that elapses from when an interrupt is generated to when the source of the interrupt is serviced. For many operating systems, devices are serviced as soon as the device's interrupt handler is executed. Interrupt latency may be affected by microprocessor design, interrupt controllers, interrupt masking, and the operating system's (OS) interrupt handling methods.
Background.
There is usually a trade-off between interrupt latency, throughput, and processor utilization. Many of the techniques of CPU and OS design that improve interrupt latency will decrease throughout and increase processor utilization. Techniques that increase throughput may increase interrupt latency and increase processor utilization. Lastly, trying to reduce processor utilization may increase interrupt latency and decrease throughput.
Minimum interrupt latency is largely determined by the interrupt controller circuit and its configuration. They can also affect the jitter in the interrupt latency, which can drastically affect the real-time schedulability of the system. The Intel APIC Architecture is well known for producing a huge amount of interrupt latency jitter.
Maximum interrupt latency is largely determined by the methods an OS uses for interrupt handling. For example, most processors allow programs to disable interrupts, putting off the execution of interrupt handlers, in order to protect critical sections of code. During the execution of such a critical section, all interrupt handlers that cannot execute safely within a critical section are blocked (they save the minimum amount of information required to restart the interrupt handler after all critical sections have exited). So the interrupt latency for a blocked interrupt is extended to the end of the critical section, plus any interrupts with equal and higher priority that arrived while the block was in place.
Many computer systems require low interrupt latencies, especially embedded systems that need to control machinery in real-time. Sometimes these systems use a real-time operating system (RTOS). An RTOS makes the promise that no more than a specified maximum amount of time will pass between executions of subroutines. In order to do this, the RTOS must also guarantee that interrupt latency will never exceed a predefined maximum.
Considerations.
There are many methods that hardware may use to increase the interrupt latency that can be tolerated. These include buffers, and flow control. For example, most network cards implement transmit and receive ring buffers, interrupt rate limiting, and hardware flow control. Buffers allow data to be stored until it can be transferred, and flow control allows the network card to pause communications without having to discard data if the buffer is full.
Modern hardware also implements interrupt rate limiting. This helps prevent interrupt storms or "live lock" by having the hardware wait a programmable minimum amount of time between each interrupt it generates. Interrupt rate limiting reduces the amount of time spent servicing interrupts, allowing the processor to spend more time doing useful work. Exceeding this time results in a soft (recoverable) or hard (non-recoverable) error.

</doc>
<doc id="15468" url="https://en.wikipedia.org/wiki?curid=15468" title="İskender kebap">
İskender kebap

İskender kebap is one of the most famous meat foods of northwestern Turkey and takes its name from its inventor, İskender Efendi, who lived in Bursa in the late 19th century.
It is a kind of döner kebab prepared from thinly cut grilled lamb basted with hot tomato sauce over pieces of pita bread and generously slathered with melted sheep butter and yogurt. Additionally, one cylindrical köfte can be placed on top. It is commonly consumed with şıra as a drink to aid digestion. Tomato sauce and melted butter are generally poured over the dish, at the table.
"Kebapçı İskender" is trademarked by Yavuz İskenderoğlu, whose family still runs the restaurant in Bursa. This dish is available in many restaurants throughout the country mostly under the name "İskender kebap", "Bursa kebabı", or at times with an alternative one made up by the serving restaurant such as "Uludağ kebabı".
Differences from döner kebab.
A serving of İskender kebap contains thin and wide strips of meat, on the other hand döner kebap has smaller pieces of meat. İskender kebap is prepared by stacking large pieces of meat vertically, which is why the meat strips are larger. In lower quality restaurants, ground meat is used, which is not suitable for İskender kebap. The fat content of Iskender meat is lower than that of döner meat. İskender kebap is served with yogurt and sauce, while döner is not.

</doc>
<doc id="15471" url="https://en.wikipedia.org/wiki?curid=15471" title="LGBT in Islam">
LGBT in Islam

LGBT and Islam is influenced by the religious, legal and cultural history of the nations with a sizable Muslim population, along with specific passages in the Quran and statements attributed to the Islamic prophet Muhammad (hadith). Hadiths traditionally are not interpreted because their language is understood to be simple matter-of-fact language. Orthodox Islam is not only a system of beliefs, but also a legal system.
The traditional schools of Islamic law based on Quranic verses and hadith consider homosexual acts a punishable crime and a sin, and influenced by Islamic scholars such as Imam Malik and Imam Shafi. The Qur'an cites the story of the "people of Lot" destroyed by the wrath of God because they engaged in "lustful" carnal acts between men. Nevertheless, homoerotic themes were present in poetry and other literature written by some Muslims from the medieval period onwards and sometimes homoeroticism in the form of pederasty was seen in a positive way.
Extreme prejudice remains, both socially and legally, in much of the Islamic world against people who engage in homosexual acts. In Afghanistan, Brunei, Iran, Mauritania, Nigeria, Saudi Arabia, Sudan, United Arab Emirates and Yemen, homosexual activity carries the death penalty. In others, such as Somalia and Malaysia, it is illegal. Same-sex sexual activity is legal in 20 Muslim-majority nations (Albania, Azerbaijan, Bahrain, Bosnia and Herzegovina, Burkina Faso, Chad, Djibouti, Guinea-Bissau, Lebanon, Iraq, Jordan, Kazakhstan, Kosovo, Kyrgyzstan, Mali, Niger, Tajikistan, Turkey, West Bank (State of Palestine), and most of Indonesia, as well as Northern Cyprus). In Albania, Lebanon, and Turkey, there have been discussions about legalizing same-sex marriage.
Most Muslim-majority countries have opposed moves to advance LGBT rights at the United Nations, in the General Assembly and/or the UNHRC. However, Albania, Guinea-Bissau and Sierra Leone have signed a UN Declaration supporting LGBT rights. Albania provide LGBT rights protections in law in the form of non-discrimination laws, and discussions on legally recognizing same-sex marriage have been held in the country. Kosovo as well as the (internationally not recognized) Muslim-majority Turkish Republic of Northern Cyprus also have anti-discrimination laws in place.
Scripture and Islamic jurisprudence.
The Quran.
The Quran contains seven references to fate of "the people of Lut", and their destruction by Allah is associated explicitly with their sexual practices:
The sins of the people of Lut became proverbial, and the Arabic words for homosexual behaviour ("liwāṭ") and for a person who performs such acts ("lūṭi") both derive from his name. The story of Lut is used to demonstrate how homosexuality is based in non-consent between the men. However, some scholars of Islam argue that the foundation of homosexual discourse in Islam cannot be rooted in consent because there are many more instances in the Quran in which there is no consent between partners. With this, sexual acts between men and youth are not considered transgressive.
Only one passage in the Qur'an prescribes a strictly legal position. It is not restricted to homosexual behaviour, however, and deals more generally with public practice of adultery:
Because the Quran is also a legal document, there are several major sins outlined in the text. Two of these consider sexual misconduct. They are "Zina" and "Liwat". Zina literally means "adultery". It is "sex between a man and a woman who is neither his wife nor his slave—the most serious of sexual transgressions described in the Qur'an". Liwat is "anal intercourse between men or anal sex between a male and a female 'stranger'—that is, a woman who is neither his wife nor his slave over whom he has no sexual rights". The issue of homosexuality comes more from a standpoint of legal sexual rights.
According to the laws of Shariah, Muslims found guilty of homosexual acts should repent rather than confess. This means that many Muslim countries tolerate same-sex acts so long as they happen in private and do not challenge the existing dominant family and social order. Many Muslim scholars have followed this idea of a "don't ask, don't tell" policy in regards to homosexuality in Islam, by treating the subject with passivity. Comparisons have been made between the imperative nature of the secrecy of homosexual acts and the secrecy of women in many Islamic societies. In other words, women have to live under a certain amount of secrecy (whether that means being veiled or otherwise), and homosexuals must keep all of their transgressions and acts a secret.
There were varying opinions on how the death penalty was to be carried out. Abu Bakr apparently recommended toppling a wall on the evil-doer, or else burning alive, while Ali bin Abi Talib ordered death by stoning for one "luti" and had another thrown head-first from the top of a minaret—according to Ibn Abbas, this last punishment must be followed by stoning.
The Hadith and Seerah.
The hadith (sayings and actions of Muhammad) show that homosexuality was not unknown in Arabia. Given that the Qur'an is allegedly vague regarding the punishment of homosexual sodomy, Islamic jurists turned to the collections of the hadith and seerah (accounts of Muhammad's life) to support their argument for Hudud punishment.
Abu `Isa Muhammad ibn `Isa at-Tirmidhi compiling the Sunan al-Tirmidhi around C.E.884 (two centuries after the death of Muhammad) wrote that Muhammad had prescribed the death penalty for both the active and the passive partner: "Whoever you find committing the sin of the people of Lut (Lot), kill them, both the one who does it and the one to whom it is done." The overall moral or theological principle is that a person who performs such actions ("luti") challenges the harmony of God's creation, and is therefore a revolt against God.
Ibn al-Jawzi (1114-1200) writing in the 12th century claimed that Muhammad had cursed "sodomites" in several hadith, and had recommended the death penalty for both the active and passive partners in homosexual acts.
Al-Nuwayri (1272-1332) in his "Nihaya" reports that Muhammad is alleged to have said what he feared most for his community were the practices of the people of Lot (although he seems to have expressed the same idea in regard to wine and female seduction).
Later medieval jurisprudence.
The four schools of shari'a (Islamic law) disagreed on what punishment is appropriate for "liwat". Abu Bakr Al-Jassas (d. 981 AD/370 AH) argued that the two hadiths on killing homosexuals "are not reliable by any means and no legal punishment can be prescribed based on them", and the Hanafi school held that it does not merit any capital punishment, on the basis of a hadith that "Muslim blood can only be spilled for adultery, apostasy and homicide"; against this the Hanbali school inferred that sodomy is a form of adultery and must incur the same penalty, i.e. death.
Modern legal views.
With few exceptions all scholars of Sharia, or Islamic law, interpret homosexual activity as a punishable offence as well as a sin. There is no specific punishment prescribed, however, and this is usually left to the discretion of the local authorities on Islam. Mohamed El-Moctar El-Shinqiti, a contemporary Mauritanian scholar, has argued that "though homosexuality is a grievous sin...no legal punishment is stated in the Qur'an for homosexuality...[b it is not reported that Prophet Muhammad has punished somebody for committing homosexuality... there is no authentic hadith reported from the Prophet prescribing a punishment for the homosexuals..." Hadith scholars such as Al-Bukhari, Yahya ibn Ma'in, Al-Nasa'i, Ibn Hazm, Al-Tirmidhi, and others have impugned these statements.
Faisal Kutty, a professor of Islamic law at Indiana-based Valparaiso University Law School and Toronto-based Osgoode Hall Law School, commented on the contemporary same-sex marriage debate in a March 27, 2014 essay in the Huffington Post. He acknowledged that while Islamic law iterations prohibits pre- and extra-marital as well as same-sex sexual activity, it does not attempt to "regulate feelings, emotions and urges, but only its translation into action that authorities had declared unlawful". Kutty, who teaches comparative law and legal reasoning, also wrote that many Islamic scholars have "even argued that homosexual tendencies themselves were not haram but had to be suppressed for the public good". He claimed that this may not be "what the LGBTQ community wants to hear", but that, "it reveals that even classical Islamic jurists struggled with this issue and had a more sophisticated attitude than many contemporary Muslims". Kutty, who in the past wrote in support of allowing Islamic principles in dispute resolution, also noted that "most Muslims have no problem extending full human rights to those—even Muslims—who live together 'in sin'". He argued that it therefore seems hypocritical to deny fundamental rights to same-sex couples. Moreover, he concurred with Islamic legal scholar Mohamed Fadel in arguing that this is not about changing Islamic marriage (nikah), but about making "sure that all citizens have access to the same kinds of public benefits".
Islamist journalist Muhammad Jalal Kishk found no prescribed punishment for homosexuality in Islamic law Several modern day scholars, including Scott Kugle, argue for a different interpretation of the Lot narrative focusing not on the sexual act but on the infidelity of the tribe and their rejection of Lot's Prophethood.
There are several methods by which sharia jurists have advocated the punishment of gays or lesbians who are sexually active. One form of execution involves an individual convicted of homosexual acts being stoned to death by a crowd of Muslims. Other Muslim jurists have established ijma ruling that those committing homosexual acts be thrown from rooftops or high places, and this is the perspective of most Salafists.
History of homosexuality in Islamic societies.
Medieval era.
The centuries immediately after Muhammad's death led to a rapid growth of the Islamic empire accompanied by increased prosperity. Some Muslims bemoaned the general "corruption" of morals in the two holy cities of Mecca and Medina, and it's clear that homosexual practice continued (in a subterranean manner) despite its growing condemnation by the religious authorities. In fact, it seems to have become less hidden as the process of acculturation sped up, such as in the area of music and dance where "mukhannathun" were prevalent. The arrival of the Abbasid army to Arabia in the 8th century seems to have meant that tolerance for homosexual practice subsequently spread even more widely under the new dynasty. The ruler Al-Amin (809-813), for example, was said to have required slave women to be dressed in masculine clothing so he could be persuaded to have sex and produce an heir.
Abu Nuwas (756-814), born in the city of Ahvaz in modern-day Iran, became a master of all the contemporary genres of Arabic poetry; sharing Al-Amin's love for men and composing poems celebrating such love.
There are other examples from the following centuries. The Aghlabid Emir, Ibrahim II of Ifriqiya (ruled 875–902), was said to have been surrounded by some sixty catamites, yet whom he was said to have treated in a most horrific manner. Caliph al-Mutasim in the 9th century and some of his successors were accused of homosexuality. The popular stories says that Cordoba, Abd al-Rahman III had executed a young man from León who was held as a hostage, because he had refused his advances during the Reconquista.
Mahmud of Ghazni (971-1030), the ruler of the Ghaznavid Empire, had a Turkish slave named Malik Ayaz as a companion. Their relationship inspired poems and stories.
Mehmed the Conqueror, the Ottoman sultan living in the 15th century, European sources say "who was known to have ambivalent sexual tastes, sent a eunuch to the house of Notaras, demanding that he supply his good looking fourteen year old son for the Sultan’s pleasure. When he refused, the Sultan instantly ordered the decapitation of Notaras, together with that of his son and his son-in-law; and their three heads … were placed on the banqueting table before him". Another youth Mehmed found attractive, and who was presumably more accommodating, was Radu III the Fair, the brother of the famous Vlad the Impaler, "Radu, a hostage in Istanbul whose good looks had caught the Sultan’s fancy, and who was thus singled out to serve as one of his most favored pages." After the defeat of Vlad, Mehmed placed Radu on the throne of Wallachia as a vassal ruler. However, Turkish sources deny these stories.
According to the "Encyclopedia of Islam and the Muslim World":
Whatever the legal strictures on sexual activity, the positive expression of male homeoerotic sentiment in literature was accepted, and assiduously cultivated, from the late eighth century until modern times. First in Arabic, but later also in Persian, Turkish and Urdu, love poetry by men about boys more than competed with that about women, it overwhelmed it. Anecdotal literature reinforces this impression of general societal acceptance of the public celebration of male-male love (which hostile Western caricatures of Islamic societies in medieval and early modern times simply exaggerate).
European travellers remarked on the taste that Shah Abbas of Iran (1588-1629) had for wine and festivities, but also for charming pages and cup bearers. A painting by Riza Abbasi with homo-erotic qualities shows the ruler enjoying such delights.
Modern era.
During the Ottoman Empire, homosexuality was decriminalized in 1858, as part of wider reforms during the Tanzimat.
Pederasty.
Despite the formal disapproval of religious authority, the segregation of women in Muslim societies and the strong emphasis on male virility leads adolescent males and unmarried young men to seek sexual outlets with boys younger than themselves—in one study in Morocco, with boys in the age-range 7 to 13. Men have sex with other males so long as they are the penetrators and their partners are boys, or in some cases effeminate men.
"Liwat" can therefore be regarded as "temptation", and anal intercourse is not seen as repulsively unnatural so much as dangerously attractive. They believe "one has to avoid getting buggered precisely in order not to acquire a taste for it and thus become addicted." Not all sodomy is homosexual: one Moroccan sociologist, in a study of sex education in his native country, notes that for many young men heterosexual sodomy is considered better than vaginal penetration, and female prostitutes likewise report the demand for anal penetration from their (male) clients.
It is not so much the penetration as the enjoyment that is considered bad. Deep shame attaches to the passive partner: "for this reason men stop getting laid at the age of 15 or 16 and 'forget' that they ever allowed it earlier." Similar sexual sociologies are reported for other Muslim societies from North Africa to Pakistan and the Far East. In Afghanistan in 2009, the British Army was forced to commission a report into the sexuality of the local men after British soldiers reported the discomfort at witnessing adult males involved in sexual relations with boys. The report stated that though illegal, there was a tradition of such relationships in the country, known as "bache bazi" or "boy play", and that it was especially strong around North Afghanistan.
Homosexuality laws in majority-Muslim countries.
Criminalized.
According to the International Lesbian and Gay Association (ILGA) seven countries still retain capital punishment for homosexual behavior: Saudi Arabia, Yemen, Iran, Afghanistan, Mauritania, Sudan, and northern Nigeria. In United Arab Emirates is a capital offense. In Qatar, Algeria, Uzbekistan, and the Maldives, homosexuality is punished with time in prison or a fine. This has led to controversy regarding Qatar, which is due to stage the 2022 FIFA World Cup. Human rights groups have questioned the awarding in 2010 of the right to host the competition, due to the possibility that gay football fans may be jailed. In response, Sepp Blatter, head of FIFA, joked that they would have to "refrain from sexual activity" while in Qatar. He later withdrew the remarks after condemnation from rights groups.
In Egypt, openly gay men have been prosecuted under general public morality laws. (See Cairo 52.) In Saudi Arabia, the maximum punishment for homosexual acts is public execution, which is often carried out. The government will sometimes use lesser punishments—for example, fines, time in prison, and whipping—as alternatives.
In India, which has the third largest Muslim population in the world, and where Muslims form a large minority, the largest Islamic seminary (Darul Uloom Deoband) has vehemently opposed recent government moves to abrogate and liberalize laws from the British Raj era that banned homosexuality.
Legal.
However, in 20 out of 57 Muslim-majority nations same-sex intercourse is not forbidden by law.
The Ottoman Empire (predecessor of Turkey) decriminalized homosexuality in 1858. In Turkey, where 99.8% of the population is Muslim, homosexuality has never been criminalized since the day it was founded in 1923. and LGBT people also have the right to seek asylum in Turkey under the Geneva Convention since 1951.
Same-sex sexual intercourse is legal in Albania, Azerbaijan, Bahrain, Bosnia and Herzegovina, Burkina Faso, Chad, Djibouti, Guinea-Bissau, Lebanon, Iraq, Kazakhstan, Kosovo, Kyrgyzstan, Mali, Niger, Tajikistan, Turkey, West Bank (State of Palestine), most of Indonesia, and in Northern Cyprus. In Albania, Lebanon, and Turkey, there have been discussions about legalizing same-sex marriage. Albania, Northern Cyprus and Kosovo also protect LGBT people with anti-discrimination laws.
Same-sex marriage.
In 2007 there was a gay party in the Moroccan town of al-Qasr al-Kabir. Rumours spread that this was a gay marriage and more than 600 people took to the streets, condemning the alleged event and protesting against leniency towards homosexuals. Several persons who attended the party were detained and eventually six Moroccan men were sentenced to between four and ten months in prison for "homosexuality".
In France there was an Islamic same-sex marriage on February 18, 2012. In Paris in November 2012 a room in a Buddhist prayer hall was used by gay Muslims and called a "gay-friendly mosque", and a French Islamic website is supporting religious same-sex marriage.
The first American Muslim in the United States Congress, Keith Ellison (D-MN) said in 2010 that all discrimination against LGBT people is wrong. He further expressed support for gay marriage stating:
I believe that the right to marry someone who you please is so fundamental it should not be subject to popular approval any more than we should vote on whether blacks should be allowed to sit in the front of the bus.
In 2014 eight men were jailed for three years by a Cairo court after the circulation of a video of them allegedly taking part in a private wedding ceremony between two men on a boat on the Nile.
Public opinion among Muslims.
In 2011, the UN Human Rights Council passed its first resolution recognizing LGBT rights, which was followed up with a report from the UN Human Rights Commission documenting violations of the rights of LGBT people. The two world maps of religions of the world and the countries that support LGBT rights at the UN give an impression of the attitude towards homosexuality on the part of many Muslim-majority governments.
The Muslim community as a whole, worldwide, has become polarized on the subject of homosexuality. There is somewhat of a consensus, though, that "individuals bear moral responsibility for any sexual acts that they engage in by free choice and that illicit desires themselves do not result in any culpability before God."
Opinion polls.
In 2013, the Pew Research Center conducted a study on the global acceptance of homosexuality and found a widespread rejection of homosexuality in many nations that are predominantly Muslim. In some countries, views were actually becoming more conservative among younger people.
A 2007 survey of British Muslims showed that 61% believe homosexuality should be illegal, with up to 71% of young British Muslims holding this belief. A later Gallup poll in 2009 showed that none of the 500 British Muslims polled believed homosexuality to be "morally acceptable". This compared with 35% of the 1001 French Muslims polled that did.
According to a 2012 poll, 51% of the Turks in Germany, who account for nearly two thirds of the total Muslim population in Germany, believe that homosexuality is an illness.
LGBT movements within Islam.
The Al-Fatiha Foundation was an organization which tried to advance the cause of gay, lesbian, and transgender Muslims. It was founded in 1998 by Faisal Alam, a Pakistani American, and was registered as a nonprofit organization in the United States. The organization was an offshoot of an internet listserve that brought together many gay, lesbian and questioning Muslims from various countries. The Foundation accepted and considered homosexuality as natural, either regarding Qur'anic verses as obsolete in the context of modern society, or stating that the Qu'ran speaks out against homosexual lust and is silent on homosexual love. After the Alam stepped down, subsequent leaders failed to sustain the organization and it began a process of legal dissolution in 2011.
In 2001, Al-Muhajiroun, a banned and now defunct international organization who sought the establishment of a global Islamic caliphate, issued a fatwa declaring that all members of Al-Fatiha were "murtadd", or apostates, and condemning them to death. Because of the threat and coming from conservative societies, many members of the foundation's site still prefer to be anonymous so as to protect their identity while continuing a tradition of secrecy. Al-Fatiha has fourteen chapters in the United States, as well as offices in England, Canada, Spain, Turkey, and South Africa. In addition, Imaan, a social support group for Muslim LGBT people and their families, exists in the UK. Both of these groups were founded by gay Pakistani activists. The UK also has the Safra Project for women.
Some Muslims, such as the lesbian writer Irshad Manji and academic author Scott Kugle, argue that Islam does not condemn homosexuality. He, as well as South Asian scholar and author Ruth Vanita and Muslim scholar and writer Saleem Kidwai, contend that ancient Islam has a rich history of homoerotic literature.
There are also a number of Islamic ex-gay (i.e. people claiming to have experienced a basic change in sexual orientation from exclusive homosexuality to exclusive heterosexuality) groups aimed at attempting to guide homosexuals towards heterosexuality. A large body of research and global scientific consensus indicates that being gay, lesbian, or bisexual is compatible with normal mental health and social adjustment. Because of this, major mental health professional organizations discourage and caution individuals against attempting to change their sexual orientation to heterosexual, and warn that attempting to do so can be harmful. People who have gone through conversion therapy face 8.9 times the rates of suicide ideation, face depression at 5.9 times the rate of their peers and are three times more likely to use illegal drugs compared to those who did not go through the therapy.
The religious conflicts and inner turmoil that Islamic homosexuals struggle over has been addressed in various media, such as the 2006 Channel 4 documentary "Gay Muslims", and the 2007 documentary film "A Jihad for Love". The latter was produced by Sandi Simcha DuBowski, who six years earlier made a Jewish-themed documentary on the same topic, titled "Trembling Before G-d".
In November 2012, a prayer room was set up in Paris by gay Islamic scholar and founder of the group 'Homosexual Muslims of France' Ludovic-Mohamed Zahed. It was described by the press as the first gay-friendly mosque in Europe, though traditional Islamic scholars disagree.
Gender variant and transgender people.
In Islam, the term mukhannathun is used to describe gender-variant people, usually male-to-female transgender. Neither this term nor the equivalent for "eunuch" occurs in the Qur'an, but the term does appear in the Hadith, the sayings of Muhammad, which have a secondary status to the central text. Moreover, within Islam, there is a tradition on the elaboration and refinement of extended religious doctrines through scholarship. This doctrine contains a passage by the scholar and hadith collector An-Nawawi:
While Iran has outlawed homosexuality, Iranian Shi'a thinkers such as Ayatollah Khomeini have allowed for transgender people to change their sex so that they can enter heterosexual relationships. This position has been confirmed by the Supreme Leader of Iran, Ayatollah Ali Khamenei, and is also supported by many other Iranian clerics.
Iran carries out more sex change operations than any other nation in the world except for Thailand. It is regarded as a cure for homosexuality, which is punishable by death under Iranian law. The government even provides up to half the cost for those needing financial assistance and a sex change is recognized on the birth certificate.

</doc>
<doc id="15474" url="https://en.wikipedia.org/wiki?curid=15474" title="Infanticide">
Infanticide

Infanticide (or infant homicide) is the intentional killing of infants or children. 
Parental infanticide researchers have found that mothers are far more likely than fathers to be the perpetrator for neonaticide and slightly more likely to commit infanticide in general.
In many past societies, certain forms of infanticide were considered permissible. In some countries, female infanticide is more common than the killing of male offspring, due to sex-selective infanticide. In China for example, the gender gap between males and females aged 0–19 year old was estimated to be 25 million in 2010 by the United Nations Population Fund.
In English law infanticide is established as a distinct offence by the Infanticide Acts. Defined as the killing of a child under 12 months of age by their mother, the effect of the Acts are to establish a partial defence to charges of murder.
History and pre-history.
The practice of infanticide has taken many forms. Child sacrifice to supernatural figures or forces, such as that believed to have been practiced in ancient Carthage, may be only the most notorious example in the ancient world. Anthropologist Laila Williamson notes that "Infanticide has been practiced on every continent and by people on every level of cultural complexity, from hunter gatherers to high civilizations, including our own ancestors. Rather than being an exception, then, it has been the rule."
A frequent method of infanticide in ancient Europe and Asia was simply to abandon the infant, leaving it to die by exposure (i.e. hypothermia, hunger, thirst, or animal attack). Infant abandonment still occurs in modern societies.
In at least one island in Oceania, infanticide was carried out until the 20th century by suffocating the infant, while in pre-Columbian Mesoamerica and in the Inca Empire it was carried out by sacrifice (see below).
Paleolithic and Neolithic.
Many Neolithic groups routinely resorted to infanticide in order to control their numbers so that their lands could support them. Joseph Birdsell believed that infanticide rates in prehistoric times were between 15% and 50% of the total number of births, while Laila Williamson estimated a lower rate ranging from 15% to 20%. Both anthropologists believed that these high rates of infanticide persisted until the development of agriculture during the Neolithic Revolution. Comparative anthropologists have calculated that 50% of female newborn babies were killed by their parents during the Paleolithic era. Decapitated skeletons of hominid children have been found with evidence of cannibalism. The children were not necessarily actively killed, but neglect and intentional malnourishment may also have occurred, as proposed by Vicente Lull as an explanation for an apparent surplus of men and the below average height of women in prehistoric Menorca.
In ancient history.
In the New World.
Archaeologists have uncovered physical evidence of child sacrifice at several locations. Some of the best attested examples are the diverse rites which were part of the religious practices in Mesoamerica and the Inca Empire.
In the Old World.
Three thousand bones of young children, with evidence of sacrificial rituals, have been found in Sardinia. Pelasgians offered a sacrifice of every tenth child during difficult times. Syrians sacrificed children to Jupiter and Juno. Many remains of children have been found in Gezer excavations with signs of sacrifice. Child skeletons with the marks of sacrifice have been found also in Egypt dating 950-720 BCE. In Carthage " sacrifice in the ancient world reached its infamous zenith." Besides the Carthaginians, other Phoenicians, and the Canaanites, Moabites and Sepharvites offered their first-born as a sacrifice to their gods.
Ancient Egypt.
In Egyptian households, at all social levels, children of both sexes were valued and there is no evidence of infanticide. The religion of the Ancient Egyptians forbade infanticide and during the Greco-Roman period they rescued abandoned babies from manure heaps, a common method of infanticide by Greeks or Romans, and were allowed to either adopt them as foundlings or raise them as slaves, often giving them names such as "copro -" to memorialise their rescue. Strabo considered it a peculiarity of the Egyptians that every child must be reared. Diodorus indicates infanticide was a punishable offence. Egypt was heavily dependent on the annual flooding of the Nile to irrigate the land and in years of low inundation severe famine could occur with breakdowns in social order resulting, notably between 930-1070 AD and 1180-1350 AD. Instances of cannibalism are recorded during these periods but it is unknown if this happened during the pharaonic era of Ancient Egypt. Beatrix Midant-Reynes describes human sacrifice as having occurred at Abydos in the early dynastic period (c. 3150-2850 BCE), while Jan Assmann asserts there is no clear evidence of human sacrifice ever happening in Ancient Egypt.
Carthage.
According to Shelby Brown, Carthaginians, descendants of the Phoenicians, sacrificed infants to their gods. Charred bones of hundreds of infants have been found in Carthaginian archaeological sites. One such area harbored as many as 20,000 burial urns. Skeptics suggest that the bodies of children found in Carthaginian and Phoenician cemeteries were merely the cremated remains of children that died naturally.
Plutarch (c. 46–120 AD) mentions the practice, as do Tertullian, Orosius, Diodorus Siculus and Philo. The Hebrew Bible also mentions what appears to be child sacrifice practiced at a place called the Tophet (from the Hebrew "taph" or "toph", to burn) by the Canaanites. Writing in the 3rd century BCE, Kleitarchos, one of the historians of Alexander the Great, described that the infants rolled into the flaming pit. Diodorus Siculus wrote that babies were roasted to death inside the burning pit of the god Baal Hamon, a bronze statue.
Greece and Rome.
The historical Greeks considered the practice of adult and child sacrifice barbarous, however, the exposure of newborns was widely practiced in ancient Greece, it was even advocated by Aristotle in the case of congenital deformity — "As to the exposure of children, let there be a law that no deformed child shall live.” In Greece the decision to expose a child was typically the father's, although in Sparta the decision was made by a group of elders. Exposure was the preferred method of disposal, as that act in itself was not considered to be murder; moreover, the exposed child technically had a chance of being rescued by the gods or any passersby. This very situation was a recurring motif in Greek mythology.
To notify the neighbors of a birth of a child, a woolen strip was hung over the front door to indicate a female baby and an olive branch to indicate a boy had been born. Families did not always keep their new child. After a woman had a baby, she would show it to her husband. If the husband accepted it, it would live, but if he refused it, it would die. Babies would often be rejected if they were illegitimate, unhealthy or deformed, the wrong sex, or too great a burden on the family. These babies would not be directly killed, but put in a clay pot or jar and deserted outside the front door or on the roadway. In ancient Greek religion, this practice took the responsibility away from the parents because the child would die of natural causes, for example hunger, asphyxiation or exposure to the elements.
The practice was prevalent in ancient Rome, as well. Philo was the first philosopher to speak out against it. A letter from a Roman citizen to his sister, or a pregnant wife from her husband, dating from 1 BC, demonstrates the casual nature with which infanticide was often viewed:
In some periods of Roman history it was traditional for a newborn to be brought to the "pater familias", the family patriarch, who would then decide whether the child was to be kept and raised, or left to die by exposure. The Twelve Tables of Roman law obliged him to put to death a child that was visibly deformed. The concurrent practices of slavery and infanticide contributed to the "background noise" of the crises during the Republic.
Infanticide became a capital offense in Roman law in 374 AD, but offenders were rarely if ever prosecuted.
According to mythology, Romulus and Remus, twin infant sons of the war god Mars, survived near-infanticide after being tossed into the Tiber River. According to the myth, they were raised by wolves, and later founded the city of Rome.
Judaism.
Judaism prohibits infanticide, and has for some time, dating back to at least early Common Era. Roman historians wrote about the ideas and customs of other peoples, which often diverged from their own. Tacitus recorded that the Jews "regard it as a crime to kill any late-born children." Josephus, whose works give an important insight into 1st-century Judaism, wrote that God "forbids women to cause abortion of what is begotten, or to destroy it afterward."
Pagan European tribes.
In his book "Germania", Tacitus wrote that the ancient Germanic tribes enforced a similar prohibition. He found such mores remarkable and commented: ""Germani hold it shameful to kill any unwanted child."" Modern scholarship differs. John Boswell believed that in ancient Germanic tribes unwanted children were exposed, usually in the forest. "It was the custom of the pagans, that if they wanted to kill a son or daughter, they would be killed before they had been given any food." Usually children born out of wedlock were disposed that way.
In his highly influential "Pre-historic Times", John Lubbock described burnt bones indicating the practice of child sacrifice in pagan Britain.
The last canto, "Marjatan poika" (Son of Marjatta), of Finnish national epic Kalevala describes an assumed infanticide. Väinämöinen orders the infant bastard son of Marjatta to be drowned in marsh.
The Íslendingabók, a main source for the early history of Iceland, recounts that on the Conversion of Iceland to Christianity in 1000 it was provided - in order to make the transition more palatable to Pagans - that "(...)the old laws allowing exposure of newborn children will remain in force".
However, this provision - like other concessions made at the time to the Pagans - was abolished some years later.
Christianity.
Christianity rejects infanticide. The "Teachings of the Apostles" or "Didache" said ""You shall not kill that which is born."" The "Epistle of Barnabas" stated an identical command. Apologists Tertullian, Athenagoras, Minucius Felix, Justin Martyr and Lactantius also maintained that exposing a baby to death was a wicked act. In 318 AD, Constantine I considered infanticide a crime, and in 374 AD, Valentinian I mandated the rearing of all children (exposing babies, especially girls, was still common). The Council of Constantinople declared that infanticide was homicide, and in 589 AD, the Third Council of Toledo took measures against the custom of killing their own children.
Middle Ages.
Whereas theologians and clerics preached sparing their lives, newborn abandonment continued as registered in both the literature record and in legal documents. According to William L. Langer, exposure in the Middle Ages "was practiced on gigantic scale with absolute impunity, noticed by writers with most frigid indifference". At the end of the 12th century, notes Richard Trexler, Roman women threw their newborns into the Tiber river in daylight.
Unlike other European regions, in the Middle Ages the German mother had the right to expose the newborn. In Gotland, Sweden, children were also sacrificed.
In the High Middle Ages, abandoning unwanted children finally eclipsed infanticide. Unwanted children were left at the door of church or abbey, and the clergy was assumed to take care of their upbringing. This practice also gave rise to the first orphanages.
However, very high sex ratios were common in even late medieval Europe, which may indicate sex-selective infanticide.
Arabia.
According to Islamic sources, pre-Islamic Arabian society practiced infanticide as a form of "post-partum birth control". Regarding the prevalence of this practice, we know it was "common enough among the pre-Islamic Arabs to be assigned a specific term, "waʾd"". Infanticide was practiced either out of destitution (thus practiced on males and females alike), or as sacrifices to gods, or as "disappointment and fear of social disgrace felt by a father upon the birth of a daughter".
Some authors believe that there is little evidence that infanticide was prevalent in pre-Islamic Arabia or early Muslim history, except for the case of the Tamim tribe, who practiced it during severe famine. Others state that "female infanticide was common all over Arabia during this period of time" (pre-Islamic Arabia), especially by burying alive a female newborn.
Islam.
Islam
Infanticide is explicitly prohibited by the Qur'an. ""And do not kill your children for fear of poverty; We give them sustenance and yourselves too; surely to kill them is a great wrong.""
Together with polytheism and homicide, infanticide is regarded as a grave sin (see and ). Infanticide is also implicitly denounced in the story of Pharaoh's slaughter of the male children of Israelites (see ; ; ; ; ;).
Ukraine and Russia.
Infanticide may have been practiced as human sacrifice, as part of the pagan cult of Perun. Ibn Fadlan describes sacrificial practices at the time of his trip to Kiev Rus (present day Ukraine) in 921-922, and describes an incident of a woman voluntarily sacrificing her life as part of a funeral rite for a prominent leader, but makes no mention of infanticide. The Primary Chronicle, one of the most important literary sources before the 12th century, indicates that human sacrifice to idols may have been introduced by Vladimir the Great in 980. The same Vladimir the Great formally converted Kiev Rus into Christianity just 8 years later, but pagan cults continued to be practiced clandestinely in remote areas as late as the 13th century.
In Kamchatka, babies were killed and thrown to the dogs. American explorer George Kennan noted that among the Koryaks, a Mongoloid people of north-eastern Siberia, infanticide was still common in the nineteenth century. One of a pair of twins was always sacrificed.
Georgia.
The Svans killed newborn females by filling their mouths with hot ashes.
Asia.
China.
Short of execution, the harshest penalties were imposed on practitioners of infanticide by the legal codes of the Qin dynasty and Han dynasty of ancient China.
Marco Polo, the famed explorer, saw newborns exposed in Manzi. China's society practiced sex selective infanticide. Philosopher Han Fei Tzu, a member of the ruling aristocracy of the 3rd century BC, who developed a school of law, wrote: ""As to children, a father and mother when they produce a boy congratulate one another, but when they produce a girl they put it to death."" Among the Hakka people, and in Yunnan, Anhui, Sichuan, Jiangxi and Fujian a method of killing the baby was to put her into a bucket of cold water, which was called "baby water".
Infanticide was known in China as early as the 3rd century BC, and, by the time of the Song dynasty (960-1279 AD), it was widespread in some provinces. Buddhist belief in transmigration allowed poor residents of the country to kill their newborn children if they felt unable to care for them, hoping that they would be reborn in better circumstances. Furthermore, some Chinese did not consider newborn children fully "human", and saw "life" beginning at some point after the sixth month after birth.
Contemporary writers from the Song dynasty note that, in Hubei and Fujian provinces, residents would only keep three sons and two daughters (among poor farmers, two sons and one daughter), and kill all babies beyond that number at birth. Initially the sex of the child was only one factor to consider. By the time of the Ming Dynasty, however (1368–1644), male infanticide was becoming increasingly uncommon. The prevalence of female infanticide remained high much longer. The magnitude of this practice is subject to some dispute; however, one commonly quoted estimate is that, by late Qing, between one fifth and one quarter of all newborn girls, across the entire social spectrum, were victims of infanticide. If one includes excess mortality among female children under 10 (ascribed to gender-differential neglect), the share of victims rises to one third.
Scottish Physician John Dudgeon, who worked in Beijing, China, during the Qing Dynasty said that in China, "Infanticide does not prevail to the extent so generally believed among us, and in the north it does not exist at all."
Gender-selected abortion, abandonment, and infanticide are illegal in present-day China. Nevertheless, the US State Department, and the human rights organization Amnesty International have all declared that China's family planning programs, called the one child policy, contribute to infanticide.
Japan.
Since feudal Japan the common slang for infanticide was ""mabiki"" (間引き) which means to pull plants from an overcrowded garden. A typical method in Japan was smothering through wet paper on the baby's mouth and nose. Mabiki persisted in the 19th century and early 20th century. To bear twins was perceived as barbarous and unlucky and efforts were made to hide or kill one or both twins.
India.
Female infanticide of newborn girls was systematic in feudatory Rajputs in South Asia for illegitimate female children during the Middle Ages. According to Firishta, as soon as the illegitimate female child was born she was held "in one hand, and a knife in the other, that any person who wanted a wife might take her now, otherwise she was immediately put to death". The practice of female infanticide was also common among the Kutch, Kehtri, Nagar, Bengal, Miazed, Kalowries in India inhabitants, and also among the Sindh in British India.
It was not uncommon that parents threw a child to the sharks in the Ganges River as a sacrificial offering. The British colonists were unable to outlaw the custom until the beginnings of the 19th century.
According to social activists, female infanticide has remained a problem in India into the 21st century, with both NGOs and the government conducting awareness campaigns to combat it.
Africa.
In some African societies some neonates were killed because of beliefs in evil omens or because they were considered unlucky. Twins were usually put to death in Arebo; as well as by the Nama Hottentots of South West Africa; in the Lake Victoria Nyanza region; by the Tswana in Portuguese East Africa; in some parts of Igboland, Nigeria twins were sometimes abandoned in a forest at birth, oftentimes one twin was killed or hidden by midwives of wealthier mothers; and by the !Kung people of the Kalahari Desert. The Kikuyu, Kenya's most populous ethnic group, practiced ritual killing of twins.
Australia.
Literature suggests infanticide may have occurred reasonably commonly amongst Indigenous Australians, in all areas of Australia prior to European settlement. Infanticide may have continued to occur quite often up until the 1960s. An 1866 issue of 'The Australian News for Home Readers' informed readers that "the crime of infanticide is so prevalent amongst the natives that it is rare to see an infant."
Author Susanna de Vries in 2007 told a newspaper that her accounts of Aboriginal violence, including infanticide, was censored by publishers in the 1980s and 1990s. She told reporters that the censorship "stemmed from guilt over the stolen children question." Keith Windschuttle weighed in on the conversation, saying this type of censorship started in the 1970s. In the same article Louis Nowra suggested that infanticide in customary Aboriginal law may have been because it was difficult to keep an abundant number of Aboriginal children alive; there were life-and-death decisions modern-day Australians no longer have to face.
South Australia and Victoria.
According to William D. Rubinstein, "Nineteenth-century European observers of Aboriginal life in South Australia and Victoria reported that about 30% of Aboriginal infants were killed at birth."
James Dawson wrote a passage about infanticide amongst Indigenous people in the western district of Victoria, which stated that "Twins are as common among them as among Europeans; but as food is occasionally very scarce, and a large family troublesome to move about, it is lawful and customary to destroy the weakest twin child, irrespective of sex.
It is usual also to destroy those which are malformed."
He also wrote "When a woman has children too rapidly for the convenience and necessities of the parents, she makes up her mind to let one be killed, and consults with her husband which it is to be. As the strength of a tribe depends more on males than females, the girls are generally sacrificed.
The child is put to death and buried, or burned without ceremony; not, however, by its father or mother, but by relatives. No one wears mourning for it.
Sickly children are never killed on account of their bad health, and are allowed to die naturally."
Western Australia.
In 1937, a reverend in the Kimberley offered a "baby bonus" to Aboriginal families as a deterrent against infanticide and to increase the birthrate of the local Indigenous population.
Australian Capital Territory.
A Canberran journalist in 1927 wrote of the 'cheapness of life' to the Aboriginal people local to the Canberra area 100 years before. "If drought or bush fires had devastated the country and curtailed food supplies, babies got short shift. Ailing babies, too would not be kept" he wrote.
New South Wales.
A bishop wrote in 1928 that it was common for Aboriginal Australians to restrict the size of their tribal groups, including by infanticide, so that the food resources of the tribal area may be sufficient for them. See also.
Northern Territory.
Annette Hamilton, a professor of anthropology at Macquarie University who carried out research in the Aboriginal community of Maningrida in Arnhem Land during the 1960s wrote that prior to that time part-European babies born to Aboriginal mothers had not been allowed to live, and that 'mixed-unions are frowned on by men and women alike as a matter of principle'.
North America.
Inuit.
There is no agreement about the actual estimates of the frequency of newborn female infanticide in the Inuit population. Carmel Schrire mentions diverse studies ranging from 15-50% to 80%.
Polar Inuit (Inughuit) killed the child by throwing him or her into the sea. There is even a legend in Inuit mythology, "The Unwanted Child", where a mother throws her child into the fjord.
The Yukon and the Mahlemuit tribes of Alaska exposed the female newborns by first stuffing their mouths with grass before leaving them to die. In Arctic Canada the Inuit exposed their babies on the ice and left them to die.
Female Inuit infanticide disappeared in the 1930s and 1940s after contact with the Western cultures from the South.
Canada.
The "Handbook of North American Indians" reports infanticide among the Dene Natives and those of the Mackenzie Mountains.
Native Americans.
In the Eastern Shoshone there was a scarcity of Indian women as a result of female infanticide. For the Maidu Native Americans twins were so dangerous that they not only killed them, but the mother as well. In the region known today as southern Texas, the Mariame Indians practiced infanticide of females on a large scale. Wives had to be obtained from neighboring groups.
Mexico.
Bernal Díaz recounted that, after landing on the Veracruz coast, they came across a temple dedicated to Tezcatlipoca. ""That day they had sacrificed two boys, cutting open their chests and offering their blood and hearts to that accursed idol"". In "The Conquest of New Spain" Díaz describes more child sacrifices in the towns before the Spaniards reached the large Aztec city Tenochtitlan.
South America.
Although academic data of infanticides among the indigenous people in South America is not as abundant as that of North America, the estimates seem to be similar.
Brazil.
The Tapirapé indigenous people of Brazil allowed no more than three children per woman, and no more than two of the same sex. If the rule was broken infanticide was practiced. The people in the Bororo tribe killed all the newborns that did not appear healthy enough. Infanticide is also documented in the case of the Korubo people in the Amazon.
The Yanomami men killed children while raiding enemy villages. Helena Valero, a Brazilian woman kidnapped by Yanomami warriors in the 1930s, witnessed a Karawetari raid on her tribe:
Peru, Paraguay and Bolivia.
While "qhapaq hucha" was practiced in the Peruvian large cities, child sacrifice in the pre-Columbian tribes of the region is less documented. However, even today studies on the Aymara Indians reveal high incidences of mortality among the newborn, especially female deaths, suggesting infanticide. The Abipones, a small tribe of Guaycuran stock, of about 5,000 by the end of the 18th century in Paraguay, practiced systematic infanticide; with never more than two children being reared in one family. The Machigenga killed their disabled children. Infanticide among the Chaco in Paraguay was estimated as high as 50% of all newborns in that tribe, who were usually buried. The infanticidal custom had such roots among the Ayoreo in Bolivia and Paraguay that it persisted until the late 20th century.
Modern times.
Infanticide has become less common in the Western world. The frequency has been estimated to be approximately 1 in 3000-5000 children of all ages and 2.1 per 100,000 newborns per year. It is thought that infanticide today continues at a much higher rate in areas of extremely high poverty and overpopulation, such as parts of China and India. Female infants, then and even now, are particularly vulnerable, a factor in sex-selective infanticide. Recent estimates suggest that over 100 million girls and women are 'missing' in Asia.
Benin.
In spite of the fact that it is illegal, in Benin, West Africa, parents secretly continue with infanticidal customs.
North Korea.
According to "The Hidden Gulag" published by the Committee for Human Rights in North Korea, the People's Republic of China returns all illegal immigrants from North Korea which usually imprisons them in a short term facility. Women who are suspected of being impregnated by Chinese fathers are subjected to forced abortions; babies born alive are killed, sometimes by exposure or being buried alive.
China.
There have been some accusations that infanticide occurs in the People's Republic of China due to the one-child policy. In the 1990s, a certain stretch of the Yangtze River was known to be a common site of infanticide by drowning, until government projects made access to it more difficult. Recent studies suggest that over 40 million girls and women are 'missing' in China (Klasen and Wink 2003).
India.
The practice has continued in some rural areas of India. Infanticide is illegal in India.
According to a recent report by the United Nations Children's Fund (UNICEF) up to 50 million girls and women are missing in India's population as a result of systematic sex discrimination and sex selective abortions.
Pakistan.
Killings of newborn babies have been on the rise in Pakistan, corresponding to an increase in poverty across the country. More than 1,000 infants, mostly girls, have been killed or abandoned to die in Pakistan in 2009 according to a Pakistani charity organization.
The Edhi Foundation found 1,210 dead babies in 2010. Many more are abandoned and left at the doorsteps of mosques. As a result, Edhi centers feature signs "Do not murder, lay them here." Though female infanticide is punishable by life in prison, such crimes are rarely prosecuted.
Oceania.
In November 2008 it was reported that in Agibu and Amosa villages of Gimi region of Eastern Highlands province of Papua New Guinea where tribal fighting in the region of Gimi has been going on since 1986 (many of the clashes arising over claims of sorcery) women had agreed that if they stopped producing males, allowing only female babies to survive, their tribe's stock of boys would go down and there would be no men in the future to fight. They agreed to have all newborn male babies killed. It is not known how many male babies were killed by being smothered, but it had reportedly happened to all males over a 10-year period and probably was still happening.
England.
In England and Wales there were typically 30 to 50 homicides per million children less than 1 year old between 1982 and 1996. The younger the infant, the higher the risk. The rate for children 1 to 5 years was around 10 per million children. The homicide rate of infants less than 1 year is significantly higher than for the general population.
United States of America.
In 1983, the United States ranked eleventh for infants under 1 year killed, and fourth for those killed from 1 through 14 years (the latter case not necessarily involving filicide). In the U.S. over six hundred children were killed by their parents in 1983.
In the United States the infanticide rate during the first hour of life dropped from 1.41 per 100,000 during 1963 to 1972 to 0.44 per 100,000 for 1974 to 1983; the rates during the first month of life also declined, whereas those for older infants rose during this time. The legalization of abortion, which was completed in 1973, was the most important factor in the decline in neonatal mortality during the period from 1964 to 1977, according to a study by economists associated with the National Bureau of Economic Research.
Canada.
In Canada 114 cases of infanticide by a parent were reported during 1964-1968. There is ongoing debate in the Canadian legal and political fields about whether section 233 of the Criminal Code, which creates the specific offence and partial defence of infanticide in Canadian law, should be amended or abolished altogether.
Modern proposals.
In a 2012 article in the "Journal of Medical Ethics", a philosopher and a bioethicist jointly proposed that infanticide be legalized, calling it "after-birth abortion", and claiming that both "the fetus and the newborn are potential persons". Many replies were published to this article.
Child euthanasia.
Euthanasia applied to children that are gravely ill or that suffer from significant birth defects is legal in the Netherlands under rigidly controlled conditions, but controversial. Some critics have compared child euthanasia to infanticide.
Explanations for the practice.
There are various reasons for infanticide. Neonaticide typically has different patterns and causes than for killing of older infants. Traditional neonaticide is often related to economic necessity - inability to provide for the infant.
In the United Kingdom and the United States, older infants are typically killed for reasons related to child abuse, domestic violence or mental illness. For infants older than one day, younger infants are more at risk, and boys are more at risk than girls. Risk factors for the parent include: Family history of violence, violence in current relationship, history of abuse or neglect of children, and personality disorder and/or depression.
Religious.
In the late seventeenth and early eighteenth centuries, "loopholes" were invented by those who wanted to avoid the damnation that was promised by most Christian doctrine as a penalty of suicide. One famous example of someone who wished to end their life but avoid the eternity in hell was Christina Johansdotter (died 1740). She was a Swedish murderer who killed a child in Stockholm with the sole purpose of being executed. She is an example of those who seek suicide through execution by committing a murder. It was a common act, frequently targeting young children or infants as they were believed to be free from sin, thus going straight to heaven.
In 1888, Lieut. F. Elton reported that Ugi beach people in the Solomon Islands killed their infants at birth by burying them, and women were also said to practice abortion. They reported that it was too much trouble to raise a child, and instead preferred to buy one from the bush people. Larry S. Milner, author of "Hardness of Heart/Hardness of Life", a treatise on infanticide, believes that superstition has always reigned supreme in tribal religion. In chapters 9 through 21 Milner explores diverse customs and taboos as possible causes of infanticide, from punishment and shame to poverty, famine, revenge, depression and insanity and superstitious omens.
Economic.
Many historians believe the reason to be primarily economic, with more children born than the family is prepared to support. In societies that are patrilineal and patrilocal, the family may choose to allow more sons to live and kill some daughters, as the former will support their birth family until they die, whereas the latter will leave economically and geographically to join their husband's family, possibly only after the payment of a burdensome dowry price. Thus the decision to bring up a boy is more economically rewarding to the parents. However, this does not explain why infanticide would occur equally among rich and poor, nor why it would be as frequent during decadent periods of the Roman Empire as during earlier, less affluent, periods.
Before the appearance of effective contraception, infanticide was a common occurrence in ancient brothels. Unlike usual infanticide - where historically girls have been more likely to be killed - prostitutes in certain areas preferred to kill their male offspring.
UK 18th and 19th century.
Instances of infanticide in Britain in 18th and 19th century is often attributed to the economic position of the women, with juries committing pious perjury in many subsequent murder cases. The knowledge of the difficulties faced in the 18th century by those women who attempted to keep their children can be seen as reason for juries to show compassion. If the woman chose to keep the child, society was not set up to ease the pressure placed upon the woman, legally, socially or economically.
In mid-18th century Britain there was assistance available for women who were not able to raise their children. The Foundling Hospital opened in 1756 and was able to take in some of the illegitimate children. However, the conditions within the hospital caused Parliament to withdraw funding and the governors to live off of their own incomes. This resulted in a stringent entrance policy, with the committee requiring that the hospital:
'Will not receive a child that is more than a year old, nor the child of a domestic servant, nor any child whose father can be compelled to maintain it'.
Once a mother had admitted her child to the hospital, the hospital did all it could to ensure that the parent and child were not re-united.
Macfarlane argues in "Illegitimacy and Illegitimates in Britain" (1980) that English society greatly concerned itself with the burden that a bastard child places upon its communities and had gone to some lengths to ensure that the father of the child is identified in order to maintain its well-being. Assistance could be gained through maintenance payments from the father, however, this was capped ‘at a miserable 2s and 6d a week’. If the father got into arrears with the payments he could only be asked ‘to pay a maximum of 13 weeks arrears’.
Despite the accusations of some that women were getting a free hand-out there is evidence that many women were far from receiving adequate assistance from their parish. "Within Leeds in 1822 … relief was limited to 1s per week". Sheffield required women to enter the workhouse, whereas Halifax gave no relief to the women who required it. The prospect of entering the workhouse was certainly something to be avoided. Lionel Rose quotes Dr Joseph Rogers in "Massacre of the Innocents …" (1986). Rogers, who was employed by a London workhouse in 1856 stated that conditions in the nursery were ‘wretchedly damp and miserable … … overcrowded with young mothers and their infants’.
The loss of social standing for a servant girl was a particular problem in respect of producing a bastard child as they relied upon a good character reference in order to maintain their job and more importantly, to get a new or better job. In a large number of trials for the crime of infanticide, it is the servant girl that stood accused. The disadvantage of being a servant girl is that they had to live to the social standards of their superiors or risk dismissal and no references. Whereas within other professions, such as in the factory, the relationship between employer and employee was much more anonymous and the mother would be better able to make other provisions, such as employing a minder. The result of the lack of basic social care in Britain in the 18th and 19th century is the numerous accounts in court records of women, particularly servant girls, standing trial for the murder of their child.
There may have been no specific offence of infanticide in England before about 1623 because infanticide was a matter for the by ecclesiastical courts, possibly because infant mortality from natural causes was high (about 15% or one in six).
Thereafter the accusation of the suppression of bastard children by lewd mothers was a crime incurring the presumption of guilt.
The Infanticide Acts are several laws. That of 1922 made the killing of an infant child by its mother during the early months of life as a lesser crime than murder. The acts of 1938 and 1939 abolished the earlier act, but introduced the idea that postpartum depression was legally to be regarded as a form of diminished responsibility.
Population control.
Marvin Harris estimated that among Paleolithic hunters 23-50% of newborn children were killed. He argued that the goal was to preserve the 0.001% population growth of that time. He also wrote that female infanticide may be a form of population control. Population control is achieved not only by limiting the number of potential mothers; increased fighting among men for access to relatively scarce wives would also lead to a decline in population. For example, on the Melanesian island of Tikopia infanticide was used to keep a stable population in line with its resource base. Research by Marvin Harris and William Divale supports this argument, it has been cited as an example of environmental determinism.
Psychological.
Evolutionary psychology.
Evolutionary psychology has proposed several theories for different forms of infanticide. Infanticide by stepfathers, as well as child abuse in general by stepfathers, has been explained by spending resources on not genetically related children reducing reproductive success (See the Cinderella effect and Infanticide (zoology)). Infanticide is one of the few forms of violence more often done by women than men. Cross-cultural research have found that this is more likely to occur when the child has deformities or illnesses as well as when there are lacking resources due to factors such as poverty, other children requiring resources, and no male support. Such a child may have a low chance of reproductive success in which case it would decrease the mother's inclusive fitness, in particular since women generally have a greater parental investment than men, to spend resources on the child. .
"Early infanticidal childrearing".
A minority of academics subscribe to an alternate school of thought, considering the practice as "early infanticidal childrearing". They attribute parental infanticidal wishes to massive projection or displacement of the parents' unconscious onto the child, because of intergenerational, ancestral abuse by their own parents. Clearly, an infanticidal parent may have multiple motivations, conflicts, emotions, and thoughts about their baby and their relationship with their baby, which are often colored both by their individual psychology, current relational context and attachment history, and, perhaps most saliently, their psychopathology (See also Psychiatric section below) Almeida, Merminod, and Schechter suggest that parents with fantasies, projections, and delusions involving infanticide need to be taken seriously and assessed carefully, whenever possible, by an interdisciplinary team that includes infant mental health specialists or mental health practitioners who have experience in working with parents, children, and families.
Wider effects.
In addition to debates over the morality of infanticide itself, there is some debate over the effects of infanticide on surviving children, and the effects of childrearing in societies that also sanction infanticide. Some argue that the practice of infanticide in any widespread form causes enormous psychological damage in children. Conversely, studying societies that practice infanticide Géza Róheim reported that even infanticidal mothers in New Guinea, who ate a child, did not affect the personality development of the surviving children; that "these are good mothers who eat their own children". Harris and Divale's work on the relationship between female infanticide and warfare suggests that there are, however, extensive negative effects.
Psychiatric.
Postpartum psychosis is also a causative factor of infanticide. Stuart S. Asch, MD, a Professor of Psychiatry at Cornell University established the connections between some cases of infanticide and post-partum depression., The books, "From Cradle to Grave", and "The Death of Innocents", describe selected cases of maternal infanticide and the investigative research of Professor Asch working in concert with the New York City Medical Examiner's Office.
Stanley Hopwood wrote that childbirth and lactation entail severe stress on the female sex, and that under certain circumstances attempts at infanticide and suicide are common. A study published in the "American Journal of Psychiatry" revealed that 44% of filicidal fathers had a diagnosis of psychosis. In addition to postpartum psychosis, dissociative psychopathology and sociopathy have also been found to be associated with neonaticide in some cases
In addition, severe postpartum depression can lead to infanticide.
Sex selection.
Sex selection may be one of the contributing factors of infanticide. In the absence of sex-selective abortion, sex-selective infanticide can be deduced from very skewed birth statistics. The biologically normal sex ratio for humans at birth is approximately 105 males per 100 females; normal ratios hardly ranging beyond 102-108. When a society has an infant male to female ratio which is significantly higher or lower than the biological norm, and biased data can be ruled out, sex selection can usually be inferred.
Current law.
Australia.
In New South Wales, infanticide is defined in Section 22A(1) of the Crimes Act 1900 (NSW) as follows:
Compliant with the provisions of murder, as per s18A, the maximum penalty for this offence is therefore 25 years imprisonment.
India.
Infanticide is illegal in India, but rarely enforced in the rural parts of India.
Canada.
In Canada, a mother commits infanticide, a lesser offence than homicide, if she killed her child while "not fully recovered from the effects of giving birth to the child and by reason thereof or of the effect of lactation consequent on the birth of the child her mind is then disturbed".
England and Wales.
In England and Wales, the Infanticide Act 1938 describes the offence of infanticide as one which would otherwise amount to murder (by his/her mother) if the victim was older than 12 months and the mother was not suffering from an imbalance of mind due to the effects of childbirth or lactation. Where a mother who has killed such an infant has been charged with murder rather than infanticide s.1(3) of the Act confirms that a jury has the power to find alternative verdicts of Manslaughter in English law or guilty but insane.
Romania.
Article 200 of the Penal Code of Romania stipulates that the killing of a newborn during the first 24 hours, by the mother who is in a state of mental distress, shall be punished with imprisonment of one to five years. The previous Romanian Penal Code also defined infanticide ("pruncucidere") as a distinct criminal offence, providing for a punishment of two to seven years imprisonment, recognizing the fact that a mother's judgment may be impaired immediately after birth, but did not define the term "infant", and this had led to debates regarding the precise moment when infanticide becomes homicide. This issue was resolved by the new Penal Code, which came into force in 2014.
United States.
In 2009, Texas state representative Jessica Farrar proposed legislation that would define infanticide as a distinct and lesser crime than homicide. Under the terms of the proposed legislation, if jurors concluded that a mother's "judgment was impaired as a result of the effects of giving birth or the effects of lactation following the birth," they would be allowed to convict her of the crime of infanticide, rather than murder. The maximum penalty for infanticide would be two years in prison. Farrar's introduction of this bill prompted liberal bioethics scholar Jacob M. Appel to call her "the bravest politician in America."
Prevention.
Since infanticide, especially neonaticide, is often a response to an unwanted birth, preventing unwanted pregnancies through improved sex education and increased contraceptive access are advocated as ways of preventing infanticide. Increased use of contraceptives and access to safe legal abortions have greatly reduced neonaticide in many developed nations. Some say that where abortion is illegal, as in Pakistan, infanticide would decline if safer legal abortions were available.
Screening for psychiatric disorders or risk factors, and providing treatment or assistance to those at risk may help prevent infanticide. However, in developed world significant proportions of neonaticides that are detected occur in young women who deny their pregnancy, and avoid outside contacts, so they may have limited contact with health care services.
In some areas baby hatches or "safe surrender sites", safe places for a mother to anonymously leave an infant, are offered, in part to reduce the rate of infanticide. In other places, like the United States, safe-haven laws allow mothers to anonymously give infants to designated officials; they are frequently located at hospitals and police and fire stations. Typically such babies are put up for adoption, or cared for in orphanages.
Granting women employment raises their status and autonomy. Having a gainful employment can raise the perceived worth of females. This can lead to an increase in the number of women getting an education and a decrease in the number of female infanticide. As a result, the infant mortality rate will decrease and economic development will increase.
In animals.
Although human infanticide has been widely studied, the practice has been observed in many other species of the animal kingdom since it was first seriously studied by Yukimaru Sugiyama. These include from microscopic rotifers and insects, to fish, amphibians, birds and mammals, including primates such as chacma baboons. Infanticide can be practiced by both males and females.

</doc>
<doc id="15476" url="https://en.wikipedia.org/wiki?curid=15476" title="Internet protocol suite">
Internet protocol suite

The Internet protocol suite is the computer networking model and set of communications protocols used on the Internet and similar computer networks. It is commonly known as TCP/IP, because its most important protocols, the Transmission Control Protocol (TCP) and the Internet Protocol (IP) were the first networking protocols defined during its development. It is occasionally known as the Department of Defense (DoD) model, because the development of the networking model was funded by DARPA, an agency of the United States Department of Defense.
TCP/IP provides end-to-end data communication specifying how data should be packetized, addressed, transmitted, routed and received. This functionality is organized into four abstraction layers which are used to sort all related protocols according to the scope of networking involved. From lowest to highest, the layers are the link layer, containing communication methods for data that remains within a single network segment (link); the internet layer, connecting independent networks, thus providing internetworking; the transport layer handling host-to-host communication; and the application layer, which provides process-to-process data exchange for applications.
The TCP/IP model and many of its protocols are maintained by the Internet Engineering Task Force (IETF).
History.
Early research.
The Internet protocol suite resulted from research and development conducted by the Defense Advanced Research Projects Agency (DARPA) in the late 1960s. After initiating the pioneering ARPANET in 1969, DARPA started work on a number of other data transmission technologies. In 1972, Robert E. Kahn joined the DARPA Information Processing Technology Office, where he worked on both satellite packet networks and ground-based radio packet networks, and recognized the value of being able to communicate across both. In the spring of 1973, Vinton Cerf, the developer of the existing ARPANET Network Control Program (NCP) protocol, joined Kahn to work on open-architecture interconnection models with the goal of designing the next protocol generation for the ARPANET.
By the summer of 1973, Kahn and Cerf had worked out a fundamental reformulation, in which the differences between network protocols were hidden by using a common internetwork protocol, and, instead of the network being responsible for reliability, as in the ARPANET, the hosts became responsible. Cerf credits Hubert Zimmermann and Louis Pouzin, designer of the CYCLADES network, with important influences on this design.
The design of the network included the recognition that it should provide only the functions of efficiently transmitting and routing traffic between end nodes and that all other intelligence should be located at the edge of the network, in the end nodes. Using a simple design, it became possible to connect almost any network to the ARPANET, irrespective of the local characteristics, thereby solving Kahn's initial problem. One popular expression is that TCP/IP, the eventual product of Cerf and Kahn's work, will run over ""two tin cans and a string."" (Years later, as a joke, the IP over Avian Carriers formal protocol specification was created and successfully tested.)
A computer called a router is provided with an interface to each network. It forwards packets back and forth between them. Originally a router was called "gateway", but the term was changed to avoid confusion with other types of gateways.
Specification.
From 1973 to 1974, Cerf's networking research group at Stanford worked out details of the idea, resulting in the first TCP specification. A significant technical influence was the early networking work at Xerox PARC, which produced the PARC Universal Packet protocol suite, much of which existed around that time.
DARPA then contracted with BBN Technologies, Stanford University, and the University College London to develop operational versions of the protocol on different hardware platforms. Four versions were developed: TCP v1, TCP v2, TCP v3 and IP v3, and TCP/IP v4. The last protocol is still in use today.
In 1975, a two-network TCP/IP communications test was performed between Stanford and University College London (UCL). In November, 1977, a three-network TCP/IP test was conducted between sites in the US, the UK, and Norway. Several other TCP/IP prototypes were developed at multiple research centers between 1978 and 1983. The migration of the ARPANET to TCP/IP was officially completed on flag day January 1, 1983, when the new protocols were permanently activated.
Adoption.
In March 1982, the US Department of Defense declared TCP/IP as the standard for all military computer networking. In 1985, the Internet Advisory Board (later renamed the Internet Architecture Board) held a three-day workshop on TCP/IP for the computer industry, attended by 250 vendor representatives, promoting the protocol and leading to its increasing commercial use.
In 1985, the first Interop conference focused on network interoperability by broader adoption of TCP/IP. The conference was founded by Dan Lynch, an early Internet activist. From the beginning, large corporations, such as IBM and DEC, attended the meeting. Interoperability conferences have been held every year since then. Every year from 1985 through 1993, the number of attendees tripled.
IBM, AT&T and DEC were the first major corporations to adopt TCP/IP, despite having competing internal protocols (SNA, XNS, DECNET). In IBM, from 1984, Barry Appelman's group did TCP/IP development. (Appelman later moved to AOL to be the head of all its development efforts.) They navigated the corporate politics to get a stream of TCP/IP products for various IBM systems, including MVS, VM, and OS/2. At the same time, several smaller companies began offering TCP/IP stacks for DOS and MS Windows, such as the company FTP Software, and the Wollongong Group. The first VM/CMS TCP/IP stack came from the University of Wisconsin.
Many of these TCP/IP stacks were written single-handedly by a few talented programmers. For example, John Romkey of FTP Software was the author of the MIT PC/IP package. John Romkey's PC/IP implementation was the first IBM PC TCP/IP stack. Jay Elinsky and Oleg Vishnepolsky of IBM Research wrote TCP/IP stacks for VM/CMS and OS/2, respectively.
The spread of TCP/IP was fueled further in June 1989, when AT&T agreed to place the TCP/IP code developed for UNIX into the public domain. Various vendors, including IBM, included this code in their own TCP/IP stacks. Many companies sold TCP/IP stacks for Windows until Microsoft released a native TCP/IP stack in Windows 95. This event was a little late in the evolution of the Internet, but it cemented TCP/IP's dominance over other protocols, which began to lose ground. These protocols included IBM Systems Network Architecture (SNA), Open Systems Interconnection (OSI), Microsoft's native NetBIOS, and Xerox Network Systems (XNS).
Key architectural principles.
An early architectural document, RFC 1122, emphasizes architectural principles over layering.
Abstraction layers.
Encapsulation is used to provide abstraction of protocols and services. Encapsulation is usually aligned with the division of the protocol suite into layers of general functionality. In general, an application (the highest level of the model) uses a set of protocols to send its data down the layers, being further encapsulated at each level.
The layers of the protocol suite near the top are logically closer to the user application, while those near the bottom are logically closer to the physical transmission of the data. Viewing layers as providing or consuming a service is a method of abstraction to isolate upper layer protocols from the details of transmitting bits over, for example, Ethernet and collision detection, while the lower layers avoid having to know the details of each and every application and its protocol.
Even when the layers are examined, the assorted architectural documents—there is no single architectural model such as ISO 7498, the Open Systems Interconnection (OSI) model—have fewer and less rigidly defined layers than the OSI model, and thus provide an easier fit for real-world protocols. One frequently referenced document, RFC 1958, does not contain a stack of layers. The lack of emphasis on layering is a major difference between the IETF and OSI approaches. It only refers to the existence of the internetworking layer and generally to "upper layers"; this document was intended as a 1996 snapshot of the architecture: "The Internet and its architecture have grown in evolutionary fashion from modest beginnings, rather than from a Grand Plan. While this process of evolution is one of the main reasons for the technology's success, it nevertheless seems useful to record a snapshot of the current principles of the Internet architecture."
RFC 1122, entitled "Host Requirements", is structured in paragraphs referring to layers, but the document refers to many other architectural principles not emphasizing layering. It loosely defines a four-layer model, with the layers having names, not numbers, as follows:
The Internet protocol suite and the layered protocol stack design were in use before the OSI model was established. Since then, the TCP/IP model has been compared with the OSI model in books and classrooms, which often results in confusion because the two models use different assumptions and goals, including the relative importance of strict layering.
This abstraction also allows upper layers to provide services that the lower layers do not provide. While the original OSI model was extended to include connectionless services (OSIRM CL), IP is not designed to be reliable and is a best effort delivery protocol. This means that all transport layer implementations must choose whether or how to provide reliability. UDP provides data integrity via a checksum but does not guarantee delivery; TCP provides both data integrity and delivery guarantee by retransmitting until the receiver acknowledges the reception of the packet.
This model lacks the formalism of the OSI model and associated documents, but the IETF does not use a formal model and does not consider this a limitation, as illustrated in the comment by David D. Clark, "We reject: kings, presidents and voting. We believe in: rough consensus and running code." Criticisms of this model, which have been made with respect to the OSI model, often do not consider ISO's later extensions to that model.
For multi-access links with their own addressing systems (e.g. Ethernet) an address mapping protocol is needed. Such protocols can be considered to be below IP but above the existing link system. While the IETF does not use the terminology, this is a subnetwork dependent convergence facility according to an extension to the OSI model, the internal organization of the network layer (IONL).
ICMP & IGMP operate on top of IP but do not transport data like UDP or TCP. Again, this functionality exists as layer management extensions to the OSI model, in its "Management Framework" (OSIRM MF)
The SSL/TLS library operates above the transport layer (uses TCP) but below application protocols. Again, there was no intention, on the part of the designers of these protocols, to comply with OSI architecture.
The link is treated as a black box. The IETF explicitly does not intend to discuss transmission systems, which is a less academic but practical alternative to the OSI model.
The following is a description of each layer in the TCP/IP networking model starting from the lowest level.
Link layer.
The link layer has the networking scope of the local network connection to which a host is attached. This regime is called the "link" in TCP/IP literature. It is the lowest component layer of the Internet protocols, as TCP/IP is designed to be hardware independent. As a result, TCP/IP may be implemented on top of virtually any hardware networking technology.
The link layer is used to move packets between the Internet layer interfaces of two different hosts on the same link. The processes of transmitting and receiving packets on a given link can be controlled both in the software device driver for the network card, as well as on firmware or specialized chipsets. These perform data link functions such as adding a packet header to prepare it for transmission, then actually transmit the frame over a physical medium. The TCP/IP model includes specifications of translating the network addressing methods used in the Internet Protocol to data link addressing, such as Media Access Control (MAC). All other aspects below that level, however, are implicitly assumed to exist in the link layer, but are not explicitly defined.
This is also the layer where packets may be selected to be sent over a virtual private network or other networking tunnel. In this scenario, the link layer data may be considered application data which traverses another instantiation of the IP stack for transmission or reception over another IP connection. Such a connection, or virtual link, may be established with a transport protocol or even an application scope protocol that serves as a tunnel in the link layer of the protocol stack. Thus, the TCP/IP model does not dictate a strict hierarchical encapsulation sequence.
The TCP/IP model's link layer corresponds to the Open Systems Interconnection (OSI) model physical and data link layers, layers one and two of the OSI model.
Internet layer.
The internet layer has the responsibility of sending packets across potentially multiple networks. Internetworking requires sending data from the source network to the destination network. This process is called routing.
The Internet Protocol performs two basic functions:
The internet layer is not only agnostic of data structures at the transport layer, but it also does not distinguish between operation of the various transport layer protocols. IP carries data for a variety of different upper layer protocols. These protocols are each identified by a unique protocol number: for example, Internet Control Message Protocol (ICMP) and Internet Group Management Protocol (IGMP) are protocols 1 and 2, respectively.
Some of the protocols carried by IP, such as ICMP which is used to transmit diagnostic information, and IGMP which is used to manage IP Multicast data, are layered on top of IP but perform internetworking functions. This illustrates the differences in the architecture of the TCP/IP stack of the Internet and the OSI model. The TCP/IP model's internet layer corresponds to layer three of the Open Systems Interconnection (OSI) model, where it is referred to as the network layer.
The internet layer provides an unreliable datagram transmission facility between hosts located on potentially different IP networks by forwarding the transport layer datagrams to an appropriate next-hop router for further relaying to its destination. With this functionality, the internet layer makes possible internetworking, the interworking of different IP networks, and it essentially establishes the Internet. The Internet Protocol is the principal component of the internet layer, and it defines two addressing systems to identify network hosts' computers, and to locate them on the network. The original address system of the ARPANET and its successor, the Internet, is Internet Protocol version 4 (IPv4). It uses a 32-bit IP address and is therefore capable of identifying approximately four billion hosts. This limitation was eliminated in 1998 by the standardization of Internet Protocol version 6 (IPv6) which uses 128-bit addresses. IPv6 production implementations emerged in approximately 2006.
Transport layer.
The transport layer establishes basic data channels that applications use for task-specific data exchange. The layer establishes process-to-process connectivity, meaning it provides end-to-end services that are independent of the structure of user data and the logistics of exchanging information for any particular specific purpose. Its responsibility includes end-to-end message transfer independent of the underlying network, along with error control, segmentation, flow control, congestion control, and application addressing (port numbers). End-to-end message transmission or connecting applications at the transport layer can be categorized as either connection-oriented, implemented in TCP, or connectionless, implemented in UDP.
For the purpose of providing process-specific transmission channels for applications, the layer establishes the concept of the port. This is a numbered logical construct allocated specifically for each of the communication channels an application needs. For many types of services, these port numbers have been standardized so that client computers may address specific services of a server computer without the involvement of service announcements or directory services.
Because IP provides only a best effort delivery, some transport layer protocols offer reliability. However, IP can run over a reliable data link protocol such as the High-Level Data Link Control (HDLC).
For example, the TCP is a connection-oriented protocol that addresses numerous reliability issues in providing a reliable byte stream:
The newer Stream Control Transmission Protocol (SCTP) is also a reliable, connection-oriented transport mechanism. It is message-stream-oriented—not byte-stream-oriented like TCP—and provides multiple streams multiplexed over a single connection. It also provides multi-homing support, in which a connection end can be represented by multiple IP addresses (representing multiple physical interfaces), such that if one fails, the connection is not interrupted. It was developed initially for telephony applications (to transport SS7 over IP), but can also be used for other applications.
The User Datagram Protocol is a connectionless datagram protocol. Like IP, it is a best effort, "unreliable" protocol. Reliability is addressed through error detection using a weak checksum algorithm. UDP is typically used for applications such as streaming media (audio, video, Voice over IP etc.) where on-time arrival is more important than reliability, or for simple query/response applications like DNS lookups, where the overhead of setting up a reliable connection is disproportionately large. Real-time Transport Protocol (RTP) is a datagram protocol that is designed for real-time data such as streaming audio and video.
The applications at any given network address are distinguished by their TCP or UDP port. By convention certain "well known ports" are associated with specific applications.
The TCP/IP model's transport or host-to-host layer corresponds to the fourth layer in the Open Systems Interconnection (OSI) model, also called the transport layer.
Application layer.
The application layer includes the protocols used by most applications for providing user services or exchanging application data over the network connections established by the lower level protocols, but this may include some basic network support services, such as many routing protocols, and host configuration protocols. Examples of application layer protocols include the Hypertext Transfer Protocol (HTTP), the File Transfer Protocol (FTP), the Simple Mail Transfer Protocol (SMTP), and the Dynamic Host Configuration Protocol (DHCP). Data coded according to application layer protocols are encapsulated into transport layer protocol units (such as TCP or UDP messages), which in turn use lower layer protocols to effect actual data transfer.
The TCP/IP model does not consider the specifics of formatting and presenting data, and does not define additional layers between the application and transport layers as in the OSI model (presentation and session layers). Such functions are the realm of libraries and application programming interfaces.
Application layer protocols generally treat the transport layer (and lower) protocols as black boxes which provide a stable network connection across which to communicate, although the applications are usually aware of key qualities of the transport layer connection such as the end point IP addresses and port numbers. Application layer protocols are often associated with particular client–server applications, and common services have "well-known" port numbers reserved by the Internet Assigned Numbers Authority (IANA). For example, the HyperText Transfer Protocol uses server port 80 and Telnet uses server port 23. Clients connecting to a service usually use ephemeral ports, i.e., port numbers assigned only for the duration of the transaction at random or from a specific range configured in the application.
The transport layer and lower-level layers are unconcerned with the specifics of application layer protocols. Routers and switches do not typically examine the encapsulated traffic, rather they just provide a conduit for it. However, some firewall and bandwidth throttling applications must interpret application data. An example is the Resource Reservation Protocol (RSVP). It is also sometimes necessary for network address translator (NAT) traversal to consider the application payload.
The application layer in the TCP/IP model is often compared as equivalent to a combination of the fifth (Session), sixth (Presentation), and the seventh (Application) layers of the Open Systems Interconnection (OSI) model.
Furthermore, the TCP/IP reference model distinguishes between "user protocols" and "support protocols". Support protocols provide services to a system. User protocols are used for actual user applications. For example, FTP is a user protocol and DNS is a system protocol.
Layer names and number of layers in the literature.
The following table shows various networking models. The number of layers varies between three and seven.
Some of the networking models are from textbooks, which are secondary sources that may conflict with the intent of RFC 1122 and other IETF primary sources.
Comparison of TCP/IP and OSI layering.
The three top layers in the OSI model, i.e. the application layer, the presentation layer and the session layer, are not distinguished separately in the TCP/IP model which only has an application layer above the transport layer. While some pure OSI protocol applications, such as X.400, also combined them, there is no requirement that a TCP/IP protocol stack must impose monolithic architecture above the transport layer. For example, the NFS application protocol runs over the eXternal Data Representation (XDR) presentation protocol, which, in turn, runs over a protocol called Remote Procedure Call (RPC). RPC provides reliable record transmission, so it can safely use the best-effort UDP transport.
Different authors have interpreted the TCP/IP model differently, and disagree whether the link layer, or the entire TCP/IP model, covers OSI layer 1 (physical layer) issues, or whether a hardware layer is assumed below the link layer.
Several authors have attempted to incorporate the OSI model's layers 1 and 2 into the TCP/IP model, since these are commonly referred to in modern standards (for example, by IEEE and ITU). This often results in a model with five layers, where the link layer or network access layer is split into the OSI model's layers 1 and 2.
The IETF protocol development effort is not concerned with strict layering. Some of its protocols may not fit cleanly into the OSI model, although RFCs sometimes refer to it and often use the old OSI layer numbers. The IETF has repeatedly stated that Internet protocol and architecture development is not intended to be OSI-compliant. RFC 3439, addressing Internet architecture, contains a section entitled: "Layering Considered Harmful".
For example, the session and presentation layers of the OSI suite are considered to be included to the application layer of the TCP/IP suite. The functionality of the session layer can be found in protocols like HTTP and SMTP and is more evident in protocols like Telnet and the Session Initiation Protocol (SIP). Session layer functionality is also realized with the port numbering of the TCP and UDP protocols, which cover the transport layer in the TCP/IP suite. Functions of the presentation layer are realized in the TCP/IP applications with the MIME standard in data exchange.
Conflicts are apparent also in the original OSI model, ISO 7498, when not considering the annexes to this model, e.g., the ISO 7498/4 Management Framework, or the ISO 8648 Internal Organization of the Network layer (IONL). When the IONL and Management Framework documents are considered, the ICMP and IGMP are defined as layer management protocols for the network layer. In like manner, the IONL provides a structure for "subnetwork dependent convergence facilities" such as ARP and RARP.
IETF protocols can be encapsulated recursively, as demonstrated by tunneling protocols such as Generic Routing Encapsulation (GRE). GRE uses the same mechanism that OSI uses for tunneling at the network layer.
Implementations.
The Internet protocol suite does not presume any specific hardware or software environment. It only requires that hardware and a software layer exists that is capable of sending and receiving packets on a computer network. As a result, the suite has been implemented on essentially every computing platform. A minimal implementation of TCP/IP includes the following: Internet Protocol (IP), Address Resolution Protocol (ARP), Internet Control Message Protocol (ICMP), Transmission Control Protocol (TCP), User Datagram Protocol (UDP), and IGMP. In addition to IP, ICMP, TCP, UDP, Internet Protocol version 6 requires Neighbor Discovery Protocol (NDP), ICMPv6, and IGMPv6 and is often accompanied by an integrated IPSec security layer.
Application programmers are typically concerned only with interfaces in the application layer and often also in the transport layer, while the layers below are services provided by the TCP/IP stack in the operating system. Most IP implementations are accessible to programmers through sockets and APIs.
Unique implementations include Lightweight TCP/IP, an open source stack designed for embedded systems, and KA9Q NOS, a stack and associated protocols for amateur packet radio systems and personal computers connected via serial lines.
Microcontroller firmware in the network adapter typically handles link issues, supported by driver software in the operating system. Non-programmable analog and digital electronics are normally in charge of the physical components below the link layer, typically using an application-specific integrated circuit (ASIC) chipset for each network interface or other physical standard. High-performance routers are to a large extent based on fast non-programmable digital electronics, carrying out link level switching.

</doc>
<doc id="15477" url="https://en.wikipedia.org/wiki?curid=15477" title="Ibn al-Shaykh al-Libi">
Ibn al-Shaykh al-Libi

Ibn al-Shaykh al-Libi (; ALFB transliteration: Ḁbnʋ ălŞɑỉƈ alLibi; born Ali Mohamed Abdul Aziz al-Fakheri, 1963 – May 10, 2009) was a Libyan national captured in Afghanistan in November 2001 after the fall of the Taliban; he was interrogated by the American and Egyptian forces. The information he gave under torture to Egyptian authorities was cited by the George W. Bush Administration in the months preceding its 2003 invasion of Iraq as evidence of a connection between Saddam Hussein and al-Qaeda. That information was frequently repeated by members of the Bush Administration, although reports from both the Central Intelligence Agency (CIA) and the Defense Intelligence Agency (DIA) strongly questioned its credibility, suggesting that al-Libi was "intentionally misleading" interrogators.
In 2006, the United States transferred al-Libi to Libya, where he was imprisoned by the government. He was reported to have tuberculosis. On May 19, 2009, the government reported that he had recently committed suicide in prison. Human Rights Watch, which had a couple of representatives who had recently visited him, called for an investigation into the circumstances of his death; "The New York Times" reported that Ayman al-Zawahiri had asserted that Libya had tortured al-Libi to death.
Training camp director.
In Afghanistan, al-Libi led the Al Khaldan training camp, where Zacarias Moussaoui and Ahmed Ressam trained for attacks in the United States. An associate of Abu Zubaydah, al-Libi had his assets frozen by the U.S. government following the September 11 attacks; it published a list of terrorists on September 26, 2002 who were covered by this restriction.
Al-Libi was captured by Pakistani officials in November 2001, as he attempted to flee Afghanistan following the collapse of the Taliban after the 2001 U.S. invasion of Afghanistan.
Department of Defense spokesmen used to routinely described the Khaldan training camp as an al-Qaeda training camp, and Al-Libi and Abu Zubaydah as senior members of al-Qaeda. But, during testimony at their Combatant Status Review Tribunals, several Guantanamo captives, including Zubaydah, described the Khaldan camp as having been run by a rival jihadist organizationone that did not support attacking civilians.
Cooperation with the FBI.
Al-Libi was turned over to the FBI and held at Bagram Air Base. When talking to the FBI interrogators Russell Fincher and Marty Mahon, he seemed "genuinely friendly" and spoke chiefly in English, calling for a translator only when necessary. He seemed to bond with Fincher, a devout Christian, and the two prayed together and discussed religion at length.
Al-Libi told the interrogators details about Richard Reid, a British citizen who had joined al-Qaeda and trained to carry out a suicide bombing of an airliner, which he unsuccessfully attempted on December 22, 2001. Al-Libi agreed to continue cooperating if the United States would allow his wife and her family to emigrate, while he was prosecuted within the American legal system.
In CIA custody.
The CIA asked President Bush for permission to take al-Libi into their own custody and rendition him to a foreign country for more "tough guy" questioning, and were granted permission. They "simply came and took al-Libi away from the FBI." One CIA officer was heard telling their new prisoner that "You know where you are going. Before you get there, I am going to find your mother and fuck her".
In the second week of January 2002, al-Libi was flown to the USS "Bataan" in the northern Arabian Sea, a ship being used to hold eight other notable prisoners, including John Walker Lindh. He was subsequently transferred to Egyptian interrogators.
Information provided.
According to the "Washington Post",
On September 15, 2002, "Time" published an article that detailed the CIA interrogations of Omar al-Faruq. It said,
"On Sept. 9, according to a secret CIA summary of the interview, al-Faruq confessed that he was, in fact, al-Qaeda's senior representative in Southeast Asia. Then came an even more shocking confession: according to the CIA document, al-Faruq said two senior al-Qaeda officials, Abu Zubaydah and Ibn al-Shaykh al-Libi, had ordered him to 'plan large-scale attacks against U.S. interests in Indonesia, Malaysia, (the) Philippines, Singapore, Thailand, Taiwan, Vietnam and Cambodia.'"
Al-Libi has been identified as a principal source of faulty prewar intelligence regarding chemical weapons training between Iraq and al-Qaeda that was used by the Bush Administration to justify the invasion of Iraq. Specifically, he told interrogators that Iraq provided training to al-Qaeda in the area of "chemical and biological weapons". In Cincinnati in October 2002, Bush informed the public: "Iraq has trained al Qaeda members in bomb making and poisons and gases."
This claim was repeated several times in the run-up to the war, including in then-Secretary of State Colin Powell's speech to the U.N Security Council on February 5, 2003, which concluded with a long recitation of the information provided by al-Libi. Powell's speech was made less than a month after a then-classified CIA report concluded that the information provided by al-Libi was unreliable, and about a year after a DIA report concluded the same thing.
Al-Libi recanted these claims in January 2004 after U.S. interrogators presented "new evidence from other detainees that cast doubt on his claims", according to "Newsweek". The DIA concluded in February 2002 that al-Libi deliberately misled interrogators, in what the CIA called an "attempt to exaggerate his importance". Some speculate that his reason for giving disinformation was in order to draw the U.S. into an attack on Iraq—Islam's "weakest" state; a remark attributed to al-Libi—which al-Qaeda believes will lead to a global jihad. Others, including al-Libi himself, have insisted that he gave false information due to the use of torture (so-called "enhanced interrogation techniques").
An article published in the November 5, 2005 "New York Times" quoted two paragraphs of a Defense Intelligence Agency report, declassified upon request by Senator Carl Levin, that expressed doubts about the results of al-Libi's interrogation in February 2002.
Al-Libi told a foreign intelligence service that:
"Iraq — acting on the request of al-Qa'ida militant Abu Abdullah, who was Muhammad Atif's emissary — agreed to provide unspecified chemical or biological weapons training for two al-Qa'ida associates beginning in December 2000. The two individuals departed for Iraq but did not return, so al-Libi was not in a position to know if any training had taken place." 
The September 2002 version of "Iraqi Support for Terrorism" stated that al-Libi said Iraq had "provided" chemical and biological weapons training for two al-Qaeda associates in 2000, but also stated that al-Libi "did not know the results of the training."
The 2006 Senate Report on Pre-war Intelligence on Iraq stated that "Although DIA coordinated on CIA's "Iraqi Support for Terrorism" paper, DIA analysis preceding that assessment was more skeptical of the al-Libi reporting." In July 2002, DIA assessed "It is plausible al-Qa'ida attempted to obtain CB assistance from Iraq and Ibn al-Shaykh is sufficiently senior to have access to such sensitive information. However, Ibn al-Shaykh's information lacks details concerning the individual Iraqis involved, the specific CB materials associated with the assistance and the location where the alleged training occurred. The information is also second hand, and not derived from Ibn al-Shaykh's personal experience."
The Senate report also states "According to al-Libi, after his decision to fabricate information for debriefers, he 'lied about being a member of al-Qa'ida. Although he considered himself close to, but not a member of, al-Qa'ida, he knew enough about the senior members, organization and operations to claim to be a member.'"
Senate Reports on Pre-war Intelligence on Iraq.
On September 8, 2006, the United States Senate Select Committee on Intelligence released "Phase II" of its report on prewar intelligence on Iraq. Conclusion 3 of the report states the following:
On June 11, 2008 "Newsweek" published an account of material from a "previously undisclosed CIA report written in the summer of 2002". The article reported that on August 7, 2002 CIA analysts had drafted a high-level report that expressed serious doubts about the information flowing from al-Libi's interrogation. The information that al-Libi acknowledged being a member of al-Qaeda's executive council was not supported by other sources. According to al-Libi, in Egypt he was locked in a tiny box less than 20 inches high and held for 17 hours and after being let out he was thrown to the floor and punched for 15 minutes. According to CIA operational cables, only then did he tell his "fabricated" story about al-Qaeda members being dispatched to Iraq.
Book: "Inside the Jihad".
In November 2006, a Moroccan using the pseudonym Omar Nasiri, having infiltrated al-Qaeda in the 1990s, wrote the book, "". In the book, Nasiri claims that al-Libi deliberately planted information to encourage the U.S. to invade Iraq. In an interview with BBC2's "Newsnight", Nasiri said Libi "needed the conflict in Iraq because months before I heard him telling us when a question was asked in the mosque after the prayer in the evening, where is the best country to fight the jihad?" Nasiri said that Libi had identified Iraq as the "weakest" Muslim country. He suggested to "Newsnight" that al-Libi wanted to overthrow Saddam and use Iraq as a jihadist base. Nasiri describes al-Libi as one of the leaders at the Afghan camp, and characterizes him as "brilliant in every way." He said that learning how to withstand interrogations and supply false information was a key part of the training in the camps. Al-Libi "knew what his interrogators wanted, and he was happy to give it to them. He wanted to see Saddam toppled even more than the Americans did."
Book: "At the Center of the Storm".
In April 2007 former Director of Central Intelligence George Tenet released his memoir titled "". With regard to al-Libi, Tenet writes the following:
Repatriation to Libya and death.
In 2006 the Bush Administration announced that it was transferring high-value al-Qaeda detainees from CIA secret prisons so they could be put on trial by military commissions.
But the Administration was conspicuously silent about al-Libi.
Noman Benotman, a former Mujahideen who knew Libi, told "Newsweek" that during a recent trip to Tripoli, he met with a senior Libyan government official who confirmed to him that Libi had been transferred to Libya and was being held in prison there. He was suffering from tuberculosis.
On May 10, 2009, the English language edition of the Libyan newspaper "Ennahar" reported that the government said that Al-Libi had been repatriated to Libyan custody in 2006, and had recently committed suicide by hanging. It attributed the information to another newspaper, "Oea". "Ennahar" reported Al-Libi's real name was Ali Mohamed Abdul Aziz Al-Fakheri. It stated he was 46 years old, and had been allowed visits with international human rights workers from Human Rights Watch. The story was widely reported by other media outlets.
Al-Libi had been visited in April 2009 by a team from Human Rights Watch. His sudden death so soon after this visit has led human rights organisations and Islamic groups to question whether it was truly a suicide. Clive Stafford Smith, Legal Director of the UK branch of the human rights group Reprieve, said, "We are told that al-Libi committed suicide in his Libyan prison. If this is true it would be because of his torture and abuse. If false, it may reflect a desire to silence one of the greatest embarrassments to the Bush administration." Hafed Al-Ghwell, a Libya expert and director of communications at the Dubai campus of Harvard's Kennedy School of Government, commented,
"This is a regime with a long history of killing people in jail and then claiming it was suicide. My guess is Libya has seen the winds of change in America and wanted to bury this man before international organisations start demanding access to him."
On June 19, 2009, Andy Worthington published new information on al-Libi's death. Worthington gave a detailed timeline of Al Libi's last years.
The head of the Washington office of Human Rights Watch said al-Libi was "Exhibit A" in hearings on the relationship between pre-Iraq War false intelligence and torture. Confirmation of al-Libi's location came two weeks prior to his death. An independent investigation of his death has been requested by Human Rights Watch.
On October 4, 2009 the "Reuters" reported that Ayman Al Zawahiri, the head of al-Qaeda, had asserted that Libya had caused al-Libi's death through torture.

</doc>
<doc id="15478" url="https://en.wikipedia.org/wiki?curid=15478" title="IDF">
IDF

IDF or idf may refer to:

</doc>
