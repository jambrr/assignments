<doc id="18131" url="https://en.wikipedia.org/wiki?curid=18131" title="Los Angeles International Airport">
Los Angeles International Airport

Los Angeles International Airport is the largest and busiest airport in the Greater Los Angeles Area and the state of California, and it is one of the most important international airports in the United States. It is most often referred to by its IATA airport code LAX, with the letters pronounced individually. LAX is located in the southwestern Los Angeles area along the Pacific Ocean between the neighborhood of Westchester to its immediate north and the city of El Segundo to its immediate south. It is owned and operated by Los Angeles World Airports, an agency of the Los Angeles city government formerly known as the Department of Airports.
In 2015, LAX handled 74,936,256 passengers, an increase of 6 percent from the previous year, making it the seventh busiest airport by passenger traffic in the world. The airport holds the claim for "the world's busiest origin and destination (O & D) airport," and has for many years. The airport also was the third busiest in the world by aircraft movements. Furthermore, it is also the only airport to rank among the top five U.S. airports for both passenger and cargo traffic.
While LAX is the busiest airport in the Greater Los Angeles Area, other airports, including Bob Hope Airport, John Wayne Airport, Long Beach Airport, and LA/Ontario International Airport, also serve the region. It is also notable for being one of the few U.S. airports with four parallel runways.
LAX serves as a hub for American Airlines, Delta Air Lines, United Airlines, Alaska Airlines, and Virgin America. The airport serves as a focus city for Allegiant Air, Air New Zealand, Qantas, Southwest Airlines, Spirit Airlines and Volaris. LAX serves as either a hub or focus city for more Mainline US Carriers than any other airport in the Country and is the only airport that all three legacy carriers have designated a hub. As of February 2016, the largest carriers were: American (20.29 percent), Delta (16.75 percent), United (14.68 percent), Southwest (11.27 percent), Alaska Airlines (4.72 percent) and Virgin America (4.19 percent).
As the largest international airport on the U.S. West Coast, LAX is a major gateway to and from Europe, Latin America, Asia and Oceania. With its deep connections to Asia and Latin America in particular, LAX is considered to be the premier "Gateway to the Pacific Rim."
History.
In 1928, the Los Angeles City Council selected in the southern part of Westchester for a new airport for the city. The fields of wheat, barley and lima beans were converted into dirt landing strips without any terminal buildings. It was named Mines Field for William W. Mines, the real estate agent who arranged the deal. The first structure, Hangar No. 1, was erected in 1929 and is in the National Register of Historic Places.
Mines Field opened as the airport of Los Angeles in 1930 and the city purchased it to be a municipal airfield in 1937. The name became Los Angeles Airport in 1941 and Los Angeles International Airport in 1949. In the 1930s the main airline airports were Burbank Airport (then known as Union Air Terminal, and later Lockheed) in Burbank and the Grand Central Airport in Glendale. (In 1940 the airlines were all at Burbank except for Mexicana's three departures a week from Glendale; in late 1946 most airline flights moved to LAX, but Burbank always retained a few.)
Mines Field did not extend west of Sepulveda Boulevard; Sepulveda was rerouted circa 1950 to loop around the west ends of the extended east–west runways (now runways 25L and 25R), which by November 1950 were long. A tunnel was completed in 1953 allowing Sepulveda Boulevard to revert to straight and pass beneath the two runways; it was the first tunnel of its kind. For the next few years the two runways were long.
On July 10, 1956, Boeing's 707 prototype (the 367–80) visited LAX. The "Los Angeles Times" said it was its first appearance at a "commercial airport" outside the Seattle area.
The April 1957 Official Airline Guide showed 66 weekday departures on United Airlines, 32 American Airlines, 32 Western Airlines, 27 TWA, nine Southwest, five Bonanza Air Lines and three Mexicana Airlines; also 22 flights a week on Pan American World Airways and five a week on Scandinavian Airlines (the only direct flights from California to Europe).
American Airlines' 707-123s flew the first jet passengers out of LAX to New York in January 1959; the first wide-body jets were TWA's Boeing 747s to New York in early 1970.
In 1958, the architecture firm Pereira & Luckman was contracted to plan the re-design of the airport for the "jet age". The plan, developed with architects Welton Becket and Paul Williams, called for a series of terminals and parking structures in the central portion of the property, with these buildings connected at the center by a huge steel-and-glass dome. The plan was never realized, and the Theme Building was built on the site intended for the dome.
In the new terminal area west of Sepulveda Blvd that started opening in 1961, each terminal had a satellite building out in the middle of the ramp, reached by underground tunnels from the ticketing area. United's satellites 7 and 8 were first to open, followed by 3, 4 and 5; satellite 2 opened as the international terminal several months later and satellite 6 was to be the last to open.
Since the 1920s, a neighborhood called Surfridge had been on the coastline west of the airport, part of the larger community of Palisades del Rey along with the neighborhood to the north now known as Playa del Rey. When the airlines switched to jet airliners during the 1960s and 1970s and Surfridge's residents complained about noise pollution, the city used its eminent domain powers to condemn and evacuate Surfridge. The government bulldozed the homes but did not bulldoze the streets, and the fenced-off "ghost" streets west of LAX are still there.
In 1981, LAX began a $700 million expansion in preparation for the 1984 Summer Olympics. The U-shaped roadway past the terminal entrances got a second level, with arriving passengers on the lower level and departing on the upper. Connector buildings between the ticketing areas and the satellite buildings were added, changing the layout to a "pier" design. Two new terminals (Terminal 1 and the Tom Bradley International Terminal) were built and Terminal 2, then two decades old, was rebuilt. Multi-story parking structures were also built in the center of the airport.
On July 8, 1982, groundbreaking for the two new terminals were conducted by Mayor Tom Bradley and World War II aviator General James Doolittle. The $123 million International Terminal opened on June 11, 1984, and was named for Bradley.
On April 29, 1992, the airport closed for violence and cleanup after the 1992 Los Angeles Riots over the Rodney King beating.
The airport closed again on January 17, 1994 after the Northridge earthquake.
In 1996, a $29 million, air traffic control tower was built near the Theme Building.
In 2000, before Los Angeles hosted the Democratic National Convention, fifteen glass pylons up to ten stories high were placed in a circle around the intersection of Sepulveda Boulevard and Century Boulevard, with more pylons of decreasing height following Century Boulevard eastward, evoking a sense of departure and arrival. Conceived by the designers at Selbert Perkins Design, the towers and "LAX" letters are a gateway to the airport and offer a welcoming landmark for visitors. Illuminated from the inside, the pylons slowly cycle through a rainbow of colors that represents the multicultural makeup of Los Angeles and can be customized to celebrate events, holidays or a season. This was part of an overall face-lift that included new signage and various other cosmetic enhancements that was led by Ted Tokio Tanaka Architects. The LAX pylons underwent improvements in 2006, as stage lighting inside the cylinders was replaced with LED lights to conserve energy, make maintenance easier and enable on-demand cycling through various color effects.
LAX has been a hub for TWA, Air California, Braniff International, Continental Airlines, Delta Air Lines, Pacific Southwest Airlines, US Airways, Western Airlines, and the Flying Tiger Line.
Starting in the mid-1990s, under Mayors Richard Riordan and James Hahn, modernization and expansion plans for LAX were prepared, only to be stymied by a coalition of residents who live near the airport. They cited increased noise, pollution and traffic impacts of the project. In late 2005, newly elected Mayor Antonio Villaraigosa was able to reach a compromise, allowing some modernization to go forward while encouraging future growth among other facilities in the region.
It is illegal to limit the number of passengers that use an airport, but in December 2005 the city agreed to limit the passenger gates to 163. Once passenger usage hits 75 million, a maximum of two gates a year for up to five years will be closed, intending to limit growth to 79 million passengers a year. In exchange civil lawsuits were abandoned, to allow the city to complete badly needed improvements to the airport.
The airport is a hub for Alaska Airlines, Delta Air Lines, United Airlines, American Airlines, and a focus city for Southwest Airlines, Allegiant Air, Air New Zealand, Qantas, Virgin America and Volaris. It is the only airport that is the hub for the three major U.S. airlines (American, Delta, and United). The airport also houses a line maintenance facility for Delta's primary maintenance, repair and overhaul arm, Delta TechOps.
In 2008, plans were unveiled for a $4.11 billion renovation and improvement program to expand and rehabilitate the Tom Bradley International Terminal to accommodate the next generation of larger aircraft, as well as handle the growing number of flights to and from the Southern California region, and to develop the Central Terminal Area (CTA) of the airport to include streamlined passenger processing, public transportation and updated central utility plants. As of 2013, Los Angeles International Airport is the biggest airport in California. The multi-year project, originally projected to be completed in 2014, is ongoing as of February 2015, and is the largest public works project in Los Angeles history.
The "X" in LAX.
Before the 1930s, existing airports used a two-letter abbreviation based on the weather stations at the airports. At that time, "LA" served as the designation for Los Angeles Airport. But with the rapid growth in the aviation industry the designations expanded to three letters c. 1947, and "LA" became "LAX." The letter "X" has no specific meaning in this identifier. "LAX" is also used for the Port of Los Angeles in San Pedro and by Amtrak for Union Station in downtown Los Angeles.
Aircraft spotting.
The "Imperial Hill" area (also known as Clutter's Park) in El Segundo is a prime location for aircraft spotting. Another popular spotting location sits under the final approach for runways 24 L&R on a lawn next to the Westchester In-N-Out Burger on Sepulveda Boulevard. This is one of the few remaining locations in Southern California from which spotters may watch such a wide variety of low-flying commercial airliners from directly underneath a flight path.
Space Shuttle "Endeavour".
At 12:51 p.m. on Friday, September 21, 2012, a Shuttle Carrier Aircraft carrying the Space Shuttle "Endeavour" landed at LAX on runway 25L. An estimated 10,000 people saw the shuttle land. Interstate 105 was backed up for miles at a standstill. Imperial Highway was shut down for spectators. It was quickly taken off the Boeing 747 and was moved to a United Airlines hangar. The shuttle spent about a month in the hangar while it was prepared for transport to the California Science Center.
Theme Building.
The distinctive white googie "Theme Building", designed by Pereira & Luckman architect Paul Williams and constructed in 1961 by Robert E. McKee Construction Co., resembles a flying saucer that has landed on its four legs. A restaurant with a sweeping view of the airport is suspended beneath two arches that form the legs. The Los Angeles City Council designated the building a Los Angeles Historic-Cultural Monument in 1992. A $4 million renovation, with retro-futuristic interior and electric lighting designed by Walt Disney Imagineering, was completed before the "Encounter Restaurant" opened there in 1997. Visitors are able to take the elevator up to the roof of the "Theme Building", which closed after the September 11, 2001 attacks for security reasons and reopened to the public on weekends beginning on July 10, 2010. Additionally, a memorial to the victims of the 9/11 attacks is located on the grounds, as three of the four hijacked planes were originally destined for LAX.
Terminals.
LAX has nine passenger terminals arranged in the shape of the letter U or a horseshoe. The terminals are served by a shuttle bus. Terminals 4, 5, 6, 7, and 8 are all connected airside via an underground tunnel between Terminals 4, 5 and 6 and above-ground walkways between Terminals 6, 7, and 8. An additional airside shuttle bus operates among Terminals 4, 6, and the American Eagle remote terminal. There are no physical airside connections between any of the other terminals.
In addition to these terminals, there are of cargo facilities at LAX, and a heliport operated by Bravo Aviation. Qantas has a maintenance facility at LAX, even though it is not a hub.
Inter-terminal connections between terminals 1, 2, and 3, and between them and the other terminals, require passengers to exit security, then walk or use a shuttle-bus to get to the other terminal, then re-clear security. Terminals 4-8, which comprise the south terminal complex, provide airside connections, which allow connecting passengers to access other terminals without having to re-clear security. The following airside connections are possible:
Terminals TBIT, 4, 5, 6, 7, and 8 have airside connection.
Terminals 4, 5 and 6 are connected via an airside underground walkway. At Terminal 6 passengers can transfer from the above ground terminal walkway to the underground walkway to access Terminals 4, 5 and 6.
Terminals 6, 7, and 8 are all connected airside via walking corridors at the same level as the terminal, allowing passengers a seamless connection (International arriving passengers must clear Customs, and then security, on a lower level first).
Some airlines provide an airside shuttle bus connection between terminals. For example, Qantas offers a late afternoon/evening shuttle bus for passengers arriving in Terminal 4 to connect with flights departing from the Tom Bradley International Terminal.
Beginning February 25, 2016, an additional airside corridor became available from Terminal 4 to the Tom Bradley International Terminal. This will allow airside connections from Terminals 8, 7, 6, 5 and 4 to the Tom Bradley International Terminal. An additional security checkpoint will be available in this connector to allow passengers to enter Terminal 4 after arriving on an international arrival in the Tom Bradley Terminal avoiding the main Terminal 4 security screening area, also allowing easier connections to Terminal 5, 6, 7 and 8.
Terminal 1.
Terminal 1 has 15 gates: Gates 9, 10, 11A-11B, 12A-12B, 13-15, 16A-16B, 17A-17B, and 18A-18B, and houses Southwest Airlines. Terminal 1 was built in 1984. Terminal 1 is presently undergoing an extensive renovation financed by Southwest Airlines. This renovation will continue through 2018 and provide updates to security screening area, curbside dropoff, terminal areas and baggage handling. Former tenants of the terminal include AirTran Airways, and US Airways.
Terminal 2.
Terminal 2 has 11 gates: Gates 21–21B, 22–22B, 23, 24–24B, and 25–28. It hosts most foreign airlines not using the Tom Bradley International Terminal along with a couple of domestic airlines: Aeroméxico, Air Canada, Avianca, Hainan Airlines, Hawaiian Airlines, Qatar Airways, Sun Country Airlines, Virgin Atlantic, Volaris, and WestJet.
Terminal 2 was built in 1962, and was the original international terminal. It was completely torn down and rebuilt in stages between 1984 and 1988 at a cost of $94 million. The rebuilt terminal was designed by Leo A Daly. Terminal 2 has CBP (Customs and Border Protection) facilities to process arriving international passengers.
Former tenants of the terminal include Air China, Air France, Air New Zealand, Alitalia, KLM, Northwest Airlines, Pan American World Airways, and TACA International Airlines. Air New Zealand moved to the Tom Bradley International Terminal on December 3, 2014 and Air China moved to the Tom Bradley International Terminal on July 1, 2015. Both Southwest Airlines and Virgin America use Terminal 2 for their international arrivals.
Terminal 3.
Terminal 3 has 12 gates: Gates 30, 31A–31B, 32, 33A–33B, 34–36, 37A–37B, and 38 (gate 39 was removed to make room for Virgin Australia Boeing 777 operations at gate 38). Terminal 3 opened in 1961 and was Trans World Airlines' terminal. The terminal was expanded in 1970 to accommodate widebody operations and between 1980 and 1987, which included a new passenger connector building and baggage system connected to the original satellite. It formerly housed some American Airlines flights after that airline acquired Reno Air and TWA in 1999 and 2001, respectively. Eventually, all American flights were moved to Terminal 4. As of July 2015, Allegiant Air, Frontier Airlines, JetBlue Airways, Spirit Airlines, and Virgin America use Terminal 3. 
Terminal 4.
Terminal 4 has 14 gates: Gates 40–41, 42A–42B, 43–45 (Gate 44 is for the bus to the American Eagle remote terminal, Gates 44A-44I), 46A–46B, 47A–47B, 48A–48B, and 49A, and houses American Airlines flights. Terminal 4 was built in 1961, was expanded in 1983 by adding a connector from the ticketing areas to the original satellite, and was renovated in 2002 at a cost of $400 million in order to improve the appearance and functionality of the facility. The renovation was designed by Rivers & Christian. An international arrivals facility was also added as part of the renovations but this has been closed due to staffing shortages. Currently American Airlines International flights arrive Terminal 4 and TBIT but passengers disembark via stairs onto buses that travel directly to Tom Bradley International Terminal arrivals. Some international departures operate at TBIT.
American Eagle regional flights operate from the "American Eagle Terminal", a satellite terminal that is located just east of Terminal 8. Gate 44 serves as the shuttle bus stop at Terminal 4. This terminal has 10 regional jet gates that supplement the 14 mainline gates at Terminal 4, giving American Airlines and American Eagle more gates than any other airline at LAX, with 24 (American Airlines operates from 32 gates in total, including 4 dedicated gates at Terminal 6, as well as 4 rotating gates at Tom Bradley International Terminal). The remote terminal is connected by shuttle buses to Terminals 6 also because of Eagle's codesharing with Alaska Airlines.
Terminal 5.
Terminal 5 has 15 gates: Gates 50A–50B, 51A–51B, 52A–52B, 53A–53B, 54A–54B, 55A, 56–57, 58, and 59, and is used for Delta Air Lines flights. Western Airlines occupied this terminal at its opening in 1962, and continued to do so until Western was merged with Delta on April 1, 1987. Terminal 5 was redesigned by Gensler, expanded to include a connector building between the original satellite and the ticketing facilities and remodeled from 1986 through early 1988. It was unofficially named 'Delta's Oasis at LAX' with the slogan 'Take Five at LAX' when construction was completed in the summer of 1988. Northwest Airlines moved all operations to Terminal 5 and Terminal 6 alongside Delta on June 30, 2009, as part of its merger with the airline. The terminal has a customs area in the arrivals floor, used for international flights served by Delta Air Lines.
Terminal 6.
Terminal 6 has 14 gates: Gates 60–63, 64A–64B, 65A-65B, 66, 67, 68A–68B, and 69A–69B. Parts of this terminal have changed little from its opening in 1961; in 1970, new gates were expanded from the main building, as is obvious from the rotunda at the end. Four of these gates have two jetways, which can accommodate large aircraft. An expansion of the terminal, including a connector of the original satellite to the ticketing areas, was completed in 1987.
Terminal 6 is currently used by Alaska Airlines, American Airlines, Copa Airlines, Delta Air Lines and Great Lakes Airlines.
Continental Airlines originally built the Connector Building (which links the Ticketing and rotunda buildings). Prior to October 2014, United Airlines used the connector gates, supplementing its base at Terminal 7. Delta also leases some space from the Airport in Terminal 6, in addition to its base at Terminal 5. Most of the rotunda gates can feed arriving passengers into a sterile corridor that shunts them to Terminal 7's customs and immigration facility.
In April 2011, Alaska Airlines agreed to a deal with Los Angeles World Airports to renovate Terminal 6 and build an Alaska Airlines Board Room Lounge. The airline moved its flights to Terminal 6 on March 20, 2012, and Spirit Airlines was relocated to Terminal 3.
Former tenants of the terminal include Continental Airlines until its merger with United Airlines in 2011 and Eastern Air Lines, which went bankrupt in 1991. The terminal also originally housed Pacific Southwest Airlines.
Terminal 7.
Terminal 7 has 11 gates: Gates 70A-70B, 71A-71B, 72, 73, 74, 75A-75B, 76, and 77. This terminal opened in 1962 and was expanded to accommodate widebody aircraft in 1970. The terminal was expanded in 1982 with the addition of a connector building, which today compromises of gates 70A-70B and 71A-71B. Four of these gates have two jetways, which accommodate large aircraft. Terminal 7 is used for United Airlines' domestic and international operations. The interior of the terminal was renovated between January 1998 and June 1999 at a cost of $250 million, was designed by HNTB, and was constructed by Hensel Phelps Construction. Added were new gate podiums, increased size of gate areas, relocated concessions, expanded restrooms, new flooring, and new signage. Also, the roof of the terminal was raised, and new, brighter light fixtures were added in order to provide more overall lighting. As of 2012, Terminal 7 is undergoing another facelift, with significant changes to concessions. The terminal also contains a United Club and an International First Class Lounge. The terminal has a customs area located on the arrivals floor, used by international flights served by United Airlines.
Terminal 8.
Terminal 8 has 9 gates: Gates 80–88. This terminal was originally constructed in 1961 as Concourse 8 and was redeveloped in 1982 and renamed Terminal 8. The terminal formerly served Shuttle by United flights. At one point, only United Express flights arrived and departed from Terminal 8. Non-United Express flights arrived and departed from Terminals 6 and 7, Terminal 8 is now used once again for some mainline flights.
Tom Bradley International Terminal.
The Tom Bradley International Terminal () has 18 gates; nine on the north concourse and nine on the south concourse. Each gate is equipped with a Safegate Advanced – Visual Docking Guidance System. In addition, there are nine satellite gates for international flights located on the west side of LAX. Passengers are ferried to the west side gates by bus. The terminal exclusively hosts most of the major international airlines, with the exception of those housed in Terminal 2.
This terminal opened for the 1984 Summer Olympic Games and is named for Tom Bradley, the first African-American and longest-serving (20 years) mayor of Los Angeles, and champion of LAX. The terminal is located at the west end of the passenger terminal area between Terminals 3 and 4. Tom Bradley International Terminal hosts 29 airlines and handles 10 million passengers per year.
In 2010, modernization efforts resulted in additional space for inline baggage screening, three large alliance-aligned lounges plus one unaffiliated lounge (to replace the multiple airline specific lounges) and cosmetic upgrades in the departures and arrivals areas.
On November 17, 2008, Mayor Antonio Villaraigosa unveiled design concepts for LAX's Bradley West and Midfield Concourse projects. Los Angeles World Airports (LAWA), along with city officials, selected Fentress Architects in association with HNTB to develop a design concept for the modernization of LAX. The emphasis of the modernization is to improve the passenger experience and to keep Los Angeles competitive with other global cities.
On February 22, 2010, construction began on the $1.5 billion Bradley West project, part of the multi-year $4.11 billion LAX improvement and redevelopment projects. The project added over of shops, restaurants, and passenger lounges, as well as new security screening, customs, immigration, and baggage claim facilities. The terminal's existing two concourses will be demolished and replaced with a larger pair with 18 gates, nine of which will be able to accommodate the Airbus A380. The terminal opened in phases beginning on September 2012, and was completed in 2014.
On September 18, 2013, the new Tom Bradley International Terminal at LAX officially opened. Airlines started to use the new, smarter gates that can handle larger aircraft, with multiple gates configured for the Airbus A380. Inside, Southern California scenes and eccentric videos are played on seven huge multimedia screens. It is the largest immersive system in an airport and the most advanced multimedia environment in a North American airport. This system was created by Moment Factory. Lounge-like seating for relaxation, and the 25,000-square-foot duty-free shopping area are located in the Great Hall. Many restaurants and high-end shops are located there, such as Chloé, Burberry, Fred Segal, and Porsche amongst others. Franchises of popular LA restaurants including Umami Burger, 800 Degrees, Larder, Ink.Sack and Lucky Fish are among the restaurant selections available.
Accommodating the Airbus A380.
On March 25, 2007, Runway 7R/25L reopened after being shifted south to prevent runway incursions and prepare the runway for the Airbus A380. Additional storm drains and enhanced runway lighting were added. Runway 25L is now south of the parallel runway centerline to centerline, allowing a parallel taxiway between the runways; the taxiway was completed in 2008.
On September 18, 2006, Los Angeles World Airports started a $503 million facelift of the Tom Bradley International Terminal. Improvements included new paging, air conditioning and electrical systems, along with new elevators, escalators, baggage carousels, and a digital sign that automatically update flight information. With federal funding, explosives-detection technology was incorporated into the terminal's underground baggage system.
According to the "Los Angeles Times", in February 2007, many Pacific Rim carriers began reducing flights to LAX in favor of more modern airports, such as San Francisco International Airport, due to the aging Tom Bradley International Terminal.
On August 15, 2007, the Los Angeles City Council approved a $1.2 billion project to construct a new 10-gate terminal to handle international flights using the Airbus A380. Adding the first new gates built since the early 1980s, the new structure was to be built directly west of the Tom Bradley International Terminal on a site that was occupied mostly by aircraft hangars.
On March 19, 2007, the Airbus A380 made its debut at LAX, landing on runway 24L. Though LAX was originally to be the first U.S. city to see the A380, Airbus later decided to forgo LAX in favor of John F. Kennedy International Airport in New York City. After city officials fought for the super-jumbo jet to land at LAX, Airbus had to get two A380s, which landed simultaneously in New York and Los Angeles.
On March 31, 2008, the "Los Angeles Times" reported that foreign carriers were once again flocking to LAX's Tom Bradley International Terminal. The weaker dollar caused a surge in demand for U.S. travel, resulting in airlines either adding new destinations or increasing frequencies to existing routes. New airlines that introduced flights to LAX included Virgin Australia, Emirates Airlines, Turkish Airlines, Iberia Airlines, Korean Air, Qantas, Air China, and Air France. The influx of new flights comes amidst the renovation of the airport and consolidates LAX's status as the premier international gateway to the Western United States.
Qantas launched service with the Airbus A380 on October 20, 2008, using the west side remote gates. Though initially deployed between LAX and Sydney, Qantas's A380 service was extended to the LAX-Melbourne route. This was followed shortly by Korean Air, which initiated nonstop A380 flights to Seoul-Incheon in October 2011. Air France has launched A380 flights between Paris–Charles de Gaulle and Los Angeles in May 2012. In addition, China Southern Airlines launched A380 service to Guangzhou in October 2012, representing an increase in capacity of 78 percent on the route. With the addition of these services, LAX boasted six daily A380 services. On October 15, 2013, British Airways' nonstop service from London–Heathrow to LAX also became an A380 route.
Asiana Airlines launched an Airbus A380 service to LAX on Wednesday, August 20, 2014. This new A380 route makes Asiana the eighth A380 operator at LAX and the eleventh airline to operate an A380. Previously, Asiana deployed regional Asian A380 routes to Osaka, Hong Kong, and Bangkok from Seoul.
LAX has more A380 services than any other North American city, with up to thirteen daily flights spread among nine operators (as of July 2015): Emirates (Dubai), China Southern Airlines (Guangzhou), British Airways (London–Heathrow), Qantas (Sydney and Melbourne), Korean Air and Asiana Airlines (Seoul), Air France (Paris), Singapore Airlines (Singapore via Tokyo), and Lufthansa (Frankfurt).
Airlines and destinations.
LAX connects to 87 domestic and 69 international destinations in North America, Latin America, Europe, the Middle East, Africa, Asia, and Oceania.
American Airlines/American Eagle operate the most departures from the airport, followed by United Airlines/United Express and Southwest Airlines. American operates the largest network of routes out of LAX serving more than 60 destinations, followed closely by Delta (58) and United (57).
Passenger.
Notes:
Traffic and statistics.
LAX handles more "origin and destination" (not connecting) passengers than any other airport in the world.
The airport handled 28,861,477 enplanements, the total number of passengers boarding an aircraft, in 2008. This makes LAX the third busiest airport in the United States in terms of enplanements.
It is the world's fifth-busiest airport by passenger traffic and fifteenth-busiest by cargo traffic, serving over 70.6 million passengers and 2 million tons of freight and mail in 2014. It is the busiest airport in the state of California, and the second-busiest airport by passenger boardings in the United States, based on final 2013 statistics.
In terms of international passengers, as of 2012, LAX is the third busiest in the United States. (behind JFK in New York City and MIA in Miami) and, as of 2006, 26th worldwide.
The number of aircraft operations (landings and takeoffs) has steadily increased to 636,706 in 2014, up from 614,917 in 2013, according to the Federal Aviation Administration. The Airports Council International places LAX at third most aircraft movements in the world, as of 2013.
Ground transportation.
Transportation between terminals.
Shuttles operate to and from the terminals, providing frequent service for connecting passengers. However, connecting passengers who use these shuttles must leave and then later reenter security. Underground tunnels connect between terminals 4, 5, 6, 7, and 8, and an above-ground connector between and terminal 4 opened in February 2016.
Freeways and roads.
LAX's terminals are immediately west of the interchange between Century Boulevard and Sepulveda Boulevard (State Route 1). The 405 Freeway can be reached to the east via Century Boulevard, and the 105 Freeway can be reached to the south via Sepulveda Boulevard.
Bus.
The closest bus stops to the terminals are the pair of opposites on Sepulveda Boulevard and Century Boulevard, served by Metro 117, Torrance 8, Metro 232, Commuter Express 574 and Metro 40 to Los Angeles Union Station (owl service only).
In addition, out of a number of bus systems, many routes (local, rapid and express) of the LACMTA Metro 232 to Long Beach, Line 8 of Torrance Transit, Line 109 of Beach Cities Transit, the Santa Monica Big Blue Bus system's Line 3 and Rapid 3 via Lincoln Boulevard to Santa Monica and the Culver CityBus's Line 6 and Rapid 6 via Sepulveda Blvd to Culver City and UCLA all make stops at the LAX Transit Center in Parking Lot C. on 96th St., where shuttle bus "C" offers free connections to and from every LAX terminal, and at the Green Line, where shuttle bus "G" connects to and from the terminals.
The Taiwanese airline China Airlines operates a bus service from LAX to Monterey Park and Rowland Heights. This service is only available for China Airlines customers.
FlyAway Bus.
The FlyAway Bus is a nonstop motorcoach/shuttle service run by the LAWA, which provides scheduled service between LAX and Downtown Los Angeles, the San Fernando Valley (Van Nuys), West Los Angeles (Westwood), Hollywood and Santa Monica. The Irvine FlyAway was discontinued on August 31, 2012. The shuttle service stops at every LAX terminal. The service hours vary based on the line. All lines use the regional system of High Occupancy Vehicle lanes to expedite their trips. The Los Angeles Union Station service, along with MTA Bus 40, are currently the only direct transport link between the airport and Downtown Los Angeles.
Metro Rail.
Shuttle bus "G" offers a free connection to and from the Aviation/LAX station on the Los Angeles Metro Rail Green Line. The line was originally intended to connect directly to the airport terminals, but budgetary restraints and opposition from local taxi and parking lot owners impeded its progress and won.
Airport Metro Connector.
In June 2014, the Los Angeles County Metropolitan Transportation Authority approved a $200 million Metro Rail infill station at Aviation Boulevard and 96th Street on the under construction Crenshaw/LAX Line to connect to an automated people mover (APM) system called the Airport Metro Connector, connecting terminals 1–8 to the light rail system. The people mover will have six stations: three serving the central terminal area, one serving a ground transportation hub, one serving the infill light rail station, and one serving a rental car hub, decreasing the need for shuttle bus services. Construction, estimated to cost $1.5 billion, is planned to start in early 2017 and to be completed by early 2024. Currently, shuttle bus "G" runs every 10–15 minutes (synched with the train schedule) from 5 am – 1:30 am.
Taxis and private shuttles.
Taxicab services are operated by nine city-authorized taxi companies and regulated by Authorized Taxicab Supervision Inc. (ATS). ATS maintains a taxicab holding lot under the 96th Street Bridge where, at peak periods, hundreds of cabs queue up to wait their turn to pull into the central terminal area to pick up passengers. A number of private shuttle companies also offer limousine and bus services to LAX Airport.
Coast Guard Air Station Los Angeles.
The airport also functions as a joint civil-military facility, providing a base for the United States Coast Guard and its Coast Guard Air Station Los Angeles facility, operating four HH-65 Dolphin helicopters, which covers Coast Guard operations in various Southern California locations, including Catalina Island. Missions include search and rescue (SAR), law enforcement, aids to navigation support (such as operating lighthouses) and various military operations. In addition, Coast Guard helicopters assigned to the air station deploy to Coast Guard cutters.
The Coast Guard is planning to close Coast Coast Guard Air Station Los Angeles and move its aircraft and personnel to Naval Air Station Point Mugu, part of Naval Base Ventura County in Oxnard, California, when the lease on the existing facility ends in 2016.
Flight Path Learning Center & Museum.
The Flight Path Learning Center is a museum located at 6661 Imperial Highway and was formerly known as the "West Imperial Terminal". This building used to house some charter flights (Condor Airlines) and regular scheduled flights by MGM Grand Air. It sat empty for 10 years until it was re-opened as a learning center for LAX.
The center contains information on the history of aviation, several pictures of the airport, as well as aircraft scale models, flight attendant uniforms, and general airline memorabilia such as playing cards, china, magazines, signs, even a TWA gate information sign. The museum also offers school tours and a guest speaker program.
The museum's library contains an extensive collection of rare items such as aircraft manufacturer company newsletters/magazines, technical manuals for both military and civilian aircraft, industry magazines dating back to World War II and before, historic photographs and other invaluable references on aircraft operation and manufacturing.
The museum has on display "The Spirit of Seventy-Six," which is a DC-3 (DC-3-262, Serial No. 3269). After being in commercial airline service, the plane served as a corporate aircraft for Union Oil Company for 32 years. The plane was built in the Douglas Aircraft Company plant in Santa Monica in January 1941, which was a major producer of both commercial and military aircraft.
The museum claims to be "the only aviation museum and research center situated at a major airport and the only facility with a primary emphasis on contributions of civil aviation to the history and development of Southern California". There are other museums at major airports, however, including the Udvar-Hazy Center of the National Air and Space Museum adjacent to Washington Dulles Airport, the Royal Thai Air Force Museum at Don Muang Airport, the Suomen ilmailumuseo (Finnish Aviation Museum) at Helsinki-Vantaa Airport, the Frontiers of Flight Museum at Dallas Love Field, the Tulsa Air and Space Museum & Planetarium at Tulsa International Airport and others.
Other facilities.
The airport has the administrative offices of Los Angeles World Airports.
Continental Airlines once had its corporate headquarters on the airport property. At a 1962 press conference in the office of Mayor of Los Angeles Sam Yorty, Continental Airlines announced that it planned to move its headquarters to Los Angeles in July 1963. In 1963 Continental Airlines headquarters moved to a two-story, $2.3 million building on the grounds of the airport. The July 2009 "Continental Magazine" issue stated that the move "underlined Continental Airlines western and Pacific orientation". On July 1, 1983 the airline's headquarters were relocated to the America Tower in the Neartown area of Houston.
In addition to Continental Airlines, Western Airlines and Flying Tiger Line also had their headquarters at LAX.
Accidents and incidents.
During its history there have been numerous incidents, but only the most notable are summarized below:
Planned modernization.
LAWA currently has several plans to modernize LAX. These include terminal and runway improvements, which will enhance the passenger experience, reduce overcrowding, and provide airport access to the latest class of very large passenger aircraft.
These improvements include:
LAWA is also planning to build and operate an automated people mover. This small train will include three stations in the central terminal area and three outside east of the terminals at a new intermodal transportation facility, connecting passengers between the central terminal area and the Metro Green Line, the future Metro Crenshaw Line, and regional and local bus lines and a consolidated car rental facility.
In popular culture.
Numerous films and television shows have been set or filmed partially at LAX, at least partly due to the airport's proximity to Hollywood studios. Film shoots at the Los Angeles airports, including LAX, produced $590 million for the Los Angeles region from 2002 to 2005.

</doc>
<doc id="18133" url="https://en.wikipedia.org/wiki?curid=18133" title="La Tène culture">
La Tène culture

The La Tène culture (; ) was a European Iron Age culture named after the archaeological site of La Tène on the north side of Lake Neuchâtel in Switzerland, where thousands of objects had been deposited in the lake, as was discovered after the water level dropped in 1857. La Tène is the type site and the term archaeologists use for the later period of the culture and art of the ancient Celts, a term that is firmly entrenched in the popular understanding, but presents numerous problems for historians and archaeologists. The culture became very widespread, and presents a wide variety of local differences. It is often distinguished from earlier and neighbouring cultures mainly by the La Tène style of Celtic art, characterized by curving "swirly" decoration, especially of metalwork.
La Tène culture developed and flourished during the late Iron Age (from about 500 BCE to the Roman conquest in the 1st century BCE) in Belgium, eastern France, Switzerland, Austria, Southern Germany, the Czech Republic, Poland, Slovakia, Slovenia, Croatia and parts of Hungary and Romania. The Celtiberians of western Iberia shared many aspects of the culture, though not generally the artistic style. To the north extended the contemporary Pre-Roman Iron Age of Northern Europe, including the Jastorf culture of Northern Germany.
La Tène culture developed out of the early Iron Age Hallstatt culture without any definite cultural break, under the impetus of considerable Mediterranean influence from the Greeks in pre-Roman Gaul, the Etruscans, and Golasecca culture. Barry Cunliffe notes localization of La Tène culture during the 5th century when there arose "two zones of power and innovation: a Marne – Moselle zone in the west with trading links to the Po Valley via the central Alpine passes and the Golasecca culture, and a Bohemian zone in the east with separate links to the Adriatic via the eastern Alpine routes and the Venetic culture". A shift of settlement centres took place in the 4th century.
La Tène cultural material appeared over a large area, including parts of Ireland and Great Britain, northern Spain, northern-central Italy, Burgundy, and Austria. Elaborate burials also reveal a wide network of trade. In Vix, France, an elite woman of the 6th century BCE was buried with a very large bronze "wine-mixer" made in Greece. Exports from La Tène cultural areas to the Mediterranean cultures were based on salt, tin and copper, amber, wool and leather, furs and gold. Although the La Tène culture had no writing of its own (rare examples of it using Greek inscriptions exist, and late Celtic coinage often uses Latin) there are several accounts of the culture and aspects of its history from classical authors, most very hostile and tending to stereotype.
History.
The preceding "Halstatt D" culture, of about 650-475, was also very widespread across Europe, and the transition over this area was gradual, and is mainly detected through La Tène style elite artefacts, which first appear in the western end of the old Hallstatt region. The establishment of a Greek colony, soon very successful, at Massalia (modern Marseilles) on the Mediterranean coast of France led to great trade with the Hallstatt areas up the Rhone and Saone river systems, and early La Tène elite burials like the Vix Grave in Burgundy contain imported luxury goods along with local production. Most areas were probably controlled by tribal chiefs living in hilltop forts, while the bulk of the population lived in small villages or farmsteads in the countryside.
By 500 the Etruscans expanded to border Celts in north Italy, and trade across the Alps began to overhaul trade with the Greeks, and the Rhone route declined. Booming areas included the middle Rhine, with large iron ore deposits, the Marne and Champagne regions, and also Bohemia, although here trade with the Mediterranean area was much less important. Trading connections and wealth no doubt played a part in the origin of the La Tène style, though how large a part remains much discussed; specific Mediterranean-derived motifs are evident, but the new style does not depend on them.
By about 400 the evidences for Mediterranean trade become few; this may have been because the expanding Celtic populations began to migrate south and west, coming into violent conflict with the established populations, including the Etruscans and Romans. The settled life in much of the La Tène homelands also seems to have become much more unstable and prone to wars. In about 387 the Celts under Brennus defeated the Romans and then sacked Rome, establishing themselves as the most prominent threats to the Roman homeland, a status they would retain through a series of Roman-Gallic wars until Julius Caesar's final conquest of Gaul in 58-50 BCE. The Romans prevented the Celts from reaching very far south of Rome, but on the other side of the Adriatic Sea groups passed through the Balkans to reach Greece, where Delphi was attacked in 279, and Asia, where Galatia was established as a Celtic area of Anatolia. By this time the La Tène style was spreading to the British Isles, though apparently without any significant movements in population.
After about 275 the relentless Roman expansion into the area occupied by La Tène culture began; it would never be complete, but lasted until the 1st century CE in Britain, leaving only the approximate areas of the modern Celtic nations (excluding Brittany) unoccupied. The Romans never attempted to invade Ireland and eventually decided that expansion into north Scotland was not worth the trouble, retreating from the line of the Antonine Wall to Hadrian's Wall in 162 CE.
La Tène "homeland".
Though there is no agreement on the precise region in which La Tène culture first developed, there is a broad consensus that the center of the culture lay on the northwest edges of Hallstatt culture, north of the Alps, within the region between in the West the valleys of the Marne and Moselle, and the part of the Rhineland nearby. In the east the western end of the old Hallstatt core area in modern Bavaria, Austria and Switzerland formed a somewhat separate "eastern style Province" in the early La Tène, joining with the western area in Alsace. In 1994 a prototypical ensemble of elite grave sites of the early 5th century BCE was excavated at Glauberg in Hesse, northeast of Frankfurt-am-Main, in a region that had formerly been considered peripheral to the La Tène sphere. The site at La Tène itself was therefore near the southern edge of the original "core" area (as is also the case for the Hallstatt site for its core).
From their homeland, La Tène culture expanded in the 4th century to more of modern France, Germany, and Central Europe, and beyond to Hispania, northern and central Italy, the Balkans, and even as far as Asia Minor, in the course of several major migrations. La Tène style artefacts start to appear in Britain around the same time, and Ireland rather later. The style of "Insular La Tène" art is somewhat different and the artefacts are initially found in some parts of the islands but not others. Migratory movements seem at best only partly responsible for the diffusion of La Tène culture there, and perhaps other parts of Europe.
Periodization.
Extensive contacts through trade are recognized in foreign objects deposited in elite burials; stylistic influences on La Tène material culture can be recognized in Etruscan, Italic, Greek, Dacian and Scythian sources. Dateable Greek pottery and analysis employing scientific techniques such as dendrochronology and thermoluminescence help provide date ranges for an absolute chronology at some La Tène sites.
As with many archaeological periods, La Tène history was originally divided into "early" (6th century BCE), "middle" (c. 450–100 BCE), and "late" (1st century BCE) stages, with the Roman occupation greatly disrupting the culture, although many elements remain in Gallo-Roman and Romano-British culture. A broad cultural unity was not paralleled by overarching social-political unifying structures, and the extent to which the material culture can be linguistically linked is debated. The art history of La Tène culture has various schemes of periodization.
Ethnology.
Our knowledge of this cultural area derives from three sources: from archaeological evidence, from Greek and Latin literary evidence, and more controversially, from ethnographical evidence suggesting some La Tène artistic and cultural survivals in traditionally Celtic regions of far western Europe. Some of the societies that are archaeologically identified with La Tène material culture were identified by Greek and Roman authors from the 5th century onwards as "Keltoi" ("Celts") and "Galli" ("Gauls"). Herodotus (iv.49) correctly placed "Keltoi" at the source of the Ister/Danube, in the heartland of La Tène material culture: "The Ister flows right across Europe, rising in the country of the Celts", whom however, apparently misunderstanding his source, he places "farthest to the west of any people of Europe"
Whether the usage of classical sources means that the whole of La Tène culture can be attributed to a unified Celtic people is difficult to assess; archaeologists have repeatedly concluded that language, material culture, and political affiliation do not necessarily run parallel. Frey notes (Frey 2004) that in the 5th century, "burial customs in the Celtic world were not uniform; rather, localised groups had their own beliefs, which, in consequence, also gave rise to distinct artistic expressions".
The spread of the Celtic languages before and during the period is also uncertain. In the 19th century it used to be thought that these only reached Ireland and Britain in the 1st millennium BCE, but it is now thought likely that they were dominant before the arrival of cultural styles associated with Celts, perhaps long before.
Material culture.
La Tène metalwork in bronze, iron and gold, developing technologically out of Hallstatt culture, is stylistically characterized by inscribed and inlaid intricate spirals and interlace, on fine bronze vessels, helmets and shields, horse trappings and elite jewelry, especially the neck rings called torcs and elaborate clasps called "fibulae". It is characterized by elegant, stylized curvilinear animal and vegetal forms, allied with the Hallstatt traditions of geometric patterning. The Early Style of La Tène art and culture mainly featured static, geometric decoration, while the transition to the Developed Style constituted a shift to movement-based forms, such as triskeles. Some subsets within the Developed Style contain more specific design trends, such as the recurrent serpentine scroll of the Waldalgesheim Style 
Initially La Tène people lived in open settlements that were dominated by the chieftains’ hill forts. The development of towns—"oppida"—appears in mid-La Tène culture. La Tène dwellings were carpenter-built rather than of masonry. La Tène peoples also dug ritual shafts, in which votive offerings and even human sacrifices were cast. Severed heads appear to have held great power and were often represented in carvings. Burial sites included weapons, carts, and both elite and household goods, evoking a strong continuity with an afterlife.
Site of La Tène.
La Tène is a village on the northern shore of Lake Neuchâtel, Switzerland, where the small river Thielle, connecting to another lake, enters the Lake Neuchâtel. It is an archaeological site and the eponymous type site for the late Iron Age La Tène culture. In 1857, prolonged drought lowered the waters of the lake by about 2 m. On the northernmost tip of the lake, between the river and a point south of the village of Marin-Epagnier, Hansli Kopp, looking for antiquities for Colonel Frédéric Schwab, discovered several rows of wooden piles that still reached up about 50 cm into the water. From among these, Kopp collected about forty iron swords.
The Swiss archaeologist Ferdinand Keller published his findings in 1868 in his influential first report on the Swiss pile dwellings ("Pfahlbaubericht"). In 1863 he interpreted the remains as a Celtic village built on piles. Eduard Desor, a geologist from Neuchâtel, started excavations on the lakeshore soon afterwards. He interpreted the site as an armory, erected on platforms on piles over the lake and later destroyed by enemy action. Another interpretation accounting for the presence of cast iron swords that had not been sharpened, was of a site for ritual depositions.
With the first systematic lowering of the Swiss lakes from 1868 to 1883, the site fell completely dry. In 1880, Emile Vouga, a teacher from Marin-Epagnier, uncovered the wooden remains of two bridges (designated "Pont Desor" and "Pont Vouga") originally over 100 m long, that crossed the little Thielle River (today a nature reserve) and the remains of five houses on the shore. After Vouga had finished, F. Borel, curator of the Marin museum, began to excavate as well. In 1885 the canton asked the Société d'Histoire of Neuchâtel to continue the excavations, the results of which were published by Vouga in the same year.
All in all, over 2500 objects, mainly made from metal, have been excavated in La Tène. Weapons predominate, there being 166 swords (most without traces of wear), 270 lanceheads, and 22 shield bosses, along with 385 brooches, tools, and parts of chariots. Numerous human and animal bones were found as well. The site was used from the 3rd century, with a peak of activity around 200 BCE and abandonment by about 60 BCE. Interpretations of the site vary. Some scholars believe the bridge was destroyed by high water, while others see it as a place of sacrifice after a successful battle (there are almost no female ornaments).
An exhibition marking the 150th anniversary of the discovery of the La Tène site opened in 2007 at the Musée Schwab in Bienne, Switzerland, moving to move to Zürich in 2008 and Mont Beuvray in Burgundy in 2009.
Sites.
Some sites are:
Artifacts.
See :Category:Celtic art.
Some outstanding La Tène artifacts are:

</doc>
<doc id="18135" url="https://en.wikipedia.org/wiki?curid=18135" title="Lorenz curve">
Lorenz curve

In economics, the Lorenz curve is a graphical representation of the distribution of income or of wealth. It was developed by Max O. Lorenz in 1905 for representing inequality of the wealth distribution.
The curve is a graph showing the proportion of overall income or wealth assumed by the bottom "x"% of the people, although this is not rigorously true for a finite population (see below). It is often used to represent income distribution, where it shows for the bottom "x"% of households, what percentage ("y"%) of the total income they have. The percentage of households is plotted on the "x"-axis, the percentage of income on the "y"-axis. It can also be used to show distribution of assets. In such use, many economists consider it to be a measure of social inequality.
The concept is useful in describing inequality among the size of individuals in ecology and in studies of biodiversity, where the cumulative proportion of species is plotted against the cumulative proportion of individuals. It is also useful in business modeling: e.g., in consumer finance, to measure the actual percentage "y"% of delinquencies attributable to the "x"% of people with worst risk scores.
Explanation.
Points on the Lorenz curve represent statements like "the bottom 20% of all households have 10% of the total income." 
A perfectly equal income distribution would be one in which every person has the same income. In this case, the bottom "N"% of society would always have "N"% of the income. This can be depicted by the straight line "y" = "x"; called the "line of perfect equality."
By contrast, a perfectly unequal distribution would be one in which one person has all the income and everyone else has none. In that case, the curve would be at "y" = 0% for all "x" < 100%, and "y" = 100% when "x" = 100%. This curve is called the "line of perfect inequality."
The Gini coefficient is the ratio of the area between the line of perfect equality and the observed Lorenz curve to the area between the line of perfect equality and the line of perfect inequality. The higher the coefficient, the more unequal the distribution is. In the diagram on the right, this is given by the ratio "A"/("A+B"), where "A" and "B" are the areas of regions as marked in the diagram.
Definition and calculation.
The Lorenz curve can usually be represented by a function "L"("F"), where "F", the cumulative portion of the population, is represented by the horizontal axis, and "L", the cumulative portion of the total wealth or income, is represented by the vertical axis.
For a population of size "n", with a sequence of values "y""i", "i" = 1 to "n", that are indexed in non-decreasing order ( "y""i" ≤ "y""i"+1), the Lorenz curve is the continuous piecewise linear function connecting the points ( "F""i", "L""i" ), "i" = 0 to "n", where "F"0 = 0, "L"0 = 0, and for "i" = 1 to "n":
Note that the statement that the Lorenz curve gives the portion of the wealth or income held by a given portion of the population is only strictly true at the points defined above, but not at the points on the line segments between these points. For instance, in a population of 10 households, it doesn't make sense to say that 45% of them earn a certain portion of the total. If the population is modeled as a continuum then this subtlety disappears.
For a discrete probability function "f"("y"), let "y""i", "i" = 1 to "n", be the points with non-zero probabilities indexed in increasing order ( "y""i" < "y""i"+1). The Lorenz curve is the continuous piecewise linear function connecting the points ( "F""i", "L""i" ), "i" = 0 to "n", where "F"0 = 0, "L"0 = 0, and for "i" = 1 to "n":
For a probability density function "f"("x") with the cumulative distribution function "F"("x"), the Lorenz curve "L"("x") is given by:
where formula_8 denotes the average. The Lorenz curve "L(F)" may then be plotted as a function parametric in x: "L(x)" vs. "F(x)".
Alternatively, for a cumulative distribution function "F"("x") with inverse "x"("F"), the Lorenz curve "L"("F") is directly given by:
The inverse "x"("F") may not exist because the cumulative distribution function has intervals of constant values. However, the previous formula can still apply by generalizing the definition of "x"("F"):
For an example of a Lorenz curve, see Pareto distribution.
Properties.
A Lorenz curve always starts at (0,0) and ends at (1,1).
The Lorenz curve is not defined if the mean of the probability distribution is zero or infinite.
The Lorenz curve for a probability distribution is a continuous function. However, Lorenz curves representing discontinuous functions can be constructed as the limit of Lorenz curves of probability distributions, the line of perfect inequality being an example.
The information in a Lorenz curve may be summarized by the Gini coefficient and the Lorenz asymmetry coefficient.
The Lorenz curve cannot rise above the line of perfect equality. If the variable being measured cannot take negative values, the Lorenz curve:
Note however that a Lorenz curve for net worth would start out by going negative due to the fact that some people have a negative net worth because of debt.
The Lorenz curve is invariant under positive scaling. If X is a random variable, for any positive number "c" the random variable "c" X has the same Lorenz curve as X.
The Lorenz curve is flipped twice, once about F = 0.5 and once about "L" = 0.5, by negation. If X is a random variable with Lorenz curve "L"X("F"), then −X has the Lorenz curve:
The Lorenz curve is changed by translations so that the equality gap "F" − "L"("F") changes in proportion to the ratio of the original and translated means. If X is a random variable with a Lorenz curve "L" X ("F") and mean "μ" X , then for any constant "c" ≠ −"μ" X , X + "c" has a Lorenz curve defined by:
For a cumulative distribution function "F"("x") with mean "μ" and (generalized) inverse "x"("F"), then for any "F" with 0 < "F" < 1 :

</doc>
<doc id="18136" url="https://en.wikipedia.org/wiki?curid=18136" title="Literate programming">
Literate programming

Literate programming is an approach to programming introduced by Donald Knuth in which a program is given as an explanation of the program logic in a natural language, such as English, interspersed with snippets of macros and traditional source code, from which a compilable source code can be generated.
The literate programming paradigm, as conceived by Knuth, represents a move away from writing programs in the manner and order imposed by the computer, and instead enables programmers to develop programs in the order demanded by the logic and flow of their thoughts. Literate programs are written as an uninterrupted exposition of logic in an ordinary human language, much like the text of an essay, in which macros are included to hide abstractions and traditional source code.
Literate programming tools are used to obtain two representations from a literate source file: one suitable for further compilation or execution by a computer, the "tangled" code, and another for viewing as formatted documentation, which is said to be "woven" from the literate source. While the first generation of literate programming tools were computer language-specific, the later ones are language-agnostic and exist above the programming languages.
Concept.
A literate program is an explanation of the program logic in a natural language, such as English, interspersed with snippets of macros and traditional source code. Macros in a literate source file are simply title-like or explanatory phrases in a human language that describe human abstractions created while solving the programming problem, and hiding chunks of code or lower-level macros. These macros are similar to the algorithms in pseudocode typically used in teaching computer science. These arbitrary explanatory phrases become precise new operators, created on the fly by the programmer, forming a "meta-language" on top of the underlying programming language.
A preprocessor is used to substitute arbitrary hierarchies, or rather "interconnected 'webs' of macros", to produce the compilable source code with one command ("tangle"), and documentation with another ("weave"). The preprocessor also provides an ability to write out the content of the macros and to add to already created macros in any place in the text of the literate program source file, thereby disposing of the need to keep in mind the restrictions imposed by traditional programming languages or to interrupt the flow of thought.
Advantages.
According to Knuth,
literate programming provides higher-quality programs, since it forces programmers to explicitly state the thoughts behind the program, making poorly thought-out design decisions more obvious. Knuth also claims that literate programming provides a first-rate documentation system, which is not an add-on, but is grown naturally in the process of exposition of one's thoughts during a program's creation. The resulting documentation allows authors to restart their own thought processes at any later time, and allows other programmers to understand the construction of the program more easily. This differs from traditional documentation, in which a programmer is presented with source code that follows a compiler-imposed order, and must decipher the thought process behind the program from the code and its associated comments. The meta-language capabilities of literate programming are also claimed to facilitate thinking, giving a higher "bird's eye view" of the code and increasing the number of concepts the mind can successfully retain and process. Applicability of the concept to programming on a large scale, that of commercial-grade programs, is proven by an edition of TeX code as a literate program.
Contrast with documentation generation.
Literate programming is very often misunderstood to refer only to formatted documentation produced from a common file with both source code and comments – which is properly called documentation generation – or to voluminous commentaries included with code. This is backwards: well-documented code or documentation extracted from code follows the structure of the code, with documentation embedded in the code; in literate programming code is embedded in documentation, with the code following the structure of the documentation.
This misconception has led to claims that comment-extraction tools, such as the Perl Plain Old Documentation or Java Javadoc systems, are "literate programming tools". However, because these tools do not implement the "web of abstract concepts" hiding behind the system of natural-language macros, or provide an ability to change the order of the source code from a machine-imposed sequence to one convenient to the human mind, they cannot properly be called literate programming tools in the sense intended by Knuth.
Example.
A classic example of literate programming is the literate implementation of the standard Unix codice_1 word counting program. Knuth presented a CWEB version of this example in Chapter 12 of his "Literate Programming" book. The same example was later rewritten for the noweb literate programming tool. This example provides a good illustration of the basic elements of literate programming.
The following snippet of the codice_1 literate program shows how arbitrary descriptive phrases in a natural language are used in a literate program to create macros, which act as new "operators" in the literate programming language, and hide chunks of code or other macros. The mark-up notation consists of double angle brackets ("codice_3") that indicate macros, the "codice_4" symbol which indicates the end of the code section in a noweb file. The "codice_5" symbol stands for the "root", topmost node the literate programming tool will start expanding the web of macros from. Actually, writing out the expanded source code can be done from any section or subsection (i.e. a piece of code designated as "codice_6", with the equal sign), so one literate program file can contain several files with machine source code.
The unraveling of the chunks can be done in any place in the literate program text file, not necessarily in the order they are sequenced in the enclosing chunk, but as is demanded by the logic reflected in the explanatory text that envelops the whole program.
Macros are not the same as "section names" in standard documentation. Literate programming macros can hide any chunk of code behind themselves, and be used inside any low-level machine language operators, often inside logical operators such as "codice_7", "codice_8" or "codice_9". This is illustrated by the following snippet of the codice_1 literate program.
In fact, macros can stand for any arbitrary chunk of code or other macros, and are thus more general than top-down or bottom-up "chunking", or than subsectioning. Knuth says that when he realized this, he began to think of a program as a "web" of various parts.
In a noweb literate program besides the free order of their exposition, the chunks behind macros, once introduced with "codice_11", can be grown later in any place in the file by simply writing "codice_6" and adding more content to it, as the following snippet illustrates ("plus" is added by the document formatter for readability, and is not in the code).
The documentation for a literate program is produced as part of writing the program. Instead of comments provided as side notes to source code a literate program contains the explanation of concepts on each level, with lower level concepts deferred to their appropriate place, which allows for better communication of thought. The snippets of the literate codice_1 above show how an explanation of the program and its source code are interwoven. Such exposition of ideas creates the flow of thought that is like a literary work. Knuth wrote a "novel" which explains the code of the computer strategy game Colossal Cave Adventure.
Tools.
The first published literate programming environment was WEB, introduced by Donald Knuth in 1981 for his TeX typesetting system; it uses Pascal as its underlying programming language and TeX for typesetting of the documentation. The complete commented TeX source code was published in Knuth's "TeX: The program", volume B of his 5-volume "Computers and Typesetting". Knuth had privately used a literate programming system called DOC as early as 1979. He was inspired by the ideas of Pierre-Arnoul de Marneffe. The free CWEB, written by Knuth and Silvio Levy, is WEB adapted for C and C++, runs on most operating systems and can produce TeX and PDF documentation.
There are various other implementations of the literate programming concept:

</doc>
<doc id="18137" url="https://en.wikipedia.org/wiki?curid=18137" title="Logistic map">
Logistic map

The logistic map is a polynomial mapping (equivalently, recurrence relation) of degree 2, often cited as an archetypal example of how complex, chaotic behaviour can arise from very simple non-linear dynamical equations. The map was popularized in a seminal 1976 paper by the biologist Robert May, in part as a discrete-time demographic model analogous to the logistic equation first created by Pierre François Verhulst.
Mathematically, the logistic map is written
where formula_2 is a number between zero and one that represents the ratio of existing population to the maximum possible population. The values of interest for the parameter "r" are those in the interval (0, 4].
This nonlinear difference equation is intended to capture two effects:
However, as a demographic model the logistic map has the pathological problem that some initial conditions and parameter values lead to negative population sizes. This problem does not appear in the older Ricker model, which also exhibits chaotic dynamics.
The formula_3 case of the logistic map is a nonlinear transformation of both the bit-shift map and the formula_4 case of the tent map.
Behavior dependent on "r".
The image below shows the amplitude and frequency content of some logistic map iterates for parameter values ranging from 2 to 4.
By varying the parameter "r", the following behavior is observed:
For any value of "r" there is at most one stable cycle. If a stable cycle exists, it is globally stable, attracting almost all points. Some values of "r" with a stable cycle of some period have infinitely many unstable cycles of various periods.
The bifurcation diagram at right summarizes this. The horizontal axis shows the possible values of the parameter "r" while the vertical axis shows the set of values of "x" visited asymptotically from almost all initial conditions by the iterates of the logistic equation with that "r" value.
[[Image:Logistic Bifurcation map High Resolution.png|thumb|right|Bifurcation diagram for the logistic map.
The attractor for any value of the parameter "r" is shown on the vertical line at that "r".]]
The bifurcation diagram is a self-similar: if you zoom in on the above-mentioned value "r" = 3.82843 and focus on one arm of the three, the situation nearby looks like a shrunk and slightly distorted version of the whole diagram. The same is true for all other non-chaotic points. This is an example of the deep and ubiquitous connection between chaos and fractals.
Chaos and the logistic map.
The relative simplicity of the logistic map makes it a widely used point of entry into a consideration of the concept of chaos. A rough description of chaos is that chaotic systems exhibit a great sensitivity to initial conditions—a property of the logistic map for most values of "r" between about 3.57 and 4 (as noted above). A common source of such sensitivity to initial conditions is that the map represents a repeated folding and stretching of the space on which it is defined. In the case of the logistic map, the quadratic difference equation (1) describing it may be thought of as a stretching-and-folding operation on the interval (0,1).
The following figure illustrates the stretching and folding over a sequence of iterates of the map. Figure (a), left, shows a two-dimensional Poincaré plot of the logistic map's state space for "r"=4, and clearly shows the quadratic curve of the difference equation (1). However, we can embed the same sequence in a three-dimensional state space, in order to investigate the deeper structure of the map. Figure (b), right, demonstrates this, showing how initially nearby points begin to diverge, particularly in those regions of "X""t" corresponding to the steeper sections of the plot.
This stretching-and-folding does not just produce a gradual divergence of the sequences of iterates, but an exponential divergence (see Lyapunov exponents), evidenced also by the complexity and unpredictability of the chaotic logistic map. In fact, exponential divergence of sequences of iterates explains the connection between chaos and unpredictability: a small error in the supposed initial state of the system will tend to correspond to a large error later in its evolution. Hence, predictions about future states become progressively (indeed, exponentially) worse when there are even very small errors in our knowledge of the initial state. This quality of unpredictability and apparent randomness led the logistic map equation to be used as a pseudo-random number generator in early computers.
Since the map is confined to an interval on the real number line, its dimension is less than or equal to unity. Numerical estimates yield a correlation dimension of 0.500 ± 0.005 (Grassberger, 1983), a Hausdorff dimension of about 0.538 (Grassberger 1981), and an information dimension of 0.5170976... (Grassberger 1983) for r=3.5699456... (onset of chaos). Note: It can be shown that the correlation dimension is certainly between 0.4926 and 0.5024.
It is often possible, however, to make precise and accurate statements about the "likelihood" of a future state in a chaotic system. If a (possibly chaotic) dynamical system has an attractor, then there exists a probability measure that gives the long-run proportion of time spent by the system in the various regions of the attractor. In the case of the logistic map with parameter  "r" = 4  and an initial state in (0,1), the attractor is also the interval (0,1) and the probability measure corresponds to the beta distribution with parameters  "a" = 0.5  and  "b" = 0.5. Specifically, the invariant measure is formula_14. Unpredictability is not randomness, but in some circumstances looks very much like it. Hence, and fortunately, even if we know very little about the initial state of the logistic map (or some other chaotic system), we can still say something about the distribution of states arbitrarily far into the future, and use this knowledge to inform decisions based on the state of the system.
Solution in some cases.
The special case of "r" = 4 can in fact be solved exactly, as can the case with "r" = 2; however the general case can only be predicted statistically. 
The solution when "r" = 4 is,
where the initial condition parameter formula_16 is given by formula_17. For rational formula_16, after a finite number of iterations formula_2 maps into a periodic sequence. But almost all formula_16 are irrational, and, for irrational formula_16, formula_2 never repeats itself – it is non-periodic. This solution equation clearly demonstrates the two key features of chaos – stretching and folding: the factor 2"n" shows the exponential growth of stretching, which results in sensitive dependence on initial conditions, while the squared sine function keeps formula_2 folded within the range [0, 1].
For "r" = 4 an equivalent solution in terms of complex numbers instead of trigonometric functions is
where formula_25 is either of the complex numbers
with modulus equal to 1. Just as the squared sine function in the trigonometric solution leads to neither shrinkage nor expansion of the set of points visited, in the latter solution this effect is accomplished by the unit modulus of formula_25.
By contrast, the solution when "r"=2 is
for formula_29. Since formula_30 for any value of formula_31 other than the unstable fixed point 0, the term formula_32 goes to 0 as "n" goes to infinity, so formula_2 goes to the stable fixed point formula_34
Finding cycles of any length when "r" = 4.
For the "r" = 4 case, from almost all initial conditions the iterate sequence is chaotic. Nevertheless, there exist an infinite number of initial conditions that lead to cycles, and indeed there exist cycles of length "k" for "all" integers "k" ≥ 1. We can exploit the relationship of the logistic map to the dyadic transformation (also known as the bit-shift map) to find cycles of any length. If "x" follows the logistic map formula_35 and "y" follows the dyadic transformation
then the two are related by
The reason that the dyadic transformation is also called the bit-shift map is that when "y" is written in binary notation, the map moves the binary point one place to the right (and if the bit to the left of the binary point has become a "1", this "1" is changed to a "0"). A cycle of length 3, for example, occurs if an iterate has a 3-bit repeating sequence in its binary expansion (which is not also a one-bit repeating sequence): 001, 010, 100, 110, 101, or 011. The iterate 001001001... maps into 010010010..., which maps into 100100100..., which in turn maps into the original 001001001...; so this is a 3-cycle of the bit shift map. And the other three binary-expansion repeating sequences give the 3-cycle 110110110... → 101101101... → 011011011... → 110110110... Either of these 3-cycles can be converted to fraction form: for example, the first-given 3-cycle can be written as 1/7 → 2/7 → 4/7 → 1/7. Using the above translation from the bit-shift map to the "r" = 4 logistic map gives the corresponding logistic cycle .611260467... → .950484434... → .188255099... → .611260467... . We could similarly translate the other bit-shift 3-cycle into its corresponding logistic cycle. Likewise, cycles of any length "k" can be found in the bit-shift map and then translated into the corresponding logistic cycles.
However, since almost all numbers in [0, 1) are irrational, almost all initial conditions of the bit-shift map lead to the non-periodicity of chaos. This is one way to see that the logistic "r" = 4 map is chaotic for almost all initial conditions.
The number of cycles of (minimal) length "k"=1, 2, 3, ... for the logistic map with "r" = 4 (tent map with formula_4) is a known integer sequence : 2, 1, 2, 3, 6, 9, 18, 30, 56, 99, 186, 335, 630, 1161 ... This tells us that the logistic map with "r" = 4 has 2 fixed points, 1 cycle of length 2, 2 cycles of length 3 and so on. This sequence takes a particularly simple form for prime "k": formula_39. For example: formula_40 is the number of cycles of length 13. Since this case of the logistic map is chaotic for almost all initial conditions, all of these finite-length cycles are unstable.

</doc>
<doc id="18138" url="https://en.wikipedia.org/wiki?curid=18138" title="Levant">
Levant

The Levant (; Arabic: المشرق ) is an approximate historical geographical term referring to a large area in the eastern Mediterranean. In its widest historical sense, the Levant included all of the eastern Mediterranean with its islands, that is, it included all of the countries along the eastern Mediterranean shores, extending from Greece to Cyrenaica. The term "Levant" entered English in the late 15th century from French. It derives from the Italian "Levante", meaning "rising", implying the rising of the sun in the east. As such, it is broadly equivalent to the Arabic term "Mashriq", 'the land where the sun rises'. The western counterpart in Arabic is the "Maghreb", and "Ponente" in Italian, meaning 'west, where the sun sets'.
In the 13th and 14th centuries, the term "levante" was used for Italian maritime commerce in the eastern Mediterranean, including Greece, Anatolia, Syria-Palestine, and Egypt, that is, the lands east of Venice. Eventually the term was restricted to the Muslim countries of Syria-Palestine and Egypt. In 1581 England set up the Levant Company to monopolize commerce with the Ottoman Empire.
The name "Levant States" was used to refer to the French mandate over Syria and Lebanon after World War I. This is probably the reason why the term "Levant" has come to be used synonymously with Syria-Palestine. Some scholars misunderstood the term thinking that it derives from the name of Lebanon. Today the term is typically used in conjunction with prehistoric or ancient historical references. It has the same meaning as Syria-Palestine or the region of Syria (Arabic: الشام ), that is, it means an area bounded by the Taurus Mountains of Turkey in the North, the Mediterranean Sea in the west, and the north Arabian Desert and Mesopotamia in the east. It does not include Anatolia (the former Asia Minor, now Asian Turkey; although at times Cilicia may be included), the Caucasus Mountains, or any part of the Arabian Peninsula proper. The Sinai Peninsula (Asian Egypt) is sometimes included, though more considered an intermediate, peripheral or marginal area forming a land bridge between the Levant and northern African Egypt.
The Levant has been described as the "crossroads of western Asia, the eastern Mediterranean and northeast Africa", and the "northwest of the Arabian plate".
Etymology.
The term "Levant", which appeared in English in 1497, originally meant the East in general or "Mediterranean lands east of Italy". It is borrowed from the French "levant" 'rising', referring to the rising of the sun in the east, or the point where the sun rises. The phrase is ultimately from the Latin word "levare," meaning 'lift, raise'. Similar etymologies are found in Greek Ἀνατολή ("Anatolē", "cf." Anatolia), in Germanic "Morgenland" (literally, "morning land"), in Italian (as in "Riviera di Levante", the portion of the Liguria coast east of Genoa), in Hungarian "Kelet", in Spanish and Catalan "Levante" and "Llevant", ("the place of rising"), and in Hebrew ("mizrah"). Most notably, "Orient" and its Latin source "oriens" meaning "east", is literally "rising", deriving from Latin "orior" "rise".
The notion of the Levant has undergone a dynamic process of historical evolution in usage, meaning, and understanding. While the term "Levantine" originally referred to the European residents of the eastern Mediterranean region, it later came to refer to regional "native" and "minority" groups.
The term became current in English in the 16th century, along with the first English merchant adventurers in the region; English ships appeared in the Mediterranean in the 1570s, and the English merchant company signed its agreement ("capitulations") with the Grand Turk in 1579 (Braudel). The English Levant Company was founded in 1581 to trade with the Ottoman Empire, and in 1670 the French Compagnie du Levant was founded for the same purpose. At this time, the Far East was known as the "Upper Levant".
In early 19th-century travel writing, the term sometimes incorporated certain Mediterranean provinces of the Ottoman empire, as well as independent Greece (and especially the Greek islands). In 19th-century archaeology, it referred to overlapping cultures in this region during and after prehistoric times, intending to reference the place instead of any one culture. The French mandate of Syria and Lebanon (1920–1946) was called the Levant states.
Geography and modern-day use of the term.
Today, "Levant" is the term typically used by archaeologists and historians with reference to the history of the region. Scholars have adopted the term Levant to identify the region due to it being a "wider, yet relevant, cultural corpus" that does not have the "political overtones" of Syria-Palestine. The term is also occasionally employed to refer to modern events, peoples, states or parts of states in the same region, namely Cyprus, Egypt, Iraq, Israel, Jordan, Lebanon, Palestine, Syria, and Turkey are sometimes considered Levant countries (compare with Near East, Middle East, Eastern Mediterranean and Western Asia). Several researchers include the island of Cyprus in Levantine studies, including the Council for British Research in the Levant, the UCLA Near Eastern Languages and Cultures department, "Journal of Levantine Studies" and the UCL Institute of Archaeology, the last of which has dated the connection between Cyprus and mainland Levant to the early Iron Age. Archaeologists seeking a neutral orientation that is neither biblical nor national have used terms such as Syro-Palestinian archaeology and archaeology of the southern Levant.
While the usage of the term "Levant" in academia has been relegated to the fields of archeology and literature, there is a recent attempt to reclaim the notion of the Levant as a category of analysis in political and social sciences. Two academic journals were recently launched: "Journal of Levantine Studies", published by the Van Leer Jerusalem Institute and "The Levantine Review", published by Boston College.
ISIL has adopted the term "Levant" within the English translation of their self-designation.
People, religion, and culture.
The populations of the Levant share not only the geographic position, but cuisine, some customs, and a very long history. The largest religious group in the Levant are the Muslims and the largest cultural-linguistic group are primarily Arab due to Arabization of the region over the centuries, but there are also many other groups.
The majority of Levantines are Sunni, Salafi, nondenominational or Shia Muslim. There are also Jews, Christians, Yazidi Kurds, Alawites, Nizari, Druze, and Ismailis.
Until the creation of the modern State of Israel in 1948, Jews lived throughout the Levant alongside Muslims and Christians; since then, almost all have been expelled from their homes and sought refuge in Israel.
There are many Levantine Christian groups such as Greek and Oriental Orthodox, Maronite, Roman Catholic, and Protestant. Armenians mostly belong to the Armenian Apostolic Church. There are Levantines or Franco-Levantines who are mostly Roman Catholic. There are also Circassians, Turks, Samaritans, and Nawars. There are Assyrian peoples belonging to the Assyrian Church of the East (autonomous) and the Chaldean Catholic Church (Catholic).
Language.
Most Levantine populations speak Levantine Arabic, also known as Mediterranean Arabic (شامي, Šāmī). In Israel, the primary language is Hebrew, while Arabic is also an official language (but de facto a minority language). In Cyprus, the majority language is Greek, followed by Turkish, and then a dialect of Levantine Arabic, Cypriot Maronite Arabic. Some communities and populations speak Aramaic, Greek, Armenian, Circassian, French, English, or other languages in addition to Levantine Arabic.
See also.
Overlapping regional designations
Sub-regional designations
Other

</doc>
<doc id="18139" url="https://en.wikipedia.org/wiki?curid=18139" title="League of Nations mandate">
League of Nations mandate

A League of Nations mandate was a legal status for certain territories transferred from the control of one country to another following World War I, or the legal instruments that contained the internationally agreed-upon terms for administering the territory on behalf of the League. These were of the nature of both a treaty and a constitution, which contained minority rights clauses that provided for the rights of petition and adjudication by the International Court. The mandate system was established under Article 22 of the Covenant of the League of Nations, entered into on 28 June 1919. With the dissolution of the League of Nations after World War II, it was stipulated at the Yalta Conference that the remaining Mandates should be placed under the trusteeship of the United Nations, subject to future discussions and formal agreements. Most of the remaining mandates of the League of Nations (with the exception of South-West Africa) thus eventually became United Nations Trust Territories. 
Basis.
The mandate system was established by Article 22 of the Covenant of the League of Nations, drafted by the victors of World War I. The article referred to territories which after the war were no longer ruled by their previous sovereign, but their peoples were not considered "able to stand by themselves under the strenuous conditions of the modern world". The article called for such people's tutelage to be "entrusted to advanced nations who by reason of their resources, their experience or their geographical position can best undertake this responsibility".
Generalities.
All of the territories subject to League of Nations mandates were previously controlled by states defeated in World War I, principally Imperial Germany and the Ottoman Empire. The mandates were fundamentally different from the protectorates in that the Mandatory power undertook obligations to the inhabitants of the territory and to the League of Nations.
The process of establishing the mandates consisted of two phases: 
Treaties.
The divestiture of Germany's overseas colonies, along with three territories disentangled from its European homeland area (the Free City of Danzig, Memel Territory, and Saar), was accomplished in the Treaty of Versailles (1919), with the territories being allotted among the Allies on May 7 of that year. Ottoman territorial claims were first addressed in the Treaty of Sèvres (1920) and finalized in the Treaty of Lausanne (1923). The Turkish territories were allotted among the Allied Powers at the San Remo conference in 1920.
Hidden agendas and objections.
Peace treaties have played an important role in the formation of the modern law of nations. Many rules that govern the relations between states have been introduced and codified in the terms of peace treaties. The first twenty-six articles of the Treaty of Versailles contained the Covenant of the League of Nations. It contained the international machinery for the enforcement of the terms of the treaty. Article 22 established a system of Mandates to administer former colonies and territories.
Legitimacy of the allocations.
Article 22 was written two months before the signing of the peace treaty, before it was known what communities, peoples, or territories were related to sub-paragraphs 4, 5, and 6. The treaty was signed, and the peace conference had been adjourned, before a formal decision was made. The mandates were arrangements guaranteed by, or arising out of the general treaty which stipulated that mandates were to be exercised on behalf of the League.
The treaty contained no provision for the mandates to be allocated on the basis of decisions taken by four members of the League acting in the name of the so-called "Principal Allied and Associated Powers". The decisions taken at the conferences of the Council of Four were not made on the basis of consultation or League unanimity as stipulated by the Covenant. As a result, the actions of the conferees were viewed by some as having no legitimacy.
In testimony before the Senate Committee on Foreign Relations a former US State Department official who had been a member of the American Commission at Paris, testified that the United Kingdom and France had simply gone ahead and arranged the world to suit themselves. He pointed out that the League of Nations could do nothing to alter their arrangements, since the League could only act by unanimous consent of its members - including the UK and France.
United States Secretary of State Robert Lansing was a member of the American Commission to Negotiate Peace at Paris in 1919. He explained that the system of mandates was a device created by the Great Powers to conceal their division of the spoils of war under the color of international law. If the former German and Ottoman territories had been ceded to the victorious powers directly, their economic value would have been credited to offset the Allies' claims for war reparations...The truth of this was very apparent at Paris. In the tentative distribution of mandates among the Powers, which took place on the strong presumption that the mandatory system would be adopted, the principal European Powers appeared to be willing and even eager to become mandatories over territories possessing natural resources which could be profitably developed and showed an unwillingness to accept mandates for territories which, barren of mineral or agricultural wealth, would be continuing liabilities rather than assets. This is not stated by way of criticism, but only in explanation of what took place.Project Gutenberg: The Peace Negotiations by Robert Lansing, Boston and New York: Houghton Mifflin Company. 1921, Chapter XIII 'THE SYSTEM OF MANDATES'</ref>
Article 243 of the treaty instructed the Reparations Commission that non-mandate areas of the Saar and Alsace-Lorraine were to be reckoned as credits to Germany in respect of its reparation obligations.
Legitimacy of the provisions.
Under the plan of the US Constitution the Congress was delegated the power to declare or define the Law of Nations in cases where its terms might be vague or indefinite. The US Senate refused to ratify the Covenant of the League of Nations. The legal issues surrounding the rule by force and the lack of self-determination under the system of mandates were cited by the Senators who withheld their consent. The US government subsequently entered into individual treaties to secure legal rights for its citizens, to protect property rights and businesses interests in the mandates, and to preclude the mandatory administration from altering the terms of the mandates without prior US approval.
The United States filed a formal protest because the preamble of the mandates indicated to the League that they had been approved by the Principal Allied and Associated Powers, when, in fact, that was not the case.
The Official Journal of the League of Nations, dated June 1922, contained a statement by Lord Balfour (UK) in which he explained that the League's authority was strictly limited. The article related that the 'Mandates were not the creation of the League, and they could not in substance be altered by the League. The League's duties were confined to seeing that the specific and detailed terms of the mandates were in accordance with the decisions taken by the Allied and Associated Powers, and that in carrying out these mandates the Mandatory Powers should be under the supervision—not under the control—of the League.'
Types of mandates.
The League of Nations decided the exact level of control by the Mandatory power over each mandate on an individual basis. However, in every case the Mandatory power was forbidden to construct fortifications or raise an army within the territory of the mandate, and was required to present an annual report on the territory to the League of Nations.
The mandates were divided into three distinct groups based upon the level of development each population had achieved at that time.
Class A mandates.
The first group, or "Class A mandates", were territories formerly controlled by the Ottoman Empire that were deemed to "... have reached a stage of development where their existence as independent nations can be provisionally recognized subject to the rendering of administrative advice and assistance by a Mandatory until such time as they are able to stand alone. The wishes of these communities must be a principal consideration in the selection of the Mandatory." 
The Class A mandates were:
Class B mandates.
The second group of mandates, or "Class B mandates", were all former "Schutzgebiete" (German territories) in West and Central Africa which were deemed to require a greater level of control by the mandatory power: "...the Mandatory must be responsible for the administration of the territory under conditions which will guarantee freedom of conscience and religion." The mandatory power was forbidden to construct military or naval bases within the mandates. 
The Class B mandates were:
Class C mandates.
The "Class C mandates", including South West Africa and certain of the South Pacific Islands, were considered to be "best administered under the laws of the Mandatory as integral portions of its territory"
The Class C mandates were former German possessions:
Rules of establishment.
According to the Council of the League of Nations, meeting of August 1920: "draft mandates adopted by the Allied and Associated Powers would not be definitive until they had been considered and approved by the League ... the legal title held by the mandatory Power must be a double one: one conferred by the Principal Powers and the other conferred by the League of Nations,"
Three steps were required to establish a Mandate under international law:
(1) The Principal Allied and Associated Powers confer a mandate on one of their number or on a third power; (2) the principal powers officially notify the council of the League of Nations that a certain power has been appointed mandatory for such a certain defined territory; and (3) the council of the League of Nations takes official cognisance of the appointment of the mandatory power and informs the latter that it council considers it as invested with the mandate, and at the same time notifies it of the terms of the mandate, after assertaining whether they are in conformance with the provisions of the covenant."
The U.S. State Department "Digest of International Law" says that the terms of the Treaty of Lausanne provided for the application of the principles of state succession to the "A" Mandates. The Treaty of Versailles (1920) provisionally recognized the former Ottoman communities as independent nations. It also required Germany to recognize the disposition of the former Ottoman territories and to recognize the new states laid down within their boundaries. The terms of the Treaty of Lausanne required the newly created states that acquired the territory detached from the Ottoman Empire to pay annuities on the Ottoman public debt and to assume responsibility for the administration of concessions that had been granted by the Ottomans. The treaty also let the States acquire, without payment, all the property and possessions of the Ottoman Empire situated within their territory. The treaty provided that the League of Nations was responsible for establishing an arbital court to resolve disputes that might arise and stipulated that its decisions were final.
A disagreement regarding the legal status and the portion of the annuities to be paid by the "A" mandates was settled when an Arbitrator ruled that some of the mandates contained more than one State:The difficulty arises here how one is to regard the Asiatic countries under the British and French mandates. Iraq is a Kingdom in regard to which Great Britain has undertaken responsibilities equivalent to those of a Mandatory Power. Under the British mandate, Palestine and Transjordan have each an entirely separate organisation. We are, therefore, in the presence of three States sufficiently separate to be considered as distinct Parties. France has received a single mandate from the Council of the League of Nations, but in the countries subject to that mandate, one can distinguish two distinct States: Syria and the Lebanon, each State possessing its own constitution and a nationality clearly different from the other.
Later history.
After the United Nations was founded in 1945 and the League of Nations was disbanded, all but one of the mandated territories that remained under the control of the mandatory power became United Nations trust territories, a roughly equivalent status. In each case, the colonial power that held the mandate on each territory became the administering power of the trusteeship, except that Japan, which had been defeated in World War II, lost its mandate over the South Pacific islands, which became a "strategic trust territory" known as the Trust Territory of the Pacific Islands under United States administration.
The sole exception to the transformation of League of Nations mandates into UN trusteeships was that South Africa refused to place South-West Africa under trusteeship. Instead, South Africa proposed that it be allowed to annex South-West Africa, a proposal rejected by the United Nations General Assembly. The International Court of Justice held that South Africa continued to have international obligations under the mandate for South-West Africa. The territory finally attained independence in 1990 as Namibia, after a long guerrilla war of independence against the apartheid regime.
Nearly all the former League of Nations mandates had become sovereign states by 1990, including all of the former United Nations Trust Territories with the exception of a few successor entities of the gradually dismembered Trust Territory of the Pacific Islands (formerly Japan's South Pacific Trust Mandate). These exceptions include the Northern Mariana Islands which is a commonwealth in political union with the United States with the status of unincorporated organized territory. The Northern Mariana Islands does elect its own governor to serve as territorial head of government, but it remains a U.S. territory with its head of state being the President of the United States and federal funds to the Commonwealth administered by the Office of Insular Affairs of the United States Department of the Interior.
Remnant Micronesia and the Marshall Islands, the heirs of the last territories of the Trust, attained final independence on 22 December 1990. (The UN Security Council ratified termination of trusteeship, effectively dissolving trusteeship status, on 10 July 1987). The Republic of Palau, split off from the Federated States of Micronesia, became the last to get its independence effectively on 1 October 1994.

</doc>
<doc id="18142" url="https://en.wikipedia.org/wiki?curid=18142" title="Loudon Classic">
Loudon Classic

The Loudon Classic, held at the New Hampshire Motor Speedway (formerly Bryar Motorsport Park and New Hampshire International Speedway) is the longest running motorcycle race in the United States, and is held every year on Father's Day. While it is popularly known as Laconia, the location of the race was moved from Belknap Recreation Area to Loudon in 1964.

</doc>
<doc id="18143" url="https://en.wikipedia.org/wiki?curid=18143" title="Lincoln, New Hampshire">
Lincoln, New Hampshire

Lincoln is a town in Grafton County, New Hampshire, United States. It is the second-largest town by area in New Hampshire. The population was 1,662 at the 2010 census. The town is home to the New Hampshire Highland Games and to a portion of Franconia Notch State Park. Set in the White Mountains, large portions of the town are within the White Mountain National Forest. The Appalachian Trail crosses in the northeast. Lincoln is the location of the Loon Mountain ski resort and associated recreation-centered development.
The primary settlement in town, where 993 people resided at the 2010 census, is defined as the Lincoln census-designated place (CDP) and is located along New Hampshire Route 112 east of Interstate 93. The town also includes the village of North Lincoln and the former village sites of Stillwater and Zealand (sometime known as Pullman) in the town's remote eastern and northern sections respectively, which are now within the White Mountain National Forest.
History.
In 1764, Colonial Governor Benning Wentworth granted to a group of approximately 70 land investors from Connecticut. Lincoln was named after Henry Fiennes Pelham-Clinton, 2nd Duke of Newcastle, 9th Earl of Lincoln – a cousin of the Wentworth governors. He held the position of comptroller of customs for the port of London under George II and George III, which was important to trade between America and England.
The town was settled about 1782. The 1790 census indicates that it had 22 inhabitants. Rocky soil yielded poor farming, but the area's abundant timber, combined with water power to run sawmills on the Pemigewasset River and its East Branch, helped Lincoln develop into a center for logging. By 1853, the Merrimack River Lumber Company was operating. The railroad transported freight, and increasingly brought tourists to the beautiful mountain region. In 1892, James E. Henry bought approximately of virgin timber and established a logging enterprise at what is today the center of Lincoln. In 1902, he built a pulp and paper mill. He erected The Lincoln House hotel in 1903, although a 1907 fire would nearly raze the community. Until he died in 1912, Henry controlled his company town, installing relatives in positions of civic authority.
In 1917, Henry's heirs sold the business to the Parker Young Company, which in turn sold it to the Marcalus Manufacturing Company in 1946. Franconia Paper took over in 1950, producing 150 tons of paper a day until bankruptcy in 1971, at which time new river classification standards discouraged further papermaking in Lincoln.
Tourism is today the principal business. Nearby Loon Mountain has long drawn skiers, and in recent years has attempted to convert itself into a four-season attraction. The Flume is one of the most visited attractions in the state. Discovered in 1808, it is a natural canyon extending at the base of Mount Liberty. Walls of Conway granite rise to a height of 70 to 90 feet (21 to 27 m) and are only 12 to 20 feet (2.5 to 6.0 m) apart.
Geography.
According to the United States Census Bureau, the town has a total area of , of which is land and is water, comprising 0.43% of the town. It is the second-largest town in area in New Hampshire, after Pittsburg.
Lincoln is drained by the Pemigewasset River and its East Branch. Lincoln lies almost fully within the Merrimack River watershed, with the western edge of town in the Connecticut River watershed. Kancamagus Pass, elevation , is on the Kancamagus Highway at the eastern boundary. The highest point in Lincoln is either the summit of Mount Carrigain, at above sea level, plus or minus , or the summit of Mount Bond at .
Demographics.
As of the census of 2000, there were 1,271 people, 583 households, and 324 families residing in the town. The population density was 9.7 people per square mile (3.8/km²). There were 2,339 housing units at an average density of 17.9 per square mile (6.9/km²). The racial makeup of the town was 97.40% White, 0.39% Native American, 0.87% Asian, 0.16% from other races, and 1.18% from two or more races. 0.71% of the population were Hispanic or Latino of any race.
There were 583 households out of which 22.8% had children under the age of 18 living with them, 43.4% were married couples living together, 8.9% had a female householder with no husband present, and 44.4% were non-families. 35.7% of all households were made up of individuals and 12.7% had someone living alone who was 65 years of age or older. The average household size was 2.18 and the average family size was 2.83.
In the town the population was spread out with 19.8% under the age of 18, 7.1% from 18 to 24, 27.9% from 25 to 44, 27.9% from 45 to 64, and 17.4% who were 65 years of age or older. The median age was 43 years. For every 100 females there were 99.2 males. For every 100 females age 18 and over, there were 96.7 males.
The median income for a household in the town was $28,523, and the median income for a family was $44,063. Males had a median income of $25,263 versus $22,784 for females. The per capita income for the town was $17,999. About 3.4% of families and 8.0% of the population were below the poverty line, including 5.3% of those under age 18 and 5.5% of those age 65 or over.

</doc>
<doc id="18145" url="https://en.wikipedia.org/wiki?curid=18145" title="List of laser applications">
List of laser applications

Many scientific, military, medical and commercial laser applications have been developed since the invention of the laser in 1958. The coherency, high monochromaticity, and ability to reach extremely high powers are all properties which allow for these specialized applications.
Scientific.
In science, lasers are used in many ways, including:
Lasers may also be indirectly used in spectroscopy as a micro-sampling system, a technique termed Laser ablation (LA), which is typically applied to ICP-MS apparatus resulting in the powerful LA-ICP-MS.
The principles of laser spectroscopy are discussed by Demtröder
and the use of tunable lasers in spectroscopy are described in Tunable Laser Applications.
Spectroscopy.
Most types of laser are an inherently pure source of light; they emit near-monochromatic light with a very well defined range of wavelengths. By careful design of the laser components, the purity of the laser light (measured as the "linewidth") can be improved more than the purity of any other light source. This makes the laser a very useful source for spectroscopy. The high intensity of light that can be achieved in a small, well collimated beam can also be used to induce a nonlinear optical effect in a sample, which makes techniques such as Raman spectroscopy possible. Other spectroscopic techniques based on lasers can be used to make extremely sensitive detectors of various molecules, able to measure molecular concentrations in the parts-per-1012 (ppt) level. Due to the high power densities achievable by lasers, beam-induced atomic emission is possible: this technique is termed Laser induced breakdown spectroscopy (LIBS).
Heat Treatment.
Heat treating with lasers allows selective surface hardening against wear with little or no distortion of the component. Because this eliminates much part reworking that is currently done, the laser system's capital cost is recovered in a short time. An inert, absorbent coating for laser heat treatment has also been developed that eliminates the fumes generated by conventional paint coatings during the heat-treating process with CO2 laser beams.
One consideration crucial to the success of a heat treatment operation is control of the laser beam irradiance on the part surface. The optimal irradiance distribution is driven by the thermodynamics of the laser-material interaction and by the part geometry.
Typically, irradiances between 500-5000 W/cm^2 satisfy the thermodynamic constraints and allow the rapid surface heating and minimal total heat input required. For general heat treatment, a uniform square or rectangular beam is one of the best options. For some special applications or applications where the heat treatment is done on an edge or corner of the part, it may be better to have the irradiance decrease near the edge to prevent melting.
Lunar laser ranging.
When the Apollo astronauts visited the moon, they planted retroreflector arrays to make possible the Lunar Laser Ranging Experiment. Laser beams are focused through large telescopes on Earth aimed toward the arrays, and the time taken for the beam to be reflected back to Earth measured to determine the distance between the Earth and Moon with high accuracy.
Photochemistry.
Some laser systems, through the process of mode locking, can produce extremely brief pulses of light - as short as picoseconds or femtoseconds (10−12 - 10−15 seconds). Such pulses can be used to initiate and analyze chemical reactions, a technique known as "photochemistry". The short pulses can be used to probe the process of the reaction at a very high temporal resolution, allowing the detection of short-lived intermediate molecules. This method is particularly useful in biochemistry, where it is used to analyse details of protein folding and function.
Laser scanner.
Laser barcode scanners are ideal for applications that require high speed reading of linear codes or stacked symbols.
Laser cooling.
A technique that has recent success is "laser cooling". This involves atom trapping, a method where a number of atoms are confined in a specially shaped arrangement of electric and magnetic fields. Shining particular wavelengths of light at the ions or atoms slows them down, thus "cooling" them. As this process is continued, they all are slowed and have the same energy level, forming an unusual arrangement of matter known as a Bose–Einstein condensate.
Nuclear fusion.
Some of the world's most powerful and complex arrangements of multiple lasers and optical amplifiers are used to produce extremely high intensity pulses of light of extremely short duration, e.g. laboratory for laser energetics, National Ignition Facility, GEKKO XII, Nike laser, Laser Mégajoule, HiPER. These pulses are arranged such that they impact pellets of tritium–deuterium simultaneously from all directions, hoping that the squeezing effect of the impacts will induce atomic fusion in the pellets. This technique, known as "inertial confinement fusion", so far has not been able to achieve "breakeven", that is, so far the fusion reaction generates less power than is used to power the lasers, but research continues.
Microscopy.
Confocal laser scanning microscopy and Two-photon excitation microscopy make use of lasers to obtain blur-free images of thick specimens at various depths. Laser capture microdissection use lasers to procure specific cell populations from a tissue section under microscopic visualization.
Additional laser microscopy techniques include harmonic microscopy, four-wave mixing microscopy and interferometric microscopy.
Military.
Military uses of lasers include applications such as target designation and ranging, defensive countermeasures, communications and directed energy weapons.
Directly as an energy weapon.
Directed energy weapons are being developed, such as Boeing's Airborne Laser which was constructed inside a Boeing 747. Designated the YAL-1, it was intended to kill short- and intermediate-range ballistic missiles in their boost phase.
Another example of direct use of a laser as a defensive weapon was researched for the Strategic Defense Initiative (SDI, nicknamed "Star Wars"), and its successor programs. This project would use ground-based or space-based laser systems to destroy incoming intercontinental ballistic missiles (ICBMs). The practical problems of using and aiming these systems were many; particularly the problem of destroying ICBMs at the most opportune moment, the "boost phase" just after launch. This would involve directing a laser through a large distance in the atmosphere, which, due to optical scattering and refraction, would bend and distort the laser beam, complicating the aiming of the laser and reducing its efficiency.
Another idea from the SDI project was the "nuclear-pumped X-ray laser". This was essentially an orbiting atomic bomb, surrounded by laser media in the form of glass rods; when the bomb exploded, the rods would be bombarded with highly-energetic gamma-ray photons, causing spontaneous and stimulated emission of X-ray photons in the atoms making up the rods. This would lead to optical amplification of the X-ray photons, producing an X-ray laser beam that would be minimally affected by atmospheric distortion and capable of destroying ICBMs in flight. The X-ray laser would be a strictly one-shot device, destroying itself on activation. Some initial tests of this concept were performed with underground nuclear testing; however, the results were not encouraging. Research into this approach to missile defense was discontinued after the SDI program was cancelled.
Defensive countermeasures.
Defensive countermeasure applications can range from compact, low power infrared countermeasures to high power, airborne laser systems. IR countermeasure systems use lasers to confuse the seeker heads on infrared homing missiles.
Disorientation.
Some weapons simply use a laser to disorient a person. One such weapon is the Thales Green Laser Optical Warner.
Guidance.
Laser guidance is a technique of guiding a missile or other projectile or vehicle to a target by means of a laser beam.
Targeting.
Target designator.
Another military use of lasers is as a "laser target designator". This is a low-power laser pointer used to indicate a target for a precision-guided munition, typically launched from an aircraft. The guided munition adjusts its flight-path to home in to the laser light reflected by the target, enabling a great precision in aiming. The beam of the laser target designator is set to a pulse rate that matches that set on the guided munition to ensure munitions strike their designated targets and do not follow other laser beams which may be in use in the area. The laser designator can be shone onto the target by an aircraft or nearby infantry. Lasers used for this purpose are usually infrared lasers, so the enemy cannot easily detect the guiding laser light.
Firearms.
Laser sight.
The laser has in most firearms applications been used as a tool to enhance the targeting of other weapon systems. For example, a "laser sight" is a small, usually visible-light laser placed on a handgun or a rifle and aligned to emit a beam parallel to the barrel. Since a laser beam has low divergence, the laser light appears as a small spot even at long distances; the user places the spot on the desired target and the barrel of the gun is aligned (but not necessarily allowing for bullet drop, windage, distance between the direction of the beam and the axis of the barrel, and the target mobility while the bullet travels).
Most laser sights use a red laser diode. Others use an infrared diode to produce a dot invisible to the naked human eye but detectable with night vision devices. The firearms adaptive target acquisition module LLM01 laser light module combines visible and infrared laser diodes. In the late 1990s, green diode pumped solid state laser (DPSS) laser sights (532 nm) became available. Modern laser sights are small and light enough for attachment to the firearms.
In 2007, LaserMax, a company specializing in manufacturing lasers for military and police firearms, introduced the first mass-production green laser available for small arms. This laser mounts to the underside of a handgun or long arm on the accessory rail. The green laser is supposed to be more visible than the red laser in bright lighting conditions because, for the same wattage, green light appears brighter than red light.
Eye-targeted lasers.
A non-lethal laser weapon was developed by the U.S. Air Force to temporarily impair an adversary's ability to fire a weapon or to otherwise threaten enemy forces. This unit illuminates an opponent with harmless low-power laser light and can have the effect of dazzling or disorienting the subject or causing them to flee. Several types of dazzlers are now available, and some have been used in combat.
There remains the possibility of using lasers to blind, since this requires such lower power levels, and is easily achievable in a man-portable unit. However, most nations regard the deliberate permanent blinding of the enemy as forbidden by the rules of war (see Protocol on Blinding Laser Weapons). Although several nations have developed blinding laser weapons, such as China's ZM-87, none of these are believed to have made it past the prototype stage.
In addition to the applications that crossover with military applications, a widely known law enforcement use of lasers is for lidar to measure the speed of vehicles.
Holographic weapon sight.
A holographic weapon sight uses a laser diode to illuminate a hologram of a reticle built into a flat glass optical window of the sight. The user looks through the optical window and sees a cross hair reticle image superimposed at a distance on the field of view.
Industrial and commercial.
Industrial laser applications can be divided into two categories depending on the power of the laser: material processing and micro-material processing.
In material processing, lasers with average optical power above 1 kilowatt are used mainly for industrial materials processing applications. Beyond this power threshold there are thermal issues related to the optics that separate these lasers from their lower-power counterparts. Laser systems in the 50-300W range are used primarily for pumping, plastic welding and soldering applications. Lasers above 300W are used in brazing, thin metal welding, and sheet metal cutting applications. The required brightness (as measured in by the beam parameter product) is higher for cutting applications than for brazing and thin metal welding. High power applications, such as hardening, cladding, and deep penetrating welding, require multiple kW of optical power, and are used in a broad range of industrial processes.
Micro material processing is a category that includes all laser material processing applications under 1 kilowatt. The use of lasers in Micro Materials Processing has found broad application in the development and manufacturing of screens for smartphones, tablet computers, and LED TVs.
A detailed list of industrial and commercial laser applications includes:
Surveying and ranging.
In surveying and construction, the laser level is affixed to a tripod, leveled and then spun to illuminate a horizontal plane. The laser beam projector employs a rotating head with a mirror for sweeping the laser beam about a vertical axis. If the mirror is not self-leveling, it is provided with visually readable level vials and manually adjustable screws for orienting the projector. A staff carried by the operator is equipped with a movable sensor, which can detect the laser beam and gives a signal when the sensor is in line with the beam (usually an audible beep). The position of the sensor on the graduated staff allows comparison of elevations between different points on the terrain.
A tower-mounted laser level is used in combination with a sensor on a wheel tractor-scraper in the process of land laser leveling to bring land (for example, an agricultural field) to near-flatness with a slight grade for drainage. The laser line level was invented in 1996 by Steve J. Orosz, Jr. This type of level does not require a heavy motor to create the illusion of a line from a dot, rather, it uses a lens to transform the dot into a line.
Bird deterrent.
Laser beams are used to disperse birds from agricultural land, industrial sites, rooftops and from airport runways. Birds tend to perceive the laser beam as a physical stick. By moving the laser beam towards the birds, they get scared and fly away. On the market are manual operated laser torches or automated robots to move the laser beam automatically. 

</doc>
<doc id="18148" url="https://en.wikipedia.org/wiki?curid=18148" title="Left-arm orthodox spin">
Left-arm orthodox spin

Left-arm orthodox spin also known as Slow Left Arm Orthodox is a type of Left Arm Finger Leg spin bowling in the sport of cricket.
Left-arm orthodox spin is bowled by a left-arm bowler using the fingers to spin the ball from right to left of the cricket pitch (from the bowler's perspective).
Left arm orthodox spin bowlers generally attempt to drift the ball in the air into a right-handed batsman, and then turn it away from the batsman (towards off-stump) upon landing on the pitch. The drift and turn in the air are attacking techniques. The left-arm orthodox spin like an off break or off spin is also a bowling action.
The major variations of a left-arm spinner are the topspinner (which turns less and bounces higher in the cricket pitch), the arm ball (which does not turn at all, drifts into a right-handed batsman in the direction of the bowler's arm movement; also called a 'floater') and the left-arm spinner's version of a doosra (which turns the other way). The left-arm unorthodox spin like a leg break or leg spin is also a bowling action.
Notes.
ggSDerxg

</doc>
<doc id="18151" url="https://en.wikipedia.org/wiki?curid=18151" title="Laser construction">
Laser construction

A laser is constructed from three principal parts:
Pump source.
The "pump source" is the part that provides energy to the laser system. Examples of pump sources include electrical discharges, flashlamps, arc lamps, light from another laser, chemical reactions and even explosive devices. The type of pump source used principally depends on the "gain medium", and this also determines how the energy is transmitted to the medium. A helium–neon (HeNe) laser uses an electrical discharge in the helium-neon gas mixture, a Nd:YAG laser uses either light focused from a xenon flash lamp or diode lasers, and excimer lasers use a chemical reaction.
Gain medium / Laser medium.
The "gain medium" is the major determining factor of the wavelength of operation, and other properties, of the laser. "Gain media" in different materials have linear spectra or wide spectra. "Gain media" with wide spectra allow tuning of the laser frequency. There are hundreds if not thousands of different gain media in which laser operation has been achieved (see list of laser types for a list of the most important ones). The gain medium is excited by the pump source to produce a population inversion, and it is in the gain medium that spontaneous and stimulated emission of photons takes place, leading to the phenomenon of optical gain, or amplification.
Examples of different gain media include:
Optical resonator.
The "optical resonator", or "optical cavity", in its simplest form is two parallel mirrors placed around the gain medium which provide feedback of the light. The mirrors are given optical coatings which determine their reflective properties. Typically one will be a high reflector, and the other will be a partial reflector. The latter is called the output coupler, because it allows some of the light to leave the cavity to produce the laser's output beam.
Light from the medium, produced by spontaneous emission, is reflected by the mirrors back into the medium, where it may be amplified by stimulated emission. The light may reflect from the mirrors and thus pass through the gain medium many hundreds of times before exiting the cavity. In more complex lasers, configurations with four or more mirrors forming the cavity are used. The design and alignment of the mirrors with respect to the medium is crucial for determining the exact operating wavelength and other attributes of the laser system.
Other optical devices, such as spinning mirrors, modulators, filters, and absorbers, may be placed within the optical resonator to produce a variety of effects on the laser output, such as altering the wavelength of operation or the production of pulses of laser light.
Some lasers do not use an optical cavity, but instead rely on very high optical gain to produce significant amplified spontaneous emission (ASE) without needing feedback of the light back into the gain medium. Such lasers are said to be superluminescent, and emit light with low coherence but high bandwidth. Since they do not use optical feedback, these devices are often not categorized as lasers.

</doc>
<doc id="18152" url="https://en.wikipedia.org/wiki?curid=18152" title="Logical conjunction">
Logical conjunction

In logic and mathematics, and is the truth-functional operator of logical conjunction; the "and" of a set of operands is true if and only if "all" of its operands are true. The logical connective that represents this operator is typically written as or . 
""A" and "B"" is true only if "A" is true and "B" is true. 
An operand of a conjunction is a conjunct.
Related concepts in other fields are:
Notation.
And is usually denoted by an infix operator: in mathematics and logic, ' or '; in electronics, ; and in programming languages, codice_1, codice_2, or codice_3. In Jan Łukasiewicz's prefix notation for logic, the operator is K, for Polish "koniunkcja".
Definition.
Logical conjunction is an operation on two logical values, typically the values of two propositions, that produces a value of "true" if and only if both of its operands are true.
The conjunctive identity is 1, which is to say that AND-ing an expression with 1 will never change the value of the expression. In keeping with the concept of vacuous truth, when conjunction is defined as an operator or function of arbitrary arity, the empty conjunction (AND-ing over an empty set of operands) is often defined as having the result 1.
Truth table.
The truth table of formula_1:
Introduction and elimination rules.
As a rule of inference, conjunction introduction is a classically valid, simple argument form. The argument form has two premises, "A" and "B". Intuitively, it permits the inference of their conjunction.
or in logical operator notation:
Here is an example of an argument that fits the form "conjunction introduction":
Conjunction elimination is another classically valid, simple argument form. Intuitively, it permits the inference from any conjunction of either element of that conjunction.
...or alternately,
In logical operator notation:
...or alternately,
Properties.
commutativity: yes
associativity: yes
distributivity: with various operations, especially with "or"
idempotency: yes<br>
monotonicity: yes
truth-preserving: yes<br>
When all inputs are true, the output is true.
falsehood-preserving: yes<br>
When all inputs are false, the output is false.
Walsh spectrum: (1,-1,-1,1)
Nonlinearity: 1 (the function is bent)
If using binary values for true (1) and false (0), then "logical conjunction" works exactly like normal arithmetic multiplication.
Applications in computer engineering.
In high-level computer programming and digital electronics, logical conjunction is commonly represented by an infix operator, usually as a keyword such as "codice_4", an algebraic multiplication, or the ampersand symbol "codice_1". Many languages also provide short-circuit control structures corresponding to logical conjunction.
Logical conjunction is often used for bitwise operations, where codice_6 corresponds to false and codice_7 to true:
The operation can also be applied to two binary words viewed as bitstrings of equal length, by taking the bitwise AND of each pair of bits at corresponding positions. For example:
This can be used to select part of a bitstring using a bit mask. For example, codice_18  =  codice_19 extracts the fifth bit of an 8-bit bitstring.
In computer networking, bit masks are used to derive the network address of a subnet within an existing network from a given IP address, by ANDing the IP address and the subnet mask.
Logical conjunction "codice_4" is also used in SQL operations to form database queries.
The Curry–Howard correspondence relates logical conjunction to product types.
Set-theoretic correspondence.
The membership of an element of an intersection set in set theory is defined in terms of a logical conjunction: "x" ∈ "A" ∩ "B" if and only if ("x" ∈ "A") ∧ ("x" ∈ "B"). Through this correspondence, set-theoretic intersection shares several properties with logical conjunction, such as associativity, commutativity, and idempotence.
Natural language.
As with other notions formalized in mathematical logic, the logical conjunction "and" is related to, but not the same as, the grammatical conjunction "and" in natural languages.
English "and" has properties not captured by logical conjunction. For example, "and" sometimes implies order. For example, "They got married and had a child" in common discourse means that the marriage came before the child. The word "and" can also imply a partition of a thing into parts, as "The American flag is red, white, and blue." Here it is not meant that the flag is "at once" red, white, and blue, but rather that it has a part of each color.

</doc>
<doc id="18153" url="https://en.wikipedia.org/wiki?curid=18153" title="Logical connective">
Logical connective

In logic, a logical connective (also called a logical operator) is a symbol or word used to connect two or more sentences (of either a formal or a natural language) in a grammatically valid way, such that the sense of the compound sentence produced depends only on the original sentences.
The most common logical connectives are binary connectives (also called dyadic connectives) which join two sentences which can be thought of as the function's operands. Also commonly, negation is considered to be a unary connective.
Logical connectives along with quantifiers are the two main types of logical constants used in formal systems such as propositional logic and predicate logic. Semantics of a logical connective is often, but not always, presented as a truth function.
A logical connective is similar to but not equivalent to a conditional operator. 
In language.
Natural language.
In the grammar of natural languages two sentences may be joined by a grammatical conjunction to form a "grammatically" compound sentence. Some but not all such grammatical conjunctions are truth functions. For example, consider the following sentences:
The words "and" and "so" are "grammatical" conjunctions joining the sentences (A) and (B) to form the compound sentences (C) and (D). The "and" in (C) is a "logical" connective, since the truth of (C) is completely determined by (A) and (B): it would make no sense to affirm (A) and (B) but deny (C). However, "so" in (D) is not a logical connective, since it would be quite reasonable to affirm (A) and (B) but deny (D): perhaps, after all, Jill went up the hill to fetch a pail of water, not because Jack had gone up the hill at all.
Various English words and word pairs express logical connectives, and some of them are synonymous. Examples (with the name of the relationship in parentheses) are:
The word "not" (negation) and the phrases "it is false that" (negation) and "it is not the case that" (negation) also express a logical connective – even though they are applied to a single statement, and do not connect two statements.
Formal languages.
In formal languages, truth functions are represented by unambiguous symbols. These symbols are called "logical connectives", "logical operators", "propositional operators", or, in classical logic, "truth-functional connectives". See well-formed formula for the rules which allow new well-formed formulas to be constructed by joining other well-formed formulas using truth-functional connectives.
Logical connectives can be used to link more than two statements, so one can speak about "-ary logical connective".
Common logical connectives.
List of common logical connectives.
Commonly used logical connectives include
Alternative names for biconditional are "iff", "xnor" and "bi-implication".
For example, the meaning of the statements "it is raining" and "I am indoors" is transformed when the two are combined with logical connectives. For statement "P" = "It is raining" and "Q" = "I am indoors":
It is also common to consider the "always true" formula and the "always false" formula to be connective:
History of notations.
Some authors used letters for connectives at some time of the history: u. for conjunction (German's "und" for "and") and o. for disjunction (German's "oder" for "or") in earlier works by Hilbert (1904); N"p" for negation, K"pq" for conjunction, D"pq" for alternative denial, A"pq" for disjunction, X"pq" for joint denial, C"pq" for implication, E"pq" for biconditional in Łukasiewicz (1929); cf. Polish notation.
Redundancy.
Such a logical connective as converse implication "←" is actually the same as material conditional with swapped arguments; thus, the symbol for converse implication is redundant. In some logical calculi (notably, in classical logic) certain essentially different compound statements are logically equivalent. A less trivial example of a redundancy is the classical equivalence between and . Therefore, a classical-based logical system does not need the conditional operator "→" if "¬" (not) and "∨" (or) are already in use, or may use the "→" only as a syntactic sugar for a compound having one negation and one disjunction.
There are sixteen Boolean functions associating the input truth values and with four-digit binary outputs. These correspond to possible choices of binary logical connectives for classical logic. Different implementations of classical logic can choose different functionally complete subsets of connectives.
One approach is to choose a "minimal" set, and define other connectives by some logical form, as in the example with the material conditional above.
The following are the minimal functionally complete sets of operators in classical logic whose arities do not exceed 2:
See more details about functional completeness in classical logic at Functional completeness in truth function.
Another approach is to use with equal rights connectives of a certain convenient and functionally complete, but "not minimal" set. This approach requires more propositional axioms, and each equivalence between logical forms must be either an axiom or provable as a theorem.
The situation, however, is more complicated in intuitionistic logic. Of its five connectives, {∧, ∨, →, ¬, ⊥}, only negation "¬" can be reduced to other connectives (see details). Neither conjunction, disjunction, nor material conditional has an equivalent form constructed of the other four logical connectives.
Properties.
Some logical connectives possess properties which may be expressed in the theorems containing the connective. Some of those properties that a logical connective may have are:
For classical and intuitionistic logic, the "=" symbol means that corresponding implications "…→…" and "…←…" for logical compounds can be both proved as theorems, and the "≤" symbol means that "…→…" for logical compounds is a consequence of corresponding "…→…" connectives for propositional variables. Some many-valued logics may have incompatible definitions of equivalence and order (entailment).
Both conjunction and disjunction are associative, commutative and idempotent in classical logic, most varieties of many-valued logic and intuitionistic logic. The same is true about distributivity of conjunction over disjunction and disjunction over conjunction, as well as for the absorption law.
In classical logic and some varieties of many-valued logic, conjunction and disjunction are dual, and negation is self-dual, the latter is also self-dual in intuitionistic logic. 
Order of precedence.
As a way of reducing the number of necessary parentheses, one may introduce precedence rules: formula_63 has higher precedence than formula_1, formula_1 higher than formula_18, and formula_18 higher than formula_3. So for example, formula_84 is short for formula_85.
Here is a table that shows a commonly used precedence of logical operators.
However, not all authors use the same order; for instance, an ordering in which disjunction is lower precedence than implication or bi-implication has also been used. Sometimes precedence between conjunction and disjunction is unspecified requiring to provide it explicitly in given formula with parentheses. The order of precedence determines which connective is the "main connective" when interpreting a non-atomic formula.
Computer science.
A truth-functional approach to logical operators is implemented as logic gates in digital circuits. Practically all digital circuits (the major exception is DRAM) are built up from NAND, NOR, NOT, and transmission gates; see more details in Truth function in computer science. Logical operators over bit vectors (corresponding to finite Boolean algebras) are bitwise operations.
But not every usage of a logical connective in computer programming has a Boolean semantic. For example, lazy evaluation is sometimes implemented for and , so these connectives are not commutative if some of expressions , has side effects. Also, a conditional, which in some sense corresponds to the material conditional connective, is essentially non-Boolean because for codice_1 the consequent Q is not executed if the antecedent P is false (although a compound as a whole is successful ≈ "true" in such case). This is closer to intuitionist and constructivist views on the material conditional, rather than to classical logic's ones.

</doc>
<doc id="18154" url="https://en.wikipedia.org/wiki?curid=18154" title="Propositional calculus">
Propositional calculus

Propositional calculus (also called propositional logic, sentential calculus, or sentential logic) is the branch of mathematical logic concerned with the study of propositions (whether they are true or false) that are formed by other propositions with the use of logical connectives, and how their value depends on the truth value of their components. Logical connectives are found in natural languages. In English for example, some examples are "and" (conjunction), "or" (disjunction), "not” (negation) and "if" (but only when used to denote material conditional).
The following is an example of a very simple inference within the scope of propositional logic:
Both premises and the conclusion are propositions. The premises are taken for granted and then with the application of modus ponens (an inference rule) the conclusion follows.
As propositional logic is not concerned with the structure of propositions beyond the point where they can't be decomposed anymore by logical connectives, this inference can be restated replacing those "atomic" statements with statement letters, which are interpreted as variables representing statements:
The same can be stated succinctly in the following way:
When is interpreted as “It's raining” and as “it's cloudy” the above symbolic expressions can be seen to exactly correspond with the original expression in natural language. Not only that, but they will also correspond with any other inference of this "form", which will be valid on the same basis that this inference is.
Propositional logic may be studied through a formal system in which formulas of a formal language may be interpreted to represent propositions. A system of inference rules and axioms allows certain formulas to be derived. These derived formulas are called theorems and may be interpreted to be true propositions. A constructed sequence of such formulas is known as a "derivation" or "proof" and the last formula of the sequence is the theorem. The derivation may be interpreted as proof of the proposition represented by the theorem.
When a formal system is used to represent formal logic, only statement letters are represented directly. The natural language propositions that arise when they're interpreted are outside the scope of the system, and the relation between the formal system and its interpretation is likewise outside the formal system itself.
Usually in truth-functional propositional logic, formulas are interpreted as having either a truth value of "true" or a truth value of "false". Truth-functional propositional logic and systems isomorphic to it, are considered to be zeroth-order logic.
History.
Although propositional logic (which is interchangeable with propositional calculus) had been hinted by earlier philosophers, it was developed into a formal logic by Chrysippus in the 3rd century BC and expanded by the Stoics. The logic was focused on propositions. This advancement was different from the traditional syllogistic logic which was focused on terms. However, later in antiquity, the propositional logic developed by the Stoics was no longer understood . Consequently, the system was essentially reinvented by Peter Abelard in the 12th century.
Propositional logic was eventually refined using symbolic logic. The 17th/18th-century mathematician Gottfried Leibniz has been credited with being the founder of symbolic logic for his work with the calculus ratiocinator. Although his work was the first of its kind, it was unknown to the larger logical community. Consequently, many of the advances achieved by Leibniz were reachieved by logicians like George Boole and Augustus De Morgan completely independent of Leibniz.
Just as propositional logic can be considered an advancement from the earlier syllogistic logic, Gottlob Frege's predicate logic was an advancement from the earlier propositional logic. One author describes predicate logic as combining "the distinctive features of syllogistic logic and propositional logic." Consequently, predicate logic ushered in a new era in logic's history; however, advances in propositional logic were still made after Frege, including Natural Deduction, Truth-Trees and Truth-Tables. Natural deduction was invented by Gerhard Gentzen and Jan Łukasiewicz. Truth-Trees were invented by Evert Willem Beth. The invention of truth-tables, however, is of controversial attribution.
Within works by Frege and Bertrand Russell, one finds ideas influential in bringing about the notion of truth tables. The actual 'tabular structure' (being formatted as a table), itself, is generally credited to either Ludwig Wittgenstein or Emil Post (or both, independently). Besides Frege and Russell, others credited with having ideas preceding truth-tables include Philo, Boole, Charles Sanders Peirce, and Ernst Schröder. Others credited with the tabular structure include Łukasiewicz, Schröder, Alfred North Whitehead, William Stanley Jevons, John Venn, and Clarence Irving Lewis. Ultimately, some have concluded, like John Shosky, that "It is far from clear that any one person should be given the title of 'inventor' of truth-tables.".
Terminology.
In general terms, a calculus is a formal system that consists of a set of syntactic expressions ("well-formed formulas"), a distinguished subset of these expressions (axioms), plus a set of formal rules that define a specific binary relation, intended to be interpreted to be logical equivalence, on the space of expressions.
When the formal system is intended to be a logical system, the expressions are meant to be interpreted to be statements, and the rules, known to be "inference rules", are typically intended to be truth-preserving. In this setting, the rules (which may include axioms) can then be used to derive ("infer") formulas representing true statements from given formulas representing true statements.
The set of axioms may be empty, a nonempty finite set, a countably infinite set, or be given by axiom schemata. A formal grammar recursively defines the expressions and well-formed formulas of the language. In addition a semantics may be given which defines truth and valuations (or interpretations).
The language of a propositional calculus consists of
A "well-formed formula" is any atomic formula, or any formula that can be built up from atomic formulas by means of operator symbols according to the rules of the grammar.
Mathematicians sometimes distinguish between propositional constants, propositional variables, and schemata. Propositional constants represent some particular proposition, while propositional variables range over the set of all atomic propositions. Schemata, however, range over all propositions. It is common to represent propositional constants by , , and , propositional variables by , , and , and schematic letters are often Greek letters, most often , , and .
Basic concepts.
The following outlines a standard propositional calculus. Many different formulations exist which are all more or less equivalent but differ in the details of:
Any given proposition may be represented with a letter called a 'propositional constant', analogous to representing a number by a letter in mathematics, for instance, . All propositions require exactly one of two truth-values: true or false. For example, let be the proposition that it is raining outside. This will be true () if it is raining outside and false otherwise ().
It is extremely helpful to look at the truth tables for these different operators, as well as the method of analytic tableaux.
Closure under operations.
Propositional logic is closed under truth-functional connectives. That is to say, for any proposition , is also a proposition. Likewise, for any propositions and , is a proposition, and similarly for disjunction, conditional, and biconditional. This implies that, for instance, is a proposition, and so it can be conjoined with another proposition. In order to represent this, we need to use parentheses to indicate which proposition is conjoined with which. For instance, is not a well-formed formula, because we do not know if we are conjoining with or if we are conjoining with . Thus we must write either to represent the former, or to represent the latter. By evaluating the truth conditions, we see that both expressions have the same truth conditions (will be true in the same cases), and moreover that any proposition formed by arbitrary conjunctions will have the same truth conditions, regardless of the location of the parentheses. This means that conjunction is associative, however, one should not assume that parentheses never serve a purpose. For instance, the sentence does not have the same truth conditions of , so they are different sentences distinguished only by the parentheses. One can verify this by the truth-table method referenced above.
Note: For any arbitrary number of propositional constants, we can form a finite number of cases which list their possible truth-values. A simple way to generate this is by truth-tables, in which one writes , , ..., , for any list of propositional constants—that is to say, any list of propositional constants with entries. Below this list, one writes rows, and below one fills in the first half of the rows with true (or T) and the second half with false (or F). Below one fills in one-quarter of the rows with T, then one-quarter with F, then one-quarter with T and the last quarter with F. The next column alternates between true and false for each eighth of the rows, then sixteenths, and so on, until the last propositional constant varies between T and F for each row. This will give a complete listing of cases or truth-value assignments possible for those propositional constants.
Argument.
The propositional calculus then defines an "argument" to be a set of propositions. A valid argument is a set of propositions, the last of which follows from—or is implied by—the rest. All other arguments are invalid. The simplest valid argument is modus ponens, one instance of which is the following set of propositions:
This is a set of three propositions, each line is a proposition, and the last follows from the rest. The first two lines are called premises, and the last line the conclusion. We say that any proposition follows from any set of propositions formula_6, if must be true whenever every member of the set formula_6 is true. In the argument above, for any and , whenever and are true, necessarily is true. Notice that, when is true, we cannot consider cases 3 and 4 (from the truth table). When is true, we cannot consider case 2. This leaves only case 1, in which is also true. Thus is implied by the premises.
This generalizes schematically. Thus, where and may be any propositions at all,
Other argument forms are convenient, but not necessary. Given a complete set of axioms (see below for one such set), modus ponens is sufficient to prove all other argument forms in propositional logic, thus they may be considered to be a derivative. Note, this is not true of the extension of propositional logic to other logics like first-order logic. First-order logic requires at least one additional rule of inference in order to obtain completeness.
The significance of argument in formal logic is that one may obtain new truths from established truths. In the first example above, given the two premises, the truth of is not yet known or stated. After the argument is made, is deduced. In this way, we define a deduction system to be a set of all propositions that may be deduced from another set of propositions. For instance, given the set of propositions formula_9, we can define a deduction system, , which is the set of all propositions which follow from . Reiteration is always assumed, so formula_10. Also, from the first element of , last element, as well as modus ponens, is a consequence, and so formula_11. Because we have not included sufficiently complete axioms, though, nothing else may be deduced. Thus, even though most deduction systems studied in propositional logic are able to deduce formula_12, this one is too weak to prove such a proposition.
Generic description of a propositional calculus.
A propositional calculus is a formal system formula_13, where:
The "language" of formula_15, also known as its set of "formulas", "well-formed formulas", is inductively defined by the following rules:
Repeated applications of these rules permits the construction of complex formulas. For example:
Example 1. Simple axiom system.
Let formula_37, where formula_14, formula_39, formula_25, formula_26 are defined as follows:
Example 2. Natural deduction system.
Let formula_58, where formula_14, formula_39, formula_25, formula_26 are defined as follows:
In the following example of a propositional calculus, the transformation rules are intended to be interpreted as the inference rules of a so-called "natural deduction system". The particular system presented here has no initial points, which means that its interpretation for logical applications derives its theorems from an empty axiom set.
Our propositional calculus has ten inference rules. These rules allow us to derive other true formulas given a set of formulas that are assumed to be true. The first nine simply state that we can infer certain well-formed formulas from other well-formed formulas. The last rule however uses hypothetical reasoning in the sense that in the premise of the rule we temporarily assume an (unproven) hypothesis to be part of the set of inferred formulas to see if we can infer a certain other formula. Since the first nine rules don't do this they are usually described as "non-hypothetical" rules, and the last one as a "hypothetical" rule.
In describing the transformation rules, we may introduce a metalanguage symbol formula_70. It is basically a convenient shorthand for saying "infer that". The format is formula_71, in which is a (possibly empty) set of formulas called premises, and is a formula called conclusion. The transformation rule formula_71 means that if every proposition in is a theorem (or has the same truth value as the axioms), then is also a theorem. Note that considering the following rule Conjunction introduction, we will know whenever has more than one formula, we can always safely reduce it into one formula using conjunction. So for short, from that time on we may represent as one formula instead of a set. Another omission for convenience is when is an empty set, in which case may not appear.
Proofs in propositional calculus.
One of the main uses of a propositional calculus, when interpreted for logical applications, is to determine relations of logical equivalence between propositional formulas. These relationships are determined by means of the available transformation rules, sequences of which are called "derivations" or "proofs".
In the discussion to follow, a proof is presented as a sequence of numbered lines, with each line consisting of a single formula followed by a "reason" or "justification" for introducing that formula. Each premise of the argument, that is, an assumption introduced as an hypothesis of the argument, is listed at the beginning of the sequence and is marked as a "premise" in lieu of other justification. The conclusion is listed on the last line. A proof is complete if every line follows from the previous ones by the correct application of a transformation rule. (For a contrasting approach, see proof-trees).
Example of a proof.
Interpret formula_110 as "Assuming , infer ". Read formula_111 as "Assuming nothing, infer that implies ", or "It is a tautology that implies ", or "It is always true that implies ".
Soundness and completeness of the rules.
The crucial properties of this set of rules are that they are "sound" and "complete". Informally this means that the rules are correct and that no other rules are required. These claims can be made more formal as follows.
We define a "truth assignment" as a function that maps propositional variables to true or false. Informally such a truth assignment can be understood as the description of a possible state of affairs (or possible world) where certain statements are true and others are not. The semantics of formulas can then be formalized by defining for which "state of affairs" they are considered to be true, which is what is done by the following definition.
We define when such a truth assignment satisfies a certain well-formed formula with the following rules:
With this definition we can now formalize what it means for a formula to be implied by a certain set of formulas. Informally this is true if in all worlds that are possible given the set of formulas the formula also holds. This leads to the following formal definition: We say that a set of well-formed formulas "semantically entails" (or "implies") a certain well-formed formula if all truth assignments that satisfy all the formulas in also satisfy .
Finally we define "syntactical entailment" such that is syntactically entailed by if and only if we can derive it with the inference rules that were presented above in a finite number of steps. This allows us to formulate exactly what it means for the set of inference rules to be sound and complete:
Soundness: If the set of well-formed formulas syntactically entails the well-formed formula then semantically entails .
Completeness: If the set of well-formed formulas semantically entails the well-formed formula then syntactically entails .
For the above set of rules this is indeed the case.
Sketch of a soundness proof.
Notational conventions: Let be a variable ranging over sets of sentences. Let and range over sentences. For " syntactically entails " we write " proves ". For " semantically entails " we write " implies ".
We want to show: (if proves , then implies ).
We note that " proves " has an inductive definition, and that gives us the immediate resources for demonstrating claims of the form "If proves , then ...". So our proof proceeds by induction.
Notice that Basis Step II can be omitted for natural deduction systems because they have no axioms. When used, Step II involves showing that each of the axioms is a (semantic) logical truth.
The Basis steps demonstrate that the simplest provable sentences from are also implied by , for any . (The proof is simple, since the semantic fact that a set implies any of its members, is also trivial.) The Inductive step will systematically cover all the further sentences that might be provable—by considering each case where we might reach a logical conclusion using an inference rule—and shows that if a new sentence is provable, it is also logically implied. (For example, we might have a rule telling us that from "" we can derive " or ". In III.a We assume that if is provable it is implied. We also know that if is provable then " or " is provable. We have to show that then " or " too is implied. We do so by appeal to the semantic definition and the assumption we just made. is provable from , we assume. So it is also implied by . So any semantic valuation making all of true makes true. But any valuation making true makes " or " true, by the defined semantics for "or". So any valuation which makes all of true makes " or " true. So " or " is implied.) Generally, the Inductive step will consist of a lengthy but simple case-by-case analysis of all the rules of inference, showing that each "preserves" semantic implication.
By the definition of provability, there are no sentences provable other than by being a member of , an axiom, or following by a rule; so if all of those are semantically implied, the deduction calculus is sound.
Sketch of completeness proof.
We adopt the same notational conventions as above.
We want to show: If implies , then proves . We proceed by contraposition: We show instead that if does not prove then does not imply .
QED
Another outline for a completeness proof.
If a formula is a tautology, then there is a truth table for it which shows that each valuation yields the value true for the formula. Consider such a valuation. By mathematical induction on the length of the subformulas, show that the truth or falsity of the subformula follows from the truth or falsity (as appropriate for the valuation) of each propositional variable in the subformula. Then combine the lines of the truth table together two at a time by using "( is true implies ) implies (( is false implies ) implies )". Keep repeating this until all dependencies on propositional variables have been eliminated. The result is that we have proved the given tautology. Since every tautology is provable, the logic is complete.
Interpretation of a truth-functional propositional calculus.
An interpretation of a truth-functional propositional calculus formula_112 is an assignment to each propositional symbol of formula_112 of one or the other (but not both) of the truth values truth (T) and falsity (F), and an assignment to the connective symbols of formula_112 of their usual truth-functional meanings. An interpretation of a truth-functional propositional calculus may also be expressed in terms of truth tables.
For formula_115 distinct propositional symbols there are formula_116 distinct possible interpretations. For any particular symbol formula_117, for example, there are formula_118 possible interpretations:
For the pair formula_117, formula_122 there are formula_123 possible interpretations:
Since formula_112 has formula_129, that is, denumerably many propositional symbols, there are formula_130, and therefore uncountably many distinct possible interpretations of formula_112.
Interpretation of a sentence of truth-functional propositional logic.
If and are formulas of formula_112 and formula_133 is an interpretation of formula_112 then:
Some consequences of these definitions:
Alternative calculus.
It is possible to define another version of propositional calculus, which defines most of the syntax of the logical operators by means of axioms, and which uses only one inference rule.
Axioms.
Let , , and stand for well-formed formulas. (The well-formed formulas themselves would not contain any Greek letters, but only capital Roman letters, connective operators, and parentheses.) Then the axioms are as follows:
Inference rule.
The inference rule is modus ponens:
Meta-inference rule.
Let a demonstration be represented by a sequence, with hypotheses to the left of the turnstile and the conclusion to the right of the turnstile. Then the deduction theorem can be stated as follows:
This deduction theorem (DT) is not itself formulated with propositional calculus: it is not a theorem of propositional calculus, but a theorem about propositional calculus. In this sense, it is a meta-theorem, comparable to theorems about the soundness or completeness of propositional calculus.
On the other hand, DT is so useful for simplifying the syntactical proof process that it can be considered and used as another inference rule, accompanying modus ponens. In this sense, DT corresponds to the natural conditional proof inference rule which is part of the first version of propositional calculus introduced in this article.
The converse of DT is also valid:
in fact, the validity of the converse of DT is almost trivial compared to that of DT:
The converse of DT has powerful implications: it can be used to convert an axiom into an inference rule. For example, the axiom AND-1,
can be transformed by means of the converse of the deduction theorem into the inference rule
which is conjunction elimination, one of the ten inference rules used in the first version (in this article) of the propositional calculus.
Example of a proof.
The following is an example of a (syntactical) demonstration, involving only axioms THEN-1 and THEN-2:
Prove: formula_167 (Reflexivity of implication).
Proof:
Equivalence to equational logics.
The preceding alternative calculus is an example of a Hilbert-style deduction system. In the case of propositional systems the axioms are terms built with logical connectives and the only inference rule is modus ponens. Equational logic as standardly used informally in high school algebra is a different kind of calculus from Hilbert systems. Its theorems are equations and its inference rules express the properties of equality, namely that it is a congruence on terms that admits substitution.
Classical propositional calculus as described above is equivalent to Boolean algebra, while intuitionistic propositional calculus is equivalent to Heyting algebra. The equivalence is shown by translation in each direction of the theorems of the respective systems. Theorems formula_176 of classical or intuitionistic propositional calculus are translated as equations formula_177 of Boolean or Heyting algebra respectively. Conversely theorems formula_178 of Boolean or Heyting algebra are translated as theorems formula_179 of classical or intuitionistic calculus respectively, for which formula_180 is a standard abbreviation. In the case of Boolean algebra formula_178 can also be translated as formula_182, but this translation is incorrect intuitionistically.
In both Boolean and Heyting algebra, inequality formula_183 can be used in place of equality. The equality formula_178 is expressible as a pair of inequalities formula_183 and formula_186. Conversely the inequality formula_183 is expressible as the equality formula_188, or as formula_189. The significance of inequality for Hilbert-style systems is that it corresponds to the latter's deduction or entailment symbol formula_70. An entailment
is translated in the inequality version of the algebraic framework as
Conversely the algebraic inequality formula_183 is translated as the entailment
The difference between implication formula_195 and inequality or entailment formula_183 or formula_194 is that the former is internal to the logic while the latter is external. Internal implication between two terms is another term of the same kind. Entailment as external implication between two terms expresses a metatruth outside the language of the logic, and is considered part of the metalanguage. Even when the logic under study is intuitionistic, entailment is ordinarily understood classically as two-valued: either the left side entails, or is less-or-equal to, the right side, or it is not.
Similar but more complex translations to and from algebraic logics are possible for natural deduction systems as described above and for the sequent calculus. The entailments of the latter can be interpreted as two-valued, but a more insightful interpretation is as a set, the elements of which can be understood as abstract proofs organized as the morphisms of a category. In this interpretation the cut rule of the sequent calculus corresponds to composition in the category. Boolean and Heyting algebras enter this picture as special categories having at most one morphism per homset, i.e., one proof per entailment, corresponding to the idea that existence of proofs is all that matters: any proof will do and there is no point in distinguishing them.
Graphical calculi.
It is possible to generalize the definition of a formal language from a set of finite sequences over a finite basis to include many other sets of mathematical structures, so long as they are built up by finitary means from finite materials. What's more, many of these families of formal structures are especially well-suited for use in logic.
For example, there are many families of graphs that are close enough analogues of formal languages that the concept of a calculus is quite easily and naturally extended to them. Indeed, many species of graphs arise as "parse graphs" in the syntactic analysis of the corresponding families of text structures. The exigencies of practical computation on formal languages frequently demand that text strings be converted into pointer structure renditions of parse graphs, simply as a matter of checking whether strings are well-formed formulas or not. Once this is done, there are many advantages to be gained from developing the graphical analogue of the calculus on strings. The mapping from strings to parse graphs is called "parsing" and the inverse mapping from parse graphs to strings is achieved by an operation that is called "traversing" the graph.
Other logical calculi.
Propositional calculus is about the simplest kind of logical calculus in current use. It can be extended in several ways. (Aristotelian "syllogistic" calculus, which is largely supplanted in modern logic, is in "some" ways simpler – but in other ways more complex – than propositional calculus.) The most immediate way to develop a more complex logical calculus is to introduce rules that are sensitive to more fine-grained details of the sentences being used.
First-order logic (a.k.a. first-order predicate logic) results when the "atomic sentences" of propositional logic are broken up into terms, variables, predicates, and quantifiers, all keeping the rules of propositional logic with some new ones introduced. (For example, from "All dogs are mammals" we may infer "If Rover is a dog then Rover is a mammal".) With the tools of first-order logic it is possible to formulate a number of theories, either with explicit axioms or by rules of inference, that can themselves be treated as logical calculi. Arithmetic is the best known of these; others include set theory and mereology. Second-order logic and other higher-order logics are formal extensions of first-order logic. Thus, it makes sense to refer to propositional logic as ""zeroth-order logic"", when comparing it with these logics.
Modal logic also offers a variety of inferences that cannot be captured in propositional calculus. For example, from "Necessarily " we may infer that . From we may infer "It is possible that ". The translation between modal logics and algebraic logics concerns classical and intuitionistic logics but with the introduction of a unary operator on Boolean or Heyting algebras, different from the Boolean operations, interpreting the possibility modality, and in the case of Heyting algebra a second operator interpreting necessity (for Boolean algebra this is redundant since necessity is the De Morgan dual of possibility). The first operator preserves 0 and disjunction while the second preserves 1 and conjunction.
Many-valued logics are those allowing sentences to have values other than "true" and "false". (For example, "neither" and "both" are standard "extra values"; "continuum logic" allows each sentence to have any of an infinite number of "degrees of truth" between "true" and "false".) These logics often require calculational devices quite distinct from propositional calculus. When the values form a Boolean algebra (which may have more than two or even infinitely many values), many-valued logic reduces to classical logic; many-valued logics are therefore only of independent interest when the values form an algebra that is not Boolean.
Solvers.
Finding solutions to propositional logic formulas is an NP-complete problem. However, practical methods exist (e.g., DPLL algorithm, 1962; Chaff algorithm, 2001) that are very fast for many useful cases. Recent work has extended the SAT solver algorithms to work with propositions containing arithmetic expressions; these are the SMT solvers.

</doc>
<doc id="18155" url="https://en.wikipedia.org/wiki?curid=18155" title="Lazy evaluation">
Lazy evaluation

In programming language theory, lazy evaluation, or call-by-need is an evaluation strategy which delays the evaluation of an expression until its value is needed (non-strict evaluation) and which also avoids repeated evaluations (sharing). The sharing can reduce the running time of certain functions by an exponential factor over other non-strict evaluation strategies, such as call-by-name. 
The benefits of lazy evaluation include: 
Lazy evaluation is often combined with memoization, as described in Jon Bentley's "Writing Efficient Programs". After a function's value is computed for that parameter or set of parameters, the result is stored in a lookup table that is indexed by the values of those parameters; the next time the function is called, the table is consulted to determine whether the result for that combination of parameter values is already available. If so, the stored result is simply returned. If not, the function is evaluated and another entry is added to the lookup table for reuse.
Lazy evaluation can lead to reduction in memory footprint, since values are created when needed. However, lazy evaluation is difficult to combine with imperative features such as exception handling and input/output, because the order of operations becomes indeterminate. Lazy evaluation can introduce space leaks.
The opposite of lazy evaluation is eager evaluation, sometimes known as strict evaluation. Eager evaluation is the evaluation strategy employed in most programming languages.
History.
Lazy evaluation was introduced for lambda calculus by Christopher Wadsworth and for programming languages independently by Peter Henderson & James H. Morris and Daniel P. Friedman & David S. Wise.
Applications.
Delayed evaluation is used particularly in functional programming languages. When using delayed evaluation, an expression is not evaluated as soon as it gets bound to a variable, but when the evaluator is forced to produce the expression's value. That is, a statement such as codice_1 (i.e. the assignment of the result of an expression to a variable) clearly calls for the expression to be evaluated and the result placed in codice_2, but what actually is in codice_2 is irrelevant until there is a need for its value via a reference to codice_2 in some later expression whose evaluation could itself be deferred, though eventually the rapidly growing tree of dependencies would be pruned to produce some symbol rather than another for the outside world to see.
Delayed evaluation has the advantage of being able to create calculable infinite lists without infinite loops or size matters interfering in computation. For example, one could create a function that creates an infinite list (often called a "stream") of Fibonacci numbers. The calculation of the "n"-th Fibonacci number would be merely the extraction of that element from the infinite list, forcing the evaluation of only the first n members of the list.
For example, in the Haskell programming language, the list of all Fibonacci numbers can be written as:
In Haskell syntax, "codice_5" prepends an element to a list, codice_6 returns a list without its first element, and codice_7 uses a specified function (in this case addition) to combine corresponding elements of two lists to produce a third.
Provided the programmer is careful, only the values that are required to produce a particular result are evaluated. However, certain calculations may result in the program attempting to evaluate an infinite number of elements; for example, requesting the length of the list or trying to sum the elements of the list with a fold operation would result in the program either failing to terminate or running out of memory.
Control structures.
In almost all common "eager" languages, "if" statements evaluate in a lazy fashion.
evaluates (a), then if and only if (a) evaluates to true does it evaluate (b), otherwise it evaluates (c). That is, either (b) or (c) will not be evaluated. Conversely, in an eager language the expected behavior is that
will still evaluate (e) when computing the value of f(d, e) even though (e) is unused in function f. However, user-defined control structures depend on exact syntax, so for example
(i) and (j) would both be evaluated in an eager language. While in
(i) or (j) would be evaluated, but never both.
Lazy evaluation allows control structures to be defined normally, and not as primitives or compile-time techniques. If (i) or (j) have side effects or introduce run time errors, the subtle differences between (l) and (l') can be complex. It is usually possible to introduce user-defined lazy control structures in eager languages as functions, though they may depart from the language's syntax for eager evaluation: Often the involved code bodies (like (i) and (j)) need to be wrapped in a function value, so that they are executed only when called.
Short-circuit evaluation of Boolean control structures is sometimes called "lazy".
Working with infinite data structures.
Many languages offer the notion of "infinite data-structures". These allow definitions of data to be given in terms of infinite ranges, or unending recursion, but the actual values are only computed when needed. Take for example this trivial program in Haskell:
In the function numberFromInfiniteList, the value of infinity is an infinite range, but until an actual value (or more specifically, a specific value at a certain index) is needed, the list is not evaluated, and even then it is only evaluated as needed (that is, until the desired index.)
Other uses.
In computer windowing systems, the painting of information to the screen is driven by "expose events" which drive the display code at the last possible moment. By doing this, windowing systems avoid computing unnecessary display content updates.
Another example of laziness in modern computer systems is copy-on-write page allocation or demand paging, where memory is allocated only when a value stored in that memory is changed.
Laziness can be useful for high performance scenarios. An example is the Unix mmap function, which provides "demand driven" loading of pages from disk, so that only those pages actually touched are loaded into memory, and unneeded memory is not allocated.
MATLAB implements "copy on edit", where arrays which are copied have their actual memory storage replicated only when their content is changed, possibly leading to an "out of memory" error when updating an element afterwards instead of during the copy operation.
Implementation.
Some programming languages delay evaluation of expressions by default, and some others provide functions or special syntax to delay evaluation. In Miranda and Haskell, evaluation of function arguments is delayed by default. In many other languages, evaluation can be delayed by explicitly suspending the computation using special syntax (as with Scheme's "codice_8" and "codice_9" and OCaml's "codice_10" and "codice_11") or, more generally, by wrapping the expression in a thunk. The object representing such an explicitly delayed evaluation is called a "lazy future." Perl 6 uses lazy evaluation of lists, so one can assign infinite lists to variables and use them as arguments to functions, but unlike Haskell and Miranda, Perl 6 doesn't use lazy evaluation of arithmetic operators and functions by default.
Laziness and eagerness.
Controlling eagerness in lazy languages.
In lazy programming languages such as Haskell, although the default is to evaluate expressions only when they are demanded, it is possible in some cases to make code more eager—or conversely, to make it more lazy again after it has been made more eager. This can be done by explicitly coding something which forces evaluation (which may make the code more eager) or avoiding such code (which may make the code more lazy). "Strict" evaluation usually implies eagerness, but they are technically different concepts.
However, there is an optimisation implemented in some compilers called strictness analysis, which, in some cases, allows the compiler to infer that a value will always be used. In such cases, this may render the programmer's choice of whether to force that particular value or not, irrelevant, because strictness analysis will force strict evaluation.
In Haskell, marking constructor fields strict means that their values will always be demanded immediately. The codice_12 function can also be used to demand a value immediately and then pass it on, which is useful if a constructor field should generally be lazy. However, neither of these techniques implements "recursive" strictness—for that, a function called codice_13 was invented.
Also, pattern matching in Haskell 98 is strict by default, so the codice_14 qualifier has to be used to make it lazy. 
Simulating laziness in eager languages.
In Python 2.x the codice_15 function computes a list of integers. The entire list is stored in memory when the first assignment statement is evaluated, so this is an example of eager or immediate evaluation:
In Python 3.x the codice_15 function returns a special range object which computes elements of the list on demand. Elements of the range object are only generated when they are needed (e.g., when codice_17 is evaluated in the following example), so this is an example of lazy or deferred evaluation:
In Python 2.x is possible to use a function called codice_18 which returns an object that generates the numbers in the range on demand. The advantage of codice_19 is that generated object will always take the same amount of memory.
From version 2.2 forward, Python manifests lazy evaluation by implementing iterators (lazy sequences) unlike tuple or list sequences. For instance (Python 2):
In the .NET Framework it is possible to do lazy evaluation using the class <syntaxhighlight enclose="none" lang="csharp">System.Lazy<T></syntaxhighlight>. The class can be easily exploited in F# using the <syntaxhighlight enclose="none" lang="fsharp">lazy</syntaxhighlight> keyword, while the <syntaxhighlight enclose="none" lang="fsharp">force</syntaxhighlight> method will force the evaluation. There are also specialized collections like <syntaxhighlight enclose="none" lang="csharp">Microsoft.FSharp.Collections.Seq</syntaxhighlight> that provide built-in support for lazy evaluation. 
<syntaxhighlight lang="fsharp">
let fibonacci = Seq.unfold (fun (x, y) -> Some(x, (y, x + y))) (0I,1I)
fibonacci |> Seq.nth 1000
</syntaxhighlight>
In C# and VB.NET, the class <syntaxhighlight enclose="none" lang="csharp">System.Lazy<T></syntaxhighlight> is directly used. 
<syntaxhighlight lang="csharp">
public int Sum()
</syntaxhighlight>
Or with a more practical example: 
<syntaxhighlight lang="csharp">
// recursive calculation of the n'th fibonacci number
public int Fib(int n)
public void Main()
</syntaxhighlight>
Another way is to use the <syntaxhighlight enclose="none" lang="csharp">yield</syntaxhighlight> keyword: 
<syntaxhighlight lang="csharp">
// eager evaluation 
public IEnumerable<int> Fibonacci(int x)
// lazy evaluation 
public IEnumerable<int> LazyFibonacci(int x)
</syntaxhighlight>

</doc>
<doc id="18156" url="https://en.wikipedia.org/wiki?curid=18156" title="Lemuridae">
Lemuridae

Lemuridae is a family of strepsirrhine primates native to Madagascar, and the Comoros Islands. They are represented by the Lemuriformes in Madagascar with one of the highest concentration of the lemurs. One of five families commonly known as lemurs. These animals were once thought to be the evolutionary predecessors of monkeys and apes, but this is no longer considered correct.
Characteristics.
Lemurids are medium-sized arboreal primates, ranging from 32 to 56 cm in length, excluding the tail, and weighing from 0.7 to 5 kg. They have long, bushy tails and soft, woolly fur of varying coloration. The hindlegs are slightly longer than the forelegs, although not enough to hamper fully quadrupedal movement (unlike the sportive lemurs). Most species are highly agile, and regularly leap several metres between trees. They have a good sense of smell and binocular vision. Unlike most other lemurs, all but one species of lemurid (the ring-tailed lemur) lack a tapetum lucidum, a reflective layer in the eye that improves night vision. Historically, activity cycles are either strictly diurnal or nocturnal however, these can widely verily across the mammalian species. Lemur activity has evolved from nocturnal to diurnal. Some lemurs are also cathemeral, an activity pattern here an animal is neither diurnal nor nocturnal, which is common among Lemurdae.
Lemurids are herbivorous, eating fruit, leaves, and, in some cases, nectar. For the most part, they have the dental formula: . A lemur’s diet is one that is not restricted since their diet consists of frugivory, granivory, folivory, insectivory, omnivory, and gumnivory foods. Some Subfossil records have contributed to the knowledge of the currently extant lemurs from the Holocene by showing the changes in their dental records in habitats near human activity. This demonstrates that lemur species such as the lemur "catta" and the common brown lemur were forced to switch their primary diet to a group of secondly food sources.
With most lemurids, the mother gives birth to one or two young after a gestation period of between 120 and 140 days, depending on species. The ruffed lemur species are the only lemurids that have true litters, consisting of anywhere from two to six offspring. They are generally sociable animals, living in groups of up to thirty individuals in some species. In some cases, such as the ring-tailed lemur, the groups are long-lasting, with distinct dominance hierarchies, while in others, such as the common brown lemur, the membership of the groups varies from day to day, and seems to have no clear social structure.
Some of the lemur traits include low basal metabolic rate, highly seasonal breeders, adaptations to unpredictable climate and female dominance. Female dominance amongst lemurs is when the females are sexually monomorphic and have priority access to food. Lemurs live in groups of 11 to 17 animals, where females tend to stay within their natal groups and the males migrate. Male lemurs are competitive to win their mates which causes instability among the other organisms. Lemurs are able to mark their territory by using scents from local areas.
A number of lemur species are considered threatened; two species are critically endangered, one species is endangered, and five species are rated as vulnerable.
Habitat.
The highly seasonal dry deciduous forest of Madagascar alternates between dry and wet seasons, making it uniquely suitable for lemurs. This is due to high tree species diversity which are essential for survival and might be 'diluted' of its resources which are of no use for lemurs, thus increasing energetic expenses for traveling between suitable patches. Evidence from the Subfossil records show that many of the now extinct lemurs actually lived in much drier climates than the currently extant lemurs.
Classification.
The family Lemuridae contains 21 extant species in five genera.
FAMILY LEMURIDAE
This family was once broken into two subfamilies, Hapalemurinae (bamboo lemurs and the greater bamboo lemur) and Lemurinae (the rest of the family), but molecular evidence and the similarity of the scent glands have since placed the ring-tailed lemur with the bamboo lemurs and the greater bamboo lemur.
Lemur species in the "Eulemur" genus are known to interbreed, despite having dramatically different chromosome numbers. Red-fronted (2N=60) and collared (2N=50–52) brown lemurs were found to hybridize at Berenty Reserve, Madagascar.
References.
Of Dental Macrowear In Subfossil Lemur Catta
From Ankilitelo Cave, Madagascar: Indications Of Ecology And Habitat Use Over Time." "Folia Primatologica"86.1-2
(2015): 140-149. "Scopus®". Retrieved from: http://eds.b.ebscohost.com/eds/detail/detail?vid=4&sid=23d095b1-b556-4a52-a066-26743f96f9d7%40sessionmgr110&hid=126&bdata=JnNpdGU9ZWRzLWxpdmU%3d#AN=edselc.2-52.0-84930250744&db=edselc

</doc>
<doc id="18157" url="https://en.wikipedia.org/wiki?curid=18157" title="Lucent">
Lucent

Lucent Technologies, Inc., was an American multinational telecommunications equipment company headquartered in Murray Hill, New Jersey, in the United States. It was established on September 30, 1996, through the divestiture of the former AT&T Technologies business unit of AT&T Corporation, which included Western Electric and Bell Labs.
Lucent was merged with Alcatel SA of France in a merger of equals on December 1, 2006, forming Alcatel-Lucent.
Name.
Lucent means "light-bearing" in Latin. The name was applied in 1996 at the time of the split from AT&T.
The name was widely criticised, as the logo was to be, both internally and externally. Corporate communications and business cards included the strapline 'Bell Labs Innovations' in a bid to retain the prestige of the internationally famous research lab, within a new business under an as-yet unknown name.
This same linguistic root also gives Lucifer, "the light bearer" (from lux, 'light', and ferre, 'to bear'), who is also a character in Dante's epic poem "Inferno". Shortly after the Lucent renaming in 1996, the Plan 9 project released a development of their work as the Inferno OS in 1997. This extended the 'Lucifer' and Dante references as a series of punning names for the components of Inferno - Dis, Limbo, Charon and Styx (9P Protocol).
When the rights to Inferno were sold in 2000, the company Vita Nuova Holdings was formed to represent them. This continues the Dante theme, although moving away from his "Divine Comedy" to the poem "La Vita Nuova".
Logo.
The Lucent logo, the Innovation Ring, was designed by Landor Associates, a prominent San Francisco-based branding consultancy. One source inside Lucent says that the logo is actually a Zen Buddhist symbol for "eternal truth", the Enso, turned 90 degrees and modified. Another source says it represents the mythic ouroboros, a snake holding its tail in its mouth. Lucent's logo also has been said to represent constant re-creating and re-thinking. Carly Fiorina picked the logo because her mother was a painter and she rejected the sterile geometric logos of most high tech companies.
After the logo was compared in the media to the ring a coffee mug leaves on paper, a "Dilbert" comic strip showed Dogbert as an overpaid consultant designing a new company logo; he takes a piece of paper that his coffee cup was sitting on and calls it the "Brown Ring of Quality". A telecommunication commentator referred to the logo as "a big red zero" and predicted financial losses.
History.
Two of the primary reasons AT&T Corporation chose to spin off its equipment manufacturing business was to permit it to profit from sales to competing telecommunications providers; these customers had previously shown reluctance to purchase from a direct competitor. Bell Labs brought prestige to the new company, as well as the revenue from thousands of patents. Lucent also operated from 666 Fifth Avenue in Manhattan, New York City.
At the time of its spinoff, Lucent was placed under the leadership of Henry Schacht, who was brought in to oversee its transition from an arm of AT&T into an independent corporation. Richard McGinn, who was serving as President and COO, succeeded Schacht as CEO in 1997 while Schacht remained chairman of the board. Lucent became a "darling" stock of the investment community in the late 1990s, rising from a split-adjusted spinoff price of $7.56/share to a high of $84. Its market capitalization reached a high of $258 billion, and it was at the time the most widely held company with 5.3 million shareholders.
In 1997, Lucent acquired Milpitas-based voice mail market leader Octel Communications Corporation for $2.1 billion, a move which immediately rendered the Business Systems Group profitable. By 1999 Lucent stock continued to soar and in that year Lucent acquired Ascend Communications, an Alameda, California–based manufacturer of communications equipment for US$24 billion. Lucent held discussions to acquire Juniper Networks but decided instead to build its own routers internally.
In 1995, Carly Fiorina led corporate operations. In that capacity, she reported to Lucent chief executive Henry B. Schacht. She played a key role in planning and implementing the 1996 initial public offering of a successful stock and company launch strategy. Under her guidance, the spin-off became one of the most successful IPOs in U.S. history, raising 3 billion. 
Later in 1996, Fiorina was appointed president of Lucent's consumer products sector, reporting to president and chief operating officer Rich McGinn. In 1997, she was named group president for Lucent's 19 billion global service-provider business, overseeing marketing and sales for the company's largest customer segment. That year, Fiorina chaired a 2.5 billion joint venture between Lucent's consumer communications and Royal Philips Electronics, under the name Philips Consumer Communications (PCC). Focus of the venture was to bring both companies to the top three in technology, distribution, and brand recognition. 
Ultimately, the project struggled and dissolved a year later after it garnered only 2% market share in mobile phones. Losses were at $500 million on sales of $2.5 billion. As a result of the failed joint venture, Philips announced the closure of one-quarter of the company's 230 factories worldwide, and Lucent closed down its wireless handset portion of the venture. Analysts suggested that the joint venture's failure was due to a combination of technology and management problems. Upon the end of the joint venture, PCC sent 5,000 employees back to Philips, many of which were laid off, and 8,400 employees back to Lucent.
On the surface, Fiorina seemed to add 22,000 jobs & revenues grew from 19 billion to 38 billion. However, the real cause of Lucent spurring sales under Fiorina was by lending money to their own customers. According to Fortune magazine, "In a neat bit of accounting magic, money from the loans began to appear on Lucent’s income statement as new revenue while the dicey debt got stashed on its balance sheet as an allegedly solid asset". Lucent's stock price grew 10-fold. 
At the start of 2000, Lucent's "private bubble burst", while other competitors like Nortel Networks and Alcatel were still going strong as it would be many months before the rest of the telecom industry bubble collapsed. Previously Lucent had 14 straight quarters where it exceeded analysts' expectations, leading to high expectations for the 15th quarter, ending Dec. 31, 1999. On January 6, 2000, Lucent made the first of a string of announcements that it had missed its quarterly estimates, as CEO Rich McGinn grimly announced that Lucent had run into special problems during that quarter—including disruptions in its optical networking business—and reported flat revenues and a big drop in profits. That caused the stock to plunge by 28%, shaving $64 billion off of the company's market capitalization. When it was later revealed that it had used dubious accounting and sales practices to generate some of its earlier quarterly numbers, Lucent fell from grace. It was said that "Rich McGinn couldn't accept Lucent's fall from its early triumphs." He described himself once as imposing "audacious" goals on his managers, believing the stretch for performance would produce dream results. Henry Schacht defended the corporate culture that McGinn created and also noted that McGinn did not sell any Lucent shares while serving as CEO. In November 2000, it also disclosed to the Securities and Exchange Commission that it had a $125 million accounting error for the third quarter of 2000, and by December 2000 it reported it had overstated its revenues for its latest quarter by nearly $700 million. Although no wrongdoing was found on his part, McGinn was forced to resign as CEO and he was replaced by Schacht on an interim basis. Subsequently, its CFO, Deborah Hopkins, left the company in May 2001 with Lucent's stock at $9.06 whereas at the time she was hired it was at $46.82.
In 2001 there were merger discussions between Lucent and Alcatel, which would have seen Lucent acquired at its current market price without a premium and the newly combined entity would have been headquartered in Murray Hill. However these negotiations collapsed when Schacht insisted on an equal 7-7 split of the merged company's board of directors, while Alcatel chief executive officer Serge Tchuruk wanted 8 of the 14 board seats for Alcatel due to it being in a stronger position. The failure of the merger talks caused Lucent's share price to collapse, and by October 2002 the stock price had bottomed at 55 cents per share.
Patricia Russo, formerly Lucent's EVP of the Corporate Office who then left for Eastman Kodak to serve as COO, was named permanent Chairman and CEO of Lucent in 2002, succeeding Schacht who remained on the Board of Directors.
In April 2000, Lucent sold its Consumer Products unit to VTech and Consumer Phone Services. In October 2000, Lucent spun off its Business Systems arm into Avaya, Inc., and in June 2002, it spun off its microelectronics division into Agere Systems. The spinoffs of enterprise networking and wireless, the industry's key growth businesses from 2003 onward, meant that Lucent no longer had the capacity to serve this market.
Lucent was reduced to 30,500 employees, down from about 165,000 employees at its zenith. The layoffs of so many experienced employees meant that the company was in a weakened position and unable to reestablish itself when the market recovered in 2003. By early 2003 Lucent's market value was $15.6 billion (which includes $6.8 billion of current value for two companies that Lucent had recently spun off, Avaya and Agere Systems), making the shares worth around $2.13, a far cry from its dotcom bubble peak of around $84, when Lucent was worth $258 billion.
Lucent continued to be active in the areas of telephone switching, optical, data and wireless networking.
On April 2, 2006, Lucent announced a merger agreement with Alcatel, which was 1.5 times the size of Lucent. Serge Tchuruk became non-executive chairman, and Russo served as CEO of the newly merged company, Alcatel-Lucent, until they were both forced to resign at the end of 2008. The merger failed to produce the expected synergies, and there were significant write-downs of Lucent's assets that Alcatel purchased.
Operations.
Divisions.
Lucent was divided into several core groups:
Murray Hill facility.
The Murray Hill anechoic chamber, built in 1940, is the world's oldest wedge-based anechoic chamber. The interior room measures approximately high by wide by deep. The exterior concrete and brick walls are about thick to keep outside noise from entering the chamber. The chamber absorbs over 99.995% of the incident acoustic energy above 200 Hz. At one time the Murray Hill chamber was cited in the Guinness Book of World Records as the world's quietest room. It is possible to hear the sounds of skeletal joints and heart beats very prominently.
The Murray Hill facility was the global headquarters for Lucent Technologies. The Murray Hill facility also has the largest copper-roof in the world. When Lucent Technologies was experiencing financial troubles in 2000 and 2001, one out of every three fluorescent lights was turned off in the facility. The same was done in the Naperville, Illinois, and Allentown, Pennsylvania, facilities for a while. The facility had a cricket field and featured a nearby station from which enthusiasts could control RC airplanes and helicopters.

</doc>
<doc id="18158" url="https://en.wikipedia.org/wiki?curid=18158" title="Lupercalia">
Lupercalia

Lupercalia was a very ancient, possibly pre-Roman pastoral festival, observed on February 13 through 15, to avert evil spirits and purify the city, releasing health and fertility. Lupercalia subsumed Februa, an earlier-origin spring cleansing ritual held on the same date, which gives the month of February "(Februarius)" its name.
The name "Lupercalia" was believed in antiquity to evince some connection with the Ancient Greek festival of the Arcadian Lykaia (from Ancient Greek: "λύκος" — "lukos", "wolf", Latin "lupus") and the worship of "Lycaean Pan", assumed to be a Greek equivalent to Faunus, as instituted by Evander.
In Roman mythology, "Lupercus" is a god sometimes identified with the Roman god Faunus, who is the Roman equivalent of the Greek god Pan. Lupercus is the god of shepherds. His festival, celebrated on the anniversary of the founding of his temple on February 15, was called the Lupercalia. His priests wore goatskins. The historian Justin mentions an image of "the Lycaean god, whom the Greeks call Pan and the Romans Lupercus," nude save for the girdle of goatskin, which stood in the Lupercal, the cave where Romulus and Remus were suckled by a she-wolf. There, on the Ides of February (in February the ides is the 13th), a goat and a dog were sacrificed, and salt mealcakes prepared by the Vestal Virgins were burnt.
Late Republic and Empire.
Plutarch described Lupercalia:
Lupercalia, of which many write that it was anciently celebrated by shepherds, and has also some connection with the Arcadian Lycaea. At this time many of the noble youths and of the magistrates run up and down through the city naked, for sport and laughter striking those they meet with shaggy thongs. And many women of rank also purposely get in their way, and like children at school present their hands to be struck, believing that the pregnant will thus be helped in delivery, and the barren to pregnancy.
The Lupercalia festival was partly in honor of Lupa, the she-wolf who suckled the infant orphans, Romulus and Remus, the founders of Rome, explaining the name of the festival, Lupercalia, or "Wolf Festival." The festival was celebrated near the cave of Lupercal on the Palatine Hill (the central hill where Rome was traditionally founded), to expiate and purify new life in the Spring. A known Lupercalia festival of 44 BC attests to the continuity of the festival but the Lupercal cave may have fallen into disrepair, and was later rebuilt by Augustus. It has been tentatively identified with a cavern discovered in 2007, below the remains of Augustus' palace.
The rites were directed by the "Luperci", the "brothers of the wolf ("lupus")", a corporation of "sacerdotes" (priests) of Faunus, dressed only in a goatskin, whose institution is attributed either to the Arcadian Evander, or to Romulus and Remus. The Luperci were divided into two collegia, called "Quinctiliani" (or "Quinctiale"s) and "Fabiani", from the gens Quinctilia (or Quinctia) and gens Fabia; at the head of each of these colleges was a "magister". In 44 BC, a third college, the "Julii", was instituted in honor of Julius Caesar, the first magister of which was Mark Antony. Antony offered Caesar a crown during the festival, an act that was widely interpreted as a sign that Caesar aspired to make himself king and was gauging the reaction of the crowd. In imperial times the members were usually of equestrian standing.
The festival began with the sacrifice by the Luperci (or the flamen dialis) of two male goats and a dog. Next two young patrician Luperci were led to the altar, to be anointed on their foreheads with the sacrificial blood, which was wiped off the bloody knife with wool soaked in milk, after which they were expected to smile and laugh.
The sacrificial feast followed, after which the Luperci cut thongs from the skins of the animals, which were called februa, dressed themselves in the skins of the sacrificed goats, in imitation of Lupercus, and ran round the walls of the old Palatine city, the line of which was marked with stones, with the thongs in their hands in two bands, striking the people who crowded near. Girls and young women would line up on their route to receive lashes from these whips. This was supposed to ensure fertility, prevent sterility in women and ease the pains of childbirth.
The Lupercalia in the 5th century.
By the 5th century, when the public performance of pagan rites had been outlawed, a nominally Christian Roman populace still clung to the Lupercalia in the time of Pope Gelasius I (494–96). It had been literally degraded since the 1st century, when in 44 BC the consul Mark Antony did not scruple to run with the Luperci; now the upper classes left the festivities to the rabble. Whatever the fortunes of the rites in the meantime, in the last decade of the 5th century they prompted Pope Gelasius I's taunt to the senators who were intent on preserving them: "If you assert that this rite has salutary force, celebrate it yourselves in the ancestral fashion; run nude yourselves that you may properly carry out the mockery." The remark was addressed to the senator Andromachus by Gelasius in an extended literary epistle that was virtually a diatribe against the Lupercalia. Gelasius finally abolished the Lupercalia after a long dispute.
Some authors claim that Gelasius replaced Lupercalia with the "Feast of the Purification of the Blessed Virgin Mary," but researcher Oruch says that there is no written record of Gelasius ever intending a replacement of Lupercalia. Some researchers, such as Kellog and Cox, have made a separate claim that the modern customs of Saint Valentine's Day originate from Lupercalia customs. Other researchers have rejected this claim: they say there is no proof that the modern customs of Saint Valentine's Day originate from Lupercalia customs, and the claim seems to originate from misconceptions about festivities.
References in art.
Horace's Ode III, 18 describes Lupercalia.
William Shakespeare's play "Julius Caesar" begins during the Lupercalia, with the tradition described above. Mark Antony is instructed by Caesar to strike his wife Calpurnia, in the hope that she will be able to conceive:
CAESAR (to Calpurnia)
ANTONY
CAESAR
Later, after Caesar's assassination, Mark Antony delivers his funeral speech (Act III, Scene II, line 74) in which he refers to how, at the Lupercal, he had offered Caesar the crown three times (see illustration).
ANTONY

</doc>
<doc id="18162" url="https://en.wikipedia.org/wiki?curid=18162" title="Lists of atheists">
Lists of atheists

Atheism is, in a broad sense, the lack of belief in the existence of deities. In a narrower sense, atheism is simply the absence of belief that any deities exist. This is a compilation of the various lists of atheists with articles in Wikipedia. Living persons in these lists are people whose atheism is relevant to their notable activities or public life, and who have publicly identified themselves as atheists.

</doc>
<doc id="18163" url="https://en.wikipedia.org/wiki?curid=18163" title="List of Buddhists">
List of Buddhists

This is a list of notable Buddhists, encompassing all the major branches of the religion, and including interdenominational and eclectic Buddhist practitioners. This list includes both formal teachers of Buddhism, and people notable in other areas who are publicly Buddhist or who have espoused Buddhism.
Historical Buddhist thinkers and founders of schools.
Individuals are grouped by nationality, except in cases where their influence was felt elsewhere. Gautama Buddha and his immediate disciples ('Buddhists') are listed separately from later Indian Buddhist thinkers, teachers and contemplatives.
Modern teachers.
Zen teachers.
American
Chinese
European
Japanese
Korean
Malaysian
Taiwanese
Vietnamese

</doc>
<doc id="18166" url="https://en.wikipedia.org/wiki?curid=18166" title="List of agnostics">
List of agnostics

Listed here are persons who have identified themselves as theologically agnostic. Also included are individuals who have expressed the view that the veracity of a god's existence is unknown or inherently unknowable.

</doc>
<doc id="18167" url="https://en.wikipedia.org/wiki?curid=18167" title="Linked list">
Linked list

In computer science, a linked list is a linear collection of data elements, called nodes pointing to the next node by means of a pointer. It is a data structure consisting of a group of nodes which together represent a sequence. Under the simplest form, each node is composed of data and a reference (in other words, a "link") to the next node in the sequence; more complex variants add additional links. This structure allows for efficient insertion or removal of elements from any position in the sequence.
<br>"A linked list whose nodes contain two fields: an integer value and a link to the next node. The last node is linked to a terminator used to signify the end of the list."
Linked lists are among the simplest and most common data structures. They can be used to implement several other common abstract data types, including lists (the abstract data type), stacks, queues, associative arrays, and S-expressions, though it is not uncommon to implement the other data structures directly without using a list as the basis of implementation.
The principal benefit of a linked list over a conventional array is that the list elements can easily be inserted or removed without reallocation or reorganization of the entire structure because the data items need not be stored contiguously in memory or on disk, while an array has to be declared in the source code, before compiling and running the program. Linked lists allow insertion and removal of nodes at any point in the list, and can do so with a constant number of operations if the link previous to the link being added or removed is maintained during list traversal.
On the other hand, simple linked lists by themselves do not allow random access to the data, or any form of efficient indexing. Thus, many basic operations — such as obtaining the last node of the list (assuming that the last node is not maintained as separate node reference in the list structure), or finding a node that contains a given datum, or locating the place where a new node should be inserted — may require sequential scanning of most or all of the list elements. The advantages and disadvantages of using linked lists are given below.
History.
Linked lists were developed in 1955–1956 by Allen Newell, Cliff Shaw and Herbert A. Simon at RAND Corporation as the primary data structure for their Information Processing Language. IPL was used by the authors to develop several early artificial intelligence programs, including the Logic Theory Machine, the General Problem Solver, and a computer chess program. Reports on their work appeared in IRE Transactions on Information Theory in 1956, and several conference proceedings from 1957 to 1959, including Proceedings of the Western Joint Computer Conference in 1957 and 1958, and Information Processing (Proceedings of the first UNESCO International Conference on Information Processing) in 1959. The now-classic diagram consisting of blocks representing list nodes with arrows pointing to successive list nodes appears in "Programming the Logic Theory Machine" by Newell and Shaw in Proc. WJCC, February 1957. Newell and Simon were recognized with the ACM Turing Award in 1975 for having "made basic contributions to artificial intelligence, the psychology of human cognition, and list processing".
The problem of machine translation for natural language processing led Victor Yngve at Massachusetts Institute of Technology (MIT) to use linked lists as data structures in his COMIT programming language for computer research in the field of linguistics. A report on this language entitled "A programming language for mechanical translation" appeared in Mechanical Translation in 1958.
LISP, standing for list processor, was created by John McCarthy in 1958 while he was at MIT and in 1960 he published its design in a paper in the Communications of the ACM, entitled "Recursive Functions of Symbolic Expressions and Their Computation by Machine, Part I". One of LISP's major data structures is the linked list.
By the early 1960s, the utility of both linked lists and languages which use these structures as their primary data representation was well established. Bert Green of the MIT Lincoln Laboratory published a review article entitled "Computer languages for symbol manipulation" in IRE Transactions on Human Factors in Electronics in March 1961 which summarized the advantages of the linked list approach. A later review article, "A Comparison of list-processing computer languages" by Bobrow and Raphael, appeared in Communications of the ACM in April 1964.
Several operating systems developed by Technical Systems Consultants (originally of West Lafayette Indiana, and later of Chapel Hill, North Carolina) used singly linked lists as file structures. A directory entry pointed to the first sector of a file, and succeeding portions of the file were located by traversing pointers. Systems using this technique included Flex (for the Motorola 6800 CPU), mini-Flex (same CPU), and Flex9 (for the Motorola 6809 CPU). A variant developed by TSC for and marketed by Smoke Signal Broadcasting in California, used doubly linked lists in the same manner.
The TSS/360 operating system, developed by IBM for the System 360/370 machines, used a double linked list for their file system catalog. The directory structure was similar to Unix, where a directory could contain files and other directories and extend to any depth.
Basic concepts and nomenclature.
Each record of a linked list is often called an 'element' or 'node'.
The field of each node that contains the address of the next node is usually called the 'next link' or 'next pointer'. The remaining fields are known as the 'data', 'information', 'value', 'cargo', or 'payload' fields.
The 'head' of a list is its first node. The 'tail' of a list may refer either to the rest of the list after the head, or to the last node in the list. In Lisp and some derived languages, the next node may be called the 'cdr' (pronounced "could-er") of the list, while the payload of the head node may be called the 'car'.
Singly linked list.
Singly linked lists contain nodes which have a data field as well as a 'next' field, which points to the next node in line of nodes. Operations that can be performed on singly linked lists include insertion, deletion and traversal.
<br>"A singly linked list whose nodes contain two fields: an integer value and a link to the next node"
Doubly linked list.
In a 'doubly linked list', each node contains, besides the next-node link, a second link field pointing to the 'previous' node in the sequence. The two links may be called 'forward('s') and 'backwards', or 'next' and 'prev'('previous').
<br>"A doubly linked list whose nodes contain three fields: an integer value, the link forward to the next node, and the link backward to the previous node"
A technique known as XOR-linking allows a doubly linked list to be implemented using a single link field in each node. However, this technique requires the ability to do bit operations on addresses, and therefore may not be available in some high-level languages.
Many modern operating systems use doubly linked lists to maintain references to active processes, threads, and other dynamic objects. A common strategy for rootkits to evade detection is to unlink themselves from these lists.
Multiply linked list.
In a 'multiply linked list', each node contains two or more link fields, each field being used to connect the same set of data records in a different order (e.g., by name, by department, by date of birth, etc.). While doubly linked lists can be seen as special cases of multiply linked list, the fact that the two orders are opposite to each other leads to simpler and more efficient algorithms, so they are usually treated as a separate case.
Circular Linked list.
In the last node of a list, the link field often contains a null reference, a special value used to indicate the lack of further nodes. A less common convention is to make it point to the first node of the list; in that case the list is said to be 'circular' or 'circularly linked'; otherwise it is said to be 'open' or 'linear'.
<br>"A circular linked list"
In the case of a circular doubly linked list, the only change that occurs is that the end, or "tail", of the said list is linked back to the front, or "head", of the list and vice versa.
Sentinel nodes.
In some implementations an extra 'sentinel' or 'dummy' node may be added before the first data record or after the last one. This convention simplifies and accelerates some list-handling algorithms, by ensuring that all links can be safely dereferenced and that every list (even one that contains no data elements) always has a "first" and "last" node.
Empty lists.
An empty list is a list that contains no data records. This is usually the same as saying that it has zero nodes. If sentinel nodes are being used, the list is usually said to be empty when it has only sentinel nodes.
Hash linking.
The link fields need not be physically part of the nodes. If the data records are stored in an array and referenced by their indices, the link field may be stored in a separate array with the same indices as the data records.
List handles.
Since a reference to the first node gives access to the whole list, that reference is often called the 'address', 'pointer', or 'handle' of the list. Algorithms that manipulate linked lists usually get such handles to the input lists and return the handles to the resulting lists. In fact, in the context of such algorithms, the word "list" often means "list handle". In some situations, however, it may be convenient to refer to a list by a handle that consists of two links, pointing to its first and last nodes.
Combining alternatives.
The alternatives listed above may be arbitrarily combined in almost every way, so one may have circular doubly linked lists without sentinels, circular singly linked lists with sentinels, etc.
Tradeoffs.
As with most choices in computer programming and design, no method is well suited to all circumstances. A linked list data structure might work well in one case, but cause problems in another. This is a list of some of the common tradeoffs involving linked list structures.
Linked lists vs. dynamic arrays.
A "dynamic array" is a data structure that allocates all elements contiguously in memory, and keeps a count of the current number of elements. If the space reserved for the dynamic array is exceeded, it is reallocated and (possibly) copied, an expensive operation.
Linked lists have several advantages over dynamic arrays. Insertion or deletion of an element at a specific point of a list, assuming that we have indexed a pointer to the node (before the one to be removed, or before the insertion point) already, is a constant-time operation (otherwise without this reference it is O(n)), whereas insertion in a dynamic array at random locations will require moving half of the elements on average, and all the elements in the worst case. While one can "delete" an element from an array in constant time by somehow marking its slot as "vacant", this causes fragmentation that impedes the performance of iteration.
Moreover, arbitrarily many elements may be inserted into a linked list, limited only by the total memory available; while a dynamic array will eventually fill up its underlying array data structure and will have to reallocate — an expensive operation, one that may not even be possible if memory is fragmented, although the cost of reallocation can be averaged over insertions, and the cost of an insertion due to reallocation would still be amortized O(1). This helps with appending elements at the array's end, but inserting into (or removing from) middle positions still carries prohibitive costs due to data moving to maintain contiguity. An array from which many elements are removed may also have to be resized in order to avoid wasting too much space.
On the other hand, dynamic arrays (as well as fixed-size array data structures) allow constant-time random access, while linked lists allow only sequential access to elements. Singly linked lists, in fact, can be easily traversed in only one direction. This makes linked lists unsuitable for applications where it's useful to look up an element by its index quickly, such as heapsort. Sequential access on arrays and dynamic arrays is also faster than on linked lists on many machines, because they have optimal locality of reference and thus make good use of data caching.
Another disadvantage of linked lists is the extra storage needed for references, which often makes them impractical for lists of small data items such as characters or boolean values, because the storage overhead for the links may exceed by a factor of two or more the size of the data. In contrast, a dynamic array requires only the space for the data itself (and a very small amount of control data). It can also be slow, and with a naïve allocator, wasteful, to allocate memory separately for each new element, a problem generally solved using memory pools.
Some hybrid solutions try to combine the advantages of the two representations. Unrolled linked lists store several elements in each list node, increasing cache performance while decreasing memory overhead for references. CDR coding does both these as well, by replacing references with the actual data referenced, which extends off the end of the referencing record.
A good example that highlights the pros and cons of using dynamic arrays vs. linked lists is by implementing a program that resolves the Josephus problem. The Josephus problem is an election method that works by having a group of people stand in a circle. Starting at a predetermined person, you count around the circle "n" times. Once you reach the "n"th person, take them out of the circle and have the members close the circle. Then count around the circle the same "n" times and repeat the process, until only one person is left. That person wins the election. This shows the strengths and weaknesses of a linked list vs. a dynamic array, because if you view the people as connected nodes in a circular linked list then it shows how easily the linked list is able to delete nodes (as it only has to rearrange the links to the different nodes). However, the linked list will be poor at finding the next person to remove and will need to search through the list until it finds that person. A dynamic array, on the other hand, will be poor at deleting nodes (or elements) as it cannot remove one node without individually shifting all the elements up the list by one. However, it is exceptionally easy to find the "n"th person in the circle by directly referencing them by their position in the array.
The list ranking problem concerns the efficient conversion of a linked list representation into an array. Although trivial for a conventional computer, solving this problem by a parallel algorithm is complicated and has been the subject of much research.
A balanced tree has similar memory access patterns and space overhead to a linked list while permitting much more efficient indexing, taking O(log n) time instead of O(n) for a random access. However, insertion and deletion operations are more expensive due to the overhead of tree manipulations to maintain balance. Schemes exist for trees to automatically maintain themselves in a balanced state: AVL trees or red-black trees.
Singly linked linear lists vs. other lists.
While doubly linked and circular lists have advantages over singly linked linear lists, linear lists offer some advantages that make them preferable in some situations.
A singly linked linear list is a recursive data structure, because it contains a pointer to a "smaller" object of the same type. For that reason, many operations on singly linked linear lists (such as merging two lists, or enumerating the elements in reverse order) often have very simple recursive algorithms, much simpler than any solution using iterative commands. While those recursive solutions can be adapted for doubly linked and circularly linked lists, the procedures generally need extra arguments and more complicated base cases.
Linear singly linked lists also allow tail-sharing, the use of a common final portion of sub-list as the terminal portion of two different lists. In particular, if a new node is added at the beginning of a list, the former list remains available as the tail of the new one — a simple example of a persistent data structure. Again, this is not true with the other variants: a node may never belong to two different circular or doubly linked lists.
In particular, end-sentinel nodes can be shared among singly linked non-circular lists. The same end-sentinel node may be used for "every" such list. In Lisp, for example, every proper list ends with a link to a special node, denoted by codice_1 or codice_2, whose codice_3 and codice_4 links point to itself. Thus a Lisp procedure can safely take the codice_3 or codice_4 of "any" list.
The advantages of the fancy variants are often limited to the complexity of the algorithms, not in their efficiency. A circular list, in particular, can usually be emulated by a linear list together with two variables that point to the first and last nodes, at no extra cost.
Doubly linked vs. singly linked.
Double-linked lists require more space per node (unless one uses XOR-linking), and their elementary operations are more expensive; but they are often easier to manipulate because they allow fast and easy sequential access to the list in both directions. In a doubly linked list, one can insert or delete a node in a constant number of operations given only that node's address. To do the same in a singly linked list, one must have the "address of the pointer" to that node, which is either the handle for the whole list (in case of the first node) or the link field in the "previous" node. Some algorithms require access in both directions. On the other hand, doubly linked lists do not allow tail-sharing and cannot be used as persistent data structures.
Circularly linked vs. linearly linked.
A circularly linked list may be a natural option to represent arrays that are naturally circular, e.g. the corners of a polygon, a pool of buffers that are used and released in FIFO ("first in, first out") order, or a set of processes that should be time-shared in round-robin order. In these applications, a pointer to any node serves as a handle to the whole list.
With a circular list, a pointer to the last node gives easy access also to the first node, by following one link. Thus, in applications that require access to both ends of the list (e.g., in the implementation of a queue), a circular structure allows one to handle the structure by a single pointer, instead of two.
A circular list can be split into two circular lists, in constant time, by giving the addresses of the last node of each piece. The operation consists in swapping the contents of the link fields of those two nodes. Applying the same operation to any two nodes in two distinct lists joins the two list into one. This property greatly simplifies some algorithms and data structures, such as the quad-edge and face-edge.
The simplest representation for an empty "circular" list (when such a thing makes sense) is a null pointer, indicating that the list has no nodes. Without this choice, many algorithms have to test for this special case, and handle it separately. By contrast, the use of null to denote an empty "linear" list is more natural and often creates fewer special cases.
Using sentinel nodes.
Sentinel node may simplify certain list operations, by ensuring that the next or previous nodes exist for every element, and that even empty lists have at least one node. One may also use a sentinel node at the end of the list, with an appropriate data field, to eliminate some end-of-list tests. For example, when scanning the list looking for a node with a given value "x", setting the sentinel's data field to "x" makes it unnecessary to test for end-of-list inside the loop. Another example is the merging two sorted lists: if their sentinels have data fields set to +∞, the choice of the next output node does not need special handling for empty lists.
However, sentinel nodes use up extra space (especially in applications that use many short lists), and they may complicate other operations (such as the creation of a new empty list).
However, if the circular list is used merely to simulate a linear list, one may avoid some of this complexity by adding a single sentinel node to every list, between the last and the first data nodes. With this convention, an empty list consists of the sentinel node alone, pointing to itself via the next-node link. The list handle should then be a pointer to the last data node, before the sentinel, if the list is not empty; or to the sentinel itself, if the list is empty.
The same trick can be used to simplify the handling of a doubly linked linear list, by turning it into a circular doubly linked list with a single sentinel node. However, in this case, the handle should be a single pointer to the dummy node itself.
Linked list operations.
When manipulating linked lists in-place, care must be taken to not use values that you have invalidated in previous assignments. This makes algorithms for inserting or deleting linked list nodes somewhat subtle. This section gives pseudocode for adding or removing nodes from singly, doubly, and circularly linked lists in-place. Throughout we will use "null" to refer to an end-of-list marker or sentinel, which may be implemented in a number of ways.
Linearly linked lists.
Singly linked lists.
Our node data structure will have two fields. We also keep a variable "firstNode" which always points to the first node in the list, or is "null" for an empty list.
Traversal of a singly linked list is simple, beginning at the first node and following each "next" link until we come to the end:
The following code inserts a node after an existing node in a singly linked list. The diagram shows how it works. Inserting a node before an existing one cannot be done directly; instead, one must keep track of the previous node and insert a node after it.
Inserting at the beginning of the list requires a separate function. This requires updating "firstNode".
Similarly, we have functions for removing the node "after" a given node, and for removing a node from the beginning of the list. The diagram demonstrates the former. To find and remove a particular node, one must again keep track of the previous element.
Notice that codice_7 sets codice_8 to codice_9 when removing the last node in the list.
Since we can't iterate backwards, efficient codice_10 or codice_11 operations are not possible.
Appending one linked list to another can be inefficient unless a reference to the tail is kept as part of the List structure, because we must traverse the entire first list in order to find the tail, and then append the second list to this. Thus, if two linearly linked lists are each of length formula_1, list appending has asymptotic time complexity of formula_2. In the Lisp family of languages, list appending is provided by the codice_12 procedure.
Many of the special cases of linked list operations can be eliminated by including a dummy element at the front of the list. This ensures that there are no special cases for the beginning of the list and renders both codice_13 and codice_7 unnecessary. In this case, the first useful data in the list will be found at codice_15.
Circularly linked list.
In a circularly linked list, all nodes are linked in a continuous circle, without using "null." For lists with a front and a back (such as a queue), one stores a reference to the last node in the list. The "next" node after the last node is the first node. Elements can be added to the back of the list and removed from the front in constant time.
Circularly linked lists can be either singly or doubly linked.
Both types of circularly linked lists benefit from the ability to traverse the full list beginning at any given node. This often allows us to avoid storing "firstNode" and "lastNode", although if the list may be empty we need a special representation for the empty list, such as a "lastNode" variable which points to some node in the list or is "null" if it's empty; we use such a "lastNode" here. This representation significantly simplifies adding and removing nodes with a non-empty list, but empty lists are then a special case.
Algorithms.
Assuming that "someNode" is some node in a non-empty circular singly linked list, this code iterates through that list starting with "someNode":
Notice that the test "while node ≠ someNode" must be at the end of the loop. If the test was moved to the beginning of the loop, the procedure would fail whenever the list had only one node.
This function inserts a node "newNode" into a circular linked list after a given node "node". If "node" is null, it assumes that the list is empty.
Suppose that "L" is a variable pointing to the last node of a circular linked list (or null if the list is empty). To append "newNode" to the "end" of the list, one may do
To insert "newNode" at the "beginning" of the list, one may do
Linked lists using arrays of nodes.
Languages that do not support any type of reference can still create links by replacing pointers with array indices. The approach is to keep an array of records, where each record has integer fields indicating the index of the next (and possibly previous) node in the array. Not all nodes in the array need be used. If records are also not supported, parallel arrays can often be used instead.
As an example, consider the following linked list record that uses arrays instead of pointers:
A linked list can be build by creating an array of these structures, and an integer variable to store the index of the first element.
Links between elements are formed by placing the array index of the next (or previous) cell into the Next or Prev field within a given element. For example:
In the above example, codice_16 would be set to 2, the location of the first entry in the list. Notice that entry 3 and 5 through 7 are not part of the list. These cells are available for any additions to the list. By creating a codice_17 integer variable, a free list could be created to keep track of what cells are available. If all entries are in use, the size of the array would have to be increased or some elements would have to be deleted before new entries could be stored in the list.
The following code would traverse the list and display names and account balance:
When faced with a choice, the advantages of this approach include:
This approach has one main disadvantage, however: it creates and manages a private memory space for its nodes. This leads to the following issues:
For these reasons, this approach is mainly used for languages that do not support dynamic memory allocation. These disadvantages are also mitigated if the maximum size of the list is known at the time the array is created.
Language support.
Many programming languages such as Lisp and Scheme have singly linked lists built in. In many functional languages, these lists are constructed from nodes, each called a "cons" or "cons cell". The cons has two fields: the "car", a reference to the data for that node, and the "cdr", a reference to the next node. Although cons cells can be used to build other data structures, this is their primary purpose.
In languages that support abstract data types or templates, linked list ADTs or templates are available for building linked lists. In other languages, linked lists are typically built using references together with records.
Internal and external storage.
When constructing a linked list, one is faced with the choice of whether to store the data of the list directly in the linked list nodes, called "internal storage", or merely to store a reference to the data, called "external storage". Internal storage has the advantage of making access to the data more efficient, requiring less storage overall, having better locality of reference, and simplifying memory management for the list (its data is allocated and deallocated at the same time as the list nodes).
External storage, on the other hand, has the advantage of being more generic, in that the same data structure and machine code can be used for a linked list no matter what the size of the data is. It also makes it easy to place the same data in multiple linked lists. Although with internal storage the same data can be placed in multiple lists by including multiple "next" references in the node data structure, it would then be necessary to create separate routines to add or delete cells based on each field. It is possible to create additional linked lists of elements that use internal storage by using external storage, and having the cells of the additional linked lists store references to the nodes of the linked list containing the data.
In general, if a set of data structures needs to be included in linked lists, external storage is the best approach. If a set of data structures need to be included in only one linked list, then internal storage is slightly better, unless a generic linked list package using external storage is available. Likewise, if different sets of data that can be stored in the same data structure are to be included in a single linked list, then internal storage would be fine.
Another approach that can be used with some languages involves having different data structures, but all have the initial fields, including the "next" (and "prev" if double linked list) references in the same location. After defining separate structures for each type of data, a generic structure can be defined that contains the minimum amount of data shared by all the other structures and contained at the top (beginning) of the structures. Then generic routines can be created that use the minimal structure to perform linked list type operations, but separate routines can then handle the specific data. This approach is often used in message parsing routines, where several types of messages are received, but all start with the same set of fields, usually including a field for message type. The generic routines are used to add new messages to a queue when they are received, and remove them from the queue in order to process the message. The message type field is then used to call the correct routine to process the specific type of message.
Example of internal and external storage.
Suppose you wanted to create a linked list of families and their members. Using internal storage, the structure might look like the following:
To print a complete list of families and their members using internal storage, we could write:
Using external storage, we would create the following structures:
To print a complete list of families and their members using external storage, we could write:
Notice that when using external storage, an extra step is needed to extract the record from the node and cast it into the proper data type. This is because both the list of families and the list of members within the family are stored in two linked lists using the same data structure ("node"), and this language does not have parametric types.
As long as the number of families that a member can belong to is known at compile time, internal storage works fine. If, however, a member needed to be included in an arbitrary number of families, with the specific number known only at run time, external storage would be necessary.
Speeding up search.
Finding a specific element in a linked list, even if it is sorted, normally requires O("n") time (linear search). This is one of the primary disadvantages of linked lists over other data structures. In addition to the variants discussed above, below are two simple ways to improve search time.
In an unordered list, one simple heuristic for decreasing average search time is the "move-to-front heuristic", which simply moves an element to the beginning of the list once it is found. This scheme, handy for creating simple caches, ensures that the most recently used items are also the quickest to find again.
Another common approach is to "index" a linked list using a more efficient external data structure. For example, one can build a red-black tree or hash table whose elements are references to the linked list nodes. Multiple such indexes can be built on a single list. The disadvantage is that these indexes may need to be updated each time a node is added or removed (or at least, before that index is used again).
Random access lists.
A random access list is a list with support for fast random access to read or modify any element in the list. One possible implementation is a skew binary random access list using the skew binary number system, which involves a list of trees with special properties; this allows worst-case constant time head/cons operations, and worst-case logarithmic time random access to an element by index. Random access lists can be implemented as persistent data structures.
Random access lists can be viewed as immutable linked lists in that they likewise support the same O(1) head and tail operations.
A simple extension to random access lists is the min-list, which provides an additional operation that yields the minimum element in the entire list in constant time (without mutation complexities).
Related data structures.
Both stacks and queues are often implemented using linked lists, and simply restrict the type of operations which are supported.
The skip list is a linked list augmented with layers of pointers for quickly jumping over large numbers of elements, and then descending to the next layer. This process continues down to the bottom layer, which is the actual list.
A binary tree can be seen as a type of linked list where the elements are themselves linked lists of the same nature. The result is that each node may include a reference to the first node of one or two other linked lists, which, together with their contents, form the subtrees below that node.
An unrolled linked list is a linked list in which each node contains an array of data values. This leads to improved cache performance, since more list elements are contiguous in memory, and reduced memory overhead, because less metadata needs to be stored for each element of the list.
A hash table may use linked lists to store the chains of items that hash to the same position in the hash table.
A heap shares some of the ordering properties of a linked list, but is almost always implemented using an array. Instead of references from node to node, the next and previous data indexes are calculated using the current data's index.
A self-organizing list rearranges its nodes based on some heuristic which reduces search times for data retrieval by keeping commonly accessed nodes at the head of the list.

</doc>
<doc id="18168" url="https://en.wikipedia.org/wiki?curid=18168" title="Logic gate">
Logic gate

In electronics, a logic gate is an idealized or physical device implementing a Boolean function; that is, it performs a logical operation on one or more logical inputs, and produces a single logical output. Depending on the context, the term may refer to an ideal logic gate, one that has for instance zero rise time and unlimited fan-out, or it may refer to a non-ideal physical device (see Ideal and real op-amps for comparison).
Logic gates are primarily implemented using diodes or transistors acting as electronic switches, but can also be constructed using vacuum tubes, electromagnetic relays (relay logic), fluidic logic, pneumatic logic, optics, molecules, or even mechanical elements. With amplification, logic gates can be cascaded in the same way that Boolean functions can be composed, allowing the construction of a physical model of all of Boolean logic, and therefore, all of the algorithms and mathematics that can be described with Boolean logic.
Logic circuits include such devices as multiplexers, registers, arithmetic logic units (ALUs), and computer memory, all the way up through complete microprocessors, which may contain more than 100 million gates. In modern practice, most gates are made from field-effect transistors (FETs), particularly MOSFETs (metal–oxide–semiconductor field-effect transistors).
Compound logic gates AND-OR-Invert (AOI) and OR-AND-Invert (OAI) are often employed in circuit design because their construction using MOSFETs is simpler and more efficient than the sum of the individual gates.
In reversible logic, Toffoli gates are used.
Electronic gates.
To build a functionally complete logic system, relays, valves (vacuum tubes), or transistors can be used. The simplest family of logic gates using bipolar transistors is called resistor-transistor logic (RTL). Unlike simple diode logic gates (which do not have a gain element), RTL gates can be cascaded indefinitely to produce more complex logic functions. RTL gates were used in early integrated circuits. For higher speed and better density, the resistors used in RTL were replaced by diodes resulting in diode-transistor logic (DTL). Transistor-transistor logic (TTL) then supplanted DTL. As integrated circuits became more complex, bipolar transistors were replaced with smaller field-effect transistors (MOSFETs); see PMOS and NMOS. To reduce power consumption still further, most contemporary chip implementations of digital systems now use CMOS logic. CMOS uses complementary (both n-channel and p-channel) MOSFET devices to achieve a high speed with low power dissipation.
For small-scale logic, designers now use prefabricated logic gates from families of devices such as the TTL 7400 series by Texas Instruments, the CMOS 4000 series by RCA, and their more recent descendants. Increasingly, these fixed-function logic gates are being replaced by programmable logic devices, which allow designers to pack a large number of mixed logic gates into a single integrated circuit. The field-programmable nature of programmable logic devices such as FPGAs has removed the 'hard' property of hardware; it is now possible to change the logic design of a hardware system by reprogramming some of its components, thus allowing the features or function of a hardware implementation of a logic system to be changed.
Other types of logic gates include, but are not limited to 
Electronic logic gates differ significantly from their relay-and-switch equivalents. They are much faster, consume much less power, and are much smaller (all by a factor of a million or more in most cases). Also, there is a fundamental structural difference. The switch circuit creates a continuous metallic path for current to flow (in either direction) between its input and its output. The semiconductor logic gate, on the other hand, acts as a high-gain voltage amplifier, which sinks a tiny current at its input and produces a low-impedance voltage at its output. It is not possible for current to flow between the output and the input of a semiconductor logic gate.
Another important advantage of standardized integrated circuit logic families, such as the 7400 and 4000 families, is that they can be cascaded. This means that the output of one gate can be wired to the inputs of one or several other gates, and so on. Systems with varying degrees of complexity can be built without great concern of the designer for the internal workings of the gates, provided the limitations of each integrated circuit are considered.
The output of one gate can only drive a finite number of inputs to other gates, a number called the 'fanout limit'. Also, there is always a delay, called the 'propagation delay', from a change in input of a gate to the corresponding change in its output. When gates are cascaded, the total propagation delay is approximately the sum of the individual delays, an effect which can become a problem in high-speed circuits. Additional delay can be caused when a large number of inputs are connected to an output, due to the distributed capacitance of all the inputs and wiring and the finite amount of current that each output can provide.
Symbols.
There are two sets of symbols for elementary logic gates in common use, both defined in ANSI/IEEE Std 91-1984 and its supplement ANSI/IEEE Std 91a-1991. The "distinctive shape" set, based on traditional schematics, is used for simple drawings, and derives from MIL-STD-806 of the 1950s and 1960s. It is sometimes unofficially described as "military", reflecting its origin. The "rectangular shape" set, based on ANSI Y32.14 and other early industry standards, as later refined by IEEE and IEC, has rectangular outlines for all types of gate and allows representation of a much wider range of devices than is possible with the traditional symbols. The IEC standard, IEC 60617-12, has been adopted by other standards, such as EN 60617-12:1999 in Europe, BS EN 60617-12:1999 in the United Kingdom, and DIN EN 60617-12:1998 in Germany.
The mutual goal of IEEE Std 91-1984 and IEC 60617-12 was to provide a uniform method of describing the complex logic functions of digital circuits with schematic symbols. These functions were more complex than simple AND and OR gates. They could be medium scale circuits such as a 4-bit counter to a large scale circuit such as a microprocessor.
IEC 617-12 and its successor IEC 60617-12 do not explicitly show the "distinctive shape" symbols, but do not prohibit them. These are, however, shown in ANSI/IEEE 91 (and 91a) with this note: "The distinctive-shape symbol is, according to IEC Publication 617, Part 12, not preferred, but is not considered to be in contradiction to that standard." IEC 60617-12 correspondingly contains the note (Section 2.1) "Although non-preferred, the use of other symbols recognized by official national standards, that is distinctive shapes in place of symbols of basic gates, shall not be considered to be in contradiction with this standard. Usage of these other symbols in combination to form complex symbols (for example, use as embedded symbols) is discouraged." This compromise was reached between the respective IEEE and IEC working groups to permit the IEEE and IEC standards to be in mutual compliance with one another.
A third style of symbols was in use in Europe and is still widely used in European academia. See the column "DIN 40700" in the table in the German Wikipedia.
In the 1980s, schematics were the predominant method to design both circuit boards and custom ICs known as gate arrays. Today custom ICs and the field-programmable gate array are typically designed with Hardware Description Languages (HDL) such as Verilog or VHDL.
The output of two input exclusive-OR is true only when the two input values are "different", false if they are equal, regardless of the value. If there are more than two inputs, the gate generates a true at its output if the number of trues at its input is "odd". In practice, these gates are built from combinations of simpler logic gates.
Universal logic gates.
Charles Sanders Peirce (winter of 1880–81) showed that NOR gates alone (or alternatively NAND gates alone) can be used to reproduce the functions of all the other logic gates, but his work on it was unpublished until 1933. The first published proof was by Henry M. Sheffer in 1913, so the NAND logical operation is sometimes called Sheffer stroke; the logical NOR is sometimes called "Peirce's arrow". Consequently, these gates are sometimes called "universal logic gates".
De Morgan equivalent symbols.
By use of De Morgan's laws, an "AND" function is identical to an "OR" function with negated inputs and outputs. Likewise, an "OR" function is identical to an "AND" function with negated inputs and outputs. A NAND gate is equivalent to an OR gate with negated inputs, and a NOR gate is equivalent to an AND gate with negated inputs.
This leads to an alternative set of symbols for basic gates that use the opposite core symbol ("AND" or "OR") but with the inputs and outputs negated. Use of these alternative symbols can make logic circuit diagrams much clearer and help to show accidental connection of an active high output to an active low input or vice versa. Any connection that has logic negations at both ends can be replaced by a negationless connection and a suitable change of gate or vice versa. Any connection that has a negation at one end and no negation at the other can be made easier to interpret by instead using the De Morgan equivalent symbol at either of the two ends. When negation or polarity indicators on both ends of a connection match, there is no logic negation in that path (effectively, bubbles "cancel"), making it easier to follow logic states from one symbol to the next. This is commonly seen in real logic diagrams - thus the reader must not get into the habit of associating the shapes exclusively as OR or AND shapes, but also take into account the bubbles at both inputs and outputs in order to determine the "true" logic function indicated.
A De Morgan symbol can show more clearly a gate's primary logical purpose and the polarity of its nodes that are considered in the "signaled" (active, on) state. Consider the simplified case where a two-input NAND gate is used to drive a motor when either of its inputs are brought low by a switch. The "signaled" state (motor on) occurs when either one OR the other switch is on. Unlike a regular NAND symbol, which suggests AND logic, the De Morgan version, a two negative-input OR gate, correctly shows that OR is of interest. The regular NAND symbol has a bubble at the output and none at the inputs (the opposite of the states that will turn the motor on), but the De Morgan symbol shows both inputs and output in the polarity that will drive the motor.
De Morgan's theorem is most commonly used to implement logic gates as combinations of only NAND gates, or as combinations of only NOR gates, for economic reasons.
Data storage.
Logic gates can also be used to store data. A storage element can be constructed by connecting several gates in a "latch" circuit. More complicated designs that use clock signals and that change only on a rising or falling edge of the clock are called edge-triggered "flip-flops". Formally, a flip-flop is called a bistable circuit, because it has two stable states which it can maintain indefinitely. The combination of multiple flip-flops in parallel, to store a multiple-bit value, is known as a register. When using any of these gate setups the overall system has memory; it is then called a sequential logic system since its output can be influenced by its previous state(s), i.e. by the "sequence" of input states. In contrast, the output from combinatorial logic is purely a combination of its present inputs, unaffected by the previous input and output states.
These logic circuits are known as computer memory. They vary in performance, based on factors of speed, complexity, and reliability of storage, and many different types of designs are used based on the application.
Three-state logic gates.
A three-state logic gate is a type of logic gate that can have three different outputs: high (H), low (L) and high-impedance (Z). The high-impedance state plays no role in the logic, which is strictly binary. These devices are used on buses of the CPU to allow multiple chips to send data. A group of three-states driving a line with a suitable control circuit is basically equivalent to a multiplexer, which may be physically distributed over separate devices or plug-in cards.
In electronics, a high output would mean the output is sourcing current from the positive power terminal (positive voltage). A low output would mean the output is sinking current to the negative power terminal (zero voltage). High impedance would mean that the output is effectively disconnected from the circuit.
History and development.
The binary number system was refined by Gottfried Wilhelm Leibniz (published in 1705) and he also established that by using the binary system, the principles of arithmetic and logic could be combined. In an 1886 letter, Charles Sanders Peirce described how logical operations could be carried out by electrical switching circuits. Eventually, vacuum tubes replaced relays for logic operations. Lee De Forest's modification, in 1907, of the Fleming valve can be used as an AND logic gate. Ludwig Wittgenstein introduced a version of the 16-row truth table as proposition 5.101 of "Tractatus Logico-Philosophicus" (1921). Walther Bothe, inventor of the coincidence circuit, got part of the 1954 Nobel Prize in physics, for the first modern electronic AND gate in 1924. Konrad Zuse designed and built electromechanical logic gates for his computer Z1 (from 1935–38). Claude E. Shannon introduced the use of Boolean algebra in the analysis and design of switching circuits in 1937. Active research is taking place in molecular logic gates.
Implementations.
Since the 1990s, most logic gates are made in CMOS technology (i.e. NMOS and PMOS transistors are used). Often millions of logic gates are packaged in a single integrated circuit.
There are several logic families with different characteristics (power consumption, speed, cost, size) such as: RDL (resistor-diode logic), RTL (resistor-transistor logic), DTL (diode-transistor logic), TTL (transistor-transistor logic) and CMOS (complementary metal oxide semiconductor). There are also sub-variants, e.g. standard CMOS logic vs. advanced types using still CMOS technology, but with some optimizations for avoiding loss of speed due to slower PMOS transistors.
Non-electronic implementations are varied, though few of them are used in practical applications. Many early electromechanical digital computers, such as the Harvard Mark I, were built from relay logic gates, using electro-mechanical relays. Logic gates can be made using pneumatic devices, such as the Sorteberg relay or mechanical logic gates, including on a molecular scale. Logic gates have been made out of DNA (see DNA nanotechnology) and used to create a computer called MAYA (see MAYA II). Logic gates can be made from quantum mechanical effects (though quantum computing usually diverges from boolean design). Photonic logic gates use non-linear optical effects.
In principle any method that leads to a gate that is functionally complete (for example, either a NOR or a NAND gate) can be used to make any kind of digital logic circuit. Note that the use of 3-state logic for bus systems is not needed, and can be replaced by digital multiplexers, which can be built using only simple logic gates (such as NAND gates, NOR gates, or AND and OR gates).

</doc>
<doc id="18171" url="https://en.wikipedia.org/wiki?curid=18171" title="Linear search">
Linear search

In computer science, linear search or sequential search is a method for finding a particular value in a list that checks each element in sequence until the desired element is found or the list is exhausted. The list need not be ordered.
Linear search is the simplest search algorithm; it is a special case of brute-force search. Its worst case cost is proportional to the number of elements in the list. Its expected cost is also proportional to the number of elements if all elements are searched equally. If the list has more than a few elements and is searched often, then more complicated search methods such as binary search or hashing may be appropriate. Those methods have faster search times but require additional resources to attain that speed.
Analysis.
For a list with "n" items, the best case is when the value is equal to the first element of the list, in which case only one comparison is needed. The worst case is when the value is not in the list (or occurs only once at the end of the list), in which case "n" comparisons are needed.
If the value being sought occurs "k" times in the list, and all orderings of the list are equally likely, the expected number of comparisons is
For example, if the value being sought occurs once in the list, and all orderings of the list are equally likely, the expected number of comparisons is formula_2. However, if it is "known" that it occurs once, then at most "n" - 1 comparisons are needed, and the expected number of comparisons is 
(for example, for "n" = 2 this is 1, corresponding to a single if-then-else construct).
Either way, asymptotically the worst-case cost and the expected cost of linear search are both O("n").
Non-uniform probabilities.
The performance of linear search improves if the desired value is more likely to be near the beginning of the list than to its end. Therefore, if some values are much more likely to be searched than others, it is desirable to place them at the beginning of the list.
In particular, when the list items are arranged in order of decreasing probability, and these probabilities are geometrically distributed, the cost of linear search is only O(1). If the table size "n" is large enough, linear search will be faster than binary search, whose cost is O(log "n").
Application.
Linear search is usually very simple to implement, and is practical when the list has only a few elements, or when performing a single search in an unordered list.
When many values have to be searched in the same list, it often pays to pre-process the list in order to use a faster method. For example, one may sort the list and use binary search, or build any efficient search data structure from it. Should the content of the list change frequently, repeated re-organization may be more trouble than it is worth.
As a result, even though in theory other search algorithms may be faster than linear search (for instance binary search), in practice even on medium-sized arrays (around 100 items or less) it might be infeasible to use anything else. On larger arrays, it only makes sense to use other, faster search methods if the data is large enough, because the initial time to prepare (sort) the data is comparable to many linear searches 
Pseudocode.
Forward iteration.
This pseudocode describes a typical variant of linear search, where the result of the search is supposed to be either the location of the list item where the desired value was found; or an invalid location "Λ", to indicate that the desired element does not occur in the list.
In this pseudocode, the last line is executed only after all list items have been examined with none matching..
If the list is stored as an array data structure, the location may be the index of the item found (usually between 1 and "n", or 0 and "n"−1). In that case the invalid location "Λ" can be any index before the first element (such as 0 or −1, respectively) or after the last one ("n"+1 or "n", respectively).
If the list is a simply linked list, then the item's location is its reference, and "Λ" is usually the null pointer.
Recursive version.
Linear search can also be described as a recursive algorithm:
Searching in reverse order.
Linear search in an array is usually programmed by stepping up an index variable until it reaches the last index. This normally requires two comparison instructions for each list item: one to check whether the index has reached the end of the array, and another one to check whether the item has the desired value. In many computers, one can reduce the work of the first comparison by scanning the items in reverse order.
Suppose an array "A" with elements indexed 1 to "n" is to be searched for a value "x". The following
pseudocode performs a forward search, returning "n + 1" if the value is not found:
The following pseudocode searches the array in the reverse order, returning 0 when the element is not found:
Most computers have a conditional branch instruction that tests the sign of a value in a register, or the sign of the result of the most recent arithmetic operation. One can use that instruction, which is usually faster than a comparison against some arbitrary value (requiring a subtraction), to implement the command "If "i" ≤ 0, then exit the loop".
This optimization is easily implemented when programming in machine or assembly language. That branch instruction is not directly accessible in typical high-level programming languages, although many compilers will be able to perform that optimization on their own.
Using a sentinel.
Another way to reduce the overhead is to eliminate all checking of the loop index. This can be done by inserting the desired item itself as a sentinel value at the far end of the list, as in this pseudocode:
With this stratagem, it is not necessary to check the value of "i" against the list length "n": even if "x" was not in "A" to begin with, the loop will terminate when "i" = "n + 1". However this method is possible only if the array slot "A"["n + 1"] exists but is not being otherwise used. Similar arrangements could be made if the array were to be searched in reverse order, and element "A" were available.
Although the effort avoided by these ploys is tiny, it is still a significant component of the overhead of performing each step of the search, which is small. Only if many elements are likely to be compared will it be worthwhile considering methods that make fewer comparisons but impose other requirements.
Linear search on an ordered list.
For ordered lists that must be accessed sequentially, such as linked lists or files with variable-length records lacking an index, the average performance can be improved by giving up at the first element which is greater than the unmatched target value, rather than examining the entire list.
If the list is stored as an ordered array, then binary search is almost always more efficient than linear search as with "n" > 8, say, unless there is some reason to suppose that most searches will be for the small elements near the start of the sorted list.

</doc>
<doc id="18172" url="https://en.wikipedia.org/wiki?curid=18172" title="Land mine">
Land mine

A land mine is an explosive device concealed under or on the ground and designed to destroy or disable enemy targets, ranging from combatants to vehicles and tanks, as they pass over or near it. Such a device is typically detonated automatically by way of pressure when a target steps on it or drives over it, though other detonation mechanisms are also sometimes used. A land mine may cause damage by direct blast effect, by fragments that are thrown by the blast, or by both.
The name originates from the ancient practice of military mining, where tunnels were dug under enemy fortifications or troop formations by sappers. These killing tunnels ("mines") were at first collapsed to destroy targets located above, but they were later filled with explosives and detonated in order to cause even greater devastation.
Nowadays, in common parlance, land mines generally refer to devices specifically manufactured as anti-personnel or anti-vehicle weapons. Though many types of improvised explosive devices ("IEDs") can technically be classified as land mines, the term "land mine" is typically reserved for manufactured devices designed to be used by recognized military services, whereas "IED" is used for makeshift devices assembled by paramilitary, insurgent, or terrorist groups.
The use of land mines is controversial because of their potential as indiscriminate weapons. They can remain dangerous many years after a conflict has ended, harming the economy and civilians of many developing nations. With pressure from a number of campaign groups organised through the International Campaign to Ban Landmines, a global movement to prohibit their use led to the 1997 Convention on the Prohibition of the Use, Stockpiling, Production and Transfer of Anti-Personnel Mines and on their Destruction, also known as the "Ottawa Treaty". To date, 162 States have joined the treaty.
Use.
Land mines were designed for two main uses: 
Land mines are currently used in large quantities mostly for this first purpose, thus their widespread use in the demilitarized zones (DMZs) of likely flashpoints such as Cyprus, Afghanistan and Korea. As of 2013, the only governments that still laid land mines were Myanmar in its internal conflict, and Syria in its civil war.
Land mines continue to kill or injure at least 4,300 people every year, even decades after the ends of the conflicts for which they were placed.
History.
Pre-modern development.
Jiao Yu in the preface to his "Huolongjing Quanzhi", written in 1412 AD, claimed that the 3rd-century Chancellor Zhuge Liang of the Kingdom of Shu, had used not only "fire weapons" but land mines in the Battle of Hulugu Valley against the forces of Sima Yi and his son Sima Zhao of the Kingdom of Wei. This claim is dubious, as gunpowder warfare did not develop in China until the advent of the flamethrower (Pen Huo Qi) in the 10th century, while the land mine was not seen in China until the late 13th century.
Explosive land mines.
East Asia.
Explosive land mines were being used in 1277 by the Chinese during the Song dynasty against an assault of the Mongols, who were besieging a city in southern China. The invention of this detonated "enormous bomb" was credited to one Lou Qianxia of the 13th century. The famous 14th-century Chinese text of the "Huolongjing", which was the first to describe hollow cast iron cannonball shells filled with gunpowder, was also the first to describe the invention of the land mine in greater detail than references found in texts written beforehand.
This mid 14th century work compiled during the late Yuan dynasty and early Ming dynasty (before 1375, when its co-editor Liu Bowen died) stated that mines were made of cast iron and were spherical in shape, filled with either 'magic gunpowder', 'poison gunpowder', or 'blinding and burning gunpowder', any one of these compositions being suitable for use. The wad of the mine was made of hard wood, carrying three different fuses in case of defective connection to the touch hole.
In those days, the Chinese relied upon command signals and carefully timed calculation of enemy movements into the minefield, since a long fuse had to be ignited by hand from the ambushers in a somewhat far-off location lying in wait. The "Huolongjing" also describes land mines that were set off by enemy movement, called the 'ground-thunder explosive camp', one of the 'self-trespassing' (zifan) types, as the text says:
The "Huolongjing" describes the trigger device used for this as a 'steel wheel', which directed sparks of flame onto the connection of fuses running to the multiple-laid land mines underneath the carefully hidden trap. Further description of how this flint device operated was not made until a Chinese text of 1606 AD revealed that a weight drive (common in medieval clockworks) had been used to work the 'steel wheel'.
The way in which the Chinese land mine trigger worked was a system of two steel wheels rotated by a falling weight, the chord of which was wound around their axle, and when the enemy stepped onto the disguised boards they released the pins that dropped the weights. In terms of global significance, the first wheellock musket in Europe was sketched by Leonardo da Vinci around 1500 AD, although no use of metal flint for gunpowder weapons were known before that point in Europe.
Besides the use of steel wheels providing sparks for the fuses, there were other methods used as well, such as the 'underground sky-soaring thunder'. The Ming Dynasty (1368–1644) text of the "Wubei Zhi" (Treatise on Armament Technology), written by Mao Yuanyi in 1628, outlined the use of land mines that were triggered by the heat of a slow-burning incandescent material in an underground bowl placed directly above the train of fuses leading to the mines buried 3 ft beneath. The booby trap of this mine system had a mound where weapons of halberds, pikes, and lances were dug in, meant to entice the enemy to walk up the small mound and claim their stolen prize of war booty.
When the weapons were removed from the mound, this movement disturbed the bowl beneath them where the butt ends of the staffs were, which in turn ignited the fuses. According to the "Wubei Huolongjing" volume of the 17th century, the formula for this slow-burning incandescent material allowed it to burn continuously for 20 to 30 days without going out. This formula included of white sandal wood powder, of iron rust (ferric oxide), of 'white' charcoal powder (from quicklime), of willow charcoal powder, of dried, ground, and powdered red dates, and of bran.
The Chinese also made use of the naval mine at sea and on the rivers of China and elsewhere in maritime battles.
Europe and the United States.
The first known land mine in Europe was created by Pedro Navarro (d. 1528), a Spanish soldier, who used it in the settles of the Italian castles, in the beginning of the sixteenth century.
At Augsburg in 1573, a German military engineer by the name of Samuel Zimmermann invented an extremely effective mine known as the "Fladdermine". It consisted of a fougasse (or later, sometimes a "shell fougasse", that is, a fougasse loaded not with stones but with early black powder mortar shells, similar to large black powder hand grenades) activated by a snaphance or flintlock mechanism connected to a tripwire on the surface. Combining the effects of a tripwire activated bounding fragmentation mine with a cluster bomb, it was devastating to massed attackers but required high maintenance due to the susceptibility of black powder to dampness. Consequently, it was mainly employed in the defenses of major fortifications, in which role it continued to be used until the 1870s.
In Europe in the early eighteenth century, improvised land mines or booby traps were constructed in the form of bombs buried in shallow wells in the earth and covered with scrap metal and/or gravel to serve as shrapnel. Known in French as "fougasse", the term is sometimes still used in the present day to describe such devices. This technique was used in several European wars of the eighteenth century, the American Revolution, and the American Civil War.
The first modern mechanically fused high explosive anti-personnel land mines were created by Confederate troops of Brigadier General Gabriel J. Rains during the Battle of Yorktown in 1862. As a Captain, Rains had earlier employed explosive booby traps during the Seminole Wars in Florida in 1840. Both mechanically and electrically fused "land torpedoes" were employed, although by the end of the war mechanical fuses had been found to be generally more reliable. Many of these designs were improvised in the field, especially from explosive shells, but by the end of the war nearly 2,000 standard pattern "Rains mines" had been deployed.
Improved designs of mines were created in Imperial Germany, circa 1912, and were copied and manufactured by all major participants in the First World War. Both sides employed land mines (defensively) and tunnel mines (offensively). Well before the war was over, the British were manufacturing land mines that contained poison gas instead of explosives. Poison gas mines were manufactured at least until the 1980s in the Soviet Union. The United States was known to have at least experimented with the concept in the 1950s.
Nuclear mines have also been developed, both land and naval varieties. An example is the British Blue Peacock project, while another was the U.S. Medium Atomic Demolition Munition.
Characteristics and functioning.
A typical land mine includes the following components:
Firing mechanisms and initiating actions.
A land mine can be triggered by a number of things including pressure, movement, sound, magnetism and vibration. Anti-personnel mines commonly use the pressure of a person's foot as a trigger, but tripwires are also frequently employed. Most modern anti-vehicle mines use a magnetic trigger to enable it to detonate even if the tires or tracks did not touch it. Advanced mines are able to sense the difference between friendly and enemy types of vehicles by way of a built-in signature catalog. This will theoretically enable friendly forces to use the mined area while denying the enemy access.
Many mines combine the main trigger with a touch or tilt trigger to prevent enemy engineers from defusing it. Land mine designs tend to use as little metal as possible to make searching with a metal detector more difficult; land mines made mostly of plastic have the added advantage of being very inexpensive.
Some types of modern mines are designed to self-destruct, or chemically render themselves inert after a period of weeks or months to reduce the likelihood of civilian casualties at the conflict's end. These self-destruct mechanisms are not absolutely reliable, and most land mines laid historically are not equipped in this manner.
Anti-handling devices.
Anti-handling devices detonate the mine if someone attempts to lift, shift or disarm it. The intention is to hinder deminers by discouraging any attempts to clear minefields. There is a degree of overlap between the function of a boobytrap and an anti-handling device insofar as some mines have optional fuze pockets into which standard pull or pressure-release boobytrap firing devices can be screwed. Alternatively, some mines may mimic a standard design, but actually be specifically intended to kill deminers, such as the MC-3 and PMN-3 variants of the PMN mine. Anti-handling devices can be found on both anti-personnel mines and anti-tank mines, either as an integral part of their design or as improvised add-ons. For this reason, the standard render safe procedure for mines is often to destroy them on site without attempting to lift them.
Anti-tank mines.
Anti-tank mines were created not long after the invention of the tank in the First World War. At first improvised, purpose-built designs were developed. Set off when a tank passes, they attack the tank at one of its weaker areas — the tracks. They are designed to immobilize or destroy vehicles and their occupants. In U.S. military terminology destroying the vehicles is referred to as a catastrophic kill (k-kill) while only disabling its movement is referred to as a mobility kill (m-kill).
Anti-tank mines are typically larger than anti-personnel mines and require more pressure to detonate. The high trigger pressure, normally requiring prevents them from being set off by infantry or smaller vehicles of lesser importance. More modern anti-tank mines use shaped charges to focus and increase the armor penetration of the explosives.
Anti-personnel mines.
Anti-personnel mines are designed to kill or injure enemy combatants as opposed to destroying vehicles. They are often designed to injure rather than kill in order to increase the logistical support (evacuation, medical) burden on the opposing force. Some types of anti-personnel mines can also damage the tracks or wheels of armored vehicles.
Under the Ottawa Treaty, the Parties undertake not to use, produce, stockpile or transfer anti-personnel mines and ensure their destruction.
As of early 2016, 162 countries have joined the Treaty. Thirty-six countries, including the People's Republic of China, the Russian Federation and the United States, which together may hold tens of millions of stockpiled antipersonnel mines, are not yet party to the Convention.
Warfare.
In military science, minefields are considered a defensive or harassing weapon, used to slow the enemy down, to help deny certain terrain to the enemy, to focus enemy movement into kill zones, or to reduce morale by randomly attacking material and personnel. In some engagements during World War II, anti-tank mines accounted for half of all vehicles disabled.
Since combat engineers with mine-clearing equipment can clear a path through a minefield relatively quickly, mines are usually considered effective only if covered by fire.
The extents of minefields are often marked with warning signs and cloth tape, to prevent friendly troops and non-combatants from entering them. Of course, sometimes terrain can be denied using dummy minefields. Most forces carefully record the location and disposition of their own minefields, because warning signs can be destroyed or removed, and minefields should eventually be cleared. Minefields may also have marked or unmarked safe routes to allow friendly movement through them.
Placing minefields without marking and recording them for later removal is considered a war crime under Protocol II of the Convention on Certain Conventional Weapons, which is itself an annex to the Geneva Conventions.
Artillery and aircraft scatterable mines allow minefields to be placed in front of moving formations of enemy units, including the reinforcement of minefields or other obstacles that have been breached by enemy engineers. They can also be used to cover the retreat of forces disengaging from the enemy, or for interdiction of supporting units to isolate front line units from resupply. In most cases these minefields consist of a combination of anti-tank and anti-personnel mines, with the anti-personnel mines making removal of the anti-tank mines more difficult. Mines of this type used by the United States are designed to self-destruct after a preset period of time, reducing the requirement for mine clearing to only those mines whose self-destruct system did not function. Some designs of these scatterable mines require an electrical charge (capacitor or battery) to detonate. After a certain period of time, either the charge dissipates, leaving them effectively inert or the circuitry is designed such that upon reaching a low level, the device is triggered, thus destroying the mine.
Guerrilla warfare.
None of the conventional tactics and norms of mine warfare applies when they are employed in a guerrilla role:
One example where such tactics were employed is in the various Southern African conflicts during the 1970s and 1980s, specifically Angola, Mozambique, Namibia, South Africa and Zimbabwe.
Laying mines.
Minefields may be laid by several means. The preferred, but most labour-intensive, way is to have engineers bury the mines, since this will make the mines practically invisible and reduce the number of mines needed to deny the enemy an area. Mines can be laid by specialized mine-laying vehicles. Mine-scattering shells may be fired by artillery from a distance of several tens of kilometers.
Mines may be dropped from helicopters or airplanes, or ejected from cluster bombs or cruise missiles.
Anti-tank minefields can be scattered with anti-personnel mines to make clearing them manually more time-consuming; and anti-personnel minefields are scattered with anti-tank mines to prevent the use of armored vehicles to clear them quickly. Some anti-tank mine types are also able to be triggered by infantry, giving them a dual purpose even though their main and official intention is to work as anti-tank weapons.
Some minefields are specifically booby-trapped to make clearing them more dangerous. Mixed anti-personnel and anti-tank minefields, anti-personnel mines "under" anti-tank mines, and fuses separated from mines have all been used for this purpose. Often, single mines are backed by a secondary device, designed to kill or maim personnel tasked with clearing the mine.
Multiple anti-tank mines have been buried in stacks of two or three with the bottom mine fuzed, in order to multiply the penetrating power. Since the mines are buried, the ground directs the energy of the blast in a single direction — through the bottom of the target vehicle or on the track.
Another specific use is to mine an aircraft runway immediately after it has been bombed in order to delay or discourage repair. Some cluster bombs combine these functions. One example is the British JP233 cluster bomb which includes munitions to damage (crater) the runway as well as anti-personnel mines in the same cluster bomb.
Demining.
Metal detectors were first used for demining, after their invention by the Polish officer Józef Kosacki. His invention, known as the Polish mine detector, was used by the Allies alongside mechanical methods, to clear the German mine fields during the Second Battle of El Alamein when 500 units were shipped to Field Marshal Montgomery's Eighth Army.
The Nazis used captured civilians who were chased across minefields to detonate the explosives. According to Laurence Rees, "Curt von Gottberg, the SS-Obergruppenfuhrer who, during 1943, conducted another huge anti-partisan action called Operation Kottbus on the eastern border of Belorussia, reported that 'approximately two to three thousand local people were blown up in the clearing of the minefields'."
Whereas the placing and arming of mines is relatively inexpensive and simple, the process of detecting and removing them is typically expensive, slow, and dangerous. This is especially true of irregular warfare where mines were used on an ad hoc basis in unmarked areas. Anti-personnel mines are most difficult to find, due to their small size and the fact that many are made almost entirely of non-metallic materials specifically to escape detection.
Manual clearing remains the most effective technique for clearing mine fields, although hybrid techniques involving the use of animals and robots are being developed. Animals are desirable due to their strong sense of smell, which is more than capable of detecting a land mine. Animals like rats and dogs can also differentiate between other metal objects and land mines because they can be trained to detect the explosive agent itself.
Other techniques involve the use of geo-location technologies. A joint team of researchers at the University of New South Wales and Ohio State University is working to develop a system based on multi-sensor integration.
The laying of land mines has inadvertently led to a positive development in the Falkland Islands. Mine fields laid near the sea during the Falklands War have become favorite places for penguins, which do not weigh enough to detonate the mines. Therefore, they can breed safely, free of human intrusion. These odd sanctuaries have proven so popular and lucrative for ecotourism that efforts exist to prevent removal of the mines.
Norwegian NGO Norwegian People's Aid is one organisation involved in the safe removal of land mines.
Anti-personnel mine ban.
The use of land mines is controversial because they are indiscriminate weapons, harming soldier and civilian alike. They remain dangerous after the conflict in which they were deployed has ended, killing and injuring civilians and rendering land impassable and unusable for decades. To make matters worse, many factions have not kept accurate records (or any at all) of the exact locations of their minefields, making removal efforts painstakingly slow. These facts pose serious difficulties in many developing nations where the presence of mines hampers resettlement, agriculture, and tourism. The International Campaign to Ban Landmines campaigned successfully to prohibit their use, culminating in the 1997 Convention on the Prohibition of the Use, Stockpiling, Production and Transfer of Anti-Personnel Mines and on their Destruction, known informally as the Ottawa Treaty.
The Treaty came into force on 1 March 1999. The treaty was the result of the leadership of the Governments of Canada, Norway, South Africa and Mozambique working with the "International Campaign to Ban Landmines", launched in 1992. The campaign and its leader, Jody Williams, won the Nobel Peace Prize in 1997 for its efforts.
The treaty does not include anti-tank mines, cluster bombs or claymore-type mines operated in command mode and focuses specifically on anti-personnel mines, because these pose the greatest long term (post-conflict) risk to humans and animals since they are typically designed to be triggered by any movement or pressure of only a few kilograms, whereas anti-tank mines require much more weight (or a combination of factors that would exclude humans). Existing stocks must be destroyed within four years of signing the treaty.
Signatories of the Ottawa Treaty agree that they will not use, produce, stockpile or trade in anti-personnel land mines. In 1997, there were 122 signatories; the Treaty has not been signed by 162 countries. Another 34 have yet to sign on. The United States is not one of the signatories, based on lacking an exception for the DMZ of Korea.
There is a clause in the treaty, Article 3, which permits countries to retain land mines for use in training or development of countermeasures. 64 countries have taken this option.
As an alternative to an outright ban, 10 countries follow regulations that are contained in a 1996 amendment of Protocol II of the Convention on Conventional Weapons (CCW). The countries are China, Finland, India, Israel, Morocco, Pakistan, South Korea and the United States. Sri Lanka, which had adhered to this regulation announced in 2016, that it would join the Ottawa Treaty.
Manufacturers.
The ICBL has identified the following countries as manufacturing land mines as of August 2004. None are signatories of the Ottawa Treaty.
Of other states which are thought to have manufactured mines recently:

</doc>
<doc id="18173" url="https://en.wikipedia.org/wiki?curid=18173" title="List of libertarian political parties">
List of libertarian political parties

Many countries and subnational political entities have libertarian political parties. Although these parties may describe themselves as "libertarian," their ideologies differ considerably and not all of them support all elements of the libertarian agenda.

</doc>
<doc id="18175" url="https://en.wikipedia.org/wiki?curid=18175" title="Loa">
Loa

Loa (also spelled "Lwa" or "L'wha") are the spirits of Haitian Vodou and Louisiana Voodoo. They are also referred to as Mystères and the Invisibles and are intermediaries between Bondye (French: "Bon Dieu", meaning "good God")—the Supreme Creator, who is distant from the world—and humanity. Unlike saints or angels, however, they are not simply prayed to, they are served. They are each distinct beings with their own personal likes and dislikes, distinct sacred rhythms, songs, dances, ritual symbols, and special modes of service. Contrary to popular belief, the loa are not deities in and of themselves; they are intermediaries for, and dependent on, a distant Bondye.
Syncretism.
The enslaved Fon and Ewe in Haiti and Louisiana syncretized the Loa with the Roman Catholic Saints—Vodoun altars will frequently display images of Catholic saints. For example, Papa Legba is syncretized with St. Peter or St. Lazarus.
Syncretism also works the other way in Haitian Vodou and many Catholic saints have become Loa in their own right, most notably Philomena, St. Michael the Archangel, St. Jude, and John the Baptist.
Rituals.
In a ritual the Loa are summoned by the "Houngan" (Priest), "Mambo" (Priestess), or "Bokor" (Sorcerers) to take part in the service, receive offerings, and grant requests. The loa arrive in the peristyle (ritual space) by mounting (possessing) a horse (ritualist) - who is said to be "ridden". This can be quite a violent occurrence as the participant can flail about or convulse before falling to the ground, but some loa, such as Ayizan, will mount their "horses" very quietly.
Certain loa display very distinctive behaviour by which they can be recognized, specific phrases, and specific actions. As soon as a loa is recognized, the symbols appropriate to them will be given to them. For example, Erzulie Freda will be given a mirror and a comb, fine cloth or jewelry; Legba will be given his cane, straw hat, and pipe; Baron Samedi will be given his top hat, sunglasses, and a cigar.
Once the loa have arrived, fed, been served, and possibly given help or advice, they leave the peristyle. Certain loa can become obstinate, for example the Ghede are notorious for wanting just one more smoke, or one more drink, but it is the job of the Houngan or Mambo to keep the spirits in line while ensuring they are adequately provided for.
Nations of Loa.
There are many families or ""nanchons"" (nations) of Loa: Rada (also Radha), Petro (also Pethro, Petwo), Nago, Kongo, and Ghede (also Guede, or Gede), among others.
Rada loa.
The Rada loa are generally the older, more beneficent loa. They include Legba, Loko, Ayizan, Anaisa Pye, Dhamballah Wedo and Ayida-Weddo, Erzulie Freda, La Sirène, and Agwé. Their traditional colour is white (as opposed to the specific colours of individual loa).
Petro loa.
The Petro loa are generally the more fiery, occasionally aggressive and warlike loa, and are associated with Haiti and the New World. They include Ezili Dantor, Marinette, and Met Kalfu (Maitre Carrefour, "Master Crossroads"). Their traditional colour is red.
Kongo loa.
Originating from the Congo region, these loa include the many Simbi loa. It also includes Marinette, a fierce and much feared female loa.
Nago loa.
Originating from Yorubaland, this nation includes many of the Ogoun loa.
Ghede loa.
The Ghede are the spirits of the dead. They are traditionally led by the Barons (La Croix, Samedi, Cimitière, Kriminel), and Maman Brigitte. The Ghede as a family are loud, rude (although rarely to the point of real insult), sexual, and usually a lot of fun. As those who have lived already, they have nothing to fear, and frequently will display how far past consequence and feeling they are when they come through in a service—eating glass, raw chillis, and anointing their sensitive areas with chilli rum, for example. Their traditional colours are black and purple.

</doc>
<doc id="18178" url="https://en.wikipedia.org/wiki?curid=18178" title="Labour economics">
Labour economics

Labour economics seeks to understand the functioning and dynamics of the markets for wage labour. Labour markets or job markets function through the interaction of workers and employers. Labour economics looks at the suppliers of labour services (workers), the demands of labour services (employers), and attempts to understand the resulting pattern of wages, employment, and income.
In economics, labour is a measure of the work done by human beings. It is conventionally contrasted with such other factors of production as land and capital. There are theories which have developed a concept called human capital (referring to the skills that workers possess, not necessarily their actual work).
Macro and micro analysis of labour markets.
There are two sides to labour economics. Labour economics can generally be seen as the application of microeconomic or macroeconomic techniques to the labour market. Microeconomic techniques study the role of individuals and individual firms in the labour market. Macroeconomic techniques look at the interrelations between the labour market, the goods market, the money market, and the foreign trade market. It looks at how these interactions influence macro variables such as employment levels, participation rates, aggregate income and gross domestic product.
The macroeconomics of labour markets.
The labour force is defined as the number of people of working age, who are either employed or actively looking for work. The participation rate is the number of people in the labour force divided by the size of the adult civilian noninstitutional population (or by the population of working age that is not institutionalised). The nonlabour force includes those who are not looking for work, those who are institutionalised such as in prisons or psychiatric wards, stay-at home spouses, children, and those serving in the military. The unemployment level is defined as the labour force minus the number of people currently employed. The unemployment rate is defined as the level of unemployment divided by the labour force. The employment rate is defined as the number of people currently employed divided by the adult population (or by the population of working age). In these statistics, self-employed people are counted as employed.
Variables like employment level, unemployment level, labour force, and unfilled vacancies are called stock variables because they measure a quantity at a point in time. They can be contrasted with flow variables which measure a quantity over a duration of time. Changes in the labour force are due to flow variables such as natural population growth, net immigration, new entrants, and retirements from the labour force. Changes in unemployment depend on: inflows made up of non-employed people starting to look for jobs and of employed people who lose their jobs and look for new ones; and outflows of people who find new employment and of people who stop looking for employment. When looking at the overall macroeconomy, several types of unemployment have been identified, including:
Neoclassical microeconomics of labour markets.
Neoclassical economists view the labour market as similar to other markets in that the forces of supply and demand jointly determine price (in this case the wage rate) and quantity (in this case the number of people employed).
However, the labour market differs from other markets (like the markets for goods or the financial market) in several ways. Perhaps the most important of these differences is the function of supply and demand in setting price and quantity. In markets for goods, if the price is high there is a tendency in the long run for more goods to be produced until the demand is satisfied. With labour, overall supply cannot effectively be manufactured because people have a limited amount of time in the day, and people are not manufactured.
The labour market also acts as a non-clearing market. While according to neoclassical theory most markets have a point of equilibrium without excess surplus or demand, this may not be true of the labor market: it may have a persistent level of unemployment. Contrasting the labour market to other markets also reveals persistent compensating differentials among similar workers.
Models that assume perfect competition in the labour market, as discussed below, conclude that workers earn their marginal product of labour.
Neoclassical microeconomic model — Supply.
Households are suppliers of labour. In microeconomic theory, people are assumed to be rational and seeking to maximize their utility function. In the labour market model, their utility function expresses trade-offs in preference between leisure time and income from time used for labour. However, they are constrained by the hours available to them.
Let "w" denote the hourly wage, "k" denote total hours available for labour and leisure, "L" denote the chosen number of working hours, π denote income from non-labour sources, and "A" denote leisure hours chosen. The individual's problem is to maximise utility "U", which depends on total income available for spending on consumption and also depends on time spent in leisure, subject to a time constraint, with respect to the chooses of labour time and leisure time:
This can be shown in a graph that illustrates the trade-off between allocating time between leisure activities and income-generating activities. The linear constraint indicates that there are only 24 hours in a day, and individuals must choose how much of this time to allocate to leisure activities and how much to working. This allocation decision is informed by the indifference curve labelled IC. The curve indicates the combinations of leisure and work that will give the individual a specific level of utility. The point where the highest indifference curve is just tangent to the constraint line (point A), illustrates the optimum for this supplier of labour services.
"The Income/Leisure trade-off in the short run"
If consumption is measured by the value of income obtained, this diagram can be used to show a variety of interesting effects. This is because the absolute value of the slope of the budget constraint is the wage rate. The point of optimisation (point A) reflects the equivalency between the wage rate and the marginal rate of substitution of leisure for income (the absolute value of the slope of the indifference curve). Because the marginal rate of substitution of leisure for income is also the ratio of the marginal utility of leisure (MUL) to the marginal utility of income (MUY), one can conclude:
where "Y" is total income and the right side is the wage rate.
"Effects of a wage increase"
If the wage rate increases, this individual's constraint line pivots up from X,Y1 to X,Y2. He/she can now purchase more goods and services. His/her utility will increase from point A on IC1 to point B on IC2.
To understand what effect this might have on the decision of how many hours to work, one must look at the income effect and substitution effect.
The wage increase shown in the previous diagram can be decomposed into two separate effects. The pure income effect is shown as the movement from point A to point C in the next diagram. Consumption increases from YA to YC and — since the diagram assumes that leisure is a normal good — leisure time increases from XA to XC. (Employment time decreases by the same amount as leisure increases.)
"The Income and Substitution effects of a wage increase"
But that is only part of the picture. As the wage rate rises, the worker will substitute away from leisure and into the provision of labour—that is, will work more hours to take advantage of the higher wage rate, or in other words substitute away from leisure because of its higher opportunity cost. This substitution effect is represented by the shift from point C to point B. The net impact of these two effects is shown by the shift from point A to point B. The relative magnitude of the two effects depends on the circumstances. In some cases, such as the one shown, the substitution effect is greater than the income effect (in which case more time will be allocated to working), but in other cases the income effect will be greater than the substitution effect (in which case less time is allocated to working). The intuition behind this latter case is that the individual decides that the higher earnings on the previous amount of labour can be "spent" by purchasing more leisure.
"The Labour Supply curve"
If the substitution effect is greater than the income effect, the labour supply curve (in the diagram to the left) will slope upwards to the right, as it does at point E for example. This individual will continue to increase his supply of labour services as the wage rate increases up to point F where he is working HF hours (each period of time). Beyond this point he will start to reduce the amount of labour hours he supplies (for example at point G he has reduced his work hours to HG) because the income effect of the wage rate has come to dominate the substitution effect. Where the supply curve is sloping upwards to the right (showing a positive wage elasticity), the substitution effect is greater than the income effect. Where it slopes upwards to the left (showing a negative wage elasticity), the income effect is greater than the substitution effect. The direction of slope may change more than once for some individuals, and the labour supply curve is different for different individuals.
Other variables that affect the labour supply decision, and can be readily incorporated into the model, include taxation, welfare, work environment, and income as a signal of ability or social contribution.
Neoclassical microeconomic model — Demand.
A firm's labour demand is based on its marginal physical product of labour (MPPL). This is defined as the additional output (or physical product) that results from an increase of one unit of labour (or from an infinitesimal increase in labour). (If you are not familiar with these concepts, you might want to look at production theory basics before continuing with this article)
Labour demand is a derived demand; that is, hiring labour is not desired for its own sake but rather because it aids in producing output, which contributes to an employer's revenue and hence profits. The demand for an additional amount of labour depends on the Marginal Revenue Product (MRP) and the marginal cost (MC) of the worker. The MRP is calculated by multiplying the price of the end product or service by the Marginal Physical Product of the worker. If the MRP is greater than a firm's Marginal Cost, then the firm will employ the worker since doing so will increase profit. The firm only employs however up to the point where MRP=MC, and not beyond, in neoclassical economic theory.
The MRP of the worker is affected by other inputs to production with which the worker can work (e.g. machinery), often aggregated under the term "capital". It is typical in economic models for greater availability of capital for a firm to increase the MRP of the worker, all else equal. Education and training are counted as "human capital". Since the amount of physical capital affects MRP, and since financial capital flows can affect the amount of physical capital available, MRP and thus wages can be affected by financial capital flows within and between countries, and the degree of capital mobility within and between countries.
"The Marginal Physical Product of Labour"
According to neoclassical theory, over the relevant range of outputs, the marginal physical product of labour is declining (law of diminishing returns). That is, as more and more units of labour are employed, their additional output begins to decline. This is reflected by the slope of the MPPL curve in the diagram to the right. If the marginal physical product of labour is multiplied by the value of the output that it produces, we obtain the Value of marginal physical product of labour:
The value of marginal physical product of labour (formula_4) is the value of the additional output produced by an additional unit of labour. This is illustrated in the diagram by the VMPPL curve that is above the MPPL.
In perfectly competitive industries, the VMPPL is in identity with the marginal revenue product of labour (MRPL). This is because in competitive markets price is equal to marginal revenue, and marginal revenue product is defined as the marginal physical product times the marginal revenue from the output (MRP = MPP * MR). The marginal revenue product of labour can be used as the demand for labour curve for this firm in the short run.
Neoclassical microeconomic model — Equilibrium.
"A firm's labour demand in the short run (D) and an horizontal supply curve (S)"
The marginal revenue product of labour can be used as the demand for labour curve for this firm in the short run. In competitive markets, a firm faces a perfectly elastic supply of labour which corresponds with the wage rate and the marginal resource cost of labour (W = SL = MFCL). In imperfect markets, the diagram would have to be adjusted because MFCL would then be equal to the wage rate divided by marginal costs. Because optimum resource allocation requires that marginal factor costs equal marginal revenue product, this firm would demand L units of labour as shown in the diagram.
The demand for labour of this firm can be summed with the demand for labour of all other firms in the economy to obtain the aggregate demand for labour. Likewise, the supply curves of all the individual workers (mentioned above) can be summed to obtain the aggregate supply of labour. These supply and demand curves can be analysed in the same way as any other industry demand and supply curves to determine equilibrium wage and employment levels.
Wage differences exist, particularly in mixed and fully/partly flexible labour markets. For example, the wages of a doctor and a port cleaner, both employed by the NHS, differ greatly. There are various factors concerning this phenomenon. This includes the MRP of the worker. A doctor's MRP is far greater than that of the port cleaner. In addition, the barriers to becoming a doctor are far greater than that of becoming a port cleaner. To become a doctor takes a lot of education and training which is costly, and only those who excel in academia can succeed in becoming doctors. The port cleaner however requires relatively less training. The supply of doctors is therefore significantly less elastic than that of port cleaners. Demand is also inelastic as there is a high demand for doctors and medical care is a necessity, so the NHS will pay higher wage rates to attract the profession.
Monopsony.
Some labour markets have a single employer and thus do not satisfy the perfect competition assumption of the neoclassical model above. The model of a monopsonistic labour market gives a lower quantity of employment and a lower equilibrium wage rate than does the competitive model.
Information approaches.
In many real-life situations the assumption of perfect information is unrealistic. An employer does not necessarily know how hard worker are working or how productive they are. This provides an incentive for workers to shirk from providing their full effort — since it is difficult for the employer to identify the hard-working and the shirking employees, there is no incentive to work hard and productivity falls overall, leading to the hiring of more workers and a lower unemployment rate.
One solution used recently - stock options - grants employees the chance to benefit directly from a firm's success. However, this solution has attracted criticism as executives with large stock-option packages have been suspected of acting to over-inflate share values to the detriment of the long-run welfare of the firm. Another solution, foreshadowed by the rise of temporary workers in Japan and the firing of many of these workers in response to the financial crisis of 2008, is more flexible job- contracts and -terms that encourage employees to work less than full-time by partially compensating for the loss of hours, relying on workers to adapt their working time in response to job requirements and economic conditions instead of the employer trying to determine how much work is needed to complete a given task and overestimating.
Another aspect of uncertainty results from the firm's imperfect knowledge about worker ability. If a firm is unsure about a worker's ability, it pays a wage assuming that the worker's ability is the average of similar workers. This wage undercompensates high-ability workers and may drive them away from the labour market. Such a phenomenon, called adverse selection, can sometimes lead to market collapse.
There are many ways to overcome adverse selection in labour market. One important mechanism is called signalling, pioneered by Michael Spence. In his classical paper on job signalling, Spence showed that even if formal education does not increase productivity, high-ability workers may still acquire it just to signal their abilities. Employers can then use education as a signal to infer worker ability and pay higher wages to better-educated workers. It may appear to an external observer that education has raised the marginal product of labour, without this necessarily being true.
Search models.
One of the major research achievements of the 1990-2010 period was the development of a framework with dynamic search, matching, and bargaining.
Personnel economics: hiring and incentives.
At the micro level, one sub-discipline eliciting increased attention in recent decades is analysis of internal labour markets, that is, "within" firms (or other organisations), studied in personnel economics from the perspective of personnel management. By contrast, external labour markets "imply that workers move somewhat fluidly between firms and wages are determined by some aggregate process where firms do not have significant discretion over wage setting." The focus is on "how firms establish, maintain, and end employment relationships and on how firms provide incentives to employees," including models and empirical work on incentive systems and as constrained by economic efficiency and risk/incentive tradeoffs relating to personnel compensation.
Criticisms.
Many sociologists, political economists, and heterodox economists claim that labour economics tends to lose sight of the complexity of individual employment decisions. These decisions, particularly on the supply side, are often loaded with considerable emotional baggage and a purely numerical analysis can miss important dimensions of the process, such as social benefits of a high income or wage rate regardless of the marginal utility from increased consumption or specific economic goals.
From the perspective of mainstream economics, neoclassical models are not meant to serve as a full description of the psychological and subjective factors that go into a given individual's employment relations, but as a useful approximation of human behavior in the aggregate, which can be fleshed out further by the use of concepts such as information asymmetry, transaction costs, contract theory etc.
Also missing from most labour market analyses is the role of unpaid labour such as unpaid internships where workers with little or no experience are allowed to work a job without pay so that they can gain experience in a particular profession. Even though this type of labour is unpaid it can nevertheless play an important part in society if not abused by employers. The most dramatic example is child raising. However, over the past 25 years an increasing literature, usually designated as the economics of the family, has sought to study within household decision making, including joint labour supply, fertility, child raising, as well as other areas of what is generally referred to as home production.
Wage slavery.
The labour market, as institutionalised under today's market economic systems, has been criticised, especially by both mainstream socialists and anarcho-syndicalists, who utilise the term wage slavery as a pejorative for wage labour. Socialists draw parallels between the trade of labour as a commodity and slavery. Cicero is also known to have suggested such parallels.
According to Noam Chomsky, analysis of the psychological implications of wage slavery goes back to the Enlightenment era. In his 1791 book "On the Limits of State Action", classical liberal thinker Wilhelm von Humboldt explained how "whatever does not spring from a man's free choice, or is only the result of instruction and guidance, does not enter into his very nature; he does not perform it with truly human energies, but merely with mechanical exactness" and so when the labourer works under external control, "we may admire what he does, but we despise what he is." Both the Milgram and Stanford experiments have been found useful in the psychological study of wage-based workplace relations.
The American philosopher John Dewey posited that until "industrial feudalism" is replaced by "industrial democracy," politics will be "the shadow cast on society by big business". Thomas Ferguson has postulated in his investment theory of party competition that the undemocratic nature of economic institutions under capitalism causes elections to become occasions when blocs of investors coalesce and compete to control the state.
As per anthropologist David Graeber, the earliest wage labour contracts we know about were in fact contracts for the rental of chattel slaves (usually the owner would receive a share of the money, and the slave, another, with which to maintain his or her living expenses.) Such arrangements, according to Graeber, were quite common in New World slavery as well, whether in the United States or Brazil. C. L. R. James argued that most of the techniques of human organisation employed on factory workers during the industrial revolution were first developed on slave plantations.
Additionally, Marxists posit that labour-as-commodity, which is how they regard wage labour, provides an absolutely fundamental point of attack against capitalism. "It can be persuasively argued," noted one concerned philosopher, "that the conception of the worker's labour as a commodity confirms Marx's stigmatisation of the wage system of private capitalism as 'wage-slavery;' that is, as an instrument of the capitalist's for reducing the worker's condition to that of a slave, if not below it."

</doc>
<doc id="18179" url="https://en.wikipedia.org/wiki?curid=18179" title="Lammas">
Lammas

In some English-speaking countries in the Northern Hemisphere, August 1 is Lammas Day (Anglo-Saxon "hlaf-mas", "loaf-mass"), the festival of the wheat harvest, and is the first harvest festival of the year. On this day it was customary to bring to church a loaf made from the new crop, which began to be harvested at Lammastide.
The loaf was blessed, and in Anglo-Saxon England it might be employed afterwards to work magic: a book of Anglo-Saxon charms directed that the lammas bread be broken into four bits, which were to be placed at the four corners of the barn, to protect the garnered grain.
In many parts of England, tenants were bound to present freshly harvested wheat to their landlords on or before the first day of August. In the "Anglo-Saxon Chronicle", where it is referred to regularly, it is called "the feast of first fruits". The blessing of first fruits was performed annually in both the Eastern and Western Churches on the first or the sixth of August (the latter being the feast of the Transfiguration of Christ).
Lammas has coincided with the feast of St. Peter in Chains, commemorating St. Peter's miraculous deliverance from prison, but in the liturgical reform of 1969, the feast of St. Alphonsus Liguori was transferred to this day, the day of St. Alphonsus' death.
History.
In medieval times the feast was sometimes known in England and Scotland as the "Gule of August", but the meaning of "gule" is unclear. Ronald Hutton suggests following the 18th-century Welsh clergyman antiquary John Pettingall that it is merely an Anglicisation of "Gŵyl Awst", the Welsh name of the "feast of August". "OED" and most etymological dictionaries give it a more circuitous origin similar to "gullet"; from O.Fr. "goulet", dim. of "goule", "throat, neck," from L. "gula" "throat,".
Several antiquaries beginning with John Brady offered a back-construction to its being originally known as "Lamb-mass", under the undocumented supposition that tenants of the Cathedral of York, dedicated to St. Peter ad Vincula, of which this is the feast, would have been required to bring a live lamb to the church, or, with John Skinner, "because Lambs then grew out of season." This is a folk etymology, of which "OED" notes that it was "subsequently felt as if from LAMB + MASS".
For many villeins, the wheat must have run low in the days before Lammas, and the new harvest began a season of plenty, of hard work and company in the fields, reaping in teams. Thus there was a spirit of celebratory play.
In the medieval agricultural year, Lammas also marked the end of the hay harvest that had begun after Midsummer. At the end of hay-making a sheep would be loosed in the meadow among the mowers, for him to keep who could catch it.
In Shakespeare's "Romeo and Juliet" (1.3.19) it is observed of Juliet, "Come Lammas Eve at night shall she be fourteen." Since Juliet was born Lammas eve, she came before the harvest festival, which is significant since her life ended before she could reap what she had sown and enjoy the bounty of the harvest, in this case full consummation and enjoyment of her love with Romeo.
Another well-known cultural reference is the opening of "The Battle of Otterburn": "It fell about the Lammas tide when the muir-men win their hay".
William Hone speaks in "The Every-Day Book" (1838) of a later festive Lammas day sport common among Scottish farmers near Edinburgh. He says that they "build towers...leaving a hole for a flag-pole in the centre so that they may raise their colours." When the flags over the many peat-constructed towers were raised, farmers would go to others' towers and attempt to "level them to the ground." A successful attempt would bring great praise. However, people were allowed to defend their towers, and so everyone was provided with a "tooting-horn" to alert nearby country folk of the impending attack and the battle would turn into a "brawl." According to Hone, more than four people had died at this festival and many more were injured. At the day's end, races were held, with prizes given to the townspeople.
Neopaganism.
Lughnasadh or Lammas is also the name used for one of the eight sabbats in the Neopagan Wheel of the Year. It is the first of the three autumn harvest festivals, the other two being the autumn equinox (also called Mabon) and Samhain. In the Northern Hemisphere it takes place around August 1, while in the Southern Hemisphere it is celebrated around February 1.
Other uses.
Lammas is one of the Scottish quarter days.
"Lammas leaves" or "Lammas growth" refers to a second crop of leaves produced in high summer by some species of trees in temperate countries to replace those lost to insect damage. They often differ slightly in shape, texture and/or hairiness from the earlier leaves.
In popular culture.
The "Doctor Who" serial, "The Image of the Fendahl", takes place on Lammas Eve.
In the "Inspector Morse" episode "Day of the Devil", Lammas Day is presented as a Satanic (un)holy day, "the Devil's day".

</doc>
<doc id="18182" url="https://en.wikipedia.org/wiki?curid=18182" title="Longmeadow, Massachusetts">
Longmeadow, Massachusetts

Longmeadow is a town in Hampden County, Massachusetts, in the United States. The population was 15,784 at the 2010 census.
History.
Longmeadow was first settled in 1644, and officially incorporated on October 17, 1783. The town was originally farmland within the limits of Springfield. It remained relatively pastoral until the street railway was built "circa" 1910, when the population tripled over a fifteen-year period. After Interstate 91 was built in the wetlands on the west side of town, population tripled again between 1960 and 1975.
During the 19th and early 20th centuries, Longmeadow was best known as the site from which Longmeadow brownstone was mined. Several famous American buildings, including Princeton University's Neo-Gothic library, are made of Longmeadow brownstone. In 1894, the more populous and industrialized "East Village" portion of the town containing the brownstone quarries split off to become East Longmeadow.
Designed by famed golf course architect Donald Ross in 1922, the Longmeadow Country Club was the proving ground for golf equipment designed and manufactured by the Spalding Co. of Chicopee. Bobby Jones, a consultant for Spalding, was also a member in standing at LCC and filmed a number of his instruction videos at LCC in the 1930s.
Geography.
Longmeadow is located in the western part of the state, just south of the city of Springfield, and is bordered on the west by the Connecticut River, to the east by East Longmeadow and to the south by Enfield, Connecticut. It extends approximately north to south and east to west. It is approximately north of Hartford.
More than 30% of the town is permanent open space. Conservation areas on the west side of town include more than bordering the Connecticut River. The area supports a wide range of wildlife including deer, beaver, wild turkeys, foxes, and eagles. Springfield's Forest Park, which, at , is the largest city park in New England, forms the northern border of the town. The private Twin Hills and public Franconia golf courses, plus town athletic fields and conservation land, cover nearly 2/3 of the eastern border of the town. Two large public parks, the Longmeadow Country Club, and three conservation areas account for the bulk of the remaining formal open space. Almost 20% of the houses in town are in proximity to a "dingle", a tree-lined steep sided sandy ravine with a wetland at the bottom that provides a privacy barrier between yards.
Longmeadow has a town common located along U.S. Route 5, on the west side of town. It is about 0.75-mile (1.2 kilometers) long. Roughly 100 houses date back before 1900, most of which are in the historic district near the town green. Houses along the photogenic main street are set back farther than in most towns of similar residential density. The town has three recently remodeled elementary schools, two secondary schools, and one high school. The commercial center of town is an area called "The Longmeadow Shops," including restaurants and clothing stores.
According to the United States Census Bureau, the town has a total area of , of which is land and (5.05%) is water.
Demographics.
As of the census of 2000, there were 15,633 people, 5,734 households, and 4,432 families residing in the town. The population density was . There were 5,879 housing units at an average density of . The racial makeup of the town was 95.42% White, 0.69% African American, 0.05% Native American, 2.90% Asian, 0.06% Pacific Islander, 0.26% from other races, and 0.62% from two or more races. Hispanic or Latino of any race were 1.09% of the population.
There were 5,734 households out of which 37.1% had children under the age of 18 living with them, 69.1% were married couples living together, 6.4% had a female householder with no husband present, and 22.7% were non-families. 20.4% of all households were made up of individuals and 14.0% had someone living alone who was 65 years of age or older. The average household size was 2.66 and the average family size was 3.09.
In the town the population was spread out with 26.8% under the age of 18, 4.6% from 18 to 24, 22.0% from 25 to 44, 28.7% from 45 to 64, and 17.8% who were 65 years of age or older. The median age was 43 years. For every 100 females there were 87.7 males. For every 100 females age 18 and over, there were 82.0 males.
The median income for a household in the town was $89,586, and the median income for a family was $105,578. Males had a median income of $68,238 versus $40,890 for females. The per capita income for the town was $38,949. About 1.0% of families and 2.1% of the population were below the poverty line, including 0.3% of those under age 18 and 8.3% of those age 65 or over.
Government.
The town government consists of a Select Board with five members, elected by the town. The public school system is governed by the School Committee. The School Committee is made up of seven voting member elected by the town, the superintendent of schools, two assistant-superidents, a secretary, and a student representative.
Education.
The Longmeadow public school system comprises six schools. Blueberry Hill School, Center School, and Wolf Swamp Road School are K−5 elementary schools. Williams Middle School and Glenbrook Middle School serve grades 6–8. Longmeadow High School serves all students in the town between grades 9 and 12. The town's elementary schools have been recently rebuilt, statements of interest for improvements to the two middle schools and Longmeadow High School were filed with the Massachusetts School Building Authority in 2007. In 2010, the voters of Longmeadow approved a 2.5% budget override to support the construction of a new 78 million dollar high school. The town received an estimated 34 million dollars in state funds to be used towards the new construction The new High School was completed and opened to students on February 26, 2013. After students and faculty had moved into the new school, the demolition of the old school was begun. The demolition was completed by June 2013. The school had its grand opening in September 2013 with both the brand new school and renovated business & administration wing open.
Longmeadow also hosts three private parochial schools, the Lubavitcher Yeshiva Academy (LYA), Heritage Academy and St. Mary's Academy. LYA was established in 1946 in response to the Greater Springfield Jewish community's need for a quality Jewish day school. In 1999, LYA became the first Jewish day school to be accredited by the New England Association of Schools and Colleges (NEASC). The more than 120 students that the school serves each year from across the spectrum of Jewish life includes orthodox, conservative, reform and unaffiliated families. St. Mary's School, located behind St. Mary's church, serves Catholic students grades Pre-K through Grade 8.
Approximately 50% of the students at Longmeadow High School participate in the music program. The choruses have won gold medals at the MICCA competition. The jazz ensemble has won numerous gold medals as well, but no longer competes. The wind ensemble and symphony orchestra have had the honor of performing in Indianapolis, Boston (Boston Symphony Hall), and New York (Carnegie Hall). In 2010, Longmeadow was awarded The American Prize in Orchestral Performance. The music program's crowning achievement has been receiving three national Grammy Awards based on the high level of excellence maintained throughout all groups in the music program.
Longmeadow also contains the 46 acre primary campus for Bay Path University, a private undergraduate and graduate institution.

</doc>
<doc id="18183" url="https://en.wikipedia.org/wiki?curid=18183" title="Relative direction">
Relative direction

The most common relative directions are left, right, forward(s), backward(s), up, and down. No absolute direction corresponds to any of the relative directions. This is a consequence of the translational invariance of the laws of physics: nature, loosely speaking, behaves the same no matter what direction one moves. As demonstrated by the Michelson-Morley null result, there is no absolute inertial frame of reference. There are definite relationships between the relative directions, however. Left and right, forward and backward, and up and down are three pairs of complementary directions, each pair orthogonal to both of the others. Relative directions are also known as egocentric coordinates.
Traditions and conventions.
Since definitions of left and right based on the geometry of the natural environment are unwieldy, in practice, the meaning of relative direction words is conveyed through tradition, acculturation, education, and direct reference. One common definition of up and down uses gravity and the planet Earth as a frame of reference. Since there is a very noticeable force of gravity acting between the Earth and any other nearby object, down is defined as that direction which an object moves in reference to the Earth when the object is allowed to fall freely. Up is then defined as the opposite direction of down. Another common definition uses a human body, standing upright, as a frame of reference. In that case, up is defined as the direction from feet to head, perpendicular to the surface of the Earth. In most cases, up is a directionally oriented position generally opposite to that of the pull of gravity.
In situations where a common frame of reference is needed, it is most common to use an egocentric view. A simple example is road signage. Another example is stage blocking, where "stage left" "stage right" "stage up" and "stage down" are, by convention, defined from the actor's point of view, but up and down stage do not follow gravitational conventions of up and down. An example of a non-egocentric view is page layout, where the relative terms "upper half" "left margin," etc. are defined in terms of the observer but employed in reverse for a type compositor, returning to an egocentric view. In medicine and science, where precise definitions are crucial, relative directions (left and right) are the sides of the organism, not those of the observer. The same is true in heraldry, where left and right in a coat of arms is treated as if the shield was being held by the armiger. To avoid confusion, Latin terminology is employed: "dexter" and "sinister" for right and left. Proper right and proper left are terms mainly used to describe images, and overcome the potential confusion that a figure's right or "proper right" hand is on the left hand as the viewer of a frontal image sees it.
Forward and backward may be defined by referring to an object's or person's motion. Forward is defined as the direction in which the object is moving. Backward is then defined as the opposite direction to forward. Alternatively, 'forward' may be the direction pointed by the observer's nose, defining 'backward' as the direction from the nose to the sagittal border in the observer's skull. With respect to a ship 'forward' would indicate the relative position of any object lying in the direction the ship is pointing. For symmetrical objects, it is also necessary to define forward and backward in terms of expected direction. Many mass transit trains are built symmetrically with paired control booths, and definitions of forward, backward, left, and right are temporary.
Given significant distance from the magnetic poles, one can figure which hand is which using a magnetic compass and the sun. Facing the sun, before noon, the north pointer of the compass points to the "left" hand. After noon, it points to the "right".
Geometry of the natural environment.
The right-hand rule is one common way to relate the three principal directions. For many years a fundamental question in physics was whether a left-hand rule would be equivalent. Many natural structures, including human bodies, follow a certain "handedness", but it was widely assumed that nature did not distinguish the two possibilities. This changed with the discovery of parity violations in particle physics. If a sample of cobalt-60 atoms is magnetized so that they spin counterclockwise around some axis, the beta radiation resulting from their nuclear decay will be preferentially directed opposite that axis. Since counter-clockwise may be defined in terms of up, forward, and right, this experiment unambiguously differentiates left from right using only natural elements: if they were reversed, or the atoms spun clockwise, the radiation would follow the spin axis instead of being opposite to it.
Nautical terminology.
Bow, stern, port, and starboard are nautical terms that convey an impersonal relative direction in the context of the moving frame of persons aboard a ship. The need for impersonal terms is most clearly seen in a rowing shell where the majority of the crew face aft ("backwards") and the oars to their right are actually on the port side.
Cultures without relative directions.
Most human cultures use relative directions for reference, but there are exceptions. The Australian Aboriginal people the Guugu Yimithirr have no words denoting the egocentric directions in their language; instead, they exclusively refer to cardinal directions, even when describing small-scale spaces. For instance, if they wanted someone to move over on the car seat to make room, they might say "move a bit to the east". To tell someone where exactly they left something in their house, they might say, "I left it on the southern edge of the western table." Or they might warn a person to "look out for that big ant just north of your foot". Other peoples "from Polynesia to Mexico and from Namibia to Bali" similarly have predominantly "geographic languages". American Sign Language makes heavy use of geographical direction through absolute orientation.
Left-right confusion.
Left-right confusion is the difficulty some people have in distinguishing the difference between the directions left and right. According to research by John R. Clarke of Drexel University, it affects about 15% of the population. These people can usually normally perform daily activities such as driving according to signs and navigating according to a map, but will often take a wrong turn when told to turn left or right and may have difficulties performing actions that require precise understanding of directional commands, such as ballroom dancing.

</doc>
<doc id="18184" url="https://en.wikipedia.org/wiki?curid=18184" title="Lizard">
Lizard

Lizards are a widespread group of squamate reptiles, with approximately over 6,000 species, ranging across all continents except Antarctica, as well as most oceanic island chains. The group, traditionally recognized as the suborder Lacertilia, is defined as all extant members of the Lepidosauria (reptiles with overlapping scales) that are neither sphenodonts (i.e., tuatara) nor snakes – they form an evolutionary grade. While the snakes are recognized as falling phylogenetically within the Toxicofera clade from which they evolved, the sphenodonts are the sister group to the squamates, the larger monophyletic group, which includes both the lizards and the snakes.
Biology.
Lizards typically have feet and external ears, while snakes lack both of these characteristics. However, because they are defined negatively as excluding snakes, lizards have no unique distinguishing characteristic as a group. Lizards and snakes share a movable quadrate bone, distinguishing them from the sphenodonts, which have more primitive and solid diapsid skulls. Many lizards can detach their tails to escape from predators, an act called autotomy. Vision, including color vision, is particularly well developed in most lizards, and most communicate with body language or bright colors on their bodies, as well as with pheromones. Lizards are the most speciose among extant reptiles, comprising about 60% of all living species.
The adult length of species within the suborder ranges from a few centimeters for chameleons such as "Brookesia micra" and geckos such as "Sphaerodactylus ariasae" to nearly in the case of the largest living varanid lizard, the Komodo dragon. Some extinct varanids reached great size. The extinct aquatic mosasaurs reached , and the giant monitor "Megalania" is estimated to have reached up to long.
The name Sauria was coined by James Macartney (1802); it was the Latinisation of the French name "Sauriens", coined by Alexandre Brongniart (1800) for an order of reptiles in the classification proposed by the author, containing lizards and crocodilians, later discovered not to be each other's closest relatives. Later authors used the term "Sauria" in a more restricted sense, i.e. as a synonym of Lacertilia, a suborder of Squamata that includes all lizards but excludes snakes. This classification is rarely used today because Sauria so-defined is a paraphyletic group. It was defined as a clade by Jacques Gauthier, Arnold G. Kluge and Timothy Rowe (1988) as the group containing the most recent common ancestor of archosaurs and lepidosaurs (the groups containing crocodiles and lizards, as per Mcartney's original definition) and all its descendants. A different definition was formulated by Michael deBraga and Olivier Rieppel (1997) who defined Sauria as the clade containing the most recent common ancestor of Choristodera, Archosauromorpha and Lepidosauromorpha and all their descendants. However, neither of these uses have gained wide acceptance among researchers specializing in lizards.
Physiology.
Sight is very important for most lizards, both for locating prey and for communication, and, as such, many lizards have highly acute color vision. Most lizards rely heavily on body language, using specific postures, gestures, and movements to define territory, resolve disputes, and entice mates. Some species of lizards also use bright colors, such as the iridescent patches on the belly of "Sceloporus". These colors would be highly visible to predators, so are often hidden on the underside or between scales and only revealed when necessary.
The particular innovation in this respect is the dewlap, a brightly colored patch of skin on the throat, usually hidden between scales. When a display is needed, a lizard can erect the hyoid bone of its throat, resulting in a large vertical flap of brightly colored skin beneath the head which can be then used for communication. Anoles are particularly famous for this display, with each species having specific colors, including patterns only visible under ultraviolet (UV) light, as many lizards can see UV light.
Shedding and regenerating tails.
Lizard tails are often a different and dramatically more vivid color than the rest of the body so as to encourage potential predators to strike for the tail first.
Many lizard species (including geckos, skinks, and others) are capable of shedding part of their tails through a process called autotomy. This is an example of the "pars pro toto" principle, sacrificing "a part for the whole", and is employed by lizards to allow them to escape when captured by the tail by a predator. The detached tail writhes and wiggles, creating a deceptive sense of continued struggle, distracting the predator's attention from the fleeing prey animal.
The lizard will partially regenerate its tail over a period of weeks. A 2014 research identified 326 genes involving the regeneration of lizard tails. The new section will contain cartilage rather than bone, and the skin may be distinctly discolored compared to the rest of the body.
Reproduction.
Most lizards are oviparous (egg laying) though in some species the eggs are retained until the live young emerge. Parthenogenesis in squamata (that is, asexual reproduction) occurs in about 50 species of lizard. and it is believed that female ability to do this in the absence of males is widespread among lizards.
Evolution and relationships.
Sexual selection in lizards shows evidence of female mate choice of males is found in their favouring males who display fitness indicators such as fewer ectoparasites. 
The retention of the basic 'reptilian' amniote body form by lizards makes it tempting to assume any similar animal, alive or extinct, is also a lizard. However, this is not the case, and lizards as squamates are part of a well-defined group.
The earliest amniotes were superficially lizard-like, but had solid, box-like skulls, with openings only for eyes and nostrils, termed the anapsid condition. Turtles retain, or have re-evolved, this skull form. Early anapsids later gave rise to two new groups with additional holes in their skulls to make room for and anchor larger jaw muscles. The synapsids, with a single fenestra, gave rise to the large, but generally lizard-like pelycosaurs, which include "Dimetrodon", a group which again gave rise to the therapsids, including the cynodonts, from which the modern mammals would evolve.
The modern "Sphenodon" retains the basic lepidosaur skull, distinguishing it from true lizards in spite of superficial similarities. Squamates, including snakes and all true lizards, further lightened the skull by eliminating the lower margin of the lower skull opening.
The earliest known fossil remains of a lizard belong to the iguanian species "Tikiguania estesi", found in the Tiki Formation of India, which dates to the Carnian stage of the Triassic period, about 220 million years ago. However, doubt has been raised over the age of "Tikiguania" because it is almost indistinguishable from modern agamid lizards. The "Tikiguania" remains may instead be late Tertiary or Quaternary in age, having been washed into much older Triassic sediments. Lizards are most closely related to a group called Rhynchocephalia, which includes the tuatara. Rhynchocephalians first appeared in the Late Triassic, so it can be inferred that the lizard-rhynchocephalian divergence occurred at this time and that the earliest lizards appeared in the Triassic.
Mitochondrial phylogenetics suggest that the first lizards evolved in the late Permian. Most evolutionary relationships within the squamates are not yet completely worked out, with the relationship of snakes to other groups being the most problematic. On the basis of morphological data, iguanid lizards were thought to have diverged from other squamates very early on, but recent molecular phylogenies, both from mitochondrial and nuclear DNA, do not support this. Because snakes have a faster molecular clock than other squamates, and few early snake and snake ancestor fossils have been found, resolving the relationship between snakes and other squamate groups is difficult.
Lizard diversification.
Lacertilia comprises four generally recognized infraorders, Iguania, Gekkota, Amphisbaenia and Autarchoglossa, with the "blind skinks" in the family Dibamidae having an uncertain position. While traditionally excluded from the lizards, the snakes are usually classified as a clade with a similar subordinal rank.
Iguania.
The suborder Iguania, found in Africa, southern Asia, Australia, the New World and the islands of the west Pacific, forms the sister group to the remainder of the squamata. The various species are largely arboreal, and have primitively fleshy, non-prehensile tongues, some even have scales, but this condition is obviously highly modified in the chameleons. This clade includes the following families:
Gekkota.
Active hunters, the Gekkota include three families comprising the distinctive cosmopolitan geckos and the legless, flap-footed lizards of Australia and New Guinea. Like snakes, the flap-footed lizards and most geckos lack eyelids. Unlike snakes, they use their tongues to clean their often highly developed eyes. While gecko feet have unique surfaces that allow them to cling to glass and run on ceilings, 
the flap-foot has lost its limbs. The three families of this suborder are:
Relationship with humans.
Most lizard species are harmless to humans. Only the largest lizard species, the Komodo dragon, which reaches 3.3 m (11 ft) in length and weighs up to 166 kg (365 lb), has been known to stalk, attack, and, on occasion, kill humans. An eight-year-old Indonesian boy died from blood loss after an attack in 2007. The venoms of the Gila monster and beaded lizard are not usually deadly, but they can inflict extremely painful bites due to powerful jaws.
Numerous species of lizard are kept as pets, including bearded dragons, iguanas, anoles, and geckos (such as the popular leopard gecko). Some lizards have an affinity for humans, but many are suspicious or skittish around them. Lizards that bite humans are very rare. Lizards are predominantly insectivorous, but some eat fruit, or vegetables. Live crickets and worms are the most typical foods for pet lizards, though the crested gecko (not a friendly lizard to humans) can feed entirely on fruit.
Lizard symbolism plays important, though rarely predominant, roles in some cultures (e.g., Tarrotarro in Australian Aboriginal mythology). The Moche people of ancient Peru worshipped animals and often depicted lizards in their art. According to a popular legend in Maharashtra, a common Indian monitor, with ropes attached, was used to scale the walls of the Sinhagad fort in the Battle of Sinhagad.
Green iguanas are eaten in Central America, where they are referred to sometimes as "chicken of the tree" after their habit of resting in trees and their supposed chicken-like taste, and spiny-tailed lizards are eaten in Africa. In North Africa, "Uromastyx" species are considered "dhaab" or 'fish of the desert' and eaten by nomadic tribes.
Classification.
Suborder Lacertilia (Sauria) – (lizards) 

</doc>
<doc id="18185" url="https://en.wikipedia.org/wiki?curid=18185" title="List of deists">
List of deists

This is a partial list of people who have been categorized as deists, the belief in a deity based on natural religion only, or belief in religious truths discovered by people through a process of reasoning, independent of any revelation through scripture or prophets. They have been selected for their influence on Deism, or for their fame in other areas.

</doc>
<doc id="18187" url="https://en.wikipedia.org/wiki?curid=18187" title="Book of Leviticus">
Book of Leviticus

The Book of Leviticus (; from Greek Λευιτικόν, "Leuitikon" — from rabbinic Hebrew "torat kohanim") is the third book of the Greek Old Testament of Christian biblical canons, and the third of five books of the Pentateuch. 
The Hebrew name of the third book of the Jewish Bible ( "Vayikra"/"Wayyiqrā"), and the third of five books of the Torah, comes from its first word "vayikraˈ", "And He [God] called." Its Greek name "Levitikon", "things pertaining to the Levites", and its Latin name "Leviticus", are based on the term "torat kohanim", "instruction of (or ′for′) the priests" from early rabbinic times. 
The English name is from the Latin "Leviticus", taken in turn from Greek and a reference to the Levites, the tribe of Aaron, from whom the Kohanim ('"priests") descended. The book, however, addresses all the people of Israel (1:2) though some passages address the priests specifically (6:8). Most of its chapters (1–7, 11–27) consist of God's speeches to Moses which he is commanded to repeat to the Israelites. This takes place within the story of the Israelites' Exodus after they escaped Egypt and reached Mt. Sinai (Exodus 19:1). The book of Exodus narrates how Moses led the Israelites in building the Tabernacle (Exodus 35–40) based on God's instructions (Exodus 25–31). Then in Leviticus, God tells the Israelites and their priests how to make offerings in the Tabernacle and how to conduct themselves while camped around the holy tent sanctuary. Leviticus takes place during the month or month-and-a-half between the completion of the Tabernacle (Exodus 40:17) and the Israelites' departure from Sinai (Numbers 1:1, 10:11).
The instructions of Leviticus emphasize ritual, legal and moral practices rather than beliefs. Nevertheless, they reflect the world view of the creation story in Genesis 1 that God wishes to live with humans. The book teaches that faithful performance of the sanctuary rituals can make that possible, so long as the people avoid sin and impurity whenever possible. The rituals, especially the sin and guilt offerings, provide the means to gain forgiveness for sins (Leviticus 4–5) and purification from impurities (Leviticus 11–16) so that God can continue to live in the Tabernacle in the midst of the people.
The traditional view is that Leviticus was compiled by Moses, or that the material in it goes back to his time, but internal clues suggest that the book developed much later in Israel's history and was completed either near the end of the Judean monarchy in the late seventh century BCE or in the exilic and post-exilic period of the sixth and fifth centuries BCE. Scholars debate whether it was written primarily for Jewish worship in exile that centered on reading or preaching, or was aimed instead at worshipers at temples in Jerusalem and Samaria. but they are practically unanimous that the book had a long period of growth, and that although it includes some material of considerable antiquity, it reached its present form in the Persian period (538–332 BCE).
Structure.
"(The outlines provided by commentaries are similar, though not identical; compare those of Wenham, Hartley, Milgrom, and Watts)
I. Laws on sacrifice (1:1–7:38)
II. Institution of the priesthood (8:1–10:20)
III. Uncleanliness and its treatment (11:1–15:33)
IV. Day of Atonement: purification of the tabernacle from the effects of uncleanliness and sin (ch. 16)
V. Prescriptions for practical holiness (the Holiness Code, chs. 17–26)
V. Redemption of votive gifts (ch. 27)
Summary.
Chapters 1–5 describe the various sacrifices from the sacrificers' point of view, although the priests are essential for handling the blood. Chapters 6–7 go over much the same ground, but from the point of view of the priest, who, as the one actually carrying out the sacrifice and dividing the "portions", needs to know how this is to be done. Sacrifices are to be divided between God, the priest, and the offerers, although in some cases the entire sacrifice is a single portion consigned to God—i.e., burnt to ashes.
Chapters 8–10 describe the consecration by Moses of Aaron and his sons as the first priests, the first sacrifices, and God's destruction of two of Aaron's sons for ritual offenses. The purpose is to underline the character of altar priesthood (i.e., those priests empowered to offer sacrifices to God) as an Aaronite privilege, and the responsibilities and dangers of their position.
With sacrifice and priesthood established, chapters 11–15 instruct the lay people on purity (or cleanliness). Eating certain animals produces uncleanliness, as does giving birth; certain skin diseases (but not all) are unclean, as are certain conditions affecting walls and clothing (mildew and similar conditions); and genital discharges, including female menses and male gonorrhea, are unclean. The reasoning behind the food rules are obscure; for the rest the guiding principle seems to be that all these conditions involve a loss of "life force", usually but not always blood.
Leviticus 16 concerns the Day of Atonement. This is the only day on which the High Priest is to enter the holiest part of the sanctuary, the holy of holies. He is to sacrifice a bull for the sins of the priests, and a goat for the sins of the laypeople. A second goat is to be sent into the desert to "Azazel", bearing the sins of the whole people. Azazel may be a wilderness-demon, but its identity is mysterious.
Chapters 17–26 are the Holiness code. It begins with a prohibition on all slaughter of animals outside the Temple, even for food, and then prohibits a long list of sexual contacts and also child sacrifice. The "holiness" injunctions which give the code its name begin with the next section: penalties are imposed for the worship of Molech, consulting mediums and wizards, cursing one's parents and engaging in unlawful sex. Priests are instructed on mourning rituals and acceptable bodily defects. Blasphemy is to be punished with death, and rules for the eating of sacrifices are set out; the calendar is explained, and rules for sabbatical and Jubilee years set out; and rules are made for oil lamps and bread in the sanctuary; and rules are made for slavery. The code ends by telling the Israelites they must choose between the law and prosperity on the one hand, or, on the other, horrible punishments, the worst of which will be expulsion from the land.
Chapter 27 is a disparate and probably late addition telling about persons and things dedicated to the Lord and how vows can be redeemed instead of fulfilled.
Line 74 edits arise from the following sources:;
Composition.
The majority of scholars have concluded that the Pentateuch received its final form during the Persian period (538–332 BCE). Nevertheless, Leviticus had a long period of growth before reaching that form.
The entire book of Leviticus is composed of Priestly literature. Most scholars see chapters 1–16 (the Priestly code) and chapters 17–26 (the Holiness code) as the work of two related schools, but while the Holiness material employs the same technical terms as the Priestly code, it broadens their meaning from pure ritual to the theological and moral, turning the ritual of the Priestly code into a model for the relationship of Israel to God: as the tabernacle is made holy by the presence of the Lord and kept apart from uncleanliness, so He will dwell among Israel when Israel is purified (made holy) and separated from other peoples. The ritual instructions in the Priestly code apparently grew from priests giving instruction and answering questions about ritual matters; the Holiness code (or H) used to be regarded as a separate document later incorporated into Leviticus, but it seems better to think of the Holiness authors as editors who worked with the Priestly code and actually produced Leviticus as we now have it.
A burnt text that was excavated from an ancient Synagogue in Ein Gedi in 1970, and has been carbon dated to the late 5th century AD, was recently discovered to contain verses from the second chapter of Leviticus, making it the oldest piece of the Torah ever discovered after the Dead Sea Scrolls. The text was unreadable until analyzed with a micro CT scanner that was then used to recreate a 3D image of the scroll. It is the first Torah scroll to be found in an ancient Synagogue.
Themes.
Sacrifice and ritual.
Many scholars argue that the rituals of Leviticus have a theological meaning concerning Israel's relationship with its God. Jacob Milgrom has been especially influential in spreading this view. He maintained that the priestly regulations in Leviticus expressed a rational system of theological thought. The writers expected them to be put into practice in Israel’s temple, so the rituals would express this theology as well, as well as ethical concern for the poor. Milgrom also argued that the book’s purity regulations (chaps. 11-15) are based in ethical thinking. Many other interpreters have followed Milgrom in exploring the theological and ethical implications of Leviticus’s regulations (e.g. Marx, Balentine), though some have questioned how systematic they really are. Ritual, therefore, is not a series of actions undertaken for their own sake, but a means of maintaining the relationship between God, the world, and humankind.
Kehuna (Jewish Priesthood).
The main function of the priests is service at the altar, and only the sons of Aaron are priests in the full sense. (Ezekiel also distinguishes between altar-priests and lower Levites, but in Ezekiel the altar-priests are called sons of Zadok instead of sons of Aaron; many scholars see this as a remnant of struggles between different priestly factions in First Temple times, resolved by the Second Temple into a hierarchy of Aaronite altar-priests and lower-level Levites, including singers, gatekeepers and the like).
In chapter 10, God kills Nadab and Abihu, the oldest sons of Aaron, for offering "strange incense". Fortunately, Aaron has two sons left. Commentators have read various messages in the incident: a reflection of struggles between priestly factions in the post–Exilic period (Gerstenberger); or a warning against offering incense outside the Temple, where there might be the risk of invoking strange gods (Milgrom). In any case, the sanctuary has been polluted by the bodies of the two dead priests, leading into the next theme, holiness.
Uncleanliness and purity.
Ritual purity is essential for an Israelite to be able to approach God and remain part of the community. Uncleanliness threatens holiness; Chapters 11–15 review the various causes of uncleanliness and describe the rituals which will restore cleanliness; cleanliness is to be maintained through observation of the rules on sexual behaviour, family relations, land ownership, worship, sacrifice, and observance of holy days.
Yahweh dwells with Israel in the holy of holies. All of the priestly ritual is focused on Yahweh and the construction and maintenance of a holy space, but sin generates impurity, as do everyday events such as childbirth; impurity pollutes the holy dwelling place. Failure to ritually purify the sacred space could result in God leaving, which would be disastrous.
Atonement.
Through sacrifice the priest "makes atonement" for sin and the offerer is forgiven (but only if God accepts the sacrifice—forgiveness comes only from God). Atonement rituals involve blood, poured or sprinkled, as the symbol of the life of the victim: the blood has the power to wipe out or absorb the sin. The role of atonement is reflected structurally in two-part division of the book: chapters 1–16 call for the establishment of the institution for atonement, and chapters 17–27 call for the life of the atoned community in holiness.
Holiness.
The consistent theme of chapters 17–26 is the repeated phrase, "Be holy, for I the Lord your God am holy." Holiness in ancient Israel had a different meaning than in contemporary usage: it might have been regarded as the "god-ness" of God, an invisible but physical and potentially dangerous force. Specific objects, or even days, can be holy, but they derive holiness from being connected with God—the seventh day, the tabernacle, and the priests all derive their holiness from God. As a result, Israel had to maintain its own holiness in order to live safely alongside God.
The need for holiness is directed to the possession of the Promised Land (Canaan), where the Jews will become a holy people: "You shall not do as they do in the land of Egypt where you dwelt, and you shall not do as they do in the land of Canaan to which I am bringing you...You shall do my ordinances and keep my statutes...I am the Lord, your God" (ch. 18:3).
Subsequent tradition.
Leviticus, as part of the Torah, became the law book of Jerusalem's second temple as well as of the Samaritan temple. Evidence of its influence was found among the Dead Sea Scrolls, which included fragments of seventeen manuscripts of Leviticus dating from the third to the first centuries BCE. Many other Qumran scrolls cite the book, especially the Temple Scroll and 4QMMT.
Leviticus's instructions for animal offerings have not been observed by Jews or Christians since the first century CE. Because of the destruction of the temple in Jerusalem in 70 CE, Jewish worship has focused on prayer and the study of Torah. Nevertheless, Leviticus constitutes a major source of Jewish law and is traditionally the first book taught to children in the Rabbinic system of education. There are two main Midrashim on Leviticus—the halakhic one (Sifra) and a more aggadic one (Vayikra Rabbah).
In the New Testament, the letter to the Hebrews in particular uses ideas and images from Leviticus to describe Christ as the high priest who offers his own blood as a sin offering. Therefore, Christians do not make animal offerings either, as Gordon Wenham summarized: "With the death of Christ the only sufficient "burnt offering" was offered once and for all, and therefore the animal sacrifices which foreshadowed Christ's sacrifice were made obsolete."
Christians generally have the view that the New Covenant supersedes (i.e., replaces) the Old Testament's ritual laws, which includes many of the rules in Leviticus. Christians therefore have usually not observed Leviticus' rules regarding diet, purity, and agriculture. Christian teachings have differed, however, as to where to draw the line between ritual and moral regulations.
External links.
Online versions of Leviticus:
Related article:
Free Online Bibliography on Leviticus:
Brief introduction

</doc>
<doc id="18188" url="https://en.wikipedia.org/wiki?curid=18188" title="L. Frank Baum">
L. Frank Baum

Childhood and early life.
Baum was born in Chittenango, New York in 1856 into a devout Methodist family. He had German, Scots-Irish, and English ancestry, and was the seventh of nine children of Cynthia Ann (née Stanton) and Benjamin Ward Baum, only five of whom survived into adulthood. "Lyman" is the name of his father's brother, but he always disliked it and preferred his middle name "Frank".
Benjamin Baum was a wealthy businessman, ultimately gained by providing barrels during the Pennsylvania oil rush. L. Frank Baum grew up on his parents' expansive estate called Rose Lawn, which he fondly recalled as a sort of paradise. Rose Lawn was located in Mattydale, New York. He was a young, sickly, and daydreaming child, tutored at home with his siblings. From the age of 12, he spent two miserable years at Peekskill Military Academy but, after being severely disciplined for daydreaming, he had a possible psychogenic heart attack and was allowed to return home.
Baum started writing early in life, possibly prompted by his father buying him a cheap printing press. He had always been close to his younger brother Henry (Harry) Clay Baum who helped in the production of "The Rose Lawn Home Journal". The brothers published several issues of the journal, which included advertisements; they may have sold issues. By the age of 17, Baum established a second amateur journal called "The Stamp Collector", printed an 11-page pamphlet called "Baum's Complete Stamp Dealers' Directory", and started a stamp dealership with friends.
At 20, Baum took on the national craze of breeding fancy poultry. He specialized in raising the Hamburg. In March 1880, he established a monthly trade journal "The Poultry Record", and in 1886, when Baum was 30 years old, his first book was published: "The Book of the Hamburgs: A Brief Treatise upon the Mating, Rearing, and Management of the Different Varieties of Hamburgs".
Baum had a flair for being the spotlight of fun in the household, including during times of financial difficulties. His selling of fireworks made the Fourth of July memorable. His skyrockets, Roman candles, and fireworks filled the sky, while many people around the neighborhood would gather in front of the house to watch the displays. Christmas was even more festive. Baum dressed as Santa Claus for the family. His father would place the Christmas tree behind a curtain in the front parlor so that Baum could talk to everyone while he decorated the tree without people managing to see him. He maintained this tradition all his life.
Career.
Theater.
Baum embarked on his lifetime infatuation—and wavering financial success—with the theater. A local theatrical company duped him into replenishing their stock of costumes on the promise of leading roles coming his way. Disillusioned, Baum left the theatre — temporarily — and went to work as a clerk in his brother-in-law's dry goods company in Syracuse. This experience may have influenced his story "The Suicide of Kiaros", first published in the literary journal "The White Elephant". A fellow clerk one day was found locked in a store room dead, probably from suicide.
Baum could never stay away long from the stage. He performed in plays under the stage names of Louis F. Baum and George Brooks. In 1880, his father built him a theatre in Richburg, New York, and Baum set about writing plays and gathering a company to act in them. "The Maid of Arran" proved a modest success, a melodrama with songs based on William Black's novel "A Princess of Thule". Baum wrote the play and composed songs for it (making it a prototypical musical, as its songs relate to the narrative), and acted in the leading role. His aunt Katharine Gray played his character's aunt. She was the founder of Syracuse Oratory School, and Baum advertised his services in her catalog to teach theatre, including stage business, playwriting, directing, translating (French, German, and Italian), revision, and operettas.
On November 9, 1882, Baum married Maud Gage, a daughter of Matilda Joslyn Gage, a famous women's suffrage and feminist activist. While Baum was touring with "The Maid of Arran", the theatre in Richburg caught fire during a production of Baum's ironically titled parlor drama "Matches", destroying the theatre as well as the only known copies of many of Baum's scripts, including "Matches", as well as costumes.
The South Dakota years.
In July 1888, Baum and his wife moved to Aberdeen, Dakota Territory where he opened a store called "Baum's Bazaar". His habit of giving out wares on credit led to the eventual bankrupting of the store, so Baum turned to editing the local newspaper "The Aberdeen Saturday Pioneer" where he wrote the column "Our Landlady". Following the death of Sitting Bull at the hands of a federal agent, Baum urged the wholesale extermination of all America's native peoples in a column that he wrote on December 20, 1890. On January 3, 1891 he returned to the subject in an editorial response to the Wounded Knee Massacre:
"The Pioneer has before declared that our only safety depends upon the total extirmination of the Indians. Having wronged them for centuries, we had better, in order to protect our civilization, follow it up by one more wrong and wipe these untamed and untamable creatures from the face of the earth."
A recent analysis of these editorials has challenged their literal interpretation, suggesting that the actual intent of Baum was to generate sympathy for the Indians via obnoxious argument, ostensibly promoting the contrary position.
Baum's description of Kansas in "The Wonderful Wizard of Oz" is based on his experiences in drought-ridden South Dakota. During much of this time, Matilda Joslyn Gage was living in the Baum household. While Baum was in South Dakota, he sang in a quartet which included James Kyle, who became one of the first Populist (People's Party) Senators in the U.S.
Writing.
Baum's newspaper failed in 1891, and he, Maud, and their four sons moved to the Humboldt Park section of Chicago, where Baum took a job reporting for the "Evening Post". Beginning in 1897, he edited a magazine for advertising agencies for several years, focused on window displays in stores. The major department stores created elaborate Christmastime fantasies, using clockwork mechanisms that made people and animals appear to move. In 1900, Baum published a book about window displays in which he stressed the importance of mannequins in drawing customers. He also had to work as a traveling salesman.
In 1897, he wrote and published "Mother Goose in Prose", a collection of Mother Goose rhymes written as prose stories and illustrated by Maxfield Parrish. "Mother Goose" was a moderate success and allowed Baum to quit his door-to-door sales job (which had had a negative impact on his health). In 1899, Baum partnered with illustrator W.W. Denslow to publish "Father Goose, His Book", a collection of nonsense poetry. The book was a success, becoming the best-selling children's book of the year.
"The Wonderful Wizard of Oz".
In 1900, Baum and Denslow (with whom he shared the copyright) published "The Wonderful Wizard of Oz" to much critical acclaim and financial success. The book was the best-selling children's book for two years after its initial publication. Baum went on to write thirteen more novels based on the places and people of the Land of Oz.
"The Wizard of Oz: Fred R. Hamlin's Musical Extravaganza".
Two years after "Wizard'"s publication, Baum and Denslow teamed up with composer Paul Tietjens and director Julian Mitchell to produce a musical stage version of the book under Fred R. Hamlin. Baum and Tietjens had worked on a musical of "The Wonderful Wizard of Oz" in 1901 and based closely upon the book, but it was rejected. This stage version opened in Chicago in 1902 (the first to use the shortened title "The Wizard of Oz"), then ran on Broadway for 293 stage nights from January to October 1903. It returned to Broadway in 1904, where it played from March to May and again from November to December. It successfully toured the United States with much of the same cast, as was done in those days, until 1911, and then became available for amateur use. The stage version starred David C. Montgomery and Fred Stone as the Tin Woodman and Scarecrow respectively, which shot the pair to instant fame.
The stage version differed quite a bit from the book, and was aimed primarily at adults. Toto was replaced with Imogene the Cow, and Tryxie Tryfle (a waitress) and Pastoria (a streetcar operator) were added as fellow cyclone victims. The Wicked Witch of the West was eliminated entirely in the script, and the plot became about how the four friends were allied with the usurping Wizard and were hunted as traitors to Pastoria II, the rightful King of Oz. It is unclear how much control or influence Baum had on the script; it appears that many of the changes were written by Baum against his wishes due to contractual requirements with Hamlin. Jokes in the script, mostly written by Glen MacDonough, called for explicit references to President Theodore Roosevelt, Senator Mark Hanna, Rev. Andrew Danquer, and oil magnate John D. Rockefeller. Although use of the script was rather free-form, the line about Hanna was ordered dropped as soon as Hamlin got word of his death in 1904.
Beginning with the success of the stage version, most subsequent versions of the story, including newer editions of the novel, have been titled "The Wizard of Oz", rather than using the full, original title. In more recent years, restoring the full title has become increasingly common, particularly to distinguish the novel from the Hollywood film.
Baum wrote a new Oz book, "The Marvelous Land of Oz", with a view to making it into a stage production, which was titled "The Woggle-Bug", but Montgomery and Stone balked at appearing when the original was still running. The Scarecrow and Tin Woodman were then omitted from this adaptation, which was seen as a self-rip-off by critics and proved to be a major flop before it could reach Broadway. He also worked for years on a musical version of "Ozma of Oz", which eventually became "The Tik-Tok Man Of Oz". This did fairly well in Los Angeles, but not well enough to convince producer Oliver Morosco to mount a production in New York. He also began a stage version of "The Patchwork Girl of Oz", but this was ultimately realized as a film".
Later life and work.
With the success of "Wizard" on page and stage, Baum and Denslow hoped that lightning would strike a third time and published "Dot and Tot of Merryland" in 1901. The book was one of Baum's weakest, and its failure further strained his faltering relationship with Denslow. It was their last collaboration. Baum worked primarily with John R. Neill on his fantasy work beginning in 1904, but Baum met Neill few times (all before he moved to California) and often found Neill's art not humorous enough for his liking. He was particularly offended when Neill published "The Oz Toy Book: Cut-outs for the Kiddies" without authorization.
Several times during the development of the Oz series, Baum declared that he had written his last Oz book and devoted himself to other works of fantasy fiction based in other magical lands, including "The Life and Adventures of Santa Claus" and "Queen Zixi of Ix". However, he returned to the series each time, persuaded by popular demand, letters from children, and the failure of his new books. Even so, his other works remained very popular after his death, with "The Master Key" appearing on "St. Nicholas Magazine"'s survey of readers' favorite books well into the 1920s.
In 1905, Baum declared plans for an Oz amusement park. In an interview, he mentioned buying Pedloe Island off the coast of California to turn it into an Oz park. However, there is no evidence that he purchased such an island, and no one has ever been able to find any island whose name even resembles Pedloe in that area. Nevertheless, Baum stated to the press that he had discovered a Pedloe Island off the coast of California and that he had purchased it to be "the Marvelous Land of Oz," intending it to be "a fairy paradise for children." Eleven year old Dorothy Talbot of San Francisco was reported to be ascendant to the throne on March 1, 1906, when the Palace of Oz was expected to be completed. Baum planned to live on the island, with administrative duties handled by the princess and her all-child advisers. Plans included statues of the Scarecrow, Tin Woodman, Jack Pumpkinhead, and H.M. Woggle-Bug, T.E. Baum abandoned his Oz park project after the failure of "The Woggle-Bug", which was playing at the Garrick Theatre in 1905.
Because of his lifelong love of theatre, he financed elaborate musicals, often to his financial detriment. One of Baum's worst financial endeavors was his "The Fairylogue and Radio-Plays" (1908), which combined a slideshow, film, and live actors with a lecture by Baum as if he were giving a travelogue to Oz. However, Baum ran into trouble and could not pay his debts to the company who produced the films. He did not get back to a stable financial situation for several years, after he sold the royalty rights to many of his earlier works, including "The Wonderful Wizard of Oz". This resulted in the M.A. Donahue Company publishing cheap editions of his early works with advertising which purported that Baum's newer output was inferior to the less expensive books that they were releasing. Baum had shrewdly transferred most of his property into Maud's name, except for his clothing, his typewriter, and his library (mostly of children's books, such as the fairy tales of Andrew Lang, whose portrait he kept in his study)—all of which, he successfully argued, were essential to his occupation. Maud handled the finances anyway, and thus Baum lost much less than he could have.
Baum made use of several pseudonyms for some of his other non-Oz books. They include:
Baum also anonymously wrote "The Last Egyptian: A Romance of the Nile". He continued theatrical work with Harry Marston Haldeman's men's social group The Uplifters, for which he wrote several plays for various celebrations. He also wrote the group's parodic by-laws. The group also included Will Rogers, but was proud to have had Baum as a member and posthumously revived many of his works despite their ephemeral intent. Many of these play's titles are known, but only "The Uplift of Lucifer" is known to survive (it was published in a limited edition in the 1960s). Prior to that, his last produced play was "The Tik-Tok Man of Oz" (based on "Ozma of Oz" and the basis for "Tik-Tok of Oz"), a modest success in Hollywood that producer Oliver Morosco decided did not do well enough to take to Broadway. Morosco, incidentally, quickly turned to film production, as did Baum.
In 1914, Baum started his own film production company The Oz Film Manufacturing Company, which came as an outgrowth of the Uplifters. He served as its president and principal producer and screenwriter. The rest of the board consisted of Louis F. Gottschalk, Harry Marston Haldeman, and Clarence R. Rundel. The films were directed by J. Farrell MacDonald, with casts that included Violet MacMillan, Vivian Reed, Mildred Harris, Juanita Hansen, Pierre Couderc, Mai Welles, Louise Emmons, J. Charles Haydon, and early appearances by Harold Lloyd and Hal Roach. Silent film actor Richard Rosson appeared in one of the films, whose younger brother Harold Rosson photographed "The Wizard of Oz" (1939). After little success probing the unrealized children's film market, Baum acknowledged his authorship of "The Last Egyptian" and made a film of it (portions of which are included in "Decasia"), but the Oz name had become box office poison for the time being, and even a name change to Dramatic Feature Films and transfer of ownership to Frank Joslyn Baum did not help. Baum invested none of his own money in the venture, unlike "The Fairylogue and Radio-Plays", but the stress probably took its toll on his health.
Death.
On May 5, 1919, Baum suffered a stroke. The following day he slipped into a coma but briefly awoke and spoke his last words to his wife, "Now we can cross the Shifting Sands." Frank died on May 6, 1919. He was buried in Glendale's Forest Lawn Memorial Park Cemetery.
His final Oz book, "Glinda of Oz", was published on July 10, 1920, a year after his death. The Oz series was continued long after his death by other authors, notably Ruth Plumly Thompson, who wrote an additional nineteen Oz books.
Baum's beliefs.
Literary.
Baum's avowed intentions with the Oz books and other fairy tales was to retell tales such as are found in the works of the Brothers Grimm and Hans Christian Andersen, make them in an American vein, update them, avoid stereotypical characters such as dwarfs or genies, and remove the association of violence and moral teachings. The first books contained a fair amount of violence, but it decreased with the series; in "The Emerald City of Oz", Ozma objected to doing violence even to the Nomes who threaten Oz with invasion. His introduction is often cited as the beginnings of the sanitization of children's stories, although he did not do a great deal more than eliminate harsh moral lessons.
Another traditional element that Baum intentionally omitted was the emphasis on romance. He considered romantic love to be uninteresting for young children, as well as largely incomprehensible. In "The Wonderful Wizard of Oz", the only element of romance lay in the background of the Tin Woodman and his love for Nimmie Amee, which explains his condition and does not otherwise affect the tale, and that of Gayelette and the enchantment of the Winged monkeys. The only other stories with such elements were "The Scarecrow of Oz" and "Tik-Tok of Oz", both based on dramatizations, which Baum regarded warily until his readers accepted them.
Political.
Women's suffrage advocate.
Sally Roesch Wagner of The Matilda Joslyn Gage Foundation has published a pamphlet titled "The Wonderful Mother of Oz" describing how Matilda Gage's radical feminist politics were sympathetically channeled by Baum into his Oz books. Much of the politics in the Republican "Aberdeen Saturday Pioneer" dealt with trying to convince the populace to vote for women's suffrage. Baum was the secretary of Aberdeen's Woman's Suffrage Club. Susan B. Anthony visited Aberdeen and stayed with the Baums. Nancy Tystad Koupal notes an apparent loss of interest in editorializing after Aberdeen failed to pass the bill for women's enfranchisement.
Some of Baum's contacts with suffragists of his day seem to have inspired much of his second Oz story "The Marvelous Land of Oz". In this story, General Jinjur leads the girls and women of Oz in a revolt, armed with knitting needles; they succeed and make the men do the household chores. Jinjur proves to be an incompetent ruler, but a female advocating gender equality is ultimately placed on the throne. His Edith Van Dyne stories depict girls and young women engaging in traditionally masculine activities, including the "Aunt Jane's Nieces", "The Flying Girl" and its sequel, and his girl sleuth Josie O'Gorman from The Bluebird Books.
Editorials about Native Americans.
During the period surrounding the 1890 Ghost Dance movement and Wounded Knee Massacre, Baum wrote two editorials about Native Americans for the "Aberdeen Saturday Pioneer" which have provoked controversy in recent times because of his assertion that the safety of White settlers depended on the wholesale genocide of American Indians. Sociologist Robert Venables has argued that Baum was not using sarcasm in the editorials.
The first piece was published on December 20, 1890, five days after the killing of the Lakota Sioux holy man, Sitting Bull (who was being held in custody at the time). Following is the complete text of the editorial:
Baum wrote a second editorial following the December 29, 1890 massacre and published on January 3, 1891:
These two short editorials continue to haunt his legacy. In 2006, two descendants of Baum apologized to the Sioux nation for any hurt that their ancestor had caused.
The short story "The Enchanted Buffalo" claims to be a legend of a tribe of bison, and states that a key element made it into legends of Native American tribes. "Father Goose, His Book" contains poems such as "There Was a Little Nigger Boy" and "Lee-Hi-Lung-Whan." In "The Last Egyptian", Lord Roane uses "nigger" to insult the title character, while in "The Daring Twins", set in the American South, the only character to use the term is a boy from Boston complaining that his mother uses their money to help "naked niggers in Africa." Baum mentions his characters' distaste for a Hopi snake dance in "Aunt Jane's Nieces and Uncle John", but also deplores the horrible situation of Indian Reservations. "Aunt Jane's Nieces on the Ranch" has a hard-working Mexican present himself as an exception to reiterate Anglo stereotypes of Mexican laziness. Baum's mother-in-law and Woman's Suffrage leader Matilda Joslyn Gage had great influence over Baum's views. Gage was initiated into the Wolf Clan and admitted into the Iroquois Council of Matrons for her outspoken respect and sympathy for Native American people; it would seem unlikely that Baum could have harbored animosity for them in his mature years.
Political imagery in "The Wizard of Oz".
Numerous political references to the "Wizard" appeared early in the 20th century. Henry Littlefield, an upstate New York high school history teacher, wrote a scholarly article which was the first full-fledged interpretation of the novel as an extended political allegory of the politics and characters of the 1890s. Special attention was paid to the Populist metaphors and debates over silver and gold. Baum was a Republican and avid supporter of Women's Suffrage, and it is thought that he personally did not support the political ideals of either the Populist movement of 1890–92 or the Bryanite-silver crusade of 1896–1900. He published a poem in support of William McKinley.
Since 1964, many scholars, economists, and historians have expanded on Littlefield's interpretation, pointing to multiple similarities between the characters (especially as depicted in Denslow's illustrations) and stock figures from editorial cartoons of the period. Littlefield himself wrote to "The New York Times" letters to the editor section spelling out that his theory had no basis in fact, but that his original point was "not to label Baum, or to lessen any of his magic, but rather, as a history teacher at Mount Vernon High School, to invest turn-of-the-century America with the imagery and wonder I have always found in his stories."
Baum's newspaper had addressed politics in the 1890s, and Denslow was an editorial cartoonist as well as an illustrator of children's books. A series of political references are included in the 1902 stage version, such as references by name to the President, to a powerful senator, and to John D. Rockefeller for providing the oil needed by the Tin Woodman. Scholars have found few political references in Baum's Oz books after 1902.
Baum himself was asked whether his stories had hidden meanings, but he always replied that they were written to "please children".
Religion.
Baum was originally a Methodist, but he joined the Episcopal Church in Aberdeen to participate in community theatricals. Later, he and his wife were encouraged by Matilda Joslyn Gage to become members of the Theosophical Society in 1892. Baum's beliefs are often reflected in his writing. The only mention of a church in his Oz books is the porcelain one which the Cowardly Lion breaks in the Dainty China Country in "The Wonderful Wizard of Oz". The Baums believed in God, but felt that religious decisions should be made by mature minds and not religious authorities. As a result, they sent their older sons to "Ethical Culture Sunday School" in Chicago, which taught morality, not religion.

</doc>
<doc id="18189" url="https://en.wikipedia.org/wiki?curid=18189" title="Lake Ladoga">
Lake Ladoga

Lake Ladoga ( or ; in Finnish "Nevajärvi"; ; ) is a freshwater lake located in the Republic of Karelia and Leningrad Oblast in northwestern Russia just outside the outskirts of Saint Petersburg. It is the largest lake in Europe, and the 15th largest freshwater lake by area in the world. "Ladoga Lacus", a methane lake on Saturn's moon Titan, is named after the lake.
Etymology.
In one of Nestor's chronicles from the 12th century he mentions a lake called "the Great Nevo" (a clear link to the Neva River; also compare "bog, quagmire"). In ancient Norse sagas and Hanseatic treaties they mention a city made of lakes called Aldoga (compare - wave).
Since the beginning of the 14th century the name of the lake was commonly called Ladoga, which was named after the city of Ladoga which in turn was named after the eponymous inflow in the lower reaches of the Volkhov River (Finnic name Alodejoki - 'River of the lowlands'). Other theories about the origin of the name comes from - wave, here - wavy; from the Russian dialectal word "алодь", meaning "open lake, extensive water field".
According to Jackson, T.N., "So far it can be considered that almost for granted, that the name of Ladoga first referred to the river, then the city, and only then the lake." Therefore, it considers the primary hydronym Ladoga to originate in the Finnic form *Aloden-joki "the river of the lowlands" (corresponding to modern ). From the river was the city , and already at this stage it was borrowed by the Slavic population and transformed by the Old Russian metathesis "ald → lad" from . The Old Norse intermediary word between Finnish and Old Russian word is fully confirmed by archeology: the Scandinavians first appeared in Ladoga in the early 750s, that is, a couple of decades before the Slavs.
Eugene Helimski by contrast, offers an etymology rooted in German. In his opinion, the primary name of the lake - "old source", similar to the open sea (the Old Open-Sea-Like-Source). This hydronym is associated with the name of the Neva River (which flows from Lake Ladoga) which comes German - the "new". Through the intermediate form *Aldaugja word gave "Ladoga (city)".
Geography.
The lake has an average surface area of 17,891 km2 (excluding the islands). Its north-to-south length is 219 km and its average width is 83 km; the average depth is 51 m, although it reaches a maximum of 230 m in the north-western part. Basin area: 276,000 km2, volume: 837 km3 (earlier estimated as 908 km3);. There are around 660 islands, with a total area of about 435 km2. Ladoga is, on average, 5 m above sea level. Most of the islands, including the famous Valaam archipelago, Kilpola and Konevets, are situated in the northwest of the lake.
Separated from the Baltic Sea by the Karelian Isthmus, it drains into the Gulf of Finland via the Neva River.
Lake Ladoga is navigable, being a part of the Volga-Baltic Waterway connecting the Baltic Sea with the Volga River. The Ladoga Canal bypasses the lake in the southern part, connecting the Neva to the Svir.
The basin of Lake Ladoga includes about 50,000 lakes and 3,500 rivers longer than 10 km. About 85% of the water inflow is due to tributaries, 13% is due to precipitation, and 2% is due to underground waters.
Geological history.
Geologically, the Lake Ladoga depression is a graben and syncline structure of Proterozoic age (Precambrian). This "Ladoga–Pasha structure", as it known, hosts Jotnian sediments. During the Pleistocene glaciations the depression was partially stripped of its sedimentary rock fill by glacial overdeepening.
Deglaciation following the Weichsel glaciation took place in the Lake Ladoga basin between 12,500 and 11,500 radiocarbon years BP. Lake Ladoga was initially part of the Baltic Ice Lake (70–80 m. above present sea-level), a historical freshwater stage of Baltic Sea. It is possible, though not certain, that Ladoga was isolated from it during regression of the subsequent Yoldia Sea brackish stage (10,200–9,500 BP). The isolation threshold should be at Heinjoki to the east of Vyborg, where the Baltic Sea and Ladoga were connected by a strait or a river outlet at least until the formation of the River Neva, and possibly even much later, until the 12th century AD or so.
At 9,500 BP, Lake Onega, previously draining into the White Sea, started emptying into Ladoga via the River Svir. Between 9,500 and 9,100 BP, during the transgression of Ancylus Lake, the next freshwater stage of the Baltic, Ladoga certainly became part of it, even if they hadn't been connected immediately before. During the Ancylus Lake subsequent regression, around 8,800 BP Ladoga became isolated.
Ladoga slowly transgressed in its southern part due to uplift of the Baltic Shield in the north. It has been hypothesized, but not proven, that waters of the Litorina Sea, the next brackish-water stage of the Baltic, occasionally invaded Ladoga between 7,000 and 5,000 BP. Around 5,000 BP the waters of the Saimaa Lake penetrated Salpausselkä and formed a new outlet, River Vuoksi, entering Lake Ladoga in the northwestern corner and raising its level by 1–2 m.
The River Neva originated when the Ladoga waters at last broke through the threshold at Porogi into the lower portions of Izhora River, then a tributary of the Gulf of Finland, between 4,000 and 2,000 BP. Dating of some sediments in the northwestern part of Lake Ladoga suggests it happened at 3,100 radiocarbon years BP (3,410–3,250 calendar years BP).
Wildlife.
The Ladoga is rich with fish. 48 forms (species and infraspecific taxa) of fish have been encountered in the lake, including roach, carp bream, zander, European perch, ruffe, endemic variety of smelt, two varieties of "Coregonus albula" (vendace), eight varieties of "Coregonus lavaretus", a number of other "Salmonidae" as well as, albeit rarely, endangered European sea sturgeon. Commercial fishing was once a major industry but has been hurt by overfishing. After the war, between 1945–1954, the total annual catch increased and reached a maximum of 4,900 tonnes. However, unbalanced fishery led to the drastic decrease of catch in 1955–1963, sometimes to 1,600 tonnes per year. Trawling has been forbidden in Lake Ladoga since 1956 and some other restrictions were imposed. The situation gradually recovered, and in 1971–1990 the catch ranged between 4,900 and 6,900 tonnes per year, about the same level as the total catch in 1938. Fish farms and recreational fishing are developing. [http://ladoga.krc.karelia.ru/resources/fish/index.shtml]
It has its own endemic ringed seal subspecies known as the Ladoga seal.
Since the beginning of the 1960s Ladoga has become considerably eutrophicated.
Nizhnesvirsky Natural Reserve is situated along the shore of Lake Ladoga immediately to the north of the mouth of the River Svir.
History.
In the Middle Ages, the lake formed a vital part of the trade route from the Varangians to the Greeks, with the Norse emporium at Staraya Ladoga defending the mouth of the Volkhov since the 8th century. In the course of the Swedish–Novgorodian Wars, the area was disputed between the Novgorod Republic and Sweden. In the early 14th century, the fortresses of Korela (Kexholm) and Oreshek (Nöteborg) were established along the banks of the lake.
The ancient Valaam Monastery was founded on the island of Valaam, the largest in Lake Ladoga, abandoned between 1611–1715, magnificently restored in the 18th century, and evacuated to Finland during the Winter War in 1940. In 1989 the monastic activities in the Valaam were resumed. Other historic cloisters in the vicinity are the Konevets Monastery, which sits on the Konevets island, and the Alexander-Svirsky Monastery, which preserves fine samples of medieval Muscovite architecture.
During the Ingrian War, a fraction of the Ladoga coast was occupied by Sweden. In 1617, by the Treaty of Stolbovo, the northern and western coast was ceded by Russia to Sweden. In 1721, after the Great Northern War, it was restituted to Russia by the Treaty of Nystad. Later, in 1812–1940 the lake was shared between Finland and Russia. According to the conditions of the 1920 Tartu Peace Treaty militarization of the lake was severely restricted. However, both Soviet Russia and Finland had flotillas in Ladoga (see also Finnish Ladoga Naval Detachment). After the Winter War (1939–40) according to the Moscow Peace Treaty, Ladoga, previously shared with Finland, became an internal basin of the Soviet Union.
During the Continuation War (1941–44) not only Finnish and Soviet, but also German and Italian vessels operated there (see also Naval Detachment K and Regia Marina). Under these circumstances, during much of the Siege of Leningrad (1941–44), Lake Ladoga provided the only access to the besieged city because a section of the eastern shore remained in Soviet hands. Supplies were transported into Leningrad with trucks on winter roads over the ice, the "Road of Life", and by boat in the summer. After World War II, Finland lost the Karelia region again to the USSR, and all Finnish citizens were evacuated from the ceded territory. Ladoga became an internal Soviet basin once again. The northern shore, Ladoga Karelia with the town of Sortavala, is now part of the Republic of Karelia. The western shore, Karelian Isthmus, became part of Leningrad Oblast.
Since 1996 the Lake Ladoga Challenge is an annual competition for 4x4 vehicles and ATVs that travels through over 1200 kilometers of the mud, swamp and bog that surround Lake Ladoga.

</doc>
<doc id="18190" url="https://en.wikipedia.org/wiki?curid=18190" title="Language family">
Language family

A language family is a group of languages related through descent from a common ancestor, called the proto-language of that family. The term 'family' reflects the tree model of language origination in historical linguistics, which makes use of a metaphor comparing languages to people in a biological family tree, or in a subsequent modification, to species in a phylogenetic tree of evolutionary taxonomy. No actual biological relationship between speakers is implied by the metaphor.
Estimates of the number of living languages vary from 5,000 to 8,000, depending on the precision of one's definition of "language", and in particular on how one classifies dialects. The 2013 edition of Ethnologue catalogs just over 7,000 living human languages. A "living language" is simply one that is used as the primary form of communication of a group of people. There are also many dead and extinct languages, as well as some that are still insufficiently studied to be classified, or even unknown outside their respective speech communities.
Membership of languages in a language family is established by comparative linguistics. Sister languages are said to have a "genetic" or "genealogical" relationship. The latter term is older, but has been revived in recent years to better distinguish the relationships between languages from the genetic relationships between people.
The evidence of linguistic relationship is found in observable shared characteristics that are not attributed to contact or borrowing. 
Genealogically related languages present shared retentions, that is, features of the proto-language (or reflexes of such features) that cannot be explained by chance or borrowing (convergence). Membership in a branch or group within a language family is established by shared innovations, that is, common features of those languages that are not found in the common ancestor of the entire family. For example, Germanic languages are "Germanic" in that they share vocabulary and grammatical features that are not believed to have been present in the Proto-Indo-European language. These features are believed to be innovations that took place in Proto-Germanic, a descendant of Proto-Indo-European that was the source of all Germanic languages.
Structure of a family.
Language families can be divided into smaller phylogenetic units, conventionally referred to as "branches" of the family because the history of a language family is often represented as a tree diagram. A family is a monophyletic unit; that is, all its members derive from a common ancestor, and all attested descendants of that ancestor are included in the family. (In this way, the term "family" is analogous to the biological term "clade".) Some taxonomists restrict the term "family" to a certain level, but there is little consensus in how to do so. Those who affix such labels also subdivide branches into "groups", and groups into "complexes". A top-level (largest) family is often called a "phylum" or "stock". The term "macrofamily" or "superfamily" is sometimes applied to proposed groupings of language families whose status as phylogenetic units is generally considered to be unsubstantiated by accepted historical linguistic methods.
For example, the Celtic, Germanic, Slavic, Romance, and Indo-Iranian language families are branches of a larger Indo-European language family. There is a remarkably similar pattern shown by the linguistic tree and the genetic tree of human ancestry 
that was verified statistically. Languages interpreted in terms of the putative phylogenetic tree of human languages are transmitted to a great extent vertically (i.e. by ancestry) as opposed to horizontally (i.e. by spatial diffusion).
Dialect continua.
Some closely knit language families, and many branches within larger families, take the form of dialect continua, in which there are no clear-cut borders that make it possible to unequivocally identify, define, or count individual languages within the family. However, when the differences between the speech of different regions at the extremes of the continuum are so great that there is no mutual intelligibility between them, as occurs for Arabic, the continuum cannot meaningfully be seen as a single language. A speech variety may also be considered either a language or a dialect depending on social or political considerations. Thus different sources give sometimes wildly different accounts of the number of languages within a family. Classifications of the Japonic family, for example, range from one language (a language isolate) to nearly twenty.
Isolates.
Most of the world's languages are known to be related to others. Those that have no known relatives (or for which family relationships are only tentatively proposed) are called language isolates, essentially language families consisting of a single language. An example is Basque. In general, it is assumed that language isolates have relatives, or had relatives at some point in their history, but at a time depth too great for linguistic comparison to recover them.
A language isolated in its own branch within a family, such as Armenian within Indo-European, is often also called an isolate, but the meaning of "isolate" in such cases is usually clarified. For instance, Armenian may be referred to as an "Indo-European isolate". By contrast, so far as is known, the Basque language is an absolute isolate: It has not been shown to be related to any other language despite numerous attempts. A language may be said to be an isolate currently but not historically if related but now extinct relatives are attested. The Aquitanian language, spoken in Roman times, may have been an ancestor of Basque, but it could also have been a sister language to the ancestor of Basque. In the latter case, Basque and Aquitanian would form a small family together. (Ancestors are not considered to be distinct members of a family.)
Proto-languages.
The common ancestor of a language family is seldom known directly since most languages have a relatively short recorded history. However, it is possible to recover many features of a proto-language by applying the comparative method, a reconstructive procedure worked out by 19th century linguist August Schleicher. This can demonstrate the validity of many of the proposed families in the list of language families. For example, the reconstructible common ancestor of the Indo-European language family is called "Proto-Indo-European". Proto-Indo-European is not attested by written records and so it is conjectured to have been spoken before the invention of writing.
Sometimes, however, a proto-language "can" be identified with a historically known language. For instance, dialects of Old Norse are the proto-language of Norwegian, Swedish, Danish, Faroese and Icelandic. Likewise, the Appendix Probi depicts Proto-Romance, a language almost unattested due to the prestige of Classical Latin, a highly stylised literary register not representative of the speech of ordinary people.
Other classifications of languages.
Sprachbund.
Shared innovations, acquired by borrowing or other means, are not considered genetic and have no bearing with the language family concept. It has been asserted, for example, that many of the more striking features shared by Italic languages (Latin, Oscan, Umbrian, etc.) might well be "areal features". However, very similar-looking alterations in the systems of long vowels in the West Germanic languages greatly postdate any possible notion of a proto-language innovation (and cannot readily be regarded as "areal", either, since English and continental West Germanic were not a linguistic area). In a similar vein, there are many similar unique innovations in Germanic, Baltic and Slavic that are far more likely to be areal features than traceable to a common proto-language. But legitimate uncertainty about whether shared innovations are areal features, coincidence, or inheritance from a common ancestor, leads to disagreement over the proper subdivisions of any large language family.
A sprachbund is a geographic area having several languages that feature common linguistic structures. The similarities between those languages are caused by language contact, not by chance or common origin, and are not recognized as criteria that define a language family. An example of a sprachbund would be the Indian subcontinent.
Contact languages.
The concept of language families is based on the historical observation that languages develop dialects, which over time may diverge into distinct languages. However, linguistic ancestry is less clear-cut than familiar biological ancestry, in which species do not crossbreed. It is more like the evolution of microbes, with extensive lateral gene transfer: Quite distantly related languages may affect each other through language contact, which in extreme cases may lead to languages with no single ancestor, whether they be creoles or mixed languages. In addition, a number of sign languages have developed in isolation and appear to have no relatives at all. Nonetheless, such cases are relatively rare and most well-attested languages can be unambiguously classified as belonging to one language family or another, even if this family's relation to other families is not known.

</doc>
<doc id="18194" url="https://en.wikipedia.org/wiki?curid=18194" title="Looe Island">
Looe Island

Looe Island (, meaning "island of the monk's enclosure"), also known as St George's Island, and historically St Michael's Island is a small island a mile from the mainland town of Looe off Cornwall, England.
According to local legend, Joseph of Arimathea landed here with the child Christ. Some scholars, including Glyn Lewis, suggest the island could be as Ictis, the location described by Diodorus Siculus as a centre for the tin trade in pre-Roman Britain.
The island is now owned and managed by the Cornwall Wildlife Trust charity where access is carefully managed for the benefit of wildlife and landing is only possible via the Cornwall Wildlife Trust authorized boatman. The waters around the island are a marine nature reserve and form part of the Looe Voluntary Marine Conservation Area (VMCA). First established in 1995, the Looe VCMA covers nearly 5 km of coastline and aims to protect the coastal and marine wildlife around Looe.
History.
People have been living on Looe Island since the Iron Age. Evidence of early habitation includes pieces of Roman amphorae as well as stone boat anchors and Roman coins. In the Dark Ages, the island was used a seat of early Christian settlement. The child Jesus was believed to have visited the Island with his uncle, Joseph of Arimathea, who traded with the Cornish tin traders. Therefore, Looe Island became a place of pilgrimage for early Christians and a small thatched roofed chapel was built there during this time.
In the later medieval period, the island came under the overall control of Glastonbury Abbey, with the Prior of Lammana being directly responsible for its governance; the island's chapel was under the care of two Benedictine monks until 1289 when the property was sold to a local landowner. The priory was replaced by a domestic chapel served by a secular priest until the Dissolution of the Monasteries in 1536 when it became property of the Crown. From the 13th to the 16th centuries it was known as St Michael's Island but after the dissolution of the monasteries, it was rededicated in 1594 as St George's Island.
Through the 17th and 18th centuries the island was a popular haunt for smugglers avoiding the British Government's revenue cutters out of Plymouth and Falmouth. The Old Guildhall Museum in Looe hold information and research about the smuggling families of Looe Island and information is also available the more recent publications about the island.
In the 20th century, Looe island was owned (and inhabited) by two sisters, Babs and Evelyn Atkins, who wrote two books: "We Bought An Island" (1976, ISBN 0-245-52940-3) and its sequel "Tales From Our Cornish Island" (1986, ISBN 0-245-54265-5). They chronicle the purchase of the island and what it was like to live there. Evelyn died in 1997 at the age of 87; Babs continued to live on the island until her death in 2004, at the age of 86. On her death, the island was bequeathed to the Cornwall Wildlife Trust; it will be preserved as a nature reserve in perpetuity. The adjoining islet, formerly known as Little Island, now renamed Trelawny Island and connected by a small bridge, was bequeathed by Miss Atkins back to the Trelawny family, who previously owned Looe Island from 1743 to 1921. The full history of the island is explained at length in "Island Life: A History of Looe Island", published in 2006, and the role of the island today is briefly described in "Looe Island Then and Now" published in 2014.
Geography.
Situated in the English Channel, about one mile from East Looe in the direction of Polperro, it is about in area and a mile (1.6 km) in circumference. Its highest point is above sea level. Looe Island, like much of south west England, has a mild climate with frost and snow being rare.
The island is owned and managed by a charity, the Cornwall Wildlife Trust. This is a non-profit making venture, the landing fees and other income being devoted to conserving the island's natural beauty and providing facilities. The island is open during the summer to day visitors arriving by the Trust's boat. After a short welcome talk visitors are directed to the small visitor centre from where they can pick up a copy of the self-guided trail. Visitors have some two hours on the Island and all trips are subject to tides and weather/sea state. While it is normally accessible only by the Cornwall Wildlife Trust's boat, at extremely low spring tides it is possible for the journey to be made by foot across the slippery, seaweed covered rocky sea floor. However you have to remain on the beach and promptly head back to the mainland.
Media appearances.
In 2008, Channel 4's archaeology series "Time Team" visited the island to carry out an investigation into its early Christian history. They excavated the sites of Christian chapels built on both the island and on the mainland opposite. During their dig they found the remains of a Benedictine chapel that was built in c.1139 by monks from Glastonbury Abbey, a reliquary, graves and the remains of much earlier Anglo-Romano places of worship built of wood with dating evidence suggesting use by Christians before the reign of Constantine the Great.
In 1994/95 Andrew Hugill composed Island Symphony, an electro-acoustic piece utilising sampled sounds sourced over the net plus recorded natural sounds from the island itself.

</doc>
<doc id="18195" url="https://en.wikipedia.org/wiki?curid=18195" title="LaTeX">
LaTeX

LaTeX ( , commonly pronounced as or , a shortening of Lamport TeX) is a document preparation system. It is distinguished from typical word processors such as Microsoft Word, LibreOffice Writer and Apple Pages in that the writer uses plain text as opposed to formatted text, relying on markup tagging conventions to define the general structure of a document (such as article, book, and letter), to stylise text throughout a document (such as bold and italic), and to add citations and cross-referencing. A TeX distribution such as TeX Live or MikTeX is used to produce an output file (such as PDF or DVI) suitable for printing or digital distribution. Within the typesetting system, its name is stylised as LaTeX.
LaTeX is used for the communication and publication of scientific documents in many fields, including mathematics, physics, computer science, statistics, economics, and political science. It also has a prominent role in the preparation and publication of books and articles that contain complex multilingual materials, such as Sanskrit and Arabic. LaTeX uses the TeX typesetting program for formatting its output, and is itself written in the TeX macro language.
LaTeX is widely used in academia. LaTeX can be used as a standalone document preparation system, or as an intermediate format. In the latter role, for example, it is sometimes used as part of a pipeline for translating DocBook and other XML-based formats to PDF. The typesetting system offers programmable desktop publishing features and extensive facilities for automating most aspects of typesetting and desktop publishing, including numbering and cross-referencing of tables and figures, chapter and section headings, the inclusion of graphics, page layout, indexing and bibliographies.
Like TeX, LaTeX started as a writing tool for mathematicians and computer scientists, but from early in its development it has also been taken up by scholars who needed to write documents that include complex math expressions or non-Latin scripts, such as Arabic, Sanskrit and Chinese.
LaTeX is intended to provide a high-level language that accesses the power of TeX in an easier way for writers. In short, TeX handles the layout side, while LaTeX handles the content side for document processing. LaTeX comprises a collection of TeX macros and a program to process LaTeX documents. Because the plain TeX formatting commands are elementary, it provides authors with ready-made commands for formatting and layout requirements such as chapter headings, footnotes, cross-references and bibliographies.
LaTeX was originally written in the early 1980s by Leslie Lamport at SRI International. The current version is LaTeX2e (stylised as LaTeX2ε). LaTeX is free software and is distributed under the LaTeX Project Public License (LPPL).
Typesetting system.
LaTeX follows the design philosophy of separating presentation from content, so that authors can focus on the content of what they are writing without attending simultaneously to its visual appearance. In preparing a LaTeX document, the author specifies the logical structure using simple, familiar concepts such as "chapter", "section", "table", "figure", etc., and lets the LaTeX system worry about the formatting and layout of these structures. It therefore encourages the separation of layout from content while still allowing manual typesetting adjustments where needed. This concept is similar to the mechanism by which many word processors allow styles to be defined globally for an entire document or the use of Cascading Style Sheets to style HTML. The LaTeX system is a markup language that also handles typesetting and rendering.
LaTeX can be arbitrarily extended by using the underlying macro language to develop custom formats. Such macros are often collected into "packages," which are available to address special formatting issues such as complicated mathematical content or graphics. Indeed, in the example below, the codice_1 environment is provided by the codice_2 package.
In order to create a document in LaTeX, you first write a file, say codice_3, using your preferred text editor. Then you give your codice_3 file as input to the TeX program (with the LaTeX macros loaded), and TeX writes out a file suitable for viewing onscreen or printing. This write-format-preview cycle is one of the chief ways in which working with LaTeX differs from what-you-see-is-what-you-get word-processing. It is similar to the code-compile-execute cycle familiar to computer programmers. Today, many LaTeX-aware editing programs make this cycle a simple matter of pressing a single key, while showing the output preview on the screen beside the input window. Some online Latex editors automatically refresh the preview.
Examples.
The example below shows the LaTeX input and corresponding output:
[[File:Latex example.png|frame|center|LaTeX output of below markups:
<syntaxhighlight lang="latex" highlight="22">
</syntaxhighlight>
Note how the equation for formula_1 (highlighted in the example code) was typeset by the markup:
The square root is denoted by "codice_5" and fractions by "codice_6".
Pronouncing and writing "LaTeX".
LaTeX is usually pronounced or in English (that is, not with the pronunciation English speakers normally associate with "X", but with a ).
The characters T, E, X in the name come from capital Greek letters tau, epsilon, and chi, as the name of TeX derives from the (skill, art, technique); for this reason, TeX's creator Donald Knuth promotes a pronunciation of () (that is, with a voiceless velar fricative as in Modern Greek, similar to the ch in loch). Lamport writes "TeX is usually pronounced "tech", making "lah"-teck, lah-"teck", and "lay"-teck the logical choices;" in the first chapter of the original LaTeX User Guide.
The name is traditionally printed in running text with a special typographical logo: LaTeX.
In media where the logo cannot be precisely reproduced in running text, the word is typically given the unique capitalization "LaTeX". The TeX, LaTeX and XeTeX logos can be rendered via pure CSS and XHTML for use in graphical web browsers following the specifications of the internal codice_7 macro.
Licensing.
LaTeX is typically distributed along with plain TeX. It is distributed under a free software license, the LaTeX Project Public License (LPPL). The LPPL is not compatible with the GNU General Public License, as it requires that modified files must be clearly differentiable from their originals (usually by changing the filename); this was done to ensure that files that depend on other files will produce the expected behavior and avoid dependency hell. The LPPL is DFSG compliant as of version 1.3. As free software, LaTeX is available on most operating systems including UNIX (Solaris, HP-UX, AIX), BSD (FreeBSD, Mac OS X, NetBSD, OpenBSD), Linux (Red Hat, Debian GNU/Linux, Arch, Gentoo), Microsoft Windows (9x, XP, Vista, 7, 8), DOS, RISC OS, AmigaOS and Plan9.
Related software.
As a macro package, LaTeX provides a set of macros for TeX to interpret. There are many other macro packages for TeX, including Plain TeX, GNU Texinfo, AMSTeX, and ConTeXt.
When TeX "compiles" a document, it follows (from the user's point of view) the following processing sequence: Macros → TeX → Driver → Output. Different implementations of each of these steps are typically available in TeX distributions. Traditional TeX will output a DVI file, which is usually converted to a PostScript file. More recently, Hàn Thế Thành and others have written a new implementation of TeX called pdfTeX, which also outputs to PDF and takes advantage of features available in that format. The XeTeX engine developed by Jonathan Kew merges modern font technologies and Unicode with TeX.
The default font for LaTeX is Knuth's Computer Modern, which gives default documents created with LaTeX the same distinctive look as those created with plain TeX. XeTeX allows the use of OpenType and TrueType (that is, outlined) fonts for output files.
There are also many editors for LaTeX.
Versions.
LaTeX2e is the current version of LaTeX, since it replaced LaTeX 2.09 in 1994. , LaTeX3, started in the early 1990s, is under a long-term development project. Planned features include improved syntax, hyperlink support, a new user interface, access to arbitrary fonts, and new documentation.
There are numerous commercial implementations of the entire TeX system. System vendors may add extra features like additional typefaces and telephone support. LyX is a free, WYSIWYM visual document processor that uses LaTeX for a back-end. TeXmacs is a free, WYSIWYG editor with similar functionalities as LaTeX but a different typesetting engine. Other WYSIWYG editors that produce LaTeX include Scientific Word on MS Windows.
A number of community-supported TeX distributions are available, including TeX Live (multiplatform), teTeX (deprecated in favor of TeX Live, UNIX), fpTeX (deprecated), MiKTeX (Windows), proTeXt (Windows), MacTeX (TeX Live with the addition of Mac specific programs), gwTeX (Mac OS X), OzTeX (Mac OS Classic), AmigaTeX (no longer available), PasTeX (AmigaOS, available on the Aminet repository), and Auto-Latex Equations (Google Docs add-on that supports MathJax LaTeX commands).
Compatibility.
LaTeX documents (codice_8) can be opened with any text editor. They consist of plain text and do not contain hidden formatting codes or binary instructions. Additionally, TeX documents can be shared by rendering the LaTeX file to Rich Text Format (codice_9) or XML. This can be done using the free software programs LaTeX2RTF or TeX4ht. LaTeX can also be rendered to PDF files using the LaTeX extension pdfLaTeX. LaTeX files containing Unicode text can be processed into PDFs by the LaTeX extension XeLaTeX.

</doc>
<doc id="18196" url="https://en.wikipedia.org/wiki?curid=18196" title="List of saints">
List of saints

This is an incomplete list of Christian saints in alphabetical order by Christian name, but, where known and given, a surname, location, or personal attribute (included as part of the name) may affect the ordering.
One list says there are 810 canonized Roman Catholic saints (who have been through the formal institutional process of canonization), although some give numbers in the thousands. (Pope John Paul II alone canonized 110 individuals, plus many group canonizations such as 110 martyr saints of China, 103 Korean martyrs, 117 Vietnamese martyrs, Mexican Martyrs, Spanish martyrs and French revolutionary martyrs.) Among the Eastern Orthodox and Oriental Orthodox Communions, the numbers may be even higher, since there is no fixed process of "canonization" and each individual jurisdiction within the two Orthodox communions independently maintains parallel lists of saints that have only partial overlap. Note that 78 popes are considered saints.
The Anglican Communion have canonized one saint—King Charles I of England (see Society of King Charles the Martyr). However, it recognizes pre-Reformation saints, as does the United Methodist Church. Persons who have led lives of celebrated sanctity or missionary zeal are included in the Calendar of the Prayer Book "without thereby enrolling or commending such persons as saints of the Church". Similarly, any individuals commemorated in the Lutheran calendar of saints will be listed as well.
Wikipedia contains calendars of saints for particular denominations, listed by the day of the year on which they are traditionally venerated, as well as a chronological list of saints and blesseds, listed by their date of death.
Notes.
4 "Common Worship" has "Commemoration".
6 Eastern Rite Catholic Churches only. 
7 Russian Orthodox Church only. 
8 exact citation is needed as to exactly which part, if any, of the Orthodox Chuch venerates this saint.
9 Ukrainian Orthodox Church.

</doc>
<doc id="18198" url="https://en.wikipedia.org/wiki?curid=18198" title="Lebesgue measure">
Lebesgue measure

In measure theory, the Lebesgue measure, named after French mathematician Henri Lebesgue, is the standard way of assigning a measure to subsets of "n"-dimensional Euclidean space. For "n" = 1, 2, or 3, it coincides with the standard measure of length, area, or volume. In general, it is also called "n"-dimensional volume, "n"-volume, or simply volume. It is used throughout real analysis, in particular to define Lebesgue integration. Sets that can be assigned a Lebesgue measure are called Lebesgue measurable; the measure of the Lebesgue measurable set "A" is denoted by λ("A").
Henri Lebesgue described this measure in the year 1901, followed the next year by his description of the Lebesgue integral. Both were published as part of his dissertation in 1902.
The Lebesgue measure is often denoted "dx", but this should not be confused with the distinct notion of a volume form.
Definition.
Given a subset formula_1, with the length of an (open, closed, semi-open) interval formula_2 given by formula_3, the Lebesgue outer measure formula_4 is defined as
The Lebesgue measure of E is given by its Lebesgue outer measure formula_6 if, for every formula_7,
Intuition.
The first part of the definition states that the subset formula_9 of the real numbers is reduced to its outer measure by coverage by sets of intervals. Each of these sets of intervals formula_10 covers formula_9 in the sense that when the intervals are combined together by union, they contain formula_9. The total length of any covering interval set can easily overestimate the measure of formula_9, because formula_9 is a subset of the union of the intervals, and so the intervals may include points which are not in formula_9. The Lebesgue outer measure emerges as the greatest lower bound (infimum) of the lengths from among all possible such sets. Intuitively, it is the total length of those interval sets which fit formula_9 most tightly and do not overlap.
That characterizes the Lebesgue outer measure. Whether this outer measure translates to the Lebesgue measure proper depends on an additional condition. This condition is tested by taking subsets of the real numbers formula_17 using formula_9 as an instrument to split formula_17 into two partitions: the part of formula_17 which intersects with formula_9 and the remaining part of formula_17 which is not in formula_9: the set difference of formula_17 and formula_9. These partitions of formula_17 are subject to the outer measure. If for all possible such subsets formula_17 of the real numbers, the partitions of formula_17 cut apart by formula_9 have outer measures which add up to the outer measure of formula_17, then the outer Lebesgue measure of formula_9 gives its Lebesgue measure. Intuitively, this condition means that the set formula_9 must not have some curious properties which causes a discrepancy in the measure of another set when formula_9 is used as a "mask" to "clip" that set, hinting at the existence of sets for which the Lebesgue outer measure does not give the Lebesgue measure. (Such sets are, in fact, not Lebesgue-measurable.)
Properties.
The Lebesgue measure on R"n" has the following properties:
All the above may be succinctly summarized as follows:
The Lebesgue measure also has the property of being σ-finite.
Null sets.
A subset of R"n" is a "null set" if, for every ε > 0, it can be covered with countably many products of "n" intervals whose total volume is at most ε. All countable sets are null sets.
If a subset of R"n" has Hausdorff dimension less than "n" then it is a null set with respect to "n"-dimensional Lebesgue measure. Here Hausdorff dimension is relative to the Euclidean metric on R"n" (or any metric Lipschitz equivalent to it). On the other hand a set may have topological dimension less than "n" and have positive "n"-dimensional Lebesgue measure. An example of this is the Smith–Volterra–Cantor set which has topological dimension 0 yet has positive 1-dimensional Lebesgue measure.
In order to show that a given set "A" is Lebesgue measurable, one usually tries to find a "nicer" set "B" which differs from "A" only by a null set (in the sense that the symmetric difference ("A" − "B") formula_43("B" − "A") is a null set) and then show that "B" can be generated using countable unions and intersections from open or closed sets.
Construction of the Lebesgue measure.
The modern construction of the Lebesgue measure is an application of Carathéodory's extension theorem. It proceeds as follows.
Fix . A box in R"n" is a set of the form
where , and the product symbol here represents a Cartesian product. The volume of this box is defined to be
For "any" subset "A" of R"n", we can define its outer measure "λ"*("A") by:
We then define the set "A" to be Lebesgue measurable if for every subset "S" of R"n",
These Lebesgue measurable sets form a σ-algebra, and the Lebesgue measure is defined by for any Lebesgue measurable set "A".
The existence of sets that are not Lebesgue measurable is a consequence of a certain set-theoretical axiom, the axiom of choice, which is independent from many of the conventional systems of axioms for set theory. The Vitali theorem, which follows from the axiom, states that there exist subsets of R that are not Lebesgue measurable. Assuming the axiom of choice, non-measurable sets with many surprising properties have been demonstrated, such as those of the Banach–Tarski paradox.
In 1970, Robert M. Solovay showed that the existence of sets that are not Lebesgue measurable is not provable within the framework of Zermelo–Fraenkel set theory in the absence of the axiom of choice (see Solovay's model).
Relation to other measures.
The Borel measure agrees with the Lebesgue measure on those sets for which it is defined; however, there are many more Lebesgue-measurable sets than there are Borel measurable sets. The Borel measure is translation-invariant, but not complete.
The Haar measure can be defined on any locally compact group and is a generalization of the Lebesgue measure (R"n" with addition is a locally compact group).
The Hausdorff measure is a generalization of the Lebesgue measure that is useful for measuring the subsets of R"n" of lower dimensions than "n", like submanifolds, for example, surfaces or curves in R³ and fractal sets. The Hausdorff measure is not to be confused with the notion of Hausdorff dimension.
It can be shown that there is no infinite-dimensional analogue of Lebesgue measure.

</doc>
<doc id="18201" url="https://en.wikipedia.org/wiki?curid=18201" title="Lake Champlain">
Lake Champlain

Lake Champlain (; French: "Lac Champlain") is a natural freshwater lake in North America, located mainly within the borders of the United States (states of Vermont and New York) but partially situated across the Canada–United States border in the Canadian province of Quebec.
The New York portion of the Champlain Valley includes the eastern portions of Clinton County and Essex County. Most of this area is part of the Adirondack Park. There are recreational opportunities in the park and along the relatively undeveloped coastline of Lake Champlain. The cities of Plattsburgh, New York and Burlington, Vermont are on the west and east shores of the lake, respectively, and the village of Ticonderoga, New York is located in the southern part of the region. The Quebec portion is located in the regional county municipalities of Le Haut-Richelieu and Brome-Missisquoi. There are a number of islands in the lake; the largest include the towns of South Hero, Grand Isle, North Hero, Alburgh, and Isle La Motte, all belonging to Grand Isle County in the state of Vermont.
Geology.
The Champlain Valley is the northernmost unit of a landform system known as the Great Appalachian Valley, which stretches from Quebec to Alabama. The Champlain Valley is a physiographic section of the larger Saint Lawrence Valley, which in turn is part of the larger Appalachian physiographic division.
It is one of numerous large lakes located in an arc from Labrador through the northern United States and into the Northwest Territories of Canada. Although it is smaller than each of the Great Lakes: Ontario, Erie, Huron, Superior, or Michigan, Lake Champlain is a large body of fresh water. Approximately in area, the lake is roughly long, and across at its widest point. The maximum depth is approximately . The lake varies seasonally from about above mean sea level.
Hydrology.
Lake Champlain is situated in the Lake Champlain Valley between the Green Mountains of Vermont and the Adirondack Mountains of New York, drained northward by the -long Richelieu River into the St. Lawrence River at Sorel-Tracy, Quebec northeast and downstream of Montreal. It also receives the waters from the -long Lake George, so its basin collects waters from the northwestern slopes of the Green Mountains of Vermont and the northernmost eastern peaks of the Adirondack Mountains of New York.
The lake drains nearly half of Vermont. About 250,000 people get their drinking water from the lake.
The lake is fed by Otter Creek, the Winooski, Poultney, Missisquoi, and Lamoille Rivers in Vermont, and the Ausable, Chazy, Boquet, Saranac and La Chute rivers in New York.
It is connected to the Hudson River by the Champlain Canal.
Portions of the lake freeze each winter, and in some winters the entire lake surface freezes, referred to as "closing". The lake temperature reaches an average of in July and August.
Chazy Reef.
The Chazy Reef is an extensive Ordovician carbonate rock formation which extends from Tennessee to Quebec and Newfoundland. It occurs in prominent outcropping at Goodsell Ridge, Isle La Motte, the northernmost island in Lake Champlain.
The oldest reefs are around "The Head" of the south end of the island; slightly younger reefs are found at the Fisk Quarry; and the youngest (the famous coral reefs) are located in fields to the north. Together, these three sites provide a unique narrative of events which took place over 450 million years ago in the ocean in the Southern Hemisphere, long before the emergence of Lake Champlain twenty thousand years ago.
History.
The lake was named after the French explorer Samuel de Champlain, who encountered it in 1609. While the ports of Burlington, Vermont; Port Henry, New York; and Plattsburgh, New York today are primarily used by small craft, ferries and lake cruise ships, they were of substantial commercial and military importance in the 18th and 19th centuries.
A variety of Native American names for the lake were recorded by historians. Many historical works give "Caniaderi Guarunte" as the Iroquois name for the lake (meaning: mouth or door of the country); the lake was an important northern gateway to their lands. A number of other sources give "Petonbowk" (meaning the lake in between) as the Abenaki name in their Algonquian language for the lake. The St. Francis/Sokoki Abenaki Band, who make their home along the "Masipskiwibi" River (in Missisquoi language, "Crooked River") in northwestern Vermont, call the lake "Bitawbagok", which has the same meaning as "Petonbowk". Some early 21st-century articles appeared during the Champlain Quadricentennial (2009) claiming "Ondakina" as the “local” native name for the lake, but none cites a verifiable source.
Colonial America and the Revolutionary War.
New France allocated concessions all along lake Champlain to French settlers, and built forts to defend the waterways. In colonial times, Lake Champlain was used as a water passage (or, in winter, ice) between the Saint Lawrence and the Hudson valleys. Travelers found it easier to journey by boats and sledges on the lake rather than to go overland on the unpaved and frequently mud-bound roads of the time. The northern tip of the lake at Saint-Jean-sur-Richelieu, Quebec (known as St. John in colonial times under British rule) is a short distance from Montreal. The southern tip at Whitehall (Skenesborough in revolutionary times) is a short distance from Saratoga, Glens Falls, and Albany, New York.
Forts were built at Ticonderoga and Crown Point (Fort St. Frederic) to control passage on the lake in colonial times. Important battles were fought at Ticonderoga in 1758 and 1775. During the Revolutionary War, the British and Americans conducted a frenetic shipbuilding race through the Spring and Summer of 1776 at opposite ends of the lake, fighting a significant naval engagement on October 11 at the Battle of Valcour Island. While it was a tactical defeat for the Americans and the small fleet led by Benedict Arnold was almost entirely destroyed, the Americans gained a strategic victory. The British invasion was delayed long enough so that the approach of winter prevented the fall of these forts until the following year. In this period, the Continental Army gained strength and was victorious at Saratoga.
The Beginning of the Revolutionary War.
At the start of the Revolutionary War, British forces occupied the Champlain Valley. However, it did not take long for rebel leaders to realize the importance of controlling Lake Champlain. Early in the war, the colonial militias were attempting to expel the British from Boston, however this undertaking could not be achieved without heavy artillery. The British forts at Ticonderoga and Crown Point on Lake Champlain were known to have ample supplies of artillery and were weakly manned by the British. Thus, the colonial militias quickly devised a plan to take control of the two forts and bring the guns back to the fight in Boston.
The necessity of controlling the two forts at Ticonderoga and Crown Point placed Lake Champlain as a strategic arena during the Revolutionary War. By taking control of these forts, Americans not only gained heavy artillery, but also control of a vast water highway. Lake Champlain provided a direct invasion route to British Canada. However, if the British controlled the lake, then they would have had the ability to divide the colonies of New England and further deplete the Continental Army.
The first offensive action conducted by the Continental Army occurred in May 1775, three weeks after the battle at Lexington and Concord. Ethan Allen, accompanied by two hundred Green Mountain Boys, was issued to capture Fort Ticonderoga and retrieve supplies for the fight in Boston. Benedict Arnold shared the command with Allen, and in early May 1775, they captured Fort Ticonderoga, Crown Point, and the southern Loyalist settlement of Skenesborough. As a result of Allen’s offensive attack on the Champlain Valley in 1775, The American forces controlled the Lake Champlain waterway.
The Siege of Quebec: 1775-1776.
The Continental Army realized the strategic advantage of controlling Lake Champlain, as it leads directly to the heart of Quebec. Immediately after taking Fort Ticonderoga and Crown Point, the Americans began planning an attack on British Canada. The American siege of Quebec was a two-pronged assault and occurred throughout the winter of 1775-1776. Brigadier General Richard Montgomery led the first assault up the Champlain Valley into Canada, while Benedict Arnold led a second army to Quebec via the Maine wilderness. Despite the strategic advantage of controlling a direct route to Quebec by way of the Champlain Valley, the American siege of British Canada during the winter of 1775 failed. The Continental Army mistakenly assumed that they would receive support from the Canadians upon their arrival at Quebec. This was not the case, as the rebel army struggled to take Quebec with diminishing supplies, support, and harsh northern winter weather. The Continental Army was forced to camp outside Quebec’s walls for the duration of the winter, with reinforcements from New York, Pennsylvania, Massachusetts, New Hampshire, and Connecticut allowing the soldiers to maintain siege of the city throughout. The reinforcements travelled hundreds of miles up the frozen Lake Champlain and St. Lawrence River, however were too late and too few to influence a successful siege of Quebec. In May 1776, with the arrival of a British convoy carrying 10,000 British and Hessian troops to Canada, the Continental forces retreated back down the Champlain Valley to reevaluate their strategy.
“I know of no better method than to secure the important posts of Ticonderoga and Crown Point, and by building a number of armed vessels to command the lakes, otherwise the forces now in Canada will be brought down upon us as quick as possible, having nothing to oppose them… They will doubtless try to construct some armed vessels and then endeavor to penetrate the country toward New York.” (Brigadier General John Sullivan to George Washington, June 24, 1776).
Both British and American forces spent the summer of 1776 building their naval fleets at opposite ends of Lake Champlain. By the October 1776, The Continental Army had 16 operating naval vessels on Lake Champlain, a great increase to the four small ships they had at the beginning of the summer. General Benedict Arnold commanded the American naval fleet on Lake Champlain, which was composed of volunteers and soldiers drafted from the Northern Army. With great contrast to the Continental navy, experienced Royal Navy officers, British seamen, and Hessian artillerymen manned the British fleet on Lake Champlain. By the end of the summer of 1776, the two opposing armies were prepared to battle over the strategic advantage of controlling Lake Champlain.
The Battle of Valcour Island.
On October 11, 1776, the British and American naval fleets met on the western side of Valcour Island on Lake Champlain. American General Benedict Arnold established the location, as it provided the Continental fleet with a natural defensive position. The British and American vessels engaged in naval combat for much of the day, only stopping due to the impending darkness of nightfall. After a long day of combat, the American fleet was in worse shape than the experienced British navy. Upon ceasefire, Arnold called a council of war with his fellow officers, proposing a plan to escape the British fleet via rowboats under the cover of night. As the British burned the Royal Savage to the east, the Americans successfully rowed past the British lines. However, upon realization of the Americans escape the following morning, the British set out after the fleeing continental vessels. On October 13, the British fleet caught up to the struggling American ships near Split Rock Mountain. With no hope of fighting off the powerful British navy, Arnold ordered his men to run their five vessels aground in Ferris Bay, Panton. The depleted Continental army successfully escaped on land back to Fort Ticonderoga and Mount Independence, however they no longer controlled the Lake Champlain waterway.
The approaching winter of 1776-1777 restricted British movement along the recently controlled Lake Champlain. As the British abandoned Crown Point and returned to Canada for the winter, the Americans reduced their garrisons in the Champlain Valley from 13,000 to 2,500 soldiers. However, this vacancy of Lake Champlain was only temporary, as the spring of 1777 brought with it bad news for the Continental Army.
General Burgoyne’s Campaign.
In the spring of 1777, British General John Burgoyne led 8,000 troops from Canada down Lake Champlain and into the Champlain Valley. The goal of this invasion was to divide the New England colonies, thus forcing the Continental Army into a separated fight on multiple fronts. Lake Champlain provided Burgoyne with protected passage deep into the American colonies. Burgoyne’s army reached Fort Ticonderoga and Mount Independence in late June of 1777. During the night of July 5, the American forces fled Fort Ticonderoga as the British took control of the fort. However, Burgoyne’s southern campaign did not go unattested. On October 7, 1777, American General Gates, who at the time occupied Bemis Heights, met Burgoyne’s army at the Second Battle of Freeman’s Farm. At Freeman’s Farm, Burgoyne’s army suffered its final defeat and ended their invasion south into the colonies. Ten days later, on October 17, 1777, British General Burgoyne surrendered his army at Saratoga. This defeat was instrumental to the momentum of the Revolutionary War, as the defeat of the British army along the Champlain-Hudson waterway convinced France to ally with the American army.
Aftermath of 1777.
Following the failed British campaign led by General Burgoyne, the British still maintained control over Lake Champlain for the duration of the Revolutionary War. The British used the Champlain waterway for to supply raids across the Champlain Valley from 1778 to 1780. Furthermore, Lake Champlain provided direct transportation of supplies from the British posts at the Northern end of the lake. With the end of the Revolutionary War in 1783, the British naval fleet on Lake Champlain retreated up to St. John’s. Americans were eager to take back control of Lake Champlain to use the water highway as a trade route.
Post-Revolutionary War Period.
As British forces retreated from the Champlain Valley following the end of the Revolutionary War, American citizens flocked to settle the banks of Lake Champlain. Many individuals emigrated from Massachusetts and other New England colonies, such as Salmon Dutton, a settler of Cavendish, Vermont. Dutton emigrated in 1782, and worked as a surveyor, town official, and toll road owner. Dutton’s home had a dooryard garden, typical of mid-nineteenth century New England village homes. His experience settling in the Champlain Valley depicts the industries and lifestyles surrounding Lake Champlain following the Revolutionary War.
Similar to the experience of Salmon Dutton, former colonial militia captain Hezekiah Barnes settled in Charlotte, Vermont, in 1787. Following the war, Barnes also worked as a road surveyor and established an inn and trading post in Charlotte, along the main trade route from Montreal down Lake Champlain. Barnes’s stagecoach inn was built in traditional Georgian style, with ten fireplaces and a ballroom on the interior, and a wraparound porch on the outside. In 1800, Continental Army Capitan Benjamin Harrington established a distillery business in Shelburne, Vermont, which supplied his nearby inn. Furthermore, Captain Stevens and Jeremiah Trescott constructed a water-powered sawmill in South Royalton, Vermont, in the late 1700s. All of these individual accounts shed light on the significance of Lake Champlain during the post-Revolutionary War period.
War of 1812.
During the War of 1812, British and American forces faced each other in the Battle of Lake Champlain, also known as the Battle of Plattsburgh, fought on September 11, 1814. This ended the final British invasion of the northern states during the War of 1812. It was fought just prior to the signing of the Treaty of Ghent, and the American victory denied the British any leverage to demand exclusive control over the Great Lakes or territorial gains against the New England states.
Three US Naval ships have been named after this battle, including , , and a cargo ship used during World War I.
Following the War of 1812, the US Army began construction on "Fort Blunder", an unnamed fortification built at the northernmost end of Lake Champlain to protect against attacks from British Canada. Its nickname came from a surveying error: the initial phase of construction on the fort turned out to be taking place on a point north of the Canadian border. Once this error was spotted, construction was abandoned. Locals scavenged materials used in the abandoned fort for use in their own homes and public buildings.
By the Webster-Ashburton Treaty of 1842, the US-Canadian border was adjusted northward to include the strategically important site of "Fort Blunder" on the US side. In 1844, work was begun to replace the remains of the 1812-era fort with a massive new Third System masonry fortification known as Fort Montgomery. Portions of this fort are still standing.
Modern history.
In the early 19th century, the construction of the Champlain Canal connected Lake Champlain to the Hudson River system, allowing north-south commerce by water from New York City to Montreal and Atlantic Canada.
In 1909, 65,000 people celebrated the 300th anniversary of the French discovery of the lake. Attending dignitaries included President William Howard Taft, along with representatives from France, Canada and the United Kingdom.
In 1929, then-New York Governor Franklin Roosevelt and Vermont Governor John Weeks, dedicated the first bridge to span the lake, built from Crown Point to Chimney Point. This bridge lasted until December 2009. Severe deterioration was found, and the bridge was demolished and replaced with the Lake Champlain Bridge, which opened in November 2011.
On February 19, 1932, boats were able to sail on Lake Champlain. It was the first time that the lake was known to be free of ice during the winter at that time.
Lake Champlain briefly became the nation's sixth Great Lake on March 6, 1998, when President Clinton signed Senate Bill 927. This bill, which was led by U.S. Senator Patrick Leahy of Vermont and reauthorized the National Sea Grant Program, contained a line declaring Lake Champlain to be a Great Lake. This status enabled its neighboring states to apply for additional federal research and education funds allocated to these national resources. Following a small uproar, the Great Lake status was rescinded on March 24 (although New York and Vermont universities continue to receive funds to monitor and study the lake).
"Champ", Lake Champlain monster.
In 1609 Samuel de Champlain wrote that he saw a lake monster five feet (1.5 m) long, as thick as a man's thigh, with silver-gray scales a dagger could not penetrate. The alleged monster had jaws with sharp and dangerous teeth. Native Americans claimed to have seen similar monsters . This mysterious creature is likely the original Lake Champlain monster. The monster has been memorialized in sports teams names and mascots: the Vermont Lake Monsters and mascot (Champ) of the state's minor league baseball team. A Vermont Historical Society publication recounts the story and offers possible explanations for accounts of the so-called monster: "floating logs, schools of large sturgeons diving in a row, or flocks of black birds flying close to the water."
Ecology.
A pollution prevention, control, and restoration plan for Lake Champlain was first endorsed in October 1996 by the governors of New York and Vermont, and the regional administrators of the United States Environmental Protection Agency (EPA). In April 2003, the plan was updated and Quebec signed onto it. The plan is being implemented by the Lake Champlain Basin Program and its partners at the state, provincial, federal and local level. It is renowned as a model for interstate and international cooperation. Its primary goals are to reduce phosphorus inputs to Lake Champlain; reduce toxic contamination; minimize the risks to humans from water-related health hazards; and control the introduction, spread, and impact of non-native nuisance species to preserve the integrity of the Lake Champlain ecosystem.
Agricultural and urban runoff from the watershed or drainage basin is the primary source of excess phosphorus, which exacerbates algae blooms in Lake Champlain. The most problematic blooms have been cyanobacteria, commonly called blue-green algae, in the northeastern part of the Lake, primarily Missisquoi Bay.
To reduce phosphorus runoff to this part of the lake, Vermont and Quebec agreed to reduce their inputs by 60% and 40%, respectively, by an agreement signed in 2002. While agricultural sources (manure and fertilizers) are the primary sources of phosphorus (about 70%) in the Missisquoi basin, runoff from developed land and suburbs is estimated to contribute about 46% of the phosphorus runoff basin-wide to Lake Champlain, and agricultural lands contributed about 38%.
In 2002, the cleanup plan noted that the lake had the capacity to absorb of phosphorus each year. In 2009, a judge noted that were still flowing in annually, more than twice what the lake could handle. Sixty municipal and industrial sewage plants discharge processed waste from the Vermont side.
In 2008, the EPA expressed concerns to the State of Vermont that the Lake's cleanup was not progressing fast enough to meet the original cleanup goal of 2016. The State, however, cites its Clean and Clear Action Plan as a model that will produce positive results for Lake Champlain.
In 2007, Vermont banned phosphates for dishwasher use starting in 2010. This will prevent an estimated from flowing into the lake. While this represents 0.6% of the phosphate pollution, it took US$1.9 million to remove the pollutant from treated wastewater, an EPA requirement.
Despite concerns about pollution, Lake Champlain is safe for swimming, fishing, and boating. It is considered a world-class fishery for salmonid species (Lake trout and Atlantic salmon) and bass. About 81 fish species live in the Lake, and more than 300 bird species rely on it for habitat and as a resource during migrations.
By 2008 at least six institutions were monitoring lake water health:
In 2001, scientists estimated that farming contributed 38% of the phosphorus runoff. By 2010, results of environmentally conscious farming practices, enforced by law, had made any positive contribution to lake cleanliness. A federally funded study was started to analyze this problem and to arrive at a solution.
Biologists have been trying to control lampreys in the lake since 1985 or earlier. Lampreys are native to the area, but have expanded in population to such an extent that they wounded nearly all lake trout in 2006 and 70-80% of salmon. The use of pesticides against the lamprey has reduced their casualties of other fish to 35% of salmon and 31% of lake trout. The goal was 15% of salmon and 25% of lake trout.
The federal and state governments originally budgeted US$18 million for lake programs for 2010. This was later supplemented by an additional US$6.5 million from the federal government.
Railroad.
Historically, four significant railroad crossings were built over the lake. As of 2016, only one remains.
Now called Colchester Park, the main three-mile (5 km) causeway has been adapted and preserved as a recreation area for cyclists, runners, and anglers. Two smaller marble rock-landfill causeways were also erected as part of this line that connected Grand Isle to North Hero and from North Hero to Alburgh.
Natural history.
In 2010, the estimate of cormorant population, now classified as a nuisance species because they take so much of the lake fish, ranged from 14,000 to 16,000. A Fish and Wildlife commissioner said that the ideal population would be 3,300 or about 3 per . Cormorants had disappeared from the lake (and all northern lakes) due to the use of DDT in the 1940s and 1950s, which made their eggs more fragile and reduced breeding populations.
Ring-billed gulls are also considered a nuisance. Measures have been taken to reduce their population. Authorities are trying to encourage the return of black crowned night herons, cattle egrets, and great blue herons, which disappeared during the time DDT was being widely used.
Infrastructure.
Lake crossings.
The Alburgh Peninsula (also known as the Alburgh Tongue), extending south from the Quebec shore of the lake into Vermont, is accessible from the rest of the state only via Canada. This is a distinction shared with Point Roberts, Washington, and the Northwest Angle in Minnesota as well as Province Point (see below). Unlike the other three cases, this is no longer of practical significance because highway bridges across the lake provide access to the peninsula from within the United States (from three directions).
Province Point, Vermont.
A few kilometres to the northeast of the town of East Alburgh, Vermont, Province Point is the southernmost tip of a small promontory approximately in size . The promontory is cut through by the US-Canadian border; as such the area is a practical exclave of the United States contiguous with Canada.
Mainland.
Two roadways cross over the lake, connecting Vermont and New York.
Since November 2011, the Lake Champlain Bridge has crossed the southern part of the lake, connecting Chimney Point in Vermont with Crown Point, New York. It replaced Champlain Bridge, which was closed in 2009 because of severe structural problems found that could have resulted in a collapse. In 2009, the bridge had been used by 3,400 drivers per day, and driving around the southern end of the lake added two hours to the trip. Ferry service was re-established to take some of the traffic burden. On December 28, 2009, the bridge was destroyed in a controlled demolition. A new bridge was rapidly constructed by a joint state commitment, opening on November 7, 2011.
To the north, US 2 runs from Rouses Point, New York to Grand Isle County, Vermont in the town of Alburgh, before continuing south along a chain of islands towards Burlington. To the east, Vermont Route 78 runs from an intersection with US 2 in Alburgh through East Alburgh to Swanton. The US 2-VT 78 route technically runs from the New York mainland to an extension of the mainland between two arms of the lake and then to the Vermont mainland, but it provides a direct route across the two main arms of the northern part of the lake.
Ferry.
North of Ticonderoga, New York, the lake widens appreciably; ferry service is operated by the Lake Champlain Transportation Company at:
While the old bridge was being demolished and the new one constructed, Lake Champlain Transportation
Company operated a free, 24-hour ferry from just south of the bridge to Chimney
Point in Vermont at the expense of the states of New York and Vermont at a cost to the states of about $10 per car.
The most southerly crossing is the Fort Ticonderoga Ferry, connecting Ticonderoga, New York with Shoreham, Vermont just north of the historic fort.
Railroad.
The Swanton, VT, to East Alburg, Vermont, rail trestle.
Waterways.
Lake Champlain has been connected to the Erie Canal via the Champlain Canal since the canal's official opening September 9, 1823, the same day as the opening of the Erie Canal from Rochester on Lake Ontario to Albany. It connects to the St. Lawrence River via the Richelieu River, with the Chambly Canal bypassing rapids on the river since 1843. Together with these waterways the lake is part of the Lakes to Locks Passage. The Lake Champlain Seaway, a project to use the lake to bring ocean-going ships from New York City to Montreal, was proposed in the late 19th century and considered as late as the 1960s, but rejected for various reasons. The lake is also part of the 740-mile Northern Forest Canoe Trail, which begins in Old Forge, NY and ends in Fort Kent, ME.
Surroundings.
Major cities.
Burlington, Vermont (pop. 42,217, 2010 Census) is the largest city on the lake. The 2nd and 3rd most populated cities/towns are Plattsburgh, New York, and Colchester, Vermont, respectively. The fourth-largest community is the city of South Burlington.
Islands.
Lake Champlain contains roughly 80 islands, three of which comprise four entire Vermont towns (most of Grand Isle County). The largest islands:
Aids to navigation.
All active navigational aids on the American portion of the lake are maintained by USCG Burlington station, along with those on international Lake Memphremagog to the east.
Aids to navigation on the Canadian portion of the lake are maintained by the Canadian Coast Guard.
Parks.
There are a number of parks in the Lake Champlain region of both Vermont and New York.
Two on the New York side of the lake include Point Au Roche State Park, which have hiking and cross country skiing trails. A public beach is located on park grounds, and the Ausable Point State Park. The Cumberland Bay State Park is located on Cumberland Head, with a campground, city beach, and sports fields.
There are various parks along the lake on the Vermont side, including Sand Bar State Park in Milton, featuring a natural sand beach, swimming,canoe and kayak rentals, food concession, picnic grounds and a play area. At , Grand Isle State Park contains camping facilities, a sand volleyball court, a nature walk trail, a horseshoe pit and a play area. Burlington's Waterfront Park is a revitalized industrial area.
Public safety.
Coast Guard Station Burlington provides "Search and Rescue, Law Enforcement and Ice Rescue services 24 hours a day, 365 days a year." Services are also provided by local, and state, and federal governments bordering on the lake, including the US Border Patrol, Royal Canadian Mounted Police, Vermont State Police, New York State Police Marine Detail, and Vermont Fish and Wildlife wardens.

</doc>
<doc id="18203" url="https://en.wikipedia.org/wiki?curid=18203" title="Lambda calculus">
Lambda calculus

Lambda calculus (also written as λ-calculus) is a formal system in mathematical logic for expressing computation based on function abstraction and application using variable binding and substitution. It was first introduced by mathematician Alonzo Church in the 1930s as part of an investigation into the foundations of mathematics. Lambda calculus is a universal model of computation equivalent to a Turing machine (Church-Turing thesis, 1937). Its namesake, Greek letter lambda (λ), is used in lambda terms (also called lambda expressions) to denote binding a variable in a function.
Lambda calculus may be "typed" and "untyped". In typed lambda calculus functions can be applied only if they are capable of accepting the given input's "type" of data.
Lambda calculus has applications in many different areas in mathematics, philosophy, linguistics, and computer science. Lambda calculus has played an important role in the development of the theory of programming languages. Functional programming languages implement the lambda calculus. Lambda calculus also is a current research topic in Category theory.
Lambda calculus in history of mathematics.
The lambda calculus was introduced by mathematician Alonzo Church in the 1930s as part of an investigation into the foundations of mathematics. The original system was shown to be logically inconsistent in 1935 when Stephen Kleene and J. B. Rosser developed the Kleene–Rosser paradox.
Subsequently, in 1936 Church isolated and published just the portion relevant to computation, what is now called the untyped lambda calculus. In 1940, he also introduced a computationally weaker, but logically consistent system, known as the simply typed lambda calculus.
Until the 1960s when its relation to programming languages was clarified, the λ-calculus was only a formalism. Thanks to Montague and other linguists' applications in the semantics of natural language, the λ-calculus has begun to enjoy a respectable place in linguistics (see Heim and Kratzer 1998) and computer science, too.
Informal description.
Motivation.
Computable functions are a fundamental concept within computer science and mathematics. The λ-calculus provides a simple semantics for computation, enabling properties of computation to be studied formally. The λ-calculus incorporates two simplifications that make this semantics simple.
The first simplification is that the λ-calculus treats functions "anonymously", without giving them explicit names. For example, the function 
can be rewritten in "anonymous form" as 
(read as "the pair of and is mapped to formula_3"). Similarly, 
can be rewritten in anonymous form as formula_5, where the input is simply mapped to itself.
The second simplification is that the λ-calculus only uses functions of a single input. An ordinary function that requires two inputs, for instance the formula_6 function, can be reworked into an equivalent function that accepts a single input, and as output returns "another" function, that in turn accepts a single input. For example, 
can be reworked into 
This method, known as currying, transforms a function that takes multiple arguments into a chain of functions each with a single argument.
Function application of the formula_9 function to the arguments (5, 2), yields at once
whereas evaluation of the curried version requires one more step
to arrive at the same result.
The lambda calculus.
The lambda calculus consists of a language of lambda terms, which is defined by a certain formal syntax, and a set of transformation rules, which allow manipulation of the lambda terms. These transformation rules can be viewed as an equational theory or as an operational definition.
As described above, all functions in the lambda calculus are anonymous functions, having no names. They only accept one input variable, with currying used to implement functions with several variables.
Lambda terms.
The syntax of the lambda calculus defines some expressions as valid lambda calculus expression and some as invalid, just as some strings of characters are valid C programs and some are not. A valid lambda calculus expression is called a "lambda term".
The following three rules give an inductive definition that can be applied to build all syntactically valid lambda terms:
Nothing else is a lambda term. Thus a lambda term is valid if and only if it can be obtained by repeated application of these three rules. However, some parentheses can be omitted according to certain rules. For example, the outermost parentheses are usually not written. See "Notation", below.
A lambda abstraction formula_24 is a definition of an anonymous function that is capable of taking a single input formula_17 and substituting it into the expression formula_18. 
It thus defines an anonymous function that takes formula_17 and returns formula_18. For example, formula_29 is a lambda abstraction for the function formula_30 using the term formula_31 for formula_18. The definition of a function with a lambda abstraction merely "sets up" the function but does not invoke it. The abstraction binds the variable formula_17 in the term formula_18.
An application formula_35 represents the application of a function formula_18 to an input formula_22, that is, it represents the act of calling function formula_18 on input formula_22 to produce formula_40.
There is no concept in lambda calculus of variable declaration. In a definition such as formula_41 (i.e. formula_42), the lambda calculus treats formula_43 as a variable that is not yet defined. The lambda abstraction formula_41 is syntactically valid, and represents a function that adds its input to the yet-unknown formula_43.
Bracketing may be used and may be needed to disambiguate terms. For example, formula_46 and formula_47 denote different terms (although they coincidentally reduce to the same value). Here the first example defines a function that defines a function and returns the result of applying x to the child-function (apply function then return), while the second example defines a function that returns a function for any input and then returns it on application of x (return function then apply).
Functions that operate on functions.
In lambda calculus, functions are taken to be 'first class values', so functions may be used as the inputs, or be returned as outputs from other functions.
For example, formula_48 represents the identity function, formula_49, and formula_50 represents the identity function applied to formula_43. Further, formula_52 represents the constant function formula_53, the function that always returns formula_43, no matter the input. In lambda calculus, function application is regarded as left-associative, so that formula_55 means formula_56.
There are several notions of "equivalence" and "reduction" that allow lambda terms to be "reduced" to "equivalent" lambda terms.
Alpha equivalence.
A basic form of equivalence, definable on lambda terms, is alpha equivalence. It captures the intuition that the particular choice of a bound variable, in a lambda abstraction, does not (usually) matter.
For instance, formula_48 and formula_58 are alpha-equivalent lambda terms, and they both represent the same function (the identity function). 
The terms formula_17 and formula_43 are not alpha-equivalent, because they are not bound in a lambda abstraction.
In many presentations, it is usual to identify alpha-equivalent lambda terms.
The following definitions are necessary in order to be able to define beta reduction.
Free variables.
The free variables of a term are those variables not bound by a lambda abstraction. The set of free variables of an expression is defined inductively:
For example, the lambda term representing the identity formula_48 has no free variables, but the function formula_70 has a single free variable, formula_43.
Capture-avoiding substitutions.
Suppose formula_18, formula_22 and formula_74 are lambda terms and formula_17 and formula_43 are variables.
The notation formula_77 indicates substitution of formula_17 for formula_74 in formula_18 in a "capture-avoiding" manner. This is defined so that:
For example, formula_92, and formula_93.
The freshness condition (requiring that formula_43 is not in the free variables of formula_74) is crucial in order to ensure that substitution does not change the meaning of functions.
For example, a substitution is made that ignores the freshness condition: formula_96. This substitution turns the constant function formula_97 into the identity formula_48 by substitution.
In general, failure to meet the freshness condition can be remedied by alpha-renaming with a suitable fresh variable.
For example, switching back to our correct notion of substitution, in formula_99 the lambda abstraction can be renamed with a fresh variable formula_100, to obtain formula_101, and the meaning of the function is preserved by substitution.
Beta reduction.
The beta reduction rule states that an application of the form formula_102 reduces to the term formula_103. The notation formula_104 is used to indicate that formula_105 beta reduces to formula_106.
For example, for every formula_22, formula_108. This demonstrates that formula_109 really is the identity.
Similarly, formula_110, which demonstrates that formula_111 is a constant function.
The lambda calculus may be seen as an idealised functional programming language, like Haskell or Standard ML.
Under this view, beta reduction corresponds to a computational step. This step can be repeated by additional beta conversions until there are no more applications left to reduce. In the untyped lambda calculus, as presented here, this reduction process may not terminate.
For instance, consider the term formula_112.
Here formula_113.
That is, the term reduces to itself in a single beta reduction, and therefore the reduction process will never terminate.
Another aspect of the untyped lambda calculus is that it does not distinguish between different kinds of data.
For instance, it may be desirable to write a function that only operates on numbers. However, in the untyped lambda calculus, there is no way to prevent a function from being applied to truth values, strings, or other non-number objects.
Formal definition.
Definition.
Lambda expressions are composed of:
The set of lambda expressions, Λ, can be defined inductively:
Instances of rule 2 are known as abstractions and instances of rule 3 are known as applications.
Notation.
To keep the notation of lambda expressions uncluttered, the following conventions are usually applied:
Free and bound variables.
The abstraction operator, λ, is said to bind its variable wherever it occurs in the body of the abstraction. Variables that fall within the scope of an abstraction are said to be "bound". All other variables are called "free". For example, in the following expression y is a bound variable and x is free: λ"y"."x" "x" "y". Also note that a variable is bound by its "nearest" abstraction. In the following example the single occurrence of x in the expression is bound by the second lambda: λ"x"."y" (λ"x"."z" "x")
The set of "free variables" of a lambda expression, M, is denoted as FV(M) and is defined by recursion on the structure of the terms, as follows:
An expression that contains no free variables is said to be "closed". Closed lambda expressions are also known as combinators and are equivalent to terms in combinatory logic.
Reduction.
The meaning of lambda expressions is defined by how expressions can be reduced.
There are three kinds of reduction:
We also speak of the resulting equivalences: two expressions are "β-equivalent", if they can be β-converted into the same expression, and α/η-equivalence are defined similarly.
The term "redex", short for "reducible expression", refers to subterms that can be reduced by one of the reduction rules. For example, (λ"x".M) N is a beta-redex in expressing the substitution of N for x in M; if "x" is not free in M, λ"x".M "x" is an eta-redex. The expression to which a redex reduces is called its reduct; using the previous example, the reducts of these expressions are respectively M["x":=N] and M.
α-conversion.
Alpha-conversion, sometimes known as alpha-renaming, allows bound variable names to be changed. For example, alpha-conversion of λ"x"."x" might yield λ"y"."y". Terms that differ only by alpha-conversion are called "α-equivalent". Frequently, in uses of lambda calculus, α-equivalent terms are considered to be equivalent.
The precise rules for alpha-conversion are not completely trivial. First, when alpha-converting an abstraction, the only variable occurrences that are renamed are those that are bound to the same abstraction. For example, an alpha-conversion of λ"x".λ"x"."x" could result in λ"y".λ"x"."x", but it could "not" result in λ"y".λ"x"."y". The latter has a different meaning from the original.
Second, alpha-conversion is not possible if it would result in a variable getting captured by a different abstraction. For example, if we replace "x" with "y" in λ"x".λ"y"."x", we get λ"y".λ"y"."y", which is not at all the same.
In programming languages with static scope, alpha-conversion can be used to make name resolution simpler by ensuring that no variable name masks a name in a containing scope (see alpha renaming to make name resolution trivial).
In the De Bruijn index notation, any two alpha-equivalent terms are literally identical.
Substitution.
Substitution, written , is the process of replacing all free occurrences of the variable in the expression with expression .
Substitution on terms of the λ-calculus is defined by recursion on the structure of terms, as follows (note: x and y are only variables while M and N are any λ expression).
To substitute into a lambda abstraction, it is sometimes necessary to α-convert the expression. For example, it is not correct for to result in , because the substituted was supposed to be free but ended up being bound. The correct substitution in this case is , up to α-equivalence. Notice that substitution is defined uniquely up to α-equivalence.
β-reduction.
Beta-reduction captures the idea of function application. Beta-reduction is defined in terms of substitution: the beta-reduction of  ((λ"V"."E") "E′")  is "E"["V" := "E′"].
For example, assuming some encoding of 2, 7, ×, we have the following β-reduction: ((λ"n"."n"×2) 7) → 7×2.
η-conversion.
Eta-conversion expresses the idea of extensionality, which in this context is that two functions are the same if and only if they give the same result for all arguments. Eta-conversion converts between λ"x".("f" "x") and "f" whenever "x" does not appear free in "f".
Normal forms and confluence.
For the untyped lambda calculus, β-reduction as a rewriting rule is neither strongly normalising nor weakly normalising.
However, it can be shown that β-reduction is confluent. (Of course, we are working up to α-conversion, i.e. we consider two normal forms to be equal, if it is possible to α-convert one into the other.)
Therefore, both strongly normalising terms and weakly normalising terms have a unique normal form. For strongly normalising terms, any reduction strategy is guaranteed to yield the normal form, whereas for weakly normalising terms, some reduction strategies may fail to find it.
Encoding datatypes.
The basic lambda calculus may be used to model booleans, arithmetic, data structures and recursion, as illustrated in the following sub-sections.
Arithmetic in lambda calculus.
There are several possible ways to define the natural numbers in lambda calculus, but by far the most common are the Church numerals, which can be defined as follows:
and so on. Or using the alternative syntax presented above in "Notation":
A Church numeral is a higher-order function—it takes a single-argument function "f", and returns another single-argument function. The Church numeral "n" is a function that takes a function "f" as argument and returns the "n"-th composition of "f", i.e. the function "f" composed with itself "n" times. This is denoted "f"("n") and is in fact the "n"-th power of "f" (considered as an operator); "f"(0) is defined to be the identity function. Such repeated compositions (of a single function "f") obey the laws of exponents, which is why these numerals can be used for arithmetic. (In Church's original lambda calculus, the formal parameter of a lambda expression was required to occur at least once in the function body, which made the above definition of 0 impossible.)
We can define a successor function, which takes a number "n" and returns "n" + 1 by adding another application of "f",where '(mf)x' means the function 'f' is applied 'm' times on 'x':
Because the "m"-th composition of "f" composed with the "n"-th composition of "f" gives the "m"+"n"-th composition of "f", addition can be defined as follows:
PLUS can be thought of as a function taking two natural numbers as arguments and returning a natural number; it can be verified that
and
are β-equivalent lambda expressions. Since adding "m" to a number "n" can be accomplished by adding 1 "m" times, an equivalent definition is:
Similarly, multiplication can be defined as
Alternatively
since multiplying "m" and "n" is the same as repeating the add "n" function "m" times and then applying it to zero.
Exponentiation has a rather simple rendering in Church numerals, namely
The predecessor function defined by PRED "n" = "n" − 1 for a positive integer "n" and PRED 0 = 0 is considerably more difficult. The formula
can be validated by showing inductively that if "T" denotes (λ"g".λ"h"."h" ("g" "f")), then T("n")(λ"u"."x") = (λ"h"."h"("f"("n"−1)("x"))) for "n" > 0. Two other definitions of PRED are given below, one using conditionals and the other using pairs. With the predecessor function, subtraction is straightforward. Defining
SUB "m" "n" yields "m" − "n" when "m" > "n" and 0 otherwise.
Logic and predicates.
By convention, the following two definitions (known as Church booleans) are used for the boolean values TRUE and FALSE:
Then, with these two λ-terms, we can define some logic operators (these are just possible formulations; other expressions are equally correct):
We are now able to compute some logic functions, for example:
and we see that AND TRUE FALSE is equivalent to FALSE.
A "predicate" is a function that returns a boolean value. The most fundamental predicate is ISZERO, which returns TRUE if its argument is the Church numeral 0, and FALSE if its argument is any other Church numeral:
The following predicate tests whether the first argument is less-than-or-equal-to the second:
and since "m" = "n", if LEQ "m" "n" and LEQ "n" "m", it is straightforward to build a predicate for numerical equality.
The availability of predicates and the above definition of TRUE and FALSE make it convenient to write "if-then-else" expressions in lambda calculus. For example, the predecessor function can be defined as:
which can be verified by showing inductively that "n" (λ"g".λ"k".ISZERO ("g" 1) "k" (PLUS ("g" "k") 1)) (λ"v".0) is the add "n" − 1 function for "n" > 0.
Pairs.
A pair (2-tuple) can be defined in terms of TRUE and FALSE, by using the Church encoding for pairs. For example, PAIR encapsulates the pair ("x","y"), FIRST returns the first element of the pair, and SECOND returns the second.
A linked list can be defined as either NIL for the empty list, or the PAIR of an element and a smaller list. The predicate NULL tests for the value NIL. (Alternatively, with NIL := FALSE, the construct "l" (λ"h".λ"t".λ"z".deal_with_head_"h"_and_tail_"t") (deal_with_nil) obviates the need for an explicit NULL test).
As an example of the use of pairs, the shift-and-increment function that maps ("m", "n") to ("n", "n" + 1) can be defined as
which allows us to give perhaps the most transparent version of the predecessor function:
Recursion and fixed points.
Recursion is the definition of a function using the function itself; on the face of it, lambda calculus does not allow this (we can't refer to a value which is yet to be defined, inside the lambda term defining that same value, as all functions are anonymous in lambda calculus). However, this impression is misleading: in  (λ"x"."x" "x") "E"  both "x" 's refer to the same lambda term, "E", so it is possible for a lambda expression – here "E" – to be arranged to receive itself as its argument value, through self-application.
Consider for instance the factorial function F("n") recursively defined by
In the lambda expression which is to represent this function, a "parameter" (typically the first one) will be assumed to receive the lambda expression itself as its value, so that calling it – applying it to an argument – will amount to recursion. Thus to achieve recursion, the intended-as-self-referencing argument (called "r" here) must always be passed to itself within the function body, at a call point:
The self-application achieves replication here, passing the function's lambda expression on to the next invocation as an argument value, making it available to be referenced and called there.
This solves it but requires re-writing each recursive call as self-application. We would like to have a generic solution, without a need for any re-writes:
Given a lambda term with first argument representing recursive call (e.g. G here), the "fixed-point" combinator FIX will return a self-replicating lambda expression representing the recursive function (here, F). The function does not need to be explicitly passed to itself at any point, for the self-replication is arranged in advance, when it is created, to be done each time it is called. Thus the original lambda expression (FIX G) is re-created inside itself, at call-point, achieving self-reference.
In fact, there are many possible definitions for this FIX operator, the simplest of them being:
In the lambda calculus, Y "g"  is a fixed-point of "g", as it expands to:
Now, to perform our recursive call to the factorial function, we would simply call (Y G) "n",  where "n" is the number we are calculating the factorial of. Given "n" = 4, for example, this gives:
Every recursively defined function can be seen as a fixed point of some suitably defined function closing over the recursive call with an extra argument, and therefore, using Y, every recursively defined function can be expressed as a lambda expression. In particular, we can now cleanly define the subtraction, multiplication and comparison predicate of natural numbers recursively.
Standard terms.
Certain terms have commonly accepted names:
Typed lambda calculus.
A typed lambda calculus is a typed formalism that uses the lambda-symbol (formula_114) to denote anonymous function abstraction. In this context, types are usually objects of a syntactic nature that are assigned to lambda terms; the exact nature of a type depends on the calculus considered (see kinds below). From a certain point of view, typed lambda calculi can be seen as refinements of the untyped lambda calculus but from another point of view, they can also be considered the more fundamental theory and "untyped lambda calculus" a special case with only one type.
Typed lambda calculi are foundational programming languages and are the base of typed functional programming languages such as ML and Haskell and, more indirectly, typed imperative programming languages. Typed lambda calculi play an important role in the design of type systems for programming languages; here typability usually captures desirable properties of the program, e.g. the program will not cause a memory access violation.
Typed lambda calculi are closely related to mathematical logic and proof theory via the Curry–Howard isomorphism and they can be considered as the internal language of classes of categories, e.g. the simply typed lambda calculus is the language of Cartesian closed categories (CCCs).
Computable functions and lambda calculus.
A function "F": N → N of natural numbers is a computable function if and only if there exists a lambda expression "f" such that for every pair of "x", "y" in N, "F"("x")="y" if and only if "f" "x" =β "y",  where "x" and "y" are the Church numerals corresponding to "x" and "y", respectively and =β meaning equivalence with beta reduction. This is one of the many ways to define computability; see the Church-Turing thesis for a discussion of other approaches and their equivalence.
Undecidability of equivalence.
There is no algorithm that takes as input two lambda expressions and outputs TRUE or FALSE depending on whether or not the two expressions are equivalent. This was historically the first problem for which undecidability could be proven. As is common for a proof of undecidability, the proof shows that no computable function can decide the equivalence. Church's thesis is then invoked to show that no algorithm can do so.
Church's proof first reduces the problem to determining whether a given lambda expression has a "normal form". A normal form is an equivalent expression that cannot be reduced any further under the rules imposed by the form. Then he assumes that this predicate is computable, and can hence be expressed in lambda calculus. Building on earlier work by Kleene and constructing a Gödel numbering for lambda expressions, he constructs a lambda expression "e" that closely follows the proof of Gödel's first incompleteness theorem. If "e" is applied to its own Gödel number, a contradiction results.
Lambda calculus and programming languages.
As pointed out by Peter Landin's 1965 paper A Correspondence between ALGOL 60 and Church's Lambda-notation, sequential procedural programming languages can be understood in terms of the lambda calculus, which provides the basic mechanisms for procedural abstraction and procedure (subprogram) application.
Lambda calculus reifies "functions" and makes them first-class objects, which raises implementation complexity when it is implemented.
Anonymous functions.
For example, in Lisp the 'square' function can be expressed as a lambda expression as follows:
The above example is an expression that evaluates to a first-class function. The symbol codice_1 creates an anonymous function, given a list of parameter names, codice_2 — just a single argument in this case, and an expression that is evaluated as the body of the function, codice_3. The Haskell example is identical. Anonymous functions are sometimes called lambda expressions.
For example, Pascal and many other imperative languages have long supported passing subprograms as arguments to other subprograms through the mechanism of function pointers. However, function pointers are not a sufficient condition for functions to be first class datatypes, because a function is a first class datatype if and only if new instances of the function can be created at run-time. And this run-time creation of functions is supported in Smalltalk, Javascript, and more recently in Scala, Eiffel ("agents"), C# ("delegates") and C++11, among others.
Reduction strategies.
Whether a term is normalising or not, and how much work needs to be done in normalising it if it is, depends to a large extent on the reduction strategy used. The distinction between reduction strategies relates to the distinction in functional programming languages between eager evaluation and lazy evaluation.
Applicative order is not a normalising strategy. The usual counterexample is as follows: define Ω = ωω where ω = λ"x"."xx". This entire expression contains only one redex, namely the whole expression; its reduct is again Ω. Since this is the only available reduction, Ω has no normal form (under any evaluation strategy). Using applicative order, the expression KIΩ = (λ"x".λ"y"."x") (λ"x"."x")Ω is reduced by first reducing Ω to normal form (since it is the rightmost redex), but since Ω has no normal form, applicative order fails to find a normal form for KIΩ.
In contrast, normal order is so called because it always finds a normalising reduction, if one exists. In the above example, KIΩ reduces under normal order to "I", a normal form. A drawback is that redexes in the arguments may be copied, resulting in duplicated computation (for example, (λ"x"."xx") ((λ"x"."x")"y") reduces to ((λ"x"."x")"y") ((λ"x"."x")"y") using this strategy; now there are two redexes, so full evaluation needs two more steps, but if the argument had been reduced first, there would now be none).
The positive tradeoff of using applicative order is that it does not cause unnecessary computation, if all arguments are used, because it never substitutes arguments containing redexes and hence never needs to copy them (which would duplicate work). In the above example, in applicative order (λ"x"."xx") ((λ"x"."x")"y") reduces first to (λ"x"."xx")"y" and then to the normal order "yy", taking two steps instead of three.
Most "purely" functional programming languages (notably Miranda and its descendents, including Haskell), and the proof languages of theorem provers, use "lazy evaluation", which is essentially the same as call by need. This is like normal order reduction, but call by need manages to avoid the duplication of work inherent in normal order reduction using "sharing". In the example given above, (λ"x"."xx") ((λ"x"."x")"y") reduces to ((λ"x"."x")"y") ((λ"x"."x")"y"), which has two redexes, but in call by need they are represented using the same object rather than copied, so when one is reduced the other is too.
A note about complexity.
While the idea of beta reduction seems simple enough, it is not an atomic step, in that it must have a non-trivial cost when estimating computational complexity. To be precise, one must somehow find the location of all of the occurrences of the bound variable "V" in the expression "E", implying a time cost, or one must keep track of these locations in some way, implying a space cost. A naïve search for the locations of "V" in "E" is "O"("n") in the length "n" of "E". This has led to the study of systems that use explicit substitution. Sinot's director strings offer a way of tracking the locations of free variables in expressions.
Parallelism and concurrency.
The Church–Rosser property of the lambda calculus means that evaluation (β-reduction) can be carried out in "any order", even in parallel. This means that various nondeterministic evaluation strategies are relevant. However, the lambda calculus does not offer any explicit constructs for parallelism. One can add constructs such as Futures to the lambda calculus. Other process calculi have been developed for describing communication and concurrency.
Semantics.
The fact that lambda calculus terms act as functions on other lambda calculus terms, and even on themselves, led to questions about the semantics of the lambda calculus. Could a sensible meaning be assigned to lambda calculus terms? The natural semantics was to find a set "D" isomorphic to the function space "D" → "D", of functions on itself. However, no nontrivial such "D" can exist, by cardinality constraints because the set of all functions from "D" to "D" has greater cardinality than "D", unless "D" is a singleton set.
In the 1970s, Dana Scott showed that, if only continuous functions were considered, a set or domain "D" with the required property could be found, thus providing a model for the lambda calculus.
This work also formed the basis for the denotational semantics of programming languages.
Further reading.
Monographs/textbooks for graduate students:
"Some parts of this article are based on material from FOLDOC, used with ."

</doc>
<doc id="18208" url="https://en.wikipedia.org/wiki?curid=18208" title="Lossy compression">
Lossy compression

In information technology, lossy compression or irreversible compression is the class of data encoding methods that uses inexact approximations and partial data discarding to represent the content. These techniques are used to reduce data size for storage, handling, and transmitting content. Different versions of the photo of the cat at the right show how higher degrees of approximation create coarser images as more details are removed. This is opposed to lossless data compression (reversible data compression) which does not degrade the image. The amount of data reduction possible using lossy compression is often much higher than through lossless techniques.
Well-designed lossy compression technology often reduces file sizes significantly before degradation is noticed by the end-user. Even when noticeable by the user, further data reduction may be desirable (e.g., for real-time communication, to reduce transmission times, or to reduce storage needs).
Lossy compression is most commonly used to compress multimedia data (audio, video, and images), especially in applications such as streaming media and internet telephony. By contrast, lossless compression is typically required for text and data files, such as bank records and text articles. In many cases it is advantageous to make a master lossless file which is to be used to produce new compressed files; for example, a multi-megabyte file can be used at full size to produce a full-page advertisement in a glossy magazine, and a 10 kilobyte lossy copy can be made for a small image on a web page.
Lossy and lossless compression.
It is possible to compress many types of digital data in a way that reduces the size of a computer file needed to store it, or the bandwidth needed to transmit it, with no loss of the full information contained in the original file. A picture, for example, is converted to a digital file by considering it to be an array of dots and specifying the color and brightness of each dot. If the picture contains an area of the same color, it can be compressed without loss by saying "200 red dots" instead of "red dot, red dot, ...(197 more times)..., red dot."
The original data contains a certain amount of information, and there is a lower limit to the size of file that can carry all the information. Basic information theory says that there is an absolute limit in reducing the size of this data. When data is compressed, its entropy increases, and it cannot increase indefinitely. As an intuitive example, most people know that a compressed ZIP file is smaller than the original file, but repeatedly compressing the same file will not reduce the size to nothing. Most compression algorithms can recognize when further compression would be pointless and would in fact increase the size of the data.
In many cases, files or data streams contain more information than is needed for a particular purpose. For example, a picture may have more detail than the eye can distinguish when reproduced at the largest size intended; likewise, an audio file does not need a lot of fine detail during a very loud passage. Developing lossy compression techniques as closely matched to human perception as possible is a complex task. Sometimes the ideal is a file that provides exactly the same perception as the original, with as much digital information as possible removed; other times, perceptible loss of quality is considered a valid trade-off for the reduced data.
The terms 'irreversible' and 'reversible' are preferred over 'lossy' and 'lossless' respectively for some applications, such as medical image compression, to circumvent the negative implications of 'loss'.
The type and amount of loss can affect the utility of the images. Artifacts of compression may be clearly discernible yet the result still useful for the intended purpose. Or lossy compressed images may be 'visually lossless', or in the case of medical images, so-called Diagnostically Acceptable Irreversible Compression (DAIC) may have been applied.
Transform coding.
More generally, some forms of lossy compression can be thought of as an application of "transform coding" – in the case of multimedia data, "perceptual coding:" it transforms the raw data to a domain that more accurately reflects the information content. For example, rather than expressing a sound file as the amplitude levels over time, one may express it as the frequency spectrum over time, which corresponds more accurately to human audio perception.
While data reduction (compression, be it lossy or lossless) is a main goal of transform coding, it also allows other goals: one may represent data more accurately for the original amount of space – for example, in principle, if one starts with an analog or high-resolution digital master, an MP3 file of a given size should provide a better representation than a raw uncompressed audio in WAV or AIFF file of the same size. This is because uncompressed audio can only reduce file size by lowering bit rate or depth, whereas compressing audio can reduce size while maintaining bit rate and depth. This compression becomes a selective loss of the least significant data, rather than losing data across the board. Further, a transform coding may provide a better domain for manipulating or otherwise editing the data – for example, equalization of audio is most naturally expressed in the frequency domain (boost the bass, for instance) rather than in the raw time domain.
From this point of view, perceptual encoding is not essentially about "discarding" data, but rather about a "better representation" of data.
Another use is for backward compatibility and graceful degradation: in color television, encoding color via a luminance-chrominance transform domain (such as YUV) means that black-and-white sets display the luminance, while ignoring the color information.
Another example is chroma subsampling: the use of color spaces such as YIQ, used in NTSC, allow one to reduce the resolution on the components to accord with human perception – humans have highest resolution for black-and-white (luma), lower resolution for mid-spectrum colors like yellow and green, and lowest for red and blues – thus NTSC displays approximately 350 pixels of luma per scanline, 150 pixels of yellow vs. green, and 50 pixels of blue vs. red, which are proportional to human sensitivity to each component.
Information loss.
Lossy compression formats suffer from generation loss: repeatedly compressing and decompressing the file will cause it to progressively lose quality.
This is in contrast with lossless data compression, where data will not be lost via the use of such a procedure.
Information-theoretical foundations for lossy data compression are provided by rate-distortion theory. Much like the use of probability in optimal coding theory, rate-distortion theory heavily draws on Bayesian estimation and decision theory in order to model perceptual distortion and even aesthetic judgment.
There are two basic lossy compression schemes:
In some systems the two techniques are combined, with transform codecs being used to compress the error signals generated by the predictive stage.
Lossy versus lossless.
The advantage of lossy methods over lossless methods is that in some cases a lossy method can produce a much smaller compressed file than any lossless method, while still meeting the requirements of the application.
Lossy methods are most often used for compressing sound, images or videos. This is because these types of data are intended for human interpretation where the mind can easily "fill in the blanks" or see past very minor errors or inconsistencies – ideally lossy compression is transparent (imperceptible), which can be verified via an ABX test.
Transparency.
When a user acquires a lossily compressed file, (for example, to reduce download time) the retrieved file can be quite different from the original at the bit level while being indistinguishable to the human ear or eye for most practical purposes. Many compression methods focus on the idiosyncrasies of human physiology, taking into account, for instance, that the human eye can see only certain wavelengths of light. The psychoacoustic model describes how sound can be highly compressed without degrading perceived quality. Flaws caused by lossy compression that are noticeable to the human eye or ear are known as compression artifacts.
Compression ratio.
The compression ratio (that is, the size of the compressed file compared to that of the uncompressed file) of lossy video codecs is nearly always far superior to that of the audio and still-image equivalents.
Transcoding and editing.
An important caveat about lossy compression (formally transcoding), is that editing lossily compressed files causes digital generation loss from the re-encoding. This can be avoided by only producing lossy files from (lossless) originals and only editing (copies of) original files, such as images in raw image format instead of JPEG.
If data which has been compressed lossily is decoded and compressed losslessly, the size of the result can be comparable with the size of the data before lossy compression, but the data already lost cannot be recovered.
When deciding to use lossy conversion without keeping the original, one should remember that format conversion may be needed in the future to achieve compatibility with software or devices (format shifting), or to avoid paying patent royalties for decoding or distribution of compressed files.
Editing of lossy files.
By modifying the compressed data directly without decoding and re-encoding, some editing of lossily compressed files without degradation of quality is possible. Editing which reduces the file size as if it had been compressed to a greater degree, but without more loss than this, is sometimes also possible.
JPEG.
The primary programs for lossless editing of JPEGs are codice_1, and the derived codice_2 (which also preserves Exif information), and Jpegcrop (which provides a Windows interface).
These allow the image to be
While unwanted information is destroyed, the quality of the remaining portion is unchanged.
Some other transforms are possible to some extent, such as joining images with the same encoding (composing side by side, as on a grid) or pasting images (such as logos) onto existing images (both via Jpegjoin), or scaling.
Some changes can be made to the compression without re-encoding:
The freeware Windows-only IrfanView has some lossless JPEG operations in its codice_3 plugin.
Metadata.
Metadata, such as ID3 tags, Vorbis comments, or Exif information, can usually be modified or removed without modifying the underlying data.
Downsampling/compressed representation scalability.
One may wish to downsample or otherwise decrease the resolution of the represented source signal and the quantity of data used for its compressed representation without re-encoding, as in bitrate peeling, but this functionality is not supported in all designs, as not all codecs encode data in a form that allows less important detail to simply be dropped.
Some well known designs that have this capability include JPEG 2000 for still images and H.264/MPEG-4 AVC based Scalable Video Coding for video. Such schemes have also been standardized for older designs as well, such as JPEG images with progressive encoding, and MPEG-2 and MPEG-4 Part 2 video, although those prior schemes had limited success in terms of adoption into real-world common usage.
Without this capacity, which is often the case in practice, to produce a representation with lower resolution or lower fidelity than a given one, one needs to start with the original source signal and encode, or start with a compressed representation and then decompress and re-encode it (transcoding), though the latter tends to cause digital generation loss.
Another approach is to encode the original signal at several different bitrates, and their either choose which to use (as when streaming over the internet – as in RealNetworks' "SureStream" – or offering varying downloads, as at Apple's iTunes Store), or broadcast several, where the best that is successfully received is used, as in various implementations of hierarchical modulation. Similar techniques are used in mipmaps, pyramid representations, and more sophisticated scale space methods.
Some audio formats feature a combination of a lossy format and a lossless correction which when combined reproduce the original signal; the correction can be stripped, leaving a smaller, lossily compressed, file. Such formats include MPEG-4 SLS (Scalable to Lossless), WavPack, OptimFROG DualStream, and DTS-HD Master Audio in lossless (XLL) mode
Methods.
Other data.
Researchers have (semi-seriously) performed lossy compression on text by either using a thesaurus to substitute short words for long ones, or generative text techniques, although these sometimes fall into the related category of lossy data conversion.
Lowering resolution.
A general kind of lossy compression is to lower the resolution of an image, as in image scaling, particularly decimation.
One may also remove less "lower information" parts of an image, such as by seam carving.
Many media transforms, such as Gaussian blur, are, like lossy compression, irreversible: the original signal cannot be reconstructed from the transformed signal. However, in general these will have the same size as the original, and are not a form of compression.
Lowering resolution has practical uses, as the NASA New Horizons craft will transmit thumbnails of its encounter with Pluto-Charon before it sends the higher resolution images.
Another solution for slow connections is the usage of Image interlacing which progressively defines the image. Thus a partial transmission is enough to preview the final image, in a lower resolution version, without creating a scaled and a full version too.

</doc>
