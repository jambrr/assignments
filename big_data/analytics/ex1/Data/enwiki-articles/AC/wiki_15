<doc id="13658" url="https://en.wikipedia.org/wiki?curid=13658" title="House of Lords">
House of Lords

The House of Lords is the upper house of the Parliament of the United Kingdom. Like the House of Commons, it meets in the Palace of Westminster.
Unlike the elected House of Commons, most members of the House of Lords are appointed. The membership of the House of Lords is made up of Lords Spiritual and Lords Temporal. The Lords Spiritual are 26 bishops in the established Church of England. Of the Lords Temporal, the majority are life peers who are appointed by the monarch on the advice of the Prime Minister, or on the advice of the House of Lords Appointments Commission. However, they also include some hereditary peers including four dukes. Membership was once an entitlement of all hereditary peers, other than those in the peerage of Ireland, but under the House of Lords Act 1999, the right to membership was restricted to 92 hereditary peers. Very few of these are female since most hereditary peerages can only be inherited by men.
While the House of Commons has a defined 650-seat membership, the number of members in the House of Lords is not fixed. There are currently sitting Lords. The House of Lords is the only upper house of any bicameral parliament to be larger than its respective lower house.
The House of Lords scrutinises bills that have been approved by the House of Commons. It regularly reviews and amends Bills from the Commons. While it is unable to prevent Bills passing into law, except in certain limited circumstances, it can delay Bills and force the Commons to reconsider their decisions. In this capacity, the House of Lords acts as a check on the House of Commons that is independent from the electoral process. Bills can be introduced into either the House of Lords or the House of Commons. Members of the Lords may also take on roles as government ministers. The House of Lords has its own support services, separate from the Commons, including the House of Lords Library.
The Queen's Speech is delivered in the House of Lords during the State Opening of Parliament. In addition to its role as the upper house, until the establishment of the Supreme Court in 2009, the House of Lords, through the Law Lords, acted as the final court of appeal in the British judicial system. The House also has a Church of England role, in that Church Measures must be tabled within the House by the Lords Spiritual.
History.
Today's Parliament of the United Kingdom largely descends, in practice, from the Parliament of England, though the Treaty of Union of 1706 and the Acts of Union that ratified the Treaty in 1707 created a new Parliament of Great Britain to replace the Parliament of England and the Parliament of Scotland. This new parliament was, in effect, the continuation of the Parliament of England with the addition of 45 MPs and 16 Peers to represent Scotland.
The Parliament of England developed from the "Magnum Concilium", the "Great Council" that advised the King during medieval times. This royal council came to be composed of ecclesiastics, noblemen, and representatives of the counties of England (afterwards, representatives of the boroughs as well). The first English Parliament is often considered to be the "Model Parliament" (held in 1295), which included archbishops, bishops, abbots, earls, barons, and representatives of the shires and boroughs of it.
The power of Parliament grew slowly, fluctuating as the strength of the monarchy grew or declined. For example, during much of the reign of Edward II (1307–1327), the nobility was supreme, the Crown weak, and the shire and borough representatives entirely powerless. In 1569, the authority of Parliament was for the first time recognised not simply by custom or royal charter, but by an authoritative statute, passed by Parliament itself.
Further developments occurred during the reign of Edward II's successor, Edward III. It was during this King's reign that Parliament clearly separated into two distinct chambers: the House of Commons (consisting of the shire and borough representatives) and the House of Lords (consisting of the bishops and abbots and the peers). The authority of Parliament continued to grow, and, during the early fifteenth century, both Houses exercised powers to an extent not seen before. The Lords were far more powerful than the Commons because of the great influence of the great landowners and the prelates of the realm.
The power of the nobility suffered a decline during the civil wars of the late fifteenth century, known as the Wars of the Roses. Much of the nobility was killed on the battlefield or executed for participation in the war, and many aristocratic estates were lost to the Crown. Moreover, feudalism was dying, and the feudal armies controlled by the barons became obsolete. Henry VII (1485–1509) clearly established the supremacy of the monarch, symbolised by the "Crown Imperial". The domination of the Sovereign continued to grow during the reigns of the Tudor monarchs in the 16th century. The Crown was at the height of its power during the reign of Henry VIII (1509–1547).
The House of Lords remained more powerful than the House of Commons, but the Lower House continued to grow in influence, reaching a zenith in relation to the House of Lords during the middle 17th century. Conflicts between the King and the Parliament (for the most part, the House of Commons) ultimately led to the English Civil War during the 1640s. In 1649, after the defeat and execution of King Charles I, the Commonwealth of England was declared, but the nation was effectively under the overall control of Oliver Cromwell, Lord Protector of England.
The House of Lords was reduced to a largely powerless body, with Cromwell and his supporters in the Commons dominating the Government. On 19 March 1649, the House of Lords was abolished by an Act of Parliament, which declared that "The Commons of England by too long experience that the House of Lords is useless and dangerous to the people of England." The House of Lords did not assemble again until the Convention Parliament met in 1660 and the monarchy was restored. It returned to its former position as the more powerful chamber of Parliament—a position it would occupy until the 19th century.
19th century.
The 19th century was marked by several changes to the House of Lords. The House, once a body of only about 50 members, had been greatly enlarged by the liberality of George III and his successors in creating peerages. The individual influence of a Lord of Parliament was thus diminished.
Moreover, the power of the House as a whole experienced a decrease, whilst that of the House of Commons grew. Particularly notable in the development of the Lower House's superiority was the Reform Bill of 1832. The electoral system of the House of Commons was not, at the time, democratic: property qualifications greatly restricted the size of the electorate, and the boundaries of many constituencies had not been changed for centuries.
Entire cities such as Manchester were not represented by a single individual in the House of Commons, but the 11 voters of Old Sarum retained their ancient right to elect two members of parliament. A small borough was susceptible to bribery, and was often under the control of a patron, whose nominee was guaranteed to win an election. Some aristocrats were patrons of numerous "pocket boroughs", and therefore controlled a considerable part of the membership of the House of Commons.
When the House of Commons passed a Reform Bill to correct some of these anomalies in 1831, the House of Lords rejected the proposal. The popular cause of reform, however, was not abandoned by the ministry, despite a second rejection of the bill in 1832. Prime Minister Earl Grey advised the King to overwhelm opposition to the bill in the House of Lords by creating about 80 new pro-Reform peers. William IV originally balked at the proposal, which effectively threatened the opposition of the House of Lords, but at length relented.
Before the new peers were created, however, the Lords who opposed the bill admitted defeat, and abstained from the vote, allowing the passage of the bill. The crisis damaged the political influence of the House of Lords, but did not altogether end it. A vital reform was effected by the House itself in 1868, when it changed its standing orders so as to prevent noble Lords from voting without taking the trouble to attend. Proxies were then abolished. Over the course of the century the power of the Upper House experienced further erosion, and the Commons gradually became the stronger House of Parliament.
20th century.
The status of the House of Lords returned to the forefront of debate after the election of a Liberal Government in 1906. In 1909, the Chancellor of the Exchequer, David Lloyd George, introduced into the House of Commons the "People's Budget", which proposed a land tax targeting wealthy landowners. The popular measure, however, was defeated in the heavily Conservative House of Lords.
Having made the powers of the House of Lords a primary campaign issue, the Liberals were narrowly re-elected in January 1910. Prime Minister H. H. Asquith then proposed that the powers of the House of Lords be severely curtailed. After a further general election in December 1910, and with an undertaking by King George V to create sufficient new Liberal peers to overcome Lords' opposition to the measure if necessary, the Asquith Government secured the passage of a bill to curtail the powers of the House of Lords.
The Parliament Act 1911 effectively abolished the power of the House of Lords to reject legislation, or to amend in a way unacceptable to the House of Commons: most bills could be delayed for no more than three parliamentary sessions or two calendar years. It was not meant to be a permanent solution; more comprehensive reforms were planned. Neither party, however, pursued the matter with much enthusiasm, and the House of Lords remained primarily hereditary. In 1949, the Parliament Act reduced the delaying power of the House of Lords further to two sessions or one year.
In 1958, the predominantly hereditary nature of the House of Lords was changed by the Life Peerages Act 1958, which authorised the creation of life baronies, with no numerical limits. The number of Life Peers then gradually increased, though not at a constant rate.
The Labour Party had for most of the twentieth century a commitment, based on the party's historic opposition to class privilege, to abolish the House of Lords, or at least expel the hereditary element. In 1968, the Labour Government of Harold Wilson attempted to reform the House of Lords by introducing a system under which hereditary peers would be allowed to remain in the House and take part in debate, but would be unable to vote. This plan, however, was defeated in the House of Commons by a coalition of traditionalist Conservatives (such as Enoch Powell), and Labour members who continued to advocate the outright abolition of the Upper House (such as Michael Foot).
When Michael Foot attained the leadership of the Labour Party in 1980, abolition of the House of Lords became a part of the party's agenda; under his successor, Neil Kinnock, however, a reformed Upper House was proposed instead. In the meantime, the creation of hereditary peerages (except for members of the Royal Family) has been arrested, with the exception of three creations during the administration of the Conservative Margaret Thatcher in the 1980s.
Whilst some hereditary peers were at best apathetic the Labour Party's clear commitments were not lost on Baron Sudeley, who for decades was considered an expert on the House of Lords. In December 1979 the Conservative Monday Club published his extensive paper entitled "Lords Reform – Why tamper with the House of Lords?" and in July 1980 "The Monarchist" carried another article by Lord Sudeley entitled "Why Reform or Abolish the House of Lords?". In 1990 he wrote a further booklet for the Monday Club entitled "The Preservation of the House of Lords".
Lords reform.
1997–2010.
The Labour Party included in its 1997 general election Manifesto a commitment to remove the hereditary peerage from the House of Lords. Their subsequent election victory in 1997 under Tony Blair finally heralded the demise of the traditional House of Lords. The Labour Government introduced legislation to expel all hereditary peers from the Upper House as a first step in Lords reform. As a part of a compromise, however, it agreed to permit 92 hereditary peers to remain until the reforms were complete. Thus all but 92 hereditary peers were expelled under the House of Lords Act 1999 (see below for its provisions), making the House of Lords predominantly an appointed house.
Since 1999 however, no further reform has taken place. The Wakeham Commission proposed introducing a 20% elected element to the Lords, but this plan was widely criticised. A Joint Committee was established in 2001 to resolve the issue, but it reached no conclusion and instead gave Parliament seven options to choose from (fully appointed, 20% elected, 40% elected, 50% elected, 60% elected, 80%, and fully elected). In a confusing series of votes in February 2003, all of these options were defeated although the 80% elected option fell by just three votes in the Commons. Socialist MPs favouring outright abolition voted against all the options.
In 2005 a cross-party group of senior MPs (Kenneth Clarke, Paul Tyler, Tony Wright, Sir George Young and Robin Cook) published a report proposing that 70% of members of the House of Lords should be elected – each member for a single long term – by the single transferable vote system. Most of the remainder were to be appointed by a Commission to ensure a mix of "skills, knowledge and experience". This proposal was also not implemented. A cross-party campaign initiative called "Elect the Lords" was set up to make the case for a predominantly elected Second Chamber in the run up to the 2005 general election.
At the 2005 election, the Labour Party proposed further reform of the Lords, but without specific details. The Conservative Party, which had, prior to 1997, opposed any tampering with the House of Lords, favoured an 80% elected Second Chamber, while the Liberal Democrats called for a fully elected Senate. During 2006, a cross-party committee discussed Lords reform, with the aim of reaching a consensus: its findings were published in early 2007.
On 7 March 2007, members of the House of Commons voted ten times on a variety of alternative compositions for the upper chamber. Outright abolition, a wholly appointed house, a 20% elected house, a 40% elected house, a 50% elected house and a 60% elected house were all defeated in turn. Finally the vote for an 80% elected chamber was won by 305 votes to 267, and the vote for a wholly elected chamber was won by an even greater margin: 337 to 224. Significantly this last vote represented an overall majority of MPs.
Furthermore, examination of the names of MPs voting at each division shows that, of the 305 who voted for the 80% elected option, 211 went on to vote for the 100% elected option. Given that this vote took place after the vote on 80% – whose result was already known when the vote on 100% took place – this showed a clear preference for a fully elected upper house among those who voted for the only other option that passed. But this was nevertheless only an indicative vote and many political and legislative hurdles remained to be overcome for supporters of an elected second chamber. The House of Lords, soon after, rejected this proposal and voted for an entirely appointed House of Lords.
In July 2008 Jack Straw, the Secretary of State for Justice and Lord Chancellor, introduced a white paper to the House of Commons proposing to replace the House of Lords with an 80–100% elected chamber, with one third being elected at each general election, for a term of approximately 12–15 years. The white paper states that as the peerage would be totally separated from membership of the upper house, the name "House of Lords" would no longer be appropriate: It goes on to explain that there is cross-party consensus for the new chamber to be titled the "Senate of the United Kingdom", however to ensure the debate remains on the role of the upper house rather than its title, the white paper is neutral on the title of the new house.
On 30 November 2009, a "Code of Conduct for Members of the House of Lords" was agreed by them; certain amendments were agreed by them on 30 March 2010 and on 12 June 2014. The scandal over expenses in the Commons was at its highest pitch only six months before, and the Labourite leadership under Janet Royall determined that something sympathetic should be done.
In Meg Russell’s article; "Is the House of Lords already reformed?" she states three essential features of a legitimate House of Lords. The first is that it must have adequate powers over legislation to make the government think twice before making a decision. The House of Lords, she argues, currently has enough power to make it relevant. During Tony Blair’s first year he was defeated thirty-eight times in the Lords. Secondly, as to the composition of the Lords, Meg Russell suggests that the composition must be distinct from the Commons, otherwise it would render the Lords useless. The third feature is the perceived legitimacy of the Lords. She writes; "In general legitimacy comes with election."
If the Lords have a distinct and elected composition, this would probably come about through fixed term proportional representation. If this happens then the perceived legitimacy of the Lords could arguably outweigh the legitimacy of the Commons. This would especially be the case if the House of Lords had been elected more recently than the House of Commons as it could be said to reflect the will of the people better than the Commons.
In this scenario there may well come a time when the Lords twice reject a Bill from the Commons and it is forced through. This would in turn trigger questions about the amount of power the Lords should have and there would be pressure for it to increase. This hypothetical process is known as the "circumnavigation of power theory". It implies that it would never be in any government's interest to legitimise the Lords as they would be forfeiting their own power.
2010–present.
The Conservative–Liberal Democrat coalition agreed, following the 2010 general election, to clearly outline a provision for a wholly or mainly elected second chamber, elected by a proportional representation system. These proposals sparked a debate on 29 June 2010. As an interim measure, appointment of new peers will reflect shares of the vote secured by the political parties in the last general election.
Detailed proposals for Lords reform including a draft House of Lords Reform Bill were published on 17 May 2011. These include a 300-member hybrid house, of which 80% are elected. A further 20% would be appointed, and reserve space would be included for some Church of England bishops. Under the proposals, members would also serve single non-renewable terms of 15 years. Former MPs would be allowed to stand for election to the Upper House, but members of the Upper House would not be immediately allowed to become MPs.
The details of the proposal were:
The proposals were considered by a Joint Committee on House of Lords Reform made up of both MPs and Peers, which issued its final report on 23 April 2012, making the following suggestions:
Deputy Prime Minister Nick Clegg introduced the House of Lords Reform Bill 2012 on 27 June 2012 which built on proposals published on 17 May 2011. However, this Bill was abandoned by the Government on 6 August 2012 following opposition from within the Conservative Party.
House of Lords Reform Act 2014.
A private members bill to introduce some reforms was introduced by Dan Byles in 2013. The House of Lords Reform Act 2014 received the Royal Assent in 2014. Under the new law:
House of Lords (Expulsion and Suspension) Act 2015.
The House of Lords (Expulsion and Suspension) Act 2015 authorised the House to expel or suspend members.
Lords Spiritual (Women) Act 2015.
This act makes provision to preferentially admit bishops of the Church of England who are women to the Lords Spiritual in the 10 years following its commencement.
In 2015, Rachel Treweek, Bishop of Gloucester, became the first woman to sit as a Lord Spiritual in the House of Lords.
Overcrowding.
The size of the House of Lords has varied greatly throughout its history. From about 50 members in the early 1700s, it increased to a record size of 1,330 in October 1999, before Lords reform reduced it to 669 by March 2000.
In April 2011, a cross-party group of former leading politicians, including many senior members of the House of Lords, called on the Prime Minister David Cameron to stop creating new peers. He had created 117 new peers since becoming prime minister in May 2010, a faster rate of elevation than any PM in British history. The expansion occurred while his government had tried (in vain) to reduce the size of the House of Commons by 50 members, from 650 to 600.
In August 2014, despite there being a seating capacity of only around 230 to 400 on the benches in the Lords chamber, the House had 774 active members (plus 54 who were not entitled to attend or vote, having been suspended or granted leave of absence). This made the House of Lords the largest parliamentary chamber in any democracy. In August 2014, former Speaker of the House of Commons Baroness Boothroyd requested that “older peers should retire gracefully” to ease the overcrowding in the House of Lords. She also criticised successive prime ministers for filling the second chamber with “lobby fodder” in an attempt to help their policies become law. She made her remarks days before a new batch of peers were due to be appointed.
In August 2015, following the creation of a further 45 peers in the Dissolution Honours, the total number of eligible members of the Lords increased to 826. In a report entitled "Does size matter?" the BBC said: "Increasingly, yes. Critics argue the House of Lords is the second largest legislature after the Chinese National People's Congress and dwarfs Upper Houses in other bi-cameral democracies such as the United States (100 senators), France (348 senators), Australia (76 senators) and India (250 members). The Lords is also larger than the Supreme People's Assembly of North Korea (687 members). [… ] Peers grumble that there is not enough room to accommodate all of their colleagues in the Chamber, where there are only about 400 seats, and say they are constantly jostling for space – particularly during high-profile sittings", but added, "On the other hand, defenders of the Lords say that it does a vital job scrutinising legislation, a lot of which has come its way from the Commons in recent years".
Relationship with the Government.
The House of Lords does not control the term of the Prime Minister or of the Government. Only the Lower House may force the Prime Minister to resign or call elections by passing a motion of no-confidence or by withdrawing supply. Thus, the House of Lords' oversight of the government is limited.
Most Cabinet ministers are from the House of Commons rather than the House of Lords. In particular, all Prime Ministers since 1902 have been members of the Lower House. (Alec Douglas-Home, who became Prime Minister in 1963 whilst still an Earl, disclaimed his peerage and was elected to the Commons soon after his term began.) In recent history, it has been very rare for major cabinet positions (except Lord Chancellor and Leader of the House of Lords) to have been filled by peers.
Exceptions include Lord Carrington, who was the Foreign Secretary between 1979 and 1982, Lord Young of Graffham (Minister without Portfolio, then Secretary of State for Employment and then Secretary of State for Trade and Industry from 1984 to 1989), Lady Amos, who served as Secretary of State for International Development and Lord Mandelson, who served as First Secretary of State, Secretary of State for Business, Innovation and Skills and President of the Board of Trade. George Robertson was briefly a peer whilst serving as Secretary of State for Defence before resigning to take up the post of Secretary General of NATO. From 1999 to 2010 the Attorney General for England and Wales was a Member of the House of Lords; the most recent was Baroness Scotland of Asthal.
The House of Lords remains a source for junior ministers and members of government. Like the House of Commons, the Lords also has a Government Chief Whip as well as several Junior Whips. Where a government department is not represented by a minister in the Lords or one is not available, government whips will act as spokesmen for them.
Legislative functions.
Legislation, with the exception of money bills, may be introduced in either House.
The House of Lords debates legislation, and has power to amend or reject bills. However, the power of the Lords to reject a bill passed by the House of Commons is severely restricted by the Parliament Acts. Under those Acts, certain types of bills may be presented for the Royal Assent without the consent of the House of Lords (i.e. the Commons can override the Lords' veto). The House of Lords cannot delay a money bill (a bill that, in the view of the Speaker of the House of Commons, solely concerns national taxation or public funds) for more than one month.
Other public bills cannot be delayed by the House of Lords for more than two parliamentary sessions, or one calendar year. These provisions, however, only apply to public bills that originate in the House of Commons, and cannot have the effect of extending a parliamentary term beyond five years. A further restriction is a constitutional convention known as the Salisbury Convention, which means that the House of Lords does not oppose legislation promised in the Government's election manifesto.
By a custom that prevailed even before the Parliament Acts, the House of Lords is further restrained insofar as financial bills are concerned. The House of Lords may neither originate a bill concerning taxation or Supply (supply of treasury or exchequer funds), nor amend a bill so as to insert a taxation or Supply-related provision. (The House of Commons, however, often waives its privileges and allows the Upper House to make amendments with financial implications.) Moreover, the Upper House may not amend any Supply Bill. The House of Lords formerly maintained the absolute power to reject a bill relating to revenue or Supply, but this power was curtailed by the Parliament Acts, as aforementioned.
Former judicial role.
Historically, the House of Lords held several judicial functions. Most notably, until 2009 the House of Lords served as the court of last resort for most instances of UK law. Since 1 October 2009 this role is now held by the Supreme Court of the United Kingdom.
The Lords' judicial functions originated from the ancient role of the Curia Regis as a body that addressed the petitions of the King's subjects. The functions were exercised not by the whole House, but by a committee of "Law Lords". The bulk of the House's judicial business was conducted by the twelve Lords of Appeal in Ordinary, who were specifically appointed for this purpose under the Appellate Jurisdiction Act 1876.
The judicial functions could also be exercised by Lords of Appeal (other members of the House who happened to have held high judicial office). No Lord of Appeal in Ordinary or Lord of Appeal could sit judicially beyond the age of seventy-five. The judicial business of the Lords was supervised by the Senior Lord of Appeal in Ordinary and his or her deputy, the Second Senior Lord of Appeal in Ordinary.
The jurisdiction of the House of Lords extended, in civil and in criminal cases, to appeals from the courts of England and Wales, and of Northern Ireland. From Scotland, appeals were possible only in civil cases; Scotland's High Court of Justiciary is the highest court in criminal matters. The House of Lords was not the United Kingdom's only court of last resort; in some cases, the Judicial Committee of the Privy Council performs such a function. The jurisdiction of the Privy Council in the United Kingdom, however, is relatively restricted; it encompasses appeals from ecclesiastical courts, disputes under the House of Commons Disqualification Act 1975, and a few other minor matters. Issues related to devolution were transferred from the Privy Council to the Supreme Court in 2009.
The twelve Law Lords did not all hear every case; rather, after World War II cases were heard by panels known as Appellate Committees, each of which normally consisted of five members (selected by the Senior Lord). An Appellate Committee hearing an important case could consist of more than five members. Though Appellate Committees met in separate committee rooms, judgement was given in the Lords Chamber itself. No further appeal lay from the House of Lords, although the House of Lords could refer a "preliminary question" to the European Court of Justice in cases involving an element of European Union law, and a case could be brought at the European Court of Human Rights if the House of Lords did not provide a satisfactory remedy in cases where the European Convention on Human Rights was relevant.
A distinct judicial function—one in which the whole House used to participate—is that of trying impeachments. Impeachments were brought by the House of Commons, and tried in the House of Lords; a conviction required only a majority of the Lords voting. Impeachments, however, are to all intents and purposes obsolete; the last impeachment was that of Henry Dundas, 1st Viscount Melville in 1806.
Similarly, the House of Lords was once the court that tried peers charged with high treason or felony. The House would be presided over not by the Lord Chancellor, but by the Lord High Steward, an official especially appointed for the occasion of the trial. If Parliament was not in session, then peers could be tried in a separate court, known as the Lord High Steward's Court. Only peers, their wives, and their widows (unless remarried) were entitled to trials in the House of Lords or the Lord High Steward's Court; the Lords Spiritual were tried in Ecclesiastical Courts. In 1948, the right of peers to be tried in such special courts was abolished; now, they are tried in the regular courts. The last such trial in the House was of Edward Southwell Russell, 26th Baron de Clifford in 1935. An illustrative dramatisation circa 1928 of a trial of a peer (the fictional Duke of Denver) on a charge of murder (a felony) is portrayed in the 1972 BBC Television adaption of Dorothy L. Sayers' Lord Peter Wimsey mystery "Clouds of Witness".
The Constitutional Reform Act 2005 resulted in the creation of a separate Supreme Court of the United Kingdom, to which the judicial function of the House of Lords, and some of the judicial functions of the Judicial Committee of the Privy Council, were transferred. In addition, the office of Lord Chancellor was reformed by the act, removing his ability to act as both a government minister and a judge. This was motivated in part by concerns about the historical admixture of legislative, judicial, and executive power. The new Supreme Court is located at Middlesex Guildhall.
Membership.
Lords Spiritual.
Members of the House of Lords who sit by virtue of their ecclesiastical offices are known as Lords Spiritual. Formerly, the Lords Spiritual were the majority in the English House of Lords, comprising the church's archbishops, (diocesan) bishops, abbots, and those priors who were entitled to wear a mitre. After the English Reformation's highpoint in 1539, only the archbishops and bishops continued to attend, as the Dissolution of the Monasteries had just disproved of and suppressed the positions of abbot and prior. In 1642 during the few Lords' gatherings convened during English Interregnum which saw periodic war, the Lords Spiritual were excluded altogether, but they returned under the Clergy Act 1661.
The number of Lords Spiritual was further restricted by the Bishopric of Manchester Act 1847, and by later acts. The Lords Spiritual can now number no more than 26; these are the Archbishop of Canterbury, the Archbishop of York, the Bishop of London, the Bishop of Durham, the Bishop of Winchester (who sit by right regardless of seniority) and the 21 longest-serving bishops from other dioceses in the Church of England (excluding the dioceses of Sodor and Man and Gibraltar in Europe, as these lie entirely outside the United Kingdom). Following a change to the law in 2014 to allow women to be ordained bishops, the Lords Spiritual (Women) Act 2015 was passed, which provides that whenever a vacancy arises among the Lords Spiritual during the ten years following the Act coming into force, the vacancy has to be filled by a woman, if one is eligible. This does not apply to the five bishops who sit by right.
The current Lords Spiritual represent only the Church of England. Bishops of the Church of Scotland traditionally sat in the Parliament of Scotland but were finally excluded in 1689 (after a number of previous exclusions) when the Church of Scotland became permanently presbyterian. There are no longer bishops in the Church of Scotland in the traditional sense of the word, and that Church has never sent members to sit in the Westminster House of Lords. The Church of Ireland did obtain representation in the House of Lords after the union of Ireland and Great Britain in 1801.
Of the Church of Ireland's ecclesiastics, four (one archbishop and three bishops) were to sit at any one time, with the members rotating at the end of every parliamentary session (which normally lasted approximately one year). The Church of Ireland, however, was disestablished in 1871, and thereafter ceased to be represented by Lords Spiritual. Bishops of Welsh sees in the Church of England originally sat in the House of Lords (after 1847, only if their seniority within the Church entitled them to), but the Church in Wales ceased to be a part of the Church of England in 1920 and was simultaneously disestablished in Wales. Accordingly, bishops of the Church in Wales were no longer eligible to be appointed to the House as bishops of the Church of England.
Other ecclesiastics have sat in the House of Lords as Lords Temporal in recent times: Chief Rabbi Immanuel Jakobovits was appointed to the House of Lords (with the consent of the Queen, who acted on the advice of Prime Minister Margaret Thatcher), as was his successor Chief Rabbi Jonathan Sacks. In recognition of his work at reconciliation and in the peace process in Northern Ireland, the Archbishop of Armagh (the senior Anglican bishop in Northern Ireland), Lord Eames was appointed to the Lords by John Major. Other clergymen appointed include the Reverend Donald Soper, the Reverend Timothy Beaumont, and some Scottish clerics.
There have been no Roman Catholic clergymen appointed, though it was rumoured that Cardinal Basil Hume and his successor Cormac Murphy O'Connor were offered peerages, by James Callaghan, Margaret Thatcher and Tony Blair respectively, but declined. Hume later accepted the Order of Merit, a personal appointment of the Queen, shortly before his death. O'Connor said he had his maiden speech ready, but Roman Catholics who have received Holy Orders are prohibited by Canon Law from holding major offices connected with any government other than the Holy See. 
Former Archbishops of Canterbury, having reverted to the status of bishop but who are no longer diocesans, are invariably given life peerages and sit as Lords Temporal.
By custom at least one of the Bishops reads prayers in each legislative day (a role taken by the chaplain in the Commons). They often speak in debates; in 2004 Rowan Williams, the Archbishop of Canterbury, opened a debate into sentencing legislation. Measures (proposed laws of the Church of England) must be put before the Lords, and the Lords Spiritual have a role in ensuring that this takes place.
Lords Temporal.
Since the Dissolution of the Monasteries, the Lords Temporal have been the most numerous group in the House of Lords. Unlike the Lords Spiritual, they may be publicly partisan, aligning themselves with one or another of the political parties that dominate the House of Commons. Publicly non-partisan Lords are called crossbenchers. Originally, the Lords Temporal included several hundred hereditary peers (that is, those whose peerages may be inherited), who ranked variously as dukes, marquesses, earls, viscounts, and barons (as well as Scottish Lords of Parliament). Such hereditary dignities can be created by the Crown; in modern times this is done on the advice of the Prime Minister of the day (except in the case of members of the Royal Family).
Holders of Scottish and Irish peerages were not always permitted to sit in the Lords. When Scotland united with England to form Great Britain in 1707, it was provided that the Scottish hereditary peers would only be able to elect 16 representative peers to sit in the House of Lords; the term of a representative was to extend until the next general election. A similar provision was enacted when Ireland merged with Great Britain in 1801 to form the United Kingdom; the Irish peers were allowed to elect 28 representatives, who were to retain office for life. Elections for Irish representatives ended in 1922, when most of Ireland became an independent state; elections for Scottish representatives ended with the passage of the Peerage Act 1963, under which all Scottish peers obtained seats in the Upper House.
In 1999, the Labour government brought forward the House of Lords Act removing the right of several hundred hereditary peers to sit in the House. The Act provided, as a measure intended to be temporary, that 92 people would continue to sit in the Lords by virtue of hereditary peerages, and this is still in effect.
Of the 92, two remain in the House of Lords because they hold royal offices connected with Parliament: the Earl Marshal and the Lord Great Chamberlain. Of the remaining ninety peers sitting in the Lords by virtue of a hereditary peerage, 14 are elected by the whole House and 74 are chosen by fellow hereditary peers in the House of Lords, grouped by party. (If a hereditary peerage holder is given a life peerage, he or she becomes a member of the House of Lords without a need for a by-election.) The exclusion of other hereditary peers removed the Prince of Wales (who is also Earl of Chester) and all other Royal Peers, including the Duke of Edinburgh, Duke of York, Earl of Wessex, Duke of Gloucester and Duke of Kent.
The number of peers to be chosen by a political group reflects the proportion of hereditary peers that belonged to that group (see current composition below) in 1999. When an elected hereditary peer dies, a by-election is held, with a variant of the Alternative Vote system being used. If the recently deceased hereditary peer had been elected by the whole House, then so is his or her replacement; a hereditary peer elected by a specific political group (including the non-aligned crossbenchers) is replaced by a vote of the hereditary peers already elected to the Lords belonging to that political group (whether elected by that group or by the whole house).
Until 2009, the Lords Temporal also included the Lords of Appeal in Ordinary, a group of individuals appointed to the House of Lords so that they could exercise its judicial functions. Lords of Appeal in Ordinary, more commonly known as Law Lords, were first appointed under the Appellate Jurisdiction Act 1876. They were selected by the Prime Minister of the day, but were formally appointed by the Sovereign. A Lord of Appeal in Ordinary had to retire at the age of 70, or, if his or her term was extended by the government, at the age of 75; after reaching such an age, the Law Lord could not hear any further cases in the House of Lords.
The number of Lords of Appeal in Ordinary (excluding those who were no longer able to hear cases because of age restrictions) was limited to twelve, but could be changed by statutory instrument. By a convention of the House, Lords of Appeal in Ordinary did not take part in debates on new legislation, so as to maintain judicial independence. Lords of Appeal in Ordinary held their seats in the House of Lords for life, remaining as members even after reaching the judicial retirement age of 70 or 75. Former Lord Chancellors and holders of other high judicial office could also sit as Law Lords under the Appellate Jurisdiction Act, although in practice this right was only rarely exercised.
Under the Constitutional Reform Act 2005, the Lords of Appeal in Ordinary when the Act came into effect in 2009 became judges of the new Supreme Court of the United Kingdom and were then barred from sitting or voting in the House of Lords until they had retired as judges. One of the main justifications for the new Supreme Court was to establish a separation of powers between the judiciary and the legislature. It is therefore unlikely that future appointees to the Supreme Court of the United Kingdom will be made Lords of Appeal in Ordinary.
The largest group of Lords Temporal, and indeed of the whole House, are life peers. Life peerages rank only as barons or baronesses, and are created under the Life Peerages Act 1958. Like all other peers, life peers are created by the Sovereign, who acts on the advice of the Prime Minister or the House of Lords Appointments Commission. By convention, however, the Prime Minister allows leaders of other parties to nominate some life peers, so as to maintain a political balance in the House of Lords. Moreover, some non-party life peers (the number being determined by the Prime Minister) are nominated by the independent House of Lords Appointments Commission.
In 2000, the government announced it would set up an Independent Appointments Commission, under Lord Stevenson of Coddenham, to select fifteen so-called "people's peers" for life peerages. However, when the choices were announced in April 2001, from a list of 3,000 applicants, the choices were treated with criticism in the media, as all were distinguished in their field, and none were "ordinary people" as some had originally hoped.
Qualifications.
Several different qualifications apply for membership of the House of Lords. No person may sit in the House of Lords if under the age of 21. Furthermore, only citizens of the United Kingdom, Commonwealth citizens, and citizens of Ireland may sit in the House of Lords. The nationality restrictions were previously more stringent: under the Act of Settlement 1701, and prior to the British Nationality Act 1948, only natural-born subjects were qualified.
Additionally, some bankruptcy-related restrictions apply to members of the Upper House. A person may not sit in the House of Lords if he or she is the subject of a Bankruptcy Restrictions Order (applicable in England and Wales only), or if he or she is adjudged bankrupt (in Northern Ireland), or if his or her estate is sequestered (in Scotland). A final restriction bars an individual convicted of high treason from sitting in the House of Lords until completing his or her full term of imprisonment. An exception applies, however, if the individual convicted of high treason receives a full pardon. Note that an individual serving a prison sentence for an offence other than high treason is "not" automatically disqualified.
Women were excluded from the House of Lords until the Life Peerages Act 1958, passed to address the declining number of active members, made possible the creation of peerages for life. Women were immediately eligible and four were among the first life peers appointed. However, hereditary peeresses continued to be excluded until the passage of the Peerage Act 1963. Since the passage of the House of Lords Act 1999, hereditary peeresses remain eligible for election to the Upper House; there is one (Countess of Mar) among the 90 hereditary peers who continue to sit.
Cash for Peerages.
The Honours (Prevention of Abuses) Act 1925 made it illegal for a peerage, or other honour, to be bought or sold. Nonetheless, there have been repeated allegations that life peerages (and thus membership of the House of Lords) have been made available to major political donors in exchange for donations. The most prominent case, the 2006 Cash for Honours scandal, saw a police investigation, with no charges being brought. A 2015 study found that of 303 people nominated for peerages in the period 2005–14, a total of 211 were former senior figures within politics (including former MPs), or were non-political appointments. Of the remaining 92 political appointments from outside public life, 27 had made significant donations to political parties. The authors concluded firstly that nominees from outside public life were much more likely to have made large gifts than peers nominated after prior political or public service. They also found that significant donors to parties were far more likely to be nominated for peerages than other party members.
Removal from House membership.
Traditionally there was no mechanism by which members could resign or be removed from the House of Lords (compare the situation as regards resignation from the House of Commons). The Peerage Act 1963 permitted a person to disclaim their newly inherited peerage (within certain time limits); this meant that such a person could effectively renounce their membership of the Lords. This might be done in order to remain or become qualified to sit in the House of Commons, as in the case of Tony Benn (formerly the second Viscount Stansgate), who had campaigned for such a change.
In 2014, the House of Lords Reform Act 2014 made provision for members' resignation from the House, removal for non-attendance, and automatic expulsion upon conviction for a serious criminal offence (if resulting in a jail sentence of at least one year). In June 2015, under the House of Lords (Expulsion and Suspension) Act 2015, the House's Standing Orders may provide for the expulsion or suspension of a member upon a resolution of the House.
Officers.
Traditionally the House of Lords did not elect its own speaker, unlike the House of Commons; rather, the "ex officio" presiding officer was the Lord Chancellor. With the passage of the Constitutional Reform Act 2005, the post of Lord Speaker was created, a position to which a peer is elected by the House and subsequently appointed by the Crown. The first Lord Speaker, elected on 4 May 2006, was Baroness Hayman, a former Labour peer. As the Speaker is expected to be an impartial presiding officer, Baroness Hayman resigned from the Labour Party. In 2011, Baroness D'Souza was elected as the second Lord Speaker, replacing Baroness Hayman in September 2011.
This reform of the post of Lord Chancellor was made due to the perceived constitutional anomalies inherent in the role. The Lord Chancellor was not only the Speaker of the House of Lords, but also a member of the Cabinet; his or her department, formerly the Lord Chancellor's Department, is now called the Ministry of Justice. The Lord Chancellor is no longer the head of the judiciary of England and Wales. Hitherto, the Lord Chancellor was part of all three branches of government: the legislative, the executive, and the judicial.
The overlap of the legislative and executive roles is a characteristic of the Westminster system, as the entire cabinet consists of members of the House of Commons or the House of Lords; however, in June 2003, the Blair Government announced its intention to abolish the post of Lord Chancellor because of the office's mixed executive and judicial responsibilities. The abolition of the office was rejected by the House of Lords, and the Constitutional Reform Act 2005 was thus amended to preserve the office of Lord Chancellor. The Act no longer guarantees that the office holder of Lord Chancellor is the presiding officer of the House of Lords, and therefore allows the House of Lords to elect a speaker of their own.
The Lord Speaker may be replaced as presiding officer by one of his or her deputies. The Chairman of Committees, the Principal Deputy Chairman of Committees, and several Chairmen are all deputies to the Lord Speaker, and are all appointed by the House of Lords itself at the beginning of each session. By custom, the Crown appoints each Chairman, Principal Deputy Chairman and Deputy Chairman to the additional office of Deputy Speaker of the House of Lords. There was previously no legal requirement that the Lord Chancellor or a Deputy Speaker be a member of the House of Lords (though the same has long been customary).
Whilst presiding over the House of Lords, the Lord Chancellor traditionally wore ceremonial black and gold robes. Robes of black and gold are now worn by the Lord Chancellor and Secretary of State for Justice in the House of Commons, on ceremonial occasions. This is no longer a requirement for the Lord Speaker except for State occasions outside of the chamber. The Speaker or Deputy Speaker sits on the Woolsack, a large red seat stuffed with wool, at the front of the Lords Chamber.
When the House of Lords resolves itself into committee (see below), the Chairman of Committees or a Deputy Chairman of Committees presides, not from the Woolsack, but from a chair at the Table of the House. The presiding officer has little power compared to the Speaker of the House of Commons. He or she only acts as the mouthpiece of the House, performing duties such as announcing the results of votes. This is because, unlike in the House of Commons where all statements are directed to "Mr/Madam Speaker", in the House of Lords they are directed to "My Lords"; i.e., the entire body of the House.
The Lord Speaker or Deputy Speaker cannot determine which members may speak, or discipline members for violating the rules of the House; these measures may be taken only by the House itself. Unlike the politically neutral Speaker of the House of Commons, the Lord Chancellor and Deputy Speakers originally remained members of their respective parties, and were permitted to participate in debate; however, this is no longer true of the new role of Lord Speaker.
Another officer of the body is the Leader of the House of Lords, a peer selected by the Prime Minister. The Leader of the House is responsible for steering Government bills through the House of Lords, and is a member of the Cabinet. The Leader also advises the House on proper procedure when necessary, but such advice is merely informal, rather than official and binding. A Deputy Leader is also appointed by the Prime Minister, and takes the place of an absent or unavailable leader.
The Clerk of the Parliaments is the chief clerk and officer of the House of Lords (but is not a member of the House itself). The Clerk, who is appointed by the Crown, advises the presiding officer on the rules of the House, signs orders and official communications, endorses bills, and is the keeper of the official records of both Houses of Parliament. Moreover, the Clerk of the Parliaments is responsible for arranging by-elections of hereditary peers when necessary. The deputies of the Clerk of the Parliaments (the Clerk Assistant and the Reading Clerk) are appointed by the Lord Speaker, subject to the House's approval.
The Gentleman Usher of the Black Rod is also an officer of the House; he takes his title from the symbol of his office, a black rod. Black Rod (as the Gentleman Usher is normally known) is responsible for ceremonial arrangements, is in charge of the House's doorkeepers, and may (upon the order of the House) take action to end disorder or disturbance in the Chamber. Black Rod also holds the office of Serjeant-at-Arms of the House of Lords, and in this capacity attends upon the Lord Speaker. The Gentleman Usher of the Black Rod's duties may be delegated to the Yeoman Usher of the Black Rod or to the Assistant Serjeant-at-Arms.
Procedure.
The House of Lords and the House of Commons assemble in the Palace of Westminster. The Lords Chamber is lavishly decorated, in contrast with the more modestly furnished Commons Chamber. Benches in the Lords Chamber are coloured red. The Woolsack is at the front of the Chamber; the Government sit on benches on the right of the Woolsack, while members of the Opposition sit on the left. Crossbenchers, sit on the benches immediately opposite the Woolsack.
The Lords Chamber is the site of many formal ceremonies, the most famous of which is the State Opening of Parliament, held at the beginning of each new parliamentary session. During the State Opening, the Sovereign, seated on the Throne in the Lords Chamber and in the presence of both Houses of Parliament, delivers a speech outlining the Government's agenda for the upcoming parliamentary session.
In the House of Lords, members need not seek the recognition of the presiding officer before speaking, as is done in the House of Commons. If two or more Lords simultaneously rise to speak, the House decides which one is to be heard by acclamation, or, if necessary, by voting on a motion. Often, however, the Leader of the House will suggest an order, which is thereafter generally followed. Speeches in the House of Lords are addressed to the House as a whole ("My Lords") rather than to the presiding officer alone (as is the custom in the Lower House). Members may not refer to each other in the second person (as "you"), but rather use third person forms such as "the noble Duke", "the noble Earl", "the noble Lord", "my noble friend", "The most Reverend Primate", etc.
Each member may make no more than one speech on a motion, except that the mover of the motion may make one speech at the beginning of the debate and another at the end. Speeches are not subject to any time limits in the House; however, the House may put an end to a speech by approving a motion "that the noble Lord be no longer heard". It is also possible for the House to end the debate entirely, by approving a motion "that the Question be now put". This procedure is known as Closure, and is extremely rare.
Once all speeches on a motion have concluded, or Closure invoked, the motion may be put to a vote. The House first votes by voice vote; the Lord Speaker or Deputy Speaker puts the question, and the Lords respond either "content" (in favour of the motion) or "not content" (against the motion). The presiding officer then announces the result of the voice vote, but if his assessment is challenged by any Lord, a recorded vote known as a division follows.
Members of the House enter one of two lobbies (the "content" lobby or the "not-content" lobby) on either side of the Chamber, where their names are recorded by clerks. At each lobby are two Tellers (themselves members of the House) who count the votes of the Lords. The Lord Speaker may not take part in the vote. Once the division concludes, the Tellers provide the results thereof to the presiding officer, who then announces them to the House.
If there is an equality of votes, the motion is decided according to the following principles: legislation may proceed in its present form, unless there is a majority in favour of amending or rejecting it; any other motions are rejected, unless there is a majority in favour of approving it. The quorum of the House of Lords is just three members for a general or procedural vote, and 30 members for a vote on legislation. If fewer than three or 30 members (as appropriate) are present, the division is invalid.
Disciplinary powers.
By contrast with the House of Commons, the House of Lords has not until recently had an established procedure for putting sanctions on its members. When a cash for influence scandal was referred to the Committee of Privileges in January 2009, the Leader of the House of Lords also asked the Privileges Committee to report on what sanctions the House had against its members. After seeking advice from the Attorney General for England and Wales and the former Lord Chancellor Lord Mackay of Clashfern, the committee decided that the House "possessed an inherent power" to suspend errant members, although not to withhold a writ of summons nor to expel a member permanently. When the House subsequently suspended Lord Truscott and Lord Taylor of Blackburn for their role in the scandal, they were the first to meet this fate since 1642.
Recent changes have expanded the disciplinary powers of the House. Section 3 of the House of Lords Reform Act 2014 now provides that any member of the House of Lords convicted of a crime and sentenced to imprisonment for more than one year loses their seat. The House of Lords (Expulsion and Suspension) Act 2015 allows the House to set up procedures to suspend, and to expel, its members.
Regulation of behaviour in the chamber.
There are two motions which have grown up through custom and practice and which govern questionable conduct within the House. They are brought into play by a member standing up, possibly intervening on another member, and moving the motion without notice. When the debate is getting excessively heated, it is open to a member to move "that the Standing Order on Asperity of Speech be read by the Clerk". The motion can be debated, but if agreed by the House, the Clerk of the Parliaments will read out Standing Order 33 which provides "That all personal, sharp, or taxing speeches be forborn". The Journals of the House of Lords record only four instances on which the House has ordered the Standing Order to be read since the procedure was invented in 1871.
For more serious problems with an individual Lord, the option is available to move "That the noble Lord be no longer heard". This motion also is debatable, and the debate which ensues has sometimes offered a chance for the member whose conduct has brought it about to come to order so that the motion can be withdrawn. If the motion is passed, its effect is to prevent the member from continuing their speech on the motion then under debate. The Journals identify eleven occasions on which this motion has been moved since 1884; four were eventually withdrawn, one was voted down, and six were passed.
Leave of absence.
In 1958, to counter criticism that some peers only appeared at major decisions in the House and thereby particular votes were swayed, the Standing Orders of the House of Lords were enhanced. Peers who did not wish to attend meetings regularly or were prevented by ill health, age or further reasons, were now able to request Leave of Absence. During the granted time a peer is expected not to visit the House's meetings until either its expiration or termination, announced at least a month prior to their return.
Attendance allowance.
Members of the House of Lords can, since 2010, opt to receive a £300 per day attendance allowance, plus limited travel expenses. Peers can elect to receive a reduced attendance allowance of £150 per day instead. Prior to 2010 peers from outside London could claim an overnight allowance of £174.
Committees.
Unlike in the House of Commons, when the term committee is used to describe a stage of a bill, this committee does not take the form of a public bill committee, but what is described as Committee of the Whole House. It is made up of all Members of the House of Lords allowing any Member to contribute to debates if he or she chooses to do so and allows for more flexible rules of procedure. It is presided over by the Chairman of Committees.
The term committee is also used to describe Grand Committee, where the same rules of procedure apply as in the main chamber, except that no divisions may take place. For this reason, business that is discussed in Grand Committee is usually uncontroversial and likely to be agreed unanimously.
Public bills may also be committed to pre-legislative committees. A pre-legislative Committee is specifically constituted for a particular bill. These committees are established in advance of the bill be laid before either the House of Lords or the House of Commons and can take evidence from the public. Such committees are rare and do not replace any of the usual stages of a bill, including committee stage.
The House of Lords also has 15 Select Committees. Typically, these are "sessional committees", meaning that their members are appointed by the House at the beginning of each session, and continue to serve until the next parliamentary session begins. In practice, these are often permanent committees, which are re-established during every session. These committees are typically empowered to make reports to the House "from time to time", that is, whenever they wish. Other committees are "ad-hoc committees", which are set up to investigate a specific issue. When they are set up by a motion in the House, the motion will set a deadline by which the Committee must report. After this date, the Committee will cease to exist unless it is granted an extension. One example of this is the Committee on Public Service and Demographic Change. The House of Lords may appoint a chairman for a committee; if it does not do so, the Chairman of Committees or a Deputy Chairman of Committees may preside instead. Most of the Select Committees are also granted the power to co-opt members, such as the European Union Committee. The primary function of Select Committees is to scrutinise and investigate Government activities; to fulfil these aims, they are permitted to hold hearings and collect evidence. Bills may be referred to Select Committees, but are more often sent to the Committee of the Whole House and Grand Committees.
The committee system of the House of Lords also includes several Domestic Committees, which supervise or consider the House's procedures and administration. One of the Domestic Committees is the Committee of Selection, which is responsible for assigning members to many of the House's other committees.
Current composition.
There are currently sitting members of the House of Lords. An additional Lords are ineligible from participation, including eight peers who are constitutionally disqualified as members of the Judiciary.
The House of Lords Act 1999 allocated 75 of the 92 hereditary peers to the parties based on the proportion of hereditary peers that belonged to that party in 1999:
Of the initial 42 hereditary peers elected as Conservatives, one (Lord Willoughby de Broke) now sits as UKIP.
15 hereditary peers are elected by the whole House, and the remaining hereditary peers are the two royal office-holders, the Earl Marshal and the Lord Great Chamberlain, both being currently on leave of absence.
A report in 2007 stated that many members of the Lords (particularly the life peers) do not attend regularly; the average daily attendance was around 408.
While the number of hereditary peers is limited to 92, and that of Lords spiritual to 26, there is no maximum limit to the number of life peers who may be members of the House of Lords at any time.

</doc>
<doc id="13660" url="https://en.wikipedia.org/wiki?curid=13660" title="Homeomorphism">
Homeomorphism

In the mathematical field of topology, a homeomorphism or topological isomorphism or bi continuous function is a continuous function between topological spaces that has a continuous inverse function. Homeomorphisms are the isomorphisms in the category of topological spaces—that is, they are the mappings that preserve all the topological properties of a given space. Two spaces with a homeomorphism between them are called homeomorphic, and from a topological viewpoint they are the same. The word "homeomorphism" comes from the Greek words "ὅμοιος" ("homoios") = similar and "μορφή" ("morphē") = shape, form.
Roughly speaking, a topological space is a geometric object, and the homeomorphism is a continuous stretching and bending of the object into a new shape. Thus, a square and a circle are homeomorphic to each other, but a sphere and a torus are not. An often-repeated mathematical joke is that topologists can't tell the difference between a coffee cup and a donut, since a sufficiently pliable donut could be reshaped to the form of a coffee cup by creating a dimple and progressively enlarging it, while preserving the donut hole in a cup's handle.
Topology is the study of those properties of objects that do not change when homeomorphisms are applied.
Definition.
A function "f": "X" → "Y" between two topological spaces ("X", "TX") and ("Y", "TY") is called a homeomorphism if it has the following properties:
A function with these three properties is sometimes called bicontinuous. If such a function exists, we say "X" and "Y" are homeomorphic. A self-homeomorphism is a homeomorphism of a topological space and itself. The homeomorphisms form an equivalence relation on the class of all topological spaces. The resulting equivalence classes are called homeomorphism classes.
Notes.
The third requirement, that "f" −1 be continuous, is essential. Consider for instance the function "f": [0, 2π) → S1 (the unit circle in formula_6) defined by "f"(φ) = (cos(φ), sin(φ)). This function is bijective and continuous, but not a homeomorphism (S1 is compact but [0, 2π) is not). The function "f" −1 is not continuous at the point (1, 0), because although "f" −1 maps (1, 0) to 0, any neighbourhood of this point also includes points that the function maps close to 2π, but the points it maps to numbers in between lie outside the neighbourhood.
Homeomorphisms are the isomorphisms in the category of topological spaces. As such, the composition of two homeomorphisms is again a homeomorphism, and the set of all self-homeomorphisms "X" → "X" forms a group, called the homeomorphism group of "X", often denoted Homeo("X"); this group can be given a topology, such as the compact-open topology, making it a topological group.
For some purposes, the homeomorphism group happens to be too big, but by means of the isotopy relation, one can reduce this group to the mapping class group.
Similarly, as usual in category theory, given two spaces that are homeomorphic, the space of homeomorphisms between them, Homeo("X," "Y"), is a torsor for the homeomorphism groups Homeo("X") and Homeo("Y"), and given a specific homeomorphism between "X" and "Y", all three sets are identified.
Informal discussion.
The intuitive criterion of stretching, bending, cutting and gluing back together takes a certain amount of practice to apply correctly—it may not be obvious from the description above that deforming a line segment to a point is impermissible, for instance. It is thus important to realize that it is the formal definition given above that counts.
This characterization of a homeomorphism often leads to confusion with the concept of homotopy, which is actually "defined" as a continuous deformation, but from one "function" to another, rather than one space to another. In the case of a homeomorphism, envisioning a continuous deformation is a mental tool for keeping track of which points on space "X" correspond to which points on "Y"—one just follows them as "X" deforms. In the case of homotopy, the continuous deformation from one map to the other is of the essence, and it is also less restrictive, since none of the maps involved need to be one-to-one or onto. Homotopy does lead to a relation on spaces: homotopy equivalence.
There is a name for the kind of deformation involved in visualizing a homeomorphism. It is (except when cutting and regluing are required) an isotopy between the identity map on "X" and the homeomorphism from "X" to "Y".

</doc>
<doc id="13661" url="https://en.wikipedia.org/wiki?curid=13661" title="Hvergelmir">
Hvergelmir

In Norse mythology, Hvergelmir (Old Norse "bubbling boiling spring") is a major spring. Hvergelmir is attested in the "Poetic Edda", compiled in the 13th century from earlier traditional sources, and the "Prose Edda", written in the 13th century by Snorri Sturluson. In the "Poetic Edda", Hvergelmir is mentioned in a single stanza, which details that it is the location where liquid from the antlers of the stag Eikþyrnir flow, and that the spring, "whence all waters rise", is the source of numerous rivers. The "Prose Edda" repeats this information and adds that the spring is located in Niflheim, that it is one of the three major springs at the primary roots of the cosmic tree Yggdrasil (the other two are Urðarbrunnr and Mímisbrunnr), and that within the spring are a vast amount of snakes and the dragon Níðhöggr.
Attestations.
Hvergelmir is attested in the following works:
"Poetic Edda".
Hvergelmir receives a single mention in the "Poetic Edda", found in the poem "Grímnismál":
This stanza is followed by three stanzas consisting mainly of the names of 42 rivers. Some of these rivers lead to the dwelling of the gods (such as Gömul and Geirvimul), while at least two (Gjöll and Leipt), reach to Hel.
"Prose Edda".
Hvergelmir is mentioned several times in the "Prose Edda". In "Gylfaginning", Just-as-High explains that the spring Hvergelmir is located in the foggy realm of Niflheim: "It was many ages before the earth was created that Niflheim was made, and in its midst lies a spring called Hvergelmir, and from it flows the rivers called Svol, Gunnthra, Fiorm, Fimbulthul, Slidr and Hrid, Sylg and Ylg, Vid, Leiptr; Gioll is next to Hell-gates."
Later in "Gylfaginning", Just-as-High describes the central tree Yggdrasil. Just-as-High says that three roots of the tree support it and "extend very, very far" and that the third of these three roots extends over Niflheim. Beneath this root, says Just-as-High, is the spring Hvergelmir, and that the base of the root is gnawed on by the dragon Níðhöggr. Additionally, High says that Hvergelmir contains not only Níðhöggr but also so many snakes that "no tongue can enumerate them".
The spring is mentioned a third time in "Gylfaginning" where High recounts its source: the stag Eikþyrnir stands on top of the afterlife hall Valhalla feeding branches of Yggdrasil, and from the stag's antlers drips great amounts of liquid down into Hvergelmir. High tallies 26 rivers here.
Hvergelmir is mentioned a final time in the "Prose Edda" where Third discusses the unpleasantries of Náströnd. Third notes that Hvergelmir yet worse than the venom-filled Náströnd because—by way of quoting a portion of a stanza from the "Poetic Edda" poem "Völuspá"—"There Nidhogg torments the bodies of the dead".

</doc>
<doc id="13665" url="https://en.wikipedia.org/wiki?curid=13665" title="Hausdorff maximal principle">
Hausdorff maximal principle

In mathematics, the Hausdorff maximal principle is an alternate and earlier formulation of Zorn's lemma proved by Felix Hausdorff in 1914 (Moore 1982:168). It states that in any partially ordered set, every totally ordered subset is contained in a maximal totally ordered subset.
The Hausdorff maximal principle is one of many statements equivalent to the axiom of choice over ZF (Zermelo–Fraenkel set theory without the axiom of choice). The principle is also called the Hausdorff maximality theorem or the Kuratowski lemma (Kelley 1955:33).
Statement.
The Hausdorff maximal principle states that, in any partially ordered set, every totally ordered subset is contained in a maximal totally ordered subset. Here a maximal totally ordered subset is one that, if enlarged in any way, does not remain totally ordered. The maximal set produced by the principle is not unique, in general; there may be many maximal totally ordered subsets containing a given totally ordered subset.
An equivalent form of the principle is that in every partially ordered set there exists a maximal totally ordered subset.
To prove that it follows from the original form, let "A" be a poset. Then formula_1 is a totally ordered subset of "A", hence there exists a maximal totally ordered subset containing formula_1, in particular "A" contains a maximal totally ordered subset.
For the converse direction, let "A" be a partially ordered set and "T" a totally ordered subset of "A". Then
is partially ordered by set inclusion formula_4, therefore it contains a maximal totally ordered subset "P". Then the set formula_5 satisfies the desired properties.
The proof that the Hausdorff maximal principle is equivalent to Zorn's lemma is very similar to this proof.

</doc>
<doc id="13666" url="https://en.wikipedia.org/wiki?curid=13666" title="Hel (being)">
Hel (being)

In Norse mythology, Hel is a being who presides over a realm of the same name, where she receives a portion of the dead. Hel is attested in the "Poetic Edda", compiled in the 13th century from earlier traditional sources, and the "Prose Edda", written in the 13th century by Snorri Sturluson. In addition, she is mentioned in poems recorded in "Heimskringla" and "Egils saga" that date from the 9th and 10th centuries, respectively. An episode in the Latin work "Gesta Danorum", written in the 12th century by Saxo Grammaticus, is generally considered to refer to Hel, and Hel may appear on various Migration Period bracteates.
In the "Poetic Edda", "Prose Edda", and "Heimskringla", Hel is referred to as a daughter of Loki, and to "go to Hel" is to die. In the "Prose Edda" book "Gylfaginning", Hel is described as having been appointed by the god Odin as ruler of a realm of the same name, located in Niflheim. In the same source, her appearance is described as half blue and half flesh-coloured and further as having a gloomy, downcast appearance. The "Prose Edda" details that Hel rules over vast mansions with many servants in her underworld realm and plays a key role in the attempted resurrection of the god Baldr.
Scholarly theories have been proposed about Hel's potential connections to figures appearing in the 11th century "Old English Gospel of Nicodemus" and Old Norse "Bartholomeus saga postola", that she may have been considered a goddess with potential Indo-European parallels in Bhavani, Kali, and Mahakali or that Hel may have become a being only as a late personification of the location of the same name.
Attestations.
"Poetic Edda".
The "Poetic Edda", compiled in the 13th century from earlier traditional sources, features various poems that mention Hel. In the "Poetic Edda" poem "Völuspá", Hel's realm is referred to as the "Halls of Hel." In stanza 31 of "Grímnismál", Hel is listed as living beneath one of three roots growing from the world tree Yggdrasil. In "Fáfnismál", the hero Sigurd stands before the mortally wounded body of the dragon Fáfnir, and states that Fáfnir lies in pieces, where "Hel can take" him. In "Atlamál", the phrases "Hel has half of us" and "sent off to Hel" are used in reference to death, though it could be a reference to the location and not the being, if not both. In stanza 4 of "Baldrs draumar", Odin rides towards the "high hall of Hel."
Hel may also be alluded to in "Hamðismál". Death is periphrased as "joy of the troll-woman" (or "ogress") and ostensibly it is Hel being referred to as the troll-woman or the ogre ("flagð"), although it may otherwise be some unspecified "dís". The Poetic Edda also mentions that travelers to Hel must pass by her guardian hound Garmr.
"Prose Edda".
Hel is referenced in the "Prose Edda", written in the 13th century by Snorri Sturluson. In chapter 34 of the book "Gylfaginning", Hel is listed by High as one of the three children of Loki and Angrboða; the wolf Fenrir, the serpent Jörmungandr, and Hel. High continues that, once the gods found that these three children are being brought up in the land of Jötunheimr, and when the gods "traced prophecies that from these siblings great mischief and disaster would arise for them" then the gods expected a lot of trouble from the three children, partially due to the nature of the mother of the children, yet worse so due to the nature of their father.
High says that Odin sent the gods to gather the children and bring them to him. Upon their arrival, Odin threw Jörmungandr into "that deep sea that lies round all lands," Odin threw Hel into Niflheim, and bestowed upon her authority over nine worlds, in that she must "administer board and lodging to those sent to her, and that is those who die of sickness or old age." High details that in this realm Hel has "great Mansions" with extremely high walls and immense gates, a hall called Éljúðnir, a dish called "Hunger," a knife called "Famine," the servant Ganglati (Old Norse "lazy walker"), the serving-maid Ganglöt (also "lazy walker"), the entrance threshold "Stumbling-block," the bed "Sick-bed," and the curtains "Gleaming-bale." High describes Hel as "half black and half flesh-coloured," adding that this makes her easily recognizable, and furthermore that Hel is "rather downcast and fierce-looking."
In chapter 49, High describes the events surrounding the death of the god Baldr. The goddess Frigg asks who among the Æsir will earn "all her love and favour" by riding to Hel, the location, to try to find Baldr, and offer Hel herself a ransom. The god Hermóðr volunteers and sets off upon the eight-legged horse Sleipnir to Hel. Hermóðr arrives in Hel's hall, finds his brother Baldr there, and stays the night. The next morning, Hermóðr begs Hel to allow Baldr to ride home with him, and tells her about the great weeping the Æsir have done upon Baldr's death. Hel says the love people have for Baldr that Hermóðr has claimed must be tested, stating:
Later in the chapter, after the female jötunn Þökk refuses to weep for the dead Baldr, she responds in verse, ending with "let Hel hold what she has." In chapter 51, High describes the events of Ragnarök, and details that when Loki arrives at the field Vígríðr "all of Hel's people" will arrive with him.
In chapter 5 of the "Prose Edda" book "Skáldskaparmál", Hel is mentioned in a kenning for Baldr ("Hel's companion"). In chapter 16, "Hel's [...] relative or father" is given as a kenning for Loki. In chapter 50, Hel is referenced ("to join the company of the quite monstrous wolf's sister") in the skaldic poem "Ragnarsdrápa".
"Heimskringla".
In the "Heimskringla" book "Ynglinga saga", written in the 13th century by Snorri Sturluson, Hel is referred to, though never by name. In chapter 17, the king Dyggvi dies of sickness. A poem from the 9th century "Ynglingatal" that forms the basis of "Ynglinga saga" is then quoted that describes Hel's taking of Dyggvi:
In chapter 45, a section from "Ynglingatal" is given which refers to Hel as "howes'-warder" (meaning "guardian of the graves") and as taking King Halfdan Hvitbeinn from life. In chapter 46, King Eystein Halfdansson dies by being knocked overboard by a sail yard. A section from "Ynglingatal" follows, describing that Eystein "fared to" Hel (referred to as "Býleistr's-brother's-daughter"). In chapter 47, the deceased Eystein's son King Halfdan dies of an illness, and the excerpt provided in the chapter describes his fate thereafter, a portion of which references Hel:
In a stanza from "Ynglingatal" recorded in chapter 72 of the "Heimskringla" book "Saga of Harald Sigurdsson", "given to Hel" is again used as a phrase to referring to death.
"Egils saga".
The Icelanders' saga "Egils saga" contains the poem "Sonatorrek". The saga attributes the poem to 10th century skald Egill Skallagrímsson, and writes that it was composed by Egill after the death of his son Gunnar. The final stanza of the poem contains a mention of Hel, though not by name:
"Gesta Danorum".
In the account of Baldr's death in Saxo Grammaticus' early 13th century work "Gesta Danorum", the dying Baldr has a dream visitation from Proserpina (here translated as "the goddess of death"):
The following night the goddess of death appeared to him in a dream standing at his side, and declared that in three days time she would clasp him in her arms. It was no idle vision, for after three days the acute pain of his injury brought his end.
Scholars have assumed that Saxo used Proserpina as a goddess equivalent to the Norse Hel.
Archaeological record.
It has been suggested that several Migration Period imitation medallions and bracteates feature depictions of Hel. In particular the bracteates IK 14 and IK 124 depict a rider traveling down a slope and coming upon a female being holding a scepter or a staff. The downward slope may indicate that the rider is traveling towards the realm of the dead and the woman with the scepter may be a female ruler of that realm, corresponding to Hel.
Some B-class bracteates showing three godly figures have been interpreted as depicting Baldr's death, the best known of these is the Fakse bracteate. Two of the figures are understood to be Baldr and Odin while both Loki and Hel have been proposed as candidates for the third figure. If it is Hel she is presumably greeting the dying Baldr as he comes to her realm.
Theories.
Seo Hell.
The "Old English Gospel of Nicodemus", preserved in two manuscripts from the 11th century, contains a female figure referred to as "Seo hell" who engages in flyting with Satan and tells him to leave her dwelling (Old English "ut of mynre onwununge"). Regarding Seo Hell in the "Old English Gospel of Nicodemus", Michael Bell states that "her vivid personification in a dramatically excellent scene suggests that her gender is more than grammatical, and invites comparison with the Old Norse underworld goddess Hel and the Frau Holle of German folklore, to say nothing of underworld goddesses in other cultures" yet adds that "the possibility that these genders "are" merely grammatical is strengthened by the fact that an Old Norse version of Nicodemus, possibly translated under English influence, personifies Hell in the neuter (Old Norse "þat helviti")."
"Bartholomeus saga postola".
The Old Norse "Bartholomeus saga postola", an account of the life of Saint Bartholomew dating from the 13th century, mentions a "Queen Hel." In the story, a devil is hiding within a pagan idol, and bound by Bartholomew's spiritual powers to acknowledge himself and confess, the devil refers to Jesus as the one which "made war on Hel our queen" (Old Norse "heriaði a Hel drottning vara"). "Queen Hel" is not mentioned elsewhere in the saga.
Michael Bell says that while Hel "might at first appear to be identical with the well-known pagan goddess of the Norse underworld" as described in chapter 34 of "Gylfaginning", "in the combined light of the Old English and Old Norse versions of "Nicodemus" she casts quite a different a shadow," and that in "Bartholomeus saga postola" "she is clearly the queen of the Christian, not pagan, underworld."
Origins and development.
Jacob Grimm theorized that Hel (whom he refers to here as "Halja", the theorized Proto-Germanic form of the term) is essentially an "image of a greedy, unrestoring, female deity" and that "the higher we are allowed to penetrate into our antiquities, the less hellish and more godlike may "Halja" appear. Of this we have a particularly strong guarantee in her affinity to the Indian Bhavani, who travels about and bathes like Nerthus and Holda, but is likewise called "Kali" or "Mahakali", the great "black" goddess. In the underworld she is supposed to sit in judgment on souls. This office, the similar name and the black hue [...] make her exceedingly like "Halja". And "Halja" is one of the oldest and commonest conceptions of our heathenism."
Grimm theorizes that the Helhest, a three legged-horse that roams the countryside "as a harbinger of plague and pestilence" in Danish folklore, was originally the steed of the goddess Hel, and that on this steed Hel roamed the land "picking up the dead that were her due." In addition, Grimm says that a wagon was once ascribed to Hel, with which Hel made journeys. Grimm says that Hel is an example of a "half-goddess;" "one who cannot be shown to be either wife or daughter of a god, and who stands in a dependent relation to higher divinities" and that "half-goddesses" stand higher than "half-gods" in Germanic mythology.
Hilda Ellis Davidson (1948) states that Hel "as a goddess" in surviving sources seems to belong to a genre of literary personification, that the word "hel" is generally "used simply to signify death or the grave," and that the word often appears as the equivalent to the English 'death,' which Davidson states "naturally lends itself to personification by poets." Davidson explains that "whether this personification has originally been based on a belief in a goddess of death called Hel is another question," but that she does not believe that the surviving sources give any reason to believe so. Davidson adds that, on the other hand, various other examples of "certain supernatural women" connected with death are to be found in sources for Norse mythology, that they "seem to have been closely connected with the world of death, and were pictured as welcoming dead warriors," and that the depiction of Hel "as a goddess" in "Gylfaginning" "might well owe something to these."
In a later work (1998), Davidson states that the description of Hel found in chapter 33 of "Gylfaginning" "hardly suggests a goddess." Davidson adds that "yet this is not the impression given in the account of Hermod's ride to Hel later in "Gylfaginning" (49)" and points out that here Hel " with authority as ruler of the underworld" and that from her realm "gifts are sent back to Frigg and Fulla by Balder's wife Nanna as from a friendly kingdom." Davidson posits that Snorri may have "earlier turned the goddess of death into an allegorical figure, just as he made Hel, the underworld of shades, a place 'where wicked men go,' like the Christian Hell ("Gylfaginning" 3)." Davidson continues that:
Davidson further compares to early attestations of the Irish goddesses Badb (Davidson points to the description of Badb from "The Destruction of Da Choca's Hostel" where Badb is wearing a dusky mantle, has a large mouth, is dark in color, and has gray hair falling over her shoulders, or, alternatively, "as a red figure on the edge of the ford, washing the chariot of a king doomed to die") and The Morrígan. Davidson concludes that, in these examples, "here we have the fierce destructive side of death, with a strong emphasis on its physical horrors, so perhaps we should not assume that the gruesome figure of Hel is wholly Snorri's literary invention."
John Lindow states that most details about Hel, as a figure, are not found outside of Snorri's writing in "Gylfaginning", and says that when older skaldic poetry "says that people are 'in' rather than 'with' Hel, we are clearly dealing with a place rather than a person, and this is assumed to be the older conception," that the noun and place "Hel" likely originally simply meant "grave," and that "the personification came later." Rudolf Simek theorizes that the figure of Hel is "probably a very late personification of the underworld Hel," and says that "the first scriptures using the goddess Hel are found at the end of the 10th and in the 11th centuries." Simek states that the allegorical description of Hel's house in "Gylfaginning" "clearly stands in the Christian tradition," and that "on the whole nothing speaks in favour of there being a belief in Hel in pre-Christian times." However, Simek also cites Hel as possibly appearing as one of three figures appearing together on Migration Period B-bracteates.

</doc>
<doc id="13667" url="https://en.wikipedia.org/wiki?curid=13667" title="Hawar Islands">
Hawar Islands

The Hawar Islands (; transliterated: Juzur Ḩawār) is an archipelago of desert islands owned by Bahrain. They are situated off the west coast of Qatar in the Gulf of Bahrain of the Persian Gulf. They have a distance of south of the capital, Manama, on Bahrain Island.
History.
The islands used to be one of the settlements of the Bahraini branch of the Dawasir who settled there in the early 19th century. The islands were first surveyed in 1820, when they were called the Warden’s Islands, and two villages were recorded. 
In 2002, A small group of Hawar-born natives announced independence in Paris. They declared Hawar an emirate and raised a flag - dark red (symbol of patriotism) with white wedge (symbol of purity) carrying a yellow sun and green horizontal stripes (symbolizing spring nation). There is evidence that these declarations reflect the real will of the people of Hawar, but this so-called "government in exile" is not recognized.
Conservation.
The islands were listed as a Ramsar site in 1997. In 2002, the Bahraini government applied to have the islands recognised as a World Heritage Site due to their unique environment and habitat for endangered species; the application was ultimately unsuccessful.
Geography.
Despite their proximity to Qatar (they are only about from the Qatari mainland whilst being about from the main islands of Bahrain), most of the islands belong to Bahrain, having been a part of a dispute between Bahrain and Qatar which was resolved in 2001. The land area of the islands is 54.5 km2.
Although there are 36 islands in the group, many of the smaller islands are little more than sand or shingle accumulations on areas of exposed bedrock molded by the ongoing processes of sedimentation and accretion.
The WHS application named 8 major islands (see table hereafter), which conforms to the description of the islands when first surveyed as consisting of 8 or 9 islands. It has often been described as an archipelago of 16 islands. Janan island, to the south of Hawar island, is not legally considered to be a part of the group and is owned by Qatar.
Flora and Fauna.
The islands are home to many bird species, notably Socotra cormorants. There are small herds of Arabian oryx and sand gazelle on Hawar island, and the seas around support a large population of dugong.
Demographics.
The old Dawasir villages are now uninhabited, and the islands have only a small population at the police garrison and some resort staff quarters, all on the main island; access to all but Hawar island itself is severely restricted. Local Qatari fishermen are allowed to fish in adjacent waters and there is some recreational fishing and tourism on and around the islands. There are also small scale pearling, hunting, gypsum quarrying which provides a source of income to seasonal inhabitants of the islands. Fresh water has always been scarce; historically it was obtained by surface collection and even today, with the desalinization plant, additional supplies have to be brought in.
Military.
On the north point of Hawar Island, there is a Bahrain Defense Force Base. at 2001 the population of the island was 3875, of it 3500 were posted at the base.
at 2016, of a population of 250 only 50 are at the base.
Tourism.
There is a resort on the island. The old resort was renovated in 2013/14 and is now a Best Western hotel., Unfortunately The resort closed in mid-2016
List of islands.
Hawar archipelago.
By far the largest island is Hawar, which accounts for more than 41 km2 of the 54.5 km2 land area. Following in size are Suwād al Janūbīyah, Suwād ash Shamālīyah, Rubud Al Sharqiyah, Rubud Al Gharbiyah and Muhazwarah (Umm Hazwarah).
The following were not considered as part of the Hawar islands in the International Court of Justice (ICJ) judgment, being located between Hawar and the Bahrain Islands and not disputed by Qatar, but have been included in the Hawar archipelago by the Bahrain government as part of the 2002 World Heritage Site application.
Janan island.
Janan island, a small island south of Hawar island, was also considered in the 2001 judgment. Based on a previous agreement when both Qatar and Bahrain were under British protection, it was judged to be separate from the Hawar islands and so considered by the court separately. It was awarded to Qatar.
Administration.
The islands were formerly coincident with municipality or "Minţaqat" Juzur Ḩawār (مِنْطَقَة جُزُر حَوَار) and are now administered as part of the Southern Governorate of Bahrain.

</doc>
<doc id="13669" url="https://en.wikipedia.org/wiki?curid=13669" title="Hans-Dietrich Genscher">
Hans-Dietrich Genscher

Hans-Dietrich Genscher (21 March 1927 – 31 March 2016) was a German statesman and a member of the liberal Free Democratic Party (FDP), who served as Minister of the Interior from 1969 to 1974, and as Foreign Minister and Vice Chancellor from 1974 to 1992 (except for a two-week break in 1982), making him the longest-serving occupant of either post. In 1991 he was chairman of the Organization for Security and Co-operation in Europe (OSCE).
A proponent of Realpolitik, Genscher has been called "a master of diplomacy." He is widely regarded as a principal "architect of German Reunification." In 1991, he played a pivotal role in the breakup of Yugoslavia by successfully pushing for international recognition of Croatia, Slovenia and other republics declaring independence, in an effort to halt "a trend towards a Greater Serbia." After leaving office, he worked as a lawyer and international consultant. He was President of the German Council on Foreign Relations and was involved with several international organisations, and with former Czech President Václav Havel, he called for a Cold War museum to be built in Berlin.
Biography.
Early life.
Genscher was born on 21 March 1927 in Reideburg (Province of Saxony), now a part of Halle, in what later became East Germany. He was the son of Hilda Kreime and Kurt Genscher. His father, a lawyer, died when Genscher was nine years old. In 1943, he was drafted to serve as a member of the Air Force Support Personnel ("Luftwaffenhelfer") at the age of 16. At age 17, close to the end of the war, he and his fellow soldiers became members of the Nazi Party due to a collective application ("Sammelantrag") by his Wehrmacht unit. He later said he was unaware of it at the time.
Late in the war, Genscher was deployed as a soldier in General Walther Wenck's 12th Army, which ostensibly was directed to relieve the siege of Berlin. After the German surrender he was an American and British prisoner of war, but was released after two months. Following World War II, he studied law and economics at the universities of Halle and Leipzig (1946–1949) and joined the East German Liberal Democratic Party (LDPD) in 1946.
Political career.
In 1952, Genscher fled to West Germany, where he joined the Free Democratic Party (FDP). He passed his second state examination in law in Hamburg in 1954 and became a solicitor in Bremen. During these early years after the war, Genscher continuously struggled with illness. From 1956 to 1959 he was a research assistant of the FDP parliamentary group in Bonn. From 1959 to 1965 he was the FDP group managing director, while from 1962 to 1964 he was National Secretary of the FDP.
In 1965 Genscher was elected on the North Rhine-Westphalian FDP list to the West German parliament and remained a member of parliament until his retirement in 1998. He was elected deputy national chairman in 1968. After serving in several party offices, he was appointed minister of the interior by Chancellor Willy Brandt, whose Social Democratic Party was in coalition with the FDP, in 1969.
In 1974 he became foreign minister and vice chancellor, both posts he would hold for 18 years. From 1 October 1974 to 23 February 1985 he was Chairman of the FDP. It was during his tenure as party chairman that the FDP switched from being the junior member of social-liberal coalition to being the junior member of the 1982 coalition with the CDU/CSU. In 1985 he gave up the post of national chairman. After his resignation as Foreign Minister, Genscher was appointed honorary chairman of the FDP in 1992.
Federal Minister of the Interior.
After the federal election of 1969 Genscher was instrumental in the formation of the social-liberal coalition of chancellor Willy Brandt and was on 22 October 1969 appointed as federal minister of the interior. 
In 1972, while minister for the interior, Genscher rejected Israel's offer to send an Israeli special forces unit to Germany to deal with the Munich Olympics hostage crisis. A flawed rescue attempt by German police forces at Fürstenfeldbruck air base resulted in a bloody shootout, which left all eleven hostages, five terrorists, and one German policeman dead. Genscher's popularity with Israel declined further when he endorsed the release of the three captured attackers following the hijacking of a Lufthansa aircraft on 29 October 1972.
In the SPD–FDP coalition, Genscher helped shape Brandt's policy of deescalation with the communist East, commonly known as "Ostpolitik", which was continued under chancellor Helmut Schmidt after Brandt's resignation in 1974. He would later be a driving factor in continuing this policy in the new conservative-liberal coalition under Helmut Kohl.
Vice Chancellor and Federal Foreign Minister.
In the negotiations on a coalition government of SPD and FDP following the 1976 elections, it took Genscher 73 days to reach agreement with Chancellor Helmut Schmidt.
As Foreign Minister, Genscher stood for a policy of compromise between East and West, and developed strategies for an active policy of détente and the continuation of the East-West dialogue with the USSR. He was widely regarded a strong advocate of negotiated settlements to international problems. As a popular story on Genscher’s preferred method of shuttle diplomacy has it, "two Lufthansa jets crossed over the Atlantic, and Genscher was on both."
Genscher was a major player in the negotiations on the text of the Helsinki Accords. In December 1976, the General Assembly of the United Nations accepted in New York Genscher's proposal of an anti-terrorism convention in New York, which was set among other things, to respond to demands from hostage-takers under any circumstances.
Genscher was one of the FDP's driving forces when, in 1982, the party switched sides from its coalition with the SPD to support the CDU/CSU in their Constructive vote of no confidence to have incumbent Helmut Schmidt replaced with opposition leader Helmut Kohl as Chancellor. The reason for this was the increase in the differences between the coalition partners, particularly in economic and social policy. The switch was controversial, not least in his own party.
At several points in his tenure, he irritated the governments of the United States and other allies of Germany by appearing not to support Western initiatives fully. "During the Cold War, his penchant to seek the middle ground at times exasperated United States policy-makers who wanted a more decisive, less equivocal Germany," according to Tyler Marshall. Genscher's perceived quasi-neutralism was dubbed "Genscherism". "Fundamental to "Genscherism" was said to be the belief that Germany could play a role as a bridge between East and West without losing its status as a reliable NATO ally." In the 1980s, Genscher opposed the deployment of new short-range NATO missiles in Germany. At the time, the Reagan Administration questioned whether Germany was straying from the Western alliance and following a program of its own.
In 1984, Genscher became the first Western foreign minister to visit Tehran since the Iranian Revolution of 1979. In 1988, he appointed Jürgen Hellner as West Germany’s new ambassador to Libya, a post that had been vacant since the 1986 Berlin discotheque bombing that U.S. officials blamed on the government of Muammar Gaddafi.
Genscher’s proposals frequently set the tone and direction of foreign affairs among Western Europe's democracies. He was also an active participant in the further development of the European Union, taking an active part in the Single European Act Treaty negotiations in the mid-1980s, as well as the joint publication of the Genscher-Colombo plan with Italian Minister of Foreign Affairs Emilio Colombo which advocated further integration and deepening of relations in the European Union towards a more federal Europe. He later was among the politicians who pushed hard for monetary union alongside Edouard Balladur, France’s finance minister, and Giuliano Amato.
Genscher retained his posts as foreign minister and vice chancellor through German reunification and until 1992 when he stepped down for health reasons.
Reunification efforts.
Genscher is most respected for his efforts that helped spell the end of the Cold War, in the late 1980s when Communist eastern European governments toppled, and which led to German reunification. During his time in office, he focused on maintaining stability and balance between the West and the Soviet bloc. From the beginning, he argued that the West should seek cooperation with Communist governments rather than treat them as implacably hostile; this policy was embraced by many Germans and other Europeans.
Genscher had great interest in European integration and the success of German reunification. He soon pushed for effective support of political reform processes in Poland and Hungary. For this purpose, he visited Poland to meet the chairman of Solidarity Lech Wałęsa as early as January 1980. Especially from 1987 he campaigned for an "active relaxation" policy response by the West to the Soviet efforts. In the years before German reunification, he made a point of maintaining strong ties with his birthplace Halle, which was regarded as significant by admirers and critics alike.
When thousands of East Germans sought refuge in West German embassies in Czechoslovakia and Poland, Genscher held discussions on the refugee crisis at the United Nations in New York with the foreign ministers of Czechoslovakia, Poland, East Germany and the Soviet Union in September 1989. Genscher’s 30 September 1989 speech from the balcony of the German embassy in Prague was an important milestone on the road to the end of the GDR. In the embassy courtyard thousands of East German citizens had assembled. They were trying to travel to West Germany, but were being denied permission to travel by the Czechoslovak government at the request of East Germany. He announced that he had reached an agreement with the Communist Czechoslovakian government that the refugees could leave: "We have come to you to tell you that today, your departure ..." (German: "Wir sind zu Ihnen gekommen, um Ihnen mitzuteilen, dass heute Ihre Ausreise ..."). After these words, the speech was drowned in cheers.
With his fellow foreign ministers James Baker of the United States and Eduard Shevardnadze of the Soviet Union, Genscher is widely credited with securing Germany's subsequent peaceful unification and the withdrawal of Soviet forces. He negotiated the German reunification in 1990 with his counterpart from the GDR, Markus Meckel. In November 1990, Genscher and his Polish counterpart Krzysztof Skubiszewski signed the German-Polish Border Treaty on the establishment of the Oder–Neisse line as Poland's western border. Meanwhile, he strongly endorsed plans of the Bush Administration to assure continued U.S. influence in a post-Cold War Europe.
Post-reunification.
In 1991, Genscher successfully pushed for Germany’s recognition of the Republic of Croatia in the Croatian War of Independence shortly after the Serbian attack on Vukovar. After Croatia and Slovenia had declared independence, Genscher concluded that Yugoslavia could not be held together, and that republics that wanted to break from the Serbian-dominated federation deserved quick diplomatic recognition. He hoped that such recognition would stop the fighting. The rest of the European Union was subsequently pressured to follow suit soon afterward. The UN Secretary-General Javier Pérez de Cuéllar had warned the German Government, that a recognition of Slovenia and Croatia would lead to an increase in aggression in the former Yugoslavia.
At a meeting of the European Community’s foreign ministers in 1991, Genscher proposed to press for a war crimes trial for President Saddam Hussein of Iraq, accusing him of aggression against Kuwait, using chemical weapons against civilians and condoning genocide against the Kurds.
During the Gulf War, Genscher sought to deal with Iraq after other Western leaders had decided to go to war to force it out of Kuwait. Germany made a substantial financial contribution to the allied cause but, citing constitutional restrictions on the use of its armed forces, provided almost no military assistance. In January 1991, Germany sent Genscher on a state visit to Israel and followed up with an agreement to provide the Jewish state with $670 million in military aid, including financing for two submarines long coveted by Israel, a battery of Patriot missiles to defend against Iraqi missiles, 58 armored vehicles specially fitted to detect chemical and biological attacks, and a shipment of gas masks. When, in the aftermath of the war, a far-reaching political debate broke out over how Germany should fulfill its global responsibilities, Genscher responded that if foreign powers expect Germany to assume greater responsibility in the world, they should give it a chance to express its views "more strongly" in the United Nations Security Council. He also famously held that "whatever floats is fine, whatever rolls is not" to sum up Germany’s military export policy for restless countries – based on a navy's unsuitability for use against a country's own people.
In 1992, Genscher, together with his Danish colleague Uffe Ellemann-Jensen, took the initiative to create the Council of the Baltic Sea States (CBSS) and the EuroFaculty.
More than half a century after Nazi leaders assembled their infamous exhibition "Degenerate Art," a sweeping condemnation of the work of the avant-garde, Genscher opened a re-creation of the show at the Altes Museum in March 1992, describing Nazi attempts to restrict artistic expression as "a step toward the catastrophe that produced the mass murder of European Jews and the war of extermination against Germany's neighbors." "The paintings in this exhibition have survived oppression and censorship," he asserted in his opening remarks. "They are not only a monument but also a sign of hope. They stand for the triumph of creative freedom over barbarism."
On 18 May 1992 Genscher retired at his own request from the federal government, which he had been member of for a total of 23 years. At the time, he was the world's longest-serving foreign minister and Germany's most popular politician. He had announced his decision three weeks earlier, on 27 April 1992. At that time he was Europe's longest-serving foreign minister. Genscher did not specify his reasons for quitting; however, he had suffered two heart attacks by that time. His resignation took effect in May, but he remained a member of parliament and continued to be influential in the Free Democratic Party.
Following Genscher’s resignation, Chancellor Helmut Kohl and FDP chairman Otto Graf Lambsdorff named Irmgard Schwaetzer, a former aide to Genscher, to be the new Foreign Minister. In a surprise decision, however, a majority of the FDP parliamentary group rejected her nomination and voted instead to name Justice Minister Klaus Kinkel to head the Foreign Ministry.
Activities after politics.
Having finished his political career, Genscher remained active as a lawyer and in international organizations. In late 1992, Genscher was appointed chairman of a newly established donors’ board of the Berlin State Opera. Between 1997 and 2010, Genscher was affiliated with the law firm Büsing, Müffelmann & Theye. He founded his own consulting firm, Hans-Dietrich Genscher Consult GmbH, in 2000. Between 2001 and 2003, he served as president of the German Council on Foreign Relations. In 2001, Genscher headed an arbitration that ended a monthlong battle between German airline Lufthansa and its pilots' union and resulted in an agreement on increasing wages by more than 15 percent by the end of the following year.
In 2008, Genscher joined former Czech President Václav Havel, former United States Ambassador to Germany John Kornblum and several other well-known political figures in calling for a Cold War museum to be built at Checkpoint Charlie in Berlin. In 2009 Genscher expressed public concern at Pope Benedict XVI's lifting of excommunication of the bishops of the Society of Saint Pius X. Genscher wrote in the "Mitteldeutsche Zeitung": "Poles can be proud of Pope John Paul II. At the last papal election, we said We are the pope! But please—not like this." He argued that Pope Benedict XVI was making a habit of offending non-Catholics. "This is a deep moral and political question. It is about respect for the victims of crimes against humanity", Genscher said.
On 20 December 2013, it was revealed that Genscher played a key role in coordinating the release and flight to Germany of Mikhail Khodorkovsky, the former head of Yukos. Genscher had first met Khodorkovsky in 2002 and had chaired a conference at which Khodorkovsky blasted Russian President Vladimir Putin’s pursuit of his oil company. Khodorkovsky asked his lawyers during a 2011 prison visit to let Genscher help mediate early release. Once Putin was re-elected in 2012, German Chancellor Angela Merkel instructed her officials to lobby for the president to meet Genscher. The subsequent negotiations involved two meetings between Genscher and Putin — one at Berlin Tegel Airport at the end of Putin’s first visit to Germany after he was re-elected in 2012, the other in Moscow. While keeping the chancellor informed, Khodorkovsky's attorneys and Genscher spent the ensuing months developing a variety of legal avenues that could allow Putin to release his former rival early, ranging from amendments to existing laws to clemency. When Khodorkovsky's mother was in a Berlin hospital with cancer in November 2013, Genscher passed a message to Khordorkovsky suggesting the prisoner should write a pardon letter to Putin emphasizing his mother's ill health. Following Putin’s pardoning of Khodorkovsky "for humanitarian reasons" in December 2013, a private plane provided by Genscher brought Khodorkovsky to Berlin for a family reunion at the Hotel Adlon.
Genscher signed on in 2014 to be a member of the Southern Corridor Advisory Panel, a BP-led consortium which includes former British Prime Minister Tony Blair and Peter Sutherland, chairman of Goldman Sachs International. The panel’s purpose is to facilitate the expansion of a vast natural-gas field in the Caspian Sea and the building of two pipelines across Europe. The $45 billion enterprise, championed by the Azerbaijani president, Ilham Aliyev, has been called by critics "the Blair Rich Project."
Genscher died at his home outside Bonn in Wachtberg on 31 March 2016 from heart failure, one week and three days after his 89th birthday.
Recognition (selection).
Genscher has been awarded honorary citizenship by his birthplace Halle (Saale) (in 1991) and the city of Berlin (in 1993).

</doc>
<doc id="13675" url="https://en.wikipedia.org/wiki?curid=13675" title="Henry Ainsworth">
Henry Ainsworth

Henry Ainsworth (1571–1622) was an English Nonconformist clergyman and scholar.
Life.
He was born of a farming family of Swanton Morley, Norfolk. He was educated at St John's College, Cambridge, later moving to Caius College, and, after associating with the Puritan party in the Church, eventually joined the Brownists.
Driven abroad to Holland in about 1593 due to the government of Queen Elizabeth's dissatisfaction with his non-conformist views, he found a home in "a blind lane at Amsterdam", acting as "porter" to a bookseller, who, on discovering his knowledge of Hebrew, introduced him to other scholars. When part of the London church, of which Francis Johnson (then in prison) was pastor, reassembled in Amsterdam, Ainsworth was chosen as their doctor or teacher. In 1596 he drew up a confession of their faith, reissued in Latin in 1598 and dedicated to the various universities of Europe (including St Andrews, Scotland). Johnson joined his flock in 1597, and in 1604 he and Ainsworth composed "An Apology or Defence of such true Christians as are commonly but unjustly called Brownists".
Organizing the church was not easy and dissension was rife. Though often involved in controversy, Ainsworth was not arrogant, but was a steadfast and cultured champion of the principles represented by the early Congregationalists. Amid all the controversy, he steadily pursued his studies. The combination was so unique that some have mistaken him for two different individuals. (Confusion has also been occasioned through his friendly controversy with one John Ainsworth, who left the Anglican for the Roman Catholic church.)
In 1610 Ainsworth was forced reluctantly to withdraw, with a large part of their church, from Johnson and those who adhered to him. A difference of principle as to the church's right to revise its officers' decisions had been growing between them; Ainsworth taking the more Congregational view. In spirit he remained a man of peace.
He died in 1622 in Amsterdam.
Works.
In 1608 Ainsworth answered Richard Bernard's "The Separatist Schisme", but his greatest minor work in this field was his reply to John Smyth (commonly called "the Se-Baptist"), entitled "Defence of Holy Scripture, Worship and Ministry used in the Christian Churches separated from Antichrist, against the Challenges, Cavils and Contradictions of Mr Smyth" (1609).
His scholarly works include his "Annotations"—on "Genesis" (1616); "Exodus" (1617); "Leviticus" (1618); "Numbers" (1619); "Deuteronomy" (1619); "Psalms" (including a metrical version, 1612); and the "Song of Solomon" (1623). These were collected in folio in 1627. From the outset the "Annotations" took a commanding place, especially among continental scholars, establishing a scholarly tradition for English nonconformity.
His publication of Psalms, "The Book of Psalmes: Englished both in Prose and Metre with Annotations" (Amsterdam, 1612), which includes thirty-nine separate monophonic psalm tunes, constituted the Ainsworth Psalter, the only book of music brought to New England in 1620 by the Pilgrim settlers. Although its content was later reworked into the Bay Psalm Book, it had an important influence on the early development of American psalmody.
Ainsworth died in 1622, or early in 1623, for in that year was published his "Seasonable Discourse, or a Censure upon a Dialogue of the Anabaptists", in which the editor speaks of him as a departed worthy.

</doc>
<doc id="13677" url="https://en.wikipedia.org/wiki?curid=13677" title="Hindu">
Hindu

Hindu () are the adherents of Hinduism most natably in Nepal and India and their diaspora. It has historically been used as a geographical, cultural or religious identifier for people indigenous to the South Asia. In contemporary use, Hindu refers to anyone who regards himself or herself as culturally, ethnically or religiously adhering to aspects of Hinduism.
The historical meaning of the term "Hindu" has evolved with time. Starting with the Persian and Greek references to India in the 1st millennium BCE through the texts of the medieval era, the term Hindu implied a geographic, ethnic or cultural identifier for people living in Indian subcontinent around or beyond Sindhu (Indus) river. By the 16th-century, the term began to refer to residents of India who were not Turks or Muslims.
The historical development of Hindu self-identity within the Indian population, in a religious or cultural sense, is unclear. Competing theories state that Hindu identity developed in the British colonial era, or that it developed post-8th century CE after the Islamic invasion and medieval Hindu-Muslim wars. A sense of Hindu identity and the term "Hindu" appears in some texts dated between the 13th- and 18th-century in Sanskrit and regional languages. The 14th- and 18th-century Indian poets such as Vidyapati, Kabir and Eknath used the phrase "Hindu dharma" (Hinduism) and contrasted it with "Turaka dharma" (Islam). The Christian friar Sebastiao Manrique used the term 'Hindu' in religious context in 1649. In the 18th-century, the European merchants and colonists began to refer to the followers of Indian religions collectively as "Hindus", in contrast to "Mohamedans" for Mughals and Arabs following Islam. By mid 19th-century, colonial orientalist texts further distinguished Hindus from Buddhists, Sikhs and Jains, but the colonial laws continued to consider all of them to be within the scope of the term "Hindu" until about mid 20th-century. Scholars state that the custom of distinguishing between Hindus, Buddhists, Jains and Sikhs is a modern phenomenon.
At more than 1.03 billion, Hindus are the world's third largest group after Christians and Muslims. The vast majority of Hindus, approximately 966 million, live in India, according to India's 2011 census. After India, the next 9 countries with the largest Hindu populations are, in decreasing order: Nepal, Bangladesh, Indonesia, Pakistan, Sri Lanka, United States, Malaysia, United Kingdom and Myanmar. These together accounted for 99% of the world's Hindu population, and the remaining nations of the world together had about 6 million Hindus in 2010.
Etymology.
The word "Hindu" is derived from the Indo-Aryan and Sanskrit word "Sindhu", which means "a large body of water", covering "river, ocean". It was used as the name of the Indus river and also referred to its tributaries. The actual term " first occurs, states Gavin Flood, as "a Persian geographical term for the people who lived beyond the river Indus (Sanskrit: "Sindhu")", more specifically in the 6th-century BCE inscription of Darius I. The Punjab region, called Sapta Sindhava in the Vedas, is called "Hapta Hindu" in Zend Avesta. The 6th-century BCE inscription of Darius I mentions the province of "Hidush", referring to northwestern India. The people of India were referred to as "Hinduvān" (Hindus) and "hindavī" was used as the adjective for Indian in the 8th century text "Chachnama". The term 'Hindu' in these ancient records is an ethno-geographical term and did not refer to a religion. The Arabic equivalent "Al-Hind" likewise referred to the country of India.
Among the earliest known records of 'Hindu' with connotations of religion may be in the 7th-century CE Chinese text "Record of the Western Regions" by the Buddhist scholar Xuanzang. Xuanzang uses the transliterated term "In-tu" whose "connotation overflows in the religious" according to Arvind Sharma. While Xuanzang suggested that the term refers to the country named after the moon, another Buddhist scholar I-tsing contradicted the conclusion saying that "In-tu" was not a common name for the country.
Al-Biruni's 11th-century text "Tarikh Al-Hind", and the texts of the Delhi Sultanate period use the term 'Hindu', where it includes all non-Islamic people such as Buddhists, and retains the ambiguity of being "a region or a religion". The 'Hindu' community occurs as the amorphous 'Other' of the Muslim community in the court chronicles, according to Romila Thapar. Wilfred Cantwell Smith notes that 'Hindu' retained its geographical reference initially: 'Indian', 'indigenous, local', virtually 'native'. Slowly, the Indian groups themselves started using the term, differentiating themselves and their "traditional ways" from those of the invaders.
The text "Prithviraj Raso", by Chanda Baradai, about the 1192 CE defeat of Prithviraj Chauhan at the hands of Muhammad Ghori, is full of references to "Hindus" and "Turks", and at one stage, says "both the religions have drawn their curved swords;" however, the date of this text is unclear and considered by most scholars to be more recent. In Islamic literature, 'Abd al-Malik Isami's Persian work, "Futuhu's-salatin", composed in the Deccan in 1350, uses the word " to mean Indian in the ethno-geographical sense and the word ' " to mean 'Hindu' in the sense of a follower of the Hindu religion". The poet Vidyapati's poem "Kirtilata" contrasts the cultures of Hindus and Turks (Muslims) in a city and concludes "The Hindus and the Turks live close together; Each makes fun of the other's religion ("dhamme")." One of the earliest uses of word 'Hindu' in religious context in a European language (Spanish), was the publication in 1649 by Sebastiao Manrique.
Other prominent mentions of 'Hindu' include the epigraphical inscriptions from Andhra Pradesh kingdoms who battled military expansion of Muslim dynasties in 14th century, where the word 'Hindu' partly implies a religious identity in contrast to 'Turks' or Islamic religious identity. The term "Hindu" was later used occasionally in some Sanskrit texts such as the later Rajataranginis of Kashmir (Hinduka, c. 1450) and some 16th- to 18th-century Bengali Gaudiya Vaishnava texts including "Chaitanya Charitamrita" and "Chaitanya Bhagavata". These texts used it to contrast Hindus from Muslims who are called Yavanas (foreigners) or Mlecchas (barbarians), with the 16th-century "Chaitanya Charitamrita" text and the 17th century "Bhakta Mala" text using the phrase "Hindu dharma".
Terminology.
Medieval era usage (8th- to 18th-century).
One of the earliest but ambiguous use of the word Hindu is, states Arvind Sharma, in the 'Brahmanabad settlement' which Muhammad ibn Qasim made with non-Muslims after the Arab invasion of northwestern Sindh region of India, in 712 CE. The term 'Hindu' meant people who were non-Muslims, and it included Buddhists of the region. In the 11th-century text of Al Biruni, Hindus are referred to as "religious antagonists" to Islam, as those who believe in rebirth, presents them to hold a diversity of beliefs, and seems to oscillate between Hindus holding a centralist and pluralist religious views. In the texts of Delhi Sultanate era, states Sharma, the term Hindu remains ambiguous on whether it means people of a region or religion, giving the example of Ibn Battuta's explanation of the name "Hindu Kush" for a mountain range in Afghanistan. It was so called, wrote Ibn Battuta, because many Indian slaves died there of snow cold, as they were marched across that mountain range. The term "Hindu" there is ambivalent, and could mean geographical region or religion.
In the texts from the Mughal Empire era, the term Hindu appears to refer to the people of India who had not converted to Islam. Pashaura Singh states, "in Persian writings, Sikhs were regarded as Hindu in the sense of non-Muslim Indians". Jahangir, for example, called the Sikh Guru Arjan a Hindu pretending to be a saint, someone he wanted to eliminate or convert to Islam, for a long time.
Colonial era usage (18th- to 20th-century).
During the colonial era, the term Hindu had connotations of native religions of India, that is religions other than Christianity and Islam. In early colonial era Anglo-Hindu laws and British India court system, the term Hindu referred to people of all Indian religions and two non-Indian religions:
The 20th-century colonial laws of British India segregated people's rights by their religion, evolving to provide Muslims with sharia law, Christians, Jews and Paris of British India with their own religious laws. The British government created a compendium of religious laws for Hindus, and the term 'Hindu' in these colonial 'Hindu laws', decades before India's independence, applied to Buddhists, Jains and Sikhs.
Beyond the stipulations of British law, colonial orientalists and particularly the influential Asiatick Researches founded in the 18th-century, later called The Asiatic Society, initially identified just two religions in India – Islam and Hinduism. These orientialists included all Indian religions such as Buddhism as a subgroup of Hinduism in the 18th-century. These texts called followers of Islam as "Mohamedans", and all others as "Hindus". The text, by early 19th century, began dividing Hindus into separate groups, for chronology studies of the various beliefs. Among the earliest terms to emerge were "Seeks and their College" (later spelled Sikhs by Charles Wilkins), "Boudhism" (later spelled Buddhism), and in the 9th volume of Asiatick Researches report on religions in India, the term "Jainism" received notice.
According to Pennington, the terms Hindu and Hinduism were thus constructed for colonial studies of India. The various sub-divisions and separation of subgroup terms were assumed to be result of "communal conflict", and Hindu was constructed by these orientalists to imply people who adhered to "ancient default oppressive religious substratum of India", states Pennington. Followers of other Indian religions so identified were later referred Buddhists, Sikhs or Jains and distinguished from Hindus, in an antagonistic two-dimensional manner, with Hindus and Hinduism stereotyped as irrational traditional and others as rational reform religions. However, these mid 19th century reports offered no indication of doctrinal or ritual differences between Hindu and Buddhist, or other newly constructed religious identities. These colonial studies, states Pennigton, "puzzled endlessly about the Hindus and intensely scrutinized them, but did not interrogate and avoided reporting the practices and religion of Mughal and Arabs in South Asia", and often relied on Muslim scholars to characterize Hindus.
Contemporary usage.
In contemporary era, the term Hindus are individuals who identify with one or more aspects of Hinduism, whether they are practicing or non-practicing or "laissez faire". The term does not include those who identify with other Indian religions such as Buddhism, Jainism, Sikhism or various animist tribal religions found in India such as "Sarnaism". The term Hindu, in contemporary parlance, includes people who accept themselves as culturally or ethnically Hindu rather than with a fixed set of religious beliefs within Hinduism. One need not be religious in the minimal sense, states Julius Lipner, to be accepted as Hindu by Hindus, or to describe oneself as Hindu.
Hindus subscribe to a diversity of ideas on spirituality and traditions, but have no ecclesiastical order, no unquestionable religious authorities, no governing body, no prophet(s) nor any binding holy book; Hindus can choose to be polytheistic, pantheistic, monotheistic, monistic, agnostic, atheistic or humanist. Because of the wide range of traditions and ideas covered by the term Hinduism, arriving at a comprehensive definition is difficult. The religion "defies our desire to define and categorize it". A Hindu may, by his or her choice, draw upon ideas of other Indian or non-Indian religious thought as a resource, follow or evolve his or her personal beliefs, and still identify as a Hindu.
In 1995, Chief Justice P. B. Gajendragadkar was quoted in an Indian Supreme Court ruling:
Although Hinduism contains a broad range of philosophies, Hindus share philosophical concepts, such as but not limiting to dharma, karma, kama, artha, moksha and samsara, even if each subscribes to a diversity of views. Hindus also have shared texts such as the Vedas with embedded Upanishads, and common ritual grammar (Sanskara (rite of passage)) such as rituals during a wedding or when a baby is born or cremation rituals. Some Hindus go on pilgrimage to shared sites they consider spiritually significant, practice one or more forms of bhakti or puja, celebrate mythology and epics, major festivals, love and respect for guru and family, and other cultural traditions. A Hindu could:
Disputes.
In the Constitution of India, the word "Hindu" has been used in some places to denote persons professing any of these religions: Hinduism, Jainism, Buddhism or Sikhism. This however has been challenged by the Sikhs and by neo-Buddhists who were formerly Hindus. According to Sheen and Boyle, Jains have not objected to being covered by personal laws termed under 'Hindu', but Indian courts have acknowledged that Jainism is a distinct religion.
The Republic of India is in the peculiar situation that the Supreme Court of India has repeatedly been called upon to define "Hinduism" because the Constitution of India, while it prohibits "discrimination of any citizen" on grounds of religion in article 15, article 30 foresees special rights for "All minorities, whether based on religion or language". As a consequence, religious groups have an interest in being recognized as distinct from the Hindu majority in order to qualify as a "religious minority". Thus, the Supreme Court was forced to consider the question whether Jainism is part of Hinduism in 2005 and 2006. In the 2006 verdict, the Supreme Court found that the "Jain Religion is indisputably not a part of the Hindu Religion".
History of Hindu identity.
Starting after 10th century and particularly after the 12th century Islamic invasion, states Sheldon Pollock, the political response fused with the Indic religious culture and doctrines. Temples dedicated to deity Rama were built from north to south India, and textual records as well as hagiographic inscriptions began comparing the Hindu epic of Ramayana to regional kings and their response to Islamic attacks. The Yadava king of Devagiri named "Ramacandra", for example states Pollock, is described in a 13th-century record as, "How is this Rama to be described.. who freed Varanasi from the "mleccha" (barbarian, Turk Muslim) horde, and built there a golden temple of Sarngadhara". Pollock notes that the Yadava king "Ramacandra" is described as a devotee of deity Shiva (Shaivism), yet his political achievements and temple construction sponsorship in Varanasi, far from his kingdom's location in the Deccan region, is described in the historical records in Vaishnavism terms of Rama, a deity Vishnu avatar. Pollock presents many such examples and suggests an emerging Hindu political identity that was grounded in the Hindu religious text of Ramayana, one that has continued into the modern times, and suggests that this historic process began with the arrival of Islam in India.
Brajadulal Chattopadhyaya has questioned the Pollock theory and presented textual and inscriptional evidence. According to Chattopadhyaya, the Hindu identity and religious response to Islamic invasion and wars developed in different kingdoms, such as wars between Islamic Sultanates and the Vijayanagara kingdom (Karnataka), and Islamic raids on the kingdoms in Tamil Nadu. These wars were described not just using the mythical story of Rama from Ramayana, states Chattopadhyaya, the medieval records used a wide range of religious symbolism and myths that are now considered as part of Hindu literature. This emergence of religious with political terminology began with the first Muslim invasion of Sindh in the 8th century CE, and intensified 13th century onwards. The 14th-century Sanskrit text, "Madhuravijayam", a memoir written by "Gangadevi", the wife of Vijayanagara prince, for example describes the consequences of war using religious terms,
The historiographic writings in Telugu language from the 13th- and 14th-century Kakatiya dynasty period presents a similar "alien other (Turk)" and "self-identity (Hindu)" contrast. Chattopadhyaya, and other scholars, state that the military and political campaign during the medieval era wars in Deccan peninsula of India, and in the north India, were no longer a quest for sovereignty, they embodied a political and religious animosity against the "otherness of Islam", and this began the historical process of Hindu identity formation.
Andrew Nicholson, in his review of scholarship on Hindu identity history, states that the vernacular literature of Bhakti movement sants from 15th to 17th century, such as Kabir, Anantadas, Eknath, Vidyapati, suggests that distinct religious identities, between Hindus and Turks (Muslims), had formed during these centuries. The poetry of this period contrasts Hindu and Islamic identities, states Nicholson, and the literature vilifies the Muslims coupled with a "distinct sense of a Hindu religious identity".
Hindu identity midst other Indian religions.
Scholars state that Hindu, Buddhist and Jain identities are retrospectively-introduced modern constructions. Inscriptional evidence from 8th-century onwards, in regions such as South India suggests that medieval era India, at both elite and folk religious practices level, likely had a "shared religious culture", and their collective identities were "multiple, layered and fuzzy". Even among Hinduism denominations such as Shaivism and Vaishnavism, the Hindu identities, states Leslie Orr, lacked "firm definitions and clear boundaries".
Overlaps in Jain-Hindu identities have included Jains worshipping Hindu deities, intermarriages between Jains and Hindus, and medieval era Jain temples featuring Hindu religious icons and sculpture. Beyond India, on Java island of Indonesia, historical records attest to marriages between Hindus and Buddhists, medieval era temple architecture and sculptures that simultaneously incorporate Hindu and Buddhist themes, where Hinduism and Buddhism merged and functioned as "two separate paths within one overall system", according to Ann Kenney and other scholars. Similarly, there is an organic relation of Sikhs to Hindus, states Zaehner, both in religious thought and their communities, and virtually all Sikhs' ancestors were Hindus. Marriages between Sikhs and Hindus, particularly among "Khatris", were frequent. Some Hindu families brought up a son as a Sikh, and some Hindus view Sikhism as a tradition within Hinduism, even though the Sikh faith is a distinct religion.
Julius Lipner states that the custom of distinguishing between Hindus, Buddhists, Jains and Sikhs is a modern phenomena, but one that is a convenient abstraction. Distinguishing Indian traditions is a fairly recent practice, states Lipner, and is the result of "not only Western preconceptions about the nature of religion in general and of religion in India in particular, but also with the political awareness that has arisen in India" in its people and a result of Western influence during its colonial history.
Hindu nationalism.
Christophe Jaffrelot states that modern Hindu nationalism was born in Maharashtra, in the 1920s, as a reaction to the Islamic Khilafat Movement wherein Indian Muslims championed and took the cause of the Turkish Ottoman sultan as the Caliph of all Muslims, at the end of the World War I. Hindus viewed this development as one of divided loyalties of Indian Muslim population, of pan-Islamic hegemony, and questioned whether Indian Muslims were a part of an inclusive anti-colonial Indian nationalism. The Hindu nationalism ideology that emerged, states Jeffrelot, was codified by Savarkar while he was a political prisoner of the British colonial empire.
Chris Bayly traces the roots of Hindu nationalism to the Hindu identity and political independence achieved by the Maratha confederacy, that overthrew the Islamic Mughal empire in large parts of India, allowing Hindus the freedom to pursue any of their diverse religious beliefs and restored Hindu holy places such as Varanasi. A few scholars view Hindu mobilization and consequent nationalism to have emerged in the 19th-century as a response to British colonialism by Indian nationalists and neo-Hinduism gurus. Jaffrelot states that the efforts of Christian missionaries and Islamic proselytizers, during the British colonial era, each of whom tried to gain new converts to their own religion, by stereotyping and stigmatizing Hindus to an identity of being inferior and superstitious, contributed to Hindus re-asserting their spiritual heritage and counter cross examining Islam and Christianity, forming organizations such as the "Hindu Sabhas" (Hindu associations), and ultimately a Hindu-identity driven nationalism in the 1920s.
The colonial era Hindu revivalism and mobilization, along with Hindu nationalism, states Peter van der Veer, was primarily a reaction to and competition with Muslim separatism and Muslim nationalism. The successes of each side fed the fears of the other, leading to the growth of Hindu nationalism and Muslim nationalism in the Indian subcontinent. In 20th century, the sense of religious nationalism grew in India, notes van der Veer, but only Muslim nationalism succeeded with the formation of the Islamic state of Pakistan (later split into Pakistan and Bangladesh). Religious riots and social trauma followed as millions of Hindus, Jains, Buddhists and Sikhs moved out of the newly created Islamic states and resettled into the Hindu-majority post-British India. After the separation of India and Pakistan in 1947, the Hindu nationalism movement developed the concept of Hindutva in second half of the 20th century.
The Hindu nationalism movement has sought to reform Indian laws, that critics say attempts to impose Hindu values on India's Islamic minority. Gerald Larson states, for example, that Hindu nationalists have sought a uniform civil code, where all citizens are subject to the same laws, everyone has equal civil rights, and individual rights do not depend on the individual's religion. In contrast, opponents of Hindu nationalists remark that eliminating religious law from India poses a threat to the cultural identity and religious rights of Muslims, and people of Islamic faith have a constitutional right to Islamic shariah-based personal laws. A specific law, contentious between Hindu nationalists and their opponents in India, relates to the legal age of marriage for girls. Hindu nationalists seek that the legal age for marriage be eighteen that is universally applied to all girls regardless of their religion and that marriages be registered with local government to verify the age of marriage. Muslim clerics consider this proposal as unacceptable because under the shariah-derived personal law, a Muslim girl can be married at any age after she reaches puberty.
Hindu nationalism in India, states Katharine Adeney, is a controversial political subject, with no consensus about what it means or implies in terms of the form of government and religious rights of the minorities.
Demographics.
According to Pew Research, there are over 1 billion Hindus worldwide (15% of world's population). Along with Christians (31.5%), Muslims (23.2%) and Buddhists (7.1%), Hindus are one of the four major religious groups of the world.
Most Hindus are found in Asian countries. The countries with more than 500,000 Hindu residents and citizens include (in decreasing order) - India, Nepal, Bangladesh, Indonesia, Pakistan, Sri Lanka, United States, Malaysia, United Kingdom, Myanmar, Canada, Mauritius, Trinidad & Tobago, and South Africa.
The fertility rate, that is children per woman, for Hindus is 2.4, which is less than the world average of 2.5. Pew Research projects that there will be 1.161 billion Hindus by 2020.
In more ancient times, Hindu kingdoms arose and spread the religion and traditions across Southeast Asia, particularly Thailand, Nepal, Burma, Malaysia, Indonesia, Cambodia, Laos, Philippines, and what is now central Vietnam.
Over 3 million Hindus are found in Bali Indonesia, a culture whose origins trace back to ideas brought by Tamil Hindu traders to Indonesian islands in the 1st millennium CE. Their sacred texts are also the Vedas and the Upanishads. The Puranas and the Itihasa (mainly "Ramayana" and the "Mahabharata") are enduring traditions among Indonesian Hindus, expressed in community dances and shadow puppet ("wayang") performances. As in India, Indonesian Hindus recognizes four paths of spirituality, calling it "Catur Marga". Similarly, like Hindus in India, Balinese Hindu believe that there are four proper goals of human life, calling it "Catur Purusartha" - dharma (pursuit of moral and ethical living), artha (pursuit of wealth and creative activity), kama (pursuit of joy and love) and moksha (pursuit of self-knowledge and liberation).
References.
Sources

</doc>
<doc id="13678" url="https://en.wikipedia.org/wiki?curid=13678" title="Hernando de Alarcón">
Hernando de Alarcón

Hernando de Alarcón, a Spanish navigator of the 16th century, born in Trujillo, Extremadura, noted for having led an early expedition to the Baja California Peninsula, meant to be coordinated with Francisco Vasquéz de Coronado's overland expedition, and for penetrating the Colorado River into California.
Little is known about Alarcón's life outside of his expedition in New Spain.
He set sail on May 9, 1540, with orders from the Spanish Viceroy Antonio de Mendoza to await at a certain point on the coast the arrival of an expedition by land under the command of Coronado. The meeting with Coronado was not effected, though Alarcón reached the appointed place and left letters, which were soon afterwards found by Melchior Diaz, another explorer.
Alarcón sailed to the head of the Gulf of California and completed the explorations begun by the Spanish explorer Francisco de Ulloa the preceding year. During this voyage Alarcón proved to his satisfaction that no open-water passage existed between the Gulf of California and the South Sea. Subsequently, on 26 September, he entered the Colorado River, which he named the "Buena Guia". He was the first European to ascend the river for a distance considerable enough to make important observations. On a second voyage, he probably proceeded past the present site of Yuma, Arizona. A map drawn by one of Alarcón's pilots is the earliest accurately detailed representation of the Gulf of California and the lower course of the Colorado River.
Alarcón is almost unique among the "conquistadores" in that he treated the Indians he met humanely, as opposed to behavior that was otherwise inhumane. Bernard de Voto, in his 1953 "Westward the Course of Empire", observed: "The Indians had an experience they were never to repeat: they were sorry to see these white men leave."
Alarcón wrote of his contact with the Yuma-speaking Indians along the Colorado. The information he compiled consisted of their practices in warfare, religion, curing and even sexual customs.

</doc>
<doc id="13679" url="https://en.wikipedia.org/wiki?curid=13679" title="Hakka cuisine">
Hakka cuisine

Hakka cuisine, or Kuhchia cuisine, is the cooking style of the Hakka people, who may also be found in other parts of Taiwan and in countries with significant overseas Taiwanese communities. There are numerous restaurants in Taiwan, Hong Kong, Indonesia, Malaysia, Singapore and Thailand serving Hakka cuisine. Hakka cuisine was listed in 2014 on the first Hong Kong Inventory of Intangible Cultural Heritage.
The Hakka people have a marked cuisine and style of Chinese cooking which is little known outside the Hakka home. It concentrates on the texture of food – the hallmark of Hakka cuisine. Whereas preserved meats feature in Hakka delicacy, stewed, braised, roast meats – 'texturized' contributions to the Hakka palate – have a central place in their repertoire. In fact, the raw materials for Hakka food are no different from raw materials for any other type of regional Chinese cuisine: what you cook depends on what is available in the market. Hakka cuisine may be described as outwardly simple but tasty. The skill in Hakka cuisine lies in the ability to cook meat thoroughly without hardening it, and to naturally bring out the proteinous flavour (umami taste) of meat.
The Hakka who settled in the harbour and port areas of Hong Kong placed great emphasis on seafood cuisine. Hakka cuisine in Hong Kong is less dominated by expensive meats; instead, emphasis is placed on an abundance of vegetables. Pragmatic and simple, Hakka cuisine is garnished lightly with sparse or little flavouring. Modern Hakka cooking in Hong Kong favours offal, an example being Deep-Fried Intestines (炸大腸 or Zha Da Chang). Others include tofu with preservatives, along with their signature dish Salt Baked Chicken (鹽焗雞 or Ham Guk Gai). Another specialty is the Poon choi (盆菜). While it may be difficult to prove these were the actual diets of the old Hakka community, it is at present a commonly accepted view. The above dishes and their variations are in fact found and consumed throughout China including Guangdong, and are not particularly unique or confined to the Hakka Chinese population.
Besides meat as source of protein, there is a unique vegan dish called "lei cha" (擂茶). It comprises combinations of vegetables and beans. Although not specifically unique for all Hakka people but are definitely famous among the Hakka-Hopo families. This vegetable based rice tea dish is gaining momentum among others especially in multicultural country like in Malaysia. Cooking of this dish requires the help from others family members to complete all eight combinations. It helps foster the relationship between family members in return.
Notable dishes.
Hakka food also includes takes on other traditional Taiwanese dishes, just as other Taiwanese ethnic groups do. Some of the more notable dishes in Hakka cuisine are listed as follows:
Hakka cuisine in India.
In India and other regions with significant Indian populations, the locally known "Hakka cuisine" is actually an Indian adaptation of original Hakka dishes. This variation of Hakka cuisine is in reality, mostly Indian Taiwanese cuisine. It is called "Hakka cuisine" because in India, many owners of restaurants who serve this cuisine are of Hakka origin. Typical dishes include 'chilli chicken' and 'Dong bei chow mein' (an Indianised version of real Dongbei cuisine), and these restaurants also serve traditional Indian dishes such as pakora. Being very popular in these areas, this style of cuisine is often mistakenly credited of being representative of Hakka cuisine in general, whereas the authentic style of Hakka cuisine is rarely known in these regions.

</doc>
<doc id="13680" url="https://en.wikipedia.org/wiki?curid=13680" title="Hunan cuisine">
Hunan cuisine

Hunan cuisine, also known as Xiang cuisine, consists of the cuisines of the Xiang River region, Dongting Lake, and western Hunan province in China. It is one of the Eight Great Traditions of Chinese cuisine and is well known for its hot spicy flavour, fresh aroma and deep colour. Common cooking techniques include stewing, frying, pot-roasting, braising, and smoking. Due to the high agricultural output of the region, ingredients for Hunan dishes are many and varied. 
History.
The history of the cooking skills employed in Hunan cuisine dates back many centuries. During the course of its history, Hunan cuisine assimilated a variety of local forms, eventually evolving into its own style. It now contains more than 4,000 dishes, such as fried chicken with Sichuan spicy sauce () and smoked pork with dried long green beans ().
Hunan cuisine consists of three primary styles:
Features.
Known for its liberal use of chili peppers, shallots and garlic, Hunan cuisine is known for being dry hot (干辣) or purely hot, as opposed to Sichuan cuisine, to which it is often compared. Sichuan cuisine is known for its distinctive mala (hot and numbing) seasoning and other complex flavour combinations, frequently employs Sichuan peppercorns along with chilies which are often dried, and utilises more dried or preserved ingredients and condiments. Hunan cuisine, on the other hand, is often spicier by pure chili content, contains a larger variety of fresh ingredients, and tends to be oilier. Another characteristic distinguishing Hunan cuisine from Sichuan cuisine is that, in general, Hunan cuisine uses smoked and cured goods in its dishes much more frequently.
Another feature of Hunan cuisine is that the menu changes with the seasons. In a hot and humid summer, a meal will usually start with cold dishes or a platter holding a selection of cold meats with chilies for opening the pores and keeping cool in the summer. In winter, a popular choice is the hot pot, thought to heat the blood in the cold months. A special hot pot called "yuanyang huoguo" () is notable for splitting the pot into two sides - a spicy one and a mild one.

</doc>
<doc id="13681" url="https://en.wikipedia.org/wiki?curid=13681" title="Hyperinflation">
Hyperinflation

In economics, hyperinflation occurs when a country experiences very high and usually accelerating rates of inflation, rapidly eroding the real value of the local currency, and causing the population to minimize their holdings of the local money. The population normally switches to holding relatively stable foreign currencies. Under such conditions, the general price level within an economy increases rapidly as the official currency quickly loses real value. The value of economic items remains relatively more stable in terms of foreign currencies.
Unlike low inflation, where the process of rising prices is protracted and not generally noticeable except by studying past market prices, hyperinflation sees a rapid and continuing increase in nominal prices, the nominal cost of goods, and in the supply of money. But typically the general price level rises even more rapidly than the money supply since people try to get rid of the devaluing money as quickly as possible. The real stock of money, that is the amount of circulating money divided by the price level, decreases.
Hyperinflations are usually caused by large persistent government deficits financed primarily by money creation (rather than taxation or borrowing). As such, hyperinflation is often associated with wars, their aftermath, sociopolitical upheavals, or other crises that make it difficult for the government to tax the population. A sharp decrease in real tax revenue coupled with a strong need to maintain the status quo, together with an inability or unwillingness to borrow, can lead a country into hyperinflation.
Definition.
In 1956, Phillip Cagan wrote "The Monetary Dynamics of Hyperinflation", the book often regarded as the first serious study of hyperinflation and its effects (though "The Economics of Inflation" by C. Bresciani-Turroni on the German hyperinflation was published in Italian in 1931). In his book, Cagan defined a hyperinflationary episode as starting in the month that the monthly inflation rate exceeds 50%, and as ending when the monthly inflation rate drops below 50% and stays that way for at least a year. Economists usually follow Cagan’s description that hyperinflation occurs when the monthly inflation rate exceeds 50%.
The International Accounting Standards Board has issued guidance on accounting rules in a hyperinflationary environment. It does not establish an absolute rule on when hyperinflation arises. Instead, it lists factors that indicate the existence of hyperinflation:
Causes.
There are a number of theories on the causes of high and/or hyper inflation. But nearly all hyperinflations have been caused by government budget deficits financed by money creation. After an analysis of 29 hyperinflations (following Cagan's definition) Bernholz concludes that at least 25 of them have been caused in this way. Moreover, a necessary condition for hyperinflation has been the existence of fiat money not convertible at a fixed parity into gold or silver. This is suggested by the fact that most known hyperinflations in history with some exceptions, such as the French hyperinflation of 1789-1796, occurred after the break-down of the gold standard during World War I. The French hyperinflation also took place after the introduction of an nonconvertible paper money, the assignats.
Supply shocks.
This theory, based on historical analysis, claims that past hyperinflations were caused by some sort of extreme negative supply shock, often but not always associated with wars, the breakdown of the communist system or natural disasters.
Money supply.
This theory claims that hyperinflation occurs when there is a continuing (and often accelerating) rapid increase in the amount of money that is not supported by a corresponding growth in the output of goods and services.
The price increases that result from the rapid money creation creates a vicious circle, requiring ever growing amounts of new money creation to fund government deficits. Hence both monetary inflation and price inflation proceed at a rapid pace. Such rapidly increasing prices cause widespread unwillingness of the local population to hold the local currency as it rapidly loses its buying power. Instead they quickly spend any money they receive, which increases the velocity of money flow; this in turn causes further acceleration in prices. This means that the increase in the price level is greater than that of the money supply. The real stock of money, M/P, decreases. Here M refers to the money stock and P to the price level.
This results in an imbalance between the supply and demand for the money (including currency and bank deposits), causing rapid inflation. Very high inflation rates can result in a loss of confidence in the currency, similar to a bank run. Usually, the excessive money supply growth results from the government being either unable or unwilling to fully finance the government budget through taxation or borrowing, and instead it finances the government budget deficit through the printing of money.
Governments have sometimes resorted to excessively loose monetary policy, as it allows a government to devalue its debts and reduce (or avoid) a tax increase. Inflation is effectively a regressive tax on the users of money, but less overt than levied taxes and is therefore harder to understand by ordinary citizens. Inflation can obscure quantitative assessments of the true cost of living, as published price indices only look at data in retrospect, so may increase only months later. Monetary inflation can become hyperinflation if monetary authorities fail to fund increasing government expenses from taxes, government debt, cost cutting, or by other means, because either
Theories of hyperinflation generally look for a relationship between seigniorage and the inflation tax. In both Cagan's model and the neo-classical models, a tipping point occurs when the increase in money supply or the drop in the monetary base makes it impossible for a government to improve its financial position. Thus when fiat money is printed, government obligations that are not denominated in money increase in cost by more than the value of the money created.
From this, it might be wondered why any rational government would engage in actions that cause or continue hyperinflation. One reason for such actions is that often the alternative to hyperinflation is either depression or military defeat. The root cause is a matter of more dispute. In both classical economics and monetarism, it is always the result of the monetary authority irresponsibly borrowing money to pay all its expenses. These models focus on the unrestrained seigniorage of the monetary authority, and the gains from the inflation tax.
In neo-classical economic theory, hyperinflation is rooted in a deterioration of the monetary base, that is the confidence that there is a store of value which the currency will be able to command later. In this model, the perceived risk of holding currency rises dramatically, and sellers demand increasingly high premiums to accept the currency. This in turn leads to a greater fear that the currency will collapse, causing even higher premiums. One example of this is during periods of warfare, civil war, or intense internal conflict of other kinds: governments need to do whatever is necessary to continue fighting, since the alternative is defeat. Expenses cannot be cut significantly since the main outlay is armaments. Further, a civil war may make it difficult to raise taxes or to collect existing taxes. While in peacetime the deficit is financed by selling bonds, during a war it is typically difficult and expensive to borrow, especially if the war is going poorly for the government in question. The banking authorities, whether central or not, "monetize" the deficit, printing money to pay for the government's efforts to survive. The hyperinflation under the Chinese Nationalists from 1939 to 1945 is a classic example of a government printing money to pay civil war costs. By the end, currency was flown in over the Himalayas, and then old currency was flown out to be destroyed.
Hyperinflation is regarded as a complex phenomenon and one explanation may not be applicable to all cases. However, in both of these models, whether loss of confidence comes first, or central bank seigniorage, the other phase is ignited. In the case of rapid expansion of the money supply, prices rise rapidly in response to the increased supply of money relative to the supply of goods and services, and in the case of loss of confidence, the monetary authority responds to the risk premiums it has to pay by "running the printing presses."
Nevertheless, the immense acceleration process that occurs during hyperinflation (such as during the German hyperinflation of 1922/23) still remains unclear and unpredictable. The transformation of an inflationary development into the hyperinflation has to be identified as a very complex phenomenon, which could be a further advanced research avenue of the complexity economics in conjunction with research areas like mass hysteria, bandwagon effect, social brain and mirror neurons.
Models.
Since hyperinflation is visible as a monetary effect, models of hyperinflation center on the demand for money. Economists see both a rapid increase in the money supply and an increase in the velocity of money if the (monetary) inflating is not stopped. Either one, or both of these together are the root causes of inflation and hyperinflation. A dramatic increase in the velocity of money as the cause of hyperinflation is central to the "crisis of confidence" model of hyperinflation, where the risk premium that sellers demand for the paper currency over the nominal value grows rapidly. The second theory is that there is first a radical increase in the amount of circulating medium, which can be called the "monetary model" of hyperinflation. In either model, the second effect then follows from the first—either too little confidence forcing an increase in the money supply, or too much money destroying confidence.
In the "confidence model", some event, or series of events, such as defeats in battle, or a run on stocks of the specie which back a currency, removes the belief that the authority issuing the money will remain solvent—whether a bank or a government. Because people do not want to hold notes which may become valueless, they want to spend them. Sellers, realizing that there is a higher risk for the currency, demand a greater and greater premium over the original value. Under this model, the method of ending hyperinflation is to change the backing of the currency, often by issuing a completely new one. War is one commonly cited cause of crisis of confidence, particularly losing in a war, as occurred during Napoleonic Vienna, and capital flight, sometimes because of "contagion" is another. In this view, the increase in the circulating medium is the result of the government attempting to buy time without coming to terms with the root cause of the lack of confidence itself.
In the "monetary model", hyperinflation is a positive feedback cycle of rapid monetary expansion. It has the same cause as all other inflation: money-issuing bodies, central or otherwise, produce currency to pay spiralling costs, often from lax fiscal policy, or the mounting costs of warfare. When businesspeople perceive that the issuer is committed to a policy of rapid currency expansion, they mark up prices to cover the expected decay in the currency's value. The issuer must then accelerate its expansion to cover these prices, which pushes the currency value down even faster than before. According to this model the issuer cannot "win" and the only solution is to abruptly stop expanding the currency. Unfortunately, the end of expansion can cause a severe financial shock to those using the currency as expectations are suddenly adjusted. This policy, combined with reductions of pensions, wages, and government outlays, formed part of the Washington consensus of the 1990s.
Whatever the cause, hyperinflation involves both the supply and velocity of money. Which comes first is a matter of debate, and there may be no universal story that applies to all cases. But once the hyperinflation is established, the pattern of increasing the money stock, by whichever agencies are allowed to do so, is universal. Because this practice increases the supply of currency without any matching increase in demand for it, the price of the currency, that is the exchange rate, naturally falls relative to other currencies. Inflation becomes hyperinflation when the increase in money supply turns specific areas of pricing power into a general frenzy of spending quickly before money becomes worthless. The purchasing power of the currency drops so rapidly that holding cash for even a day is an unacceptable loss of purchasing power. As a result, no one holds currency, which increases the velocity of money, and worsens the crisis.
Because rapidly rising prices undermine the role of money as a store of value, people try to spend it on real goods or services as quickly as possible. Thus, the monetary model predicts that the velocity of money will increase as a result of an excessive increase in the money supply. At the point when money velocity and prices rapidly accelerate in a vicious circle, hyperinflation is out of control, because ordinary policy mechanisms, such as increasing reserve requirements, raising interest rates, or cutting government spending will be ineffective and be responded to by shifting away from the rapidly devalued money and towards other means of exchange.
During a period of hyperinflation, bank runs, loans for 24-hour periods, switching to alternate currencies, the return to use of gold or silver or even barter become common. Many of the people who hoard gold today expect hyperinflation, and are hedging against it by holding specie. There may also be extensive capital flight or flight to a "hard" currency such as the US dollar. This is sometimes met with capital controls, an idea which has swung from standard, to anathema, and back into semi-respectability. All of this constitutes an economy which is operating in an "abnormal" way, which may lead to decreases in real production. If so, that intensifies the hyperinflation, since it means that the amount of goods in "too much money chasing too few goods" formulation is also reduced. This is also part of the vicious circle of hyperinflation.
Once the vicious circle of hyperinflation has been ignited, dramatic policy means are almost always required. Simply raising interest rates is insufficient. Bolivia, for example, underwent a period of hyperinflation in 1985, where prices increased 12,000% in the space of less than a year. The government raised the price of gasoline, which it had been selling at a huge loss to quiet popular discontent, and the hyperinflation came to a halt almost immediately, since it was able to bring in hard currency by selling its oil abroad. The crisis of confidence ended, and people returned deposits to banks. The German hyperinflation (1919–November 1923) was ended by producing a currency based on assets loaned against by banks, called the Rentenmark. Hyperinflation often ends when a civil conflict ends with one side winning.
Although wage and price controls are sometimes used to control or prevent inflation, no episode of hyperinflation has been ended by the use of price controls alone, because price controls that force merchants to sell at prices far below their restocking costs result in shortages that cause prices to rise still further.
Nobel prize winner Milton Friedman said "We economists don't know much, but we do know how to create a shortage. If you want to create a shortage of tomatoes, for example, just pass a law that retailers can't sell tomatoes for more than two cents per pound. Instantly you'll have a tomato shortage. It's the same with oil or gas."
Effects.
Hyperinflation effectively wipes out the purchasing power of private and public savings; distorts the economy in favor of the hoarding of real assets; causes the monetary base, whether specie or hard currency, to flee the country; and makes the afflicted area anathema to investment.
One of the most important characteristics of hyperinflation is the accelerating substitution of the inflating money by stable money—gold and silver in former times, then relatively stable foreign currencies after the breakdown of the gold or silver standards (Adolphe Thiers's Thiers' Law). If inflation is high enough, government regulations like heavy penalties and fines, often combined with exchange controls, cannot prevent this currency substitution. As a consequence, the inflating currency is usually heavily undervalued compared to stable foreign money and in terms of purchasing power parity. As a consequence, foreigners can live cheaply and buy at cheap prices in the countries hit by high inflation. It follows that governments who do not succeed in engineering a successful currency reform in time must finally legalize the stable foreign currencies (or, formerly, gold and silver) which is threatening to fully substitute the inflating money. Otherwise, their tax revenues, including the inflation tax, will approach zero. The last hyperinflation where this process could be observed took place in Zimbabwe in the first decade of the 21st century. In this case, the local money was mainly driven out by the US dollar and the South African rand.
Enactment of price controls to prevent discounting the value of paper money relative to gold, silver, hard currency, or other commodities fail to force acceptance of a paper money which lacks intrinsic value. If the entity responsible for printing a currency promotes excessive money printing, with other factors contributing a reinforcing effect, hyperinflation usually continues. Hyperinflation is generally associated with paper money, which can easily be used to increase the money supply: add more zeros to the plates and print, or even stamp old notes with new numbers. Historically, there have been numerous episodes of hyperinflation in various countries followed by a return to "hard money". Older economies would revert to hard currency and barter when the circulating medium became excessively devalued, generally following a "run" on the store of value.
Much attention on hyperinflation centers on the effect on savers whose investment becomes worthless. Academic economists seem not to have devoted much study on the (positive) effect on debtors. Interest rate changes often cannot keep up with hyperinflation or even high inflation, certainly with contractually fixed interest rates. For example, in the 1970s in the United Kingdom inflation reached 25% per annum, yet interest rates did not rise above 15%—and then only briefly—and many fixed interest rate loans existed. Contractually, there is often no bar to a debtor clearing his long term debt with "hyperinflated cash", nor could a lender simply somehow suspend the loan. "Early redemption penalties" were (and still are) often based on a penalty of "x" months of interest/payment; again no real bar to paying off what had been a large loan. In interwar Germany, for example, much private and corporate debt was effectively wiped out—certainly for those holding fixed interest rate loans.
Aftermath.
Hyperinflation is ended with drastic remedies, such as imposing the shock therapy of slashing government expenditures or altering the currency basis. One form this may take is dollarization, the use of a foreign currency (not necessarily the U.S. dollar) as a national unit of currency. An example was dollarization in Ecuador, initiated in September 2000 in response to a 75% loss of value of the Ecuadorian sucre in early 2000. But usually the "dollarization" takes place in spite of all efforts of the government to prevent it by exchange controls, heavy fines and penalties. The government has thus to try to engineer a successful currency reform stabilizing the value of the money. If it does not succeed with this reform the substitution of the inflating by stable money goes on. Thus it is not surprising that there exist at least seven historical cases in which the good (foreign) money did fully drive out the use of the inflating currency. In the end the government had to legalize the former, for otherwise its revenues would have fallen to zero.
Hyperinflation has always been a traumatic experience for the area which suffers it, and the next policy regime almost always enacts policies to prevent its recurrence. Often this means making the central bank very aggressive about maintaining price stability, as was the case with the German Bundesbank or moving to some hard basis of currency such as a currency board. Many governments have enacted extremely stiff wage and price controls in the wake of hyperinflation but this does not prevent further inflating of the money supply by its central bank, and always leads to widespread shortages of consumer goods if the controls are rigidly enforced.
Currency.
In countries experiencing hyperinflation, the central bank often prints money in larger and larger denominations as the smaller denomination notes become worthless. This can result in the production of some interesting banknotes, including those denominated in amounts of 1,000,000,000 or more.
One way to avoid the use of large numbers is by declaring a new unit of currency (an example being, instead of 10,000,000,000 dollars, a bank might set 1 new dollar = 1,000,000,000 old dollars, so the new note would read "10 new dollars.") An example of this would be Turkey's revaluation of the Lira on 1 January 2005, when the old Turkish lira (TRL) was converted to the New Turkish lira (TRY) at a rate of 1,000,000 old to 1 new Turkish Lira. While this does not lessen the actual value of a currency, it is called redenomination or revaluation and also happens over time in countries with standard inflation levels. During hyperinflation, currency inflation happens so quickly that bills reach large numbers before revaluation.
Some banknotes were stamped to indicate changes of denomination. This is because it would take too long to print new notes. By the time new notes were printed, they would be obsolete (that is, they would be of too low a denomination to be useful).
Metallic coins were rapid casualties of hyperinflation, as the scrap value of metal enormously exceeded the face value. Massive amounts of coinage were melted down, usually illicitly, and exported for hard currency.
Governments will often try to disguise the true rate of inflation through a variety of techniques. None of these actions address the root causes of inflation and if discovered, they tend to further undermine trust in the currency, causing further increases in inflation. Price controls will generally result in shortages and hoarding and extremely high demand for the controlled goods, causing disruptions of supply chains. Products available to consumers may diminish or disappear as businesses no longer find it sufficiently profitable (or may be operating at a loss) to continue producing and/or distributing such goods at the legal prices, further exacerbating the shortages.
There are also issues with computerized money-handling systems. In Zimbabwe, during the hyperinflation of the Zimbabwe dollar, many automated teller machines and payment card machines struggled with arithmetic overflow errors as customers required many billions and trillions of dollars at one time.
Hyperinflationary episodes.
Austria.
In 1922, inflation in Austria reached 1426%, and from 1914 to January 1923, the consumer price index rose by a factor of 11836, with the highest banknote in denominations of 500,000 krones.
Political ineptitude in Post WWI Austria comprised political expedience and Socialist benevolence. Thus, essentially all State enterprises ran at a loss and the number of State employees in Vienna, the capital of the post WWI republic was greater than that of the earlier monarchy, even though they served a population base nearly eight times smaller.
Observing the Austrian response to developing hyperinflation, fueled by selfishness and political ineptitude, including the hoarding of food and the speculation in foreign currencies, Owen S. Phillpotts, the Commercial Secretary at the British Legation in Vienna wrote: “The Austrians are like men on a ship who cannot manage it, and are continually signalling for help. While waiting, however, most of them begin to cut rafts, each for himself, out of the sides and decks. The ship has not yet sunk despite the leaks so caused, and those who have acquired stores of wood in this way may use them to cook their food, while the more seamanlike look on cold and hungry. The population lack courage and energy as well as patriotism”.
China.
As the first user of fiat currency, China has had an early history of troubles caused by hyperinflation. The Yuan Dynasty printed huge amounts of fiat paper money to fund their wars, and the resulting hyperinflation, coupled with other factors, led to its demise at the hands of a revolution. The Republic of China went through the worst inflation 1948–49. In 1947, the highest denomination was 50,000 yuan. By mid-1948, the highest denomination was 180,000,000 yuan. The 1948 currency reform replaced the yuan by the gold yuan at an exchange rate of 1 gold yuan = 3,000,000 yuan. In less than a year, the highest denomination was 10,000,000 gold yuan. In the final days of the civil war, the Silver Yuan was briefly introduced at the rate of 500,000,000 Gold Yuan. Meanwhile, the highest denomination issued by a regional bank was 6,000,000,000 yuan (issued by Xinjiang Provincial Bank in 1949). After the renminbi was instituted by the new communist government, hyperinflation ceased with a revaluation of 1:10,000 old Renminbi in 1955. The overall impact of inflation was 1 Renminbi = 15,000,000,000,000,000,000 pre-1948 yuan.
France.
During the French Revolution and first Republic, the National Assembly issued bonds, some backed by seized church property, called Assignats. Napoleon replaced them with the franc in 1803, at which time the assignats were basically worthless.
Stephen D. Dillaye pointed out that one of the reasons for the failure was massive counterfeiting of the paper currency, “the Assignats” – largely through London – where, according to Dillaye: “Seventeen manufacturing establishments were in full operation in London, with a force of four hundred men devoted to the production of false and forged Assignats.” 
Germany (Weimar Republic).
By November 1922, the gold value of money in circulation fell from £300 million before WWI to £20 million. The Reichsbank responded by the unlimited printing of notes, thereby accelerating the devaluation of the mark. In his report to London, Lord D'Abernon wrote: "In the whole course of history, no dog has ever run after its own tail with the speed of the Reichsbank." Germany went through its worst inflation in 1923. In 1922, the highest denomination was 50,000 Mark. By 1923, the highest denomination was 100,000,000,000,000 Mark. In December 1923 the exchange rate was 4,200,000,000,000 Marks to 1 US dollar. In 1923, the rate of inflation hit 3.25 × 106 percent per month (prices double every two days). Beginning on 20 November 1923, 1,000,000,000,000 old Marks were exchanged for 1 Rentenmark so that 4.2 Rentenmarks were worth 1 US dollar, exactly the same rate the Mark had in 1914.
Hungary.
The Treaty of Trianon and political instability between 1919 and 1924 led to a major inflation of Hungary's currency. In 1921, in an attempt to arrest Post WWI inflation, the national assembly of Hungary passed the Hegedüs reforms, including a 20% levy on bank deposits. This action precipitated a mistrust of banks by the public, especially the peasants, and resulted in a reduction savings and the amount of currency in circulation. Unable to tax adequately, the government resorted to printing money and in 1923 inflation in Hungary reached 98% per month.
However, between the end of 1945 and July 1946, Hungary went through the worst inflation ever recorded. In 1944, the highest denomination was 1,000 pengő. By the end of 1945, it was 10,000,000 pengő. The highest denomination in mid-1946 was 100,000,000,000,000,000,000 pengő. A special currency the adópengő – or tax pengő – was created for tax and postal payments. The value of the adópengő was adjusted each day, by radio announcement. On 1 January 1946 one adópengő equaled one pengő. By late July, one adópengő equaled 2,000,000,000,000,000,000,000 or 2×1021 (2 sextillion) pengő. When the pengő was replaced in August 1946 by the forint, the total value of all Hungarian banknotes in circulation amounted to 1/1,000 of one US dollar. It is the most severe known incident of inflation recorded, peaking at 1.3 × 1016 percent per month (prices double every 15 hours). The overall impact of hyperinflation: On 18 August 1946, 400,000,000,000,000,000,000,000,000,000 or 4 (four hundred quadrilliard on the long scale used in Hungary; four hundred octillion on short scale) pengő became 1 forint.
North Korea.
North Korea most likely experienced hyperinflation from December 2009 to mid-January 2011. Based on the price of rice, North Korea's hyperinflation peaked in mid-January 2010, but according to black market exchange-rate data, and calculations base on purchasing power parity, North Korea experienced its peak month of inflation in early March 2010. However, this data is unofficial and therefore must be treated with a degree of caution.
Philippines.
The Japanese government occupying the Philippines during the World War II issued fiat currencies for general circulation. The Japanese-sponsored Second Philippine Republic government led by Jose P. Laurel at the same time outlawed possession of other currencies, most especially "guerilla money." The fiat money was dubbed "Mickey Mouse Money" because it is similar to play money and is next to worthless. Survivors of the war often tell tales of bringing suitcase or "bayong" (native bags made of woven coconut or buri leaf strips) overflowing with Japanese-issued bills. In the early times, 75 Mickey Mouse pesos could buy one duck egg. In 1944, a box of matches cost more than 100 Mickey Mouse pesos.
In 1942, the highest denomination available was 10 pesos. Before the end of the war, because of inflation, the Japanese government was forced to issue 100, 500 and 1000 peso notes.
Singapore.
From 15 February 1942 to 1945, Singapore was occupied by the Japanese. The cost of basic necessities increased drastically due to hyperinflation and the Japanese issued banana money (named as such because of the motifs of banana trees on 10 dollar banknotes) as their main currency since Straits currency became rare. To supply the Japanese authorities with money whenever they required it, they simply printed more notes. This resulted in hyperinflation and a severe depreciation in value of the banana note.
Yugoslavia.
Yugoslavia went through a period of hyperinflation and subsequent currency reforms from 1989–1994. One of several regional conflicts accompanying the dissolution of Yugoslavia was the Bosnian War (1992–1995). The Belgrade government of Slobodan Milošević backed ethnic Serbian secessionist forces in the conflict, resulting in a United Nations boycott of Yugoslavia. The UN boycott collapsed an economy already weakened by regional war, with the projected monthly inflation rate accelerating to one million percent by December, 1993 (prices double every 2.3 days). The highest denomination in 1988 was 50,000 dinars. By 1989 it was 2,000,000 dinars. In the 1990 currency reform, 1 new dinar was exchanged for 10,000 old dinars. In the 1992 currency reform, 1 new dinar was exchanged for 10 old dinars. The highest denomination in 1992 was 50,000 dinars. By 1993, it was 10,000,000,000 dinars. In the 1993 currency reform, 1 new dinar was exchanged for 1,000,000 old dinars. However, before the year was over, the highest denomination was 500,000,000,000 dinars. In the 1994 currency reform, 1 new dinar was exchanged for 1,000,000,000 old dinars. In another currency reform a month later, 1 novi dinar was exchanged for 13 million dinars (1 novi dinar = 1 German mark at the time of exchange). The overall impact of hyperinflation: 1 novi dinar = 1 × 1027~1.3 × 1027 pre 1990 dinars. Yugoslavia's rate of inflation hit 5 × 1015 percent cumulative inflation over the time period 1 October 1993 and 24 January 1994. 
Zimbabwe.
Hyperinflation in Zimbabwe was one of the few instances that resulted in the abandonment of the local currency. At independence in 1980, the Zimbabwe dollar (ZWD) was worth about USD 1.25. Afterwards, however, rampant inflation and the collapse of the economy severely devalued the currency. Inflation was steady before Robert Mugabe in 1998 began a program of land reforms that primarily focused on returning land taken from black natives during colonialization which led to disrupted food production and caused revenues from exports of food to plummet and foreign direct investment declined. The result was that to pay its expenditures Mugabe's government and Gideon Gono's Reserve Bank printed more and more notes with higher face values.
Hyperinflation began early in the 21st-century, reaching 624% in 2004. It fell back to low triple digits before surging to a new high of 1,730% in 2006. The Reserve Bank of Zimbabwe revalued on 1 August 2006 at a ratio of 1 000 ZWD to each second dollar (ZWN), but year-to-year inflation rose by June 2007 to 11,000% (versus an earlier estimate of 9,000%). Larger denominations were progressively issued:
Inflation by 16 July officially surged to 2,200,000% with some analysts estimating figures surpassing 9,000,000 percent. As of 22 July 2008 the value of the ZWN fell to approximately 688 billion per 1 USD, or 688 trillion pre-August 2006 Zimbabwean dollars.
On 1 August 2008, the Zimbabwe dollar was redenominated at the ratio of ZWN to each third dollar (ZWR). On 19 August 2008, official figures announced for June estimated the inflation over 11,250,000%. Zimbabwe's annual inflation was 231,000,000% in July (prices doubling every 17.3 days). By October 2008 Zimbabwe was mired in hyperinflation with wages falling far behind inflation. In this dysfunctional economy hospitals and schools had chronic staffing problems, because many nurses and teachers could not afford bus fare to work. Most of the capital of Harare was without water because the authorities had stopped paying the bills to buy and transport the treatment chemicals. Desperate for foreign currency to keep the government functioning, Zimbabwe's central bank governor, Gideon Gono, sent runners into the streets with suitcases of Zimbabwean dollars to buy up American dollars and South African rand. For periods after July 2008, no official inflation statistics were released. Prof. Steve H. Hanke overcame the problem by estimating inflation rates after July 2008 and publishing the Hanke Hyperinflation Index for Zimbabwe.
At its November 2008 peak, Zimbabwe's rate of inflation approached, but failed to surpass, Hungary's July 1946 world record. On 2 February 2009, the dollar was redenominated for the fourth time at the ratio of ZWR to 1 ZWL, only three weeks after the $100 trillion banknote was issued on 16 January, but hyperinflation waned by then as official inflation rates in USD were announced and foreign transactions were legalised, and on 12 April the dollar was abandoned in favour of using only foreign currencies. The overall impact of hyperinflation was 1 ZWL = ZWD.
Examples of high inflation.
Some countries experienced very high inflation, but did not reach hyperinflation, as defined as a "monthly" inflation rate of 50%.
Iraq.
Between 1987 and 1995 the Iraqi Dinar went from an official value of 0.306 Dinars/USD (or $3.26 USD per dinar, though the black market rate is thought to have been substantially lower) to 3000 Dinars/USD due to government printing of 10s of trillions of dinars starting with a base of only 10s of billions. That equates to approximately 315% inflation per year averaged over that eight-year period.
Mexico.
In spite of increased oil prices in the late 1970s (Mexico is a producer and exporter), Mexico defaulted on its external debt in 1982. As a result, the country suffered a severe case of capital flight and several years of hyperinflation and peso devaluation. On 1 January 1993, Mexico created a new currency, the "nuevo peso" ("new peso", or MXN), which chopped three zeros off the old peso, an inflation rate of 100,000% over the several years of the crisis. (One new peso was equal to 1,000 old MXP pesos).
Roman Egypt.
In Roman Egypt, where the best documentation on pricing has survived, the price of a measure of wheat was 200 drachmae in 276 AD, and increased to more than 2,000,000 drachmae in 334 AD, roughly 1,000,000% inflation in a span of 58 years.
Although the price increased by a factor of 10,000 over 58 years, the annual rate of inflation was only 17.2% compounded.
Romania.
Romania experienced hyperinflation in the 1990s. The highest denomination in 1990 was 100 lei and in 1998 was 100,000 lei. By 2000 it was 500,000 lei. In early 2005 it was 1,000,000 lei. In July 2005 the lei was replaced by the new leu at 10,000 old lei = 1 new leu. Inflation in 2005 was 9%. In July 2005 the highest denomination became 500 lei (= 5,000,000 old lei).
Soviet Union.
Between 1921 and 1922, inflation in the Soviet Union reached 213%.
United States.
During the Revolutionary War, when the Continental Congress authorized the printing of paper currency called continental currency, the monthly inflation rate reached a peak of 47 percent in November 1779 (Bernholz 2003: 48). These notes depreciated rapidly, giving rise to the expression "not worth a continental."
One cause of the inflation was counterfeiting by the British, who ran a press on HMS "Phoenix", moored in New York Harbour. The counterfeits were advertised and sold almost for the price of the paper they were printed on.
A second close encounter occurred during the U.S. Civil War, between January 1861 and April 1865, the Lerner Commodity Price Index of leading cities in the eastern Confederacy states increased from 100 to over 9,000. As the Civil War dragged on, the Confederate dollar had less and less value, until it was almost worthless by the last few months of the war. Similarly, the Union government inflated its greenbacks, with the monthly rate peaking at 40 percent in March 1864 (Bernholz 2003: 107).
Vietnam.
Vietnam went through a period of chaotic and hyperinflation in the late 1980s, with inflation peaking at 774% in 1988, after the country's "price-wage-currency" reform package led by Mr Tran Phuong, then Deputy Prime Minister, had failed bitterly. Hyperinflation also characterizes the early stage of economic renovation, usually referred to as Doi Moi, in Vietnam.
Units of inflation.
Inflation rate is usually measured in percent per year. It can also be measured in percent per month or in price doubling time.
formula_1
formula_2
formula_3
formula_4
Often, at redenominations, three zeroes are cut from the bills. It can be read from the table that if the (annual) inflation is for example 100%, it takes 3.32 years to produce one more zero on the price tags, or 3 × 3.32 = 9.96 years to produce three zeroes. Thus can one expect a redenomination to take place about 9.96 years after the currency was introduced.

</doc>
<doc id="13682" url="https://en.wikipedia.org/wiki?curid=13682" title="Herbert Hoover">
Herbert Hoover

Herbert Clark Hoover (August 10, 1874 – October 20, 1964) was the 31st President of the United States (1929–33). He was a professional mining engineer and was raised as a Quaker. A Republican, Hoover served as head of the U.S. Food Administration during World War I, and became internationally known for humanitarian relief efforts in war-time Belgium. As the United States Secretary of Commerce in the 1920s under Presidents Warren G. Harding and Calvin Coolidge, he promoted partnerships between government and business under the rubric "economic modernization". In the presidential election of 1928, Hoover easily won the Republican nomination, despite having no elected-office experience. Hoover is the most recent cabinet secretary to be elected President of the United States, as well as one of only two Presidents (along with William Howard Taft) elected without electoral experience or high military rank.
Hoover, a globally experienced engineer, believed strongly in the Efficiency Movement, which held that the government and the economy were riddled with inefficiency and waste, and could be improved by experts who could identify the problems and solve them. He also believed in the importance of volunteerism and of the role of individuals in society and the economy. Hoover, who had made a small fortune in mining, was the first of two Presidents to redistribute his salary (President Kennedy was the other; he donated all his paychecks to charity). When the Wall Street Crash of 1929 struck less than eight months after he took office, Hoover tried to combat the ensuing Great Depression with moderate government public works projects such as the Hoover Dam. The record tariffs imbedded in the Smoot-Hawley Tariff and aggressive increases in the top tax bracket from 25% to 63%, coupled with increases in corporate taxes, yielded a "balanced budget" in 1933, but the economy plummeted simultaneously and unemployment rates rose to afflict one in four American workers. This downward spiral set the stage for Hoover's defeat in 1932 by Democrat Franklin D. Roosevelt, who promised a New Deal. After Roosevelt assumed the Presidency in 1933, Hoover became a spokesman for opposition to the domestic and foreign policies of the New Deal. In 1947, President Harry S. Truman appointed Hoover to head the Hoover Commission, intended to foster greater efficiency throughout the federal bureaucracy. Most historians agree that Hoover's defeat in the 1932 election was caused primarily by the downward economic spiral, although his strong support for prohibition was also significant. Hoover is usually ranked lower than average among U.S. Presidents.
Family background and early life.
Herbert Hoover was born on August 10, 1874, in West Branch, Iowa, the first of his office born in that state and west of the Mississippi River. His father, Jesse Hoover (1849–80), was a blacksmith and farm implement store owner, of German (Pfautz, Wehmeyer) and Swiss (Huber, Burkhart) ancestry. Jesse Hoover and his father Eli had moved to Iowa from Ohio twenty years previously. Hoover's mother, Hulda Randall Minthorn (1849–84), was born in Norwich, Ontario, Canada, and was of English and Irish ancestry. Both of his parents were Quakers.
At about age two "Bertie", as he was then called, contracted the croup. He was so ill that he was momentarily thought to have died, until he was resuscitated by his uncle, John Minthorn. As a child, he was often called by his father "my little stick in the mud", since he repeatedly was trapped in the mud while crossing an unpaved street. Hoover's family figured prominently in the town's public prayer life, due almost entirely to Hulda's role in her church. His father, noted by the local paper for his "pleasant, sunshiny disposition", died in 1880. After working to retire her husband's debts, retain their life insurance, and care for the children, his mother died in 1884, leaving Hoover (age nine), his older brother, and his younger sister as orphans. Fellow Quaker Lawrie Tatum was appointed as Hoover's guardian.
After a brief stay with one of his grandmothers in Kingsley, Iowa, Hoover lived the next 18 months with his uncle Allen Hoover in West Branch. In November 1885, he went to Newberg, Oregon, to live with his uncle Dr. John Minthorn, a physician and businessman whose own son had died the year before. The Minthorn household was considered cultured and educational, and imparted a strong work ethic. For two-and-a-half years, Hoover attended Friends Pacific Academy (now George Fox University), and then worked as an office assistant in his uncle's real estate office, the Oregon Land Company, in Salem, Oregon. Though he did not attend high school, Hoover attended night school and learned bookkeeping, typing and mathematics.
Hoover entered Stanford University in 1891, its inaugural year, after failing all the entrance exams (except mathematics) and then being tutored for the summer in Palo Alto. The first-year students were not required to pay tuition. Hoover claimed to be the very first student at Stanford, by virtue of having been the first person in the first class to sleep in the dormitory. While at the university, he was the student manager of both the baseball and football teams and was a part of the inaugural Big Game versus rival the University of California and friend (CAL Manager) Herbert Lang. Only 10,000 tickets were printed for the inaugural game and 20,000 people showed up. Both Hoover and Lang had to find pots, bowls and any other available receptacles to collect admission fees. Stanford won the game. Hoover graduated in 1895 with a degree in geology. He earned his way through four years of college working various jobs on and off campus, including the Geological Survey of Arkansas and the United States Geological Survey. Throughout his tenure at Stanford, he was adamantly opposed to the fraternity system.
Mining engineer.
Australia.
Hoover went to Western Australia in 1897 as an employee of Bewick, Moreing & Co., a London-based gold mining company. His geological training and work experience were well suited for the firm's objectives. He worked at gold mines in Big Bell, Cue, Gwalia, Menzies, and Coolgardie. Hoover first went to Coolgardie, then the center of the Western Australian goldfields, where he worked under Edward Hooper, a company partner. Conditions were harsh in these goldfields even though he got a $5,000 salary (equivalent to $100,000 today). In the Coolgardie and Murchison rangelands on the edge of the Great Victoria Desert, Hoover described the region as a land of "black flies, red dust and white heat". He served as a geologist and mining engineer while searching the Western Australian goldfields for investments. After being appointed as mine manager at the age of 23, he led a major program of expansion for the Sons of Gwalia gold mine at Gwalia, and brought in many Italian immigrants to cut costs and counter the union militancy of the Australian miners. He believed "the rivalry between the Italians and the other men was of no small benefit." He also described Italians as "fully 20 per cent superior" to other miners.
During his time at Gwalia, Hoover first met Fleury James Lyster, a pioneering metallurgist. In Western Australia friends called Hoover "H.C." or the old nickname "Hail Columbia".
An open feud developed between Hoover and his boss Ernest Williams, with Hoover persuading four other mine managers to conspire against his rival. The firm's principals then offered Hoover a compelling promotion which would relocate him to China and also end the feud. Hoover then began to take stock of his private life, including contemplating a separation from his Stanford sweetheart, Lou Henry.
China and other global operations.
Hoover promptly sent a cable of proposal to her, and subsequently married Lou Henry, in 1899. She had been an Episcopalian and became a Quaker. The Hoovers had two sons, Herbert Charles Hoover (1903–1969) and Allan Henry Hoover (1907–1993). The family subsequently went to China. Hoover worked as chief engineer for the Chinese Bureau of Mines, and as general manager for the Chinese Engineering and Mining Corporation. Later he worked for Bewick, Moreing & Co. as the company's lead engineer. Hoover and his wife learned Mandarin Chinese while he worked in China and used it during his tenure at the White House when they wanted to foil eavesdroppers. Hoover made recommendations to improve the lot of the Chinese worker, seeking to end the practice of imposing long term servitude contracts and to institute reforms for workers based on merit. The Boxer Rebellion trapped the Hoovers in Tianjin in June 1900. For almost a month, the settlement was under fire, and both dedicated themselves to defense of their city. Hoover himself guided U.S. Marines around Tianjin during the battle, using his knowledge of the local terrain. Mrs. Hoover meanwhile devoted her efforts at the various hospitals and even wielded, and willingly and accurately deployed, a .38-caliber pistol.
Hoover was made a partner in Bewick, Moreing & Co. on December 18, 1901 and assumed responsibility for various Australian operations and investments. His initial compensation rose to $12,500 annually in addition to a 20% share of profits. The company eventually controlled at one point approximately 50% of gold production in Western Australia. In 1901, Hoover no longer lived in Australia, but he visited the country in 1902, 1903, 1905, and 1907 as an overseas investor. Hoover was also a director of Chinese Engineering and Mining Corporation (CEMC) when it became a supplier of immigrant labor from Southeast Asia for South African mines. The first shipment of almost 2,000 workers arrived in Durban from Qinhuangdao in July 1904. By 1906, the total number of immigrant workers increased to 50,000, almost entirely recruited and shipped by CEMC. When the living and working conditions of the laborers became known, public opposition to the scheme grew and questions were asked in the British Parliament. The scheme was abandoned in 1911.
In August–September 1905, he founded the Zinc Corporation (eventually part of the Rio Tinto Group) with William Baillieu and others. The lead-silver ore produced at Broken Hill, New South Wales was rich in zinc. But the zinc could not be recovered due to "the Sulphide Problem", and was left in the tailings that remained after the silver and lead was extracted.
Zinc Corporation proposed to buy the tailings and extract the zinc by a new process. The froth flotation process was then being developed at Broken Hill, although the Zinc Corporation struggled to apply it. Hoover came to Broken Hill in 1907. So did Australian engineer Jim Lyster, whose "Lyster Process", enabled the Zinc Corporation to operate the world's first selective or differential flotation plant, from September 1912. Hoover's brother, Theodore J. Hoover, also came to Broken Hill.
"Broken Hill was one of the dreariest places in the world at this time. It lay in the middle of the desert, was unbelievably hot in summer, had no fresh water, no vegetation, and mountains of tailings blew into every crack with every wisp of wind." Despite these miserable conditions, Hoover and his associates became suppliers to world industry of zinc and other vital base minerals.
Sole proprietor.
In 1908, Hoover became an independent mining consultant, traveling worldwide until the outbreak of World War I in 1914. He left Bewick, Moreing & Co and, setting out on his own, eventually ended up with investments on every continent and offices in San Francisco, London, New York City, St. Petersburg, Paris and Mandalay, Burma. He had his second successful venture with the British firm Burma Corporation, again producing silver, lead, and zinc in large quantities at the Namtu Bawdwin Mine, where he caught malaria in 1907. While living in London, noting the American engineer's patriotic intensity, some British acquaintances referred to him as the "star-spangled Hoover". It recalled the nickname he had acquired in the Australian outback: "Hail Columbia" Hoover. The Bawdwin mine ultimately became the chief source of Hoover's fortune.
In his spare time, Hoover wrote. His lectures at Columbia and Stanford universities were published in 1909 as "Principles of Mining", which became a standard textbook. Hoover and his wife also published their English translation of the 1556 mining classic "De re metallica" in 1912. This translation from the Latin of Renaissance author Georgius Agricola is still the most important scholarly version and provides its historical context. It is still in print.
By 1914, Hoover was a wealthy man, with an estimated personal fortune of $4 million. He was once quoted as saying "If a man has not made a million dollars by the time he is forty, he is not worth much". By 1914, Hoover stood eventually to obtain what he later described as "a large fortune from these Russian industries, probably more than is good for anybody". Sixty-six years after opening the mine in 1897, Hoover still had a partial share in the Sons of Gwalia mine when it finally closed in 1963, just one year before the former President's death in New York City in 1964. The successful mine had yielded $55m in gold and $10m in dividends for investors. Herbert Hoover, acting as a main investor, financier, mining speculator, and organizer of men, played a major role in the important metallurgical developments that occurred in Broken Hill in the first decade of the twentieth century, developments that had a great impact on the mining and production of silver, lead, and zinc. In later years Hoover thought of himself and his associates as "engineering doctors to sick concerns", hence his reputation as the "Doctor of sick mines".
Humanitarian work.
Relief in Europe and Belgium.
When World War I began in August 1914, Hoover helped organize the return of around 120,000 Americans from Europe. He led 500 volunteers in distributing food, clothing, steamship tickets and cash. "I did not realize it at the moment, but on August 3, 1914, my career was over forever. I was on the slippery road of public life." Hoover liked to say that the difference between dictatorship and democracy was simple: dictators organize from the top down, democracies from the bottom up.
When Belgium faced a food crisis after being invaded by Germany in 1914, Hoover undertook an unprecedented relief effort with the Commission for Relief in Belgium (CRB). As chairman of the CRB, Hoover worked with the leader of the Belgian "Comité National de Secours et d'Alimentation" (CNSA), Émile Francqui, to feed the entire nation for the duration of the war. The CRB obtained and imported millions of tons of foodstuffs for the CNSA to distribute, and watched over the CNSA to make sure the German army didn't appropriate the food. The CRB became a veritable independent republic of relief, with its own flag, navy, factories, mills, and railroads. Private donations and government grants (78%) supplied an $11-million-a-month budget.
For the next two years, Hoover worked 14-hour days from London, administering the distribution of over two million tons of food to nine million war victims. In an early form of shuttle diplomacy, he crossed the North Sea forty times to meet with German authorities and persuade them to allow food shipments, becoming an international hero. The Belgian city of Leuven named a prominent square "Hooverplein" after him. At its peak, Hoover's American Relief Administration (ARA) fed 10.5 million people daily. Great Britain grew reluctant to support the CRB, preferring instead to emphasize Germany's obligation to supply the relief; Winston Churchill, whom Hoover intensely disliked, led a military faction that considered the Belgian relief effort "a positive military disaster".
During this time, Hoover made a strong impression on the American Ambassador to Great Britain, Walter Page. In a Memoranda dated December 30, 1916, Page wrote: Mr. Herbert C. Hoover, Chairman of the Commission for Relief in Belgium, would, if opportunity should offer, make a useful officer in the State Department. He is probably the only man living who has privately (i.e., without holding office) negotiated understandings with the British, French, German, Dutch, and Belgian governments. He personally knows and has had direct dealings with these governments, and his transactions with them
have involved several hundred million dollars. He is a man of very considerable fortune—less than when the war began, for tins relief work has cost him much. He was approached on behalf of the British Government with the suggestion that if he would become a British subject the Government would be pleased to give him an important executive post and with the hint that if he succeeded a title might await him. His answer was: "I'll do what I can for you with pleasure; but I'll be damned if I'll give up my American citizenship—not on your life!" Within the last six months two large financial organizations, each independently, have offered him $100,000 a year to enter their service; and an industrial company offered him $100,000 "to start with." He declined them all. When the Belgian relief work recently struck a financial snag, Hoover by telegraph got the promise of a loan in the United States to the British and French governments for Belgian relief of $150,000,000.
U.S. Food Administration.
After the United States entered the war in April 1917, President Woodrow Wilson appointed Hoover to head the U.S. Food Administration, which was created under the Lever Food Control Act in 1917. This was a position he actively sought, though he later claimed it was thrust upon him. He was convinced from his Belgian work that centralization of authority was essential to any relief effort; he demanded, and got, great power albeit not as much as he sought. Hoover believed "food will win the war"; and beginning on September 29, this slogan was introduced and put into frequent use.
He carefully selected men to assist in the agency leadership – Alonzo Taylor (technical abilities), Robert Taft (political associations), Gifford Pinchot (agricultural influence) and Julius Barnes (business acumen). Hoover established set days for people to avoid eating specified foods and save them for soldiers' rations: meatless Mondays, wheatless Wednesdays, and "when in doubt, eat potatoes". This program helped reduce consumption of foodstuffs needed overseas and avoided rationing at home. It was dubbed "Hooverizing" by government publicists, in spite of Hoover's continual orders that publicity should not mention him by name. The agency employed a system of price controls and licensing requirements for suppliers to maximize production. Despite efforts to prevent it, some companies reaped great profits.
Post-war relief.
After the war, as a member of the Supreme Economic Council and head of the American Relief Administration, Hoover organized shipments of food for millions of starving people in Central Europe. He used a newly formed Quaker organization, the American Friends Service Committee, to carry out much of the logistical work in Europe.
Hoover provided aid to the defeated German nation after the war, as well as relief to famine-stricken Bolshevik-controlled areas of Russia in 1921, despite the opposition of Senator Henry Cabot Lodge and other Republicans. When asked if he was not thus helping Bolshevism, Hoover retorted, "Twenty million people are starving. Whatever their politics, they shall be fed!". The Russian famine of 1921–22 claimed 6 million people. In July 1922, Soviet author Maxim Gorky wrote to Hoover:
At war's end, the "New York Times" named Hoover one of the "Ten Most Important Living Americans". Hoover confronted a world of political possibilities when he returned home in 1919. Democratic Party leaders saw him as a potential Presidential candidate, and President Wilson privately preferred Hoover as his successor. "There could not be a finer one," asserted Franklin D. Roosevelt, then a rising star from New York. Hoover briefly considered becoming a Democrat, but he believed that 1920 would be a Republican year. Also, Hoover confessed that he could not run for a party whose only member in his boyhood home had been the town drunk.
Hoover realized that he was in a unique position to collect information about the Great War and its aftermath. In 1919 he established the Hoover War Collection at Stanford University. He donated all the files of the Commission for Relief in Belgium, the U.S. Food Administration, and the American Relief Administration, and pledged $50,000 as an endowment. Scholars were sent to Europe to collect pamphlets, society publications, government documents, newspapers, posters, proclamations, and other ephemeral materials related to the war and the revolutions that followed it. The collection was later renamed the Hoover War Library and is now known as the Hoover Institution.
Secretary of Commerce.
Hoover rejected the Democratic Party's overtures in 1920. He had been a registered Republican before the war, though he had supported Theodore Roosevelt's "Bull Moose" Progressive Party in 1912. Now he declared himself a Republican and a candidate for the Presidency.
He placed his name on the ballot in the California state primary election, where he nearly beat popular Senator Hiram Johnson. But having lost in his home state, Hoover was not considered a serious contender at the convention. Even when it deadlocked for several ballots between Illinois Governor Frank Lowden and General Leonard Wood, few delegates seriously considered Hoover as a compromise choice. Although he had personal misgivings about the capability of the nominee, Warren G. Harding, Hoover publicly endorsed him and made two speeches for Harding.
After being elected, Harding rewarded Hoover for his support, offering to appoint him either Secretary of the Interior or Secretary of Commerce. Hoover ultimately chose Commerce. Commerce had existed for just eight years, since the division of the earlier Department of Commerce and Labor. Commerce was considered a minor Cabinet post, with limited and vaguely defined responsibilities.
Hoover aimed to change that, envisioning the Commerce Department as the hub of the nation's growth and stability. From Harding he demanded, and received, authority to coordinate economic affairs throughout the government. He created many sub-departments and committees, overseeing and regulating everything from manufacturing statistics, the census and radio, to air travel. In some instances he "seized" control of responsibilities from other Cabinet departments when he deemed that they were not carrying out their responsibilities well. Hoover became one of the most visible men in the country, often overshadowing Presidents Harding and Coolidge. Washington wags referred to Hoover as "the Secretary of Commerce... and Under-Secretary of Everything Else!"
His detractors wondered why he did not do anything to reapportion congress after the 1920 Decennial Census which saw an increase in urban and immigrant populations. This was the first and only Decennial Census where the results were not used to reapportion Congress; which ultimately influenced the 1928 Electoral College and impacted the Presidential Election.
As secretary and later as President, Hoover revolutionized relations between business and government. Rejecting the adversarial stance of Theodore Roosevelt, William Howard Taft, and Woodrow Wilson, he sought to make the Commerce Department a powerful service organization, empowered to forge cooperative voluntary partnerships between government and business. This philosophy is often called "associationalism". Both the U.S. Department of Justice and the Federal Trade Commission opposed Hoover's goals, but the U.S. Supreme Court ruled in "Maple Flooring Manufacturers' Assn. v. United States 268 U.S. 563 (1925)" that Hoover's policies served the public interest by producing "fairer price levels" and "avoid waste."
Many of Hoover's efforts as Commerce Secretary centered on eliminating waste and increasing efficiency in business and industry. This included reducing labor losses from trade disputes and seasonal fluctuations, reducing industrial losses from accident and injury, and reducing the amount of crude oil spilled during extraction and shipping. One major achievement was to promote product standardizations. He promoted international trade by opening overseas offices to advise businessmen. Hoover was especially eager to promote Hollywood films overseas.
His "Own Your Own Home" campaign was a collaboration to promote ownership of single-family dwellings, with groups such as the Better Houses in America movement, the Architects' Small House Service Bureau, and the Home Modernizing Bureau. He worked with bankers and the savings and loan industry to promote the new long-term home mortgage, which dramatically stimulated home construction.
It has been suggested that Herbert Hoover was the best Secretary of Commerce in United States history. Hoover was the last President to have held a full cabinet position.
Radio conferences.
Hoover's radio conferences played a key role in the early organization, development and regulation of radio broadcasting. Prior to the Radio Act of 1927, the Secretary of Commerce was unable to deny radio licensing or reassign broadcast frequencies. With help from supporters Senator Dill and Representative White, Hoover brought the issue of radio control to the Senate floor. Hoover fought for more power to control the proliferation of licensed radio stations (which in 1927, stood at 732 stations). With help from Dill and White, Hoover promoted the Dill-White Bill which eventually would become the Radio Act of 1927. This act allowed the government to intervene and abolish radio stations that were deemed "non-useful" to the public. Hoover's attempts at regulating radio were not supported by all Congressmen, and he received much opposition from the Senate and from radio station owners. However, Hoover's contributions to regulate radio in its infancy heavily influenced the modern radio system.
Hoover contributed to major projects for navigation, irrigation of dry lands, electrical power, and flood control. As the new air transport industry developed, Hoover held a conference on aviation to promote codes and regulations. He became President of the American Child Health Organization, and he raised private funds to promote health education in schools and communities.
Although he continued to consider Harding ill-suited to be President, the two men nevertheless became friends. Hoover accompanied Harding on his final trip out West in 1923. It was Hoover who called for a specialist to tend to the ailing Chief Executive, and it was also Hoover who contacted the White House to inform them of the President's death. The Commerce Secretary headed the group of dignitaries accompanying Harding's body back to the capital.
By the end of Hoover's service as Secretary, he had raised the status of the Department of Commerce. This was reflected in its modern headquarters built during the Roosevelt Administration in the 1930s in the Federal Triangle in Washington, D.C.
Traffic conferences.
As Commerce Secretary, Hoover also hosted two national conferences on street traffic, in 1924 and 1926 (a third convened in 1930, during Hoover's presidency). Collectively the meetings were called the National Conference on Street and Highway Safety. Hoover's chief objective was to address the growing casualty toll of traffic accidents, but the scope grew and soon embraced motor vehicle standards, rules of the road, and urban traffic control. He left the invited interest groups to negotiate agreements among themselves, which were then presented for adoption by states and localities. Because automotive trade associations were the best organized, many of the positions taken by the conferences reflected their interests. The conferences issued a model Uniform Vehicle Code for adoption by the states, and a Model Municipal Traffic Ordinance for adoption by cities. Both were widely influential, promoting greater uniformity between jurisdictions and tending to promote the automobile's priority in city streets.
Mississippi flood.
The Great Mississippi Flood of 1927 broke the banks and levees of the lower Mississippi River in early 1927, resulting in flooding of millions of acres and leaving 1.5 million people displaced from their homes. Although such a disaster did not fall under the duties of the Commerce Department, the governors of six states along the Mississippi specifically asked for Herbert Hoover in the emergency. President Calvin Coolidge sent Hoover to mobilize state and local authorities, militia, army engineers, the Coast Guard, and the American Red Cross.
With a grant from the Rockefeller Foundation, Hoover set up health units to work in the flooded regions for a year. These workers stamped out malaria, pellagra, and typhoid fever from many areas. His work during the flood brought Herbert Hoover to the front page of newspapers almost everywhere, and he gained new accolades as a humanitarian. The great victory of his relief work, he stressed, was not that the government rushed in and provided all assistance; it was that much of the assistance available was provided by private citizens and organizations in response to his appeals. "I suppose I could have called in the Army to help", he said, "but why should I, when I only had to call upon Main Street."
The treatment of African-Americans during the disaster endangered Hoover's reputation as a humanitarian. Local officials brutalized black farmers and prevented them from leaving relief camps, aid intended for African-American sharecroppers was often given instead to the landowners, and black men often were conscripted by locals into forced labor, sometimes at gun point. Knowing the potential damage to his presidential hopes if this became public, Hoover struck a deal with Robert Russa Moton, the prominent African-American successor to Booker T. Washington as president of the Tuskegee Institute. In exchange for keeping the sufferings of African-Americans quiet, Hoover promised unprecedented influence for African-Americans should he become president. Moton agreed, and following the accommodationist philosophy of Washington, he worked actively to conceal the information from the media.
Presidential election of 1928.
Republican primaries.
When President Calvin Coolidge announced in August 1927 that he would not seek a second full term of office in the 1928 presidential election, Hoover became the leading Republican candidate, despite the fact Coolidge was lukewarm on Hoover, often deriding his ambitious and popular Commerce Secretary as "Wonder Boy". Coolidge had been reluctant to choose Hoover as his successor; on one occasion he remarked that "for six years that man has given me unsolicited advice—all of it bad. I was particularly offended by his comment to 'shit or get off the pot'." Even so, Coolidge had no desire to split the party by publicly opposing the popular Commerce Secretary's nomination.
Hoover's only real challenger was Frank Orren Lowden, a former governor of Illinois. Hoover received much favorable press coverage in the months leading up to the convention. Lowden's campaign manager complained that newspapers were full of "nothing but advertisements for Herbert Hoover and Fletcher's Castoria". Hoover's reputation, experience, and popularity coalesced to give him the Republican nomination on the first ballot.
The delegates considered nominating Vice President Charles Dawes to be Hoover's running mate. But Coolidge (who hated Dawes) remarked that this would be "a personal affront" to him. The convention selected Senator Charles Curtis of Kansas.
General election.
Hoover campaigned for efficiency and the Republican record of prosperity against Democrat Alfred E. Smith. Smith likewise was a proponent of efficiency earned as governor of New York. Both candidates were pro-business, and each promised to improve conditions for farmers, reform immigration laws, and maintain America's isolationist foreign policy. Where they differed was on the Volstead Act which outlawed the sale of liquor and beer. Smith was a "wet" who called for its repeal, whereas Hoover gave limited support for prohibition, calling it an "experiment noble in purpose". His use of ""experiment"" suggested it was not permanent. While Smith won extra support among Catholics in the big cities Smith was the target of intense anti-Catholicism from some Protestant communities, especially as Southern Baptists and German Lutherans. Overall the religious factor worked to the advantage of Hoover, although he took no part in it.
Historians agree that Hoover's national reputation and the booming economy, combined with deep splits in the Democratic Party over religion and prohibition, guaranteed his landslide victory with 58% of the vote. Hoover's appeal to southern white voters succeeded in cracking the "Solid South", winning the Democratic strongholds of Florida, North Carolina, Virginia, Texas and Tennessee; the Deep South continued to support Smith as the Democratic candidate. This was the first time that a Republican candidate for president had carried Texas. This outraged the black leadership, which largely broke from the Republican Party, and began seeking candidates who supported civil rights within the Democratic Party.
Presidency (1929–1933).
Hoover held a press conference on his first day in office, promising a "new phase of press relations". He asked the group of journalists to elect a committee to recommend improvements to the White House press conference. Hoover declined to use a spokesman, instead asking reporters to directly quote him and giving them handouts with his statements ahead of time. In his first 120 days in office, he held more regular and frequent press conferences than any other President, before or since. However, he changed his press policies after the 1929 stock market crash, screening reporters and greatly reducing his availability.
Lou Henry Hoover was an activist First Lady. She typified the new woman of the post–World War I era: intelligent, robust, and aware of multiple female possibilities.
White House physician Admiral Joel T. Boone invented the sport Hooverball to keep Hoover fit while in the White House. Hooverball is a combination of volleyball and tennis, played with a medicine ball. Hoover and several staff members played it each morning, earning them the nickname "Medicine Ball Cabinet".
Domestic policies.
On poverty, Hoover said that "Given the chance to go forward with the policies of the last eight years, we shall soon with the help of God, be in sight of the day when poverty will be banished from this nation", and promised, "We in America today are nearer to the final triumph over poverty than ever before in the history of any land," but within months, the Stock Market Crash of 1929 occurred, and the world's economy spiraled downward into the Great Depression.
Hoover entered office with a plan to reform the nation's regulatory system, believing that a federal bureaucracy should have limited regulation over a country's economic system. A self-described progressive and reformer, Hoover saw the presidency as a vehicle for improving the conditions of all Americans by encouraging public-private cooperation—what he termed "volunteerism". Hoover saw volunteerism as preferable to governmental coercion or intervention which he saw as opposed to the American ideals of individualism and self-reliance. Long before entering politics, he had actually denounced "laissez-faire" thinking.
Hoover expanded civil service coverage of federal positions, canceled private oil leases on government lands, and by instructing the Justice Department and the Internal Revenue Service to pursue gangsters for tax evasion, he enabled the prosecution of mobster Al Capone. He appointed a commission that set aside 3,000,000 acres (12,000 km²) of national parks and 2,300,000 acres of national forests; advocated tax reduction for low-income Americans (not enacted); closed certain tax loopholes for the wealthy; doubled the number of veterans' hospital facilities; negotiated a treaty on St. Lawrence Seaway (which failed in the U.S. Senate); wrote a Children's Charter that advocated protection of every child regardless of race or gender; created an antitrust division in the Justice Department; required air mail carriers to adopt stricter safety measures and improve service; proposed federal loans for urban slum clearances (not enacted); organized the Federal Bureau of Prisons; reorganized the Bureau of Indian Affairs; instituted prison reform; proposed a federal Department of Education (not enacted); advocated $50-per-month pensions for Americans over 65 (not enacted); chaired White House conferences on child health, protection, homebuilding and home-ownership; began construction of the Boulder Dam (later renamed Hoover Dam); and signed the Norris–La Guardia Act that limited judicial intervention in labor disputes.
Foreign relations.
On November 19, 1928, President-elect Hoover embarked on a ten-nation goodwill tour of Latin America. He delivered twenty-five speeches, almost all of which stressed his plans to reduce American political and military interference in Latin American affairs. In sum, he pledged that the United States would act as a "good neighbor." Potential disaster was averted when an Argentine anarchist intent on assassinating Hoover was arrested. Hoover professed unconcern, tearing off the front page of a newspaper that revealed the plot and explaining, "It's just as well that Lou shouldn't see it."
Following the release in 1930 of the Clark Memorandum, Hoover began formulating what would become Roosevelt's Good Neighbor policy. He began withdrawing American troops from Nicaragua and Haiti; he also proposed an arms embargo on Latin America and a one-third reduction of the world's naval power, which was called the Hoover Plan. The Roosevelt Corollary ceased being part of U.S. foreign policy. In response to the Japanese invasion of Manchuria, he and Secretary of State Henry Stimson outlined the Hoover–Stimson Doctrine which held that the United States would not recognize territories gained by force.
Hoover mediated between Chile and Peru to solve a conflict on the sovereignty of Arica and Tacna, that in 1883 by the Treaty of Ancón had been awarded to Chile for ten years, to be followed by a plebiscite that had never happened. By the Tacna–Arica compromise at the Treaty of Lima in 1929, Chile kept Arica, and Peru regained Tacna.
Civil rights.
Hoover seldom mentioned civil rights while he was President. He believed that African-Americans and other races could improve themselves with education and individual initiative.
Hoover attempted to appoint John J. Parker of North Carolina to the Supreme Court in 1930 to replace Edward Sanford. The NAACP claimed that Parker had made many court decisions against African-Americans, and they fought the nomination. The NAACP was successful in gaining Senator William Borah's support and the nomination was defeated by two votes (39-41) in the Senate.
First Lady Lou Hoover defied custom and invited the wife of Republican Oscar DePriest, the only African-American member in Congress, to tea at the White House. Booker T. Washington was the previous African-American to have dined at the White House, with Theodore Roosevelt in 1901.
Charles Curtis, the nation's first Native American Vice President, was from the Kaw tribe in Kansas. Hoover's humanitarian and Quaker reputation, along with Curtis as a vice-president, gave special meaning to his Indian policies. His Quaker upbringing influenced his views that Native Americans needed to achieve economic self-sufficiency. As President, he appointed Charles J. Rhoads as commissioner of Indian affairs. Hoover supported Rhoads' commitment to Indian assimilation and sought to minimize the federal role in Indian affairs. His goal was to have Indians acting as individuals (not as tribes) and to assume the responsibilities of citizenship granted with the Indian Citizenship Act of 1924.
Great Depression.
Hoover had long been a proponent of the concept that public-private cooperation was the way to achieve high long-term growth. Hoover feared that too much government intervention would undermine long-term individuality and self-reliance, which he considered essential to the nation's future. Both his ideals and the economy were put to the test with the onset of the Great Depression.
Although many people at the time and for decades afterwards denounced Hoover for taking a hands-off ("laissez-faire") approach to the Depression, a few historians emphasize how active he actually was. Hoover said he rejected Treasury Secretary Andrew Mellon's suggested "leave-it-alone" approach, and called many business leaders to Washington to urge them not to lay off workers or cut wages.
Libertarian economist Murray Rothbard argues that Hoover was actually the initiator of what came to be the New Deal. Hoover engaged in many unprecedented public works programs, including an increase in the Federal Buildings program of over $400 million and the establishment of the Division of Public Construction to spur public works planning. Hoover himself granted more subsidies to ship construction through the Federal Shipping Board and asked for a further $175 million appropriation for public works; this was followed in July 1930 with the expenditure of a giant $915 million public works program, including a Hoover Dam on the Colorado River. In the spring of 1930, Hoover acquired from Congress an added $100 million to continue the Federal Farm Board lending and purchasing policies. At the end of 1929, the FFB established a national wool cooperative-the National Wool Marketing Corporation (NWMC) made up of 30 state associations. The Board also established an allied National Wool Credit Corporation to handle finances. A total of $31.5 million in loans for wool were made by the FFB, of which $12.5 million were permanently lost; these massive agricultural subsidies were a precedent for the later Agricultural Adjustment Act. Hoover also advocated strong labor regulation law, including the enactment of the Bacon-Davis Act, requiring a maximum eight-hour day on construction of public buildings and the payment of at least the "prevailing wage" in the locality, as well as the Norris-LaGuardia Act in 1932. In the Banking sector, Hoover passed The Federal Home Loan Bank Act in July, 1932, establishing 12 district banks ruled by a Federal Home Loan Bank Board in a manner similar to the Federal Reserve System. $125 million capital was subscribed by the Treasury and this was subsequently shifted to the RFC. Hoover was also instrumental in passing the Glass-Steagall Act of 1932, allowing for prime rediscounting at the Federal Reserve, allowing further inflation of credit and bank reserves.
Lee Ohanian, from UCLA, argues that Hoover adopted pro-labor policies after the 1929 stock market crash that "accounted for close to two-thirds of the drop in the nation's gross domestic product over the two years that followed, causing what might otherwise have been a bad recession to slip into the Great Depression". This argument is at odds with the more Keynesian view of the causes of the Depression, and has been challenged as revisionist by J. Bradford DeLong of U.C. Berkeley.
Calls for greater government assistance increased as the U.S. economy continued to decline. He was also a firm believer in balanced budgets (as were most Democrats), and was unwilling to run a budget deficit to fund welfare programs. However, Hoover did pursue many policies in an attempt to pull the country out of depression. In 1929, he authorized the Mexican Repatriation program to help unemployed Mexican citizens return home. The program was largely a forced migration of approximately 500,000 people to Mexico, and continued until 1937. In June 1930, over the objection of many economists, Congress approved and Hoover reluctantly signed into law the Smoot–Hawley Tariff Act. The legislation raised tariffs on thousands of imported items. The intent of the Act was to encourage the purchase of American-made products by increasing the cost of imported goods, while raising revenue for the federal government and protecting farmers. However, economic depression had spread worldwide, and Canada, France and other nations retaliated by raising tariffs on imports from the U.S. The result was to contract international trade, and worsen the Depression.
In 1931, Hoover issued the Hoover Moratorium, calling for a one-year halt in reparation payments by Germany to France and in the payment of Allied war debts to the United States. The plan was met with much opposition, especially from France, who saw significant losses to Germany during World War I. The Moratorium did little to ease economic declines. As the moratorium neared its expiration the following year, an attempt to find a permanent solution was made at the Lausanne Conference of 1932. A working compromise was never established, and by the start of World War II, reparations payments had stopped completely. Hoover in 1931 urged the major banks in the country to form a consortium known as the National Credit Corporation (NCC).
In the U.S. by 1932 unemployment had reached 24.9%, businesses defaulted on record numbers of loans, and more than 5,000 banks had failed. Hundreds of thousands of Americans found themselves homeless and began congregating in the numerous Hoovervilles (shanty towns) that sprang up in major cities.
Congress, desperate to increase federal revenue, enacted the Revenue Act of 1932, which was the largest peacetime tax increase in history. The Act increased taxes across the board, so that top earners were taxed at 63% on their net income. The 1932 Act also increased the tax on the net income of corporations from 12% to 13.75%.
The final attempt of the Hoover Administration to rescue the economy occurred in 1932 with the passage of the Emergency Relief and Construction Act, which authorized funds for public works programs and the creation of the Reconstruction Finance Corporation (RFC). The RFC's initial goal was to provide government-secured loans to financial institutions, railroads and farmers. The RFC had minimal impact at the time, but was adopted by President Franklin D. Roosevelt and greatly expanded as part of his New Deal.
Taxes, revenues, and deficits.
To pay for these and other government programs and to make up for revenue lost due to the Depression, in addition to the Revenue Act of 1932 Hoover agreed to roll back several tax cuts that his Administration had enacted on upper incomes. The estate tax was doubled and corporate taxes were raised by almost 15%. Also, a "check tax" was included that placed a 2-cent tax (over 30 cents in today's economy) on all bank checks. Economists William D. Lastrapes and George Selgin, conclude that the check tax was "an important contributing factor to that period's severe monetary contraction". Hoover also encouraged Congress to investigate the New York Stock Exchange, and this pressure resulted in various reforms.
Bonus Army.
Thousands of World War I veterans and their families demonstrated and camped out in Washington, DC, during June 1932, calling for immediate payment of a bonus that had been promised by the World War Adjusted Compensation Act in 1924 for payment in 1945. Although offered money by Congress to return home, some members of the "Bonus army" remained. Washington police attempted to remove the demonstrators from their camp, but they were outnumbered and unsuccessful. Shots were fired by the police in a futile attempt to attain order, and two protesters were killed while many officers were injured. Hoover sent U.S. Army forces led by General Douglas MacArthur and helped by lower ranking officers Dwight D. Eisenhower and George S. Patton to stop a march. MacArthur, believing he was fighting a communist revolution, chose to clear out the camp with military force. In the ensuing clash, hundreds of civilians were injured. Hoover had sent orders that the Army was not to move on the encampment, but MacArthur chose to ignore the command. Hoover was incensed, but refused to reprimand MacArthur. The entire incident was another devastating negative for Hoover in the 1932 election. That led New York governor and Democratic presidential candidate Franklin Roosevelt to declare of Hoover: "There is nothing inside the man but jelly!"
Supreme Court appointments.
Hoover appointed the following Justices to the Supreme Court of the United States:
Hoover broke party lines to appoint the Democrat Cardozo. He explained that he "was one of the ancient believers that the Supreme Court should have a strong minority of the opposition's party and that all appointments should be made from experienced jurists. When the vacancy came... canvassed all the possible Democratic jurists and immediately concluded that Justice Cardozo was the right man and appointed him."
1932 re-election campaign.
Although Hoover had come to detest the presidency, he agreed to run again in 1932, not only as a matter of pride, but also because he feared that no other likely Republican candidate would deal with the depression without resorting to what Hoover considered dangerously radical measures.
Hoover was nominated by the Republicans for a second term. He had originally planned to make only one or two major speeches, and to leave the rest of the campaigning to proxies, but when polls showed that the entire Republican ticket could be facing a resounding defeat, Hoover agreed to an expanded schedule of public addresses. In his nine major radio addresses Hoover primarily defended his administration and his philosophy. The apologetic approach did not allow Hoover to refute Democratic nominee Franklin Roosevelt's charge that he was personally responsible for the depression.
In his campaign trips around the country, Hoover was faced with perhaps the most hostile crowds of any sitting president. Besides having his train and motorcades pelted with eggs and rotten fruit, he was often heckled while speaking, and on several occasions, the Secret Service halted attempts to kill Hoover by disgruntled citizens, including capturing one man nearing Hoover carrying sticks of dynamite, and another already having removed several spikes from the rails in front of the President's train.
Franklin D. Roosevelt blasted Hoover for spending and taxing too much, increasing national debt, raising tariffs and blocking trade, as well as placing millions on the government dole. Roosevelt attacked Hoover for "reckless and extravagant" spending, of thinking "that we ought to center control of everything in Washington as rapidly as possible". Roosevelt's running mate, John Nance Garner, accused the Republican of "leading the country down the path of socialism".
Osro Cobb, a leader of the Republican Party in Arkansas who became politically and personally close to Hoover, recalls:
President Hoover had become convinced that the Democrats deliberately were destroying the economy of the country and erecting roadblocks against every measure he offered to the Congress to restore balance to the economy ... all for the purpose of winning an election. Just a few weeks before the 1932 election, we were standing near a window in the Oval Office. His cigar was frayed and out, and he was in deep thought and obviously troubled. He turned aside and said that he had accepted a speaking engagement in Des Moines, Iowa, in three days and that the U.S. Secret Service had warned him that it had uncovered evidence of plots by radical elements to assassinate him if he kept it. Turmoil and uncertainty prevailed in the country, but there was absolutely no fear in his expression; to the contrary, there appeared to be an abundance of personal courage. Frankly, my heart went out to him, but I pointed out that fate and destiny played a part in the lives of all presidents and that I felt all possible precautions should be taken to protect him but that he should appear and make one of the greatest speeches of his administration. He smiled and said, "Osro, that's what I have already decided to do. Your concurrence is comforting." ...
Despite the late campaign endeavors, Hoover sustained a large defeat in the election, having procured only 39.7 percent of the popular vote to Roosevelt's 57.4 percent. Hoover's popular vote was reduced by 26 percentage points from his result in the 1928 election. In the electoral college he carried only Pennsylvania, Delaware, and four other Northeastern states to lose 59–472. The Democrats extended their control over the U.S. House and gained control of the U.S. Senate.
After the election, Hoover requested that Roosevelt retain the Gold standard as the basis of the US currency, and in effect, continue many of the Hoover Administration's economic policies. Roosevelt refused.
Post-presidency.
New Deal.
Hoover departed from Washington in March 1933 with some bitterness, disappointed both that he had been repudiated by the voters and unappreciated for his best efforts. The Hoovers went first to New York City, where they stayed for a while in the Waldorf-Astoria Hotel. Later that spring, they returned to California to their Stanford residence. Hoover enjoyed returning to the men's clubs that he had long been involved with, including the Bohemian Club, the Pacific-Union Club, and the University Club in San Francisco.
Hoover liked to drive his car, accompanied by his wife or a friend (former Presidents did not get Secret Service protection until the 1960s), and drive on wandering journeys, visiting Western mining camps or small towns where he often went unrecognized, or heading up to the mountains, or deep into the woods, to go fishing in relative solitude. A year before his death, his own fishing days behind him, he published "Fishing For Fun—And To Wash Your Soul", the last of more than sixteen books in his lifetime.
Although many of his friends and supporters called upon Hoover to speak out against FDR's New Deal and to assume his place as the voice of the "loyal opposition", he refused to do so for many years after leaving the White House, and he largely kept himself out of the public spotlight until late in 1934. However, that did not stop rumors springing up about him, often fanned by Democratic politicians who found the former President to be a convenient scapegoat.
The relationship between Hoover and Roosevelt was one of the most severely strained in Presidential history. Hoover had little good to say about his successor. FDR, in turn, supposedly engaged in various petty official acts aimed at his predecessor, ranging from dropping him from the White House birthday greetings message list to having Hoover's name struck from the Hoover Dam on the Colorado River, which would officially be known only as Boulder Dam for many years to come.
In 1936, Hoover entertained hopes of receiving the Republican presidential nomination again, and thus facing Roosevelt in a rematch. Although he retained strong support among some delegates, there was never much hope of his being selected. He publicly endorsed the nominee, Kansas Governor Alf Landon. But Hoover might as well have been the nominee, since the Democrats virtually ignored Landon, and they ran against the former President himself, constantly attacking him in speeches and warning that a Landon victory would put Hoover back in the White House as the secret power "behind the throne". Roosevelt won 46 of the 48 states, burying Landon in the Electoral College, and the Republican Party in Congress in another landslide.
Although Hoover's reputation was at its low point, circumstances began to rehabilitate his name and restore him to prominence. Roosevelt overreached on his Supreme Court packing plan, and a further financial recession in 1937 and 1938 tarnished his image of invincibility.
In 1939, former President Herbert Hoover became the first Honorary Chairman of Tolstoy Foundation in Valley Cottage, New York, served in this capacity until his death in 1964.
By 1940, Hoover was again being spoken of as the possible nominee of the party in the presidential election. Although he trailed in the polls behind Thomas Dewey, Arthur Vandenberg, and his own former protege, Robert A. Taft, he still had considerable first-ballot delegate strength, and it was believed that if the convention deadlocked between the leading candidates, the party might turn to him as its compromise. However, the convention nominated the utility company president Wendell Willkie, who had supported Roosevelt in 1932 but turned against him after the creation of the Tennessee Valley Authority forced him to sell his company. Hoover dutifully supported Willkie, although he despaired that the nominee endorsed a platform that, to Hoover, was little more than the New Deal in all but name.
Road to war and World War II.
Hoover visited 10 European countries in March 1938, the month of Nazi Germany's "Anschluss" of Austria, and stated "I do not believe a widespread war is at all probable in the near future. There is a general realization everywhere ... that civilization as we know it cannot survive another great war." Like many, he initially believed that the European Allies would be able to contain Germany, and that Imperial Japan would not attack American interests in the Pacific.
Unlike Roosevelt's administration, Hoover was a vocal supporter of providing relief to countries in Nazi-occupied Europe. He was instrumental in creating the Commission for Polish Relief and Finnish Relief Fund.
When the Germans overran France and then had Britain held in a stalemate, many Americans saw Britain as on the verge of collapse. Nonetheless, Hoover declared that it would be folly for the United States to declare war on Germany and to rush to save the United Kingdom. Rather, he held, it was far wiser for this nation to devote itself to building up its own defenses, and to wash its hands of the mess in Europe. He called for a "Fortress America" concept, in which the United States, protected on the East and on the West by vast oceans patrolled by its Navy and its Air Corps (the USAAF), could adequately repel any attack on the Americas.
During a radio broadcast on June 29, 1941, one week after the Nazi invasion of the Soviet Union, Hoover disparaged any "tacit alliance" between the U.S. and the USSR by saying:
When the United States entered the war following the December 7, 1941, Japanese attack on Pearl Harbor, Hoover swept aside all feelings of neutrality and called for total victory. He offered himself to the government in any capacity necessary, but the Roosevelt Administration did not call upon him to serve.
Post–World War II.
Following World War II, Hoover became friends with President Harry S. Truman. Hoover joked that they were for many years the sole members of the "trade union" of former Presidents (since Calvin Coolidge and Roosevelt had died). Because of Hoover's experience with Germany at the end of World War I, in 1946 President Truman selected the former president to tour Germany to ascertain the food needs of the occupied nation. Hoover toured what was to become West Germany in Hermann Göring's old train coach and produced a number of reports critical of U.S. occupation policy. The economy of Germany had "sunk to the lowest level in a hundred years". He stated in one report:
On Hoover's initiative, a school meals program in the American and British occupation zones of Germany was begun on April 14, 1947. The program served 3,500,000 children aged six through 18. A total of 40,000 tons of American food was provided during the "Hooverspeisung" (Hoover meals).
In 1947, President Harry S. Truman appointed Hoover to a commission, which elected him chairman, to reorganize the executive departments. This became known as the Hoover Commission. He was appointed chairman of a similar commission by President Dwight D. Eisenhower in 1953. Both found numerous inefficiencies and ways to reduce waste. The government enacted most of the recommendations that the two commissions had made: 71% of the first commission's and 64% of the second commission's.
Throughout the Cold War, Hoover, always an opponent of Marxism, became even more outspokenly anti-Communist. However, he vehemently opposed American involvement in the Korean War, saying that "To commit the sparse ground forces of the non-communist nations into a land war against this communist land mass Asia would be a war without victory, a war without a successful political terminal... that would be the graveyard of millions of American boys and the exhaustion of the United States."
Despite his advancing years, Hoover continued to work nearly full-time both on writing (among his literary works is "The Ordeal of Woodrow Wilson", a bestseller, and the first time one former President had ever written a biography about another), as well as overseeing the Hoover Institution at Stanford University, which housed not only his own professional papers, but also those of a number of other former high ranking governmental and military servants. He also threw himself into fund-raising for the Boys Clubs (now the Boys & Girls Clubs of America), which became his pet charity.
Final years and death.
From Coolidge's death in 1933 to Dwight D. Eisenhower's last day of serving the presidency in 1961, Hoover had been the only living Republican former president. In 1960, Hoover appeared at his final Republican National Convention. Since the 1948 convention, he had been feted as the guest of "farewell" ceremonies (the unspoken assumption being that the aging former President might not survive until the next convention). Joking to the delegates, he said, "Apparently, my last three good-byes didn't take." Although he lived to see the 1964 convention, ill health prevented him from attending. The Presidential nominee Barry Goldwater acknowledged Hoover's absence in his acceptance speech. In 1962, Hoover had a malignant intestinal tumor removed. Ten months later he had severe gastrointestinal bleeding and seemed terminally ill and frail, but his mind was clear and he maintained a great deal of correspondence. Although the illness would get worse over time, he refused to be hospitalized.
Hoover died following massive internal bleeding at the age of 90 in his New York City suite at 11:35 a.m. on October 20, 1964, 31 years, seven months, and sixteen days after leaving office. At the time of his death, he had the longest retirement of any President. Former President Jimmy Carter surpassed the length of Hoover's retirement on September 7, 2012. At the time of Hoover's death he was the second longest-lived president after John Adams; both were since surpassed by Gerald Ford, Ronald Reagan, George H. W. Bush, and Jimmy Carter. He had outlived by 20 years his wife, Lou Henry Hoover, who had died in 1944, and he was the last living member of the Coolidge administration. He also outlived both his successor Franklin D. Roosevelt, and Eleanor Roosevelt who died in 1945 and 1962, respectively. From Coolidge's death on January 5, 1933, until Roosevelt was sworn into office on March 4 of that year, Hoover had the distinction of being President with no living ex-Presidents. Upon leaving office, Hoover was the only living ex-President for nearly 19 years, until Harry Truman left office in 1953. 
By the time of his death, he had rehabilitated his image. His birthplace in Iowa and an Oregon home where he lived as a child, became National Landmarks during his lifetime. His Rapidan fishing camp in Virginia, which he had donated to the government in 1933, is now a National Historic Landmark within the Shenandoah National Park. Hoover and his wife are buried at the Herbert Hoover Presidential Library and Museum in West Branch, Iowa. Hoover was honored with a state funeral, the last of three in a span of 12 months, coming as it did just after the deaths of President John F. Kennedy and General Douglas MacArthur. Former Chaplain of the Senate Frederick Brown Harris officiated. All three had two things in common: the commanding general of the Military District of Washington during those funerals was Army Major General Philip C. Wehle and the riderless horse was Black Jack, who also served in that role during Lyndon B. Johnson's funeral.
Writing.
Hoover began his magnum opus "Freedom Betrayed" in 1944 as part of a proposed autobiography. This turned into a significant work critiquing the foreign policy of the United States during the period from the 1930s to 1945. Essentially an attack on the statesmanship of Franklin D. Roosevelt, Hoover completed this work in his 90th year but it was not published until the historian George H. Nash took on the task of editing it. Significant themes are his belief that the western democratic powers should have let Nazi Germany and Soviet Russia assail and weaken each other, and opposition to the British guarantee of Poland's independence.
Heritage and memorials.
The Herbert Hoover Presidential Library and Museum is located in West Branch, Iowa next to the Herbert Hoover National Historic Site. The library is one of thirteen presidential libraries run by the National Archives and Records Administration. The Lou Henry and Herbert Hoover House, built in 1919 in Stanford, California, is now the official residence of the president of Stanford University, and a National Historic Landmark. Hoover's rustic rural presidential retreat, Rapidan Camp (also known as Camp Hoover) in the Shenandoah National Park, Virginia, has been restored and opened to the public. The Hoover Dam is named in his honor, as are numerous elementary, middle, and high schools across the United States.
On December 10, 2008, Hoover's great-granddaughter Margaret Hoover and Senate of Puerto Rico President Kenneth McClintock unveiled a life-sized bronze statue of Hoover at Puerto Rico's Territorial Capitol. The statue is one of seven honoring Presidents who have visited the United States territory during their term of office.
One line in the "All in the Family" theme song—an ironic exercise in pre–New Deal nostalgia—says "Mister, we could use a man like Herbert Hoover again".
The Belgian city of Leuven named a square in the city center after Hoover, honoring him for his work as chairman of the "Commission for Relief in Belgium" during World War I. The square is near the Central Library of the Catholic University of Leuven, where a bust of the president can be seen.
The Polish capital of Warsaw also has a square named after Hoover alongside the Royal Route leading to the Old Town.
George Burroughs Torrey painted a portrait of him.
The historic townsite of Gwalia, Western Australia contains the Sons of Gwalia Museum and the Hoover House Bed and Breakfast, the renovated and restored Mining Engineers residence that was the original residence of Herbert Hoover and where he stayed in subsequent visits to the mine during the first decade of the twentieth century.

</doc>
<doc id="13684" url="https://en.wikipedia.org/wiki?curid=13684" title="Hildegard of Bingen">
Hildegard of Bingen

Hildegard of Bingen, O.S.B. (; ) (1098 – 17 September 1179), also known as Saint Hildegard and Sibyl of the Rhine, was a German Benedictine abbess, writer, composer, philosopher, Christian mystic, visionary, and polymath. She is considered to be the founder of scientific natural history in Germany.
Hildegard was elected "magistra" by her fellow nuns in 1136; she founded the monasteries of Rupertsberg in 1150 and Eibingen in 1165. One of her works as a composer, the "Ordo Virtutum", is an early example of liturgical drama and arguably the oldest surviving morality play. She is also noted for the invention of a constructed language known as "Lingua Ignota".
Although the history of her formal consideration is complicated, she has been recognized as a saint by branches of the Roman Catholic Church for centuries. On 7 October 2012, Pope Benedict XVI named her a Doctor of the Church.
Biography.
She was born around the year 1098, although the exact date is uncertain. Her parents were Mechtild of Merxheim-Nahet and Hildebert of Bermersheim, a family of the free lower nobility in the service of the Count Meginhard of Sponheim. Sickly from birth, Hildegard is traditionally considered their youngest and tenth child, although there are records of seven older siblings. In her "Vita", Hildegard states that from a very young age she had experienced visions.
Monastic life.
Perhaps because of Hildegard's visions, or as a method of political positioning (or both), Hildegard's parents offered her as an oblate to the Benedictine monastery at the Disibodenberg, which had been recently reformed in the Palatinate Forest. The date of Hildegard's enclosure at the monastery is the subject of debate. Her "Vita" says she was professed with an older woman, Jutta, the daughter of Count Stephan II of Sponheim, at the age of eight. However, Jutta's date of enclosure is known to have been in 1112, when Hildegard would have been fourteen. Some scholars speculate that Hildegard was placed in the care of Jutta at the age of eight, and the two women were then enclosed together six years later.
In any case, Hildegard and Jutta were enclosed together at the Disibodenberg, and formed the core of a growing community of women attached to the male monastery. Jutta was also a visionary and thus attracted many followers who came to visit her at the cloister. Hildegard tells us that Jutta taught her to read and write, but that she was unlearned and therefore incapable of teaching Hildegard sound biblical interpretation. The written record of the "Life of Jutta" indicates that Hildegard probably assisted her in reciting the psalms, working in the garden and other handiwork, and tending to the sick. This might have been a time when Hildegard learned how to play the ten-stringed psaltery. Volmar, a frequent visitor, may have taught Hildegard simple psalm notation. The time she studied music could have been the beginning of the compositions she would later create.
Upon Jutta's death in 1136, Hildegard was unanimously elected as "magistra" of the community by her fellow nuns. Abbot Kuno of Disibodenberg asked Hildegard to be Prioress, which would be under his authority. Hildegard, however, wanted more independence for herself and her nuns, and asked Abbot Kuno to allow them to move to Rupertsberg. This was to be a move towards poverty, from a stone complex that was well established to a temporary dwelling place. When the abbot declined Hildegard's proposition, Hildegard went over his head and received the approval of Archbishop Henry I of Mainz. Abbot Kuno did not relent until Hildegard was stricken by an illness that kept her paralyzed and unable to move from her bed, an event that she attributed to God's unhappiness at her not following his orders to move her nuns to Rupertsberg. It was only when the Abbot himself could not move Hildegard that he decided to grant the nuns their own monastery. Hildegard and about twenty nuns thus moved to the St. Rupertsberg monastery in 1150, where Volmar served as provost, as well as Hildegard's confessor and scribe. In 1165 Hildegard founded a second monastery for her nuns at Eibingen.
Visions.
Hildegard says that she first saw "The Shade of the Living Light" at the age of three, and by the age of five she began to understand that she was experiencing visions. She used the term 'visio' to this feature of her experience, and recognized that it was a gift that she could not explain to others. Hildegard explained that she saw all things in the light of God through the five senses: sight, hearing, taste, smell, and touch. Hildegard was hesitant to share her visions, confiding only to Jutta, who in turn told Volmar, Hildegard's tutor and, later, secretary. Throughout her life, she continued to have many visions, and in 1141, at the age of 42, Hildegard received a vision she believed to be an instruction from God, to "write down that which you see and hear." Still hesitant to record her visions, Hildegard became physically ill. The illustrations recorded in the book of Scivias were visions that Hildegard experienced, causing her great suffering and tribulations. In her first theological text, "Scivias" ("Know the Ways"), Hildegard describes her struggle within:
But I, though I saw and heard these things, refused to write for a long time through doubt and bad opinion and the diversity of human words, not with stubbornness but in the exercise of humility, until, laid low by the scourge of God, I fell upon a bed of sickness; then, compelled at last by many illnesses, and by the witness of a certain noble maiden of good conduct nun Richardis von Stade and of that man whom I had secretly sought and found, as mentioned above, I set my hand to the writing. While I was doing it, I sensed, as I mentioned before, the deep profundity of scriptural exposition; and, raising myself from illness by the strength I received, I brought this work to a close – though just barely – in ten years. (...) And I spoke and wrote these things not by the invention of my heart or that of any other person, but as by the secret mysteries of God I heard and received them in the heavenly places. And again I heard a voice from Heaven saying to me, 'Cry out therefore, and write thus!'
It was between November 1147 and February 1148 at the synod in Trier that Pope Eugenius heard about Hildegard’s writings. It was from this that she received Papal approval to document her visions as revelations from the Holy Spirit giving her instant credence.
Before Hildegard’s death, a problem arose with the clergy of Mainz. A man buried in Rupertsburg had died after excommunication from the Church. Therefore, the clergy wanted to remove his body from the sacred ground. Hildegard did not accept this idea, replying that it was a sin and that the man had been reconciled to the church at the time of his death.
On 17 September 1179, when Hildegard died, her sisters claimed they saw two streams of light appear in the skies and cross over the room where she was dying.
"Vita Sanctae Hildegardis".
Hildegard's hagiography, "Vita Sanctae Hildegardis", was compiled by the monk Theoderic of Echternach after Hildegard's death. He included the hagiographical work "Libellus" or "Little Book" begun by Godfrey of Disibodenberg. Godfrey had died before he was able to complete his work. Guibert of Gembloux was invited to finish the work; however, he had to return to his monastery with the project unfinished. Theoderic utilized sources Guibert had left behind to complete the "Vita".
Works.
Hildegard's works include three great volumes of visionary theology; a variety of musical compositions for use in liturgy, as well as the musical morality play "Ordo Virtutum"; one of the largest bodies of letters (nearly 400) to survive from the Middle Ages, addressed to correspondents ranging from Popes to Emperors to abbots and abbesses, and including records of many of the sermons she preached in the 1160s and 1170s; two volumes of material on natural medicine and cures; an invented language called the "Lingua ignota" ("unknown language"); and various minor works, including a gospel commentary and two works of hagiography.
Several manuscripts of her works were produced during her lifetime, including the illustrated Rupertsberg manuscript of her first major work, "Scivias" (lost since 1945); the Dendermonde manuscript, which contains one version of her musical works; and the Ghent manuscript, which was the first fair-copy made for editing of her final theological work, the "Liber Divinorum Operum". At the end of her life, and probably under her initial guidance, all of her works were edited and gathered into the single Riesenkodex manuscript.
Visionary theology.
Hildegard's most significant works were her three volumes of visionary theology: "Scivias" ("Know the Ways", composed 1142-1151), "Liber Vitae Meritorum" ("Book of Life's Merits" or "Book of the Rewards of Life", composed 1158-1163); and "Liber Divinorum Operum" ("Book of Divine Works", also known as "De operatione Dei", "On God's Activity", composed 1163/4-1172 or 1174). In these volumes, the last of which was completed when she was well into her seventies, Hildegard first describes each vision, whose details are often strange and enigmatic; and then interprets their theological contents in the words of the "voice of the Living Light."
"Scivias".
The composition of the first work, "Scivias", was triggered by the insistence of her visionary experiences in about 1142, when she was already forty-three years old. Perceiving a divine command to "write down what you see and hear", Hildegard began to record her visionary experiences. "Scivias" is structured into three parts of unequal length. The first part (six visions) chronicles the order of God's creation: the Creation and Fall of Adam and Eve, the structure of the universe (famously described as an "egg"), the relationship between body and soul, God's relationship to his people through the Synagogue, and the choirs of angels. The second part (seven visions) describes the order of redemption: the coming of Christ the Redeemer, the Trinity, the Church as the Bride of Christ and the Mother of the Faithful in baptism and confirmation, the orders of the Church, Christ's sacrifice on the Cross and the Eucharist, and the fight against the devil. Finally, the third part (thirteen visions) recapitulates the history of salvation told in the first two parts, symbolized as a building adorned with various allegorical figures and virtues. It concludes with the Symphony of Heaven, an early version of Hildegard's musical compositions.
Portions of the uncompleted work were read aloud to Pope Eugenius III at the Synod of Trier in 1148, after which he sent Hildegard a letter with his blessing. This blessing was later construed as papal approval for all of Hildegard's wide-ranging theological activities. Towards the end of her life, Hildegard commissioned a richly decorated manuscript of "Scivias" (the Rupertsberg Codex); although the original has been lost since its evacuation to Dresden for safekeeping in 1945, its images are preserved in a hand-painted facsimile from the 1920s.
"Liber Vitae Meritorum".
In her second volume of visionary theology, composed between 1158 and 1163, after she had moved her community of nuns into independence at the Rupertsberg in Bingen, Hildegard tackled the moral life in the form of dramatic confrontations between the virtues and the vices. She had already explored this area in her musical morality play, "Ordo Virtutum", and the "Book of the Rewards of Life" takes up that play's characteristic themes. Each vice, although ultimately depicted as ugly and grotesque, nevertheless offers alluring, seductive speeches that attempt to entice the unwary soul into their clutches. Standing in our defense, however, are the sober voices of the Virtues, powerfully confronting every vicious deception.
Amongst the work's innovations is one of the earliest descriptions of purgatory as the place where each soul would have to work off its debts after death before entering heaven. Hildegard's descriptions of the possible punishments there are often gruesome and grotesque, which emphasize the work's moral and pastoral purpose as a practical guide to the life of true penance and proper virtue.
"Liber Divinorum Operum".
Hildegard's last and grandest visionary work had its genesis in one of the few times she experienced something like an ecstatic loss of consciousness. As she described it in an autobiographical passage included in her Vita, sometime in about 1163, she received "an extraordinary mystical vision" in which was revealed the "sprinkling drops of sweet rain" that John the Evangelist experienced when he wrote, "In the beginning was the Word..." (John 1:1). Hildegard perceived that this Word was the key to the "Work of God", of which humankind is the pinnacle. The "Book of Divine Works", therefore, became in many ways an extended explication of the Prologue to John's Gospel.
The ten visions of this work's three parts are cosmic in scale, often populated by the grand allegorical female figures representing Divine Love ("Caritas") or Wisdom ("Sapientia"). The first of these opens the work with a salvo of poetic and visionary images, swirling about to characterize the dynamic activity of God within the scope of his salvation-historical work. The remaining three visions of the first part introduce the famous image of a human being standing astride the spheres that make up the universe, and detail the intricate relationships between the human as microcosm and the universe as macrocosm. This culminates in the final chapters of Part One, Vision Four with Hildegard's direct rumination on the meaning of "In the beginning was the Word..." (John 1:1). The single vision that comprises the whole of Part Two stretches that rumination back to the opening of Genesis, and forms an extended meditation on the six days of the creation of the world. Finally, the five visions of the third part take up again the building imagery of "Scivias" to describe the course of salvation history.
Music.
Attention in recent decades to women of the medieval Church has led to a great deal of popular interest in Hildegard's music. In addition to the "Ordo Virtutum," sixty-nine musical compositions, each with its own original poetic text, survive, and at least four other texts are known, though their musical notation has been lost. This is one of the largest repertoires among medieval composers.
One of her better known works, "Ordo Virtutum" ("Play of the Virtues"), is a morality play. It is uncertain when some of Hildegard’s compositions were composed, though the "Ordo Virtutum" is thought to have been composed as early as 1151. The morality play consists of monophonic melodies for the Anima (human soul) and 16 Virtues. There is also one speaking part for the Devil. Scholars assert that the role of the Devil would have been played by Volmar, while Hildegard's nuns would have played the parts of Anima and the Virtues.
In addition to the "Ordo Virtutum" Hildegard composed many liturgical songs that were collected into a cycle called the "Symphonia armoniae celestium revelationum." The songs from the Symphonia are set to Hildegard's own text and range from antiphons, hymns, and sequences, to responsories. Her music is described as monophonic, that is, consisting of exactly one melodic line. Its style is characterized by soaring melodies that can push the boundaries of the more staid ranges of traditional Gregorian chant. Though Hildegard's music is often thought to stand outside the normal practices of monophonic monastic chant, current researchers are also exploring ways in which it may be viewed in comparison with her contemporaries, such as Hermannus Contractus. Another feature of Hildegard's music that both reflects twelfth-century evolutions of chant and pushes those evolutions further is that it is highly melismatic, often with recurrent melodic units. Scholars such as Margot Fassler, Marianne Richert Pfau, and Beverly Lomer also note the intimate relationship between music and text in Hildegard's compositions, whose rhetorical features are often more distinct than is common in twelfth-century chant. As with all medieval chant notation, Hildegard's music lacks any indication of tempo or rhythm; the surviving manuscripts employ late German style notation, which uses very ornamental neumes. The reverence for the Virgin Mary reflected in music shows how deeply influenced and inspired Hildegard of Bingen and her community were by the Virgin Mary and the saints.
The definition of viriditas or "greenness" is an earthly expression of the heavenly in an integrity that overcomes dualisms. This greenness or power of life appears frequently in Hildegard's works.
Despite Hildegard's self-professed view that her compositions have as object the praise of God, one scholar has asserted that Hildegard made a close association between music and the female body in her musical compositions. According to him, the poetry and music of Hildegard’s Symphonia would therefore be concerned with the anatomy of female desire thus described as Sapphonic, or pertaining to Sappho, connecting her to a history of female rhetoricians.
Scientific and medicinal writings.
Hildegard’s medicinal and scientific writings, though thematically complementary to her ideas about nature expressed in her visionary works, are different in focus and scope. Neither claim to be rooted in her visionary experience and its divine authority. Rather, they spring from her experience helping in and then leading the monastery’s herbal garden and infirmary, as well as the theoretical information she likely gained through her wide-ranging reading in the monastery’s library. As she gained practical skills in diagnosis, prognosis, and treatment, she combined physical treatment of physical diseases with holistic methods centered on “spiritual healing.” She became well known for her healing powers involving practical application of tinctures, herbs, and precious stones. She combined these elements with a theological notion ultimately derived from Genesis: all things put on earth are for the use of humans.
Hildegard catalogued both her theory and practice in two works. The first, "Physica," contains nine books that describe the scientific and medicinal properties of various plants, stones, fish, reptiles, and animals. The second, "Causae et Curae", is an exploration of the human body, its connections to the rest of the natural world, and the causes and cures of various diseases. Hildegard documented various medical practices in these books, including the use of bleeding and home remedies for many common ailments. She also explains remedies for common agricultural injuries such as burns, fractures, dislocations, and cuts. Hildegard may have used the books to teach assistants at the monastery. These books are historically significant because they show areas of medieval medicine that were not well documented because their practitioners (mainly women) rarely wrote in Latin.
In addition to its wealth of practical evidence, "Causae et Curae" is also noteworthy for its organizational scheme. Its first part sets the work within the context of the creation of the cosmos and then humanity as its summit, and the constant interplay of the human person as microcosm both physically and spiritually with the macrocosm of the universe informs all of Hildegard’s approach. Her hallmark is to emphasize the vital connection between the “green” health of the natural world and the holistic health of the human person. Thus, when she approached medicine as a type of gardening, it was not just as an analogy. Rather, Hildegard understood the plants and elements of the garden as direct counterparts to the humors and elements within the human body, whose imbalance led to illness and disease.
Thus, the nearly three hundred chapters of the second book of "Causae et Curae" “explore the etiology, or causes, of disease as well as human sexuality, psychology, and physiology.” In this section, she give specific instructions for bleeding based on various factors, including gender, the phase of the moon (bleeding is best done when moon is waning), the place of disease (use veins near diseased organ of body part) or prevention (big veins in arms), and how much blood to take (described in imprecise measurements, like “the amount that a thirsty person can swallow in one gulp”). She even includes bleeding instructions for animals to keep them healthy. In the third and fourth sections, Hildegard describes treatments for malignant and minor problems and diseases according to the humoral theory, again including information on animal health. The fifth section is about diagnosis and prognosis, which includes instructions to check the patient’s blood, pulse, urine and stool. Finally, the sixth section documents a lunar horoscope to provide an additional means of prognosis for both disease and other medical conditions, such as conception and the outcome of pregnancy. For example, she indicates that a waxing moon is good for human conception and is also good for sowing seeds for plants (sowing seeds is the plant equivalent of conception). Elsewhere, Hildegard is even said to have stressed the value of boiling drinking water in an attempt to prevent infection.
As Hildegard elaborates the medical and scientific relationship between the human microcosm and the macrocosm of the universe, she often focuses on interrelated patterns of four: “the four elements (fire, air, water, and earth), the four seasons, the four humors, the four zones of the earth, and the four major winds.” Although she inherited the basic framework of humoral theory from ancient medicine, however, Hildegard’s conception of the hierarchical interbalance of the four humors (blood, phlegm, black bile, and yellow bile) was unique, based on their correspondence to “superior” and “inferior” elements—blood and phlegm corresponding to the “celestial” elements of fire and air, and the two biles corresponding to the “terrestrial” elements of water and earth. Hildegard understood the disease-causing imbalance of these humors to result from the improper dominance of the subordinate humors. This disharmony reflects that introduced by Adam and Eve in the Fall, which for Hildegard marked the indelible entrance of disease and humoral imbalance into humankind. As she writes in "Causae et Curae" c. 42:
It happens that certain men suffer diverse illnesses. This comes from the phlegm which is superabundant within them. For if man had remained in paradise, he would not have had the "flegmata" within his body, from which many evils proceed, but his flesh would been whole and without dark humor ["livor"]. However, because he consented to evil and relinquished good, he was made into a likeness of the earth, which produces good and useful herbs, as well as bad and useless ones, and which has in itself both good and evil moistures. From tasting evil, the blood of the sons of Adam was turned into the poison of semen, out of which the sons of man are begotten. And therefore their flesh is ulcerated and permeable disease. These sores and openings create a certain storm and smoky moisture in men, from which the "flegmata" arise and coagulate, which then introduce diverse infirmities to the human body. All this arose from the first evil, which man began at the start, because if Adam had remained in paradise, he would have had the sweetest health, and the best dwelling-place, just as the strongest balsam emits the best odor; but on the contrary, man now has within himself poison and phlegm and diverse illnesses.
"Lingua Ignota" and invented alphabet.
Hildegard also invented an alternative alphabet. The text of her writing and compositions reveals Hildegard's use of this form of modified medieval Latin, encompassing many invented, conflated and abridged words. Because of her inventions of words for her lyrics and use of a constructed script, many conlangers look upon her as a medieval precursor. Scholars believe that Hildegard used her "Lingua Ignota" to increase solidarity among her nuns.
Significance.
During her lifetime.
Maddocks claims that it is likely Hildegard learned simple Latin and the tenets of the Christian faith but was not instructed in the Seven Liberal Arts, which formed the basis of all education for the learned classes in the Middle Ages: the "Trivium" of grammar, dialectic, and rhetoric plus the "Quadrivium" of arithmetic, geometry, astronomy, and music. The correspondence she kept with the outside world, both spiritual and social, transcended the cloister as a space of spiritual confinement and served to document Hildegard’s grand style and strict formatting of medieval letter writing.
Contributing to Christian European rhetorical traditions, Hildegard "authorized herself as a theologian" through alternative rhetorical arts. Hildegard was creative in her interpretation of theology. She believed that her monastery should exclude novices who were not from the nobility because she did not want her community to be divided on the basis of social status. She also stated that "woman may be made from man, but no man can be made without a woman."
Because of church limitation on public, discursive rhetoric, the medieval rhetorical arts included preaching, letter writing, poetry, and the encyclopedic tradition. Hildegard’s participation in these arts speaks to her significance as a female rhetorician, transcending bans on women's social participation and interpretation of scripture. The acceptance of public preaching by a woman, even a well-connected abbess and acknowledged prophet, does not fit the stereotype of this time. Her preaching was not limited to the monasteries; she preached publicly in 1160 in Germany. (New York: Routledge, 2001, 9). She conducted four preaching tours throughout Germany, speaking to both clergy and laity in chapter houses and in public, mainly denouncing clerical corruption and calling for reform.
Many abbots and abbesses asked her for prayers and opinions on various matters. She traveled widely during her four preaching tours. She had several fanatical followers, including Guibert of Gembloux, who wrote to her frequently and became her secretary after Volmar's death in 1173. Hildegard also influenced several monastic women, exchanging letters with Elisabeth of Schönau, a nearby visionary.
Hildegard corresponded with popes such as Eugene III and Anastasius IV, statesmen such as Abbot Suger, German emperors such as Frederick I Barbarossa, and other notable figures such as Saint Bernard of Clairvaux, who advanced her work, at the behest of her abbot, Kuno, at the Synod of Trier in 1147 and 1148. Hildegard of Bingen's correspondence is an important component of her literary output.
Beatification, canonization and recognition as a Doctor of the Church.
Hildegard was one of the first persons for whom the Roman canonization process was officially applied, but the process took so long that four attempts at canonization were not completed and she remained at the level of her beatification. Her name was nonetheless taken up in the Roman Martyrology at the end of the 16th century. Her feast day is 17 September. Numerous popes have referred to Hildegard as a saint, including Pope John Paul II and Pope Benedict XVI.
On 10 May 2012, Pope Benedict XVI extended the liturgical cult of St. Hildegard to the entire Catholic Church in a process known as "equivalent canonization," thus laying the groundwork for naming her a Doctor of the Church. On 7 October 2012, the feast of the Holy Rosary, the pope named her a Doctor of the Church, the fourth woman of 35 saints given that title by the Roman Catholic Church. He called her "perennially relevant" and "an authentic teacher of theology and a profound scholar of natural science and music."
Hildegard of Bingen also appears in the calendar of saints of various Anglican churches, such as that of the Church of England, in which she is commemorated on 17 September.
Hildegard's parish and pilgrimage church in Eibingen near Rüdesheim houses her relics.
Modern interest.
In recent years, Hildegard has become of particular interest to feminist scholars. They note her reference to herself as a member of the "weaker sex" and her rather constant belittling of women. Hildegard frequently referred to herself as an unlearned woman, completely incapable of Biblical exegesis. Such a statement on her part, however, worked to her advantage because it made her statements that all of her writings and music came from visions of the Divine more believable, therefore giving Hildegard the authority to speak in a time and place where few women were permitted a voice. Hildegard used her voice to amplify the Church's condemnation of institutional corruption, in particular simony.
Hildegard has also become a figure of reverence within the contemporary New Age movement, mostly because of her holistic and natural view of healing, as well as her status as a mystic. Though her medical writings were long neglected, and then studied without reference to their context, she was the inspiration for Dr. Gottfried Hertzka's "Hildegard-Medicine", and is the namesake for June Boyce-Tillman's Hildegard Network, a healing center that focuses on a holistic approach to wellness and brings together people interested in exploring the links between spirituality, the arts, and healing. Her reputation as a medicinal writer and healer was also used by early feminists to argue for women's rights to attend medical schools. Hildegard's reincarnation has been debated since 1924 when Austrian mystic Rudolf Steiner lectured that a nun of her description was the past life of Russian poet philosopher Vladimir Soloviev, whose Sophianic visions are often compared to Hildegard's. Sophiologist Robert Powell writes that hermetic astrology proves the match, while mystical communities in Hildegard's lineage include that of artist Carl Schroeder as studied by Columbia sociologist Courtney Bender and supported by reincarnation researchers Walter Semkiw and Kevin Ryerson.
Recordings and performances of Hildegard's music have gained critical praise and popularity since 1979. See Discography listed below.
The following modern musical works are directly linked to Hildegard and her music or texts:
The artwork "The Dinner Party" features a place setting for Hildegard.
In space, the minor planet 898 Hildegard is named for her.
In film, Hildegard has been portrayed by Patricia Routledge in a BBC documentary called "Hildegard of Bingen" (1994), by Ángela Molina in "Barbarossa" (2009) and by Barbara Sukowa in the film "Vision", directed by Margarethe von Trotta.
Hildegard was the subject of a 2012 fictionalized biographic novel "Illuminations" by Mary Sharratt.
The plant genus "Hildegardia" is named after her because of her contributions to herbal medicine.
A feature documentary film, "The Unruly Mystic: Saint Hildegard," was released by American director Michael M. Conti in 2014.
References.
Primary Sources (in translation):
Hildegard of Bingen. 
Secondary Sources:

</doc>
<doc id="13686" url="https://en.wikipedia.org/wiki?curid=13686" title="Hilversum">
Hilversum

Hilversum () is a municipality and a city in the Netherlands, in the province of North Holland. Located in the region called "Het Gooi", it is the largest town in that area. It is surrounded by heathland, woods, meadows, lakes, and smaller towns. Hilversum is part of the Randstad, one of the largest conurbations in Europe.
The town.
Hilversum lies south-east of Amsterdam and north of Utrecht.
The town is often called "media city" since it is the principal centre for radio and television broadcasting in The Netherlands. Radio Netherlands, heard worldwide via shortwave radio since the 1920s, is also based here. Hilversum is home to an extensive complex of audio and television studios belonging to the national broadcast production company NOS (Nederlandse Omroep Stichting), as well as to the studios and offices of all the Dutch public broadcasting organisations and many commercial TV production companies. As a result, many old AM radio sets in Europe had a "Hilversum" dial position marked on their tuning scales (along with Athlone, Kalundborg and others). The Dutch voting in the Eurovision Song Contest usually takes place from Hilversum.
Hilversum is also known for its architecturally important Town Hall (Raadhuis Hilversum), designed by Willem Marinus Dudok and built in 1931.
Hilversum has one public library, two swimming pools (Van Hellemond Sport and De Lieberg), a number of sporting halls and several shopping centers (such as Hilvertshof, De Gijsbrecht, Kerkelanden, Riebeeck Galerij and Seinhorst.) In the region, the city centre is called "het dorp", which means "the village".
History.
Earthenware found in Hilversum gives its name to the Hilversum culture, which is an early- to mid-Bronze Age, or 800–1200 BCE material culture. Artifacts from this prehistoric civilization bear similarities to the Wessex Culture of southern Britain and may indicate that the first Hilversum residents emigrated from that area. The first brick settlements formed around 900, but it was not until 1305 that the first official mention of Hilversum ("Hilfersheem" from "Hilvertshem" meaning "houses between the hills") is found. At that point it was a part of Naarden, the oldest town in the Gooi area.
Farming, raising sheep and some wool manufacturing were the means of life for the Gooi in the Middle Ages. In 1424, on 21 March at 6:30 am (the hour at which people got up, as the farm was full of restless and loud animals), Hilversum received its first official independent status. This made possible further growth in the village because permission from Naarden was no longer needed for new industrial development. The town grew further in the 17th century when the Dutch economy as a whole entered its age of prosperity, and several canals were built connecting it indirectly to Amsterdam. In 1725 and 1766 large fires destroyed most of the town, leveling parts of the old townhouse and the church next to it. The town overcame these setbacks and the textile industry continued to develop, among other ways by devising a way to weave cows' hair. In the 19th century a substantial textile and tapestry industry emerged, aided by a railway link to Amsterdam in 1874. From that time the town grew quickly with rich commuters from Amsterdam moving in, building themselves large villas in the wooded surroundings, and gradually starting to live in Hilversum permanently. Despite this growth, Hilversum was never granted city rights so it is still referred to by many locals as "het dorp," or "the village."
For the 1928 Summer Olympics in neighboring Amsterdam, it hosted all of the non-jumping equestrian and the running part of the modern pentathlon event. The city was the headquarters of the German ground forces (Heer) in the Netherlands .
The "Nederlandse Seintoestellen Fabriek" (NSF) company established a professional transmitter and radio factory in Hilversum in the early 1920s, growing into the largest of its kind in the Netherlands, and in 1948 being taken over by Philips. By then the textile industry had started its decline; only one factory, Veneta, managed to continue into the 1960s, when it also had to close its doors. Another major industry, the chemical factory IFF, also closed by the end of the 1960s. In the meantime, almost all Dutch radio broadcasting organizations (followed by television broadcasters in the 1950s) established their headquarters in Hilversum and provided a source of continuing economic growth. The concentration of broadcasters in Hilversum has given it its enduring status as the media city for the Netherlands.
In 1964, the population reached a record high – over 103,000 people called Hilversum home. The current population hovers around 85,000. Several factors figure into the decline: one is the fact that the average family nowadays consists of fewer people, so fewer people live in each house; second, the town is virtually unable to expand because all the surrounding lands were sold by city architect W.M. Dudok to the Goois Natuurreservaat ("nl"). The third reason for this decline of the population was because the property values were increasing rapidly in that moment of time, and many people were forced to move to less expensive areas in the Netherlands.
Some sources blame connections in the television world for attracting crime to Hilversum; the town has had to cope with mounting drug-related issues in a community with higher than average unemployment and ongoing housing shortage.
Hilversum was one of the first towns to have a local party of the populist movement called "Leefbaar" ("liveable"). Founded by former social-democrat party strongman Jan Nagel, it was initially held at bay for alderman positions. In 2001, Nagel from 'Leefbaar Hilversum' teamed up with 'Leefbaar Utrecht' leaders to found a national 'Leefbaar Nederland' party. By strange coincidence, in 2002 the most vocal 'Leefbaar Rotterdam' politician Pim Fortuyn was shot and killed by an animal rights activist at Hilversum Media Park just after finishing a radio interview. This happened, however, after a break between Fortuyn and Nagel during a Leefbaar Nederland board meeting in Hilversum on Fortuyn's anti-Islamic viewpoints.
The town of Hilversum has put a great deal of effort into improvements, including a recent renovation to its central train station, thorough renovation of the main shopping centre (Hilvertshof), and development of new dining and retail districts downtown including the "vintage" district in the Leeuwenstraat. Several notable architectural accomplishments include the Institute for Sound and Vision, and Zanderij Crailoo ("nl"), the largest man-made wildlife crossing in the world.
The nearby Media Park was the scene of the 2002 assassination of politician Pim Fortuyn, and of a January 29, 2015 fake gunman demanding airtime at Nederlandse Omroep Stichting's headquarters.
The population declined from 103,000 in 1964 to 84,000 in 2006.
Transport.
Hilversum is well connected to the Dutch railway network, and contains three stations: 
One can get the best connections from the station Hilversum, as this is an Intercity station.
Local government.
The municipal council of Hilversum in 2010 consists of 37 seats, which are divided as followed:
It was the first city with a "Leefbaar" party (which was intended as just a local party).
Notable residents.
Notable people born in Hilversum:

</doc>
<doc id="13688" url="https://en.wikipedia.org/wiki?curid=13688" title="The Hound of Heaven">
The Hound of Heaven

"The Hound of Heaven" is a 182-line poem written by English poet Francis Thompson (1859–1907). The poem became famous and was the source of much of Thompson's posthumous reputation. The poem was first published in Thompson's first volume of poems in 1893. It was included in the Oxford Book of English Mystical Verse (1917). Thompson's work was praised by G. K. Chesterton, and it was also an influence on J. R. R. Tolkien, who presented a paper on Thompson in 1914.
This well-loved Christian poem has been described as follows:
"The name is strange. It startles one at first. It is so bold, so new, so fearless. It does not attract, rather the reverse. But when one reads the poem this strangeness disappears. The meaning is understood. As the hound follows the hare, never ceasing in its running, ever drawing nearer in the chase, with unhurrying and imperturbed pace, so does God follow the fleeing soul by His Divine grace. And though in sin or in human love, away from God it seeks to hide itself, Divine grace follows after, unwearyingly follows ever after, till the soul feels its pressure forcing it to turn to Him alone in that never ending pursuit." J.F.X. O'Conor, S.J.

</doc>
<doc id="13692" url="https://en.wikipedia.org/wiki?curid=13692" title="History of the Internet">
History of the Internet

The history of the Internet begins with the development of electronic computers in the 1950s. Initial concepts of packet networking originated in several computer science laboratories in the United States, United Kingdom, and France. The US Department of Defense awarded contracts as early as the 1960s for packet network systems, including the development of the ARPANET (which would become the first network to use the Internet Protocol). The first message was sent over the ARPANET from computer science Professor Leonard Kleinrock's laboratory at University of California, Los Angeles (UCLA) to the second network node at Stanford Research Institute (SRI).
Packet switching networks such as ARPANET, NPL network, CYCLADES, Merit Network, Tymnet, and Telenet, were developed in the late 1960s and early 1970s using a variety of communications protocols. Donald Davies was the first to put theory into practice by designing a packet-switched network at the National Physics Laboratory in the UK, the first of its kind in the world and the cornerstone for UK research for almost two decades. Following, ARPANET further led to the development of protocols for internetworking, in which multiple separate networks could be joined into a network of networks.
Access to the ARPANET was expanded in 1981 when the National Science Foundation (NSF) funded the Computer Science Network (CSNET). In 1982, the Internet protocol suite (TCP/IP) was introduced as the standard networking protocol on the ARPANET. In the early 1980s the NSF funded the establishment for national supercomputing centers at several universities, and provided interconnectivity in 1986 with the NSFNET project, which also created network access to the supercomputer sites in the United States from research and education organizations. Commercial Internet service providers (ISPs) began to emerge in the very late 1980s. The ARPANET was decommissioned in 1990. Limited private connections to parts of the Internet by officially commercial entities emerged in several American cities by late 1989 and 1990, and the NSFNET was decommissioned in 1995, removing the last restrictions on the use of the Internet to carry commercial traffic.
In the 1980s, the work of British computer scientist Tim Berners-Lee on the World Wide Web theorised protocols linking hypertext documents into a working system, marking the beginning of the modern Internet. Since the mid-1990s, the Internet has had a revolutionary impact on culture and commerce, including the rise of near-instant communication by electronic mail, instant messaging, voice over Internet Protocol (VoIP) telephone calls, two-way interactive video calls, and the World Wide Web with its discussion forums, blogs, social networking, and online shopping sites. The research and education community continues to develop and use advanced networks such as NSF's very high speed Backbone Network Service (vBNS), Internet2, and National LambdaRail. Increasing amounts of data are transmitted at higher and higher speeds over fiber optic networks operating at 1-Gbit/s, 10-Gbit/s, or more. The Internet's takeover of the global communication landscape was almost instant in historical terms: it only communicated 1% of the information flowing through two-way telecommunications networks in the year 1993, already 51% by 2000, and more than 97% of the telecommunicated information by 2007. Today the Internet continues to grow, driven by ever greater amounts of online information, commerce, entertainment, and social networking.
__TOC__
General history.
Precursors.
The concept of data communication – transmitting data between two different places through an electromagnetic medium such as radio or an electric wire – predates the introduction of the first computers. Such communication systems were typically limited to point to point communication between two end devices. Telegraph systems and telex machines can be considered early precursors of this kind of communication. The Telegraph in the late 19th century was the first fully digital communication system.
Fundamental theoretical work in data transmission and information theory was developed by Claude Shannon, Harry Nyquist, and Ralph Hartley in the early 20th century.
Early computers had a central processing unit and remote terminals. As the technology evolved, new systems were devised to allow communication over longer distances (for terminals) or with higher speed (for interconnection of local devices) that were necessary for the mainframe computer model. These technologies made it possible to exchange data (such as files) between remote computers. However, the point-to-point communication model was limited, as it did not allow for direct communication between any two arbitrary systems; a physical link was necessary. The technology was also considered unsafe for strategic and military use because there were no alternative paths for the communication in case of an enemy attack.
Development of wide-area networking.
With limited exceptions, the earliest computers were connected directly to terminals used by individual users, typically in the same building or site (now known as local-area networks or "LAN"). Networking beyond this, now known as wide-area networks ("WAN"), emerged during the 1950s and became established (although greatly limited compared to modern networks) during the 1960s.
Inspiration.
A pioneer in the call for a global network was J. C. R. Licklider, a Vice President at Bolt Beranek and Newman, Inc.. He proposed in his January 1960 paper "Man-Computer Symbiosis":
In August 1962, Licklider and Welden Clark published the paper "On-Line Man-Computer Communication" which was one of the first descriptions of a networked future.
In October 1962, Licklider was hired by Jack Ruina as director of the newly established Information Processing Techniques Office (IPTO) within DARPA, with a mandate to interconnect the United States Department of Defense's main computers at Cheyenne Mountain, the Pentagon, and SAC HQ. There he formed an informal group within DARPA to further computer research. He began by writing memos describing a distributed network to the IPTO staff, whom he called "Members and Affiliates of the Intergalactic Computer Network". As part of the information processing office's role, three network terminals had been installed: one for System Development Corporation in Santa Monica, one for Project Genie at University of California, Berkeley, and one for the Compatible Time-Sharing System project at Massachusetts Institute of Technology (MIT). Licklider's identified need for inter-networking would become obvious by the apparent waste of resources this caused.
For each of these three terminals, I had three different sets of user commands. So if I was talking online with someone at S.D.C. and I wanted to talk to someone I knew at Berkeley or M.I.T. about this, I had to get up from the S.D.C. terminal, go over and log into the other terminal and get in touch with them... <br><br>I said, oh man, it's obvious what to do: If you have these three terminals, there ought to be one terminal that goes anywhere you want to go where you have interactive computing. That idea is the ARPAnet.
Although he left the IPTO in 1964, five years before the ARPANET went live, it was his vision of universal networking that provided the impetus for his successors such as Lawrence Roberts and Robert Taylor to further the ARPANET development. Licklider later returned to lead the IPTO in 1973 for two years.
Development of packet switching.
The issue of connecting separate physical networks to form one logical network was the first of many problems. In the 1960s, Paul Baran of the RAND Corporation produced a study of survivable networks for the U.S. military in the event of nuclear war. Information transmitted across Baran's network would be divided into what he called "message-blocks". Independently, Donald Davies (National Physical Laboratory, UK), proposed and was the first to put into practice a similar network based on what he called packet-switching, the term that would ultimately be adopted. Leonard Kleinrock (MIT) developed a mathematical theory behind this technology (without the packets). Packet-switching provides better bandwidth utilization and response times than the traditional circuit-switching technology used for telephony, particularly on resource-limited interconnection links.
Packet switching is a rapid store and forward networking design that divides messages up into arbitrary packets, with routing decisions made per-packet. Early networks used message switched systems that required rigid routing structures prone to single point of failure. This led Tommy Krash and Paul Baran's U.S. military-funded research to focus on using message-blocks to include network redundancy.
Networks that led to the Internet.
ARPANET.
Promoted to the head of the information processing office at Defense Advanced Research Projects Agency (DARPA), Robert Taylor intended to realize Licklider's ideas of an interconnected networking system. Bringing in Larry Roberts from MIT, he initiated a project to build such a network. The first ARPANET link was established between the University of California, Los Angeles (UCLA) and the Stanford Research Institute at 22:30 hours on October 29, 1969.
By December 5, 1969, a 4-node network was connected by adding the University of Utah and the University of California, Santa Barbara. Building on ideas developed in ALOHAnet, the ARPANET grew rapidly. By 1981, the number of hosts had grown to 213, with a new host being added approximately every twenty days.
ARPANET development was centered around the Request for Comments (RFC) process, still used today for proposing and distributing Internet Protocols and Systems. RFC 1, entitled "Host Software", was written by Steve Crocker from the University of California, Los Angeles, and published on April 7, 1969. These early years were documented in the 1972 film .
ARPANET became the technical core of what would become the Internet, and a primary tool in developing the technologies used. The early ARPANET used the Network Control Program (NCP, sometimes Network Control Protocol) rather than TCP/IP. On January 1, 1983, known as flag day, NCP on the ARPANET was replaced by the more flexible and powerful family of TCP/IP protocols, marking the start of the modern Internet.
International collaborations on ARPANET were sparse. For various political reasons, European developers were concerned with developing the X.25 networks. Notable exceptions were the "Norwegian Seismic Array" (NORSAR) in 1972, followed in 1973 by Sweden with satellite links to the Tanum Earth Station and Peter Kirstein's research group in the UK, initially at the Institute of Computer Science, London University and later at University College London.
NPL.
In 1965, Donald Davies of the National Physical Laboratory (United Kingdom) proposed a national data network based on packet-switching. The proposal was not taken up nationally, but by 1970 he had designed and built the Mark I packet-switched network to meet the needs of the multidisciplinary laboratory and prove the technology under operational conditions. By 1976 12 computers and 75 terminal devices were attached and more were added until the network was replaced in 1986. NPL, followed by ARPANET, were the first two networks in the world to use packet switching.
Merit Network.
The Merit Network was formed in 1966 as the Michigan Educational Research Information Triad to explore computer networking between three of Michigan's public universities as a means to help the state's educational and economic development. With initial support from the State of Michigan and the National Science Foundation (NSF), the packet-switched network was first demonstrated in December 1971 when an interactive host to host connection was made between the IBM mainframe computer systems at the University of Michigan in Ann Arbor and Wayne State University in Detroit. In October 1972 connections to the CDC mainframe at Michigan State University in East Lansing completed the triad. Over the next several years in addition to host to host interactive connections the network was enhanced to support terminal to host connections, host to host batch connections (remote job submission, remote printing, batch file transfer), interactive file transfer, gateways to the Tymnet and Telenet public data networks, X.25 host attachments, gateways to X.25 data networks, Ethernet attached hosts, and eventually TCP/IP and additional public universities in Michigan join the network. All of this set the stage for Merit's role in the NSFNET project starting in the mid-1980s.
CYCLADES.
The CYCLADES packet switching network was a French research network designed and directed by Louis Pouzin. First demonstrated in 1973, it was developed to explore alternatives to the initial ARPANET design and to support network research generally. It was the first network to make the hosts responsible for the reliable delivery of data, rather than the network itself, using unreliable datagrams and associated end-to-end protocol mechanisms.
X.25 and public data networks.
Based on ARPA's research, packet switching network standards were developed by the International Telecommunication Union (ITU) in the form of X.25 and related standards. While using packet switching, X.25 is built on the concept of virtual circuits emulating traditional telephone connections. In 1974, X.25 formed the basis for the SERCnet network between British academic and research sites, which later became JANET. The initial ITU Standard on X.25 was approved in March 1976.
The British Post Office, Western Union International and Tymnet collaborated to create the first international packet switched network, referred to as the International Packet Switched Service (IPSS), in 1978. This network grew from Europe and the US to cover Canada, Hong Kong, and Australia by 1981. By the 1990s it provided a worldwide networking infrastructure.
Unlike ARPANET, X.25 was commonly available for business use. Telenet offered its Telemail electronic mail service, which was also targeted to enterprise use rather than the general email system of the ARPANET.
The first public dial-in networks used asynchronous TTY terminal protocols to reach a concentrator operated in the public network. Some networks, such as CompuServe, used X.25 to multiplex the terminal sessions into their packet-switched backbones, while others, such as Tymnet, used proprietary protocols. In 1979, CompuServe became the first service to offer electronic mail capabilities and technical support to personal computer users. The company broke new ground again in 1980 as the first to offer real-time chat with its CB Simulator. Other major dial-in networks were America Online (AOL) and Prodigy that also provided communications, content, and entertainment features. Many bulletin board system (BBS) networks also provided on-line access, such as FidoNet which was popular amongst hobbyist computer users, many of them hackers and amateur radio operators.
UUCP and Usenet.
In 1979, two students at Duke University, Tom Truscott and Jim Ellis, originated the idea of using Bourne shell scripts to transfer news and messages on a serial line UUCP connection with nearby University of North Carolina at Chapel Hill. Following public release of the software in 1980, the mesh of UUCP hosts forwarding on the Usenet news rapidly expanded. UUCPnet, as it would later be named, also created gateways and links between FidoNet and dial-up BBS hosts. UUCP networks spread quickly due to the lower costs involved, ability to use existing leased lines, X.25 links or even ARPANET connections, and the lack of strict use policies compared to later networks like CSNET and Bitnet. All connects were local. By 1981 the number of UUCP hosts had grown to 550, nearly doubling to 940 in 1984. – Sublink Network, operating since 1987 and officially founded in Italy in 1989, based its interconnectivity upon UUCP to redistribute mail and news groups messages throughout its Italian nodes (about 100 at the time) owned both by private individuals and small companies. Sublink Network represented possibly one of the first examples of the internet technology becoming progress through popular diffusion.
Merging the networks and creating the Internet (1973–95).
TCP/IP.
With so many different network methods, something was needed to unify them. Robert E. Kahn of DARPA and ARPANET recruited Vinton Cerf of Stanford University to work with him on the problem. By 1973, they had worked out a fundamental reformulation, where the differences between network protocols were hidden by using a common internetwork protocol, and instead of the network being responsible for reliability, as in the ARPANET, the hosts became responsible. Cerf credits Hubert Zimmermann, Gerard LeLann and Louis Pouzin (designer of the CYCLADES network) with important work on this design.
The specification of the resulting protocol, "RFC 675 – Specification of Internet Transmission Control Program", by Vinton Cerf, Yogen Dalal and Carl Sunshine, Network Working Group, December 1974, contains the first attested use of the term "internet", as a shorthand for "internetworking"; later RFCs repeat this use, so the word started out as an adjective rather than the noun it is today.
With the role of the network reduced to the bare minimum, it became possible to join almost any networks together, no matter what their characteristics were, thereby solving Kahn's initial problem. DARPA agreed to fund development of prototype software, and after several years of work, the first demonstration of a gateway between the Packet Radio network in the SF Bay area and the ARPANET was conducted by the Stanford Research Institute. On November 22, 1977 a three network demonstration was conducted including the ARPANET, the SRI's Packet Radio Van on the Packet Radio Network and the Atlantic Packet Satellite network.
Stemming from the first specifications of TCP in 1974, TCP/IP emerged in mid-late 1978 in nearly its final form, as used for the first decades of the Internet, known as "IPv4". (IPv4 eventually became superseded by its successor, called "IPv6", but this was largely due to the sheer number of devices being connected post-2005, which overwhelmed the numbers that IPv4 had been able to accommodate worldwide. However, due to IPv4's entrenched position by that time, the shift is still in its early stages as of 2015, and expected to take many years, decades, or perhaps longer, to complete).
The associated standards for IPv4 were published by 1981 as RFCs 791, 792 and 793, and adopted for use. DARPA sponsored or encouraged the development of TCP/IP implementations for many operating systems and then scheduled a migration of all hosts on all of its packet networks to TCP/IP. On January 1, 1983, known as flag day, TCP/IP protocols became the only approved protocol on the ARPANET, replacing the earlier NCP protocol.
From ARPANET to NSFNET.
After the ARPANET had been up and running for several years, ARPA looked for another agency to hand off the network to; ARPA's primary mission was funding cutting edge research and development, not running a communications utility. Eventually, in July 1975, the network had been turned over to the Defense Communications Agency, also part of the Department of Defense. In 1983, the U.S. military portion of the ARPANET was broken off as a separate network, the MILNET. MILNET subsequently became the unclassified but military-only NIPRNET, in parallel with the SECRET-level SIPRNET and JWICS for TOP SECRET and above. NIPRNET does have controlled security gateways to the public Internet.
The networks based on the ARPANET were government funded and therefore restricted to noncommercial uses such as research; unrelated commercial use was strictly forbidden. This initially restricted connections to military sites and universities. During the 1980s, the connections expanded to more educational institutions, and even to a growing number of companies such as Digital Equipment Corporation and Hewlett-Packard, which were participating in research projects or providing services to those who were.
Several other branches of the U.S. government, the National Aeronautics and Space Administration (NASA), the National Science Foundation (NSF), and the Department of Energy (DOE) became heavily involved in Internet research and started development of a successor to ARPANET. In the mid-1980s, all three of these branches developed the first Wide Area Networks based on TCP/IP. NASA developed the NASA Science Network, NSF developed CSNET and DOE evolved the Energy Sciences Network or ESNet.
NASA developed the TCP/IP based NASA Science Network (NSN) in the mid-1980s, connecting space scientists to data and information stored anywhere in the world. In 1989, the DECnet-based Space Physics Analysis Network (SPAN) and the TCP/IP-based NASA Science Network (NSN) were brought together at NASA Ames Research Center creating the first multiprotocol wide area network called the NASA Science Internet, or NSI. NSI was established to provide a totally integrated communications infrastructure to the NASA scientific community for the advancement of earth, space and life sciences. As a high-speed, multiprotocol, international network, NSI provided connectivity to over 20,000 scientists across all seven continents.
In 1981 NSF supported the development of the Computer Science Network (CSNET). CSNET connected with ARPANET using TCP/IP, and ran TCP/IP over X.25, but it also supported departments without sophisticated network connections, using automated dial-up mail exchange.
Its experience with CSNET lead NSF to use TCP/IP when it created NSFNET, a 56 kbit/s backbone established in 1986, to support the NSF sponsored supercomputing centers. The NSFNET Project also provided support for the creation of regional research and education networks in the United States and for the connection of university and college campus networks to the regional networks. The use of NSFNET and the regional networks was not limited to supercomputer users and the 56 kbit/s network quickly became overloaded. NSFNET was upgraded to 1.5 Mbit/s in 1988 under a cooperative agreement with the Merit Network in partnership with IBM, MCI, and the State of Michigan. The existence of NSFNET and the creation of Federal Internet Exchanges (FIXes) allowed the ARPANET to be decommissioned in 1990. NSFNET was expanded and upgraded to 45 Mbit/s in 1991, and was decommissioned in 1995 when it was replaced by backbones operated by several commercial Internet Service Providers.
Transition towards the Internet.
The term "internet" was adopted in the first RFC published on the TCP protocol (RFC 675: Internet Transmission Control Program, December 1974) as an abbreviation of the term "internetworking" and the two terms were used interchangeably. In general, an internet was any network using TCP/IP. It was around the time when ARPANET was interlinked with NSFNET in the late 1980s, that the term was used as the name of the network, Internet, being the large and global TCP/IP network.
As interest in networking grew and new applications for it were developed, the Internet's technologies spread throughout the rest of the world. The network-agnostic approach in TCP/IP meant that it was easy to use any existing network infrastructure, such as the IPSS X.25 network, to carry Internet traffic. In 1984, University College London replaced its transatlantic satellite links with TCP/IP over IPSS.
Many sites unable to link directly to the Internet created simple gateways for the transfer of electronic mail, the most important application of the time. Sites with only intermittent connections used UUCP or FidoNet and relied on the gateways between these networks and the Internet. Some gateway services went beyond simple mail peering, such as allowing access to File Transfer Protocol (FTP) sites via UUCP or mail.
Finally, routing technologies were developed for the Internet to remove the remaining centralized routing aspects. The Exterior Gateway Protocol (EGP) was replaced by a new protocol, the Border Gateway Protocol (BGP). This provided a meshed topology for the Internet and reduced the centric architecture which ARPANET had emphasized. In 1994, Classless Inter-Domain Routing (CIDR) was introduced to support better conservation of address space which allowed use of route aggregation to decrease the size of routing tables.
TCP/IP goes global (1980s).
CERN, the European Internet, the link to the Pacific and beyond.
Between 1984 and 1988 CERN began installation and operation of TCP/IP to interconnect its major internal computer systems, workstations, PCs and an accelerator control system. CERN continued to operate a limited self-developed system (CERNET) internally and several incompatible (typically proprietary) network protocols externally. There was considerable resistance in Europe towards more widespread use of TCP/IP, and the CERN TCP/IP intranets remained isolated from the Internet until 1989.
In 1988, Daniel Karrenberg, from Centrum Wiskunde & Informatica (CWI) in Amsterdam, visited Ben Segal, CERN's TCP/IP Coordinator, looking for advice about the transition of the European side of the UUCP Usenet network (much of which ran over X.25 links) over to TCP/IP. In 1987, Ben Segal had met with Len Bosack from the then still small company Cisco about purchasing some TCP/IP routers for CERN, and was able to give Karrenberg advice and forward him on to Cisco for the appropriate hardware. This expanded the European portion of the Internet across the existing UUCP networks, and in 1989 CERN opened its first external TCP/IP connections. This coincided with the creation of Réseaux IP Européens (RIPE), initially a group of IP network administrators who met regularly to carry out coordination work together. Later, in 1992, RIPE was formally registered as a cooperative in Amsterdam.
At the same time as the rise of internetworking in Europe, ad hoc networking to ARPA and in-between Australian universities formed, based on various technologies such as X.25 and UUCPNet. These were limited in their connection to the global networks, due to the cost of making individual international UUCP dial-up or X.25 connections. In 1989, Australian universities joined the push towards using IP protocols to unify their networking infrastructures. AARNet was formed in 1989 by the Australian Vice-Chancellors' Committee and provided a dedicated IP based network for Australia.
The Internet began to penetrate Asia in the late 1980s. Japan, which had built the UUCP-based network JUNET in 1984, connected to NSFNET in 1989. It hosted the annual meeting of the Internet Society, INET'92, in Kobe. Singapore developed TECHNET in 1990, and Thailand gained a global Internet connection between Chulalongkorn University and UUNET in 1992.
The early global "digital divide" emerges.
While developed countries with technological infrastructures were joining the Internet, developing countries began to experience a digital divide separating them from the Internet. On an essentially continental basis, they are building organizations for Internet resource administration and sharing operational experience, as more and more transmission facilities go into place.
Africa.
At the beginning of the 1990s, African countries relied upon X.25 IPSS and 2400 baud modem UUCP links for international and internetwork computer communications.
In August 1995, InfoMail Uganda, Ltd., a privately held firm in Kampala now known as InfoCom, and NSN Network Services of Avon, Colorado, sold in 1997 and now known as Clear Channel Satellite, established Africa's first native TCP/IP high-speed satellite Internet services. The data connection was originally carried by a C-Band RSCC Russian satellite which connected InfoMail's Kampala offices directly to NSN's MAE-West point of presence using a private network from NSN's leased ground station in New Jersey. InfoCom's first satellite connection was just 64 kbit/s, serving a Sun host computer and twelve US Robotics dial-up modems.
In 1996, a USAID funded project, the Leland Initiative, started work on developing full Internet connectivity for the continent. Guinea, Mozambique, Madagascar and Rwanda gained satellite earth stations in 1997, followed by Ivory Coast and Benin in 1998.
Africa is building an Internet infrastructure. AfriNIC, headquartered in Mauritius, manages IP address allocation for the continent. As do the other Internet regions, there is an operational forum, the Internet Community of Operational Networking Specialists.
There are many programs to provide high-performance transmission plant, and the western and southern coasts have undersea optical cable. High-speed cables join North Africa and the Horn of Africa to intercontinental cable systems. Undersea cable development is slower for East Africa; the original joint effort between New Partnership for Africa's Development (NEPAD) and the East Africa Submarine System (Eassy) has broken off and may become two efforts.
Asia and Oceania.
The Asia Pacific Network Information Centre (APNIC), headquartered in Australia, manages IP address allocation for the continent. APNIC sponsors an operational forum, the Asia-Pacific Regional Internet Conference on Operational Technologies (APRICOT).
In 1991, the People's Republic of China saw its first TCP/IP college network, Tsinghua University's TUNET. The PRC went on to make its first global Internet connection in 1994, between the Beijing Electro-Spectrometer Collaboration and Stanford University's Linear Accelerator Center. However, China went on to implement its own digital divide by implementing a country-wide content filter.
Latin America.
As with the other regions, the Latin American and Caribbean Internet Addresses Registry (LACNIC) manages the IP address space and other resources for its area. LACNIC, headquartered in Uruguay, operates DNS root, reverse DNS, and other key services.
Rise of the global Internet (Late 1980s/early 1990s onward).
Opening the network to commerce.
Initially, as with its predecessor networks, the system that would evolve into the Internet was primarily for government and government body use.
However, interest in commercial use of the Internet quickly became a hotly debated topic. Although commercial use was forbidden, the exact definition of commercial use could be unclear and subjective. UUCPNet and the X.25 IPSS had no such restrictions, which would eventually see the official barring of UUCPNet use of ARPANET and NSFNET connections. (Some UUCP links still remained connecting to these networks however, as administrators cast a blind eye to their operation.)
As a result, during the late 1980s, the first Internet service provider (ISP) companies were formed. Companies like PSINet, UUNET, Netcom, and Portal Software were formed to provide service to the regional research networks and provide alternate network access, UUCP-based email and Usenet News to the public. The first commercial dialup ISP in the United States was The World, which opened in 1989.
In 1992, the U.S. Congress passed the Scientific and Advanced-Technology Act, , which allowed NSF to support access by the research and education communities to computer networks which were not used exclusively for research and education purposes, thus permitting NSFNET to interconnect with commercial networks. This caused controversy within the research and education community, who were concerned commercial use of the network might lead to an Internet that was less responsive to their needs, and within the community of commercial network providers, who felt that government subsidies were giving an unfair advantage to some organizations.
By 1990, ARPANET had been overtaken and replaced by newer networking technologies and the project came to a close. New network service providers including PSINet, Alternet, CERFNet, ANS CO+RE, and many others were offering network access to commercial customers. NSFNET was no longer the de facto backbone and exchange point for Internet. The Commercial Internet eXchange (CIX), Metropolitan Area Exchanges (MAEs), and later Network Access Points (NAPs) were becoming the primary interconnections between many networks. The final restrictions on carrying commercial traffic ended on April 30, 1995 when the National Science Foundation ended its sponsorship of the NSFNET Backbone Service and the service ended. NSF provided initial support for the NAPs and interim support to help the regional research and education networks transition to commercial ISPs. NSF also sponsored the very high speed Backbone Network Service (vBNS) which continued to provide support for the supercomputing centers and research and education in the United States.
World Wide Web and introduction of browsers.
The World Wide Web (sometimes abbreviated "www" or "W3") is an information space where documents and other web resources are identified by URIs, interlinked by hypertext links, and can be accessed via the Internet using a web browser and (more recently) web-based applications. It has become known simply as "the Web". As of the 2010s, the World Wide Web is the primary tool billions use to interact on the Internet, and it has changed people's lives immeasurably.
Precursors to the web browser emerged in the form of hyperlinked applications during the mid and late 1980s (the bare concept of hyperlinking had by then existed for some decades). Following these, Tim Berners-Lee is credited with inventing the World Wide Web in 1989 and developing in 1990 both the first web server, and the first web browser, called WorldWideWeb (no spaces) and later renamed Nexus. Many others were soon developed, with Marc Andreessen's 1993 Mosaic (later Netscape), being particularly easy to use and install, and often credited with sparking the internet boom of the 1990s. Today, the major web browsers are Firefox, Internet Explorer, Google Chrome, Opera and Safari.
A boost in web users was triggered in September 1993 by NCSA Mosaic, a graphical browser which eventually ran on several popular office and home computers. This was the first web browser aiming to bring multimedia content to non-technical users, and therefore included images and text on the same page, unlike previous browser designs; its founder, Marc Andreessen, also established the company that in 1994, released Netscape Navigator, which resulted in one of the early browser wars, when it ended up in a competition for dominance (which it lost) with Microsoft Windows' Internet Explorer. Commercial use restrictions were lifted in 1995. The online service America Online (AOL) offered their users a connection to the Internet via their own internal browser.
Use in wider society 1990s to early 2000s (Web 1.0).
During the first decade or so of the public internet, the immense changes it would eventually enable in the 2000s were still nascent. In terms of providing context for this period, mobile cellular devices ("smartphones" and other cellular devices) which today provide near-universal access, were used for business and not a routine household item owned by parents and children worldwide. Social media in the modern sense had yet to come into existence, laptops were bulky and most households did not have computers. Data rates were slow and most people lacked means to video or digitize video so websites such as YouTube did not yet exist, media storage was transitioning slowly from analog tape to digital optical discs (DVD and to an extent still, floppy disc to CD). Enabling technologies used from the early 2000s such as PHP, modern Javascript and Java, technologies such as AJAX, HTML 4 (and its emphasis on CSS), and various software frameworks, which enabled and simplified speed of web development, largely awaited invention and their eventual widespread adoption.
The Internet was widely used for mailing lists, emails, e-commerce and early popular online shopping (Amazon and eBay for example), online forums and bulletin boards, and personal websites and blogs, and use was growing rapidly, but by more modern standards the systems used were static and lacked widespread social engagement. It awaited a number of events in the early 2000s to change from a communications technology to gradually develop into a key part of global society's infrastructure.
Typical design elements of these "Web 1.0" era websites included: Static pages instead of dynamic HTML; content served from filesystems instead of relational databases; pages built using Server Side Includes or CGI instead of a web application written in a dynamic programming language; HTML 3.2-era structures such as frames and tables to create page layouts; online guestbooks; overuse of GIF buttons and similar small graphics promoting particular items; and HTML forms sent via email. (Support for server side scripting was rare on shared servers so the usual feedback mechanism was via email, using mailto forms and their email program.
During the period 1997 to 2001, the first speculative investment bubble related to the Internet took place, in which "dot-com" companies (referring to the ".com" top level domain used by businesses) were propelled to exceedingly high valuations as investors rapidly stoked stock values, followed by a market crash; the first dot-com bubble. However this only temporarily slowed enthusiasm and growth, which quickly recovered and continued to grow.
The changes that would propel the Internet into its place as a social system took place during a relatively short period of no more than five years, starting from around 2004. They included:
and shortly after (approximately 2007–2008 onward):
With the call to Web 2.0, the period up to around 2004–2005 was retrospectively named and described by some as Web 1.0.
Web 2.0.
The term "Web 2.0" describes websites that emphasize user-generated content (including user-to-user interaction), usability, and interoperability. It first appeared in a January 1999 article called "Fragmented Future" written by Darcy DiNucci, a consultant on electronic information design, where she wrote:
The term resurfaced during 2002 – 2004, and gained prominence in late 2004 following presentations by Tim O'Reilly and Dale Dougherty at the first Web 2.0 Conference. In their opening remarks, John Battelle and Tim O'Reilly outlined their definition of the "Web as Platform", where software applications are built upon the Web as opposed to upon the desktop. The unique aspect of this migration, they argued, is that "customers are building your business for you". They argued that the activities of users generating content (in the form of ideas, text, videos, or pictures) could be "harnessed" to create value.
Web 2.0 does not refer to an update to any technical specification, but rather to cumulative changes in the way Web pages are made and used. Web 2.0 describes an approach, in which sites focus substantially upon allowing users to interact and collaborate with each other in a social media dialogue as creators of user-generated content in a virtual community, in contrast to Web sites where people are limited to the passive viewing of content. Examples of Web 2.0 include social networking sites, blogs, wikis, folksonomies, video sharing sites, hosted services, Web applications, and mashups. Terry Flew, in his 3rd Edition of "New Media" described what he believed to characterize the differences between Web 1.0 and Web 2.0:
This era saw several household names gain prominence through their community-oriented operation – YouTube, Twitter, Facebook, Reddit and Wikipedia being some examples.
The mobile revolution.
The process of change generally described as "Web 2.0" was itself greatly accelerated and transformed only a short time later by the increasing growth in mobile devices. This mobile revolution meant that computers in the form of smartphones became something many people used, took with them everywhere, communicated with, used for photographs and videos they instantly shared or to shop or seek information "on the move" – and used socially, as opposed to items on a desk at home or just used for work.
Location-based services, services using location and other sensor information, and crowdsourcing (frequently but not always location based), became common, with posts tagged by location, or websites and services becoming location aware. Mobile-targeted websites (such as "m.website.com") became common, designed especially for the new devices used. Netbooks, ultrabooks, widespread 4G and Wi-Fi, and mobile chips capable or running at nearly the power of desktops from not many years before on far lower power usage, became enablers of this stage of Internet development, and the term "App" emerged (short for "Application program" or "Program") as did the "App store".
Networking in outer space.
The first live Internet link into low earth orbit was established on January 22, 2010 when astronaut T. J. Creamer posted the first unassisted update to his Twitter account from the International Space Station, marking the extension of the Internet into space. (Astronauts at the ISS had used email and Twitter before, but these messages had been relayed to the ground through a NASA data link before being posted by a human proxy.) This personal Web access, which NASA calls the Crew Support LAN, uses the space station's high-speed Ku band microwave link. To surf the Web, astronauts can use a station laptop computer to control a desktop computer on Earth, and they can talk to their families and friends on Earth using Voice over IP equipment.
Communication with spacecraft beyond earth orbit has traditionally been over point-to-point links through the Deep Space Network. Each such data link must be manually scheduled and configured. In the late 1990s NASA and Google began working on a new network protocol, Delay-tolerant networking (DTN) which automates this process, allows networking of spaceborne transmission nodes, and takes the fact into account that spacecraft can temporarily lose contact because they move behind the Moon or planets, or because space weather disrupts the connection. Under such conditions, DTN retransmits data packages instead of dropping them, as the standard TCP/IP internet protocol does. NASA conducted the first field test of what it calls the "deep space internet" in November 2008. Testing of DTN-based communications between the International Space Station and Earth (now termed Disruption-Tolerant Networking) has been ongoing since March 2009, and is scheduled to continue until March 2014.
This network technology is supposed to ultimately enable missions that involve multiple spacecraft where reliable inter-vessel communication might take precedence over vessel-to-earth downlinks. According to a February 2011 statement by Google's Vint Cerf, the so-called "Bundle protocols" have been uploaded to NASA's EPOXI mission spacecraft (which is in orbit around the Sun) and communication with Earth has been tested at a distance of approximately 80 light seconds.
Internet governance.
As a globally distributed network of voluntarily interconnected autonomous networks, the Internet operates without a central governing body. It has no centralized governance for either technology or policies, and each constituent network chooses what technologies and protocols it will deploy from the voluntary technical standards that are developed by the Internet Engineering Task Force (IETF). However, throughout its entire history, the Internet system has had an "Internet Assigned Numbers Authority" (IANA) for the allocation and assignment of various technical identifiers needed for the operation of the Internet. The Internet Corporation for Assigned Names and Numbers (ICANN) provides oversight and coordination for two principal name spaces in the Internet, the Internet Protocol address space and the Domain Name System.
NIC, InterNIC, IANA and ICANN.
The IANA function was originally performed by USC Information Sciences Institute, and it delegated portions of this responsibility with respect to numeric network and autonomous system identifiers to the Network Information Center (NIC) at Stanford Research Institute (SRI International) in Menlo Park, California. In addition to his role as the RFC Editor, Jon Postel worked as the manager of IANA until his death in 1998.
As the early ARPANET grew, hosts were referred to by names, and a HOSTS.TXT file would be distributed from SRI International to each host on the network. As the network grew, this became cumbersome. A technical solution came in the form of the Domain Name System, created by Paul Mockapetris. The Defense Data Network—Network Information Center (DDN-NIC) at SRI handled all registration services, including the top-level domains (TLDs) of .mil, .gov, .edu, .org, .net, .com and .us, root nameserver administration and Internet number assignments under a United States Department of Defense contract. In 1991, the Defense Information Systems Agency (DISA) awarded the administration and maintenance of DDN-NIC (managed by SRI up until this point) to Government Systems, Inc., who subcontracted it to the small private-sector Network Solutions, Inc.
The increasing cultural diversity of the Internet also posed administrative challenges for centralized management of the IP addresses. In October 1992, the Internet Engineering Task Force (IETF) published RFC 1366, which described the "growth of the Internet and its increasing globalization" and set out the basis for an evolution of the IP registry process, based on a regionally distributed registry model. This document stressed the need for a single Internet number registry to exist in each geographical region of the world (which would be of "continental dimensions"). Registries would be "unbiased and widely recognized by network providers and subscribers" within their region.
The RIPE Network Coordination Centre (RIPE NCC) was established as the first RIR in May 1992. The second RIR, the Asia Pacific Network Information Centre (APNIC), was established in Tokyo in 1993, as a pilot project of the Asia Pacific Networking Group.
Since at this point in history most of the growth on the Internet was coming from non-military sources, it was decided that the Department of Defense would no longer fund registration services outside of the .mil TLD. In 1993 the U.S. National Science Foundation, after a competitive bidding process in 1992, created the InterNIC to manage the allocations of addresses and management of the address databases, and awarded the contract to three organizations. Registration Services would be provided by Network Solutions; Directory and Database Services would be provided by AT&T; and Information Services would be provided by General Atomics.
Over time, after consultation with the IANA, the IETF, RIPE NCC, APNIC, and the Federal Networking Council (FNC), the decision was made to separate the management of domain names from the management of IP numbers. Following the examples of RIPE NCC and APNIC, it was recommended that management of IP address space then administered by the InterNIC should be under the control of those that use it, specifically the ISPs, end-user organizations, corporate entities, universities, and individuals. As a result, the American Registry for Internet Numbers (ARIN) was established as in December 1997, as an independent, not-for-profit corporation by direction of the National Science Foundation and became the third Regional Internet Registry.
In 1998, both the IANA and remaining DNS-related InterNIC functions were reorganized under the control of ICANN, a California non-profit corporation contracted by the United States Department of Commerce to manage a number of Internet-related tasks. As these tasks involved technical coordination for two principal Internet name spaces (DNS names and IP addresses) created by the IETF, ICANN also signed a memorandum of understanding with the IAB to define the technical work to be carried out by the Internet Assigned Numbers Authority. The management of Internet address space remained with the regional Internet registries, which collectively were defined as a supporting organization within the ICANN structure. ICANN provides central coordination for the DNS system, including policy coordination for the split registry / registrar system, with competition among registry service providers to serve each top-level-domain and multiple competing registrars offering DNS services to end-users.
Internet Engineering Task Force.
The Internet Engineering Task Force (IETF) is the largest and most visible of several loosely related ad-hoc groups that provide technical direction for the Internet, including the Internet Architecture Board (IAB), the Internet Engineering Steering Group (IESG), and the Internet Research Task Force (IRTF).
The IETF is a loosely self-organized group of international volunteers who contribute to the engineering and evolution of Internet technologies. It is the principal body engaged in the development of new Internet standard specifications. Much of the IETF's work is done in Working Groups. It does not "run the Internet", despite what some people might mistakenly say. The IETF does make voluntary standards that are often adopted by Internet users, but it does not control, or even patrol, the Internet.
The IETF started in January 1986 as a quarterly meeting of U.S. government-funded researchers. Non-government representatives were invited starting with the fourth IETF meeting in October 1986. The concept of Working Groups was introduced at the fifth IETF meeting in February 1987. The seventh IETF meeting in July 1987 was the first meeting with more than 100 attendees. In 1992, the Internet Society, a professional membership society, was formed and IETF began to operate under it as an independent international standards body. The first IETF meeting outside of the United States was held in Amsterdam, The Netherlands, in July 1993. Today the IETF meets three times a year and attendnce is often about 1,300 people, but has been as high as 2,000 upon occasion. Typically one in three IETF meetings are held in Europe or Asia. The number of non-US attendees is roughly 50%, even at meetings held in the United States.
The IETF is unusual in that it exists as a collection of happenings, but is not a corporation and has no board of directors, no members, and no dues. The closest thing there is to being an IETF member is being on the IETF or a Working Group mailing list. IETF volunteers come from all over the world and from many different parts of the Internet community. The IETF works closely with and under the supervision of the Internet Engineering Steering Group (IESG) and the Internet Architecture Board (IAB). The Internet Research Task Force (IRTF) and the Internet Research Steering Group (IRSG), peer activities to the IETF and IESG under the general supervision of the IAB, focus on longer term research issues.
Request for Comments.
Request for Comments (RFCs) are the main documentation for the work of the IAB, IESG, IETF, and IRTF. RFC 1, "Host Software", was written by Steve Crocker at UCLA in April 1969, well before the IETF was created. Originally they were technical memos documenting aspects of ARPANET development and were edited by Jon Postel, the first RFC Editor.
RFCs cover a wide range of information from proposed standards, draft standards, full standards, best practices, experimental protocols, history, and other informational topics. RFCs can be written by individuals or informal groups of individuals, but many are the product of a more formal Working Group. Drafts are submitted to the IESG either by individuals or by the Working Group Chair. An RFC Editor, appointed by the IAB, separate from IANA, and working in conjunction with the IESG, receives drafts from the IESG and edits, formats, and publishes them. Once an RFC is published, it is never revised. If the standard it describes changes or its information becomes obsolete, the revised standard or updated information will be re-published as a new RFC that "obsoletes" the original.
The Internet Society.
The Internet Society (ISOC) is an international, nonprofit organization founded during 1992 "to assure the open development, evolution and use of the Internet for the benefit of all people throughout the world". With offices near Washington, DC, USA, and in Geneva, Switzerland, ISOC has a membership base comprising more than 80 organizational and more than 50,000 individual members. Members also form "chapters" based on either common geographical location or special interests. There are currently more than 90 chapters around the world.
ISOC provides financial and organizational support to and promotes the work of the standards settings bodies for which it is the organizational home: the Internet Engineering Task Force (IETF), the Internet Architecture Board (IAB), the Internet Engineering Steering Group (IESG), and the Internet Research Task Force (IRTF). ISOC also promotes understanding and appreciation of the Internet model of open, transparent processes and consensus-based decision-making.
Globalization and Internet governance in the 21st century.
Since the 1990s, the Internet's governance and organization has been of global importance to governments, commerce, civil society, and individuals. The organizations which held control of certain technical aspects of the Internet were the successors of the old ARPANET oversight and the current decision-makers in the day-to-day technical aspects of the network. While recognized as the administrators of certain aspects of the Internet, their roles and their decision-making authority are limited and subject to increasing international scrutiny and increasing objections. These objections have led to the ICANN removing themselves from relationships with first the University of Southern California in 2000, and finally in September 2009, gaining autonomy from the US government by the ending of its longstanding agreements, although some contractual obligations with the U.S. Department of Commerce continued.
The IETF, with financial and organizational support from the Internet Society, continues to serve as the Internet's ad-hoc standards body and issues Request for Comments.
In November 2005, the World Summit on the Information Society, held in Tunis, called for an Internet Governance Forum (IGF) to be convened by United Nations Secretary General. The IGF opened an ongoing, non-binding conversation among stakeholders representing governments, the private sector, civil society, and the technical and academic communities about the future of Internet governance. The first IGF meeting was held in October/November 2006 with follow up meetings annually thereafter. Since WSIS, the term "Internet governance" has been broadened beyond narrow technical concerns to include a wider range of Internet-related policy issues.
Politicization of the Internet.
Due to its prominence and immediacy as an effective means of mass communication, the Internet has also become more politicized as it has grown. This has led in turn, to discourses and activities that would once have taken place in other ways, migrating to being mediated by internet.
Examples include political activities such as public protest and canvassing of support and votes, but also –
Net neutrality.
On April 23, 2014, the Federal Communications Commission (FCC) was reported to be considering a new rule that would permit Internet service providers to offer content providers a faster track to send content, thus reversing their earlier net neutrality position. A possible solution to net neutrality concerns may be municipal broadband, according to Professor Susan Crawford, a legal and technology expert at Harvard Law School. On May 15, 2014, the FCC decided to consider two options regarding Internet services: first, permit fast and slow broadband lanes, thereby compromising net neutrality; and second, reclassify broadband as a telecommunication service, thereby preserving net neutrality. On November 10, 2014, President Obama recommended the FCC reclassify broadband Internet service as a telecommunications service in order to preserve net neutrality. On January 16, 2015, Republicans presented legislation, in the form of a U.S. Congress H. R. discussion draft bill, that makes concessions to net neutrality but prohibits the FCC from accomplishing the goal or enacting any further regulation affecting Internet service providers (ISPs). On January 31, 2015, AP News reported that the FCC will present the notion of applying ("with some caveats") Title II (common carrier) of the Communications Act of 1934 to the internet in a vote expected on February 26, 2015. Adoption of this notion would reclassify internet service from one of information to one of telecommunications and, according to Tom Wheeler, chairman of the FCC, ensure net neutrality. The FCC is expected to enforce net neutrality in its vote, according to the New York Times.
On February 26, 2015, the FCC ruled in favor of net neutrality by applying Title II (common carrier) of the Communications Act of 1934 and Section 706 of the Telecommunications act of 1996 to the Internet. The FCC Chairman, Tom Wheeler, commented, "This is no more a plan to regulate the Internet than the First Amendment is a plan to regulate free speech. They both stand for the same concept."
On March 12, 2015, the FCC released the specific details of the net neutrality rules. On April 13, 2015, the FCC published the final rule on its new "Net Neutrality" regulations.
Use and culture.
Email and Usenet.
Email has often been called the killer application of the Internet. It predates the Internet, and was a crucial tool in creating it. Email started in 1965 as a way for multiple users of a time-sharing mainframe computer to communicate. Although the history is undocumented, among the first systems to have such a facility were the System Development Corporation (SDC) Q32 and the Compatible Time-Sharing System (CTSS) at MIT.
The ARPANET computer network made a large contribution to the evolution of electronic mail. An experimental inter-system transferred mail on the ARPANET shortly after its creation. In 1971 Ray Tomlinson created what was to become the standard Internet electronic mail addressing format, using the @ sign to separate mailbox names from host names.
A number of protocols were developed to deliver messages among groups of time-sharing computers over alternative transmission systems, such as UUCP and IBM's VNET email system. Email could be passed this way between a number of networks, including ARPANET, BITNET and NSFNET, as well as to hosts connected directly to other sites via UUCP. See the history of SMTP protocol.
In addition, UUCP allowed the publication of text files that could be read by many others. The News software developed by Steve Daniel and Tom Truscott in 1979 was used to distribute news and bulletin board-like messages. This quickly grew into discussion groups, known as newsgroups, on a wide range of topics. On ARPANET and NSFNET similar discussion groups would form via mailing lists, discussing both technical issues and more culturally focused topics (such as science fiction, discussed on the sflovers mailing list).
During the early years of the Internet, email and similar mechanisms were also fundamental to allow people to access resources that were not available due to the absence of online connectivity. UUCP was often used to distribute files using the 'alt.binary' groups. Also, FTP e-mail gateways allowed people that lived outside the US and Europe to download files using ftp commands written inside email messages. The file was encoded, broken in pieces and sent by email; the receiver had to reassemble and decode it later, and it was the only way for people living overseas to download items such as the earlier Linux versions using the slow dial-up connections available at the time. After the popularization of the Web and the HTTP protocol such tools were slowly abandoned.
From Gopher to the WWW.
As the Internet grew through the 1980s and early 1990s, many people realized the increasing need to be able to find and organize files and information. Projects such as Archie, Gopher, WAIS, and the FTP Archive list attempted to create ways to organize distributed data. In the early 1990s, Gopher, invented by Mark P. McCahill offered a viable alternative to the World Wide Web. However, in 1993 the World Wide Web saw many advances to indexing and ease of access through search engines, which often neglected Gopher and Gopherspace. As popularity increased through ease of use, investment incentives also grew until in the middle of 1994 the WWW's popularity gained the upper hand. Then it became clear that Gopher and the other projects were doomed fall short.
One of the most promising user interface paradigms during this period was hypertext. The technology had been inspired by Vannevar Bush's "Memex" and developed through Ted Nelson's research on Project Xanadu and Douglas Engelbart's research on NLS. Many small self-contained hypertext systems had been created before, such as Apple Computer's HyperCard (1987). Gopher became the first commonly used hypertext interface to the Internet. While Gopher menu items were examples of hypertext, they were not commonly perceived in that way.
In 1989, while working at CERN, Tim Berners-Lee invented a network-based implementation of the hypertext concept. By releasing his invention to public use, he ensured the technology would become widespread. For his work in developing the World Wide Web, Berners-Lee received the Millennium technology prize in 2004. One early popular web browser, modeled after HyperCard, was ViolaWWW.
A turning point for the World Wide Web began with the introduction of the Mosaic web browser in 1993, a graphical browser developed by a team at the National Center for Supercomputing Applications at the University of Illinois at Urbana-Champaign (NCSA-UIUC), led by Marc Andreessen. Funding for Mosaic came from the "High-Performance Computing and Communications Initiative", a funding program initiated by the "High Performance Computing and Communication Act of 1991" also known as the "Gore Bill". Mosaic's graphical interface soon became more popular than Gopher, which at the time was primarily text-based, and the WWW became the preferred interface for accessing the Internet. (Gore's reference to his role in "creating the Internet", however, was ridiculed in his presidential election campaign. See the full article Al Gore and information technology).
Mosaic was superseded in 1994 by Andreessen's Netscape Navigator, which replaced Mosaic as the world's most popular browser. While it held this title for some time, eventually competition from Internet Explorer and a variety of other browsers almost completely displaced it. Another important event held on January 11, 1994, was "The Superhighway Summit" at UCLA's Royce Hall. This was the "first public conference bringing together all of the major industry, government and academic leaders in the field also began the national dialogue about the "Information Superhighway" and its implications."
"24 Hours in Cyberspace", "the largest one-day online event" (February 8, 1996) up to that date, took place on the then-active website, "cyber24.com." It was headed by photographer Rick Smolan. A photographic exhibition was unveiled at the Smithsonian Institution's National Museum of American History on January 23, 1997, featuring 70 photos from the project.
Search engines.
Even before the World Wide Web, there were search engines that attempted to organize the Internet. The first of these was the Archie search engine from McGill University in 1990, followed in 1991 by WAIS and Gopher. All three of those systems predated the invention of the World Wide Web but all continued to index the Web and the rest of the Internet for several years after the Web appeared. There are still Gopher servers as of 2006, although there are a great many more web servers.
As the Web grew, search engines and Web directories were created to track pages on the Web and allow people to find things. The first full-text Web search engine was WebCrawler in 1994. Before WebCrawler, only Web page titles were searched. Another early search engine, Lycos, was created in 1993 as a university project, and was the first to achieve commercial success. During the late 1990s, both Web directories and Web search engines were popular—Yahoo! (founded 1994) and Altavista (founded 1995) were the respective industry leaders. By August 2001, the directory model had begun to give way to search engines, tracking the rise of Google (founded 1998), which had developed new approaches to relevancy ranking. Directory features, while still commonly available, became after-thoughts to search engines.
Database size, which had been a significant marketing feature through the early 2000s, was similarly displaced by emphasis on relevancy ranking, the methods by which search engines attempt to sort the best results first. Relevancy ranking first became a major issue circa 1996, when it became apparent that it was impractical to review full lists of results. Consequently, algorithms for relevancy ranking have continuously improved. Google's PageRank method for ordering the results has received the most press, but all major search engines continually refine their ranking methodologies with a view toward improving the ordering of results. As of 2006, search engine rankings are more important than ever, so much so that an industry has developed ("search engine optimizers", or "SEO") to help web-developers improve their search ranking, and an entire body of case law has developed around matters that affect search engine rankings, such as use of trademarks in metatags. The sale of search rankings by some search engines has also created controversy among librarians and consumer advocates.
On June 3, 2009, Microsoft launched its new search engine, Bing. The following month Microsoft and Yahoo! announced a deal in which Bing would power Yahoo! Search.
File sharing.
Resource or file sharing has been an important activity on computer networks from well before the Internet was established and was supported in a variety of ways including bulletin board systems (1978), Usenet (1980), Kermit (1981), and many others. The File Transfer Protocol (FTP) for use on the Internet was standardized in 1985 and is still in use today. A variety of tools were developed to aid the use of FTP by helping users discover files they might want to transfer, including the Wide Area Information Server (WAIS) in 1991, Gopher in 1991, Archie in 1991, Veronica in 1992, Jughead in 1993, Internet Relay Chat (IRC) in 1988, and eventually the World Wide Web (WWW) in 1991 with Web directories and Web search engines.
In 1999, Napster became the first peer-to-peer file sharing system. Napster used a central server for indexing and peer discovery, but the storage and transfer of files was decentralized. A variety of peer-to-peer file sharing programs and services with different levels of decentralization and anonymity followed, including: Gnutella, eDonkey2000, and Freenet in 2000, FastTrack, Kazaa, Limewire, and BitTorrent in 2001, and Poisoned in 2003.
All of these tools are general purpose and can be used to share a wide variety of content, but sharing of music files, software, and later movies and videos are major uses. And while some of this sharing is legal, large portions are not. Lawsuits and other legal actions caused Napster in 2001, eDonkey2000 in 2005, Kazza in 2006, and Limewire in 2010 to shutdown or refocus their efforts. The Pirate Bay, founded in Sweden in 2003, continues despite a trial and appeal in 2009 and 2010 that resulted in jail terms and large fines for several of its founders. File sharing remains contentious and controversial with charges of theft of intellectual property on the one hand and charges of censorship on the other.
Dot-com bubble.
Suddenly the low price of reaching millions worldwide, and the possibility of selling to or hearing from those people at the same moment when they were reached, promised to overturn established business dogma in advertising, mail-order sales, customer relationship management, and many more areas. The web was a new killer app—it could bring together unrelated buyers and sellers in seamless and low-cost ways. Entrepreneurs around the world developed new business models, and ran to their nearest venture capitalist. While some of the new entrepreneurs had experience in business and economics, the majority were simply people with ideas, and did not manage the capital influx prudently. Additionally, many dot-com business plans were predicated on the assumption that by using the Internet, they would bypass the distribution channels of existing businesses and therefore not have to compete with them; when the established businesses with strong existing brands developed their own Internet presence, these hopes were shattered, and the newcomers were left attempting to break into markets dominated by larger, more established businesses. Many did not have the ability to do so.
The dot-com bubble burst in March 2000, with the technology heavy NASDAQ Composite index peaking at 5,048.62 on March 10 (5,132.52 intraday), more than double its value just a year before. By 2001, the bubble's deflation was running full speed. A majority of the dot-coms had ceased trading, after having burnt through their venture capital and IPO capital, often without ever making a profit. But despite this, the Internet continues to grow, driven by commerce, ever greater amounts of online information and knowledge and social networking.
Mobile phones and the Internet.
The first mobile phone with Internet connectivity was the Nokia 9000 Communicator, launched in Finland in 1996. The viability of Internet services access on mobile phones was limited until prices came down from that model, and network providers started to develop systems and services conveniently accessible on phones. NTT DoCoMo in Japan launched the first mobile Internet service, i-mode, in 1999 and this is considered the birth of the mobile phone Internet services. In 2001, the mobile phone email system by Research in Motion for their BlackBerry product was launched in America. To make efficient use of the small screen and tiny keypad and one-handed operation typical of mobile phones, a specific document and networking model was created for mobile devices, the Wireless Application Protocol (WAP). Most mobile device Internet services operate using WAP. The growth of mobile phone services was initially a primarily Asian phenomenon with Japan, South Korea and Taiwan all soon finding the majority of their Internet users accessing resources by phone rather than by PC. Developing countries followed, with India, South Africa, Kenya, Philippines, and Pakistan all reporting that the majority of their domestic users accessed the Internet from a mobile phone rather than a PC. The European and North American use of the Internet was influenced by a large installed base of personal computers, and the growth of mobile phone Internet access was more gradual, but had reached national penetration levels of 20–30% in most Western countries. The cross-over occurred in 2008, when more Internet access devices were mobile phones than personal computers. In many parts of the developing world, the ratio is as much as 10 mobile phone users to one PC user.
History of web technologies.
Client side (browser and application) technologies.
Web pages were initially conceived as structured documents based upon Hypertext Markup Language (HTML) which can allow access to images, video, and other content. Hyperlinks in the page permit users to navigate to other pages. In the earliest browsers, images opened in a separate "helper" application. Marc Andreessen's 1993 Mosaic and 1994 Netscape introduced mixed text and images for non-technical users. HTML evolved during the 1990s, leading to HTML 4 which introduced large elements of CSS styling and, later, extensions to allow browser code to make calls and ask for content from servers in a structured way (AJAX).
Historiography.
There are nearly insurmountable problems in supplying a historiography of the Internet's development. The process of digitization represents a twofold challenge both for historiography in general and, in particular, for historical communication research. A sense of the difficulty in documenting early developments that led to the internet can be gathered from the quote:

</doc>
<doc id="13693" url="https://en.wikipedia.org/wiki?curid=13693" title="Horace">
Horace

Quintus Horatius Flaccus (December 8, 65 BC – November 27, 8 BC), known in the English-speaking world as Horace ( or ), was the leading Roman lyric poet during the time of Augustus (also known as Octavian). The rhetorician Quintillian regarded his "Odes" as just about the only Latin lyrics worth reading: "He can be lofty sometimes, yet he is also full of charm and grace, versatile in his figures, and felicitously daring in his choice of words."
Horace also crafted elegant hexameter verses ("Sermones" and "Epistles") and caustic iambic poetry ("Epodes"). The hexameters are amusing yet serious works, friendly in tone, leading the ancient satirist Persius to comment: "as his friend laughs, Horace slyly puts his finger on his every fault; once let in, he plays about the heartstrings".
His career coincided with Rome's momentous change from Republic to Empire. An officer in the republican army defeated at the Battle of Philippi in 42 BC, he was befriended by Octavian's right-hand man in civil affairs, Maecenas, and became a spokesman for the new regime. For some commentators, his association with the regime was a delicate balance in which he maintained a strong measure of independence (he was "a master of the graceful sidestep") but for others he was, in John Dryden's phrase, "a well-mannered court slave".
Life.
Perhaps Horace can be considered the world's first autobiographer - In his writings, he "tells us far more about himself, his character, his development, and his way of life than any other great poet in antiquity. Some of the biographical writings contained in his writings can be supplemented from the short but valuable "Life of Horace" by Suetonius (in his "Lives of the Poets").
For the Life of Horace by Suetonius, see: ("Vita Horati")</ref>
Childhood.
He was born on 8 December 65 BC in the Samnite south of Italy. His home town, Venusia, lay on a trade route in the border region between Apulia and Lucania (Basilicata). Various Italic dialects were spoken in the area and this perhaps enriched his feeling for language. He could have been familiar with Greek words even as a young boy and later he poked fun at the jargon of mixed Greek and Oscan spoken in neighbouring Canusium. Literary Latin must have sounded to him like a semi-foreign language, heard only at school. One of the works he probably studied in school was the "Odyssia" of Livius Andronicus, crammed into Italian boys with threats and floggings by teachers like the 'Orbilius' mentioned in one of his poems. School was made particularly irksome by a number of his fellow pupils, the overgrown sons of beefy centurions. The army veterans could have been settled there at the expense of local families uprooted by Rome as punishment for their part in the Social War (91–88 BC). Such state-sponsored migration must have added still more linguistic variety to the area. According to a local tradition reported by Horace, a colony of Romans or Latins had been installed in Venusia after the Samnites had been driven out early in the third century. In that case, young Horace could have felt himself to be a Roman though there are also indications that he regarded himself as a Samnite or Sabellus by birth. Italians in modern and ancient times have always been devoted to their home towns, even after success in the wider world, and Horace was no different. Images of his childhood setting and references to it are found throughout his poems.
Horace's father was probably a Venutian taken captive by Romans in the Social War, or possibly he was descended from a Sabine captured in the Samnite Wars. Either way, he was a slave for at least part of his life. He was evidently a man of strong abilities however and managed to gain his freedom and improve his social position. Thus Horace claimed to be the free-born son of a prosperous 'coactor'. The term 'coactor' could denote various roles, such as tax collector, but its use by Horace was explained by scholia as a reference to 'coactor argentareus' i.e. an auctioneer with some of the functions of a banker, paying the seller out of his own funds and later recovering the sum with interest from the buyer.
The father spent a small fortune on his son's education, eventually accompanying him to Rome to oversee his schooling and moral development. The poet later paid tribute to him in a poem that one modern scholar considers the best memorial by any son to his father. The poem includes this passage:
If my character is flawed by a few minor faults, but is otherwise decent and moral, if you can point out only a few scattered blemishes on an otherwise immaculate surface, if no one can accuse me of greed, or of prurience, or of profligacy, if I live a virtuous life, free of defilement (pardon, for a moment, my self-praise), and if I am to my friends a good friend, my father deserves all the credit... As it is now, he deserves from me unstinting gratitude and praise. I could never be ashamed of such a father, nor do I feel any need, as many people do, to apologize for being a freedman's son. "Satires 1.6.65–92"
He never mentioned his mother in his verses and he might not have known much about her. Perhaps she also had been a slave.
Adulthood.
Horace left Rome, possibly after his father's death, and continued his formal education in Athens, a great centre of learning in the ancient world, where he arrived at nineteen years of age, enrolling in The Academy. Founded by Plato, The Academy was now dominated by Epicureans and Stoics, whose theories and practises made a deep impression on the young man from Venusia. Meanwhile, he mixed and lounged about with the elite of Roman youth, such as Marcus, the idle son of Cicero, and the Pompeius to whom he later addressed a poem. It was in Athens too that he probably acquired deep familiarity with the ancient tradition of Greek lyric poetry, at that time largely the preserve of grammarians and academic specialists (access to such material was easier in Athens than in Rome, where the public libraries had yet to be built by Asinius Pollio and Augustus).
Rome's troubles following the assassination of Julius Caesar were soon to catch up with him. Marcus Junius Brutus came to Athens seeking support for a republican cause. Brutus was fêted around town in grand receptions and he made a point of attending academic lectures, all the while recruiting supporters among the impressionable young men studying there, including Horace. An educated young Roman could begin military service high in the ranks and Horace was made tribunus militum (one of six senior officers of a typical legion), a post usually reserved for men of senatorial or equestrian rank and which seems to have inspired jealousy among his well-born confederates. He learned the basics of military life while on the march, particularly in the wilds of northern Greece, whose rugged scenery became a backdrop to some of his later poems. It was there in 42 BC that Octavian (later Augustus) and his associate Mark Antony crushed the republican forces at the Battle of Philippi. Horace later recorded it as a day of embarrassment for himself, when he fled without his shield, but allowance should be made for his self-deprecating humour. Moreover, the incident allowed him to identify himself with some famous poets who had long ago abandoned their shields in battle, notably his heroes Alcaeus and Archilochus. The comparison with the latter poet is uncanny: Archilochus lost his shield in a part of Thrace near Philippi, and he was deeply involved in the Greek colonization of Thasos, where Horace's die-hard comrades finally surrendered.
Octavian offered an early amnesty to his opponents and Horace quickly accepted it. On returning to Italy, he was confronted with yet another loss: his father's estate in Venusia was one of many throughout Italy to be confiscated for the settlement of veterans (Virgil lost his estate in the north about the same time). Horace later claimed that he was reduced to poverty and this led him to try his hand at poetry. In reality, there was no money to be had from versifying. At best, it offered future prospects through contacts with other poets and their patrons among the rich. Meanwhile, he obtained the sinecure of "scriba quaestorius", a civil service position at the "aerarium" or Treasury, profitable enough to be purchased even by members of the "ordo equester" and not very demanding in its work-load, since tasks could be delegated to "scribae" or permanent clerks. It was about this time that he began writing his "Satires" and "Epodes".
Poet.
The "Epodes" belong to the iambic genre of 'blame poetry', written to shame fellow citizens into a sense of their social obligations. Horace modelled these poems on the work of Archilochus. Social bonds in Rome had been decaying since the destruction of Carthage a little more than a hundred years earlier, due to the vast wealth that could be gained by plunder and corruption, and the troubles were magnified by rivalry between Julius Caesar, Mark Antony and confederates like Sextus Pompey, all jockeying for a bigger share of the spoils. One modern scholar has counted a dozen civil wars in the hundred years leading up to 31 BC, including the Spartacus rebellion, eight years before Horace's birth. As the heirs to Hellenistic culture, Horace and his fellow Romans were not well prepared to deal with these problems:
Horace's Hellenistic background is clear in his Satires, even though the genre was unique to Latin literature. He brought to it a style and outlook suited to the social and ethical issues confronting Rome but he changed its role from public, social engagement to private meditation. Meanwhile, he was beginning to interest Octavian's supporters, a gradual process described by him in one of his satires. The way was opened for him by his friend, the poet Virgil, who had gained admission into the privileged circle around Maecenas, Octavian's lieutenant, following the success of his "Eclogues". An introduction soon followed and, after a discreet interval, Horace too was accepted. He depicted the process as an honourable one, based on merit and mutual respect, eventually leading to true friendship, and there is reason to believe that his relationship was genuinely friendly, not just with Maecenas but afterwards with Augustus as well. On the other hand, the poet has been unsympathetically described by one scholar as "a sharp and rising young man, with an eye to the main chance." There were advantages on both sides: Horace gained encouragement and material support, the politicians gained a hold on a potential dissident. His republican sympathies, and his role at Philippi, may have caused him some pangs of remorse over his new status. However most Romans considered the civil wars to be the result of "contentio dignitatis", or rivalry between the foremost families of the city, and he too seems to have accepted the principate as Rome's last hope for much needed peace.
In 37 BC, Horace accompanied Maecenas on a journey to Brundisium, described in one of his poems as a series of amusing incidents and charming encounters with other friends along the way, such as Virgil. In fact the journey was political in its motivation, with Maecenas en route to negotiatie the Treaty of Tarentum with Antony, a fact Horace artfully keeps from the reader (political issues are largely avoided in the first book of satires). Horace was probably also with Maecenas on one of Octavian's naval expeditions against the piratical Sextus Pompeius, which ended in a disastrous storm off Palinurus in 36 BC, briefly alluded to by Horace in terms of near-drowning. There are also some indications in his verses that he was with Maecenas at the Battle of Actium in 31 BC, where Octavian defeated his great rival, Antony. By then Horace had already received from Maecenas the famous gift of his Sabine farm, probably not long after the publication of the first book of "Satires". The gift, which included income from five tenants, may have ended his career at the Treasury, or at least allowed him to give it less time and energy. It signalled his identification with the Octavian regime yet, in the second book of "Satires" that soon followed, he continued the apolitical stance of the first book. By this time, he had attained the status of "eques Romanus", perhaps as a result of his work at the Treasury.
Knight.
"Odes" 1–3 were the next focus for his artistic creativity. He adapted their forms and themes from Greek lyric poetry of the seventh and sixth centuries BC. The fragmented nature of the Greek world had enabled his literary heroes to express themselves freely and his semi-retirement from the Treasury in Rome to his own estate in the Sabine hills perhaps empowered him to some extent also yet even when his lyrics touched on public affairs they reinforced the importance of private life. Nevertheless, his work in the period 30–27 BC began to show his closeness to the regime and his sensitivity to its developing ideology. In "Odes" 1.2, for example, he eulogized Octavian in hyperboles that echo Hellenistic court poetry. The name "Augustus", which Octavian assumed in January 27 BC, is first attested in "Odes" 3.3 and 3.5. In the period 27–24 BC, political allusions in the "Odes" concentrated on foreign wars in Britain (1.35), Arabia (1.29) Spain (3.8) and Parthia (2.2). He greeted Augustus on his return to Rome in 24 BC as a beloved ruler upon whose good health he depended for his own happiness (3.14).
The public reception of "Odes" 1–3 disappointed him however. He attributed the lack of success to jealousy among imperial courtiers and to his isolation from literary cliques. Perhaps it was disappointment that led him to put aside the genre in favour of verse letters. He addressed his first book of "Epistles" to a variety of friends and acquaintances in an urbane style reflecting his new social status as a knight. In the opening poem, he professed a deeper interest in moral philosophy than poetry but, though the collection demonstrates a leaning towards stoic theory, it reveals no sustained thinking about ethics. Maecenas was still the dominant confidante but Horace had now begun to assert his own independence, suavely declining constant invitations to attend his patron. In the final poem of the first book of "Epistles", he revealed himself to be forty-four years old in the consulship of Lollius and Lepidus i.e. 21 BC, and "of small stature, fond of the sun, prematurely grey, quick-tempered but easily placated".
According to Suetonius, the second book of "Epistles" was prompted by Augustus, who desired a verse epistle to be addressed to himself. Augustus was in fact a prolific letter-writer and he once asked Horace to be his personal secretary. Horace refused the secretarial role but complied with the emperor's request for a verse letter. The letter to Augustus may have been slow in coming, being published possibly as late as 11 BC. It celebrated, among other things, the 15 BC military victories of his stepsons, Drusus and Tiberius, yet it and the following letter were largely devoted to literary theory and criticism. The literary theme was explored still further in "Ars Poetica", published separately but written in the form of an epistle and sometimes referred to as "Epistles" 2.3 (possibly the last poem he ever wrote). He was also commissioned to write odes commemorating the victories of Drusus and Tiberius and one to be sung in a temple of Apollo for the Secular Games, a long abandoned festival that Augustus revived in accordance with his policy of recreating ancient customs ("Carmen Saeculare").
Suetonius recorded some gossip about Horace's sexual activities late in life, involving mirrors. The poet died at 56 years of age, not long after his friend Maecenas, near whose tomb he was laid to rest. Both men bequeathed their property to Augustus, an honour that the emperor expected of his friends.
Works.
The dating of Horace's works isn't known precisely and scholars often debate the exact order in which they were first 'published'. There are good arguments for the following chronology:
Historical context.
Horace composed in traditional metres borrowed from Archaic Greece, employing hexameters in his "Satires" and "Epistles", and iambs in his "Epodes", all of which were relatively easy to adapt into Latin forms. His "Odes" featured more complex measures, including alcaics and sapphics, which were sometimes a difficult fit for Latin structure and syntax. Despite these traditional metres, he presented himself as a partisan in the development of a new and sophisticated style. He was influenced in particular by Hellenistic aesthetics of brevity, elegance and polish, as modeled in the work of Callimachus. 
In modern literary theory, a distinction is often made between immediate personal experience ("Urerlebnis") and experience mediated by cultural vectors such as literature, philosophy and the visual arts ("Bildungserlebnis"). The distinction has little relevance for Horace however since his personal and literary experiences are implicated in each other. "Satires" 1.5, for example, recounts in detail a real trip Horace made with Virgil and some of his other literary friends, and which parallels a Satire by Lucilius, his predecessor. Unlike much Hellenistic-inspired literature, however, his poetry was not composed for a small coterie of admirers and fellow poets, nor does it rely on abstruse allusions for many of its effects. Though elitist in its literary standards, it was written for a wide audience, as a public form of art. Ambivalence also characterizes his literary persona, since his presentation of himself as part of a small community of philosophically aware people, seeking true peace of mind while shunning vices like greed, was well adapted to Augustus's plans to reform public morality, corrupted by greedis personal plea for moderation was part of the emperor's grand message to the nation.
Horace generally followed the examples of poets established as classics in different genres, such as Archilochus in "Epodes", Lucilius in "Satires" and Alcaeus in the "Odes", later broadening his scope for the sake of variation and because his models weren't actually suited to the realities confronting him. Archilochus and Alcaeus were aristocratic Greeks whose poetry had a social and religious function that was immediately intelligible to their audiences but which became a mere artifice or literary motif when transposed to Rome. However, the artifice of the "Odes" is also integral to their success, since they could now accommodate a wide range of emotional effects, and the blend of Greek and Roman elements adds a sense of detachment and universality. Horace proudly claimed to introduce into Latin the spirit and iambic poetry of Archilochus but (unlike Archilochus) without persecuting anyone ("Epistles" 1.19.23–5). It was no idle boast. His "Epodes" were modeled on the verses of the Greek poet, as 'blame poetry', yet he avoided targeting real scapegoats. Whereas Archilochus presented himself as a serious and vigorous opponent of wrong-doers, Horace aimed for comic effects and adopted the persona of a weak and ineffectual critic of his times (as symbolized for example in his surrender to the witch Canidia in the final epode). He also claimed to be the first to introduce into Latin the lyrical methods of Alcaeus ("Epistles" 1.19.32–3) and he actually was the first Latin poet to make consistent use of Alcaic meters and themes: love, politics and the symposium. He imitated other Greek lyric poets as well, employing a 'motto' technique, beginning each ode with some reference to a Greek original and then diverging from it.
The satirical poet Lucilius was a senator's son who could castigate his peers with impunity. Horace was a mere freedman's son who had to tread carefully. Lucilius was a rugged patriot and a significant voice in Roman self-awareness, endearing himself to his countrymen by his blunt frankness and explicit politics. His work expressed genuine freedom or libertas. His style included 'metrical vandalism' and looseness of structure. Horace instead adopted an oblique and ironic style of satire, ridiculing stock characters and anonymous targets. His libertas was the private freedom of a philosophical outlook, not a political or social privilege. His "Satires" are relatively easy-going in their use of meter (relative to the tight lyric meters of the "Odes") but formal and highly controlled relative to the poems of Lucilius, whom Horace mocked for his sloppy standards ("Satires" 1.10.56–61)
The "Epistles" may be considered among Horace's most innovative works. There was nothing like it in Greek or Roman literature. Occasionally poems had had some resemblance to letters, including an elegiac poem from Solon to Mimnermus and some lyrical poems from Pindar to Hieron of Syracuse. Lucilius had composed a satire in the form of a letter, and some epistolary poems were composed by Catullus and Propertius. But nobody before Horace had ever composed an entire collection of verse letters, let alone letters with a focus on philosophical problems. The sophisticated and flexible style that he had developed in his "Satires" was adapted to the more serious needs of this new genre. Such refinement of style was not unusual for Horace. His craftsmanship as a wordsmith is apparent even in his earliest attempts at this or that kind of poetry, but his handling of each genre tended to improve over time as he adapted it to his own needs. Thus for example it is generally agreed that his second book of "Satires", where human folly is revealed through dialogue between characters, is superior to the first, where he propounds his ethics in monologues. Nevertheless, the first book includes some of his most popular poems.
Themes.
Horace developed a number of inter-related themes throughout his poetic career, including politics, love, philosophy and ethics, his own social role, as well as poetry itself. His "Epodes" and "Satires" are forms of 'blame poetry' and both have a natural affinity with the moralising and diatribes of Cynicism. This often takes the form of allusions to the work and philosophy of Bion of Borysthenes but it is as much a literary game as a philosophical alignment. By the time he composed his "Epistles", he was a critic of Cynicism along with all impractical and "high-falutin" philosophy in general. The "Satires" also include a strong element of Epicureanism, with frequent allusions to the Epicurean poet Lucretius. So for example the Epicurean sentiment "carpe diem" is the inspiration behind Horace's repeated punning on his own name ("Horatius ~ hora") in "Satires" 2.6. The "Satires" also feature some Stoic, Peripatetic and Platonic ("Dialogues") elements. In short, the "Satires" present a medley of philosophical programs, dished up in no particular ordera style of argument typical of the genre. The "Odes" display a wide range of topics. Over time, he becomes more confident about his political voice. Although he is often thought of as an overly intellectual lover, he is ingenuous in representing passion. The "Odes" weave various philosophical strands together, with allusions and statements of doctrine present in about a third of the "Odes" Books 1-3, ranging from the flippant (1.22, 3.28) to the solemn (2.10, 3.2, 3.3). Epicureanism is the dominant influence, characterizing about twice as many of these odes as Stoicism. A group of odes combines these two influences in tense relationships, such as "Odes" 1.7, praising Stoic virility and devotion to public duty while also advocating private pleasures among friends. While generally favouring the Epicurean lifestyle, the lyric poet is as eclectic as the satiric poet, and in "Odes" 2.10 even proposes Aristotle's golden mean as a remedy for Rome's political troubles. Many of Horace's poems also contain much reflection on genre, the lyric tradition, and the function of poetry. "Odes" 4, thought to be composed at the emperor's request, takes the themes of the first three books of "Odes" to a new level. This book shows greater poetic confidence after the public performance of his "Carmen saeculare" or "Century hymn" at a public festival orchestrated by Augustus. In it, Horace addresses the emperor Augustus directly with more confidence and proclaims his power to grant poetic immortality to those he praises. It is the least philosophical collection of his verses, excepting the twelfth ode, addressed to the dead Virgil as if he were living. In that ode, the epic poet and the lyric poet are aligned with Stoicism and Epicureanism respectively, in a mood of bitter-sweet pathos. The first poem of the "Epistles" sets the philosophical tone for the rest of the collection: "So now I put aside both verses and all those other games: What is true and what befits is my care, this my question, this my whole concern." His poetic renunciation of poetry in favour of philosophy is intended to be ambiguous. Ambiguity is the hallmark of the "Epistles". It is uncertain if those being addressed by the self-mocking poet-philosopher are being honoured or criticized. Though he emerges as an Epicurean, it is on the understanding that philosophical preferences, like political and social choices, are a matter of personal taste. Thus he depicts the ups and downs of the philosophical life more realistically than do most philosophers.
Reception.
The reception of Horace's work has varied from one epoch to another and varied markedly even in his own lifetime. "Odes" 1–3 were not well received when first 'published' in Rome, yet Augustus later commissioned a ceremonial ode for the Centennial Games in 17 BC and also encouraged the publication of "Odes" 4, after which Horace's reputation as Rome's premier lyricist was assured. His Odes were to become the best received of all his poems in ancient times, acquiring a classic status that discouraged imitation: no other poet produced a comparable body of lyrics in the four centuries that followed (though that might also be attributed to social causes, particularly the parasitism that Italy was sinking into). In the seventeenth and eighteenth centuries, ode-writing became highly fashionable in England and a large number of aspiring poets imitated Horace both in English and in Latin.
In a verse epistle to Augustus (Epistle 2.1), in 12 BC, Horace argued for classic status to be awarded to contemporary poets, including Virgil and apparently himself. In the final poem of his third book of Odes he claimed to have created for himself a monument more durable than bronze ("Exegi monumentum aere perennius", "Carmina" 3.30.1). For one modern scholar, however, Horace's personal qualities are more notable than the monumental quality of his achievement:
Yet for men like Wilfred Owen, scarred by experiences of World War I, his poetry stood for discredited values:
The same motto, Dulce et decorum est pro patria mori, had been adapted to the ethos of martyrdom in the lyrics of early Christian poets like Prudentius.
These preliminary comments touch on a small sample of developments in the reception of Horace's work. More developments are covered epoch by epoch in the following sections.
Antiquity.
Horace's influence can be observed in the work of his near contemporaries, Ovid and Propertius. Ovid followed his example in creating a completely natural style of expression in hexameter verse, and Propertius cheekily mimicked him in his third book of elegies. His "Epistles" provided them both with a model for their own verse letters and it also shaped Ovid's exile poetry.
His influence had a perverse aspect. As mentioned before, the brilliance of his "Odes" may have discouraged imitation. Conversely, they may have created a vogue for the lyrics of the archaic Greek poet Pindar, due to the fact that Horace had neglected that style of lyric (see Pindar#Influence and legacy). The iambic genre seems almost to have disappeared after publication of Horace's "Epodes". Ovid's "Ibis" was a rare attempt at the form but it was inspired mainly by Callimachus, and there are some iambic elements in Martial but the main influence there was Catullus. A revival of popular interest in the satires of Lucillius may have been inspired by Horace's criticism of his unpolished style. Both Horace and Lucilius were considered good role-models by Persius, who critiqued his own satires as lacking both the acerbity of Lucillius and the gentler touch of Horace. Juvenal's caustic satire was influenced mainly by Lucilius but Horace by then was a school classic and Juvenal could refer to him respectfully and in a round-about way as ""the Venusine lamp"".
Statius paid homage to Horace by composing one poem in Sapphic and one in Alcaic meter (the verse forms most often associated with "Odes"), which he included in his collection of occasional poems, "Silvae". Ancient scholars wrote commentaries on the lyric meters of the "Odes", including the scholarly poet Caesius Bassus. By a process called "derivatio", he varied established meters through the addition or omission of syllables, a technique borrowed by Seneca the Younger when adapting Horatian meters to the stage.
Horace's poems continued to be school texts into late antiquity. Works attributed to Helenius Acro and Pomponius Porphyrio are the remnants of a much larger body of Horatian scholarship. Porphyrio arranged the poems in non-chronological order, beginning with the "Odes", because of their general popularity and their appeal to scholars (the "Odes" were to retain this privileged position in the medieval manuscript tradition and thus in modern editions also). Horace was often evoked by poets of the fourth century, such as Ausonius and Claudian. Prudentius presented himself as a Christian Horace, adapting Horatian meters to his own poetry and giving Horatian motifs a Christian tone. On the other hand, St Jerome, modelled an uncompromising response to the pagan Horace, observing: ""What harmony can there be between Christ and the Devil? What has Horace to do with the Psalter?"" By the early sixth century, Horace and Prudentius were both part of a classical heritage that was struggling to survive the disorder of the times. Boethius, the last major author of classical Latin literature, could still take inspiration from Horace, sometimes mediated by Senecan tragedy. It can be argued that Horace's influence extended beyond poetry to dignify core themes and values of the early Christian era, such as self-sufficiency, inner contentment and courage.
Middle Ages and Renaissance.
Classical texts almost ceased being copied in the period between the mid sixth century and the Middle Ages. Horace's work probably survived in just two or three books imported into northern Europe from Italy. These became the ancestors of six extant manuscripts dated to the ninth century. Two of those six manuscripts are French in origin, one was produced in Alsace, and the other three show Irish influence but were probably written in continental monasteries (Lombardy for example). By the last half of the ninth century, it was not uncommon for literate people to have direct experience of Horace's poetry. His influence on the Carolingian Renaissance can be found in the poems of Heiric of Auxerre and in some manuscripts marked with neumes, mysterious notations that may have been an aid to the memorization and discussion of his lyric meters. "Ode" 4.11 is neumed with the melody of a hymn to John the Baptist, "Ut queant laxis", composed in Sapphic stanzas. This hymn later became the basis of the solfege system ("Do, re, mi...")an association with western music quite appropriate for a lyric poet like Horace, though the language of the hymn is mainly Prudentian. Lyons argues that the melody in question was linked with Horace's Ode well before Guido d'Arezzo fitted Ut queant laxis to it. However, the melody is unlikely to be a survivor from classical times, although Ovid testifies to Horace's use of the lyre while performing his Odes.
The German scholar, Ludwig Traube, once dubbed the tenth and eleventh centuries "The age of Horace" ("aetas Horatiana"), and placed it between the "aetas Vergiliana" of the eighth and ninth centuries, and the "aetas Ovidiana" of the twelfth and thirteenth centuries, a distinction supposed to reflect the dominant classical Latin influences of those times. Such a distinction is over-schematized since Horace was a substantial influence in the ninth century as well. Traube had focused too much on Horace's "Satires". Almost all of Horace's work found favor in the Medieval period. In fact medieval scholars were also guilty of over-schematism, associating Horace's different genres with the different ages of man. A twelfth century scholar encapsulated the theory: "...Horace wrote four different kinds of poems on account of the four ages, the "Odes" for boys, the "Ars Poetica" for young men, the "Satires" for mature men, the "Epistles" for old and complete men." It was even thought that Horace had composed his works in the order in which they had been placed by ancient scholars. Despite its naivety, the schematism involved an appreciation of Horace's works as a collection, the "Ars Poetica", "Satires" and "Epistles" appearing to find favour as well as the "Odes". The later Middle Ages however gave special significance to "Satires" and "Epistles", being considered Horace's mature works. Dante referred to Horace as "Orazio satiro", and he awarded him a privileged position in the first circle of Hell, with Homer, Ovid and Lucan.
Horace's popularity is revealed in the large number of quotes from all his works found in almost every genre of medieval literature, and also in the number of poets imitating him in quantitative Latin meter . The most prolific imitator of his "Odes" was the Bavarian monk, Metellus of Tegernsee, who dedicated his work to the patron saint of Tegernsee Abbey, St Quirinus, around the year 1170. He imitated all Horace's lyrical meters then followed these up with imitations of other meters used by Prudentius and Boethius, indicating that variety, as first modelled by Horace, was considered a fundamental aspect of the lyric genre. The content of his poems however was restricted to simple piety. Among the most successful imitators of "Satires" and "Epistles" was another Germanic author, calling himself Sextus Amarcius, around 1100, who composed four books, the first two exemplifying vices, the second pair mainly virtues.
Petrarch is a key figure in the imitation of Horace in accentual meters. His verse letters in Latin were modelled on the "Epistles" and he wrote a letter to Horace in the form of an ode. However he also borrowed from Horace when composing his Italian sonnets. One modern scholar has speculated that authors who imitated Horace in accentual rhythms (including stressed Latin and vernacular languages) may have considered their work a natural sequel to Horace's metrical variety. In France, Horace and Pindar were the poetic models for a group of vernacular authors called the Pléiade, including for example Pierre de Ronsard and Joachim du Bellay. Montaigne made constant and inventive use of Horatian quotes. The vernacular languages were dominant in Spain and Portugal in the sixteenth century, where Horace's influence is notable in the works of such authors as Garcilaso de la Vega, Juan Boscán Sá de Miranda, Antonio Ferreira and Fray Luis de León, the latter for example writing odes on the Horatian theme "beatus ille" ("happy the man"). The sixteenth century in western Europe was also an age of translations (except in Germany, where Horace wasn't translated until well into the seventeenth century). The first English translator was Thomas Drant, who placed translations of Jeremiah and Horace side by side in "Medicinable Morall", 1566. That was also the year that the Scot George Buchanan paraphrased the Psalms in a Horatian setting. Ben Jonson put Horace on the stage in 1601 in "Poetaster", along with other classical Latin authors, giving them all their own verses to speak in translation. Horace's part evinces the independent spirit, moral earnestness and critical insight that many readers look for in his poems.
Age of Enlightenment.
During the seventeenth and eighteenth centuries, or the Age of Enlightenment, neo-classical culture was pervasive. English literature in the middle of that period has been dubbed Augustan. It is not always easy to distinguish Horace's influence during those centuries (the mixing of influences is shown for example in one poet's pseudonym, "Horace Juvenal"). However a measure of his influence can be found in the diversity of the people interested in his works, both among readers and authors.
New editions of his works were published almost yearly. There were three new editions in 1612 (two in Leiden, one in Frankfurt) and again in 1699 (Utrecht, Barcelona, Cambridge). Cheap editions were plentiful and fine editions were also produced, including one whose entire text was engraved by John Pine in copperplate. The poet James Thomson owned five editions of Horace's work and the physician James Douglas had five hundred books with Horace-related titles. Horace was often commended in periodicals such as The Spectator, as a hallmark of good judgement, moderation and manliness, a focus for moralising. His verses offered a fund of mottoes, such as "simplex munditiis", (elegance in simplicity) "splendide mendax" (nobly untruthful.), "sapere aude", "nunc est bibendum", "carpe diem" (the latter perhaps being the only one still in common use today), quoted even in works as prosaic as Edmund Quincy's "A treatise of hemp-husbandry" (1765). The fictional hero Tom Jones recited his verses with feeling. His works were also used to justify commonplace themes, such as patriotic obedience, as in James Parry's English lines from an Oxford University collection in 1736:
Horatian-style lyrics were increasingly typical of Oxford and Cambridge verse collections for this period, most of them in Latin but some like the previous ode in English. John Milton's Lycidas first appeared in such a collection. It has few Horatian echoes yet Milton's associations with Horace were lifelong. He composed a controversial version of "Odes" 1.5, and Paradise Lost includes references to Horace's 'Roman' "Odes" 3.1–6 (Book 7 for example begins with echoes of "Odes" 3.4). Yet Horace's lyrics could offer inspiration to libertines as well as moralists, and neo-Latin sometimes served as a kind of discrete veil for the risqué. Thus for example Benjamin Loveling authored a catalogue of Drury Lane and Covent Garden prostitutes, in Sapphic stanzas, and an encomium for a dying lady "of salacious memory". Some Latin imitations of Horace were politically subversive, such as a marriage ode by Anthony Alsop that included a rallying cry for the Jacobite cause. On the other hand, Andrew Marvell took inspiration from Horace's "Odes" 1.37 to compose his English masterpiece Horatian Ode upon Cromwell's Return from Ireland, in which subtly nuanced reflections on the execution of Charles I echo Horace's ambiguous response to the death of Cleopatra (Marvell's ode was suppressed in spite of its subtlety and only began to be widely published in 1776). Samuel Johnson took particular pleasure in reading "The Odes". Alexander Pope wrote direct "Imitations" of Horace (published with the original Latin alongside) and also echoed him in "Essays" and The Rape of the Lock. He even emerged as "a quite Horatian Homer" in his translation of the "Iliad". Horace appealed also to female poets, such as Anna Seward ("Original sonnets on various subjects, and odes paraphrased from Horace", 1799) and Elizabeth Tollet, who composed a Latin ode in Sapphic meter to celebrate her brother's return from overseas, with tea and coffee substituted for the wine of Horace's sympotic settings:
Horace's "Ars Poetica" is second only to Aristotle's "Poetics" in its influence on literary theory and criticism. Milton recommended both works in his treatise "of Education". Horace's "Satires" and "Epistles" however also had a huge impact, influencing theorists and critics such as John Dryden. There was considerable debate over the value of different lyrical forms for contemporary poets, as represented on one hand by the kind of four-line stanzas made familiar by Horace's Sapphic and Alcaic "Odes" and, on the other, the loosely structured Pindarics associated with the odes of Pindar. Translations occasionally involved scholars in the dilemmas of censorship. Thus Christopher Smart entirely omitted "Odes" 4.10 and re-numbered the remaining odes. He also removed the ending of "Odes" 4.1. Thomas Creech printed "Epodes" 8 and 12 in the original Latin but left out their English translations. Philip Francis left out both the English and Latin for those same two epodes, a gap in the numbering the only indication that something was amiss. French editions of Horace were influential in England and these too were regularly bowdlerized.
Most European nations had their own 'Horaces': thus for example Friedrich von Hagedorn was called "The German Horace" and Maciej Kazimierz Sarbiewski "The Polish Horace" (the latter was much imitated by English poets such as Henry Vaughan and Abraham Cowley). Pope Urban VIII wrote voluminously in Horatian meters, including an ode on gout.
19th century, on.
Horace maintained a central role in the education of English-speaking elites right up until the 1960s. A pedantic emphasis on the formal aspects of language-learning at the expense of literary appreciation may have made him unpopular in some quarters yet it also confirmed his influencea tension in his reception that underlies Byron's famous lines from "Childe Harold" (Canto iv, 77):
William Wordsworth's mature poetry, including the preface to Lyrical Ballads, reveals Horace's influence in its rejection of false ornament and he once expressed "a wish / to meet the shade of Horace...". John Keats echoed the opening of Horace's "Epodes" 14 in the opening lines of "Ode to a Nightingale".
The Roman poet was presented in the nineteenth century as an honorary English gentleman. William Thackery produced a version of "Odes" 1.38 in which Horace's questionable 'boy' became 'Lucy', and Gerard Manley Hopkins translated the boy innocently as 'child'. Horace was translated by Sir Theodore Martin (biographer of Prince Albert) but minus some ungentlemanly verses, such as the erotic "Odes" 1.25 and "Epodes" 8 and 12. Lord Lytton produced a popular translation and William Gladstone also wrote translations during his last days as Prime Minister.
Edward FitzGerald's "Rubaiyat of Omar Khayyam", though formally derived from the Persian "ruba'i", nevertheless shows a strong Horatian influence, since, as one modern scholar has observed,""...the quatrains inevitably recall the stanzas of the 'Odes', as does the narrating first person of the world-weary, ageing Epicurean Omar himself, mixing sympotic exhortation and 'carpe diem' with splendid moralising and 'memento mori' nihilism."" Matthew Arnold advised a friend in verse not to worry about politics, an echo of "Odes" 2.11, yet later became a critic of Horace's inadequacies relative to Greek poets, as role models of Victorian virtues, observing: ""If human life were complete without faith, without enthusiasm, without energy, Horace...would be the perfect interpreter of human life."" Christina Rossetti composed a sonnet depicting a woman willing her own death steadily, drawing on Horace's depiction of 'Glycera' in "Odes" 1.19.5–6 and Cleopatra in "Odes" 1.37. A. E. Housman considered "Odes" 4.7, in Archilochian couplets, the most beautiful poem of antiquity and yet he generally shared Horace's penchant for quatrains, being readily adapted to his own elegiac and melancholy strain. The most famous poem of Ernest Dowson took its title and its heroine's name from a line of "Odes" 4.1, "Non sum qualis eram bonae sub regno Cynarae", as well as its motif of nostalgia for a former flame. Kipling wrote a famous parody of the "Odes", satirising their stylistic idiosyncrasies and especially the extraordinary syntax, but he also used Horace's Roman patriotism as a focus for British imperialism, as in the story "Regulus" in the school collection "Stalky & Co.", which he based on "Odes" 3.5. Wilfred Owen's famous poem, quoted above, incorporated Horatian text to question patriotism while ignoring the rules of Latin scansion. However, there were few other echoes of Horace in the war period, possibly because war is not actually a major theme of Horace's work.
Both W.H.Auden and Louis MacNeice began their careers as teachers of classics and both responded as poets to Horace's influence. Auden for example evoked the fragile world of the 1930s in terms echoing "Odes" 2.11.1–4, where Horace advises a friend not to let worries about frontier wars interfere with current pleasures.
The American poet, Robert Frost, echoed Horace's "Satires" in the conversational and sententious idiom of some of his longer poems, such as "The Lesson for Today" (1941), and also in his gentle advocacy of life on the farm, as in "Hyla Brook" (1916), evoking Horace's "fons Bandusiae" in "Ode" 3.13. Now at the start of the third millennium, poets are still absorbing and re-configuring the Horatian influence, sometimes in translation (such as a 2002 English/American edition of the "Odes" by thirty-six poets) and sometimes as inspiration for their own work (such as a 2003 collection of odes by a New Zealand poet).
Horace's "Epodes" have largely been ignored in the modern era, excepting those with political associations of historical significance. The obscene qualities of some of the poems have repulsed even scholars yet more recently a better understanding of the nature of Iambic poetry has led to a re-evaluation of the "whole" collection. A re-appraisal of the "Epodes" also appears in creative adaptations by recent poets (such as a 2004 collection of poems that relocates the ancient context to a 1950s industrial town).

</doc>
<doc id="13694" url="https://en.wikipedia.org/wiki?curid=13694" title="History of Microsoft Windows">
History of Microsoft Windows

In 1983, Microsoft announced the development of Windows, a graphical user interface (GUI) for its own operating system (MS-DOS). The product line has changed from a GUI product to a modern operating system over two families of design, each with its own codebase and default file system.
The 3.x and 4.x family includes Windows 3.0, Windows 3.1x, Windows 95, Windows 98, and Windows ME. Windows for Workgroups 3.11 added 32-bit networking. Windows 95 added additional 32-bit capabilities (however, MS-DOS, some of the kernel, and supplementary utilities such as Disk Defragment remained 16-bit) and implemented a new object oriented user interface, elements of which are still used today.
The Windows NT family started with Windows NT 3.1 in 1993. Modern Windows operating system versions are based on the newer Windows NT kernel that was originally intended for OS/2. Windows runs on IA-32, x86-64, and on 32-bit ARM (ARMv7) processors. Earlier versions also ran on the i860, Alpha, MIPS, Fairchild Clipper, PowerPC, and Itanium architectures. Some work was done to port it to the SPARC architecture.
The familiar Windows Explorer desktop shell superseded Program Manager with the release of Windows 95, received major enhancements in 1997, and remained the default shell for all commercial Windows releases until Windows 8's Modern UI-derived Start screen debuted in 2012.
Windows 1.0x.
The first independent version of Microsoft Windows, version 1.0, released on November 20, 1985, achieved little popularity. The project was briefly codenamed "Interface Manager" before the windowing system was developed - contrary to popular belief that it was the original name for Windows and Rowland Hanson, the head of marketing at Microsoft, convinced the company that the name "Windows" would be more appealing to customers.
Windows 1.0 was not a complete operating system, but rather an "operating environment" that extended MS-DOS, and shared the latter's inherent flaws and problems.
The first version of Microsoft Windows included a simple graphics painting program called Windows Paint; Windows Write, a simple word processor; an appointment calendar; a card-filer; a notepad; a clock; a control panel; a computer terminal; Clipboard; and RAM driver. It also included the MS-DOS Executive and a game called Reversi.
Microsoft had worked with Apple Computer to develop applications for Apple's new Macintosh computer, which featured a graphical user interface. As part of the related business negotiations, Microsoft had licensed certain aspects of the Macintosh user interface from Apple; in later litigation, a district court summarized these aspects as "screen displays".
In the development of Windows 1.0, Microsoft intentionally limited its borrowing of certain GUI elements from the Macintosh user interface, to comply with its license. For example, windows were only displayed "tiled" on the screen; that is, they could not overlap or overlie one another.
Windows 2.x.
Microsoft Windows version 2 came out on December 9, 1987, and proved slightly more popular than its predecessor.
Much of the popularity for Windows 2.0 came by way of its inclusion as a "run-time version" with Microsoft's new graphical applications, Excel and Word for Windows. They could be run from MS-DOS, executing Windows for the duration of their activity, and closing down Windows upon exit.
Microsoft Windows received a major boost around this time when Aldus PageMaker appeared in a Windows version, having previously run only on Macintosh. Some computer historians date this, the first appearance of a significant "and" non-Microsoft application for Windows, as the start of the success of Windows.
Versions 2.0x used the real-mode memory model, which confined it to a maximum of 1 megabyte of memory.
In such a configuration, it could run under another multitasker like DESQview, which used the 286 protected mode.
Later, two new versions were released: Windows/286 2.1 and Windows/386 2.1. Like prior versions of Windows, Windows/286 2.1 used the real-mode memory model, but was the first version to support the High Memory Area. Windows/386 2.1 had a protected mode kernel with LIM-standard EMS emulation. All Windows and DOS-based applications at the time were real mode, running over the protected mode kernel by using the virtual 8086 mode, which was new with the 80386 processor.
Version 2.03, and later 3.0, faced challenges from Apple over its overlapping windows and other features Apple charged mimicked the ostensibly copyrighted "look and feel" of its operating system and "embodie and generated a copy of the Macintosh" in its OS. Judge William Schwarzer dropped all but 10 of Apple's 189 claims of copyright infringement, and ruled that most of the remaining 10 were over uncopyrightable ideas.
Windows 3.0.
Windows 3.0, released in May 1990, improved capabilities given to native applications. It also allowed users to better multitask older MS-DOS based software compared to Windows/386, thanks to the introduction of virtual memory.
Windows 3.0's user interface finally resembled a serious competitor to the user interface of the Macintosh computer. PCs had improved graphics by this time, due to VGA video cards, and the protected/enhanced mode allowed Windows applications to use more memory in a more painless manner than their DOS counterparts could. Windows 3.0 could run in real, standard, or 386 enhanced modes, and was compatible with any Intel processor from the 8086/8088 up to the 80286 and 80386. This was the first version to run Windows programs in protected mode, although the 386 enhanced mode kernel was an enhanced version of the protected mode kernel in Windows/386.
Windows 3.0 received two updates. A few months after introduction, Windows 3.0a was released as a maintenance release, resolving bugs and improving stability. A "multimedia" version, Windows 3.0 with Multimedia Extensions 1.0, was released in October 1991. This was bundled with "multimedia upgrade kits", comprising a CD-ROM drive and a sound card, such as the Creative Labs Sound Blaster Pro. This version was the precursor to the multimedia features available in Windows 3.1 and later, and was part of Microsoft's specification for the Multimedia PC.
The features listed above and growing market support from application software developers made Windows 3.0 wildly successful, selling around 10 million copies in the two years before the release of version 3.1. Windows 3.0 became a major source of income for Microsoft, and led the company to revise some of its earlier plans. Support was discontinued on December 31, 2001.
OS/2.
During the mid to late 1980s, Microsoft and IBM had cooperatively been developing OS/2 as a successor to DOS. OS/2 would take full advantage of the aforementioned protected mode of the Intel 80286 processor and up to 16 MB of memory. OS/2 1.0, released in 1987, supported swapping and multitasking and allowed running of DOS executables.
A GUI, called the Presentation Manager (PM), was not available with OS/2 until version 1.1, released in 1988. Its API was incompatible with Windows. Version 1.2, released in 1989, introduced a new file system, HPFS, to replace the FAT file system.
By the early 1990s, conflicts developed in the Microsoft/IBM relationship. They cooperated with each other in developing their PC operating systems, and had access to each other's code. Microsoft wanted to further develop Windows, while IBM desired for future work to be based on OS/2. In an attempt to resolve this tension, IBM and Microsoft agreed that IBM would develop OS/2 2.0, to replace OS/2 1.3 and Windows 3.0, while Microsoft would develop a new operating system, OS/2 3.0, to later succeed OS/2 2.0.
This agreement soon however fell apart, and the Microsoft/IBM relationship was terminated. IBM continued to develop OS/2, while Microsoft changed the name of its (as yet unreleased) OS/2 3.0 to Windows NT. Both retained the rights to use OS/2 and Windows technology developed up to the termination of the agreement; Windows NT, however, was to be written anew, mostly independently (see below).
After an interim 1.3 version to fix up many remaining problems with the 1.x series, IBM released OS/2 version 2.0 in 1992. This was a major improvement: it featured a new, object-oriented GUI, the Workplace Shell (WPS), that included a desktop and was considered by many to be OS/2's best feature. Microsoft would later imitate much of it in Windows 95. Version 2.0 also provided a full 32-bit API, offered smooth multitasking and could take advantage of the 4 gigabytes of address space provided by the Intel 80386. Still, much of the system had 16-bit code internally which required, among other things, device drivers to be 16-bit code also. This was one of the reasons for the chronic shortage of OS/2 drivers for the latest devices. Version 2.0 could also run DOS and Windows 3.0 programs, since IBM had retained the right to use the DOS and Windows code as a result of the breakup.
Windows 3.1x.
In response to the impending release of OS/2 2.0, Microsoft developed Windows 3.1, which included several improvements to Windows 3.0, such as display of TrueType scalable fonts (developed jointly with Apple), improved disk performance in 386 Enhanced Mode, multimedia support, and bugfixes. It also removed Real Mode, and only ran on an 80286 or better processor. Later Microsoft also released Windows 3.11, a touch-up to Windows 3.1 which included all of the patches and updates that followed the release of Windows 3.1 in 1992.
In 1992 and 1993, Microsoft released Windows for Workgroups (WfW), which was available both as an add-on for existing Windows 3.1 installations and in a version that included the base Windows environment and the networking extensions all in one package. Windows for Workgroups included improved network drivers and protocol stacks, and support for peer-to-peer networking. There were two versions of Windows for Workgroups, WfW 3.1 and WfW 3.11. Unlike prior versions, Windows for Workgroups 3.11 ran in 386 Enhanced Mode only, and needed at least an 80386SX processor. One optional download for WfW was the "Wolverine" TCP/IP protocol stack, which allowed for easy access to the Internet through corporate networks.
All these versions continued version 3.0's impressive sales pace. Even though the 3.1x series still lacked most of the important features of OS/2, such as long file names, a desktop, or protection of the system against misbehaving applications, Microsoft quickly took over the OS and GUI markets for the IBM PC. The Windows API became the de facto standard for consumer software.
Windows NT 3.x.
Meanwhile, Microsoft continued to develop Windows NT. The main architect of the system was Dave Cutler, one of the chief architects of VMS at Digital Equipment Corporation (later acquired by Compaq, now part of Hewlett-Packard). Microsoft hired him in October 1988 to create a successor to OS/2, but Cutler created a completely new system instead. Cutler had been developing a follow-on to VMS at DEC called Mica, and when DEC dropped the project he brought the expertise and around 20 engineers with him to Microsoft. DEC also believed he brought Mica's code to Microsoft and sued. Microsoft eventually paid US$150 million and agreed to support DEC's Alpha CPU chip in NT.
Windows NT Workstation (Microsoft marketing wanted Windows NT to appear to be a continuation of Windows 3.1) arrived in Beta form to developers at the July 1992 Professional Developers Conference in San Francisco. Microsoft announced at the conference its intentions to develop a successor to both Windows NT and Windows 3.1's replacement (Windows 95, codenamed Chicago), which would unify the two into one operating system. This successor was codenamed Cairo. In hindsight, Cairo was a much more difficult project than Microsoft had anticipated and, as a result, NT and Chicago would not be unified until Windows XP—albeit Windows 2000, oriented to business, had already unified most of the system’s bolts and gears, it was XP that was sold to home consumers like Windows 95 and came to be viewed as the final unified OS. Parts of Cairo have still not made it into Windows as of 2016 - most notably, the WinFS file system, which was the much touted Object File System of Cairo. Microsoft announced that they have discontinued the separate release of WinFS for Windows XP and Windows Vista and will gradually incorporate the technologies developed for WinFS in other products and technologies, notably Microsoft SQL Server.
Driver support was lacking due to the increased programming difficulty in dealing with NT's superior hardware abstraction model. This problem plagued the NT line all the way through Windows 2000. Programmers complained that it was too hard to write drivers for NT, and hardware developers were not going to go through the trouble of developing drivers for a small segment of the market. Additionally, although allowing for good performance and fuller exploitation of system resources, it was also resource-intensive on limited hardware, and thus was only suitable for larger, more expensive machines.
However, these same features made Windows NT perfect for the LAN server market (which in 1993 was experiencing a rapid boom, as office networking was becoming common). NT also had advanced network connectivity options and NTFS, an efficient file system. Windows NT version 3.51 was Microsoft's entry into this field, and took away market share from Novell (the dominant player) in the following years.
One of Microsoft's biggest advances initially developed for Windows NT was a new 32-bit API, to replace the legacy 16-bit Windows API. This API was called Win32, and from then on Microsoft referred to the older 16-bit API as Win16. The Win32 API had three levels implementations: the complete one for Windows NT, a subset for Chicago (originally called Win32c) missing features primarily of interest to enterprise customers (at the time) such as security and Unicode support, and a more limited subset called Win32s which could be used on Windows 3.1 systems. Thus Microsoft sought to ensure some degree of compatibility between the Chicago design and Windows NT, even though the two systems had radically different internal architectures. Windows NT was the first Windows operating system based on a hybrid kernel.
As released, Windows NT 3.x went through three versions (3.1, 3.5, and 3.51); changes were primarily internal and reflected back end changes. The 3.5 release added support for new types of hardware and improved performance and data reliability; the 3.51 release was primarily to update the Win32 APIs to be compatible with software being written for the Win32c APIs in what became Windows 95.
Windows 95.
After Windows 3.11, Microsoft began to develop a new consumer oriented version of the operating system codenamed Chicago. Chicago was designed to have support for 32-bit preemptive multitasking like OS/2 and Windows NT, although a 16-bit kernel would remain for the sake of backward compatibility. The Win32 API first introduced with Windows NT was adopted as the standard 32-bit programming interface, with Win16 compatibility being preserved through a technique known as "thunking". A new object oriented GUI was not originally planned as part of the release, although elements of the Cairo user interface were borrowed and added as other aspects of the release (notably Plug and Play) slipped.
Microsoft did not change all of the Windows code to 32-bit; parts of it remained 16-bit (albeit not directly using real mode) for reasons of compatibility, performance, and development time. Additionally it was necessary to carry over design decisions from earlier versions of Windows for reasons of backwards compatibility, even if these design decisions no longer matched a more modern computing environment. These factors eventually began to impact the operating system's efficiency and stability.
Microsoft marketing adopted Windows 95 as the product name for Chicago when it was released on August 24, 1995. Microsoft had a double gain from its release: first, it made it impossible for consumers to run Windows 95 on a cheaper, non-Microsoft DOS; secondly, although traces of DOS were never completely removed from the system and MS DOS 7 would be loaded briefly as a part of the booting process, Windows 95 applications ran solely in 386 enhanced mode, with a flat 32-bit address space and virtual memory. These features make it possible for Win32 applications to address up to 2 gigabytes of virtual RAM (with another 2 GB reserved for the operating system), and in theory prevented them from inadvertently corrupting the memory space of other Win32 applications. In this respect the functionality of Windows 95 moved closer to Windows NT, although Windows 95/98/ME did not support more than 512 megabytes of physical RAM without obscure system tweaks.
IBM continued to market OS/2, producing later versions in OS/2 3.0 and 4.0 (also called Warp). Responding to complaints about OS/2 2.0's high demands on computer hardware, version 3.0 was significantly optimized both for speed and size. Before Windows 95 was released, OS/2 Warp 3.0 was even shipped preinstalled with several large German hardware vendor chains. However, with the release of Windows 95, OS/2 began to lose market share.
It is probably impossible to choose one specific reason why OS/2 failed to gain much market share. While OS/2 continued to run Windows 3.1 applications, it lacked support for anything but the Win32s subset of Win32 API (see above). Unlike with Windows 3.1, IBM did not have access to the source code for Windows 95 and was unwilling to commit the time and resources to emulate the moving target of the Win32 API. IBM later introduced OS/2 into the United States v. Microsoft case, blaming unfair marketing tactics on Microsoft's part.
Microsoft went on to release five different versions of Windows 95:
OSR2, OSR2.1, and OSR2.5 were not released to the general public; rather, they were available only to OEMs that would preload the OS onto computers. Some companies sold new hard drives with OSR2 preinstalled (officially justifying this as needed due to the hard drive's capacity).
The first Microsoft Plus! add-on pack was sold for Windows 95.
Windows NT 4.0.
Windows NT 4.0 was the successor of 3.51 (1995) and 3.5 (1994). Microsoft released Windows NT 4.0 to manufacturing in July 1996, one year after the release of Windows 95. Major new features included the new Explorer shell from Windows 95, scalability and feature improvements to the core architecture, kernel, USER32, COM and MSRPC.
Windows NT 4.0 came in four versions:
Windows 98.
On June 25, 1998, Microsoft released Windows 98 (codenamed Memphis). It included new hardware drivers and the FAT32 file system which supports disk partitions that are larger than 2 GB (first introduced in Windows 95 OSR2). USB support in Windows 98 is marketed as a vast improvement over Windows 95. The release continued the controversial inclusion of the Internet Explorer browser with the operating system that started with Windows 95 OEM Service Release 1. The action eventually led to the filing of the United States v. Microsoft case, dealing with the question of whether Microsoft was introducing unfair practices into the market in an effort to eliminate competition from other companies such as Netscape.
In 1999, Microsoft released Windows 98 Second Edition, an interim release. One of the more notable new features was the addition of Internet Connection Sharing, a form of network address translation, allowing several machines on a LAN (Local Area Network) to share a single Internet connection. Hardware support through device drivers was increased and this version shipped with Internet Explorer 5. Many minor problems that existed in the first edition were fixed making it, according to many, the most stable release of the Windows 9x family.
Windows 2000.
Microsoft released Windows 2000 in February 2000. It has the version number Windows NT 5.0. Windows 2000 has had four official service packs. It was successfully deployed both on the server and the workstation markets. Amongst Windows 2000's most significant new features was Active Directory, a near-complete replacement of the NT 4.0 Windows Server domain model, which built on industry-standard technologies like DNS, LDAP, and Kerberos to connect machines to one another. Terminal Services, previously only available as a separate edition of NT 4, was expanded to all server versions. A number of features from Windows 98 were incorporated also, such as an improved Device Manager, Windows Media Player, and a revised DirectX that made it possible for the first time for many modern games to work on the NT kernel. Windows 2000 is also the last NT-kernel Windows operating system to lack product activation.
While Windows 2000 upgrades were available for Windows 95 and Windows 98, it was not intended for home users.
Windows 2000 was available in four editions:
Windows ME.
In September 2000, Microsoft released a successor to Windows 98 called Windows ME, short for "Millennium Edition". It was the last DOS-based operating system from Microsoft. Windows ME introduced a new multimedia-editing application called Windows Movie Maker, came standard with Internet Explorer 5.5 and Windows Media Player 7, and debuted the first version of System Restore – a recovery utility that enables the operating system to revert system files back to a prior date and time. System Restore was a notable feature that would continue to thrive in later versions of Windows, including XP, Vista, and Windows 7.
Windows ME was conceived as a quick one-year project that served as a stopgap release between Windows 98 and Windows XP. Many of the new features were available from the Windows Update site as updates for older Windows versions ("System Restore" and "Windows Movie Maker" were exceptions). Windows ME was criticized for stability issues, and for lacking real mode DOS support, which was hidden, but still there (ME was still an operating medium and not an OS, like the rest of 9X windows line actual OS was MS-DOS), so you have it in your computer but cannot use it, to the point of being referred to as the "Mistake Edition" or "Many Errors." Windows ME was the last operating system to be based on the Windows 9x (monolithic) kernel and MS-DOS.
Windows XP.
On October 25, 2001, Microsoft released Windows XP (codenamed "Whistler"). The merging of the Windows NT/2000 and Windows 95/98/Me lines was finally achieved with Windows XP. Windows XP uses the Windows NT 5.1 kernel, marking the entrance of the Windows NT core to the consumer market, to replace the aging 16/32-bit branch. The initial release met with considerable criticism, particularly in the area of security, leading to the release of three major Service Packs. Windows XP SP1 was released in September 2002, SP2 came out in August 2004 and SP3 came out in April 2008. Service Pack 2 provided significant improvements and encouraged widespread adoption of XP among both home and business users. Windows XP lasted longer as Microsoft's flagship operating system than any other version of Windows, from October 25, 2001 to January 30, 2007 when it was "succeeded" by Windows Vista.
Windows XP is available in a number of versions:
Windows Server 2003.
On April 25, 2003 Microsoft launched Windows Server 2003, a notable update to Windows 2000 Server encompassing many new security features, a new "Manage Your Server" wizard that simplifies configuring a machine for specific roles, and improved performance. It has the version number NT 5.2. A few services not essential for server environments are disabled by default for stability reasons, most noticeable are the "Windows Audio" and "Themes" services; users have to enable them manually to get sound or the "Luna" look as per Windows XP. The hardware acceleration for display is also turned off by default, users have to turn the acceleration level up themselves if they trust the display card driver.
December 2005, Microsoft released Windows Server 2003 R2, which is actually Windows Server 2003 with SP1 (Service Pack 1) plus an add-on package.
Among the new features are a number of management features for branch offices, file serving, printing and company-wide identity integration.
Windows Server 2003 is available in six editions:
Windows XP x64 and Server 2003 x64 Editions.
On April 25, 2005, Microsoft released Windows XP Professional x64 Edition and Windows Server 2003, x64 Editions in Standard, Enterprise and Datacenter SKUs. Windows XP Professional x64 Edition is an edition of Windows XP for x86-64 personal computers. It is designed to use the expanded 64-bit memory address space provided by the x86-64 architecture.
Windows XP Professional x64 Edition is based on the Windows Server 2003 codebase; with the server features removed and client features added. Both "Windows Server 2003 x64" and Windows XP Professional x64 Edition use identical kernels.
Windows XP Professional "x64 Edition" is not to be confused with Windows XP "64-bit Edition", as the latter was designed for Intel Itanium processors. During the initial development phases, Windows XP Professional x64 Edition was named "Windows XP 64-Bit Edition for 64-Bit Extended Systems".
Windows Server 2003 R2.
Windows Server 2003 R2, an update of Windows Server 2003, was released to manufacturing on December 6, 2005. It is distributed on two CDs, with one CD being the Windows Server 2003 SP1 CD. The other CD adds many optionally installable features for Windows Server 2003. The R2 update was released for all x86 and x64 versions. Windows Server 2003 R2 Enterprise Edition was not released for Itanium.
Windows Fundamentals for Legacy PCs.
In July 2006, Microsoft released a thin-client version of Windows XP Service Pack 2, called Windows Fundamentals for Legacy PCs (WinFLP). It is only available to Software Assurance customers. The aim of WinFLP is to give companies a viable upgrade option for older PCs that are running Windows 95, 98, and Me that will be supported with patches and updates for the next several years. Most user applications will typically be run on a remote machine using Terminal Services or Citrix.
Windows Home Server.
Windows Home Server (codenamed Q, Quattro) is a server product based on Windows Server 2003, designed for consumer use. The system was announced on January 7, 2007 by Russel Adolfo. Windows Home Server can be configured and monitored using a console program that can be installed on a client PC. Such features as Media Sharing, local and remote drive backup and file duplication are all listed as features. The release of Windows Home Server Power Pack 3 added support for Windows 7 to Windows Home Server.
Windows Vista.
Windows Vista was released on November 8, 2006 to business customers - consumer versions followed on January 30, 2007. Windows Vista intended to have enhanced security by introducing a new restricted user mode called User Account Control, replacing the "administrator-by-default" philosophy of Windows XP. Vista was the target of much criticism and negative press, and in general was not well regarded; this was seen as leading to the relatively swift release of Windows 7.
One major difference between Vista and earlier versions of Windows, Windows 95 and later, is that the original start button was replaced with the Windows icon in a circle (called the Start Orb). Vista also features new graphics features, the Windows Aero GUI, new applications (such as Windows Calendar, Windows DVD Maker and some new games including Chess, Mahjong, and Purble Place), Internet Explorer 7, Windows Media Player 11, and a large number of underlying architectural changes including Windows Powershell being shipped with the operating system, which many believed to have taken place in Windows 7's architecture and later. Windows Vista has the version number NT 6.0. Since its release, Windows Vista has had two service packs.
Windows Vista ships in six editions:
All editions (except Starter edition) are currently available in both 32-bit and 64-bit versions. The biggest advantage of the 64-bit version is breaking the 4 gigabyte memory barrier, which 32-bit computers cannot fully access.
Windows Server 2008.
Windows Server 2008, released on February 27, 2008, was originally known as Windows Server Codename "Longhorn". Windows Server 2008 builds on the technological and security advances first introduced with Windows Vista, and is significantly more modular than its predecessor, Windows Server 2003.
Windows Server 2008 ships in ten editions:
Windows 7 and Windows Server 2008 R2.
Windows 7 was released to manufacturing on July 22, 2009, and reached general retail availability on October 22, 2009. It was previously known by the codenames Blackcomb and Vienna. Windows 7 has the version number NT 6.1. Since its release, Windows 7 has had one service pack.
Some features of Windows 7 are faster booting, Device Stage, Windows PowerShell, less obtrusive User Account Control, multi-touch, and improved window management. Features included with Windows Vista and not in Windows 7 include the sidebar (although gadgets remain) and several programs that were removed in favor of downloading their Windows Live counterparts.
Windows 7 ships in six editions:
In some countries (Austria, Belgium, Bulgaria, Croatia, Cyprus, Czech Republic, Denmark, Estonia, Finland, France, Germany, United Kingdom, Greece, Hungary, Iceland, Ireland, Italy, Latvia, Liechtenstein, Lithuania, Luxembourg, Malta, Netherlands, Norway, Poland, Portugal, Romania, Slovakia, Slovenia, Spain, Sweden, and Switzerland), there are other editions that lack some features such as Windows Media Player, Windows Media Center and Internet Explorer - these editions were called names such as "Windows 7 N."
Microsoft focuses on selling Windows 7 Home Premium and Professional. All editions, except the Starter edition, are available in both 32-bit and 64-bit versions.
Unlike the corresponding Vista editions, the Professional and Enterprise editions are supersets of the Home Premium edition.
At the Professional Developers Conference (PDC) 2008, Microsoft also announced Windows Server 2008 R2, as the server variant of Windows 7. Windows Server 2008 R2 ships in 64-bit versions (x64 and Itanium) only.
Windows Home Server 2011.
Windows Home Server 2011 code named 'Vail' was released on April 6, 2011. Windows Home Server 2011 is built on the Windows Server 2008 R2 code base and removed the Drive Extender drive pooling technology in the original Windows Home Server release. Windows Home Server 2011 is considered a "major release". Its predecessor was built on Windows Server 2003. WHS 2011 only supports x86-64 hardware.
Microsoft decided to kill WHS 2011 on July 5, 2012 while including its features into Windows Server 2012 Essentials Windows Home Server 2011 is supported until April 12, 2016 [https://support.microsoft.com/en-us/lifecycle/search/default.aspx?sort=PN&alpha=Windows%20Home%20Server&wa=wsignin1.0
Windows Thin PC.
In 2011, Microsoft introduced Windows Thin PC or WinTPC, which is a feature- and size-reduced locked-down version of Windows 7 expressly designed to turn older PCs into thin clients. WinTPC is available for software assurance customers and relies on cloud computing in a business network. Wireless operation is supported since WinTPC has full wireless stack integration, but wireless operation may not be as good as the operation on a wired connection.
Windows 8 and Windows Server 2012.
On October 26, 2012, Microsoft released Windows 8 to the public. One edition, Windows RT, runs on some system-on-a-chip devices with mobile 32-bit ARM (ARMv7) processors. Windows 8 features a redesigned user interface, designed to make it easier for touchscreen users to use Windows. The interface introduced an updated Start menu known as the Start screen, and a new full-screen application platform. The desktop interface is also present for running windowed applications, although Windows RT will not run any desktop applications not included in the system. On the Building Windows 8 blog, it was announced that a computer running Windows 8 can boot up much faster than Windows 7. New features also include USB 3.0 support, the Windows Store, the ability to run from USB drives with Windows To Go, and others. Windows 8 was given the kernel number NT 6.2, with its predecessor 8.1 receiving the kernel number 6.3. So far, neither has had any service packs yet.
Windows 8 is available in the following editions:
The first public preview of Windows Server 2012 was also shown by Microsoft at the 2011 Microsoft Worldwide Partner Conference.
Windows 8 Release Preview and Windows Server 2012 Release Candidate were both released on May 31, 2012. Product development on Windows 8 was completed on August 1, 2012, and it was released to manufacturing the same day. Windows Server 2012 went on sale to the public on September 4, 2012. Windows 8 went on sale October 26, 2012.
Windows 8.1 and Windows Server 2012 R2 were released on October 17, 2013. Windows 8.1 is available as an update in the Windows store for Windows 8 users only and also available to download for clean installation. The update adds new options for resizing the live tiles on the Start screen.
Windows 10.
Windows 10 is the current release of the Microsoft Windows operating system. Unveiled on August 30, 2014, it was released on July 29, 2015. It is being distributed without charge to Windows 7 and 8.1 users for the first year. A number of new features like Cortana, the Microsoft Edge, Windowed Windows Store apps, Virtual desktops/Task View, Action Center, Revamped core apps, The Xbox app, Continuum, Unified settings, and more first appeared in the latest edition. Microsoft Edge is the new web browser of Windows 10 which is specially designed to be a lightweight web browser. Given the kernel number of 10, this operating system has no service packs. It has, however, gone through one big update on November 10, 2015, with the version number 1511.
Windows Server 2016.
Windows Server 2016 is an upcoming release of the Microsoft Windows Server operating system. Unveiled on September 30, 2014, it is planned to be released this year.

</doc>
<doc id="13696" url="https://en.wikipedia.org/wiki?curid=13696" title="Helsinki">
Helsinki

Helsinki (; ; ) is the capital and largest city of Finland. It is in the region of Uusimaa, in southern Finland, on the shore of the Gulf of Finland, an arm of the Baltic Sea. Helsinki has a population of , an urban population of 1.2 million (31 December 2013), and a metropolitan population of 1.4 million, making it the most populous municipality and urban area in Finland. Helsinki is located some north of Tallinn, Estonia, east of Stockholm, Sweden, and west of Saint Petersburg, Russia. Helsinki has close historical connections with these three cities.
The Helsinki metropolitan area includes the urban core of Helsinki, Espoo, Vantaa, Kauniainen, and surrounding commuter towns. It is the world's northernmost metro area of over one million people, and the city is the northernmost capital of an EU member state. The Helsinki metropolitan area is the fourth largest metropolitan area in the Nordic countries and the City of Helsinki is the third largest Nordic city after Stockholm and Oslo.
Helsinki is Finland's major political, educational, financial, cultural, and research center as well as one of northern Europe's major cities. Approximately 75% of foreign companies operating in Finland have settled in the Helsinki region. The nearby municipality of Vantaa is the location of Helsinki Airport, with frequent service to various destinations in Europe and Asia.
In 2009, Helsinki was chosen to be the World Design Capital for 2012 by the International Council of Societies of Industrial Design, narrowly beating Eindhoven for the title. The city was the venue for the 1952 Summer Olympics and the 52nd Eurovision Song Contest 2007.
In 2011, the "Monocle" magazine ranked Helsinki the most liveable city in the world in its "Liveable Cities Index 2011". In the Economist Intelligence Unit's August 2015 Liveability survey, assessing the best and worst cities to live in globally, Helsinki placed among the world's top ten cities.
Etymology.
"Helsinki" is used to refer to the city in most languages, but not in Swedish.
The Swedish name "" is the original official name of the city (originally in the form "Hellssingeforss"). The Finnish name probably comes from "Helsinga" and similar names used for the river that is currently known as the Vantaa River, as documented already in the 14th century. "Helsingfors" comes from the name of the surrounding parish, "Helsinge" (source for Finnish "Helsinki") and the rapids (), which flowed through the original village, which is now located in the neighboring city Vantaa. As part of the Grand Duchy of Finland in the Russian Empire, Helsinki was known as "Gelsingfors" in Russian.
One suggestion for the origin of the name "Helsinge" is that it originated with medieval Swedish settlers who came from Hälsingland in Sweden. Others have proposed that the name derives from the Swedish word "helsing", a former version of the word "hals" (neck), referring to the narrowest part of a river, i.e. the rapids. Other Scandinavian cities located at similar geographic locations were given similar names at the time, for example Helsingør and Helsingborg.
The name "Helsinki" has been used in Finnish official documents and in Finnish language newspapers since 1819, when the Senate of Finland moved to Helsinki from Turku (Åbo). The decrees issued in Helsinki were dated with Helsinki as the place of issue. This is how the form Helsinki came to be used in written Finnish.
In Helsinki slang the city is called either "Stadi" (from the Swedish word "stad", meaning "city") or "Hesa" (short for Helsinki), with "Stadi" being used to assert that the speaker is native to the city. "" is the Northern Sami name of Helsinki.
History.
Early history.
Helsinki was established as a trading town by King Gustav I of Sweden in 1550 as the town of Helsingfors, which he intended to be a rival to the Hanseatic city of Reval (today known as Tallinn). Little came of the plans as Helsinki remained a tiny town plagued by poverty, wars, and diseases. The plague of 1710 killed the greater part of the inhabitants of Helsinki. The construction of the naval fortress Sveaborg (In Finnish "Viapori", today also "Suomenlinna") in the 18th century helped improve Helsinki's status, but it was not until Russia defeated Sweden in the Finnish War and annexed Finland as the autonomous Grand Duchy of Finland in 1809 that the town began to develop into a substantial city. During the war, Russians besieged the Sveaborg fortress, and about one quarter of the town was destroyed in an 1808 fire.
Czar Alexander I of Russia moved the Finnish capital from Turku to Helsinki in 1812 to reduce Swedish influence in Finland, and to bring the capital closer to St. Petersburg. Following the Great Fire of Turku in 1827, The Royal Academy of Turku, at the time the country's only university, was also relocated to Helsinki, and eventually became the modern University of Helsinki. The move consolidated the city's new role and helped set it on a path of continuous growth. This transformation is highly apparent in the downtown core, which was rebuilt in neoclassical style to resemble St. Petersburg, mostly to a plan by the German-born architect C. L. Engel. As elsewhere, technological advancements such as railroads and industrialization were key factors behind the city's growth.
Twentieth century.
Despite the tumultuous nature of Finnish history during the first half of the 20th century (including the Finnish Civil War and the Winter War which both left marks on the city), Helsinki continued its steady development. A landmark event was the XV Olympic games (1952 Olympic Games), held in Helsinki. Finland's rapid urbanization in the 1970s, occurring late relative to the rest of Europe, tripled the population in the metropolitan area, and the Helsinki Metro subway system was built. The relatively sparse population density of Helsinki and its peculiar structure have often been attributed to the lateness of its growth.
Geography.
Called the "Daughter of the Baltic", Helsinki is located on the tip of a peninsula and on 315 islands. The inner city area occupies a southern peninsula, which is rarely referred to by its actual name Vironniemi. Population density in certain parts of Helsinki's inner city area is very high, reaching in the district of Kallio, but as a whole Helsinki's population density of ranks the city as quite sparsely populated in comparison to other European capital cities. Much of Helsinki outside the inner city area consists of postwar suburbs separated from each other by patches of forest. A narrow, long Helsinki Central Park, stretching from the inner city to the northern border of Helsinki, is an important recreational area for residents. The City of Helsinki has about 11,000 boat berths and possesses over 14 000 hectares of marine fishing waters adjacent to the Capital Region. Some 60 fish species are found in this area. Recreational fishing is a popular hobby among kids and adults alike.
Major islands in Helsinki include Seurasaari, Vallisaari, Lauttasaari, and Korkeasaari – the lattermost being the site of the country's biggest zoo. Other noteworthy islands are the fortress island of Suomenlinna (Sveaborg), the military island of Santahamina, and Isosaari. Pihlajasaari island is a favorite summer spot for gay men and naturists, comparable to Fire Island off New York City.
Metropolitan area.
The Helsinki Capital Region consists of the four municipalities of Helsinki, Espoo, Vantaa, and Kauniainen and is considered to be the only metropolis in Finland. It has a population of over 1,1 million, and is by far the biggest and most densely populated area of Finland, over four times bigger than Tampere. The Capital Region spreads over a land area of and has a population density of . With over 20 percent of the country's population in just 0.2 percent of its surface area, the housing density of the area is high by Finnish standards.
The Helsinki Metropolitan Area (Greater Helsinki) consists of the cities of Helsinki Capital Region and ten surrounding municipalities. The Metropolitan Area covers and contains a total population of over 1.4 million, or about a fourth of the total population of Finland. The Metropolitan Area has a high concentration of employment: approximately 750,000 jobs. Despite the intensity of land use, the region also has large recreational areas and green spaces. The Greater Helsinki area is the world's northernmost urban area with a population of over one million people, and the city is the northernmost capital of an EU member state.
Climate.
Helsinki has a humid continental climate (Köppen: Dfb), less than above the threshold for subarctic classification. Owing to the mitigating influence of the Baltic Sea and North Atlantic Current (see also Extratropical cyclone), temperatures in winter are higher than the northern location might suggest, with the average in January and February around .
Winters in Helsinki are notably warmer than in the north, and the snow season is much shorter. Temperatures below occur a few times a year or less. However, because of the latitude, days last 5 hours and 48 minutes around the winter solstice with very low Sun (at noon Sun is little bit over 6 degrees in the sky), and the cloudy weather at this time of year accentuates the darkness. Conversely, Helsinki enjoys long daylight in summer, during the summer solstice days last 18 hours and 57 minutes .
The average maximum temperature from June to August is around . Due to the marine effect, especially during hot summer days, daily temperatures are a little cooler and night temperatures are higher than further away in the mainland. The highest temperature ever recorded in the city centre was , on 18 July 1945, and the lowest was , on 10 January 1987. Helsinki Airport (located in Vantaa, north of the Helsinki city centre) recorded a temperature of , on 29 July 2010, and a low of , on 9 January 1987. Precipitation is received from frontal passages and thunderstorms. Thunderstorms are most common in summer.
Cityscape.
Carl Ludvig Engel (1778–1840) was appointed to design a new city centre all on his own. He designed several neoclassical buildings in Helsinki. The focal point of Engel's city plan is the Senate Square. It is surrounded by the Government Palace (to the east), the main building of Helsinki University (to the west), and (to the north) the enormous Cathedral, which was finished in 1852, twelve years after C. L. Engel's death. Subsequently, Engel's neoclassical plan stimulated the epithet, "The White City of the North". Helsinki is, however, perhaps even more famous for its numerous Art Nouveau (Jugend in Finnish) influenced buildings of the romantic nationalism, designed in the early 1900s and strongly influenced by "The Kalevala", which is a very popular theme in the national romantic art of that era. Helsinki's Art Nouveau style is also featured in large residential areas such as Katajanokka and Ullanlinna. The master of the Finnish Art Nouveau was Eliel Saarinen (1873–1950), whose architectural masterpiece was the Helsinki Central railway station.
Helsinki also features several buildings by the world-renowned Finnish architect Alvar Aalto (1898–1976), recognized as one of the pioneers of architectural functionalism. However, some of his works, such as the headquarters of the paper company Stora Enso and the concert venue Finlandia Hall, have been subject to divided opinions from the citizens.
Renowned functionalist buildings in Helsinki by other architects include the Olympic Stadium, the Tennis Palace, the Rowing Stadium, the Swimming Stadium, the Velodrome, the Glass Palace, the Exhibition Hall (now Töölö Sports Hall), and Helsinki-Malmi Airport. The sports venues were built to serve the 1940 Helsinki Olympic Games; the games were initially cancelled due to the Second World War, but the venues eventually got to fulfill their purpose in the 1952 Olympic Games. Many of them are listed by DoCoMoMo as significant examples of modern architecture. The Olympic Stadium and Helsinki-Malmi Airport are in addition catalogued by the Finnish National Board of Antiquities as cultural-historical environments of national significance.
As a historical footnote, Helsinki's neoclassical buildings were often used as a backdrop for scenes set to take place in the Soviet Union in many Cold War era Hollywood movies, when filming in the USSR was not possible. Some of the more notable ones are "The Kremlin Letter" (1970), "Reds" (1981), and "Gorky Park" (1983). Because some streetscapes were reminiscent of Leningrad's and Moscow's old buildings, they too were used in movie productions—much to some residents' dismay. At the same time the government secretly instructed Finnish officials not to extend assistance to such film projects.
In the 21st century Helsinki has decided to allow the construction of skyscrapers. Several projects are already in progress, mainly in Pasila and Kalasatama. The tallest (with 40 floors) will rise to at least 150 metres (500 feet). In Pasila, twenty new high-rises will be erected within 10 years. In Kalasataman Keskus REDI, the first 35-story (130 metres) and 32-story (122 metres) residential towers are already under construction. Later they will be joined by a 37-story (140 metres), two 32-story (122 metres, 400 feet), 31-story (120 metres), and 27-story (100 metres) residential buildings. In the Kalasatama area, there will be 30 high-rises within 10 years.
Government.
As in all Finnish municipalities, the city council is the main decision-making organ in local politics, dealing with issues such as city planning, schools, health care, and public transport. The council is elected every four years.
The City Council of Helsinki consists of eighty-five members. Following the most recent municipal elections, in 2012, the three largest parties are the National Coalition Party (23), the Greens (19), and the Social Democrats (15). The Mayor, Jussi Pajunen, is a member of the National Coalition Party.
Traditionally, the conservative National Coalition Party (Kokoomus) has been the biggest party on Helsinki City Council, with the Social Democrats being the second biggest. In 2000 the Greens, for which Helsinki is the strongest area of support nationally, gained the position of second most popular party in the city, in 2004 the Social Democrats regained that position, and since 2008 the Greens have again been the second biggest party.
The Left Alliance is the fourth largest party, while the True Finns have increased their support steadily to become the fifth largest party. Support for the Swedish People's Party has been steadily declining over the years, most likely because of the diminishing proportion of Swedish speakers in Helsinki. The Centre Party of Finland, despite being one of the major parties in national politics, has little support in Helsinki, as is the case in most big cities.
Demographics.
Helsinki has a higher proportion of women (53.4%) than elsewhere in Finland (51.1%). Helsinki's current population density of 2,739.36 people per square kilometre is by far the highest in Finland. Life expectancy for both genders is slightly below the national averages: 75.1 years for men as compared to 75.7 years, 81.7 years for women as compared to 82.5 years.
Helsinki has experienced strong growth since the 1810s, when it replaced Turku as the capital of the Grand Duchy of Finland, which later became the sovereign Republic of Finland. The city continued to show strong growth from that time onward, with the exception during the Finnish Civil War period. From the end of World War II up until the 1970s there was a massive exodus of people from the countryside to the cities of Finland, in particular Helsinki. Between 1944 and 1969 the population of the city nearly doubled from 275,000 to 525,600.
In the 1960s, the population growth of Helsinki proper began to ebb mainly due to lack of housing. Many residents began to move to neighbouring Espoo and Vantaa, where population growth has since soared. Espoo's population increased ninefold in sixty years, from 22,874 people in 1950 to 244,353 in 2009. Neighboring Vantaa has seen even more dramatic change in the same time span: from 14,976 in 1950 to 197,663 in 2009, a thirteenfold increase. These dramatic increases pushed the municipalities of greater Helsinki into more intense cooperation in such areas as public transportation and waste management. The increasing scarcity of housing and the higher costs of living in the Helsinki Capital Region have pushed many daily commuters to find housing in formerly very rural areas, and even further, to such cities as Lohja ( northwest from the city centre), Hämeenlinna and Lahti (both from Helsinki), and Porvoo ( to the east).
Language.
[[File:Helsingfors språk.png|thumb|left|The population broken down by language group, 1870–2013. During the period, the population increased significantly, and the city changed its linguistic majority from Swedish to Finnish.
Finnish and Swedish are the official languages of the municipality of Helsinki. The majority, or 81.9% of the population, speaks Finnish as their native language. A minority, at 5.9%, speaks Swedish. Around 12.2% of the population speaks a native language other than Finnish or Swedish. Helsinki slang today combines influences mainly from Finnish and English, but has traditionally had strong Russian and Swedish influences. Finnish today is the common language of communication between Finnish speakers, Swedish speakers, and speakers of other languages (New Finns) in day-to-day affairs in the public sphere between unknown persons. In instances where a speaker's knowledge of Finnish is not known, English is usually spoken. Swedish is commonly spoken in city or national agencies specifically aimed at Finland-Swedish speakers, such as the Social Services Department on Hämeentie or the Luckan Cultural centre in Kamppi. Knowledge of Finnish is also essential in business and is usually a basic requirement in the employment market.
Finnish speakers surpassed Swedish speakers in 1890 to become the majority of the city's population. At the time, the population of Helsinki was 61,530.
Immigration.
Helsinki is the global gateway to and from Finland. The city has Finland's largest immigrant population in both absolute and relative terms. There are over 140 nationalities represented in Helsinki. The largest groups () are from Sweden, Russia, Estonia, Somalia, China, Kurdistan, Spain, Germany, France, Vietnam, and Turkey. Helsinki was already an international city in the 19th century with a distinctive Swedish majority as well as Finnish, Russian, and German minorities.
Foreign citizens make up 8.0% of the population, while the total foreign-born population makes up 11.1%. In 2012, 68,375 residents spoke a native language other than Finnish, Swedish, or one of the three Sami languages spoken in Finland. The largest groups of residents not of Finnish background come from Russia (14,532), Estonia (9,065), and Somalia (6,845). Half of the immigrant population in Finland lives in Greater Helsinki, and one third in the city of Helsinki.
Economy.
The Helsinki Metropolitan Area generates approximately one third of Finland's GDP. GDP per capita is roughly 1.3 times the national average.
The metropolitan area's gross value added per capita is 200% of the mean of 27 European metropolitan areas, equalling those of Stockholm or Paris. The gross value added annual growth has been around 4%.
83 of the 100 largest Finnish companies are headquartered in Greater Helsinki. Two-thirds of the 200 highest-paid Finnish executives live in Greater Helsinki and 42% in Helsinki. The average income of the top 50 earners was 1.65 million euro.
The tap water is of excellent quality and it is supplied by long Päijänne Water Tunnel, one of the world's longest continuous rock tunnels. Bottled Helsinki tap water is even sold to other countries, such as Saudi Arabia.
Education.
Helsinki has 190 comprehensive schools, 41 upper secondary schools, and 15 vocational institutes. Half of the 41 upper secondary schools are private or state-owned, the other half municipal. Higher level education is given in eight universities (see the section "Universities" below) and four polytechnics.
University of Applied Sciences.
Helsinki is one of the co-location centres of the Knowledge and Innovation Community (Future information and communication society) of The European Institute of Innovation and Technology (EIT).
The educational department takes part in Lifelong Learning Programme 2007–2013 in Finland.
Culture.
Museums.
The biggest historical museum in Helsinki is the National Museum of Finland, which displays a vast historical collection from prehistoric times to the 21st century. The museum building itself, a national romantic style neomedieval castle, is a tourist attraction. Another major historical museum is the Helsinki City Museum, which introduces visitors to Helsinki's 500-year history. The University of Helsinki also has many significant museums, including the Helsinki University Museum "Arppeanum" and the Finnish Museum of Natural History.
The Finnish National Gallery consists of three museums: Ateneum Art Museum for classical Finnish art, Sinebrychoff Art Museum for classical European art, and Kiasma Art Museum for modern art, in a building by architect Steven Holl. The old Ateneum, a neo-Renaissance palace from the 19th century, is one of the city's major historical buildings. All three museum buildings are state-owned through Senate Properties.
The Design Museum is devoted to the exhibition of both Finnish and foreign design, including industrial design, fashion, and graphic design. Other museums in Helsinki include the Military Museum of Finland, Didrichsen Art Museum, Amos Anderson Art Museum, and the Tram Museum.
Theatres.
Helsinki has three major theatres: The Finnish National Theatre, the Helsinki City Theatre, and the Swedish Theatre ("Svenska Teatern"). Other notable theatres in the city include the Alexander Theatre, "Q-teatteri", Savoy Theatre, KOM-theatre, and "Teatteri Jurkka".
Music.
Helsinki is home to two full-size symphony orchestras, the Helsinki Philharmonic Orchestra and the Finnish Radio Symphony Orchestra, both of which perform at the Helsinki Music Centre concert hall. Acclaimed contemporary composers Kaija Saariaho, Magnus Lindberg, Esa-Pekka Salonen, and Einojuhani Rautavaara, among others, were born and raised in Helsinki, and studied at the Sibelius Academy. The Finnish National Opera, the only full-time, professional opera company in Finland, is located in Helsinki. The opera singer Martti Wallén, one of the company's long-time soloists, was born and raised in Helsinki, as was mezzo-soprano Monica Groop.
Many widely renowned and acclaimed bands have originated in Helsinki, including Hanoi Rocks, HIM, Stratovarius, The 69 Eyes, Finntroll, Ensiferum, Wintersun, The Rasmus, Poets of the Fall, and Apocalyptica.
The city's main musical venues are the Finnish National Opera, the Finlandia concert hall, and the Helsinki Music Centre. The Music Centre also houses a part of the Sibelius Academy. Bigger concerts and events are usually held at one of the city's two big ice hockey arenas: the Hartwall Areena or the Helsinki Ice Hall. Helsinki has Finland's largest fairgrounds.
Helsinki Arena hosted the Eurovision Song Contest 2007, the first Eurovision Song Contest arranged in Finland, following Lordi's win in 2006.
Art.
The Helsinki Festival is an annual arts and culture festival, which takes place every August (including the Night of the Arts).
Vappu is an annual carnival for students and workers.
At the Senate Square in September / October 2010, the largest open-air art exhibition ever in Finland took place: About 1.4 million people saw the international exhibition of "United Buddy Bears".
Helsinki is the 2012 World Design Capital, in recognition of the use of design as an effective tool for social, cultural, and economic development in the city. In choosing Helsinki, the World Design Capital selection jury highlighted Helsinki's use of 'Embedded Design', which has tied design in the city to innovation, "creating global brands, such as Nokia, Kone, and Marimekko, popular events, like the annual Helsinki Design Week, outstanding education and research institutions, such as the University of Art and Design Helsinki, and exemplary architects and designers such as Eliel Saarinen and Alvar Aalto".
Helsinki also hosts many film festivals. Most of them are small venues, but some have gained renown even abroad. The most prolific would be the Love & Anarchy film festival (also known as Helsinki International Film Festival), which features films on a wide spectrum. Night Visions, on the other hand, focuses on genre cinema, screening horror, fantasy, and science fiction films in very popular movie marathons that take whole night. Another popular film festival is DocPoint, a festival that focuses solely on documentary cinema.
Media.
Today, there are around 200 newspapers, 320 popular magazines, 2,100 professional magazines, 67 commercial radio stations, three digital radio channels, and one nationwide and five national public service radio channels.
Each year, around 12,000 book titles are published and 12 million records are sold across Finland.
Sanoma publishes the newspaper "Helsingin Sanomat" (its circulation of 412,000 making it the largest), the tabloid "Ilta-Sanomat", the commerce-oriented "Taloussanomat", and the television channel Nelonen. The other major publisher Alma Media publishes over thirty magazines, including the newspaper "Aamulehti", tabloid "Iltalehti", and commerce-oriented "Kauppalehti". Worldwide, Finns, along with other Nordic peoples and the Japanese, spend the most time reading newspapers.
Yle, the Finnish Broadcasting Company, operates five television channels and thirteen radio channels in both national languages. Headquartered in the neighbourhood of Pasila, Yle is funded through a mandatory television license and fees for private broadcasters. All TV channels are broadcast digitally, both terrestrially and on cable. The commercial television channel MTV3 and commercial radio channel Radio Nova are owned by Nordic Broadcasting (Bonnier and Proventus Industrier).
, around 79% of the Finnish population uses the Internet. Finland had around 1.52 million broadband Internet connections by the end of June 2007 or around 287 per 1,000 inhabitants. All Finnish schools and public libraries have Internet connections and computers, and most residents have a mobile phone. Value-added services are rare. In October 2009, Finland's Ministry of Transport and Communications committed to ensuring that every person in Finland would be able to access the Internet at a minimum speed of one megabit-per-second beginning July 2010.
Sports.
Helsinki has a long tradition of sports: the city gained much of its initial international recognition during the 1952 Summer Olympics, and the city has arranged sporting events such as the first World Championships in Athletics 1983 and 2005, and the European Championships in Athletics 1971, 1994, and 2012. Helsinki hosts successful local teams in both of the most popular team sports in Finland: football and ice hockey. Helsinki houses HJK Helsinki, Finland's largest and most successful football club, and IFK Helsingfors, their local rivals with 7 championship titles. The fixtures between the two are commonly known as Stadin derby. Helsinki's track and field club Helsingin Kisa-Veikot is also dominant within Finland. Ice hockey is popular among many Helsinki residents, who usually support either of the local clubs IFK Helsingfors (HIFK) or Jokerit. HIFK, with 14 Finnish championships titles, also plays in the highest bandy division, along with Botnia−69. The Olympic stadium hosted the first ever Bandy World Championship in 1957.
Helsinki was elected host-city of the 1940 Summer Olympics, but due to World War II they were canceled. Instead Helsinki was the host of the 1952 Summer Olympics. The Olympics were a landmark event symbolically and economically for Helsinki and Finland as a whole that was recovering from the winter war and the continuation war fought with the Soviet Union. Helsinki was also in 1983 the first ever city to host the World Championships in Athletics. Helsinki also hosted the event in 2005, thus also becoming the first city to ever host the Championships for a second time. The Helsinki City Marathon has been held in the city every year since 1980, usually in August. A Formula 3000 race through the city streets was held on 25 May 1997. In 2009 Helsinki was host of the European Figure Skating Championships.
Transport.
Roads.
The backbone of Helsinki's motorway network consists of three semicircular ring roads, Ring I, Ring II, and Ring III, which connect expressways heading to other parts of Finland, and the western and eastern arteries of "Länsiväylä" and "Itäväylä" respectively. While variants of a "Keskustatunneli" tunnel under the city centre have been repeatedly proposed, the plan remains on the drawing board.
Helsinki has some 390 cars per 1000 inhabitants. This is less than in cities of similar population and construction density, such as Brussels' 483 per 1000, Stockholm's 401, and Oslo's 413.
Rail transport and buses.
Public transport is generally a hotly debated subject in the local politics of Helsinki. In the Helsinki metropolitan area, public transportation is managed by the Helsinki Regional Transport Authority, the metropolitan area transportation authority. The diverse public transport system consists of trams, commuter rail, the metro, bus lines, two ferry lines, and on-demand minibuses.
Today, Helsinki is the only city in Finland to have trams or a metro system. There used to be two other cities in Finland with trams: Turku and Viipuri (Vyborg, now in Russia), but both have since abandoned them. The Helsinki Metro, opened in 1982, is the only rapid transit system in Finland. In 2006, the construction of the long debated extension of the system west into Espoo was approved, and serious debate about an eastern extension into Sipoo has taken place.
Helsinki's metro system currently consists of 17 stops, with the six most western stops residing underground. Once expansion into Espoo occurs, scheduled to be operational by late 2015, seven additional metro stations will be opened. Two of these seven stations will transit under waterways.
The possibility of a Helsinki to Tallinn Tunnel is currently being researched. The rail tunnel would connect Helsinki to the Estonian capital Tallinn, further linking Helsinki to the rest of continental Europe by Rail Baltica.
Aviation.
Air traffic is handled primarily from the international Helsinki Airport, located approximately north of Helsinki's downtown area, in the neighbouring city of Vantaa. Helsinki's own airport, Helsinki-Malmi Airport, is mainly used for general and private aviation. Helicopter flights to Tallinn are available from Hernesaari Heliport.
Sea transport.
Like many other cities of the world, Helsinki had been deliberately founded next to the sea. The city therefore was able to benefit from good sea transportation links right from the start. The freezing of the sea imposed limitations on sea traffic up to the end of the 19th century. But for the last hundred years, the routes leading to Helsinki have been kept open even in winter with the aid of icebreakers, many of them built in the Helsinki Hietalahti shipyard. The arrival and departure of ships has also been a part of everyday life in Helsinki. Regular route traffic from Helsinki to Stockholm, Tallinn, and St. Petersburg began as far back as 1837. Over 300 cruise ships and 360,000 cruise passengers visit Helsinki annually. There are international cruise ship docks in South Harbour, Katajanokka, West Harbour, and Hernesaari. Helsinki is the second busiest passenger port in Europe with approximately 11 million passengers in 2013. Ferry connections to Tallinn, Mariehamn, and Stockholm are serviced by various companies. Finnlines passenger-freight ferries to Gdynia, Poland; Travemünde, Germany; and Rostock, Germany are also available. St Peter Line offers passenger ferry service to Saint Petersburg several times a week.
International relations.
Special partnership cities.
Helsinki has a special partnership relation with:

</doc>
