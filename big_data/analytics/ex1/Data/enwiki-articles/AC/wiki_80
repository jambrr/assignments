<doc id="17318" url="https://en.wikipedia.org/wiki?curid=17318" title="Konstantin Stanislavsky">
Konstantin Stanislavsky

Konstantin Sergeyevich Stanislavsky ("né" Alexeyev; ; also transliterated as Constantin Stanislavski; 7 August 1938) was a Russian actor and theatre director. The Stanislavsky system would inspire numerous acting teachers in America whose teachings became a dominant force in film acting, especially in the period after World War II.
Stanislavsky treated theatre-making as a serious endeavour requiring dedication, discipline and integrity. Throughout his life, he subjected his own acting to a process of rigorous artistic self-analysis and reflection. His development of a theorized praxis—in which practice is used as a mode of inquiry and theory as a catalyst for creative development—identifies him as one of the great modern theatre practitioners.
Stanislavsky's work was as important to the development of socialist realism in the Soviet Union as it was to that of psychological realism in the United States. It draws on a wide range of influences and ideas, including his study of the modernist and avant-garde developments of his time (naturalism, symbolism and Meyerhold's constructivism), Russian formalism, Yoga, Pavlovian behavioural psychology, James-Lange (via Ribot) psychophysiology and the aesthetics of Pushkin, Gogol, and Tolstoy. He described his approach as 'spiritual Realism'. Stanislavsky wrote several works, including "An Actor Prepares", "An Actor's Work on a Role", and his autobiography, "My Life in Art".
Biography.
Family background.
Stanislavsky grew up in one of the richest families in Russia, the Alexeyevs. He was born Konstantin Sergeyevich Alexeyev—"Stanislavsky" was a stage name that he adopted in 1884 in order to keep his performance activities secret from his parents. The prospect of becoming a professional actor was taboo for someone of his social class; actors had an even lower social status in Russia than in the rest of Europe, having only recently been serfs and the property of the nobility. The Alexeyevs were a prosperous, bourgeois family, whose factories manufactured gold and silver braiding for military decorations and uniforms. Until the Russian revolution in 1917, Stanislavsky often used his inherited wealth to fund his theatrical experiments in acting and directing. His family's discouragement meant that he appeared only as an amateur onstage and as a director until he was thirty-three.
As a child, Stanislavsky was exposed to the rich cultural life of his family. His interests included the circus, the ballet, and puppetry. In 1877, his father, Sergei Vladimirovich Alexeyev, was elected head of the merchant class in Moscow (one of the most important and influential positions in the city); that year, he had a fully equipped theatre built on his estate at Lyubimovka, providing a forum for Stanislavsky's adolescent theatrical impulses. After his debut performance there, Stanislavsky started what would become a lifelong series of notebooks filled with critical observations on his acting, aphorisms, and problems. It was from this habit of self-analysis and critique that Stanislavski's system later emerged. The family's second theatre was added in 1881 to their mansion at Red Gates, on Sadovaya Street in Moscow (where Stanislavsky lived from 1863 to 1903); their house became a focus for the artistic and cultural life of the city. Stanislavsky chose not to attend university, preferring to work in the family business.
Early influences.
Increasingly interested in "living the part," Stanislavsky experimented with the ability to maintain a characterization in real life, disguising himself as a tramp or drunk and visiting the railway station, or disguising himself as a fortune-telling gypsy; he extended the experiment to the rest of the cast of a short comedy in which he performed in 1883, and as late as 1900 he amused holiday-makers in Yalta by taking a walk each morning "in character". In 1884, he began vocal training under Fyodor Petrovich Komissarzhevsky, a professor at the Moscow Conservatory and leading tenor of the Bolshoi (and father of the famous actress Vera Komissarzhevskaya), with whom he also explored the coordination of voice and body. Together they devised exercises in moving and sitting stationary "rhythmically", which anticipated Stanislavsky's later use of physical rhythm when teaching his 'system' to opera singers. Komissarzhevsky provided one of the models (the other was Stanislavsky himself) for the character of Tortsov in his actor's manual "An Actor's Work" (1938). A year later, in 1885, Stanislavsky briefly studied at the Moscow Theatre School, where students were encouraged to mimic the theatrical tricks and conventions of their tutors. Disappointed by this approach, he left after little more than two weeks.
Instead, Stanislavsky devoted particular attention to the performances of the Maly Theatre, the home of psychological realism in Russia. Psychological realism had been developed here by Alexander Pushkin, Nikolai Gogol and Mikhail Shchepkin. In 1823, Pushkin had concluded that what united the diverse classical authors—Shakespeare, Racine, Corneille and Calderón—was their common concern for truth of character and situation, understood as credible behaviour in believable circumstances:
Gogol, meanwhile, campaigned against overblown, effect-seeking acting. In an article of 1846, he advises a modest, dignified mode of comic performance in which the actor seeks to grasp "what is dominant in the role" and considers "the character's main concern, which consumes his life, the constant object of his thought, the 'bee in his bonnet.'" This inner desire forms the "heart of the role," to which the "tiny quirks and tiny external details" are added as embellishment. The Maly soon became known as the House of Shchepkin, the father of Russian realistic acting who, in 1848, promoted the idea of an "actor of feeling." This actor would "become the character" and identify with his thoughts and feelings: he would "walk, talk, think, feel, cry, laugh as the author wants him to." A copy of Shchepkin's "Memoirs of a Serf-Actor", in which the actor describes his struggle to achieve a naturalness of style, was heavily annotated by Stanislavsky. Shchepkin's student, Glikeriya Fedotova, was Stanislavsky's teacher (she was responsible for instilling the rejection of inspiration as the basis of the actor's art, along with the stress on the importance of training and discipline, and the practice of responsive interaction with other actors that Stanislavsky came to call "communication"). Shchepkin's legacy included the emphasis on a disciplined, ensemble approach, the importance of extensive rehearsals, and the use of careful observation, self-knowledge, imagination and emotion as the cornerstones of the craft.
As well as the artists of the Maly company, performances given by foreign star actors—who would often come to Moscow during Lent (when Russian actors were prohibited from appearing)—also influenced Stanislavsky. The effortless, emotive and clear playing of the Italian actor Ernesto Rossi, who performed major Shakespearean tragic protagonists in Moscow in 1877, particularly impressed Stanislavsky. So too did Tommaso Salvini's 1882 performance of Othello. Years later, Stanislavsky wrote that Salvini was the "finest representative" of the "art of experiencing" approach to acting.
The Society of Art and Literature.
By the age of 25, Stanislavsky was well known as an amateur actor. He made a proposal to Fyodor Sollogub and Alexander Fedotov (a theatre director and estranged husband of Glikeriya Fedotova) to establish a society that would unite amateur and professional actors and artists. The profits from his family's factory were particularly high in 1887–1888; Stanislavsky decided to use the surplus 25,000–30,000 roubles to form the Society of Art and Literature, for which he had the Ginzburg House on Tverskaya Street converted into a luxurious clubhouse with its own large stage and exhibition rooms. Fedotov became head of the dramatic section, Komissarzhevsky was the head of the operatic and musical section, while Sollogub was appointed head of the graphic arts section; the drama and opera sections each had a school. To research the curriculum of the society's drama school, Stanislavsky spent the summer of 1888 studying the classes and performances of the Comédie-Française in Paris.
The society's school was to offer classes in dramatic art, the history of costume, make-up, drama, Russian literature, aesthetics, fencing and dancing. The school opened on 8 October 1888 while the society itself was officially inaugurated on 3 November with a ceremony attended by Anton Chekhov. Under the auspices of the society, Stanislavsky performed in plays by Molière, Schiller, Pushkin, and Ostrovsky, as well as gaining his first experiences as a director. With the guidance of Fedotov and Sollogub, Stanislavsky finally abandoned the operatic conventions and theatrical clichés in his acting that he had mimicked from other actors' performances. He also became interested in the aesthetic theories of Vissarion Belinsky. From Belinsky he took his conception of the role of the artist, on which he based a moral justification for his desire to perform that accorded with his family's sense of social responsibility and ethics. At this time Stanislavsky warned in his diary:
On 5 July 1889, Stanislavsky married Lilina (the stage name of Maria Petrovna Perevoschikova), with whom he had just performed in "Intrigue and Love". Their first child, Ksenia, died of pneumonia in May 1890 less than two months after she was born. Their second daughter, Kira, was born on 21 July 1891. In January 1893, Stanislavsky's father died. Their son Igor was born 14 September 1894.
In 1889 in the society's production of Aleksey Pisemsky's historical play "Men Above The Law," Stanislavsky discovered his "principle of opposites," as expressed in his aphoristic advice to the actor: "When you play a good man, try to find out where he is bad, and when you play a villain, try to find where he is good." Stanislavsky insisted that the actors learnt their parts thoroughly, almost entirely removing the prompter from the society's productions.
Stanislavsky described his production of Leo Tolstoy's "The Fruits of Enlightenment" in February 1891 as his first fully independent directorial work. His directorial methods at this time were closely modeled on the disciplined, autocratic approach of Ludwig Chronegk, the director of the Meiningen Ensemble, whose productions of "Julius Caesar", "The Merchant of Venice" and "Twelfth Night", as well as a number of plays by Schiller, Stanislavsky had studied enthusiastically during their second visit to Moscow in 1890. The Ensemble's general approach included historical accuracy in set, props and costumes and complex crowd effects achieved through a tightly drilled rehearsal process. Its use of off-stage sound to produce the illusion of a reality beyond the visible stage particularly impressed Stanislavsky. Their productions demonstrated a model for artistic achievement with relatively unskilled actors that Stanislavsky was to adopt for the early part of his career as a director. By means of a rigid and detailed control of the "mise-en-scène", including the strict choreography of the actors' every gesture, in Stanislavsky's words "the inner kernel of the play was revealed by itself." Whereas the Ensemble's effects tended toward the grandiose, however, Stanislavsky introduced lyrical elaborations through the "mise-en-scène" that dramatised more mundane and ordinary elements of life, in keeping with Belinsky's ideas about the "poetry of the real":
Writing years later in his autobiography "My Life in Art" (1925), Stanislavsky described Chronegk's approach as one in which the director is "forced to work without the help of the actor." Jean Benedetti suggests that Stanislavsky's task at this stage was to unite the realistic tradition of the creative actor inherited from Shchepkin and Gogol with the director-centered, organically unified naturalistic aesthetic of the Ensemble's approach.
It was at this time that Stanislavsky first met Leo Tolstoy. Tolstoy re-wrote the fourth act of his "The Power of Darkness" along the lines of Stanislavsky's suggestions in 1896. Tolstoy was another important influence on the development of Stanislavsky's thought; his "What Is Art?" (1898) promoted immediate intelligibility and transparency as an aesthetic principle. On the eve of creating the Moscow Art Theatre, Stanislavsky wrote of the importance of simplicity, directness and accessibility in art.
From 1894 onwards, as part of his painstaking rehearsals for Karl Gutzkow's melodrama "Uriel Acosta" and Shakespeare's "Othello", Stanislavsky began to assemble detailed prompt-books that included a directorial commentary on the entire play and from which not even the smallest detail was allowed to deviate in rehearsals. Stanislavsky's "Othello" (1896) made a strong impression on the 22-year-old Vsevolod Meyerhold, who was later to work with him before becoming an important director and theatre practitioner in his own right. "The task of our generation," Stanislavsky wrote at this time, is "to liberate art from outmoded tradition, from tired cliché and to give greater freedom to imagination and creative ability."
The Moscow Art Theatre.
In 1896 Stanislavsky discussed with Nikolai Efros his ideas for a scheme to establish a network of touring theatre companies that would bring high-quality drama to the surrounding area of selected towns. He proposed to call them "open" or "accessible" theatres, in a bid to avoid alarming the authorities with their connection to the dangerously democratizing "popular theatre" movement that was spreading across Europe, spearheaded by Romain Rolland. In February 1897 Stanislavsky joined Anton Chekhov, whom he had met on 15 February at a literary-musical evening, in an open public discussion on the creation of a popular theatre that was reported in the press. At this time he also helped to organise the first all-Russian conference on the theatre, whose keynote speaker, Yevtikhiy Karpov, urged the creation of a "Russian people's theatre."
It was Stanislavsky's historic meeting with Vladimir Nemirovich-Danchenko on 22 June 1897, however, that would create what was called initially the "Moscow Public-Accessible Theatre" but which came to be known as the Moscow Art Theatre (MAT). Their 18-hour discussion—lasting from lunch at 2pm in a private room in the Slavic Bazaar restaurant to 8am the following morning over breakfast at Stanislavsky's family estate at Lyubimovka—has acquired a legendary status in the history of theatre. Nemirovich was a successful playwright (whose work was performed by the Maly and whose play "The Worth of Life" had beaten Chekhov's "The Seagull" to win the Griboyedov prize, much to the author's dismay), critic, theatre director and acting teacher at the Philharmonic school (where he taught Vsevolod Meyerhold and Olga Knipper), who was also committed to the idea of a popular theatre. Their abilities complemented one another: Nemirovich needed Stanislavsky's directorial talent for creating vivid stage images and selecting significant details, while Stanislavsky needed Nemirovich's talent for dramatic and literary analysis, his professional expertise and his ability to manage a theatre. Stanislavsky later compared their discussions to the Treaty of Versailles, their scope was so wide-ranging; they agreed on the conventional practices they wished to abandon and, on the basis of the working method they found they had in common, they worked out the policy of their new theatre. Together they would forge a professional company with an ensemble ethos that discouraged individual vanity, selecting actors from Nemirovich's class at the Philharmonic school and Stanislavsky's amateur Society of Art and Literature group, along with other professional actors; they would create a realistic theatre of international renown, with popular prices for seats, whose organically unified aesthetic would bring together the techniques of the Meiningen Ensemble and those of André Antoine's Théâtre Libre (which Stanislavsky had seen during trips to Paris). Responsibility was to be shared between them on the basis of their individual strengths, with Stanislavsky overseeing production and Nemirovich in charge of the repertoire and literary decisions; each had a veto.
Given that Stanislavsky's family's assets amounted to some 8 million roubles at the time, Nemirovich assumed initially that Stanislavsky would fund the theatre as a privately owned business, but Stanislavsky insisted on a limited, joint stock company. Stanislavsky would only ever invest an initial 10,000 roubles in the MAT. To raise the rest of the theatre's 28,000 roubles launch capital, Nemirovich persuaded some of the directors of the Philharmonic Society to contribute, members of the board of the Society of Art and Literature also invested, but the theatre's principal shareholder was to be Savva Timofeyevich Morozov, who invested 10,000 roubles. The company had 13 shareholders, who signed an agreement on 10 April 1898. With an annual salary of 4,200 roubles each, Stanislavsky and Nemirovich were to represent the interests of the acting company in the business, though with the aim of transferring control to the actors eventually. The company consisted of 39 actors, 23 men and 16 women, 30% of whom came from Nemirovich's Philharmonic class and 35% of whom came with Stanislavsky from the Society of Art and Literature, with a total staff numbering 323. Viktor Simov, whom Stanislavsky had met in 1896, was engaged as the company's principal designer.
For want of suitable rehearsal space in Moscow, the company met in Pushkino, isolated 50 miles from the city. In his opening speech on the first day of rehearsals, 14 June 1898, Stanislavsky stressed the "social character" of their collective undertaking: "We are striving to create the first rational, moral, and public-accessible theatre," he said, "and we dedicate our lives to this high goal." In an atmosphere more like a university than a theatre, as Stanislavsky described it, the company was introduced to his working method of extensive reading and research and detailed rehearsals in which the action was defined at the table before being explored physically. Throughout June and July the company rehearsed productions of Shakespeare's "The Merchant of Venice", Sophocles' "Antigone", Hauptmann's "Hannele", Pisemsky's "Men Above The Law", Lenz's "The Tutor" and Alexei Tolstoy's "Tsar Fyodor Ioannovich". It was at these rehearsals that Stanislavsky's lifelong relationship with Vsevolod Meyerhold began; by the end of June, Meyerhold was so impressed with Stanislavsky's directorial skills that he declared him a genius. On his death-bed or Stanislavsky was to declare Meyerhold "my sole heir in the theatre—here or anywhere else."
In 1898, Stanislavsky co-directed with Nemirovich the first of his productions of the work of Anton Chekhov. The MAT production of "The Seagull" was a crucial milestone for the fledgling company that has been described as "one of the greatest events in the history of Russian theatre and one of the greatest new developments in the history of world drama." Despite its 80 hours of rehearsal—a considerable length by the standards of the conventional practice of the day—Stanislavsky felt it was under-rehearsed and threatened to have his name removed from the posters when Nemirovich refused his demand to postpone its opening by a week. Stanislavsky played Trigorin, Meyerhold played Konstantin, and Olga Knipper played Arkadnia. The production's success was due to the fidelity of its delicate representation of everyday life, its intimate, ensemble playing, and the resonance of its mood of despondent uncertainty with the psychological disposition of the Russian intelligentsia of the time. To commemorate this historic production, which gave the MAT its sense of identity, the company to this day bears the seagull as its emblem. Stanislavsky went on to direct the successful premières of Chekhov's other major plays: "Uncle Vanya" in 1899, "Three Sisters" in 1901, and "The Cherry Orchard" in 1904.
His encounter with Chekhov's drama proved crucial to the creative development of both men. His ensemble approach and attention to the psychological realities of its characters revived Chekhov's interest in writing for the stage, while Chekhov's unwillingness to explain or expand on the text forced Stanislavsky to dig beneath its surface in ways that were new in theatre. By 1922, however, Stanislavsky had become disenchanted with the MAT's productions of Chekhov's plays—"After all we have lived through," he remarked to Nemirovich, "it is impossible to weep over the fact that an officer is going and leaving his lady behind" (referring to the conclusion of "Three Sisters").
Stanislavsky's system.
Stanislavsky's "system" is a systematic approach to training actors. Areas of study include concentration, voice, physical skills, emotion memory, observation, and dramatic analysis. Stanislavsky's goal was to find a universally applicable approach that could be of service to all actors. Yet he said of his system: "Create your own method. Don't depend slavishly on mine. Make up something that will work for you! But keep breaking traditions, I beg you."
His system is often erroneously identified with Lee Strasberg's Method approach, which claimed inspiration from Stanislavsky's approach. However, Strasberg's adaptation relied exclusively on psychological techniques and contrasted sharply with Stanislavsky's multivariate, holistic and psychophysical approach, which explores character and action both from the "inside out" and the "outside in." In this respect his system is far more similar to other classical acting techniques than to method acting.
Emotion memory.
Stanislavsky's system focused on the development of artistic truth onstage by teaching actors to "experience the part" during performance. Stanislavsky hoped that the 'system' could be applied to all forms of drama, including melodrama, vaudeville, and opera. He organised a series of theatre studios in which young actors were trained in his 'system.' At the First Studio, actors were instructed to use their own memories in order to express emotion.
Stanislavsky soon observed that some of the actors using or abusing this technique were given to hysteria. He began to search for more reliable means to access emotion, eventually emphasizing the actor's use of imagination and belief in the given circumstances of the text rather than her/his private and often painful memories.
Legacy.
Stanislavsky had different pupils during each of the phases of discovering and experimenting with his 'system' of acting. Two of his former students, Richard Boleslavsky and Maria Ouspenskaya, founded the American Laboratory Theatre in 1925. One of their students, Lee Strasberg, went on to co-found the Group Theatre (1931–1940) with Harold Clurman and Cheryl Crawford, which was the first American acting company to put Stanislavsky's initial discoveries into practice. Clurman and Strasberg had a profound influence on American acting, both on stage and film, as did Stella Adler, who was also part of the Group Theatre and who had studied briefly with Stanislavsky and quarreled with Strasberg's approach to the work. Sanford Meisner, another Group member, joined with Adler in opposing Strasberg's approach. This conflict was the partial cause of the Group Theatre's dissolution. After the Group broke up, Strasberg, Adler and Meisner each went on to found their own acting studios which trained many of the most prominent actors in American theater and film.
Lord Olivier wrote that Stanislavsky's "My Life in Art" was a source of great enlightenment" when he was a young actor.
Sir John Gielgud said, "This director found time to explain a thousand things that have always troubled actors and fascinated students." Gielgud is also quoted as saying, "Stanislavski's now famous book is a contribution to the Theatre and its students all over the world."
Fictional reference.
Mikhail Bulgakov, best known for "Master and Margarita", wrote another novel, "Black Snow", that portrays Stanislavsky in a satirical roman à clef manner, as “Ivan Vasilievich”, a theatre director who loves to eat jam, who leads his cast in wild acting exercises, and who mutilates Vasilievich’s play (which began as a very bad novel). At the fictional theatre, the two co-directors have been feuding, there are clashing egos, and intrigues. Then there is the famous acting method. The novel contains caricatures of the Moscow Art Theatre, Stanislavsky, Nemirovich-Danchenko, and others. The primary target of the satire is censorship, which Bulgakov had to deal with. The novel, written in the 1930s, reflects Bulgakov’s actual experiences with The Moscow Art Theatre, which included the eventually aborted production of his play "A Cabal of Hypocrites".

</doc>
<doc id="17319" url="https://en.wikipedia.org/wiki?curid=17319" title="K cell">
K cell

K cell may refer to:

</doc>
<doc id="17320" url="https://en.wikipedia.org/wiki?curid=17320" title="Khartoum">
Khartoum

Khartoum ( ) is the capital and second largest city of Sudan and Khartoum state. It is located at the confluence of the White Nile, flowing north from Lake Victoria, and the Blue Nile, flowing west from Ethiopia. The location where the two Niles meet is known as ""al-Mogran"" , meaning the confluence. The main Nile continues to flow north towards Egypt and the Mediterranean Sea.
Divided by the Niles, Khartoum is a tripartite metropolis with an estimated overall population of over five million people, consisting of Khartoum proper, and linked by bridges to Khartoum North ( ') and Omdurman ( ') to the west.
Greater Khartoum.
Need to distinguish between Khartoum national capital or triangular capital Metropolitan area as they are sometimes called, and Khartoum state, and between them and Khartoum the City, or Khartoum general as it is called. The capital of Sudan or the triangular capital consists of the three towns of Khartoum, Omdurman and Khartoum North or Bahri. It is located at the confluence of the White and Blue Niles. 
History.
Etymology.
The origin of the word Khartoum is uncertain. One theory argues that "khartoum" is derived from Arabic "kharṭūm" meaning 'trunk' or 'hose', probably referring to the narrow strip of land extending between the Blue and White Niles. Captain J. A. Grant, who reached Khartoum in 1863 with Captain Speke's expedition, thought that the name was most probably from the Arabic "qurtum", safflower ("Carthamus tinctorius"), which was cultivated extensively in Egypt for its oil to be used as fuel. Some scholars speculate that the word derives from "Agartum" in the Nubian language, which stands for "the abode of Atum"; the Nubian and Egyptian god of creation. Other Beja scholars suggest Khartoum is derived from "Hartooma" which means meeting in the Beja Language.
Founding (1821–1899).
Khartoum was established north of the ancient city of Soba in 1821 by Ibrahim Pasha, the son of Egypt's ruler, Muhammad Ali Pasha, who had just incorporated Sudan into his realm. Originally, Khartoum served as an outpost for the Egyptian Army, but the settlement quickly grew into a regional centre of trade. It also became a focal point for the slave trade. Later, it became the administrative center of Sudan and official capital.
Troops loyal to the Mahdi Muhammad Ahmad started a siege of Khartoum on 13 March 1884, against defenders led by British General Charles George Gordon. The siege ended in a massacre of the Anglo-Egyptian garrison. The heavily damaged city fell to the Mahdists on 26 January 1885 and all its inhabitants were put to death.
Omdurman was the scene of the bloody Battle of Omdurman on 2 September 1898, during which British forces under Herbert Kitchener defeated the Mahdist forces defending the city.
Modern history (20th–21st centuries).
In 1973, the city was the site of an anomalous hostage crisis in which members of Black September held ten hostages at the Saudi Arabian embassy, five of whom were diplomats. The US ambassador, the US deputy ambassador, and the Belgian chargé d'affaires were murdered. The remaining hostages were released. A 1973 United States Department of State document, declassified in 2006, concluded "The Khartoum operation was planned and carried out with the full knowledge and personal approval of Yasser Arafat."
The first oil pipeline between Khartoum and the Port of Sudan was completed in 1977.
Throughout the 1970s and 1980s, Khartoum was the destination for hundreds of thousands of refugees fleeing conflicts in neighboring nations such as Chad, Eritrea, Ethiopia and Uganda. Many Eritrean and Ethiopian refugees assimilated into society, while others settled in large slums at the outskirts of the city. Since the mid-1980s, large numbers of refugees from South Sudan and Darfur fleeing the violence of the Second Sudanese Civil War and Darfur conflict have settled around Khartoum.
In 1991, Osama bin Laden purchased a house in the affluent al-Riyadh neighborhood of the city and another in Soba. He lived there until 1996 when he was banished from the country. Following the 1998 U.S. embassy bombings, the United States accused bin Laden's al-Qaeda group and launched cruise missile attacks (20 August) on the al-Shifa pharmaceutical factory in northern Khartoum. The destruction of the factory produced diplomatic tension between the U.S. and Sudan. The ruins of the factory are now a tourist attraction.
After the sudden death of SPLA head and vice-president of Sudan, John Garang, at the end of July 2005, there were three days of violent riots in the capital. The riots finally died down after Southern Sudanese politicians and tribal leaders sent strong messages to the rioters. The situation could have been much more dire; even so, the death toll was at least 24, as youths from southern Sudan attacked northern Sudanese and clashed with security forces.
The Organisation of African Unity summit of 18–22 July 1978 was held in Khartoum, during which Sudan was awarded the OAU presidency.
The African Union summit of 16–24 January 2006 was held in Khartoum.
The Arab League summit of 28–29 March 2006 was held in Khartoum, during which the Arab League awarded Sudan the Arab League presidency.
On 10 May 2008 the Darfur rebel group of the Justice and Equality Movement moved into the city where they engaged in heavy fighting with Sudanese government forces. Their soldiers included minors and their goal was the toppling of Omar al-Bashir's government, though the Sudanese government succeeded in beating back the assault.
On 23 October 2012 an explosion at the Yarmouk munitions factory killed two people and injured another person. The Sudanese government has claimed that the explosion was the result of an Israeli airstrike.
Geography.
Location.
Sudan, in northeast Africa, measures about one-fourth the size of the United States. Its neighbors are Chad and the Central African Republic on the west, Egypt and Libya on the north, Ethiopia and Eritrea on the east, and South Sudan, Kenya, Uganda, and Democratic Republic of the Congo on the south. The Red Sea washes about 500 mi of the eastern coast. It is traversed from north to south by the Nile, all of whose great tributaries are partly or entirely within its borders.
Khartoum is located in the middle of the populated areas in Sudan almost northeast center of the country between 16 degrees latitude north and 15 degrees latitude south and longitude 21 degrees west and 24 degrees longitude east, and expands an area amounting to 20,736 km (12884 Mile) square between the West Bank of the River Nile, from North Khartoum Bahri, Shendi, River Nile State, from both the East Kassala, Kassala State, Port Sudan, Red Sea State and North East Blue Nile, from the West White Nile, Omdurman, North Kordofan and Northwest Omdurman, Northern State, from South Wad Madani, Al Jazirah (state) and Southwest Ed Dueim, White Nile State.
Climate.
Khartoum features a hot desert climate (Köppen climate classification "BWhw") with a dry season occurring during "wintertime", typical of the Saharo-Sahelian zone which marks the progressive passage between the Sahara Desert, vast arid areas and the Sahel, vast semi-arid areas. The climate is extremely arid for most of the year with about nine months where average rainfall is lower than . The very long dry season is itself divided into a hot, very dry season between November and March as well as a very hot, dry season between April and June. During this part of the year, hot, dry continental trade winds from deserts sweep over the region such as the harmattan (a northerly or northeasterly wind): the sky is perfectly clear, the weather is stable, very dry and the rainfall inhibition is total. The very irregular, very brief, rainy season lasts about 1 month as the maximum rainfall is recorded in August with about . The rainy season is characterized by a seasonal reverse of wind regimes, when the Intertropical Convergence Zone goes northerly. Average annual rainfall is very low, with only of precipitation. Khartoum records on average six days with or more and 19 days with or more of rainfall. The highest temperatures occur during two periods in the year: the first at the late dry season, when average high temperatures consistently exceed from April to June, and the second at the early dry season, when average high temperatures exceed in September and October months. Khartoum is one of the hottest major cities on Earth, with annual mean temperatures hovering around . The city has also truly hot "winters" for a such dry climate. In no month does the average monthly high temperature fall below . This is something not seen in other major cities with hot desert climates such as Riyadh, Baghdad and Phoenix. Temperatures cool off enough during the night, with Khartoum's lowest average low temperature of the year just above . Khartoum is also one of the sunniest major cities in the world, with an annual sunshine duration around 3,700 hours.
Economy.
After the signing of the historic Comprehensive Peace Agreement between the government of Sudan and the Sudan People's Liberation Movement (SPLA), the Government of Sudan has begun a massive development project. In 2007, the biggest projects in Khartoum were the Al-Mogran Development Project, two five-star hotels, a new airport, Mac Nimir Bridge (finished in October 2007) and the Tuti Bridge that links Khartoum to Tuti Island.
In the 21st century, Khartoum has developed based on Sudan's oil wealth. The center of the city has tree-lined streets. Khartoum has the highest concentration of economic activity in the country. This has changed as major economic developments take place in other parts of the country, like oil exploration in the South, the Giad Industrial Complex in Al Jazirah state and White Nile Sugar Project in Central Sudan, and the Merowe Dam in the North.
Among the city's industries are printing, glass manufacturing, food processing, and textiles. Petroleum products are now produced in the far north of Khartoum state, providing fuel and jobs for the city. One of Sudan's largest refineries is located in northern Khartoum.
Retailing.
The Souq Al Arabi is Khartoum's largest open air market. The "souq" is spread over several blocks in the center of Khartoum proper just south of the Great Mosque (Mesjid al-Kabir) and the minibus station. It is divided into separate sections, including one focused entirely on gold.
Al Qasr Street and Al Jamhoriyah Street are considered the most famous high streets in Khartoum State.
Afra Mall is located in the southern suburb Arkeweet. The Afra Mall has a supermarket, retail outlets, coffee shops, a bowling alley, movie theaters, and a children's playground.
In 2011, Sudan opened the Hotel Section and part of the food court of the new, Cornithia hotel Tower. The Mall/Shopping section is still under construction.
Education.
Khartoum is the main location for most of Sudan's top educational bodies.In Khartoum, the capital of Sudan, there are four main levels of education.
First: kindergarten and day-care. It begins in the age of 3-4, consists of 1-2 grades, (depending on the parents).
Second: elementary school. the first grade pupils enter at the age of 6-7 .and It consists of 8 grades, each year there is more academic efforts and main subjects added plus more school methods improvements. By the 8th grade a student is 13–14 years old ready to take the certificate exams and entering high school.
Third: upper second school and high school. At this level the school methods add some main academic subjects such as chemistry, biology, physics, geography, etc...
there are three grades in this level. The students ages are about 14-15 to 17-18.
Higher Education: there are many universities in Sudan such as the university of Khartoum, even foreigners attend universities here, because the reputation of the universities are very good and the living expenses are low compared to other countries.
After all, the education system in Sudan went through many changes in the late 1980s and early 1990s.
Transportation.
Air.
Khartoum is home to the largest airport in Sudan, Khartoum International Airport. It is the main hub for Sudan Airways, Sudan's main carrier. The airport was built at the southern edge of the city; but with Khartoum's rapid growth and consequent urban sprawl, the airport is now located in the heart of the city.
A new international airport is currently being built about south of the city center.There have been delays to start construction because lack of funding of the project but it's known that the airport will be completed sometime in 2017. It will replace the current airport in Khartoum as Sudan's main airport.
Rail.
Khartoum has rail lines from Wadi Halfa, Port Sudan on the Red Sea, and El Obeid. All are operated by Sudan Railways. Some lines also extended to some parts of south Sudan
Architecture.
Architecture of Khartoum can’t be identified by one style or even two styles its as diverse as it’s culture, where 597 different cultural groups meet. In this article I chose 10 buildings of Khartoum to showcase this diversity in buildings’ shapes, materials, treatments. Sudan was home to numerous ancient civilizations, such as the Kingdom of Kush, Kerma, Nobatia, Alodia, Makuria, Meroë and others, most of which flourished along the Nile. During the pre-dynastic period Nubia and Nagadan Upper Egypt were identical, simultaneously evolved systems of Pharaonic kingship by 3300 BC.
In response to the worldwide deterioration of the environment and the increase in pollution levels, there has been a strong movement towards sustainable architecture across the globe. This movement has received attention and concern from governments as well as private sectors. In the past decades, Sudan has seen a huge surge in infrastructure and technology, which has led to many new and innovative building concepts, ideas and construction techniques. There is now a constant flow of new projects arising, thus leading to a new, transformed, modernised form of architecture.
Masjids and Places of worship
Culture.
Museums.
The largest museum in all of Sudan is the National Museum of Sudan. Founded in 1971, it contains works from different epochs of Sudanese history. Among the exhibits are two Egyptian temples of Buhen and Semna which were originally built by Queen Hatshepsut and Pharaoh Tuthmosis III respectively but relocated to Khartoum upon the flooding of Lake Nasser.
The Republican Palace Museum opened in 2000 is located in the former Anglican All Saints' cathedral on Sharia al-Jama'a, next to the historical Presidential Palace.
The Ethnographic Museum is located on Sharia al-Jama'a, close to the Mac Nimir Bridge.
Botanical gardens.
Khartoum is home to a small botanical garden, in the Mogran district of the city.
Clubs.
Khartoum is home to several clubs such as the Blue Nile Sailing Club, the German Club, the Greek Hotel, the Coptic Club, the Syrian Club and the International Club.
There is also a football club situated in Khartoum – Al Khartoum

</doc>
<doc id="17322" url="https://en.wikipedia.org/wiki?curid=17322" title="Alpha-Ketoglutaric acid">
Alpha-Ketoglutaric acid

α-Ketoglutaric acid is one of two ketone derivatives of glutaric acid. (The term "ketoglutaric acid," when not further qualified, almost always refers to the alpha variant. β-Ketoglutaric acid varies only by the position of the ketone functional group, and is much less common.)
Its anion, α-ketoglutarate (α-KG, also called oxo-glutarate) is an important biological compound. It is the keto acid produced by deamination of glutamate, and is an intermediate in the Krebs cycle.
Functions.
Krebs cycle.
α-Ketoglutarate is a key intermediate in the Krebs cycle, coming after isocitrate and before succinyl CoA. Anaplerotic reactions can replenish the cycle at this juncture by synthesizing α-ketoglutarate from transamination of glutamate, or through action of glutamate dehydrogenase on glutamate.
Formation of amino acids.
Glutamine is synthesized from glutamate by glutamine synthetase, which utilizes an ATP to form glutamyl phosphate; this intermediate is attacked by ammonia as a nucleophile giving glutamine and inorganic phosphate.
Nitrogen transporter.
Another function is to combine with nitrogen released in the cell, therefore preventing nitrogen overload.
α-Ketoglutarate is one of the most important nitrogen transporters in metabolic pathways. The amino groups of amino acids are attached to it (by transamination) and carried to the liver where the urea cycle takes place.
α-Ketoglutarate is transaminated, along with glutamine, to form the excitatory neurotransmitter glutamate. Glutamate can then be decarboxylated (requiring vitamin B6) into the inhibitory neurotransmitter GABA.
It is reported that high ammonia and/or high nitrogen levels may occur with high protein intake, excessive aluminum exposure, Reye's syndrome, cirrhosis, and urea cycle disorder.
It plays a role in detoxification of ammonia in brain.
Relationship to molecular oxygen.
Acting as a co-substrate, it also plays important function in oxidation reactions involving molecular oxygen.
Molecular oxygen (O2) directly oxidizes many compounds to produce useful products in an organism, such as antibiotics, etc., in reactions catalyzed by oxygenases. In many oxygenases, α-ketoglutarate helps the reaction by being oxidized together with the main substrate. In fact, one of the α-ketoglutarate-dependent oxygenases is an O2 sensor, informing the organism the oxygen level in its environment.
In combination with molecular oxygen, alpha-ketoglutarate is one of the requirements for the hydroxylation of proline to hydroxyproline in the production of Type 1 Collagen.
Antioxidant.
α-Ketoglutarate, which is known to be released by several cell types, decreased the levels of hydrogen peroxide, and the α-ketoglutarate was depleted and converted to succinate in cell culture media.
Longevity.
A study released on May 14, 2014 links α-ketoglutarate with significantly increased lifespan in nematode worms.
Immune Regulation.
A recent study has shown that α-ketoglutarate promotes TH1 differentiation and depletion of Glutamine (by depleting its metabolite α-ketoglutarate favors Treg differentiation. It might play a role in skewing the balance in favor of Tregs in the setting of amino acid deprivation that can be seen in the tumor microenvironment.
Production.
α-Ketoglutarate can be produced by:
Alpha-ketoglutarate can be used to produce:

</doc>
<doc id="17326" url="https://en.wikipedia.org/wiki?curid=17326" title="Keynesian economics">
Keynesian economics

Keynesian economics ( ; or Keynesianism) are the various theories about how in the short run, and especially during recessions, economic output is strongly influenced by aggregate demand (total spending in the economy). In the Keynesian view, aggregate demand does not necessarily equal the productive capacity of the economy; instead, it is influenced by a host of factors and sometimes behaves erratically, affecting production, employment, and inflation.
The theories forming the basis of Keynesian economics were first presented by the British economist John Maynard Keynes during the Great Depression in his 1936 book, "The General Theory of Employment, Interest and Money". Keynes contrasted his approach to the aggregate supply-focused 'classical' economics that preceded his book. The interpretations of Keynes that followed are contentious and several schools of economic thought claim his legacy.
Keynesian economists often argue that private sector decisions sometimes lead to inefficient macroeconomic outcomes which require active policy responses by the public sector, in particular, monetary policy actions by the central bank and fiscal policy actions by the government, in order to stabilize output over the business cycle. Keynesian economics advocates a mixed economy – predominantly private sector, but with a role for government intervention during recessions.
Keynesian economics served as the standard economic model in the developed nations during the later part of the Great Depression, World War II, and the post-war economic expansion (1945–1973), though it lost some influence following the oil shock and resulting stagflation of the 1970s. The advent of the financial crisis of 2007–08 caused a resurgence in Keynesian thought, which continues as new Keynesian economics.
Historical context.
Prior to the publication of Keynes's General Theory, mainstream economic thought held that a state of general equilibrium existed in the economy: because the needs of consumers are always greater than the capacity of the producers to satisfy those needs, everything that is produced will eventually be consumed once the appropriate price is found for it. This perception is reflected in Say's Law and in the writing of David Ricardo, which state that individuals produce so that they can either consume what they have manufactured or sell their output so that they can buy someone else's output. This argument rests upon the assumption that if a surplus of goods or services exists, they would naturally drop in price to the point where they would be consumed.
Keynes's theory overturned the mainstream thought of the time and brought about a greater awareness of structural inadequacies: problems such as unemployment, for example, are not viewed as a result of moral deficiencies like laziness, but rather result from imbalances in demand and whether the economy was expanding or contracting. Keynes argued that because there was no guarantee that the goods that individuals produce would be met with demand, unemployment was a natural consequence especially in the instance of an economy undergoing contraction.
He saw the economy as unable to maintain itself at full employment and believed that it was necessary for the government to step in and put under-utilized savings to work through government spending. Thus, according to Keynesian theory, some individually rational microeconomic-level actions such as not investing savings in the goods and services produced by the economy, if taken collectively by a large proportion of individuals and firms, can lead to outcomes wherein the economy operates below its potential output and growth rate.
Prior to Keynes, a situation in which aggregate demand for goods and services did not meet supply was referred to by classical economists as a "general glut", although there was disagreement among them as to whether a general glut was possible. Keynes argued that when a glut occurred, it was the over-reaction of producers and the laying off of workers that led to a fall in demand and perpetuated the problem. Keynesians therefore advocate an active stabilization policy to reduce the amplitude of the business cycle, which they rank among the most serious of economic problems. According to the theory, government spending can be used to increase aggregate demand, thus increasing economic activity, reducing unemployment and deflation.
Theory.
Keynes argued that the solution to the Great Depression was to stimulate the country ("inducement to invest") through some combination of two approaches:
If the interest rate at which businesses and consumers can borrow is decreased, investments which were previously uneconomic become profitable, and large consumer sales which are normally financed through debt (such as houses, automobiles, and, historically, even appliances like refrigerators) become more affordable. A principle function of central banks in countries which have them is to influence this interest rate through a variety of mechanisms which are collectively called "fiscal policy". This is how monetary policy which reduces interest rates is thought to stimulate economic activity, i.e. "grow the economy," and why it is called "expansionary" monetary policy.
Expansionary fiscal policy consists of increasing net public spending, which the government can effect by a) taxing less, b) spending more, or c) both. Investment and consumption by government raises demand for businesses' products and for employment, reversing the effects of the aforementioned imbalance. If desired spending exceeds revenue, the government finance the difference by borrowing from capital markets by issuing government bonds. This is called deficit spending. Two points are important to note at this point. First, deficits are not required for expansionary monetary policy, and second, it is only "change" in net spending that can stimulate or depress the economy. For example, if a government ran a deficit of 10% both last year and this year, this would represent neutral fiscal policy. In fact, if it ran a deficit of 10% last year and 5% this year, this would actually be contractionary. On the other hand, if the government ran a surplus of 10% of GDP last year and 5% this year, that would be expansionary fiscal policy, despite never running a deficit at all.
In the price mechanism of neoclassical economics, it is predicted that, in a competitive market, if demand for a particular good or service falls, that would immediately cause the price for that good or service to fall, which in turn would decrease supply and increase demand, thereby bringing them back to equilibrium. A central conclusion of Keynesian economics, in strong contrast to the previously dominant models of neoclassical synthesis, is that there are some situations in which a depressed economy would not quickly self-correct towards full employment and potential output, but could remain trapped indefinitely with both high unemployment and mothballed factories. To the observation that these were, in fact, the prevailing conditions throughout the industrialized world for many years during the Great Depression, classical models could only conclude that it was a temporary aberration. The purpose of Keynes' theory was to show such conditions could, without intervention, persist in a stable, though dismal, equilibrium.
By the end of the Second World War, Keynesianism was the most popular school of economic theory in the non-Communist world. Beginning in the late 1960s, a new classical macroeconomics movement arose, critical of Keynesian assumptions (see sticky prices), and seemed, especially in the 1970s, to explain certain phenomena (e.g. the co-existence of high unemployment and high inflation, or "stagflation") better. It was characterized by explicit and rigorous adherence to microfoundations, as well as use of increasingly sophisticated mathematical modelling. However, by the late 1980s, certain failures of the new classical models, both theoretical (see Real business cycle theory) and empirical (see the "Volcker recession") hastened the emergence of New Keynesian economics, a school which sought to unite the most realistic aspects of Keynesian and neo-classical assumptions and place them on more rigorous theoretical foundation than ever before.
Interpretations of Keynes have emphasized his stress on the international coordination of Keynesian policies, the need for international economic institutions, and the ways in which economic forces could lead to war or could promote peace.
Concept.
Wages and spending.
During the Great Depression, the classical theory attributed mass unemployment to high and rigid real wages.
To Keynes, the determination of wages was more complicated. First, he argued that it is not "real" but "nominal" wages that are set in negotiations between employers and workers, as opposed to a barter relationship. Second, nominal wage cuts would be difficult to put into effect because of laws and wage contracts. Even classical economists admitted that these exist; unlike Keynes, they advocated abolishing minimum wages, unions, and long-term contracts, increasing labour market flexibility. However, to Keynes, people will resist nominal wage reductions, even without unions, until they see other wages falling and a general fall of prices.
Keynes rejected the idea that cutting wages would cure recessions. He examined the explanations for this idea and found them all faulty. He also considered the most likely consequences of cutting wages in recessions, under various different circumstances. He concluded that such wage cutting would be more likely to make recessions worse rather than better.
Further, if wages and prices were falling, people would start to expect them to fall. This could make the economy spiral downward as those who had money would simply wait as falling prices made it more valuable – rather than spending. As Irving Fisher argued in 1933, in his "Debt-Deflation Theory of Great Depressions", deflation (falling prices) can make a depression deeper as falling prices and wages made pre-existing nominal debts more valuable in real terms.
Excessive saving.
To Keynes, excessive saving, i.e. saving beyond planned investment, was a serious problem, encouraging recession or even depression. Excessive saving results if investment falls, perhaps due to falling consumer demand, over-investment in earlier years, or pessimistic business expectations, and if saving does not immediately fall in step, the economy would decline.
The classical economists argued that interest rates would fall due to an increase in savings. The first diagram, adapted from the only graph in "The General Theory", shows this process. (For simplicity, other sources of the demand for or supply of savings are ignored here.) Assume that fixed investment in capital goods falls from "old I" to "new I" (step a). Second (step b), the resulting excess of saving causes interest-rate cuts, abolishing the excess supply: so again we have saving (S) equal to investment. The interest-rate (i) fall prevents that of production and employment.
Keynes had a complex argument against this "laissez-faire" response. The graph below summarizes his argument, assuming again that fixed investment falls (step A). "First", saving does not fall much as interest rates fall, since the income and substitution effects of falling rates go in conflicting directions. "Second", since planned fixed investment in plant and equipment is based mostly on long-term expectations of future profitability, that spending does not rise much as interest rates fall.
So S and I are drawn as steep (inelastic) in the graph. Given the inelasticity of both demand and supply, a "large" interest-rate fall is needed to close the saving/investment gap. As drawn, this requires a "negative" interest rate at equilibrium (where the new I line would intersect the old S line). However, this negative interest rate is not necessary to Keynes's argument.
"Third", Keynes argued that saving and investment are not the main determinants of interest rates, especially in the short run. Instead, the supply of and the demand for the stock of "money" determine interest rates in the short run. (This is not drawn in the graph.) Neither changes quickly in response to excessive saving to allow fast interest-rate adjustment.
Finally, Keynes suggested that, because of fear of capital losses on assets besides money, there may be a "liquidity trap" setting a floor under which interest rates cannot fall. While in this trap, interest rates are so low that any increase in money supply will cause bond-holders (fearing rises in interest rates and hence capital losses on their bonds) to sell their bonds to attain money (liquidity).
In the diagram, the equilibrium suggested by the new I line and the old S line cannot be reached, so that excess saving persists. Some (such as Paul Krugman) see this latter kind of liquidity trap as prevailing in Japan in the 1990s. Most economists agree that nominal interest rates cannot fall below zero. However, some economists (particularly those from the Chicago school) reject the existence of a liquidity trap.
Even if the liquidity trap does not exist, there is a "fourth" (perhaps most important) element to Keynes's critique. Saving involves not spending all of one's income. Thus, it means insufficient demand for business output, unless it is balanced by other sources of demand, such as fixed investment. Therefore, "excessive" saving corresponds to an unwanted accumulation of inventories, or what classical economists called a general glut.
This pile-up of unsold goods and materials encourages businesses to decrease both production and employment. This in turn lowers people's incomes – and saving, causing a leftward shift in the S line in the diagram (step B). For Keynes, the fall in income did most of the job by ending excessive saving and allowing the loanable funds market to attain equilibrium. Instead of interest-rate adjustment solving the problem, a recession does so. Thus in the diagram, the interest-rate change is small.
Whereas the classical economists assumed that the level of output and income was constant and given at any one time (except for short-lived deviations), Keynes saw this as the key variable that adjusted to equate saving and investment.
"Finally", a recession undermines the business incentive to engage in fixed investment. With falling incomes and demand for products, the desired demand for factories and equipment (not to mention housing) will fall. This accelerator effect would shift the I line to the left again, a change not shown in the diagram above. This recreates the problem of excessive saving and encourages the recession to continue.
In sum, to Keynes there is interaction between excess supplies in different markets, as unemployment in labour markets encourages excessive saving – and "vice versa". Rather than prices adjusting to attain equilibrium, the main story is one of quantity adjustment allowing recessions and possible attainment of underemployment equilibrium.
Active fiscal policy.
Classical economists have traditionally advocated balanced government budgets. Keynesians, on the other hand, believe that it is entirely legitimate and appropriate for governments to incur expenditure in excess of taxation revenues during periods of economic stagnation such as the Great Depression, which dominated economic life at the time he was developing and publicizing his theories.
Contrary to some critical characterizations of it, Keynesianism does not consist solely of deficit spending. Keynesianism recommends counter-cyclical policies. An example of a counter-cyclical policy is raising taxes to cool the economy and to prevent inflation when there is abundant demand-side growth, and engaging in deficit spending on labour-intensive infrastructure projects to stimulate employment and stabilize wages during economic downturns. Classical economics, on the other hand, argues that one should "cut" taxes when there are budget surpluses, and cut spending – or, less likely, increase taxes – during economic downturns.
Keynes's ideas influenced Franklin D. Roosevelt's view that insufficient buying-power caused the Depression. During his presidency, Roosevelt adopted some aspects of Keynesian economics, especially after 1937, when, in the depths of the Depression, the United States suffered from recession yet again following fiscal contraction. But to many the true success of Keynesian policy can be seen at the onset of World War II, which provided a kick to the world economy, removed uncertainty, and forced the rebuilding of destroyed capital. Keynesian ideas became almost official in social-democratic Europe after the war and in the U.S. in the 1960s.
Keynes developed a theory which suggested that active government policy could be effective in managing the economy. Rather than seeing unbalanced government budgets as wrong, Keynes advocated what has been called countercyclical fiscal policies, that is, policies that acted against the tide of the business cycle: deficit spending when a nation's economy suffers from recession or when recovery is long-delayed and unemployment is persistently high – and the suppression of inflation in boom times by either increasing taxes or cutting back on government outlays. He argued that governments should solve problems in the short run rather than waiting for market forces to do it in the long run, because, "in the long run, we are all dead."
This contrasted with the classical and neoclassical economic analysis of fiscal policy. Fiscal stimulus could actuate production. But, to these schools, there was no reason to believe that this stimulation would outrun the side-effects that "crowd out" private investment: first, it would increase the demand for labour and raise wages, hurting profitability; Second, a government deficit increases the stock of government bonds, reducing their market price and encouraging high interest rates, making it more expensive for business to finance fixed investment. Thus, efforts to stimulate the economy would be self-defeating.
The Keynesian response is that such fiscal policy is appropriate only when unemployment is persistently high, above the non-accelerating inflation rate of unemployment (NAIRU). In that case, crowding out is minimal. Further, private investment can be "crowded in": Fiscal stimulus raises the market for business output, raising cash flow and profitability, spurring business optimism. To Keynes, this accelerator effect meant that government and business could be "complements" rather than substitutes in this situation.
Second, as the stimulus occurs, gross domestic product rises, raising the amount of saving, helping to finance the increase in fixed investment. Finally, government outlays need not always be wasteful: government investment in public goods that will not be provided by profit-seekers will encourage the private sector's growth. That is, government spending on such things as basic research, public health, education, and infrastructure could help the long-term growth of potential output.
In Keynes's theory, there must be significant slack in the labour market before fiscal expansion is justified.
Keynesian economists believe that adding to profits and incomes during boom cycles through tax cuts, and removing income and profits from the economy through cuts in spending during downturns, tends to exacerbate the negative effects of the business cycle. This effect is especially pronounced when the government controls a large fraction of the economy, as increased tax revenue may aid investment in state enterprises in downturns, and decreased state revenue and investment harm those enterprises.
"Multiplier effect" and interest rates.
Two aspects of Keynes's model have implications for policy:
First, there is the "Keynesian multiplier", first developed by Richard F. Kahn in 1931. Exogenous increases in spending, such as an increase in government outlays, increases total spending by a multiple of that increase. A government could stimulate a great deal of new production with a modest outlay if:
This process continues. At each step, the increase in spending is smaller than in the previous step, so that the multiplier process tapers off and allows the attainment of an equilibrium. This story is modified and moderated if we move beyond a "closed economy" and bring in the role of taxation: The rise in imports and tax payments at each step reduces the amount of induced consumer spending and the size of the multiplier effect.
Second, Keynes re-analyzed the effect of the interest rate on investment. In the classical model, the supply of funds (saving) determines the amount of fixed business investment. That is, under the classical model, since all savings are placed in banks, and all business investors in need of borrowed funds go to banks, the amount of savings determines the amount that is available to invest. Under Keynes's model, the amount of investment is determined independently by long-term profit expectations and, to a lesser extent, the interest rate. The latter opens the possibility of regulating the economy through money supply changes, via monetary policy. Under conditions such as the Great Depression, Keynes argued that this approach would be relatively ineffective compared to fiscal policy. But, during more "normal" times, monetary expansion can stimulate the economy.
IS–LM model.
The IS–LM model is nearly as influential as Keynes's original analysis in determining actual policy and economics education. It relates aggregate demand and employment to three exogenous quantities, i.e., the amount of money in circulation, the government budget, and the state of business expectations. This model was very popular with economists after World War II because it could be understood in terms of general equilibrium theory. This encouraged a much more static vision of macroeconomics than that described above.
History.
Precursors.
Keynes's work was part of a long-running debate within economics over the existence and nature of general gluts. While a number of the policies Keynes advocated (the notable one being government deficit spending at times of low private investment or consumption) and the theoretical ideas he proposed (effective demand, the multiplier, the paradox of thrift) were advanced by various authors in the 19th and early 20th centuries, Keynes's unique contribution was to provide a "general theory" of these, which proved acceptable to the political and economic establishments.
Schools.
An intellectual precursor of Keynesian economics was underconsumption theory in classical economics, dating from such 19th-century economists as Thomas Malthus, the Birmingham School of Thomas Attwood, and the American economists William Trufant Foster and Waddill Catchings, who were influential in the 1920s and 1930s. Underconsumptionists were, like Keynes after them, concerned with failure of aggregate demand to attain potential output, calling this "underconsumption" (focusing on the demand side), rather than "overproduction" (which would focus on the supply side), and advocating economic interventionism. Keynes specifically discussed underconsumption (which he wrote "under-consumption") in the "General Theory," in Chapter 22, Section IV and Chapter 23, Section VII.
Numerous concepts were developed earlier and independently of Keynes by the Stockholm school during the 1930s; these accomplishments were described in a 1937 article, published in response to the 1936 "General Theory," sharing the Swedish discoveries.
Concepts.
The multiplier dates to work in the 1890s by the Australian economist Alfred De Lissa, the Danish economist Julius Wulff, and the German American economist Nicholas Johannsen, the latter being cited in a footnote of Keynes. Nicholas Johannsen also proposed a theory of effective demand in the 1890s.
The paradox of thrift was stated in 1892 by John M. Robertson in his "The Fallacy of Saving," in earlier forms by mercantilist economists since the 16th century, and similar sentiments date to antiquity.
Today these ideas, regardless of provenance, are referred to in academia under the rubric of "Keynesian economics", due to Keynes's role in consolidating, elaborating, and popularizing them.
Keynes and the classics.
Keynes sought to distinguish his theories from and oppose them to "classical economics," by which he meant the economic theories of David Ricardo and his followers, including John Stuart Mill, Alfred Marshall, Francis Ysidro Edgeworth, and Arthur Cecil Pigou. A central tenet of the classical view, known as Say's law, states that "supply creates its own demand." Say's Law can be interpreted in two ways. First, the claim that the total value of output is equal to the sum of income earned in production is a result of a national income accounting identity, and is therefore indisputable. A second and stronger claim, however, that the ""costs" of output are always covered in the aggregate by the sale-proceeds resulting from demand" depends on how consumption and saving are linked to production and investment. In particular, Keynes argued that the second, strong form of Say's Law only holds if increases in individual savings exactly match an increase in aggregate investment.
Keynes sought to develop a theory that would explain determinants of saving, consumption, investment and production. In that theory, the interaction of aggregate demand and aggregate supply determines the level of output and employment in the economy.
Because of what he considered the failure of the “Classical Theory” in the 1930s, Keynes firmly objects to its main theory – adjustments in prices would automatically make demand tend to the full employment level.
Neo-classical theory supports that the two main costs that shift demand and supply are labour and money. Through the distribution of the monetary policy, demand and supply can be adjusted. If there were more labour than demand for it, wages would fall until hiring began again. If there were too much saving, and not enough consumption, then interest rates would fall until people either cut their savings rate or started borrowing.
Postwar Keynesianism.
Keynes's ideas became widely accepted after World War II, and until the early 1970s, Keynesian economics provided the main inspiration for economic policy makers in Western industrialized countries. Governments prepared high quality economic statistics on an ongoing basis and tried to base their policies on the Keynesian theory that had become the norm. In the early era of social liberalism and social democracy, most western capitalist countries enjoyed low, stable unemployment and modest inflation, an era called the Golden Age of Capitalism.
In terms of policy, the twin tools of post-war Keynesian economics were fiscal policy and monetary policy. While these are credited to Keynes, others, such as economic historian David Colander, argue that they are, rather, due to the interpretation of Keynes by Abba Lerner in his theory of Functional Finance, and should instead be called "Lernerian" rather than "Keynesian".
Through the 1950s, moderate degrees of government demand leading industrial development, and use of fiscal and monetary counter-cyclical policies continued, and reached a peak in the "go go" 1960s, where it seemed to many Keynesians that prosperity was now permanent. In 1971, Republican US President Richard Nixon even proclaimed "I am now a Keynesian in economics."
However, with the oil shock of 1973, and the economic problems of the 1970s, Keynesian economics began to fall out of favour. During this time, many economies experienced high and rising unemployment, coupled with high and rising inflation, contradicting the Phillips curve's prediction. This stagflation meant that the simultaneous application of expansionary (anti-recession) and contractionary (anti-inflation) policies appeared to be necessary. This dilemma led to the end of the Keynesian near-consensus of the 1960s, and the rise throughout the 1970s of ideas based upon more classical analysis, including monetarism, supply-side economics, and new classical economics.
At the same time, Keynesians began during the period to reorganize their thinking (some becoming associated with New Keynesian economics). One strategy, utilized also as a critique of the notably high unemployment and potentially disappointing GNP growth rates associated with the latter two theories by the mid-1980s, was to emphasize low unemployment and maximal economic growth at the cost of somewhat higher inflation (its consequences kept in check by indexing and other methods, and its overall rate kept lower and steadier by such potential policies as Martin Weitzman's share economy).
Multiple schools of economic thought that trace their legacy to Keynes currently exist, the notable ones being Neo-Keynesian economics, New Keynesian economics, and Post-Keynesian economics. Keynes's biographer Robert Skidelsky writes that the post-Keynesian school has remained closest to the spirit of Keynes's work in following his monetary theory and rejecting the neutrality of money.
In the postwar era, Keynesian analysis was combined with neoclassical economics to produce what is generally termed the "neoclassical synthesis", yielding Neo-Keynesian economics, which dominated mainstream macroeconomic thought. Though it was widely held that there was no strong automatic tendency to full employment, many believed that if government policy were used to ensure it, the economy would behave as neoclassical theory predicted. This post-war domination by Neo-Keynesian economics was broken during the stagflation of the 1970s. There was a lack of consensus among macroeconomists in the 1980s. However, the advent of New Keynesian economics in the 1990s, modified and provided microeconomic foundations for the neo-Keynesian theories. These modified models now dominate mainstream economics.
Post-Keynesian economists, on the other hand, reject the neoclassical synthesis and, in general, neoclassical economics applied to the macroeconomy. Post-Keynesian economics is a heterodox school that holds that both Neo-Keynesian economics and New Keynesian economics are incorrect, and a misinterpretation of Keynes's ideas. The Post-Keynesian school encompasses a variety of perspectives, but has been far less influential than the other more mainstream Keynesian schools.
Other schools of economics.
The Keynesian schools of economics are situated alongside a number of other schools that have the same perspectives on what the economic issues are, but differ on what causes them and how to best resolve them:
Stockholm School.
The Stockholm School rose to prominence at about the same time that Keynes published his General Theory and shared a common concern in business cycles and unemployment. The second generation of Swedish economists also advocated government intervention through spending during economic downturns although opinions are divided over whether they conceived the essence of Keynes's theory before he did.
Monetarism.
There was debate between Monetarists and Keynesians in the 1960s over the role of government in stabilizing the economy. Both Monetarists and Keynesians are in agreement over the fact that issues such as business cycles, unemployment, and deflation are caused by inadequate demand. However, they had fundamentally different perspectives on the capacity of the economy to find its own equilibrium, and the degree of government intervention that would be appropriate. Keynesians emphasized the use of discretionary fiscal policy and monetary policy, while monetarists argued the primacy of monetary policy, and that it should be rules-based.
The debate was largely resolved in the 1980s. Since then, economists have largely agreed that central banks should bear the primary responsibility for stabilizing the economy, and that monetary policy should largely follow the Taylor rule – which many economists credit with the Great Moderation. The financial crisis of 2007–08, however, has convinced many economists and governments of the need for fiscal interventions and highlighted the difficulty in stimulating economies through monetary policy alone during a liquidity trap.
Public choice theory.
Some Marxist economists criticized Keynesian economics. For example, in his 1946 appraisal Paul Sweezy, while admitting that there was much in the "General Theory"'s analysis of effective demand which Marxists could draw upon, described Keynes as in the last resort a prisoner of his neoclassical upbringing. Sweezy argued Keynes had never been able to view the capitalist system as a totality. He argued Keynes had regarded the class struggle carelessly, and overlooked the class role of the capitalist state, which he treated as a "deus ex machina", and some other points. 
While Michał Kalecki was generally enthusiastic about the Keynesian revolution, he predicted that it would not endure, in his article "Political Aspects of Full Employment". In the article Kalecki predicted that the full employment delivered by Keynesian policy would eventually lead to a more assertive working class and weakening of the social position of business leaders, causing the elite to use their political power to force the displacement of the Keynesian policy even though profits would be higher than under a laissez faire system: The erosion of social prestige and political power would be unacceptable to the elites despite higher profits.
James M. Buchanan criticized Keynesian economics on the grounds that governments would in practice be unlikely to implement theoretically optimal policies. The implicit assumption underlying the Keynesian fiscal revolution, according to Buchanan, was that economic policy would be made by wise men, acting without regard to political pressures or opportunities, and guided by disinterested economic technocrats. He argued that this was an unrealistic assumption about political, bureaucratic and electoral behaviour. Buchanan blamed Keynesian economics for what he considered a decline in America’s fiscal discipline. Buchanan argued that deficit spending would evolve into a permanent disconnect between spending and revenue, precisely because it brings short-term gains, so, ending up institutionalizing irresponsibility in the federal government, the largest and most central institution in our society.
Martin Feldstein argues that the legacy of Keynesian economics–the misdiagnosis of unemployment, the fear of saving, and the unjustified government intervention–affected the fundamental ideas of policy makers.
Milton Friedman thought that Keynes’s political bequest was harmful for two reasons. First, he thought whatever the economic analysis, benevolent dictatorship is likely sooner or later to lead to a totalitarian society. Second, he thought Keynes’s economic theories appealed to a group far broader than economists primarily because of their link to his political approach.
Alex Tabarrok argues that Keynesian politics–as distinct from Keynesian policies–has failed pretty much whenever it's been tried, at least in liberal democracies.
In response to this argument, John Quiggin, wrote about these theories' implication for a liberal democratic order. He thought if it is generally accepted that democratic politics is nothing more than a battleground for competing interest groups, then reality will come to resemble the model.
Paul Krugman wrote "I don’t think we need to take that as an immutable fact of life; but still, what are the alternatives?" 
Daniel Kuehn, criticized James M. Buchanan. He argued, "if you have a problem with politicians - criticize politicians," not Keynes. He also argued that empirical evidence makes it pretty clear that Buchanan was wrong.
James Tobin argued, if advising government officials, politicians, voters, it’s not for economists to play games with them.
Keynes implicitly rejected this argument, in “soon or late it is ideas not vested interests which are dangerous for good or evil.”
Brad DeLong has argued that politics is the main motivator behind objections to the view that government should try to serve a stabilizing macroeconomic role. Paul Krugman argued that in conjunction with theory from America's first Nobel Prize winning economist, Paul Samuelson, he continues to employ Keynesian economics, so as to avoid intellectual instability, political instability, and financial instability.
New Classical.
Another influential school of thought was based on the Lucas critique of Keynesian economics. This called for greater consistency with microeconomic theory and rationality, and in particular emphasized the idea of rational expectations. Lucas and others argued that Keynesian economics required remarkably foolish and short-sighted behaviour from people, which totally contradicted the economic understanding of their behaviour at a micro level. New classical economics introduced a set of macroeconomic theories that were based on optimizing microeconomic behaviour. These models have been developed into the Real Business Cycle Theory, which argues that business cycle fluctuations can to a large extent be accounted for by real (in contrast to nominal) shocks.
Beginning in the late 1950s new classical macroeconomists began to disagree with the methodology employed by Keynes and his successors. Keynesians emphasized the dependence of consumption on disposable income and, also, of investment on current profits and current cash flow. In addition, Keynesians posited a Phillips curve that tied nominal wage inflation to unemployment rate. To support these theories, Keynesians typically traced the logical foundations of their model (using introspection) and supported their assumptions with statistical evidence. New classical theorists demanded that macroeconomics be grounded on the same foundations as microeconomic theory, profit-maximizing firms and rational, utility-maximizing consumers.
The result of this shift in methodology produced several important divergences from Keynesian Macroeconomics:

</doc>
<doc id="17327" url="https://en.wikipedia.org/wiki?curid=17327" title="Kinetic energy">
Kinetic energy

In physics, the kinetic energy of an object is the energy that it possesses due to its motion.
It is defined as the work needed to accelerate a body of a given mass from rest to its stated velocity. Having gained this energy during its acceleration, the body maintains this kinetic energy unless its speed changes. The same amount of work is done by the body in decelerating from its current speed to a state of rest.
In classical mechanics, the kinetic energy of a non-rotating object of mass "m" traveling at a speed "v" is . In relativistic mechanics, this is a good approximation only when "v" is much less than the speed of light.
The standard unit of kinetic energy is the joule.
History and etymology.
The adjective "kinetic" has its roots in the Greek word κίνησις "kinesis", meaning "motion". The dichotomy between kinetic energy and potential energy can be traced back to Aristotle's concepts of actuality and potentiality.
The principle in classical mechanics that "E ∝ mv2" was first developed by Gottfried Leibniz and Johann Bernoulli, who described kinetic energy as the "living force", "vis viva". Willem 's Gravesande of the Netherlands provided experimental evidence of this relationship. By dropping weights from different heights into a block of clay, Willem 's Gravesande determined that their penetration depth was proportional to the square of their impact speed. Émilie du Châtelet recognized the implications of the experiment and published an explanation.
The terms "kinetic energy" and "work" in their present scientific meanings date back to the mid-19th century. Early understandings of these ideas can be attributed to Gaspard-Gustave Coriolis, who in 1829 published the paper titled "Du Calcul de l'Effet des Machines" outlining the mathematics of kinetic energy. William Thomson, later Lord Kelvin, is given the credit for coining the term "kinetic energy" c. 1849–51.
Introduction.
Energy occurs in many forms, including chemical energy, thermal energy, electromagnetic radiation, gravitational energy, electric energy, elastic energy, nuclear energy, and rest energy. These can be categorized in two main classes: potential energy and kinetic energy. Kinetic energy is the movement energy of an object.
Kinetic energy may be best understood by examples that demonstrate how it is transformed to and from other forms of energy. For example, a cyclist uses chemical energy provided by food to accelerate a bicycle to a chosen speed. On a level surface, this speed can be maintained without further work, except to overcome air resistance and friction. The chemical energy has been converted into kinetic energy, the energy of motion, but the process is not completely efficient and produces heat within the cyclist.
The kinetic energy in the moving cyclist and the bicycle can be converted to other forms. For example, the cyclist could encounter a hill just high enough to coast up, so that the bicycle comes to a complete halt at the top. The kinetic energy has now largely been converted to gravitational potential energy that can be released by freewheeling down the other side of the hill. Since the bicycle lost some of its energy to friction, it never regains all of its speed without additional pedaling. The energy is not destroyed; it has only been converted to another form by friction. Alternatively the cyclist could connect a dynamo to one of the wheels and generate some electrical energy on the descent. The bicycle would be traveling slower at the bottom of the hill than without the generator because some of the energy has been diverted into electrical energy. Another possibility would be for the cyclist to apply the brakes, in which case the kinetic energy would be dissipated through friction as heat.
Like any physical quantity that is a function of velocity, the kinetic energy of an object depends on the relationship between the object and the observer's frame of reference. Thus, the kinetic energy of an object is not invariant.
Spacecraft use chemical energy to launch and gain considerable kinetic energy to reach orbital velocity. In a perfectly circular orbit, this kinetic energy remains constant because there is almost no friction in near-earth space. However it becomes apparent at re-entry when some of the kinetic energy is converted to heat. If the orbit is elliptical or hyperbolic, then throughout the orbit kinetic and potential energy are exchanged; kinetic energy is greatest and potential energy lowest at closest approach to the earth or other massive body, while potential energy is greatest and kinetic energy the lowest at maximum distance. Without loss or gain, however, the sum of the kinetic and potential energy remains constant.
Kinetic energy can be passed from one object to another. In the game of billiards, the player imposes kinetic energy on the cue ball by striking it with the cue stick. If the cue ball collides with another ball, it slows down dramatically and the ball it collided with accelerates to a speed as the kinetic energy is passed on to it. Collisions in billiards are effectively elastic collisions, in which kinetic energy is preserved. In inelastic collisions, kinetic energy is dissipated in various forms of energy, such as heat, sound, binding energy (breaking bound structures).
Flywheels have been developed as a method of energy storage. This illustrates that kinetic energy is also stored in rotational motion.
Several mathematical descriptions of kinetic energy exist that describe it in the appropriate physical situation. For objects and processes in common human experience, the formula ½mv² given by Newtonian (classical) mechanics is suitable. However, if the speed of the object is comparable to the speed of light, relativistic effects become significant and the relativistic formula is used. If the object is on the atomic or sub-atomic scale, quantum mechanical effects are significant and a quantum mechanical model must be employed.
Newtonian kinetic energy.
Kinetic energy of rigid bodies.
In classical mechanics, the kinetic energy of a "point object" (an object so small that its mass can be assumed to exist at one point), or a non-rotating rigid body depends on the mass of the body as well as its speed. The kinetic energy is equal to 1/2 the product of the mass and the square of the speed. In formula form:
where formula_2 is the mass and formula_3 is the speed (or the velocity) of the body. In SI units (used for most modern scientific work), mass is measured in kilograms, speed in metres per second, and the resulting kinetic energy is in joules.
For example, one would calculate the kinetic energy of an 80 kg mass (about 180 lbs) traveling at 18 metres per second (about 40 mph, or 65 km/h) as
When you throw a ball, you do work on it to give it speed as it leaves your hand. The moving ball can then hit something and push it, doing work on what it hits. The kinetic energy of a moving object is equal to the work required to bring it from rest to that speed, or the work the object can do while being brought to rest: net force × displacement = kinetic energy, i.e.,
Since the kinetic energy increases with the square of the speed, an object doubling its speed has four times as much kinetic energy. For example, a car traveling twice as fast as another requires four times as much distance to stop, assuming a constant braking force. As a consequence of this quadrupling, it takes four times the work to double the speed.
The kinetic energy of an object is related to its momentum by the equation:
where:
For the "translational kinetic energy," that is the kinetic energy associated with rectilinear motion, of a rigid body with constant mass formula_8, whose center of mass is moving in a straight line with speed formula_10, as seen above is equal to
where:
The kinetic energy of any entity depends on the reference frame in which it is measured. However the total energy of an isolated system, i.e. one in which energy can neither enter nor leave, does not change over time in the reference frame in which it is measured. Thus, the chemical energy converted to kinetic energy by a rocket engine is divided differently between the rocket ship and its exhaust stream depending upon the chosen reference frame. This is called the Oberth effect. But the total energy of the system, including kinetic energy, fuel chemical energy, heat, etc., is conserved over time, regardless of the choice of reference frame. Different observers moving with different reference frames would however disagree on the value of this conserved energy.
The kinetic energy of such systems depends on the choice of reference frame: the reference frame that gives the minimum value of that energy is the center of momentum frame, i.e. the reference frame in which the total momentum of the system is zero. This minimum kinetic energy contributes to the invariant mass of the system as a whole.
Derivation.
The work done in accelerating a particle during the infinitesimal time interval "dt" is given by the dot product of "force" and "displacement":
where we have assumed the relationship p = "m" v. (However, also see the special relativistic derivation below.)
Applying the product rule we see that:
Therefore, (assuming constant mass so that "dm"=0), the following can be seen:
Since this is a total differential (that is, it only depends on the final state, not how the particle got there), we can integrate it and call the result kinetic energy. Assuming the object was at rest at time 0, we integrate from time 0 to time t because the work done by the force to bring the object from rest to velocity v is equal to the work necessary to do the reverse:
This equation states that the kinetic energy ("E"k) is equal to the integral of the dot product of the velocity (v) of a body and the infinitesimal change of the body's momentum (p). It is assumed that the body starts with no kinetic energy when it is at rest (motionless).
Rotating bodies.
If a rigid body Q is rotating about any line through the center of mass then it has "rotational kinetic energy" (formula_18) which is simply the sum of the kinetic energies of its moving parts, and is thus given by:
where:
(In this equation the moment of inertia must be taken about an axis through the center of mass and the rotation measured by ω must be around that axis; more general equations exist for systems where the object is subject to wobble due to its eccentric shape).
Kinetic energy of systems.
A system of bodies may have internal kinetic energy due to the relative motion of the bodies in the system. For example, in the Solar System the planets and planetoids are orbiting the Sun. In a tank of gas, the molecules are moving in all directions. The kinetic energy of the system is the sum of the kinetic energies of the bodies it contains.
A macroscopic body that is stationary (i.e. a reference frame has been chosen to correspond to the body's center of momentum) may have various kinds of internal energy at the molecular or atomic level, which may be regarded as kinetic energy, due to molecular translation, rotation, and vibration, electron translation and spin, and nuclear spin. These all contribute to the body's mass, as provided by the special theory of relativity. When discussing movements of a macroscopic body, the kinetic energy referred to is usually that of the macroscopic movement only. However all internal energies of all types contribute to body's mass, inertia, and total energy.
Frame of reference.
The speed, and thus the kinetic energy of a single object is frame-dependent (relative): it can take any non-negative value, by choosing a suitable inertial frame of reference. For example, a bullet passing an observer has kinetic energy in the reference frame of this observer. The same bullet is stationary from the point of view of an observer moving with the same velocity as the bullet, and so has zero kinetic energy. By contrast, the total kinetic energy of a system of objects cannot be reduced to zero by a suitable choice of the inertial reference frame, unless all the objects have the same velocity. In any other case the total kinetic energy has a non-zero minimum, as no inertial reference frame can be chosen in which all the objects are stationary. This minimum kinetic energy contributes to the system's invariant mass, which is independent of the reference frame.
The total kinetic energy of a system depends on the inertial frame of reference: it is the sum of the total kinetic energy in a center of momentum frame and the kinetic energy the total mass would have if it were concentrated in the center of mass.
This may be simply shown: let formula_22 be the relative velocity of the center of mass frame "i" in the frame "k".
Since formula_23,
However, let formula_25 the kinetic energy in the center of mass frame, formula_26 would be simply the total momentum that is by definition zero in the center of mass frame, and let the total mass: formula_27. Substituting, we get:
Thus the kinetic energy of a system is lowest with respect to center of momentum reference frames, i.e., frames of reference in which the center of mass is stationary (either the center of mass frame or any other center of momentum frame). In any other frame of reference there is additional kinetic energy corresponding to the total mass moving at the speed of the center of mass. The kinetic energy of the system in the center of momentum frame is a quantity that is invariant (all observers see it to be the same).
Rotation in systems.
It sometimes is convenient to split the total kinetic energy of a body into the sum of the body's center-of-mass translational kinetic energy and the energy of rotation around the center of mass (rotational energy):
where:
Thus the kinetic energy of a tennis ball in flight is the kinetic energy due to its rotation, plus the kinetic energy due to its translation.
Relativistic kinetic energy of rigid bodies.
If a body's speed is a significant fraction of the speed of light, it is necessary to use relativistic mechanics to calculate its kinetic energy. In special relativity theory, the expression for linear momentum is modified.
With "m" being an object's rest mass, v and "v" its velocity and speed, and "c" the speed of light in vacuum, we use the expression for linear momentum formula_30, where formula_31.
Integrating by parts yields
Since formula_33,
formula_35 is a constant of integration for the indefinite integral.
Simplifying the expression we obtain
formula_35 is found by observing that when formula_38 and formula_39, giving
resulting in the formula
This formula shows that the work expended accelerating an object from rest approaches infinity as the velocity approaches the speed of light. Thus it is impossible to accelerate an object across this boundary.
The mathematical by-product of this calculation is the mass-energy equivalence formula—the body at rest must have energy content
At a low speed (formula_3Â«formula_44), the relativistic kinetic energy is approximated well by the classical kinetic energy. This is done by binomial approximation or by taking the first two terms of the Taylor expansion for the reciprocal square root:
So, the total energy formula_46 can be partitioned into the rest mass energy plus the Newtonian kinetic energy at low speeds.
When objects move at a speed much slower than light (e.g. in everyday phenomena on Earth), the first two terms of the series predominate. The next term in the Taylor series approximation
is small for low speeds. For example, for a speed of the correction to the Newtonian kinetic energy is 0.0417 J/kg (on a Newtonian kinetic energy of 50 MJ/kg) and for a speed of 100 km/s it is 417 J/kg (on a Newtonian kinetic energy of 5 GJ/kg).
The relativistic relation between kinetic energy and momentum is given by
This can also be expanded as a Taylor series, the first term of which is the simple expression from Newtonian mechanics.
What this suggests is that the formulae for energy and momentum are not special and axiomatic, but rather concepts that emerge from the equivalence of mass and energy and the principles of relativity.
General relativity.
Using the convention that
where the four-velocity of a particle is
and formula_51 is the proper time of the particle, there is also an expression for the kinetic energy of the particle in general relativity.
If the particle has momentum
as it passes by an observer with four-velocity "u"obs, then the expression for total energy of the particle as observed (measured in a local inertial frame) is
and the kinetic energy can be expressed as the total energy minus the rest energy:
Consider the case of a metric that is diagonal and spatially isotropic ("g"tt,"g"ss,"g"ss,"g"ss). Since
where "v"α is the ordinary velocity measured w.r.t. the coordinate system, we get
Solving for "u"t gives
Thus for a stationary observer ("v"= 0)
and thus the kinetic energy takes the form
Factoring out the rest energy gives:
This expression reduces to the special relativistic case for the flat-space metric where
In the Newtonian approximation to general relativity
where Φ is the Newtonian gravitational potential. This means clocks run slower and measuring rods are shorter near massive bodies.
Kinetic energy in quantum mechanics.
In quantum mechanics, observables like kinetic energy are represented as operators. For one particle of mass "m", the kinetic energy operator appears as a term in the Hamiltonian and is defined in terms of the more fundamental momentum operator formula_65 as
Notice that this can be obtained by replacing formula_67 by formula_65 in the classical expression for kinetic energy in terms of momentum,
In the Schrödinger picture, formula_65 takes the form formula_71 where the derivative is taken with respect to position coordinates and hence
The expectation value of the electron kinetic energy, formula_73, for a system of "N" electrons described by the wavefunction formula_74 is a sum of 1-electron operator expectation values:
where formula_76 is the mass of the electron and formula_77 is the Laplacian operator acting upon the coordinates of the "i"th electron and the summation runs over all electrons.
The density functional formalism of quantum mechanics requires knowledge of the electron density "only", i.e., it formally does not require knowledge of the wavefunction. Given an electron density formula_78, the exact N-electron kinetic energy functional is unknown; however, for the specific case of a 1-electron system, the kinetic energy can be written as
where formula_80 is known as the von Weizsäcker kinetic energy functional.

</doc>
<doc id="17330" url="https://en.wikipedia.org/wiki?curid=17330" title="King's Royal Rifle Corps">
King's Royal Rifle Corps

The King's Royal Rifle Corps was an infantry rifle regiment of the British Army, originally raised in North America as the Royal Americans, and recruited from North American colonists. Later ranked as the 60th Regiment of Foot, the regiment served for more than 200 years throughout the British Empire. In 1966 the regiment amalgamated with the Oxfordshire and Buckinghamshire Light Infantry and the Rifle Brigade (Prince Consort's Own) to become the 2nd Battalion, Royal Green Jackets, a new large regiment. In 2007 the Royal Green Jackets was merged with the Devonshire and Dorset Regiment, the Royal Gloucestershire, Berkshire and Wiltshire Regiment and The Light Infantry to form The Rifles, where the lineage of the King's Royal Rifle Corps continues on as the 2nd Battalion, The Rifles.
History.
The King's Royal Rifle Corps was raised in the American colonies in 1756 as the 62nd (Royal American) Regiment to defend the thirteen colonies against attack by the French and their Native American allies. After Braddock's defeat in 1755, royal approval for a new regiment, as well as funds, were granted by parliament just before Christmas 1755 – hence the regiment's traditional birthday of Christmas Day. However parliamentary delays meant it was 4 March 1756 before a special act of parliament created four battalions of 1,000 men each to include foreigners for service in the Americas.
A regimental history compiled in 1879 by a captain in the Kings Royal Rifle Corps, states that in November 1755 Parliament voted the sum of £81,000 for the purpose of raising a regiment of four battalions, each one thousand strong for service in British North America. Parliament approved “An Act to enable His Majesty to grant commissions to a certain number of foreign Protestants, who have served abroad as officers or engineers, to act and rank as officers or engineers in America only under certain restrictions and regulations.” The Earl of Loudoun, who as commander-in-chief of the Forces in North America, was appointed colonel-in-chief of the regiment. About fifty officers’ commissions were given to Germans and Swiss, and none were allowed to rise above the rank of lieutenant-colonel.
According to a modern history of the regiment, the idea for creating this unique force was proposed by Jacques Prevost, a Swiss soldier and adventurer who was a friend of the Duke of Cumberland (William, who was the King's second son and was Commander-in-Chief of the Forces.) Prevost recognised the need for soldiers who understood forest warfare, unlike the regulars who were brought to America in 1755 by General Braddock.
The regiment was intended to combine the characteristics of a colonial corps with those of a foreign legion. Swiss and German forest fighting experts, American colonists and British volunteers from other British regiments were recruited. These men were Protestants, an important consideration for fighting against the predominantly Catholic French. The officers were also recruited from Europe – not from the American colonies – and consisted of English, Scots, Irish, Dutch, Swiss and Germans. It was the first time foreign officers were commissioned as British Army officers. The total regiment consisted of 101 officers, 240 non-commissioned officers and 4,160 enlisted men. The battalions were raised on Governors Island, New York. The regiment was renumbered the 60th (Royal American) Regiment in February 1757 when the 50th (Shirley's) and 51st (Pepperel's) foot regiments were removed from the British Army roll after their surrender at Fort Oswego.
Among the distinguished foreign officers given commissions in the 60th (Royal Americans) was Henri Bouquet, a Swiss citizen, whose ideas on tactics, training and man-management (including the unofficial introduction of the rifle and 'battle-dress') would become universal in the British Army some 150 years later. Bouquet was commanding officer of the 1st battalion, and with his fellow battalion commanders, set about creating units that were better suited to warfare in the forests and lakes of northeast America. The Royal Americans represented an attempt to produce a more able soldier who was encouraged to use his initiative while retaining the discipline that was noticeably lacking in the irregular units of colonial Rangers that were being raised at the same time.
The new regiment fought at Louisbourg in 1758, the Cape Sable Campaign and Quebec in 1759 in the campaign which finally wrested Canada from France; at Quebec it won from General James Wolfe the motto (Swift and Bold). These were conventional battles on the European model, but fighting during Pontiac's Rebellion in 1763 was of a very different character. The frontier war threatened the British control of North America. The new regiment at first lost several outlying garrisons but finally proved its mastery of forest warfare under Bouquet's leadership at the victory of Bushy Run.
The 60th were uniformed and equipped in a similar manner to other British regiments with red coats and cocked hats or grenadier caps, but on campaign, swords were replaced with hatchets, and coats and hats cut down for ease of movement in the woods.
Napoleonic Wars.
During the Napoleonic Wars the regiment saw action in the Peninsular War. The first four battalions had been raised as regular line battalions, but in 1797 a 5th battalion had been raised at Cowes on the Isle of Wight and equipped entirely with the Baker rifle, and wore green jackets with red facings. The mixing of rifle troops and muskets proved so effective that eventually line battalion light companies were replaced with rifle companies. The line battalions found themselves in several different theatres, including the West Indies. The rifle battalion was soon joined by a second, and these found themselves in the Peninsula with Wellington's army, serving along with the 95th Rifles, and the King's German Legion rifle units. A 7th battalion was eventually raised as a rifle battalion specifically for service in the American War of 1812.
After the Napoleonic Wars the regiment received a new title: first, in 1815, its name was changed to The Duke of York's Own Rifle Corps and then, in 1830, to the King's Royal Rifle Corps (KRRC). In 1858 the Rifle Depot at Winchester was made their headquarters. The regiment served in the 1882 Anglo-Egyptian War. During the rest of the 1800s the unit also was active in China, Canada (Wolseley Expedition), Afghanistan, India, Burma and South Africa. The regiment was deployed during the Second Boer War from the outset playing a key role in the first battle at Talana Hill. Two officers from the regiment were awarded the Victoria Cross; Lieutenant Frederick Hugh Sherston Roberts and Lieutenant Llewelyn Alberic Emilius Price-Davies. Private Frederick Corbett also receive the Victoria Cross for his action at Kafr Dowar, Egypt, on 5 August 1882; his VC was later rescinded when he was convicted of embezzlement, theft, and being absent without leave.
First World War.
In the First World War the KRRC was expanded to twenty-eight battalions and saw much action on the Western Front, Macedonia and Italy with sixty battle honours awarded. 12,840 men of the regiment were killed while seven members received the Victoria Cross and over 2,000 further decorations were awarded.
After 1918 the unit returned to garrison duties in India, iPalestine and Ireland. In 1922 the regiment was reduced from four to two battalions with the third and fourth being disbanded. In 1926 the Regiment was reorganised as one of the first mechanised infantry regiments.
Second World War.
In the Second World War after initial deployment to France as part of the BEF, the regiment lost two battalions (2nd Bn KRRC and 1st Bn, Queen Victoria's Rifles, a Territorial Army unit) at the defence of Calais, where a Green Jacket Brigade held up the German advance and enabled the Dunkirk evacuation.
In the Mediterranean theatre, the KRRC saw action in the Greek campaign, including the Battle of Vevi and Battle of Crete, where 9th Battalion, The Rangers, served with 1st Armoured Brigade Group. It later served in Italy and Austria.
Following the D-Day landings, the 1st Battalion served in the 4th Armoured Brigade at the Battle of Arnhem (when the brigade failed to link up with the 1st Airborne Division). Rifleman John Beeley was awarded a posthumous Victoria Cross for his actions during Operation Crusader. The regiment was awarded 41 battle honours for service in World War II. Following the war's end the unit was deployed in Germany.
Royal Green Jackets.
In 1948, for administrative purposes the KRRC was brigaded with the Ox & Bucks Light Infantry and the Rifle Brigade to form the Green Jackets Brigade.
In 1958 the Regiment was re-titled the 2nd Green Jackets, the King's Royal Rifle Corps, as were the two other regiments of the Green Jackets Brigade, re-titled 1st and 3rd Green Jackets respectively. In 1966 the three regiments were amalgamated to form the three battalions of the Royal Green Jackets Regiment (RGJ). In 1992 the 1st Battalion, Royal Green Jackets was disbanded, and the KRRC were renumbered the 1st Battalion, with the 3rd Battalion (former Rifle Brigade) becoming the 2nd Battalion. In 2007, the two-battalion RGJ regiment was amalgamated with the remaining Light Infantry regiments, to form the five Regular and two Territorial battalions of the Rifles.
The regiment's traditions are preserved by the 2nd Battalion, The Rifles, which is a redesignation of the 1st Battalion, Royal Green Jackets.
Territorial Battalions.
In World War II these territorial battalions were made formally part of the KRRC as follows:
Cadet Battalions.
Over the years the formation of the cadet battalions was changed regularly, due to the changes to do with rules and commanding officer.
The 1st Cadet Battalion owes its foundation to the Reverend Freeman Wills, who
was commissioned into the Volunteer Army in the rank of Captain on 26 July 1890.
He was also Vicar of St Agatha’s just behind Sun Street, Finsbury Square. On
receiving his commission he decided to form a cadet company within the 1st Cadet
Battalion, The Royal West Surrey Regiment. The Company quickly expanded to
become the 2nd Cadet Battalion, The Royal West Surrey Regiment, at which point
he moved the Battalion Headquarters to No. 2 Finsbury Square (and in 1904 to 42
Sun Street, which he had specially built for the purpose). In 1894 he applied to
HRH Prince George, Duke of Cambridge, Colonel-in-Chief, to affiliate to the
Regiment, with the title of 1st Cadet Battalion, The King’s Royal Rifle Corps.
Consent was granted on 8 November 1894 and the Battalion has remained a part of
the Regiment ever since.
In the days of its foundation Cadet battalions were privately organized and
funded. On becoming a part of The King’s Royal Rifle Corps subscriptions began
to flow in and after the commanding officer had spent nearly £1,000, the Battalion
was placed on a financial basis, which many Volunteer Corps would have envied.
There were to be many ups and down in later years, especially when recognition of
the Cadet Force was withdrawn between the two World Wars, but fortunately the
enthusiasm and commitment of those involved consistently triumphed over the
parsimony of Governments.
In 1900, when volunteers were urgently needed for the South African War, The
Commanding Officer, Colonel Freeman Croft-Wills persuaded the War Office to accept a
Company of the older Cadets, principally N.C.O.s (Non-Commissioned Officers), the
company being enrolled in the City Imperial Volunteers. Around 100 cadets thus served
in South Africa with this unit, whilst other Cadets and ex-Cadets served in the R.A.M.C.
(Royal Army Medical Corps), and other units.
Four were killed in action, one serving with the 1st KRRC at the battle of Dundee, and the
others with units of the C.I.V.s. Their comrades erected brass plaques in their memory in
the Drill Hall at Sun Street. These are now displayed in the Cadet Company Office here
at Davies Street.
In recognition of this service, King Edward VII granted the Battalion the honour of wearing
on its accoutrements the Battle Honour “South Africa 1900-1902” (Army Order 151 of
1905). The announcement of this privilege was made to the Battalion by His late Majesty,
King George V, then Prince of Wales, when, accompanied by Her Majesty Queen Mary,
he distributed the prizes at the Guild Hall in the City of London.
The 1st Cadet Battalion KRRC are the only Cadet Unit in the United Kingdom to have
been granted such an honour and are permitted to wear the miniature 60th Cap Badge
with the single Battle Honour, and call their Cadets “Riflemen”.
The 2nd Cadet Battalion, The King’s Royal Rifle Corps was formed in 1942 when a Home Guard instruction was issued ordering each Home Guard battalion to raise a cadet unit. Lieutenant-Colonel R.L. Clark of Queen Victoria’s Rifles was given the task and on 15 May the Queen Victoria’s Rifles Cadet Corps was born.
Over the next three years the unit expanded to five companies, which in April 1945 led to it being re-titled the 2nd Cadet Battalion, The King’s Royal Rifle Corps. In1951 the 1st and 2nd Cadet Battalion were amalgamated. This resulted in the disposal of the Headquarters of the 1st Cadet Battalion at 42 Sun Street. In 1954 the Battalion office of the ‘new’ 1st Cadet Battalion was established at 56 Davies Street, where it remains to this day.
Today the KRRC 1st Cadet Battalion still exists,with the following units making up the Battalion:

</doc>
<doc id="17333" url="https://en.wikipedia.org/wiki?curid=17333" title="Khoisan languages">
Khoisan languages

The Khoisan languages (; also "Khoesan" or "Khoesaan") are the languages of Africa that have click consonants but do not belong to other language families. For much of the 20th century they were thought to have a genealogical relationship with each other, but this is no longer accepted.
All Khoisan languages but two are indigenous to southern Africa, and belong to three language families, of which the Khoi family appears to have migrated to southern Africa not long before the Bantu expansion. Ethnically, their speakers are the Khoikhoi and the San (Bushmen). Two languages of east Africa, those of the Sandawe and Hadza, are also called Khoisan, although their speakers are ethnically neither Khoikhoi nor San.
Before the Bantu expansion, Khoisan languages, or languages like them, were likely spread throughout southern and eastern Africa. They are currently restricted to the Kalahari Desert, primarily in Namibia and Botswana, and to the Rift Valley in central Tanzania.
Most of the languages are endangered, and several are moribund or extinct. Most have no written record. The only widespread Khoisan language is Khoekhoe ("Nàmá") of Namibia, with a quarter of a million speakers; Sandawe in Tanzania is second in number with some 40–80,000, some monolingual; and the !Kung language of the northern Kalahari is spoken by some 15,000 or so people. Language use is quite strong among the 20,000 speakers of Naro, half of whom speak it as a second language.
Khoisan languages are best known for their use of click consonants as phonemes. These are typically written with characters such as ǃ and ǂ. Clicks are quite versatile as consonants, as they involve two articulations of the tongue which can operate partially independently. Consequently, the languages with the greatest numbers of consonants in the world are Khoisan. The Juǀʼhoan language has 48 click consonants, among nearly as many non-click consonants, strident and pharyngealized vowels, and four tones. The ǃXóõ and ǂHõã languages are even more complex.
Grammatically, the southern Khoisan languages are generally fairly analytic, having several inflectional morphemes, but not as many as in the languages of Tanzania for example.
Validity.
Khoisan was proposed as one of the four families of African languages in Greenberg's classification (1949–1954, revised in 1963). However, linguists who study Khoisan languages reject their unity, and the name "Khoisan" is used by them as a term of convenience without any implication of linguistic validity, much as "Papuan" and "Australian" are. It has been suggested that the similarities of the Tuu and Kx'a families are due to a southern African Sprachbund rather than a genealogical relationship, whereas the Khoe (or perhaps Kwadi–Khoe) family is a more recent migrant to the area, and may be related to Sandawe in East Africa.
E.O.J. Westphal is known for his early rejection of the Khoisan language family (Starostin 2003). Bonny Sands (1998) concluded that the family is not demonstrable with current evidence. Anthony Traill at first accepted Khoisan (Traill 1986), but by 1998 concluded that it could not be demonstrated with current data and methods, rejecting it as based on a single typological criterion: the presence of clicks. Dimmendaal (2008) summarized the general view with, "it has to be concluded that Greenberg's intuitions on the genetic unity of Khoisan could not be confirmed by subsequent research. Today, the few scholars working on these languages treat the three groups as independent language families that cannot or can no longer be shown to be genetically related" (p. 841). Starostin (2013) accepts a relationship between Sandawe and Khoi is plausible, as is one between Tuu and Kx'a, but sees no indication of a relationship between these two groups or with Hadza.
Families.
The putative branches of Khoisan are often considered independent families, in the absence of a demonstration that they are related according to the standard comparative method.
See Khoe languages for speculations on the linguistic history of the region.
Hadza.
With about 800 speakers in Tanzania, Hadza appears to be unrelated to any other language; genetically, the Hadza people are unrelated to the Khoisan peoples of Southern Africa, and their closest relatives may be among the Pygmies of Central Africa.
Sandawe.
There is some indication that Sandawe (about 40,000 speakers in Tanzania) may be related to the Khoe family, such as a congruent pronominal system and some good Swadesh-list matches, but not enough to establish regular sound correspondences. Sandawe is not related to Hadza, despite their proximity.
Khoe.
The Khoe family is both the most numerous and diverse family of Khoisan languages, with seven living languages and over a quarter million speakers. Although little Kwadi data is available, proto-Kwadi–Khoe reconstructions have been made for pronouns and some basic vocabulary. 
A Haiǁom language is listed in most Khoisan references. A century ago the Haiǁom people spoke a Ju dialect, probably close to ǃKung, but they now speak a divergent dialect of Nama. Thus their language is variously said to be extinct or to have 18,000 speakers, to be Ju or to be Khoe. (Their numbers have been included under Nama above.) They are known as the "Saa" by the Nama, and this is the source of the word "San".
Tuu.
The Tuu family consists of two language clusters, which are related to each other at about the distance of Khoekhoe and Tshukhwe within Khoe. They are typologically very similar to the Kx'a languages (below), but have not been demonstrated to be related to them genealogically (the similarities may be an areal feature).
Kx'a.
The Kx'a family is a relatively distant relationship formally proposed in 2010.
Other "click languages".
Not all languages using clicks as phonemes are considered Khoisan. Most others are neighboring Bantu languages in southern Africa: the Nguni languages (Xhosa, Zulu, Swazi, Phuthi, and Northern Ndebele); Sotho; Yeyi in Botswana; and Mbukushu, Kwangali, and Gciriku in the Caprivi Strip. Clicks are spreading to a few additional neighboring languages. Of these languages, Xhosa, Zulu, Ndebele and Yeyi have intricate systems of click consonants; the others, despite the click in the name "Gciriku," more rudimentary ones. There is also the South Cushitic language Dahalo in Kenya, which has dental clicks in a few score words, and an extinct and presumably artificial Australian ritual language called Damin, which had only nasal clicks.
The Bantu languages adopted the use of clicks from neighboring, displaced, or absorbed Khoisan populations (or from other Bantu languages), often through intermarriage, while the Dahalo are thought to have retained clicks from an earlier language when they shifted to speaking a Cushitic language; if so, the pre-Dahalo language may have been something like Hadza or Sandawe. Damin is an invented ritual language, and has nothing to do with Khoisan.
These are the only languages known to have clicks in normal vocabulary. Occasionally other languages are said to have "click" sounds. This is usually a misnomer for ejective consonants, which are found across much of the world, or is a reference to paralinguistic use of clicks such as English "tsk! tsk!".

</doc>
<doc id="17334" url="https://en.wikipedia.org/wiki?curid=17334" title="Katina Paxinou">
Katina Paxinou

Katina Paxinou (; born Aikaterini Konstantopoulou (); 17 December 1900 – 22 February 1973) was a Greek film and stage actress.
She started her stage career in Greece in 1928 and was one of the founding members of the National Theatre of Greece in 1932. The outbreak of World War II found her in the United Kingdom and she later moved to the United States, where she made her film debut in "For Whom the Bell Tolls" (1943) and won the Academy Award for Best Supporting Actress and the Golden Globe Award for Best Supporting Actress.
Paxinou appeared in a few more Hollywood films, before returning to Greece in the early 1950s. She then focused on her stage career and appeared in European films. She died in 1973, after a long-term battle with cancer.
Early life.
Paxinou was born Aikaterini Konstantopoulou (Αικατερίνη Κωνσταντοπούλου) in Piraeus, Greece, she trained as an opera singer at the Conservatoire de Musique de Genève and later in Berlin and Vienna. According to her biography in "Playbill" (1942), Paxinou was disowned by her family after she decided to seek a permanent stage career.
Career.
Paxinou made her debut at the Municipal Theatre of Piraeus in 1920 in the operatic version of Maurice Maeterlinck's "Sister Beatrice", with a score by Dimitri Mitropoulos. She first appeared in a play in 1928, as a member of Marika Kotopouli's troupe, in an Athens production of Henry Bataille's "The Naked Woman". In 1931, she joined Aimilios Veakis' troupe along with Alexis Minotis, where she translated and appeared in the first of Eugene O'Neill's plays to be staged in Greece, "Desire Under the Elms". She also appeared in Anton Chekhov's "Uncle Vanya" and August Strindberg's "The Father".
In 1932, Paxinou was among the actors that inaugurated the recently re-founded National Theatre of Greece, where she worked until 1940. During her stay in the National Theatre, she distinguished herself on Greek stage starring in major plays, such as Sophocles' "Electra", Henrik Ibsen's "Ghosts" and William Shakespeare's "Hamlet", which were also performed in London, Frankfurt and Berlin.
When World War II broke out, Paxinou was performing in London. Unable to return to Greece, she emigrated to the United States, where she had earlier appeared in 1931, performing Clytemnestra in a modern Greek version of "Electra".
She was selected to play the role of Pilar in the film "For Whom the Bell Tolls" (1943), for which she won an Oscar and a Golden Globe Award for Best Supporting Actress - Motion Picture. She made one British film, "Uncle Silas" (1947), which features Jean Simmons in the main female role and, worked in Italy for 20th Century Fox, played the mother of Tyrone Power's character in "Prince of Foxes" (1949). After that film, Paxinou worked for a Hollywood studio only once more, to again play a gypsy woman, this time in the religious epic, "The Miracle" (1959).
In 1950, Paxinou resumed her stage career. In her native Greece, she formed the Royal Theatre of Athens with Alexis Minotis, her principal director and her husband since 1940.
Paxinou made several appearances on the Broadway stage and television as well. She played the lead in Ibsen's "Hedda Gabler" for 12 performances at New York City's Longacre Theatre, opening on June 28, 1942. She also played the principal role in the first production in English of Federico Garcia Lorca's "The House of Bernarda Alba", at the ANTA Playhouse in New York in 1951, and a BBC television production of Lorca's "Blood wedding" ("Bodas de sangre"), broadcast on June 2, 1959.
Death.
Paxinou died from cancer in Athens in 1973 at the age of 72. She was survived by her husband, and her one daughter from her first marriage to Ioannis Paxinos, whose surname she had been using after their divorce. Her remains are buried at First Cemetery of Athens.
Museum.
The Paxinou-Minotis Museum is a museum in Athens, Greece featuring memorabilia of the life of Paxinou, an academy award winner Greek film and theatre actress. Items include her furniture, paintings and sketches, photographs, books and personal effects. The items were donated by director Alexis Minotis, Patinou's husband, and include his personal library and theatrical archive.

</doc>
<doc id="17335" url="https://en.wikipedia.org/wiki?curid=17335" title="Klaus Barbie">
Klaus Barbie

Nikolaus "Klaus" Barbie (25 October 1913 – 23 September 1991) was an SS-"Hauptsturmführer" (rank equivalent to army captain) and Gestapo member. He was known as the "Butcher of Lyon" for having personally tortured French prisoners of the Gestapo while stationed in Lyon, France. After the war, United States intelligence services employed him for their anti-Marxist efforts, and also helped him escape to South America. The "Bundesnachrichtendienst", (the West German intelligence agency), recruited him, and he may have helped the CIA capture Argentine revolutionary Che Guevara in 1967. Barbie is suspected of having had a hand in the Bolivian "coup d'état" orchestrated by Luis García Meza Tejada in 1980. After the fall of the dictatorship, Barbie no longer had the protection of the Bolivian government, and in 1983 was extradited to France, where he was convicted of crimes against humanity and died in prison of cancer.
Early life and education.
Nikolaus "Klaus" Barbie was born on 25 October 1913 in Godesberg, later renamed Bad Godesberg, which is today part of Bonn. The Barbie family came from Merzig, in the Saar near the French border. His patrilineal ancestors were likely French Roman Catholics named Barbier who had left France at the time of the French Revolution. In 1914, his father, also named Nickolaus, was conscripted to fight in the First World War. He returned an angry, bitter man. Wounded in the neck at Verdun and captured by the French, whom he hated, he never recovered his health. He became an alcoholic who abused his children. Until 1923, when he was 10, Klaus Barbie attended the local school where his father taught. Afterward, he attended a boarding school in Trier, and was relieved to be away from his abusive father. In 1925, the entire Barbie family moved to Trier.
In June 1933, Barbie's younger brother, Kurt, died at the age of eighteen of chronic illness. Later that year, their father died. The death of his father derailed plans for the 20-year-old Barbie to study theology, or otherwise become an academic, as his peers had expected. While unemployed, Barbie was conscripted into the Nazi labour service, the "Reichsarbeitsdienst". On 26 September 1935, aged 22, he joined the SS (member 272,284), and began working in the "Sicherheitsdienst" (SD), the SS security service, which acted as the intelligence-gathering arm of the Nazi Party. On 1 May 1937, he became member 4,583,085 of the Nazi Party. In April 1939, Barbie became engaged to Regina Margaretta Willms, the 23-year-old daughter of a postal clerk.
Second World War.
After the German conquest and occupation of the Netherlands, Barbie was assigned to Amsterdam. In 1942, he was sent to Dijon, France, in the Occupied Zone. In November of the same year, at the age of 29, he was assigned to Lyon as the head of the local Gestapo. He established his headquarters at the Hôtel Terminus in Lyon, where he personally tortured prisoners: men, women, and children alike,—breaking extremities, using electroshock, and sexually abusing them (including with dogs), among other methods. He became known as the "Butcher of Lyon". In Marcel Ophüls's Oscar-winning documentary film, "", the daughter of a French Resistance leader based in Lyon recounts her father's torture by Barbie – her father was beaten, skinned alive, and his head immersed in a bucket of ammonia; he died shortly after.
Historians estimate that Barbie was directly responsible for the deaths of up to 14,000 people. He arrested Jean Moulin, one of the highest-ranking members of the French Resistance and his most prominent enemy figure. In 1943, he was awarded the "Knight's Cross of the Iron Cross with Swords" for his campaign against the French Resistance, and the capture of Moulin by Adolf Hitler.
In April 1944, Barbie ordered the deportation to Auschwitz of a group of 44 Jewish children from an orphanage at Izieu. After his operations in Lyon, he rejoined the SiPo-SD of Lyon in Bruyères, where he led an anti-partisan attack in Rehaupal in September 1944.
US intelligence and Bolivia.
In 1947, Barbie was recruited as an agent for the 66th Detachment of the U.S. Army Counter Intelligence Corps (CIC). The U.S. used Barbie and other Nazi Party members to further anti-Communist efforts in Europe. Specifically, they were interested in British interrogation techniques, which Barbie had experienced firsthand, and the identities of SS officers that the British were using for their own ends. Later, the CIC housed him in a hotel in Memmingen, and he reported on French intelligence activities in the French zone of occupied Germany because they felt the French were infiltrated with Communists.
The French discovered that Barbie was in U.S. hands and, having sentenced him to death "in absentia" for war crimes, made a plea to John J. McCloy, U.S. High Commissioner for Germany, to hand him over for execution, but McCloy allegedly refused. Instead, the CIC allegedly helped him flee to Bolivia with the help of "ratlines" organized by U.S. intelligence services, and Croatian Roman Catholic clergy, including Father Krunoslav Draganović. The CIC asserted that Barbie knew too much about the network of German spies the CIC had planted in various European Communist organizations, and were suspicious of the Communist influence within the French government, but their protection of Barbie may have been as much to avoid the embarrassment of having recruited him.
In 1965, Barbie was recruited by the West German foreign intelligence agency "Bundesnachrichtendienst" (BND), under the codename "Adler" ("Eagle") and the registration number V-43118. His initial monthly salary of 500 Deutsche Mark was transferred in May 1966 to an account of the Chartered Bank of London in San Francisco. During his time with the BND, Barbie made at least 35 reports to the BND headquarters in Pullach. 
Barbie emigrated to Bolivia, where he lived under the alias, Klaus Altmann. He had less embarrassment being employed there than in Europe, and enjoyed excellent relations with high-ranking Bolivian officials, including Bolivian dictators Hugo Banzer and Luis García Meza Tejada. "Altmann" was known for his nationalist and anti-communist stances. While conducting his arms trade operations in Bolivia, he was appointed to the rank of Lt. Colonel within the Bolivian Armed Forces.
Extradition, trial and death.
Barbie was identified as living in Bolivia in 1971 by the Klarsfelds (Nazi hunters from France). The Testimony of the Italian insurgent Stefano Delle Chiaie before the Italian Parliamentary Commission on Terrorism suggests that Barbie took part in the "Cocaine Coup" of Luis García Meza Tejada, when the regime forced its way to power in Bolivia in 1980. On 19 January 1983, the newly elected government of Hernán Siles Zuazo arrested Barbie and extradited him to France to stand trial.
In 1984, Barbie was indicted for crimes committed while he directed the Gestapo in Lyon between 1942 and 1944. The jury trial started on 11 May 1987, in Lyon, before the Rhône "Cour d'assises". Unusually, the court allowed the trial to be filmed because of its historical value. A special court room with seating for an audience of about 700 was constructed. The head prosecutor was Pierre Truche.
At the trial, Barbie was supported by financier François Genoud, and defended by the lawyer Jacques Vergès. Barbie was tried on 41 separate counts of crimes against humanity, based on the depositions of 730 Jews and resistance figures, who cited his torture practices and murders. 
The father of then-French Minister for Justice, Robert Badinter, had died in Sobibor after being deported from Lyon during Barbie's tenure.
Barbie gave his name as Klaus Altmann (the name he used while in Bolivia). Claiming his extradition was technically illegal, he asked to be excused from the trial and returned to his cell at Prison Saint-Paul. This was granted. He was brought back to court on 26 May 1987 to face some of his accusers, during which time he stated that he had "nothing to say". 
Vergès had a reputation for attacking the French political system, particularly in the historic French colonial empire. His strategy was to use the trial to talk about war crimes committed by France since 1945. This had less to do with the trial than with Verges' desire to undermine the French Fifth Republic. The prosecution dropped some of the charges against Barbie due to French legislation that had protected French citizens accused of the same crimes under the Vichy regime and in French Algeria. Vergès tried to argue that Barbie's actions were no worse than the supposedly ordinary actions of colonialists worldwide, and that his trial was selective prosecution. During his trial, Barbie said, "When I stand before the throne of God I shall be judged innocent".
The court did not accept the defence argument. On 4 July 1987, Barbie was convicted and sentenced to life imprisonment. He died in prison in Lyon of leukemia and cancer of the spine and prostate four years later, at the age of 77.
References.
Further reading

</doc>
<doc id="17337" url="https://en.wikipedia.org/wiki?curid=17337" title="Kashmir">
Kashmir

Kashmir is a geographical region situated in northern part of South Asia. Throughout the course of the history the geographical extent of the kingdom of Kashmir kept changing, however by the mid-19th century, the term "Kashmir" geographically denoted only the valley between the Great Himalayas and the Pir Panjal mountain range. Today, it denotes a larger area that includes the Indian administered territories of Jammu and Kashmir (which consists of Jammu, the Kashmir Valley, and Ladakh), the Pakistan administered territories of Azad Kashmir and Gilgit–Baltistan, and the Chinese-administered regions of Aksai Chin and the Trans-Karakoram Tract.
Although Jammu is part of the disputed Kashmir region, it is not geographically part of the Kashmir valley nor the Ladakh region. The Jammu Division is inhabited by the Dogra people who are historically, culturally, linguistically, and geographically connected with the Punjab region and the "Pahari" regions of the former Punjab Hills States that now comprise the state of Himachal Pradesh.
In the first half of the 1st millennium, the Kashmir region became an important centre of Kambojas and later of Buddhism; later still, in the ninth century, Kashmir Shaivism arose. In 1339, Shah Mir became the first Muslim ruler of Kashmir, inaugurating the "Salatin-i-Kashmir" or Swati dynasty. For the next five centuries, Muslim monarchs ruled Kashmir, including the Mughals, who ruled from 1586 until 1751, and the Afghan Durrani Empire, which ruled from 1751 until 1820. That year, the Sikhs, under Ranjit Singh, annexed Kashmir. In 1846, after the Sikh defeat in the First Anglo-Sikh War, and upon the purchase of the region from the British under the Treaty of Amritsar, the Raja of Jammu, Gulab Singh, became the new ruler of Kashmir. The rule of his descendants, under the "paramountcy" (or tutelage) of the British Crown, lasted until 1947, when the former princely state of the British Indian Empire became a disputed territory, now administered by three countries: India, Pakistan, and the People's Republic of China.
Etymology.
The Sanskrit word for Kashmir was ("") and, as with many ancient toponyms, its source and original meaning remain unknown. Kashmir was archaically spelled "Cashmere" in English.
Over the centuries, various "Puranas" linked the word ' to the name of the mythical sage Kashyapa. The word ' was thus said to be a contraction of ' meaning "Kashyapa's sea" (and the Kashmir Valley is then claimed to have formerly been a lake) or, alternately, ', or "Kashyapa's mountain".
History.
Hinduism and Buddhism in Kashmir.
Since medieval times, Kashmir has been an important centre for the development of a Buddhist-Hinduist syncretism, in which Madhyamaka and Yogacara were blended with Saivism and Advaita Vedanta. The Buddhist Mauryan emperor Ashoka is often credited with having founded the old capital of Kashmir, Shrinagari, now ruins on the outskirts of modern Srinagar. Kashmir was long to be a stronghold of Buddhism. As a Buddhist seat of learning, the Sarvāstivādan school strongly influenced Kashmir. East and Central Asian Buddhist monks are recorded as having visited the kingdom. In the late 4th century CE, the famous Kuchanese monk Kumārajīva, born to an Indian noble family, studied Dīrghāgama and Madhyāgama in Kashmir under Bandhudatta. He later became a prolific translator who helped take Buddhism to China. His mother Jīva is thought to have retired to Kashmir. Vimalākṣa, a Sarvāstivādan Buddhist monk, travelled from Kashmir to Kucha and there instructed Kumārajīva in the "Vinayapiṭaka".
According to tradition, Adi Shankara visited the pre-existing "" (Sharada Peeth) in Kashmir in the late 8th century or early 9th century CE. The "Madhaviya Shankaravijayam" states this temple had four doors for scholars from the four cardinal directions. The southern door (representing South India) had never been opened, indicating that no scholar from South India had entered the Sarvajna Pitha. According to tradition, Adi Shankara opened the southern door by defeating in debate all the scholars there in all the various scholastic disciplines such as Mimamsa, Vedanta and other branches of Hindu philosophy; he ascended the throne of Transcendent wisdom of that temple.
Abhinavagupta (c. 950–1020 CE) was one of India's greatest philosophers, mystics and aestheticians. He was also considered an important musician, poet, dramatist, exeget, theologian, and logician – a polymathic personality who exercised strong influences on Indian culture. He was born in the Kashmir Valley in a family of scholars and mystics and studied all the schools of philosophy and art of his time under the guidance of as many as fifteen (or more) teachers and gurus. In his long life he completed over 35 works, the largest and most famous of which is Tantrāloka, an encyclopaedic treatise on all the philosophical and practical aspects of Trika and Kaula (known today as Kashmir Shaivism). Another one of his very important contributions was in the field of philosophy of aesthetics with his famous Abhinavabhāratī commentary of Nāṭyaśāstra of Bharata Muni.
In the 10th century "Moksopaya" or "Moksopaya Shastra", a philosophical text on salvation for non-ascetics ("moksa-upaya": 'means to release'), was written on the Pradyumna hill in Śrīnagar. It has the form of a public sermon and claims human authorship and contains about 30,000 shloka's (making it longer than the "Ramayana"). The main part of the text forms a dialogue between Vasistha and Rama, interchanged with numerous short stories and anecdotes to illustrate the content. This text was later (11th to the 14th century CE) expanded and vedanticised, which resulted in the "Yoga Vasistha".
Muslim rule.
Shams-ud-Din Shah Mir (reigned 1339–42) was a ruler of Kashmir and the founder of the Shah Miri dynasty named after him. Sams'd-Din (ruled 1339-1342) also Dhams-ud-din and Shah Mir, was the first Muslim ruler of Kashmir and founder of the Shah Mir Dynasty. Jonaraja, in his "Dvitīyā Rājataraṅginī" mentioned him as Sahamera. He came from sawat according to some sources. However, Jonaraja a credible historian informs us that Shahmir was not from Swat so some historians say he was not from Swat but was a Kshatriya descended from Arjuna whose ancestors had taken up Islam.
Rinchan from Ladakh, and Lankar Chak from Dard territory near Gilgit came to Kashmir, and played a notable role in the subsequent political history of the valley. All the three men were granted Jagirs by the King Rinchan for three years became the ruler of Kashmir, Shah Mir was the first rular of Shah mir dynasty, which had established in 1339.
Sikh rule.
In 1819, the Kashmir valley passed from the control of the Durrani Empire of Afghanistan, and four centuries of Muslim rule under the Mughals and the Afghans, to the conquering armies of the Sikhs under Ranjit Singh of Lahore. As the Kashmiris had suffered under the Afghans, they initially welcomed the new Sikh rulers. However, the Sikh governors turned out to be hard taskmasters, and Sikh rule was generally considered oppressive, protected perhaps by the remoteness of Kashmir from the capital of the Sikh empire in Lahore. The Sikhs enacted a number of anti-Muslim laws, which included handing out death sentences for cow slaughter, closing down the Jamia Masjid in Srinagar, and banning the "azaan", the public Muslim call to prayer. Kashmir had also now begun to attract European visitors, several of whom wrote of the abject poverty of the vast Muslim peasantry and of the exorbitant taxes under the Sikhs. High taxes, according to some contemporary accounts, had depopulated large tracts of the countryside, allowing only one-sixteenth of the cultivable land to be cultivated. However, after a famine in 1832, the Sikhs reduced the land tax to half the produce of the land and also began to offer interest-free loans to farmers; Kashmir became the second highest revenue earner for the Sikh empire. During this time Kashmiri shawls became known worldwide, attracting many buyers, especially in the West.
Earlier, in 1780, after the death of Ranjit Deo , the Raja of Jammu, the kingdom of Jammu (to the south of the Kashmir valley) was also captured by the Sikhs and afterwards, until 1846, became a tributary to Sikh power. Ranjit Deo's grandnephew, Gulab Singh, subsequently sought service at the court of Ranjit Singh, distinguished himself in later campaigns, especially the annexation of the Kashmir valley, and, for his services, was appointed governor of Jammu in 1820. With the help of his officer, Zorawar Singh, Gulab Singh soon captured for the Sikhs the lands of Ladakh and Baltistan to the east and north-east, respectively, of Jammu.
Dogra rule.
In 1845, the First Anglo-Sikh War broke out. According to the Imperial Gazetteer of India, "Gulab Singh contrived to hold himself aloof till the battle of Sobraon (1846), when he appeared as a useful mediator and the trusted adviser of Sir Henry Lawrence. Two treaties were concluded. By the first the State of Lahore (i.e. West Punjab) handed over to the British, as equivalent for one crore indemnity, the hill countries between the rivers Beas and Indus; by the second the British made over to Gulab Singh for 7.5 million all the hilly or mountainous country situated to the east of the Indus and the west of the Ravi ("i.e." the Vale of Kashmir)."
Drafted by a treaty and a bill of sale, and constituted between 1820 and 1858, the Princely State of Kashmir and Jammu (as it was first called) combined disparate regions, religions, and ethnicities: to the east, Ladakh was ethnically and culturally Tibetan and its inhabitants practised Buddhism; to the south, Jammu had a mixed population of Hindus, Muslims and Sikhs; in the heavily populated central Kashmir valley, the population was overwhelmingly "Sunni" Muslim, however, there was also a small but influential Hindu minority, the Kashmiri brahmins or pandits; to the northeast, sparsely populated Baltistan had a population ethnically related to Ladakh, but which practised "Shi'a" Islam; to the north, also sparsely populated, Gilgit Agency, was an area of diverse, mostly "Shi'a" groups; and, to the west, Punch was Muslim, but of different ethnicity than the Kashmir valley. After the Indian Rebellion of 1857, in which Kashmir sided with the British, and the subsequent assumption of direct rule by Great Britain, the princely state of Kashmir came under the suzerainty of the British Crown.
In the British census of India of 1941, Kashmir registered a Muslim majority population of 77%, a Hindu population of 20% and a sparse population of Buddhists and Sikhs comprising the remaining 3%. That same year, Prem Nath Bazaz, a Kashmiri Pandit journalist wrote: "The poverty of the Muslim masses is appalling. ... Most are landless laborers, working as serfs for absentee landlords ... Almost the whole brunt of official corruption is borne by the Muslim masses." For almost a century until the census, a small Hindu elite had ruled over a vast and impoverished Muslim peasantry. Driven into docility by chronic indebtedness to landlords and moneylenders, having no education besides, nor awareness of rights, the Muslim peasants had no political representation until the 1930s.
1947 and 1948.
Ranbir Singh's grandson Hari Singh, who had ascended the throne of Kashmir in 1925, was the reigning monarch in 1947 at the conclusion of British rule of the subcontinent and the subsequent partition of the British Indian Empire into the newly independent Union of India and the Dominion of Pakistan. According to Burton Stein's "History of India", "Kashmir was neither as large nor as old an independent state as Hyderabad; it had been created rather off-handedly by the British after the first defeat of the Sikhs in 1846, as a reward to a former official who had sided with the British. The Himalayan kingdom was connected to India through a district of the Punjab, but its population was 77 per cent Muslim and it shared a boundary with Pakistan. Hence, it was anticipated that the maharaja would accede to Pakistan when the British paramountcy ended on 14–15 August. When he hesitated to do this, Pakistan launched a guerrilla onslaught meant to frighten its ruler into submission. Instead the Maharaja appealed to Mountbatten for assistance, and the governor-general agreed on the condition that the ruler accede to India. Indian soldiers entered Kashmir and drove the Pakistani-sponsored irregulars from all but a small section of the state. The United Nations was then invited to mediate the quarrel. The UN mission insisted that the opinion of Kashmiris must be ascertained, while India insisted that no referendum could occur until all of the state had been cleared of irregulars."
In the last days of 1948, a ceasefire was agreed under UN auspices. However, since the plebiscite demanded by the UN was never conducted, relations between India and Pakistan soured, and eventually led to two more wars over Kashmir in 1965 and 1999. India has control of about half the area of the former princely state of Jammu and Kashmir, while Pakistan controls a third of the region, the Northern Areas and Kashmir. According to Encyclopædia Britannica, "Although there was a clear Muslim majority in Kashmir before the 1947 partition and its economic, cultural, and geographic contiguity with the Muslim-majority area of the Punjab (in Pakistan) could be convincingly demonstrated, the political developments during and after the partition resulted in a division of the region. Pakistan was left with territory that, although basically Muslim in character, was thinly populated, relatively inaccessible, and economically underdeveloped. The largest Muslim group, situated in the Valley of Kashmir and estimated to number more than half the population of the entire region, lay in Indian-administered territory, with its former outlets via the Jhelum valley route blocked."
Current status and political divisions.
The eastern region of the former princely state of Kashmir is also involved in a boundary dispute that began in the late 19th century and continues into the 21st. Although some boundary agreements were signed between Great Britain, Afghanistan and Russia over the northern borders of Kashmir, China never accepted these agreements, and China's official position has not changed following the communist revolution of 1949 that established the People's Republic of China. By the mid-1950s the Chinese army had entered the north-east portion of Ladakh.
The region is divided amongst three countries in a territorial dispute: Pakistan controls the northwest portion (Northern Areas and Kashmir), India controls the central and southern portion (Jammu and Kashmir) and Ladakh, and the People's Republic of China controls the northeastern portion (Aksai Chin and the Trans-Karakoram Tract). India controls the majority of the Siachen Glacier area, including the Saltoro Ridge passes, whilst Pakistan controls the lower territory just southwest of the Saltoro Ridge. India controls of the disputed territory, Pakistan controls , and the People's Republic of China controls the remaining .
Jammu and Pakistan administered Kashmir lie outside Pir Panjal range, and are under Indian and Pakistani control respectively. These are populous regions. The main cities are Mirpur, Dadayal, Kotli, Bhimber Jammu, Muzaffarabad and Rawalakot. Gilgit–Baltistan, formerly known as the "Northern Areas", is a group of territories in the extreme north, bordered by the Karakoram, the western Himalayas, the Pamir, and the Hindu Kush ranges. With its administrative centre in the town of Gilgit, the Northern Areas cover an area of and have an estimated population approaching 1 million (10 lakhs). The other main city is Skardu.
Ladakh is a region in the east, between the Kunlun mountain range in the north and the main Great Himalayas to the south. Main cities are Leh and Kargil. It is under Indian administration and is part of the state of Jammu and Kashmir. It is one of the most sparsely populated regions in the area and is mainly inhabited by people of Indo-Aryan and Tibetan descent. Aksai Chin is a vast high-altitude desert of salt that reaches altitudes up to . Geographically part of the Tibetan Plateau, Aksai Chin is referred to as the Soda Plain. The region is almost uninhabited, and has no permanent settlements.
Though these regions are in practice administered by their respective claimants, neither India nor Pakistan has formally recognised the accession of the areas claimed by the other. India claims those areas, including the area "ceded" to China by Pakistan in the Trans-Karakoram Tract in 1963, are a part of its territory, while Pakistan claims the entire region excluding Aksai Chin and Trans-Karakoram Tract. The two countries have fought several declared wars over the territory. The Indo-Pakistani War of 1947 established the rough boundaries of today, with Pakistan holding roughly one-third of Kashmir, and India one-half, with a dividing line of control established by the United Nations. The Indo-Pakistani War of 1965 resulted in a stalemate and a UN-negotiated ceasefire.
Demographics.
In the 1901 Census of the British Indian Empire, the population of the princely state of "Kashmir and Jammu" was 2,905,578. Of these, 2,154,695 (74.16%) were Muslims, 689,073 (23.72%) Hindus, 25,828 (0.89%) Sikhs, and 35,047 (1.21%) Buddhists (implying 935 (0.032%) others).
Among the Muslims of the princely state, four divisions were recorded: "Shaikhs, Saiyids, Mughals, and Pathans. The Shaikhs, who are by far the most numerous, are the descendants of Hindus, but have retained none of the caste rules of their forefathers. They have clan names known as "krams" ..." It was recorded that these "kram" names included "Tantray", "Shaikh", "Bat", "Mantu", "Ganai", "Dar", "Damar", "Lon", etc. The Saiyids were found to be the second most numerous group, it was recorded that they "could be divided into those who follow the profession of religion and those who have taken to agriculture and other pursuits. Their "kram" name is 'Mir.' While a Saiyid retains his saintly profession Mir is a prefix; if he has taken to agriculture, Mir is a suffix to his name." The "Mughals" who were not numerous were recorded to have "kram" names like "Mir" (a corruption of "Mirza"), "Beg", "Bandi", "Bach" and "Ashaye". Finally, it was recorded that the Pathans "who are more numerous than the Mughals, ... are found chiefly in the south-west of the valley, where Pathan colonies have from time to time been founded. The most interesting of these colonies is that of Kuki-Khel Afridis at Dranghaihama, who retain all the old customs and speak Pashto." Among the main tribes of Muslims in the princely state are the Butts, Dar, Lone, Jat, Gujjar, Rajput, Sudhan and Khatri. A small number of Butts, Dar and Lone use the title Khawaja and the Khatri use the title Shaikh the Gujjar use the title of Chaudhary. All these tribes are indigenous of the princely state which converted to Islam from Hinduism during its arrival in region.
The Hindus were found mainly in Jammu, where they constituted a little less than 60% of the population. In the "Kashmir Valley", the Hindus represented "524 in every 10,000 of the population ("i.e." 5.24%), and in the frontier "wazarats" of Ladhakh and Gilgit only 94 out of every 10,000 persons (0.94%)." In the same Census of 1901, in the Kashmir Valley, the total population was recorded to be 1,157,394, of which the Muslim population was 1,083,766, or 93.6% and the Hindu population 60,641. Among the Hindus of "Jammu" province, who numbered 626,177 (or 90.87% of the Hindu population of the princely state), the most important castes recorded in the census were "Brahmans (186,000), the Rajputs (167,000), the Khattris (48,000) and the Thakkars (93,000)."
In the 1911 Census of the British Indian Empire, the total population of "Kashmir and Jammu" had increased to 3,158,126. Of these, 2,398,320 (75.94%) were Muslims, 696,830 (22.06%) Hindus, 31,658 (1%) Sikhs, and 36,512 (1.16%) Buddhists. In the last census of British India in 1941, the total population of Kashmir and Jammu (which as a result of the second world war, was estimated from the 1931 census) was 3,945,000. Of these, the total Muslim population was 2,997,000 (75.97%), the Hindu population was 808,000 (20.48%), and the Sikh 55,000 (1.39%).
The Kashmiri Pandits, the only Hindus of the Kashmir valley, who had stably constituted approximately 4 to 5% of the population of the valley during Dogra rule (1846–1947), and 20% of whom had left the Kashmir valley by 1950, began to leave in much greater numbers in the 1990s. According to a number of authors, approximately 100,000 of the total Kashmiri Pandit population of 140,000 left the valley during that decade. Other authors have suggested a higher figure for the exodus, ranging from the entire population of over 150 to 190 thousand (1.5 to 190,000) of a total Pandit population of 200 thousand (200,000) to a number as high as 300 thousand (300,000).
The total population of India's division of Jammu and Kashmir is 12,541,302 and Pakistan's division of Kashmir is 2,580,000 and Gilgit-Baltistan is 870,347.
Culture and cuisine.
Kashmiri cuisine includes dum aloo (boiled potatoes with heavy amounts of spice), tzaman (a solid cottage cheese), rogan josh (lamb cooked in heavy spices), yakhiyn (lamb cooked in curd with mild spices), hakh (a spinach-like leaf), rista-gushtaba (minced meat balls in tomato and curd curry), danival korme, and the signature rice which is particular to Asian cultures. The traditional wazwan feast involves cooking meat or vegetables, usually mutton, in several different ways. Alcohol is strictly prohibited in most places. There are two styles of making tea in the region: Noon Chai, or salt tea, which is pink in colour (known as chinen posh rang or peach flower colour) and popular with locals; and kahwah, a tea for festive occasions, made with saffron and spices (cardamom, cinamon, sugar, noon chai leaves), and black tea.
Economy.
Kashmir's economy is centred around agriculture. Traditionally the staple crop of the valley was rice, which formed the chief food of the people. In addition, Indian corn, wheat, barley and oats were also grown. Given its temperate climate, it is suited for crops like asparagus, artichoke, seakale, broad beans, scarletrunners, beetroot, cauliflower and cabbage. Fruit trees are common in the valley, and the cultivated orchards yield pears, apples, peaches, and cherries. The chief trees are deodar, firs and pines, chenar or plane, maple, birch and walnut, apple, cherry.
Historically, Kashmir became known worldwide when Cashmere wool was exported to other regions and nations (exports have ceased due to decreased abundance of the cashmere goat and increased competition from China). Kashmiris are well adept at knitting and making Pashmina shawls, silk carpets, rugs, kurtas, and pottery. Saffron, too, is grown in Kashmir. Efforts are on to export the naturally grown fruits and vegetables as organic foods mainly to the Middle East. Srinagar is known for its silver-work, papier mache, wood-carving, and the weaving of silk. The economy was badly damaged by the 2005 Kashmir earthquake which, as of 8 October 2005, resulted in over 70,000 deaths in the Pakistan-controlled part of Kashmir and around 1,500 deaths in Indian controlled Kashmir. The Indian-administered portion of Kashmir is believed to have potentially rich rocks containing hydrocarbon reserves.
Transport.
Transport is predominantly by air or road vehicles in the region. Kashmir has a long modern railway line that started in October 2009, and was last extended in 2013 and connects Baramulla in the western part of Kashmir to Srinagar and Banihal. It is expected to link Kashmir to the rest of India after the construction of the railway line from Katra to Banihal is completed.

</doc>
<doc id="17339" url="https://en.wikipedia.org/wiki?curid=17339" title="Kendall Square Research">
Kendall Square Research

Kendall Square Research (KSR) was a supercomputer company headquartered originally in Kendall Square in Cambridge, Massachusetts in 1986, near Massachusetts Institute of Technology (MIT). It was co-founded by Steven Frank and Henry Burkhardt III, who had formerly helped found Data General and Encore Computer and was one of the original team that designed the PDP-8. KSR produced two models of supercomputer, the KSR1 and KSR2.
Technology.
The KSR systems ran a specially customized version of the OSF/1 operating system, a Unix variant, with programs compiled by a KSR-specific port of the Green Hills Software C and FORTRAN compilers. The architecture was shared memory implemented as a cache-only memory architecture or "COMA". Being all cache, memory dynamically migrated and replicated in a coherent manner based on the access pattern of individual processors. The processors were arranged in a hierarchy of rings, and the operating system mediated process migration and device access. Instruction decode was hardwired, and pipelining was used. Each KSR1 processor was a custom 64-bit reduced instruction set computing (RISC) CPU clocked at 20 MHz and capable of a peak output of 20 million instructions per second (MIPS) and 40 million floating-point operations per second (MFLOPS). Up to 1088 of these processors could be arranged in a single system, with a minimum of eight. The KSR2 doubled the clock rate to 40 MHz and supported over 5000 processors. The KSR-1 chipset was fabricated by Sharp Corporation while the KSR-2 chipset was built by Hewlett-Packard.
Software.
Besides the traditional scientific applications, KSR with Oracle Corporation, addressed the massively parallel database market for commercial applications. The KSR-1 and -2 supported Micro Focus COBOL and C/C++ programming languages, and the Oracle PRDBMS and the MATISSE OODBMS from ADB, Inc. Their own product, the KSR Query Decomposer, complemented the functions of the Oracle product for SQL uses. The TUXEDO transaction monitor for OLTP was also provided. The KAP program (Kuck & Associate Preprocessor) provided for pre-processing for source code analysis and parallelization. The runtime environment was termed PRESTO, and was a POSIX compliant multithreading manager.
Hardware.
The KSR-1 processor was implemented as a four-chip set in 1.2 micrometer complementary metal–oxide–semiconductor (CMOS). These chips were: the cell execution unit, the floating point unit, the arithmetic logic unit, and the external I/O unit (XIO). The CEU handled instruction fetch (two per clock), and all operations involving memory, such as loads and stores. 40-bit addresses were used, going to full 64-bit addresses later. The integer unit had 32, 64-bit-wide registers. The floating point unit is discussed below. The XIO had the capacity of 30 MB/s throughput to I/O devices. It included 64 control and data registers.
The KSR processor was a 2-wide VLIW, with instructions of 6 types: memory reference (load and store), execute, control flow, memory control, I/O, and inserted. Execute instructions included arithmetic, logical, and type conversion. They were usually triadic register in format. Control flow refers to branches and jumps. Branch instructions were two cycles. The programmer (or compiler) could implicitly control the "quashing" behavior of the subsequent two instructions that would be initiated during the branch. The choices were: always retain the results, retain results if branch test is true, or retain results if branch test is false. Memory control provided synchronization primitives. I/O instructions were provided. Inserted instructions were forced into a flow by a coprocessor. Inserted load and store were used for direct memory access (DMA) transfers. Inserted memory instructions were used to maintain cache coherency. New coprocessors could be interfaced with the inserted instruction mechanism. IEEE standard floating point arithmetic was supported. Sixty-four 64-bit wide registers were included.
The following example of KSR assembly performs an indirect procedure call to an address held in the procedure's constant block, saving the return address in register codice_1. It also saves the frame pointer, loads integer regstier zero with the value 3, and increments integer register 31 without changing the condition codes. Most instructions have a delay slot of 2 cycles and the delay slots are not interlocked, so must be scheduled explicitly, else the resulting hazard means wrong values are sometimes loaded.
In the KSR design, all of the memory was treated as cache. The design called for no "home" location- to reduce storage overheads and to software transparently, dynamically migrate/replicate memory based on where it was be utilized; A Harvard architecture, separate bus for instructions and memory was used. Each node board contained 256 kB of I-cache and D-cache, essentially primary cache. At each node was 32 MB of memory for main cache. The system level architecture was shared virtual memory, which was physically distributed in the machine. The programmer or application only saw one contiguous address space, which was spanned by a 40-bit address. Traffic between nodes traveled at up to 4 gigabytes per second. The 32 megabytes per node, in aggregate, formed the physical memory of the machine.
Specialized input/output processors could be used in the system, providing scalable I/O. A 1088 node KSR1 could have 510 I/O channels with an aggregate in excess of 15 GB/s. Interfaces such as Ethernet, FDDI, and HIPPI were supported.
History.
As the company scaled up quickly to enter production, they moved in the late 1980s to 170 Tracer Lane, Waltham, Massachusetts.
KSR refocused its efforts from the scientific to the commercial marketplace, with emphasis on parallel relational databases and OLTP operations. It then got out of the hardware business, but continued to market some of its data warehousing and analysis software products.
The first KSR1 system was installed in 1991. With new processor hardware, new memory hardware and a novel memory architecture, a new compiler port, a new port of a relatively new operating system, and exposed memory hazards, early systems were noted for frequent system crashes. KSR called their cache-only memory architecture (COMA) by the trade name "Allcache"; reliability problems with early systems earned it the nickname "Allcrash", although memory was not necessarily the root cause of crashes. A few KSR1 models were sold, and as the KSR2 was being rolled out, the company collapsed amid accounting irregularities involving the overstatement of revenue.
KSR used a proprietary processor because 64-bit processors were not commercially available. However, this put the small company in the difficult position of doing both processor design and system design. The KSR processors were introduced in 1991 at 20 MHz and 40 MFlops. At that time, the 32-bit Intel 80486 ran at 50 MHz and 50 MFlops. When the 64-bit DEC Alpha was introduced in 1992, it ran at up to 192 MHz and 192 MFlops, while the 1992 KSR2 ran at 40 MHz and 80 MFlops.
One customer of the KSR2, the Pacific Northwest National Laboratory, a United States Department of Energy facility, purchased an enormous number of spare parts, and kept their machines running for years after the demise of KSR.
KSR, along with many of its competitors (see below), went bankrupt during the collapse of the supercomputer market in the early 1990s. KSR went out of business in February 1994, when their stock was delisted from the stock exchange.
Competition.
KSR's competitors included MasPar Computer Corporation, Thinking Machines, Meiko Scientific, and various old-line (and still surviving) companies like IBM, Intel, and Sun Microsystems.
Further Reading.
"BUSINESS TECHNOLOGY; Pools of Memory, Waves of Dispute" John Markoff, The New York Times - 29 January 1992

</doc>
<doc id="17341" url="https://en.wikipedia.org/wiki?curid=17341" title="Kinglassie">
Kinglassie

Kinglassie (Gaelic: "Cille MoGhlasaidh") is a small village in central Fife, Scotland. It is located two miles southwest of Glenrothes. In 2011, the population of the village was 1,684.
History.
The village of Kinglassie (pronounced Kin-glassie) lies to the north of the Lochty Burn, two miles (5 km) southwest of Glenrothes in Fife, and two miles southeast of Perth and Kinross district. In 830 AD, the village was known as Kinglace.
The village has never been known as Goatmilkshire, though the area northeast of the village has always had that name or Gaitmilkshire.
In the year 1231, the village was known as Kinglassin and was in the Lochoreshire area. However, this changed in 1235 when Constantine II of Lochore renounced his claim to the lands in favour of the Abbey of Dunfermline. From this time on, Kinglassie ceased to be part of Lochoreshire. Little of antiquity remains, except for the Dogton Stone, with its Celtic cross, situated in a field about a mile (1.5 km) to the south. For many years, Kinglassie was a weaving village, but in the 19th and 20th centuries it developed as a mining town.
From a very early period through to the Reformation, Scotland was dotted over with certain divisions of lands known as "Schyres." Thus, in the immediate neighbourhood of Kinross were "Kynros-Schyre", "Portmocke-Schyre", "Kinglassy-Schyre", "Muchard-Schyre", and "Doloure-Schyre". These Schyres must not be confused with the shire of the present day; they were simply divisions of land, similar in extent to an average modern parish.
Kinglassie has a primary school, Mitchell Hall (1896), a library, and the Miners' Welfare Institute (est. 1931). Fife Airport lies about a mile (1.5 km) to the north and, on a hill overlooking the farm of Redwells, stands Blythe's Folly, a 15.6 m (52 ft) high tower built in 1812 by an eccentric Leith ship owner. Kinglassie's development during the late 19th and early 20th centuries was marked by its rapid expansion to house mine workers. Many mine workers perished or were injured during the life of the mine. The mine was plagued by water flooding problems. The Kinglassie Pit started in 1908 and closed in 1967. The Westfield open cast coal mine lies to the west of the village and is still regarded as the biggest man-made hole in Europe by local people.
Glastian of Kinglassie B (AC) (also known as Glastian of MacGlastian) was born in Fife, Scotland. He died at Kinglassie (Kinglace), Scotland, in 830. As bishop of Fife, Saint Glastian mediated in the bloody civil war between the Picts and the Scots. When the Picts were subjugated, Glastian did much to alleviate their lot. He is the patron saint of Kinglassie in Fife and is venerated in Kyntire (Benedictines, Husenbeth).
School.
Kinglassie primary school has a role of approximately 150 pupils. The school was built to designs by the architect George Charles Campbell in 1912. It has a butterfly type plan consisting of two single storey rendered wings either side of a hexagon shaped hall. The central portion of the façade is two storeys high and of red sandstone, with generous steps leading to a central formal entrance. It is a category B listed building.
The Pupil Council represents pupils in the school. The eco-committee consists of pupils, staff, parents, and members of the wider community, and is proactive in promoting conservation initiatives throughout the school. A parent council represents the parent body and raises funds for various initiatives. In addition, children are supported in class by a growing number of parent helpers and the school is well supported by parents generally.
Local landmarks.
Blythe's Tower, built in 1812, is a four-storey square tower, 15.8 m (54 ft) high, built of rubble with ashlar string courses and a crenellated parapet. It is a category B listed building. The tower's interior was formerly floored to afford access to an observation platform. The tower was built by a linen merchant to view ships as they entered the Forth, affording him the opportunity to procure the best goods at port. During World War II, the tower was used as a look out tower by the home guard.
The Dogton Stone, lies in a field to the south of Kinglassie at Grid reference - NT 236 968. The stone is a fragment of a free standing cross erected by the Picts, it probably dates form the 9th Century. The lower portion of the stone is all that remains of the cross and badly eroded decoration including a figure of an armed horseman above two beasts can be discerned. No one is certain why the stone was erected at this spot.
The Mitchell Hall, built in 1896, was donated to the community by Alexander Mitchell. Mitchell also donated the first Parish Church organ.The Mitchell Hall is used by local community groups and is an asset to the wider Fife community.

</doc>
<doc id="17342" url="https://en.wikipedia.org/wiki?curid=17342" title="Kalat, Pakistan">
Kalat, Pakistan

Qalat or Kalat (), (Balochi: Kalát,قلات) is a historical town located in Kalat District, Balochistan, Pakistan. Kalat is the capital of Kalat District and is known locally as Kalat-e-Baloch and Kalat-e-Sewa.
Qalat, formerly Qilat, is located roughly in the center of Balochistan, Pakistan, south and slightly west of the provincial capital Quetta. It was the capital of the Kalat Khanate.
Demographics.
The population is mostly Muslim, with a Hindu population of two percent. In addition, there are some Hindu Hindkowan merchants who have settled in Kalat. As such, there is a Hindu mandir below citadel of the city, dedicated to Goddess Kali; this mandir dates back to pre-Islamic era of South Asia.
History.
The town of Kalat is said to have been founded by and named Qalat-e Sewa (Sewa's Fort), after Sewa, a legendary hero of the Baloch people.
The Brahui Speaking Baloch arrived in the Qalat area at around the same time as the Balochi speaking Baloch tribes from the west. The Balochis established a large kingdom in the 15th century, but it soon declined and the region fell to Mughals for a short period. The brahui speaking Baloch Khans of Qalat were dominant from the 17th century onwards until the arrival of the British in the 19th century. A treaty was signed in 1876 to make Qalat part of the British Empire.
The Khan of Kalat in March 1946 deputed Samad Khan - a member of the All India Congress Committee (AICC) - to plead Kalat's (the then Balochistan's) case with the Congress leadership.
In 1947, the Khan of Kalat reportedly acceded to the dominion of India. But his accession papers were returned by Jawaharlal Nehru, the Prime Minister of India.
Afterwards, Mir Ghaus Baksh Bizenjo, President of the Kalat State National Party, went to Delhi and met Congress President Maulana Abul Kalam Azad. Azad argued that Kalat would never be able to survive as a sovereign, independent state and would have to ask for British protection. Such a demand, Azad said, would render the sovereignty of the subcontinent meaningless. This was why Indian help for Kalat was ruled out.
Later, an All India Radio (AIR) broadcast of March 27, 1948, reported a press conference by VP Menon, Secretary in the Ministry of States. Menon revealed that the Khan of Kalat was pressing India to accept Kalat's accession, but added that India would have nothing to do with it. This was rejected by the Khan of Kalat who, upset by the claim, issued a communiqué: "On the night of March 27, All India Radio, Delhi announced that two months ago Kalat State had approached the Indian Union to accept its accession to India and that the Indian Union had rejected the request…It had never been my intention to accede to India…It is, therefore, declared that from 9 pm on March 27th – the time when I heard the false news over the air, I forthwith decide to accede to Pakistan, and that whatever differences now exist between Kalat and Pakistan be placed in writing before Mr Jinnah, the Governor-General of Pakistan, whose decision I shall accept". He reportedly told Pakistan's President Muhammad Ali Jinnah to begin negotiations for Kalat's treaty of accession to Pakistan.
In 1948, the Khan of Qalat, Mir Ahmad Yar Khan, decided to join Pakistan on request of Muhammad Ali Jinnah (founder of Pakistan), and decision was made that defence, currency, foreign office and finance will be controlled by federal govt. rest the province will control by itself. but later, after death of Muhammad Ali Jinnah, formation of one unit changed this situation and it was merged into Pakistan like other areas.
In 1948, Qalat became part of Pakistan when the British withdrew. The last Khan of Qalat was formally removed from power in 1955, but the title is still claimed by his descendents. The current Khan of Qalat is Mir Suleman Dawood Khan. There is a considerable section of the Balochi population and overseas community who consider the accession to Pakistan an act of aggression where Pakistan took over Balochistan by force.

</doc>
<doc id="17348" url="https://en.wikipedia.org/wiki?curid=17348" title="Kordofanian languages">
Kordofanian languages

The Kordofanian languages are a "geographic" grouping of half a dozen language families spoken in the Nuba Mountains of Kordofan Province, Sudan. In 1963 Joseph Greenberg added them to the Niger–Congo family, creating his Niger–Kordofanian proposal. The Kordofanian languages have not been shown to be more distantly related than other branches of Niger–Congo, however, nor have they been shown to constitute a valid group. Today the Kadu family is excluded, and the others usually included in Niger–Congo proper.
Roger Blench notes that the Talodi and Heiban families have the noun-class systems characteristic of the Atlantic–Congo core of Niger–Congo, but that the two Katla languages have no trace of ever having had such a system, whereas the Kadu languages and some of the Rashad languages appear to have acquired noun classes as part of a Sprachbund rather than having inherited them. He concludes that Talodi and Heiban are core Niger–Congo whereas Katla and Rashad form a peripheral branch along the lines of Mande.
Talodi–Heiban.
The Heiban languages, also called Koalib or Koalib–Moro, and the Talodi languages, also called Talodi–Masakin, are closely related.
Lafofa.
Lafofa (Tegem) was for a time classified with Talodi, but appears to be a separate branch of Niger–Congo.
Rashad.
The number of Rashad languages, also called Tegali–Tagoi, varies among descriptions, from two (Williamson & Blench 2000), three (Ethnologue), to eight (Blench "ms"). Tagoi has a noun-class system like the Atlantic–Congo languages—apparently borrowed,—while Tegali does not.
Katla languages.
The two Katla languages have no trace of ever having had a Niger–Congo-type noun-class system.
Kadu languages.
Since Schadeberg 1981c, the "Tumtum" or Kadu branch is now widely seen as Nilo-Saharan. However, the evidence is slight, and a conservative classification would treat it as an independent family.

</doc>
<doc id="17350" url="https://en.wikipedia.org/wiki?curid=17350" title="Khuriya Muriya Islands">
Khuriya Muriya Islands

The Khuriya Muriya Islands (or "Kuria Muria") (; transliterated: "Juzur Khurīyā Murīyā" or "Khūryān Mūryān)" are a group of five islands in the Arabian Sea, off the southeastern coast of the Sultanate of Oman. The islands form part of the province of Shalim and the Hallaniyat Islands in the governorate of Dhofar.
History.
In antiquity the islands were called the "Zenobii" or "Zenobiou Islands" (; "") or "Doliche" (). The islands were mentioned by several early writers including Ptolemy (vi. 7. § 47) who numbered them as seven small islands lying in Khuriya Muriya Bay (; ), towards the entrance of the "Persian Gulf" (most likely the modern Gulf of Aden).
In 1854 the "hami" (sultan) of Muscat (later Muscat and Oman now Oman) ceded the islands to Britain and in 1868 they were attached to the Aden Settlement (in what is now modern Yemen). Guano was mined there and as part of Britain's communication system with India the islands had a telegraph cable station. 
As a British possession until 1967, they were administered by the British Governor of Aden until 1953, then by the British High Commissioner until 1963, and finally by the British Chief Political Resident of the Persian Gulf (based in Bahrain). On 30 November 1967, Lord Caradon, the British Ambassador to the United Nations, announced that in accordance with the wishes of the local inhabitants, the islands would be returned to Muscat and Oman, despite criticism from President Qahtan Muhammad al-Shaabi that the islands should be transferred to the People's Republic of South Yemen. The boundary between the two countries was not formally settled until 1995 when it was agreed that the islands were on Oman's side of the line.

</doc>
<doc id="17351" url="https://en.wikipedia.org/wiki?curid=17351" title="Khwaja Ahmad Abbas">
Khwaja Ahmad Abbas

Khwaja Ahmad Abbas (Urdu/Persian: ; ) (7 June 1914 – 1 June 1987), popularly known as K. A. Abbas, was an Indian film director, novelist, screenwriter, and a journalist in the Urdu, Hindi and English languages. He was the maker of important Hindi films such as "Saat Hindustani" (1969) and "Do Boond Pani" (1972), both of which won the National Film Award for Best Feature Film on National Integration, Palme d'Or nominated (Cannes Film Festival) "Pardesi" (1957) and "Shehar Aur Sapna" (1963), which won the National Film Award for Best Feature Film.
As a screenwriter, Khwaja Ahmad Abbas is considered one of pioneers of Indian parallel or neo-realistic cinema, having penned films like the Palme d'Or winner at the Cannes Film Festival, "Neecha Nagar" (1946), Jagte Raho, Dharti Ke Lal, Awara, Saat Hindustani and Naya Sansar. Apart from this, he wrote the best of Raj Kapoor films, "Awaara, Shri 420, Mera Naam Joker, Bobby" and "Henna".
His column ‘Last Page’, holds the distinction of being one of the longest-running columns in the history of Indian journalism. The column began in 1935, in "Bombay Chronicle", and moved to the "Blitz" after the "Chronicle"'s closure, where it continued until his death in 1987. He was awarded the Padma Shri in 1969, by Government of India.
Biography.
Early life and education.
Khwaja Ahmad Abbas was born in Panipat, Haryana. He was born in the home of celebrated Urdu poet, 'Khwaja Altaf Husain Hali', a student of Mirza Ghalib. His grandfather Khwaja Gulam Abbas was one of the chief rebels of the 1857 Rebellion movement, and the first martyr of Panipat to be blown from the mouth of a cannon. Abbas's father Ghulam-Us-Sibtain graduated from Aligarh Muslim University, was a tutor of a prince and a prosperous businessman, who modernised the preparation of Unani medicines. Abbas's mother, 'Masroor Khatoon', was the daughter of Sajjad Husain, an enlightened educationist. Abbas took his early education in 'Hali Muslim High School', which was established by his great grand father Hali. He had his early education till 7th in Panipat. He was instructed to read the Arabic text of the Quran and his childhood dreams swung at the compulsive behest of his father. Abbas completed his matriculation at the age of fifteen. He did his B.A. with English literature in 1933 and LL.B. in 1935 from Aligarh Muslim University.
Abbas's family tree goes back to Aiyub Ansari, the companion of the Islamic prophet Muhammad.
Career.
Abbas began his career as a journalist, when he joined 'National Call', a New Delhi based paper after finishing his B.A.. Later while studying law in 1934, started 'Aligarh Opinion', India's first university students' weekly during the pre-independence period.
After completing his education at Aligarh Muslim University, Abbas joined the "Bombay Chronicle" in 1935. He occasionally served a film critic, but after the film critic of the paper died, he was made the editor of the film section.
He entered films as a part-time publicist for Bombay Talkies in 1936, a production house owned by Himanshu Rai and Devika Rani, to whom he sold his first screenplay "Naya Sansar" (1941).
While at the "Bombay Chronicle", (1935–1947), he started a weekly column called 'Last Page', which he continued when he joined the Blitz magazine. "The Last Page", (‘Azad Kalam’ in the Urdu edition), thus became the longest-running political column in India's history (1935–87). A collection of these columns was later published as two books. He continued to write for The Blitz and Mirror till his last days.
Meanwhile he had started writing scripts for other directors, "Neecha Nagar" for Chetan Anand and "Dr. Kotnis Ki Amar Kahani" for V. Shantaram.
In 1945, he made his directorial debut with a film based on the Bengal famine of 1943, "Dharti Ke Lal" ("Children of the Earth") for the Indian People's Theatre Association (IPTA). In 1951, he founded his own production company called Naya Sansar, which consistently produced films that were socially relevant including, "Anhonee", "Munna", "Rahi" (1953), based on a Mulk Raj Anand story, was on the plight of workers on tea plantations, the National Film Award winner, "Shehar Aur Sapna" (1964) and "Saat Hindustani" (1969), which won the Nargis Dutt Award for Best Feature Film on National Integration and is also remembered as Bollywood icon, Amitabh Bachchan's debut film.
A prolific writer, and novelist, during his illustrious career spanning five decades, Abbas wrote over 73 books in English, Hindi and Urdu. Abbas was considered a leading light of the Urdu short story. His best known fictional work remains 'Inquilab', based Communal violence, which made him a household name in Indian literature. Like Inquilab, many of his works were translated into many Indian, and foreign languages, like Russian, German, Italian, French and Arabic.
Abbas interviewed several renowned personalities in literary and non-literary fields, including the Russian Prime Minister Khrushchov, American President Roosevelt, Charlie Chaplin, Mao-Tse-Tung and Yuri Gagarin.
He went on to write scripts for Jagte Raho, and most of the prominent Raj Kapoor films including "Awaara, Shri 420, Mera Naam Joker, Bobby" and "Henna".
His autobiography, "I Am not an Island: An Experiment in Autobiography", was first published in 1977 and later released in 2010.
Awards and honours.
Literary.
Haryana State Robe of Honour for literary achievements in 1969, the prestigious Ghalib Award for his contribution to Urdu prose literature in 1983
Vorosky Literary Award of the Soviet Union in 1984, Urdu Akademi Delhi Special Award 1984, Maharashtra State Urdu Akademi Award in 1985 and the Soviet Award for his contribution to the cause of Indo-Soviet Friendship in 1985.
Books.
"For detailed listing" :

</doc>
<doc id="17353" url="https://en.wikipedia.org/wiki?curid=17353" title="Katherine MacLean">
Katherine MacLean

Katherine Anne MacLean (born January 22, 1925) is an American science fiction author best known for her short fiction of the 1950s which examined the impact of technological advances on individuals and society.
Profile.
Damon Knight wrote, "As a science fiction writer she has few peers; her work is not only technically brilliant but has a rare human warmth and richness." Brian Aldiss noted that she could "do the hard stuff magnificently," while Theodore Sturgeon observed that she "generally starts from a base of hard science, or rationalizes psi phenomena with beautifully finished logic."
According to "The Encyclopedia of Science Fiction", she "was in the vanguard of those sf writers trying to apply to the soft sciences the machinery of the hard sciences".
Her stories have been included in anthologies and a few have had radio and television adaptations. One collection of her stories has been published.
It was while she worked as a laboratory technician in 1947 that she began writing science fiction. Strongly influenced by Ludwig von Bertalanffy's General Systems Theory, her fiction has often demonstrated a foresight in scientific advancements.
Awards.
MacLean received a Nebula Award in 1971, for her novella "The Missing Man" ("Analog", March, 1971) and she was a Professional Guest of Honor at the first WisCon in 1977. She was honored in 2003 by the Science Fiction Writers of America as an SFWA Author Emeritus.
Collections.
"The Diploids and Other Flights of Fancy" (Avon, 1962), her first short story collection, includes "The Diploids" (a.k.a. "Six Fingers"), "Feedback", "Pictures Don't Lie", "Incommunicado", "The Snow Ball Effect", "Defense Mechanism" and "And Be Merry" (a.k.a. "The Pyramid in the Desert").
Her second collection, "The Trouble with You Earth People" (Donning/Starblaze, 1980) contains "The Trouble with You Earth People", "The Gambling Hell and the Sinful Girl", "Syndrome Johnny", "Trouble with Treaties" (with Tom Condit), "The Origin of the Species", "Collision Orbit", "The Fittest", "These Truths", "Contagion", "Brain Wipe" and her Nebula Award-winning "The Missing Man".

</doc>
<doc id="17354" url="https://en.wikipedia.org/wiki?curid=17354" title="Kuru">
Kuru

Kuru may refer to:

</doc>
<doc id="17355" url="https://en.wikipedia.org/wiki?curid=17355" title="Kenneth Kaunda">
Kenneth Kaunda

Kenneth David Kaunda (born 28 April 1924), also known as KK, served as the first President of Zambia, from 1964 to 1991.
Kaunda is the youngest of eight children born to an ordained Church of Scotland missionary and teacher. He followed his father's steps in becoming a teacher.
He was at the forefront of the struggle for independence from British rule. Dissatisfied with Nkumbula's leadership of the Northern Rhodesian African National Congress, he broke away and founded the Zambian African National Congress, later becoming the head of the United National Independence Party. He was the first President of the independent Zambia.
In 1972 all political parties except UNIP were banned. At the same time, Kaunda oversaw the acquisition of majority stakes in key foreign-owned companies. The oil crisis of 1973 and a slump in export revenues put Zambia in a state of economic crisis.
International pressure forced Kaunda to change the rules that had kept him in power. Multi-party elections took place in 1991, in which Frederick Chiluba, the leader of the Movement for Multiparty Democracy, ousted Kaunda.
Kaunda was briefly stripped of Zambian citizenship in 1999 but the decision was overturned the following year.
Early life.
Kaunda is the youngest of eight children. He was born at Lubwa Mission in Chinsali, Northern Province of Northern Rhodesia, now Zambia. His father was the Reverend David Kaunda, an ordained Church of Scotland missionary and teacher, who was born in Nyasaland (now Malawi) and had moved to Chinsali to work at Lubwa Mission. He attended Munali Training Centre in Lusaka (August 1941 – 1943).
Kaunda was a teacher at the Upper Primary School and Boarding Master at Lubwa and then Headmaster at Lubwa from 1943 to 1945. He was for a time working at the Salisbury and Bindura Mine. In early 1948, he became a teacher in Mufulira for the United Missions to the Copperbelt (UMCB). He was then assistant at an African Welfare Centre and Boarding Master of a Mine School in Mufulira. In this period, he was leading a Pathfinder Scout Group and was Choirmaster at a Church of Central Africa Congregation. He was also for a time Vice-Secretary of the Nchanga Branch of Congress.
Independence struggle.
In April 1949, Kaunda returned to Lubwa to become a part-time teacher, but resigned got in 1951. In that year he became Organising Secretary of Northern Province's Northern Rhodesian African National Congress. On 11 November 1953 he moved to Lusaka to take up the post of Secretary General of the ANC, under the presidency of Harry Nkumbula. The combined efforts of Kaunda and Nkumbula failed to mobilise native African peoples against the European-dominated Federation of Rhodesia and Nyasaland. In 1955 Kaunda and Nkumbula were imprisoned for two months with hard labour for distributing subversive literature; such imprisonment and other forms of harassment were normal rites of passage for African nationalist leaders. The experience of imprisonment had a radicalising impact on Kaunda. The two leaders drifted apart as Nkumbula became increasingly influenced by white liberals and was seen as being willing to compromise on the issue of black majority rule, waiting until most of the indigenous population was responsibly educated before extending the franchise. The franchise was to be determined by existing property and literacy qualifications, dropping race altogether. Nkumbula's allegedly autocratic leadership of the ANC eventually resulted in a split. Kaunda broke from the ANC and formed the Zambian African National Congress (ZANC) in October 1958. ZANC was banned in March 1959. In June Kaunda was sentenced to nine months' imprisonment, which he spent first in Lusaka, then in Salisbury.
While Kaunda was in prison, Mainza Chona and other nationalists broke away from the ANC and, in October 1959, Chona became the first president of the United National Independence Party (UNIP), the successor to ZANC. However, Chona did not see himself as the party's main founder. When Kaunda was released from prison in January 1960 he was elected President of UNIP. In 1960 he visited Martin Luther King Jr. in Atlanta and afterwards, in July 1961, Kaunda organised a civil disobedience campaign in Northern Province, the so-called Cha-cha-cha campaign, which consisted largely of arson and obstructing significant roads. Kaunda subsequently ran as a UNIP candidate during the 1962 elections. This resulted in a UNIP–ANC Coalition Government, with Kaunda as Minister of Local Government and Social Welfare. In January 1964, UNIP won the next major elections, defeating their ANC rivals and securing Kaunda's position as prime minister. On 24 October 1964 he became the first President of an independent Zambia, appointing Reuben Kamanga as his Vice-President.
Educational policies.
At the time of its independence, Zambia's modernisation process was far from complete. It had just 109 university graduates and less than 0.5% of the population was estimated to have completed primary education. The nation's educational system was one of the most poorly developed in all of Britain's former colonies. Because of this, Zambia had to invest heavily in education at all levels. Kaunda instituted a policy where all children, irrespective of their parents' ability to pay, were given free exercise books, pens and pencils. The parents' main responsibility was to buy uniforms, pay a token "school fee" and ensure that the children attended school. This approach meant that the best pupils were promoted to achieve their best results, all the way from primary school to university level. Not every child could go to secondary school, for example, but those who did were well educated.
The University of Zambia was opened in Lusaka in 1966, after Zambians all over the country had been encouraged to donate whatever they could afford towards its construction. Kaunda was appointed Chancellor and officiated at the first graduation ceremony in 1969. The main campus was situated on the Great East Road, while the medical campus was located at Ridgeway near the University Teaching Hospital. In 1979 another campus was established at the Zambia Institute of Technology in Kitwe. In 1988 the Kitwe campus was upgraded and renamed the Copperbelt University, offering business studies, industrial studies and environmental studies.
Other tertiary-level institutions established during Kaunda's era were vocationally focused and fell under the aegis of the Department of Technical Education and Vocational Training. They include the Evelyn Hone College of Applied Arts and Commerce and the Natural Resources Development College (both in Lusaka), the Northern Technical College at Ndola, the Livingstone Trades Training Institute in Livingstone, and teacher-training colleges.
Economic policies.
At independence Kaunda's government inherited a country with an economy that was completely under the control of foreigners. For example, the British South Africa Company (founded by the British imperialist Cecil Rhodes) still retained commercial assets and mineral rights that it had acquired from a concession signed with the Litunga of Bulozi in 1890. Only by threatening to expropriate it on the eve of independence did Kaunda manage to get favourable concessions from the BSAC.
Deciding on a planned economy, Zambia instituted a program of national development, under the direction of the National Commission for Development Planning, which instituted a "Transitional Development Plan" and the "First National Development Plan". These two operations, which attempted to secure major investment in infrastructure and manufacturing sectors, were generally regarded as successful.
A major change in the structure of Zambia's economy came with the Mulungushi Reforms of April 1968: Kaunda declared his intention to acquire an equity holding (usually 51% or more) in a number of key foreign-owned firms, to be controlled by his Industrial Development Corporation (INDECO). By January 1970, Zambia had acquired majority holding in the Zambian operations of the two major foreign mining interests, the Anglo American Corporation and the Rhodesian Selection Trust (RST); the two became the Nchanga Consolidated Copper Mines (NCCM) and Roan Consolidated Mines (RCM), respectively. Kaunda also announced the creation of a new parastatal body, the Mining Development Corporation (MINDECO), while a Finance and Development Corporation (FINDECO) allowed the Zambian government to gain control of insurance companies and building societies. Major foreign-owned banks, such as Barclays, Standard Chartered and Grindlays Bank, successfully resisted takeover. In 1971, INDECO, MINDECO, and FINDECO were brought together under an omnibus parastatal, the Zambia Industrial and Mining Corporation (ZIMCO), to create one of the largest companies in sub-Saharan Africa, with Francis Kaunda as chairman of the board. The management contracts under which day-to-day operations of the mines had been carried out by Anglo American and RST were terminated in 1973. In 1982, NCCM and RCM were merged into the giant Zambia Consolidated Copper Mines Ltd (ZCCM).
Unfortunately this nationalisation policy was ill-timed. In 1973, the massive increase in the price of oil was followed by a slump in copper prices and a diminution of export earnings. In early 1973, the price of copper accounted for 95% of all export earnings; this had halved in value on the world market by early 1975. By 1976, Zambia had a balance-of-payments crisis, and rapidly fell into debt with the International Monetary Fund (IMF). The Third National Development Plan had to be abandoned as crisis management replaced long-term planning.
By the mid-1980s, Zambia had one of the highest debts of any nation on the globe, relative to its gross domestic product (GDP). The IMF insisted that the Zambian government should focus on stabilising the economy and restructuring it to reduce dependence on copper. The proposed measures included the ending of price controls, devaluation of the kwacha, reining in of government spending, cancellation of subsidies on food and fertiliser, and increased prices for farm produce. Kaunda's removal of food subsidies caused the prices of basic foodstuffs to skyrocket, sparking riots and disorder. In desperation, Kaunda attempted to sever his ties with the IMF in May 1987 and introduce a New Economic Recovery Programme in 1988. However, this was not ultimately successful and he eventually moved toward a new understanding with the IMF in 1989. In 1990 Kaunda was forced to make major policy shifts; he announced the intention to partially privatise the parastatals. However, these changes were too little and came too late to prevent his fall from power as a result of Zambia's economic woes.
One-party state and "African socialism".
In the wake of the Lumpa Uprising, Kaunda proclaimed a state of emergency, banning the Lumpa Church, which he considered a major source of opposition because it refused to allow its members to participate in compulsory voting. This created animosity between the Church and UNIP, resulting in some low-level conflict which claimed numerous lives. Kaunda tried to mediate the differences between the Church, local authorities and UNIP party members but was eventually unable to control party cadres in the North.
From 1964 onwards, Kaunda's government developed clearly authoritarian characteristics. Becoming increasingly intolerant of opposition, Kaunda banned all parties except UNIP, following violence during the 1968 elections. However, in early 1972 he faced a new threat in the form of Simon Kapwepwe's decision to leave UNIP and found a rival party, the United Progressive Party, which Kaunda immediately attempted to suppress. Next, he appointed the Chona Commission, which was set up under the chairmanship of Mainza Chona in February 1972. Chona's task was to make recommendations for a new Zambian constitution which would effecively reduce the nation to a one-party state. The commission's terms of reference did not permit it to discuss the possible faults of Kaunda's decision. ANC party members boycotted Chona's efforts and unsuccessfully challenged the constitutional change in the courts. The Chona report was based on four months of public hearings and was submitted in October 1972 as a 'liberal' document. Finally, Kaunda neutralised Nkumbula by getting him to join UNIP and accept the Choma Declaration on 27 June 1973. The new constitution was formally promulgated on 25 August. At the first elections under the new system held that December, Kaunda was the sole candidate.
With no more opposition to him, Kaunda allowed the creation of a personality cult. He developed a left nationalist-socialist ideology, called Zambian Humanism. This was based on a combination of mid-20th-century ideas of central planning/state control and what he considered basic African values: mutual aid, trust and loyalty to the community. Similar forms of African socialism were introduced inter alia in Ghana by Kwame Nkrumah ("Consciencism") and Tanzania by Julius Nyerere ("Ujamaa"), while in Zaire, President Mobutu Sese Seko, a much less "benevolent" ruler than Kaunda or Nyerere, was at a loss until he hit on the ideal ideology – 'Mobutuism'. To elaborate his ideology, Kaunda published several books: "Humanism in Zambia and a Guide to its Implementation, Parts 1, 2 and 3". Other publications on Zambian Humanism are: "Fundamentals of Zambian Humanism", by Timothy Kandeke; "Zambian Humanism, religion and social morality", by Cleve Dillion-Malone S.J. and "Zambian Humanism: some major spiritual and economic challenges", by Justin B. Zulu. "Kaunda on Violence", (US title, "The Riddle of Violence"), was published in 1980. He is known as "Gandhi of Africa" or "African Gandhi."
Foreign policy.
During his early presidency Kaunda was an outspoken supporter of the anti-apartheid movement and opposed white minority rule in Southern Rhodesia. Although his nationalisation of the copper mining industry in the late 1960s and the volatility of international copper prices contributed to increased economic problems, matters were aggravated by his logistical support for the black nationalist movements in Ian Smith's Rhodesia, South West Africa, Angola, and Mozambique. Kaunda's administration later attempted to serve the role of a mediator between the entrenched white minority and colonial governments and the various guerilla movements which were aimed at overthrowing these respective administrations. Beginning in the early 1970s, he began permitting the most prominent guerilla organisations, such as the Rhodesian ZANU and the African National Congress, to use Zambia as a base for their operations. Former ANC president Oliver Tambo even spent a significant proportion of his 30-year exile living and working in Zambia. Joshua Nkomo, leader of ZAPU, also erected military encampments there.
On 26 August 1975, Kaunda acted as mediator along with the Prime Minister of South Africa, B. J. Vorster at Victoria Falls to discuss possibilities for an internal settlement in Southern Rhodesia with Ian Smith and the black nationalists. After the Lancaster House Agreement, Kaunda attempted to seek similar majority rule in South West Africa. He met with P. W. Botha in Botswana to debate this proposal, but apparently failed to make a serious impression.
Meanwhile, the anti-white minority insurgency conflicts of southern Africa continued to place a huge economic burden on Zambia as white minority governments were the country's main trading partners. In response, Kaunda negotiated the TAZARA Railway (Tanzam) linking Kapiri Mposhi on the Zambian Copperbelt with Tanzania's port of Dar-es-Salaam on the Indian Ocean. Completed in 1975, this was the only route for bulk trade which did not have to transit white-dominated territories. This precarious situation lasted more than 20 years, until the abolition of apartheid in South Africa.
For much of the Cold War Kaunda was a strong supporter of the Non-Aligned Movement. He hosted a NAM summit in Lusaka in 1970 and served as the movement's chairman from 1970 to 1973. He maintained a close friendship with Yugoslavia's long-time leader Josip Broz Tito and is remembered by many Yugoslav officials for weeping openly over the latter's casket in 1980. He even had a special house constructed in Lusaka for Tito's visits to the country. He also visited and welcomed Romania's dictator, Nicolae Ceaușescu in the 1970s. In 1986, the University of Belgrade (Yugoslavia) awarded him an honorary doctorate. Kaunda had frequent but cordial differences with US President Ronald Reagan whom he met 1983 and Margaret Thatcher mainly over what he saw as a blind eye being turned towards South African apartheid. He always maintained warm relations with the People's Republic of China who had provided assistance on many projects in Zambia, including the TAZARA Railway.
Prior to the first Gulf War, Kaunda cultivated a friendship with Iraqi strongman Saddam Hussein, with whom he secured oil resources for his nation. He even went so far as to name Zambian streets in Saddam's honour.
In August 1989, Farzad Bazoft was detained in Iraq for alleged espionage. He was accompanied by a British nurse, Daphne Parish, who was arrested as well. Bazoft was an Iranian-born freelance journalist attempting to expose Saddam's mass murder of Iraqi Kurds. Bazoft was later tried and condemned to death, but Kaunda managed to negotiate for his female companion's release. Kaunda served as chairman of the Organization of African Unity (OAU) from 1970 to 1973.
UNIP and Kaunda's autocracy during the Second Republic.
From 1973 onward, Kaunda's leadership took on more autocratic characteristics. He personally appointed the Central Committee of UNIP, although the process was given a veneer of legitimacy by being "approved" by a National Congress of the party. In theory, Kaunda's nominations could be discarded by Congress, but in practice they were always accepted without modification. The argument used was that "the President knows the people who can work well with him, so if we modify the nominations we will end up with a less effective team". In turn, the Central Committee nominated a sole candidate for the post of president of the party. Of course, since the members of the Central Committee had been nominated by him, Kaunda was always the sole candidate. Constitutionally, whoever was in good standing with the party was at liberty to challenge him, but in practice no one did so because of his charisma and intolerance for dissent.
The president of UNIP was the only candidate for president of the republic. After the formalities of nominating him, the rest of the Zambian population was given the opportunity to express approval or disapproval of that candidate by voting either "Yes" or "No". Since the presidential "election" was always accompanied by parliamentary elections, there was great pressure placed on parliamentary candidates to "campaign" for the president's "Yes" vote, in addition to their own campaigns. Parastatal companies (which were controlled through ZIMCO – Zambia Industrial and Mining Corporation) were also under pressure to "campaign" for Kaunda by buying advertising space in the two national newspapers (Times of Zambia and Zambia Daily Mail) exhorting the electorate to give the president a "massive 'Yes' vote".
The parliamentary elections were also controlled by Kaunda: the names of candidates had to be submitted to UNIP's Central Committee, which then selected three people to stand for any particular constituency. The Central Committee could veto any candidate for any reason. Using these methods, Kaunda kept any potential rivals at bay by ensuring that they never got into political power.
This was the tactic he used when he saw off Nkumbula and Kapwepwe's challenges to his sole candidacy for the 1978 UNIP elections. On that occasion, the UNIP's constitution was "amended" overnight to bring in rules that invalidated the two challengers' nominations: Kapwepwe was told he could not stand because only people who had been members for five years could be nominated to the presidency (he had only rejoined UNIP three years before); Nkumbula was outmaneuvered by introducing a new rule that said each candidate needed the signatures of 200 delegates from "each" province to back his candidacy. Less creative tactics were used on a third candidate called Chiluwe; he was just beaten up by the UNIP Youth Wing to within an inch of his life. This meant that he was in no state to submit his nomination.
Fall from power.
Eventually, however, economic troubles and increasing international pressure to bring more democracy to Africa forced Kaunda to change the rules that kept him in power. People who had been afraid to criticise him were now emboldened to challenge his competence. His close friend Julius Nyerere had stepped down from the republican presidency of Tanzania in 1985 and was quietly encouraging Kaunda to follow suit. Pressure for a return to multiparty politics increased and Kaunda voluntarily yielded and called for multiparty elections in 1991, in which the Movement for Multiparty Democracy (MMD) won. One of the issues in the campaign was a plan by Kaunda to turn over one quarter of the nation's land to Maharishi Mahesh Yogi, an Indian guru who promised that he would use it for a network of utopian agricultural enclaves that proponents said would create "heaven on earth". Kaunda was forced in a television interview to deny practising Transcendental Meditation. Kaunda left office with the inauguration of MMD leader Frederick Chiluba as president on 2 November 1991. He was the second mainland African head of state to allow free multiparty elections and to have relinquished power when he lost: the first, Mathieu Kérékou of Benin, had done so in March of that year.
Post-presidency.
After leaving office, Kaunda clashed frequently with Chiluba’s government and the MMD. Chiluba later attempted to deport Kaunda on the grounds that he was a Malawian. The MMD dominated government under the leadership of Chiluba had the constitution amended, barring citizens with foreign parentage from standing for the presidency, to prevent Kaunda from contesting the next elections in 1996. Kaunda retired from politics after he was accused of involvement in the failed 1997 coup attempt. After the coup, on Boxing Day in 1997 he was placed under arrest by Chiluba. However, many officials in the region appealed against this; on New Year's Eve of the same year, he was placed under house arrest until the court date. In 1999 Kaunda was declared stateless by the Ndola High Court in a judgment delivered by Justice Chalendo Sakala. A full transcript of the judgment was published in the "Times of Zambia" edition of 1 April 1999. Kaunda however successfully challenged this decision in the Supreme Court of Zambia, which declared him to be a Zambian citizen in 2000.
After retiring, he has been involved in various charitable organisations. His most notable contribution has been his zeal in the fight against the spread of HIV/AIDS. One of Kaunda's children was claimed by the pandemic in the 1980s. From 2002 to 2004, he was an "African President-in-Residence" at the African Presidential Archives and Research Center at Boston University.
Recently, he was seen in attendance at an episode of "Dancing with the Stars"; Kaunda is an avid ballroom dancer.
President Michael Sata made use of Kaunda as a roving ambassador for Zambia. In February 2014 Kaunda was hospitalized for a fever at Lusaka Trust Hospital.
Awards.
On 19 October 2007 Kaunda was the recipient of the 2007 Ubuntu Award.

</doc>
<doc id="17357" url="https://en.wikipedia.org/wiki?curid=17357" title="K9">
K9

K9 or K-9 may refer to:

</doc>
<doc id="17359" url="https://en.wikipedia.org/wiki?curid=17359" title="K2">
K2

K2, also known as Mount Godwin-Austen, and Chhogori (چھوغوری) Balti, is the second highest mountain in the world, after Mount Everest, at above sea level. It is located on the China-Pakistan border between Baltistan, in the Gilgit–Baltistan region of northern Pakistan, and the Taxkorgan Tajik Autonomous County of Xinjiang, China. K2 is the highest point of the Karakoram range and the highest point in Pakistan.
K2 is known as the "Savage Mountain" due to the extreme difficulty of ascent. It has the second-highest fatality rate among the eight thousanders. With around 300 successful summits and 80 fatalities, about one person dies on the mountain for every four who summit. It is more difficult and hazardous to reach the peak of K2 from the Chinese side; thus, it is usually climbed from the Pakistani side. Unlike Annapurna, the mountain with the highest fatality-to-summit rate (191 summits and 61 fatalities), K2 has never been climbed during winter.
Name.
The name K2 is derived from the notation used by the Great Trigonometric Survey of British India. Thomas Montgomerie made the first survey of the Karakoram from Mount Haramukh, some to the south, and sketched the two most prominent peaks, labeling them K1 and K2.
The policy of the Great Trigonometric Survey was to use local names for mountains wherever possible and K1 was found to be known locally as Masherbrum. K2, however, appeared not to have acquired a local name, possibly due to its remoteness. The mountain is not visible from Askole, the last village to the south, or from the nearest habitation to the north, and is only fleetingly glimpsed from the end of the Baltoro Glacier, beyond which few local people would have ventured. The name "Chogori", derived from two Balti words, "chhogo" ("big") and "ri" ("mountain") (چھوغوری) has been suggested as a local name, but evidence for its widespread use is scant. It may have been a compound name invented by Western explorers or simply a bemused reply to the question "What's that called?" It does, however, form the basis for the name "Qogir" () by which Chinese authorities officially refer to the peak. Other local names have been suggested including "Lamba Pahar" ("Tall Mountain" in Urdu) and "Dapsang", but are not widely used.
Lacking a local name, the name "Mount Godwin-Austen" was suggested, in honor of Henry Godwin-Austen, an early explorer of the area, and while the name was rejected by the Royal Geographical Society, it was used on several maps, and continues to be used occasionally.
The surveyor's mark, K2, therefore continues to be the name by which the mountain is commonly known. It is now also used in the Balti language, rendered as "Kechu" or "Ketu" (). The Italian climber Fosco Maraini argued in his account of the ascent of Gasherbrum IV that while the name of K2 owes its origin to chance, its clipped, impersonal nature is highly appropriate for so remote and challenging a mountain. He concluded that it was ...
Andre Weil named K3 surfaces in mathematics partly after the mountain K2.
Geographical setting.
K2 lies in the northwestern Karakoram Range. It is located in the Baltistan region of Gilgit–Baltistan, Pakistan and the Taxkorgan Tajik Autonomous County of Xinjiang, China. The Tarim sedimentary basin borders the range on the north and the Lesser Himalayas on the south. Melt waters from vast glaciers, such as those south and east of K2, feed agriculture in the valleys and contribute significantly to the regional fresh-water supply.
K2 is merely ranked 22nd by topographic prominence, a measure of a mountain's independent stature, because it is part of the same extended area of uplift (including the Karakoram, the Tibetan Plateau, and the Himalaya) as Mount Everest, in that it is possible to follow a path from K2 to Everest that goes no lower than , at Mustang Lo. Many other peaks, that are far lower than K2, are more independent in this sense. It is, however, the most prominent peak within the Karakoram range.
K2 is notable for its local relief as well as its total height. It stands over above much of the glacial valley bottoms at its base. It is a consistently steep pyramid, dropping quickly in almost all directions. The north side is the steepest: there it rises over above the K2 (Qogir) Glacier in only of horizontal distance. In most directions, it achieves over of vertical relief in less than .
A 1986 expedition led by George Wallerstein made an inaccurate measurement incorrectly showing that K2 was taller than Mount Everest, and therefore the tallest mountain in the world. A corrected measurement was made in 1987, but by that point the claim that K2 was the tallest mountain in the world had already made it into many news reports and reference works.
Geology.
The mountains of K2 and Broad Peak, and the area westward to the lower reaches of Sarpo Laggo glacier consist of metamorphic rocks, known as the "K2 Gneiss" and part of the Karakroam Metamorphic Complex. The K2 Gneiss consists of a mixture of orthogneiss and biotite-rich paragneiss. On the south and southeast face of K2, the orthogneiss consists of a mixture of a strongly foliated plagioclase-hornblende gneiss and a biotite-hornblende-K-feldspar orthogneiss, which has been intruded by garnet-mica leucogranitic dikes. In places, the paragneisses include clinopyroxene-hornblende-bearing psammites, garnet (grossular)-diopside marbles, and biotite-graphite phyllites. Near the memorial to the climbers, who have died on K2, above Base Camp on the south spur, thin impure marbles with quartzites and mica schists, called the "Gilkey-Puchoz sequence", are interbanded within the orthogneisses. On the west face of Broad Peak and south spur of K2, lamprophyre dikes, which consist of clinopyroxene and biotite-porphyritic vogesites and minettes, have intruded the K2 gneiss. The K2 Gneiss is separated from the surrounding sedimentary and metasedimentary rocks of the surrounding Karakroam Metamorphic Complex by normal faults. For example, a fault separates the K2 gneiss of the east face of K2 from limestones and slates comprising nearby Skyang Kangri.
40Ar/39Ar ages of 115 to 120 million years ago obtained from and geochemical analyses of the K2 Gneiss demonstrate that it is a metamorphosed, older, Cretaceous, pre-collisional granite. The granitic precursor (protolith) to the K2 Gneiss originated as the result of the production of large bodies of magma by a northward-dipping subduction zone along what was the continental margin of Asia at that time and their intrusion as batholiths into its lower continental crust. During the initial collision of the Asia and Indian plates, this granitic batholith was buried to depths of about or more, highly metamorphosed, highly deformed, and partially remelted during the Eocene Period to form gneiss. Later, the K2 Gneiss was then intruded by leucogranite dikes and finally exhumed and uplifted along major breakback thrust faults during post-Miocene time. The K2 Gneiss was exposed as the entire K2-Broad Peak-Gasherbrum range experienced rapid uplift with which erosion rates have been unable to keep pace.
Climbing history.
Early attempts.
The mountain was first surveyed by a European survey team in 1856. Team member Thomas Montgomerie designated the mountain "K2" for being the second peak of the Karakoram range. The other peaks were originally named K1, K3, K4, and K5, but were eventually renamed Masherbrum, Gasherbrum IV, Gasherbrum II, and Gasherbrum I, respectively. In 1892, Martin Conway led a British expedition that reached "Concordia" on the Baltoro Glacier.
The first serious attempt to climb K2 was undertaken in 1902 by Oscar Eckenstein, Aleister Crowley, Jules Jacot-Guillarmod, Heinrich Pfannl, Victor Wessely, and Guy Knowles via the Northeast Ridge. In the early 1900s, modern transportation did not exist: it took "fourteen days just to reach the foot of the mountain".
After five serious and costly attempts, the team reached —although considering the difficulty of the challenge, and the lack of modern climbing equipment or weatherproof fabrics, Crowley's statement that "neither man nor beast was injured" highlights the pioneering spirit and bravery of the attempt. The failures were also attributed to sickness (Crowley was suffering the residual effects of malaria), a combination of questionable physical training, personality conflicts, and poor weather conditions—of 68 days spent on K2 (at the time, the record for the longest time spent at such an altitude) only eight provided clear weather.
The next expedition to K2, in 1909, led by Prince Luigi Amedeo, Duke of the Abruzzi, reached an elevation of around on the South East Spur, now known as the "Abruzzi Spur" (or Abruzzi Ridge). This would eventually become part of the standard route but was abandoned at the time due to its steepness and difficulty. After trying and failing to find a feasible alternative route on the West Ridge or the North East Ridge, the Duke declared that K2 would never be climbed, and the team switched its attention to Chogolisa, where the Duke came within of the summit before being driven back by a storm.
The next attempt on K2 was not made until 1938, when an American expedition led by Charles Houston made a reconnaissance of the mountain. They concluded that the Abruzzi Spur was the most practical route and reached a height of around before turning back due to diminishing supplies and the threat of bad weather.
The following year, an expedition led by Fritz Wiessner came within of the summit but ended in disaster when Dudley Wolfe, Pasang Kikuli, Pasang Kitar, and Pintso disappeared high on the mountain.
Charles Houston returned to K2 to lead the 1953 American expedition. The expedition failed due to a storm that pinned the team down for 10 days at , during which time Art Gilkey became critically ill. A desperate retreat followed, during which Pete Schoening saved almost the entire team during a mass fall, and Gilkey was killed, either in an avalanche or in a deliberate attempt to avoid burdening his companions. Despite the failure and tragedy, the courage shown by the team has given the expedition iconic status in mountaineering history.
Success and repeats.
An Italian expedition finally succeeded in ascending to the summit of K2 via the Abruzzi Spur on 31 July 1954. The expedition was led by Ardito Desio, and the two climbers who reached the summit were Lino Lacedelli and Achille Compagnoni. The team included a Pakistani member, Colonel Muhammad Ata-ullah, who had been a part of the 1953 American expedition. Also on the expedition were Walter Bonatti and Pakistani Hunza porter Amir Mehdi, who both proved vital to the expedition's success in that they carried oxygen tanks to for Lacedelli and Compagnoni. The ascent is controversial because Lacedelli and Compagnoni established their camp at a higher elevation than originally agreed with Medhi and Bonatti. It being too dark to ascend or descend, Medhi and Bonatti were forced to overnight without shelter above 8,000 meters leaving the oxygen tanks behind as requested when they descended. Bonatti and Mehdi survived, but Mehdi was hospitalized for months and had to have his toes amputated because of frostbite. Efforts in the 1950s to suppress these facts to protect Lacedelli and Compagnoni's reputations as Italian national heroes were later brought to light. It was also revealed that the moving of the camp was deliberate, a move apparently made because Compagnoni feared being outshone by the younger Bonatti. Bonatti was given the blame for Medhi's hospitalization.
On 9 August 1977, 23 years after the Italian expedition, Ichiro Yoshizawa led the second successful ascent, with Ashraf Aman as the first native Pakistani climber. The Japanese expedition took the Abruzzi Spur, and used more than 1,500 porters.
The third ascent of K2 was in 1978, via a new route, the long and corniced Northeast Ridge. The top of the route traversed left across the East Face to avoid a vertical headwall and joined the uppermost part of the Abruzzi route. This ascent was made by an American team, led by James Whittaker; the summit party was Louis Reichardt, Jim Wickwire, John Roskelley, and Rick Ridgeway. Wickwire endured an overnight bivouac about below the summit, one of the highest bivouacs in history. This ascent was emotional for the American team, as they saw themselves as completing a task that had been begun by the 1938 team forty years earlier.
Another notable Japanese ascent was that of the difficult North Ridge on the Chinese side of the peak in 1982. A team from the Mountaineering Association of Japan led by Isao Shinkai and Masatsugo Konishi put three members, Naoe Sakashita, Hiroshi Yoshino, and Yukihiro Yanagisawa, on the summit on 14 August. However Yanagisawa fell and died on the descent. Four other members of the team achieved the summit the next day.
The first climber to reach the summit of K2 twice was Czech climber Josef Rakoncaj. Rakoncaj was a member of the 1983 Italian expedition led by Francesco Santon, which made the second successful ascent of the North Ridge (31 July 1983). Three years later, on 5 July 1986, he reached the summit via the Abruzzi Spur (double with Broad Peak West Face solo) as a member of Agostino da Polenza's international expedition.
The first woman to summit K2 was Pole Wanda Rutkiewicz on 23 June 1986.
In 1986, two Polish expeditions climbed via two new routes, the Magic Line and the Polish Line. This second has not yet been repeated.
In 2004 the Spanish climber Carlos Soria Fontán became the oldest person ever to summit K2, at the age of 65.
The peak has now been climbed by almost all of its ridges. Although the summit of Everest is at a higher altitude, K2 is a much more difficult and dangerous climb, due in part to its more inclement weather and comparatively greater height from base to peak. The mountain is believed by many to be the world's most difficult and dangerous climb, hence its nickname "the Savage Mountain". It, and the surrounding peaks, have claimed more lives than any others. , only 302 people have completed the ascent, compared with over 2,700 who have ascended Everest. At least 80 (as of September 2010) people have died attempting the climb.
Thirteen climbers from several expeditions died in the 1986 K2 Disaster. Another six mountaineers died on 13 August 1995, while eleven climbers died in the 2008 K2 disaster.
Recent attempts.
Despite several attempts, nobody reached the summit.
Climbing routes and difficulties.
There are a number of routes on K2, of somewhat different character, but they all share some key difficulties. First is the extreme high altitude and resulting lack of oxygen: there is only one-third as much oxygen available to a climber on the summit of K2 as there is at sea level.
Second is the propensity of the mountain to experience extreme storms of several days' duration, which have resulted in many of the deaths on the peak. Third is the steep, exposed, and committing nature of all routes on the mountain, which makes retreat more difficult, especially during a storm. Despite many attempts there have been no successful winter ascents. All major climbing routes lie on the Pakistani side, which is also where base camp is located.
Abruzzi Spur.
The standard route of ascent, used far more than any other route (75% of all climbers use this route) is the Abruzzi Spur, located on the Pakistani side, first attempted by Prince Luigi Amedeo, Duke of the Abruzzi in 1909. This is the southeast ridge of the peak, rising above the Godwin Austen Glacier. The spur proper begins at an altitude of , where Advanced Base Camp is usually placed. The route follows an alternating series of rock ribs, snow/ice fields, and some technical rock climbing on two famous features, "House's Chimney" and the "Black Pyramid." Above the Black Pyramid, dangerously exposed and difficult to navigate slopes lead to the easily visible "Shoulder", and thence to the summit. The last major obstacle is a narrow couloir known as the "Bottleneck", which places climbers dangerously close to a wall of seracs which form an ice cliff to the east of the summit. It was partly due to the collapse of one of these seracs around 2001 that no climbers summitted the peak in 2002 and 2003.
On 1 August 2008, 11 climbers from several expeditions died during a series of accidents, including several ice falls in the Bottleneck.
North Ridge.
Almost opposite from the Abruzzi Spur is the North Ridge, which ascends the Chinese side of the peak. It is rarely climbed, partly due to very difficult access, involving crossing the Shaksgam River, which is a hazardous undertaking. In contrast to the crowds of climbers and trekkers at the Abruzzi basecamp, usually at most two teams are encamped below the North Ridge. This route, more technically difficult than the Abruzzi, ascends a long, steep, primarily rock ridge to high on the mountain—Camp IV, the "Eagle's Nest" at —and then crosses a dangerously slide-prone hanging glacier by a leftward climbing traverse, to reach a snow couloir which accesses the summit.
Besides the original Japanese ascent, a notable ascent of the North Ridge was the one in 1990 by Greg Child, Greg Mortimer, and Steve Swenson, which was done alpine style above Camp 2, though using some fixed ropes already put in place by a Japanese team.
Other routes.
Because 75% of people who climb K2 use the Abruzzi Spur, these listed routes are rarely climbed. No one has climbed the East Face of the mountain, due to the instability of the snow and ice formations on that side.
Use of supplemental oxygen.
For most of its climbing history, K2 was not usually climbed with supplemental oxygen, and small, relatively lightweight teams were the norm. However the 2004 season saw a great increase in the use of oxygen: 28 of 47 summitteers used oxygen in that year.
Acclimatisation is essential when climbing without oxygen to avoid some degree of altitude sickness.
K2's summit is well above the altitude at which high altitude pulmonary edema (HAPE), or high altitude cerebral edema (HACE) can occur. In mountaineering, when ascending above an altitude of , the climber enters what is known as the "death zone".

</doc>
<doc id="17360" url="https://en.wikipedia.org/wiki?curid=17360" title="Komodo dragon">
Komodo dragon

The Komodo dragon ("Varanus komodoensis"), also known as the Komodo monitor, is a large species of lizard found in the Indonesian islands of Komodo, Rinca, Flores, Gili Motang, and Padar. A member of the monitor lizard family Varanidae, it is the largest living species of lizard, growing to a maximum length of in rare cases and weighing up to approximately .
Their unusually large size has been attributed to island gigantism, since no other carnivorous animals fill the niche on the islands where they live. However, recent research suggests the large size of Komodo dragons may be better understood as representative of a relict population of very large varanid lizards that once lived across Indonesia and Australia, most of which, along with other megafauna, died out after the Pleistocene. Fossils very similar to "V. komodoensis" have been found in Australia dating to greater than 3.8 million years ago, and its body size remained stable on Flores, one of the handful of Indonesian islands where it is currently found, over the last 900,000 years, "a time marked by major faunal turnovers, extinction of the island's megafauna, and the arrival of early hominids by 880 ka ."
As a result of their size, these lizards dominate the ecosystems in which they live. Komodo dragons hunt and ambush prey including invertebrates, birds, and mammals. It has been claimed that they have a venomous bite; there are two glands in the lower jaw which secrete several toxic proteins. The biological significance of these proteins is disputed, but the glands have been shown to secrete an anticoagulant. Komodo dragon group behaviour in hunting is exceptional in the reptile world. The diet of big Komodo dragons mainly consists of deer, though they also eat considerable amounts of carrion. Komodo dragons also occasionally attack humans in the area of West Manggarai Regency where they live in Indonesia.
Mating begins between May and August, and the eggs are laid in September. About 20 eggs are deposited in abandoned megapode nests or in a self-dug nesting hole. The eggs are incubated for seven to eight months, hatching in April, when insects are most plentiful. Young Komodo dragons are vulnerable and therefore dwell in trees, safe from predators and cannibalistic adults. They take 8 to 9 years to mature, and are estimated to live up to 30 years.
Komodo dragons were first recorded by Western scientists in 1910. Their large size and fearsome reputation make them popular zoo exhibits. In the wild, their range has contracted due to human activities, and they are listed as vulnerable by the IUCN. They are protected under Indonesian law, and a national park, Komodo National Park, was founded to aid protection efforts.
Etymology.
The Komodo dragon is also known as the Komodo monitor or the Komodo Island monitor in scientific literature, although this is not very common. To the natives of Komodo Island, it is referred to as "ora", "buaya darat" (land crocodile), or "biawak raksasa" (giant monitor).
Evolutionary history.
The evolutionary development of the Komodo dragon started with the "Varanus" genus, which originated in Asia about 40 million years ago and migrated to Australia. Around 15 million years ago, a collision between Australia and Southeast Asia allowed the varanids to move into what is now the Indonesian archipelago, extending their range as far east as the island of Timor. The Komodo dragon was believed to have differentiated from its Australian ancestors 4 million years ago. However, recent fossil evidence from Queensland suggests the Komodo dragon evolved in Australia before spreading to Indonesia. Dramatic lowering of sea level during the last glacial period uncovered extensive stretches of continental shelf that the Komodo dragon colonised, becoming isolated in their present island range as sea levels rose afterwards.
Description.
In the wild, an adult Komodo dragon usually weighs around , although captive specimens often weigh more. According to the Guinness Book of World Records, an average adult male will weigh and measure , while an average female will weigh and measure . The largest verified wild specimen was long and weighed , including undigested food. The Komodo dragon has a tail as long as its body, as well as about 60 frequently replaced, serrated teeth that can measure up to in length. Its saliva is frequently blood-tinged, because its teeth are almost completely covered by gingival tissue that is naturally lacerated during feeding. This creates an ideal culture for the bacteria that live in its mouth. It also has a long, yellow, deeply forked tongue. Komodo dragon skin is reinforced by armoured scales, which contain tiny bones called osteoderms that function as a sort of natural chain-mail. This rugged hide makes Komodo dragon skin poorly suited for making into leather.
Senses.
As with other varanids, Komodo dragons have only a single ear bone, the stapes, for transferring vibrations from the tympanic membrane to the cochlea. This arrangement means they are likely restricted to sounds in the 400 to 2,000 hertz range, compared to humans who hear between 20 and 20,000 hertz. It was formerly thought to be deaf when a study reported no agitation in wild Komodo dragons in response to whispers, raised voices, or shouts. This was disputed when London Zoological Garden employee Joan Proctor trained a captive specimen to come out to feed at the sound of her voice, even when she could not be seen.
The Komodo dragon can see objects as far away as , but because its retinas only contain cones, it is thought to have poor night vision. The Komodo dragon is able to see in colour, but has poor visual discrimination of stationary objects.
The Komodo dragon uses its tongue to detect, taste, and smell stimuli, as with many other reptiles, with the vomeronasal sense using the Jacobson's organ, rather than using the nostrils. With the help of a favorable wind and its habit of swinging its head from side to side as it walks, a Komodo dragon may be able to detect carrion from away. It only has a few taste buds in the back of its throat. Its scales, some of which are reinforced with bone, have sensory plaques connected to nerves to facilitate its sense of touch. The scales around the ears, lips, chin, and soles of the feet may have three or more sensory plaques.
Behaviour and ecology.
The Komodo dragon prefers hot and dry places, and typically lives in dry, open grassland, savanna, and tropical forest at low elevations. As an ectotherm, it is most active in the day, although it exhibits some nocturnal activity. Komodo dragons are solitary, coming together only to breed and eat. They are capable of running rapidly in brief sprints up to , diving up to , and climbing trees proficiently when young through use of their strong claws. To catch out-of-reach prey, the Komodo dragon may stand on its hind legs and use its tail as a support. As it matures, its claws are used primarily as weapons, as its great size makes climbing impractical.
For shelter, the Komodo dragon digs holes that can measure from wide with its powerful forelimbs and claws. Because of its large size and habit of sleeping in these burrows, it is able to conserve body heat throughout the night and minimise its basking period the morning after. The Komodo dragon hunts in the afternoon, but stays in the shade during the hottest part of the day. These special resting places, usually located on ridges with cool sea breezes, are marked with droppings and are cleared of vegetation. They serve as strategic locations from which to ambush deer.
Diet.
Komodo dragons are carnivores. Although they eat mostly carrion, they will also ambush live prey with a stealthy approach. When suitable prey arrives near a dragon's ambush site, it will suddenly charge at the animal and go for the underside or the throat. It is able to locate its prey using its keen sense of smell, which can locate a dead or dying animal from a range of up to . Komodo dragons have been observed knocking down large pigs and deer with their strong tails.
Komodo dragons eat by tearing large chunks of flesh and swallowing them whole while holding the carcass down with their forelegs. For smaller prey up to the size of a goat, their loosely articulated jaws, flexible skulls, and expandable stomachs allow them to swallow prey whole. The vegetable contents of the stomach and intestines are typically avoided. Copious amounts of red saliva the Komodo dragons produce help to lubricate the food, but swallowing is still a long process (15–20 minutes to swallow a goat). A Komodo dragon may attempt to speed up the process by ramming the carcass against a tree to force it down its throat, sometimes ramming so forcefully, the tree is knocked down. A small tube under the tongue that connects to the lungs allows it to breathe while swallowing. After eating up to 80% of its body weight in one meal, it drags itself to a sunny location to speed digestion, as the food could rot and poison the dragon if left undigested for too long. Because of their slow metabolism, large dragons can survive on as few as 12 meals a year. After digestion, the Komodo dragon regurgitates a mass of horns, hair, and teeth known as the gastric pellet, which is covered in malodorous mucus. After regurgitating the gastric pellet, it rubs its face in the dirt or on bushes to get rid of the mucus, suggesting it does not relish the scent of its own excretions.
The largest animals eat first, while the smaller ones follow a hierarchy. The largest male asserts his dominance and the smaller males show their submission by use of body language and rumbling hisses. Dragons of equal size may resort to "wrestling". Losers usually retreat, though they have been known to be killed and eaten by victors.
The Komodo dragon's diet is wide-ranging, and includes invertebrates, other reptiles (including smaller Komodo dragons), birds, bird eggs, small mammals, monkeys, wild boar, goats, deer, horses, and water buffalo. Young Komodos will eat insects, eggs, geckos, and small mammals. Occasionally, they attack and bite humans (see first paragraphs of this article). Sometimes they consume human corpses, digging up bodies from shallow graves. This habit of raiding graves caused the villagers of Komodo to move their graves from sandy to clay ground and pile rocks on top of them to deter the lizards. The Komodo dragon may have evolved to feed on the extinct dwarf elephant "Stegodon" that once lived on Flores, according to evolutionary biologist Jared Diamond.
The Komodo dragon drinks by sucking water into its mouth via buccal pumping (a process also used for respiration), lifting its head, and letting the water run down its throat.
Saliva.
Auffenberg described the Komodo dragon as having septic pathogens in its saliva (he described the saliva as "reddish and copious"), specifically the bacteria "E. coli", "Staphylococcus sp.", "Providencia sp.", "Proteus morgani", and "P. mirabilis". He noted, while these pathogens can be found in the mouths of wild Komodo dragons, they disappear from the mouths of captive animals, due to cleaner diets and the use of antibiotics. This was verified by taking mucous samples from the external gum surfaces of the upper jaws of two freshly captured individuals. Saliva samples were analysed by researchers at the University of Texas, who found 57 strains of bacteria growing in the mouths of three wild Komodo dragons, including "Pasteurella multocida". The rapid growth of these bacteria was noted by Fredeking: "Normally it takes about three days for a sample of "P. multocida" to cover a Petri dish; ours took eight hours. We were very taken aback by how virulent these strains were". This study supported the observation that wounds inflicted by the Komodo dragon are often associated with sepsis and subsequent infections in prey animals. How the Komodo dragon is unaffected by these virulent bacteria remains a mystery.
Research in 2013 suggested that the bacteria in the mouths of Komodo dragons are ordinary and similar to those found in other carnivores. They actually have surprisingly good mouth hygiene. As Bryan Fry put it: "After they are done feeding, they will spend 10 to 15 minutes lip-licking and rubbing their head in the leaves to clean their mouth... Unlike people have been led to believe, they do not have chunks of rotting flesh from their meals on their teeth, cultivating bacteria." The observation of prey dying of sepsis would then be explained by the natural instinct of water buffalos, who are not native to the islands where the Komodo dragon lives, to run into water when attacked. The warm, faeces-filled water would then cause the infections. The study used samples from 16 captive dragons (10 adults and six neonates) from three U.S. zoos.
Venom.
In late 2005, researchers at the University of Melbourne speculated the perentie ("Varanus giganteus"), other species of monitors, and agamids may be somewhat venomous. The team believes the immediate effects of bites from these lizards were caused by mild envenomation. Bites on human digits by a lace monitor ("V. varius"), a Komodo dragon, and a spotted tree monitor ("V. scalaris") all produced similar effects: rapid swelling, localised disruption of blood clotting, and shooting pain up to the elbow, with some symptoms lasting for several hours.
In 2009, the same researchers published further evidence demonstrating Komodo dragons possess a venomous bite. MRI scans of a preserved skull showed the presence of two glands in the lower jaw. The researchers extracted one of these glands from the head of a terminally ill specimen in the Singapore Zoological Gardens, and found it secreted several different toxic proteins. The known functions of these proteins include inhibition of blood clotting, lowering of blood pressure, muscle paralysis, and the induction of hypothermia, leading to shock and loss of consciousness in envenomated prey. As a result of the discovery, the previous theory that bacteria were responsible for the deaths of Komodo victims was disputed.
Kurt Schwenk, an evolutionary biologist at the University of Connecticut, finds the discovery of these glands intriguing, but considers most of the evidence for venom in the study to be "meaningless, irrelevant, incorrect or falsely misleading". Even if the lizards have venom-like proteins in their mouths, Schwenk argues, they may be using them for a different function, and he doubts venom is necessary to explain the effect of a Komodo dragon bite, arguing that shock and blood loss are the primary factors.
Other scientists such as Washington State University's Biologist Kenneth V. Kardong and Toxicologists Scott A. Weinstein and Tamara L. Smith, have stated that this allegation of venom glands "has had the effect of underestimating the variety of complex roles played by oral secretions in the biology of reptiles, produced a very narrow view of oral secretions and resulted in misinterpretation of reptilian evolution". According to these scientists "reptilian oral secretions contribute to many biological roles other than to quickly dispatch prey". These researchers concluded that, "Calling all in this clade venomous implies an overall potential danger that does not exist, misleads in the assessment of medical risks, and confuses the biological assessment of squamate biochemical systems".
Reproduction.
Mating occurs between May and August, with the eggs laid in September. During this period, males fight over females and territory by grappling with one another upon their hind legs, with the loser eventually being pinned to the ground. These males may vomit or defecate when preparing for the fight. The winner of the fight will then flick his long tongue at the female to gain information about her receptivity. Females are antagonistic and resist with their claws and teeth during the early phases of courtship. Therefore, the male must fully restrain the female during coitus to avoid being hurt. Other courtship displays include males rubbing their chins on the female, hard scratches to the back, and licking. Copulation occurs when the male inserts one of his hemipenes into the female's cloaca. Komodo dragons may be monogamous and form "pair bonds", a rare behavior for lizards.
Female Komodos lay their eggs from August to September and may use several types of locality; in one study, 60% laid their eggs in the nests of orange-footed scrubfowl (a moundbuilder or megapode), 20% on ground level and 20% in hilly areas. The females make many camouflage nests/holes to prevent other dragons from eating the eggs. Clutches contain an average of 20 eggs, which have an incubation period of 7–8 months. Hatching is an exhausting effort for the neonates, which break out of their eggshells with an egg tooth that falls off soon after. After cutting themselves out, the hatchlings may lie in their eggshells for hours before starting to dig out of the nest. They are born quite defenseless and are vulnerable to predation. Sixteen youngsters from a single nest were on average 46.5 cm long and weighed 105.1 grams.
Young Komodo dragons spend much of their first few years in trees, where they are relatively safe from predators, including cannibalistic adults, as juvenile dragons make up 10% of their diets. The habit of cannibalism may be advantageous in sustaining the large size of adults, as medium-sized prey on the islands is rare. When the young approach a kill, they roll around in faecal matter and rest in the intestines of eviscerated animals to deter these hungry adults. Komodo dragons take approximately 8 to 9 years to mature, and may live for up to 30 years.
Parthenogenesis.
A Komodo dragon at London Zoo named Sungai laid a clutch of eggs in late 2005 after being separated from male company for more than two years. Scientists initially assumed she had been able to store sperm from her earlier encounter with a male, an adaptation known as superfecundation. On 20 December 2006, it was reported that Flora, a captive Komodo dragon living in the Chester Zoo in England, was the second known Komodo dragon to have laid unfertilised eggs: she laid 11 eggs, and seven of them hatched, all of them male. Scientists at Liverpool University in England performed genetic tests on three eggs that collapsed after being moved to an incubator, and verified Flora had never been in physical contact with a male dragon. After Flora's eggs' condition had been discovered, testing showed Sungai's eggs were also produced without outside fertilization. On 31 January 2008, the Sedgwick County Zoo in Wichita, Kansas, became the first zoo in the Americas to document parthenogenesis in Komodo dragons. The zoo has two adult female Komodo dragons, one of which laid about 17 eggs on 19–20 May 2007. Only two eggs were incubated and hatched due to space issues; the first hatched on 31 January 2008, while the second hatched on 1 February. Both hatchlings were males.
Komodo dragons have the ZW chromosomal sex-determination system, as opposed to the mammalian XY system. Male progeny prove Flora's unfertilised eggs were haploid (n) and doubled their chromosomes later to become diploid (2n) (by being fertilised by a polar body, or by chromosome duplication without cell division), rather than by her laying diploid eggs by one of the meiosis reduction-divisions in her ovaries failing. When a female Komodo dragon (with ZW sex chromosomes) reproduces in this manner, she provides her progeny with only one chromosome from each of her pairs of chromosomes, including only one of her two sex chromosomes. This single set of chromosomes is duplicated in the egg, which develops parthenogenetically. Eggs receiving a Z chromosome become ZZ (male); those receiving a W chromosome become WW and fail to develop, meaning that only males are produced by parthenogenesis in this species.
It has been hypothesised that this reproductive adaptation allows a single female to enter an isolated ecological niche (such as an island) and by parthenogenesis produce male offspring, thereby establishing a sexually reproducing population (via reproduction with her offspring that can result in both male and female young). Despite the advantages of such an adaptation, zoos are cautioned that parthenogenesis may be detrimental to genetic diversity.
History.
Discovery by the Western world.
Komodo dragons were first documented by Europeans in 1910, when rumors of a "land crocodile" reached Lieutenant van Steyn van Hensbroek of the Dutch colonial administration. Widespread notoriety came after 1912, when Peter Ouwens, the director of the Zoological Museum at Bogor, Java, published a paper on the topic after receiving a photo and a skin from the lieutenant, as well as two other specimens from a collector. The first two live Komodo dragons to arrive in Europe were exhibited in the Reptile House at London Zoo when it opened in 1927. Joan Beauchamp Procter made some of the earliest observations of these animals in captivity and she demonstrated the behaviour of one of these animals at a Scientific Meeting of the Zoological Society of London in 1928. The Komodo dragon was the driving factor for an expedition to Komodo Island by W. Douglas Burden in 1926. After returning with 12 preserved specimens and 2 live ones, this expedition provided the inspiration for the 1933 movie "King Kong". It was also Burden who coined the common name "Komodo dragon." Three of his specimens were stuffed and are still on display in the American Museum of Natural History.
Studies.
The Dutch, realizing the limited number of individuals in the wild, outlawed sport hunting and heavily limited the number of individuals taken for scientific study. Collecting expeditions ground to a halt with the occurrence of World War II, not resuming until the 1950s and 1960s, when studies examined the Komodo dragon's feeding behavior, reproduction, and body temperature. At around this time, an expedition was planned in which a long-term study of the Komodo dragon would be undertaken. This task was given to the Auffenberg family, who stayed on Komodo Island for 11 months in 1969. During their stay, Walter Auffenberg and his assistant Putra Sastrawan captured and tagged more than 50 Komodo dragons. The research from the Auffenberg expedition would prove to be enormously influential in raising Komodo dragons in captivity. Research after that of the Auffenberg family has shed more light on the nature of the Komodo dragon, with biologists such as Claudio Ciofi continuing to study the creatures.
Conservation.
The Komodo dragon is a vulnerable species and is on the IUCN Red List. The Komodo National Park was founded in 1980 to protect Komodo dragon populations on islands including Komodo, Rinca, and Padar. Later, the Wae Wuul and Wolo Tado Reserves were opened on Flores to aid with Komodo dragon conservation.
Komodo dragons avoid encounters with humans. Juveniles are very shy and will flee quickly into a hideout if a human comes closer than about . Older animals will also retreat from humans from a shorter distance away. If cornered, they will react aggressively by gaping their mouth, hissing, and swinging their tail. If they are disturbed further, they may start an attack and bite. Although there are anecdotes of unprovoked Komodo dragons attacking or preying on humans, most of these reports are either not reputable or caused by defensive bites. Only a very few cases are truly the result of unprovoked attacks by abnormal individuals, which lost their fear towards humans.
Volcanic activity, earthquakes, loss of habitat, fire, loss of prey due to poaching, tourism, and illegal poaching of the dragons themselves have all contributed to the vulnerable status of the Komodo dragon. Under Appendix I of CITES (the Convention on International Trade in Endangered Species), commercial trade of skins or specimens is illegal.
In 2013, total population in the wild was assessed as 3,222 individuals, declining to 3,092 in 2014 and 3,014 in 2015. Populations remained relatively stable on the bigger islands (Komodo and Rinca), but decreased on smaller island such as Nusa Kode and Gili Motang, likely due to diminishing prey availability.
On Padar, a former population of the Komodo dragon became extinct, of which the last individuals were seen in 1975. It is widely assumed that the Komodo dragon died out on Padar after a strong decline of the populations of large ungulate prey, for which poaching was most likely responsible.
In captivity.
Komodo dragons have long been great zoo attractions, where their size and reputation make them popular exhibits. They are, however, rare in zoos because they are susceptible to infection and parasitic disease if captured from the wild, and do not readily reproduce. As of May 2009, there were 13 European, 2 African, 35 North American, 1 Singaporean, and 2 Australian institutions that kept Komodo dragons.
The first Komodo dragons were displayed at London Zoo in 1927. A Komodo dragon was exhibited in 1934 at the National Zoo in Washington, D.C., but it lived for only two years. More attempts to exhibit Komodo dragons were made, but the lifespan of these animals was very short, averaging five years in the National Zoological Park. Studies done by Walter Auffenberg, which were documented in his book "The Behavioral Ecology of the Komodo Monitor", eventually allowed for more successful managing and reproducing of the dragons in captivity.
A variety of behaviors have been observed from captive specimens. Most individuals are relatively tame within a short time, and are capable of recognising individual humans and discriminating between familiar keepers. Komodo dragons have also been observed to engage in play with a variety of objects, including shovels, cans, plastic rings, and shoes. This behavior does not seem to be "food-motivated predatory behavior".
Even seemingly docile dragons may become unpredictably aggressive, especially when the animal's territory is invaded by someone unfamiliar. In June 2001, a Komodo dragon seriously injured Phil Bronstein, the then husband of actress Sharon Stone, when he entered its enclosure at the Los Angeles Zoo after being invited in by its keeper. Bronstein was bitten on his bare foot, as the keeper had told him to take off his white shoes and socks, which the keeper stated could potentially excite the Komodo dragon as they were the same colour as the white rats the zoo fed the dragon. Although he escaped, Bronstein needed to have several tendons in his foot reattached surgically.

</doc>
<doc id="17361" url="https://en.wikipedia.org/wiki?curid=17361" title="Kiln">
Kiln

A kiln (originally pronounced “"kill",” with the “"n"” silent, however vocalizing the “"n"” has become popular.) is a thermally insulated chamber, a type of oven, that produces temperatures sufficient to complete some process, such as hardening, drying, or chemical changes. Various industries and trades use kilns to harden objects made from clay into pottery, tiles and bricks. Various industries use rotary kilns for pyroprocessing—to calcinate ores, produce cement, lime, and many other materials.
Pronunciation.
Kiln was originally pronounced /kɪl/, with a silent "n". The spelling pronunciation /kɪln/ is now also common.
Uses of kilns.
The earliest known kiln dates to around 6000 BC, and was found at the Yarim Tepe site in modern Iraq. Neolithic kilns were able to produce temperatures greater than 900 °C (1652 °F).
Uses include:
Ceramic kilns.
Kilns are an essential part of the manufacture of all ceramics. Ceramics require heat at high temperatures so chemical and physical reactions will occur to permanently alter the unfired body. In the case of pottery, clay materials are shaped, dried and then fired in a kiln. The final characteristics are determined by the composition and preparation of the clay body, by the temperature at which it is fired. After a first firing glazes may be used and the ware is fired a second time to fuse the glaze into the body. A third firing as a lower temperature may be required to fix overglaze decoration. Modern kilns often have sophisticated electrical control systems to firing regime, although pyrometric devices are often also used.
Clay consists of fine-grained particles, that are relatively weak and porous. Clay is combined with other minerals to create a workable clay body. Part of the firing process includes sintering. This heats the clay until the particles partially melt and flow together, creating a strong, single mass, composed of a glassy phase interspersed with pores and crystalline material. Through firing, the pores are reduced in size, causing the material to shrink slightly. This crystalline material is predominantly consists of silicon and aluminium oxides.
Types of kiln.
In the broadest terms, there are two types of kiln: intermittent and continuous, both sharing the same basic characteristics of being an insulated box with a controlled inner temperature and atmosphere.
A continuous kiln, sometimes called a tunnel kiln, is a long structure in which only the central portion is directly heated. From the cool entrance, ware is slowly transported through the kiln, and its temperature is increased steadily as it approaches the central, hottest part of the kiln. From there, it continues through the kiln, and the surrounding temperature is reduced until it exits the kiln nearly at room temperature. A continuous kiln is energy-efficient, because heat given off during cooling is recycled to pre-heat the incoming ware. In some designs, the ware is left in one place, while the heating zone moves across it. Kilns in this type include:
In the intermittent kiln. the ware to be fired is placed into the kiln. The kiln is closed, and the internal temperature increased according to a schedule. After the firing is completed, both the kiln and the ware are cooled. The ware is removed, the kiln is cleaned and the next cycle begins. Kilns in this type include:
Traditional kilns.
Kiln technology is very old. The development of the kiln from a simple earthen trench filled with pots and fuel, pit firing, to modern methods happened in stages. One improvement was to build a firing chamber around pots with baffles and a stoking hole. This conserved heat. A chimney stack improves the air flow or "draw" of the kiln, thus burning the fuel more completely. Early examples of kilns found in Britain include those that made roof-tiles during the Roman occupation. These kilns were built up the side of a slope, such that a fire could be lit at the bottom and the heat would rise up into the kiln.
Traditional kilns include:
Modern kilns.
With the industrial age, kilns were designed to use electricity and more refined fuels, including natural gas and propane. Many large industrial pottery kilns use natural gas, as it is generally clean, efficient and easy to control. Modern kilns can be fitted with computerized controls allowing for fine adjustments during the firing. A user may choose to control the rate of temperature climb or "ramp", "hold" or "soak" the temperature at any given point, or control the rate of cooling. Both electric and gas kilns are common for smaller scale production in industry and craft, handmade and sculptural work.
Modern kilns include:
Wood-drying kiln.
Green wood coming straight from the felled tree has far too high a moisture content to be commercially useful and will rot, warp and split. Both hardwoods and softwood must be left to dry out until the moisture content is between 18% and 8%. This can be a long process, or it is speeded up by use of a kiln. A variety of kiln technologies exist today: conventional, dehumidification, solar, vacuum and radio frequency.
The economics of different wood drying technologies are based on the total energy, capital, insurance/risk, environmental impacts, labor, maintenance, and product degradation costs. These costs which can be a significant part of plant costs, involve the differential impact of the presence of drying equipment in a specific plant. Every piece of equipment from the green trimmer to the infeed system at the planer mill is part the "drying system". The true costs of the drying system can only be determined when comparing the total plant costs and risks with and without drying.
Kiln dried firewood was pioneered during the 1980s, this was later adopted extensively in Europe due to the economic and practical benefits of selling wood with a lower moisture content.
The total (harmful) air emissions produced by wood kilns, including their heat source, can be significant. Typically, the higher the temperature the kiln operates at, the larger amount of emissions are produced (per pound of water removed). This is especially true in the drying of thin veneers and high-temperature drying of softwoods.

</doc>
<doc id="17362" url="https://en.wikipedia.org/wiki?curid=17362" title="Kiwi">
Kiwi

Kiwi (pronounced ) or kiwis are flightless birds native to New Zealand, in the genus Apteryx and family Apterygidae. At around the size of a domestic chicken, kiwi are by far the smallest living ratites (which also consist of ostriches, emus, rheas, and cassowaries), and lay the largest egg in relation to their body size of any species of bird in the world. DNA sequence comparisons have yielded the surprising conclusion that kiwi are much more closely related to the extinct Malagasy elephant birds than to the moa with which they shared New Zealand. There are five recognised species, two of which are currently vulnerable, one of which is endangered, and one of which is critically endangered. All species have been negatively affected by historic deforestation but currently the remaining large areas of their forest habitat are well protected in reserves and national parks. At present, the greatest threat to their survival is predation by invasive mammalian predators.
The kiwi is a national symbol of New Zealand, and the association is so strong that the term "Kiwi" is used internationally as the colloquial demonym for New Zealanders.
Etymology.
The Māori language word "kiwi" ( ) is generally accepted to be "of imitative origin" from the call. However, some linguists derive the word from Proto-Nuclear Polynesian "*kiwi", which refers to "Numenius tahitiensis", the bristle-thighed curlew, a migratory bird that winters in the tropical Pacific islands. With its long decurved bill and brown body, the curlew resembles the kiwi. So when the first Polynesian settlers arrived, they may have applied the word kiwi to the new-found bird. The genus name "Apteryx" is derived from Ancient Greek "without wing": "a-", "without" or "not"; "pterux", "wing".
Taxonomy and systematics.
Although it was long presumed that the kiwi was closely related to the other New Zealand ratites, the moa, recent DNA studies have identified its closest relative as the extinct elephant bird of Madagascar, and among extant ratites, the kiwi is more closely related to the emu and the cassowaries than to the moa.
Research published in 2013 on an extinct genus, "Proapteryx", known from the Miocene deposits of the Saint Bathans Fauna, found that it was smaller and probably capable of flight, supporting the hypothesis that the ancestor of the kiwi reached New Zealand independently from moas, which were already large and flightless by the time kiwis appeared.
Species.
There are five known species of kiwi, as well as a number of subspecies.
Relationships in the genus "Apteryx"
Description.
Their adaptation to a terrestrial life is extensive: like all the other ratites (ostrich, emu, rhea and cassowary), they have no keel on the sternum to anchor wing muscles. The vestigial wings are so small that they are invisible under the bristly, hair-like, two-branched feathers. While most adult birds have bones with hollow insides to minimise weight and make flight practicable, kiwi have marrow, like mammals and the young of other birds. With no constraints on weight due to flight requirements, brown kiwi females carry and lay a single egg that may weigh as much as . Like most other ratites, they have no uropygial gland (preen gland). Their bill is long, pliable and sensitive to touch, and their eyes have a reduced pecten. Their feathers lack barbules and aftershafts, and they have large vibrissae around the gape. They have 13 flight feathers, no tail and a small pygostyle. Their gizzard is weak and their caecum is long and narrow.
Behaviour and ecology.
Before the arrival of humans in the 13th century or earlier, New Zealand's only endemic mammals were , and the ecological niches that in other parts of the world were filled by creatures as diverse as horses, wolves and mice were taken up by birds (and, to a lesser extent, reptiles, insects and gastropods).
Kiwi are shy and usually nocturnal. Their mostly nocturnal habits may be a result of habitat intrusion by predators, including humans. In areas of New Zealand where introduced predators have been removed, such as sanctuaries, kiwi are often seen in daylight. They prefer subtropical and temperate podocarp and beech forests, but they are being forced to adapt to different habitat, such as sub-alpine scrub, tussock grassland, and the mountains. Kiwi have a highly developed sense of smell, unusual in a bird, and are the only birds with nostrils at the end of their long beaks. Kiwi eat small invertebrates, seeds, grubs, and many varieties of worms. They also may eat fruit, small crayfish, eels and amphibians. Because their nostrils are located at the end of their long beaks, kiwi can locate insects and worms underground using their keen sense of smell, without actually seeing or feeling them.
Once bonded, a male and female kiwi tend to live their entire lives as a monogamous couple. During the mating season, June to March, the pair call to each other at night, and meet in the nesting burrow every three days. These relationships may last for up to 20 years. They are unusual among other birds in that, along with some raptors, they have a functioning pair of ovaries. (In most birds and in platypuses, the right ovary never matures, so that only the left is functional.) Kiwi eggs can weigh up to one-quarter the weight of the female. Usually, only one egg is laid per season. The kiwi lays the biggest egg in proportion to its size of any bird in the world, so even though the kiwi is about the size of a domestic chicken, it is able to lay eggs that are about six times the size of a chicken's egg. The eggs are smooth in texture, and are ivory or greenish white. The male incubates the egg, except for the Great Spotted Kiwi, "A. haastii", in which both parents are involved. The incubation period is 63–92 days. Producing the huge egg places significant physiological stress on the female; for the thirty days it takes to grow the fully developed egg, the female must eat three times her normal amount of food. Two to three days before the egg is laid there is little space left inside the female for her stomach and she is forced to fast.
Status and conservation.
Nationwide studies show that, on average, only five percent of kiwi chicks survive to adulthood. However, in areas under active pest management, survival rates for North Island brown kiwi can be far higher. For example, prior to a joint 1080 poison operation undertaken by DOC and the Animal Health Board in Tongariro Forest in 2006, 32 kiwi chicks were radio-tagged. 57% of the radio-tagged chicks survived to adulthood. Thanks to ongoing pest control, the adult kiwi population at Tongariro has almost doubled since 1998.
Sanctuaries.
In 2000, the Department of Conservation set up five kiwi sanctuaries focused on developing methods to protect kiwi and to increase their numbers.
A number of other mainland conservation islands and fenced sanctuaries have significant populations of kiwi, including:
North island brown kiwi were introduced to the Cape Sanctuary in Hawke's Bay between 2008 and 2011, which in turn provided captive-raised chicks that were released back into Maungataniwha Native Forest.
Operation "Nest Egg".
Operation Nest Egg is a programme run by the BNZ Save the Kiwi Trust—a partnership between the Bank of New Zealand, the Department of Conservation and the Royal Forest and Bird Protection Society. Kiwi eggs and chicks are removed from the wild and hatched and/or raised in captivity until big enough to fend for themselves—usually when they weigh around 1200 grams (42 ounces). They are then returned to the wild. An Operation Nest Egg bird has a 65% chance of surviving to adulthood—compared to just 5% for wild-hatched and raised chicks. The tool is used on all kiwi species except little spotted kiwi.
1080 poison.
In 2004, anti-1080 activist Phillip Anderton posed for the New Zealand media with a kiwi he claimed had been poisoned. An investigation revealed that Anderton lied to journalists and the public. He had used a kiwi that had been caught in a possum trap. Extensive monitoring shows that kiwi are not at risk from the use of biodegradable 1080 poison.
Threats.
Introduced mammalian predators, namely stoats, dogs, ferrets, and cats, are the number one threat to kiwi. Other threats include habitat modification/loss and motor vehicle strike. The restricted distribution and small size of some kiwi populations increases their vulnerability to inbreeding.
Stoats are responsible for approximately half of kiwi chick deaths in many areas through New Zealand. Cats also to a lesser extent prey on kiwi chicks. Research has shown that the combined effect of predators and other mortality (accidents etc.) results in less than 5% of kiwi chicks surviving to adulthood. Young kiwi chicks are vulnerable to stoat predation until they reach about 1–1.2 kg in weight, at which time they can usually defend themselves.
Ferrets and dogs often kill adult kiwi. These predators can cause large and abrupt declines in populations. In particular, dogs find the distinctive strong scent of kiwi irresistible and easy to track, such that they can catch and kill kiwi in seconds. Motor vehicle strike is a threat to all kiwi where roads cross through their habitat. Badly set possum traps often kill or maim kiwi.
Relationship to humans.
The Māori traditionally believed that kiwi were under the protection of Tane Mahuta, god of the forest. They were used as food and their feathers were used for kahu kiwi—ceremonial cloaks. Today, while kiwi feathers are still used, they are gathered from birds that die naturally or through road accidents or predation, or from captive birds. Kiwi are no longer hunted and some Maori consider themselves the birds' guardians.
Scientific documentation.
The first kiwi specimen to be studied by Europeans was a kiwi skin brought to George Shaw by Captain Andrew Barclay aboard the ship "Providence", who was reported to have been given it by a sealer in Port Jackson (Sydney Harbour) around 1811. George Shaw gave the bird its scientific name and drew sketches of the way he imagined a live bird to look which appeared as plates 1057 and 1058 in volume 24 of "The Naturalist's Miscellany" in 1813. 
Zoos.
In 1851, London Zoo became the first zoo to keep kiwi. The first captive breeding took place in 1945. As of 2007 only 13 zoos outside New Zealand hold kiwi. The Frankfurt Zoo has 12, the Berlin Zoo has seven, Walsrode Bird Park has one, the Washington Zoo has three, the Avifauna Bird Park in the Netherlands has three, the San Diego Zoo has five, the San Diego Zoo Safari Park has one, the National Zoo in Washington, DC has five, the Smithsonian Conservation Biology Institute has one, and the Columbus Zoo and Aquarium has three.
As a national symbol.
The kiwi as a symbol first appeared in the late 19th century in New Zealand regimental badges. It was later featured in the badges of the South Canterbury Battalion in 1886 and the Hastings Rifle Volunteers in 1887. Soon after, the kiwi appeared in many military badges; and in 1906, when Kiwi Shoe Polish was widely sold in the UK and the US, the symbol became more widely known.
During the First World War, the name ""kiwi"" for New Zealand soldiers came into general use, and a giant kiwi (now known as the Bulford kiwi), was carved on the chalk hill above Sling Camp in England. Usage has become so widespread that all New Zealanders overseas and at home are now commonly referred to as "Kiwis".
The kiwi has since become the most well-known national symbol for New Zealand, and the bird is prominent in the coat of arms, crests and badges of many New Zealand cities, clubs and organisations; at the national level, the red silhouette of a kiwi is in the centre of the roundel of the Royal New Zealand Air Force. The kiwi is featured in the logo of the New Zealand Rugby League, and the New Zealand national rugby league team are nicknamed the Kiwis.
The reverse of a New Zealand dollar coin contains an image of a kiwi, and in currency trading the New Zealand dollar is often referred to as "the kiwi".
References.
Notes
Further reading

</doc>
<doc id="17363" url="https://en.wikipedia.org/wiki?curid=17363" title="Kiwifruit">
Kiwifruit

The kiwifruit or Chinese gooseberry (often shortened to kiwi) is the edible berry of a woody vine in the genus "Actinidia". The most common cultivar group of kiwifruit ('Hayward') is oval, about the size of a large hen's egg ( in length and in diameter). It has a fibrous, dull greenish-brown skin and bright green or golden flesh with rows of tiny, black, edible seeds. The fruit has a soft texture and a sweet but unique flavor. It is a commercial crop in several countries, such as Italy, New Zealand, Chile, Greece, and France.
Etymology.
The word kiwifruit and shortened "kiwi" have been used since around 1966 when the fruit was first imported from New Zealand to the United States.
The alternate name, "Chinese gooseberry", arose among growers and consumers in Europe when Chinese imports began in the early 1900s. It replaced the Chinese name, "yang tao", meaning "strawberry peach", which was used in Europe apparently due to the similarity of taste and color of the flesh with the "Ribes" gooseberry already popular throughout Europe. In 1962, New Zealand growers began calling it "kiwifruit" to give it more market appeal, a name becoming commercially adopted in 1974.
Kiwifruit has since become a common name for all commercially grown fruit from the genus "Actinidia". In New Zealand, however, the word "kiwi" usually refers to the kiwi bird or the people of New Zealand, and is seldom used to refer to the fruit.
History.
Kiwifruit is native to north-central and eastern China. Cultivation of the fuzzy kiwifruit spread from China in the early 20th century to New Zealand, where the first commercial plantings occurred. Although kiwifruit is a national fruit of China, until recently, China was not a major producing country of kiwifruit, as it was traditionally collected from the wild. The fruit became popular with American servicemen stationed in New Zealand during World War II and later exported to California using the names "Chinese gooseberry" and "melonette". In 1962, New Zealand growers began calling it "kiwifruit" to give it more market appeal, and a California-based importer subsequently used that name when introducing the fruit to the American market.
Cultivars.
The genus "Actinidia" contains around 60 species. Though most kiwifruit are easily recognized as kiwifruit (due to basic shape) their fruit is quite variable. The skin of the fruit can vary in size, shape, hairiness, and color. The flesh can vary in color, juiciness, texture, and taste. Some fruits are unpalatable while others taste considerably better than the majority of the commercial varieties.
The most common kiwifruit is the fuzzy kiwifruit, from the species "A. deliciosa". Other species that are commonly eaten include golden kiwifruit ("A. chinensis"), Chinese egg gooseberry ("A. coriacea"), baby kiwifruit ("A. arguta"), Arctic kiwifruit ("A. kolomikta"), red kiwifruit ("A. melanandra"), silver vine ("A. polygama"), purple kiwifruit ("A. purpurea").
Fuzzy kiwifruit.
Almost all kiwifruit sold belong to a few cultivars of fuzzy kiwi ("Actinidia deliciosa"): 'Hayward', 'Blake', and 'Saanichton 12'. They have a fuzzy, dull-brown skin, and bright-green flesh. The familiar cultivar 'Hayward' was developed by Hayward Wright in Avondale, New Zealand, around 1924. It was initially grown in domestic gardens, but commercial planting began in the 1940s.
'Hayward' is the most commonly available cultivar in stores. It is a large, egg-shaped fruit with a sweet flavor. 'Saanichton 12', from British Columbia, is somewhat more rectangular than 'Hayward' and comparably sweet, but the inner core of the fruit can be tough. 'Blake' can self-pollinate, but it has a smaller, more oval fruit and the flavor is considered inferior.
Kiwi berries.
Kiwi berries are edible berry- or grape-sized fruits similar to the fuzzy kiwifruit in taste and appearance, with thin, smooth skin. They are primarily produced by three species of kiwifruit; hardy kiwi ("Actinidia arguta"), Arctic beauty ("A. kolomikta"), and silver vine ("A. polygama"). They are fast-growing, climbing vines, durable over their growing season. They are referred to as kiwi berry, baby kiwi, dessert kiwi, grape kiwi, or cocktail kiwi.
The cultivar 'Issai' is a hybrid of hardy kiwi and silver vine which can self-pollinate. Grown commercially because of its relatively large fruit, Issai is less hardy than most hardy kiwi.
Golden kiwifruit.
The golden kiwifruit ("Actinidia chinensis") has a smooth, bronze skin, with a beak shape at the stem attachment. Flesh color varies from bright green to a clear, intense yellow. This species is sweeter and more aromatic in flavor; the flavor is reminiscent of some subtropical fruit. Its short storage life currently limits its commercial potential. One of the most attractive varieties has a red 'iris' around the center of the fruit and yellow flesh outside. The yellow fruit fetches a higher market price and, being less hairy than the fuzzy kiwifruit, is more palatable for consumption without peeling.
A commercially viable variety of this red-ringed kiwifruit, patented as the EnzaRed™, is a cultivar of the Chinese "hong yang" variety.
Hort16A is a golden kiwifruit marketed worldwide in decreasing volumes because this variety suffered significant losses in New Zealand from late 2010 to 2013 due to the PSA bacterium. A new variety of golden kiwifruit, 'Gold3', has been found to be more disease-resistant and most growers have now grafted over to this variety. The Gold3 variety, marketed by Zespri as 'SunGold', is not quite as sweet as the previous Hort16A, with a hint of tanginess, and lacks the Hort16A's usually slightly pointy tip.
Cultivation.
Kiwifruit can be grown in most temperate climates with adequate summer heat. Where fuzzy kiwifruit ("A. deliciosa") is not hardy, other species can be grown as substitutes.
Breeding.
Often in commercial farming, different breeds are used for rootstock, fruit bearing plants, and pollinators. Therefore, the seeds produced are crossbreeds of their parents. Even if the same breeds are used for pollinators and fruit bearing plants, there is no guarantee that the fruit will have the same quality as the parent. Additionally, seedlings take seven years before they flower, so determining whether the kiwi is fruit bearing or a pollinator is time consuming. Therefore, most kiwifruits, with the exception of rootstock and new cultivars, are propagated asexually. This is done by grafting the fruit producing plant onto rootstock grown from seedlings or, if the plant is desired to be a true cultivar, rootstock grown from cuttings of a mature plant.
Pollination.
Most of the plants require a male plant to pollinate a female plant for the female plant to produce fruit (dioecious). For a good yield of fruit, one male vine for every three to eight female vines is required. Other varieties can self pollinate, but they produce a greater and more reliable yield when pollinated by male kiwifruit vines.
Kiwifruit is notoriously difficult to pollinate, because the flowers are not very attractive to bees. Some producers blow collected pollen over the female flowers. Generally, the most successful approach, though, is saturation pollination, where the bee populations are made so large (by placing hives in the orchards at a concentration of about 8 hives per hectare) that bees are forced to use this flower because of intense competition for all flowers within flight distance. This is also increased by using varieties specifically developed for pollination.
Maturation and harvest.
Kiwifruit is picked by hand, and commercially grown on sturdy support structures, as it can produce several tonnes per hectare, more than the rather weak vines can support. These are generally equipped with a watering system for irrigation and frost protection in the spring.
Kiwifruit vines require vigorous pruning, similar to that of grapevines. Fruit is borne on one-year-old and older canes, but production declines as each cane ages. Canes should be pruned off and replaced after their third year. In the northern hemisphere the fruit ripens in November, while in the southern it ripens in May. Four year-old plants can produce up to 14,000 lbs per acre while Eight year-old plants can produce 18,000 lbs per acre. The plants produce their maximum at 8 to 10 years old. The seasonal yields are variable, a heavy crop on a vine one season generally comes with a light crop the following season.
Storage.
Fruits harvested when firm will not ripen when stored properly for long periods. This allows fruit to be sent to market up to 8 weeks after harvest.
Firm kiwifruit ripen after a few days to a week when stored at room temperature, but should not be kept in direct sunlight. Faster ripening occurs when placed in a paper bag with an apple, pear, or banana. Once a kiwifruit is ripe, however, it is preserved optimally when stored far from other fruits, as it is very sensitive to the ethylene gas they may emit, thereby tending to over-ripen even in the refrigerator. If stored appropriately, ripe kiwifruit normally keep for about one to two weeks.
Pests and diseases.
"Pseudomonas syringae actinidiae" (PSA) was first identified in Japan in the 1980s. This bacterial strain has been controlled and managed successfully in orchards in Asia. In 1992, it was found in northern Italy. In 2007/2008, economic losses were observed, as a more virulent strain became more dominant (PSA V). In 2010 it was found in New Zealand's Bay of Plenty kiwifruit orchards in the North Island.
Scientists reported they had worked out the strain of PSA affecting kiwifruit from New Zealand, Italy, and Chile originated in China.
Production.
Kiwifruit exports rapidly increased from the late 1960s to early 1970s in New Zealand. By 1976, exports exceeded the amount consumed domestically. Outside of Australasia, all New Zealand kiwifruits are now marketed under the brand-name label Zespri.
Over 70% of kiwifruit production is in Italy, New Zealand, and Chile. Italy produces roughly 10% more kiwifruit than New Zealand, and Chile produces 40% less. With these three main production centers, kiwifruit is produced for worldwide consumption roughly all year long.
In the 1980s, countries outside New Zealand began to export kiwifruit. In Italy, the infrastructure and techniques required to support grape production have been adapted to the kiwifruit. This, coupled with being very close to the European kiwifruit market, led to Italians becoming the leading producer of kiwifruit. The growing season of Italian kiwifruit does not overlap much with the New Zealand or the Chilean growing seasons, therefore direct competition between New Zealand or Chile was not much of a factor.
Although kiwifruit is a national fruit of China, until recently, China was not a major producing country of kiwifruit, as it was traditionally collected from the wild. In China, it is grown mainly in the mountainous area upstream of the Yangtze River, as well as Sichuan.
Human consumption.
Kiwifruit may be eaten raw, made into juices, used in baked goods, prepared with meat or used as a garnish. The whole fruit including the skin is suitable for human consumption, but the skin is often discarded due to its texture. Sliced kiwifruit has long been used as a garnish atop whipped cream on pavlova, a meringue-based dessert. Traditionally in China, kiwifruit was not eaten for pleasure, but was given as medicine to children to help them grow and to women who have given birth to help them recover.
Raw kiwifruit contains actinidain which is commercially useful as a meat tenderizer. Actinidin also makes raw kiwifruit unsuitable for use in desserts containing milk or any other dairy products which are not going to be served within hours, because the enzyme soon begins to digest milk proteins. This applies to gelatin-based desserts, as well, as the actinidin will dissolve the proteins in gelatin very quickly, either liquefying the dessert, or preventing it from solidifying.
Nutrition.
A medium size kiwifruit (76 grams) provides 46 calories, 0.3 g fat, 1 g protein, 11 g carbohydrates, and 2.6 g dietary fiber found partly in the edible skin. Kiwifruit is a rich source of vitamin C (112% of the Daily Value per 100 grams) and vitamin K, and a good source of dietary fiber and vitamin E (nutrient tables, right).
Kiwifruit seed oil contains on average 62% alpha-linolenic acid, an omega-3 fatty acid. Kiwifruit pulp contains carotenoids, such as provitamin A beta-carotene, lutein and zeaxanthin.
Allergies.
The actinidin found in kiwifruit can be an allergen for some individuals. The most common symptoms are unpleasant itching and soreness of the mouth, with the most common severe symptom being wheezing, with anaphylaxis also being reported.
The fruit is responsible for 10% of all allergic food reactions in children, making it a significant food allergen.

</doc>
<doc id="17364" url="https://en.wikipedia.org/wiki?curid=17364" title="Kiel Canal">
Kiel Canal

The Kiel Canal is a long freshwater canal in the German state of Schleswig-Holstein. The canal was finished in 1895, but later widened, and links the North Sea at Brunsbüttel to the Baltic Sea at Kiel-Holtenau. An average of is saved by using the Kiel Canal instead of going around the Jutland Peninsula. This not only saves time but also avoids potentially dangerous storm-prone seas.
Besides its two sea entrances, the Kiel Canal is linked, at Oldenbüttel, to the navigable River Eider by the short Gieselau Canal.
History.
The first connection between the North and Baltic Seas was constructed while the area was ruled by Denmark-Norway. It was called the Eider Canal, which used stretches of the Eider River for the link between the two seas. The "Eiderkanal" was completed during the reign of Christian VII of Denmark in 1784 and was a part of a long waterway from Kiel to the Eider River's mouth at Tönning on the west coast. It was only wide with a depth of , which limited the vessels that could use the canal to 300 tonnes displacement.
During the 19th century, after Schleswig-Holstein had come under the government of Prussia (from 1871 the German Empire) following the Second Schleswig War in 1864, a combination of naval interests—the German navy wanted to link its bases in the Baltic and the North Sea without the need to sail around Denmark—and commercial pressure encouraged the development of a new canal.
In June 1887 construction started at Holtenau, near Kiel. The canal took over 9,000 workers eight years to build. On 20 June 1895 the canal was officially opened by Kaiser Wilhelm II for transiting from Brunsbüttel to Holtenau. The next day, a ceremony was held in Holtenau where Wilhelm II named it the Kaiser Wilhelm Kanal (after his grandfather, Kaiser Wilhelm I), and laid the final stone. The opening of the canal was filmed by British director Birt Acres and surviving footage of this early film is preserved in the Science Museum in London. The first Trans-Atlantic sailing ship to pass through the canal was "Lilly", commanded by Johan Pitka. "Lilly". a barque, was a wooden sailing ship of about 390 tons displacement built 1866 in Sunderland, U.K. She had a length of 127.5 ft., beam 28.7 ft., depth of 17.6 ft and a 32-ft keel.
Widening (1907–14).
In order to meet the increasing traffic and the demands of the Imperial German Navy, between 1907 and 1914 the canal width was increased. The widening of the canal allowed the passage of a "Dreadnought"-sized battleship. This meant that these battleships could travel from the Baltic Sea to the North Sea without having to go around Denmark. The enlargement projects were completed by the installation of two larger canal locks in Brunsbüttel and Holtenau.
After World War I, the Treaty of Versailles required the canal to be open to vessels of commerce and of war of any nation at peace with Germany, while leaving it under German administration. The government under Adolf Hitler repudiated its international status in 1936. After World War II the canal was reopened to all traffic.
The canal was partially closed in March 2013 after two lock gates failed at the western end near Brunsbüttel. Ships larger than were forced to navigate via Skagerrak, a detour. The failure was blamed on neglect and a lack of funding by the German Federal Government which has been in financial dispute with the state of Schleswig-Holstein regarding the canal. Germany's Transport Ministry promised rapid repairs.
Operation.
There are detailed traffic rules for the canal. Each vessel in passage is classified in one of six traffic groups according to its dimensions. Larger ships are obliged to accept pilots and specialised canal helmsmen, in some cases even the assistance of a tugboat. Furthermore, there are regulations regarding the passing of oncoming ships. Larger ships may also be required to moor at the bollards provided at intervals along the canal to allow the passage of oncoming vessels. Special rules apply to pleasure craft.
While most large, modern cruise ships cannot pass through this canal due to clearance limits under bridges, the "SuperStar Gemini" has special funnels and masts that can be lowered for passage. Swan Hellenic's "Minerva", P&O Cruises's "Adonia", Fred Olsen Cruises' ship "Balmoral", 'Cruise and Maritime Voyages' ship "MS Marco Polo", ' Oceania Cruises' "Regatta", and "Nautica", and MS "Prinsendam" of Holland America Line are able to transit the canal.
All permanent, fixed bridges crossing the canal since its construction have a clearance of .
Maximum length for ships passing the Kiel Canal is ; with the maximum width of these ships can have a draught of up to . Ships up to a length of may have a draught up to . The bulker "Ever Leader" (deadweight 74001 t) is considered to be the cargo ship that to date has come closest to the overall limits.
Crossings.
Several railway lines and federal roads (Autobahnen and Bundesstraßen) cross the canal on eleven fixed links. The bridges have a clearance of allowing for ship heights up to . The oldest bridge still in use is the "Levensau High Bridge" from 1893; however, the bridge will have to be replaced in the course of a canal expansion already underway. In sequence and in the direction of the official kilometre count from west (Brunsbüttel) to east (Holtenau) these crossings are:
Local traffic is catered for by 14 ferry lines and a pedestrian tunnel. Most noteworthy is the “hanging ferry” () beneath the "Rendsburg High Bridge". All ferries are run by the Canal Authority and their use is free of charge.

</doc>
<doc id="17367" url="https://en.wikipedia.org/wiki?curid=17367" title="Konrad Emil Bloch">
Konrad Emil Bloch

Konrad Emil Bloch, ForMemRS (January 21, 1912 – October 15, 2000) was a German American biochemist. Bloch received the Nobel Prize in Physiology or Medicine in 1964 (joint with Feodor Lynen) for discoveries concerning the mechanism and regulation of the cholesterol and fatty acid metabolism.
Life and career.
Bloch was born in Neisse (Nysa), in the German Empire's Prussian Province of Silesia. He was the second child of middle-class parents Hedwig (Striemer) and Frederich D. "Fritz" Bloch. From 1930 to 1934, he studied chemistry at the Technical University of Munich. In 1934, due to the Nazi persecutions of Jews, he fled to the "Schweizerische Forschungsinstitut" in Davos, Switzerland, before moving to the United States in 1936. Later he was appointed to the department of biological chemistry at Yale Medical School.
In the United States, Bloch enrolled at Columbia University, and received a Ph.D in biochemistry in 1938. He taught at Columbia from 1939 to 1946. From there he went to the University of Chicago and then to Harvard University as Higgins Professor of Biochemistry in 1954, a post he held until 1982. After retirement at Harvard, he served as the Mack and Effie Campbell Tyner Eminent Scholar Chair in the College of Human Sciences at Florida State University.
Bloch shared the Nobel Prize in Physiology or Medicine in 1964 with Feodor Lynen, for their discoveries concerning the mechanism and regulation of the cholesterol and fatty acid metabolism. Their work showed that the body first makes squalene from acetate over many steps and then converts the squalene to cholesterol. He traced all the carbon atoms in cholesterol back to acetate. Some of his research was conducted using radioactive acetate in bread mold: this was possible because fungi also produce squalene. He confirmed his results using rats. He was one of several researchers who showed that acetyl Coenzyme A is turned into mevalonic acid. Both Bloch and Lynen then showed that mevalonic acid is converted into chemically active isoprene, the precursor to squalene. Bloch also discovered that bile and a female sex hormone were made from cholesterol, which led to the discovery that all steroids were made from cholesterol. His Nobel Lecture was "The Biological Synthesis of Cholesterol."
In 1985, Bloch became a Fellow of the Royal Society. In 1988, he was awarded the National Medal of Science.
Bloch and Lore Teutsch first met in Munich; in 1941 they married in the U.S. They had two children, Peter C. Bloch and Susan E. Bloch. He had two grandchildren, Benjamin N. Bloch and Emilie Bloch Sondel. He was fond of skiing, tennis, and music. Konrad Bloch died in Lexington, Massachusetts of congestive heart failure, aged 88.

</doc>
<doc id="17370" url="https://en.wikipedia.org/wiki?curid=17370" title="KSC">
KSC

KSC may refer to:

</doc>
<doc id="17372" url="https://en.wikipedia.org/wiki?curid=17372" title="Klement Gottwald">
Klement Gottwald

Klement Gottwald (23 November 1896 – 14 March 1953) was a Czechoslovak Communist politician and longtime leader of the Communist Party of Czechoslovakia (KSČ). He was Prime Minister of Czechoslovakia from 1946 to 1948 and President from 1948 to 1953.
Early life.
Gottwald was born on 23 November 1896, in Deditz, Wischau (Vyškov), Moravia, Austria-Hungary (now in the Czech Republic).
Career.
A cabinet maker by training, he joined the Social Democratic Party in 1912. He was drafted into the Austro-Hungarian Army during World War I, but defected to the Russians late in the war.
A charter member of the KSČ in 1921, he edited the party's newspaper in Bratislava from 1921 to 1926. From 1925 onward he was a member of the KSČ Central Committee. In 1927, he became secretary-general of the KSČ, and two years later he was elected to the National Assembly. He became a secretary of the Comintern in 1935, a post he held until its dissolution in 1943.
After the Munich Agreement of 1938, Gottwald spent the next seven years in exile in Moscow. From 1939 onward he was one of the leaders of the Czech resistance.
In March 1945, Edvard Beneš, who had been elected President of Czechoslovakia 1935–38 and who had been head of the Czechoslovak Government-in-Exile in London since 1941, agreed to form a National Front government with Gottwald. As part of the deal, Gottwald became deputy premier under Zdeněk Fierlinger.
In 1946, Gottwald gave up the secretary-general's post to Rudolf Slánský and was elected to the new position of party chairman. That March, he led the party to an astonishing 38% of the votes. This was easily the KSČ's best performance in an election. As it turned out, it would be the best showing by a European Communist party in a free election.
Coup.
By the summer of 1947, however, the KSČ's popularity had significantly dwindled, and most observers believed Gottwald would be turned out of office at the elections due for May 1948. The Communists' dwindling popularity, combined with France and Italy dropping the Communists from their coalition governments, prompted Joseph Stalin to order Gottwald to begin efforts to set up an undisguised Communist regime in Czechoslovakia.
Outwardly, though, Gottwald kept up the appearance of working within the system, announcing that he intended to lead the Communists to an absolute majority in the upcoming election—something no Czechoslovak party had ever done. The endgame began in February 1948, when a majority of the Cabinet directed the Communist interior minister, , to stop packing the police force with Communists. Nosek ignored this directive, with Gottwald's support. In response, 12 non-Communist ministers resigned. They believed that without their support, Gottwald would be unable to govern and be forced to either give way or resign himself. Beneš initially supported their position, and refused to accept their resignations. Gottwald not only refused to resign, but demanded the appointment of a Communist-dominated government under threat of a general strike. His Communist colleagues occupied the offices of the non-Communist ministers.
On 25 February, Beneš, fearing Soviet intervention, gave in. He accepted the resignations of the non-Communist ministers and appointed a new government in accordance with Gottwald's specifications. Although ostensibly still a coalition, it was dominated by Communists and pro-Moscow Social Democrats. The other parties were still nominally represented, but with the exception of Foreign Minister Jan Masaryk they were fellow travellers handpicked by the Communists. From this date forward, Gottwald was effectively the most powerful man in Czechoslovakia.
On 9 May, the National Assembly approved the so-called Ninth-of-May Constitution. While it was not a completely Communist document, its Communist imprint was strong enough that Beneš refused to sign it. He resigned on 2 June. In accordance with the 1920 Constitution, Gottwald took over most presidential functions until 14 June, when he was formally elected as President.
Leadership of Czechoslovakia.
Gottwald initially tried to take a semi-independent line. However, that changed shortly after a meeting with Stalin. Under his direction, Gottwald imposed the Soviet model of government on the country. He nationalized the country's industry and collectivised its farms. There was considerable resistance within the government to Soviet influence on Czechoslovak politics. In response, Gottwald instigated a series of purges, first to remove non-communists, later to remove some communists as well. Prominent Communists who became victims of these purges and were defendants in the Prague Trials included Rudolf Slánský, the party's general secretary, Vlado Clementis (the Foreign Minister) and Gustáv Husák (the leader of an administrative body responsible for Slovakia), who was dismissed from office for "bourgeois nationalism". Slánský and Clementis were executed in December 1952, and hundreds of other government officials were sent to prison. Husák was rehabilitated in 1960s and became the leader of Czechoslovakia in 1969.
In a famous photograph from 21 February 1948, described also in "The Book of Laughter and Forgetting" by Milan Kundera, Clementis stands next to Gottwald. When Vladimír Clementis was charged in 1950, he was erased from the photograph (along with the photographer Karel Hájek) by the state propaganda department.
Death.
Gottwald had suffered from heart disease for several years. Shortly after attending Stalin's funeral on 9 March 1953, one of his arteries burst. He died five days later on 14 March 1953, aged 56.
His body was initially displayed in a mausoleum at the site of the Jan Žižka national monument in the district of Žižkov, Prague. In 1962 the personality cult ended and it was no longer possible to show Gottwald's body. There are accounts that in 1962 Gottwald's body had blackened and was decomposing due to a botched embalming, although other witnesses have disputed this. His body was cremated, the ashes returned to the Žižka Monument and placed in a sarcophagus.
After the end of the communist period, Gottwald's ashes were removed from the Žižka Monument and placed in a common grave at Prague's Olšany Cemetery, together with the ashes of about 20 other communist leaders which had also originally been placed in the Žižka Monument. The Communist Party of Bohemia and Moravia now maintains that common grave.
After Gottwald.
He was succeeded by Antonín Zápotocký, the Premier of Czechoslovakia from 1948–1953.
In tribute, Zlín, a city in Moravia, now the Czech Republic, was renamed "Gottwaldov" after him from 1949 to 1990. Zmiiv, a city in Kharkiv Oblast, Ukrainian SSR, was named "Gotvald" after him from 1976 to 1990.
Námestie Slobody "(Freedom square)" in Bratislava was named "Gottwaldovo námestie" after him. A bridge in Prague that is now called Nuselský Most was once called Gottwaldův Most, and the abutting metro station now called Vyšehrad was called Gottwaldova.
A Czechoslovakian 100 Koruna banknote issued on 1 October 1989 as part of the 1985–89 banknote series included a portrait of Gottwald. This note was so poorly received by Czechoslovakians that it was removed from official circulation on 31 December 1990 and was promptly replaced with the previous banknote issue of the same denomination. All Czechoslovakian banknotes were removed from circulation in 1993 and replaced by separate Czech and Slovakian notes.
In 2005 he was voted the Worst Czech in a ČT poll (a program under the BBC licence 100 Greatest Britons). He received 26% of the votes.

</doc>
<doc id="17375" url="https://en.wikipedia.org/wiki?curid=17375" title="Kettlebaston">
Kettlebaston

Kettlebaston is a village and a civil parish with just over 30 inhabitants in the Babergh district of Suffolk, England located around east of Lavenham. It derives its name from Kitelbeornastuna, (Kitelbjorn's farmstead - O.Scand. pers. name + O.E. Tun), later evolving to Kettlebarston, (which is how the name is still pronounced), and finally to the current spelling. Its existence was first recorded in 1086 in the "Domesday Book".
History.
Once in an area of great wealth, the demise of the mediaeval wool trade was indirectly the saving of the village, (as we know it today), since the locals were unable to afford the expense of upgrading their houses with the latest architectural fashions. The number of timber-framed houses slowly declined over the years, as did the population - from over 200 at its peak, to the point when the village was on the brink of extinction. By the 1960s, with the road no more than an unmade track, and no electricity or mains water supplies, (it still has no gas or main drains), Kettlebaston was barely standing. In the "Spotlight On The Suffolk Scene" article, of the "Chronicle & Mercury" in June 1949, it was noted that a great many houses were category five - derelict, and ready for demolition.
As the agricultural workers left the land in search of other jobs, due to the increased mechanisation of farm work, "outsiders" discovered the secluded beauty of the rural Suffolk countryside, and a new age dawned. The tiny workmen's cottages, which once housed huge families - and some stock and chickens according to local accounts - were lovingly renovated and converted, and the village was reborn, and went on to proudly win Babergh Best Kept Village, and runner up in the Suffolk Community Council Best Kept Village Competition, in 1989.
The village sign, bearing two crossed sceptres topped with doves, was erected to mark the coronation of King George VI and Queen Elizabeth. It also commemorates that, in 1445, Henry VI granted the manor of Kettlebaston to William de la Pole, 1st Marquess of Suffolk, in return for the service of carrying a golden sceptre at the coronation of all the future Kings of England, and an ivory sceptre to carry at the coronation of Margaret of Anjou, and all future Queens. This honour continued until Henry VIII resumed the manor, although it was later regranted it was without the royal service.
The parish church of St. Mary the Virgin has Norman origins, and features a font from around 1200. It is recorded that the church was then "built anew" in 1342, remaining largely unchanged until targeted by Protestant vandals in the 1540s. Today it features one of Suffolk's finest modern rood screens, designed by Father Ernest Geldart and decorated by Patrick Osborne, and Enid Chadwick, and a rare Sacred Heart altar upon a Stuart Holy Table. It now lacks the small lead spire which once topped the tower.
Regarded as a place of pilgrimage to the followers of the Anglo-Catholic movement from all over the UK, Kettlebaston was the liturgically highest of all Suffolk's Anglican churches. From 1930, until his retirement in 1964, Reverend Father Harold Clear Butler said Roman Mass every day, and celebrated High Mass and Benediction on Sundays. He also removed state notices from the porch, and refused to keep registers, or to recognise the office of the local Archdeacon of Sudbury.The church was electrified finally in 2014..
The current-day village has no shop, school, or pub.

</doc>
<doc id="17378" url="https://en.wikipedia.org/wiki?curid=17378" title="Karl Amadeus Hartmann">
Karl Amadeus Hartmann

Karl Amadeus Hartmann (2 August 1905 – 5 December 1963) was a German composer. Some have lauded him as the greatest German symphonist of the 20th century, although he is now largely overlooked, particularly in English-speaking countries.
Life.
Born in Munich, well known there for his flower paintings, the son of Friedrich Richard Hartmann, and the youngest of four brothers of whom the elder three also became painters, Hartmann was himself torn, early in his career, between music and the visual arts. He was much affected in his early political development by the events of the unsuccessful Workers’ Revolution in Bavaria that followed the collapse of the German empire at the end of World War I (see Bavarian Soviet Republic). He remained an idealistic socialist for the rest of his life.
At the Munich Academy in the 1920s, Hartmann studied with Joseph Haas, a pupil of Max Reger, and later he received intellectual stimulus and encouragement from the conductor Hermann Scherchen, an ally of the Schoenberg school, with whom he had a nearly lifelong mentor-protégé relationship. He voluntarily withdrew completely from musical life in Germany during the Nazi era, while remaining in Germany, and refused to allow his works to be played there. An early symphonic poem, "Miserae" (1933–1934, first performed in Prague, 1935) was condemned by the Nazi regime; but his work continued to be performed, and his fame grew, abroad.
During World War II, though already an experienced composer, Hartmann submitted to a course of private tuition in Vienna by Schoenberg’s pupil Anton Webern (with whom he often disagreed on a personal and political level). Although stylistically their music had little in common, he clearly felt that he needed, and benefitted from, Webern’s acute perfectionism.
After the fall of Hitler, Hartmann was one of the few prominent surviving anti-fascists in Bavaria whom the postwar Allied administration could appoint to a position of responsibility. In 1945, he became a "Dramaturg" at the Bavarian State Opera and there, as one of the few internationally recognized figures who had survived untainted by any collaboration with the Nazi regime, he became a vital figure in the rebuilding of (West) German musical life. Perhaps his most notable achievement was the Musica Viva concert series, which he founded and ran for the rest of his life in Munich. Beginning in November 1945, the concerts reintroduced the German public to 20th-century repertoire, which had been banned since 1933 under National Socialist aesthetic policy. Hartmann also provided a platform for the music of the young composers who came to the fore in the late 1940s and early 1950s, helping to establish such figures as Hans Werner Henze, Luigi Nono, Luigi Dallapiccola, Carl Orff, Iannis Xenakis, Olivier Messiaen, Luciano Berio, Bernd Alois Zimmermann and many others. Hartmann also involved sculptors and artists such as Jean Cocteau, Le Corbusier, and Joan Miró in exhibitions at Musica Viva.
He was accorded numerous honours after the war, including the Musikpreis of the city of Munich in March 1949. This was followed by the Kunstpreis of the Bayrische Akademie der Schönen Künste (1950), the Arnold Schönberg Medal of the IGNM (1954), the Große Kunstpreis of the Land Nordrhein-Westfalen (1957), as well as the Ludwig Spohr Award of the city of Braunschweig, the Schwabing Kunstpreis (1961) and the Bavarian Medal of Merit (1959). In addition, Hartmann became a member of the Academy of Arts in Munich (1952) and Berlin (1955) and received an honorary doctorate from Spokane Conservatory, Washington (1962). It should be noted that his socialist sympathies did not at all include communism. In the 1950s, he refused an offer to move to East Germany.
He continued to base his activities in Munich for the remainder of his life, and his administrative duties came to absorb much of his time and energy. This reduced his opportunities for composition, and his last years were dogged by serious illness. In 1963, he died from stomach cancer at the age of 58, leaving his last work – an extended symphonic "Gesangsszene" for voice and orchestra on words from Jean Giraudoux’s apocalyptic drama "Sodom and Gomorrah" – unfinished.
Output and style.
Hartmann completed a number of fine works, most notably eight symphonies. The first of these, and perhaps emblematic of the difficult genesis of many of his works, is Symphony No. 1, titled "Essay for a requiem" ("Versuch eines Requiems"). This work began in 1936 as a cantata for alto solo and orchestra, loosely based on a few select poems by Walt Whitman. It soon became known as "Our Life: Symphonic Fragment" ("Unser Leben: Symphonisches Fragment") and was intended as a comment on the generally miserable conditions for artists and liberal minded individuals under the early Nazi regime. After the defeat of the Third Reich in World War II, the real victims of the regime had become clear, and the cantata's title was changed to "Symphonic Fragment: Attempt at a Requiem" to honor the millions killed in the Holocaust. Hartmann revised the work in 1954-1955 as his Symphony No. 1, and finally published it in 1956. As this example indicates, Hartmann was a highly self-critical composer and many of his works went through successive stages of revision. He also suppressed most of his substantial orchestral works of the late 1930s and the war years, either allowing them to remain unpublished or, in several cases, reworking them - or portions of them - into the series of numbered symphonies that he produced in the late 1940s and early 1950s.
Perhaps the most frequently-performed of his symphonies are No. 4, for strings, and No. 6; probably his most widely known work, through performances and recordings, is his "Concerto funebre" for violin and strings, composed at the beginning of World War II and making use of a Hussite chorale and a Russian revolutionary song of 1905.
As a composer, Hartmann attempted a difficult synthesis of many different idioms, including musical expressionism and jazz stylization, into organic symphonic forms in the tradition of Bruckner and Mahler. His early works contain music that is both satirical and politically engaged. But he admired the polyphonic mastery of J.S. Bach, the profound expressive irony of Mahler, the neoclassicism of Igor Stravinsky and Paul Hindemith. He, also in the 1930s, developed close ties with Béla Bartók and Zoltán Kodály in Hungary, and this is reflected in his own music to some extent. In the 1940s, he began to take an interest in Schoenbergian twelve-tone technique; though he studied with Webern his own idiom was closer to Alban Berg. In the 1950s, Hartmann started to explore the metrical techniques pioneered by Boris Blacher and Elliott Carter. He especially makes use of the forms of three-part Adagio slow movements, Fugue, Variations and Toccata.
Reputation and legacy.
Significantly, championed his music following his death: Scherchen, his most noted advocate, died in 1966. Some have suggested that this accelerated the disappearance of Hartmann's music from public view in the years following his death. Conductors who regularly performed Hartmann's music include Rafael Kubelik and Ferdinand Leitner, who also recorded the third and sixth symphonies. More recent champions of works by Hartmann include Ingo Metzmacher and Mariss Jansons.
Henze said of Hartmann's music: 
Symphonic architecture was essential for him... as a suitable medium for reflecting the world as he experienced and understood it – as an agonizingly dramatic battle, as contradiction and conflict – in order to be able to achieve self-realization in its dialectic and to portray himself as a man among men, a man of this world, and not out of this world.
The English composer John McCabe wrote his "Variations on a Theme of Karl Amadeus Hartmann" (1964) in tribute. It uses the opening of Hartmann's Fourth Symphony as its theme. Hans Werner Henze has made a version of Hartmann's Piano Sonata No. 2 for full orchestra.
List of works.
Symphonic works.
(i) Up to 1945 – mostly later suppressed
(ii) After 1945

</doc>
<doc id="17379" url="https://en.wikipedia.org/wiki?curid=17379" title="Kami">
Kami

In Shinto, Kami are not separate from nature, but are of nature, possessing positive and negative, good and evil characteristics. They are manifestations of musubi (結び), the interconnecting energy of the universe, and are considered exemplary of what humanity should strive towards. Kami are believed to be “hidden” from this world, and inhabit a complementary existence that mirrors our own, "shinkai "world of the Kami (神界). To be in harmony with the awe inspiring aspects of nature is to be conscious of "kannagara no michi" way of the Kami (随神の道 or 惟神の道). 
Though the word Kami is translated in multiple ways into English, no one English word expresses its full meaning. In this way, the ambiguity of the meaning of Kami is necessary, as it conveys the ambiguous nature of Kami themselves.
Etymology.
Kami is the Japanese word for a god, deity, divinity, or spirit. It has been used to describe "mind" (心霊), "God" (ゴッド), "supreme being" (至上者), "one of the Shinto deities", an effigy, a principle and anything that is worshipped.
Although "deity" is the common interpretation of kami, some Shinto scholars argue that such a translation can cause a misunderstanding of the term. The wide variety of usage of the word can be compared to the Sanskrit "Deva" and the Hebrew "Elohim" and the arabic "Allah", which also refer to God, gods, angels or spirits.
Some etymological suggestions are:
Because Japanese does not normally distinguish grammatical number in nouns, it is sometimes unclear whether Kami refers to a single or multiple entities. When a singular concept is needed, or is used as a suffix. The logograms for "kami-sama" are those used for "shén yàng" in Pŭtōnghuà; "kami-sama" can be used for a divinity, or for an outstanding human, such as Tezuka Osamu, "the god of manga." The term generally used to refer to multiple Kami is Kamigami.
Gender is also not implied in the word Kami, and as such it can be used to reference either male or female. The word , the use of female Kami is a fairly new tradition. 
History.
While Shinto has no founder, no overarching doctrine, and no religious texts, the "Kojiki" (the Ancient Chronicles of Japan), written in 712 CE, and the "Nihonshoki "(Chronicles of Japan), written in 720 CE, contain the earliest record of Japanese creation myths. The Kojiki also includes descriptions of various Kami.
In the ancient Shinto traditions there were 5 defining characteristics of Kami.
Kami are an ever-changing concept, but their presence in Japanese life has remained constant. The Kami’s earliest roles were as earth-based spirits, assisting the early hunter-gatherer groups in their daily lives, worshipped as gods of earth (mountains) and sea. As the cultivation of rice became increasingly important and predominant in Japan the Kami’s identity shifted to more sustaining roles that were directly involved in the growth of crops, such as rain, earth, and rice. This relationship between early Japanese people and the Kami was manifested in rituals and ceremonies meant to entreat the Kami to grow and protect the harvest. These rituals also became a symbol of power and strength for the early emperors. (See Niinamesai).
There is a strong tradition of myth-histories in the Shinto faith; one such myth details the appearance of the first emperor, grandson of the Sun Goddess Amaterasu. In this myth when Amaterasu sends her grandson to earth to rule she gave him five rice grains, which had been grown in the fields of heaven (Takamagahara). This rice made it possible for him to transform the “wilderness”.
Social and political strife have played a key role in the development of new sorts of Kami, specifically the goryo-shin (the sacred spirit Kami). The goryo are the vengeful spirits of the dead whose lives were cut short, but they were calmed by the devotion of Shinto followers and are now believed to punish those who do not honor the Kami.
The pantheon of Kami, like the Kami themselves, is forever changing in definition and scope. As the needs of the people have shifted, so too have the domains and roles of the various Kami. Some examples of this are related to health, such as the Kami of small pox whose role was expanded to include all contagious diseases, or the Kami of boils and growths who has also come to preside over cancers and cancer treatments.
In the ancient animistic religions, Kami were understood as simply the divine forces of nature. Worshippers in ancient Japan revered creations of nature which exhibited a particular beauty and power such as waterfalls, mountains, boulders, animals, trees, grasses and even rice paddies. They strongly believed the spirits or resident Kami deserved respect.
Shinto belief.
Kami are the central objects of worship for the Shinto faith. The ancient animistic spirituality of Japan was the beginning of modern Shinto, which became a formal spiritual institution later in an effort to preserve the traditional beliefs from encroachment of imported religious ideas. As a result, the nature of what can be called Kami is very general and encompasses many different concepts and phenomena.
Some of the objects or phenomena designated as Kami are qualities of growth, fertility, and production; natural phenomena like wind and thunder; natural objects like the sun, mountains, rivers, trees, and rocks; some animals; and ancestral spirits. Included within the designation of ancestral spirits are spirits of the ancestors of the Imperial House of Japan, but also ancestors of noble families as well as the spirits of the ancestors of all people, which when they died were believed to be the guardians of their descendants.
There are other spirits designated as Kami as well. For example, the guardian spirits of the land, occupations, and skills; spirits of Japanese heroes, men of outstanding deeds or virtues, and those who have contributed to civilization, culture and human welfare; those who have died for the state or the community; and the pitiable dead. Not only spirits superior to man can be considered Kami, but also spirits that are considered pitiable or weak have been considered Kami in Shinto.
The concept of Kami has been changed and refined since ancient times, although anything that was considered to be Kami by ancient people will still be considered Kami in modern Shinto. Even within modern Shinto, there are no clearly defined criteria for what should or should not be worshipped as Kami. The difference between modern Shinto and the ancient animistic religions is mainly a refinement of the Kami-concept, rather than a difference in definitions.
Although the ancient designations are still adhered to, in modern Shinto many priests also consider Kami to be anthropomorphic spirits, with nobility and authority. These include such mythological figures as Amaterasu Omikami, the sun goddess of the Shinto pantheon. Although these Kami can be considered deities, they are not necessarily considered omnipotent or omniscient, and like the Greek Gods, they had flawed personalities and were quite capable of ignoble acts. In the myths of Amaterasu, for example, she could see the events of the human world, but had to use divination rituals to see the future.
There are considered to be three main variations of Kami, "amatsu-kami" ("the heavenly deities"), "kunitsu-kami" ("the gods of the earthly realm"), and . ("八百万" literally means eight million, but idiomatically it expresses "uncountably many" and "all around"—like many East Asian cultures, the Japanese often use the number 8, representing the cardinal and ordinal directions, to symbolize ubiquity.) These classifications are not considered strictly divided, due to the fluid and shifting nature of Kami, but are instead held as guidelines for grouping Kami.
The ancestors of a particular family can also be worshipped as Kami. In this sense, these Kami are worshipped not because of their godly powers, but because of a distinctive quality or virtue. These Kami are celebrated regionally, and several miniature shrines ("hokora") have been built in their honor. In many cases, people who once lived are thus revered; an example of this is Tenjin, who was Sugawara no Michizane (845-903) in life.
Within Shinto, it is believed that the nature of life is sacred because the Kami began human life. Yet, man cannot perceive this divine nature, which the Kami created, on his own; therefore, magokoro, or purification, is necessary in order to see the divine nature. This purification can only be granted by the Kami. In order to please the Kami and earn magokoro, Shinto followers are taught to uphold the four affirmations of Shinto.
The first affirmation is to hold on to tradition and the family. Family is seen as the main mechanism by which traditions are preserved. For instance, with marriages or births, traditions can be practiced repeatedly. The second affirmation is to have a love of nature. Nature objects are worshipped as sacred because the Kami live within them. Therefore, to be in contact with nature means to be in contact with the gods. The third affirmation is to maintain physical cleanliness. Followers of Shinto take baths, wash their hands, and rinse out their mouths often. The last affirmation is to practice matsuri, which is the worship and honor given to the Kami and the ancestral spirits.
Additionally, Shinto followers believe that the Kami are the ones who can either grant blessings or curses to a person. Shinto believers desire to appease the evil Kami to 'stay on their good side,' and also to please the good Kami. In addition to practicing the four affirmations daily, Shinto believers also wear "omamori" to aid them in remaining pure and protected. Mamori are charms that keep the evil kami from striking a human with sickness or causing disaster to befall him.
The Kami are both worshipped and respected within the religion of Shinto. The goal of life to Shinto believers is to obtain "magokoro", a pure sincere heart, which can only be granted by the Kami. As a result, Shinto followers are taught that humankind should venerate both the living and the nonliving, because both possess a divine superior spirit within: the Kami.
Ceremonies and festivals.
One of the first recorded rituals we know of is Niinamesai, the ceremony in which the Emperor offers newly harvested rice to the kami to secure their blessing for a bountiful harvest. A yearly festival, Niinamesai is also performed when a new Emperor comes to power, in which case it is called Onamesai. In the ceremony the Emperor offers crops from the new harvest to the kami, including rice, fish, fruits, soup and stew. The Emperor first feasts with the deities, then the guests. The feast could go on for some time, for example the Showa Emperor's feast spanned two days.
Visitors to a Shinto shrine follow a purification ritual before presenting themselves to the kami. This ritual begins with hand washing, and swallowing a small amount of water in front of the shrine to purify the body, heart, and mind. Once this is complete they turn their focus to gaining the kami’s attention. The traditional method of doing this is to bow twice, clap twice and bow again, alerting the kami to their presence and desire to commune with them. During the last bow, the supplicant offers words of gratitude and praise to the kami; if they are offering a prayer for aid they will also state their name and address. After the prayer and/or worship they repeat the two bows, two claps and a final bow in conclusion.
Shinto practitioners also worship at home. This is done at a kamidana (house hold shrine), on which an ofuda (kami name card or charm card) with the name of their protector or ancestral kami is positioned. Their protector kami is determined by their or their ancestors’ relationship to the kami.
Ascetic practices, shrine rituals and ceremonies and Japanese festivals are the most public ways that Shinto devotees celebrate and offer adoration for the kami. Kami are celebrated during their distinct festivals that usually take place at the shrines dedicated to their worship. Many festivals involve believers, who are usually intoxicated, parading, sometimes running, toward the shrine while carrying mikoshi (portable shrines) as the community gathers for the festival ceremony. Yamamoto Guji, the high priest at the Tsubaki Grand Shrine, explains that this practice honors the kami because “it is in the festival, the matsuri, the greatest celebration of life can be seen in the world of Shinto and it is the people of the community who attend festivals as groups, as a whole village who are seeking to unlock the human potential as children of kami.” During the New Year Festival is when families purify and clean their houses in preparation for the upcoming year. Offerings are also made to the ancestors so that they will bless the family in the future year.
Shinto ceremonies are so long and complex that in some temples it can take ten years for the priests to learn them. The priesthood was traditionally hereditary. Some temples have drawn their priests from the same families for over a hundred generations. It is not uncommon for the clergy to be female priestesses. The priests may be assisted by miko, young unmarried women acting as shrine maidens. Neither priests nor priestesses live as ascetics; in fact, it is common for them to be married, and they are not traditionally expected to meditate. Rather, they are considered specialists in the arts of maintaining the connection between the kami and the people.
In addition to these festivals, ceremonies marking rites of passage are also performed within the shrines. Two such ceremonies are the birth of a child and the Shichi-Go-San. When a child is born they are brought to a shrine so that they can be initiated as a new believer and the kami can bless them and their future life. The Shichi-Go-San, the Seven-Five-Three, is a rite of passage for five-year-old boys and three- or seven-year-old girls. It is a time for these young children to personally offer thanks for the kami’s protection and to pray for continued health.
Many other rites of passage and festivals are practiced by Shinto believers. The main reason for these ceremonies is so that Shinto followers can appease the kami in order to reach magokoro. Magokoro can only be received through the kami. Ceremonies and festivals are long and complex because they need to be perfect to satisfy the kami. If the kami are not pleased with these ceremonies, they will not grant a Shinto believer magokoro.

</doc>
<doc id="17381" url="https://en.wikipedia.org/wiki?curid=17381" title="Koalang">
Koalang

Koalang is a term coined by Janusz A. Zajdel, a Polish science-fiction writer. It denotes a language used by people in a totalitarian world called "Paradyzja", in Zajdel's 1984 novel of the same name. The ""ko-al"" in ""koalang"" derives from the Polish words 'kojarzeniowo-aluzyjny' ("associative-allusive").
Because Paradyzja was a space station, where activity was tracked by automatic cameras and analysed (mostly) by computers, the people there created an Aesopian language, full of metaphors, impossible for computers to grasp. The meaning of every sentence depended on the context. For example, the sentence, "I dreamt about blue angels last night", meant: "I was visited by the police last night."
The software that analyzed sentences was self-learning; therefore a phrase used to describe something metaphorically, should not be used again in the same context.
Zajdel paid a tribute to George Orwell's newspeak, and to Aldous Huxley, by naming one of the main characters "Nikor Orley Huxwell".
In the 1980s, the youth magazine "Na Przełaj" (Short Cut) printed rock-song lyrics in a column titled "KOALANG", hinting that the songs' texts contained content camouflaged from censorship.

</doc>
<doc id="17382" url="https://en.wikipedia.org/wiki?curid=17382" title="Kobellite">
Kobellite

Kobellite (Pb22Cu4(Bi,Sb)30S69) is a gray, fibrous, metallic mineral, a sulfide of antimony, bismuth, and lead. It is a member of the izoklakeite - berryite series with silver and iron substituting in the copper site and a vaying ratio of bismuth, antimony, and lead. It crystallizes with orthorhombic dipyramidal crystals. Locations include Hvena, Sweden; Ouray, Colorado; and Wake County, North Carolina, US. Named after Wolfgang Franz von Kobell (1803–1882), German mineralogist. 

</doc>
<doc id="17383" url="https://en.wikipedia.org/wiki?curid=17383" title="Kayak">
Kayak

A kayak is a small, narrow boat which is propelled by means of a double-bladed paddle. The word kayak originates from the Greenlandic language, where it is the word "qajaq" (pronounced ). In the UK the term "canoe" is often used when referring to a kayak.
The traditional kayak has a covered deck and one or more cockpits, each seating one paddler. The cockpit is sometimes covered by a spray deck that prevents the entry of water from waves or spray and makes it possible for suitably skilled kayakers to roll the kayak: that is, to capsize and right it without it filling with water or ejecting the paddler.
Some modern boats vary considerably from a traditional design but still claim the title "kayak", for instance in eliminating the cockpit by seating the paddler on top of the boat ("sit-on-top" kayaks); having inflated air chambers surrounding the boat; replacing the single hull by twin hulls ("W" kayak), and replacing paddles with other human-powered propulsion methods, such as foot-powered rotational propellers and "flippers". Kayaks are also being sailed, as well as propelled by means of small electric motors, and even by outboard gas engines, when possible.
The kayak was first used by the indigenous Aleut, Inuit, Yupik and possibly Ainu hunters in subarctic regions of the world.
History.
Kayaks (Inuktitut: "qajaq" (ᖃᔭᖅ ), Yup'ik: "qayaq" (from "qai-" "surface; top"), Aleut: "Iqyax") were originally developed by the Inuit, Yup'ik, and Aleut. They used the boats to hunt on inland lakes, rivers and coastal waters of the Arctic Ocean, North Atlantic, Bering Sea and North Pacific oceans. These first kayaks were constructed from stitched seal or other animal skins stretched over a wood or whalebone-skeleton frame. (Western Alaskan Natives used wood whereas the eastern Inuit used whalebone due to the treeless landscape). Kayaks are believed to be at least 4,000 years old. The oldest existing kayaks are exhibited in the North America department of the State Museum of Ethnology in Munich.
Native people made many types of boat for different purposes. The baidarka, developed by indigenous cultures in Alaska, was also made in double or triple cockpit designs, for hunting and transporting passengers or goods. An umiak is a large open sea canoe, ranging from , made with seal skins and wood. It is considered a kayak although it was originally paddled with single-bladed paddles, and typically had more than one paddler.
Native builders designed and built their boats based on their own experience and that of the generations before them, passed on through oral tradition. The word "kayak" means "man's boat" or "hunter's boat", and native kayaks were a personal craft, each built by the man who used it—with assistance from his wife, who sewed the skins—and closely fitting his size for maximum maneuverability. A special skin jacket, Tuilik, was then laced to the kayak, creating a waterproof seal. This enabled the eskimo roll to become the preferred method of regaining posture after capsizing, especially as few Eskimos could swim; their waters are too cold for a swimmer to survive for long.
Instead of a "tuilik", most traditional kayakers today use a spray deck made of waterproof synthetic material stretchy enough to fit tightly around the cockpit rim and body of the kayaker, and which can be released rapidly from the cockpit to permit easy exit.
Eskimo kayak builders had specific measurements for their boats. The length was typically three times the span of his outstretched arms. The width at the cockpit was the width of the builder's hips plus two fists (and sometimes less). The typical depth was his fist plus the outstretched thumb (hitch hiker). Thus typical dimensions were about long by wide by deep. This measurement system confounded early European explorers who tried to duplicate the kayak, because each kayak was a little different.
Traditional kayaks encompass three types: "Baidarkas", from the Bering sea & Aleutian islands, the oldest design, whose rounded shape and numerous chines give them an almost Blimp-like appearance; "West Greenland" kayaks, with fewer chines and a more angular shape, with gunwales rising to a point at the bow and stern; and "East Greenland" kayaks that appear similar to the West Greenland style, but often fit more snugly to the paddler and possess a steeper angle between gunwale and stem, which lends maneuverability.
Most of the Aleut people in the Aleutian Islands eastward to Greenland Inuit relied on the kayak for hunting a variety of prey—primarily seals, though whales and caribou were important in some areas.
Skin-on-frame kayaks are still being used for hunting by Inuit people in Greenland, because the smooth and flexible skin glides silently through the waves. In other parts of the world home builders are continuing the tradition of skin on frame kayaks, usually with modern skins of canvas or synthetic fabric, such as sc. ballistic nylon.
Contemporary traditional-style kayaks trace their origins primarily to the native boats of Alaska, northern Canada, and Southwest Greenland. Wooden kayaks and fabric kayaks on wooden frames dominated the market up until the 1950s, when fiberglass boats were first introduced in the US, and inflatable rubberized fabric boats were first introduced in Europe. Rotomolded plastic kayaks first appeared in 1973, and most kayaks today are made from roto-molded polyethylene resins. The development of plastic and rubberized inflatable kayaks arguably initiated the development of freestyle kayaking as we see it today, since these boats could be made smaller, stronger and more resilient than fiberglass boats.
Design principles.
Typically, kayak design is largely a matter of trade-offs: directional stability ("tracking") vs maneuverability; stability vs speed; and primary vs secondary stability. This is true for single hull (a.k.a. mono-hull) kayaks, but does not necessarily encompass newer types of hulls, such as twin hulls.
Length.
As a general rule, a longer kayak is faster. See Hull speed. Kayaks that are built to cover longer distances such as touring and sea kayaks are longer, generally . With touring kayaks the keel is generally more defined (helping the kayaker track in a straight line.) Whitewater kayaks, which generally depend upon river current for their forward motion, are short, to maximize maneuverability. These kayaks rarely exceed in length, and "play boats" may be only long. Recreational kayak designers try to provide more stability at the price of reduced speed, and compromise between tracking and maneuverability, ranging from .
Primary and secondary stability.
"Primary" (sometimes called "initial") stability describes how much a boat tips, or rocks back and forth, when displaced from level by paddler weight shifts. Secondary ("final") stability describes how stable a kayak feels when put on edge or when waves are passing under the hull perpendicular to the length of the boat. Primary stability is often a big concern to a beginner, while secondary stability matters both to beginners and experienced travelers. By example, a wide, flat-bottomed kayak will have high primary stability and feel very stable on flat water. However, when a steep wave breaks on such a boat, it can be easily overturned because the flat bottom is no longer level. By contrast, a kayak with a narrower, more rounded hull can be edged or leaned into waves and (in the hands of a skilled kayaker) provides a safer, more comfortable response on stormy seas.
Beam profile.
The most important attribute in determining the stability of a single hulled kayak is the overall width of its cross section. The shape of the cross section can affect both maneuverability and stability. Hull shapes are categorized by roundness/flatness, whether it has a "V" shape at various points, and by the presence and severity of a chine, where the side and bottom of a hull meet at an angle, creating an edge below the gunwales. This cross–section may vary along the length of the boat. Kayaks with only moderate primary, but excellent secondary stability are, in general, considered more seaworthy, especially in challenging conditions.
A V-shaped hull tends to ease traveling straight (track), but makes turning harder. V-shaped hulls also have the greatest secondary stability.
Conversely, flat-bottomed hulls are easy to turn, but harder to direct in a constant direction.
The chine typically increases secondary stability by effectively widening the beam of the boat when it heels (tips).
Sea kayaks, designed for open water and rough conditions, are generally narrower and have more secondary stability than recreational kayaks, which are wider , have a flatter hull shape, and more primary stability.
Hull surface profile.
Traditional kayak hulls are categorized according to the shape from bow to stern
Common shapes include:
Rocker.
Length alone does not fully predict a kayak's maneuverability: a second design element is "rocker", i.e. its lengthwise curvature. A heavily "rockered" boat curves more, shortening its effective waterline. For example, an kayak with no rocker is in the water from end to end. In contrast, the bow and stern of a rockered boat are out of the water, shortening its lengthwise waterline to only . Rocker is generally most evident at the ends, and in moderation improves handling. Similarly, although a rockered whitewater boat may only be a few feet shorter than a typical recreational kayak, its waterline is far shorter and its maneuverability far greater. When surfing, a heavily rockered boat is less likely to lock into the wave as the bow and stern are still above water. A boat with less rocker cuts into the wave and makes it harder to turn while surfing.
Paddling ease and ergonomics.
Some recreational kayak makers try to maximize hull volume (weight capacity) for a given length as shorter kayaks are easier to transport and store. Many paddlers who use a sit-in kayak feel more secure in a kayak with a weight capacity substantially more than their own weight. Maximum volume in a sit-in kayak is helped by a wide hull with high sides. But paddling ease is helped by lower sides where the paddler sits and a narrower width. A narrower kayak makes a somewhat shorter paddle appropriate and a shorter paddle puts less strain on the shoulder joints. Some paddlers are comfortable with a sit-in kayak so narrow that their legs extend fairly straight out. Others want sufficient width to permit crossing their legs inside the kayak.
Traditional-style and most modern types of kayaks (e.g. sit-on-top) require that paddler be seated with their legs stretched in front of them, in a right angle, in a position called the "L" kayaking position. Most modern kayaks feature a system comprising footrests and a backrest, designed to provide the paddler with means to support their paddling effort by allowing them to push the footrests with their feet, and the backrest with their lower back (lumbar spine). Such arrangements were not included in kayaks made by native peoples of the arctic regions, who were fit enough to paddle their kayaks without needing such devices. These devices are not required in new twin hull kayaks of the "W" type that offer a different sitting position called the "Riding" position, in which the paddler's legs are not stretched out in front of them.
Materials and construction.
Today almost all kayaks are commercial products intended for sale rather than for the builder's personal use.
Fiberglass hulls are stiffer than polyethylene hulls, but they are more prone to damage from impact, including cracking. Most modern kayaks have steep V sections at the bow and stern, and a shallow V amidships. Fiberglass kayaks need to be "laid-up" in a mold by hand, so are usually more expensive than polyethylene kayaks, which are rotationally molded in a machine.
Plastic kayaks are rotationally molded ('rotomolded') from a various grades and types of polyethylene resins ranging from soft to hard. Such kayaks are particularly resistant to impact.
Wooden hulls don't necessarily require significant skill and handiwork, depending on how they are made. Kayaks made from thin strips of wood sheathed in fiberglass have proven successful, especially as the price of epoxy resin has decreased in recent years. A plywood, stitch and glue (S&G) doesn't need fiberglass sheathing though some builders do. Three main types are popular, especially for the home builder: Stitch & Glue, Strip-Built, and hybrids which have a stitch & glue hull and a strip-built deck.
Stitch & Glue designs typically use modern, marine-grade plywood — eighth-inch, or up to quarter-inch, thick. After cutting out the required pieces of hull and deck (kits often have these pre-cut), a series of small holes are drilled along the edges. Copper wire is then used to "stitch" the pieces together through the holes. After the pieces are temporarily stitched together, they are glued with epoxy and the seams reinforced with fiberglass. When the epoxy dries, the copper stitches are removed. Sometimes the entire boat is then covered in fiberglass for additional strength and waterproofing though this adds greatly to the weight and is unnecessary. Construction is fairly straightforward, but because plywood does not bend to form compound curves, design choices are limited. This is a good choice for the first-time kayak builder as the labor and skills required (especially for kit versions) is considerably less than for strip-built boats which can take 3 times as long to build.
Strip-built designs are similar in shape to rigid fiberglass kayaks but are generally both lighter and tougher. Like their fiberglass counterparts the shape and size of the boat determines performance and optimal uses. The hull and deck are built with thin strips of lightweight wood, often cedar, pine or Redwood. The strips are edge-glued together around a form, stapled or clamped in place, and allowed to dry. Structural strength comes from a layer of fiberglass cloth and epoxy resin, layered inside and outside the hull. Strip–built kayaks are sold commercially by a few companies, priced USD 4,000 and up. An experienced woodworker can build one for about USD 400 in 200 hours, though the exact cost and time depend on the builder's skill, the materials and the size and design. As a second kayak project, or for the serious builder with some woodworking expertise, a strip–built boat can be an impressive piece of work. Kits with pre-cut and milled wood strips are commercially available.
Skin on frame boats are more traditional in design, materials, and construction. They were traditionally made of driftwood, pegged or lashed together, and stretched seal skin, as those were the most readily available materials in the Arctic regions. Today, seal skin is usually replaced with canvas or nylon cloth covered with paint, polyurethane, or a hypalon rubber coating and a wooden or aluminum frame. Modern skin-on-frame kayaks often possess greater impact resistance than their fiberglass counterparts, but are less durable against abrasion or sharp objects. They are often the lightest kayaks.
A special type of skin-on-frame kayak is the folding kayak. It has a collapsible frame, of wood, aluminum or plastic, or a combination thereof, and a skin of water-resistant and durable fabric. Many types have air sponsons built into the hull, making the kayak float even if flooded.
Modern design.
Modern kayaks differ greatly from native kayaks in every aspect—from initial form through conception, design, manufacturing and usage. Modern kayaks are designed with CAD (Computer Aided Design) software, often in combination with CAD customized for naval design.
Modern kayaks serve diverse purposes, ranging from slow and easy touring on placid water, to racing and complex maneuvering in fast-moving whitewater, to fishing and long-distance ocean excursions. Modern forms, materials and construction techniques make it possible to effectively serve these needs while continuing to leverage the insights of the original Arctic inventors.
Kayaks are long—, short—, wide—, or as narrow as the paddler's hips. They may attach one or two stabilizing hulls (outriggers), have twin hulls like catamarans, inflate or fold. They move via paddles, pedals that turn propellers or underwater flippers, under sail, or motor. They're made of wood/canvas, wood, carbon fiber, fiberglass, Kevlar, polyethylene, polyester, rubberized fabric, neoprene, Nitrylon, polyvinyl chloride (PVC), polyurethane, and aluminum. They may sport rudders, fins, bulkheads, seats, eyelets, foot braces and cargo hatches. They accommodate 1-3 or more paddlers/riders.
Types.
Modern kayaks have evolved into specialized types that may be broadly categorized according to their application as "sea or touring kayaks", "whitewater" (or "river") "kayaks", "surf kayaks", "racing kayaks", "fishing kayaks"' and "recreational" kayaks. The broader kayak categories today are 'Sit-In', which is inspired mainly by traditional kayak forms, 'Sit-On-Top' (SOT), which evolved from paddle boards that were outfitted with footrests and a backrest, 'Hybrid', which are essentially canoes featuring a narrower beam and a reduced free board enabling the paddler to propel them from the middle of the boat, using a double blade paddle (i.e. 'kayak paddle'), and twin hull kayaks offering each of the paddler's legs a narrow hull of its own.
In recent decades, kayaks design have proliferated to a point where the only broadly accepted denominator for them is their being designed mainly for paddling using a kayak paddle featuring two blades i.e. 'kayak paddle'. However, even this inclusive definition is being challenged by other means of human powered propulsion, such as foot activated pedal drives combined with rotating or sideways moving propellers, electric motors, and even outboard motors.
Recreational.
Recreational kayaks are designed for the casual paddler interested in fishing, photography, or a peaceful paddle on a lake, flatwater stream or protected salt water away from strong ocean waves. These boats presently make up the largest segment of kayak sales. Compared to other kayaks, recreational kayaks have a larger cockpit for easier entry and exit and a wider beam ( for more stability. They are generally less than in length and have limited cargo capacity. Less expensive materials like polyethylene and fewer options keep these boats relatively inexpensive. Most canoe/kayak clubs offer introductory instruction in recreational boats. They do not perform as well in the sea. The recreational kayak is usually a type of touring kayak.
Sea.
"Sea kayaks" are typically designed for travel by one, two or even three paddlers on open water and in many cases trade maneuverability for seaworthiness, stability, and cargo capacity. Sea-kayak sub-types include "skin-on-frame" kayaks with traditionally constructed frames, open-deck "sit-on-top" kayaks, and recreational kayaks.
The sea kayak, though descended directly from traditional types, is implemented in a variety of materials. Sea kayaks typically have a longer waterline, and provisions for below-deck storage of cargo. Sea kayaks may also have rudders or skegs (fixed rudder) and upturned bow or stern profiles for wave shedding. Modern sea kayaks usually have two or more internal bulkheads. Some models can accommodate two or sometimes three paddlers.
Sit-on-top.
Sealed-hull (unsinkable) craft were developed for leisure use, as derivatives of surfboards (e.g. paddle or wave skis), or for surf conditions. Variants include planing surf craft, touring kayaks, and sea marathon kayaks. Increasingly, manufacturers build leisure 'sit-on-top' variants of extreme sports craft, typically using polyethylene to ensure strength and affordability,
often with a skeg for directional stability. Water that enters the cockpit drains out through scupper holes—tubes that run from the cockpit to the bottom of the hull.
Sit-on-top kayaks come in 1-4 paddler configurations. Sit-on-top kayaks are particularly popular for fishing and SCUBA diving, since participants need to easily enter and exit the water, change seating positions, and access hatches and storage wells. Ordinarily the seat of a sit-on-top is slightly above water level, so the center of gravity for the paddler is higher than in a traditional kayak. To compensate for the higher center of gravity, sit-on-tops are often wider and slower than a traditional kayak of the same length.
Contrary to popular belief, the sit-on-top kayak hull is not self bailing, since water penetrating it does not drain out automatically, as it does in bigger boats equipped with self bailing systems. Furthermore, the sit-on-top hull cannot be molded in a way that would assure water tightness, and water may get in through various holes in its hull, usually around hatches and deck accessories. If the sit-on-top kayak is loaded to a point where such perforations are covered with water, or if the water paddled is rough enough that such perforations often go under water, the sit-on-top hull may fill with water without the paddler noticing it in time.
Surf.
Specialty surf boats typically have flat bottoms, and hard edges, similar to surf boards. The design of a surf kayak promotes the use of an ocean surf wave (moving wave) as opposed to a river or feature wave (moving water). They are typically made from rotomolded plastic, or fiberglass.
Surf kayaking comes in two main varieties, High Performance (HP) and International Class (IC). HP boats tend to have a lot of nose rocker, little to no tail rocker, flat hulls, sharp rails and up to four fins set up as either a three fin thruster or a quad fin. This enables them to move at high speed and maneuver dynamically. IC boats have to be at least long and until a recent rule change had to have a convex hull; now flat and slightly concave hulls are also allowed, although fins are not. Surfing on international boats tends to be smoother and more flowing, and they are thought of as kayaking's "long boarding". Surf boats come in a variety of materials ranging from tough but heavy plastics to super light, super stiff but fragile foam–cored carbon fiber. Surf kayaking has become popular in traditional surfing locations, as well as new locations such as the Great Lakes.
"Surf skis", are specialized narrow and long boats for racing, surfing breaking waves and surf-zone rescues.
Waveskis.
A variation on the closed cockpit surf kayak is called a waveski. Although the waveski offers dynamics similar to a sit–on–top, its paddling technique and surfing performance and construction can be similar to surfboard designs.
Whitewater.
Whitewater kayaks are rotomolded in a semi-rigid, high impact plastic, usually polyethylene. Careful construction ensures that the boat remains structurally sound when subjected to fast-moving water. The plastic hull allows these kayaks to bounce off rocks without leaking, although they scratch and eventually wear through with enough use. Whitewater kayaks range from long. There are two main types of whitewater kayak:
Playboat.
One type, the "playboat", is short, with a scooped bow and blunt stern. These trade speed and stability for high maneuverability. Their primary use is performing tricks in individual water features or short stretches of river. In playboating or "freestyle" competition (also known as "rodeo" boating), kayakers exploit the complex currents of rapids to execute a series of tricks, which are scored for skill and style.
Creekboat.
The other primary type is the creek boat, which gets its name from its purpose: running narrow, low-volume waterways. Creekboats are longer and have far more volume than playboats, which makes them more stable, faster and higher-floating. Many paddlers use creekboats in "short boat" downriver races, and they are often seen on large rivers where their extra stability and speed may be necessary to get through rapids.
Between the creekboat and playboat extremes is a category called "river–running" kayaks. These medium–sized boats are designed for rivers of moderate to high volume, and some, known as "river running playboats", are capable of basic playboating moves. They are typically owned by paddlers who do not have enough whitewater involvement to warrant the purchase of more–specialized boats.
Squirt Boating involves paddling both on the surface of the river and underwater. Squirt boats must be custom-fitted to the paddler to ensure comfort while maintaining the low interior volume necessary to allow the paddler to submerge completely in the river.
Racing.
Whitewater.
White water racers combine a fast, unstable lower hull portion with a flared upper hull portion to combine flat water racing speed with extra stability in open water: they are not fitted with rudders and have similar maneuverability to flat water racers. They usually require substantial skill to achieve stability, due to extremely narrow hulls. Whitewater racing kayaks, like all racing kayaks, are made to regulation lengths, usually of fiber reinforced resin (usually epoxy or polyester reinforced with Kevlar, glass fiber, carbon fiber, or some combination). This form of construction is stiffer and has a harder skin than non-reinforced plastic construction such as rotomolded polyethylene: stiffer means faster, and harder means fewer scratches and therefore also faster.
Flatwater sprint.
Sprint kayak is a sport held on calm water. Crews or individuals race over 200 m, 500 m, 1000 m or 5000 m with the winning boat being the first to cross the finish line. The paddler is seated, facing forward, and uses a double-bladed paddle pulling the blade through the water on alternate sides to propel the boat forward. In competition the number of paddlers within a boat is indicated by a figure besides the type of boat; K1 signifies an individual kayak race, K2 pairs, and K4 four-person crews. Kayak sprint has been in every summer olympics since it debuted at the 1936 summer olympics. racing is governed by the International Canoe Federation.
Slalom.
Slalom kayaks are flat–hulled, and—since the early 1970s—feature low profile decks. They are highly maneuverable, and stable but not fast in a straight line.
Surf ski.
A specialized variant of racing kayak called a "surf ski" has an open cockpit and can be up to long but only wide, requiring expert balance and paddling skill. Surf skis were originally created for surf and are still used in races in New Zealand, Australia, and South Africa. They have become popular in the United States for ocean races, lake races and even downriver races.
Specialty and hybrids.
The term "kayak" increasingly applies to craft that look little like traditional kayaks.
Inflatable.
Inflatables, also known as the "duckies" or "IKs", can usually be transported by hand using a carry bag. They are generally made of hypalon (a kind of neoprene), Nytrylon (a rubberized fabric), PVC, or polyurethane coated cloth. They can be inflated with foot, hand or electric pumps. Multiple compartments in all but the least expensive increase safety. They generally use low pressure air, almost always below 3 psi.
While many inflatables are non-rigid, essentially pointed rafts, best suited for use on rivers and calm water, the higher end inflatables are designed to be hardy, seaworthy vessels. Recently some manufacturers have added an internal frame (folding-style) to a multi-section inflatable sit-on-top kayak to produce a seaworthy boat.
The appeal of inflatable kayaks is their portability, their durability (they don't dent), ruggedness in white water (they bounce off rocks rather than break) and their easy storage. In addition, inflatable kayaks generally are stable, have a small turning radius and are easy to master, although some models take more effort to paddle and are slower than traditional kayaks.
Because inflatable kayaks aren't as sturdy as traditional, hard-shelled kayaks, a lot of people tend to steer away from them. However, there have been considerable advancements in inflatable kayak technology over recent years.
Folding.
Folding kayaks are direct descendants of the skin-on-frame boats used by the Inuit and Greenlandic peoples. Modern folding kayaks are constructed from a wooden or aluminum frame over which is placed a synthetic skin made of polyester, cotton canvas, polyurethane, or Hypalon. They are more expensive than inflatable kayaks, but have the advantage of greater stiffness and consequently better seaworthiness.
Pedal.
A kayak with pedals allows the kayaker to propel the vessel with a rotating propeller or underwater "flippers" rather than with a paddle. In contrast to paddling, kayakers who pedal kayaks use their legs rather than their arms.
Twin hull and outrigger.
Traditional multi-hull vessels such as catamarans and outrigger canoes benefit from increased lateral stability without sacrificing speed, and these advantages have been successfully applied in twin hull kayaks of the W type. "Outrigger kayaks" attach one or two smaller hulls to the main hull to enhance stability, especially for fishing, touring, kayak sailing and motorized kayaking.
Twin hull kayaks feature two long and narrow hulls, and since all their buoyancy is distributed as far as possible from their center line, they are stabler than mono hull kayaks outfitted with outriggers.
Fishing.
While native people of the Arctic regions hunted rather than fished from kayaks, in recent years kayak sport fishing has become popular in both fresh and salt water, especially in warmer regions. Traditional fishing kayaks are characterized by wide beams of up to that increase their lateral stability. Some are equipped with outriggers that increase their stability, and others feature twin hulls enabling stand up paddling and fishing. Compared with motorboats, fishing kayaks are inexpensive and have few maintenance costs. Many kayak anglers like to customize their kayaks for fishing, a process known as 'rigging'.
Standing-up paddling.
While paddling in the standing position has been practised for centuries in canoes (including Umiaks, Pirogues, and native dugout canoes) recently kayakers have attempted paddling while standing up, but so far, this has been made possible only in twin hull kayaks of the W type, mainly due to comfort and safety reasons.
Military.
Kayaks were adapted for military use in the Second World War. Used mainly by British Commando and special forces, principally the Combined Operations Pilotage Parties (COPPs), the Special Boat Service and the Royal Marines Boom Patrol Detachment. The latter made perhaps the best known use of them in the Operation Frankton raid on Bordeaux harbor. Both the Special Air Service (SAS) and the Special Boat Service (SBS) used kayaks for reconnaissance in the 1982 Falklands War. US Navy SEALs reportedly used them at the start of Unified Task Force operations in Somalia in 1992. The SBS currently use Klepper two-man folding kayaks that can be launched from surfaced submarines or carried to the surface by divers from submerged ones. They can be parachuted from transport aircraft into the ocean or dropped from the back of Chinook helicopters. US Special Forces have used Kleppers but now primarily use Long Haul folding kayaks, which are made in the US.

</doc>
<doc id="17385" url="https://en.wikipedia.org/wiki?curid=17385" title="Imperial German Navy">
Imperial German Navy

The Imperial German Navy was the Imperial Navy () – the navy created at the time of the formation of the German Empire. It existed between 1871 and 1919, growing out of the small Prussian Navy (from 1867 the "Norddeutsche Bundesmarine"), which primarily had the mission of coastal defence. Kaiser Wilhelm II greatly expanded the navy, and enlarged its mission. The key leader was Admiral Alfred von Tirpitz, who greatly expanded the size and quality of the navy, while adopting the sea power theories of American strategist Alfred Thayer Mahan. The result was a naval arms race with Britain as the German navy grew to become one of the greatest maritime forces in the world, second only to the Royal Navy. The German surface navy proved ineffective during World War I; its only major engagement, the Battle of Jutland, was indecisive. However, the submarine fleet was greatly expanded and posed a major threat to the British supply system. The Imperial Navy's main ships were turned over to the Allies, but then were sunk at Scapa Flow in 1919 by German crews.
All ships of the Imperial Navy were designated "SMS", for "Seiner Majestät Schiff" (His Majesty's Ship).
Achievements.
The Imperial Navy achieved some important operational feats. At the Battle of Coronel, it inflicted the first major defeat on the Royal Navy in over one hundred years, although the German squadron of ships was subsequently defeated at the Battle of the Falkland Islands, only one ship escaping destruction. The Navy also emerged from the fleet action of the Battle of Jutland having destroyed more ships than it lost, although the strategic value of both of these encounters was minimal.
The Imperial Navy was the first to operate submarines successfully on a large scale in wartime, with 375 submarines commissioned by the end of the First World War, and it also operated zeppelins. Although it was never able to match the number of ships of the Royal Navy, it had technological advantages, such as better shells and propellant for much of the Great War, meaning that it never lost a ship to a catastrophic magazine explosion from an above-water attack, although the elderly pre-dreadnought sank rapidly at Jutland after a magazine explosion caused by an underwater attack.
1871 to 1888, Kaiser Wilhelm I.
The unification of Germany under Prussian leadership was the defining point for the creation of the Imperial Navy in 1871. The newly created emperor, Wilhelm I, as King of Prussia, had previously been head of state of the strongest state forming part of the new empire. The navy remained the same as that operated by the empire's predecessor organisation in the unification of Germany, the North German Federation, which itself in 1867 had inherited the navy of the Kingdom of Prussia. Article 53 of the new Empire's constitution recognised the existence of the Navy as an independent organisation, but until 1888 it was commanded by army officers and initially adopted the same regulations as the Prussian army. Supreme command was vested in the emperor, but its first appointed chief was "General der Infanterie" (General of the Infantry) Albrecht von Stosch. Kiel on the Baltic Sea and Wilhelmshaven on the North Sea served as the Navy's principal naval bases. The former Navy Ministry became the Imperial Admiralty on 1 February 1872, while Stosch became formally an admiral in 1875. Initially the main task of the new Imperial Navy was coastal protection, with France and Russia seen as Germany's most likely future enemies. The Imperial Navy's tasks were then to prevent any invasion force from landing and to protect coastal towns from possible bombardment.
In March 1872 a German Imperial Naval Academy was created at Kiel for training officers, followed in May by the creation of a 'Machine Engineer Corps', and in February 1873 a 'Medical Corps'. In July 1879 a separate 'Torpedo Engineer Corps' was created dealing with torpedoes and mines.
In May 1872 a ten-year building programme was instituted to modernise the fleet. This called for eight armoured frigates, six armoured corvettes, twenty light corvettes, seven monitors, two floating batteries, six avisos, eighteen gunboats and twenty-eight torpedo boats, at an estimated cost of 220 million gold marks. The building plan had to be approved by the "Reichstag", which controlled the allocation of funds, although one-quarter of the money came from French war reparations.
In 1883 Stosch was replaced by another general, Count Leo von Caprivi. At this point the navy had seven armoured frigates and four armoured corvettes, 400 officers and 5,000 ratings. The objectives of coastal defence remained largely unchanged, but there was a new emphasis on development of the torpedo, which offered the possibility of relatively small ships successfully attacking much larger ones. In October 1887 the first torpedo division was created at Wilhelmshaven and the second torpedo division based at Kiel. In 1887 Caprivi requested the construction of ten armoured frigates.
Greater importance was placed at this time on development of the army, which was expected to be more important in any war. However, the Kiel Canal was commenced in June 1887, which connected the North Sea with the Baltic through the Jutland peninsula, allowing German ships to travel between the two seas avoiding waters controlled by other countries. This shortened the journey for commercial ships, but specifically united the two areas principally of concern to the German navy, at a cost of 150 million marks.
Later, the protection of German maritime trade routes became important. This soon involved the setting up of some overseas supply stations, so called Auslandsstationen (foreign stations) and in the 1880s the Imperial Navy played a part in helping to secure the establishment of German colonies and protectorates in Africa, Asia and Oceania.
1888 to 1897, Kaiser Wilhelm II.
In June 1888 Wilhelm II became Emperor after the death of his father Frederick III, who ruled for only 99 days. He started his reign with the intention of doing for the navy what his grandfather Wilhelm I had done for the army. The creation of a maritime empire to rival the British and French empires became an ambition to mark Germany as a truly global great power. Wilhelm became Grand Admiral of the German Navy, but also was awarded honorific titles from all over Europe, becoming admiral in the British, Russian, Swedish, Danish, Norwegian, Austro-Hungarian and Greek navies. On one occasion he wore the uniform of a British admiral to receive the visiting British ambassador. At this time the Imperial Navy had 534 officers and 15,480 men.
The concept of expanding naval power, inevitably at the cost of not expanding other forces, was opposed by the three successive heads of the German armed forces, Waldersee, Schlieffen and Moltke between 1888 and 1914. It would also have been more widely opposed, had the Kaiser's intentions been widely known. Instead, he proceeded with a plan to expand the navy slowly, justifying enlargement step by step.
In July 1888 Wilhelm II appointed Vice-Admiral Alexander von Monts as head of the admiralty. Monts oversaw the design of the , four of which were constructed by 1894 at a cost of 16 million marks each and displacement of 10,000 tons.
In 1889 Wilhelm II reorganised top level control of the navy by creating a Navy Cabinet (Marine-Kabinett) equivalent to the German Imperial Military Cabinet which had previously functioned in the same capacity for both the army and navy. The Head of the navy cabinet was responsible for promotions, appointments, administration and issuing orders to naval forces. Captain Gustav von Senden-Bibran was appointed as its first head and remained so until 1906, when he was replaced by the long-serving Admiral Georg Alexander von Müller. The existing Imperial admiralty was abolished and its responsibilities divided between two organisations. A new position of Chief of the Imperial Naval High Command was created, being responsible for ship deployments, strategy and tactics, an equivalent to the supreme commander of the Army. Vice admiral Max von der Goltz was appointed in 1889 and remained in post until 1895. Construction and maintenance of ships and obtaining supplies was the responsibility of the State Secretary of the Imperial Navy Office (Reichsmarineamt), responsible to the chancellor and advising the Reichstag on naval matters. The first appointee was Rear Admiral Karl Eduard Heusner, followed shortly by Rear Admiral Friedrich von Hollmann from 1890 to 1897. Each of these three heads of department reported separately to Wilhelm II.
In 1895 funding was agreed for five battleships of the , completed by 1902. The ships were innovative for their time, introducing a complex system of watertight compartments and storing coal along the sides of the ship to help absorb explosions. However, the ships went against the trend for increasingly larger main guns, having smaller diameter guns than the "Brandenburg" design, but with a quick-loading design and more powerful secondary armaments. Costs rose to 21 million marks each, as had size to 11,500 tons.
In 1892 Germany had launched the protected cruiser , the first navy ship to have triple propellers. She was succeeded by five protected cruisers, the last 'protected', as distinct from 'armoured' cruiser class constructed by Germany. The ships, completed between 1898 and 1900, had deck armour but not side armour and were intended for overseas duties. Shortages of funding meant it was not possible to create several designs of cruisers specialised for long range work, or more heavily armoured for fleet work. Work commenced on an armoured cruiser design, started in 1896 and commissioned in 1900.
1897 to 1906 Tirpitz and the Navy Bills.
On 18 June 1897 Rear-Admiral Alfred von Tirpitz was appointed State Secretary of the Navy, where he remained for nineteen years. Tirpitz advocated the cause of an expanded navy necessary for Germany to defend her territories abroad. He had great success in persuading parliament to pass successive Navy bills authorising expansions of the fleet. German foreign policy as espoused by Otto von Bismarck had been to deflect the interest of great powers abroad while Germany consolidated her integration and military strength. Now Germany was to compete with the rest. Tirpitz started with a publicity campaign aimed at popularising the navy. He created popular magazines about the navy, arranged for Alfred Thayer Mahan's "The Influence of Sea Power upon History", which argued the importance of naval forces, to be translated into German and serialised in newspapers, arranged rallies in support and invited politicians and industrialists to naval reviews. Various pressure groups were formed to lobby politicians and spread publicity. One such organisation, the navy league or "Flottenverein", was organized by principals in the steel industry (Alfred Krupp), ship yards and banks, gaining more than one million members. Political parties were offered concessions, such as taxes on imported grain, in exchange for their support for naval bills.
On 10 April 1898 the first Navy Bill was passed by the Reichstag. It authorised the maintenance of a fleet of 19 battleships, 8 armoured cruisers, 12 large cruisers and 30 light cruisers to be constructed by 1 April 1904. Existing ships were counted in the total, but the bill provided for ships to be replaced every 25 years on an indefinite basis. Five million marks annually was allocated to run the navy, with a total budget of 408 million marks for shipbuilding. This would bring the German fleet to a strength where it could contemplate challenging France or Russia, but would remain clearly inferior to the world's largest fleet, the Royal Navy.
Following the Boxer rebellion in China and the Boer War, a second navy bill was passed on 14 June 1900. This approximately doubled the allocated number of ships to 38 battleships, 20 armoured cruisers, 38 light cruisers. Significantly, the bill set no overall cost limit for the building program. Expenditure for the navy was too great to be met from taxation: the Reichstag had limited powers to extend taxation without entering into negotiations with the constituent German states, and this was considered politically unviable. Instead, the bill was financed by massive loans. Tirpitz, in 1899 was already exploring the possibilities for extending the battleship total to 45, a target which rose to 48 by 1909.
Tirpitz’s ultimate goal was a fleet capable of rivaling the British fleet.. As British public opinion was turned against Germany, Admiral Sir John Fisher twice – in 1904 and 1908 – proposed using Britain’s current naval superiority to 'Copenhagen' the German fleet, that is, to launch pre-emptive strikes against the Kiel and Wilhelmshaven naval bases as the Royal Navy had done against the Danish navy in 1801 and 1807." Tirpitz argued that if the fleet could achieve two-thirds the number of capital ships possessed by Britain then it stood a chance of winning in a conflict. Britain had to maintain a fleet throughout the world and consider other naval powers, whereas the German fleet could be concentrated in German waters . Attempts were made to play down the perceived threat to Britain, but once the German fleet reached the position of equalling the other second-rank navies, it became impossible to avoid mention of the one great fleet it was intended to challenge. Tirpitz hoped that other second-rank powers might ally with Germany, attracted by its navy. The policy of commencing what amounted to a naval arms race did not properly consider how Britain might respond. British policy, stated in the Naval Defence Act of 1889, was to maintain a navy superior to Britain's two largest rivals combined. The British Admiralty estimated that the German navy would be the world's second largest by 1906.
Major reforms of the Royal Navy were undertaken, particularly by Admiral Jackie Fisher as First Sea Lord from 1904 to 1909. 154 older ships, including 17 battleships, were scrapped to make way for newer vessels. Reforms in training and gunnery were introduced to make good perceived deficiencies, which in part Tirpitz had counted upon to provide his ships with a margin of superiority. More capital ships were stationed in British home waters. A treaty with Japan in 1902 meant that ships could be withdrawn from East Asia, while the "Entente Cordiale" with France in 1904 meant that Britain could concentrate on guarding Channel waters, including the French coast, while France would protect British interests in the Mediterranean. By 1906 it was considered that Britain's only likely naval enemy was Germany.
Five battleships of the were constructed from 1899 to 1904 at a cost of 22 million marks per ship. Five ships of the were built between 1901 and 1906 for the slightly greater 24 million marks each. Technological improvements meant that rapid fire guns could be made larger, so the "Braunschweig" class had a main armament of guns. Due to torpedo improvements in range and accuracy, emphasis was placed on a secondary armament of smaller guns to defend against them. The five s constructed between 1903 and 1908 had similar armament as the "Braunschweig" class, but heavier armour, for the slightly greater sum of 24.5 million marks each.
Development of armoured cruisers also continued. "Fürst Bismarck"s design was improved upon in the subsequent , completed in 1902. Two ships of the were commissioned in 1904, followed by two similar armoured cruisers commissioned in 1905 and 1906, at costs around 17 million marks each. and followed, between 1904 and 1908, and cost an estimated for 20.3 million marks. Main armament was eight guns, but with six and eighteen guns for smaller targets. Eight light cruisers were constructed between 1902 and 1907, developed from the earlier . The ships had ten guns and were named after German towns. was the first German cruiser to be fitted with turbine engines, which were also trialled in torpedo boat "S-125". Turbines were faster, quieter, lighter, more reliable and more fuel efficient at high speeds. The first British experimental design (the destroyer ) had been constructed in 1901 and as a result Tirpitz had set up a special commission to develop turbines. No reliable German design was available by 1903, so British Parsons turbines were purchased.
Command reorganisation.
In 1899, the Imperial Naval High Command was replaced by the German Imperial Admiralty Staff ("Admiralstab") responsible for planning, the training of officers, and naval intelligence. In time of war it was to assume overall command, but in peace acted only advisory. Direct control of various elements of the fleet was subordinated to officers commanding those elements, accountable to the Kaiser.
The reorganisations suited the Kaiser who wanted to maintain direct control of his ships. A disadvantage was that it split apart the integrated military command structure which before had balanced the importance of the navy within overall defence considerations. It suited Alfred von Tirpitz, because it removed the influence of the admiralty staff from naval planning, but left him the possibility, in wartime, to reorganise command around himself. Wilhelm II, however, never agreed to relinquish direct control of his fleet.
1906 to 1908, The Dreadnought and innovation: First "Novelle".
On 3 December 1906 the Royal Navy received a new battleship, . She became famous as the first of a new concept in battleship design, using all big gun, single size of calibre armament. She used turbine propulsion for greater speed and less space required by the machinery, and guns arranged so that three times as many could be brought to bear when firing ahead, and twice as many when firing broadside. The design was not a uniquely British concept as similar ships were being built around the world, nor was it uniquely intended as a counter to German naval expansion, but the effect was to immediately require Germany to reconsider its naval building program. The battleship design was complemented by the introduction of a variant with lighter armour and greater speed, which became the battlecruiser.
The revolution in design, together with improvements in personnel and training severely brought into question the German assumption that a fleet of two-thirds the size of the Royal Navy would at least stand a chance in an engagement. By 1906 Germany was already spending 60% of revenue upon the army. Either an enormous sum now had to be found to develop the navy further, or naval expansion had to be abandoned. The decision to continue was taken by Tirpitz in September 1905 and agreed by Chancellor Bernhard von Bülow and the Kaiser, while "Dreadnought" was still at the planning stage. The larger ships would naturally be more expensive, but also would require enlargement of harbours, locks and the Kiel canal, all of which would be enormously expensive. Estimated cost for new dreadnoughts was placed at 36.5 million marks for 19,000 tons displacement ships (larger than "Dreadnought" at 17,900 tons), and 27.5 million marks for battle-cruisers. 60 million mark was allocated for dredging the canal. The Reichstag was persuaded to agree to the program and passed a "Novelle" (a supplementary law) amending the navy bills and allocating 940 million marks for a dreadnought program and the necessary infrastructure. Two dreadnoughts and one battlecruiser were to be built each year.
Construction of four s began in 1907 under the greatest possible secrecy. The chief German naval designer was Hans Bürkner. A principle was introduced that the thickness of side armour on a ship would equal the calibre of the large guns, while ships were increasingly divided internally into watertight compartments to make them more resistant to flooding when damaged. The design was hampered by the necessity to use reciprocating engines instead of the smaller turbines, since no sufficiently powerful design was available and acceptable to the German navy. Turrets could not be placed above the centre of the ship and instead had to be placed at the side, meaning two of the six turrets would always be on the wrong side of the ship when firing broadsides. Main armament was twelve 28 cm guns. The ships were all completed by 1910, over budget, averaging 37.4 million marks each. In 1910 they were transferred from Kiel to Wilhelmshaven, where two new large docks had been completed and more were under construction.
The first German battlecruiser——was commenced March 1908. Four Parsons turbines were used, improving speed to 27 knots and reducing weight. Four twin turrets mounted 28 cm guns; although the two centre turrets were still placed one either side of the ship, they were offset so could now fire either side. The design was considered a success, but the cost at 35.5 million marks was significantly above the 1906 allocation. Light cruiser development continued with the light cruisers, which were to become famous for their actions in the start of World War I in the Pacific. The ships were 3,300 tons, and armed with ten 10.5 cm rapid fire guns and a speed around 24 knots. cost 7.5 million marks, and 6 million marks. Four were produced between 1907 and 1911 at 4,400 tons and around 8 million marks each. These had turbines, twelve 10.5 cm guns as main armament, but were also equipped to carry and lay 100 mines. From 1907 onward, all torpedo boats were constructed using turbine engines.
Despite their ultimate importance, the German navy declined to take up the cause of another experiment, the submarine, until 1904. The first submarine, was delivered in December 1906, built by Krupp's Germania yard in Kiel. The first submarine had 238 ton displacement on the surface and 283 tons submerged. The kerosene engine developed 10 knots on the surface with a range of . Submerged, the ship could manage 50 nautical miles at 5 knots using battery electric propulsion. The ships followed a design by Maxime Laubeuf first used successfully in 1897, having a double hull and flotation tanks around the outside of the main crew compartments. The submarine had just one torpedo tube at the front and a total of three torpedoes. The early engines were noisy and smoky, so that a considerable boost to the usefulness of the submarine came with the introduction of quieter and cleaner diesel engines in 1910, which were much more difficult for an enemy to detect.
1908 to 1912, Second "Novelle".
German expenditure on ships was steadily rising. In 1907, 290 million marks was spent on the fleet, rising to 347 million marks or 24 percent of the national budget in 1908, with a predicted budget deficit of 500 million marks. By the outbreak of World War I, one billion marks had been added to Germany's national debt because of naval expenditures. While each German ship was more expensive than the last, the British managed to reduce the cost of the succeeding generations of (3 ships) and (3) battleships. Successive British battle-cruisers were more expensive, but less so than their German equivalents. Overall, German ships were some 30% more expensive than the British. This all contributed to growing opposition in the Reichstag to any further expansion, particularly when it was clear that Britain intended to match and exceed any German expansion program. In the fleet itself, complaints were beginning to be made in 1908 about underfunding and shortages of crews for the new ships. The State Secretary of the Treasury, Hermann von Stengel, resigned because he could see no way to resolve the budget deficit.
The elections of 1907 had returned a Reichstag more favourable to military exploits, following the refusal of the previous parliament to grant funds to suppress uprisings in colonies in German South-West Africa. Despite the difficulties, Tirpitz persuaded the Reichstag to pass a further "Novelle" in March 1908. This reduced the service life for ships from 25 years to 20 years, allowing for faster modernisation, and increased the building rate to four capital ships per year. Tirpitz' target was a fleet of 16 battleships and 5 battle-cruisers by 1914, and 38 battleships and 20 battle-cruisers by 1920. There were also to be 38 light cruisers, and 144 torpedo boats. The bill contained a restriction, that building would fall to two ships per year in 1912, but Tirpitz was confident of changing this at a later date. He anticipated that German industry, now heavily involved in shipbuilding, would back a campaign to maintain a higher construction rate.
Four battleships of the were laid down in 1909–1910, with displacements of 22,800 tons, twelve guns in 6 turrets, reciprocating engines generating a maximum speed of 21 knots, and a price tag of 46 million marks. Again, the turret configuration was dictated by the need to use the centre of the ship for machinery, despite the disadvantage of the turret layout. The ships were now equipped with torpedoes.
The s built between 1909 and 1913 introduced a change in design as turbine engines were finally approved. The ships had ten 30.5 cm guns, losing two of the centre side turrets but gaining an additional turret astern on the centre line. As with the "Von der Tann" design, which was drawn up at a similar time, all guns could be fired either side in broadsides, meaning more guns could come to bear than with the "Helgoland" design, despite having fewer in total. Five ships were constructed rather than the usual four, one to act as a fleet flagship. One ship, the , was equipped with only two turbines rather than three, with the intention of having an additional diesel engine for cruising, but the Howaldt engine could not be developed in time. "Luitpold" had a top speed of 20 knots as a result, compared to 22 knots for the other ships. The ships were larger than the preceding class at 24,700 tons, but cheaper at 45 million marks. They formed part of the third squadron of the High Seas Fleet as it was constituted for World War I.
Between 1908 and 1912 two s were constructed, adding an extra turret on the centre line astern, raised above the aft turret, but still using 28 cm guns. became part of the High Seas Fleet, but became part of the Mediterranean squadron and spent World War I as part of the Ottoman navy. The ships cost 42.6 and 41.6 million marks, with maximum speed of 28 knots. was constructed as a slightly enlarged version of the "Moltke" design, reaching a maximum speed of 29 knots. All cruisers were equipped with turbine engines from 1908 onwards. Between 1910 and 1912 four light cruisers were constructed of 4,600 tons, at around 7.4 million marks each. The ships were fitted with oil burners to improve the effectiveness of their main coal fueling. These were followed by the similar but slightly enlarged and marginally faster and light cruisers.
In 1907 a naval artillery school was established at Sonderburg (north of Kiel). This aimed to address the difficulties with the new generation of guns, which with potentially greater range required aiming devices capable of directing them at targets at those extreme ranges. By 1914, experiments were being conducted with guns in increasing sizes up to . Capital ships were fitted with spotting tops high up on masts with range finding equipment, while ship design was altered to place turrets on the centre line of the ship for improved accuracy.
The four s were commenced between October 1911 and May 1912 and entered service in 1914 at a cost of 45 million marks, forming the other part of the Third Squadron of the High Seas Fleet. They were 28,500 tons, with a maximum speed of 21 knots from three triple-stage Brown-Boverie-Parsons turbines. Main armament was five double turrets housing twin 30.5 cm guns, arranged with two turrets fore and aft and one in the centre of the ship. The second turret at either end was raised higher than the outer so that it could fire over the top (superfiring). As with "Prinzregent Luitpold", the ships were originally intended to have one diesel engine for cruising, but these were never developed and turbines were fitted instead. The ships were equipped with torpedo nets, trailed along the hull intended to stop torpedoes, but these reduced maximum speed to an impractical 8 knots and were later removed.
Construction began in 1910 of the first submarine powered by twin diesel engines. "U-19" was twice the size of the first German submarine, had five times the range at cruising at 8 knots, or 15 knots maximum. There were now two bow and two stern torpedo tubes, with six torpedoes carried. The ships were designed to operate at a depth of , though could go to .
1912 to 1914, Third "Novelle".
Spending on the navy increased inexorably year by year. In 1909 Chancellor Bernhard von Bülow and Treasury Secretary Reinhold von Sydow attempted to pass a new budget boosting taxes in an attempt to reduce the deficit. The Social Democratic parties refused to accept the increased taxes on goods, while the conservatives opposed increases in inheritance taxes. Bülow and Sydow resigned in defeat and Theobald von Bethmann-Hollweg became Chancellor. His attempted solution was to initiate negotiations with Britain for an agreed slow down in naval building. Negotiations came to nothing when in 1911 the Agadir Crisis brought France and Germany into conflict. Germany attempted to 'persuade' France to cede territory in the Middle Congo in return for giving France a free hand in Morocco. The effect was to raise concerns in Britain over Germany's expansionist aims, and encouraged Britain to form a closer relationship with France, including naval cooperation. Tirpitz saw this once again as an opportunity to press for naval expansion and the continuation of the four capital ships per year building rate into 1912. The January 1912 elections brought a Reichstag where the Social Democrats, opposed to military expansion, became the largest party.
The German army, mindful of the steadily increasing proportion of spending going to the navy, demanded an increase of 136,000 men to bring its size closer to that of France. In February 1912 the British war minister, Viscount Haldane, came to Berlin to discuss possible limits to naval expansion. Meanwhile, in Britain, the First Lord of the Admiralty Winston Churchill made a speech describing the German navy as a 'luxury', which was considered an insult when reported in Germany. The talks came to nothing, ending in recriminations over who had offered what. Bethmann-Hollweg argued for a guaranteed proportion of expenditure for the army, but failed when army officers refused to support him publicly. Tirpitz argued for six new capital ships, and got three, together with 15,000 additional sailors in a new combined military budget passed in April 1912. The new ships, together with the existing reserve flagship and four reserve battleships were to become one new squadron for the High Seas Fleet. In all the fleet would have five squadrons of eight battleships, twelve large cruisers and thirty small, plus additional cruisers for overseas duties. Tirpitz intended that with the rolling program of replacements, the existing coastal defence squadron of old ships would become a sixth fleet squadron, while the eight existing battle-cruisers would be joined by eight more as replacements for the large cruisers presently in the overseas squadrons. The plan envisaged a main fleet of 100,000 men, 49 battleships and 28 battle-cruisers by 1920. The Kaiser commented of the British, "... we have them up against the wall."
Although Tirpitz had succeeded in getting more ships, the proportion of military expenditure on the navy declined in 1912 and thereafter, from 35% in 1911 to 33% in 1912 and 25% in 1913. This reflected a change in attitude amongst military planners that a land war in Europe was increasingly likely, and a turning away from Tirpitz's scheme for worldwide expansion using the navy. In 1912 General von Moltke commented, "I consider war to be unavoidable, and the sooner the better." The Kaiser's younger brother, Admiral Prince Heinrich of Prussia, considered that the cost of the navy was now too great. In Britain, Churchill announced an intention to build two capital ships for every one constructed by Germany, and reorganised the fleet to move battleships from the Mediterranean to Channel waters. A policy was introduced of promoting British naval officers by merit and ability rather than time served, which saw rapid promotions for Jellicoe and Beatty, both of whom had important roles in the forthcoming World War I. By 1913 the French and British had plans in place for joint naval action against Germany, and France moved its Atlantic fleet from Brest to Toulon, replacing British ships.
Britain also escalated the arms race by expanding the capabilities of its new battleships. The five 1912 of 32,000 tons would have guns and would be completely oil-fuelled, allowing a speed of 25 knots. For 1912–13 Germany concentrated on battlecruisers, with three ships of 27,000 tons and 26–27 knots maximum speed, costing 56–59 million marks each. These had four turrets mounting two 30.5 cm guns arranged in two turrets either end, with the inner turret superfiring over the outer. was the first German ship to have anti-aircraft guns fitted.
In 1913, Germany responded to the British challenge by laying down two battleships. These did not enter service until after the Battle of Jutland, so failed to take part in any major naval action of the war. They had displacement of 28,600 tons, a crew of 1,100 and a speed of 22 knots, costing 50 million marks. Guns were arranged in the same pattern as the preceding battle-cruisers, but were now increased to diameter. The ships had four 8.8 cm anti-aircraft and also sixteen 15 cm lighter guns, but were coal fuelled. It was considered that coal bunkers at the sides of the ship added to protection against penetrating shells, but Germany also did not have a reliable supply of fuel oil. Two more ships of the class were later laid down, but never completed.
Three light cruisers commenced construction in German yards in 1912–1913 ordered by the Russian Navy, costing around 9 million marks. The ships were seized at the outbreak of World War I becoming , and . Two larger cruisers, and were also commenced and entered service in 1915. More torpedo boats were constructed, with gradually increasing sizes having reached 800 tons for the V-25 to V-30 craft constructed by AG Vulcan in Kiel before 1914. In 1912 Germany created a Mediterranean squadron consisting of the battle-cruiser "Goeben" and light cruiser "Breslau".
Air Power.
Naval trials of balloons began in 1891, but the results were unsatisfactory and none were purchased by the navy. In 1895 Count Ferdinand von Zeppelin attempted to interest both the army and navy in his new dirigibles, but without success. The dirigibles were considered too slow and there were concerns with their reliability operating over water. In 1909 the navy rejected proposals for aircraft to be launched from ships, and again in 1910 declined Zeppelin's dirigibles. Finally in 1911, trials with aircraft began and in 1912 Tirpitz agreed to purchase the first dirigible for naval reconnaissance at a cost of 850,000 marks.
The machine had insufficient range () to operate over Britain, but had machine guns for use against aircraft and experimental bombs. The following year ten more were ordered and a new naval air division was created at Johannisthal, near Berlin. However, in September 1913 L 1 was destroyed in a storm, while the following month L 2 was lost in a gas explosion. Orders for the undelivered machines were cancelled, leaving the navy with one machine, the L 3.
In 1910 Prince Heinrich had learnt to fly and supported the cause of naval aviation. In 1911 experiments took place with Albatros seaplanes and in 1912 Tirpitz authorized 200,000 marks for seaplane trials. The Curtiss seaplane was adopted. By 1913 there were four aeroplanes, now including a British Sopwith, and long term plans to create six naval air stations by 1918. By 1914 there were twelve seaplanes and one landplane and an 8.5 million marks budget. Trials in 1914 using seaplanes operating with the fleet were less than impressive; out of four taking part one crashed, one was unable to take off and only one succeeded in all tasks. The most successful aircraft had been the British design, and indeed experiments in Britain had been proceeding with the support of Winston Churchill, and included converting ferries and liners into seaplane carriers.
World War I.
By the start of the First World War, the German Imperial Navy possessed 22 pre-Dreadnoughts, 14 dreadnought battleships and 4 battle-cruisers. A further three ships of the were completed between August and November 1914, and two s entered service in 1916. The battlecruisers , , and were completed in September 1914, March 1916, and May 1917, respectively.
Admiral von Tirpitz became the commander of the Navy. The main fighting forces of the navy were to become the High Seas Fleet, and the U-boat fleet. Smaller fleets were deployed to the German overseas protectorates, the most prominent being assigned to the East Asia Station at Tsingtao.
The German Navy's U-boats were also instrumental in the sinking of the passenger liner and auxiliary cruiser, the on 7 May 1915, which was one of the main events that led to the USA joining the war two years later in 1917.
Engagements.
Notable battles fought by the Navy were:
Notable minor battles:
Minor engagements included the commerce raiding carried out by the , , and the sailing ship and commerce raider .
The Imperial Navy carried out land operations, e.g. operating the long-range Paris Gun which was based on a naval gun. The Siege of Tsingtao used naval troops as Tsingtao was a naval base, and also as the Imperial Navy was directly under the Imperial Government (the German Army was made up of regiments from the various states).
Following the Battle of Jutland, the capital ships of the Imperial Navy had been confined to inactive service in harbor. In October 1918, the Imperial Naval Command in Kiel under Admiral Franz von Hipper, without authorization, planned to dispatch the fleet for a last battle against the Royal Navy in the English Channel. The naval order of 24 October 1918 and the preparations to sail first triggered the Kiel Mutiny among the affected sailors and then a general revolution which was to sweep aside the monarchy within a few days.
Marines.
The Marines were referred to as Seebataillone (sea battalions). They served in the Prussian Navy, the navy of the North German Confederation, the Imperial German Navy and briefly in the modern Federal German Navy, the Bundesmarine.
Naval aviation.
The main use of the airships was in reconnaissance over the North Sea and the Baltic, where the endurance of the craft led German warships to a number of Allied vessels. Zeppelin patrolling had priority over any other airship activity. During the entire war around 1,200 scouting flights were made. During 1915 the German Navy had some 15 Zeppelins in commission and was able to have two or more patrolling continuously at any one time. They kept the British ships from approaching Germany, spotted when and where the British were laying sea-mines, and later aided in the destruction of those mines. Zeppelins would sometimes land on the sea surface next to a minesweeper, bring aboard an officer and show him the lay of the mines. 
The Naval and Army Air Services also directed a number of strategic raids against Britain, leading the way in bombing techniques and also forcing the British to bolster their anti-aircraft defences. The possibility of airship raids were approved by the Kaiser on 9 January 1915, although he excluded London as a target and further demanded that no attacks be made on historic or government buildings or museums. The night-time raids were intended to target only military sites on the east coast and around the Thames estuary, but difficulties in navigation and the height from which the bombs were dropped made accurate bombing impossible, and most bombs fell on civilian targets or open countryside.
The German Navy also had its equivalent of the Royal Naval Air Service in the Marinefliegerkorps. Theo Osterkamp was one of the original German naval pilots and its leading ace with 32 victories. By war's end, the roster of German naval flying aces also included such luminaries as Gotthard Sachsenberg (31 victories), Alexander Zenzes (18 victories), Friedrich Christiansen (13 victories), Karl Meyer (8 victories). Karl Scharon (8 victories), and Hans Goerth (7 victories).
Post war.
After the end of World War I, the bulk of the Navy's modern ships (74 in all) were interned at Scapa Flow, where the entire fleet (with a few exceptions) was scuttled by its crews on 21 June 1919 on orders from its commander, Rear Admiral Ludwig von Reuter.
Many of the ships were subsequently salvaged by Ernest Cox.
Ranks and rates of the Imperial Navy (English translation).
The Imperial German Navy's rank and rating system combined that of Prussia's with the navies of other northern states, thus the resulting system became one of Europe's best.

</doc>
<doc id="17386" url="https://en.wikipedia.org/wiki?curid=17386" title="Kriegsmarine">
Kriegsmarine

The Kriegsmarine (, "War Navy") was the navy of Nazi Germany from 1935 to 1945. It superseded the "Imperial German Navy" of World War I and the inter-war "Reichsmarine". The "Kriegsmarine" was one of three official branches of the Wehrmacht, the armed forces of Nazi Germany.
The "Kriegsmarine" grew rapidly during German naval rearmament in the 1930s (the Treaty of Versailles had limited the size of the German navy previously, and prohibited building of submarines). "Kriegsmarine" ships were deployed to the waters around Spain during the Spanish Civil War, under the guise of enforcing non-intervention, but in reality supporting the Franco side of the war.
In January 1939 Plan Z was ordered, calling for naval parity with the Royal Navy by 1944. However, when World War II broke out in September 1939, Plan Z was shelved in favour of building submarines (U-boats) and prioritizing land and air forces.
The Commander-in-Chief of the "Kriegsmarine" (as for all branches of armed forces during the period of absolute Nazi power) was Adolf Hitler, who exercised his authority through the "Oberkommando der Marine".
The "Kriegsmarine"s most famous ships were the U-boats, most of which were constructed after Plan Z was abandoned at the beginning of World War II. Wolfpacks were rapidly assembled groups of submarines which attacked British convoys during the first half of the Battle of the Atlantic but this tactic was largely abandoned in the second half of the war. Along with the U-boats, surface commerce raiders (including auxiliary cruisers) were used to disrupt Allied shipping in the early years of the war, the most famous of these being the heavy cruisers "Admiral Graf Spee" and "Admiral Scheer" and the battleship "Bismarck". However, the adoption of convoy escorts, especially in the Atlantic, greatly reduced the effectiveness of commerce raiders against convoys.
After the Second World War, the "Kriegsmarine"'s remaining ships were divided up amongst the Allied powers and were used for various purposes including minesweeping.
Command structure.
Adolf Hitler was the Commander-in-Chief of all German armed forces, including the "Kriegsmarine". His authority was exercised through the "Oberkommando der Marine", or OKM, with a Commander-in-Chief ("Oberbefehlshaber der Kriegsmarine"), a Chief of Naval General Staff ("Chef des Stabes der Seekriegsleitung") and a Chief of Naval Operations ("Chef der Operationsabteilung"). The first Commander-in-Chief of the OKM was Erich Raeder who was the Commander-in-Chief of the "Reichsmarine" when it was renamed and reorganized in 1935. Raeder held the post until falling out with Hitler after the German failure in the Battle of the Barents Sea. He was replaced by Karl Dönitz on 30 January 1943 who held the command until he was appointed President of Germany upon Hitler's suicide in April 1945. Hans-Georg von Friedeburg was then Commander-in-Chief of the OKM for the short period of time until Germany surrendered in May 1945.
Subordinate to these were regional, squadron and temporary flotilla commands. Regional commands covered significant naval regions and were themselves sub-divided, as necessary. They were commanded by a "Generaladmiral" or an Admiral. There was a "Marineoberkommando" for the Baltic Fleet, Nord, Nordsee, Norwegen, Ost/Ostsee (formerly Baltic), Süd and West. The "Kriegsmarine" used a form of encoding called "Gradnetzmeldeverfahren" to denote regions on a map.
Each squadron (organized by type of ship) also had a command structure with its own Flag Officer. The commands were Battleships, Cruisers, Destroyers, Submarines ("Führer der Unterseeboote"), Torpedo Boats, Minesweepers, Reconnaissance Forces, Naval Security Forces, Big Guns and Hand Guns, and Midget Weapons.
Major naval operations were commanded by a "Flottenchef". The "Flottenchef" controlled a flotilla and organized its actions during the operation. The commands were, by their nature, temporary.
The "Kriegsmarine"'s ship design bureau, known as the "Marineamt", was administered by officers with experience in sea duty but not in ship design, while the naval architects who did the actual design work had only a theoretical understanding of design requirements. As a result the German surface fleet was plagued by design flaws throughout the war.
History.
Post–World War I origins.
Under the terms of the Treaty of Versailles in 1919, Germany was only allowed a minimal navy of 15,000 personnel, six capital ships of no more than 10,000 tons, six cruisers, twelve destroyers, twelve torpedo boats and no submarines or aircraft carriers. Military aircraft were also banned, so Germany could have no naval aviation. Under the treaty Germany could only build new ships to replace old ones. All the ships allowed and personnel were taken over from the "Kaiserliche Marine", renamed "Reichsmarine".
Build-up during the interwar period.
The launching of the first pocket battleship, in 1931 (as a replacement for the old pre-dreadnought battleship "Preussen") was a sign for the rebuilding of a modern German fleet. Modern destroyers and light cruisers were also built. All of these new ships were built in accordance with the terms of the Treaty of Versailles that allowed replacements of the old ships taken over from the German World War I fleet. The building of the "Deutschland" caused consternation among the French and the British as they had expected that the restrictions of the Treaty of Versailles would limit the replacement of the pre-dreadnought battleships to coastal defence ships, suitable only for defensive warfare. By using innovative construction techniques, the Germans had built a heavy ship suitable for offensive warfare on the high seas while still abiding by the letter of the treaty.
From the start, through German-owned front companies, the Germans continued the banned work on U-boats through a submarine design office in the Netherlands and a torpedo research program in Sweden.
Even before the Nazi takeover on 30 January 1933 the German government decided on 15 November 1932 to launch a naval re-armament program that included U-boats, airplanes and an aircraft carrier which were not allowed under the terms of the Treaty of Versailles.
When the Nazis came to power in 1933, Adolf Hitler soon began to ignore many of the Treaty restrictions and accelerated German naval rearmament. The Anglo-German Naval Agreement of 18 June 1935 allowed Germany to build a navy equivalent to 35% of the British surface ship tonnage and 45% of British submarine tonnage; battleships were to be limited to no more than 35,000 tons. That same year the "Reichsmarine" was renamed as the "Kriegsmarine". In April 1939, as tensions escalated between the United Kingdom and Germany over Poland, Hitler unilaterally rescinded the restrictions of the Anglo-German Naval Agreement.
The building-up of the German fleet in the time period of 1935–1939 was slowed by problems with marshaling enough manpower and material for ship building. This was because of the simultaneous and rapid build-up of the German army and air force which demanded substantial effort and resources. Some projects, like the D-class cruisers and the P-class cruisers, had to be cancelled.
Spanish Civil War.
The first military action of the "Kriegsmarine" came during the Spanish Civil War (1936–1939). Following the outbreak of hostilities in July 1936 several large warships of the German fleet were sent to the region. The heavy cruisers and , and the light cruiser were the first to be sent in July 1936. These large ships were accompanied by the 2nd Torpedo-boat Flotilla. The German presence was used to covertly support Franco's Nationalists although the immediate involvement of the "Deutschland" was humanitarian relief operations and the rescuing of 9,300 refugees from the fighting, including 4,550 Germans. Following the brokering of the International Non-Intervention Patrol to enforce an international arms embargo the "Kriegsmarine" was allotted the patrol area between Cabo de Gata (Almeria) and Cabo de Oropesa. Numerous vessels served as part of these duties including . U-Boats also participated in covert action against Republican shipping as part of Operation "Ursula". At least eight U-Boats engaged a small number of targets in the area throughout the conflict. By way of comparison the Italian Navy, "Regia Marina", operated 58 submarines in the area as part of "Sottomarini Legionari". On 29 May 1937 the "Deutschland" was attacked in the Deutschland incident off Ibiza by two bombers from the Republican Airforce. Total casualties from the Republican attack were 31 dead and 110 wounded, 71 seriously, mostly burn victims. In retaliation the "Admiral Scheer" shelled the harbour of Almeria on 31 May. Following further attacks by Republican submarine forces against the off the port of Oran between 15–18 June 1937 Germany withdrew from the Non-Intervention Patrol although the "Kriegsmarine" maintained a continuous presence in the area until the end of the conflict.
Plan Z.
The "Kriegsmarine" saw as her main tasks the controlling of the Baltic Sea and winning a war against France in connection with the German army, because France was seen as the most likely enemy in the event of war. But in 1938 Hitler wanted to have the possibility of winning a war against Great Britain at sea in the coming years. Therefore he ordered plans for such a fleet from the "Kriegsmarine". From the three proposed plans (X, Y and Z) he approved Plan Z in January 1939. This blueprint for the new German naval construction program envisaged building a navy of approximately 800 ships during the period 1939–1947. Hitler demanded that the program was to be completed by 1945. The main force of Plan Z were six H-class battleships. In the version of Plan Z drawn up in August 1939 the German fleet was planned to consist of the following ships by 1945:
Personnel strength was planned to rise to over 200,000.
The planned naval program was not very far advanced by the time World War II began. In 1939 two s and two H-class battleships were laid down and parts for two further H-class battleships and three s were in production. The strength of the German fleet at the beginning of the war was not even 20% of Plan Z. On 1 September 1939, the navy still had a total personnel strength of only 78,000, and it was not at all ready for a major role in the war. Because of the long time it would take to get the Plan Z fleet ready for action and shortage in workers and material in wartime, Plan Z was essentially shelved in September 1939 and the resources allocated for its realization were largely redirected to the construction of U-boats, which would be ready for war against the United Kingdom quicker.
World War II.
The "Kriegsmarine" was involved in World War II from its outset and participated in the Battle of Westerplatte and the Battle of the Danzig Bay during the Invasion of Poland. In 1939, major events for the "Kriegsmarine" were the sinking of the British aircraft carrier and the British battleship and the loss of the at the Battle of the River Plate. Submarine attacks on Britain's vital maritime supply routes (Battle of the Atlantic) started immediately at the outbreak of war, although they were hampered by the lack of well placed ports from which to operate. Throughout the war the "Kriegsmarine" was responsible for coastal artillery protecting major ports and important coastal areas. It also operated anti-aircraft batteries protecting major ports.
In April 1940, the German Navy was heavily involved in the invasion of Norway, where it suffered significant losses, which included the heavy cruiser sunk by artillery and torpedoes from Norwegian shore batteries at the Oscarsborg Fortress in Oslofjord. Ten destroyers were lost in the Battles of Narvik (half of German destroyer strength at the time), and two light cruisers, the Königsberg which was bombed and sunk by Royal Navy aircraft in Bergen, and the Karlsruhe which was sunk off the coast of Kristiansand by a British submarine. The "Kriegsmarine" did in return sink some British warships during this campaign, including the aircraft carrier .
The losses in the Norwegian Campaign left only a handful of undamaged heavy ships available for the planned, but never executed, invasion of Britain (Operation "Sea Lion") in the summer of 1940. There were serious doubts that the invasion sea routes could have been protected against British naval interference. The fall of France and the conquest of Norway gave German submarines greatly improved access to British shipping routes in the Atlantic. At first, British convoys lacked escorts that were adequate either in numbers or equipment and, as a result, the submarines had much success for few losses (this period was dubbed the First Happy Time by the Germans).
Italy entered the war in June 1940, and the Battle of the Mediterranean began: from September 1941 to May 1944 some 62 German submarines were transferred there, sneaking past the British naval base at Gibraltar. The Mediterranean submarines sank 24 major Allied warships (including 12 destroyers, 4 cruisers, 2 aircraft carriers and 1 battleship) and 94 merchant ships (449,206 tons of shipping). None of the Mediterranean submarines made it back to their home bases, as they were all either sunk in battle or scuttled by their crews at the end of the war
In 1941 one of the four modern German battleships, the sank while breaking out into the Atlantic for commerce raiding. The "Bismarck" was in turn hunted down by much superior British forces after being crippled by an airborne torpedo. She was subsequently scuttled after being rendered defenceless by two British battleships.
During 1941, the "Kriegsmarine" and the United States Navy became de facto belligerents, although war was not formally declared, leading to the sinking of the . This hostility was the result of the American decision to support Britain with its Lend-Lease program and the subsequent decision to escort Lend-Lease convoys with American war ships through the western part of the Atlantic.
The Japanese attack on Pearl Harbor and the subsequent German declaration of war against the United States in December 1941 led to another phase of the Battle of the Atlantic. In Operation "Drumbeat" and subsequent operations until August 1942, a large number of Allied merchant ships were sunk by submarines off the American coast as the Americans had not prepared for submarine warfare, despite clear warnings (this was the so-called Second happy time for the German navy). The situation became so serious that military leaders feared for the whole Allied strategy. The vast American ship building capabilities and naval forces were however now brought into the war and soon more than offset any losses inflicted by the German submariners. In 1942, the submarine warfare continued on all fronts, and when German forces in the Soviet Union reached the Black Sea, a few submarines were eventually transferred there.
Hitler, fearing a British invasion of Norway, forced the leadership of the "Kriegsmarine" to transfer her big ships based in the French Atlantic port of Brest to Norway. Thus, in February 1942, the two battleships and and the heavy cruiser passed through the English Channel (Channel Dash) on their way to Norway despite British efforts to stop them. Not since the Spanish Armada in 1588 had any warships in wartime done this. It was a tactical victory for the "Kriegsmarine" and a blow to British morale, but the German fleet lost the possibility to attack allied convoys with heavy surface ships in the Atlantic (which was its wish) because of Hitler's decision.
With the German attack on the Soviet Union in June 1941 Britain started to send Arctic convoys with military goods around Norway to support their new ally. In 1942 German forces began heavily attacking these convoys, mostly with bombers and U-boats. The big ships of the "Kriegsmarine" in Norway were seldom involved in these attacks, because of the inferiority of German radar technology, and because Hitler and the leadership of the "Kriegsmarine" feared losses of these precious ships. The most effective of these attacks was the near destruction of Convoy PQ 17 in July 1942. Later in the war German attacks on these convoys were mostly reduced to U-boat activities and the mass of the allied freighters reached their destination in Soviet ports.
The Battle of the Barents Sea in December 1942 was an attempt by a German naval force to attack an Allied Arctic convoy. However, the advantage was not pressed home and they returned to base. There were serious implications: this failure infuriated Hitler, who nearly enforced a decision to scrap the surface fleet. Instead, resources were diverted to new U-boats, and the surface fleet became a lesser threat to the Allies.
After December 1943 when the had been sunk in an attack on an Arctic convoy in the Battle of North Cape by , most German surface ships in bases at the Atlantic were blockaded in, or close to, their ports as a "fleet in being", for fear of losing them in action and to tie up British naval forces. The largest of these ships, the battleship , was stationed in Norway as a threat to Allied shipping and also as a defence against a potential Allied invasion. When she was sunk, after several attempts, by British bombers in November 1944 (Operation "Catechism"), several British capital ships could be moved to the Far East.
From late 1944 until the end of the war, the surviving surface fleet of the "Kriegsmarine" (heavy cruisers: , , , , light cruisers: , , ) was heavily engaged in providing artillery support to the retreating German land forces along the Baltic coast and in ferrying civilian refugees to the western Baltic Sea parts of Germany (Mecklenburg, Schleswig-Holstein) in large rescue operations. Large parts of the population of eastern Germany fled the approaching Red Army out of fear for Soviet retaliation (mass rapes, killings and looting by Soviet troops did occur). The "Kriegsmarine" evacuated two million civilians and troops in the evacuation of East Prussia and Danzig from January to May 1945. It was during this activity that the catastrophic sinking of several large passenger ships occurred: the and the were sunk by Soviet submarines, while the was sunk by British bombers, each sinking claiming thousands of civilian lives. The "Kriegsmarine" also provided important assistance in the evacuation of the fleeing German civilians of Pomerania and Stettin in March and April 1945.
A desperate measure of the "Kriegsmarine" to fight the superior strength of the Western Allies from 1944 was the formation of the "Kleinkampfverbände" (Small Battle Units). These were special naval units with frogmen, manned torpedoes, motorboats laden with explosives and so on. The more effective of these weapons and units were the development and deployment of midget submarines like the "Molch" and "Seehund". In the last stage of the war, the "Kriegsmarine" also organized a number of divisions of infantry from its personnel.
Between 1943 and 1945, a group of U-boats known as the "Monsun" Boats ("Monsun Gruppe") operated in the Indian Ocean from Japanese bases in the occupied Dutch East Indies and Malaya. Allied convoys had not yet been organized in those waters, so initially many ships were sunk. However, this situation was soon remedied. During the later war years, the "Monsun Boats" were also used as a means of exchanging vital war supplies with Japan.
During 1943 and 1944, due to Allied anti-submarine tactics and better equipment the U-boat fleet started to suffer heavy losses. The turning point of the Battle of the Atlantic was during Black May in 1943, when the U-boat fleet started suffering heavy losses and the number of Allied ships sunk started to decrease. Radar, longer range air cover, Sonar, improved tactics and new weapons all contributed. German technical developments, such as the "Schnorchel", attempted to counter these. Near the end of the war a small number of the new "Elektroboot" U-boats (XXI and XXIII) became operational, the first submarines designed to operate submerged at all times. The "Elektroboote" had the potential to negate the Allied technological and tactical advantage, although they were deployed too late to see combat in the war.
War crimes.
After the German conquest on 29 June 1941, the naval base at Liepāja, Latvia came under the command of the "Kriegsmarine". On 1 July 1941, town commandant "Korvettenkapitän" Stein ordered that ten hostages be shot for every act of sabotage, and further put civilians in the zone of targeting by declaring that Red Army soldiers were hiding among them in civilian attire. On 5 July 1941 "Korvettenkapitän" Brückner, who had taken over for Stein, issued a set of anti-Jewish regulations in the local newspaper, "Kurzemes Vārds". Summarized these were as follows:
On 16 July 1941, "Fregattenkapitän" Dr. Hans Kawelmacher was appointed the German naval commandant in Liepāja. On 22 July, Kawelmacher sent a telegram to the German Navy's Baltic Command in Kiel, which stated that he wanted 100 SS and fifty "Schutzpolizei" ("protective police") men sent to Liepāja for "quick implementation Jewish problem". Kawelmacher hoped to accelerate killings complaining: "Here about 8,000 Jews... with present SS-personnel, this would take one year, which is untenable for pacification of Liepāja." Kawelmacher on 27 July 1941: "Jewish problem Libau largely solved by execution of about 1,100 male Jews by Riga SS commando on 24 and 25.7."
In 1945 U-boat Commander Heinz-Wilhelm Eck of U-852 was executed with two of his crewman for shooting at survivors; likewise German submarine U-247 was also involved in shooting at sunken ship survivors-although they were not tried as they were lost at sea.
Post-war division.
After the war, the German surface ships that remained afloat (only the cruisers and , and a dozen destroyers were operational) were divided among the victors. The officer in charge of the "Kriegsmarine" division and appropriation, was Lt. Gerald Ivers of the US Navy, who used mathematical analysis to assign the remaining ships to their respective nations. The US used the heavy cruiser in nuclear testing at Bikini Atoll in 1946 as target ship. Some (like the unfinished aircraft carrier ) were used for target practice with conventional weapons, while others (mostly destroyers and torpedo boats) were put into the service of Allied navies that lacked surface ships after the war. The training barque SSS "Horst Wessel" was recommissioned USCGC "Eagle" and remains in active service, assigned to the United States Coast Guard Academy. The British, French and Soviet navies received the destroyers, and some torpedo boats went to the Danish and Norwegian navies. For the purpose of mine clearing, the Royal Navy employed German crews and minesweepers from June 1945 to January 1948, organized in the German Mine Sweeping Administration, the GMSA, which consisted of 27,000 members of the former "Kriegsmarine" and 300 vessels.
The destroyers and the Soviet share light cruiser were all retired by the end of the 1950s, but five escort destroyers were returned from the French to the new West German navy in the 1950s and three 1945 scuttled type XXI and XXIII U-boats were raised by West Germany and integrated into their new navy. In 1956, with West Germany's accession to NATO, a new navy was established and was referred to as the "Bundesmarine" (Federal Navy). Some "Kriegsmarine" commanders like Erich Topp and Otto Kretschmer went on to serve in the "Bundesmarine". In East Germany the "Volksmarine" (People's Navy) was established in 1956. With the reunification of Germany in 1990, it was decided to simply use the name "Deutsche Marine" (German Navy).
Ships.
By the start of World War II, much of the "Kriegsmarine" were modern ships: fast, well-armed and well-armoured. This had been achieved by concealment but also by deliberately flouting World War I peace terms and those of various naval treaties. However, the war started with the German Navy still at a distinct disadvantage in terms of sheer size with what were expected to be its primary adversaries – the navies of France and Great Britain. Although a major re-armament of the navy (Plan Z) was planned, and initially begun, the start of the war in 1939 meant that the vast amounts of material required for the project were diverted to other areas. The sheer disparity in size when compared to the other European powers navies prompted German naval commander in chief Grand Admiral Erich Raeder to write of his own navy once the war began "The surface forces can do no more than show that they know how to die gallantly." A number of captured ships from occupied countries were added to the German fleet as the war progressed. Though six major units of the "Kriegsmarine" were sunk during the war (both "Bismarck"-class battleships and both "Scharnhorst"-class battleships, as well as two heavy cruisers), there were still many ships afloat (including four heavy cruisers and four light cruisers) as late as March 1945.
Some ship types do not fit clearly into the commonly used ship classifications. Where there is argument, this has been noted.
Surface ships.
The main combat ships of the Kriegsmarine (excluding U-boats):
Aircraft carriers.
Construction of the was started in 1936 and construction of an unnamed sister ship was started two years later in 1938, but neither ship was completed. In 1942 conversion of three German passenger ships ("Europa", "Potsdam", "Gneisenau") and two unfinished cruisers—the captured French light cruiser and the German heavy cruiser — to auxiliary carriers was begun. In November 1942 the conversion of the passenger ships was stopped because these ships were now seen as too slow for operations with the fleet. But conversion of one of these ships, the "Potsdam", to a training carrier was begun instead. In February 1943 all the work on carriers was halted because of the German failure during the Battle of the Barents Sea which convinced Hitler that big warships were useless.
All engineering of the aircraft carriers like catapults, arresting gears and so on were tested and developed at the "Erprobungsstelle See" Travemünde (Experimental Place Sea in Travemünde) including the airplanes for the aircraft carriers, the Fieseler Fi 167 ship-borne biplane torpedo and reconnaissance bomber and the navalized versions of two key early war "Luftwaffe" aircraft: the Messerschmitt Bf 109T fighter and Junkers Ju 87C "Stuka" dive bomber.
Battleships.
The "Kriegsmarine" completed four battleships during its existence. The first pair were the 11-inch gun , consisting of the and , which participated in the invasion of Norway (Operation Weserübung) in 1940, and then in commerce raiding until the "Gneisenau" was heavily damaged by a British air raid in 1942 and the "Scharnhorst" was sunk in the Battle of the North Cape in late 1943. The second pair were the 15-inch gun , consisting of the and . The "Bismarck" was sunk on her first sortie into the Atlantic in 1941 (Operation "Rheinübung") although she did sink the battlecruiser "Hood" and severely damage the battleship "Prince of Wales", while the "Tirpitz" was based in Norwegian ports during most of the war as a fleet in being, tying up Allied naval forces, and subject to a number of attacks by British aircraft and submarines. More battleships were planned (the H-class), but construction was abandoned in September 1939.
Pocket battleships ("Panzerschiffe").
The "Pocket battleships" were the (renamed "Lützow"), , and . Modern commentators favour classifying these as "heavy cruisers" and the "Kriegsmarine" itself reclassified these ships as such ("Schwere Kreuzer") in 1940. In German language usage these three ships were designed and built as "armoured ships" (Panzerschiffe) – "pocket battleship" is an English label.
The "Graf Spee" was scuttled by her own crew in the Battle of the River Plate, in the Rio de la Plata estuary in December 1939. "Admiral Scheer" was bombed on 9 April 1945 in port at Kiel and badly damaged, essentially beyond repair, and rolled over at her moorings. After the war that part of the harbor was filled in with rubble and the hulk buried. "Lützow" (ex-"Deutschland") was bombed 16 April 1945 in the Baltic off Schwinemünde just west of Stettin, and settled on the shallow bottom. With the Soviet Army advancing across the Oder, the ship was destroyed in place to prevent the Soviets capturing anything useful. The wreck was dismantled and scrapped in 1948–1949.
Pre-dreadnought battleships.
The World War I era Pre-dreadnought battleships and were used mainly as training ships, although they also participated in several military operations, with the latter bearing the distinction of firing the opening shots of WWII. and were converted into radio-guided target ships in 1928 and 1930 respectively. was decommissioned in 1931 and struck from the naval register in 1936. Plans to convert her into a radio-controlled target ship for aircraft was canceled because of the outbreak of war in 1939.
Battlecruisers.
Three s were ordered in 1939, but with the start of the war the same year there were not enough resources to build the ships.
Heavy cruisers.
, , and 
Never completed: , 
Light cruisers.
The term "light cruiser" is a shortening of the phrase "light armoured cruiser." Light cruisers were defined under the Washington Naval Treaty by gun caliber. Light cruiser describes a small ship that was armoured in the same way as an armoured cruiser. In other words, like standard cruisers, light cruisers possessed a protective belt and a protective deck. Prior to this, smaller cruisers tended to be of the protected cruiser model and possessed only an armoured deck. The Kriegsmarine light cruisers were as follows:
Never completed: three s
Never Completed: KH-1 and KH-2 (Kreuzer (cruiser) Holland 1 and 2). Captured in the Netherlands 1940. Both being on the stocks and building continued for the "Kriegsmarine".
In addition, the former "Kaiserliche Marine" light cruiser was captured by Germans on 11 September 1943 after the capitulation of Italy. She was pressed into "Kriegsmarine" service for a brief time before being destroyed by British MTBs.
Auxiliary cruisers.
During the war, some merchant ships were converted into "auxiliary cruisers" and nine were used as commerce raiders sailing under false flags to avoid detection, and operated in all oceans with considerable effect. The German designation for the ships was 'Handelstörkreuzer' thus the HSK serial assigned. Each had as well an administrative label more commonly used, e.g. Schiff 16 = Atlantis, Schiff 41 = Kormoran, etc. The auxiliary cruisers were:
Destroyers.
Although the German World War II destroyer ("Zerstörer") fleet was modern and the ships were larger than conventional destroyers of other navies, they had problems. Early classes were unstable, wet in heavy weather, suffered from engine problems and had short range. Some problems were solved with the evolution of later designs, but further developments were curtailed by the war and, ultimately, by Germany's defeat. In the first year of World War II, they were used mainly to sow offensive minefields in shipping lanes close to the British coast.
Torpedo boats.
These vessels evolved through the 1930s from small vessels, relying almost entirely on torpedoes, to what were effectively small destroyers with mines, torpedoes and guns. Two classes of fleet torpedo boats were planned, but not built, in the 1940s.
E-boats ("Schnellboote").
The E-boats were fast attack craft with torpedo tubes. Over 200 boats of this type were built for the "Kriegsmarine".
Miscellaneous.
Thousands of smaller warships and auxiliaries served in the Kriegsmarine, including minelayers, minesweepers, mine transports, netlayers, floating AA and torpedo batteries, command ships, decoy ships (small merchantmen with hidden weaponry), gunboats, monitors, escorts, patrol boats, sub-chasers, landing craft, landing support ships, training ships, test ships, torpedo recovery boats, dispatch boats, aviso, fishery protection ships, survey ships, harbor defense boats, target ships and their radio control vessels, motor explosive boats, weather ships, tankers, colliers, tenders, supply ships, tugs, barges, icebreakers, hospital and accommodation ships, floating cranes and docks, and many others. The Kriegsmarine employed hundreds of auxiliary "Vorpostenboote" during the war, mostly civilian ships that were drafted and fitted with military equipment, for use in coastal operations.
Submarines (U-boat).
At the outbreak of war, the "Kriegsmarine" had a relatively small fleet of 57 submarines (U-boats). This was increased steadily until mid-1943, when losses from Allied counter-measures matched the new vessels launched.
The principal types were the Type IX, a long range type used in the western and southern Atlantic, Indian and Pacific Oceans; and the Type VII, the most numerous type, used principally in the north Atlantic. Type X was a small class of minelayers and Type XIV was a specialized type used to support distant U-boat operations – the ""Milchkuh"" (Milkcow).
Types XXI and XXIII, the ""Elektroboot"", would have negated much of the Allied anti-submarine tactics and technology, but only a few of this new type of U-boat became ready for combat at the end of the war. Post-war, they became the prototypes for modern submarines, in particular, the Soviet .
During World War II, about 60% of all U-boats commissioned were lost in action; 28,000 of the 40,000 U-boat crewmen were killed during the war and 8,000 were captured. The remaining U-boats were either surrendered to the Allies or scuttled by their own crews at the end of the war.
Captured ships.
The military campaigns in Europe yielded a large number of captured vessels, many of which were under construction. Nations represented included Austria (riverine craft), Czechoslovakia (riverine craft), Poland, Norway, Denmark, the Netherlands, Belgium, France, Yugoslavia, Greece, Soviet Union, United Kingdom, United States (several landing craft) and Italy (after the armistice). Few of the incomplete ships of destroyer size or above were completed, but many smaller warships and auxiliaries were completed and commissioned into Kriegsmarine during the war. Additionally many captured or confisticated foreign civilian ships (merchantmen, fishing boats, tugboats etc.) were converted into auxiliary warships or support ships.
Major enemy warships sunk or destroyed.
The first warship sunk in World War II was the destroyer of the Polish Navy by Junkers Ju 87 dive bombers of the carrier air group of aircraft carrier on 3 September 1939. This carrier air group (Trägergeschwader 186) was part of the Luftwaffe but at that time under command of the Kriegsmarine.
Air and land units.
Air units.
The Luftwaffe had a near-complete monopoly on all German military aviation, including naval aviation, a major source of ongoing interservice rivalry with the "Kriegsmarine". Catapult-launched spotter planes like Arado Ar 196 twin-float seaplanes were manned by the so-called "Bordfliegergruppen" ("shipboard flying group"). In addition, "Trägergeschwader 186" (Carrier Air Wing 186) operated two "Gruppen" ("Trägergruppe I/186" and "Trägergruppe II/186") equipped with navalized Messerschmitt Bf 109T and Junkers Ju 87C Stuka; these units were intended to serve aboard the aircraft carrier which was never completed, yet provided the Kriegsmarine with some air-power from bases on land. Furthermore, five coastal groups ("Küstenfliegergruppen") with reconnaissance aircraft, torpedo bombers, "Minensuch" aerial minesweepers and air-sea rescue seaplanes supported the "Kriegsmarine", although with lesser resources as the war progressed.
Coastal artillery, flak and radar units.
The coastal batteries of the "Kriegsmarine" were stationed on the German coasts. With the conquering and occupation of other countries coastal artillery was stationed along the coasts of these countries, especially in France and Norway as part of the Atlantic Wall.
Naval bases were protected by Flak-batteries of the "Kriegsmarine" against enemy air raids. The "Kriegsmarine" also manned the Seetakt sea radars on the coasts.
Marines.
At the beginning of World War II, on 1 September 1939, the "Marine Stoßtrupp Kompanie" (Marine Attack Troop Company) landed in Danzig from the old battleship for conquering a Polish bastion. A reinforced platoon of the "Marine Stoßtrupp Kompanie" landed with soldiers of the German Army from destroyers on 9 April 1940 in Narvik. In June 1940 the "Marine Stoßtrupp Abteilung" (Marine Attack Troop Battalion) was flown in from France to the Channel Islands to occupy this British territory.
In September 1944 amphibious units unsuccessfully tried to capture the strategic island Suursaari in the Gulf of Finland from Germany's former ally Finland (Operation Tanne Ost).
With the Invasion of Normandy in June 1944 and the Soviet advance from the summer of 1944 the Kriegsmarine started to form regiments and divisions for the battles on land with superfluous personnel. With the loss of naval bases because of the Allied advance more and more navy personnel were available for the ground troops of the "Kriegsmarine". About 40 regiments were raised and from January 1945 on six divisions. Half of the regiments were absorbed by the divisions.
Personnel.
Comparative ranks (during World War II).
Officer candidate ranks.
Officer candidates (with PO-ranks) where as follows:
Uniforms.
Many different types of uniforms were worn by the Kriegsmarine; here is a list of the main ones:

</doc>
<doc id="17387" url="https://en.wikipedia.org/wiki?curid=17387" title="Knights of Labor">
Knights of Labor

The Knights of Labor (K of L), officially Noble and Holy Order of the Knights of Labor, was the largest and one of the most important American labor organizations of the 1880s. Its most important leader was Terence V. Powderly. The Knights promoted the social and cultural uplift of the workingman, rejected socialism and anarchism, demanded the eight-hour day, and promoted the producers ethic of republicanism. In some cases it acted as a labor union, negotiating with employers, but it was never well organized, and after a rapid expansion in the mid-1880s, it suddenly lost its new members and became a small operation again.
It was founded by Uriah Stephens on December 28, 1869, reached 28,000 members in 1880, then jumped to 100,000 in 1884. By 1886 20% of all workers were affiliated with the KOL, ballooning to nearly 800,000 members. Its frail organizational structure could not cope as it was battered by charges of failure and violence and calumnies of the association with the Haymarket Square riot. Most members abandoned the movement in 1886-87, leaving at most 100,000 in 1890. Many of them chose to join groups that helped to identify their specific need, instead of the KOL that addressed many different types of issues. Furthermore, the Panic of 1893 terminated the Knights of Labor's importance. Remnants of the Knights of Labor continued in existence until 1949, when the group's last 50-member local dropped its affiliation.
Organizational history.
Origins.
In 1870, Daniel Spahr and his friend, Sam Catri, the lead member of the Philadelphia tailors' union, headed by Uriah Smith Stephens, established a secret union under the name the Noble Order of the Knights of Labor. The collapse of the National Labor Union in 1873 left a vacuum for workers looking for organization. The Knights became better organized with a national vision when they replaced Stephens with Terence V. Powderly. The body became popular with Pennsylvania coal miners during the economic depression of the mid-1870s, then it grew rapidly. The KOL was a diverse industrial union open to all workers. The leaders felt that it was best to have a versatile population in order to get points of view from all aspects. Its members included low skilled workers, railroad workers, immigrants, and steel workers.
As membership expanded, the Knights began to function more as a labor union and less like a fraternal organization. Local assemblies began not only to emphasize cooperative enterprises, but to initiate strikes to win concessions from employers. Powderly opposed strikes as a "relic of barbarism," but the size and the diversity of the Knights afforded local assemblies a great deal of autonomy.
In 1882, the Knights ended their membership rituals and removed the words "Noble Order" from their name. This was to mollify the concerns of Catholic members and the bishops who wanted to avoid any resemblance to freemasonry.
Though initially averse to strikes as a method to advance their goals, the Knights aided various strikes and boycotts. The Wabash Railroad strike in 1885 was also a significant success, as Powderly finally supported what became a successful strike on Jay Gould's Wabash Line. Gould met with Powderly and agreed to call off his campaign against the Knights of Labor, which had caused the turmoil originally. These positive developments gave momentum and a surge of members, so by 1886, the Knights had over 700,000 members.
The Knights' primary demand was for an eight-hour day; they also called for legislation to end child and convict labor, as well as a graduated income tax. They were eager supporters of cooperatives. The only woman to hold office in the Knights of Labor, Leonora Barry worked as an investigator and described the horrific conditions in factories, conditions tantamount to the abuse of women and children. These reports made Barry the first person to collect national statistics on the American working woman.
Powderly and the Organization tried to avoid divisive political issues, but in the early 1880s, many Knights were had become followers of Henry George's radical ideology known now as georgism. In 1883, Powderly officially recommended George's book and announced his support of "single tax" on land values. During the New York mayoral election of 1886, Powderly was able to successfully push the organization in an even more Georgist direction.
The Knights of Labor was an organization that helped to join together many different types of people from all different walks of life; for example Catholic and Protestant Irish-born workers. The KOL was appealing to them because they worked very closely with the Irish Land League. The Knights of Labor had a mixed history of inclusiveness and exclusiveness, accepting women and blacks (after 1878) and their employers as members, and advocating the admission of blacks into local assemblies, but tolerating the segregation of assemblies in the South. Bankers, doctors, lawyers, stockholders, and liquor manufacturers were excluded because they were considered unproductive members of society. Asians were also excluded, and in November 1885, a branch of the Knights in Tacoma, Washington worked to expel the city's Chinese, who amounted to nearly a tenth of the overall city population at the time. The Union Pacific Railroad came into conflict with the Knights. When the Knights in Wyoming refuse extra work in Wyoming in 1885, the railroad hired Chinese workers. The result was the Rock Springs massacre, that killed scores of Chinese, and drove all the rest out of Wyoming. About 50 African-American sugar-cane laborers organized by the Knights went on strike and were murdered by strikebreaking thugs in the 1887 Thibodaux massacre in Louisiana. The Knights strongly supported the Chinese Exclusion Act of 1882 and the Contract Labor Law of 1885, as did many other labor groups.
Decline.
The Knights of Labor attracted many Catholics, who were a large part of the membership, perhaps a majority. Powderly was a Catholic. However, the Knights's use of secrecy, similar to the Masons, during its early years concerned many bishops. The Knights used secrecy and deception to help prevent employers from firing members. After the Archbishop of Quebec condemned the Knights in 1884, twelve American archbishops voted 10 to 2 against doing likewise in the United States. Furthermore, Cardinals James Gibbons and John Ireland defended the Knights. Gibbons went to the Vatican to talk to the hierarchy.
Legacy.
Though often overlooked, the Knights of Labor contributed to the tradition of labor protest songs in America. The Knights frequently included music in their regular meetings, and encouraged local members to write and perform their work. In Chicago, James and Emily Talmadge, printers and supporters of the Knights of Labor, published the songbook "Labor Songs Dedicated to the Knights of Labor" (1885). The song "Hold the Fort" "Storm the Fort", a Knights of Labor pro-labor revision of the hymn by the same name, became the most popular labor song prior to Ralph Chaplin's IWW anthem "Solidarity Forever". Pete Seeger often performed this song and it appears on a number of his recordings. Songwriter and labor singer Bucky Halker includes the Talmadge version, entitled "Labor's Battle Song," on his CD "Don't Want Your Millions" (Revolting Records 2000). Halker also draws heavily on the Knights songs and poems in his book on labor song and poetry, "For Democracy, Workers and God: Labor Song-Poems and Labor Protest, 1865-1895" (University of Illinois Press, 1991).

</doc>
<doc id="17390" url="https://en.wikipedia.org/wiki?curid=17390" title="Kryptonite">
Kryptonite

Kryptonite is a material from the superman fictional universe, specifically the ore form of a radioactive element from Superman's home planet of Krypton. First mentioned in "The Adventures of Superman" radio show in June 1943, the material has been featured in a variety of forms and colors (each with its own effect) in DC Comics publications and other media, including feature films, television series, and novelty items such as toys and trading card sets.
The established premise is that Superman and other Kryptonian characters are susceptible to its radiation, which created usage of the term in popular culture as a reference to an individual's perceived weakness, irrespective of its nature.
Origin.
An unpublished 1940 story titled "The K-Metal from Krypton," which Superman creator Jerry Siegel himself wrote, featured a prototype of kryptonite, a mineral from the planet Krypton that drained Superman of his strength while giving humans superhuman powers.
A mineral actually named "kryptonite" was introduced in the story arc "The Meteor from Krypton" in June of 1943 on "The Adventures of Superman" radio series. Since radio shows had to be performed by real people, when Clayton "Bud" Collyer, the actor who played Superman and Clark Kent, wanted permission to take a vacation from the series, Superman was placed in a kryptonite trap, and a stand-in groaned with pain for several episodes until Collyer came back from his vacation.
In 1949, kryptonite was incorporated into the comic mythos with issue #61 of "Superman." In August of 1993, pioneering female editor Dorothy Woolfolk stated in an interview with Florida newspaper "Today" that she had found Superman's invulnerability dull, and that DC's flagship hero might be more interesting with an Achilles' heel such as adverse reactions to a fragment of his home planet.
Kryptonite gradually appeared more frequently, causing science fiction writer Larry Niven to theorize in tongue-in-cheek fashion that Krypton was in fact a Dyson sphere, and that this was the underlying reason for so much of the mineral finding its way to Earth courtesy of meteor showers. In an effort to reduce the use of kryptonite in storylines, all known forms on Earth were transmuted into "k-iron" in a 1971 story arc titled "The Sandman Saga".
Forms, colors and their effects.
Various forms of the fictional mineral have been created in the "Superman" publications:
Other varieties of the mineral have appeared but have been revealed to be hoaxes, such as yellow ("Action Comics" #277, June 1961), "kryptonite plus" ("Superman's Pal, Jimmy Olsen" #126 Jan. 1970) and "blood" ("52" #13, Aug. 2006).
In other media.
Serials.
Columbia Pictures produced two 15-part motion picture serials that used kryptonite as a plot device: "Superman" (1948) and "Atom Man vs. Superman" (1950).
Music.
Songs:

</doc>
