<doc id="15036" url="https://en.wikipedia.org/wiki?curid=15036" title="Information security">
Information security

Information security, sometimes shortened to InfoSec, is the practice of defending information from unauthorized access, use, disclosure, disruption, modification, perusal, inspection, recording or destruction. It is a general term that can be used regardless of the form the data may take (e.g. electronic, physical).
Overview.
Threats.
Computer system threats come in many different forms. Some of the most common threats today are software attacks, theft of intellectual property, identity theft, theft of equipment or information, sabotage, and information extortion. Most people have experienced software attacks of some sort. Viruses, worms, phishing attacks, and trojan horses are a few common examples of software attacks. The theft of intellectual property has also been an extensive issue for many businesses in the IT field. Intellectual property is the ownership of property usually consisting of some form of protection. Theft of software is probably the most common in IT businesses today. Identity theft is the attempt to act as someone else usually to obtain that person's personal information or to take advantage of their access to vital information. Theft of equipment or information is becoming more prevalent today due to the fact that most devices today are mobile. Cell phones are prone to theft and have also become far more desirable as the amount of data capacity increases. Sabotage usually consists of the destruction of an organization′s website in an attempt to cause loss of confidence to its customers. Information extortion consists of theft of a company′s property or information as an attempt to receive a payment in exchange for returning the information or property back to its owner. There are many ways to help protect yourself from some of these attacks but one of the most functional precautions is user carefulness.
Governments, military, corporations, financial institutions, hospitals and private businesses amass a great deal of confidential information about their employees, customers, products, research and financial status. Most of this information is now collected, processed and stored on electronic computers and transmitted across networks to other computers.
Should confidential information about a business' customers or finances or new product line fall into the hands of a competitor or a black hat hacker, a business and its customers could suffer widespread, irreparable financial loss, as well as damage to the company's reputation. Protecting confidential information is a business requirement and in many cases also an ethical and legal requirement. Hence a key concern for organizations today is to derive the optimal information security investment. The renowned Gordon-Loeb Model actually provides a powerful mathematical economic approach for addressing this critical concern.
For the individual, information security has a significant effect on privacy, which is viewed very differently in different cultures.
The field of information security has grown and evolved significantly in recent years. There are many ways of gaining entry into the field as a career. It offers many areas for specialization including securing network(s) and allied infrastructure, securing applications and databases, security testing, information systems auditing, business continuity planning and digital forensics.
History.
Since the early days of communication, diplomats and military commanders understood that it was necessary to provide some mechanism to protect the confidentiality of correspondence and to have some means of detecting tampering. Julius Caesar is credited with the invention of the Caesar cipher c. 50 B.C., which was created in order to prevent his secret messages from being read should a message fall into the wrong hands, but for the most part protection was achieved through the application of procedural handling controls. Sensitive information was marked up to indicate that it should be protected and transported by trusted persons, guarded and stored in a secure environment or strong box. As postal services expanded, governments created official organizations to intercept, decipher, read and reseal letters (e.g. the UK Secret Office and Deciphering Branch in 1653).
In the mid-19th century more complex classification systems were developed to allow governments to manage their information according to the degree of sensitivity. The British Government codified this, to some extent, with the publication of the Official Secrets Act in 1889. By the time of the First World War, multi-tier classification systems were used to communicate information to and from various fronts, which encouraged greater use of code making and breaking sections in diplomatic and military headquarters. In the United Kingdom this led to the creation of the Government Code and Cypher School in 1919. Encoding became more sophisticated between the wars as machines were employed to scramble and unscramble information. The volume of information shared by the Allied countries during the Second World War necessitated formal alignment of classification systems and procedural controls. An arcane range of markings evolved to indicate who could handle documents (usually officers rather than men) and where they should be stored as increasingly complex safes and storage facilities were developed. Procedures evolved to ensure documents were destroyed properly and it was the failure to follow these procedures which led to some of the greatest intelligence coups of the war (e.g. U-570).
The end of the 20th century and early years of the 21st century saw rapid advancements in telecommunications, computing hardware and software, and data encryption. The availability of smaller, more powerful and less expensive computing equipment made electronic data processing within the reach of small business and the home user. These computers quickly became interconnected through the Internet.
The rapid growth and widespread use of electronic data processing and electronic business conducted through the Internet, along with numerous occurrences of international terrorism, fueled the need for better methods of protecting the computers and the information they store, process and transmit. The academic disciplines of computer security and information assurance emerged along with numerous professional organizations – all sharing the common goals of ensuring the security and reliability of information systems.
Definitions.
The definitions of InfoSec suggested in different sources are summarized below (adopted from).
1. "Preservation of confidentiality, integrity and availability of information. Note: In addition, other properties, such as authenticity, accountability, non-repudiation and reliability can also be involved." (ISO/IEC 27000:2009)
2. "The protection of information and information systems from unauthorized access, use, disclosure, disruption, modification, or destruction in order to provide confidentiality, integrity, and availability." (CNSS, 2010)
3. "Ensures that only authorized users (confidentiality) have access to accurate and complete information (integrity) when required (availability)." (ISACA, 2008)
4. "Information Security is the process of protecting the intellectual property of an organisation." (Pipkin, 2000)
5. "...information security is a risk management discipline, whose job is to manage the cost of information risk to the business." (McDermott and Geer, 2001)
6. "A well-informed sense of assurance that information risks and controls are in balance." (Anderson, J., 2003)
7. "Information security is the protection of information and minimises the risk of exposing information to unauthorised parties." (Venter and Eloff, 2003)
8. "Information Security is a multidisciplinary area of study and professional activity which is concerned with the development and implementation of security mechanisms of all available types (technical, organisational, human-oriented and legal) in order to keep information in all its locations (within and outside the organisation's perimeter) and, consequently, information systems, where information is created, processed, stored, transmitted and destroyed, free from threats.
Threats to information and information systems may be categorised and a corresponding security goal may be defined for each category of threats. A set of security goals, identified as a result of a threat analysis, should be revised periodically to ensure its adequacy and conformance with the evolving environment. The currently relevant set of security goals may include: "confidentiality, integrity, availability, privacy, authenticity & trustworthiness, non-repudiation, accountability and auditability."" (Cherdantseva and Hilton, 2013)
Profession.
Information security is a stable and growing profession. Information security professionals are very stable in their employment; more than 80 percent had no change in employer or employment in the past year, and the number of professionals is projected to continuously grow more than 11 percent annually from 2014 to 2019.
Basic principles.
Key concepts.
The CIA triad of confidentiality, integrity, and availability is at the heart of information security. (The members of the classic InfoSec triad — confidentiality, integrity and availability — are interchangeably referred to in the literature as security attributes, properties, security goals, fundamental aspects, information criteria, critical information characteristics and basic building blocks.) There is continuous debate about extending this classic trio. Other principles such as Accountability have sometimes been proposed for addition – it has been pointed out that issues such as Non-Repudiation do not fit well within the three core concepts.
In 1992 and revised in 2002, the OECD's Guidelines for the Security of Information Systems and Networks proposed the nine generally accepted principles: Awareness, Responsibility, Response, Ethics, Democracy, Risk Assessment, Security Design and Implementation, Security Management, and Reassessment. Building upon those, in 2004 the NIST's Engineering Principles for Information Technology Security proposed 33 principles. From each of these derived guidelines and practices.
In 2002, Donn Parker proposed an alternative model for the classic CIA triad that he called the six atomic elements of information. The elements are confidentiality, possession, integrity, authenticity, availability, and utility. The merits of the Parkerian hexad are a subject of debate amongst security professionals.
In 2011, The Open Group published the information security management standard O-ISM3. This standard proposed an Operational definition of the key concepts of security, with elements called "security objectives", related to Access Control (9), Availability (3), Data Quality (1),Compliance and Technical (4). This model is not currently widely adopted.
In 2013, based on a thorough analysis of Information Assurance and Security (IAS) literature, the IAS-octave was proposed as an extension of the CIA-triad. The IAS-octave includes Confidentiality, Integrity, Availability, Accountability, Auditability, Authenticity/Trustworthiness, Non-repudiation and Privacy. The completeness and accuracy of the IAS-octave was evaluated via a series of interviews with IAS academics and experts. The IAS-octave is one of the dimensions of a Reference Model of Information Assurance and Security (RMIAS), which summarizes the IAS knowledge in one all-encompassing model.
Confidentiality.
In information security, confidentiality "is the property, that information is not made available or disclosed to unauthorized individuals, entities, or processes" (Except ISO27000).
Integrity.
In information security, data integrity means maintaining and assuring the accuracy and completeness of data over its entire life-cycle. This means that data cannot be modified in an unauthorized or undetected manner. This is not the same thing as referential integrity in databases, although it can be viewed as a special case of consistency as understood in the classic ACID model of transaction processing. Information security systems typically provide message integrity in addition to data confidentiality.
Availability.
For any information system to serve its purpose, the information must be available when it is needed. This means that the computing systems used to store and process the information, the security controls used to protect it, and the communication channels used to access it must be functioning correctly. High availability systems aim to remain available at all times, preventing service disruptions due to power outages, hardware failures, and system upgrades. Ensuring availability also involves preventing denial-of-service attacks, such as a flood of incoming messages to the target system essentially forcing it to shut down.
Non-repudiation.
In law, non-repudiation implies one's intention to fulfill their obligations to a contract. It also implies that one party of a transaction cannot deny having received a transaction nor can the other party deny having sent a transaction. Note: This is also regarded as part of Integrity.
It is important to note that while technology such as cryptographic systems can assist in non-repudiation efforts, the concept is at its core a legal concept transcending the realm of technology. It is not, for instance, sufficient to show that the message matches a digital signature signed with the sender's private key, and thus only the sender could have sent the message and nobody else could have altered it in transit. The alleged sender could in return demonstrate that the digital signature algorithm is vulnerable or flawed, or allege or prove that his signing key has been compromised. The fault for these violations may or may not lie with the sender himself, and such assertions may or may not relieve the sender of liability, but the assertion would invalidate the claim that the signature necessarily proves authenticity and integrity and thus prevents repudiation.
Risk management.
The "Certified Information Systems Auditor (CISA) Review Manual 2006" provides the following definition of risk management: "Risk management is the process of identifying vulnerabilities and threats to the information resources used by an organization in achieving business objectives, and deciding what countermeasures, if any, to take in reducing risk to an acceptable level, based on the value of the information resource to the organization."
There are two things in this definition that may need some clarification. First, the "process" of risk management is an ongoing, iterative process. It must be repeated indefinitely. The business environment is constantly changing and new threats and vulnerabilities emerge every day. Second, the choice of countermeasures (controls) used to manage risks must strike a balance between productivity, cost, effectiveness of the countermeasure, and the value of the informational asset being protected.
Risk analysis and risk evaluation processes have their limitations since, when security incidents occur, they emerge in a context, and their rarity and even their uniqueness give rise to unpredictable threats. The analysis of these phenomena which are characterized by breakdowns, surprises and side-effects, requires a theoretical approach which is able to examine and interpret subjectively the detail of each incident.
Risk is the likelihood that something bad will happen that causes harm to an informational asset (or the loss of the asset). A vulnerability is a weakness that could be used to endanger or cause harm to an informational asset. A threat is anything (man-made or act of nature) that has the potential to cause harm.
The likelihood that a threat will use a vulnerability to cause harm creates a risk. When a threat does use a vulnerability to inflict harm, it has an impact. In the context of information security, the impact is a loss of availability, integrity, and confidentiality, and possibly other losses (lost income, loss of life, loss of real property). It should be pointed out that it is not possible to identify all risks, nor is it possible to eliminate all risk. The remaining risk is called "residual risk".
A risk assessment is carried out by a team of people who have knowledge of specific areas of the business. Membership of the team may vary over time as different parts of the business are assessed. The assessment may use a subjective qualitative analysis based on informed opinion, or where reliable dollar figures and historical information is available, the analysis may use quantitative analysis.
The research has shown that the most vulnerable point in most information systems is the human user, operator, designer, or other human. The ISO/IEC 27002:2005 Code of practice for information security management recommends the following be examined during a risk assessment:
In broad terms, the risk management process consists of:
For any given risk, management can choose to accept the risk based upon the relative low value of the asset, the relative low frequency of occurrence, and the relative low impact on the business. Or, leadership may choose to mitigate the risk by selecting and implementing appropriate control measures to reduce the risk. In some cases, the risk can be transferred to another business by buying insurance or outsourcing to another business. The reality of some risks may be disputed. In such cases leadership may choose to deny the risk.
Controls.
Selecting proper controls and implementing those will initially help an organization to bring down risk to acceptable levels. Control selection should follow and should be based on the risk assessment. Controls can vary in nature but fundamentally they are ways of protecting the confidentiality, integrity or availability of information. has defined 133 controls in different areas, but this is not exhaustive. Organizations can implement additional controls according to requirement of the organization. has cut down the number of controls to 113. From 08.11.2013 the technical standard of information security in place is: ABNT NBR ISO/IEC 27002:2013.
Administrative.
Administrative controls (also called procedural controls) consist of approved written policies, procedures, standards and guidelines. Administrative controls form the framework for running the business and managing people. They inform people on how the business is to be run and how day-to-day operations are to be conducted. Laws and regulations created by government bodies are also a type of administrative control because they inform the business. Some industry sectors have policies, procedures, standards and guidelines that must be followed – the Payment Card Industry Data Security Standard (PCI DSS) required by Visa and MasterCard is such an example. Other examples of administrative controls include the corporate security policy, password policy, hiring policies, and disciplinary policies.
Administrative controls form the basis for the selection and implementation of logical and physical controls. Logical and physical controls are manifestations of administrative controls. Administrative controls are of paramount importance.
Logical.
Logical controls (also called technical controls) use software and data to monitor and control access to information and computing systems. For example: passwords, network and host-based firewalls, network intrusion detection systems, access control lists, and data encryption are
logical controls.
An important logical control that is frequently overlooked is the principle of least privilege. The principle of least privilege requires that an individual, program or system process is not granted any more access privileges than are necessary to perform the task. A blatant example of the failure to adhere to the principle of least privilege is logging into Windows as user Administrator to read email and surf the web. Violations of this principle can also occur when an individual collects additional access privileges over time. This happens when employees' job duties change, or they are promoted to a new position, or they transfer to another department. The access privileges required by their new duties are frequently added onto their already existing access privileges which may no longer be necessary or appropriate.
Physical.
Physical controls monitor and control the environment of the work place and computing facilities. They also monitor and control access to and from such facilities. For example: doors, locks, heating and air conditioning, smoke and fire alarms, fire suppression systems, cameras, barricades, fencing, security guards, cable locks, etc. Separating the network and workplace into functional areas are also physical controls.
An important physical control that is frequently overlooked is the separation of duties. Separation of duties ensures that an individual can not complete a critical task by himself. For example: an employee who submits a request for reimbursement should not also be able to authorize payment or print the check. An applications programmer should not also be the server administrator or the database administrator – these roles and responsibilities must be separated from one another.
Defense in depth.
Information security must protect information throughout the life span of the information, from the initial creation of the information on through to the final disposal of the information. The information must be protected while in motion and while at rest. During its lifetime, information may pass through many different information processing systems and through many different parts of information processing systems. There are many different ways the information and information systems can be threatened. To fully protect the information during its lifetime, each component of the information processing system must have its own protection mechanisms. The building up, layering on and overlapping of security measures is called defense in depth. The strength of any system is no greater than its weakest link. Using a defense in depth strategy, should one defensive measure fail there are other defensive measures in place that continue to provide protection.
Recall the earlier discussion about administrative controls, logical controls, and physical controls. The three types of controls can be used to form the basis upon which to build a defense-in-depth strategy. With this approach, defense-in-depth can be conceptualized as three distinct layers or planes laid one on top of the other. Additional insight into defense-in- depth can be gained by thinking of it as forming the layers of an onion, with data at the core of the onion, people the next outer layer of the onion, and network security, host-based security and application security forming the outermost layers of the onion. Both perspectives are equally valid and each provides valuable insight into the implementation of a good defense-in-depth strategy.
Security classification for information.
An important aspect of information security and risk management is recognizing the value of information and defining appropriate procedures and protection requirements for the information. Not all information is equal and so not all information requires the same degree of protection. This requires information to be assigned a security classification.
The first step in information classification is to identify a member of senior management as the owner of the particular information to be classified. Next, develop a classification policy. The policy should describe the different classification labels, define the criteria for information to be assigned a particular label, and list the required security controls for each classification.
Some factors that influence which classification information should be assigned include how much value that information has to the organization, how old the information is and whether or not the information has become obsolete. Laws and other regulatory requirements are also important considerations when classifying information.
The Business Model for Information Security enables security professionals to examine security from systems perspective, creating an environment where security can be managed holistically, allowing actual risks to be addressed.
The type of information security classification labels selected and used will depend on the nature of the organization, with examples being:
All employees in the organization, as well as business partners, must be trained on the classification schema and understand the required security controls and handling procedures for each classification. The classification of a particular information asset that has been assigned should be reviewed periodically to ensure the classification is still appropriate for the information and to ensure the security controls required by the classification are in place and are followed in their right procedures.
Access control.
Access to protected information must be restricted to people who are authorized to access the information. The computer programs, and in many cases the computers that process the information, must also be authorized. This requires that mechanisms be in place to control the access to protected information. The sophistication of the access control mechanisms should be in parity with the value of the information being protected – the more sensitive or valuable the information the stronger the control mechanisms need to be. The foundation on which access control mechanisms are built start with identification and authentication.
Access control is generally considered in three steps: Identification, Authentication, and Authorization.
Identification.
Identification is an assertion of who someone is or what something is. If a person makes the statement "Hello, my name is John Doe" they are making a claim of who they are. However, their claim may or may not be true. Before John Doe can be granted access to protected information it will be necessary to verify that the person claiming to be John Doe really is John Doe. Typically the claim is in the form of a username. By entering that username you are claiming "I am the person the username belongs to".
Authentication.
Authentication is the act of verifying a claim of identity. When John Doe goes into a bank to make a withdrawal, he tells the bank teller he is John Doe—a claim of identity. The bank teller asks to see a photo ID, so he hands the teller his driver's license. The bank teller checks the license to make sure it 
has John Doe printed on it and compares the photograph on the license against the person claiming to be John Doe. If the photo and name match the person, then the teller has authenticated that John Doe is who he claimed to be. Similarly by entering the correct password, the user is providing evidence that they are the person the username belongs to.
There are three different types of information that can be used for authentication:
Strong authentication requires providing more than one type of authentication information (two-factor authentication). The username is the most common form of identification on computer systems today and the password is the most common form of authentication. Usernames and passwords have served their purpose but in our modern world they are no longer adequate. Usernames and passwords are slowly being replaced with more sophisticated authentication mechanisms.
Authorization.
After a person, program or computer has successfully been identified and authenticated then it must be determined what informational resources they are permitted to access and what actions they will be allowed to perform (run, view, create, delete, or change). This is called authorization. Authorization to access information and other computing services begins with administrative policies and procedures. The policies prescribe what information and computing services can be accessed, by whom, and under what conditions. The access control mechanisms are then configured to enforce these policies. Different computing systems are equipped with different kinds of access control mechanisms—some may even offer a choice of different access control mechanisms. The access control mechanism a system offers will be based upon one of three approaches to access control or it may be derived from a combination of the three approaches.
The non-discretionary approach consolidates all access control under a centralized administration. The access to information and other resources is usually based on the individuals function (role) in the organization or the tasks the individual must perform. The discretionary approach gives the creator or owner of the information resource the ability to control access to those resources. In the Mandatory access control approach, access is granted or denied basing upon the security classification assigned to the information resource.
Examples of common access control mechanisms in use today include role-based access control available in many advanced database management systems—simple file permissions provided in the UNIX and Windows operating systems, Group Policy Objects provided in Windows network systems, Kerberos, RADIUS, TACACS, and the simple access lists used in many firewalls and routers.
To be effective, policies and other security controls must be enforceable and upheld. Effective policies ensure that people are held accountable for their actions. All failed and successful authentication attempts must be logged, and all access to information must leave some type of audit trail.
Also, need-to-know principle needs to be in affect when talking about access control. Need-to-know principle gives access rights to a person to perform their job functions. This principle is used in the government, when dealing with difference clearances. Even though two employees in different departments have a top-secret clearance, they must have a need-to-know in order for information to be exchanged. Within the need-to-know principle, network administrators grant the employee least amount privileges to prevent employees access and doing more than what they are supposed to. Need-to-know helps to enforce the confidentiality-integrity-availability (C‑I‑A) triad. Need-to-know directly impacts the confidential area of the triad.
Cryptography.
Information security uses cryptography to transform usable information into a form that renders it unusable by anyone other than an authorized user; this process is called encryption. Information that has been encrypted (rendered unusable) can be transformed back into its original usable form by an authorized user, who possesses the cryptographic key, through the process of decryption. Cryptography is used in information security to protect information from unauthorized or accidental disclosure while the information is in transit (either electronically or physically) and while information is in storage.
Cryptography provides information security with other useful applications as well including improved authentication methods, message digests, digital signatures, non-repudiation, and encrypted network communications. Older less secure applications such as telnet and ftp are slowly being replaced with more secure applications such as ssh that use encrypted network communications. Wireless communications can be encrypted using protocols such as WPA/WPA2 or the older (and less secure) WEP. Wired communications (such as ITU‑T G.hn) are secured using AES for encryption and X.1035 for authentication and key exchange. Software applications such as GnuPG or PGP can be used to encrypt data files and Email.
Cryptography can introduce security problems when it is not implemented correctly. Cryptographic solutions need to be implemented using industry accepted solutions that have undergone rigorous peer review by independent experts in cryptography. The length and strength of the encryption key is also an important consideration. A key that is weak or too short will produce weak encryption. The keys used for encryption and decryption must be protected with the same degree of rigor as any other confidential information. They must be protected from unauthorized disclosure and destruction and they must be available when needed. Public key infrastructure (PKI) solutions address many of the problems that surround key management.
Process.
The terms reasonable and prudent person, due care and due diligence have been used in the fields of Finance, Securities, and Law for many years. In recent years these terms have found their way into the fields of computing and information security. U.S.A. Federal Sentencing Guidelines now make it possible to hold corporate officers liable for failing to exercise due care and due diligence in the management of their information systems.
In the business world, stockholders, customers, business partners and governments have the expectation that corporate officers will run the business in accordance with accepted business practices and in compliance with laws and other regulatory requirements. This is often described as the "reasonable and prudent person" rule. A prudent person takes due care to ensure that everything necessary is done to operate the business by sound business principles and in a legal ethical manner. A prudent person is also diligent (mindful, attentive, and ongoing) in their due care of the business.
In the field of Information Security, Harris
offers the following definitions of due care and due diligence:
""Due care are steps that are taken to show that a company has taken responsibility for the activities that take place within the corporation and has taken the necessary steps to help protect the company, its resources, and employees."" And, diligence are the ""continual activities that make sure the protection mechanisms are continually maintained and operational.""
Attention should be made to two important points in these definitions. First, in due care, steps are taken to show - this means that the steps can be verified, measured, or even produce tangible artifacts. Second, in due diligence, there are continual activities - this means that people are actually doing things to monitor and maintain the protection mechanisms, and these activities are ongoing.
Security governance.
The Software Engineering Institute at Carnegie Mellon University, in a publication titled "Governing for Enterprise Security (GES)", defines characteristics of effective security governance. These include:
Incident response plans.
"1 to 3 paragraphs (non technical) that discuss:"
Change management.
Change management is a formal process for directing and controlling alterations to the information processing environment. This includes alterations to desktop computers, the network, servers and software. The objectives of change management are to reduce the risks posed by changes to the information processing environment and improve the stability and reliability of the
processing environment as changes are made. It is not the objective of change management to prevent or hinder necessary changes from being implemented.
Any change to the information processing environment introduces an element of risk. Even apparently simple changes can have unexpected effects. One of Management's many responsibilities is the management of risk. Change management is a tool for managing the risks introduced by changes to the information processing environment. Part of the change management process ensures that changes are not implemented at inopportune times when they may disrupt critical business processes or interfere with other changes being implemented.
Not every change needs to be managed. Some kinds of changes are a part of the everyday routine of information processing and adhere to a predefined procedure, which reduces the overall level of risk to the processing environment. Creating a new user account or deploying a new desktop computer are examples of changes that do not generally require change management. However, relocating user file shares, or upgrading the Email server pose a much higher level of risk to the processing environment and are not a normal everyday activity. The critical first steps in change management are (a) defining change (and communicating that definition) and (b) defining the scope of the change system.
Change management is usually overseen by a Change Review Board composed of representatives from key business areas, security, networking, systems administrators, Database administration, applications development, desktop support and the help desk. The tasks of the Change Review Board can be facilitated with the use of automated work flow application. The responsibility of the Change Review Board is to ensure the organizations documented change management procedures are followed. The change management process is as follows:
Change management procedures that are simple to follow and easy to use can greatly reduce the overall risks created when changes are made to the information processing environment. Good change management procedures improve the overall quality and success of changes as they are implemented. This is accomplished through planning, peer review, documentation and communication.
ISO/IEC 20000, The Visible OPS Handbook: Implementing ITIL in 4 Practical and Auditable Steps (Full book summary), and Information Technology Infrastructure Library all provide valuable guidance on implementing an efficient and effective change management program information security.
Business continuity.
Business continuity is the mechanism by which an organization continues to operate its critical business units, during planned or unplanned disruptions that affect normal business operations, by invoking planned and managed procedures.
Not only is business continuity simply about the business, but it is also an IT system and process. Today disasters or disruptions to business are a reality. Whether the disaster is natural or man-made, it affects normal life and so business. Therefore, planning is important.
The planning is merely getting better prepared to face it, knowing fully well that the best plans may fail. Planning helps to reduce cost of recovery, operational overheads and most importantly sail through some smaller ones effortlessly.
For businesses to create effective plans they need to focus upon the following key questions. Most of these are common knowledge, and anyone can do a BCP.
Disaster recovery planning.
While a business continuity plan (BCP) takes a broad approach to dealing with organizational-wide effects of a disaster, a disaster recovery plan (DRP), which is a subset of the business continuity plan, is instead focused on taking the necessary steps to resume normal business operations as quickly as possible. A disaster recovery plan is executed immediately after the disaster occurs and details what steps are to be taken in order to recover critical information technology infrastructure. Disaster recovery planning includes establishing a planning group, performing risk assessment, establishing priorities, developing recovery strategies, preparing inventories and documentation of the plan, developing verification criteria and procedure, and lastly implementing the plan.
Laws and regulations.
"Below is a partial listing of European, United Kingdom, Canadian and US governmental laws and regulations that have, or will have, a significant effect on data processing and information security. Important industry sector regulations have also been included when they have a significant impact on information security."
Information security culture.
Employee’s behavior has a big impact to information security in organizations. Cultural concept can help different segments of the organization to concern about the information security within the organization.″Exploring the Relationship between Organizational Culture and Information Security Culture″ provides the following definition of information security culture: ″ISC is the totality of patterns of behavior in an organization that contribute to the protection of information of all kinds.″
Information security culture needs to be improved continuously. In ″Information Security Culture from Analysis to Change″, authors commented, ″It′s a never ending process, a cycle of evaluation and change or maintenance.″ To manage the information security culture, five steps should be taken: Pre-evaluation, strategic planning, operative planning, implementation, and post-evaluation.
Sources of standards.
International Organization for Standardization (ISO) is a consortium of national standards
institutes from 157 countries, coordinated through a secretariat in Geneva, Switzerland. ISO is
the world's largest developer of standards. ISO 15443: "Information technology - Security techniques - A framework
for IT security assurance", ISO/IEC 27002: "Information technology - Security techniques - Code of practice for information security management", ISO-20000: "Information technology - Service management", and ISO/IEC 27001: "Information technology - Security techniques - Information security management systems - Requirements" are of particular interest to information security professionals.
The US National Institute of Standards and Technology (NIST) is a non-regulatory federal agency
within the U.S. Department of Commerce. The NIST Computer Security Division
develops standards, metrics, tests and validation programs as well as publishes standards and guidelines to
increase secure IT planning, implementation, management and operation. NIST is also the custodian of the US Federal Information Processing Standard publications (FIPS).
The Internet Society is a professional membership society with more than 100 organizations
and over 20,000 individual members in over 180 countries. It provides leadership in addressing issues that confront the
future of the Internet, and is the organization home for the groups responsible for Internet infrastructure standards,
including the Internet Engineering Task Force (IETF) and the Internet Architecture Board (IAB). The ISOC hosts the Requests for Comments (RFCs) which includes the Official Internet Protocol Standards and the RFC-2196 Site Security Handbook.
The Information Security Forum is a global nonprofit organization of several hundred leading organizations in financial services, manufacturing, telecommunications, consumer goods, government, and other areas. It undertakes research into information security practices and offers advice in its biannual Standard of Good Practice and more detailed advisories for members.
The Institute of Information Security Professionals (IISP) is an independent, non-profit body governed by its members, with the principal objective of advancing the professionalism of information security practitioners and thereby the professionalism of the industry as a whole. The Institute developed the IISP Skills Framework©. This framework describes the range of competencies expected of Information Security and Information Assurance Professionals in the effective performance of their roles. It was developed through collaboration between both private and public sector organisations and world-renowned academics and security leaders.
The German Federal Office for Information Security (in German "Bundesamt für Sicherheit in der Informationstechnik (BSI)") BSI-Standards 100-1 to 100-4 are a set of recommendations including "methods, processes, procedures, approaches and measures relating to information security". The BSI-Standard 100-2 "IT-Grundschutz Methodology" describes how an information security management can be implemented and operated. The Standard includes a very specific guide, the IT Baseline Protection Catalogs (also known as IT-Grundschutz Catalogs). Before 2005 the catalogs were formerly known as "IT Baseline Protection Manual". The Catalogs are a collection of documents useful for detecting and combating security-relevant weak points in the IT environment (IT cluster). The collection encompasses as of September 2013 over 4.400 pages with the introduction and catalogs. The IT-Grundschutz approach is aligned with to the ISO/IEC 2700x family.
At the European Telecommunications Standards Institute a catalog of Information security indicators have been standardized by the Industrial Specification Group (ISG) ISI.

</doc>
<doc id="15037" url="https://en.wikipedia.org/wiki?curid=15037" title="Income">
Income

Income is the consumption and savings opportunity gained by an entity within a specified timeframe, which is generally expressed in monetary terms. However, for households and individuals, "income is the sum of all the wages, salaries, profits, interests payments, rents, and other forms of earnings received... in a given period of time."
In the field of public economics, the term may refer to the accumulation of both monetary and non-monetary consumption ability, with the former (monetary) being used as a proxy for total income
Increase in income.
Income per capita has been increasing steadily in almost every country. Many factors contribute to people having a higher income such as education, globalisation and favorable political circumstances such as economic freedom and peace. Increase in income also tends to lead to people choosing to work less hours.
Developed countries (defined as countries with a "developed economy") have higher incomes as opposed to developing countries tending to have lower incomes.
Economic definitions.
In economics, "factor income" is the return accruing for a person, or a nation, derived from the "factors of production": rental income, wages generated by labor, the interest created by capital, and profits from entrepreneurial ventures.
From labor services, as well as ownership of land and capital.
In consumer theory 'income' is another name for the "budget constraint," an amount formula_1 to be spent on different goods x and y in quantities formula_2 and formula_3 at prices formula_4 and formula_5. The basic equation for this is
This equation implies two things. First buying one more unit of good x implies buying formula_7 less units of good y. So, formula_7 is the "relative" price of a unit of x as to the number of units given up in y. Second, if the price of x falls for a fixed formula_1, then its relative price falls. The usual hypothesis is that the quantity demanded of x would increase at the lower price, the law of demand. The generalization to more than two goods consists of modelling y as a composite good.
The theoretical generalization to more than one period is a multi-period wealth and income constraint. For example, the same person can gain more productive skills or acquire more productive income-earning assets to earn a higher income. In the multi-period case, something might also happen to the economy beyond the control of the individual to reduce (or increase) the flow of income. Changing measured income and its relation to consumption over time might be modeled accordingly, such as in the permanent income hypothesis.
Full and Haig-Simons income.
"Full income" refers to the accumulation of both the monetary and the non-monetary consumption-ability of any given entity, such as a person or a household. According to the what economist Nicholas Barr describes as the "classical definition of income" (the 1938 Haig-Simons definition): "income may be defined as the... sum of (1) the market value of rights exercised in consumption and (2) the change in the value of the store of property rights..." Since the consumption potential of non-monetary goods, such as leisure, cannot be measured, monetary income may be thought of as a proxy for full income. As such, however, it is criticized for being unreliable, "i.e." failing to accurately reflect affluence (and thus the consumption opportunities) of any given agent. It omits the utility a person may derive from non-monetary income and, on a macroeconomic level, fails to accurately chart social welfare. According to Barr, "in practice money income as a proportion of total income varies widely and unsystematically. Non-observability of full-income prevent a complete characterization of the individual opportunity set, forcing us to use the unreliable yardstick of money income.
Income inequality.
Income inequality refers to the extent to which income is distributed in an uneven manner. Within a society can be measured by various methods, including the Lorenz curve and the Gini coefficient. Economists generally agree that certain amounts of inequality are necessary and desirable but that excessive inequality leads to efficiency problems and social injustice.
National income, measured by statistics such as the Net National Income (NNI), measures the total income of individuals, corporations, and government in the economy. For more information see measures of national income and output.
Income in philosophy and ethics.
Throughout history, many have written about the impact of income on morality and society. Saint Paul wrote 'For the love of money is a root of all kinds of evil:' ( (ASV)).
Some scholars have come to the conclusion that material progress and prosperity, as manifested in continuous income growth at both individual and national level, provide the indispensable foundation for sustaining any kind of morality. This argument was explicitly given by Adam Smith in his "Theory of Moral Sentiments", and has more recently been developed by Harvard economist Benjamin Friedman in his book "The Moral Consequences of Economic Growth".
Accountancy.
The International Accounting Standards Board (IASB) uses the following definition: "Income is increases in economic benefits during the accounting period in the form of inflows or enhancements of assets or decreases of liabilities that result in increases in equity, other than those relating to contributions from equity participants." [F.70] (IFRS Framework)

</doc>
<doc id="15039" url="https://en.wikipedia.org/wiki?curid=15039" title="Iona">
Iona

Iona () is a small island in the Inner Hebrides off the Ross of Mull on the western coast of Scotland. It was a centre of Gaelic monasticism for four centuries and is today renowned for its tranquility and natural beauty. It is a popular tourist destination and a place for retreats. Its modern Gaelic name means "Iona of (Saint) Columba" (formerly anglicised "Icolmkill").
Etymology.
The Hebrides have been occupied by the speakers of several languages since the Iron Age, and as a result many of the names of these islands have more than one possible meaning. Nonetheless few, if any, can have accumulated so many different names over the centuries as the island now known in English as "Iona".
The earliest forms of the name enabled place-name scholar William J. Watson to show that the name originally meant something like "yew-place". The element "Ivo-", denoting "yew", occurs in Ogham inscriptions ("Iva-cattos" "Iva-geni" [genitive) and in Gaulish names ("Ivo-rix", "Ivo-magus") and may form the basis of early Gaelic names like "Eogan" (ogham: "Ivo-genos"). It is possible that the name is related to the mythological figure, "Fer hÍ mac Eogabail", foster-son of Manannan, the forename meaning "man of the yew".
Mac an Tàilleir (2003) lists the more recent Gaelic names of "Ì", "Ì Chaluim Chille" and "Eilean Idhe" noting that the first named is "generally lengthened to avoid confusion" to the second, which means "Calum's (i.e. in latinised form "Columba's") Iona" or "island of Calum's monastery". The possible confusion results from "ì", despite its original etymology, becoming a Gaelic noun (now obsolete) meaning simply "island". "Eilean Idhe" means "the isle of Iona", also known as "Ì nam ban bòidheach" ("the isle of beautiful women"). The modern English name comes of yet another variant, "Ioua", which was either just Adomnán's attempt to make the Gaelic name fit Latin grammar or else a genuine derivative from "Ivova" ("yew place"). "Ioua"'s change to "Iona", attested from c.1274, results from a transcription mistake resulting from the similarity of "n" and "u" in Insular Minuscule.
Despite the continuity of forms in Gaelic between the pre-Norse and post-Norse eras, Haswell-Smith (2004) speculates that the name may have a Norse connection, "Hiōe" meaning "island of the den of the brown bear", "island of the den of the fox", or just "island of the cave".; Nynorsk "Hy" (from Old Norse "Hý") means "sparse grass", or otherwise something furry or shaggy. The medieval English language version was "Icolmkill" (and variants thereof).
Folk etymology.
Murray (1966) claims that the "ancient" Gaelic name was "Innis nan Druinich" ("the isle of Druidic hermits") and repeats a Gaelic story (which he admits is apocryphal) that as Columba's coracle first drew close to the island one of his companions cried out ""Chì mi i"" meaning "I see her" and that Columba's response was "Henceforth we shall call her Ì".
Geography.
Iona lies about from the coast of Mull. It is about wide and long with a resident population of 125. The geology of the island consists mainly of Precambrian Lewisian gneiss with Torridonian sedimentary rocks on the eastern side and small outcrops of pink granite on the eastern beaches. Like other places swept by ocean breezes, there are few trees; most of them are near the parish church.
Iona's highest point is Dùn Ì, , an Iron Age hill fort dating from 100 BC – AD 200. Iona's geographical features include the Bay at the Back of the Ocean and "Càrn Cùl ri Éirinn" (the Hill/Cairn of the Back to Ireland), said to be adjacent to the beach where St. Columba first landed.
The main settlement, located at St. Ronan's Bay on the eastern side of the island, is called "Baile Mòr" and is also known locally as "The Village". The primary school, post office, the island's two hotels, the Bishop's House and the ruins of the Nunnery are here. The Abbey and MacLeod Centre are a short walk to the north. Port Bàn (white port) beach on the west side of the island is home to the Iona Beach Party.
There are numerous offshore islets and skerries: Eilean Annraidh (island of storm) and Eilean Chalbha (calf island) to the north, Rèidh Eilean and Stac MhicMhurchaidh to the west and Eilean Mùsimul (mouse holm island) and Soa Island to the south are amongst the largest. The steamer "Cathcart Park" carrying a cargo of salt from Runcorn to Wick ran aground on Soa on 15 April 1912, the crew of 11 escaping in two boats.
History.
Dál Riata.
In the early Historic Period Iona lay within the Gaelic kingdom of Dál Riata. The island was the site of a highly important monastery (see Iona Abbey) during the Early Middle Ages. According to tradition the monastery was founded in 563 by the monk Columba, also known as Colm Cille, who had been exiled from his native Ireland as a result of his involvement in the Battle of Cul Dreimhne. Columba and twelve companions went into exile on Iona and founded a monastery there. The monastery was hugely successful, and played a crucial role in the conversion to Christianity of the Picts of present-day Scotland in the late 6th century and of the Anglo-Saxon kingdom of Northumbria in 635. Many satellite institutions were founded, and Iona became the centre of one of the most important monastic systems in Great Britain and Ireland.
Iona became a renowned centre of learning, and its scriptorium produced highly important documents, probably including the original texts of the Iona Chronicle, thought to be the source for the early Irish annals. The monastery is often associated with the distinctive practices and traditions known as Celtic Christianity. In particular, Iona was a major supporter of the "Celtic" system for calculating the date of Easter at the time of the Easter controversy, which pitted supporters of the Celtic system against those favoring the "Roman" system used elsewhere in Western Christianity. The controversy weakened Iona's ties to Northumbria, which adopted the Roman system at the Synod of Whitby in 664, and to Pictland, which followed suit in the early 8th century. Iona itself did not adopt the Roman system until 715, according to the Anglo-Saxon historian Bede. Iona's prominence was further diminished over the next centuries as a result of Viking raids and the rise of other powerful monasteries in the system, such as the Abbey of Kells.
The Book of Kells may have been produced or begun on Iona towards the end of the 8th century. Around this time the island's exemplary high crosses were sculpted; these may be the first such crosses to contain the ring around the intersection that became characteristic of the "Celtic cross". The series of Viking raids on Iona began in 794 and, after its treasures had been plundered many times, Columba's relics were removed and divided two ways between Scotland and Ireland in 849 as the monastery was abandoned.
Kingdom of the Isles.
As the Norse domination of the west coast of Scotland advanced, Iona became part of the Kingdom of the Isles. The Norse "Rex plurimarum insularum" Amlaíb Cuarán died in 980 or 981 whilst in "religious retirement" on Iona. Nonetheless the island was sacked twice by his successors, on Christmas night 986 and again in 987. Although Iona was never again important to Ireland, it rose to prominence once more in Scotland following the establishment of the Kingdom of Alba in the later 9th century. The ruling dynasty of Alba traced its origin to Iona, and the island thus became an important spiritual centre of the new kingdom, with many of its early kings buried there.
A convent for Benedictine nuns was established in about 1208, with Bethóc, daughter of Somerled, as first prioress. The present Benedictine abbey, Iona Abbey, was built in about 1203. The monastery itself flourished until the Reformation when buildings were demolished and all but three of the 360 carved crosses destroyed.
Kingdom of Scotland.
Following the 1266 Treaty of Perth the Hebrides were restored to Scottish rule. An Augustine nunnery survives as a number of 13th century ruins, including a church and cloister. The nunnery continued to be active until the Reformation. By the 1760s little more of the nunnery remained standing than at present, though it is the most complete remnant of a medieval nunnery in Scotland.
Post-Union.
After a visit in 1773, the English writer Samuel Johnson described the island as "fruitful", but backward and impoverished:
He estimated the population of the village at 70 families or perhaps 350 inhabitants.
In the 19th century green-streaked marble was commercially mined in the south-east of Iona; the quarry and machinery survive.
Iona Abbey.
Iona Abbey, now an ecumenical church, is of particular historical and religious interest to pilgrims and visitors alike. It is the most elaborate and best-preserved ecclesiastical building surviving from the Middle Ages in the Western Isles of Scotland. Though modest in scale in comparison to medieval abbeys elsewhere in Western Europe, it has a wealth of fine architectural detail, and monuments of many periods. The 8th Duke of Argyll presented the sacred buildings and sites of the island to the Iona Cathedral trust in 1899.
In front of the Abbey stands the 9th century St Martin's Cross, one of the best-preserved Celtic crosses in the British Isles, and a replica of the 8th century St John's Cross (original fragments in the Abbey museum).
The ancient burial ground, called the Rèilig Odhrain (Eng: Oran's "burial place" or "cemetery"), contains the 12th century chapel of St Odhrán (said to be Columba's uncle), restored at the same time as the Abbey itself. It contains a number of medieval grave monuments. The abbey graveyard contains the graves of many early Scottish Kings, as well as kings from Ireland, Norway and France. Iona became the burial site for the kings of Dál Riata and their successors. Notable burials there include:
In 1549 an inventory of 48 Scottish, 8 Norwegian and 4 Irish kings was recorded. None of these graves are now identifiable (their inscriptions were reported to have worn away at the end of the 17th century). Saint Baithin and Saint Failbhe may also be buried on the island. The Abbey graveyard is also the final resting place of John Smith, the former Labour Party leader, who loved Iona. His grave is marked with an epitaph quoting Alexander Pope: "An honest man's the noblest work of God".
Other early Christian and medieval monuments have been removed for preservation to the cloister arcade of the Abbey, and the Abbey museum (in the medieval infirmary). The ancient buildings of Iona Abbey are now cared for by Historic Scotland (entrance charge).
Present day.
The island, other than the land owned by the Iona Cathedral Trust, was purchased from the Duke of Argyll by Hugh Fraser in 1979 and donated to the National Trust for Scotland. In 2001 Iona's population was 125 and by the time of the 2011 census this had grown to 177 usual residents. During the same period Scottish island populations as a whole grew by 4% to 103,702.
Iona Community.
In 1938 George MacLeod founded the Iona Community, an ecumenical Christian community of men and women from different walks of life and different traditions in the Christian church committed to seeking new ways of living the Gospel of Jesus in today's world. This community is a leading force in the present Celtic Christian revival.
The Iona Community runs 3 residential centres on the Isle of Iona and on Mull, where one can live together in community with people of every background from all over the world. Weeks at the centres often follow a programme related to the concerns of the Iona Community.
The 8 tonne "Fallen Christ" sculpture by Ronald Rae was permanently situated outside the MacLeod Centre in February 2008.
Transport.
Visitors can reach Iona by the 10-minute ferry trip across the Sound of Iona from Fionnphort on Mull. The most common route is via Oban in Argyll and Bute. Regular ferries connect to Craignure on Mull, from where the scenic road runs 37 miles to Fionnphort. Tourist coaches and local bus services meet the ferries.
There are very few cars on the island, as they are tightly regulated and vehicular access is not allowed for non-residents, who have to leave their car in Fionnphort. Bike hire is available at the pier, and on Mull.
Accommodation.
In addition to the hotels, there are several bed and breakfasts on Iona and various self-catering properties. The Iona Hostel at Lagandorain and the Iona campsite at Cnoc Oran also offer accommodation.
Media and the arts.
Samuel Johnson wrote "That man is little to be envied whose patriotism would not gain force upon the plains of Marathon, or whose piety would not grow warmer amid the ruins of Iona."
"Peace of Iona" is a song written by Mike Scott that appears on the studio album "Universal Hall" and on the live recording "Karma to Burn" by The Waterboys. Iona is the setting for the song "Oran" on the 1997 Steve McDonald album "Stone of Destiny".
Kenneth C. Steven published an anthology of poetry entitled "Iona: Poems" in 2000 inspired by his association with the island and the surrounding area.
Iona is featured prominently in the first episode ("By the Skin of Our Teeth") of the celebrated arts series "" (1969).
Iona is the setting of Jeanne M. Dams' Dorothy Martin mystery "Holy Terror of the Hebrides" (1998).
The Academy Award–nominated Irish animated film "The Secret of Kells" is about the creation of the Book of Kells. One of the characters, Brother Aiden, is a master illuminator from Iona Abbey who had helped to illustrate the Book, but had to escape the island with it during a Viking invasion.
After his death in 2011, the cremated remains of songwriter/recording artist Gerry Rafferty were scattered on Iona.

</doc>
<doc id="15040" url="https://en.wikipedia.org/wiki?curid=15040" title="Ido (language)">
Ido (language)

Ido is a constructed language created to be a universal second language for speakers of diverse backgrounds. Ido was specifically designed to be grammatically, orthographically, and lexicographically regular, and above all easy to learn and use. In this sense, Ido is classified as a constructed international auxiliary language. It is the most successful of many Esperanto derivatives, called Esperantidos.
Ido was created in 1907 out of a desire to reform perceived flaws in Esperanto, a language that had been created for the same purpose 20 years earlier. The name of the language traces its origin to the Esperanto word "", meaning "offspring", since the language is a "descendant" of Esperanto. After its inception, Ido gained support from some in the Esperanto community, but following the sudden death in 1914 of one of its most influential proponents, Louis Couturat, it declined in popularity. There were two reasons for this: first, the emergence of further schisms arising from competing reform projects; and second, a general lack of awareness of Ido as a candidate for an international language. These obstacles weakened the movement and it was not until the rise of the Internet that it began to regain momentum.
Ido uses the same 26 letters as the English (Latin) alphabet with no diacritics. It draws its vocabulary from French, Italian, Spanish, English, German, and Russian, and is largely intelligible to those who have studied Esperanto.
Several works of literature have been translated into Ido, including "The Little Prince" and the Gospel of Luke. As of the year 2000, there were approximately 100–200 Ido speakers in the world.
History.
The idea of a universal second language is not new, and constructed languages are not a recent phenomenon. The first known constructed language was Lingua Ignota, created in the 12th century. But the idea did not catch on in large numbers until the language Volapük was created in 1879. Volapük was popular for some time and apparently had a few thousand users, but was later eclipsed by the popularity of Esperanto, which arose in 1887. Several other languages such as Latino sine Flexione and Idiom Neutral had also been put forward. It was during this time that French mathematician Louis Couturat formed the "Delegation for the Adoption of an International Auxiliary Language".
This delegation made a formal request to the International Association of Academies in Vienna to select and endorse an international language; the request was rejected in May 1907. The Delegation then met as a Committee in Paris in October 1907 to discuss the adoption of a standard international language. Among the languages considered was a new language anonymously submitted at the last moment (and therefore against the Committee rules) under the pen name "Ido". In the end the Committee, always without plenary sessions and consisting in only 12 members, concluded the last day with 4 votes for and 1 abstention. They concluded that no language was completely acceptable, but that Esperanto could be accepted "on condition of several modifications to be realized by the permanent Commission in the direction defined by the conclusions of the Report of the Secretaries Couturat and Léopold Leau and by the Ido project".
Esperanto's inventor, L. L. Zamenhof, having heard a number of complaints, had suggested in 1894 a proposition for a Reformed Esperanto with several changes that Ido adopted and made it closer to French: eliminating the accented letters and the accusative case, changing the plural to an Italianesque "-i", and replacing the table of correlatives with more Latinate words. However, the Esperanto community voted and rejected Reformed Esperanto, and likewise most rejected the recommendations of the 1907 Committee composed by 12 members. Zamenhof deferred to their judgment, although doubtful. Furthermore, controversy ensued when the "Ido project" was found to have been primarily devised by Louis de Beaufront, who was elected by Zamenhof to represent Esperanto before the Committee, as the Committee's rules dictated the creator of a submitted language could not defend it. The Committee's language was French and not everyone could speak in French. When the president of the Committee asked who was the author of Ido's project, Couturat, Beaufront and Leau answered they were not. Beaufront was the person who presented Ido's project and gave a description as a better, richer version of Esperanto. Couturat, Leau, Beaufront and Jespersen were finally the only members who voted, all of them for Ido's project. A month later, Couturat accidentally put Jespersen in copy of a letter in which he acknowledged that Beaufront was the author of the Ido project. Jespersen was angered by this, and asked for a public confession which was never forthcoming.
It is estimated that some 20% of Esperanto leaders and 3–4% of ordinary Esperantists defected to Ido, which from then on suffered constant modifications seeking to perfect it, but which ultimately had the effect of causing many Ido speakers to give up on trying to learning it. Although it fractured the Esperanto movement, the schism gave the remaining Esperantists the freedom to concentrate on using and promoting their language as it stood. At the same time, it gave the Idists freedom to continue working on their own language for several more years before actively promoting it. The "Uniono di la Amiki di la Linguo Internaciona" ("Union of Friends of the International Language") was established along with an Ido Academy to work out the details of the new language.
Couturat, who was the leading proponent of Ido, was killed in an automobile accident in 1914. This, along with World War I, practically suspended the activities of the Ido Academy from 1914 to 1920. In 1928 Ido's major intellectual supporter, the Danish linguist Otto Jespersen, published his own planned language, Novial. His defection from the Ido movement set it back even further.
Digital era.
The language still has active speakers today, and the Internet has sparked a renewal of interest in the language in recent years. A sample of 24 Idists on the Yahoo! group "Idolisto" during November 2005 showed that 57% had begun their studies of the language during the preceding three years, 32% from the mid-1990s to 2002, and 8% had known the language from before.
Changes.
Few changes have been made to Ido since 1922.
Camiel de Cock was named secretary of linguistic issues in 1990, succeeding Roger Moureaux. He resigned after the creation of a linguistic committee in 1991. De Cock was succeeded by Robert C. Carnaghan, who held the position from 1992 to 2008. No new words were adopted between 2001 and 2006. Following the 2008–2011 elections of ULI's direction committee, Gonçalo Neves replaced Carnaghan as secretary of linguistic issues in February 2008. Neves resigned in August 2008. A new linguistic committee was formed in 2010. In April 2010, Tiberio Madonna was appointed as secretary of linguistic issues, succeeding Neves. In January 2011, ULI approved eight new words. This was the first addition of words in many years. As of April 2012, the secretary of linguistic issues remains Tiberio Madonna.
Phonology.
Ido has five vowel phonemes. The vowels and are interchangeable depending on speaker preference, as are and . The combinations /au/ and /eu/ become diphthongs in word roots but not when adding affixes.
All polysyllable words are stressed on the second-to-last syllable except for verb infinitives, which are stressed on the last syllableskolo, kafeo and lernas for "school", "coffee" and the present tense of "to learn", but irar, savar and drinkar for "to go", "to know" and "to drink". If an i or u precedes another vowel, the pair is considered part of the same syllable when applying the accent rulethus radio, familio and manuo for "radio", "family" and "hand", unless the two vowels are the only ones in the word, in which case the "i" or "u" is stressed: dio, frua for "day" and "early".
Orthography.
Ido uses the same 26 letters as the English alphabet with three digraphs and no ligatures. Where the table below lists two pronunciations, either is perfectly acceptable.
The digraphs are:
Grammar.
Each word in the Ido vocabulary is built from a root word. A word consists of a root and a grammatical ending. Other words can be formed from that word by removing the grammatical ending and adding a new one, or by inserting certain affixes between the root and the grammatical ending.
Some of the grammatical endings are defined as follows:
These are the same as in Esperanto except for "-i", "-ir", "-ar", "-or" and "-ez". Esperanto marks noun plurals by an "agglutinative" ending "-j" (so plural nouns end in "-oj"), uses "-i" for verb infinitives (Esperanto infinitives are tenseless), and uses "-u" for the imperative. Verbs in Ido do not conjugate depending on person, number or gender; the -as, -is, and -os endings suffice whether the subject is I, you, he, she, they, or anything else.
Syntax.
Ido word order is generally the same as English (subject–verb–object), so the sentence "Me havas la blua libro" is the same as the English "I have the blue book", both in meaning and word order. There are a few differences, however:
Ido generally does not impose rules of grammatical agreement between grammatical categories within a sentence. For example, the verb in a sentence is invariable regardless of the number and person of the subject. Nor must the adjectives be pluralized as well the nounsin Ido "the large books" would be "la granda libri" as opposed to the French "les grands livres" or the Esperanto "la grandaj libroj".
Negation occurs in Ido by simply adding ne before a verb: Me ne havas libro means "I do not have a book". This as well does not vary, and thus the "I do not", "He does not", "They do not" before a verb are simply Me ne, Il ne, and Li ne. In the same way, past tense and future tense negatives are formed by ne before the conjugated verb. "I will not go" and "I did not go" become Me ne iros and Me ne iris respectively.
Yes/no questions are formed by the particle ka in front of the question. "I have a book" (me havas libro) becomes Ka me havas libro? (do I have a book?). Ka can also be placed in front of a noun without a verb to make a simple question, corresponding to the English "is it?" Ka Mark? can mean, "Are you Mark?", "Is it Mark?", "Do you mean Mark?" depending on the context.
Pronouns.
The pronouns of Ido were revised to make them more acoustically distinct than those of Esperanto, which all end in "i". Especially the singular and plural first-person pronouns "mi" and "ni" may be difficult to distinguish in a noisy environment, so Ido has "me" and "ni" instead. Ido also distinguishes between intimate ("tu") and formal ("vu") second-person singular pronouns as well as plural second-person pronouns ("vi") not marked for intimacy. Furthermore, Ido has a pan-gender third-person pronoun "lu" (it can mean "he", "she", or "it", depending on the context) in addition to its masculine ("il"), feminine ("el"), and neuter ("ol") third-person pronouns.
It should be noted that "ol", like English "it" and Esperanto "ĝi", is not limited to inanimate objects, but can be used "for entities whose sex is indeterminate: "babies, children, humans, youths, elders, people, individuals, horses, , cats," etc."
"Lu" is often mistakenly labeled an epicene pronoun, that is, one that refers to both masculine and feminine beings, but in fact, "lu" is more properly a "pan-gender" pronoun, as it is also used for referring to inanimate objects. From "Kompleta Gramatiko Detaloza di la Linguo Internaciona Ido" by Beaufront:
Table of correlatives.
Ido makes correlatives by combining entire words together and changing the word ending, with some irregularities to show distinction.
Vocabulary.
Vocabulary in Ido is derived from French, Italian, Spanish, English, German, and Russian. Basing the vocabulary on various widespread languages was intended to make Ido as easy as possible for the greatest number of people possible. Early on, the first 5,371 Ido word roots were analyzed compared to the vocabulary of the six source languages, and the following result was found:
Another analysis showed that:
Vocabulary in Ido is often created through a number of official prefixes and suffixes that alter the meaning of the word. This allows a user to take existing words and modify them to create neologisms when necessary, and allows for a wide range of expression without the need to learn new vocabulary each time. Though their number is too large to be included in one article, some examples include:
New vocabulary is generally created through an analysis of the word, its etymology, and reference to the six source languages. If a word can be created through vocabulary already existing in the language then it will usually be adopted without need for a new radical (such as wikipedio for "Wikipedia", which consists of wiki + enciklopedio for "encyclopedia"), and if not an entirely new word will be created. The word alternatoro for example was adopted in 1926, likely because five of the six source languages used largely the same orthography for the word, and because it was long enough to avoid being mistaken for other words in the existing vocabulary. Adoption of a word is done through consensus, after which the word will be made official by the union. Care must also be taken to avoid homonyms if possible, and usually a new word undergoes some discussion before being adopted. Foreign words that have a restricted sense and are not likely to be used in everyday life (such as the word "intifada" to refer to the conflict between Israel and Palestine) are left untouched, and often written in italics.
Ido, unlike Esperanto, does not assume the male sex by default. For example, Ido does not derive the word for “waitress” by adding a feminine suffix to “waiter”, as Esperanto does. Instead, Ido words are defined as sex-neutral, and two different suffixes derive masculine and feminine words from the root: ' for a waiter of either sex, ' for a male waiter, and ' for a waitress. There are only two exceptions to this rule: First, ' for “father”, ' for “mother”, and ' for “parent”, and second, ' for “man”, ' for “woman”, and "" for “adult”.
Sample.
The Lord's Prayer:
Literature and publications.
Ido has a number of publications that can be subscribed to or downloaded for free in most cases. "Kuriero Internaciona" is a magazine produced in France every few months with a range of topics. "Adavane!" is a magazine produced by the Spanish Ido Society every two months that has a range of topics, as well as a few dozen pages of work translated from other languages. "Progreso" is the official organ of the Ido movement and has been around since the inception of the movement in 1908. Other sites can be found with various stories, fables or proverbs along with a few books of the Bible translated into Ido on a smaller scale. The site "publikaji" has a few podcasts in Ido along with various songs and other recorded material.
The online encyclopedia Wikipedia includes an Ido-language edition (known in Ido as "Wikipedio"); in January 2012 it was the 81st most visited Wikipedia.
International Ido conventions.
ULI organises Ido conventions yearly, and the conventions include a mix of tourism and work.

</doc>
<doc id="15041" url="https://en.wikipedia.org/wiki?curid=15041" title="Improvisational theatre">
Improvisational theatre

Improvisational theatre, often called improv or impro, is a form of theater where most or all of what is performed is created at the moment it is performed. In its purest form, the dialogue, action, story, and characters are created collaboratively by the players as the improvisation unfolds in present time, without use of an already prepared, written script.
Improvisational theatre exists in performance as a range of styles of improvisational comedy as well as some non-comedic theatrical performances. It is sometimes used in film and television, both to develop characters and scripts and occasionally as part of the final product.
Improvisational techniques are often used extensively in drama programs to train actors for stage, film, and television and can be an important part of the rehearsal process. However, the skills and processes of improvisation are also used outside of the context of performing arts. It is used in classrooms as an educational tool and in businesses as a way to develop communication skills, creative problem solving, and supportive team-work abilities that are used by improvisational, ensemble players. It is sometimes used in psychotherapy as a tool to gain insight into a person's thoughts, feelings, and relationships.
History.
The earliest well documented use of improvisational theatre in Eastern history is found in the Atellan Farce of Africa 391 BC. From the 16th to the 18th centuries, "commedia dell'arte" performers improvised based on a broad outline in the streets of Italy. In the 1890s, theatrical theorists and directors such as the Russian Konstantin Stanislavski and the French Jacques Copeau, founders of two major streams of acting theory, both heavily utilized improvisation in acting training and rehearsal.
Modern.
Modern theatrical improvisation games began as drama exercises for children, which were a staple of drama education in the early 20th century thanks in part to the progressive education movement initiated by John Dewey in 1916. Some people credit American Dudley Riggs as the first vaudevillian to use audience suggestions to create improvised sketches on stage. Improvisation exercises were developed further by Viola Spolin in the 1940s, 50s, and 60s, and codified in her book "Improvisation For The Theater", the first book that gave specific techniques for learning to do and teach improvisational theater. In the 1970s in Canada, British playwright and director Keith Johnstone wrote "", a book outlining his ideas on improvisation, and invented Theatresports, which has become a staple of modern improvisational comedy and is the inspiration for the popular television show "Whose Line Is It Anyway?"
Spolin influenced the first generation of modern American improvisers at The Compass Players in Chicago, which led to The Second City. Her son, Paul Sills, along with David Shepherd, started The Compass Players. Following the demise of the Compass Players, Paul Sills began The Second City. They were the first organized troupes in Chicago, and the modern Chicago improvisational comedy movement grew from their success.
Many of the current "rules" of comedic improv were first formalized in Chicago in the late 1950s and early 1960s, initially among The Compass Players troupe, which was directed by Paul Sills. From most accounts, David Shepherd provided the philosophical vision of the Compass Players, while Elaine May was central to the development of the premises for its improvisations. Mike Nichols, Ted Flicker, and Del Close were her most frequent collaborators in this regard. When The Second City opened its doors on December 16, 1959, directed by Paul Sills, his mother Viola Spolin began training new improvisers through a series of classes and exercises which became the cornerstone of modern improv training. By the mid-1960s, Viola Spolin's classes were handed over to her protégé, Jo Forsberg, who further developed Spolin's methods into a one-year course, which eventually became The Players Workshop, the first official school of improvisation in the USA. During this time, Forsberg trained many of the performers who went on to star on The Second City stage.
Many of the original cast of "Saturday Night Live" came from The Second City, and the franchise has produced such comedy stars as Mike Myers, Tina Fey, Bob Odenkirk, Amy Sedaris, Stephen Colbert, Eugene Levy, Jack McBrayer, Steve Carell, Chris Farley, Dan Aykroyd, and John Belushi.
Simultaneously, Keith Johnstone's group The Theatre Machine, which originated in London, was touring Europe. This work gave birth to Theatresports, at first secretly in Johnstone's workshops, and eventually in public when he moved to Canada. Toronto has been home to a rich improv tradition.
In 1984, Dick Chudnow (Kentucky Fried Theater) founded ComedySportz in Milwaukee, WI. Expansion began with the addition of ComedySportz-Madison (WI), in 1985. The first Comedy League of America National Tournament was held in 1988, with 10 teams participating. The league is now known as World Comedy League and boasts a roster of 24 international cities.
In San Francisco, The Committee theater was active in North Beach during the 1960s. It was founded by alumni of Chicago's Second City, Alan Myerson and his wife Jessica. When The Committee disbanded in 1972, three major companies were formed: The Pitchell Players, The Wing, and Improvisation Inc. The only company that continued to perform Close's Harold was the latter one. Its two former members, Michael Bossier and John Elk, formed Spaghetti Jam in San Francisco's Old Spaghetti Factory in 1976, where shortform improv and Harolds were performed through 1983. Stand-up comedians performing down the street at the Intersection for the Arts would drop by and sit in. In 1979, Elk brought shortform to England, teaching workshops at Jacksons Lane Theatre, and he was the first American to perform at The Comedy Store, London, above a Soho strip club.
Modern political improvisation's roots include Jerzy Grotowski's work in Poland during the late 1950s and early 1960s, Peter Brook's "happenings" in England during the late 1960s, Augusto Boal's "Forum Theatre" in South America in the early 1970s, and San Francisco's The Diggers' work in the 1960s. Some of this work led to pure improvisational performance styles, while others simply added to the theatrical vocabulary and were, on the whole, avant-garde experiments.
Joan Littlewood, an English actress and director who was active from the 1930s to 1970s, made extensive use of improv in developing plays for performance. However, she was successfully prosecuted twice for allowing her actors to improvise in performance. Until 1968, British law required scripts to be approved by the Lord Chamberlain's Office. The department also sent inspectors to some performances to check that the approved script was performed exactly as approved.
In 2012, Lebanese writer and director Lucien Bourjeily used improvisational theater techniques to create a multi-sensory play entitled "66 Minutes in Damascus". This play premiered at the London International Festival of Theater, and is considered one of the most extreme kinds of interactive improvised theater put on stage. The audience play the part of kidnapped tourists in today's Syria in a hyperreal sensory environment.
Rob Wittig and Mark C. Marino have developed a form of improv for online theatrical improvisation called netprov. The form relies on social media to engage audiences in the creation of dynamic fictional scenarios that evolve in real-time.
Improvisational comedy.
Modern improvisational comedy, as it is practiced in the West, falls generally into two categories: shortform and longform.
Shortform improv consists of short scenes usually constructed from a predetermined game, structure, or idea and driven by an audience suggestion. Many shortform exercises were first created by Viola Spolin, who called them theater games, influenced by her training from recreational games expert Neva Boyd. The shortform improv comedy television series "Whose Line Is It Anyway?" has familiarized American and British viewers with shortform.
Longform improv performers create shows in which short scenes are often interrelated by story, characters, or themes. Longform shows may take the form of an existing type of theatre, for example a full-length play or Broadway-style musical such as Spontaneous Broadway. One of the better-known longform structures is the Harold, developed by ImprovOlympic co-founder Del Close. Many such longform structures now exist. Longform improvisation is especially performed in Chicago, New York City, Los Angeles; has a strong presence in Austin, Boston, Minneapolis, Phoenix, Philadelphia, San Francisco, Seattle, Detroit, Toronto, Vancouver, Washington, D.C.; and is building a growing following in Denver, Kansas City, Columbus, New Orleans, Omaha, Rochester, and Hawaii. 
Examples of improv comedy games.
There are several types of improv comedy games that follow these general fundamental rules of improv: Improv games are fun to make up and can be experimental. Games can be adapted to tell a different story with the same or similar triggers.
Good Cop Bad Cop
"Players": (3) good cop, bad cop, the criminal
"How to play:" The game begins with the criminal leaving the room - before any of the callouts are taken. Once out of sounds reach, the host asks the audience 3 questions to establish why the person was arrested.
Call Outs: 
1) What crime did they commit?
2) Where did they commit the crime?
3) With whom did they commit the crime?
The prisoner is brought into the holding cell (stage) by one of the cops. They cops interrogate the prisoner in their personas as good cop (gives the prisoner a break, not as aggressive) and bad cop (forceful, doubts prisoner). Through this back and forth between the prisoner and the cops the prisoner tries to answer the 3 questions above - in this order. Once the prisoner solves all 3, they must write a confession. "I stole a goat from my neighbors closet with Jimmy Fallon."
I Once Dated A ...
"Players": All + host
"How to play:" all the players line up shoulder to shoulder on the stage with one microphone in the center (you can play without a mic too). The host has a list of objects from the audience (distribute paper and pens before the performance to get call outs for games like this). The host begins the game by picking an object from the audience's list and prompts the players by saying, "I once dated an octopus" (repeats it again for clarity). Each player - not necessarily in order - step up to the mic and finish the statement. "I once dated an octopus. It was always awkward when we went out for sushi." After this, either another player goes forward with another idea or the host calls out another object.
Non-comedic, experimental, and dramatic, narrative-based improvisational theater.
Other forms of improvisational theatre training and performance techniques are experimental and avant-garde in nature and not necessarily intended to be comedic. These include Playback Theatre and Theatre of the Oppressed, the Poor Theatre, the Open Theatre, to name only a few.
The Open Theatre was founded in New York City by a group of former students of acting teacher Nola Chilton, and joined shortly thereafter by director Joseph Chaikin, formerly of The Living Theatre, and Peter Feldman. This avante-garde theatre group explored political, artistic, and social issues. The company, developing work through an improvisational process drawn from Chilton and Viola Spolin, created well-known exercises, such as "sound and movement" and "transformations", and originated radical forms and techniques that anticipated or were contemporaneous with Jerzy Grotowski's "poor theater" in Poland. During the sixties Chaikin and the Open Theatre developed full theatrical productions with nothing but the actors, a few chairs and a bare stage, creating character, time and place through a series of transformations the actors physicalized and discovered through improvisations.
Longform, dramatic, and narrative-based improvisation is well-established on the west coast with companies such as San Francisco's BATS Improv, Un-Scripted Theater Company® and LA's Impro Theatre, and groups such as True Fiction Magazine, Three for All and Awkward Dinner Party. This format allows for full-length plays and musicals to be created improvisationally.
Applying improv principles in life.
Many people who have studied improv have noted that the guiding principles of improv are useful, not just on stage, but in everyday life. For example, Stephen Colbert in a commencement address said,
Tina Fey in her book Bossypants lists several rules of improv that apply in the workplace. There has been much interest in bringing lessons from improv into the corporate world. In a New York Times article titled "Can Executives Learn to Ignore the Script?", Stanford professor and author, Patricia Ryan Madson notes, "executives and engineers and people in transition are looking for support in saying yes to their own voice. Often, the systems we put in place to keep us secure are keeping us from our more creative selves." Madson explores the application of thirteen "maxims of improvisational theater" to real-life in the book Improv Wisdom: Don't Prepare, Just Show Up.
In film and television.
Many directors have made use of improvisation in the creation of both main-stream and experimental films. Many silent filmmakers such as Charlie Chaplin and Buster Keaton used improvisation in the making of their films, developing their gags while filming and altering the plot to fit. The Marx Brothers were notorious for deviating from the script they were given, their ad libs often becoming part of the standard routine and making their way into their films. Many people, however, make a distinction between ad-libbing and improvising.
The British director Mike Leigh makes extensive use of improvisation in the creation of his films, including improvising important moments in the characters' lives that will not even appear in the film. "This Is Spinal Tap" and other mockumentary films of director Christopher Guest are created with a mix of scripted and unscripted material and "Blue in the Face" is a 1995 comedy directed by Wayne Wang and Paul Auster created in part by the improvisations during the filming of "Smoke".
Some of the best known American film directors who are noted for their use of improvisation in their work with actors are John Cassavetes, Robert Altman and Rob Reiner.
Improv comedy techniques have also been used in television and stand-up comedy, in hit shows such as the recent HBO television show "Curb Your Enthusiasm" created by Larry David, the UK Channel 4 and ABC television series "Whose Line Is It Anyway" (and its spinoffs "Drew Carey's Green Screen Show" and "Drew Carey's Improv-A-Ganza"), Nick Cannon's improv comedy show "Wild 'N Out", and "Thank God You're Here". In Canada, the series "Train 48" was improvised from scripts which contained a minimal outline of each scene, and the comedy series "This Sitcom Is...Not to Be Repeated" incorporated dialogue drawn from a hat during the course of an episode. The American show "Reno 911!" also contained improvised dialogue based on a plot outline. "Fast and Loose" is an improvisational game show, much like "Whose Line Is It Anyway?". The BBC sitcoms "Outnumbered" and "The Thick of It" also had some improvised elements in them.
Psychology.
In the field of the psychology of consciousness, Eberhard Scheiffele explored the altered state of consciousness experienced by actors and improvisers in his scholarly paper "Acting: an altered state of consciousness". According to G. William Farthing in "The Psychology of Consciousness" comparative study, actors routinely enter into an altered state of consciousness (ASC). Acting is seen as altering most of the 14 dimensions of changed subjective experience which characterize ASCs according to Farthing, namely: attention, perception, imagery and fantasy, inner speech, memory, higher-level thought processes, meaning or significance of experiences, time experience, emotional feeling and expression, level of arousal, self-control, suggestibility, body image, and sense of personal identity.
In the growing field of Drama Therapy, psychodramatic improvisation, along with other techniques developed for Drama Therapy, are used extensively. The ""Yes, and"" rule has been compared to Milton Erickson's "utilization" process and to a variety of acceptance-based psychotherapies. Improv training has been recommended for couples therapy and therapist training, and it has been speculated that improv training may be helpful in some cases of social anxiety disorder.
Structure and process.
Improvisational theatre often allows an interactive relationship with the audience. Improv groups frequently solicit suggestions from the audience as a source of inspiration, a way of getting the audience involved, and as a means of proving that the performance is not scripted. That charge is sometimes aimed at the masters of the art, whose performances can seem so detailed that viewers may suspect the scenes are planned.
In order for an improvised scene to be successful, the improvisers involved must work together responsively to define the parameters and action of the scene, in a process of co-creation. With each spoken word or action in the scene, an improviser makes an "offer", meaning that he or she defines some element of the reality of the scene. This might include giving another character a name, identifying a relationship, location, or using mime to define the physical environment. These activities are also known as "endowment". It is the responsibility of the other improvisers to accept the offers that their fellow performers make; to not do so is known as blocking, negation, or denial, which usually prevents the scene from developing. Some performers may deliberately block (or otherwise break out of character) for comedic effect—this is known as "gagging"—but this generally prevents the scene from advancing and is frowned upon by many improvisers. Accepting an offer is usually accompanied by adding a new offer, often building on the earlier one; this is a process improvisers refer to as ""Yes, And..."" and is considered the cornerstone of improvisational technique. Every new piece of information added helps the improvisers to refine their characters and progress the action of the scene. The ""Yes, And..."" rule, however, applies to a scene's early stage since it is in this stage that a "base (or shared) reality" is established in order to be later redefined by applying the ""if (this is true), then (what else can also be true)"" practice progressing the scene into comedy, as explained in the 2013 manual by the "Upright Citizens Brigade" members.
The unscripted nature of improv also implies no predetermined knowledge about the props that might be useful in a scene. Improv companies may have at their disposal some number of readily accessible props that can be called upon at a moment's notice, but many improvisers eschew props in favor of the infinite possibilities available through mime. In improv, this is more commonly known as 'space object work' or 'space work', not 'mime', and the props and locations created by this technique, as 'space objects' created out of 'space substance,' developed as a technique by Viola Spolin. As with all improv "offers", improvisers are encouraged to respect the validity and continuity of the imaginary environment defined by themselves and their fellow performers; this means, for example, taking care not to walk through the table or "miraculously" survive multiple bullet wounds from another improviser's gun.
Because improvisers may be required to play a variety of roles without preparation, they need to be able to construct characters quickly with physicality, gestures, accents, voice changes, or other techniques as demanded by the situation. The improviser may be called upon to play a character of a different age or sex. Character motivations are an important part of successful improv scenes, and improvisers must therefore attempt to act according to the objectives that they believe their character seeks.
Community.
Many theatre troupes are devoted to staging improvisational performances and growing the improv community through their training centers.
In addition to for-profit theatre troupes, there are many college-based improv groups in the United States and around the world.
In Europe the special contribution to the theatre of the abstract, the surreal, the irrational and the subconscious have been part of the stage tradition for centuries. From the 1990s onwards a growing number of European Improv groups have been set up specifically to explore the possibilities offered by the use of the abstract in improvised performance, including dance, movement, sound, music, mask work, lighting, and so on. These groups are not especially interested in comedy, either as a technique or as an effect, but rather in expanding the improv genre so as to incorporate techniques and approaches that have long been a legitimate part of European theatre.
Notable contributors to the field.
Some key figures in the development of improvisational theatre are Viola Spolin and her son Paul Sills, founder of Chicago's famed Second City troupe and originator of Theater Games, and Del Close, founder of ImprovOlympic (along with Charna Halpern) and creator of a popular longform improv format known as The Harold. Other luminaries include Keith Johnstone, the British teacher and writer–author of "Impro", who founded the Theatre Machine and whose teachings form the foundation of the popular shortform Theatresports format, Dick Chudnow, founder of ComedySportz which evolved its family-friendly show format from Johnstone's Theatersports, and Bill Johnson, creator/director of The Magic Meathands, who pioneered the concept of "Commun-edy Outreach" by tailoring performances to non-traditional audiences, such as the homeless and foster children.
David Shepherd, with Paul Sills, founded The Compass Players in Chicago. Shepherd was intent on developing a true "people's Theatre", and hoped to bring political drama to the stockyards. The Compass went on to play in numerous forms and companies, in a number of cities including NY and Hyannis, after the founding of The Second City. A number of Compass members were also founding members of The Second City. In the 1970s, Shepherd began experimenting with group-created videos. He is the author of "That Movie In Your Head", about these efforts. In the 1970s, David Shepherd and Howard Jerome created the Improvisational Olympics, a format for competition based improv. The Improv Olympics were first demonstrated at Toronto's Homemade Theatre in 1976 and have been continued on as the Canadian Improv Games. In the United States, the Improv Olympics were later produced by Charna Halpern under the name "ImprovOlympic" and now as "IO"; IO operates training centers and theaters in Chicago and Los Angeles. At IO, Halpern combined Shepherd's "Time Dash" game with Del Close's "Harold" game; the revised format for the Harold became the fundamental structure for the development of modern longform improvisation.
In 1975 Jonathan Fox founded Playback Theatre, a form of improvised community theatre which is often not comedic and replays stories as shared by members of the audience.
The Groundlings is a popular and influential improv theatre and training center in Los Angeles, California. Gary Austin, founder of The Groundlings, continues to teach improvisation around the country, focusing especially in Los Angeles. He is widely acclaimed as one of the greatest acting teachers in America. His work is grounded in the lessons he learned as an improviser at The Committee with Del Close, as well as in his experiences as founding director of The Groudlings. The Groundlings is often seen as the Los Angeles training ground for the "second generation" of improv luminaries and troupes. Stan Wells developed the "Clap-In" style of longform improvisation here, later using this as the basis for his own theatre, The Empty Stage which in turn bred multiple troupes utilizing this style.
In the late 1990s, Matt Besser, Amy Poehler, Ian Roberts, and Matt Walsh founded the Upright Citizens Brigade Theatre in New York and later they founded one in Los Angeles, each with an accompanying improv/sketch comedy school. In September 2011 the UCB opened a third theatre in New York City's East Village, known as UCBeast.
Gunter Lösel compared the existing improvisational theater theories (from Moreno, Spolin, Johnstone, Close...), structured them and wrote a general theory of improvisational theater.
In 2012, Eran Thomson founded LaughMasters Academy, the first and only not-for-profit "Chicago Style" longform improv and sketch comedy school in Australia.

</doc>
<doc id="15043" url="https://en.wikipedia.org/wiki?curid=15043" title="International Space Station">
International Space Station

The International Space Station (ISS) is a space station, or a habitable artificial satellite, in low Earth orbit. Its first component launched into orbit in 1998, and the ISS is now the largest artificial body in orbit and can often be seen with the naked eye from Earth. The ISS consists of pressurised modules, external trusses, solar arrays, and other components. ISS components have been launched by Russian Proton and Soyuz rockets, and American Space Shuttles.
The ISS serves as a microgravity and space environment research laboratory in which crew members conduct experiments in biology, human biology, physics, astronomy, meteorology, and other fields. The station is suited for the testing of spacecraft systems and equipment required for missions to the Moon and Mars. The ISS maintains an orbit with an altitude of between by means of reboost manoeuvres using the engines of the Zvezda module or visiting spacecraft. It completes  orbits per day.
ISS is the ninth space station to be inhabited by crews, following the Soviet and later Russian Salyut, Almaz, and Mir stations as well as Skylab from the US. The station has been continuously occupied for since the arrival of Expedition 1 on 2 November 2000. This is the longest continuous human presence in space, having surpassed the previous record of held by Mir. The station is serviced by a variety of visiting spacecraft: the Space Shuttle, Soyuz, Progress, the Automated Transfer Vehicle, the H-II Transfer Vehicle, Dragon, and Cygnus. It has been visited by astronauts, cosmonauts and space tourists from 17 different nations.
After the US Space Shuttle programme ended in 2011, Soyuz rockets became the only provider of transport for astronauts at the International Space Station, and Dragon became the only provider of bulk cargo-return-to-Earth services (downmass capability of Soyuz capsules is very limited).
The ISS programme is a joint project among five participating space agencies: NASA, Roscosmos, JAXA, ESA, and CSA. The ownership and use of the space station is established by intergovernmental treaties and agreements. The station is divided into two sections, the Russian Orbital Segment (ROS) and the United States Orbital Segment (USOS), which is shared by many nations. , the American portion of ISS is being funded until 2024. Roscosmos has endorsed the continued operation of ISS through 2024, but has proposed using elements of the Russian Orbital Segment to construct a new Russian space station called OPSEK.
On 28 March 2015, Russian sources announced that Roscosmos and NASA had agreed to collaborate on the development of a replacement for the current ISS. NASA later issued a guarded statement expressing thanks for Russia's interest in future cooperation in space exploration, but fell short of confirming the Russian announcement.
Purpose.
According to the original Memorandum of Understanding between NASA and Rosaviakosmos, the International Space Station was intended to be a laboratory, observatory and factory in low Earth orbit. It was also planned to provide transportation, maintenance, and act as a staging base for possible future missions to the Moon, Mars and asteroids. In the 2010 United States National Space Policy, the ISS was given additional roles of serving commercial, diplomatic and educational purposes.
Scientific research.
The ISS provides a platform to conduct scientific research. Small unmanned spacecraft can provide platforms for zero gravity and exposure to space, but space stations offer a long-term environment where studies can be performed potentially for decades, combined with ready access by human researchers over periods that exceed the capabilities of manned spacecraft.
The Station simplifies individual experiments by eliminating the need for separate rocket launches and research staff. The wide variety of research fields include astrobiology, astronomy, human research including space medicine and life sciences, physical sciences, materials science, space weather, and weather on Earth (meteorology). Scientists on Earth have access to the crew's data and can modify experiments or launch new ones, which are benefits generally unavailable on unmanned spacecraft. Crews fly expeditions of several months duration, providing approximately 160-man-hours per week of labour with a crew of 6.
To detect dark matter and answer other fundamental questions about our universe, engineers and scientists from all over the world built the Alpha Magnetic Spectrometer (AMS), which NASA compares to the Hubble space telescope, and says could not be accommodated on a free flying satellite platform partly because of its power requirements and data bandwidth needs. On 3 April 2013, NASA scientists reported that hints of dark matter may have been detected by the Alpha Magnetic Spectrometer. According to the scientists, "The first results from the space-borne Alpha Magnetic Spectrometer confirm an unexplained excess of high-energy positrons in Earth-bound cosmic rays."
The space environment is hostile to life. Unprotected presence in space is characterised by an intense radiation field (consisting primarily of protons and other subatomic charged particles from the solar wind, in addition to cosmic rays), high vacuum, extreme temperatures, and microgravity. Some simple forms of life called extremophiles, including small invertebrates called tardigrades can survive in this environment in an extremely dry state called desiccation.
Medical research improves knowledge about the effects of long-term space exposure on the human body, including muscle atrophy, bone loss, and fluid shift. This data will be used to determine whether lengthy human spaceflight and space colonisation are feasible. , data on bone loss and muscular atrophy suggest that there would be a significant risk of fractures and movement problems if astronauts landed on a planet after a lengthy interplanetary cruise, such as the six-month interval required to travel to Mars.
Medical studies are conducted aboard the ISS on behalf of the National Space Biomedical Research Institute (NSBRI). Prominent among these is the Advanced Diagnostic Ultrasound in Microgravity study in which astronauts perform ultrasound scans under the guidance of remote experts. The study considers the diagnosis and treatment of medical conditions in space. Usually, there is no physician on board the ISS and diagnosis of medical conditions is a challenge. It is anticipated that remotely guided ultrasound scans will have application on Earth in emergency and rural care situations where access to a trained physician is difficult.
Microgravity.
The Earth's gravity is only slightly weaker at the altitude of the ISS than at the surface, but objects in orbit are in a continuous state of freefall, resulting in an apparent state of weightlessness. This perceived weightlessness is disturbed by five separate effects:
Researchers are investigating the effect of the station's near-weightless environment on the evolution, development, growth and internal processes of plants and animals. In response to some of this data, NASA wants to investigate microgravity's effects on the growth of three-dimensional, human-like tissues, and the unusual protein crystals that can be formed in space.
The investigation of the physics of fluids in microgravity will allow researchers to model the behaviour of fluids better. Because fluids can be almost completely combined in microgravity, physicists investigate fluids that do not mix well on Earth. In addition, an examination of reactions that are slowed by low gravity and temperatures will give scientists a deeper understanding of superconductivity.
The study of materials science is an important ISS research activity, with the objective of reaping economic benefits through the improvement of techniques used on the ground. Other areas of interest include the effect of the low gravity environment on combustion, through the study of the efficiency of burning and control of emissions and pollutants. These findings may improve current knowledge about energy production, and lead to economic and environmental benefits. Future plans are for the researchers aboard the ISS to examine aerosols, ozone, water vapour, and oxides in Earth's atmosphere, as well as cosmic rays, cosmic dust, antimatter, and dark matter in the universe.
Exploration.
The ISS provides a location in the relative safety of Low Earth Orbit to test spacecraft systems that will be required for long-duration missions to the Moon and Mars. This provides experience in operations, maintenance as well as repair and replacement activities on-orbit, which will be essential skills in operating spacecraft farther from Earth, mission risks can be reduced and the capabilities of interplanetary spacecraft advanced. Referring to the MARS-500 experiment, ESA states that "Whereas the ISS is essential for answering questions concerning the possible impact of weightlessness, radiation and other space-specific factors, aspects such as the effect of long-term isolation and confinement can be more appropriately addressed via ground-based simulations". Sergey Krasnov, the head of human space flight programmes for Russia's space agency, Roscosmos, in 2011 suggested a "shorter version" of MARS-500 may be carried out on the ISS.
In 2009, noting the value of the partnership framework itself, Sergey Krasnov wrote, "When compared with partners acting separately, partners developing complementary abilities and resources could give us much more assurance of the success and safety of space exploration. The ISS is helping further advance near-Earth space exploration and realisation of prospective programmes of research and exploration of the Solar system, including the Moon and Mars." A manned mission to Mars may be a multinational effort involving space agencies and countries outside the current ISS partnership. In 2010, ESA Director-General Jean-Jacques Dordain stated his agency was ready to propose to the other four partners that China, India and South Korea be invited to join the ISS partnership. NASA chief Charlie Bolden stated in February 2011, "Any mission to Mars is likely to be a global effort". Currently, American legislation prevents NASA co-operation with China on space projects.
Education and cultural outreach.
The ISS crew provides opportunities for students on Earth by running student-developed experiments, making educational demonstrations, allowing for student participation in classroom versions of ISS experiments, and directly engaging students using radio, videolink and email. ESA offers a wide range of free teaching materials that can be downloaded for use in classrooms. In one lesson, students can navigate a 3-D model of the interior and exterior of the ISS, and face spontaneous challenges to solve in real time.
JAXA aims both to "Stimulate the curiosity of children, cultivating their spirits, and encouraging their passion to pursue craftsmanship", and to "Heighten the child's awareness of the importance of life and their responsibilities in society." Through a series of education guides, a deeper understanding of the past and near-term future of manned space flight, as well as that of Earth and life, will be learned. In the JAXA Seeds in Space experiments, the mutation effects of spaceflight on plant seeds aboard the ISS is explored. Students grow sunflower seeds which flew on the ISS for about nine months as a start to 'touch the Universe'. In the first phase of Kibō utilisation from 2008 to mid-2010, researchers from more than a dozen Japanese universities conducted experiments in diverse fields.
Cultural activities are another major objective. Tetsuo Tanaka, director of JAXA's Space Environment and Utilization Center, says "There is something about space that touches even people who are not interested in science."
Amateur Radio on the ISS (ARISS) is a volunteer programme which encourages students worldwide to pursue careers in science, technology, engineering and mathematics through amateur radio communications opportunities with the ISS crew. ARISS is an international working group, consisting of delegations from nine countries including several countries in Europe as well as Japan, Russia, Canada, and the United States. In areas where radio equipment cannot be used, speakerphones connect students to ground stations which then connect the calls to the station.
"First Orbit" is a feature-length documentary film about Vostok 1, the first manned space flight around the Earth. By matching the orbit of the International Space Station to that of Vostok 1 as closely as possible, in terms of ground path and time of day, documentary filmmaker Christopher Riley and ESA astronaut Paolo Nespoli were able to film the view that Yuri Gagarin saw on his pioneering orbital space flight. This new footage was cut together with the original Vostok 1 mission audio recordings sourced from the Russian State Archive. Nespoli, during Expedition 26/27, filmed the majority of the footage for this documentary film, and as a result is credited as its director of photography. The film was streamed through the website firstorbit.org in a global YouTube premiere in 2011, under a free license.
In May 2013, commander Chris Hadfield shot a music video of David Bowie's "Space Oddity" on board the station; the film was released freely on YouTube. It was the first music video ever to be filmed in space.
Assembly.
The assembly of the International Space Station, a major endeavour in space architecture, began in November 1998. Russian modules launched and docked robotically, with the exception of "Rassvet". All other modules were delivered by the Space Shuttle, which required installation by ISS and shuttle crewmembers using the Canadarm2 (SSRMS) and EVAs; , they had added 159 components during more than 1,000 hours of EVA. 127 of these spacewalks originated from the station, and the remaining 32 were launched from the airlocks of docked Space Shuttles. The beta angle of the station had to be considered at all times during construction, as the station's beta angle is directly related to the percentage of its orbit that the station (as well as any docked or docking spacecraft) is exposed to the sun; the Space Shuttle would not perform optimally above a limit called the "beta cutoff". Many of the modules that launched on the Space Shuttle were integrated and tested on the ground at the Space Station Processing Facility to find and correct issues prior to launch.
The first module of the ISS, "Zarya", was launched on 20 November 1998 on an autonomous Russian Proton rocket. It provided propulsion, attitude control, communications, electrical power, but lacked long-term life support functions. Two weeks later a passive NASA module "Unity" was launched aboard Space Shuttle flight STS-88 and attached to Zarya by astronauts during EVAs. This module has two Pressurized Mating Adapters (PMAs), one connects permanently to Zarya, the other allows the Space Shuttle to dock to the space station. At this time, the Russian station Mir was still inhabited. The ISS remained unmanned for two years, during which time Mir was de-orbited. On 12 July 2000 Zvezda was launched into orbit. Preprogrammed commands on board deployed its solar arrays and communications antenna. It then became the passive vehicle for a rendezvous with the Zarya and Unity. As a passive "target" vehicle, the Zvezda maintained a stationkeeping orbit as the Zarya-Unity vehicle performed the rendezvous and docking via ground control and the Russian automated rendezvous and docking system. Zarya's computer transferred control of the station to Zvezda's computer soon after docking. Zvezda added sleeping quarters, a toilet, kitchen, CO2 scrubbers, dehumidifier, oxygen generators, exercise equipment, plus data, voice and television communications with mission control. This enabled permanent habitation of the station.
The first resident crew, Expedition 1, arrived in November 2000 on Soyuz TM-31. At the end of the first day on the station, astronaut Bill Shepherd requested the use of the radio call sign ""Alpha"", which he and cosmonaut Krikalev preferred to the more cumbersome ""International Space Station"". The name ""Alpha"" had previously been used for the station in the early 1990s, and following the request, its use was authorised for the whole of Expedition 1. Shepherd had been advocating the use of a new name to project managers for some time. Referencing a naval tradition in a pre-launch news conference he had said: "For thousands of years, humans have been going to sea in ships. People have designed and built these vessels, launched them with a good feeling that a name will bring good fortune to the crew and success to their voyage." Yuri Semenov, the President of Russian Space Corporation Energia at the time, disapproved of the name ""Alpha""; he felt that "Mir" was the first space station, and so he would have preferred the names ""Beta"" or ""Mir 2"" for the ISS.
Expedition 1 arrived midway between the flights of STS-92 and STS-97. These two Space Shuttle flights each added segments of the station's Integrated Truss Structure, which provided the station with Ku-band communication for US television, additional attitude support needed for the additional mass of the USOS, and substantial solar arrays supplementing the station's existing 4 solar arrays.
Over the next two years, the station continued to expand. A Soyuz-U rocket delivered the "Pirs" docking compartment. The Space Shuttles "Discovery", "Atlantis", and "Endeavour" delivered the "Destiny" laboratory and "Quest" airlock, in addition to the station's main robot arm, the "Canadarm2", and several more segments of the Integrated Truss Structure.
The expansion schedule was interrupted by the disaster in 2003, with the resulting two year hiatus in the Space Shuttle programme halting station assembly. The space shuttle was grounded until 2005 with STS-114 flown by "Discovery".
Assembly resumed in 2006 with the arrival of STS-115 with "Atlantis", which delivered the station's second set of solar arrays. Several more truss segments and a third set of arrays were delivered on STS-116, STS-117, and STS-118. As a result of the major expansion of the station's power-generating capabilities, more pressurised modules could be accommodated, and the "Harmony" node and "Columbus" European laboratory were added. These were followed shortly after by the first two components of "Kibō". In March 2009, STS-119 completed the Integrated Truss Structure with the installation of the fourth and final set of solar arrays. The final section of "Kibō" was delivered in July 2009 on STS-127, followed by the Russian "Poisk" module. The third node, "Tranquility", was delivered in February 2010 during STS-130 by the Space Shuttle "Endeavour", alongside the Cupola, closely followed in May 2010 by the penultimate Russian module, "Rassvet". Rassvet was delivered by Space Shuttle "Atlantis" on STS-132 in exchange for the Russian Proton delivery of the Zarya Module in 1998 which had been funded by the United States. The last pressurised module of the USOS, "Leonardo", was brought to the station by "Discovery" on her final flight, STS-133, followed by the Alpha Magnetic Spectrometer on STS-134, delivered by "Endeavour".
, the station consisted of fifteen pressurised modules and the Integrated Truss Structure. Five modules are still to be launched, including the Bigelow Expandable Activity Module, the Nauka with the European Robotic Arm, the Uzlovoy Module, and two power modules called NEM-1 and NEM-2. When attached, the Bigelow Expandable Activity Module will be the first inflatable module to become part of a space station. , Russia's future primary research module Nauka is set to launch in February 2017, along with the European Robotic Arm which will be able to relocate itself to different parts of the Russian modules of the station. After the Nauka module is attached, the Uzlovoy Module will be attached to one of its docking ports. When completed, the station will have a mass in excess of .
The gross mass of the station changes over time. The total launch mass of the modules on orbit is about (as of 3 September 2011). The mass of experiments, spare parts, personal effects, crew, foodstuff, clothing, propellants, water supplies, gas supplies, docked spacecraft, and other items add to the total mass of the station. Hydrogen gas is constantly vented overboard by the oxygen generators.
Station structure.
The ISS is a third generation modular space station. Modular stations can allow the mission to be changed over time and new modules can be added or removed from the existing structure, allowing greater flexibility.
Below is a diagram of major station components. The blue areas are pressurised sections accessible by the crew without using spacesuits. The station's unpressurised superstructure is indicated in red. Other unpressurised components are yellow. Note that the Unity node joins directly to the Destiny laboratory. For clarity, they are shown apart.
Pressurised modules.
Zarya.
Zarya (Russian: Заря́; lit. dawn), also known as the Functional Cargo Block or FGB (from the Russian "Функционально-грузовой блок", Funktsionalno-gruzovoy blok or ФГБ), was the first module of the International Space Station to be launched. The FGB provided electrical power, storage, propulsion, and guidance to the ISS during the initial stage of assembly. With the launch and assembly in orbit of other modules with more specialized functionality, Zarya is now primarily used for storage, both inside the pressurized section and in the externally mounted fuel tanks. The Zarya is a descendant of the TKS spacecraft designed for the Soviet Salyut program. The name Zarya was given to the FGB because it signified the dawn of a new era of international cooperation in space. Although it was built by a Russian company, it is owned by the United States. Zarya weighs , is long and wide, discounting solar arrays.
Built from December 1994 to January 1998 in Russia at the Khrunichev State Research and Production Space Center (KhSC) in Moscow, Zarya's control system was developed by the Khartron Corp. (Kharkiv, Ukraine).
Zarya was launched on 20 November 1998, on a Russian Proton rocket from Baikonur Cosmodrome Site 81 in Kazakhstan to a high orbit with a designed lifetime of at least 15 years. After Zarya reached orbit, STS-88 launched on 4 December 1998, to attach the Unity Module.
Although only designed to fly autonomously for six to eight months, Zarya did so for almost two years because of delays with the Russian Service Module, Zvezda, which finally launched on 12 July 2000, and docked with Zarya on 26 July using the Russian Kurs docking system.
Unity.
Unity, or Node 1, is one of three nodes, or passive connecting modules, in the US Orbital Segment of the station. It was the first US-built component of the Station to be launched. Cylindrical in shape, with six berthing locations facilitating connections to other modules, Unity was carried into orbit by as the primary cargo of STS-88 in 1998. Essential space station resources such as fluids, environmental control and life support systems, electrical and data systems are routed through Unity to supply work and living areas of the station. More than 50,000 mechanical items, 216 lines to carry fluids and gases, and 121 internal and external electrical cables using six miles of wire were installed in the Unity node. Unity is made of aluminium. Prior to its launch aboard Endeavour, conical Pressurized Mating Adapters (PMAs) were attached to the aft and forward berthing mechanisms of Unity. Unity and the two mating adapters together weighed about . The adapters allow the docking systems used by the Space Shuttle and by Russian modules to attach to the node's hatches and berthing mechanisms.
Unity was carried into orbit as the primary cargo of the Space Shuttle Endeavour on STS-88, the first Space Shuttle mission dedicated to assembly of the station. On 6 December 1998, the STS-88 crew mated the aft berthing port of Unity with the forward hatch of the already orbiting Zarya module.
Zvezda.
Zvezda (, meaning "star"), also known as DOS-8, Service Module or SM (). It provides all of the station's critical systems, its addition rendered the station permanently habitable for the first time, adding life support for up to six crew and living quarters for two. Zvezda's DMS-R computer handles guidance, navigation and control for the entire space station. A second computer which performs the same functions will be installed in the Nauka module, FGB-2.
The hull of Zvezda was completed in February 1985, with major internal equipment installed by October 1986. The module was launched by a Proton-K rocket from Site 81/23 at Baikonur, on 12 July 2000. Zvezda is at the rear of the station according to its normal direction of travel and orientation, its engines are used to boost the station's orbit. Alternatively Russian and European spacecraft can dock to Zvezda's aft port and use their engines to boost the station.
Destiny.
Destiny is the primary research facility for United States payloads aboard the ISS. In 2011, NASA solicited proposals for a not-for-profit group to manage all American science on the station which does not relate to manned exploration. The module houses 24 International Standard Payload Racks, some of which are used for environmental systems and crew daily living equipment. "Destiny" also serves as the mounting point for the station's Truss Structure.
Quest.
Quest is the only USOS airlock, and hosts spacewalks with both United States EMU and Russian Orlan spacesuits. It consists of two segments: the equipment lock, which stores spacesuits and equipment, and the crew lock, from which astronauts can exit into space. This module has a separately controlled atmosphere. Crew sleep in this module, breathing a low nitrogen mixture the night before scheduled EVAs, to avoid decompression sickness (known as "the bends") in the low-pressure suits.
Pirs and Poisk.
Pirs (, meaning "pier"), (), "docking module", SO-1 or DC-1 (docking compartment), and Poisk (; lit. "Search"), also known as the Mini-Research Module 2 (MRM 2), , or МИМ 2. Pirs and Poisk are Russian airlock modules. Each of these modules have 2 identical hatches. An outward opening hatch on the MIR space station failed after it swung open too fast after unlatching, because of a small amount of air pressure remaining in the airlock. A different entry was used, and the hatch repaired. All EVA hatches on the ISS open inwards and are pressure sealing. Pirs was used to store, service, and refurbish Russian Orlan suits and provided contingency entry for crew using the slightly bulkier American suits. The outermost docking ports on both airlocks allow docking of Soyuz and Progress spacecraft, and the automatic transfer of propellants to and from storage on the ROS.
Harmony.
Harmony is the second of the station's node modules and the utility hub of the USOS. The module contains four racks that provide electrical power, bus electronic data, and acts as a central connecting point for several other components via its six Common Berthing Mechanisms (CBMs). The European Columbus and Japanese Kibō laboratories are permanently berthed to the starboard and port radial ports respectively. The nadir and zenith ports can be used for docking visiting spacecraft including HTV, Dragon, and Cygnus, with the nadir port serving as the primary docking port. American Shuttle Orbiters docked with the ISS via PMA-2, attached to the forward port.
Tranquility.
Tranquility is the third and last of the station's US nodes, it contains an additional life support system to recycle waste water for crew use and supplements oxygen generation. Like the other US nodes, it has six berthing mechanisms, five of which are currently in use. The first one connects to the station's core via the Unity module, others host the "Cupola", the PMA docking port #3, the "Leonardo" PMM and the Bigelow Expandable Activity Module. The final zenith port remains free.
Columbus.
Columbus, the primary research facility for European payloads aboard the ISS, provides a generic laboratory as well as facilities specifically designed for biology, biomedical research and fluid physics. Several mounting locations are affixed to the exterior of the module, which provide power and data to external experiments such as the European Technology Exposure Facility (EuTEF), Solar Monitoring Observatory, Materials International Space Station Experiment, and Atomic Clock Ensemble in Space. A number of expansions are planned for the module to study quantum physics and cosmology. ESA's development of technologies on all the main areas of life support has been ongoing for more than 20 years and are/have been used in modules such as Columbus and the ATV. The German Aerospace Center DLR manages ground control operations for Columbus and the ATV is controlled from the French CNES Toulouse Space Center.
Kibō.
Kibō (, "hope") is the largest single ISS module. This laboratory is used to carry out research in space medicine, biology, Earth observations, materials production, biotechnology, communications research, and has facilities for growing plants and fish. During August 2011, the MAXI observatory mounted on Kibō, which utilises the ISS's orbital motion to image the whole sky in the X-ray spectrum, detected for the first time the moment a star was swallowed by a black hole. The laboratory contains a total of 23 racks, including 10 experiment racks and has a dedicated airlock for experiments. In a 'shirt sleeves' environment, crew attach an experiment to the sliding drawer within the airlock, close the inner, and then open the outer hatch. By extending the drawer and removing the experiment using the dedicated robotic arm, payloads are placed on the external platform. The process can be reversed and repeated quickly, allowing access to maintain external experiments without the delays caused by EVAs.
A smaller pressurised module is attached to the top of Kibō, serving as a cargo bay. The dedicated Interorbital communications system allows large amounts of data to be beamed from Kibō's ICS, first to the Japanese KODAMA satellite in geostationary orbit, then to Japanese ground stations. When a direct communication link is used, contact time between the ISS and a ground station is limited to approximately 10 minutes per visible pass. When KODAMA relays data between a LEO spacecraft and a ground station, real-time communications are possible in 60% of the flight path of the spacecraft. Ground staff use telepresence robotics to conduct on-orbit research without crew intervention.
Cupola.
Cupola is a seven window observatory, used to view Earth and docking spacecraft. Its name derives from the Italian word cupola, which means "dome". The Cupola project was started by NASA and Boeing, but cancelled because of budget cuts. A barter agreement between NASA and the ESA resulted in the Cupola's development being resumed in 1998 by the ESA. It was built by Thales Alenia Space in Torino, Italy. The module comes equipped with robotic workstations for operating the station's main robotic arm and shutters to protect its windows from damage caused by micrometeorites. It features 7 windows, with an round window, the largest window on the station (and the largest flown in space to date). The distinctive design has been compared to the 'turret' of the fictitious "Millennium Falcon" from the motion picture "Star Wars"; the original prop lightsaber used by actor Mark Hamill as Luke Skywalker in the 1977 film was flown to the station in 2007.
Rassvet.
Rassvet (; lit. "dawn"), also known as the Mini-Research Module 1 (MRM-1) (, ) and formerly known as the Docking Cargo Module (DCM), is similar in design to the Mir Docking Module launched on STS-74 in 1995. "Rassvet" is primarily used for cargo storage and as a docking port for visiting spacecraft. It was flown to the ISS aboard NASA's on the STS-132 mission and connected in May 2010, Rassvet is the only Russian owned module launched by NASA, to repay for the launch of Zarya, which is Russian designed and built, but partially paid for by NASA. Rassvet was launched with the Russian Nauka Laboratory's Experiments airlock temporarily attached to it, and spare parts for the European Robotic Arm.
Leonardo.
"Leonardo" Permanent Multipurpose Module (PMM) is a storage module attached to the "Tranquility" node. The three NASA Space Shuttle MPLM cargo containers—Leonardo, Raffaello and Donatello—were built for NASA in Turin, Italy by Alcatel Alenia Space, now Thales Alenia Space. The MPLMs were provided to NASA's ISS programme by Italy (independent of their role as a member state of ESA) and are considered to be US elements. In a bartered exchange for providing these containers, the US gave Italy research time aboard the ISS out of the US allotment in addition to that which Italy receives as a member of ESA. The Permanent Multipurpose Module was created by converting Leonardo into a module that could be permanently attached to the station.
Bigelow Expandable Activity Module.
On 16 January 2013, Bigelow Aerospace was contracted by NASA to provide a Bigelow Expandable Activity Module (BEAM) for a two-year technology demonstration. BEAM was delivered to the ISS aboard SpaceX CRS-8 on 10 April 2016, and was berthed to the aft port of the Tranquility node on 16 April.
During its two-year test run, instruments are expected to measure its structural integrity and leak rate, along with temperature and radiation levels. The hatch leading into the module will remain mostly closed except for periodic visits by space station crew members for inspections and data collection. Following the test run, the module will be detached and jettisoned from the station.
Scheduled additional modules.
Nauka.
Nauka (; lit. "science"), also known as the Multipurpose Laboratory Module (MLM) or FGB-2 (, ), is the major Russian laboratory module. It was scheduled to arrive at the station in 2014, docking to the port that was occupied by the Pirs module. Due to deteriorations following many years spent in storage, it proved necessary to build a new propulsion module, and the launch date was postponed to November 2017. Prior to the arrival of the Nauka module, a Progress spacecraft will remove Pirs from the station, deorbiting it to reenter over the Pacific Ocean. Nauka contains an additional set of life support systems and attitude control. Originally it would have routed power from the single Science-and-Power Platform, but that single module design changed over the first ten years of the ISS mission, and the two science modules, which attach to Nauka via the Uzlovoy Module, or Russian node, each incorporate their own large solar arrays to power Russian science experiments in the ROS.
Nauka's mission has changed over time. During the mid-1990s, it was intended as a backup for the FGB, and later as a universal docking module (UDM); its docking ports will be able to support automatic docking of both spacecraft, additional modules and fuel transfer. Nauka has its own engines. Smaller Russian modules such as Pirs and Poisk were delivered by modified Progress spacecraft, and the larger modules; Zvezda, Zarya, and Nauka, were launched by Proton rockets. Russia plans to separate Nauka, along with the rest of the Russian Orbital Segment, before the ISS is deorbited, to form the OPSEK space station.
Uzlovoy Module.
The Uzlovoy Module (UM), or Node Module is a 4 metric ton ball shaped module that will support the docking of two scientific and power modules during the final stage of the station assembly and provide the Russian segment additional docking ports to receive Soyuz TMA and Progress M spacecraft. UM is to be incorporated into the ISS in 2016. It will be integrated with a special version of the Progress cargo ship and launched by a standard Soyuz rocket. The Progress would use its own propulsion and flight control system to deliver and dock the Node Module to the nadir (Earth-facing) docking port of the Nauka MLM/FGB-2 module. One port is equipped with an active hybrid docking port, which enables docking with the MLM module. The remaining five ports are passive hybrids, enabling docking of Soyuz and Progress vehicles, as well as heavier modules and future spacecraft with modified docking systems. The node module was conceived to serve as the only permanent element of the future Russian successor to the ISS, OPSEK. Equipped with six docking ports, the Node Module would serve as a single permanent core of the future station with all other modules coming and going as their life span and mission required. This would be a progression beyond the ISS and Russia's modular MIR space station, which are in turn more advanced than early monolithic first generation stations such as Skylab, and early Salyut and Almaz stations.
Science Power Modules 1 & 2 (NEM-1, NEM-2) ()
Cancelled components.
Several modules planned for the station have been cancelled over the course of the ISS programme, whether for budgetary reasons, because the modules became unnecessary, or following a redesign of the station after the 2003 "Columbia" disaster. The US Centrifuge Accommodations Module was intended to host science experiments in varying levels of artificial gravity. The US Habitation Module would have served as the station's living quarters. Instead, the sleep stations are now spread throughout the station. The US Interim Control Module and ISS Propulsion Module were intended to replace functions of "Zvezda" in case of a launch failure. The Russian Universal Docking Module, to which the cancelled Russian Research modules and spacecraft would have docked. The Russian Science Power Platform would have provided the Russian Orbital Segment with a power supply independent of the ITS solar arrays, and two Russian Research Modules that were planned to be used for scientific research.
Unpressurised elements.
The ISS features a large number of external components that do not require pressurisation. The largest such component is the Integrated Truss Structure (ITS), to which the station's main solar arrays and thermal radiators are mounted. The ITS consists of ten separate segments forming a structure 108.5 m (356 ft) long.
The station in its complete form has several smaller external components, such as the six robotic arms, the three External Stowage Platforms (ESPs) and four ExPRESS Logistics Carriers (ELCs). Whilst these platforms allow experiments (including MISSE, the STP-H3 and the Robotic Refueling Mission) to be deployed and conducted in the vacuum of space by providing electricity and processing experimental data locally, the platforms' primary function is to store Orbital Replacement Units (ORUs). ORUs are spare parts that can be replaced when the item either passes its design life or fails. Examples of ORUs include pumps, storage tanks, antennas and battery units. Such units are replaced either by astronauts during EVA or by robotic arms. Spare parts were routinely transported to and from the station via Space Shuttle resupply missions, with a heavy emphasis on ORU transport once the NASA Shuttle approached retirement. Several shuttle missions were dedicated to the delivery of ORUs, including STS-129, STS-133 and STS-134. , only one other mode of transportation of ORUs had been utilised – the Japanese cargo vessel HTV-2 – which delivered an FHRC and CTC-2 via its Exposed Pallet (EP).
There are also smaller exposure facilities mounted directly to laboratory modules; the JEM Exposed Facility serves as an external 'porch' for the Japanese Experiment Module complex, and a facility on the European "Columbus" laboratory provides power and data connections for experiments such as the European Technology Exposure Facility and the Atomic Clock Ensemble in Space. A remote sensing instrument, SAGE III-ISS, was delivered to the station in 2014 aboard a Dragon capsule, and the NICER experiment is scheduled to be delivered in 2016. The largest such scientific payload externally mounted to the ISS is the Alpha Magnetic Spectrometer (AMS), a particle physics experiment launched on STS-134 in May 2011, and mounted externally on the ITS. The AMS measures cosmic rays to look for evidence of dark matter and antimatter.
Robotic arms and cargo cranes.
The Integrated Truss Structure serves as a base for the station's primary remote manipulator system, called the Mobile Servicing System (MSS), which is composed of three main components. Canadarm2, the largest robotic arm on the ISS, has a mass of and is used to dock and manipulate spacecraft and modules on the USOS, hold crew members and equipment in place during EVAs and move Dextre around to perform tasks. Dextre is a robotic manipulator with two arms, a rotating torso and has power tools, lights and video for replacing orbital replacement units (ORUs) and performing other tasks requiring fine control. The Mobile Base System (MBS) is a platform which rides on rails along the length of the station's main truss. It serves as a mobile base for Canadarm2 and Dextre, allowing the robotic arms to reach all parts of the USOS. To gain access to the Russian Segment a grapple fixture was added to Zarya on STS-134, so that Canadarm2 can inchworm itself onto the ROS. Also installed during STS-134 was the Orbiter Boom Sensor System (OBSS), which had been used to inspect head shield tiles on Space Shuttle missions and can be used on station to increase the reach of the MSS. Staff on Earth or the station can operate the MSS components via remote control, performing work outside the station without space walks.
Japan's Remote Manipulator System, which services the JEM Exposed Facility, was launched on STS-124 and is attached to the JEM Pressurised Module. The arm is similar to the Space Shuttle arm as it is permanently attached at one end and has a latching end effector for standard grapple fixtures at the other.
The European Robotic Arm, which will service the Russian Orbital Segment, will be launched alongside the Multipurpose Laboratory Module in 2017. The ROS does not require spacecraft or modules to be manipulated, as all spacecraft and modules dock automatically and may be discarded the same way. Crew use the two "Strela" (; lit. Arrow) cargo cranes during EVAs for moving crew and equipment around the ROS. Each Strela crane has a mass of .
Comparison.
The ISS follows Salyut and Almaz series, Cosmos 557, Skylab, and Mir as the 11th space station launched, as the Genesis prototypes were never intended to be manned. Other examples of modular station projects include the Soviet/Russian Mir and the planned Russian OPSEK and Chinese space station. The first space station, Salyut 1, and other one-piece or 'monolithic' first generation space stations, such as Salyut 2,3,4,5, DOS 2, Kosmos 557, Almaz and NASA's Skylab stations were not designed for re-supply. Generally, each crew had to depart the station to free the only docking port for the next crew to arrive, Skylab had more than one docking port but was not designed for resupply. Salyut 6 and 7 had more than one docking port and were designed to be resupplied routinely during crewed operation.
Station systems.
Life support.
The critical systems are the atmosphere control system, the water supply system, the food supply facilities, the sanitation and hygiene equipment, and fire detection and suppression equipment. The Russian Orbital Segment's life support systems are contained in the Service Module Zvezda. Some of these systems are supplemented by equipment in the USOS. The MLM Nauka laboratory has a complete set of life support systems.
Atmospheric control systems.
The atmosphere on board the ISS is similar to the Earth's. Normal air pressure on the ISS is 101.3 kPa (14.7 psi); the same as at sea level on Earth. An Earth-like atmosphere offers benefits for crew comfort, and is much safer than the alternative, a pure oxygen atmosphere, because of the increased risk of a fire such as that responsible for the deaths of the Apollo 1 crew. Earth-like atmospheric conditions have been maintained on all Russian and Soviet spacecraft.
The "Elektron" system aboard "Zvezda" and a similar system in "Destiny" generate oxygen aboard the station. The crew has a backup option in the form of bottled oxygen and Solid Fuel Oxygen Generation (SFOG) canisters, a chemical oxygen generator system. Carbon dioxide is removed from the air by the Vozdukh system in "Zvezda". Other by-products of human metabolism, such as methane from the intestines and ammonia from sweat, are removed by activated charcoal filters.
Part of the ROS atmosphere control system is the oxygen supply, triple-redundancy is provided by the Elektron unit, solid fuel generators, and stored oxygen. The Elektron unit is the primary oxygen supply, and are produced by electrolysis, with the being vented overboard. The 1 kW system uses approximately 1 litre of water per crew member per day from stored water from Earth, or water recycled from other systems. MIR was the first spacecraft to use recycled water for oxygen production. The secondary oxygen supply is provided by burning -producing Vika cartridges (see also ISS ECLSS). Each 'candle' takes 5–20 minutes to decompose at 450–500 °C, producing 600 litres of . This unit is manually operated.
The US Orbital Segment has redundant supplies of oxygen, from a pressurised storage tank on the Quest airlock module delivered in 2001, supplemented ten years later by ESA built Advanced Closed-Loop System (ACLS) in the Tranquility module (Node 3), which produces by electrolysis. Hydrogen produced is combined with carbon dioxide from the cabin atmosphere and converted to water and methane.
Power and thermal control.
Double-sided solar, or Photovoltaic arrays, provide electrical power for the ISS. These bifacial cells are more efficient and operate at a lower temperature than single-sided cells commonly used on Earth, by collecting sunlight on one side and light reflected off the Earth on the other. 
The Russian segment of the station, like the Space Shuttle and most spacecraft, uses 28 volt DC from four rotating solar arrays mounted on "Zarya" and "Zvezda". The USOS uses 130–180 V DC from the USOS PV array, power is stabilised and distributed at 160 V DC and converted to the user-required 124 V DC. The higher distribution voltage allows smaller, lighter conductors, at the expense of crew safety. The ROS uses low voltage. The two station segments share power with converters.
The USOS solar arrays are arranged as four wing pairs, with each wing producing nearly 32.8 kW. These arrays normally track the sun to maximise power generation. Each array is about 375 m2 (450 yd2) in area and long. In the complete configuration, the solar arrays track the sun by rotating the "alpha gimbal" once per orbit; the "beta gimbal" follows slower changes in the angle of the sun to the orbital plane. The Night Glider mode aligns the solar arrays parallel to the ground at night to reduce the significant aerodynamic drag at the station's relatively low orbital altitude.
The station uses rechargeable nickel-hydrogen batteries (NiH2) for continuous power during the 35 minutes of every 90-minute orbit that it is eclipsed by the Earth. The batteries are recharged on the day side of the Earth. They have a 6.5-year lifetime (over 37,000 charge/discharge cycles) and will be regularly replaced over the anticipated 20-year life of the station.
The station's large solar panels generate a high potential voltage difference between the station and the ionosphere. This could cause arcing through insulating surfaces and sputtering of conductive surfaces as ions are accelerated by the spacecraft plasma sheath. To mitigate this, plasma contactor units (PCU)s create current paths between the station and the ambient plasma field.
The large amount of electrical power consumed by the station's systems and experiments is turned almost entirely into heat. The heat which can be dissipated through the walls of the stations modules is insufficient to keep the internal ambient temperature within comfortable, workable limits. Ammonia is continuously pumped through pipework throughout the station to collect heat, then into external radiators exposed to the cold of space, and back into the station.
The International Space Station (ISS) External Active Thermal Control System (EATCS) maintains an equilibrium when the ISS environment or heat loads exceed the capabilities of the Passive Thermal Control System (PTCS). Note Elements of the PTCS are external surface materials, insulation such as MLI, or Heat Pipes. The EATCS provides heat rejection capabilities for all the US pressurised modules, including the JEM and COF as well as the main power distribution electronics of the S0, S1 and P1 Trusses. The EATCS consists an internal, non-toxic, water coolant loop used to cool and dehumidify the atmosphere, which transfers collected heat into an external liquid ammonia loop capable of withstanding the much lower temperature of space, which is then circulated through radiators to remove the heat. The EATCS is capable of rejecting up to 70 kW, and provides a substantial upgrade in heat rejection capacity from the 14 kW capability of the Early External Active Thermal Control System (EEATCS) via the Early Ammonia Servicer (EAS), which was launched on STS-105 and installed onto the P6 Truss.
Communications and computers.
Radio communications provide telemetry and scientific data links between the station and Mission Control Centres. Radio links are also used during rendezvous and docking procedures and for audio and video communication between crewmembers, flight controllers and family members. As a result, the ISS is equipped with internal and external communication systems used for different purposes.
The Russian Orbital Segment communicates directly with the ground via the "Lira" antenna mounted to "Zvezda". The "Lira" antenna also has the capability to use the "Luch" data relay satellite system. This system, used for communications with "Mir", fell into disrepair during the 1990s, and as a result is no longer in use, although two new "Luch" satellites—"Luch"-5A and "Luch"-5B—were launched in 2011 and 2012 respectively to restore the operational capability of the system. Another Russian communications system is the Voskhod-M, which enables internal telephone communications between "Zvezda", "Zarya", "Pirs", "Poisk" and the USOS, and also provides a VHF radio link to ground control centres via antennas on "Zvezda" exterior.
The US Orbital Segment (USOS) makes use of two separate radio links mounted in the Z1 truss structure: the S band (used for audio) and Ku band (used for audio, video and data) systems. These transmissions are routed via the United States Tracking and Data Relay Satellite System (TDRSS) in geostationary orbit, which allows for almost continuous real-time communications with NASA's Mission Control Center (MCC-H) in Houston. Data channels for the Canadarm2, European "Columbus" laboratory and Japanese "Kibō" modules are routed via the S band and Ku band systems, although the European Data Relay System and a similar Japanese system will eventually complement the TDRSS in this role. Communications between modules are carried on an internal digital wireless network.
UHF radio is used by astronauts and cosmonauts conducting EVAs. UHF is employed by other spacecraft that dock to or undock from the station, such as Soyuz, Progress, HTV, ATV and the Space Shuttle (except the shuttle also makes use of the S band and Ku band systems via TDRSS), to receive commands from Mission Control and ISS crewmembers. Automated spacecraft are fitted with their own communications equipment; the ATV uses a laser attached to the spacecraft and equipment attached to "Zvezda", known as the Proximity Communications Equipment, to accurately dock to the station.
The ISS is equipped with approximately 100 IBM and Lenovo ThinkPad model A31 and T61P laptop computers. Each computer is a commercial off-the-shelf purchase which is then modified for safety and operation including updates to connectors, cooling and power to accommodate the station's 28V DC power system and weightless environment. Heat generated by the laptops does not rise, but stagnates surrounding the laptop, so additional forced ventilation is required. Laptops aboard the ISS are connected to the station's wireless LAN via Wi-Fi and to the ground via Ku band. This provides speeds of 10 Mbit/s to and 3 Mbit/s from the station, comparable to home DSL connection speeds.
The operating system used for key station functions is the Debian Linux distribution. The migration from Microsoft Windows was made in May 2013 for reasons of reliability, stability and flexibility.
Station operations.
Expeditions and private flights.
"See also the list of International Space Station expeditions (professional crew), space tourism (private travellers), and the list of human spaceflights to the ISS (both)."
Each permanent crew is given an expedition number. Expeditions run up to six months, from launch until undocking, an 'increment' covers the same time period, but includes cargo ships and all activities. Expeditions 1 to 6 consisted of 3 person crews, Expeditions 7 to 12 were reduced to the safe minimum of two following the destruction of the NASA Shuttle Columbia. From Expedition 13 the crew gradually increased to 6 around 2010. With the arrival of the American Commercial Crew vehicles in the middle of the 2010s, expedition size may be increased to seven crew members, the number ISS is designed for.
Sergei Krikalev, member of Expedition 1 and Commander of Expedition 11, has spent more time in space than anyone else, a total of 803 days and 9 hours and 39 minutes. His awards include the Order of Lenin, Hero of the Soviet Union, Hero of the Russian Federation, and 4 NASA medals. On 16 August 2005 at 1:44 am EDT, he passed the record of 748 days held by Sergei Avdeyev, who had 'time travelled' 1/50th of a second into the future on board MIR. He participated in psychosocial experiment SFINCSS-99 (Simulation of Flight of International Crew on Space Station), which examined inter-cultural and other stress factors affecting integration of crew in preparation for the ISS spaceflights. Scott Kelly has spent the most time in space of any American. Kelly returned from the ISS on March 1, 2016 having spent 340 consecutive days in orbit.
Travellers who pay for their own passage into space are termed spaceflight participants by Roscosmos and NASA, and are sometimes informally referred to as space tourists, a term they generally dislike. All seven were transported to the ISS on Russian Soyuz spacecraft. When professional crews change over in numbers not divisible by the three seats in a Soyuz, and a short-stay crewmember is not sent, the spare seat is sold by MirCorp through Space Adventures. When the space shuttle retired in 2011, and the station's crew size was reduced to 6, space tourism was halted, as the partners relied on Russian transport seats for access to the station. Soyuz flight schedules increase after 2013, allowing 5 Soyuz flights (15 seats) with only two expeditions (12 seats) required. The remaining seats are sold for around to members of the public who can pass a medical exam. ESA and NASA criticised private spaceflight at the beginning of the ISS, and NASA initially resisted training Dennis Tito, the first man to pay for his own passage to the ISS. Toyohiro Akiyama was flown to Mir for a week, he was classed as a business traveller, as his employer, Tokyo Broadcasting System, paid for his ticket, and he gave a daily TV broadcast from orbit.
Anousheh Ansari () became the first Iranian in space and the first self-funded woman to fly to the station. Officials reported that her education and experience make her much more than a tourist, and her performance in training had been "excellent." Ansari herself dismisses the idea that she is a tourist. She did Russian and European studies involving medicine and microbiology during her 10-day stay. The documentary "Space Tourists" follows her journey to the station, where she fulfilled "an age-old dream of man: to leave our planet as a «normal person» and travel into outer space."
In 2008, spaceflight participant Richard Garriott placed a geocache aboard the ISS during his flight. This is currently the only non-terrestrial geocache in existence.
Orbit.
The ISS is maintained in a nearly circular orbit with a minimum mean altitude of 330 km (205 mi) and a maximum of 410 km (255 mi), in the centre of the thermosphere, at an inclination of 51.6 degrees to Earth's equator, necessary to ensure that Russian Soyuz and Progress spacecraft launched from the Baikonur Cosmodrome may be safely launched to reach the station. Spent rocket stages must be dropped into uninhabited areas and this limits the directions rockets can be launched from the spaceport. The orbital inclination chosen was also low enough to allow American space shuttles launched from Florida to reach the ISS.
It travels at an average speed of , and completes  orbits per day (93 minutes per orbit). The station's altitude was allowed to fall around the time of each NASA shuttle mission. Orbital boost burns would generally be delayed until after the shuttle's departure. This allowed shuttle payloads to be lifted with the station's engines during the routine firings, rather than have the shuttle lift itself and the payload together to a higher orbit. This trade-off allowed heavier loads to be transferred to the station. After the retirement of the NASA shuttle, the nominal orbit of the space station was raised in altitude. Other, more frequent supply ships do not require this adjustment as they are substantially lighter vehicles.
Orbital boosting can be performed by the station's two main engines on the "Zvezda" service module, or Russian or European spacecraft docked to Zvezda's aft port. The ATV has been designed with the possibility of adding a second docking port to its other end, allowing it to remain at the ISS and still allow other craft to dock and boost the station. It takes approximately two orbits (three hours) for the boost to a higher altitude to be completed. In December 2008 NASA signed an agreement with the Ad Astra Rocket Company which may result in the testing on the ISS of a VASIMR plasma propulsion engine. This technology could allow station-keeping to be done more economically than at present.
The Russian Orbital Segment contains the Data Management System, which handles Guidance, Navigation and Control (ROS GNC) for the entire station. Initially, Zarya, the first module of the station, controlled the station until a short time after the Russian service module Zvezda docked and was transferred control. Zvezda contains the ESA built DMS-R Data Management System. Using two fault-tolerant computers (FTC), Zvezda computes the station's position and orbital trajectory using redundant Earth horizon sensors, Solar horizon sensors as well as Sun and star trackers. The FTCs each contain three identical processing units working in parallel and provide advanced fault-masking by majority voting. Zvezda uses gyroscopes and thrusters to turn itself around. Gyroscopes do not require propellant, rather they use electricity to 'store' momentum in flywheels by turning in the opposite direction to the station's movement. The USOS has its own computer controlled gyroscopes to handle the extra mass of that section. When gyroscopes 'saturate', reaching their maximum speed, thrusters are used to cancel out the stored momentum. During Expedition 10, an incorrect command was sent to the station's computer, using about 14 kilograms of propellant before the fault was noticed and fixed. When attitude control computers in the ROS and USOS fail to communicate properly, it can result in a rare 'force fight' where the ROS GNC computer must ignore the USOS counterpart, which has no thrusters. When an ATV, NASA Shuttle, or Soyuz is docked to the station, it can also be used to maintain station attitude such as for troubleshooting. Shuttle control was used exclusively during installation of the S3/S4 truss, which provides electrical power and data interfaces for the station's electronics.
Mission controls.
The components of the ISS are operated and monitored by their respective space agencies at mission control centres across the globe, including:
Repairs.
Orbital Replacement Units (ORUs) are spare parts that can be readily replaced when a unit either passes its design life or fails. Examples of ORUs are pumps, storage tanks, controller boxes, antennas, and battery units. Some units can be replaced using robotic arms. Many are stored outside the station, either on small pallets called ExPRESS Logistics Carriers (ELCs) or share larger platforms called External Stowage Platforms which also hold science experiments. Both kinds of pallets have electricity as many parts which could be damaged by the cold of space require heating. The larger logistics carriers also have computer local area network connections (LAN) and telemetry to connect experiments. A heavy emphasis on stocking the USOS with ORU's occurred around 2011, before the end of the NASA shuttle programme, as its commercial replacements, Cygnus and Dragon, carry one tenth to one quarter the payload.
Unexpected problems and failures have impacted the station's assembly time-line and work schedules leading to periods of reduced capabilities and, in some cases, could have forced abandonment of the station for safety reasons, had these problems not been resolved. During STS-120 in 2007, following the relocation of the P6 truss and solar arrays, it was noted during the redeployment of the array that it had become torn and was not deploying properly. An EVA was carried out by Scott Parazynski, assisted by Douglas Wheelock. The men took extra precautions to reduce the risk of electric shock, as the repairs were carried out with the solar array exposed to sunlight. The issues with the array were followed in the same year by problems with the starboard Solar Alpha Rotary Joint (SARJ), which rotates the arrays on the starboard side of the station. Excessive vibration and high-current spikes in the array drive motor were noted, resulting in a decision to substantially curtail motion of the starboard SARJ until the cause was understood. Inspections during EVAs on STS-120 and STS-123 showed extensive contamination from metallic shavings and debris in the large drive gear and confirmed damage to the large metallic race ring at the heart of the joint, and so the joint was locked to prevent further damage. Repairs to the joint were carried out during STS-126 with lubrication of both joints and the replacement of 11 out of 12 trundle bearings on the joint.
2009 saw damage to the S1 radiator, one of the components of the station's cooling system. The problem was first noticed in Soyuz imagery in September 2008, but was not thought to be serious. The imagery showed that the surface of one sub-panel has peeled back from the underlying central structure, possibly because of micro-meteoroid or debris impact. It is also known that a Service Module thruster cover, jettisoned during an EVA in 2008, had struck the S1 radiator, but its effect, if any, has not been determined. On 15 May 2009 the damaged radiator panel's ammonia tubing was mechanically shut off from the rest of the cooling system by the computer-controlled closure of a valve. The same valve was used immediately afterwards to vent the ammonia from the damaged panel, eliminating the possibility of an ammonia leak from the cooling system via the damaged panel.
Early on 1 August 2010, a failure in cooling Loop A (starboard side), one of two external cooling loops, left the station with only half of its normal cooling capacity and zero redundancy in some systems. The problem appeared to be in the ammonia pump module that circulates the ammonia cooling fluid. Several subsystems, including two of the four CMGs, were shut down.
Planned operations on the ISS were interrupted through a series of EVAs to address the cooling system issue. A first EVA on 7 August 2010, to replace the failed pump module, was not fully completed because of an ammonia leak in one of four quick-disconnects. A second EVA on 11 August successfully removed the failed pump module. A third EVA was required to restore Loop A to normal functionality.
The USOS's cooling system is largely built by the American company Boeing, which is also the manufacturer of the failed pump.
An air leak from the USOS in 2004, the venting of fumes from an "Elektron" oxygen generator in 2006, and the failure of the computers in the ROS in 2007 during STS-117 left the station without thruster, "Elektron", "Vozdukh" and other environmental control system operations, the root cause of which was found to be condensation inside the electrical connectors leading to a short-circuit.
The four Main Bus Switching Units (MBSUs, located in the S0 truss), control the routing of power from the four solar array wings to the rest of the ISS. In late 2011 MBSU-1, while still routing power correctly, ceased responding to commands or sending data confirming its health, and was scheduled to be swapped out at the next available EVA. In each MBSU, two power channels feed 160V DC from the arrays to two DC-to-DC power converters (DDCUs) that supply the 124V power used in the station. A spare MBSU was already on board, but 30 August 2012 EVA failed to be completed when a bolt being tightened to finish installation of the spare unit jammed before electrical connection was secured. The loss of MBSU-1 limits the station to 75% of its normal power capacity, requiring minor limitations in normal operations until the problem can be addressed.
On 5 September 2012, in a second, 6 hr, EVA to replace MBSU-1, astronauts Sunita Williams and Akihiko Hoshide successfully restored the ISS to 100% power.
On 24 December 2013, astronauts made a rare Christmas Eve space walk, installing a new ammonia pump for the station's cooling system. The faulty cooling system had failed earlier in the month, halting many of the station's science experiments. Astronauts had to brave a "mini blizzard" of ammonia while installing the new pump. It was only the second Christmas Eve spacewalk in NASA history.
Fleet operations.
A wide variety of manned and unmanned spacecraft have supported the station's activities. More than 60 Progress spacecraft, including M-MIM2 and M-SO1 which installed modules, and more than 40 Soyuz spacecraft have flown to the ISS. 35 flights of the retired NASA Space Shuttle were made to the station. There have been five European ATV, five Japanese HTV 'Kounotori', eight SpaceX Dragon and four Orbital ATK Cygnus flights.
Currently docked/berthed.
"See also the list of professional crew, private travellers, both or just unmanned spaceflights."
Docking.
All Russian spacecraft and self-propelled modules are able to rendezvous and dock to the space station without human intervention using the Kurs docking system. Radar allows these vehicles to detect and intercept ISS from over 200 kilometres away. The European ATV uses star sensors and GPS to determine its intercept course. When it catches up it then uses laser equipment to optically recognise Zvezda, along with the Kurs system for redundancy. Crew supervise these craft, but do not intervene except to send abort commands in emergencies. The Japanese H-II Transfer Vehicle parks itself in progressively closer orbits to the station, and then awaits 'approach' commands from the crew, until it is close enough for a robotic arm to grapple and berth the vehicle to the USOS. The American Space Shuttle was manually docked, and on missions with a cargo container, the container would be berthed to the Station with the use of manual robotic arms. Berthed craft can transfer International Standard Payload Racks. Japanese spacecraft berth for one to two months. Russian and European Supply craft can remain at the ISS for six months, allowing great flexibility in crew time for loading and unloading of supplies and trash. NASA Shuttles could remain docked for 11–12 days.
The American manual approach to docking allows greater initial flexibility and less complexity. The downside to this mode of operation is that each mission becomes unique and requires specialised training and planning, making the process more labour-intensive and expensive. The Russians pursued an automated methodology that used the crew in override or monitoring roles. Although the initial development costs were high, the system has become very reliable with standardisations that provide significant cost benefits in repetitive routine operations. An automated approach could allow assembly of modules orbiting other worlds prior to crew arrival.
Soyuz spacecraft used for crew rotation also serve as lifeboats for emergency evacuation; they are replaced every six months and have been used once to remove excess crew after the Columbia disaster. Expeditions require, on average, of supplies, and , crews had consumed a total of around . Soyuz crew rotation flights and Progress resupply flights visit the station on average two and three times respectively each year, with the ATV and HTV planned to visit annually from 2010 onwards. Cygnus and Dragon were contracted to fly cargo to the station after retirement of the NASA Shuttle.
From 26 February 2011 to 7 March 2011 four of the governmental partners (United States, ESA, Japan and Russia) had their spacecraft (NASA Shuttle, ATV, HTV, Progress and Soyuz) docked at the ISS, the only time this has happened to date. On 25 May 2012, SpaceX became the world's first privately held company to send cargo, via the Dragon spacecraft, to the International Space Station.
Launch and docking windows.
Prior to a ship's docking to the ISS, navigation and attitude control (GNC) is handed over to the ground control of the ships' country of origin. GNC is set to allow the station to drift in space, rather than fire its thrusters or turn using gyroscopes. The solar panels of the station are turned edge-on to the incoming ships, so residue from its thrusters does not damage the cells. When a NASA shuttle docked to the station, other ships were grounded, as the carbon wingtips, cameras, windows, and instruments aboard the shuttle were at too much risk from damage from thruster residue from other ships movements.
Approximately 30% of NASA shuttle launch delays were caused by poor weather. Occasional priority was given to the Soyuz arrivals at the station where the Soyuz carried crew with time-critical cargoes such as biological experiment materials, also causing shuttle delays. Departure of the NASA shuttle was often delayed or prioritised according to weather over its two landing sites. Whilst the Soyuz is capable of landing anywhere, anytime, its planned landing time and place is chosen to give consideration to helicopter pilots and ground recovery crew, to give acceptable flying weather and lighting conditions. Soyuz launches occur in adverse weather conditions, but the cosmodrome has been shut down on occasions when buried by snow drifts up to 6 metres in depth, hampering ground operations.
Life aboard.
Crew activities.
A typical day for the crew begins with a wake-up at 06:00, followed by post-sleep activities and a morning inspection of the station. The crew then eats breakfast and takes part in a daily planning conference with Mission Control before starting work at around 08:10. The first scheduled exercise of the day follows, after which the crew continues work until 13:05. Following a one-hour lunch break, the afternoon consists of more exercise and work before the crew carries out its pre-sleep activities beginning at 19:30, including dinner and a crew conference. The scheduled sleep period begins at 21:30. In general, the crew works ten hours per day on a weekday, and five hours on Saturdays, with the rest of the time their own for relaxation or work catch-up.
The time zone used on board the ISS is Coordinated Universal Time (UTC). The windows are covered at night hours to give the impression of darkness because the station experiences 16 sunrises and sunsets a day. During visiting space shuttle missions, the ISS crew will mostly follow the shuttle's Mission Elapsed Time (MET), which is a flexible time zone based on the launch time of the shuttle mission.
The station provides crew quarters for each member of the expedition's crew, with two 'sleep stations' in the "Zvezda" and four more installed in "Harmony". The American quarters are private, approximately person-sized soundproof booths. The Russian crew quarters include a small window, but do not provide the same amount of ventilation or block the same amount of noise as their American counterparts. A crewmember can sleep in a crew quarter in a tethered sleeping bag, listen to music, use a laptop, and store personal items in a large drawer or in nets attached to the module's walls. The module also provides a reading lamp, a shelf and a desktop. Visiting crews have no allocated sleep module, and attach a sleeping bag to an available space on a wall—it is possible to sleep floating freely through the station, but this is generally avoided because of the possibility of bumping into sensitive equipment. It is important that crew accommodations be well ventilated; otherwise, astronauts can wake up oxygen-deprived and gasping for air, because a bubble of their own exhaled carbon dioxide has formed around their heads.
Food.
Most of the food on board is vacuum sealed in plastic bags. Cans are heavy and expensive to transport, so there are not as many. The preserved food is generally not held in high regard by the crew, and when combined with the reduced sense of taste in a microgravity environment, a great deal of effort is made to make the food more palatable. More spices are used than in regular cooking, and the crew looks forward to the arrival of any ships from Earth, as they bring fresh fruit and vegetables with them. Care is taken that foods do not create crumbs. Sauces are often used to ensure station equipment is not contaminated. Each crew member has individual food packages and cooks them using the on-board galley. The galley features two food warmers, a refrigerator added in November 2008, and a water dispenser that provides both heated and unheated water. Drinks are provided in dehydrated powder form and are mixed with water before consumption. Drinks and soups are sipped from plastic bags with straws; solid food is eaten with a knife and fork, which are attached to a tray with magnets to prevent them from floating away. Any food that floats away, including crumbs, must be collected to prevent it from clogging up the station's air filters and other equipment.
Hygiene.
Showers on space stations were introduced in the early 1970s on Skylab and Salyut 3.--> By Salyut 6, in the early 1980s, the crew complained of the complexity of showering in space, which was a monthly activity. The ISS does not feature a shower; instead, crewmembers wash using a water jet and wet wipes, with soap dispensed from a toothpaste tube-like container. Crews are also provided with rinseless shampoo and edible toothpaste to save water.
There are two space toilets on the ISS, both of Russian design, located in "Zvezda" and "Tranquility". These Waste and Hygiene Compartments use a fan-driven suction system similar to the Space Shuttle Waste Collection System. Astronauts first fasten themselves to the toilet seat, which is equipped with spring-loaded restraining bars to ensure a good seal. A lever operates a powerful fan and a suction hole slides open: the air stream carries the waste away. Solid waste is collected in individual bags which are stored in an aluminium container. Full containers are transferred to Progress spacecraft for disposal. Liquid waste is evacuated by a hose connected to the front of the toilet, with anatomically correct "urine funnel adapters" attached to the tube so both men and women can use the same toilet. Waste is collected and transferred to the Water Recovery System, where it is recycled back into drinking water.
Crew health and safety.
Radiation.
The ISS is partially protected from the space environment by the Earth's magnetic field. From an average distance of about , depending on Solar activity, the magnetosphere begins to deflect solar wind around the Earth and ISS. Solar flares are still a hazard to the crew, who may receive only a few minutes warning. The crew of Expedition 10 took shelter as a precaution in 2005 in a more heavily shielded part of the ROS designed for this purpose during the initial 'proton storm' of an X-3 class solar flare.
Subatomic charged particles, primarily protons from cosmic rays and solar wind, are normally absorbed by the Earth's atmosphere. When they interact in sufficient quantity, their effect becomes visible to the naked eye in a phenomenon called an aurora. Without the protection of the Earth's atmosphere, which absorbs this radiation, crews are exposed to about 1 millisievert each day, which is about the same as someone would get in a year on Earth from natural sources. This results in a higher risk of astronauts developing cancer. Radiation can penetrate living tissue, damage DNA, and cause damage to the chromosomes of lymphocytes. These cells are central to the immune system, and so any damage to them could contribute to the lowered immunity experienced by astronauts. Radiation has also been linked to a higher incidence of cataracts in astronauts. Protective shielding and protective drugs may lower the risks to an acceptable level.
The radiation levels experienced on the ISS are about five times greater than those experienced by airline passengers and crew. The Earth's electromagnetic field provides almost the same level of protection against solar and other radiation in low Earth orbit as in the stratosphere. Airline passengers experience this level of radiation for no more than 15 hours for the longest intercontinental flights. For example, on a 12-hour flight an airline passenger would experience 0.1 millisieverts of radiation, or a rate of 0.2 millisieverts per day; only 1/5 the rate experienced by an astronaut in LEO.
Stress.
There has been considerable evidence that psychosocial stressors are among the most important impediments to optimal crew morale and performance. Cosmonaut Valery Ryumin wrote in his journal during a particularly difficult period on board the Salyut 6 space station: "All the conditions necessary for murder are met if you shut two men in a cabin measuring 18 feet by 20 and leave them together for two months."
NASA's interest in psychological stress caused by space travel, initially studied when their manned missions began, was rekindled when astronauts joined cosmonauts on the Russian space station Mir. Common sources of stress in early American missions included maintaining high performance under public scrutiny, as well as isolation from peers and family. The latter is still often a cause of stress on the ISS, such as when the mother of NASA Astronaut Daniel Tani died in a car accident, and when Michael Fincke was forced to miss the birth of his second child.
A study of the longest spaceflight concluded that the first three weeks represent a critical period where attention is adversely affected because of the demand to adjust to the extreme change of environment. Skylab's three crews remained one, two, and three months respectively, long term crews on Salyut 6, Salyut 7, and the ISS last about five to six months and Mir's expeditions often lasted longer. The ISS working environment includes further stress caused by living and working in cramped conditions with people from very different cultures who speak a different language. First generation space stations had crews who spoke a single language; second and third-generation stations have crew from many cultures who speak many languages. The ISS is unique because visitors are not classed automatically into 'host' or 'guest' categories as with previous stations and spacecraft, and may not suffer from feelings of isolation in the same way. Crew members with a military pilot background and those with an academic science background or teachers and politicians may have problems understanding each other's jargon and worldview.
Medical.
Medical effects of long-term weightlessness include muscle atrophy, deterioration of the skeleton (osteopenia), fluid redistribution, a slowing of the cardiovascular system, decreased production of red blood cells, balance disorders, and a weakening of the immune system. Lesser symptoms include loss of body mass, and puffiness of the face.
Sleep is disturbed on the ISS regularly because of mission demands, such as incoming or departing ships. Sound levels in the station are unavoidably high; because the atmosphere is unable to thermosiphon, fans are required at all times to allow processing of the atmosphere which would stagnate in the freefall (zero-g) environment.
To prevent some of these adverse physiological effects, the station is equipped with two treadmills (including the COLBERT), and the aRED (advanced Resistive Exercise Device) which enables various weightlifting exercises which add muscle but do not compensate for or raise astronauts' reduced bone density, and a stationary bicycle; each astronaut spends at least two hours per day exercising on the equipment. Astronauts use bungee cords to strap themselves to the treadmill.
Microbiological environmental hazards.
Hazardous moulds which can foul air and water filters may develop aboard space stations. They can produce acids which degrade metal, glass, and rubber. They can also be harmful for the crew's health. Microbiological hazards have led to a development of the LOCAD-PTS that can identify common bacteria and moulds faster than standard methods of culturing, which may require a sample to be sent back to Earth. , 76 types of unregulated micro-organisms have been detected on the ISS.
Reduced humidity, paint with mould killing chemicals, and antiseptic solutions can be used to prevent contamination in space stations. All materials used in the ISS are tested for resistance against fungi.
Threat of orbital debris.
At the low altitudes at which the ISS orbits there is a variety of space debris, consisting of many different objects including entire spent rocket stages, defunct satellites, explosion fragments—including materials from anti-satellite weapon tests, paint flakes, slag from solid rocket motors, and coolant released by US-A nuclear-powered satellites. These objects, in addition to natural micrometeoroids, are a significant threat. Large objects could destroy the station, but are less of a threat as their orbits can be predicted. Objects too small to be detected by optical and radar instruments, from approximately 1 cm down to microscopic size, number in the trillions. Despite their small size, some of these objects are still a threat because of their kinetic energy and direction in relation to the station. Spacesuits of spacewalking crew could puncture, causing exposure to vacuum.
The station's shields and structure are divided between the ROS and the USOS, with completely different designs. On the USOS, a thin aluminium sheet is held apart from the hull, the sheet causes objects to shatter into a cloud before hitting the hull thereby spreading the energy of the impact. On the ROS, a carbon plastic honeycomb screen is spaced from the hull, an aluminium honeycomb screen is spaced from that, with a screen-vacuum thermal insulation covering, and glass cloth over the top. It is about 50% less likely to be punctured, and crew move to the ROS when the station is under threat. Punctures on the ROS would be contained within the panels which are 70 cm square.
Space debris objects are tracked remotely from the ground, and the station crew can be notified. This allows for a Debris Avoidance Manoeuvre (DAM) to be conducted, which uses thrusters on the Russian Orbital Segment to alter the station's orbital altitude, avoiding the debris. DAMs are not uncommon, taking place if computational models show the debris will approach within a certain threat distance. --> Eight DAMs had been performed prior to March 2009, the first seven between October 1999 and May 2003. Usually the orbit is raised by one or two kilometres by means of an increase in orbital velocity of the order of 1 m/s. Unusually there was a lowering of 1.7 km on 27 August 2008, the first such lowering for 8 years. There were two DAMs in 2009, on 22 March and 17 July. If a threat from orbital debris is identified too late for a DAM to be safely conducted, the station crew close all the hatches aboard the station and retreat into their Soyuz spacecraft, so that they would be able to evacuate in the event the station was seriously damaged by the debris. This partial station evacuation has occurred on 13 March 2009, 28 June 2011, 24 March 2012 and 16 June 2015.
Ballistic panels, also called micrometeorite shielding, are incorporated into the station to protect pressurised sections and critical systems. The type and thickness of these panels varies depending upon their predicted exposure to damage.
End of mission.
According to a 2009 report, Space Corporation Energia is considering methods to remove from the station some modules of the Russian Orbital Segment when the end of mission is reached and use them as a basis for a new station, known as the Orbital Piloted Assembly and Experiment Complex (OPSEK). The modules under consideration for removal from the current ISS include the Multipurpose Laboratory Module (MLM), currently scheduled to be launched in 2017, with other Russian modules which are currently planned to be attached to the MLM afterwards. Neither the MLM nor any additional modules attached to it would have reached the end of their useful lives in 2016 or 2020. The report presents a statement from an unnamed Russian engineer who believes that, based on the experience from "Mir", a thirty-year life should be possible, except for micrometeorite damage, because the Russian modules have been built with on-orbit refurbishment in mind.
According to the Outer Space Treaty the United States and Russia are legally responsible for all modules they have launched. In ISS planning, NASA examined options including returning the station to Earth via shuttle missions (deemed too expensive, as the station (USOS) is not designed for disassembly and this would require at least 27 shuttle missions), natural orbital decay with random reentry similar to Skylab, boosting the station to a higher altitude (which would delay reentry) and a controlled targeted de-orbit to a remote ocean area.
The technical feasibility of a controlled targeted deorbit into a remote ocean was found to be possible only with Russia's assistance. The Russian Space Agency has experience from de-orbiting the Salyut 4, 5, 6, 7 and Mir space stations; NASA's first intentional controlled de-orbit of a satellite (the Compton Gamma Ray Observatory) occurred in 2000. As of late 2010, the preferred plan is to use a slightly modified Progress spacecraft to de-orbit the ISS. This plan was seen as the simplest, most cost efficient one with the highest margin. Skylab, the only space station built and launched entirely by the US, decayed from orbit slowly over 5 years, and no attempt was made to de-orbit the station using a deorbital burn. Remains of Skylab hit populated areas of Esperance, Western Australia without injuries or loss of life.
The Exploration Gateway Platform, a discussion by NASA and Boeing at the end of 2011, suggested using leftover USOS hardware and 'Zvezda 2' as a refuelling depot and servicing station located at one of the Earth Moon Lagrange points, L1 or L2. The entire USOS cannot be reused and will be discarded, but some other Russian modules are planned to be reused. Nauka, the Node module, two science power platforms and Rassvet, launched between 2010 and 2015 and joined to the ROS may be separated to form OPSEK. The Nauka module of the ISS will be used in the station, whose main goal is supporting manned deep space exploration. OPSEK will orbit at a higher inclination of 71 degrees, allowing observation to and from all of the Russian Federation.
In February 2015, Roscosmos announced that it would remain a part of the international space station programme until 2024. Nine months earlier—in response to US sanctions against Russia over the conflict in the Crimea—Russian Deputy Prime Minister Dmitry Rogozin had stated that Russia would reject a US request to prolong the orbiting station's use beyond 2020, and would only supply rocket engines to the US for non-military satellite launches.
A proposed modification that would allow some of the ISS American and European segments to be reused would be to attach a VASIMR drive module to the vacated Node with its own onboard power source. It would allow long term reliability testing of the concept for less cost than building a dedicated space station from scratch.
On 28 March 2015, Russian sources announced that Roscosmos and NASA had agreed to collaborate on the development of a replacement for the current ISS. Igor Komarov, the head of Russia's Roscosmos, made the announcement with NASA administrator Charles Bolden at his side. Komarov said "Roscosmos together with NASA will work on the programme of a future orbital station", "We agreed that the group of countries taking part in the ISS project will work on the future project of a new orbital station", "The first step is that the ISS will operate until 2024", and that Roscosmos and NASA "do not rule out that the station's flight could be extended". In a statement provided to SpaceNews on 28 March, NASA spokesman David Weaver said the agency appreciated the Russian commitment to extending the ISS, but did not confirm any plans for a future space station.
On 30 September 2015, Boeing's contract with NASA as prime contractor for the ISS was extended to 30 September 2020. Part of Boeing's services under the contract will relate to extending the station's primary structural hardware past 2020 to the end of 2028.
Cost.
The ISS is arguably the most expensive single item ever constructed. In 2010 the cost was expected to be $150 billion. This includes NASA's budget of $58.7 billion (inflation-unadjusted) for the station from 1985 to 2015 ($72.4 billion in 2010 dollars), Russia's $12 billion, Europe's $5 billion, Japan's $5 billion, Canada's $2 billion, and the cost of 36 shuttle flights to build the station; estimated at $1.4 billion each, or $50.4 billion in total. Assuming 20,000 person-days of use from 2000 to 2015 by two- to six-person crews, each person-day would cost $7.5 million, less than half the inflation-adjusted $19.6 million ($5.5 million before inflation) per person-day of Skylab.
Sightings from Earth.
Naked eye.
The ISS is visible to the naked eye as a slow-moving, bright white dot because of reflected sunlight, and can be seen in the hours after sunset and before sunrise when the station remains sunlit but the ground and sky are dark. The ISS takes about ten minutes to move from one horizon to another, and will only be visible part of that time because of moving into or out of the Earth's shadow. Because of the size of its reflective surface area, the ISS is the brightest man-made object in the sky excluding flares, with an approximate maximum magnitude of −4 when overhead, similar to Venus. The ISS, like many satellites including the Iridium constellation, can also produce flares of up to 8 or 16 times the brightness of Venus as sunlight glints off reflective surfaces. The ISS is also visible during broad daylight conditions, albeit with a great deal more effort.
Tools are provided by a number of websites such as Heavens-Above (see "Live viewing" below) as well as smartphone applications that use the known orbital data and the observer's longitude and latitude to predict when the ISS will be visible (weather permitting), where the station will appear to rise to the observer, the altitude above the horizon it will reach and the duration of the pass before the station disappears to the observer either by setting below the horizon or entering into Earth's shadow.
In November 2012 NASA launched its 'Spot the Station' service, which sends people text and email alerts when the station is due to fly above their town.
The station is visible from 95% of the inhabited land on Earth, but is not visible from extreme northern or southern latitudes.
Astrophotography.
Using a telescope mounted camera to photograph the station is a popular hobby for astronomers, whilst using a mounted camera to photograph the Earth and stars is a popular hobby for crew. The use of a telescope or binoculars allows viewing of the ISS during daylight hours.
Parisian engineer and astrophotographer Thierry Legault, known for his photos of spaceships crossing the Sun (called occultation), travelled to Oman in 2011, to photograph the Sun, moon and space station all lined up. Legault, who received the Marius Jacquemetton award from the Société astronomique de France in 1999, and other hobbyists, use websites that predict when the ISS will pass in front of the Sun or Moon and from what location those passes will be visible.

</doc>
<doc id="15044" url="https://en.wikipedia.org/wiki?curid=15044" title="Irish">
Irish

Irish may refer to :

</doc>
<doc id="15045" url="https://en.wikipedia.org/wiki?curid=15045" title="Cosmicomics">
Cosmicomics

Cosmicomics is a collection of twelve short stories by Italo Calvino first published in Italian in 1965 and in English in 1968. The stories were originally published between 1964 and 1965 in the Italian periodicals "Il Caffè" and "Il Giorno". Each story takes a scientific "fact" (though sometimes a falsehood by today's understanding), and builds an imaginative story around it. An always extant being called Qfwfq narrates all of the stories save two, each of which is a memory of an event in the history of the universe. Qfwfq also narrates some stories in Calvino's "t zero".
All of the stories in "Cosmicomics", together with those from "t zero" and other sources, are now available in a single volume collection, "The Complete Cosmicomics" (Penguin UK, 2009).
The first U.S. edition, translated by William Weaver, won the National Book Award in the Translation category.There was a "Translation" award from 1967 to 1983.</ref>
Contents.
All of the stories feature non-human characters which have been heavily anthropomorphized.

</doc>
<doc id="15046" url="https://en.wikipedia.org/wiki?curid=15046" title="IA-32">
IA-32

IA-32 (short for "Intel Architecture, 32-bit", sometimes also called i386 through metonymy) is the 32-bit version of the x86 instruction set architecture (ISA), first implemented in the Intel 80386 microprocessors in 1985. IA-32 is the first incarnation of x86 that supports 32-bit computing; as a result, the "IA-32" term may be used as a metonym to refer to all x86 versions that support 32-bit computing.
The IA-32 instruction set was introduced in the Intel 80386 microprocessor in 1985 and, , remains supported by contemporary PC microprocessors. Even though the instruction set has remained intact, the successive generations of microprocessors that run it have become much faster. Within various programming language directives, IA-32 is still sometimes referred to as the "i386" architecture.
Intel is the inventor and the biggest supplier of IA-32 processors, and the second biggest supplier is AMD. For a while, VIA, Transmeta and others also produced IA-32 processors, but since the 2000s all manufacturers moved to the 64-bit variant of x86, x86-64.
Architectural features.
The primary defining characteristic of IA-32 is the availability of 32-bit general-purpose processor registers (for example, EAX and EBX), 32-bit integer arithmetic and logical operations, 32-bit offsets within a segment in protected mode, and the translation of segmented addresses to 32-bit linear addresses. The designers took the opportunity to make other improvements as well. Some of the most significant changes are described below.

</doc>
<doc id="15047" url="https://en.wikipedia.org/wiki?curid=15047" title="Internalism and externalism">
Internalism and externalism

Internalism and externalism are two opposing ways of explaining various subjects in several areas of philosophy. These include human motivation, knowledge, justification, meaning, and truth. The distinction arises in many areas of debate with similar but distinct meanings. Usually 'internalism' refers to the belief that an explanation can be given of the given subject by pointing to things which are internal to the person or their mind which is considering them. Conversely, externalism holds that it is things about the world which motivate us, justify our beliefs, determine meaning, etc.
Moral philosophy.
Motivation.
In contemporary moral philosophy, motivational internalism (or moral internalism) is the view that moral convictions (which are not necessarily beliefs, e.g. feelings of moral approval or disapproval) are intrinsically motivating. That is, the motivational internalist believes that there is an internal, necessary connection between one's conviction that X ought to be done and one's motivation to do X. Conversely, the motivational externalist (or moral externalist) claims that there is no necessary internal connection between moral convictions and moral motives. That is, there is no necessary connection between the conviction that X is wrong and the motivational drive not to do X. (The use of these terms has roots in W.D. Falk's (1947) paper ""Ought" and Motivation").
These views in moral psychology have various implications. In particular, if motivational internalism is true, then an amoralist is unintelligible (and metaphysically impossible). An amoralist is not simply someone who is immoral, rather it is someone who knows what the moral things to do are, yet is not motivated to do them. Such an agent is unintelligible to the motivational internalist, because moral judgments about the right thing to do have built into them corresponding motivations to do those things that are judged by the agent to be the moral things to do. On the other hand, an amoralist is entirely intelligible to the motivational "externalist", because the motivational externalist thinks that moral judgments about the right thing to do not necessitate some motivation to do those things that are judged to be the right thing to do; rather, an independent desire—such as the desire to do the right thing—is required (Brink, 2003),(Rosati, 2006).
Reasons.
There is also a distinction in ethics and action theory, largely made popular by Bernard Williams (1979, reprinted in 1981), concerning internal and external reasons for action. An "internal reason" is, roughly, something that one has in light of one's own "subjective motivational set"---one's own commitments, desires (or wants), goals, etc. On the other hand, an "external reason" is something that one has independent of one's subjective motivational set. For example, suppose that Sally is going to drink a glass of poison, because she wants to commit suicide and believes that she can do so by drinking the poison. Sally has an internal reason to drink the poison, because she wants to commit suicide. However, one might say that she has an external reason not to drink the poison because, even though she wants to die, one ought not kill oneself no matter what—regardless of whether one wants to die.
Some philosophers embrace the existence of both kinds of reason, while others deny the existence of one or the other. For example, Bernard Williams (1981) argues that there are really only internal reasons for action. Such a view is called "internalism about reasons" (or "reasons internalism"). "Externalism about reasons" (or "reasons externalism") is the denial of reasons internalism. It is the view that there are external reasons for action; that is, there are reasons for action that one can have even if the action is not part of one's subjective motivational set.
Consider the following situation. Suppose that it's against the moral law to steal from the poor, and Sasha knows this. However, Sasha doesn't desire to follow the moral law, and there is currently a poor person next to him. Is it intelligible to say that Sasha has a reason to follow the moral law right now (to not steal from the poor person next to him), even though he doesn't care to do so? The reasons externalist answers in the affirmative ("Yes, Sasha has a reason not to steal from that poor person."), since he believes that one can have reasons for action even if one does not have the relevant desire. Conversely, the reasons internalist answers the question in the negative ("No, Sasha does not have a reason not to steal from that poor person, though others might."). The reasons internalist claims that external reasons are unintelligible; one has a reason for action only if one has the relevant desire (that is, only internal reasons can be reasons for action). The reasons internalist claims the following: the moral facts are a reason "for Sasha's action" not to steal from the poor person next to him only if he currently "wants" to follow the moral law (or if not stealing from the poor person is a way to satisfy his other current goals—that is, part of what Williams calls his "subjective motivational set"). In short, the reasoning behind reasons internalism, according to Williams, is that reasons for action must be able to explain one's action; and only internal reasons can do this.
Epistemology.
Justification.
In contemporary epistemology, internalism about justification is the idea that everything necessary to provide justification for a belief must be immediately available to an agent's consciousness. Externalism in this context is the view that factors other than those internal to the believer can affect the justificatory status of a belief. One strand of externalism is reliabilism, and the causal theory of knowledge is sometimes considered to be another strand. It is important to distinguish internalism about justification from internalism about knowledge. An internalist about knowledge will likely hold that the conditions that distinguish mere true belief from knowledge are similarly internal to the individual's perspective or grounded in the subject's mental states. Whereas internalism about justification is a widely endorsed view, there is debate about knowledge internalism, due to Edmund Gettier and his Gettier-examples. These are claimed to show that knowledge is not simply justified true belief. In a short but influential paper published in 1963, Gettier produced examples that seemed to show that someone could be justified in believing something which is actually false, and inferring from it a further belief, this belief being coincidentally true. In this way, he claimed that someone could be justified in believing something true but nevertheless not be considered to have knowledge of that thing.
One line of argument in favor of externalism begins with the observation that if what justified our beliefs failed to eliminate significantly the risk of error, then it does not seem that knowledge would be attainable as it would appear that when our beliefs did happen to be correct, this would really be a matter of good fortune. While many will agree with this last claim, the argument seems inconclusive. Setting aside sceptical concerns about the possession of knowledge, Gettier cases have suggested the need to distinguish justification from warrant where warrant is that which distinguishes justified true belief from knowledge by eliminating the kind of accidentality often present in Gettier-type cases. Even if something must significantly reduce the risk of error, it is not clear why justification is what must fill the bill.
One of the more popular arguments for internalism begins with the observation, perhaps first due to Stewart Cohen, that when we imagine subjects completely cut off from their surroundings (thanks to a malicious Cartesian demon, perhaps) we do not think that in cutting these individuals off from their surroundings, these subjects cease to be rational in taking things to be as they appear. The 'new evil demon' argument for internalism (and against externalism) begins with the observation that individuals like us on the inside will be as justified as we are in believing what we believe. As it is part of the story that these individuals' beliefs are not produced by reliable mechanisms or backed by veridical perceptual experiences, the claim that the justification of our beliefs depends upon such things appears to be seriously challenged. Externalists have offered a variety of responses but there is no consensus among epistemologists as to whether these replies are successful (Cohen, 1984; Sosa, 1991).
As a response to skepticism.
In responding to skepticism, Hilary Putnam (1982 ) claims that semantic externalism yields "an argument we can give that shows we are not brains in a vat (BIV). (See also DeRose, 1999.) If semantic externalism is true, then the meaning of a word or sentence is not wholly determined by what individuals think those words mean. For example, semantic externalists maintain that the word "water" referred to the substance whose chemical composition is H2O even before scientists had discovered that chemical composition. The fact that the substance out in the world we were calling "water" actually had that composition at least partially determined the meaning of the word. One way to use this in a response to skepticism is to apply the same strategy to the terms used in a skeptical argument in the following way (DeRose, 1999 ):
To clarify how this argument is supposed to work: Imagine that there is brain in a vat, and a whole world is being simulated for it. Call the individual who is being deceived "Steve." When Steve is given an experience of walking through a park, semantic externalism allows for his thought, "I am walking through a park" to be true so long as the simulated reality is one in which he is walking through a park. Similarly, what it takes for his thought, "I am a brain in a vat," to be true is for the simulated reality to be one where he is a brain in a vat. But in the simulated reality, he is not a brain in a vat.
Apart from disputes over the success of the argument or the plausibility of the specific type of semantic externalism required for it to work, there is question as to what is gained by defeating the skeptical worry with this strategy. Skeptics can give new skeptical cases that wouldn't be subject to the same response (e.g., one where the person was very recently turned into a brain in a vat, so that their words "brain" and "vat" still pick out real brains and vats, rather than simulated ones). Further, if even brains in vats can correctly believe "I am not a brain in a vat," then the skeptic can still press us on how we know we are not in that situation (though the externalist will point out that it may be difficult for the skeptic to describe that situation).
Another attempt to use externalism to refute skepticism is done by Brueckner and Warfield. It involves the claim that our thoughts are "about" things, unlike a BIV's thoughts, which cannot be "about" things (DeRose, 1999 ).
Semantics.
Semantic externalism comes in two varieties, depending on whether meaning is construed cognitively or linguistically. On a cognitive construal, externalism is the thesis that what concepts (or contents) are available to a thinker is determined by their environment, or their relation to their environment. On a linguistic construal, externalism is the thesis that the meaning of a word is environmentally determined. Likewise, one can construe semantic internalism in two ways, as a denial of either of these two theses.
Externalism and internalism in semantics is closely tied to the distinction in philosophy of mind concerning mental content, since the contents of one's thoughts (specifically, intentional mental states) are usually taken to be semantic objects that are truth-evaluable.
See also:
Philosophy of mind.
Within the context of the philosophy of mind, externalism is the theory that the contents of at least some of one's mental states are dependent in part on their relationship to the external world or one's environment.
The traditional discussion on externalism was centered around the semantic aspect of mental content. This is by no means the only meaning of externalism now. Externalism is now a broad collection of philosophical views considering all aspects of mental content and activity. There are various forms of externalism that consider either the content or the vehicles of the mind or both. Furthermore, externalism could be limited to cognition, or it could address broader issues of consciousness.
As to the traditional discussion on semantic externalism (often dubbed "content externalism"), some mental states, such as believing that water is wet, and fearing that the Queen has been insulted, have contents we can capture using 'that' clauses. The content externalist often appeal to observations found as early as Hilary Putnam's seminal essay, "The Meaning of 'Meaning'," (1975). Putnam stated that we can easily imagine pairs of individuals that are microphysical duplicates embedded in different surroundings who use the same words but mean different things when using them.
For example, suppose that Ike and Tina's mothers are identical twins and that Ike and Tina are raised in isolation from one another in indistinguishable environments. When Ike says, "I want my mommy," he expresses a want satisfied only if he is brought to his mommy. If we brought Tina's mommy, Ike might not notice the difference, but he doesn't get what he wants. It seems that what he wants and what he says when he says, "I want my mommy," will be different from what Tina wants and what she says she wants when she says, "I want my mommy."
Externalists say that if we assume competent speakers know what they think, and say what they think, the difference in what these two speakers mean corresponds to a difference in the thoughts of the two speakers that is not (necessarily) reflected by a difference in the internal make up of the speakers or thinkers. They urge us to move from externalism about meaning of the sort Putnam defended to externalism about contentful states of mind. The example pertains to singular terms, but has been extended to cover kind terms as well such as natural kinds (e.g., 'water') and for kinds of artifacts (e.g., 'espresso maker'). There is no general agreement amongst content externalists as to the scope of the thesis.
Philosophers now tend to distinguish between "wide content" (externalist mental content) and "narrow content" (anti-externalist mental content). Some, then, align themselves as endorsing one view of content exclusively, or both. For example, Jerry Fodor (1980) argues for narrow content (although he comes to reject that view in his 1995), while David Chalmers (2002) argues for a two dimensional semantics according to which the contents of mental states can have both wide and narrow content.
Critics of the view have questioned the original thought experiments saying that the lessons that Putnam and later writers such as Tyler Burge (1979, 1982) have urged us to draw can be resisted. Frank Jackson and John Searle, for example, have defended internalist accounts of thought content according to which the contents of our thoughts are fixed by descriptions that pick out the individuals and kinds that our thoughts intuitively pertain to the sorts of things that we take them to. In the Ike/Tina example, one might agree that Ike's thoughts pertain to Ike's mother and that Tina's thoughts pertain to Tina's but insist that this is because Ike thinks of that woman as his mother and we can capture this by saying that he thinks of her as 'the mother of the speaker'. This descriptive phrase will pick out one unique woman. Externalists claim this is implausible, as we would have to ascribe to Ike knowledge he wouldn't need to successfully think about or refer to his mother.
Critics have also claimed that content externalists are committed to epistemological absurdities. Suppose that a speaker can have the concept of water we do only if the speaker lives in a world that contains H2O. It seems this speaker could know a priori that she thinks that water is wet. This is the thesis of privileged access. It also seems that she could know on the basis of simple thought experiments that she can only think that water is wet if she lives in a world that contains water. What would prevent her from putting these together and coming to know a priori that the world contains water? If we should say that no one could possibly know whether water exists a priori, it seems either we cannot know content externalism to be true on the basis of thought experiments or we cannot know what we are thinking without first looking into the world to see what it is like.
As mentioned, content externalism (limited to the semantic aspects) is only one among many other options offered by externalism by and large.
See also
Historiography of science.
Externalism in the historiography of science is the view that the history of science is due to its social context – the socio-political climate and the surrounding economy determines scientific progress.
Internalism in the historiography of science claims that science is completely distinct from social influences and pure natural science can exist in any society and at any time given the intellectual capacity.

</doc>
<doc id="15048" url="https://en.wikipedia.org/wiki?curid=15048" title="Isolationism">
Isolationism

Isolationism is a category of foreign policies institutionalized by leaders who asserted that their nations' best interests were best served by keeping the affairs of other countries at a distance. One possible motivation for limiting international involvement is to avoid being drawn into dangerous and otherwise undesirable conflicts. There may also be a perceived benefit from avoiding international trade agreements or other mutual assistance pacts. 
Introduction.
"Isolationism" is currently a somewhat controversial style of policy. Whether or not a country should be isolationist affects both its people's living standards and the ability of its political rulers to benefit favored firms and industries.
Isolationism by country.
Albania.
Albania was isolated from other countries while it was under communist control from 1944 to 1990. Known officially as the People's Republic of Albania from 1946 to 1976, and then as the People's Socialist Republic of Albania from 1976 to 1991, Albania spent much of this time under the regime of socialist leader Enver Hoxha, who ruled from 1944 to his death in 1985.
Bhutan.
Before 1999, Bhutan had banned television and the Internet to preserve its culture, environment, identity etc. Eventually, Jigme Singye Wangchuck lifted the ban on television and the Internet. His son, Jigme Khesar Namgyel Wangchuck was elected as Druk Gyalpo of Bhutan which is being transformed into a democracy.
China.
After Zheng He's voyages in the 15th century, the foreign policy of the Ming dynasty in China became increasingly isolationist. The Hongwu Emperor was the first to propose the policy to ban all maritime shipping in 1371. The Qing dynasty that came after the Ming dynasty often continued the Ming dynasty's isolationist policies. Wokou, which literally translates to "Japanese pirates" or "dwarf pirates" were pirates who raided the coastlines of China, Japan and Korea, were one of the key primary concerns, although the maritime ban was not without some control.
At the end of the Chinese Civil War, the country quickly closed off its borders to many outside countries and only maintained diplomatic ties with the Soviet Union. In 1949, Mao Zedong turned China into an isolationist, and communist country, along the lines of its Soviet benefactors. For a period of time the Chinese attempted to become self-reliant, but found that in doing so the country could not break even economically, especially when attempting to maintain a communist vision when it came to economics. In the 1970s the People's Republic of China began large radical economic reforms, which forced the country to change from a zero competition nation to one of the most capitalistic nations in the world. In doing so it quickly began to open its borders to the trade of various other countries thus adding itself to a global trade economy. While the government still regulates many of the country's cultural interactions with others, it is very open to the concept of an open market and competition with other countries, allowing free flow of technological innovations in and out of its borders.
Japan.
From 1641 to 1853, the Tokugawa shogunate of Japan enforced a policy which it called "kaikin". The policy prohibited foreign contact with most outside countries. However, the commonly held idea that Japan was entirely closed is misleading. In fact, Japan maintained limited-scale trade and diplomatic relations with China, Korea, the Ryukyu Islands and the Netherlands.
The culture of Japan developed with limited influence from the outside world and had one of the longest stretches of peace in history. During this period, Japan developed thriving cities and castle towns and increasing commodification of agriculture and domestic trade, wage labor, increasing literacy and concomitant print culture, laying the groundwork for modernization, even as the shogunate itself grew weak.
Korea.
Joseon Dynasty.
In 1863, King Gojong took the throne of the Joseon Dynasty when he was a child. His father, Regent Heungseon Daewongun, ruled for him until Gojong reached adulthood. During the mid-1860s he was the main proponent of isolationism and the principal instrument of the persecution of both native and foreign Catholics.
North Korea.
The foreign relations of North Korea are often tense and unpredictable. Since the ended the armed conflict that existed during the active part of the Korean War in 1953, leaving a de facto truce in place ever since, the North Korean government has been largely isolationist, becoming one of the world's most authoritarian societies. While no formal peace treaty exists between North and South Korea, both diplomatic discussions and clashes have occurred between the two. North Korea has maintained close relations with the People's Republic of China and has often limited its contact with other nations. The North Korean government has banned all media from other countries (such as video games, newspapers, and goods), especially South Korea and the United States, and smuggling these products is illegal.
Paraguay.
Just after independence was achieved, Paraguay was governed from 1814 by the dictator José Gaspar Rodríguez de Francia, who closed the country's borders and prohibited trade or any relation with the outside world until his death in 1840. The Spanish settlers who had arrived just before independence had to intermarry with either the old colonists or with the native Guarani, in order to create a single Paraguayan people.
Francia had a particular dislike of foreigners and any who came to Paraguay during his rule (which would have been very difficult) were not allowed to leave for the rest of their lives. An independent character, he hated European influences and the Catholic Church, turning church courtyards into artillery parks and confession boxes into border sentry posts, in an attempt to keep foreigners at bay.
Soviet Union.
The USSR exerted measures to isolate its constituent republics from the First World, including culturally, such as by Cyrillization in the Moldavian SSR. Using what is known as the Iron Curtain, they attempted to hold themselves and their satellite states within their own power, without interruption or intervention from other countries. They also served the purpose of keeping people in, a tragic fact that separated families and loved ones from each other over the course of many years.
Switzerland.
Switzerland has been neutral in foreign relations since the Battle of Marignano in 1515. Switzerland did not participate in either of the World Wars and it joined the United Nations as late as 2002, leaving only the Vatican City as the last widely recognized non-UN member at the time of joining. Switzerland is not a member of the European Union or the European Economic Area and the general public remains opposed to full EU membership.
In February 2014, Swiss voters narrowly approved a referendum to restrict immigration and reintroduce quotas on foreigners originating from the EU.
United States.
While some scholars, such as Robert J. Art, believe that the United States has an isolationist history, other scholars dispute this by describing the United States as following a strategy of unilateralism or non-interventionism instead. Robert Art makes his argument in "A Grand Strategy for America" (2003). Books that have made the argument that the United States followed unilaterism instead of isolationism include Walter A. McDougall's "Promised Land, Crusader State" (1997), John Lewis Gaddis's "Surprise, Security, and the American Experience" (2004), and Bradley F. Podliska's "Acting Alone" (2010). Both sides claim policy prescriptions from George Washington's Farewell Address as evidence for their argument. Bear F. Braumoeller argues that even the best case for isolationism, the United States in the interwar period, has been widely misunderstood and that Americans proved willing to fight as soon as they believed a genuine threat existed.

</doc>
<doc id="15049" url="https://en.wikipedia.org/wiki?curid=15049" title="Indianapolis Colts">
Indianapolis Colts

The Indianapolis Colts are an American football team based in Indianapolis, Indiana. The Colts compete in the National Football League (NFL) as a member club of the American Football Conference (AFC) South division. Since the 2008 season, the Colts have played their games in Lucas Oil Stadium. Previously, the team had played for over two decades (1984–2007) at the RCA Dome.
The Colts have been a member club of the NFL since their founding in 1953 in Baltimore, Maryland. The Colts were one of three NFL teams to join the teams of the American Football League (AFL) to form the American Football Conference following the 1970 merger. While in Baltimore, the team advanced to the playoffs 10 times and won three NFL Championship games: in 1958, 1959, and 1968. The Colts played in two Super Bowl games while it was based in Baltimore, losing to the New York Jets in Super Bowl III, while defeating the Dallas Cowboys in Super Bowl V. The Colts relocated to Indianapolis in 1984 and have since appeared in the playoffs 16 times, won two conference championships, and won one Super Bowl, defeating the Chicago Bears in Super Bowl XLI.
The 2014 season was the team's 31st in Indianapolis, equaling their length of tenure in Baltimore.
Franchise history.
1953–1983: the Baltimore era.
Following World War II, a competing professional football league was organized known as the All America Football Conference which began play in the 1946 season. In its second year the franchise assigned to the Miami Seahawks was relocated to Maryland's major commercial and manufacturing city of Baltimore, which after a fan contest was renamed the Baltimore Colts and used the team colors of silver and green. These Colts played for the next three seasons in the old AAFC. until it agreed to merge with the old National Football League (of 1920-1922 to 1950), bringing into the merger of the new reorganized NFL of three former AAFC powerhouse teams: the San Francisco 49ers, Cleveland Browns and the Baltimore Colts (known by the designation of "AAFC" or "1947-50"). This new Colts team, now in the "big league" of professional American football for the first time, although with shaky financing and ownership however, played only one season of 1950 in the new reorganized "third" NFL, and was later disbanded and moved.
Two years later, in 1953, a new Baltimore-based group, heavily supported by the City's municipal government and with a large subscription-base of fan-purchased season tickets, led by local owner Carroll Rosenbloom won the rights to a new Baltimore NFL franchise. Rosenbloom was awarded the remains of the former Dallas Texans team, who themselves had a long and winding history starting as the Boston Yanks in 1944, merging later with the Brooklyn Tigers, and who were previously known as the Dayton Triangles, one of the original old NFL teams established even before the League itself, in 1913. With the organization in 1920 of the original , [APF.]), then two years later in 1922, renamed a second time, now permanently as the "National Football League". That team later became the New York Yanks in 1950, and many of the players from the New York Yankees of the former competing All-America Football Conference, (1946-1949), were added to the team to begin playing in the newly merged League for the 1950 season. The Yanks then moved to Dallas in Texas after the 1951 season having competed for two seasons, but played their final two "home" games of the 1952 season as a so-called "road team" at the Rubber Bowl football stadium in Akron, Ohio. The NFL considers the Texans and Colts to be separate teams, although many of the earlier teams shared the same colors of blue and white. Thus, the Indianapolis Colts are legally considered to be a 1953 expansion team.
The third (and current) version of the Colts football team played their first season in Baltimore in 1953, where the team compiled a 3–9 record under first year head coach Keith Molesworth. The franchise struggled during the first few years in Baltimore, with the team not achieving their first winning record until the 1957 season. However, under head coach Weeb Ewbank and the leadership of quarterback Johnny Unitas, the Colts went on to a 9–3 record during the 1958 season and reached the NFL Championship Game for the first time in their history by winning the NFL Western Conference. The Colts faced the New York Giants in the 1958 NFL Championship Game in what is considered to be among the greatest contests in professional football history. The Colts defeated the Giants 23–17 in the first game ever to utilize the overtime rule, a game seen by 45 million people.
Following the Colts first NFL championship, the team once again posted a 9–3 record during the 1959 season and once again defeated the Giants in the NFL Championship Game to claim their second title in back to back fashion. Following the two championships in 1958 and 1959, the Colts did not return to the NFL Championship for four seasons and saw a transition from head coach Ewbank to a young Don Shula in 1963. In Shula's second season the Colts compiled a 12–2 record, but lost to the Cleveland Browns in the NFL Championship. However, in 1968 the Colts returned with the continued leadership of Unitas and Shula and went on to win the Colts' third NFL Championship and made an appearance in Super Bowl III.
Leading up to the Super Bowl and following the 34–0 trouncing of the Cleveland Browns in the NFL Championship, many were calling the 1968 Colts team one of the "greatest pro football teams of all time" and were favored by 18 points against their counterparts from the American Football League, the New York Jets. The Colts, however, were stunned by the Jets, who won the game 16–7 in the first Super Bowl victory for the young AFL. The result of the game surprised many in the sports media as Joe Namath and Matt Snell led the Jets to the Super Bowl victory under head coach Weeb Ewbank, who had previously won two NFL Championships with the Colts.
Rosenbloom of the Colts, Art Modell of the Browns, and Art Rooney of the Pittsburgh Steelers agreed to have their teams join the ten AFL teams in the American Football Conference as part of the AFL-NFL merger in 1970. The Colts immediately went on a rampage in the new league, as new head coach Don McCafferty led the 1970 team to an 11–2–1 regular season record, winning the AFC East title. In the first round of the NFL Playoffs, the Colts beat the Cincinnati Bengals 17–0; one week later in the first ever AFC Championship Game, they beat the Oakland Raiders 27–17. Baltimore went on to win the first post-merger Super Bowl (Super Bowl V), defeating the National Football Conference's Dallas Cowboys 16–13 on a Jim O'Brien field goal with five seconds left to play. The victory gave the Colts their fourth NFL championship and first Super Bowl victory. Following the championship, the Colts returned to the playoffs in 1971 and defeated the Cleveland Browns in the first round, but lost to the Miami Dolphins in the AFC Championship.
Citing friction with the City of Baltimore and the local press, Rosenbloom traded the Colts franchise to Robert Irsay on July 13, 1972 and received the Los Angeles Rams in return. Under the new ownership, the Colts did not reach the postseason for three consecutive seasons after 1971, and after the 1972 season, starting quarterback and legend Johnny Unitas was traded to the San Diego Chargers. Following Unitas' departure, the Colts made the playoffs three consecutive seasons from 1975 to 1977, losing in the divisional round each time. The Colts 1977 playoff loss in double overtime against the Oakland Raiders was famous for the fact that it was the last playoff game for the Colts in Baltimore and is also known for the Ghost to the Post play. These consecutive championship teams featured 1976 NFL Most Valuable Player Bert Jones at quarterback and an outstanding defensive line, nicknamed the "Sack Pack."
Following the 1970s success, the team endured nine consecutive losing seasons beginning in 1978. In 1981, the Colts defense allowed an NFL-record 533 points, set an all-time record for fewest sacks (13), and also set a modern record for fewest punt returns (12). The following year the offense collapsed, including a game against the Buffalo Bills where the Colts' offense did not cross mid-field the entire game. The Colts finished 0–8–1 in the strike-shortened 1982 season, thereby earning the right to select Stanford quarterback John Elway with the first overall pick. Elway, however, refused to play for Baltimore, and using leverage as a draftee of the New York Yankees baseball club, forced a trade to Denver. Behind an improved defense the team finished 7–9 in 1983, but that would be their last season in Baltimore.
Relocation to Indianapolis.
The Baltimore Colts played their final home game in Baltimore on December 18, 1983, against the then Houston Oilers. Irsay continued to request upgrades to Memorial Stadium or construction of a new stadium. As a result of the poor performance on the field and the stadium issues, fan attendance and team revenue continued to dwindle. City officials were precluded from using tax-payer funds for the building of a new stadium, and the modest proposals that were offered by the city were not acceptable to either the Colts or the city's MLB franchise the Orioles. However, all sides continued to negotiate. Relations between Irsay and the city of Baltimore deteriorated. While Irsay assured fans that his ultimate desire was to stay in Baltimore, he nevertheless began discussions with several other cities willing to build new football stadiums, eventually narrowing the list of cities to two: Phoenix and Indianapolis. Under the administration of mayors Richard Lugar and then William Hudnut, Indianapolis had undertaken an ambitious effort to reinvent itself into a 'Great American City'. The Hoosier Dome, which was later renamed the RCA Dome, had been built specifically for, and was ready to host, an NFL expansion team.
Meanwhile, in Baltimore, the situation worsened. The Maryland General Assembly intervened when a bill was introduced to give the city of Baltimore the right to seize ownership of the team by eminent domain. As a result, Irsay began serious negotiations with Indianapolis Mayor William Hudnut in order to move the team before the Maryland legislature could pass the law. Indianapolis offered loans as well as the Hoosier Dome and a training complex. After the deal was reached, moving vans from Indianapolis-based Mayflower Transit were dispatched overnight to the team's Maryland training complex, arriving on the morning of March 29, 1984. Once in Maryland, workers loaded all of the team's belongings, and by midday the trucks departed for Indianapolis, leaving nothing of the Colts organization that could be seized by Baltimore. The Baltimore Colts' Marching Band had to scramble to retrieve their equipment and uniforms before they were shipped to Indianapolis as well.
The move triggered a flurry of legal activity that ended when representatives of the city of Baltimore and the Colts organization reached a settlement in March 1986. Under the agreement, all lawsuits regarding the relocation were dismissed, and the Colts agreed to endorse a new NFL team for Baltimore.
1984–97.
Upon the Colts' arrival in Indianapolis over 143,000 requests for season tickets were received in just two weeks. The move to Indianapolis, however, did not change the recent fortune of the Colts, with the team appearing in the postseason only once in the first eleven seasons in Indianapolis. During the 1984 season, the first in Indianapolis, the team went 4–12 and accounted for the lowest offensive yardage in the league that season. The 1985 and 1986 teams combined for only eight wins, including an 0–13 start in 1986 which prompted the firing of head coach Rod Dowhower, who was replaced by Ron Meyer. The Colts, however, did receive eventual Hall of Fame running back Eric Dickerson as a result of a trade during the 1987 season, and went on to compile a 9–6 record, thereby winning the AFC East and advancing to the postseason for the first time in Indianapolis; they lost that game to the Cleveland Browns.
Following 1987, the Colts did not see any real success for quite some time, with the team missing the postseason for seven consecutive seasons. The struggles came to a climax in 1991 when the team went 1–15 and was just one point away from the first "imperfect" season in the history of a 16-game schedule. The season resulted in the firing of head coach Ron Meyer and the return of former head coach Ted Marchibroda to the organization in 1992; he had coached the team from 1975 to 1979. The team continued to struggle under Marchibroda and Jim Irsay, son of Robert Irsay and general manager at the time. It was in 1994 that Robert Irsay brought in Bill Tobin to become the general manager of the Indianapolis Colts.
Under Tobin, the Colts drafted running back Marshall Faulk with the second overall pick in the 1994 and acquired quarterback Jim Harbaugh as well. These moves along with others saw the Colts begin to turn their fortunes around with playoff appearances in 1995 and 1996. The Colts won their first postseason game as the Indianapolis Colts in 1995 and advanced to the AFC Championship Game against the Pittsburgh Steelers, coming just a Hail Mary pass reception away from a trip to Super Bowl XXX.
Marchibroda retired following the 1995 season and was replaced by Lindy Infante in 1996. After two consecutive playoff appearances, the Colts regressed and went 3–13 during the 1997 season. Along with the disappointing season, the principal owner and man who moved the team to Indianapolis, Robert Irsay, died in January 1997 after years of declining health. Jim Irsay, Robert Irsay's son, entered the role of principal owner following his father's death and quickly began to change the organization. Irsay replaced general manager Tobin with Bill Polian in 1997 as the team decided to build through their number one overall pick in the 1998 draft.
1998–2011: the Peyton Manning era.
Jim Irsay began to shape the Colts one year after assuming control from his father by firing head coach Lindy Infante and hiring Bill Polian as the general manager of the organization. Polian in turn hired Jim Mora to become the next head coach of the team and drafted Tennessee Volunteer quarterback Peyton Manning, the son of New Orleans Saints legend Archie Manning, with the first overall pick in the 1998 draft.
The team and Manning struggled during the 1998 season, winning only three games; Manning threw a league high 28 interceptions. However, Manning did pass for 3,739 yards and threw 26 touchdown passes while also being named to the NFL All-Rookie First Team. The Colts began to improve towards the end of the 1998 season and showed continued growth in 1999. Indianapolis drafted Edgerrin James in 1999 and continued to improve their roster heading into the upcoming season. The Colts went 13–3 in 1999 and finished first in the AFC East, their first division title since 1987. Indianapolis lost to the eventual AFC champion Tennessee Titans in the divisional playoffs.
The 2000 and 2001 Colts teams were considerably less successful compared to the 1999 team, and pressure began to mount on team administration and the coaching staff following a 6–10 season in 2001. Head coach Jim Mora was fired at the end of the season and was replaced by former Tampa Bay Buccaneers head coach Tony Dungy. Dungy and the team quickly changed the atmosphere of the organization and returned to the playoffs in 2002 with a 10–6 record. The Colts also returned to the playoffs in 2003 and 2004 with 12–4 records and AFC South championships. The Colts lost to the New England Patriots and Tom Brady in the 2003 AFC Championship Game and in the 2004 divisional playoffs, thereby beginning a rivalry between the two teams, and between Manning and Brady. Following two consecutive playoff losses to the Patriots, the Colts began the 2005 season with a 13–0 record, including a regular season victory over the Patriots, the first in the Manning era. During the season Manning and Marvin Harrison broke the NFL record for touchdowns by a quarterback and receiver tandem. Indianapolis finished the 2005 season with a 14–2 record, the best record in the league that year and the best in a 16 games season for the franchise, but lost to the Pittsburgh Steelers in the divisional round, a disappointing end to the season.
Indianapolis entered the 2006 season with a veteran quarterback, receivers, and defenders, and chose running back Joseph Addai in the 2006 draft. As in the previous season, the Colts began the season undefeated and went 9–0 before losing their first game against the Dallas Cowboys. Indianapolis finished the season with a 12–4 record and entered the playoffs for the fifth consecutive year, this time as the number three seed in the AFC. The Colts won their first two playoff games against the Kansas City Chiefs and the Baltimore Ravens to return to the AFC Championship Game for the first time since the 2003 playoffs, where they faced their rivals, the New England Patriots. In a classic game, the Colts overcame a 21–3 first half deficit to win the game 38–34 and earned a trip to Super Bowl XLI, the franchise's first Super Bowl appearance since 1970 and for the first as Indianapolis. The Colts faced the Chicago Bears in the Super Bowl, winning the game 29–17 and giving Manning, Polian, Irsay, and Dungy, as well as the city of Indianapolis, their first Super Bowl title.
Jim Caldwell was hired as head coach of the team following Dungy, and led the team during the 2009 season. The Colts for the second time in the Manning era entered the playoffs with the best record in the AFC. The Colts managed victories over the Baltimore Ravens and New York Jets to advance to Super Bowl XLIV against the New Orleans Saints, but lost to the Saints 31–17 to end the season in disappointment.
At the completion of the 2009 season, the Colts had finished the first decade of the 2000s (2000–2009) with the most regular season wins (115) and highest winning percentage (.719) of any team in the NFL during that span.
The 2010 team compiled a 10–6 record, the first time the Colts did not win 12 games since 2002, and lost to the New York Jets in the wild card round of the playoffs. The loss to the Jets would be the last game for Peyton Manning as a Colt.
After missing the preseason, Manning was ruled out for the Colts' opening game in Houston and eventually the entire 2011 season. Taking over as starter was veteran quarterback Kerry Collins, who had been signed to the team after dissatisfaction with backup quarterback Curtis Painter and Dan Orlovsky. However, even with a veteran quarterback, the Colts lost their first 13 games and finished the season with a 2–14 record, enough to receive the first overall pick in the 2012 draft. Immediately following the season, team president Bill Polian was fired, ending his 14-year tenure with the team. The change built the anticipation of the organization's decision regarding Manning's future with the team. The Peyton Manning era came to an end on March 8, 2012 when Jim Irsay announced that Manning was being released from the roster after 13 seasons.
2012–present: the Andrew Luck era.
During the 2012 off-season owner Jim Irsay hired Ryan Grigson to be the General Manager. Grigson decided to let Head Coach Jim Caldwell go and Chuck Pagano was hired as the new Head Coach shortly thereafter. The Colts also began to release some higher paid and oft-injured veteran players, including Joseph Addai, Dallas Clark, and Gary Brackett. The Colts used their number one overall draft pick in 2012 to draft Stanford Cardinal quarterback Andrew Luck and also drafted his teammate Coby Fleener in the second round. The team also switched to a 3–4 defensive scheme.
With productive seasons from both Luck and veteran receiver Reggie Wayne, the Colts rebounded from the 2–14 season of 2011 with a 2012 season record of 11–5. The franchise, team, and fan base rallied behind Head Coach Chuck Pagano during his fight with leukemia. Clinching an unexpected playoff spot in the 2012–13 NFL playoffs, the 14th playoff berth for the club since 1995. The season ended in a 24–9 playoff loss to the eventual Super Bowl Champion Baltimore Ravens.
Two weeks into the 2013 season, the Colts traded their 1st round selection in the 2014 NFL Draft to the Cleveland Browns for running back Trent Richardson. In week 7, Luck led the Colts to a 39–33 win over his predecessor, Peyton Manning, and the undefeated Broncos. Luck went on to lead the Colts to a 15th division championship later that season. In the first round of the 2013 NFL Playoffs, Andrew Luck led the Colts to a 45-44 victory over Kansas City, outscoring the Chiefs 35-13 in the second half in 
the 2nd biggest comeback in NFL Playoff history.
During the 2014 season, Luck would lead the Colts to the AFC Championship game for the first time in his career after breaking the Colts' single season passing yardage record previously held by Manning.
Logos and uniforms.
The Colts' helmets in 1953 were white with a blue stripe. In 1954–55 they were blue with a white stripe and a pair of horseshoes at the rear of the helmet. For 1956 the colors were reversed. white helmet, blue stripe and horseshoes at the rear. In 1957 the horseshoes moved to their current location, one per side The blue jerseys have white shoulder stripes while the white jerseys have blue stripes. The team also wears white pants with blue stripes down the sides.
From 1982 through 1986, the Colts wore gray pants with their blue jerseys. The gray pants featured a horseshoe on the top of the sides with the player's number inside the horseshoe. The Colts continued to wear white pants with their white jerseys throughout this period, and in 1987, the gray pants were retired.
The Colts wore blue pants with their white jerseys for the first three games of the 1995 season, but then returned to white pants with both the blue and white jerseys. The team made some minor uniform adjustments before the start of the 2004 season, including reverting from blue to the traditional gray face masks, darkening their blue colors from a royal blue to speed blue, as well as adding two white stripes to the socks. In 2006, the stripes were removed from the socks.
In 2002, the Colts made a minor striping pattern change on their jerseys, having the stripes only on top of the shoulders then stop completely. Previously, the stripes used to go around to underneath the jersey sleeves. This was done because the Colts, like many other football teams, were beginning to manufacture the jerseys to be tighter to reduce holding calls and reduce the size of the sleeves. Although the white jerseys of the Minnesota Vikings at the time also had a similar striping pattern and continued as such (as well as the throwbacks the New England Patriots wore in the Thanksgiving game against the Detroit Lions in 2002, though the Patriots later wore the same throwbacks in 2009 with truncated stripes and in 2010 became their official alternate uniform), the Colts and most college teams with this striping pattern did not make this adjustment.
Lucas Oil Stadium.
After 24 years of playing at the RCA Dome, the Colts moved to their new home Lucas Oil Stadium in the fall of 2008. In December 2004, the City of Indianapolis and Jim Irsay agreed to a new stadium deal at an estimated cost of $1 billion (including the Indiana Convention Center upgrades). In a deal estimated at $122 million, Lucas Oil Products won the naming rights to the stadium for 20 years.
It is a seven-level stadium which seats 63,000 for football. It can be reconfigured to seat 70,000 or more for NCAA basketball and football and concerts. It covers . The stadium features a retractable roof allowing the Colts to play home games outdoors for the first time since arriving in Indianapolis. Using FieldTurf, the playing surface is roughly below ground level. In addition to being larger than the RCA Dome, the new stadium features: 58 permanent concession stands, 90 portable concession stands, 13 escalators, 11 passenger elevators, 800 restrooms, high definition video displays from Daktronics and replay monitors and 142 luxury suites. The stadium also features a retractable roof, with electrification technology developed by VAHLE, Inc. Other than being the home of the Colts, the stadium will host games in both the Men's and Women's NCAA Basketball Tournaments and will serve as the back up host for all NCAA Final Four Tournaments. The stadium hosted the Super Bowl for the 2011 season (Super Bowl XLVI) and has a potential economic impact estimated at $286 million. Lucas Oil Stadium will also host the Drum Corps International World Championships from 2009 until 2018.
Rivalries.
New England Patriots.
The rivalry between the Indianapolis Colts and New England Patriots is one of the NFL's newest rivalries. The rivalry is fueled by the quarterback comparison between Peyton Manning and Tom Brady. The Patriots owned the beginning of the series, defeating the Colts in six consecutive contests including the 2003 AFC Championship game and a 2004 AFC Divisional game. The Colts won the next three matches, notching two regular season victories and a win in the 2006 AFC Championship game on the way to their win in Super Bowl XLI. On November 4, 2007 the Patriots defeated the Colts 24–20; in the next matchup on November 2, 2008, the Colts won 18–15 in a game that was one of the reasons the Patriots failed to make the playoffs; in the 2009 meeting, the Colts staged a spirited comeback to beat the Patriots 35–34; in 2010 the Colts almost staged another comeback, pulling within 31–28 after trailing 31–14 in the fourth quarter, but fell short due to a Patriots interception of a Manning pass late in the game; it turned out to be Manning's final meeting against the Patriots as a member of the Colts. After a dismal 2011 season that included a 31–24 loss to the Patriots, the Colts drafted Andrew Luck and in November of 2012 the two teams met with identical 6–3 records; the Patriots erased a 14–7 gap to win 59–24. The nature of this rivalry is ironic because while the Colts and Patriots were division rivals from 1970 to 2001, it did not become prominent in league circles until after Indianapolis was relocated to the AFC South. On November 16, 2014, the New England Patriots traveled at 7-2 to play the 6-3 Colts at Lucas Oil Stadium. After a stellar 4 touchdown performance by New England running back Jonas Gray, the Patriots defeated the Colts 42-20. The Patriots followed up with a 45-7 defeat of the Colts in the 2014 AFC Championship Game.
Earliest rivalries.
In the years 1953–66 the Colts played in the NFL Western Conference (also known as division), but were never known to have a significant rivalry with any of the other franchises in that alignment, seeing as they were the eastern-most team and the rest of the division included the Great Lakes franchises Green Bay, Detroit Lions, Chicago Bears, and after 1961, the Minnesota Vikings. The closest team to Baltimore was the Washington Redskins, but they were not in the same division, and they were not very competitive at that time.
New York Giants.
In 1958 Baltimore played its first NFL Championship Game against the 10–3 New York Giants. The Giants qualified for the championship after a tie-breaking playoff against the Cleveland Browns. Having already been defeated by the Giants in the regular season, Baltimore was not favored to win, yet proceeded to take the title in sudden death overtime. The Colts then repeated the feat by posting an identical record and routing the Giants in the 1959 final. Up until the Colts' back-to-back titles, the Giants had been the premier club in the NFL, and would continue to be post-season stalwarts the next decade losing three straight finals. The situation was reversed by the end of the decade, with Baltimore winning the 1968 NFL title while New York would arrive at continuously less impressive results. In recent years, the Colts and Giants featured brothers as their starting quarterbacks (Peyton and Eli Manning respectively) leading to their occasional match-up being referred to as the "Manning Bowl".
New York Jets.
Super Bowl III became the most famous upset in pro sports history as the American Football League's New York Jets won 16–7 over the overwhelmingly favored Colts. With the merger of the AFL and NFL the Colts and Jets were placed in the new AFC East. The two teams met twice a year (interrupted in 1982 by a player strike) 1970–2001; with the move of the Colts to the AFC South the two teams' rivalry actually escalated, as they met three times in the playoffs in the South's first nine seasons of existence; the Jets crushed the Colts 41–0 in the 2002 Wild Card playoff round; the Colts then defeated the Jets 30–17 in the 2009 AFC Championship Game; but the next year in the Wild Card round the Jets pulled off another playoff upset of the Colts, winning 17–16; it was Peyton Manning's final game with the Colts. The Jets defeated the Colts 35–9 in 2012 in Andrew Luck's debut season.
Joe Namath and Johnny Unitas were the focal point of the rivalry at its beginning, but they did not meet for a full game until September 24, 1972. Namath erupted with six touchdowns and 496 passing yards despite only 28 throws and 15 completions. Unitas threw for 376 yards and two scores but was sacked six times as the Jets won 44–34; the game was considered one of the top ten passing duels in NFL history.
Miami Dolphins.
Baltimore's post NFL-AFL merger passage to the AFC saw them thrust into a new environment with little in common with its fellow divisional teams, the Jets, Miami Dolphins, Buffalo Bills, and Boston Patriots. One angle where the two clubs did have something in common, however, lay in new Miami coach Don Shula. Shula had coached the Colts the previous seven pre-merger seasons (1963–9) and was signed by Joe Robbie after the merger was consummated; because the signing came after the merger the NFL's rules on tampering came into play, and the Dolphins had to give up their first-round pick to the Colts.
Powered by QB Earl Morrall Baltimore would be the first non-AFL franchise to win a division title in the conference, outlasting the Miami Dolphins by one game, and leading the division since Week 3 of 1970. The two franchises were denied a playoff confrontation by Miami's first-round defeat to the Oakland Raiders, whereas Baltimore would win its first Super Bowl title that year.
Yet in 1971 the teams were engaged in a heated race that went down to the final week of the season, where Miami won its first division title with a 10–3–1 title compared to the 10–4 Baltimore record after the Colts won the Week 13 matchup between them at home, but proceeded to lose the last game of the season to Boston. In the playoffs Baltimore advanced to the AFC title game after a 20–3 rout of the Cleveland Browns, whereas Miami survived a double-overtime nailbiter against the Kansas City Chiefs. This set up a title game that was favored for the defending league champion Colts. Yet Miami won the AFC championship with a 21–0 shutout and advanced to lose Super Bowl VI to Dallas. In 1975 Baltimore and Miami tied with 10–4 records, yet the Colts advanced to the playoffs based on a head-to-head sweep of their series. In 1977 Baltimore tied for first for the third straight year (in 1976 they tied with the now-New England Patriots) with Miami, and this time advanced to the playoffs on even slimmer pretenses, with a conference record of 9–3 compared to Miami's 8–4, as they had split the season series. The rivalry would in the following years be virtually negated by very poor play of the Colts; the Colts would win just 117 games in the twenty-one seasons (1978–98) that bracketed their 1977 playoff loss to the Oakland Raiders and the 1999 trade of star running back Marshall Faulk; this included a 0–8–1 record during the NFL's strike shortened 1982 season.
In 1995, now as Indianapolis, the two both posted borderline 9–7 records to tie for second against Buffalo, yet the Colts once again reached the post-season having swept the season series. The following season they edged out Miami by posting a 9–7 record and winning the ordinarily meaningless 3rd-place position, but qualifying for the wild card. The two clubs' 1999 meetings were dramatic affairs between Hall Of Fame-bound Dan Marino and up-and-coming star Peyton Manning. Marino led a 25-point fourth quarter comeback for a 34–31 Dolphins win at the RCA Dome, then in Miami Marino led another comeback to tie the game 34–34 with 36 seconds remaining; Manning, however, drove the Colts in range for a 53-yard field goal as time expired (37–34 Colts win).
The last truly meaningful matchup between the two franchises would be in the 2000 season, when Miami edged out Indianapolis with an 11–5 record for the division championship. The two then met in the wild-card round where the Dolphins won 23–17 before being blown out by Oakland 27–0 (the Colts themselves had suffered a bitter loss to the Raiders in Week Two of the season when the Raiders erased a 24–7 gap to win 38–31). In 2002 the Colts moved to the newly created AFC South division; the two clubs met at the RCA Dome on September 15 where the Dolphins edged the Colts 21–13 after stopping a late Colts drive. The rivalry was effectively retired after this; the two clubs did meet in a memorable "Monday Night Football" matchup in 2009 where the Colts, despite having the ball for only 15 minutes, defeated the Dolphins 27–23.
The rivalry saw a rekindling after the 2012 NFL Draft brought new quarterbacks to both teams in Ryan Tannehill and Luck. The two met during the 2012 season with Luck breaking the rookie record for passing yards in a game in a 23-20 win over the Dolphins, although Tannehill and the Dolphins would beat the Colts 24-20 the next season.
Players.
Pro Football Hall of Famers.
Players.
Baltimore Colts
Indianapolis Colts
Coaches.
Baltimore Colts
Indianapolis Colts
Contributors.
Indianapolis Colts
Ring of Honor.
The Ring of Honor was established on September 23, 1996. There have been 12 inductees.
Radio and television coverage.
The Colts' flagship radio station since relocating from Baltimore in 1984 to 1998 and again starting in the 2007 season is WIBC 1070 AM (later renamed WFNI as of December 26, 2007); under the new contract, games are also simulcast on WLHK at 97.1 FM. From 1998 through 2006, the Colts' flagship radio station was WFBQ at 94.7 FM (with additional programming on station WNDE at 1260 AM). Bob Lamey is the team's play-by-play announcer, holding that title from 1984 to 1991 and again since 1995. Former Colts offensive lineman, Will Wolford serves as the "color commentator". Ted Marchibroda, who had been the head coach of the Colts in both Baltimore and Indianapolis and who served as color commentator from 1999 to 2006, is now an analyst on the Colts pre-game show. Mike Jansen serves as the public address announcer at all Colts home games. Mike has been the public address announcer since the 1998 season.
Until 2011, WTTV (channel 4/29) carried the team's preseason games, when WNDY-TV (channel 23) began to carry them as part of an agreement with sister station WISH-TV (channel 8) to become the team's official station; WISH had carried most of the team's games through the NFL on CBS since the 1998 season. Indiana University's "Hoosiers" announcer Don Fischer provides play-by-play. "Monday Night Football" broadcasts are usually carried by ABC affiliate WRTV (channel 6).
The team's carriage rights were shaken up in mid-2014 when WTTV's owner Tribune Media came to terms with CBS to become the network's Indianapolis affiliate as of January 1, 2015, leaving WISH with the market's affiliation with The CW. With the deal, both Tribune Media stations, including WXIN (channel 59) will carry the bulk of the team's regular season games starting with the 2015 NFL season (WXIN will carry a minimum of two home games against NFC opponents under the NFL on Fox deal, along with flex-scheduled Sunday games no matter the division matchup), with the team's Wild Card playoff game against the Cincinnati Bengals on January 4, 2015 on WTTV rather than new CW affiliate WISH. Also as of the 2015 season, WTTV and WXIN will become the official Colts stations and air the team's preseason games, along with official team programming and coach's shows, and have some kind of signage and presence at Lucas Oil Stadium.
Radio station affiliates.
Indianapolis Colts Radio Affiliates

</doc>
<doc id="15051" url="https://en.wikipedia.org/wiki?curid=15051" title="Immigration to the United States">
Immigration to the United States

Immigration to the United States is a complex demographic phenomenon that has been a major source of population growth and cultural change throughout much of the history of the United States. The economic, social, and political aspects of immigration have caused controversy regarding ethnicity, economic benefits, jobs for non-immigrants, settlement patterns, impact on upward social mobility, crime, and voting behavior. In 2005, the United States per capita ranked 34th out of 179 world nations in the number of immigrants allowed into the country.
Prior to 1965, policies such as the national origins formula limited immigration and naturalization opportunities for people from areas outside Western Europe. Exclusion laws enacted as early as the 1880s generally prohibited or severely restricted immigration from Asia, and quota laws enacted in the 1920s curtailed Eastern European immigration.
The civil rights movement of the 1960s led to the replacement of these ethnic quotas with per-country limits. Since then, the number of first-generation immigrants living in the United States has quadrupled, from 9.6 million in 1970 to about 38 million in 2007. Nearly 14 million immigrants entered the United States from 2000 to 2010, and over one million persons were naturalized as U.S. citizens in 2008. The per-country limit applies the same maximum on the number of visas to all countries regardless of their population and has therefore had the effect of significantly restricting immigration of persons born in populous nations such as Mexico, China, India, and the Philippines – the leading countries of origin for legally admitted immigrants to the United States in 2013; nevertheless, China, India, and Mexico were the leading countries of origin for immigrants overall to the United States in 2013, regardless of legal status, according to a U.S. Census Bureau study. , 66% of legal immigrants were admitted on the basis of family ties, along with 13% admitted for their employment skills and 17% for humanitarian reasons.
For those who enter the US illegally across the Mexico–United States border and elsewhere, migration is difficult, expensive and dangerous. Virtually all undocumented immigrants have no avenues for legal entry to the United States due the restrictive legal limits on green cards, and lack of immigrant visas for low skilled workers. Participants in debates on immigration in the early twenty-first century called for increasing enforcement of existing laws governing illegal immigration to the United States, building a barrier along some or all of the U.S.-Mexico border, or creating a new guest worker program. Through much of 2006 the country and Congress was immersed in a debate about these proposals. few of these proposals had become law, though a partial border fence had been approved and subsequently canceled.
History.
American immigration history can be viewed in four epochs: the colonial period, the mid-19th century, the start of the 20th century, and post-1965. Each period brought distinct national groups, races and ethnicities to the United States. During the 17th century, approximately 400,000 English people migrated to Colonial America. Over half of all European immigrants to Colonial America during the 17th and 18th centuries arrived as indentured servants. The mid-19th century saw mainly an influx from northern Europe; the early 20th-century mainly from Southern and Eastern Europe; post-1965 mostly from Latin America and Asia.
Historians estimate that fewer than 1 million immigrants came to the United States from Europe between 1600 and 1799. The 1790 Act limited naturalization to "free white persons"; it was expanded to include blacks in the 1860s and Asians in the 1950s. In the early years of the United States, immigration was fewer than 8,000 people a year, including French refugees from the slave revolt in Haiti. After 1820, immigration gradually increased. From 1836 to 1914, over 30 million Europeans migrated to the United States. The death rate on these transatlantic voyages was high, during which one in seven travelers died. In 1875, the nation passed its first immigration law, the Page Act of 1875.
After an initial wave of immigration from China following the California Gold Rush, Congress passed a series of laws culminating in the Chinese Exclusion Act of 1882, banning virtually all immigration from China until the law's repeal in 1943. In the late 1800s, immigration from other Asian countries, especially to the West Coast, became more common.
The peak year of European immigration was in 1907, when 1,285,349 persons entered the country. By 1910, 13.5 million immigrants were living in the United States. In 1921, the Congress passed the Emergency Quota Act, followed by the Immigration Act of 1924. The 1924 Act was aimed at further restricting immigrants from Southern and Eastern Europe, particularly Jews, Italians, and Slavs, who had begun to enter the country in large numbers beginning in the 1890s, and consolidated the prohibition of Asian immigration. 
Immigration patterns of the 1930s were dominated by the Great Depression. In the final prosperous year, 1929, there were 279,678 immigrants recorded, but in 1933, only 23,068 came to the U.S. In the early 1930s, more people emigrated from the United States than to it. The U.S. government sponsored a Mexican Repatriation program which was intended to encourage people to voluntarily move to Mexico, but thousands were deported against their will. Altogether about 400,000 Mexicans were repatriated. Most of the Jewish refugees fleeing the Nazis and World War II were barred from coming to the United States. In the post-war era, the Justice Department launched Operation Wetback, under which 1,075,168 Mexicans were deported in 1954.
The Immigration and Nationality Act of 1965, also known as the Hart-Cellar Act, abolished the system of national-origin quotas. By equalizing immigration policies, the act resulted in new immigration from non-European nations, which changed the ethnic make-up of the United States. In 1970, 60% of immigrants were from Europe; this decreased to 15% by 2000. In 1990, George H. W. Bush signed the Immigration Act of 1990, which increased legal immigration to the United States by 40%. In 1991, Bush signed the Armed Forces Immigration Adjustment Act 1991, allowing foreign service members who had serve 12 or more years in the US Armed Forces to qualify for permanent residency and, in some cases, citizenship.
In November 1994, California voters passed Proposition 187 amending the state constitution, denying state financial aid to illegal immigrants. The federal courts voided this change, ruling that it violated the federal constitution.
Appointed by Bill Clinton, the U.S. Commission on Immigration Reform recommended reducing legal immigration from about 800,000 people per year to approximately 550,000. While an influx of new residents from different cultures presents some challenges, "the United States has always been energized by its immigrant populations," said President Bill Clinton in 1998. "America has constantly drawn strength and spirit from wave after wave of immigrants [...] They have proved to be the most restless, the most adventurous, the most innovative, the most industrious of people."
In 2001, President George W. Bush discussed an accord with Mexican President Vincente Fox. Possible accord was derailed by the September 11 attacks. From 2005 to 2013, the US Congress discussed various ways of controlling immigration. The Senate and House are unable to reach an agreement. In 2012 and 2014, President Obama initiated policies that were intended to ease the pressure on deporting people using anchor babies as a means of immigrating to the United States.
Nearly 8 million people immigrated to the United States from 2000 to 2005; 3.7 million of them entered without papers. Since 1986 Congress has passed seven amnesties for undocumented immigrants. In 1986 president Ronald Reagan signed immigration reform that gave amnesty to 3 million undocumented immigrants in the country. Hispanic immigrants suffered job losses during the late-2000s recession, but since the recession's end in June 2009, immigrants posted a net gain of 656,000 jobs. Over 1 million immigrants were granted legal residence in 2011.
Source: US Department of Homeland Security, "Persons Obtaining Legal Permanent Resident Status: Fiscal Years 1950 to 2013"
Contemporary immigration.
Until the 1930s most legal immigrants were male. By the 1990s women accounted for just over half of all legal immigrants. Contemporary immigrants tend to be younger than the native population of the United States, with people between the ages of 15 and 34 substantially overrepresented. Immigrants are also more likely to be married and less likely to be divorced than native-born Americans of the same age.
Immigrants are likely to move to and live in areas populated by people with similar backgrounds. This phenomenon has held true throughout the history of immigration to the United States. Seven out of ten immigrants surveyed by Public Agenda in 2009 said they intended to make the U.S. their permanent home, and 71% said if they could do it over again they would still come to the US. In the same study, 76% of immigrants say the government has become stricter on enforcing immigration laws since the September 11, 2001 attacks ("9/11"), and 24% report that they personally have experienced some or a great deal of discrimination.
Public attitudes about immigration in the U.S. were heavily influenced in the aftermath of the 9/11 attacks. After the attacks, 52% of Americans believed that immigration was a good thing overall for the U.S., down from 62% the year before, according to a 2009 Gallup poll. A 2008 Public Agenda survey found that half of Americans said tighter controls on immigration would do "a great deal" to enhance U.S. national security. Harvard political scientist and historian Samuel P. Huntington argued in Who Are We? The Challenges to America's National Identity that a potential future consequence of continuing massive immigration from Latin America, especially Mexico, might lead to the bifurcation of the United States.
The population of illegal Mexican immigrants in the US fell from approximately 7 million in 2007 to 6.1 million in 2011 Commentators link the reversal of the immigration trend to the economic downturn that started in 2008 and which meant fewer available jobs, and to the introduction of tough immigration laws in many states. According to the Pew Hispanic Center the total number of Mexican born persons had stagnated in 2010, and tended toward going into negative figures.
More than 80 cities in the United States, including Washington D.C., New York City, Los Angeles, Chicago, San Francisco, San Diego, San Jose, Salt Lake City, Phoenix, Dallas, Fort Worth, Houston, Detroit, Jersey City, Minneapolis, Miami, Denver, Baltimore, Seattle, Portland, Oregon and Portland, Maine, have sanctuary policies, which vary locally.
Ethnicity.
Source: US Department of Homeland Security, Office of Immigration Statistics
New reasons for immigrating to the US.
Froma Harrop, of the "Providence Journal", has written about "environmental immigration," specifically wealthier Chinese nationals moving to or buying real estate in the US to escape China's heavy industrial pollution.
Demography.
The United States admitted more legal immigrants from 1991 to 2000, between ten and eleven million, than in any previous decade. In the most recent decade, the ten million legal immigrants that settled in the U.S. represent an annual growth of only about 0.3% as the U.S. population grew from 249 million to 281 million. By comparison, the highest previous decade was the 1900s, when 8.8 million people arrived, increasing the total U.S. population by one percent every year. Specifically, "nearly 15% of Americans were foreign-born in 1910, while in 1999, only about 10% were foreign-born."
By 1970, immigrants accounted for 4.7 percent of the US population and rising to 6.2 percent in 1980, with an estimated 12.5 percent in 2009. , 25% of US residents under age 18 were first- or second-generation immigrants. Eight percent of all babies born in the U.S. in 2008 belonged to illegal immigrant parents, according to a recent analysis of U.S. Census Bureau data by the Pew Hispanic Center.
Legal immigration to the U.S. increased from 250,000 in the 1930s, to 2.5 million in the 1950s, to 4.5 million in the 1970s, and to 7.3 million in the 1980s, before resting at about 10 million in the 1990s. Since 2000, legal immigrants to the United States number approximately 1,000,000 per year, of whom about 600,000 are "Change of Status" who already are in the U.S. Legal immigrants to the United States now are at their highest level ever, at just over 37,000,000 legal immigrants. Illegal immigration may be as high as 1,500,000 per year with a net of at least 700,000 illegal immigrants arriving every year. Immigration led to a 57.4% increase in foreign born population from 1990 to 2000.
While immigration has increased drastically over the last century, the foreign born share of the population was still higher in 1900 (about 20%) than it is today (about 10%). A number of factors may be attributed to the decrease in the representation of foreign born residents in the United States. Most significant has been the change in the composition of immigrants; prior to 1890, 82% of immigrants came from North and Western Europe. From 1891 to 1920, that number dropped to 25%, with a rise in immigrants from East, Central, and South Europe, summing up to 64%. Animosity towards these different and foreign immigrants rose in the United States, resulting in much legislation to limit immigration.
Contemporary immigrants settle predominantly in seven states, California, New York, Florida, Texas, Pennsylvania, New Jersey and Illinois, comprising about 44% of the U.S. population as a whole. The combined total immigrant population of these seven states was 70% of the total foreign-born population in 2000. If current birth rate and immigration rates were to remain unchanged for another 70 to 80 years, the U.S. population would double to nearly 600 million.
The top twelve emigrant countries in 2006 were Mexico (173,753), People's Republic of China (87,345), Philippines (74,607), India (61,369), Cuba (45,614), Colombia (43,151), Dominican Republic (38,069), El Salvador (31,783), Vietnam (30,695), Jamaica (24,976), South Korea (24,386), and Guatemala (24,146). Other countries comprise an additional 606,370.
In 1900, when the U.S. population was 76 million, there were an estimated 500,000 Hispanics. The Census Bureau projects that by 2050, one-quarter of the population will be of Hispanic descent. This demographic shift is largely fueled by immigration from Latin America.
Origin.
A country is included in the table if it exceeded 50,000 in either category.
Note: Counts of immigrants since 1986 for Russia includes "Soviet Union (former)", and for Czech Republic includes "Czechoslovakia (former)".
Effects of immigration.
Demographics.
The Census Bureau estimates the US population will grow from 281 million in 2000 to 397 million in 2050 with immigration, but only to 328 million with no immigration. A 2008 report from the Pew Research Center projects that by 2050, non-Hispanic whites will account for 47% of the population, down from the 2005 figure of 67%. Non-Hispanic whites made up
85% of the population in 1960. It also foresees the Hispanic population rising from 14% in 2005 to 29% by 2050. The Asian population is expected to more than triple by 2050. Overall, the Pew Report predicts the population of the United States will rise from 296 million in 2005 to 438 million in 2050, with 82% of the increase from immigrants.
In 35 of the country's 50 largest cities, non-Hispanic whites were at the last census or are predicted to be in the minority. In California, non-Hispanic whites slipped from 80% of the state's population in 1970 to 42.3% in 2001 and 39% in 2013.
Immigrant segregation declined in the first half of the 20th century, but has been rising over the past few decades. This has caused questioning of the correctness of describing the United States as a melting pot. One explanation is that groups with lower socioeconomic status concentrate in more densely populated area that have access to public transit while groups with higher socioeconomic status move to suburban areas. Another is that some recent immigrant groups are more culturally and linguistically different from earlier groups and prefer to live together due to factors such as communication costs. Another explanation for increased segregation is white flight.
Source: 1990 and 2000 decennial Census and 2010 American Community Survey
Economic.
In a late 1980s study, economists overwhelmingly viewed immigration, including illegal immigration, as a positive for the economy. Recent surveys of leading economists shows a consensus behind the notion that high-skilled immigration makes the average American better off, and strong support behind the notion that low-skilled immigration also makes the average American better off. According to James Smith, a senior economist at Santa Monica-based RAND Corporation and lead author of the United States National Research Council's study """", immigrants contribute as much as $10 billion to the U.S. economy each year. The NRC report found that although immigrants, especially those from Latin America, caused a net loss in terms of taxes paid versus social services received, immigration can provide an overall gain to the domestic economy due to an increase in pay for higher-skilled workers, lower prices for goods and services produced by immigrant labor, and more efficiency and lower wages for some owners of capital. The report also notes that although immigrant workers compete with domestic workers for low-skilled jobs, some immigrants specialize in activities that otherwise would not exist in an area, and thus can be beneficial for all domestic residents. A non-partisan report in 2007 from the Congressional Budget Office concluded that most estimates show that illegal immigrants impose a net cost to state and local governments, but "that no agreement exists as to the size of, or even the best way of measuring, the cost on a national level." Estimates of the net national cost that illegal immigrants impose on the United States vary greatly, with the Urban Institute saying it was $1.9 billion in 1992, and a Rice University professor putting it at $19.3 billion in 1993. About twenty-one million immigrants, or about fifteen percent of the labor force, hold jobs in the United States; however, the number of unemployed is only seven million, meaning that immigrant workers are not taking jobs from domestic workers, but rather are doing jobs that would not have existed had the immigrant workers not been in the United States. U.S. Census Bureau's "Survey of Business Owners: Hispanic-Owned Firms: 2002" indicated that the number of Hispanic-owned businesses in the United States grew to nearly 1.6 million in 2002. Those businesses generated about $222 billion in gross revenue. The report notes that the burden of poor immigrants is not borne equally among states, and is most heavy in California. Another claim supporting expanding immigration levels is that immigrants mostly do jobs Americans do not want. A 2006 Pew Hispanic Center report added evidence to support this claim, when they found that increasing immigration levels have not hurt employment prospects for American workers. Research shows an economic consensus that, taken as a whole, immigrants raise living standards for American workers by boosting demand and increasing productivity, contributing to innovation, and lowering prices.
In 2009, a study by the Cato Institute, a free market think tank, found that legalization of low-skilled illegal resident workers in the US would result in a net increase in US GDP of $180 billion over ten years. The Cato Institute study did not examine the impact on per capita income for most Americans. Jason Riley notes that because of progressive income taxation, in which the top 1% of earners pay 37% of federal income taxes (even though they actually pay a lower tax percentage based on their income), 60% of Americans collect more in government services than they pay in, which also reflects on immigrants. In any event, the typical immigrant and his children will pay a net $80,000 more in their lifetime than they collect in government services according to the NAS. Legal immigration policy is set to maximize net taxation. Illegal immigrants even after an amnesty tend to be recipients of more services than they pay in taxes. In 2010, an econometrics study by a Rutgers economist found that immigration helped increase bilateral trade when the incoming people were connected via networks to their country of origin, particularly boosting trade of final goods as opposed to intermediate goods, but that the trade benefit weakened when the immigrants became assimilated into American culture.
The Kauffman Foundation's index of entrepreneurial activity is nearly 40% higher for immigrants than for natives. Immigrants were involved in the founding of many prominent American high-tech companies, such as Google, Yahoo, YouTube, Sun Microsystems, and eBay. On the poor end of the spectrum, the "New Americans" report found that low-wage immigration does not, on aggregate, lower the wages of most domestic workers. The report also addresses the question of if immigration affects black Americans differently from the population in general: "While some have suspected that blacks suffer disproportionately from the inflow of low-skilled immigrants, none of the available evidence suggests that they have been particularly hard-hit on a national level. Some have lost their jobs, especially in places where immigrants are concentrated. But the majority of blacks live elsewhere, and their economic fortunes are tied to other factors."
A study done in 2005 showed that a third of adult immigrants had not finished high school, and a third had no health insurance. Robert Samuelson points out that poor immigrants strain public services such as local schools and health care. He points out that "from 2000 to 2006, 41 percent of the increase in people without health insurance occurred among Hispanics." According to the immigration reduction advocacy group Center for Immigration Studies, 25.8% of Mexican immigrants live in poverty, which is more than double the rate for natives in 1999. In another report, The Heritage Foundation notes that from 1990 to 2006, the number of poor Hispanics increased by 3.2 million, from 6 million to 9.2 million.
U.S. citizens will not take certain jobs usually done by foreign workers, like manual labor involving agriculture. Fruit picking labor costs are estimated at $0.36 per pound, so a production rate of 1 pound per minute is required to earn minimum wage after fees are deducted. Hard physical labor and dangerous jobs with a small paycheck create labor shortages in certain job markets that can only be satisfied using foreign labor. Foreign laborers often work for no pay for several months each year to earn enough to pay their employer for the cost of their H series visa. Hispanic immigrants in the United States were hit hard by the subprime mortgage crisis. There was a disproportionate level of foreclosures in some immigrant neighborhoods. The banking industry provided home loans to undocumented immigrants, viewing it as an untapped resource for growing their own revenue stream. In October 2008, KFYI reported that according to the U.S. Department of Housing and Urban Development, five million illegal immigrants held fraudulent home mortgages. The story was later pulled from their website and replaced with a correction. The Phoenix Business Journal cited a HUD spokesman saying that there was no basis to news reports that more than five million bad mortgages were held by illegal immigrants, and that the agency had no data showing the number of illegal immigrants holding foreclosed or bad mortgages.
Immigration and foreign labor documentation fees increased over 80% in 2007, with over 90% of funding for USCIS derived from immigration application fees, creating many USCIS jobs involving immigration to US, such as immigration interview officials, finger print processor, Department of Homeland Security, etc. An article by American Enterprise Institute researcher Jason Richwine states that while earlier European immigrants were often poor when they arrived, by the third generation they had economically assimilated to be indistinguishable from the general population. However, for the Hispanic immigrants the process stalls at the second generation and the third generation continues to be substantially poorer than whites. Despite apparent disparities between different communities, Asians, a significant number of whom arrived in the United States after 1965, had the highest median income per household among all race groups as of 2008.
According to NPR in 2005, about 3% of illegal immigrants were working in agriculture. The H-2A visa allows U.S. employers to bring foreign nationals to the United States to fill temporary agricultural jobs. The passing of tough immigration laws in several states from around 2009 provides a number of practical case studies. The state of Georgia passed immigration law HB 87 in 2011; this led, according to the coalition of top Kansas businesses, to 50% of its agricultural produce being left to rot in the fields, at a cost to the state of more than $400m. Overall losses caused by the act were $1bn; it was estimated that the figure would become over $20bn if all the estimated 325,000 undocumented workers left Georgia. The cost to Alabama of its crackdown in June 2011 has been estimated at almost $11bn, with up to 80,000 unauthorised immigrant workers leaving the state.
While immigration from Latin America has kept the United States from falling off a Japanese or European style demographic cliff, this is a limited resource as fertility rates continue to decline throughout the Americas and the world.
According to Congressional Research Service, since the 1970s while immigration to the United States increased, the lower 90% of tax filers incomes became stagnant, and eventually began to decrease since 2000.
Social.
Irish immigration was opposed in the 1850s by the nativist Know Nothing movement, originating in New York in 1843. It was engendered by popular fears that the country was being overwhelmed by Irish Catholic immigrants. On March 14, 1891, a lynch mob stormed a local jail and lynched several Italians following the acquittal of several Sicilian immigrants alleged to be involved in the murder of New Orleans police chief David Hennessy. The Congress passed the Emergency Quota Act in 1921, followed by the Immigration Act of 1924. The Immigration Act of 1924 was aimed at limiting immigration overall, and making sure that the nationalities of new arrivals matched the overall national profile.
After the September 11 attacks, many Americans entertained doubts and suspicions about people apparently of Middle-Eastern origins. NPR in 2010 fired a prominent black commentator, Juan Williams, when he talked publicly about his fears on seeing people dressed like Muslims on airplanes.
Racist thinking among and between minority groups does occur; examples of this are conflicts between blacks and Korean immigrants, notably in the 1992 Los Angeles Riots, and between African Americans and non-white Latino immigrants. There has been a long running racial tension between African American and Mexican prison gangs, as well as significant riots in California prisons where they have targeted each other, for ethnic reasons. There have been reports of racially motivated attacks against African Americans who have moved into neighborhoods occupied mostly by people of Mexican origin, and vice versa. There has also been an increase in violence between non-Hispanic Anglo Americans and Latino immigrants, and between African immigrants and African Americans.
A 2007 study on assimilation found that Mexican immigrants are less fluent in English than both non-Mexican Hispanic immigrants and other immigrants. While English fluency increases with time stayed in the United States, although further improvements after the first decade are limited, Mexicans never catch up with non-Mexican Hispanics, who never catch up with non-Hispanics. The study also writes that "Even among immigrants who came to the United States before they were ﬁve years old and whose entire schooling was in the United States, those Mexican born have average education levels of 11.7 years, whereas those from other countries have average levels of education of 14.1 years." Unlike other immigrants, Mexicans have a tendency to live in communities with many other Mexicans which decreases incentives for assimilation. Correcting for this removes about half the fluency difference between Mexicans and other immigrants.
Religious diversity.
Immigration from South Asia and elsewhere has contributed to enlarging the religious composition of the United States. Islam in the United States is growing mainly due to immigration. Hinduism in the United States, Buddhism in the United States, and Sikhism in the United States are other examples.
Since 1992, an estimated 1.7 million Muslims, approximately 1 million Hindus, and approximately 1 million Buddhists have immigrated legally to the United States.
Political.
A "Boston Globe" article attributed Barack Obama's win in the 2008 U.S. Presidential election to a marked reduction over the preceding decades in the percentage of whites in the American electorate, attributing this demographic change to the Immigration Act of 1965. The article quoted Simon Rosenberg, president and founder of the New Democrat Network, as having said that the Act is "the most important piece of legislation that no one's ever heard of," and that it "set America on a very different demographic course than the previous 300 years."
Immigrants differ on their political views; however, the Democratic Party is considered to be in a far stronger position among immigrants overall. Research shows that religious affiliation can also significantly impact both their social values and voting patterns of immigrants, as well as the broader American population. Hispanic evangelicals, for example, are more strongly conservative than non-Hispanic evangelicals. This trend is often similar for Hispanics or others strongly identifying with the Catholic Church, a religion that strongly opposes abortion and gay marriage.
The key interests groups that lobby on immigration are religious, ethnic and business groups, together with some liberals and some conservative public policy organizations. Both the pro- and anti- groups affect policy.
Studies have suggested that some special interest group lobby for less immigration for their own group and more immigration for other groups since they see effects of immigration, such as increased labor competition, as detrimental when affecting their own group but beneficial when affecting other groups.
A 2007 paper found that both pro- and anti-immigration special interest groups play a role in migration policy. "Barriers to migration are lower in sectors in which business lobbies incur larger lobbying expenditures and higher in sectors where labor unions are more important." A 2011 study examining the voting of US representatives on migration policy suggests that "representatives from more skilled labor abundant districts are more likely to support an open immigration policy towards the unskilled, whereas the opposite is true for representatives from more unskilled labor abundant districts."
After the 2010 election, Gary Segura of Latino Decisions stated that Hispanic voters influenced the outcome and "may have saved the Senate for Democrats". Several ethnic lobbies support immigration reforms that would allow illegal immigrants that have succeeded in entering to gain citizenship. They may also lobby for special arrangements for their own group. The Chairman for the Irish Lobby for Immigration Reform has stated that "the Irish Lobby will push for any special arrangement it can get — 'as will every other ethnic group in the country.'" The irrendentist and ethnic separatist movements for Reconquista and Aztlán see immigration from Mexico as strengthening their cause.
The book "Ethnic Lobbies and US Foreign Policy" (2009) states that several ethnic special interest groups are involved in pro-immigration lobbying. Ethnic lobbies also influence foreign policy. The authors write that "Increasingly, ethnic tensions surface in electoral races, with House, Senate, and gubernatorial contests serving as proxy battlegrounds for antagonistic ethnoracial groups and communities. In addition, ethnic politics affect party politics as well, as groups compete for relative political power within a party". However, the authors argue that currently ethnic interest groups, in general, do not have too much power in foreign policy and can balance other special interest groups.
In a 2012 news story, "Reuters" reported, "Strong support from Hispanics, the fastest-growing demographic in the United States, helped tip President Barack Obama's fortunes as he secured a second term in the White House, according to Election Day polling."
Lately, there is talk among several Republican leaders, such as governors Bobby Jindal and Susana Martinez, of taking a new, friendlier approach to immigration. Former US Secretary of Commerce Carlos Gutierrez is promoting the creation of Republicans for Immigration Reform.
Health.
The issue of the health of immigrants and the associated cost to the public has been largely discussed. The non-emergency use of emergency rooms ostensibly indicates an incapacity to pay, yet some studies allege disproportionately lower access to unpaid health care by immigrants. For this and other reasons, there have been various disputes about how much immigration is costing the United States public health system. University of Maryland economist and Cato Institute scholar Julian Lincoln Simon concluded in 1995 that while immigrants probably pay more into the health system than they take out, this is not the case for elderly immigrants and refugees, who are more dependent on public services for survival.
Immigration from areas of high incidences of disease is thought to have fueled the resurgence of tuberculosis (TB), chagas, and hepatitis in areas of low incidence. According to Centers for Disease Control and Prevention (CDC), TB cases among foreign-born individuals remain disproportionately high, at nearly nine times the rate of U.S.-born persons. To reduce the risk of diseases in low-incidence areas, the main countermeasure has been the screening of immigrants on arrival. HIV/AIDS entered the United States in around 1969, likely through a single infected immigrant from
Haiti. Conversely, many new HIV infections in Mexico can be traced back to the United States. People infected with HIV were banned from entering the United States in 1987 by executive order, but the 1993 statute supporting the ban was lifted in 2009. The executive branch is expected to administratively remove HIV from the list of infectious diseases barring immigration, but immigrants generally would need to show that they would not be a burden on public welfare. Researchers have also found what is known as the "healthy immigrant effect", in which immigrants in general tend to be healthier than individuals born in the U.S.
Crime.
Empirical studies on links between immigration and crime are mixed.
Some writers have suggested that immigrants are under-represented in criminal statistics. In his 1999 book "Crime and Immigrant Youth", sociologist Tony Waters argued that immigrants themselves are less likely to be arrested and incarcerated; he also argued, however, that the children of some immigrant groups are more likely to be arrested and incarcerated. This is a by-product of the strains that emerge between immigrant parents living in poor, inner city neighborhoods. This occurs particularly in immigrant groups with many children as they begin to form particularly strong peer sub-cultures. A 1999 paper by John Hagan and Alberto Palloni estimated that the involvement in crime by Hispanic immigrants is less than that of other citizens. A 2006 Op-Ed in "The New York Times" by Harvard University Professor in Sociology Robert J. Sampson says that immigration of Hispanics may in fact be associated with decreased crime. In 2008, Sampson wrote in Contexts that first-generation immigrants to the U.S. are 45% less likely to commit violence than third-generation immigrants. Research also suggests that immigration between 1990 and 2000 (and between 1994 and 2004) may partly explain the reduction in the US crime rate. Furthermore, one study finds that Secure Communities, an immigration enforcement program which led to a quarter of a million of detentions (when the study was published; November 2014), had no observable impact on the crime rate. A 2015 study found that the 1986 Immigration Reform and Control Act, which legalized almost 3 million immigrants, led to "decreases in crime of 3-5 percent, primarily due to decline in property crimes, equivalent to 120,000-180,000 fewer violent and property crimes committed each year due to legalization". A 2009 study found evidence that immigration to the U.S. lowered crime rates by preserving two-parent family structures. 
A 2006 article by Migration Policy Institute cited data from the 2000 US Census as evidence for that foreign-born men had lower incarceration rates than native-born men. According to a 2007 report by the Immigration Policy Center, the American Immigration Law Foundation, citing data from the 2000 US Census, native-born American men between 18–39 are five times more likely to be incarcerated than immigrants in the same demographic. A 2008 study by the Public Policy Institute of California, found that, "...on average, between 2000 and 2005, cities that had a higher share of recent immigrants (those arriving between 2000 and 2005) saw their crime rates fall further than cities with a lower share" but adds, "As with most studies, we do not have ideal data. This lack of data restricts the questions we will be able to answer. In particular, we cannot focus on the undocumented population explicitly". In a study released by the same Institute, immigrants were ten times less likely to be incarcerated than native born Americans.
Explanations for the lower incarceration rates of immigrants include:
Heather MacDonald at the Manhattan Institute in a 2004 article argued that sanctuary policies has caused large problems with crime by illegal aliens since the police cannot report them for deportation before a felony or a series of misdemeanors takes place. In Los Angeles, 95 percent of all outstanding warrants for homicide are for illegal aliens. Up to two-thirds of all fugitive felony warrants (17,000) are for illegal aliens. 60 percent of the 20,000-strong 18th Street Gang in southern California were illegal aliens in a 1995 report.
The Center for Immigration Studies in a 2009 report argued that "New government data indicate that immigrants have high rates of criminality, while older academic research found low rates. The overall picture of immigrants and crime remains confused due to a lack of good data and contrary information." It also criticized the reports by the Public Policy Institute of California and Immigration Policy Center for using data from the 2000 Census according to which 4% of prisoners were immigrants. Non-citizens often have a strong incentive to deny this in order to prevent deportation and there are also other problems. Better methods have found 20–22% immigrants. It also criticized studies looking at percentages of immigrants in a city and crime for only looking at overall crime and not immigrant crime. A 2009 analysis by the Department of Homeland Security found that crime rates were higher in metropolitan areas that received large numbers of legal immigrants, contradicting several older cross-city comparisons.
Environment.
Some commentators have suggested that increased immigration has a negative effect on the environment, especially as the level of economic development of the United States (and by extension, its energy, water and other needs that underpin its prosperity) means that the impact of a larger population is greater than what would be experienced in other countries.
Perceived heavy immigration, especially in the southwest, has led to some fears about population pressures on the water supply in some areas. California continues to grow by more than a half-million a year and is expected to reach 48 million in 2030. According to the California Department of Water Resources, if more supplies are not found by 2020, residents will face a water shortfall nearly as great as the amount consumed today. Los Angeles is a coastal desert able to support at most one million people on its own water. California is considering using desalination to solve this problem.
Education.
Scientific laboratories and startup internet opportunities have been a powerful American magnet. By 2000, 23% of scientists with a PhD in the U.S. were immigrants, including 40% of those in engineering and computers. Roughly a third of the United State's college and universities graduate students in STEM fields are foreign nationals – in some states it is well over half of their graduate students. On Ash Wednesday, March 5, 2014, the presidents of 28 Catholic and Jesuit colleges and universities, joined the "Fast for Families" movement. The "Fast for Families" movement reignited the immigration debate in the fall of 2013 when the movement's leaders, supported by many members of Congress and the President, fasted for twenty-two days on the National Mall in Washington, D.C.
A study on public schools in California found that white enrollment declined in response to increases in the number of Spanish-speaking Limited English Proficient and Hispanic students. This white flight was greater for schools with relatively larger proportions of Spanish-speaking Limited English Proficient.
Among 25- to 44-year-olds, 55% of Hispanic immigrants that arrived after age 13 had not completed high school.
Effects on African Americans.
Harvard economist George J. Borjas stated that there is controversy over the "huge redistribution wealth away from U.S. Citizen workers to employers who use illegal immigrants." He suggested that immigration had detrimental effects on African-American employment in terms of lower wages and the number of persons employed. He also reported that a 10% increase in the supply of workers reduced the black wage of that group by 2.5%, lowered the employment rate by 5.9% and increased the black incarceration rate by 1.3%.
Public opinion.
The ambivalent feeling of Americans toward immigrants is shown by a positive attitude toward groups that have been visible for a century or more, and much more negative attitude toward recent arrivals. For example, a 1982 national poll by the Roper Center at the University of Connecticut showed respondents a card listing a number of groups and asked, "Thinking both of what they have contributed to this country and have gotten from this country, for each one tell me whether you think, on balance, they've been a good or a bad thing for this country," which produced the results shown in the table. "By high margins, Americans are telling pollsters it was a very good thing that Poles, Italians, and Jews emigrated to America. Once again, it's the newcomers who are viewed with suspicion. This time, it's the Mexicans, the Filipinos, and the people from the Caribbean who make Americans nervous."
In a 2002 study, which took place soon after the September 11 attacks, 55% of Americans favored decreasing legal immigration, 27% favored keeping it at the same level, and 15% favored increasing it.
In 2006, the immigration-reduction advocacy think tank the Center for Immigration Studies released a poll that found that 68% of Americans think U.S. immigration levels are too high, and just 2% said they are too low. They also found that 70% said they are less likely to vote for candidates that favor increasing legal immigration. In 2004, 55% of Americans believed legal immigration should remain at the current level or increased and 41% said it should be decreased. The less contact a native-born American has with immigrants, the more likely one would have a negative view of immigrants.
One of the most important factors regarding public opinion about immigration is the level of unemployment; anti-immigrant sentiment is where unemployment is highest, and vice versa.
Surveys indicate that the U.S. public consistently makes a sharp distinction between legal and illegal immigrants, and generally views those perceived as "playing by the rules" with more sympathy than immigrants that have entered the country illegally.
Legal issues.
Laws concerning immigration and naturalization.
Laws concerning immigration and naturalization include:
AEDPA and IIRARA exemplify many categories of criminal activity for which immigrants, including green card holders, can be deported and have imposed mandatory detention for certain types of cases.
Asylum for refugees.
In contrast to economic migrants, who generally do not gain legal admission, refugees, as defined by international law, can gain legal status through a process of seeking and receiving asylum, either by being designated a refugee while abroad, or by physically entering the United States and requesting asylum status thereafter. A specified number of legally defined refugees, who either apply for asylum overseas or after arriving in the U.S., are admitted annually. Refugees compose about one-tenth of the total annual immigration to the United States, though some large refugee populations are very prominent. In the year 2014, the number of asylum seekers accepted into the U.S. was about 120,000. This compared with about 31,000 in the UK and 13,500 in Canada. Japan accepted just 41 refugees for resettlement in 2007.
Since 1975, more than 1.3 million refugees from Asia have been resettled in the United States. Since 2000 the main refugee-sending regions have been Somalia, Liberia, Sudan, and Ethiopia. The ceiling for refugee resettlement for fiscal year 2008 was 80,000 refugees. The United States expected to admit a minimum of 17,000 Iraqi refugees during fiscal year 2009. The U.S. has resettled more than 42,000 Bhutanese refugees from Nepal since 2008.
In fiscal year 2008, the Office of Refugee Resettlement (ORR) appropriated over $655 million for long-term services provided to refugees after their arrival in the US. The Obama administration has kept to about the same level.
Miscellaneous documented immigration.
In removal proceedings in front of an immigration judge, cancellation of removal is a form of relief that is available for certain long-time residents of the United States. It allows a person being faced with the threat of removal to obtain permanent residence if that person has been physically present in the U.S. for at least ten years, has had good moral character during that period, has not been convicted of certain crimes, and can show that removal would result in exceptional and extremely unusual hardship to his or her U.S. citizen or permanent resident spouse, children, or parent. This form of relief is only available when a person is served with a Notice to Appear to appear in the proceedings in the court.
Members of Congress may submit private bills granting residency to specific named individuals. A special committee vets the requests, which require extensive documentation. The Central Intelligence Agency has the statutory authority to admit up to one hundred people a year outside of normal immigration procedures, and to provide for their settlement and support. The program is called "PL110", named after the legislation that created the agency, Public Law 110, the Central Intelligence Agency Act.
Illegal immigration.
The Illegal immigrant population of the United States is estimated to be between 7 and 20 million. The majority of the illegal immigrants are from Mexico.
In a 2011 news story, "Los Angeles Times" reported, "The annual report, relied upon by both sides in the contentious immigration debate, found 11.2 million illegal immigrants living in the U.S., statistically identical to the 11.1 million estimated in 2009. ... The report also found that illegal immigrants in 2010 were parents of 5.5 million children, 4.5 million of whom were born in the U.S. and are citizens. Because illegal immigrants are younger and more likely to be married, they represented a disproportionate share of births — 8% of the babies born in the U.S. between March 2009 and March 2010 were to at least one illegal immigrant parent."
In June 2012, President Obama issued a memorandum instructing officers of the federal government to defer deporting young illegal immigrants who had been brought to the U.S. as children. Obama's new non-deportation policy allows 1.7 million illegal immigrants to apply for the temporary right to live and work in the United States. The memorandum is the move by the Obama administration to use its executive powers to revise immigration procedures without changing the law. Beginning March 4, 2013, illegal immigrants who can show that time apart from a U.S. spouse, child or parent would create "extreme hardship" can apply for legal visas without leaving the U.S.
On November 25, 2013, Ju Hong, a 24-year-old South Korean immigrant without legal documentation, shouted at Obama to use his executive power to stop deportation of illegal immigrants. Obama said "If, in fact, I could solve all these problems without passing laws in Congress, then I would do so." "But we're also a nation of laws, that's part of our tradition," he continued. "And so the easy way out is to try to yell and pretend like I can do something by violating our laws. And what I'm proposing is the harder path, which is to use our democratic processes to achieve the same goal."
On November 20, 2014, President Obama announced a set of executive actions which could allow up to an estimated 45% of undocumented immigrants to legally stay and work in the United States. Although not without precedent under prior presidents, these actions do amount to at least a change in tactics on the part of the Obama administration. In his announcement, the president said he still hoped for "a more permanent legislative solution" that would make his actions "no longer necessary."
Military immigration.
On November 15, 2013 the United States Citizenship and Immigration Services announced that they would be issuing a new policy memorandum called "parole in place." Parole in place would offer green cards to immigrant parents, spouses and children of active military duty personnel. Prior to this law relatives of military personnel-excluding husbands and wives- were forced to leave the United States and apply for green cards in their home countries. The law allows for family members to avoid the possible ten-year bar from the United States and remain in the United States while applying for lawful permanent residence. The parole status, given in one year terms, will be subject to the family member being "absent a criminal conviction or other serious adverse factors."
Immigration in popular culture.
The history of immigration to the United States is the history of the country itself, and the journey from beyond the sea is an element found in American folklore, appearing over and over again in everything from "The Godfather" to "Gangs of New York" to "The Song of Myself" to Neil Diamond's "America" to the animated feature "An American Tail".
From the 1880s to the 1910s, vaudeville dominated the popular image of immigrants, with very popular caricature portrayals of ethnic groups. The specific features of these caricatures became widely accepted as accurate portrayals.
In "The Melting Pot" (1908), playwright Israel Zangwill (1864–1926) explored issues that dominated Progressive Era debates about immigration policies. Zangwill's theme of the positive benefits of the American melting pot resonated widely in popular culture and literary and academic circles in the 20th century; his cultural symbolism – in which he situated immigration issues – likewise informed American cultural imagining of immigrants for decades, as exemplified by Hollywood films.
The popular culture's image of ethnic celebrities often includes stereotypes about immigrant groups. For example, Frank Sinatra's public image as a superstar contained important elements of the "American Dream" while simultaneously incorporating stereotypes about Italian Americans that were based in nativist and Progressive responses to immigration.
The process of assimilation has been a common theme of popular culture. For example, "lace-curtain Irish" refers to middle-class Irish Americans desiring assimilation into mainstream society in counterpoint to the older, more raffish "shanty Irish". The occasional malapropisms and left-footed social blunders of these upward mobiles were gleefully lampooned in vaudeville, popular song, and the comic strips of the day such as "Bringing Up Father", starring Maggie and Jiggs, which ran in daily newspapers for 87 years (1913 to 2000). In "The Departed" (2006), Staff Sergeant Dignam regularly points out the dichotomy between the lace curtain Irish lifestyle Billy Costigan enjoyed with his mother, and the shanty Irish lifestyle of Costigan's father. In recent years the popular culture has paid special attention to Mexican immigration and the film "Spanglish" (2004) tells of a friendship of a Mexican housemaid (Paz Vega) and her boss played by Adam Sandler.
Immigration in literature.
Novelists and writers have captured much of the color and challenge in their immigrant lives through their writings.
Regarding Irish women in the 19th century, there were numerous novels and short stories by Harvey O'Higgins, Peter McCorry, Bernard O'Reilly and Sarah Orne Jewett that emphasize emancipation from Old World controls, new opportunities and expansiveness of the immigrant experience.
On the other hand, Hladnik studies three popular novels of the late 19th century that warned Slovenes not to immigrate to the dangerous new world of the United States.
Jewish American writer Anzia Yezierska wrote her novel "Bread Givers" (1925) to explore such themes as Russian-Jewish immigration in the early 20th century, the tension between Old and New World Yiddish culture, and women's experience of immigration. A well established author Yezierska focused on the Jewish struggle to escape the ghetto and enter middle- and upper-class America. In the novel, the heroine, Sara Smolinsky, escape from New York City's "down-town ghetto" by breaking tradition. She quits her job at the family store and soon becomes engaged to a rich real-estate magnate. She graduates college and takes a high-prestige job teaching public school. Finally Sara restores her broken links to family and religion.
The Swedish author Vilhelm Moberg in the mid-20th century wrote a series of four novels describing one Swedish family's migration from Småland to Minnesota in the late 19th century, a destiny shared by almost one million people. The author emphasizes the authenticity of the experiences as depicted (although he did change names). These novels have been translated into English ("The Emigrants", 1951, "Unto a Good Land", 1954, "The Settlers", 1961, "The Last Letter Home", 1961). The musical Kristina från Duvemåla by ex-ABBA members Björn Ulvaeus and Benny Andersson is based on this story.
"The Immigrant" is a musical by Steven Alper, Sarah Knapp, and Mark Harelik. The show is based on the story of Harelik's grandparents, Matleh and Haskell Harelik, who traveled to Galveston, Texas in 1909.
Documentary films.
In their documentary "", filmmakers Shari Robertson and Michael Camerini examine the American political system through the lens of immigration reform from 2001 to 2007. Since the debut of the first five films, the series has become an important resource for advocates, policy-makers and educators.
That film series premiered nearly a decade after the filmmakers' landmark documentary film "Well-Founded Fear" which provided a behind-the-scenes look at the process for seeking asylum in the United States. That film still marks the only time that a film-crew was privy to the private proceedings at the U.S. Immigration and Naturalization Service (INS), where individual asylum officers ponder the often life-or-death fate of immigrants seeking asylum.
Legal perspectives.
University of North Carolina law professor Hiroshi Motomura has identified three approaches the United States has taken to the legal status of immigrants in his book "Americans in Waiting: The Lost Story of Immigration and Citizenship in the United States". The first, dominant in the 19th century, treated immigrants as in transition; in other words, as prospective citizens. As soon as people declared their intention to become citizens, they received multiple low-cost benefits, including the eligibility for free homesteads in the Homestead Act of 1869, and in many states, the right to vote. The goal was to make the country more attractive, so large numbers of farmers and skilled craftsmen would settle new lands. By the 1880s, a second approach took over, treating newcomers as "immigrants by contract". An implicit deal existed where immigrants who were literate and could earn their own living were permitted in restricted numbers. Once in the United States, they would have limited legal rights, but were not allowed to vote until they became citizens, and would not be eligible for the New Deal government benefits available in the 1930s. The third and more recent policy is "immigration by affiliation", which Motomura argues is the treatment which depends on how deeply rooted people have become in the country. An immigrant who applies for citizenship as soon as permitted, has a long history of working in the United States, and has significant family ties, is more deeply affiliated and can expect better treatment.
It has been suggested that the US should adopt policies similar to those in Canada and Australia and select for desired qualities such as education and work experience. Another suggestion is to reduce legal immigration because of being a relative, except for nuclear family members, since such immigrations of extended relatives, who in turn bring in their own extended relatives, may cause a perpetual cycle of "chain immigration".
Interpretive perspectives.
The American Dream is the belief that through hard work and determination, any United States immigrant can achieve a better life, usually in terms of financial prosperity and enhanced personal freedom of choice. According to historians, the rapid economic and industrial expansion of the U.S. is not simply a function of being a resource rich, hard working, and inventive country, but the belief that anybody could get a share of the country's wealth if he or she was willing to work hard. This dream has been a major factor in attracting immigrants to the United States.

</doc>
<doc id="15052" url="https://en.wikipedia.org/wiki?curid=15052" title="Image and Scanner Interface Specification">
Image and Scanner Interface Specification

Image and Scanner Interface Specification (ISIS) is an industry standard interface for image scanning technologies, developed by Pixel Translations in 1990 (today: EMC captiva).
ISIS is an open standard for scanner control and a complete image-processing framework. It is currently supported by a number of application and scanner vendors.
Functions.
The modular design allows the scanner to be accessed both directly or with built-in routines to handle most situations automatically.
A message-based interface with tags is used so that features, operations, and formats not yet supported by ISIS can be added as desired without waiting for a new version of the specification.
The standard addresses all of the issues that an application using a scanner needs to be concerned with. Functions include but are not limited to selecting, installing, and configuring a new scanner; setting scanner-specific parameters; scanning, reading and writing files, and fast image scaling, rotating, displaying, and printing. Drivers have been written to dynamically process data for operations such as converting grayscale to binary image data.
An ISIS interface can run scanners at or above their rated speed by linking drivers together in a pipe so that data flows from a scanner driver to compression driver, to packaging driver, to a file, viewer, or printer in a continuous stream, usually without the need to buffer more than a small portion of the full image. As a result of using the piping method, each driver can be optimised to perform one function well. Drivers are typically small and modular in order to make it simple to add new functionality to an existing application.

</doc>
<doc id="15053" url="https://en.wikipedia.org/wiki?curid=15053" title="Ivo Caprino">
Ivo Caprino

Ivo Caprino (17 February 1920 – 8 February 2001) was a Norwegian film director and writer, best known for his puppet films. His most famous film is "Flåklypa Grand Prix" ("Pinchcliffe Grand Prix"), made in 1975.
Early career.
In the mid-1940s, Caprino helped his mother design puppets for a puppet theatre, which inspired him to try making a film using his mother's designs. The result of their collaboration was "Tim og Tøffe", an 8-minute film released in 1949 . Several films followed in the next couple of years, including two 15-minute shorts that are still shown regularly in Norway today, "Veslefrikk med Fela" (Little Freddy and his Fiddle), based on a Norwegian folk tale, and "Karius og Baktus", a story by Thorbjørn Egner of two little trolls, representing Caries and Bacterium, living in a boy's teeth. Ingeborg Gude made the puppets for these films as well, as she would continue to do up until her death in the mid sixties.
When making "Tim og Tøffe", Caprino invented an ingenious method for controlling the puppet's movements in real time. The technique can be described as a primitive, mechanical version of animatronics.
Caprino's films received rave reviews, and he quickly became a celebrity in Norway. In particular, the public were fascinated with the secret technology used to make his films. When he switched to traditional stop motion, Caprino tried to maintain the impression that he was still using some kind of "magic" technology to make the puppets move, even though all his later films were made with traditional stop motion techniques.
In addition to the short films, Caprino produced dozens of advertising films with puppets. In 1959, he directed a live action feature film, "Ugler i Mosen", which also contained stop motion sequences. He then embarked on his most ambitious project, a feature film about Peter Christen Asbjørnsen, who travelled around Norway in the 19th century collecting traditional folk tales. The plan was to use live action for the sequences showing Asbjørnsen, and then to realise the folk tales using stop motion. Unfortunately, Caprino was unable to secure funding for the project, so he ended up making the planned folk tale sequences as separate 16-minute puppet films, bookended by live action sequences showing Asbjørnsen.
"The Pinchcliffe Grand Prix".
In 1970, Caprino and his small team of collaborators, started work on a 25 minutes TV special, which would eventually become "The Pinchcliffe Grand Prix". Based on a series of books by Norwegian cartoonist and author Kjell Aukrust, it featured a group of eccentric characters all living in the small village of Pinchcliffe. The TV special was a collection of sketches based on Aukrust's books, with no real story line. After 1.5 years of work, it was decided that it didn't really work as a whole, so production on the TV special was stopped (with the exception of some very short clips, no material from it has ever been seen by the public), and Caprino and Aukrust instead wrote a screenplay for a feature film using the characters and environments that had already been built.
The result was "The Pinchcliffe Grand Prix", which stars Theodore Rimspoke (No. Reodor Felgen) and his two assistants, Sonny Duckworth (No. Solan Gundersen), a cheerful and optimistic bird, and Lambert (No. Ludvig), a nervous, pessimistic and melancholic hedgehog. Theodore works as a bicycle repairman, though he spends most of his time inventing weird Rube Goldberg-like contraptions. One day, the trio discover that one of Theodore's former assistants, Rudolph Gore-Slimey (), has stolen his design for a race car engine, and has become a world champion Formula One driver.
Sonny secures funding from an Arab oil sheik who happens to be vacationing in Pinchcliffe, and the trio then build a gigantic racing car, "Il Tempo Gigante" – a fabulous construction with two engines, radar and its own blood bank. Theodore then enters a race, and ends up winning, beating Gore-Slimey despite his attempts at sabotage.
The film was made in 3.5 years by a team of approximately 5 people. Caprino directed and animated, Bjarne Sandemose (Caprino's principal collaborator throughout his career) built the sets and the cars, and was in charge of the technical side, Ingeborg Riiser modeled the puppets and Gerd Alfsen made the costumes and props.
When it came out in 1975, The Pinchcliffe Grand Prix was an enormous success in Norway, selling 1 million tickets in its first year of release. It remains the biggest box office hit of all time in Norway (Caprino Studios claim it has sold 5.5 million tickets to date) and was also released in many other countries.
To help promote the film abroad, Caprino and Sandemose built a full-scale replica of Il Tempo Gigante that is legal for public roads, but is usually exposited at Hunderfossen Familiepark.
Later career.
Except for some TV work in the late seventies, Caprino made no more puppet films, focusing instead on creating attractions for the "Hunderfossen" theme park outside Lillehammer based on his folk tale movies, and making tourist films using a custom built multi camera setup of his own design that shoots 280 degrees panorama movies.
Personal life.
Caprino was the son of furniture designer Mario Caprino and the artist Ingeborg Gude, who was a granddaughter of the painter Hans Gude. He was born and died in Oslo, but lived all of his life at Snarøya in Bærum. He died in 2001 after having lived several years with a cancer diagnosis. Since Caprino's death, his son Remo has had great success developing a computer game based on "Flåklypa Grand Prix".

</doc>
<doc id="15054" url="https://en.wikipedia.org/wiki?curid=15054" title="Intel 80286">
Intel 80286

The Intel 80286 (also marketed as the iAPX 286 and often called Intel 286) is a 16-bit microprocessor that was introduced on 1 February 1982. It was the first 8086 based CPU with separate, non-multiplexed, address and data buses and also the first with memory management and wide protection abilities. The 80286 used approximately 134,000 transistors in its original nMOS (HMOS) incarnation and, just like the contemporary 80186, it could correctly execute most software written for the earlier Intel 8086 and 8088 processors.
The 80286 was employed for the IBM PC/AT, introduced in 1984, and then widely used in most PC/AT compatible computers until the early 1990s. Although now long since obsolete for use in personal computers, 80286 based processors are still widely used in embedded microcontroller applications.
History and performance.
After the 6 and 8 MHz initial releases, Intel subsequently scaled it up to 12.5 MHz. AMD and Harris later pushed the architecture to 20 MHz and 25 MHz, respectively. Intersil and Fujitsu also designed fully static CMOS versions of Intel's original depletion-load nMOS implementation, largely aimed at battery powered devices.
On average, the 80286 was reportedly measured to have a speed of about 0.21 instructions per clock on "typical" programs, although it could be significantly faster on optimized code and in tight loops as many instructions could execute in 2 clock cycles. The 6 MHz, 10 MHz and 12 MHz models were reportedly measured to operate at 0.9 MIPS, 1.5 MIPS and 2.66 MIPS respectively.
The later E-stepping level of the 80286 was free of the several significant errata that caused problems for programmers and operating system writers in the earlier B-step and C-step CPUs (common in the AT and AT clones).
Architecture.
The 80286 was designed for multi-user systems with multitasking applications, including communications (such as automated PBXs) and real-time process control. It had 134,000 transistors and consisted of four independent units: address unit, bus unit, instruction unit and execution unit organized into a loosely coupled (buffered) pipeline just as in the 8086. The significantly increased performance over the 8086 was primarily due to the non-multiplexed address and data buses, more address calculation hardware (most importantly a dedicated adder) and a faster (more hardware based) multiplier. It was produced in a 68-pin package including PLCC (Plastic Leaded Chip Carrier), LCC (Leadless chip carrier) and PGA (Pin Grid Array) packages.
The performance increase of the 80286 over the 8086 (or 8088) could be more than 100% per clock cycle in many programs (i.e. a doubled performance at the same clock speed). This was a large increase, fully comparable to the speed improvements around a decade later when the i486 (1989) or the original Pentium (1993) were introduced. This was partly due to the non-multiplexed address and data buses but mainly to the fact that address calculations (such as base+index) were less expensive. They were performed by a dedicated unit in the 80286 while the older 8086 had to do effective address computation using its general ALU, consuming several extra clock cycles in many cases. Also, the 80286 was more efficient in the prefetch of instructions, buffering, execution of jumps, and in complex microcoded numerical operations such as MUL/DIV than its predecessor.
The intel 80286 had a 24-bit address bus and was able to address up to 16 MB of RAM, compared to 1 MB for its predecessor. However cost and initial rarity of software using the memory above 1 MB meant that 80286 computers were rarely shipped with more than one megabyte of RAM. Additionally, there was a performance penalty involved in accessing extended memory from real mode, as noted below.
Features.
Protected mode.
The 286 was the first of the x86 CPU family to support protected mode. In addition, it was the first commercially available microprocessor with on-chip MMU capabilities. (Systems using the contemporaneous Motorola 68010 and NS320xx could be equipped with an optional MMU controller.) This would allow IBM compatibles to have advanced multitasking OSes for the first time and compete in the Unix-dominated server/workstation market.
Several additional instructions were introduced in protected mode of 80286, which are helpful for multitasking operating systems.
Another important feature of 80286 is Prevention of Unauthorized Access. This is achieved by:
In 80286 (and in its co-processor Intel 80287), arithmetic operations can be performed on the following different types of numbers:
By design, the 286 could not revert from protected mode to the basic 8086-compatible "real mode" without a hardware-initiated reset. In the PC/AT introduced in 1984, IBM added external circuitry as well as specialized code in the ROM BIOS to enable special series of program instructions to cause the reset, allowing real-mode reentry (while retaining active memory and control). Though it worked correctly, the method imposed a huge performance penalty.
In theory, real-mode applications could be directly executed in 16-bit protected mode if certain rules were followed; however, as many DOS programs broke those rules, protected mode was not widely used until the appearance of its successor, the 32-bit Intel 80386, which was designed to go back and forth between modes easily. When Intel designed the 286, it was not designed to be able to multitask real-mode applications; real mode was intended to be a simple way for a bootstrap loader to prepare the system and then switch to protected mode.
OS support.
The protected mode of the 80286 was not utilized until many years after its release, in part because of the high cost of adding extended memory to a PC, but also because of the need for software to support the large user base of 8086 PCs. For example, in 1986 the only program that made use of it was VDISK, a RAM disk driver included with PC DOS 3.0 and 3.1. A DOS could utilize the additional RAM available in protected mode (extended memory) either via a BIOS call (INT 15h, AH=87h), as a RAM disk, or as emulation of expanded memory. The difficulty lay in the incompatibility of older real mode DOS programs with protected mode. They simply could not natively run in this new mode without significant modification. In protected mode, memory management and interrupt handling were done differently than in real mode. In addition, DOS programs were not allowed to access or modify data and code segments that did not belong to them, as they could in real mode. The choice that OS developers faced was either to start from scratch and create an OS that would not run the vast majority of the old programs, or to come up with a version of DOS that was slow and ugly but would still run a majority of the old programs. Protected mode also did not provide a significant enough performance advantage over the 8086-compatible real mode to justify supporting its capabilities. Registers were still 16-bit and when put into protected mode, the programmer was forced to use a memory map composed of 64k segments just like in real mode.
In January 1985, Digital Research previewed the Concurrent DOS 286 operating system made in cooperation with Intel. The product would function strictly as an 80286 native mode operating system, allowing users to take full advantage of the protected mode to perform multi-user, multitasking operations while running 8086 emulation. This worked on the B-1 prototype step of the chip, but Digital Research discovered problems with the emulation on the production level C-1 step in May, which would not allow Concurrent DOS 286 to run 8086 software in protected mode. The release of Concurrent DOS 286 was delayed until Intel would develop a new version of the chip. In August, after extensive testing E-1 step samples of the 80286, Digital Research acknowledged that Intel corrected all documented 286 errata, but said there were still undocumented chip performance problems with the prerelease version of Concurrent DOS 286 running on the E-1 step. Intel said the approach Digital Research wished to take in emulating 8086 software in protected mode differed from the original specifications. Nevertheless, they implemented minor changes in the microcode that would allow Digital Research to run emulation mode much faster, incorporated into the E-2 step. Named IBM 4680 OS, IBM originally chose DR Concurrent DOS 286 as the basis of their IBM 4680 computer for IBM Plant System products and Point-of-Sale terminals in 1986. The same limitations affected Digital Research's FlexOS 286 version 1.0, a derivation of Concurrent DOS 286, developed in 1986, introduced in January 1987, and later adopted by IBM for their IBM 4690 OS.
The problems led to Bill Gates famously referring to the 80286 as a "brain dead chip", since it was clear that the new Microsoft Windows environment would not be able to run multiple MS-DOS applications with the 286. It was arguably responsible for the split between Microsoft and IBM, since IBM insisted that OS/2, originally a joint venture between IBM and Microsoft, would run on a 286 (and in text mode).
Other operating systems that used the protected mode of the 286 were Microsoft Xenix (around 1984), Coherent, and Minix. These were less hindered by the limitations of the 80286 protected mode because they did not aim to run MS-DOS programs. In its successor 80386 chip, Intel enhanced the protected mode to address more memory, but also added the separate virtual 8086 mode, which had much better MS-DOS compatibility, in order to satisfy the diverging needs of the market.

</doc>
<doc id="15055" url="https://en.wikipedia.org/wiki?curid=15055" title="Ivanhoe">
Ivanhoe

Ivanhoe is a historical novel by Sir Walter Scott, first published in 1820 in three volumes and subtitled "A Romance". "Ivanhoe", set in 12th century England, has been credited for increasing interest in romance and medievalism; John Henry Newman claimed Scott "had first turned men's minds in the direction of the Middle Ages", while Carlyle and Ruskin made similar assertions of Scott's overwhelming influence over the revival, based primarily on the publication of this novel.
Plot introduction.
"Ivanhoe" is the story of one of the remaining Saxon noble families at a time when the nobility in England was overwhelmingly Norman. It follows the Saxon protagonist, Sir Wilfred of Ivanhoe, who is out of favour with his father for his allegiance to the Norman king Richard the Lionheart. The story is set in 1194, after the failure of the Third Crusade, when many of the Crusaders were still returning to their homes in Europe. King Richard, who had been captured by Leopold of Austria on his return journey to England, was believed to be still in captivity.
The legendary Robin Hood, initially under the name of Locksley, is also a character in the story, as are his "merry men". The character that Scott gave to Robin Hood in "Ivanhoe" helped shape the modern notion of this figure as a cheery noble outlaw.
Other major characters include Ivanhoe's intractable father, Cedric, one of the few remaining Saxon lords; various Knights Templar, most notable of which is Brian de Bois-Guilbert, Ivanhoe's main rival; a number of clergymen; the loyal serfs Gurth the swineherd and the jester Wamba, whose observations punctuate much of the action; and the Jewish moneylender, Isaac of York, who is equally passionate about his people and his beautiful daughter, Rebecca. The book was written and published during a period of increasing struggle for the emancipation of the Jews in England, and there are frequent references to injustices against them.
Plot summary.
Opening.
Protagonist Wilfred of Ivanhoe is disinherited by his father Cedric of Rotherwood for supporting the Norman King Richard and for falling in love with the Lady Rowena, Cedric's ward and a descendant of the Saxon Kings of England, after Cedric planned to marry her to the powerful Lord Aethelstane, a pretender to the Crown of England through his descent from the last Saxon King, Harold Godwinson. Ivanhoe accompanies King Richard on the Crusades, where he is said to have played a notable role in the Siege of Acre; and tends to Louis of Thuringia, who suffers from malaria.
The book opens with a scene of Norman knights and prelates seeking the hospitality of Cedric. They are guided there by a pilgrim, known at that time as a palmer, (one who carried blessed palms leaves such as those that were scattered at the feet of Jesus Christ by residents of Jerusalem when he entered seated on an donkey's colt on the Sunday before his arrest, trial and crucifixion; hence the name Palm Sunday). Also returning from the Holy Land that same night, Isaac of York, a Jewish moneylender, seeks refuge at Rotherwood. Following the night's meal, the palmer observes one of the Normans, the Templar Brian de Bois-Guilbert, issue orders to his Saracen soldiers to capture Isaac. 
The palmer then assists in Isaac's escape from Rotherwood, with the additional aid of the swineherd Gurth. 
Isaac of York offers to repay his debt to the palmer with a suit of armour and a war horse to participate in the tournament at Ashby-de-la-Zouch, on his inference that the palmer was secretly a knight. The palmer is taken by surprise; but accepts the offer.
The tournament.
The story then moves to the scene of the tournament, presided over by Prince John. Other characters in attendance are Cedric, Aethelstane, Lady Rowena, Isaac of York, his daughter Rebecca, Robin of Locksley and his men, Prince John's advisor Waldemar Fitzurse, and numerous Norman knights.
On the first day of the tournament, a bout of individual jousting, a mysterious knight, identifying himself only as "Desdichado" (described in the book as Spanish for the "Disinherited", though actually meaning "Unfortunate"), defeats some of the best Norman competitors, including Bois-Guilbert, Maurice de Bracy (a leader of a group of "Free Companions"), and the baron Reginald Front-de-Boeuf. The masked knight declines to reveal himself despite Prince John's request, but is nevertheless declared the champion of the day and is permitted to choose the Queen of the Tournament. He bestows this honour upon the Lady Rowena.
On the second day, at a melee, Desdichado is the leader of one party, opposed by his former adversaries. Desdichado's side is soon hard pressed and he himself beset by multiple foes, until rescued by a knight nicknamed 'Le Noir Faineant' ("the Black Sluggard"), who thereafter departs in secret. When forced to unmask himself to receive his coronet (the sign of championship), Desdichado is identified as Wilfred of Ivanhoe, returned from the Crusades. This causes much consternation to Prince John and his court who now fear the imminent return of King Richard.
Because he is severely wounded in the competition, Ivanhoe is taken into the care of Rebecca, the daughter of Isaac, who is a skilled healer. She convinces her father to take him with them to York, where he can be best treated. The story then glosses the conclusion of the tournament including feats of archery by Locksley.
Capture and rescue.
In the forests between Ashby and York, the Lady Rowena, Cedric and Aethelstane acquire Isaac, Rebecca and the wounded Ivanhoe, who have been abandoned by their servants for fear of bandits. "En route", the party is captured by de Bracy and his companions and taken to Torquilstone, the castle of Front-de-Boeuf. The swineherd Gurth, who had served Ivanhoe as squire at the tournament and who was recaptured by Cedric when Ivanhoe was identified, manages to escape.
The Black Knight, having taken refuge for the night in the hut of a local friar, the Holy Clerk of Copmanhurst, volunteers his assistance on learning about the captives from Robin of Locksley. They then besiege the Castle of Torquilstone with Robin's own men, including the friar and assorted Saxon yeomen. At Torquilstone, de Bracy expresses his love for the Lady Rowena, but is refused. Brian de Bois-Guilbert tries to seduce Rebecca, and is rebuffed. Front-de-Boeuf tries to wring a hefty ransom from Isaac of York; but Isaac refuses to pay unless his daughter is freed.
When the besiegers deliver a note to yield up the captives, their Norman captors demand a priest to administer the Final Sacrament to Cedric; whereupon Cedric's jester Wamba slips in disguised as a priest, and takes the place of Cedric, who then escapes and brings important information to the besiegers on the strength of the garrison and its layout. Then follows an account of the storming of the castle. The castle is set aflame during the assault by Ulrica, the daughter of the original lord of the castle, Lord Torquilstone, as revenge for her father's death. Front-de-Boeuf is killed in the fire while de Bracy surrenders to the Black Knight, who identifies himself as King Richard and releases de Bracy. Bois-Guilbert escapes with Rebecca while Isaac is rescued by the Clerk of Copmanhurst. The Lady Rowena is saved by Cedric, while the still-wounded Ivanhoe is rescued from the burning castle by King Richard. In the fighting, Aethelstane is wounded and presumed dead while attempting to rescue Rebecca, whom he mistakes for Rowena.
Rebecca's trial and Ivanhoe's reconciliation.
Following the battle, Locksley plays host to King Richard. Word is also conveyed by de Bracy to Prince John of the King's return and the fall of Torquilstone. In the meantime, Bois-Guilbert rushes with his captive to the nearest Templar Preceptory, where Lucas de Beaumanoir, the Grand-Master of the Templars, takes umbrage at Bois-Guilbert's infatuation, and subjects Rebecca to a trial for witchcraft. At Bois-Guilbert's secret request, she claims the right to trial by combat; and Bois-Guilbert, who had hoped for the position, is devastated when the Grand-Master orders him to fight against Rebecca's champion. Rebecca then writes to her father to procure a champion for her. Cedric organises Aethelstane's funeral at Coningsburgh, in the midst of which the Black Knight arrives with a companion. Cedric, who had not been present at Locksley's carousal, is ill-disposed towards the knight upon learning his true identity; but Richard calms Cedric and reconciles him with his son. During this conversation, Aethelstane emerges – not dead, but laid in his coffin alive by monks desirous of the funeral money. Over Cedric's renewed protests, Aethelstane pledges his homage to the Norman King Richard and urges Cedric to marry Rowena to Ivanhoe; to which Cedric finally agrees.
Soon after this reconciliation, Ivanhoe receives word from Isaac beseeching him to fight on Rebecca's behalf. Accordingly, Ivanhoe overcomes Bois-Guilbert but does not kill him, and the Templar dies of internal causes which is pronounced by the Grand Master as the judgement of God and proof of Rebecca's innocence. King Richard then punishes the Templars and their allies against himself. 
Fearing further persecution, Rebecca and her father leave England for Granada. Before leaving, Rebecca comes to bid Rowena a fond farewell. Finally, Ivanhoe and Rowena marry and live a long and happy life together, though the final paragraphs of the book note that Ivanhoe's military service ended with the death of King Richard.
Characters.
Wilfred of Ivanhoe, the titular character, is a knight and son of Cedric the Saxon. Ivanhoe, though of a more noble lineage than some of the other characters, represents a middling individual in the medieval class system who is not exceptionally outstanding in his abilities, as is expected of other quasi-historical fictional characters, such as the Greek heroes. Critic Georg Lukács points to middling main characters like Ivanhoe in Sir Walter Scott's other novels as one of the primary reasons Scott's historical novels depart from previous historical works and better explore social and cultural history.
Style.
Critics of the novel have treated it as a romance intended mainly to entertain boys. "Ivanhoe" maintains many of the elements of the Romance genre, including the quest, a chivalric setting, and the overthrowing of a corrupt social order in order to bring on a time of happiness. Other critics assert that the novel creates a realistic and vibrant story, idealising neither the past nor its main character.
Themes.
Scott treats themes similar to those of some of his earlier novels, like "Rob Roy" and "The Heart of Midlothian", examining the conflict between heroic ideals and modern society. In the latter novels, industrial society becomes the centre of this conflict as the backward Scottish nationalists and the "advanced" English have to arise from chaos to create unity. Similarly, the Normans in "Ivanhoe", who represent a more sophisticated culture, and the Saxons, who are poor, disenfranchised, and resentful of Norman rule, band together and begin to mould themselves into one people. The conflict between the Saxons and Normans focuses on the losses both groups must experience before they can be reconciled and thus forge a united England. The particular loss is in the extremes of their own cultural values, which must be disavowed in order for the society to function. For the Saxons, this value is the final admission of the hopelessness of the Saxon cause. The Normans must learn to overcome the materialism and violence in their own codes of chivalry. Ivanhoe and Richard represent the hope of reconciliation for a unified future.
Allusions to real history and geography.
The location of the novel is centred upon southern Yorkshire and northern Nottinghamshire in England. Castles mentioned within the story include Ashby de la Zouch Castle (now a ruin in the care of English Heritage), York (though the mention of Clifford's Tower, likewise an extant English Heritage property, is anachronistic, it not having been called that until later after various rebuilds) and 'Coningsburgh', which is based upon Conisbrough Castle, in the ancient town of Conisbrough near Doncaster (the castle also being a popular English Heritage site). Reference is made within the story to York Minster, where the climactic wedding takes place, and to the Bishop of Sheffield, although the Diocese of Sheffield did not exist at either the time of the novel or the time Scott wrote the novel and was not founded until 1914. Such references suggest that Robin Hood lived or travelled in the region.
Conisbrough is so dedicated to the story of "Ivanhoe" that many of its streets, schools, and public buildings are named after characters from the book.
Lasting influence on the Robin Hood legend.
Our modern conception of Robin Hood as a cheerful, decent, patriotic rebel owes much to "Ivanhoe".
"Locksley" becomes Robin Hood's title in the Scott novel, and it has been used ever since to refer to the fictional outlaw. Scott appears to have taken the name from an anonymous manuscript – written in 1600 – that employs "Locksley" as an epithet for Robin Hood. Owing to Scott's decision to make use of the manuscript, Robin Hood from Locksley has been transformed for all time into "Robin of Locksley", alias Robin Hood. (There is, incidentally, a village called Loxley in Yorkshire.)
Scott makes the 12th-century's Saxon-Norman conflict a major theme in his novel. Recent re-tellings of the story retain his emphasis. Scott also shunned the late 16th-century depiction of Robin as a dispossessed nobleman (the Earl of Huntingdon). This, however, has not prevented Scott from making an important contribution to the noble-hero strand of the legend, too, because some subsequent motion picture treatments of the Robin Hood's adventures give Robin traits that are characteristic of Ivanhoe as well. The most notable Robin Hood films are the lavish Douglas Fairbanks 1922 silent film, the 1938 triple Academy Award winning "Adventures of Robin Hood" with Errol Flynn as Robin (which contemporary reviewer Frank Nugent links specifically with "Ivanhoe"), and the 1991 box-office success "" with Kevin Costner). There is also the Mel Brooks spoof, . In most versions of Robin Hood, both Ivanhoe and Robin, for instance, are returning Crusaders. They have quarreled with their respective fathers, they are proud to be Saxons, they display a highly evolved sense of justice, they support the rightful king even though he is of Norman-French ancestry, they are adept with weapons, and they each fall in love with a "fair maid" (Rowena and Marian, respectively).
This particular time-frame was popularised by Scott. He borrowed it from the writings of the 16th-century chronicler John Mair or a 17th-century ballad presumably to make the plot of his novel more gripping. Medieval balladeers had generally placed Robin about two centuries later in the reign of Edward I, II or III.
Robin's familiar feat of splitting his competitor's arrow in an archery contest appears for the first time in "Ivanhoe".
Historical accuracy.
The general political events depicted in the novel are relatively accurate; the novel tells of the period just after King Richard's imprisonment in Austria following the Crusade and of his return to England after a ransom is paid. Yet the story is also heavily fictionalised. Scott himself acknowledged that he had taken liberties with history in his "Dedicatory Epistle" to "Ivanhoe". Modern readers are cautioned to understand that Scott's aim was to create a compelling novel set in a historical period, not to provide a book of history.
There has been criticism of Scott's portrayal of the bitter extent of the "enmity of Saxon and Norman, represented as persisting in the days of Richard" as "unsupported by the evidence of contemporary records that forms the basis of the story." However, Scott may have intended to suggest parallels between the Norman conquest of England, about 130 years previously, and the prevailing situation in Scott's native Scotland (Scotland's union with England in 1707 – about the same length of time had elapsed before Scott's writing and the resurgence in his time of Scottish nationalism evidenced by the cult of Robert Burns, the famous poet who deliberately chose to work in Scots vernacular though he was an educated man and spoke modern English eloquently). Indeed, some experts suggest that Scott deliberately used "Ivanhoe" to illustrate his own combination of Scottish patriotism and pro-British Unionism.
The novel generated a new name in English – Cedric. The original Saxon name had been "Cerdic" but Sir Walter misspelled it – an example of metathesis. "It is not a name but a misspelling," said satirist H. H. Munro.
In 1194 England, it would have been unlikely for Rebecca to face the threat of being burned at the stake on charges of witchcraft. It is thought that it was shortly afterwards, from the 1250s, that the Church began to undertake the finding and punishment of witches and death did not become the usual penalty until the 15th century. Even then, the form of execution used for witches in England (unlike Scotland and Continental Europe) was hanging, burning being reserved for those also convicted of treason. There are various minor errors e.g. the description of the tournament at Ashby owes more to the 14th century, and most of the coins mentioned by Scott are exotic.
"For a writer whose early novels [all set in Scotland were prized for their historical accuracy, Scott was remarkably loose with the facts when he wrote Ivanhoe... But it is crucial to remember that Ivanhoe, unlike the Waverly books, is entirely a romance. It is meant to please, not to instruct, and is more an act of imagination than one of research. Despite this fancifulness, however, Ivanhoe does make some prescient historical points. The novel is occasionally quite critical of King Richard, who seems to love adventure more than he loves the well-being of his subjects. This criticism did not match the typical idealised, romantic view of Richard the Lion-Hearted that was popular when Scott wrote the book, and yet it accurately echoes the way King Richard is often judged by historians today."
It has been conjectured that the character of Rebecca in the book was inspired by Rebecca Gratz, a Philadelphia teacher and philanthropist and the first Jewish female college student in America. Scott's attention had been drawn to Gratz's character by novelist Washington Irving, who was a close friend of the Gratz family. The assertion has been disputed, but it has been supported by "The Original of Rebecca in Ivanhoe", an article that appeared in "The Century Magazine" in 1882.
Moreover, there are some inaccuracies about English kings' history and genealogy. For instance, William II of England, cited as William Rufus (in the scene of the Joust in Ashby-de-la-Zouch), is said to have been John Lackland's grandfather, whereas he was actually his great-grand-uncle. Furthermore, while describing the violence and lack of respect of Norman barons towards women, Scott refers to Matilda's temporary vows in a nunnery, but it is unclear whether he refers to Matilda of Scotland, wife of Henry I of England (since she is defined as queen of England and daughter of the king of Scotland) or to her daughter Empress Matilda (since she is said to have been Empress of Germany and daughter and mother of kings, characteristics which can be applied only to her).
Legacy.
Film, TV or theatrical adaptations.
The novel has been the basis for several motion pictures:
There have also been many television adaptations of the novel, including:
Victor Sieg's dramatic cantata "Ivanhoé" won the Prix de Rome in 1864 and premiered in Paris the same year. An operatic adaptation of the novel by Sir Arthur Sullivan (entitled "Ivanhoe") ran for over 150 consecutive performances in 1891. Other operas based on the novel have been composed by Gioachino Rossini ("Ivanhoé"), Thomas Sari ("Ivanhoé"), Bartolomeo Pisani ("Rebecca"), A. Castagnier ("Rébecca"), Otto Nicolai ("Il Templario"), and Heinrich Marschner ("Der Templer und die Jüdin"). Rossini's opera is a "pasticcio" (an opera in which the music for a new text is chosen from pre-existent music by one or more composers). Scott attended a performance of it and recorded in his journal, "It was an opera, and, of course, the story sadly mangled and the dialogue, in part nonsense."
Other.
The railway running through Ashby-de-la-Zouch has the epithet The Ivanhoe Line in reference to the book's setting in the locality.

</doc>
<doc id="15056" url="https://en.wikipedia.org/wiki?curid=15056" title="Isoelectric point">
Isoelectric point

The isoelectric point (pI, pH(I), IEP), is the pH at which a particular molecule carries no net electrical charge. The standard nomenclature to represent the isoelectric point is pH(I), although pI is also commonly seen, and is used in this article for brevity. The net charge on the molecule is affected by pH of its surrounding environment and can become more positively or negatively charged due to the gain or loss, respectively, of protons (H+).
Surfaces naturally charge to form a double layer. In the common case when the surface charge-determining ions are H+/OH−, the net surface charge is affected by the pH of the liquid in which the solid is submerged.
The pI value can affect the solubility of a molecule at a given pH. Such molecules have minimum solubility in water or salt solutions at the pH that corresponds to their pI and often precipitate out of solution. Biological amphoteric molecules such as proteins contain both acidic and basic functional groups. Amino acids that make up proteins may be positive, negative, neutral, or polar in nature, and together give a protein its overall charge. At a pH below their pI, proteins carry a net positive charge; above their pI they carry a net negative charge. Proteins can, thus, be separated by net charge in a polyacrylamide gel using either preparative gel electrophoresis or isoelectric focusing, which uses a pH gradient to separate proteins. Isoelectric focusing is also the first step in 2-D gel polyacrylamide gel electrophoresis.
Calculating pI values.
For an amino acid with only one amine and one carboxyl group, the pI can be calculated from the mean of the pKas of this molecule.
The pH of an electrophoretic gel is determined by the buffer used for that gel. If the pH of the buffer is above the pI of the protein being run, the protein will migrate to the positive pole (negative charge is attracted to a positive pole). If the pH of the buffer is below the pI of the protein being run, the protein will migrate to the negative pole of the gel (positive charge is attracted to the negative pole). If the protein is run with a buffer pH that is equal to the pI, it will not migrate at all. This is also true for individual amino acids.
Examples.
In these two examples the isoelectric point is shown by the green vertical line. In glycine the pK values are separated by nearly 7 units so the concentration of the neutral species, glycine (GlyH), is effectively 100% of the analytical glycine concentration. Glycine may exist as a zwitterion at the isoelectric point, but the equilibrium constant for the isomerization reaction in solution
is not known.
The other example, adenosine monophosphate is shown to illustrate the fact that a third species may, in principle, be involved. In fact the concentration of (AMP)H32+ is negligible at the isoelectric point in this case.
If the pI is greater than the pH, the molecule will have a positive charge.
Ceramic materials.
The isoelectric points (IEP) of metal oxide ceramics are used extensively in material science in various aqueous processing steps (synthesis, modification, etc.). In the absence of chemisorbed or physisorbed species particle surfaces in aqueous suspension are generally assumed to be covered with surface hydroxyl species, M-OH (where M is a metal such as Al, Si, etc.). At pH values above the IEP, the predominate surface species is M-O−, while at pH values below the IEP, M-OH2+ species predominate. Some approximate values of common ceramics are listed below (Haruta and Brunelle, except where noted). The exact value can vary widely, depending on material factors such as purity and phase as well as physical parameters such as temperature. In addition, precise measurement of isoelectric points is difficult and requires careful techniques, even with modern methods. Thus, many sources often cite differing values for isoelectric points of these materials.
Examples of isoelectric points.
The following list gives the pH25 °C of isoelectric point at 25 °C for selected materials in water:
"Note: The list is ordered by increasing pH values."
Mixed oxides may exhibit isoelectric point values that are intermediate to those of the corresponding pure oxides. For example, Jara "et al." measured an IEP of 4.5 for a synthetically prepared amorphous aluminosilicate (Al2O3-SiO2). The researchers noted that the electrokinetic behavior of the surface was dominated by surface Si-OH species, thus explaining the relatively low IEP value. Significantly higher IEP values (pH 6 to 8) have been reported for 3Al2O3-2SiO2 by others (see Lewis). Lewis also lists the IEP of barium titanate, BaTiO3 as being between pH 5 and 6, while Vamvakaki et al. reported a value of 3, although these authors note that a wide range of values have been reported, a result of either residual barium carbonate on the surface or TiO2-rich surfaces.
The farther the pH of an Amino Acid solution is from its pl the greater the electric charge on that population of molecules.
Isoelectric point versus point of zero charge.
The terms isoelectric point (IEP) and point of zero charge (PZC) are often used interchangeably, although under certain circumstances, it may be productive to make the distinction.
In systems in which H+/OH− are the interface potential-determining ions, the point of zero charge is given in terms of pH. The pH at which the surface exhibits a neutral net electrical charge is the point of zero charge at the surface. Electrokinetic phenomena generally measure zeta potential, and a zero zeta potential is interpreted as the point of zero net charge at the shear plane. This is termed the isoelectric point. Thus, the isoelectric point is the value of pH at which the colloidal particle remains stationary in an electrical field. The isoelectric point is expected to be somewhat different than the point of zero charge at the particle surface, but this difference is often ignored in practice for so-called pristine surfaces, i.e., surfaces with no specifically adsorbed positive or negative charges. In this context, specific adsorption is understood as adsorption occurring in a Stern layer or chemisorption. Thus, point of zero charge at the surface is taken as equal to isoelectric point in the absence of specific adsorption on that surface.
According to Jolivet, in the absence of positive or negative charges, the surface is best described by the point of zero charge. If positive and negative charges are both present in equal amounts, then this is the isoelectric point. Thus, the PZC refers to the absence of any type of surface charge, while the IEP refers to a state of neutral net surface charge. The difference between the two, therefore, is the quantity of charged sites at the point of net zero charge. Jolivet uses the intrinsic surface equilibrium constants, p"K"− and p"K"+ to define the two conditions in terms of the relative number of charged sites:
For large Δp"K" (>4 according to Jolivet), the predominant species is MOH while there are relatively few charged species - so the PZC is relevant. For small values of Δp"K", there are many charged species in approximately equal numbers, so one speaks of the IEP.

</doc>
<doc id="15058" url="https://en.wikipedia.org/wiki?curid=15058" title="International reply coupon">
International reply coupon

An international reply coupon (IRC) is a coupon that can be exchanged for one or more postage stamps representing the minimum postage for an unregistered priority airmail letter of up to twenty grams sent to another Universal Postal Union (UPU) member country. IRCs are accepted by all UPU member countries.
UPU member postal services are obliged to exchange an IRC for postage, but are not obliged to sell them.
The purpose of the IRC is to allow a person to send someone in another country a letter, along with the cost of postage for a reply. If the addressee is within the same country, there is no need for an IRC because a self-addressed stamped envelope (SASE) or return postcard will suffice; but if the addressee is in another country an IRC removes the necessity of acquiring foreign postage or sending appropriate currency.
History.
The IRC was introduced in 1906 at a Universal Postal Union congress in Rome. At the time an IRC could be exchanged for a single-rate, ordinary postage stamp for surface delivery to a foreign country, as this was before the introduction of airmail services. An IRC is exchangeable in a UPU member country for the minimum postage of a priority or unregistered airmail letter to a foreign country.
The current IRC, which features the theme "Water for Life," designed by Czech artist and graphic designer Michal Sindelar, was issued in 2013 and is valid until 31 December 2017. IRCs are ordered from the UPU headquarters in Bern, Switzerland by postal authorities. They are generally available at large post offices; in the U.S., they are requisitioned along with regular domestic stamps by any post office that has sufficient demand for them.
Prices for IRCs vary by country. In the United States in November 2012, the purchase price was $2.20 USD; however, the US Postal Service discontinued sales of IRCs on 27 January 2013 due to declining demand. Britain's Royal Mail also stopped selling IRCs on 18 February 2012, citing minimal sales and claiming that the average post office sold less than one IRC per year. IRCs purchased in foreign countries may be used in the United States toward the purchase of postage stamps and embossed stamped envelopes at the current one-ounce First Class International rate ($1.05 USD as of April 2012) per coupon.
IRCs are often used by amateur radio operators sending QSL cards to each other; it has traditionally been considered good practice and common courtesy to include an IRC when writing to a foreign operator and expecting a reply by mail. If the operator's home country does not sell IRCs, then a foreign IRC may be used.
Previous editions of the IRC, the "Beijing" model and all subsequent versions, bear an expiration date. Consequently, a new IRC will be issued every three years.
The Ponzi scheme.
The profit that could be made by taking advantage of the differing postal rates in different countries to buy IRCs cheaply in one country and exchange them for stamps of a higher value in another country was the intended profit generator for a scheme operated by Charles Ponzi, which became the fraudulent Ponzi scheme. In practice, the overhead on buying and selling large numbers of the very low-value IRCs precluded any profitability.
The selling price and exchange value in stamps in each country have been adjusted to some extent to remove some of the potential for profit, but ongoing fluctuations in cost of living and exchange rates make it impossible to achieve this completely.
U.S. Postal Service description of international reply coupons.
International reply coupons (in French, "Coupons-Reponse Internationaux") are printed in blue ink on paper that has the letters “UPU” in large characters in the watermark. The front of each coupon is printed in French. The reverse side of the coupon, which has text relating to its use, is printed in German, English, Arabic, Chinese, Spanish, or Russian. Under Universal Postal Union’s regulations, participating member countries are not required to place a control stamp or postmark on the international reply coupons that they sell. Therefore, some foreign issue reply coupons that are tendered for redemption may bear the name of the issuing country (generally in French) rather than the optional control stamp or postmark.
USPS Item Number 330800 is an international reply coupon printed by the Universal Postal Union which is approximately 3.75 inches by 6 inches, has a barcode on the reverse side, and has an expiration date of December 31, 2013 (in French, A échanger jusqu'au 31 décembre 2013 (31.12.2013)). New coupons are valid until December 31, 2017.
The Nairobi Model was designed by Rob Van Goor, a graphic artist from the Luxembourg Post. It was selected from among 10 designs presented by Universal Postal Union member countries. Van Goor interpreted the theme of the contest – "The Postage Stamp: A Vehicle for Exchange" – by depicting the world being cradled by a hand and the perforated outline of a postage stamp.

</doc>
<doc id="15059" url="https://en.wikipedia.org/wiki?curid=15059" title="Isaac Bonewits">
Isaac Bonewits

Phillip Emmons Isaac Bonewits (October 1, 1949 – August 12, 2010) was an influential American Druid who published a number of books on the subject of Neopaganism and magic. He was also a public speaker, liturgist, singer and songwriter, and founded the Druidic organization Ár nDraíocht Féin, as well as the Neopagan civil rights group, the Aquarian Anti-Defamation League. Born in Royal Oak, Michigan, Bonewits had been heavily involved in occultism since the 1960s. He died in 2010.
Early life and education.
Bonewits was born on October 1, 1949 in Royal Oak, Michigan, as the fourth of five children. His mother and father were Roman Catholics. Spending much of his childhood in Ferndale, he was moved at age 12 to San Clemente, California, where he spent a short time in a Catholic high school before he went back to public school to graduate from high school a year early. He enrolled at UC Berkeley in 1966; he graduated from the university in 1970 with a Bachelor of Arts in Magic, becoming the first and only person known to have ever received any kind of academic degree in Magic from an accredited university.
Personal life.
Bonewits was married five times. He was married to Rusty Elliot from 1973 to 1976. His second wife was Selene Kumin Vega, followed by marriage to Sally Eaton (1980 to 1985). His fourth wife was author Deborah Lipp, from 1988 to 1998. On July 23, 2004, he was married in a handfasting ceremony to a former vice-president of the Covenant of Unitarian Universalist Pagans, Phaedra Heyman Bonewits. At the time of the handfasting, the marriage was not yet legal because he had not yet been legally divorced from Lipp, although they had been separated for several years. Paperwork and legalities caught up on December 31, 2007, making them legally married.
Bonewits' only child, Arthur Shaffrey Lipp-Bonewits, was born to Deborah Lipp in 1990.
Career.
Early years.
In 1966, while enrolled at UC Berkeley, Bonewits joined the Reformed Druids of North America, or RDNA. Bonewits was ordained as a Neo-druid priest in 1969. During this period, the 18-year-old Bonewits was recruited by the Church of Satan, but left due to political and philosophical conflicts with Anton LaVey. During his stint in the Church of Satan, Bonewits appeared in some scenes of the 1970 documentary "Satanis: The Devil's Mass". Bonewits, in his article "My Satanic Adventure", asserts that the rituals in "Satanis" were staged for the movie at the behest of the filmmakers and were not authentic ceremonies.
Author and Druid.
His first book, "Real Magic", was published in 1972. Between 1973 and 1975 Bonewits was employed as editor of "Gnostica" magazine in Minnesota (published by Llewellyn Publications), established an offshoot group of the RDNA called the Schismatic Druids of North America, and helped create a group called the Hasidic Druids of North America (despite his lifelong status as a "gentile"). He also founded the short-lived Aquarian Anti-Defamation League (AADL), an early Pagan civil rights group.
In 1976, Bonewits moved back to Berkeley and rejoined his original grove there, now part of the New Reformed Druids of North America (NRDNA). He was later elected Archdruid of the Berkeley Grove.
Founder of Ár nDraíocht Féin.
Over the years Bonewits also had varying degrees of involvement with mystical organizations such as Ordo Templi Orientis, Gardnerian Wicca, and the New Reformed Orthodox Order of the Golden Dawn (a Wiccan organization not to be confused with the Hermetic Order of the Golden Dawn) as well as others. Bonewits was a regular presenter at Neopagan conferences and festivals all over the US, as well as attending gaming conventions in the Bay Area. He promoted his book 'Authentic Thaumaturgy' to gamers as a way of organizing Dungeons and Dragons games and to give a background to games of .
In 1983, Bonewits founded Ár nDraíocht Féin (also known as "A Druid Fellowship" or ADF), which was incorporated in 1990 in the state of Delaware as a U.S. 501(c)3 non-profit organization. He made the organization's first public announcement in 1984, and began the membership sign-up at the first WinterStar Symposium in 1984. Since that time, ADF has developed into one of the world's largest forms of contemporary Druidism practiced as a religion.
Although illness curtailed many of his activities and travels for a time, he remained Archdruid of ADF until 1996. In that year, he resigned from the position of Archdruid but retained the lifelong title of ADF Archdruid Emeritus.
Musician and activist.
A songwriter, singer, and recording artist, he produced two CDs of pagan music and numerous recorded lectures and panel discussions, produced and distributed by the Association for Consciousness Exploration. He lived in Rockland County, New York, and was a member of the Covenant of Unitarian Universalist Pagans (CUUPS).
Bonewits encouraged charity programs to help Neopagan seniors, and in January 2006 was the keynote speaker at the Conference On Current Pagan Studies at the Claremont Graduate University in Claremont, CA.
Illness and death.
In 1990, Bonewits was diagnosed with Eosinophilia-myalgia syndrome. The illness was a factor in his eventual resignation from the position of Archdruid of the ADF.
On October 25, 2009, Bonewits was diagnosed with a rare form of colon cancer, for which he underwent treatment. He died at home, on August 12, 2010, surrounded by his family.
Contributions to Neopaganism.
In his book "Real Magic" (1971), Bonewits proposed his "Laws of Magic." These "laws" are synthesized from a multitude of belief systems from around the world to explain and categorize magical beliefs within a cohesive framework. Many interrelationships exist, and some belief systems are subsets of others. This work was chosen by Dennis Wheatley in the 1970s to be part of his publishing project 'Library of the Occult'.
Bonewits also coined much of the modern terminology used to articulate the themes and issues that affect the North American Neopagan community.

</doc>
<doc id="15062" url="https://en.wikipedia.org/wiki?curid=15062" title="Intel 8080">
Intel 8080

The Intel 8080 (""eighty-eighty"") was the second 8-bit microprocessor designed and manufactured by Intel and was released in April 1974. It was an extended and enhanced variant of the earlier 8008 design, although without binary compatibility. The initial specified clock frequency limit was 2 MHz, and with common instructions using 4, 5, 7, 10, or 11 cycles this meant that it operated at a typical speed of a few hundred thousand instructions per second.
The 8080 required two support chips to function in most applications, the i8224 clock generator/driver and the i8228 bus controller, and it was implemented in NMOS using non-saturated enhancement mode transistors as loads, therefore demanding a +12 V and a −5 V voltage in addition to the main TTL compatible +5 V.
Although earlier microprocessors were used for calculators, cash registers, computer terminals, industrial robots, and other applications, the 8080 became one of the first really widespread microprocessors. This was partly due to its simplifying NMOS implementation (making it TTL compatible), but also to its enhanced instruction set (over the 8008) and its subsequent role as the original target CPU for CP/M, the first de facto standard personal computer operating system.
The 8080 was successful enough that compatibility at the assembly language level became a design requirement for the 8086 when design for it was started in 1976. This also means that the 8080 indirectly influenced the ubiquitous 32-bit and 64-bit x86 architectures of today.
Description.
Programming model.
The Intel 8080 was the successor to the 8008. It used the same basic instruction set and register model as the 8008 (developed by Computer Terminal Corporation), even though it was not source code compatible nor binary compatible with its predecessor. Every instruction in the 8008 has an equivalent instruction in the 8080 (even though the actual opcodes differ between the two CPUs). The 8080 also added a few 16-bit operations to its instruction set as well. Whereas the 8008 required the use of the HL register pair to indirectly access its 14-bit memory space, the 8080 added addressing modes to allow direct access to its full 16-bit memory space. In addition, the internal 7-level push-down call stack of the 8008 was replaced by a dedicated 16-bit stack pointer (SP) register. The 8080's large 40-pin DIP packaging permitted it to provide a 16-bit address bus and an 8-bit data bus, allowing easy access to 64 KB of memory.
Registers.
The processor has seven 8-bit registers (A, B, C, D, E, H, and L), where A is the primary 8-bit accumulator and the other six registers can be used as either individual 8-bit registers or as three 16-bit register pairs (BC, DE, and HL) depending on the particular instruction. Some instructions also enable the HL register pair to be used as a (limited) 16-bit accumulator, and a pseudo-register, M, can be used almost anywhere that any other register can be used, referring to the memory address pointed to by the HL pair. It also has a 16-bit stack pointer to memory (replacing the 8008's internal stack), and a 16-bit program counter.
Flags.
The processor maintains internal flag bits (a status register) which indicates the results of arithmetic and logical instructions. The flags are:
The carry bit can be set, or complemented, by specific instructions. Conditional branch instructions test the various flag status bits. The flags can be copied as a group to the accumulator. The A accumulator and the flags together are called the AF register.
Commands/instructions.
As with many other 8-bit processors, all instructions are encoded in a single byte (including register-numbers, but excluding immediate data), for simplicity. Some of them are followed by one or two bytes of data, which can be an immediate operand, a memory address, or a port number. Like larger processors, it has automatic CALL and RET instructions for multi-level procedure calls and returns (which can even be conditionally executed, like jumps), and instructions to save and restore any 16-bit register-pair on the machine stack. There are also eight one-byte call instructions () for subroutines located at the fixed addresses 00h, 08h, 10h, ..., and 38h. These were intended to be supplied by external hardware in order to invoke a corresponding interrupt-service routine, but were also often employed as fast system calls. The most sophisticated command is , which is used for exchanging the register pair HL with the value stored at the address indicated by the stack pointer.
8-bit instructions.
Most 8-bit operations can only be performed on the 8-bit accumulator (the A register). For 8-bit operations with two operands, the other operand can be either an immediate value, another 8-bit register, or a memory byte addressed by the 16-bit register pair HL. Direct copying is supported between any two 8-bit registers and between any 8-bit register and an HL-addressed memory byte. Due to the regular encoding of the instruction (using a quarter of available opcode space), there are redundant codes to copy a register into itself (, for instance), which were of little use, except for delays. However, what would have been a copy from the HL-addressed cell into itself (i.e., ) is instead used to encode the halt (HLT) instruction, halting execution until an external reset or interrupt occurs.
16-bit operations.
Although the 8080 is generally an 8-bit processor, it also has limited abilities to perform 16-bit operations: Any of the three 16-bit register pairs (BC, DE, or HL) or SP can be loaded with an immediate 16-bit value (using ), incremented or decremented (using and ), or added to HL (using ). The instruction exchanges the values of the HL and DE register pairs. By adding HL to itself, it is possible to achieve the same result as a 16-bit arithmetical left shift with one instruction. The only 16-bit instructions that affect any flag are , which set the CY (carry) flag in order to allow for programmed 24-bit or 32-bit arithmetic (or larger), needed to implement floating point arithmetics, for instance.
Input/output scheme.
Input output port space.
The 8080 supported up to 256 input/output (I/O) ports, accessed via dedicated I/O instructions—taking port addresses as operands. This I/O mapping scheme was regarded as an advantage, as it freed up the processor's limited address space. Many CPU architectures instead use so-called memory mapped I/O, in which a common address space is used for both RAM and peripheral chips. This removes the need for dedicated I/O instructions, although a drawback in such designs may be that special hardware must be used to insert wait states as peripherals are often slower than memory. However, in some simple 8080 computers, I/O was indeed addressed as if they were memory cells, "memory mapped", leaving the I/O commands unused. I/O addressing could also sometimes employ the fact that the processor would output the same 8-bit port address to both the lower and the higher address byte (i.e. IN 05h would put the address 0505h on the 16-bit address bus). Similar I/O-port schemes were used in the backward compatible Zilog Z80 and Intel 8085 as well as the closely related x86 families of microprocessors.
Separate stack space.
One of the bits in the processor state word (see below) indicates that the processor is accessing data from the stack. Using this signal, it is possible to implement a separate stack memory space. However, this feature was seldom used.
The internal state word.
For more advanced systems, during one phase of its working loop, the processor set its "internal state byte" on the data bus. This byte contains flags which determine if the memory or I/O port is accessed, and whether it was necessary to handle an interrupt.
The interrupt system state (enabled or disabled) was also output on a separate pin. For simple systems, where the interrupts were not used, it is possible to find cases where this pin is used as an additional single-bit output port (the popular Radio86RK computer made in the Soviet Union, for instance).
Example code.
The following 8080/8085 assembler source code is for a subroutine named codice_1 that copies a block of data bytes of a given size from one location to another. The data block is copied one byte at a time, and the data movement and looping logic utilizes 16-bit operations.
Pin usage.
The address bus had its own 16 pins, and the data bus had eight pins that were possible to use without any multiplexing. Using the two additional pins (read and write signals), it was possible to assemble simple microprocessor devices very easily. Only the separate IO space, interrupts and DMA required additional chips to decode the processor pin signals. However, the processor load capacity was limited, and even simple computers frequently contained bus amplifiers.
The processor required three power sources (−5, +5 and +12 V) and two non-interlacing high-amplitude synchronization signals. However, at least the late Soviet version КР580ВМ80А was able to work with a single +5 V power source, the +12 V pin being connected to +5 V and the −5 V pin to ground. The processor consumed about 1.3 W of power.
The pin-out table, from the chip's accompanying documentation, described the pins as follows:
Support chips.
A key factor in the success of the 8080 was the broad range of support chips available, providing serial communications, counter/timing, input/output, direct memory access, and programmable interrupt control amongst other functions.
Physical implementation.
The 8080 integrated circuit used non-saturated enhancement load nMOS gates, demanding extra voltages (for the load-gate bias). It was manufactured in a silicon gate process using a minimum feature size of 6 µm. A single layer of metal was used to interconnect the approximately 6,000 transistors in the design, but the higher resistance polysilicon layer, which required higher voltage for some interconnects, was implemented with transistor gates. The die size was approximately 20 mm2.
The industrial impact.
Applications and successors.
The 8080 was used in many early microcomputers, such as the MITS Altair 8800 Computer, Processor Technology SOL-20 Terminal Computer and IMSAI 8080 Microcomputer, forming the basis for machines running the CP/M operating system (the later, almost fully compatible and more capable, Zilog Z80 processor would capitalize on this, with Z80 & CP/M becoming the dominant CPU & OS combination of the period circa 1976 to 1983 much as did the x86 & MS-DOS for the PC a decade later). Even in 1979 after introduction of the Z80 and 8085 processors, five manufacturers of the 8080 were selling an estimated 500,000 units per month at a price around $3 to $4 per unit. The first single-board microcomputers, such as MYCRO-1 and the "dyna-micro" were based on the Intel 8080. One of the early uses of the 8080 was made in the late 1970s by Cubic-Western Data of San Diego, CA in its Automated Fare Collection Systems custom designed for mass transit systems around the world. An early industrial use of the 8080 was as the "brain" of the DatagraphiX Auto-COM (Computer Output Microfiche) line of products which took large amounts of user data from reel-to-reel tape and imaged it onto microfiche. The Auto-COM instruments also included an entire automated film cutting, processing, washing, and drying sub-system – quite a feat, both then and in the 21st century, to all be accomplished successfully with only an 8-bit microprocessor running at a clock speed of less than 1 MHz with a 64 KB memory limit. In addition, several early arcade video games were built around the 8080 microprocessor, including "Space Invaders", one of the most popular arcade games ever made.
Shortly after the launch of the 8080, the Motorola 6800 competing design was introduced, and after that, the MOS Technology 6502 variation of the 6800. Zilog introduced the Z80, which had a compatible machine-language instruction set and initially used the same assembly language as the 8080, but for legal reasons, Zilog developed a syntactically-different (but code compatible) alternative assembly language for the Z80. At Intel, the 8080 was followed by the compatible and electrically more elegant 8085, and later by the assembly language compatible 16-bit 8086 and then the 8/16-bit 8088, which was selected by IBM for its new PC to be launched in 1981. Later NEC made a NEC V20 (an 8088 clone with Intel 80186 instruction set compatibility) which also supported an 8080 emulation mode. This was also supported by NEC's V30 (a similarly enhanced 8086 clone). Thus, the 8080, via its ISA, made a lasting impact on computer history.
In the Soviet Union, manufacturers cloned the 8080 microprocessor's layout geometry, even using an identical pin arrangement, and started to produce the clone under the name KP580ИK80 (later marked as KP580BM80). This processor was the base of the Radio86RK (Радио 86РК in Russian), probably the most popular amateur single-board computer in the Soviet Union. Radio86RK's predecessor was the Micro-80 (Микро-80 in Russian), and its successor the Orion-128 (Орион-128 in Russian) which had a graphical display. Both were built on the KP580 processor.
Another model compatible with Intel 8080A, named MMN8080, was produced at Microelectronica Bucharest in Romania. There was also a compatible Polish CPU named MCY7880 and the Slovak-made Tesla MHB 8080A.
Industry change.
The 8080 also changed how computers were created. When the 8080 was introduced, computer systems were usually created by computer manufacturers such as Digital Equipment Corporation, Hewlett Packard, or IBM. A manufacturer would produce the entire computer, including processor, terminals, and system software such as compilers and operating system. The 8080 was actually designed for just about any application "except" a complete computer system. Hewlett Packard developed the HP 2640 series of smart terminals around the 8080. The HP 2647 was a terminal which ran BASIC on the 8080. Microsoft would market as its founding product the first popular programming language for the 8080, and would later acquire DOS for the IBM-PC.
The 8080 and 8085 gave rise to the 8086, which was designed as a source compatible (although not binary compatible) extension of the 8085. This design, in turn, later spawned the x86 family of chips, the basis for most CPUs in use today. Many of the 8080's core machine instructions and concepts, for example, registers named "A", "B", "C" and "D", as well as many of the flags used to control conditional jumps, are still in use in the widespread x86 platform. 8080 Assembler code can still be directly translated into x86 instructions; all of its core elements are still present.
PCs based upon the 8086 design and its successors evolved into workstations and servers of 16, 32 and 64 bits, with advanced memory protection, segmentation, and multiprocessing features, blurring the difference between small and large computers (the 80286 and 80386's protected mode were important in doing so). The size of chips has grown so that the size and power of large x86 chips is not much different from high end architecture chips, and a common strategy to produce a very large computer is to network many x86 processors.
The basic architecture of the 8080 and its successors has replaced many proprietary mid-range and mainframe computers, and withstood challenges of technologies such as RISC. Most computer manufacturers have abandoned producing their own processors below the highest performance points. Though x86 may not be the most elegant, or theoretically most efficient design, the sheer market force of so many dollars going into refining a design has made the x86 family today, and will remain for some time, the dominant processor architecture, even bypassing Intel's attempts to replace it with incompatible architectures such as the iAPX 432 and Itanium.
History.
Federico Faggin, the originator of the 8080 architecture in early 1972, proposed it to Intel's management and pushed for its implementation. He finally got the permission to develop it six months later. Faggin hired Masatoshi Shima from Japan who did the detailed design under his direction, using the design methodology for random logic with silicon gate that Faggin had created for the 4000 family. Stanley Mazor contributed a couple of instructions to the instruction set.

</doc>
<doc id="15063" url="https://en.wikipedia.org/wiki?curid=15063" title="Intel 8086">
Intel 8086

The 8086 (""eighty eighty-six"", also called iAPX 86) is a 16-bit microprocessor chip designed by Intel between early 1976 and mid-1978, when it was released. The Intel 8088, released in 1979, was a slightly modified chip with an external 8-bit data bus (allowing the use of cheaper and fewer supporting ICs), and is notable as the processor used in the original IBM PC design, including the widespread version called IBM PC XT.
The 8086 gave rise to the x86 architecture which eventually became Intel's most successful line of processors.
History.
Background.
In 1972, Intel launched the 8008, the first 8-bit microprocessor. It implemented an instruction set designed by Datapoint corporation with programmable CRT terminals in mind, which also proved to be fairly general purpose. The device needed several additional ICs to produce a functional computer, in part due to it being packaged in a small 18-pin "memory package", which ruled out the use of a separate address bus (Intel was primarily a DRAM manufacturer at the time).
Two years later, Intel launched the 8080, employing the new 40-pin DIL packages originally developed for calculator ICs to enable a separate address bus. It had an extended instruction set that was source (not binary) compatible with the 8008 and also included some 16-bit instructions to make programming easier. The 8080 device, often described as "the first truly useful microprocessor", was eventually replaced by the depletion-load based 8085 (1977) which sufficed with a single +5 V power supply instead of the three different operating voltages of earlier chips. Other well known 8-bit microprocessors that emerged during these years were Motorola 6800 (1974), General Instrument PIC16X (1975), MOS Technology 6502 (1975), Zilog Z80 (1976), and Motorola 6809 (1978).
The first x86 design.
The 8086 project started in May 1976 and was originally intended as a temporary substitute for the ambitious and delayed iAPX 432 project. It was an attempt to draw attention from the less-delayed 16- and 32-bit processors of other manufacturers (such as Motorola, Zilog, and National Semiconductor) and at the same time to counter the threat from the Zilog Z80 (designed by former Intel employees), which became very successful. Both the architecture and the physical chip were therefore developed rather quickly by a small group of people, and using the same basic microarchitecture elements and physical implementation techniques as employed for the slightly older 8085 (and for which the 8086 also would function as a continuation).
Marketed as source compatible, the 8086 was designed to allow assembly language for the 8008, 8080, or 8085 to be automatically converted into equivalent (suboptimal) 8086 source code, with little or no hand-editing. The programming model and instruction set was (loosely) based on the 8080 in order to make this possible. However, the 8086 design was expanded to support full 16-bit processing, instead of the fairly basic 16-bit capabilities of the 8080/8085.
New kinds of instructions were added as well; full support for signed integers, base+offset addressing, and self-repeating operations were akin to the Z80 design but were all made slightly more general in the 8086. Instructions directly supporting nested ALGOL-family languages such as Pascal and PL/M were also added. According to principal architect Stephen P. Morse, this was a result of a more software centric approach than in the design of earlier Intel processors (the designers had experience working with compiler implementations). Other enhancements included microcoded multiply and divide instructions and a bus structure better adapted to future coprocessors (such as 8087 and 8089) and multiprocessor systems.
The first revision of the instruction set and high level architecture was ready after about three months, and as almost no CAD tools were used, four engineers and 12 layout people were simultaneously working on the chip. The 8086 took a little more than two years from idea to working product, which was considered rather fast for a complex design in 1976–1978.
The 8086 was sequenced using a mixture of random logic and microcode and was implemented using depletion-load nMOS circuitry with approximately 20,000 active transistors (29,000 counting all ROM and PLA sites). It was soon moved to a new refined nMOS manufacturing process called HMOS (for High performance MOS) that Intel originally developed for manufacturing of fast static RAM products. This was followed by HMOS-II, HMOS-III versions, and, eventually, a fully static CMOS version for battery powered devices, manufactured using Intel's CHMOS processes. The original chip measured 33 mm² and minimum feature size was 3.2 μm.
The architecture was defined by Stephen P. Morse with some help and assistance by Bruce Ravenel (the architect of the 8087) in refining the final revisions. Logic designer Jim McKevitt and John Bayliss were the lead engineers of the hardware-level development team and Bill Pohlman the manager for the project. The legacy of the 8086 is enduring in the basic instruction set of today's personal computers and servers; the 8086 also lent its last two digits to later extended versions of the design, such as the Intel 286 and the Intel 386, all of which eventually became known as the x86 family. (Another reference is that the PCI Vendor ID for Intel devices is 8086h.)
Details.
Buses and operation.
All internal registers, as well as internal and external data buses, are 16 bits wide, which firmly established the "16-bit microprocessor" identity of the 8086. A 20-bit external address bus provides a 1 MB physical address space (220 = 1,048,576). This address space is addressed by means of internal memory "segmentation". The data bus is multiplexed with the address bus in order to fit all of the control lines into a standard 40-pin dual in-line package. It provides a 16-bit I/O address bus, supporting 64 KB of separate I/O space. The maximum linear address space is limited to 64 KB, simply because internal address/index registers are only 16 bits wide. Programming over 64 KB memory boundaries involves adjusting the segment registers (see below); this difficulty existed until the 80386 architecture introduced wider (32-bit) registers (the memory management hardware in the 80286 did not help in this regard, as its registers are still only 16 bits wide).
Some of the control pins, which carry essential signals for all external operations, have more than one function depending upon whether the device is operated in "min" or "max" mode. The former mode was intended for small single-processor systems, while the latter was for medium or large systems using more than one processor.
Registers and instructions.
The 8086 has eight more or less general 16-bit registers (including the stack pointer but excluding the instruction pointer, flag register and segment registers). Four of them, AX, BX, CX, DX, can also be accessed as twice as many 8-bit registers (see figure) while the other four, BP, SI, DI, SP, are 16-bit only.
Due to a compact encoding inspired by 8-bit processors, most instructions are one-address or two-address operations, which means that the result is stored in one of the operands. At most one of the operands can be in memory, but this memory operand can also be the "destination", while the other operand, the "source", can be either "register" or "immediate". A single memory location can also often be used as both "source" and "destination" which, among other factors, further contributed to a code density comparable to (and often better than) most eight-bit machines at the time.
The degree of generality of most registers are much greater than in the 8080 or 8085. However, 8086 registers were more specialized than in most contemporary minicomputers and are also used implicitly by some instructions. While perfectly sensible for the assembly programmer, this made register allocation for compilers more complicated compared to more orthogonal 16-bit and 32-bit processors of the time such as the PDP-11, VAX, 68000, 32016 etc. On the other hand, being more regular than the rather minimalistic but ubiquitous 8-bit microprocessors such as the 6502, 6800, 6809, 8085, MCS-48, 8051, and other contemporary accumulator based machines, it was significantly easier to construct an efficient code generator for the 8086 architecture.
Another factor for this was that the 8086 also introduced some new instructions (not present in the 8080 and 8085) to better support stack-based high-level programming languages such as Pascal and PL/M; some of the more useful instructions were push "mem-op", and ret "size", supporting the "Pascal calling convention" directly. (Several others, such as push "immed" and enter, were added in the subsequent 80186, 80286, and 80386 processors.)
A 64 KB (one segment) stack growing towards lower addresses is supported in hardware; 16-bit words are pushed onto the stack, and the top of the stack is pointed to by SS:SP. There are 256 interrupts, which can be invoked by both hardware and software. The interrupts can cascade, using the stack to store the return addresses.
The 8086 has 64 K of 8-bit (or alternatively 32 K of 16-bit word) I/O port space.
Flags.
8086 has a 16-bit flags register. Nine of these condition code flags are active, and indicate the current state of the processor: Carry flag (CF), Parity flag (PF), Auxiliary carry flag (AF), Zero flag (ZF), Sign flag (SF), Trap flag (TF), Interrupt flag (IF), Direction flag (DF), and Overflow flag (OF).
Segmentation.
There are also four 16-bit segment registers (see figure) that allow the 8086 CPU to access one megabyte of memory in an unusual way. Rather than concatenating the segment register with the address register, as in most processors whose address space exceeded their register size, the 8086 shifts the 16-bit segment only four bits left before adding it to the 16-bit offset (16×segment + offset), therefore producing a 20-bit external (or effective or physical) address from the 32-bit segment:offset pair. As a result, each external address can be referred to by 212 = 4096 different segment:offset pairs.
Although considered complicated and cumbersome by many programmers, this scheme also has advantages; a small program (less than 64 KB) can be loaded starting at a fixed offset (such as 0000) in its own segment, avoiding the need for relocation, with at most 15 bytes of alignment waste.
Compilers for the 8086 family commonly support two types of pointer, "near" and "far". Near pointers are 16-bit offsets implicitly associated with the program's code or data segment and so can be used only within parts of a program small enough to fit in one segment. Far pointers are 32-bit segment:offset pairs resolving to 20-bit external addresses. Some compilers also support "huge" pointers, which are like far pointers except that pointer arithmetic on a huge pointer treats it as a linear 20-bit pointer, while pointer arithmetic on a far pointer wraps around within its 16-bit offset without touching the segment part of the address.
To avoid the need to specify "near" and "far" on numerous pointers, data structures, and functions, compilers also support "memory models" which specify default pointer sizes. The "tiny" (max 64K), "small" (max 128K), "compact" (data > 64K), "medium" (code > 64K), "large" (code,data > 64K), and "huge" (individual arrays > 64K) models cover practical combinations of near, far, and huge pointers for code and data. The "tiny" model means that code and data are shared in a single segment, just as in most 8-bit based processors, and can be used to build ".com" files for instance. Precompiled libraries often came in several versions compiled for different memory models.
According to Morse et al.. the designers actually contemplated using an 8-bit shift (instead of 4-bit), in order to create a 16 MB physical address space. However, as this would have forced segments to begin on 256-byte boundaries, and 1 MB was considered very large for a microprocessor around 1976, the idea was dismissed. Also, there were not enough pins available on a low cost 40-pin package for the additional four address bus pins
In principle, the address space of the x86 series "could" have been extended in later processors by increasing the shift value, as long as applications obtained their segments from the operating system and did not make assumptions about the equivalence of different segment:offset pairs. In practice the use of "huge" pointers and similar mechanisms was widespread and the flat 32-bit addressing made possible with the 32-bit offset registers in the 80386 eventually extended the limited addressing range in a more general way (see below).
Intel could have decided to implement memory in 16 bit words (which would have eliminated the signal along with much of the address bus complexities already described). This would mean that all instruction object codes and data would have to be accessed in 16-bit units. Users of the 8080 long ago realized, in hindsight, that the processor makes very efficient use of its memory. By having a large number of 8-bit object codes, the 8080 produces object code as compact as some of the most powerful minicomputers on the market at the time.
If the 8086 is to retain 8-bit object codes and hence the efficient memory use of the 8080, then it cannot guarantee that (16-bit) opcodes and data will lie on an even-odd byte address boundary. The first 8-bit opcode will shift the next 8-bit instruction to an odd byte or a 16-bit instruction to an odd-even byte boundary. By implementing the signal and the extra logic needed, the 8086 has allowed instructions to exist as 1-byte, 3-byte or any other odd byte object codes.
Simply put: this is a trade off. If memory addressing is simplified so that memory is only accessed in 16-bit units, memory will be used less efficiently. Intel decided to make the logic more complicated, but memory use more efficient. This was at a time when memory size was considerably smaller, and at a premium, than that which users are used to today.
Porting older software.
Small programs could ignore the segmentation and just use plain 16-bit addressing. This allowed 8-bit software to be quite easily ported to the 8086. The authors of MS-DOS took advantage of this by providing an Application Programming Interface very similar to CP/M as well as including the simple ".com" executable file format, identical to CP/M. This was important when the 8086 and MS-DOS were new, because it allowed many existing CP/M (and other) applications to be quickly made available, greatly easing acceptance of the new platform.
Example code.
The following 8086/8088 assembler source code is for a subroutine named codice_1 that copies a block of data bytes of a given size from one location to another. The data block is copied one byte at a time, and the data movement and looping logic utilizes 16-bit operations.
The code above uses the BP (base pointer) register to establish a call frame, an area on the stack that contains all of the parameters and local variables for the execution of the subroutine. This kind of calling convention supports reentrant and recursive code, and has been used by most ALGOL-like languages since the late 1950s. The ES segment register is saved on the stack and replaced with the value of the DS segment register, so that the   instructions will operate within the same source and destination data segment. Before returning, the subroutine restores the previous value of the ES register.
The above routine is a rather cumbersome way to copy blocks of data. Provided the source and the destination blocks reside within single 65,536 byte segments (a requirement of the above routine), advantage can be taken of the 8086's block codice_2 instructions. The loop section of the above can be replaced by:
This copies the block of data one word at a time. The codice_3 instruction causes the following codice_4 to repeat until CX=0, automatically incrementing SI and DI as it repeats. Alternatively the codice_5 or codice_6 instructions can be used to copy single bytes or double words at a time. Most assemblers will assemble correctly if the codice_3 instruction is used as a prefix to codice_4 as in codice_9.
This routine will operate correctly if interrupted, because the program counter will continue to point to the codice_3 instruction until the block copy is completed. The copy will therefore continue from where it left off when the interrupt service routine returns control.
Performance.
Although partly shadowed by other design choices in this particular chip, the multiplexed address and data buses limited performance slightly; transfers of 16-bit or 8-bit quantities were done in a four-clock memory access cycle, which was faster on 16-bit, although slower on 8-bit quantities, compared to many contemporary 8-bit based CPUs. As instructions varied from one to six bytes, fetch and execution were made concurrent and decoupled into separate units (as it remains in today's x86 processors): The "bus interface unit" fed the instruction stream to the "execution unit" through a 6-byte prefetch queue (a form of loosely coupled pipelining), speeding up operations on registers and immediates, while memory operations unfortunately became slower (four years later, this performance problem was fixed with the 80186 and 80286). However, the full (instead of partial) 16-bit architecture with a full width ALU meant that 16-bit arithmetic instructions could now be performed with a single ALU cycle (instead of two, via internal carry, as in the 8080 and 8085), speeding up such instructions considerably. Combined with orthogonalizations of operations versus operand types and addressing modes, as well as other enhancements, this made the performance gain over the 8080 or 8085 fairly significant, despite cases where the older chips may be faster (see below).
As can be seen from these tables, operations on registers and immediates were fast (between 2 and 4 cycles), while memory-operand instructions and jumps were quite slow; jumps took more cycles than on the simple 8080 and 8085, and the 8088 (used in the IBM PC) was additionally hampered by its narrower bus. The reasons why most memory related instructions were slow were threefold:
However, memory access performance was drastically enhanced with Intel's next generation chips. The 80186 and 80286 both had dedicated address calculation hardware, saving many cycles, and the 80286 also had separate (non-multiplexed) address and data buses.
Floating point.
The 8086/8088 could be connected to a mathematical coprocessor to add hardware/microcode-based floating point performance. The Intel 8087 was the standard math coprocessor for the 8086 and 8088, operating on 80-bit numbers. Manufacturers like Cyrix (8087-compatible) and Weitek ("non" 8087-compatible) eventually came up with high performance floating point coprocessors that competed with the 8087 as well as with the subsequent, higher performing Intel 80387.
Chip versions.
The clock frequency was originally limited to 5 MHz (IBM PC used 4.77 MHz, 4/3 the standard NTSC color burst frequency), but the last versions in HMOS were specified for 10 MHz. HMOS-III and CMOS versions were manufactured for a long time (at least a while into the 1990s) for embedded systems, although its successor, the 80186/80188 (which includes some on-chip peripherals), has been more popular for embedded use.
The 80C86, the CMOS version of the 8086, was used in the GRiDPad, Toshiba T1200, HP 110, and finally the 1998-1999 Lunar Prospector.
Derivatives and clones.
Compatible—and, in many cases, enhanced—versions were manufactured by Fujitsu, Harris/Intersil, OKI, Siemens AG, Texas Instruments, NEC, Mitsubishi, AMD. For example, the NEC V20 and NEC V30 pair were hardware compatible with the 8088 and 8086 even though NEC made original Intel clones μPD8088D and μPD8086D, respectively, but incorporated the instruction set of the 80186 along with some (but not all) of the 80186 speed enhancements, providing a drop-in capability to upgrade both instruction set and processing speed without manufacturers having to modify their designs. Such relatively simple and low-power 8086-compatible processors in CMOS are still used in embedded systems.
The electronics industry of the Soviet Union was able to replicate the 8086 through The resulting chip, K1810BM86, was binary and pin-compatible with the 8086.
i8086 and i8088 were respectively the cores of the Soviet-made PC-compatible EC1831 and EC1832 desktops (EC1831 is the EC identification of IZOT 1036C and EC1832 is the EC identification of IZOT 1037C, developed and manufactured in Bulgaria. EC stands for Единая Система.). However, EC1831 computer (IZOT 1036C) had significant hardware differences from its authentic prototype. EC1831 was the first PC compatible computer with dynamic bus sizing (US Pat. No 4,831,514). Later some of the ES1831 principles were adopted in PS/2 (US Pat. No 5,548,786) and some other machines (UK Patent Application, Publication No. GB-A-2211325, Published June 28, 1989).
Hardware modes.
The 8086 and 8088 support two hardware modes: maximum mode and minimum mode. Maximum mode is for large applications such as multiprocessing and is also required to support the 8087 coprocessor. The mode is usually hardwired into the circuit and cannot be changed by software. Specifically, pin #33 (MN/) is either wired to voltage or to ground to determine the mode. Changing the state of pin #33 changes the function of certain other pins, most of which have to do with how the CPU handles the (local) bus. The IBM PC and PC/XT use an Intel 8088 running in maximum mode, which allows the CPU to work with an optional 8087 coprocessor installed in the math coprocessor socket on the PC or PC/XT mainboard. (The PC and PC/XT may require maximum mode for other reasons, such as perhaps to support the DMA controller.) The workings of minimum mode configuration can be described in the terms of timing diagrams.
In a minimum mode 8086-based system, the 8086 microprocessor is placed into minimum mode by strapping its MN/ pin to logic high, i.e. +5V. In minimum mode, all control signals are generated by the 8086 microprocessor itself. Components in minimum mode are latches, trans-receiver, clock generator, memory and I/O device.

</doc>
<doc id="15064" url="https://en.wikipedia.org/wiki?curid=15064" title="Intel 8088">
Intel 8088

The Intel 8088 (""eighty-eighty-eight"", also called iAPX 88) microprocessor is a variant of the Intel 8086. Introduced on July 1, 1979, the 8088 had an 8-bit external data bus instead of the 16-bit bus of the 8086. The 16-bit registers and the one megabyte address range were unchanged, however. In fact, according to the Intel documentation, the 8086 and 8088 have the same execution unit (EU)—only the bus interface unit (BIU) is different. The original IBM PC was based on the 8088.
History and description.
The 8088 was designed in Israel, at Intel's Haifa laboratory, as with a large number of Intel's processors. The 8088 was targeted at economical systems by allowing the use of an 8-bit data path and 8-bit support and peripheral chips; complex circuit boards were still fairly cumbersome and expensive when it was released. The prefetch queue of the 8088 was shortened to four bytes, from the 8086's six bytes, and the prefetch algorithm was slightly modified to adapt to the narrower bus. These modifications of the basic 8086 design were one of the first jobs assigned to Intel's then new design office and laboratory in Haifa, Israel.
Variants of the 8088 with more than 5 MHz maximum clock frequency include the 8088-2, which was fabricated using Intel's new enhanced nMOS process called HMOS and specified for a maximum frequency of 8 MHz. Later followed the 80C88, a fully static CHMOS design, which could operate with clock speeds from 0 to 8 MHz. There were also several other, more or less similar, variants from other manufacturers. For instance, the NEC V20 was a pin compatible and slightly faster (at the same clock frequency) variant of the 8088, designed and manufactured by NEC. Successive NEC 8088 compatible processors would run at up to 16 MHz. In 1984, Commodore International signed a deal to manufacture the 8088 for use in a licensed Dynalogic Hyperion clone, in a move that was regarded as signalling a major new direction for the company.
When announced, the list price of the 8088 was US $124.80.
Differences from the 8086.
The 8088 is architecturally very similar to the 8086. The main difference is that there are only 8 data lines instead of the 8086's 16 lines. All of the other pins of the device perform the same function as they do with the 8086 with two exceptions. First, pin 34 is no longer (this is the high order byte select on the 8086 - the 8088 does not have a high order byte on its 8 bit data bus). Instead it outputs a maximum mode status, . Combined with the IO/ and DT/ signals, the bus cycles can be decoded (It generally indicates when a write operation or an interrupt is in progress). The second change is the pin that signals if a memory access or input/output access is being made has had it sense reversed. The pin on the 8088 is IO/. On the 8086 part it is /M. The reason for the reversal is that it makes the 8088 compatible with the 8085.
Performance.
Depending on the clock frequency, the number of memory wait states, as well as on the characteristics of the particular application program, the average performance for the Intel 8088 ranged from approximately 0.33–1 million instructions per second. Meanwhile, the mov "reg,reg" and ALU "reg,reg" instructions taking two and three cycles respectively yielded an "absolute peak" performance of between 1/3 and 1/2 MIPS per MHz, that is, somewhere in the range 3–5 MIPS at 10 MHz.
The speed of the execution unit (EU) and the bus of the 8086 CPU was well balanced; with a typical instruction mix, an 8086 could execute instructions out of the prefetch queue a good bit of the time. Cutting down the bus to 8 bits made it a serious bottleneck in the 8088. With the speed of instruction fetch reduced by 50% in the 8088 as compared to the 8086, a sequence of fast instructions can quickly drain the 4-byte prefetch queue. When the queue is empty, instructions take as long to complete as they take to fetch. Both the 8086 and 8088 take four clock cycles to complete a bus cycle; whereas for the 8086 this means 4 clocks to transfer 2 bytes, on the 8088 it is 4 clocks per byte. Therefore, for example, a 2-byte shift or rotate instruction, which takes the EU only 2 clock cycles to execute, actually takes eight clocks to complete if it is not in the prefetch queue. A sequence of such fast instructions prevents the queue from being filled as fast as it is drained, and 
In general, because so many basic instructions execute in fewer than four clocks per instruction byte—including almost all the ALU and data-movement instructions on register operands and some of these on memory operands—it is practically impossible to avoid idling the EU in the 8088 at least 1/4 of the time while executing useful real-world programs, and it is not hard to idle it half the time. In short, an 8088 typically runs about half as fast as 8086 clocked at the same rate, because of the bus bottleneck (the only major difference).
A side effect of the 8088 design, with the slow bus and the small prefetch queue, is that the speed of code execution can be very dependent on instruction order. When programming the 8088, for CPU efficiency, it is vital to interleave long-running instructions with short ones whenever possible. For example, a repeated string operation or a shift by three or more will take long enough to allow time for the 4-byte prefetch queue to completely fill. If short instructions (i.e. ones totaling few bytes) are placed between slower instructions like these, the short ones can execute at full speed out of the queue. If, on the other hand, the slow instructions are executed sequentially, back to back, then after the first of them the bus unit will be forced to idle because the queue will already be full, with the consequence that later more of the faster instructions will suffer fetch delays that might have been avoidable. As some instructions, such as single-bit-position shifts and rotates, take literally 4 times as long to fetch as to execute, the overall effect can be a slowdown by a factor of two or more. If those code segments are the bodies of loops, the difference in execution time may be very noticeable on the human timescale.
The 8088 is also (like the 8086) slow at accessing memory. The same ALU that is used to execute arithmetic and logic instructions is also used to calculate effective addresses. (There is a separate adder for adding a shifted segment register to the offset address, but the offset EA itself is always calculated entirely in the main ALU.) Furthermore, the loose coupling of the EU and BIU (bus unit) inserts communication overhead between the units, and the four clock period bus transfer cycle is not particularly streamlined. (Contrast the two clock period bus cycle of the 6502 CPU and the 80286's three clock period bus cycle with pipelining down to two cycles for most transfers.) Most 8088 instructions that can operate on either registers or memory, including common ALU and data-movement operations, are at least four times slower for memory operands than for only register operands. Therefore, efficient 8088 (and 8086) programs avoid repeated access of memory operands when possible, loading operands from memory into registers to work with them there and storing back only the finished results. The relatively large general register set of the 8088 compared to its contemporaries assists this strategy. When there are not enough registers for all variables that are needed at once, saving registers by pushing them onto the stack and popping them back to restore them is the fastest way to use memory to augment the registers, as the stack PUSH and POP instructions are the fastest memory operations. (The same is probably not true on the 80286 and later; they have dedicated address ALUs and perform memory accesses much faster than the 8088 and 8086.)
Finally, because calls, jumps, and interrupts reset the prefetch queue, and because loading the IP register requires communication between the EU and the BIU (since the IP register is in the BIU, not in the EU where the general registers are), these operations are costly. All jumps and calls take at least 15 clock cycles. Any conditional jump requires 4 clock cycles if not taken, but if taken it requires 16 cycles in addition to resetting the prefetch queue; therefore, conditional jumps should be arranged to be not taken most of the time, especially inside loops. In some cases, a sequence of logic and movement operations is faster than a conditional jump that skips over one or two instructions to achieve the same result.
Intel datasheets for the 8086 and 8088 advertised the dedicated multiply and divide instructions (MUL, IMUL, DIV, and IDIV), but they are very slow, on the order of 100-200 clock cycles each. Many simple multiplications by small constants (besides powers of two, for which shifts can be used) can be done much faster using dedicated short subroutines. (The 80286 and 80386 each greatly increased the execution speed of these multiply and divide instructions.)
Selection for use in the IBM PC.
The original IBM PC was the most influential microcomputer to use the 8088. It used a clock frequency of 4.77 MHz (4/3 the NTSC colorburst frequency). Some of IBM's engineers and other employees wanted to use the IBM 801 processor, some would have preferred the new Motorola 68000, while others argued for a small and simple microprocessor, such as the MOS Technology 6502 or Zilog Z80, which had been used in earlier personal computers. However, IBM already had a history of using Intel chips in its products and had also acquired the rights to manufacture the 8086 family.
IBM chose the 8088 over the 8086 because Intel offered a better price for the former, and could supply more units. Another factor was that the 8088 allowed the computer to be based on a modified 8085 design, as it could easily interface with most nMOS chips with 8-bit databuses, i.e. existing and mature, and therefore economical, components. This included ICs originally intended for support and peripheral functions around the 8085 and similar processors (not exclusively Intel's) which were already well known by many engineers, further reducing cost.
The descendants of the 8088 include the 80188, 80186, 80286, 80386, 80486, and later software compatible processors, which are in use today.

</doc>
<doc id="15066" url="https://en.wikipedia.org/wiki?curid=15066" title="Insulator (electricity)">
Insulator (electricity)

An electrical insulator is a material whose internal electric charges do not flow freely, and therefore make it nearly impossible to conduct an electric current under the influence of an electric field. This contrasts with other materials, semiconductors and conductors, which conduct electric current more easily. The property that distinguishes an insulator is its resistivity; insulators have higher resistivity than semiconductors or conductors.
A perfect insulator does not exist, because even insulators contain small numbers of mobile charges (charge carriers) which can carry current. In addition, all insulators become electrically conductive when a sufficiently large voltage is applied that the electric field tears electrons away from the atoms. This is known as the breakdown voltage of an insulator. Some materials such as glass, paper and Teflon, which have high resistivity, are very good electrical insulators. A much larger class of materials, even though they may have lower bulk resistivity, are still good enough to prevent significant current from flowing at normally used voltages, and thus are employed as insulation for electrical wiring and cables. Examples include rubber-like polymers and most plastics.
Insulators are used in electrical equipment to support and separate electrical conductors without allowing current through themselves. An insulating material used in bulk to wrap electrical cables or other equipment is called "insulation". The term "insulator" is also used more specifically to refer to insulating supports used to attach electric power distribution or transmission lines to utility poles and transmission towers. They support the weight of the suspended wires without allowing the current to flow through the tower to ground.
Physics of conduction in solids.
Electrical insulation is the absence of electrical conduction. Electronic band theory (a branch of physics) says that a charge flows if states are available into which electrons can be excited. This allows electrons to gain energy and thereby move through a conductor such as a metal. If no such states are available, the material is an insulator.
Most (though not all, see Mott insulator) insulators have a large band gap. This occurs because the "valence" band containing the highest energy electrons is full, and a large energy gap separates this band from the next band above it. There is always some voltage (called the breakdown voltage) that gives electrons enough energy to be excited into this band. Once this voltage is exceeded the material ceases being an insulator, and charge begins to pass through it. However, it is usually accompanied by physical or chemical changes that permanently degrade the material's insulating properties.
Materials that lack electron conduction are insulators if they lack other mobile charges as well. For example, if a liquid or gas contains ions, then the ions can be made to flow as an electric current, and the material is a conductor. Electrolytes and plasmas contain ions and act as conductors whether or not electron flow is involved.
Breakdown.
When subjected to a high enough voltage, insulators suffer from the phenomenon of electrical breakdown. When the electric field applied across an insulating substance exceeds in any location the threshold breakdown field for that substance, the insulator suddenly becomes a conductor, causing a large increase in current, an electric arc through the substance. Electrical breakdown occurs when the electric field in the material is strong enough to accelerate free charge carriers (electrons and ions, which are always present at low concentrations) to a high enough velocity to knock electrons from atoms when they strike them, ionizing the atoms. These freed electrons and ions are in turn accelerated and strike other atoms, creating more charge carriers, in a chain reaction. Rapidly the insulator becomes filled with mobile charge carriers, and its resistance drops to a low level. In a solid, the breakdown voltage is proportional to the band gap energy. The air in a region around a high-voltage conductor can break down and ionise without a catastrophic increase in current; this is called "corona discharge". However, if the region of air breakdown extends to another conductor at a different voltage it creates a conductive path between them, and a large current flows through the air, creating an "electric arc". Even a vacuum can suffer a sort of breakdown, but in this case the breakdown or vacuum arc involves charges ejected from the surface of metal electrodes rather than produced by the vacuum itself.
In case of some insulators, the conduction may take place at a very high temperature as then the energy acquired by the valence electrons is sufficient to take them into conduction band.
Uses.
A very flexible coating of an insulator is often applied to electric wire and cable, this is called "insulated wire". Since air is an insulator, in principle no other substance is needed to keep power where it should be. High-voltage power lines commonly use just air, since a solid (e.g. plastic) coating is impractical. However, wires that touch each other produce cross connections, short circuits, and fire hazards. In coaxial cable the center conductor must be supported exactly in the middle of the hollow shield in order to prevent EM wave reflections. Finally, wires that expose voltages higher than 60 V can cause human shock and electrocution hazards. Insulating coatings help to prevent all of these problems.
Some wires have a mechanical covering with no voltage rating—e.g.: service-drop, welding, doorbell, thermostat wire. An insulated wire or cable has a voltage rating and a maximum conductor temperature rating. It may not have an ampacity (current-carrying capacity) rating, since this is dependent upon the surrounding environment (e.g. ambient temperature).
In electronic systems, printed circuit boards are made from epoxy plastic and fibreglass. The nonconductive boards support layers of copper foil conductors. In electronic devices, the tiny and delicate active components are embedded within nonconductive epoxy or phenolic plastics, or within baked glass or ceramic coatings.
In microelectronic components such as transistors and ICs, the silicon material is normally a conductor because of doping, but it can easily be selectively transformed into a good insulator by the application of heat and oxygen. Oxidised silicon is quartz, i.e. silicon dioxide, the primary component of glass.
In high voltage systems containing transformers and capacitors, liquid insulator oil is the typical method used for preventing arcs. The oil replaces air in spaces that must support significant voltage without electrical breakdown. Other high voltage system insulation materials include ceramic or glass wire holders, gas, vacuum, and simply placing wires far enough apart to use air as insulation.
Telegraph and power transmission insulators.
Overhead conductors for high-voltage electric power transmission are bare, and are insulated by the surrounding air. Conductors for lower voltages in distribution may have some insulation but are often bare as well. Insulating supports called "insulators" are required at the points where they are supported by utility poles or transmission towers. Insulators are also required where the wire enters buildings or electrical devices, such as transformers or circuit breakers, to insulate the wire from the case. These hollow insulators with a conductor inside them are called bushings.
Material.
Insulators used for high-voltage power transmission are made from glass, porcelain or composite polymer materials. Porcelain insulators are made from clay, quartz or alumina and feldspar, and are covered with a smooth glaze to shed water. Insulators made from porcelain rich in alumina are used where high mechanical strength is a criterion. Porcelain has a dielectric strength of about 4–10 kV/mm. Glass has a higher dielectric strength, but it attracts condensation and the thick irregular shapes needed for insulators are difficult to cast without internal strains. Some insulator manufacturers stopped making glass insulators in the late 1960s, switching to ceramic materials.
Recently, some electric utilities have begun converting to polymer composite materials for some types of insulators. These are typically composed of a central rod made of fibre reinforced plastic and an outer weathershed made of silicone rubber or ethylene propylene diene monomer rubber (EPDM). Composite insulators are less costly, lighter in weight, and have excellent hydrophobic capability. This combination makes them ideal for service in polluted areas. However, these materials do not yet have the long-term proven service life of glass and porcelain.
Design.
The electrical breakdown of an insulator due to excessive voltage can occur in one of two ways:
Most high voltage insulators are designed with a lower flashover voltage than puncture voltage, so they flash over before they puncture, to avoid damage.
Dirt, pollution, salt, and particularly water on the surface of a high voltage insulator can create a conductive path across it, causing leakage currents and flashovers. The flashover voltage can be reduced by more than 50% when the insulator is wet. High voltage insulators for outdoor use are shaped to maximise the length of the leakage path along the surface from one end to the other, called the creepage length, to minimise these leakage currents. To accomplish this the surface is moulded into a series of corrugations or concentric disc shapes. These usually include one or more "sheds"; downward facing cup-shaped surfaces that act as umbrellas to ensure that the part of the surface leakage path under the 'cup' stays dry in wet weather. Minimum creepage distances are 20–25 mm/kV, but must be increased in high pollution or airborne sea-salt areas.
Types of Insulators.
These are the common classes of insulator:
Cap and pin insulators.
Higher voltage transmission lines usually use modular "cap and pin" insulator designs "(pictures, left)". The wires are suspended from a 'string' of identical disc-shaped insulators that attach to each other with metal clevis pin or ball and socket links. The advantage of this design is that insulator strings with different breakdown voltages, for use with different line voltages, can be constructed by using different numbers of the basic units. Also, if one of the insulator units in the string breaks, it can be replaced without discarding the entire string.
Each unit is constructed of a ceramic or glass disc with a metal cap and pin cemented to opposite sides. In order to make defective units obvious, glass units are designed with Class B construction, so that an overvoltage causes a puncture arc through the glass instead of a flashover. The glass is heat-treated so it shatters, making the damaged unit visible. However the mechanical strength of the unit is unchanged, so the insulator string stays together.
Standard disc insulator units are in diameter and long, can support a load of 80-120 kN (18-27 klbf), have a dry flashover voltage of about 72 kV, and are rated at an operating voltage of 10-12 kV. However, the flashover voltage of a string is less than the sum of its component discs, because the electric field is not distributed evenly across the string but is strongest at the disc nearest to the conductor, which flashes over first. Metal "grading rings" are sometimes added around the disc at the high voltage end, to reduce the electric field across that disc and improve flashover voltage.
In very high voltage lines the insulator may be surrounded by corona rings. These typically consist of toruses of aluminium (most commonly) or copper tubing attached to the line. They are designed to reduce the electric field at the point where the insulator is attached to the line, to prevent corona discharge, which results in power losses.
History.
The first electrical systems to make use of insulators were telegraph lines; direct attachment of wires to wooden poles was found to give very poor results, especially during damp weather.
The first glass insulators used in large quantities had an unthreaded pinhole. These pieces of glass were positioned on a tapered wooden pin, vertically extending upwards from the pole's crossarm (commonly only two insulators to a pole and maybe one on top of the pole itself). Natural contraction and expansion of the wires tied to these "threadless insulators" resulted in insulators unseating from their pins, requiring manual reseating.
Amongst the first to produce ceramic insulators were companies in the United Kingdom, with Stiff and Doulton using stoneware from the mid-1840s, Joseph Bourne (later renamed Denby) producing them from around 1860 and Bullers from 1868. Utility patent number 48,906 was granted to Louis A. Cauvet on 25 July 1865 for a process to produce insulators with a threaded pinhole: pin-type insulators still have threaded pinholes.
The invention of suspension-type insulators made high-voltage power transmission possible. Pin-type insulators were unsatisfactory over about 60,000 volts.
A large variety of telephone, telegraph and power insulators have been made; some people collect them, both for their historic interest and for the aesthetic quality of many insulator designs and finishes. One collectors organisation is the US National Insulator Association, which has over 9,000 members.
Insulation of antennas.
Often a broadcasting radio antenna is built as a mast radiator, which means that the entire mast structure is energised with high voltage and must be insulated from the ground. Steatite mountings are used. They have to withstand not only the voltage of the mast radiator to ground, which can reach values up to 400 kV at some antennas, but also the weight of the mast construction and dynamic forces. Arcing horns and lightning arresters are necessary because lightning strikes to the mast are common.
Guy wires supporting antenna masts usually have strain insulators inserted in the cable run, to keep the high voltages on the antenna from short circuiting to ground or creating a shock hazard. Often guy cables have several insulators, placed to break up the cable into lengths unwanted electrical resonances in the guy. These insulators are usually ceramic and cylindrical or egg-shaped (see picture). This construction has the advantage that the ceramic is under compression rather than tension, so it can withstand greater load, and that if the insulator breaks, the cable ends are still linked.
These insulators also have to be equipped with overvoltage protection equipment. For the dimensions of the guy insulation, static charges on guys have to be considered. At high masts these can be much higher than the voltage caused by the transmitter, requiring guys divided by insulators in multiple sections on the highest masts. In this case, guys which are grounded at the anchor basements via a coil - or if possible, directly - are the better choice.
Feedlines attaching antennas to radio equipment, particularly twin lead type, often must be kept at a distance from metal structures. The insulated supports used for this purpose are called "standoff insulators".
Insulation in electrical apparatus.
The most important insulation material is air. A variety of solid, liquid, and gaseous insulators are also used in electrical apparatus. In smaller transformers, generators, and electric motors, insulation on the wire coils consists of up to four thin layers of polymer varnish film. Film insulated magnet wire permits a manufacturer to obtain the maximum number of turns within the available space. Windings that use thicker conductors are often wrapped with supplemental fiberglass insulating tape. Windings may also be impregnated with insulating varnishes to prevent electrical corona and reduce magnetically induced wire vibration. Large power transformer windings are still mostly insulated with paper, wood, varnish, and mineral oil; although these materials have been used for more than 100 years, they still provide a good balance of economy and adequate performance. Busbars and circuit breakers in switchgear may be insulated with glass-reinforced plastic insulation, treated to have low flame spread and to prevent tracking of current across the material.
In older apparatus made up to the early 1970s, boards made of compressed asbestos may be found; while this is an adequate insulator at power frequencies, handling or repairs to asbestos material can release dangerous fibers into the air and must be carried cautiously. Wire insulated with felted asbestos was used in high-temperature and rugged applications from the 1920s. Wire of this type was sold by General Electric under the trade name "Deltabeston."
Live-front switchboards up to the early part of the 20th century were made of slate or marble. Some high voltage equipment is designed to operate within a high pressure insulating gas such as sulfur hexafluoride. Insulation materials that perform well at power and low frequencies may be unsatisfactory at radio frequency, due to heating from excessive dielectric dissipation.
Electrical wires may be insulated with polyethylene, crosslinked polyethylene (either through electron beam processing or chemical crosslinking), PVC, Kapton, rubber-like polymers, oil impregnated paper, Teflon, silicone, or modified ethylene tetrafluoroethylene (ETFE). Larger power cables may use compressed inorganic powder, depending on the application.
Flexible insulating materials such as PVC (polyvinyl chloride) are used to insulate the circuit and prevent human contact with a 'live' wire – one having voltage of 600 volts or less. Alternative materials are likely to become increasingly used due to EU safety and environmental legislation making PVC less economic.
Class 1 and Class 2 insulation.
All portable or hand-held electrical devices are insulated to protect their user from harmful shock.
Class 1 insulation requires that the metal body and other exposed metal parts of the device is connected to earth via a "grounding wire" that is earthed at the main service panel—but only needs basic insulation on the conductors. This equipment needs an extra pin on the power plug for the grounding connection.
Class 2 insulation means that the device is "double insulated". This is used on some appliances such as electric shavers, hair dryers and portable power tools. Double insulation requires that the devices have both basic and supplementary insulation, each of which is sufficient to prevent electric shock. All internal electrically energized components are totally enclosed within an insulated body that prevents any contact with "live" parts. In the EU, double insulated appliances all are marked with a symbol of two squares, one inside the other.

</doc>
<doc id="15067" url="https://en.wikipedia.org/wiki?curid=15067" title="Internetworking">
Internetworking

Internetworking is the practice of connecting a computer network with other networks through the use of gateways that provide a common method of routing information packets between the networks. The resulting system of interconnected networks are called an "internetwork", or simply an "internet". Internetworking is a combination of the words "inter" ("between") and networking; not "internet-working" or "international-network".
The most notable example of internetworking is the Internet, a network of networks based on many underlying hardware technologies, but unified by an internetworking protocol standard, the Internet Protocol Suite, often also referred to as TCP/IP.
The smallest amount of effort to create an internet (an internetwork, not "the" Internet), is to have two LANs of computers connected to each other via a router. Simply using either a switch or a hub to connect two local area networks together doesn't imply internetworking, it just expands the original LAN.
Interconnection of networks.
Internetworking started as a way to connect disparate types of networking technology, but it became widespread through the developing need to connect two or more local area networks via some sort of wide area network. The original term for an internetwork was catenet.
The definition of an internetwork today includes the connection of other types of computer networks such as personal area networks.
The network elements used to connect individual networks in the ARPANET, the predecessor of the Internet, were originally called gateways, but the term has been deprecated in this context, because of possible confusion with functionally different devices. Today the interconnecting gateways are called routers.
Another type of interconnection of networks often occurs within enterprises at the Link Layer of the networking model, i.e. at the hardware-centric layer below the level of the TCP/IP logical interfaces. Such interconnection is accomplished with network bridges and network switches. This is sometimes incorrectly termed internetworking, but the resulting system is simply a larger, single subnetwork, and no internetworking protocol, such as Internet Protocol, is required to traverse these devices. However, a single computer network may be converted into an internetwork by dividing the network into segments and logically dividing the segment traffic with routers.
The Internet Protocol is designed to provide an unreliable (not guaranteed) packet service across the network. The architecture avoids intermediate network elements maintaining any state of the network. Instead, this function is assigned to the endpoints of each communication session. To transfer data reliably, applications must utilize an appropriate Transport Layer protocol, such as Transmission Control Protocol (TCP), which provides a reliable stream. Some applications use a simpler, connection-less transport protocol, User Datagram Protocol (UDP), for tasks which do not require reliable delivery of data or that require real-time service, such as video streaming or voice chat.
Networking models.
Two architectural models are commonly used to describe the protocols and methods used in internetworking.
The Open System Interconnection (OSI) reference model was developed under the auspices of the International Organization for Standardization (ISO) and provides a rigorous description for layering protocol functions from the underlying hardware to the software interface concepts in user applications. Internetworking is implemented in the Network Layer (Layer 3) of the model.
The Internet Protocol Suite, also called the TCP/IP model of the Internet was not designed to conform to the OSI model and does not refer to it in any of the normative specifications in Requests for Comment and Internet standards. Despite similar appearance as a layered model, it uses a much less rigorous, loosely defined architecture that concerns itself only with the aspects of logical networking. It does not discuss hardware-specific low-level interfaces, and assumes availability of a Link Layer interface to the local network link to which the host is connected. Internetworking is facilitated by the protocols of its Internet Layer.

</doc>
<doc id="15068" url="https://en.wikipedia.org/wiki?curid=15068" title="Infantry">
Infantry

The infantry is the branch of a military force that fights on foot. As the troops who are intended to engage, fight, and defeat the enemy in face-to-face combat, they bear the brunt of warfare and typically suffer the greatest number of casualties. Historically, as the oldest branch of the combat arms, the infantry are the tip of the spear of a modern army, and continually undergo training that is more physically stressful and psychologically demanding than that of any other branch of the combat arms.
Infantry can enter and maneuver in terrain that is inaccessible to military vehicles and employ crew-served infantry weapons that provide greater and more sustained firepower. The transport and delivery techniques of modern infantrymen to engage in battle include marching, mechanised transport, airborne by parachute or by air assault from helicopter and amphibious landing from the sea.
History and etymology.
In English, the 16th-century term Infantry (ca. 1570s) describes soldiers who walk to the battlefield, and there engage, fight, and defeat the enemy in direct combat, usually to take and occupy the terrain. As describing the branch of the combat arms, the term "Infantry" derives from the French Infanterie, which, in turn, is derived from the Italian Fanteria and ultimately from the Latin Infantera; the individual-soldier term Infantryman (1837) was not coined until the 19th century. Historically, before the invention and the introduction of firearms to warfare, the foot soldiers of previous eras—armed with blunt and edged weapons, and a shield—also are considered and identified as infantrymen.
The term arose in Sixteenth-Century Spain, which boasted the first professional standing army seen in Europe since the days of Rome. It was common to appoint royal princes (Infantes) to military commands, and the men under them became known as Infanteria.
In the Western world, during the Græco–Roman Antiquity (8th–7th centuries BC), and during the Middle Ages (AD 476–1453), infantry soldiers were categorized, characterised, and identified according to the type of weapons and armour with which they were armed, thus heavy infantry (hoplite) and light infantry (Greek peltasts, Roman velites). Since the application of firearms to warfare, the classifications of infantrymen have changed to reflect their formations on the battlefield, such as line infantry, and to reflect the modes of transporting them to the battlefield, and the tactics deployed by specific types of combat units, such as mechanized infantry and airborne infantry.
Combat role.
As a branch of the armed forces, the role of the infantry in warfare is to engage, fight, and kill the enemy at close range—using either a firearm (rifle, pistol, machine gun), an edged-weapon (knife, bayonet), or bare hands (close quarters combat)—as required by the mission to hand; thus
Beginning with the Napoleonic Wars of the early 19th century, artillery has become an increasingly dominant force on the battlefield. Since World War I, combat aircraft and armoured vehicles have also become dominant. However, the most effective method for locating all enemy forces on a battlefield is still the infantry patrol, and it is the presence or absence of infantry that ultimately determines whether a particular piece of ground has been captured or held. In 20th and 21st century warfare, infantry functions most effectively as part of a combined arms team including artillery, armour, and combat aircraft. Studies have shown that of all casualties, 50% or more were caused by artillery; about 10% were caused by machine guns; 2–5% by rifle fire; and 1% or less by hand grenades, bayonets, knives, and unarmed combat combined. Several infantry divisions both Allied and Axis in the European theatre of WWII suffered higher than 100% combat and non combat casualties and some above 200%, meaning that the number of service personnel that became casualties was greater than the sum of the divisions' available service positions at full strength.
Organization.
Infantry relies on organized formations to be employed in battle. These have evolved over time, but remain a key element to effective infantry development and deployment. Until the end of the 19th century, infantry units were for the most part employed in close formations up until contact with the enemy. This allowed commanders to retain control of the unit, especially while maneuvering, as well as allowing officers to retain discipline amongst the ranks.
The development of machine guns and other weapons with increased firepower forced infantry units to disperse in order to make them less vulnerable to such weapons. This decentralization of command was made possible by improved communications equipment and greater focus on small unit training. From World War I, it was recognized that infantry were best employed when using their ability to maneuver in constricted terrain, and evade detection in ways not possible for other weapons such as vehicles.
Among the various subtypes of infantry is "Medium infantry." This refers to infantry which are less heavily armed and armored than heavy infantry, but more so than light infantry. In the early modern period, medium infantry were largely eliminated due to discontinued use of body armour up until the 20th century. In the United States Army, Stryker Infantry is considered Medium Infantry, since they are "heavier" than light infantry but "lighter" than mechanized infantry.
Doctrine.
Infantry doctrine is the concise expression of how infantry forces contribute to campaigns, major operations, battles, and engagements. It is a guide to action, not a set of hard and fast rules.
Doctrine provides a very common frame of reference across the military forces, allowing the infantry to function cooperatively in what are now called combined arms operations. Doctrine helps standardise operations, facilitating readiness by establishing common ways of accomplishing infantry tasks. Doctrine links theory, history, experimentation, and practice. Its objective is to foster initiative and creative thinking in the infantry's tactical combat environment.
Doctrine provides the infantry with an authoritative body of statements on how infantry forces conduct operations and provides a common lexicon for use by infantry planners and leaders.
Until the development of effective artillery doctrines, and more recently precision guided air delivered ordnance, the most recent important role of the infantry has been as the primary force of inflicting casualties on the enemy through aimed fire. The infantry is also the only combat arm which can ultimately decide whether any given tactical position is occupied, and it is the presence of infantry that assures control of terrain. While the tactics of employment in battle have changed, the basic missions of the infantry have not.
Retractions to the infantry concept.
Although it has been argued that infantrymen and infantry tactics are an antiquated and careless use of military manpower and resources, the infantryman has proven quite capable against many units, some much more sophisticated. For instance, light infantry has proven to be effective against tank units by exploiting a tank's limited field of fire and view when using grenades or antitank rockets. Air bombardment that otherwise have flattened cities may be ineffective against a dug in infantry force, such as at the Battle of Stalingrad 1942–1943.
Operations.
Attack operations.
Attack operations are the most basic role of the infantry, and along with defense, form the main stances of the infantry on the battlefield. Traditionally, in an open battle, or meeting engagement, two armies would maneuver to contact, at which point they would form up their infantry and other units opposite each other. Then one or both would advance and attempt to defeat the enemy force. The goal of an attack remains the same: to advance into an enemy-held "objective," most frequently a hill, river crossing, city or other dominant terrain feature, and dislodge the enemy, thereby establishing control of the objective.
Attacks are often feared by the infantry conducting them because of the high number of casualties suffered while advancing to close with and destroy the enemy while under enemy fire. In mechanized infantry the armored personnel carrier (APC) is considered the assaulting position. These APCs can deliver infantrymen through the front lines to the battle and—in the case of infantry fighting vehicles—contribute supporting firepower to engage the enemy. Successful attacks rely on sufficient force, preparative reconnaissance and battlefield preparation with bomb assets. Retention of discipline and cohesion throughout the attack is paramount to success. A subcategory of attacks is the ambush, where infantrymen lie in wait for enemy forces before attacking at a vulnerable moment. This gives the ambushing infantrymen the combat advantage of surprise, concealment and superior firing positions, and causes confusion. The ambushed unit does not know what it is up against, or where they are attacking from.
Patrol operations.
Patrolling is the most common infantry mission. Full-scale attacks and defensive efforts are occasional, but patrols are constant. Patrols consist of small groups of infantry moving about in areas of possible enemy activity to locate the enemy and destroy them when found. Patrols are used not only on the front-lines, but in rear areas where enemy infiltration or insurgencies are possible.
Pursuit operations.
Pursuit is a role that the infantry often assumes. The objective of pursuit operations is the destruction of withdrawing enemy forces which are not capable of effectively engaging friendly units, before they can build their strength to the point where they are effective. Infantry traditionally have been the main force to overrun these units in the past, and in modern combat are used to pursue enemy forces in constricted terrain (urban areas in particular), where faster forces, such as armoured vehicles are incapable of going or would be exposed to ambush.
Defense operations.
Defense operations are the natural counter to attacks, in which the mission is to hold an objective and defeat enemy forces attempting to dislodge the defender. Defensive posture offers many advantages to the infantry, including the ability to use terrain and constructed fortifications to advantage; these reduce exposure to enemy fire compared with advancing forces. Effective defense relies on minimizing losses to enemy fire, breaking the enemy's cohesion before their advance is completed, and preventing enemy penetration of defensive positions.
Escort operations.
Escorting consists of protecting support units from ambush, particularly from hostile infantry forces. Combat support units (a majority of the military) are not as well armed or trained as infantry units and have a different mission. Therefore, they need the protection of the infantry, particularly when on the move. This is one of the most important roles for the modern infantry, particularly when operating alongside armored vehicles. In this capacity, infantry essentially conducts patrol on the move, scouring terrain which may hide enemy infantry waiting to ambush friendly vehicles, and identifying enemy strong points for attack by the heavier units.
Base defense.
Infantry units are tasked to protect certain areas like command posts or airbases. Units assigned to this job usually have a large number of military police attached to them for control of checkpoints and prisons.
Maneouvering operations.
Maneouvering consumes much of an infantry unit's time. Infantry, like all combat arms units, are often maneuvered to meet battlefield needs, and often must do so under enemy attack. The infantry must maintain their cohesion and readiness during the move to ensure their usefulness when they reach their objective. Traditionally, infantry have relied on their own legs for mobility, but mechanised or armoured infantry often uses trucks and armored vehicles for transport. These units can quickly disembark and transition to light infantry, without vehicles, to access terrain which armoured vehicles can't effectively access.
Reconnaissance/intelligence gathering.
Surveillance operations are often carried out with the employment of small recon units or sniper teams which gather information about the enemy, reporting on characteristics such as size, activity, location, unit and equipment. These infantry units typically are known for their stealth and ability to operate for periods of time within close proximity of the enemy without being detected. They may engage high profile targets, or be employed to hunt down terrorist cells and insurgents within a given area. These units may also entice the enemy to engage a located recon unit, thus disclosing their location to be destroyed by more powerful friendly forces.
Military reserve force.
Some assignments for infantry units involve deployment behind the front, although patrol and security operations are usually maintained in case of enemy infiltration. This is usually the best time for infantry units to integrate replacements into units and to maintain equipment. Additionally, soldiers can be rested and general readiness should improve. However, the unit must be ready for deployment at any point.
Construction/engineering.
This can be undertaken either in reserve or on the front, but consists of using infantry troops as labor for construction of field positions, roads, bridges, airfields, and all other manner of structures. The infantry is often given this assignment because of the physical quantity of strong men within the unit, although it can lessen a unit's morale and limit the unit's ability to maintain readiness and perform other missions. More often, such jobs are given to specialist engineering corps.
Raids/Hostage rescue.
Infantry units are trained to quickly mobilise, infiltrate, enter and neutralise threat forces when appropriate combat intelligence indicates to secure a location, rescue or capture high profile targets.
Urban combat.
Urban combat poses unique challenges to the combat forces. It is one of the most complicated type of operations an infantry unit will undertake. With many places for the enemy to hide and ambush from, infantry units must be trained in how to enter a city, and systematically clear the buildings, which most likely will be booby trapped, in order to kill or capture enemy personnel within the city. Care must be taken to differentiate innocent civilians who often hide and support the enemy from the non-uniformed armed enemy forces. Civilian and military casualties both are usually very high.
Day to day service.
Because of an infantryman's duties with firearms, explosives, physical and emotional stress, physical violence, casualties and deaths are not uncommon in both war and in peacetime training or operations. It is a highly dangerous and demanding combat service and in World War II military doctors concluded that even physically unwounded soldiers were psychologically worn out after about 200 days of combat.
The physical, mental and environmental operating demands of the infantryman are high. All of the combat necessities such as ammunition, weapon systems, food, water, clothing and shelter are carried on the backs of the infantrymen, at least in light role as opposed to mounted/mechanised. Combat loads of over 36 kg (80 lbs) are standard, and greater loads in excess of 45 kg (100 lbs) are very common. These heavy loads, combined with long foot patrols of over a day, in any climate from in temperature, require the infantryman to be in good physical and mental shape. Infantrymen live, fight and die outdoors in all types of brutal climates, often with no physical shelter. Poor climate conditions adds misery to this already demanding existence. Disease epidemics, frostbite, heat stroke, trench foot, insect and wild animal bites are common along with stress disorders and these have sometimes caused more casualties than enemy action.
Despite the hardships, infantrymen are expected to continue with their combat missions despite death and injury of friends, fear, despair, fatigue and bodily injury.
Some infantry units are considered Special Forces. The earliest Special Forces commando units were more highly trained infantrymen, with special weapons, equipment and missions. Special Forces units recruit heavily from regular infantry units to fill their ranks.
Foreign and domestic militaries typically have a slang term for their infantrymen. In the U.S. military, the slang term among both Marine and Army infantrymen for themselves is "grunt." In the British Army, they are the "squaddies." The infantry is a small close-knit community, and the slang names are terms of endearment that convey mutual respect and shared experiences.
Equipment and training.
In the past infantrymen were just a mass of hastily trained conscripts hastily armed with whatever could be quickly provided. In modern times, the infantryman can be a highly trained and equipped specialist in his own right.
The equipment of infantry forces has evolved along with the development of military technology and tactics in general, but certain constants remain regarding the design and selection of this equipment. Primary types of equipment are weaponry, protective gear, survival gear, and special, mission specific equipment. Infantry tactics have become much more involved, and yet must be learned and rehearsed until they become second nature when the infantry soldier is stumbling with fatigue and in the middle of the "fog of war." Spreading out, making use of cover and concealment, monitoring team-mates and leaders, and watching for the enemy must all become instinctive and simultaneous.
Infantry weapons have included all types of personal weapons, i.e., anything that can be handled by individual soldiers, as well as some light crew-served weapons that can be carried. During operations, especially in modern times, the infantry often scavenge and employ whatever weapons and equipment they can acquire from both friend and foe, in addition to those issued to them by their available supply chain.
Infantry of ancient times through the Renaissance wielded a wide array of non-gunpowder weaponry. Infantry formations used all sorts of melee weapons, such as various types of swords, axes, and maces; shock weapons, such as spears and pikes; and ranged weapons such as javelins, bows, and slings. Their crew-served weapons were the ballista and the battering ram. Infantry of these premodern periods also wore a variety of personal body armour, including chain mail and cuirasses. Many of the premodern infantry weapons evolved over time to counter these advances in body armor, such as the falchion and crossbow, which were designed to pierce chain mail armor and wound the underlying body.
Modern infantrymen may be trained to use equipment in addition to their personal rifles, such as hand guns or pistols, shotguns, machine guns, anti-tank missiles, infantry mortars, anti-personnel mines, other incendiary and explosive devices, bayonets, GPS, map and compass, encrypted communications equipment, booby traps, surveillance equipment, night vision equipment, sensitive intelligence documents, classified weapon systems and other sensitive equipment.
Protective equipment and survival gear.
Infantry protective gear includes all equipment designed to protect the soldier against enemy attack. Most protective gear comprises personal armor of some type. Ancient and medieval infantry used shields and wore leather and metal alloys for armour, as defense against both missile and hand-to-hand weapons. With the advent of effective firearms such as the arquebus, large numbers of men could be quickly trained into effective fighting forces, and such armour became thicker while providing less overall coverage to meet the threat of early firearms, which could only pierce this armour at close range. Generally, only pikemen were armoured in this fashion; gunners went largely unarmoured, due to the expense as well as the impracticality of armouring large numbers of men who were not expected to fight in close quarters where it would be most useful. As firearms became more powerful and armour became less useful against gunfire, the ratio of gunners to pikemen increased, until the advent of the bayonet rendered the latter entirely obsolete. While it became clear to most military leaders that the pikeman was now outdated, some armies stubbornly clung to the pike, though pikemen, too, would abandon their armour, until only specialized and prestigious cavalry units retained any significant armour coverage; the infantryman from this point went entirely unarmoured. The return to the use of the helmet was prompted by the need to defend against high explosive fragmentation and concussion, and further developments in materials led to effective bullet-defeating body armour such as Kevlar, within the weight acceptable for infantry use.
Beginning in the Vietnam War, the use of personal body armour has again become widespread among infantry units. Infantrymen must also often carry protective measures against chemical and biological attack, including gas masks, counter-agents, and protective suits. All of these protective measures add to the weight an infantryman must carry, and may decrease combat efficiency. Modern militaries are struggling to balance the value of personal body protection versus the weight burden and ability to move under such weight.
Infantry survival gear includes all of the items soldiers require for day-to-day survival in the combat environment. These include basic environmental protections, medical supplies, food, and sundries. As the amount of equipment a soldier can carry is very limited, efforts have been made to make equipment light and compact. Equipment is carried in tactical gear (such as ALICE), which should be comfortable to wear for extended periods of time, hamper movement as little as possible and be compatible with other things a soldier can be expected to carry, such as field radios and spare ammunition. Infantry have suffered high casualty rates from disease, exposure, and privation—often in excess of the casualties suffered from enemy attacks. Better equipment of troops to protect against these environmental factors greatly reduces these rates of loss. One of the most valuable pieces of gear is the entrenching tool—basically a folding spade—which can be employed not only to dig important defenses, but also in a variety of other daily tasks and even as a weapon.
Specialized equipment consists of a variety of gear which may or may not be carried, depending on the mission and the level of equipment of an army. Communications gear has become a necessity, as it allows effective command of infantry units over greater distances, and communication with artillery and other support units. In some units, individual communications are being used to allow the greatest level of flexibility. Engineering equipment, including explosives, mines, and other gear, is also commonly carried by the infantry or attached specialists. A variety of other gear, often relating to a specific mission, or to the particular terrain in which the unit is employed, can be carried by infantry units.
Other infantry.
Infantry in air forces, such as the Royal Air Force Regiment and the Royal Australian Air Force Airfield Defence Guards, are used primarily for ground-based defence of air bases and other, air force facilities. They also have a number of other, specialist roles, including Chemical, Biological, Radiological and Nuclear (CBRN) defence and training other, air force personnel in basic ground defence tactics.
Naval infantry, commonly known as marines, are a category of infantry that form part of a state’s naval forces and perform roles on land and at sea, including amphibious operations, as well as other, naval roles. They also perform other tasks, including land warfare, separate from naval operations.

</doc>
<doc id="15069" url="https://en.wikipedia.org/wiki?curid=15069" title="Identity function">
Identity function

In mathematics, an identity function, also called an identity relation or identity map or identity transformation, is a function that always returns the same value that was used as its argument. In equations, the function is given by .
Definition.
Formally, if "M" is a set, the identity function "f" on "M" is defined to be that function with domain and codomain "M" which satisfies
In other words, the function assigns to each element "x" of "M" the element "x" of "M".
The identity function "f" on "M" is often denoted by id"M".
In set theory, where a function is defined as a particular kind of binary relation, the identity function is given by the identity relation, or "diagonal" of "M".
Algebraic property.
If is any function, then we have (where "∘" denotes function composition). In particular, id"M" is the identity element of the monoid of all functions from "M" to "M".
Since the identity element of a monoid is unique, one can alternately define the identity function on "M" to be this identity element. Such a definition generalizes to the concept of an identity morphism in category theory, where the endomorphisms of "M" need not be functions.

</doc>
<doc id="15070" url="https://en.wikipedia.org/wiki?curid=15070" title="Intel 80386">
Intel 80386

The Intel 80386 (""eight-oh-three-eighty-six""), also known as i386 or just 386, is a 32-bit microprocessor introduced in 1985. The first versions had 275,000 transistors and were the CPU of many workstations and high-end personal computers of the time. As the original implementation of the 32-bit extension of the 80286 architecture, the 80386 instruction set, programming model, and binary encodings are still the common denominator for all 32-bit x86 processors, which is termed the "i386-architecture", "x86", or "IA-32", depending on context.
The 32-bit 80386 can correctly execute most code intended for the earlier 16-bit processors such as 8088 and 80286 that were ubiquitous in early PCs. (Following the same tradition, modern 64-bit x86 processors are able to run most programs written for older x86 CPUs, all the way back to the original 16-bit 8086 of 1978.) Over the years, successively newer implementations of the same architecture have become several hundreds of times faster than the original 80386 (and thousands of times faster than the 8086). A 33 MHz 80386 was reportedly measured to operate at about 11.4 MIPS.
The 80386 was introduced in October 1985, while manufacturing of the chips in significant quantities commenced in June 1986. Mainboards for 80386-based computer systems were cumbersome and expensive at first, but manufacturing was rationalized upon the 80386's mainstream adoption. The first personal computer to make use of the 80386 was designed and manufactured by Compaq and marked the first time a fundamental component in the IBM PC compatible "de facto"-standard was updated by a company other than IBM.
In May 2006, Intel announced that 80386 production would stop at the end of September 2007. Although it had long been obsolete as a personal computer CPU, Intel and others had continued making the chip for embedded systems. Such systems using an 80386 or one of many derivatives are common in aerospace technology and electronic musical instruments, among others. Some mobile phones also used (later fully static CMOS variants of) the 80386 processor, such as BlackBerry 950 and Nokia 9000 Communicator.
Architecture.
The processor was a significant evolution in the x86 architecture, and extended a long line of processors that stretched back to the Intel 8008. The predecessor of the 80386 was the Intel 80286, a 16-bit processor with a segment-based memory management and protection system. The 80386 added a 32-bit architecture and a paging translation unit, which made it much easier to implement operating systems that used virtual memory. It also offered support for register debugging.
The 80386 featured three operating modes: real mode, protected mode and virtual mode. The protected mode which debuted in the 286 was extended to allow the 386 to address up to 4 GB of memory. The all new virtual 8086 mode (or "VM86") made it possible to run one or more real mode programs in a protected environment, although some programs were not compatible.
The ability for a 386 to be set up to act like it had a flat memory model in protected mode despite the fact that it uses a segmented memory model in all modes would arguably be the most important feature change for the x86 processor family until AMD released x86-64 in 2003.
Chief architect in the development of the 80386 was John H. Crawford. He was responsible for extending the 80286 architecture and instruction set to 32-bit, and then led the microprogram development for the 80386 chip.
The 80486 and P5 Pentium line of processors were descendants of the 80386 design.
Datatypes of 80386.
The following data types are directly supported and thus implemented by one or more 80386 machine instructions; these data types are described here in brief. (source:, page 514):
Example code.
The following 80386 assembly source code is for a subroutine named codice_1 that copies a null-terminated ASCIIZ character string from one location to another, converting all alphabetic characters to lower case. The string is copied one byte (8-bit character) at a time.
The example code uses the EBP (base pointer) register to establish a call frame, an area on the stack that contains all of the parameters and local variables for the execution of the subroutine. This kind of calling convention supports reentrant and recursive code, and has been used by Algol-like languages since the late 1950s. A flat memory model is assumed, specifically, that the DS and ES segments address the same region of memory.
Chip variants.
The 80386SX variant.
In 1988, Intel introduced the 80386SX, most often referred to as the 386SX, a cut-down version of the 80386 with a 16-bit data bus mainly intended for lower cost PCs aimed at the home, educational, and small business markets while the 386DX would remain the high end variant used in workstations, servers, and other demanding tasks. The CPU remained fully 32-bit internally, but the 16-bit bus was intended to simplify circuit board layout and reduce total cost. The 16-bit bus simplified designs but hampered performance. Only 24 pins were connected to the address bus, therefore limiting addressing to but this was not a critical constraint at the time. Performance differences were due not only to differing data bus-widths, but also due to performance-enhancing cache memories often employed on boards using the original chip.
The original 80386 was subsequently renamed 80386DX to avoid confusion. However, Intel subsequently used the 'DX' suffix to refer to the floating-point capability of the 80486DX. The 80387SX was an 80387 part that was compatible with the 386SX (i.e. with a 16-bit databus). The 386SX was packaged in a surface-mount QFP, and sometimes offered in a socket to allow for an upgrade.
The i386SL variant.
The i386SL was introduced as a power efficient version for laptop computers. The processor offered several power management options (e.g. SMM), as well as different "sleep" modes to conserve battery power. It also contained support for an external cache of 16 to 64 kB. The extra functions and circuit implementation techniques caused this variant to have over 3 times as many transistors as the i386DX. The i386SL was first available at 20 MHz clock speed, with the 25 MHz model later added.
Business importance.
The first company to design and manufacture a PC based on the Intel 80386 was Compaq. By extending the 16/24-bit IBM PC/AT standard into a natively 32-bit computing environment, Compaq became the first third party to implement a major technical hardware advance on the PC platform. IBM was offered use of the 80386, but had manufacturing rights for the earlier 80286. IBM therefore chose to rely on that processor for a couple more years. The early success of the Compaq 386 PC played an important role in legitimizing the PC "clone" industry, and in de-emphasizing IBM's role within it.
Prior to the 386, the difficulty of manufacturing microchips and the uncertainty of reliable supply made it desirable that any mass-market semiconductor be multi-sourced, that is, made by two or more manufacturers, the second and subsequent companies manufacturing under license from the originating company. The 386 was for "a time" (4.7 yrs) only available from Intel, since Andy Grove, Intel's CEO at the time, made the decision not to encourage other manufacturers to produce the processor as second sources. This decision was ultimately crucial to Intel's success in the market. The 386 was the first significant microprocessor to be single-sourced. Single-sourcing the 386 allowed Intel greater control over its development and substantially greater profits in later years.
AMD introduced its compatible Am386 processor in March 1991 after overcoming legal obstacles, thus ending Intel's 4.7 yr monopoly on 386-compatible processors. From 1991 IBM also manufactured 386 chips under license for use only in IBM PCs and boards.
Early problems.
Intel originally intended for the 80386 to debut at 16 MHz. However, due to poor yields, it was instead introduced at 12 MHz.
Early in production, Intel discovered a marginal circuit that could cause a system to return incorrect results from 32-bit multiply operations. Not all of the processors already manufactured were affected, so Intel tested its inventory. Processors that were found to be bug-free were marked with a double-sigma (ΣΣ), and affected processors were marked "16 BIT S/W ONLY". These latter processors were sold as good parts, since at the time 32 bit capability was not relevant for most users. Such chips are now extremely rare.
The i387 math coprocessor was not ready in time for the introduction of the 80386, and so many of the early 80386 motherboards instead provided a socket and hardware logic to make use of an 80287. In this configuration the FPU would operate asynchronously to the CPU, usually with a clock rate of 10 MHz. The original Compaq Deskpro 386 is an example of such design. However, this was an annoyance to those who depended on floating point performance, as the performance advantages of the 80387 over the 80287 were significant.
Pin-compatible upgrades.
Intel later offered a modified version of its 80486DX in 80386 packaging, branded as the Intel RapidCAD. This provided an upgrade path for users with 80386-compatible hardware. The upgrade was a pair of chips that replaced both the 80386 and 80387. Since the 80486DX design contained an FPU, the chip that replaced the 80386 contained the floating point functionality, and the chip that replaced the 80387 served very little purpose. However, the latter chip was necessary in order to provide the FERR signal to the mainboard and appear to function as a normal floating point unit. The CAD branding referred to the ease of upgrading existing OEM designs from 386 to 486 CPUs with rapid turn-around in the CAD room.
Third parties offered a wide range of upgrades, for both SX and DX systems. The most popular ones were based on the Cyrix 486DLC/SLC core, which typically offered a substantial speed improvement due to its more efficient instruction pipeline and internal L1 SRAM cache. The cache was usually 1 kB, or sometimes 8 kB in the TI variant. Some of these upgrade chips (such as the 486DRx2/SRx2) were marketed by Cyrix themselves, but they were more commonly found in kits offered by upgrade specialists such as Kingston, Evergreen and Improve-It Technologies. Some of the fastest CPU upgrade modules featured the IBM SLC/DLC family (notable for its 16 kB L1 cache), or even the Intel 486 itself. Many 386 upgrade kits were advertised as being simple drop-in replacements, but often required complicated software to control the cache or clock doubling. Part of the problem was that on most 386 motherboards, the A20 line was controlled entirely by the motherboard with the CPU being unaware, which caused problems on CPUs with internal caches.
Overall it was very difficult to configure upgrades to produce the results advertised on the packaging, and upgrades were often less than 100% stable or less than 100% compatible.
Models and variants.
Early 5V models.
80386DX.
Original version, released in October 1985.
RapidCAD.
A specially packaged Intel 486DX and a dummy floating point unit (FPU) designed as pin-compatible replacements for an Intel 80386 processor and 80387 FPU.
Versions for embedded systems.
80376.
This was an embedded version of the 80386SX which did not support real mode and paging in the MMU.
i386EX, i386EXTB and i386EXTC.
System and power management and built in peripheral and support functions: Two 82C59A interrupt controllers; Timer, Counter (3 channels); Asynchronous SIO (2 channels); Synchronous SIO (1 channel); Watchdog timer (Hardware/Software); PIO. Usable with 80387SX or i387SL FPUs.
i386CXSA and i386SXSA (or i386SXTA).
Transparent power management mode, integrated MMU and TTL compatible inputs (only 386SXSA). Usable with i387SX or i387SL FPUs.
i386CXSB.
Transparent power management mode and integrated MMU. Usable with i387SX or i387SL FPUs.

</doc>
<doc id="15072" url="https://en.wikipedia.org/wiki?curid=15072" title="Instruction register">
Instruction register

In computing, an instruction register (IR) is the part of a CPU's control unit that stores the instruction currently being executed or decoded. In simple processors each instruction to be executed is loaded into the instruction register which holds it while it is decoded, prepared and ultimately executed, which can take several steps. 
Some of the complicated processors use a pipeline of instruction registers where each stage of the pipeline does part of the decoding, preparation or execution and then passes it to the next stage for its step. Modern processors can even do some of the steps out of order as decoding on several instructions is done in parallel.
Decoding the op-code in the instruction register includes determining the instruction, determining where its operands are in memory, retrieving the operands from memory, allocating processor resources to execute the command (in super scalar processors), etc.
The output of IR is available to control circuits which generate the timing signals that control the various processing elements involved in executing the instruction.
In the Instruction cycle, the instruction is loaded into the Instruction register after the processor fetches it from the memory location pointed by the Program counter.

</doc>
<doc id="15073" url="https://en.wikipedia.org/wiki?curid=15073" title="List of islands">
List of islands

This is a list of islands in the world grouped by oceans, by continents, and by other classifications. For rank-order lists, see the other lists of islands below.

</doc>
<doc id="15075" url="https://en.wikipedia.org/wiki?curid=15075" title="INTERCAL">
INTERCAL

The Compiler Language With No Pronounceable Acronym, abbreviated INTERCAL, is an esoteric programming language that was created as a parody by Don Woods and James M. Lyon, two Princeton University students, in 1972. It satirizes aspects of the various programming languages at the time, as well as the proliferation of proposed language constructs and notations in the 1960s.
There are two currently maintained versions of INTERCAL: C-INTERCAL, maintained by Eric S. Raymond, and CLC-INTERCAL, maintained by Claudio Calvelli.
History.
According to the original manual by the authors,
The original Princeton implementation used punched cards and the EBCDIC character set. To allow INTERCAL to run on computers using ASCII, substitutions for two characters had to be made: $ substituted for ¢ as the "mingle" operator, "represent the increasing cost of software in relation to hardware", and ? was substituted for ⊻ as the unary exclusive-or operator to "correctly express the average person's reaction on first encountering exclusive-or". In recent versions of C-INTERCAL, the older operators are supported as alternatives; INTERCAL programs may now be encoded in ASCII, Latin-1, or UTF-8.
Details.
INTERCAL was intended to be completely different from all other computer languages. Common operations in other languages have cryptic and redundant syntax in INTERCAL. From the INTERCAL Reference Manual:
INTERCAL has many other features designed to make it even more aesthetically unpleasing to the programmer: it uses statements such as "READ OUT", "IGNORE", "FORGET", and modifiers such as "PLEASE". This last keyword provides two reasons for the program's rejection by the compiler: if "PLEASE" does not appear often enough, the program is considered insufficiently polite, and the error message says this; if too often, the program could be rejected as excessively polite. Although this feature existed in the original INTERCAL compiler, it was undocumented.
Despite the language's intentionally obtuse and wordy syntax, INTERCAL is nevertheless Turing-complete: given enough memory, INTERCAL can solve any problem that a Universal Turing machine can solve. Most implementations of INTERCAL do this very slowly, however. A Sieve of Eratosthenes benchmark, computing all prime numbers less than 65536, was tested on a Sun SPARCstation 1. In C, it took less than half a second; the same program in INTERCAL took over seventeen hours.
Documentation.
The INTERCAL Reference Manual contains many paradoxical, nonsensical, or otherwise humorous instructions:
The manual also contains a "tonsil", as explained in this footnote: "4) Since all other reference manuals have Appendices, it was decided that the INTERCAL manual should contain some other type of removable organ."
The INTERCAL manual gives unusual names to all non-alphanumeric ASCII characters: single and double quotes are "sparks" and "rabbit ears" respectively. (The exception is the ampersand: as the Jargon File states, "what could be sillier?") The assignment operator, represented as an equals sign (INTERCAL's "half mesh") in many other programming languages, is in INTERCAL a left-arrow, codice_1, referred to as "gets" and made up of an "angle" and a "worm".
Syntax.
Input (using the codice_2 instruction) and output (using the codice_3 instruction) do not use the usual formats; in INTERCAL-72, WRITE IN inputs a number written out as digits in English (such as SIX FIVE FIVE THREE FIVE), and READ OUT outputs it in "butchered" Roman numerals. More recent versions have their own I/O systems.
Comments can be achieved by using the inverted statement identifiers involving NOT or N'T; these cause lines to be initially ABSTAINed so that they have no effect. (A line can be ABSTAINed from even if it doesn't have valid syntax; syntax errors happen at runtime, and only then when the line is un-ABSTAINed.)
Data structures.
INTERCAL-72 (the original version of INTERCAL) had only four data types: the 16-bit integer (represented with a codice_4, called a "spot"), the 32-bit integer (codice_5, a "twospot"), the array of 16-bit integers (codice_6, a "tail"), and the array of 32-bit integers (codice_7, a "hybrid"). There are 65535 available variables of each type, numbered from codice_8 to codice_9 for 16-bit integers, for instance. However, each of these variables has its own stack on which it can be pushed and popped (STASHed and RETRIEVEd, in INTERCAL terminology), increasing the possible complexity of data structures. (More modern versions of INTERCAL have by and large kept the same data structures, with appropriate modifications; TriINTERCAL, which modifies the radix with which numbers are represented, can use a 10-trit type rather than a 16-bit type), and CLC-INTERCAL implements many of its own data structures, such as "classes and lectures", by making the basic data types store more information rather than adding new types. Arrays are dimensioned by assigning to them as if they were a scalar variable. Constants can also be used, and are represented by a codice_10 ("mesh") followed by the constant itself, written as a decimal number; only integer constants from 0 to 65535 are supported.
Operators.
There are only five operators in INTERCAL-72. Implementations vary in which characters represent which operation, and many accept more than one character, so more than one possibility is given for many of the operators.
Contrary to most other languages, AND, OR, and XOR are unary operators, which work on consecutive bits of their argument; the most significant bit of the result is the operator applied to the most significant and least significant bits of the input, the second-most-significant bit of the result is the operator applied to the most and second-most significant bits, the third-most-significant bit of the result is the operator applied to the second-most and third-most bits, and so on. The operator is placed between the punctuation mark specifying a variable name or constant and the number that specifies which variable it is, or just inside grouping marks (i.e. one character later than it would be in programming languages like C.) SELECT and INTERLEAVE (which is also known as MINGLE) are infix binary operators; SELECT takes the bits of its first operand that correspond to "1" bits of its second operand and removes the bits that correspond to "0" bits, shifting towards the least significant bit and padding with zeroes (so 51 (110011 in binary) SELECT 21 (10101 in binary) is 5 (101 in binary)); MINGLE alternates bits from its first and second operands (in such a way that the least significant bit of its second operand is the least significant bit of the result). There is no operator precedence; grouping marks must be used to disambiguate the precedence where it would otherwise be ambiguous (the grouping marks available are codice_11 ("spark"), which matches another spark, and codice_12 ("rabbit ears"), which matches another rabbit ears; the programmer is responsible for using these in such a way that they make the expression unambiguous).
Control structures.
INTERCAL statements all start with a "statement identifier"; in INTERCAL-72, this can be codice_13, codice_14, or codice_15, all of which mean the same to the program (but using one of these too heavily causes the program to be rejected, an undocumented feature in INTERCAL-72 that was mentioned in the C-INTERCAL manual), or an inverted form (with codice_16 or codice_17 appended to the identifier). Backtracking INTERCAL, a modern variant, also allows variants using codice_18 (possibly combined with PLEASE or DO) as a statement identifier, which introduces a choice-point. Before the identifier, an optional line number (an integer enclosed in parentheses) can be given; after the identifier, a percent chance of the line executing can be given in the format codice_19, which defaults to 100%.
In INTERCAL-72, the main control structures are NEXT, RESUME, and FORGET. codice_20 branches to the line specified, remembering the next line that would be executed if it weren't for the NEXT on a call stack (other identifiers than DO can be used on any statement, DO is given as an example); codice_21 removes "expression" entries from the top of the call stack (this is useful to avoid the error that otherwise happens when there are more than 80 entries), and codice_22 removes "expression" entries from the call stack and jumps to the last line remembered.
C-INTERCAL also provides the COME FROM instruction, written codice_23; CLC-INTERCAL and the most recent C-INTERCAL versions also provide computed COME FROM (codice_24 and NEXT FROM, which is like COME FROM but also saves a return address on the NEXT STACK.
Alternative ways to affect program flow, originally available in INTERCAL-72, are to use the IGNORE and REMEMBER instructions on variables (which cause writes to the variable to be silently ignored and to take effect again, so that instructions can be disabled by causing them to have no effect), and the ABSTAIN and REINSTATE instructions on lines or on types of statement, causing the lines to have no effect or to have an effect again respectively.
Hello, world.
The traditional "Hello, world!" program demonstrates how different INTERCAL is from standard programming languages. In C, it could read as follows:
The equivalent program in C-INTERCAL is longer and harder to read:
Dialects.
The original Woods–Lyon INTERCAL was very limited in its input/output capabilities: the only acceptable input were numbers with the digits spelled out, and the only output was an extended version of Roman numerals.
The C-INTERCAL reimplementation, being available on the Internet, has made the language more popular with devotees of esoteric programming languages. The C-INTERCAL dialect has a few differences from original INTERCAL and introduced a few new features, such as a COME FROM statement and a means of doing text I/O based on the Turing Text Model.
The authors of C-INTERCAL also created the TriINTERCAL variant, based on the Ternary numeral system and generalizing INTERCAL's set of operators.
A more recent variant is Threaded Intercal, which extends the functionality of COME FROM to support multithreading.
Impact and discussion.
In the article "A Box, Darkly: Obfuscation, Weird Languages, and Code Aesthetics", INTERCAL is described under the heading "Abandon all sanity, ye who enter here: INTERCAL". The compiler and commenting strategy are among the "weird" features described:
In "Technomasochism", Lev Bratishenko characterizes the INTERCAL compiler as a dominatrix:

</doc>
