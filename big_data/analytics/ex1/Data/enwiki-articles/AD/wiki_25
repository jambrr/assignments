<doc id="19989" url="https://en.wikipedia.org/wiki?curid=19989" title="Murad III">
Murad III

Murad III (Ottoman Turkish: مراد ثالث "Murād-i sālis", Turkish:"III.Murat") (4 July 1546 – 15/16 January 1595) was the Sultan of the Ottoman Empire from 1574 until his death in 1595. Murad's reign is most remembered as a period of economic stagnation and failures in seemingly endless warfare on two fronts.
Biography.
Born in Bozdağan or Manisa, Şehzade Murad was the son of Sultan Selim II and Afife Nurbanu Sultan. After his ceremonial circumcision in 1557, Murad was appointed "sancakbeyi" of Akşehir by Suleyman I (his grandfather) in 1558. At the age of 18 he was appointed "sancakbeyi" of Saruhan. Suleiman died when Murad was 20, and his father became the new Sultan. Selim II broke with tradition by sending only his oldest son out of the palace to govern a province, and Murad was sent to Manisa.
Selim died in 1574 and was succeeded by Murad, who began his reign by having his five younger brothers strangled. His authority was undermined by the harem influences, more specifically, those of his mother and later of his favorite wife Safiye Sultan. The power had only been maintained under Selim II by the genius of the all-powerful Grand Vizier Mehmed Sokollu who remained in office until his assassination in October 1579. During Murad's reign the northern borders with the Habsburg Monarchy were defended by the Bosnian kapetan Hasan Predojević. The reign of Murad III was marked by exhausting wars on the empire's western and eastern fronts and Ottoman economic decline and institutional decay. The Ottomans also faced defeats during battles such as the Battle of Sisak.
The Ottomans had been at peace with the neighbouring rivalling Safavid Empire since 1555, per the Treaty of Amasya, that for some time had settled border disputed. But in 1577 Murad declared war, starting the Ottoman–Safavid War (1578–90), seeking to take advantage of the chaos in the Safavid court after the death of Shah Tahmasp I. He was influenced by viziers Lala Kara Mustafa Pasha and Sinan Pasha and disregarded the opposing counsel of Grand Vizier Mehmed Sokollu. The war would drag on for 12 years, ending with the Istanbul Treaty of 1590, which resulted in temporaray significant territorial gains for the Ottomans.
Murad's reign was a time of financial stress for the Ottoman state. To keep up with advances in European technology, the Ottomans trained infantrymen in the use of firearms, paying them directly from the treasury. By 1580 an influx of silver from the New World had caused high inflation and social unrest, especially among Janissaries and government officials who were paid in debased currency. Deprivation from the resulting rebellions, coupled with the pressure of over-population, was especially felt in Anatolia. Competition for positions within the government grew fierce, leading to bribery and corruption. Ottoman and Habsburg sources accuse Murad himself of accepting enormous bribes, including 20,000 ducats from a statesman in exchange for the governorship of Tripoli and Tunisia, thus outbidding a rival who had tried bribing the Grand Vizier.
From Murad descend all succeeding Sultans,
through his marriage to Safiye Sultan, mother of Mehmed III.
Numerous envoys and letters were exchanged between Elizabeth I and Sultan Murad III. In one correspondence, Murad entertained the notion that Islam and Protestantism had "much more in common than either did with Roman Catholicism, as both rejected the worship of idols", and argued for an alliance between England and the Ottoman Empire. To the dismay of Catholic Europe, England exported tin and lead (for cannon-casting) and ammunitions to the Ottoman Empire, and Elizabeth seriously discussed joint military operations with Murad III during the outbreak of war with Spain in 1585, as Francis Walsingham was lobbying for a direct Ottoman military involvement against the common Spanish enemy. This diplomacy would be continued under Murad's successor Mehmed III, by both the sultan and Safiye Sultan alike.
Murad died from what is assumed to be natural causes in the Topkapı Palace and was buried in tomb next to the Hagia Sofia. 54 of his wives and children are also buried there. He is also responsible for changing the burial customs of the Sultans' mothers. Murad had his mother Nurbanu buried next to her husband Selim II, making her the first concubine to share a Sultan's tomb.
Palace life.
Following the example of his father Selim II, Murad was the second Ottoman Sultan to never go on campaign during his reign, which spent entirely in Istanbul. During the final years of his reign, he did not even leave Topkapı Palace. For two consecutive years he did not attend the Friday procession to the imperial mosque—an unprecedented breaking of custom. The Ottoman historian Mustafa Selaniki wrote that whenever Murad planned to go out to Friday prayer, he changed his mind after hearing of alleged plots by the Janissaries to dethrone him once he left the palace. Murad withdrew from his subjects and spent the majority of his reign keeping to the company of few people and abiding by a daily routine structured by the five daily Islamic prayers. Murad's personal physician Domenico Hierosolimitano described a typical day in the life of the Sultan:
Murad's sedentary lifestyle and lack of participation in military campaigns earned him the disapproval of Mustafa Ali and Mustafa Selaniki, the major Ottoman historians who lived during his reign. Their negative portrayals of Murad influenced later historians. 
Both historians also accused Murad of sexual excess. Before becoming Sultan, Murad had been loyal to Safiye Sultan, his Albanian-born concubine who had given him a son, Mehmed, and two daughters. His monogamy was disapproved of by his mother Nurbanu, who worried that Murad needed more sons to succeed him in case Mehmed died young. She also worried about Safiye's influence over her son and the Ottoman dynasty. 
Five or six years after his accession to the throne, Murad was given a pair of concubines by his sister Ismihan. Upon attempting sexual intercourse with them, he proved impotent. "The arrow Murad, keeping with his created nature, for many times [and for many days has been unable to reach at the target of union and pleasure," wrote Mustafa Ali. Nurbanu accused Safiyye and her retainers of causing Murad's impotence with witchcraft. Several of Safiye's servants were tortured by eunuchs in order to discover a culprit. Court physicians, working under Nurbanu's orders, eventually prepared a successful cure, but a side effect was a drastic increase in sexual appetite—by the time Murad died, he was said to have fathered over a hundred children. 19 of these were executed by Mehmed III when he became Sultan.
Murad and the arts.
Murad took great interest in the arts, particularly miniatures and books. He actively supported the court Society of Miniaturists, commissioning several volumes including the "Siyer-i Nebi", the most heavily illustrated biographical work on the life of the Islamic prophet Muhammad, the "Book of Skills", the "Book of Festivities" and the "Book of Victories". He had two large alabaster urns transported from Pergamon and placed on two sides of the nave in the Hagia Sophia in Constantinople and a large wax candle dressed in tin which was donated by him to the Rila monastery in Bulgaria is on display in the monastery museum.
Murad also furnished the content of "Kitabü’l-Menamat" ("The Book of Dreams"), addressed to Murad's spiritual advisory, Şüca Dede. A collection of first person accounts, it tells of Murad's spiritual experiences as a Sufi disciple. Compiled from thousands of letters Murad wrote describing his dream visions, it presents a hagiographical self-portrait. Murad dreams of various activities, including being stripped naked by his father and having to sit on his lap, single-handedly killing 12,000 infidels in battle, walking on water, ascending to heaven, and producing milk from his fingers. He frequently encounters the Prophet Muhammed, and in one dream sits in the Prophet's lap and kisses his mouth.
In another letter addressed to Şüca Dede, Murad wrote "I wish that the True Reality/God, "Celle ve Ala", had not created this poor servant as the descendant of the Ottomans so that I would not hear this and that, and would not worry. I wish I were of unknown pedigree. Then, I would have one single task, and could ignore the whole world."
The diplomatic edition of these dream letters have been recently published by Ozgen Felek in Turkish.
Consorts and children.
Murad had more than forty consorts, including:
He also had other known concubines :
Daughters.
During Ahmed I's reign, four daughters of Murad III (his aunts), were married in 1613 (Mihrimah, Fahriye, Mihriban and Rukiye)
In fiction.
Orhan Pamuk's historical novel "Benim Adım Kırmızı" ("My Name is Red", 1998) takes place at the court of Murad III, during nine snowy winter days of 1591, which the writer uses in order to convey the tension between East and West.
The Harem Midwife by Roberta Rich - a historical fiction set in Constantinople (1578) which follows Hannah, a midwife, who tends to many of the women in Sultan Murad III's harem.
External links.
48

</doc>
<doc id="19990" url="https://en.wikipedia.org/wiki?curid=19990" title="Mehmed III">
Mehmed III

Mehmed III Adli (Ottoman Turkish: محمد ثالث "Meḥmed-i sālis", ; May 26, 1566 – December 21/22, 1603) was Sultan of the Ottoman Empire from 1595 until his death in 1603. His reign is mostly remembered from his murders of his siblings and causing a general period of revolts and unease.
Early life.
He was born during the reign of his great-grandfather, Suleiman the Magnificent, in 1566. He was the son of Şehzade Murad (later Murad III), himself the son of Şehzade Selim (later Selim II), who was the son of Sultan Suleiman and Hürrem Sultan. His great-grandfather died the year he was born and his grandfather became the new Sultan, Selim II. His grandfather Sultan Selim II died when Mehmed was 8 and Mehmed's father, Murad III, became Sultan in 1574. Mehmed thus became Crown Prince until his father's death in 1595, when he was 28 years old.
Reign.
Mehmed III was more conservative than his predecessor and largely halted artistic patronage, including support of the Society of Miniaturists. His reign saw no major setbacks for the supposedly declining Ottoman Empire. He died at Topkapı Palace, Constantinople.
Power struggle in Constantinople.
Mehmed III remains notorious even in Ottoman history for having nineteen of his brothers and half-brothers executed to secure power. They were all strangled by his deaf-mutes.
Mehmed III was an idle ruler, leaving government to his mother Safiye Sultan, the valide sultan. His first major problem was the rivalry between two of his viziers, Serdar Ferhad Pasha and Koca Sinan Pasha, and their supporters. His mother and her son-in-law Damat Ibrahim Pasha supported Koca Sinan Pasha, and prevented Mehmed III from taking control of the issue himself. The issue grew to cause major disturbances by janissaries. On 7 July 1595, Mehmed III finally sacked Serdar Ferhad Pasha from the position of Grand Vizier due to his failure in Wallachia and replaced him with Sinan.
Austro-Hungarian War.
The major event of his reign was the Austro-Ottoman War in Hungary (1593–1606). Ottoman defeats in the war caused Mehmed III to take personal command of the army, the first sultan to do so since Suleiman I in 1566. Accompanied by the Sultan, the Ottomans conquered Eger in 1596. Upon hearing of the Habsburg army's approach, Mehmed wanted to dismiss the army and return to Istanbul. However, the Ottomans eventually decided to face the enemy and defeated the Habsburg and Transylvanian forces at the Battle of Keresztes (known in Turkish as the Battle of Haçova), during which the Sultan had to be dissuaded from fleeing the field halfway through the battle. Upon returning to Istanbul in victory, Mehmed told his Vezirs that he would campaign again. The next year the Venetian Bailo in Istanbul noted, "the doctors declared that the Sultan cannot leave for war on account of his bad health, produced by excesses of eating and drinking".
In reward for his services at the war, Cigalazade Yusuf Sinan Pasha was made Grand Vizier in 1596. However, with pressure from the court and his mother, Mehmed reinstated Damat Ibrahim Pasha to this position shortly afterwards.
However, the victory at the Battle of Keresztes was soon set back by some important losses, including the loss of Győr () to the Austrians and the defeat of the Ottoman forces led by Hafız Ahmet Pasha by the Wallachian forces under Michael the Brave in Nikopol in 1599. In 1600, Ottoman forces under Tiryaki Hasan Pasha captured Nagykanizsa after a 40-day siege and later successfully held it against a much greater attacking force in the Siege of Nagykanizsa.
Jelali revolts.
Another major event of his reign was the Jelali revolts in Anatolia. Karayazıcı Abdülhalim, a former Ottoman official, captured the city of Urfa and declared himself sultan in 1600. The rumors of his claim to the throne spread to Constantinople and Mehmed ordered the rebels to be treated harshly to dispel the rumors, among these was the execution of Hüseyin Pasha, whom Karayazıcı Abdülhalim styled as Grand Vizier. In 1601, Abdülhalim fled to the vicinity of Samsun after being defeated by the forces under Sokulluzade Hasan Pasha, the governor of Baghdad. However, his brother, Deli Hasan, killed Sokulluzade Hasan Pasha and defeated troops under the command of Hadım Hüsrev Pasha. He then marched on to Kütahya, captured and burned the city.
Relationship with England.
In 1599, the fourth year of Mehmed III's reign, Queen Elizabeth I sent a convoy of gifts to the Ottoman court. These gifts were originally intended for the sultan's predecessor, Murad III, who had died before they had arrived. Included in these gifts was a large jewel-studded clockwork organ that was assembled on the slope of the Royal Private Garden by a team of engineers including Thomas Dallam. The organ took many weeks to complete and featured dancing sculptures such as a flock of blackbirds that sung and shook their wings at the end of the music. The musical clock organ was destroyed by the succeeding Sultan Ahmed I. Also among the English gifts was a ceremonial coach, accompanied by a letter from the Queen to Mehmed's mother, Safiye Sultan. These gifts were intended to cement relations between the two countries, building on the trade agreement signed in 1581 that gave English merchants priority in the Ottoman region. Under the looming threat of Spanish military presence, England was eager to secure an alliance with the Ottomans, the two nations together having the capability to divide the power. Elizabeth's gifts arrived in a large 27-gun merchantman ship that Mehmed personally inspected, a clear display of English maritime strength that would prompt him to build up his fleet over the following years of his reign. The Anglo-Ottoman alliance would never see consummation, however, with relations between the nations growing stagnant due to anti-European sentiments reaped from the worsening Austro-Ottoman War and the deaths of Safiye Sultan's interpreter and the pro-English chief Hasan Pasha.
Personal life.
He was born at Manisa Palace, the son of Sultan Murad III, whom he succeeded in 1595. His mother was Safiye Sultan, probably an Albanian from the Principality of Dukagjini.
Mehmed had two known consorts:
None of Mehmed's consorts bore the title "haseki sultan".
He died in 1603 at the age of 37. The cause is unknown, but natural causes or disease are most likely.
External links.
37

</doc>
<doc id="19991" url="https://en.wikipedia.org/wiki?curid=19991" title="Mustafa I">
Mustafa I

Mustafa I (1591 – January 20, 1639) (), often called Mustafa the Mad, was the son of Mehmed III and was the Sultan of the Ottoman Empire from 1617 to 1618 and from 1622 to 1623. His two reigns are generally ill reputed points in history for the Empire.
Biography.
He was born in the Manisa Palace, as the younger brother of Ahmed I (1603–17). His mother was an Abkhazian concubine whose name is lost.
Before 1603 it was customary for an Ottoman Sultan to have his brothers executed shortly after he gained the throne (Mustafa's father Mehmed III had executed 19 of his own brothers). But when the thirteen-year-old Ahmed I was enthroned in 1603, he spared the life of the twelve-year-old Mustafa.
Mustafa might have been left alive because Ahmed had not yet produced any sons, so at the time Mustafa was his only heir. Though Ahmed went on to father several sons, he did not execute Mustafa, perhaps because of his brother's apparent mental problems. Another factor in Mustafa's survival is the influence of Kösem Sultan (Ahmed's favorite concubine), who may have wished to preempt the succession of Osman, Ahmed’s first-born son from another concubine. If Osman became Sultan, he would likely try to execute his half-brothers, the sons of Ahmed and Kösem. This scenario later became a reality when Osman II executed his brother Mehmed in 1621.
Until the death of Ahmed in 1617, Mustafa was confined to the palace, in the virtual imprisonment of a system called the Kafes ("the cage"). Fourteen years in this condition may have further damaged his mental health and made him fearful of execution.
First reign (1617-1618).
Ahmed's death created a dilemma never before experienced by the Ottoman Empire. Multiple princes were now eligible for the Sultanate, and all of them lived in Topkapı Palace. A court faction headed by the Şeyhülislam Esad Efendi and Sofu Mehmed Pasha (who represented the Grand Vizier when he was away from Istanbul) decided to enthrone Mustafa instead of Ahmed's son Osman. Sofu Mehmed argued that Osman was too young to be enthroned without causing adverse comment among the populace. The Chief Black Eunuch Mustafa Agha objected, citing Mustafa's mental problems, but he was overruled. Mustafa's rise created a new succession principle of seniority that would last until the end of the Empire. It was the first time an Ottoman Sultan was succeeded by his brother instead of his son.
It was hoped that regular social contact would improve Mustafa's mental health, but his behavior remained eccentric. He pulled off the turbans of his viziers and yanked their beards. Others observed him throwing coins to birds and fish. The Ottoman historian İbrahim Peçevi wrote "this situation was seen by all men of state and the people, and they understood that he was psychologically disturbed."
Mustafa was never more than a tool of court cliques at the Topkapı Palace. In 1618, after a short rule, another palace faction deposed him in favour of his young nephew Osman II (1618–22), and Mustafa was sent back to the Kafes. The conflict between the Janissaries and Osman II presented him with a second chance. After a Janissary rebellion led to the deposition and assassination of Osman II in 1622, Mustafa was restored to the throne and held it for another year.
Second reign (1622-1623).
His mental condition unimproved, Mustafa was a puppet controlled by his mother and brother-in-law, the grand vizier Kara Davud Pasha. He believed that Osman II was still alive and was seen searching for him throughout the palace, knocking on doors and crying out to his nephew to relieve him from the burden of sovereignty. "The present emperor being a fool" (according to English Ambassador Sir Thomas Roe), he was compared unfavorably with his predecessor.
Political instability was generated by conflict between the Janissaries and the sipahis (Ottoman cavalry), followed by the Abaza rebellion, which occurred when the governor-general of Erzurum, Abaza Mehmed Pasha, decided to march to Istanbul to avenge the murder of Osman II. The regime tried to end the conflict by executing Kara Davud Pasha, but Abaza Mehmed continued his advance. Clerics and the new Grand Vizier (Kemankeş Kara Ali Pasha) prevailed upon Mustafa's mother to allow the deposition of her son. She agreed, on condition that Mustafa's life would be spared.
The 11-year-old Murad IV, son of Ahmed I and Kösem, was enthroned on September 10, 1623. Mustafa was sent back into palace confinement and died in 1639. He was buried in the courtyard of Haghia Sophia.
Legacy.
Along with Ibrahim I, Mustafa is generally considered one of the least effect rulers of the Ottoman Empire.
External links.
<BR>

</doc>
<doc id="19992" url="https://en.wikipedia.org/wiki?curid=19992" title="Murad IV">
Murad IV

Murad IV (; July 26/27, 1612 – February 8, 1640) was the Sultan of the Ottoman Empire from 1623 to 1640, known both for restoring the authority of the state and for the brutality of his methods. Murad IV was born in Constantinople, the son of Sultan Ahmed I (r. 1603–17) and the ethnic Greek Valide Kösem Sultan. Brought to power by a palace conspiracy in 1623, he succeeded his uncle Mustafa I (r. 1617–18, 1622–23). He was only 11 when he took the throne. His reign is most notable for the Ottoman–Safavid War (1623–39), of which the outcome would permanently part the Caucasus between the two Imperial powers for around two centuries, while it also roughly laid the foundation for the current Turkey - Iran - Iraq borders.
Biography.
In the early years of Murad's reign, he was under the control of his relatives. During this period, peace and harmony in the Ottoman lands were completely lost, and tyrants took control of the cities. His absolute rule started around 1632, when he took the authority and repressed all the tyrants, and he re-established the supremacy of Sultan.
Early reign (1623–32).
Murad IV was for a long time under the control of his relatives and during his early years as Sultan, his mother, Kösem Sultan, essentially ruled through him. The Empire fell into anarchy; the Safavid Empire invaded Iraq almost immediately, Northern Anatolia erupted in revolts, and in 1631 the Janissaries stormed the palace and killed the Grand Vizier, among others. Murad IV feared suffering the fate of his elder brother, Osman II (1618–22), and decided to assert his power.
At the age of 16 in 1628, he had his brother-in-law (his sister Fatma Sultan's husband) and the former governor of Egypt Kara Mustafa Pasha executed for a claimed action "against the law of God".
Absolute rule and imperial policies (1632–40).
Murad IV tried to quell the corruption that had grown during the reigns of previous Sultans, and that had not been checked while his mother was ruling through proxy.
Murad IV also banned alcohol, tobacco, and coffee in Constantinople. He ordered execution for breaking this ban. He would reportedly patrol the streets and the lowest taverns of Istanbul in civilian clothes at night, policing the enforcement of his command by casting off his disguise on the spot and beheading the offender with his own hands. Rivaling the exploits of Selim the Grim, he would sit in a kiosk by the water near his Seraglio Palace and shoot arrows at any boat man who rowed too close to his imperial compound. He restored the judicial regulations by very strict punishments, including execution, he once strangled a grand vizier for the reason that the official had beaten his mother-in-law. Historians including Halil İnalcık as well as primary sources report that even though he was a ruthless supporter of alcohol prohibition, Murad IV was a habitual drinker himself.
War against Safavid Iran.
Murad IV's reign is most notable for the Ottoman–Safavid War (1623–39) against Persia in which Ottoman forces managed to conquer Azerbaijan, occupying Tabriz, Hamadan, and capturing Baghdad in 1638. Murad IV himself commanded the invasion of Mesopotamia and proved to be an outstanding field commander. By the Treaty of Zuhab which followed after the war, it roughly comprised and confirmed the borders as per the Peace of Amasya, with Eastern Armenia, Eastern Georgia, Azerbaijan, and Dagestan staying Persian, while Western Armenia, and Western Georgia staying Ottoman. Mesopotamia was irrevocably lost for the Persians. the borders per the outcome of the war is more or less the present border line between Turkey - Iraq and Iran.
During the siege of Baghdad, the city withstood the siege for forty days, but was compelled to surrender, and the bulk of the population were butchered by the conquerors, in spite of the promises which they had made to spare them. It is said that the officers of Murad arranged a sort of tableau, in which the heads were struck off one thousand captives by one thousand headsmen at the same moment, and that Murad IV enjoyed the sight. The sultan had a famous quote about the fall of Baghdad: ("Trying to conquer Baghdad, was almost more beautiful than Baghdad itself.").
Murad IV himself commanded the Ottoman army in the last years of the war, and proved to be an outstanding field commander. He was the second Ottoman Sultan to command an army on the battlefield since the death of Suleiman the Magnificent in 1566. During his campaign to Persia, he annihilated all rebels in Anatolia and restored order in the Empire. 
After his return to Constantinople, he ordered respected statesmen of the Empire to prepare a new economic and political project to return the Empire to the old successful days.
Physical power.
Murad IV was the last Warrior Sultan who led campaigns in front of his army and fought on the battlefield. His physical strength was phenomenal, which is described in detail on the books of Evliya Çelebi. He was especially known for his exceptional strength in wrestling - capable of fighting several opponents at the same time. His favorite weapon was a huge mace, which, according to legend, he wielded effortlessly with a single hand despite it reportedly weighed 60 kilograms (132 lbs). Among his other favorite weapons are a longbow and a large two-handed broadsword apparently weighing more than 50 kilograms (110 lbs), despite the fact that even the largest known swords (used for combat) rarely weigh more than 10 lbs, and ceremonial parade swords weighing more than 15 lbs are exceedingly rare. His weapons are today displayed at the Topkapı Palace Museum in Fatih, intact and well preserved.
Architecture.
Sultan Murad IV put emphasis on architecture and in his period many monuments were erected. Some of them are Meydanı Mosque, Bayram Pasha Dervish Lodge, Tomb, Fountain, Primary School, Konya Serefeddin Mosque.
The Mughal Emperor Shah Jahan had exchanged ambassadors with the Ottoman Sultan Murad IV, it was through these exchanges that he received Isa Muhammad Effendi and Ismail Effendi, two Turkish architects and students of the famous Koca Mimar Sinan Agha. Both of them later comprised among the Mughal team that would design and build the Taj Mahal.
Relations with the Mughal Empire.
In the year 1626, the Mughal Emperor Jahangir began to contemplate an alliance between the Ottomans, Mughals and Uzbeks against the Safavids, who had defeated the Mughals at Kandahar. He even wrote a letter to the Ottoman Sultan Murad IV, Jahangir's ambition however did not materialize due to his death in 1627. However, Jahangir's son and successor Shah Jahan pursued the goal of alliance with the Ottoman Empire.
While he was encamped in Baghdad, Murad IV is known to have met the Mughal Emperor Shah Jahan's ambassadors: Mir Zarif and Mir Baraka, who presented 1000 pieces of finely embroidered cloth and even armor. Murad IV gave them the finest weapons, saddles and Kaftans and ordered his forces to accompany the Mughals to the port of Basra, where they set sail to Thatta and finally Surat.
Death.
Murad IV died from cirrhosis in İstanbul at the age of 27 in 1640.
Rumours had circulated that on his deathbed, Murad IV ordered the execution of his mentally disabled brother, Ibrahim I (reigned 1640–48), which would have meant the end of the Ottoman line. However, the order was not carried out.

</doc>
<doc id="19994" url="https://en.wikipedia.org/wiki?curid=19994" title="Masamune Shirow">
Masamune Shirow

Career.
Born in the Hyōgo Prefecture capital city of Kobe, he studied oil painting at Osaka University of Arts. While in college, he developed an interest in manga, which led him to create his own complete work, "Black Magic", which was published in the manga fanzine "Atlas". His work caught the eye of Seishinsha President Harumichi Aoki, who offered to publish him.
The result was best-selling manga "Appleseed", a full volume of densely plotted drama taking place in an ambiguous future. The story was a sensation, and won the 1986 Seiun Award for Best Manga. After a professional reprint of "Black Magic" and a second volume of "Appleseed", he released "Dominion" in 1986. Two more volumes of "Appleseed" followed before he began work on "Ghost in the Shell".
In 2007, he collaborated again with Production I.G to co-create the original concept for the anime television series "Shinreigari/Ghost Hound", Production I.G's 20th year anniversary project. A further original collaboration with Production I.G began airing in April, 2008, titled "Real Drive".
Bibliography.
Art books.
A substantial amount of Shirow's work has been released in art book or poster book format. The following is an incomplete list.
Galgrease.
"Galgrease" (published in "Uppers Magazine", 2002) is the collected name of several erotic manga and poster books by Shirow. The name comes from the fact that the women depicted often look "greased".
The first series of "Galgrease" booklets included four issues each in the following settings:
The second series included another run of 12 booklets in the following worlds:
After each regular series, there were one or more bonus poster books that revisited the existing characters and settings.
Minor works.
Main source:

</doc>
<doc id="19995" url="https://en.wikipedia.org/wiki?curid=19995" title="Musical saw">
Musical saw

A musical saw, also called a singing saw, is a hand saw used as a musical instrument. Capable of continuous glissando (portamento), the sound creates an ethereal tone, very similar to the theremin. The musical saw is classified as a friction idiophone with direct friction (131.22) under the Hornbostel-Sachs system of musical instrument classification.
Playing.
The saw is generally played seated with the handle squeezed between the legs, and the far end held with one hand. Some sawers play standing, either with the handle between the knees and the blade sticking out in front of them, or with the handle under the chin (like a violin). The saw is usually played with the serrated edge, or "teeth", facing the body, though some players face them away. Some saw players file down the teeth for added comfort. To sound a note, a sawer first bends the blade into an S-curve. The parts of the blade that are curved are damped from vibration, and do not sound. At the center of the S-curve a section of the blade remains relatively flat. This section, the "sweet spot", can vibrate across the width of the blade, producing a distinct pitch: the wider the section of blade, the lower the sound. Sound is usually created by drawing a bow across the back edge of the saw at the sweet spot, or sometimes by striking the sweet spot with a mallet. The sawist controls the pitch by adjusting the S-curve, making the sweet spot travel up the blade (toward a thinner width) for a higher pitch, or toward the handle for a lower pitch. Harmonics can be created by playing at varying distances on either side of the sweet spot. Sawers can add vibrato by shaking one of their legs or by wobbling the hand that holds the tip of the blade. Once a sound is produced, it will sustain for quite a while, and can be carried through several notes of a phrase. On occasion the Musical Saw is called for in orchestral music, but orchestral percussionists are seldom also sawists. If a note outside of the saw's range is called for, an electric guitar with a slide can be substituted.
Types.
Sawers often use standard wood-cutting saws, although special musical saws are also made. As compared with wood-cutting saws, the blades of musical saws are generally wider, for range, and longer, for finer control. They do not have set or sharpened teeth, and may have grain running parallel to the back edge of the saw, rather than parallel to the teeth. Some musical saws are made with thinner metal, to increase flexibility, while others are made thicker, for a richer tone, longer sustain, and stronger harmonics. A typical musical saw is 5 inches wide at the handle end and 1 inch wide at the tip. A saw will generally produce about two octaves, regardless of length. A bass saw may be 6 inches at the handle and produce about two-and-a-half octaves. Two-person saws, also called "misery whips", can also be played, though with less virtuosity, and they produce an octave or less of range.
Most sawers use cello or violin bows, using violin rosin, but some may use improvised home-made bows, such as a wooden dowel.
Producers.
Musical saws have been produced for over a century, primarily in the United States, though there are some producers in other countries.
United States.
In the early 1900s, there were at least ten companies in the United States manufacturing musical saws. These saws ranged from the familiar steel variety to gold-plated masterpieces worth hundreds of dollars. However, with the start of World War II the demand for metals made the manufacture of saws too expensive and many of these companies went out of business. By the year 2000, only three companies in the United States — Mussehl & Westphal, Charlie Blacklock, and Wentworth — were making saws. In 2012, a company called Index Drums started producing a saw that had a built-in transducer in the handle, called the "JackSaw".
Outside the United States.
Outside the United States, makers of musical saws include Bahco, makers of the limited edition Stradivarius, Alexis in France, which produces a toothless saw, "La Lame Sonore", with a range of three and a half octaves (Patent: № E31975), and Thomas Flinn & Company in the United Kingdom, based in Sheffield, who produce three different sized musical saws, as well as accessories.
Events and world records.
The International Musical Saw Association (IMSA) produces an annual International Musical Saw Festival (including a "Saw-Off" competition every August in Santa Cruz and Felton, California. An International Musical Saw Festival is held every other summer in New York City, produced by Natalia Paruz. Paruz also produced a musical saw festival in Israel. There are also annual saw festivals in Japan and China.
A Guinness World Record for the Largest Musical Saw Ensemble was established July 18, 2009, at the annual NYC Musical Saw Festival. Organized by Paruz, 53 musical saw players performed together.
Performers.
This is a list of people notable for playing the musical saw. 
Compositions.
Some artists have composed music specifically for the musical saw. The composer Krzysztof Penderecki wrote regularly for the musical saw, including several obbligato parts in his comic opera "Ubu Rex", and Canadian composer Robert Minden has written extensively for the musical saw. The Romanian composer George Enescu uses the musical saw at the end of the second act of his opera "Œdipe" to express the death of the sphinx killed by Oedipus. Michael A. Levine composed "Divination By Mirrors" for musical saw soloist and two string ensembles tuned a quarter tone apart, taking advantage of the saws ability to play in both tunings. The composer Chaya Czernowin included a musical saw soloist in her opera "Pnima...Ins Innere". In 1975, film composer Jack Nitzsche used the musical saw for dramatic effect in the score for "One Flew Over the Cuckoo's Nest". Perhaps the most prolific composer for the musical saw is Scott Munson, who wrote many contemporary pieces for musical saw, as well as music for theater plays, film and television using the instrument.

</doc>
<doc id="19996" url="https://en.wikipedia.org/wiki?curid=19996" title="MIDI">
MIDI

MIDI (; short for Musical Instrument Digital Interface) is a technical standard that describes a protocol, digital interface and connectors and allows a wide variety of electronic musical instruments, computers and other related devices to connect and communicate with one another. A single MIDI link can carry up to sixteen channels of information, each of which can be routed to a separate device.
MIDI carries event messages that specify notation, pitch and velocity, control signals for parameters such as volume, vibrato, audio panning, cues, and clock signals that set and synchronize tempo between multiple devices. These messages are sent via a MIDI cable to other devices where they control sound generation and other features. This data can also be recorded into a hardware or software device called a sequencer, which can be used to edit the data and to play it back at a later time. Advantages of MIDI include compactness (an entire song can be coded in a few hundred lines, i.e. in a few kilobytes), ease of modification and manipulation and choice of instruments.
MIDI technology was standardized in 1983 by a panel of music industry representatives, and is maintained by the MIDI Manufacturers Association (MMA). All official MIDI standards are jointly developed and published by the MMA in Los Angeles, California, US, and for Japan, the MIDI Committee of the Association of Musical Electronics Industry (AMEI) in Tokyo. In 2016, the MMA established The MIDI Association (TMA) to nurture an inclusive global community of people who work, play, or create with MIDI, establishing the www.MIDI.org website as the central repository of information about anything related to MIDI technology, from classic legacy gear to next-gen protocols on the horizon.
History.
Early development.
In June 1981, Roland founder Ikutaro Kakehashi proposed the idea of standardization to Oberheim Electronics founder Tom Oberheim, who then talked it over with Sequential Circuits president Dave Smith. In October 1981, Kakehashi, Oberheim and Smith discussed the idea with representatives from Yamaha, Korg and Kawai.
Sequential Circuits engineers and synthesizer designers Dave Smith and Chet Wood devised a universal synthesizer interface, which would allow direct communication between equipment from different manufacturers. Smith proposed this standard at the Audio Engineering Society show in November 1981. Over the next two years, the standard was discussed and modified by representatives of companies such as Roland, Yamaha, Korg, Kawai, Oberheim, and Sequential Circuits, and was renamed Musical Instrument Digital Interface. MIDI's development was announced to the public by Robert Moog, in the October 1982 edition of "Keyboard magazine".
By the time of the January 1983 Winter NAMM Show, Smith was able to demonstrate a MIDI connection between his Prophet 600 analog synthesizer and a Roland JP-6. The MIDI Specification was published in August 1983. The MIDI standard was unveiled by Ikutaro Kakehashi and Dave Smith, who both later received Technical Grammy Awards in 2013 for their key roles in the development of MIDI.
Impact on the music industry.
MIDI's appeal was originally limited to those who wanted to use electronic instruments in the production of popular music. In August 1983, New Order released their iconic "Blue Monday", which was composed on prototype-level homebrew "step-time" binary code programming, morphologically a MIDI rendition. The standard allowed different instruments to "speak" with each other and with computers, and this spurred a rapid expansion of the sales and production of electronic instruments and music software. This interoperability allowed one device to be controlled from another, which reduced the amount of hardware musicians needed to own. MIDI's introduction coincided with the dawn of the personal computer era and the introductions of samplers and digital synthesizers. The creative possibilities brought about by MIDI technology have been credited as having helped to revive the music industry in the 1980s.
MIDI introduced many capabilities which transformed the way musicians work. MIDI sequencing made it possible for a user with no notation skills to build complex arrangements. A musical act with as few as one or two members, each operating multiple MIDI-enabled devices, could deliver a performance which sounds similar to that of a much larger group of musicians. The expense of hiring outside musicians for a project could be reduced or eliminated, and complex productions could be realized on a system as small as a synthesizer with integrated keyboard and sequencer.
MIDI helped establish home recording. By performing preproduction in such an environment, an artist can reduce recording costs by arriving at a recording studio with a work that is already partially completed. Educational technology enabled by MIDI has transformed music education.
MIDI and digital audio contrasted.
Those new to the subject of MIDI might confuse it with digital audio. To those it may appear that MIDI and digital audio equipment do the same task - that is the recording of multiple channels of music using digital equipment.
Applications.
Instrument control.
MIDI was invented so that musical instruments could communicate with each other and so that one instrument can control another. Analog synthesizers that have no digital component and were built prior to MIDI's development can be retrofit with kits that convert MIDI messages into analog control voltages. When a note is played on a MIDI instrument, it generates a digital signal that can be used to trigger a note on another instrument. The capability for remote control allows full-sized instruments to be replaced with smaller sound modules, and allows musicians to combine instruments to achieve a fuller sound, or to create combinations such as acoustic piano and strings. MIDI also enables other instrument parameters to be controlled remotely. Synthesizers and samplers contain various tools for shaping a sound. Filters adjust timbre, and envelopes automate the way a sound evolves over time. The frequency of a filter and the envelope attack, or the time it takes for a sound to reach its maximum level, are examples of synthesizer parameters, and can be controlled remotely through MIDI. Effects devices have different parameters, such as delay feedback or reverb time. When a MIDI continuous controller number is assigned to one of these parameters, the device will respond to any messages it receives that are identified by that number. Controls such as knobs, switches, and pedals can be used to send these messages. A set of adjusted parameters can be saved to a device's internal memory as a "patch", and these patches can be remotely selected by MIDI program changes. The MIDI standard allows selection of 128 different programs, but devices can provide more by arranging their patches into banks of 128 programs each, and combining a program change message with a bank select message.
Composition.
MIDI events can be sequenced with computer software, or in specialized hardware music workstations. Many digital audio workstations (DAWs) are specifically designed to work with MIDI as an integral component. MIDI piano rolls have been developed in many DAWs so that the recorded MIDI messages can be extensively modified. These tools allow composers to audition and edit their work much more quickly and efficiently than did older solutions, such as multitrack recording. They improve the efficiency of composers who lack strong pianistic abilities, and allow untrained individuals the opportunity to create polished arrangements.
Because MIDI is a set of commands that create sound, MIDI sequences can be manipulated in ways that prerecorded audio cannot. It is possible to change the key, instrumentation or tempo of a MIDI arrangement, and to reorder its individual sections. The ability to compose ideas and quickly hear them played back enables composers to experiment. Algorithmic composition programs provide computer-generated performances that can be used as song ideas or accompaniment.
Some composers may take advantage of MIDI 1.0 and General MIDI (GM) technology to allow musical data files to be shared among various electronic instruments by using a standard, portable set of commands and parameters. The data composed via the sequenced MIDI recordings can be saved as a Standard MIDI File (SMF), digitally distributed, and reproduced by any computer or electronic instrument that also adheres to the same MIDI, GM, and SMF standards. MIDI data files are much smaller than recorded audio files.
MIDI and computers.
At the time of MIDI's introduction, the computing industry was mainly devoted to mainframe computers, and personal computers were not commonly owned. The personal computer market stabilized at the same time that MIDI appeared, and computers became a viable option for music production. In the years immediately after the 1983 ratification of the MIDI specification, MIDI features were adapted to several early computer platforms, including Apple II Plus, IIe and Macintosh, Commodore 64 and Amiga, Atari ST, Acorn Archimedes, and PC DOS. The Macintosh was the favorite among US musicians, as it was marketed at a competitive price, and would be several years before PC systems would catch up to its efficiency and graphical interface. The Atari ST was favored in Europe, where Macintoshes were more expensive. The Apple IIGS used a digital sound chip designed for the Ensoniq Mirage synthesizer, and later models used a custom sound system and upgraded processors, which drove other companies to improve their own offerings. The Atari ST was favored for its MIDI ports that were built directly into the computer. Most music software in MIDI's first decade was published for either the Apple or the Atari. By the time of Windows 3.0's 1990 release, PCs had caught up in processing power and had acquired a graphical interface, and software titles began to see release on multiple platforms.
Standard MIDI files.
The Standard MIDI File (SMF) is a file format that provides a standardized way for sequences to be saved, transported, and opened in other systems. The compact size of these files has led to their widespread use in computers, mobile phone ringtones, webpage authoring and greeting cards. They are intended for universal use, and include such information as note values, timing and track names. Lyrics may be included as metadata, and can be displayed by karaoke machines. The SMF specification was developed and is maintained by the MMA. SMFs are created as an export format of software sequencers or hardware workstations. They organize MIDI messages into one or more parallel tracks, and timestamp the events so that they can be played back in sequence. A header contains the arrangement's track count, tempo and which of three SMF formats the file is in. A type 0 file contains the entire performance, merged onto a single track, while type 1 files may contain any number of tracks that are performed in synchrony. Type 2 files are rarely used and store multiple arrangements, with each arrangement having its own track and intended to be played in sequence.
Microsoft Windows bundles SMFs together with Downloadable Sounds (DLS) in a Resource Interchange File Format (RIFF) wrapper, as RMID files with a codice_1 extension. RIFF-RMID has been deprecated in favor of Extensible Music Files (XMF).
File sharing.
A MIDI file is not a recording of actual audio. Rather, it is a set of instructions, and can use a thousand times less disk space than the equivalent recorded audio. This made MIDI file arrangements an attractive way to share music, before the advent of broadband internet access and multi-gigabyte hard drives. Licensed MIDI files on floppy disks were commonly available in stores in Europe and Japan during the 1990s. The major drawback to this is the wide variation in quality of users' audio cards, and in the actual audio contained as samples or synthesized sound in the card that the MIDI data only refers to symbolically. Even a sound card that contains high-quality sampled sounds can have inconsistent quality from one instrument to another, while different model cards have no guarantee of consistent sound of the same instrument. Early budget cards, such as the AdLib and the Sound Blaster and its compatibles, used a stripped-down version of Yamaha's frequency modulation synthesis (FM synthesis) technology played back through low-quality digital-to-analog converters. The low-fidelity reproduction of these ubiquitous cards was often assumed to somehow be a property of MIDI itself. This created a perception of MIDI as low-quality audio, while in reality MIDI itself contains no sound, and the quality of its playback depends entirely on the quality of the sound-producing device (and of samples in the device).
MIDI software.
The main advantage of the personal computer in a MIDI system is that it can serve a number of different purposes, depending on the software that is loaded. Multitasking allows simultaneous operation of programs that may be able to share data with each other.
Sequencers.
Sequencing software provides a number of benefits to a composer or arranger. It allows recorded MIDI to be manipulated using standard computer editing features such as cut, copy and paste and drag and drop. Keyboard shortcuts can be used to streamline workflow, and editing functions are often selectable via MIDI commands. The sequencer allows each channel to be set to play a different sound, and gives a graphical overview of the arrangement. A variety of editing tools are made available, including a notation display that can be used to create printed parts for musicians. Tools such as looping, quantization, randomization, and transposition simplify the arranging process. Beat creation is simplified, and groove templates can be used to duplicate another track's rhythmic feel. Realistic expression can be added through the manipulation of real-time controllers. Mixing can be performed, and MIDI can be synchronized with recorded audio and video tracks. Work can be saved, and transported between different computers or studios.
Sequencers may take alternate forms, such as drum pattern editors that allow users to create beats by clicking on pattern grids, and loop sequencers such as ACID Pro, which allow MIDI to be combined with prerecorded audio loops whose tempos and keys are matched to each other. Cue list sequencing is used to trigger dialogue, sound effect, and music cues in stage and broadcast production.
Notation/scoring software.
With MIDI, notes played on a keyboard can automatically be transcribed to sheet music. Scorewriting software typically lacks advanced sequencing tools, and is optimized for the creation of a neat, professional printout designed for live instrumentalists. These programs provide support for dynamics and expression markings, chord and lyric display, and complex score styles. Software is available that can print scores in braille.
ScoreCloud is software that can transcribe from MIDI to scores in real time. SmartScore software performs the reverse process and can produce MIDI files from scanned sheet music. Other notation programs include Finale, Encore and Sibelius.
Editor/librarians.
Patch editors allow users to program their equipment through the computer interface. These became essential with the appearance of complex synthesizers such as the Yamaha FS1R, which contained several thousand programmable parameters, but had an interface that consisted of fifteen tiny buttons, four knobs and a small LCD. Digital instruments typically discourage users from experimentation, due to their lack of the feedback and direct control that switches and knobs would provide, but patch editors give owners of hardware instruments and effects devices the same editing functionality that is available to users of software synthesizers. Some editors are designed for a specific instrument or effects device, while other, "universal" editors support a variety of equipment, and ideally can control the parameters of every device in a setup through the use of System Exclusive commands.
Patch librarians have the specialized function of organizing the sounds in a collection of equipment, and allow transmission of entire banks of sounds between an instrument and a computer. This allows the user to augment the device's limited patch storage with a computer's much greater disk capacity, and to share custom patches with other owners of the same instrument. Universal editor/librarians that combine the two functions were once common, and included Opcode Systems' Galaxy and eMagic's SoundDiver. These programs have been largely abandoned with the trend toward computer-based synthesis, although Mark of the Unicorn's (MOTU)'s Unisyn and Sound Quest's Midi Quest remain available. Native Instruments' Kore was an effort to bring the editor/librarian concept into the age of software instruments.
Auto-accompaniment programs.
Programs that can dynamically generate accompaniment tracks are called "auto-accompaniment" programs. These create a full band arrangement in a style that the user selects, and send the result to a MIDI sound generating device for playback. The generated tracks can be used as educational or practice tools, as accompaniment for live performances, or as a songwriting aid. Examples include Band-in-a-Box, which originated on the Atari platform in the 1980s, One Man Band, Busker, MiBAC Jazz, SoundTrek JAMMER and DigiBand.
Synthesis and sampling.
Computers can use software to generate sounds, which are then passed through a digital-to-analog converter (DAC) to a loudspeaker system. Polyphony, the number of sounds that can be played simultaneously, is dependent on the power of the computer's Central Processing Unit, as are the sample rate and bit depth of playback, which directly affect the quality of the sound. Synthesizers implemented in software are subject to timing issues that are not present with hardware instruments, whose dedicated operating systems are not subject to interruption from background tasks as desktop operating systems are. These timing issues can cause distortion as recorded tracks lose synchronization with each other, and clicks and pops when sample playback is interrupted. Software synthesizers also exhibit a noticeable delay in their sound generation, because computers use an audio buffer that delays playback and disrupts MIDI timing.
Software synthesis' roots go back as far as the 1950s, when Max Mathews of Bell Labs wrote the MUSIC-N programming language, which was capable of non-real-time sound generation. The first synthesizer to run directly on a host computer's CPU was Reality, by Dave Smith's Seer Systems, which achieved a low latency through tight driver integration, and therefore could run only on Creative Labs soundcards. Some systems use dedicated hardware to reduce the load on the host CPU, as with Symbolic Sound Corporation's Kyma System, and the Creamware/Sonic Core Pulsar/SCOPE systems, which used several DSP chips hosted on a PCI card to power an entire studio's worth of instruments, effects, and mixers.
The ability to construct full MIDI arrangements entirely in computer software allows a composer to render a finalized result directly as an audio file.
Game music.
Early PC games were distributed on floppy disks, and the small size of MIDI files made them a viable means of providing soundtracks. Games of the DOS and early Windows eras typically required compatibility with either Ad Lib or SoundBlaster audio cards. These cards used FM synthesis, which generates sound through modulation of sine waves. John Chowning, the technique's pioneer, theorized that the technology would be capable of accurate recreation of any sound if enough sine waves were used, but budget computer audio cards performed FM synthesis with only two sine waves. Combined with the cards' 8-bit audio, this resulted in a sound described as "artificial" and "primitive". Wavetable daughterboards that were later available provided audio samples that could be used in place of the FM sound. These were expensive, but often used the sounds from respected MIDI instruments such as the E-mu Proteus. The computer industry moved in the mid-1990s toward wavetable-based soundcards with 16-bit playback, but standardized on a 2MB ROM, a space too small in which to fit good-quality samples of 128 instruments plus drum kits. Some manufacturers used 12-bit samples, and padded those to 16 bits.
Other applications.
MIDI has been adopted as a control protocol in a number of non-musical applications. MIDI Show Control uses MIDI commands to direct stage lighting systems and to trigger cued events in theatrical productions. VJs and turntablists use it to cue clips, and to synchronize equipment, and recording systems use it for synchronization and automation. Apple Motion allows control of animation parameters through MIDI. The 1987 first-person shooter game "MIDI Maze" and the 1990 Atari ST computer puzzle game "Oxyd" used MIDI to network computers together, and kits are available that allow MIDI control over home lighting and appliances.
Despite its association with music devices, MIDI can control any device that can read and process a MIDI command. It is therefore possible to send a spacecraft from earth to another destination in space, control home lighting, heating and air conditioning and even sequence traffic light signals all through MIDI commands. The receiving device or object would require a General MIDI processor, however in this instance, the program changes would trigger a function on that device rather than notes from MIDI instrument. Each function can be set to a timer (also controlled by MIDI) or other condition determined by the device's creator.
MIDI devices.
Connectors.
The cables terminate in a 180° five-pin DIN connector. Standard applications use only three of the five conductors: a ground wire, and a balanced pair of conductors that carry a +5 volt signal. This connector configuration can only carry messages in one direction, so a second cable is necessary for two-way communication. Some proprietary applications, such as phantom-powered footswitch controllers, use the spare pins for direct current (DC) power transmission.
Opto-isolators keep MIDI devices electrically separated from their connectors, which prevents the occurrence of ground loops and protects equipment from voltage spikes. There is no error detection capability in MIDI, so the maximum cable length is set at 15 meters (50 feet) in order to limit interference.
Most devices do not copy messages from their input to their output port. A third type of port, the "thru" port, emits a copy of everything received at the input port, allowing data to be forwarded to another instrument in a "daisy chain" arrangement. Not all devices contain thru ports, and devices that lack the ability to generate MIDI data, such as effects units and sound modules, may not include out ports.
Electrical Specification.
The MIDI specification for the electrical interface is based on a fully isolated current loop. The MIDI OUT port nominally sources a +5 volt source through a 220 ohm resistor out through pin 4 on the MIDI OUT DIN connector, in on pin 4 of the receiving device's MIDI IN DIN connector, through a 220 ohm protection resistor and the LED of an opto-isolator. The current then returns via pin 5 on the MIDI IN port to the originating device's MIDI OUT port pin 5, again with a 220 ohm resistor in the path, giving a nominal current of about 5 milliamperes. Despite the cable's appearance, there is no conductive path between the two MIDI devices, only an optically isolated one (thus, properly designed MIDI devices are relatively immune to ground loops and similar interference). The data rate on this system is a relatively slow (by 2015 standards) 31,250 bits per second, logic 0 being current ON.
Although MIDI nominally uses a +5 volt source, it is possible to change the resistance values in the MIDI OUT circuit to achieve a similar current with other voltage supplies (in particular, for 3.3 volt systems).
The MIDI specification provides for a ground "wire" and a braid or foil shield, connected on pin 2, protecting the two signal-carrying conductors on pins 4 and 5. Although the MIDI cable is supposed to connect pin 2 and the braid or foil shield to chassis ground, it should do so only at the MIDI OUT port; the MIDI IN port should leave pin 2 unconnected (and preferably, isolated). Some large manufacturers of MIDI devices use modified MIDI IN-only DIN 5-pin sockets with the metallic conductors intentionally omitted at pin positions 1, 2, and 3 so that the maximum voltage isolation is obtained.
Management devices.
Each device in a daisy chain adds delay to the system. This is avoided with a MIDI thru box, which contains several outputs that provide an exact copy of the box's input signal. A MIDI merger is able to combine the input from multiple devices into a single stream, and allows multiple controllers to be connected to a single device. A MIDI switcher allows switching between multiple devices, and eliminates the need to physically repatch cables. MIDI patch bays combine all of these functions. They contain multiple inputs and outputs, and allow any combination of input channels to be routed to any combination of output channels. Routing setups can be created using computer software, stored in memory, and selected by MIDI program change commands. This enables the devices to function as standalone MIDI routers in situations where no computer is present. MIDI patch bays also clean up any skewing of MIDI data bits that occurs at the input stage.
MIDI data processors are used for utility tasks and special effects. These include MIDI filters, which remove unwanted MIDI data from the stream, and MIDI delays, effects which send a repeated copy of the input data at a set time.
Interfaces.
A computer MIDI interface's main function is to match clock speeds between the MIDI device and the computer. Some computer sound cards include a standard MIDI connector, whereas others connect by any of various means that include the D-subminiature DA-15 game port, USB, FireWire, Ethernet or a proprietary connection.
The increasing use of USB connectors in the 2000s has led to the availability of MIDI-to-USB data interfaces that can transfer MIDI channels to USB-equipped computers. Some MIDI keyboard controllers are equipped with USB jacks, and can be plugged into computers that run music software.
MIDI's serial transmission leads to timing problems. Experienced musicians can detect time differences of as small as 1/3 of a millisecond (ms) (which is how long it takes sound to travel 4 inches), and a three-byte MIDI message requires nearly 1ms for transmission. Because MIDI is serial, it can only send one event at a time. If an event is sent on two channels at once, the event on the higher-numbered channel cannot transmit until the first one is finished, and so is delayed by 1ms. If an event is sent on all channels at the same time, the highest-numbered channel's transmission will be delayed by as much as 16ms. This contributed to the rise of MIDI interfaces with multiple in- and out-ports, because timing improves when events are spread between multiple ports as opposed to multiple channels on the same port. The term "MIDI slop" refers to audible timing errors that result when MIDI transmission is delayed.
Controllers.
There are two types of MIDI controllers: performance controllers that generate notes and are used to perform music, and controllers which may not send notes, but transmit other types of real-time events. Many devices are some combination of the two types.
Performance controllers.
MIDI was designed with keyboards in mind, and any controller that is not a keyboard is considered an "alternative" controller. This was seen as a limitation by composers who were not interested in keyboard-based music, but the standard proved flexible, and MIDI compatibility was introduced to other types of controllers, including guitars, wind instruments and drum machines.
Keyboards.
Keyboards are by far the most common type of MIDI controller. These are available in sizes that range from 25-key, 2-octave models, to full-sized 88-key instruments. Some are keyboard-only controllers, though many include other real-time controllers such as sliders, knobs, and wheels. Commonly, there are also connections for sustain and expression pedals. Most keyboard controllers offer the ability to split the playing area into "zones", which can be of any desired size and can overlap with each other. Each zone can respond to a different MIDI channel and a different set of performance controllers, and can be set to play any desired range of notes. This allows a single playing surface to target a number of different devices. MIDI capabilities can also be built into traditional keyboard instruments, such as grand pianos and Rhodes pianos. Pedal keyboards can operate the pedal tones of a MIDI organ, or can drive a bass synthesizer such as the revived Moog Taurus.
Wind controllers.
Wind controllers allow MIDI parts to be played with the same kind of expression and articulation that is available to players of wind and brass instruments. They allow breath and pitch glide control that provide a more versatile kind of phrasing, particularly when playing sampled or physically modeled wind instrument parts. A typical wind controller has a sensor that converts breath pressure to volume information, and may allow pitch control through a lip pressure sensor and a pitch-bend wheel. Some models include a configurable key layout that can emulate different instruments' fingering systems. Examples of such controllers include Akai's Electronic Wind Instrument (EWI) and Electronic Valve Instrument (EVI). The EWI uses a system of keypads and rollers modeled after a traditional woodwind instrument, while the EVI is based on an acoustic brass instrument, and has three switches that emulate a trumpet's valves. Simpler breath controllers are also available: unlike wind controllers, they do not trigger notes and are intended for use in conjunction with a keyboard or synthesizer. Examples of breath controllers are the Yamaha BC3 (now discontinued) and the TEControl USB-MIDI Breath Controller.
Drum and percussion controllers.
Keyboards can be used to trigger drum sounds, but are impractical for playing repeated patterns such as rolls, due to the length of key travel. After keyboards, drum pads are the next most significant MIDI performance controllers. Drum controllers may be built into drum machines, may be standalone control surfaces, or may emulate the look and feel of acoustic percussion instruments. The pads built into drum machines are typically too small and fragile to be played with sticks, and are played with fingers. Dedicated drum pads such as the Roland Octapad or the DrumKAT are playable with the hands or with sticks, and are often built in the form of a drum kit. There are also percussion controllers such as the vibraphone-style MalletKAT, and Don Buchla's Marimba Lumina. MIDI triggers can also be installed into acoustic drum and percussion instruments. Pads that can trigger a MIDI device can be homemade from a piezoelectric sensor and a practice pad or other piece of foam rubber.
Stringed instrument controllers.
A guitar can be fit with special pickups that digitize the instrument's output, and allow it to play a synthesizer's sounds. These assign a separate MIDI channel for each string, and may give the player the choice of triggering the same sound from all six strings, or playing a different sound from each. Some models, such as Yamaha's G10, dispense with the traditional guitar body and replace it with electronics. Other systems, such as Roland's MIDI pickups, are included with or can be retrofitted to a standard instrument. Max Mathews designed a MIDI violin for Laurie Anderson in the mid-1980s, and MIDI-equipped violas, cellos, contrabasses, and mandolins also exist.
Specialized performance controllers.
DJ digital controllers may be standalone units such as the Faderfox or the Allen & Heath Xone 3D, or may be integrated with a specific piece of software, such as Traktor or Scratch Live. These typically respond to MIDI clock sync, and provide control over mixing, looping, effects, and sample playback.
MIDI triggers attached to shoes or clothing are sometimes used by stage performers. The Kroonde Gamma wireless sensor can capture physical motion as MIDI signals. Sensors built into a dance floor at the University of Texas at Austin convert dancers' movements into MIDI messages, and David Rokeby's "Very Nervous System" art installation created music from the movements of passers-through. Software applications exist which enable the use of iOS devices as gesture controllers.
Numerous experimental controllers exist which abandon traditional musical interfaces entirely. These include the gesture-controlled Buchla Thunder, sonomes such as the C-Thru Music Axis, which rearrange the scale tones into an isometric layout, and Haken Audio's keyless, touch-sensitive Continuum playing surface. Experimental MIDI controllers may be created from unusual objects, such as an ironing board with heat sensors installed, or a sofa equipped with pressure sensors.
Auxiliary controllers.
Software synthesizers offer great power and versatility, but some players feel that division of attention between a MIDI keyboard and a computer keyboard and mouse robs some of the immediacy from the playing experience. Devices dedicated to real-time MIDI control provide an ergonomic benefit, and can provide a greater sense of connection with the instrument than can an interface that is accessed through a mouse or a push-button digital menu. Controllers may be general-purpose devices that are designed to work with a variety of equipment, or they may be designed to work with a specific piece of software. Examples of the latter include Akai's APC40 controller for Ableton Live, and Korg's MS-20ic controller that is a reproduction of their MS-20 analog synthesizer. The MS-20ic controller includes patch cables that can be used to control signal routing in their virtual reproduction of the MS-20 synthesizer, and can also control third-party devices.
Control surfaces.
Control surfaces are hardware devices that provide a variety of controls that transmit real-time controller messages. These enable software instruments to be programmed without the discomfort of excessive mouse movements, or adjustment of hardware devices without the need to step through layered menus. Buttons, sliders, and knobs are the most common controllers provided, but rotary encoders, transport controls, joysticks, ribbon controllers, vector touchpads in the style of Korg's Kaoss pad, and optical controllers such as Roland's D-Beam may also be present. Control surfaces may be used for mixing, sequencer automation, turntablism, and lighting control.
Specialized real-time controllers.
Audio control surfaces often resemble mixing consoles in appearance, and enable a level of hands-on control for changing parameters such as sound levels and effects applied to individual tracks of a multitrack recording or live performance output.
MIDI footswitches are commonly used to send MIDI program change commands to effects devices, but may be combined with pedals in a pedalboard that allows detailed programming of effects units. Pedals are available in the form of on/off switches, either momentary or latching, or as "rocker" pedals whose position determines the value of a MIDI continuous controller.
Drawbar controllers are for use with MIDI and virtual organs. Along with a set of drawbars for timbre control, they may provide controls for standard organ effects such as rotating speaker speed, vibrato and chorus.
Instruments.
A MIDI instrument contains ports to send and receive MIDI signals, a CPU to process those signals, an interface that allows user programming, audio circuitry to generate sound, and controllers. The operating system and factory sounds are often stored in a Read-only memory (ROM) unit.
A MIDI Instrument can also be a stand-alone module (without a piano style keyboard) consisting of a General MIDI soundboard (GM, GS and/XG), onboard editing, including transposing/pitch changes, MIDI instrument changes and adjusting volume, pan, reverb levels and other MIDI controllers. Typically, the MIDI Module will include a large screen, enabling the user to view information depending on the function selected at that time. Features can include scrolling lyrics, usually embedded in a MIDI File or Karaoke MIDI, playlists, song library and editing screens. Some MIDI Modules include a Harmonizer and the ability to playback and transpose MP3 audio files.
Synthesizers.
Synthesizers may employ any of a variety of sound generation techniques. They may include an integrated keyboard, or may exist as "sound modules" or "expanders" that generate sounds when triggered by an external controller. Sound modules are typically designed to be mounted in a 19-inch rack. Manufacturers commonly produce a synthesizer in both standalone and rack-mounted versions, and often offer the keyboard version in a variety of sizes.
Samplers.
A sampler can record and digitize audio, store it in random-access memory (RAM), and play it back. Samplers typically allow a user to edit a sample and save it to a hard disk, apply effects to it, and shape it with the same tools that synthesizers use. They also may be available in either keyboard or rack-mounted form. Instruments that generate sounds through sample playback, but have no recording capabilities, are known as "ROMplers".
Samplers did not become established as viable MIDI instruments as quickly as synthesizers did, due to the expense of memory and processing power at the time. The first low-cost MIDI sampler was the Ensoniq Mirage, introduced in 1984. MIDI samplers are typically limited by displays that are too small to use to edit sampled waveforms, although some can be connected to a computer monitor.
Drum machines.
Drum machines typically are sample playback devices that specialize in drum and percussion sounds. They commonly contain a sequencer that allows the creation of drum patterns, and allows them to be arranged into a song. There often are multiple audio outputs, so that each sound or group of sounds can be routed to a separate output. The individual drum voices may be playable from another MIDI instrument, or from a sequencer.
Workstations and hardware sequencers.
Sequencer technology predates MIDI. Analog sequencers use CV/Gate signals to control pre-MIDI analog synthesizers. MIDI sequencers typically are operated by transport features modeled after those of tape decks. They are capable of recording MIDI performances, and arranging them into individual tracks along a multitrack recording concept. Music workstations combine controller keyboards with an internal sound generator and a sequencer. These can be used to build complete arrangements and play them back using their own internal sounds, and function as self-contained music production studios. They commonly include file storage and transfer capabilities.
Effects devices.
Audio effects units that are frequently used in stage and recording, such as reverbs, delays and choruses, can be remotely adjusted via MIDI signals. Some units allow only a limited number of parameters to be controlled this way, but most will respond to program change messages. The Eventide H3000 Ultra-harmonizer is an example of a unit that allows such extensive MIDI control that it is playable as a synthesizer.
Technical specifications.
MIDI messages are made up of 8-bit "word"s (commonly called "bytes") that are transmitted serially at 31.25 kbit/s. This rate was chosen because it is an exact division of 1 MHz, the speed at which many early microprocessors operated. The first bit of each word identifies whether the word is a status byte or a data byte, and is followed by seven bits of information. A start bit and a stop bit are added to each byte for framing purposes, so a MIDI byte requires ten bits for transmission.
A MIDI link can carry sixteen independent channels of information. The channels are numbered 1–16, but their actual corresponding binary encoding is 0–15. A device can be configured to only listen to specific channels and to ignore the messages sent on other channels ("Omni Off" mode), or it can listen to all channels, effectively ignoring the channel address ("Omni On"). An individual device may be monophonic (the start of a new "note-on" MIDI command implies the termination of the previous note), or polyphonic (multiple notes may be sounding at once, until the polyphony limit of the instrument is reached, or the notes reach the end of their decay envelope, or explicit "note-off" MIDI commands are received). Receiving devices can typically be set to all four combinations of "omni off/on" versus "mono/poly" modes.
Messages.
A MIDI message is an instruction that controls some aspect of the receiving device. A MIDI message consists of a status byte, which indicates the type of the message, followed by up to two data bytes that contain the parameters. MIDI messages can be "channel messages", which are sent on only one of the 16 channels and can be heard only by devices receiving on that channel, or "system messages", which are heard by all devices. Any data not relevant to a receiving device is ignored. There are five types of message: Channel Voice, Channel Mode, System Common, System Real-Time, and System Exclusive.
Channel Voice messages transmit real-time performance data over a single channel. Examples include "note-on" messages which contain a MIDI note number that specifies the note's pitch, a velocity value that indicates how forcefully the note was played, and the channel number; "note-off" messages that end a note; program change messages that change a device's patch; and control changes that allow adjustment of an instrument's parameters. Channel Mode messages include the Omni/mono/poly mode on and off messages, as well as messages to reset all controllers to their default state or to send "note-off" messages for all notes. System messages do not include channel numbers, and are received by every device in the MIDI chain. MIDI time code is an example of a System Common message. System Real-Time messages provide for synchronization, and include MIDI clock and Active Sensing.
System Exclusive messages.
System Exclusive (SysEx) messages are a major reason for the flexibility and longevity of the MIDI standard. They allow manufacturers to create proprietary messages which provide control over their equipment in a way that is more thorough than is provided for by standard MIDI messages. SysEx messages are addressed to a specific device in a system. Each manufacturer has a unique identifier that is included in its SysEx messages, which helps ensure that the messages will be heard only by the targeted device, and ignored by all others. Many instruments also include a SysEx ID setting, which allows two devices of the same model to be addressed independently while connected to the same system. SysEx messages may include functionality beyond what the MIDI standard provides. They are targeted at a specific instrument, and are ignored by all other devices on the system.
The MIDI Implementation Chart.
Devices typically do not respond to every type of message defined by the MIDI specification. The MIDI Implementation Chart was standardized by the MMA as a way for users to see what specific capabilities an instrument has, and how it responds to messages. A specific MIDI Implementation Chart is usually published for each MIDI device within the device documentation.
Extensions.
MIDI's flexibility and widespread adoption have led to many refinements of the standard, and have enabled its application to purposes beyond those for which it was originally intended.
General MIDI.
MIDI allows selection of an instrument's sounds through program change messages, but there is no guarantee that any two instruments have the same sound at a given program location. Program #0 may be a piano on one instrument, or a flute on another. The General MIDI (GM) standard was established in 1991, and provides a standardized sound bank that allows a Standard MIDI File created on one device to sound similar when played back on another. GM specifies a bank of 128 sounds arranged into 16 families of eight related instruments, and assigns a specific program number to each instrument. Percussion instruments are placed on channel 10, and a specific MIDI note value is mapped to each percussion sound. GM-compliant devices must offer 24-note polyphony. Any given program change will select the same instrument sound on any GM-compatible instrument.
The GM standard eliminates variation in note mapping. Some manufacturers had disagreed over what note number should represent middle C, but GM specifies that note number 69 plays A440, which in turn fixes middle C as note number 60. GM-compatible devices are required to respond to velocity, aftertouch, and pitch bend, to be set to specified default values at startup, and to support certain controller numbers such as for sustain pedal, and Registered Parameter Numbers. A simplified version of GM, called "GM Lite", is used in mobile phones and other devices with limited processing power.
GS, XG, and GM2.
A general opinion quickly formed that the GM's 128-instrument sound set was not large enough. Roland's General Standard, or GS, system included additional sounds, drumkits and effects, provided a "bank select" command that could be used to access them, and used MIDI Non-Registered Parameter Numbers (NRPNs) to access its new features. Yamaha's Extended General MIDI, or XG, followed in 1994. XG similarly offered extra sounds, drumkits and effects, but used standard controllers instead of NRPNs for editing, and increased polyphony to 32 voices. Both standards feature backward compatibility with the GM specification, but are not compatible with each other. Neither standard has been adopted beyond its creator, but both are commonly supported by music software titles.
Member companies of Japan's AMEI developed the General MIDI Level 2 specification in 1999. GM2 maintains backward compatibility with GM, but increases polyphony to 32 voices, standardizes several controller numbers such as for sostenuto and soft pedal ("una corda"), RPNs and Universal System Exclusive Messages, and incorporates the MIDI Tuning Standard. GM2 is the basis of the instrument selection mechanism in Scalable Polyphony MIDI (SP-MIDI), a MIDI variant for low power devices that allows the device's polyphony to scale according to its processing power.
MIDI Tuning Standard.
Most MIDI synthesizers use equal temperament tuning. The MIDI Tuning Standard (MTS),ratified in 1992, allows alternate tunings. MTS allows microtunings that can be loaded from a bank of up to 128 patches, and allows real-time adjustment of note pitches. Manufacturers are not required to support the standard. Those who do are not required to implement all of its features.
MIDI Time Code.
A sequencer can drive a MIDI system with its internal clock, but when a system contains multiple sequencers, they must synchronize to a common clock. MIDI Time Code (MTC), developed by Digidesign, implements SysEx messages that have been developed specifically for timing purposes, and is able to translate to and from the SMPTE time code standard. MIDI Clock is based on tempo, but SMPTE time code is based on frames per second, and is independent of tempo. MTC, like SMPTE code, includes position information, and can adjust itself if a timing pulse is lost. MIDI interfaces such as Mark of the Unicorn's MIDI Timepiece can convert SMPTE code to MTC.
MIDI Machine Control.
MIDI Machine Control (MMC) consists of a set of SysEx commands that operate the transport controls of hardware recording devices. MMC allows a sequencer to send "Start", "Stop", and "Record" commands to a connected tape deck or hard disk recording system, and to fast-forward or rewind the device so that it starts playback at the same point as the sequencer. No synchronization data is involved, although the devices may synchronize through MTC.
MIDI Show Control.
MIDI Show Control (MSC) is a set of SysEx commands which allows sequencing and remote cueing of show control devices such as lighting, music and sound playback, and motion control systems. Applications include stage productions, museum exhibits, recording studio control systems, and amusement park attractions.
MIDI timestamping.
One solution to MIDI timing problems is to mark MIDI events with the times they are to be played, and store them in a buffer in the MIDI interface ahead of time. Sending data beforehand reduces the likelihood that a busy passage will send a large amount of information that will overwhelm the transmission link. Once stored in the interface, the information is no longer subject to timing issues associated with USB jitter and computer operating system interrupts, and can be transmitted with a high degree of accuracy. MIDI timestamping only works when both the hardware and software support it. MOTU's MTS, eMagic's AMT, and Steinberg's Midex 8 were implementations that were incompatible with each other, and required users to own software and hardware manufactured by the same company in order to gain its benefits. Timestamping is built into FireWire MIDI interfaces and Mac OS X Core Audio.
MIDI Sample Dump Standard.
An unforeseen capability of SysEx messages was their use for transporting audio samples between instruments. SysEx is poorly suited to this purpose, as MIDI words are limited to seven bits of information, so an 8-bit sample requires two bytes for transmission instead of one. This led to the development of the Sample Dump Standard (SDS), which established a protocol for sample transmission. The SDS was later augmented with a pair of commands that allow the transmission of information about sample loop points, without requiring that the entire sample be transmitted.
Downloadable Sounds.
The Downloadable Sounds (DLS) specification, ratified in 1997, allows mobile devices and computer sound cards to expand their wave tables with downloadable sound sets. The DLS Level 2 Specification followed in 2006, and defined a standardized synthesizer architecture. The Mobile DLS standard calls for DLS banks to be combined with SP-MIDI, as self-contained Mobile XMF files.
Alternative hardware transports.
In addition to the original 31.25 kbit/s current-loop transported on 5-pin DIN, other connectors have been used for the same electrical data, and transmission of MIDI streams in different forms over USB, IEEE 1394 a.k.a. FireWire, and Ethernet is now common. Some samplers and hard drive recorders can also pass MIDI data between each other over SCSI.
USB and FireWire.
Members of the USB-IF in 1999 developed a standard for MIDI over USB, the "Universal Serial Bus Device Class Definition for MIDI Devices" MIDI over USB has become increasingly common as other interfaces that had been used for MIDI connections (serial, joystick, etc.) disappeared from personal computers. Microsoft Windows, Macintosh OS X, and Apple iOS operating systems include standard class drivers to support devices that use the "Universal Serial Bus Device Class Definition for MIDI Devices". Drivers are also available for Linux. Some manufacturers choose to implement a MIDI interface over USB that is designed to operate differently from the class specification, using custom drivers.
Apple Computer developed the FireWire interface during the 1990s. It began to appear on digital video cameras toward the end of the decade, and on G3 Macintosh models in 1999. It was created for use with multimedia applications. Unlike USB, FireWire uses intelligent controllers that can manage their own transmission without attention from the main CPU. As with standard MIDI devices, FireWire devices can communicate with each other with no computer present.
XLR connectors.
The Octave-Plateau Voyetra-8 synthesizer was an early MIDI implementation using XLR3 connectors in place of the 5-pin DIN. It was released in the pre-MIDI years and later retrofitted with a MIDI interface but keeping its XLR connector.
Serial parallel, and joystick port MIDI.
As computer-based studio setups became common, MIDI devices that could connect directly to a computer became available. These typically used the 8-pin mini-DIN connector that was used by Apple for serial and printer ports prior to the introduction of the Blue & White G3 models. MIDI interfaces intended for use as the centerpiece of a studio, such as the Mark of the Unicorn MIDI Time Piece, were made possible by a "fast" transmission mode that could take advantage of these serial ports' ability to operate at 20 times the standard MIDI speed. Mini-DIN ports were built into some late-1990s MIDI instruments, and enabled such devices to be connected directly to a computer. Some devices connected via PCs' DB-25 parallel port, or through the joystick port found in many PC sound cards.
mLAN.
Yamaha introduced the mLAN protocol in 1999. It was conceived as a Local Area Network for musical instruments using FireWire as the transport, and was designed to carry multiple MIDI channels together with multichannel digital audio, data file transfers, and time code. mLan was used in a number of Yamaha products, notably digital mixing consoles and the Motif synthesizer, and in third-party products such as the PreSonus FIREstation and the Korg Triton Studio. No new mLan products have been released since 2007.
Ethernet.
The computer network implementation of MIDI provides network routing capabilities, and provides the high-bandwidth channel that earlier alternatives to MIDI, such as ZIPI, were intended to bring. Proprietary implementations have existed since the 1980s, some of which use fiber optic cables for transmission. The Internet Engineering Task Force's RTP MIDI open specification is gaining industry support, as proprietary MIDI/IP protocols require expensive licensing fees, or provide no advantage, apart from speed, over the original MIDI protocol. Apple has supported this protocol from Mac OS X 10.4 onwards, and a Windows driver based on Apple's implementation exists for Windows XP and newer versions.
Wireless MIDI.
Systems for wireless MIDI transmission have been available since the 1980s. Several commercially available transmitters allow wireless transmission of MIDI and OSC signals over Wi-Fi and Bluetooth. iOS devices are able to function as MIDI control surfaces, using Wi-Fi and OSC. An XBee radio can be used to build a wireless MIDI transceiver as a do-it-yourself project.
Android devices are able to function as full MIDI control surfaces using several different protocols over Wi-Fi and Bluetooth.
3.5mm Audio jack.
Some devices use standard TRS audio minijack connectors for MIDI data, including the Korg Electribe 2 and the Arturia Beatstep Pro. Both come with adaptors that break out to standard 5-pin DIN connectors.
The future of MIDI.
A new version of MIDI tentatively called "HD Protocol" or "High-Definition Protocol" has been under discussion since 2005, when it was announced as "HD-MIDI". This new standard offers full backward compatibility with MIDI 1.0 and is intended to support higher-speed transports, allow plug-and-play device discovery and enumeration, and provide greater data range and resolution. The numbers of channels and controllers are to be increased, new kinds of events are to be added, and messages are to be simplified. Entirely new kinds of events will be supported, such as a Note Update message and Direct Pitch in the Note message which are aimed at guitar controllers. Proposed physical layer transports include Ethernet-based protocols such as RTP MIDI and Audio Video Bridging. The HD Protocol and a User Datagram Protocol (UDP)-based transport are under review by MMA's High-Definition Protocol Working Group (HDWG), which includes representatives from all sizes and types of companies. Prototype devices based on the draft standard have been shown privately at NAMM using wired and wireless connections, however it is uncertain if and when the new protocol will be picked up by the industry. As of 2015, the HD Protocol specifications are nearing completion and MMA develops the policies on licensing and product certification. MIDI connectivity and a software synthesizer is still included in Windows, OS X, iOS. USB MIDI interfaces are supported by Android 6.0.

</doc>
<doc id="19999" url="https://en.wikipedia.org/wiki?curid=19999" title="Microcode">
Microcode

Microcode is "a technique that imposes an interpreter between the hardware and the architectural level of a computer." As such, the microcode is a layer of hardware-level instructions that implement higher-level machine code instructions or internal state machine sequencing in many digital processing elements. Microcode is used in general-purpose central processing units, in more specialized processors such as microcontrollers, digital signal processors, channel controllers, disk controllers, network interface controllers, network processors, graphics processing units, and in other hardware.
Microcode typically resides in special high-speed memory and translates machine instructions, state machine data or other input into sequences of detailed circuit-level operations. It separates the machine instructions from the underlying electronics so that instructions can be designed and altered more freely. It also facilitates the building of complex multi-step instructions, while reducing the complexity of computer circuits. Writing microcode is often called microprogramming and the microcode in a particular processor implementation is sometimes called a microprogram.
More extensive microcoding allows small and simple microarchitectures to emulate more powerful architectures with wider word length, more execution units and so on, which is a relatively simple way to achieve software compatibility between different products in a processor family.
Some hardware vendors, especially IBM, use the term "microcode" as a synonym for "firmware". In that way, all code within a device is termed "microcode" regardless of it being microcode or machine code; for example, hard disk drives are said to have their microcode updated, though they typically contain both microcode and firmware.
Overview.
When compared to normal application programs, the elements composing a microprogram exist on a lower conceptual level. To avoid confusion, each microprogram-related element is differentiated by the "micro" prefix: microinstruction, microassembler, microprogrammer, microarchitecture, etc.
Engineers normally write the microcode during the design phase of a processor, storing it in a ROM (read-only memory) or PLA (programmable logic array) structure, or in a combination of both. However, machines also exist that have some or all microcode stored in SRAM or flash memory. This is traditionally denoted as "writeable control store" in the context of computers, which can be either read-only or read-write memory. In the latter case, the CPU initialization process loads microcode into the control store from another storage medium, with the possibility of altering the microcode to correct bugs in the instruction set, or to implement new machine instructions.
Complex digital processors may also employ more than one (possibly microcode-based) control unit in order to delegate sub-tasks that must be performed essentially asynchronously in parallel. A high-level programmer, or even an assembly programmer, does not normally see or change microcode. Unlike machine code, which often retains some compatibility among different processors in a family, microcode only runs on the exact electronic circuitry for which it is designed, as it constitutes an inherent part of the particular processor design itself.
Microprograms consist of series of microinstructions, which control the CPU at a very fundamental level of hardware circuitry. For example, a single typical microinstruction might specify the following operations:
To simultaneously control all processor's features in one cycle, the microinstruction is often wider than 50 bits, e.g., 128 bits on a 360/85 with an emulator feature. Microprograms are carefully designed and optimized for the fastest possible execution, as a slow microprogram would result in a slow machine instruction and degraded performance for related application programs that use such instructions.
Justification.
Microcode was originally developed as a simpler method of developing the control logic for a computer. Initially, CPU instruction sets were "hardwired". Each step needed to fetch, decode, and execute the machine instructions (including any operand address calculations, reads, and writes) was controlled directly by combinational logic and rather minimal sequential state machine circuitry. While very efficient, the need for powerful instruction sets with multi-step addressing and complex operations ("see below") made such hard-wired processors difficult to design and debug; highly encoded and varied-length instructions can contribute to this as well, especially when very irregular encodings are used.
Microcode simplified the job by allowing much of the processor's behaviour and programming model to be defined via microprogram routines rather than by dedicated circuitry. Even late in the design process, microcode could easily be changed, whereas hard-wired CPU designs were very cumbersome to change. Thus, this greatly facilitated CPU design.
From the 1940s to the late 1970s, a large portion of programming was done in assembly language; higher-level instructions mean greater programmer productivity, so an important advantage of microcode was the relative ease by which powerful machine instructions can be defined. The ultimate extension of this are "Directly Executable High Level Language" designs, in which each statement of a high-level language such as PL/I is entirely and directly executed by microcode, without compilation. The IBM Future Systems project and Data General Fountainhead Processor are examples of this. During the 1970s, CPU speeds grew more quickly than memory speeds and numerous techniques such as memory block transfer, memory pre-fetch and multi-level caches were used to alleviate this. High-level machine instructions, made possible by microcode, helped further, as fewer more complex machine instructions require less memory bandwidth. For example, an operation on a character string can be done as a single machine instruction, thus avoiding multiple instruction fetches.
Architectures with instruction sets implemented by complex microprograms included the IBM System/360 and Digital Equipment Corporation VAX. The approach of increasingly complex microcode-implemented instruction sets was later called CISC. An alternate approach, used in many microprocessors, is to use PLAs or ROMs (instead of combinational logic) mainly for instruction decoding, and let a simple state machine (without much, or any, microcode) do most of the sequencing. The MOS Technology 6502 is an example of a microprocessor using a PLA for instruction decode and sequencing. The PLA is visible in photomicrographs of the chip, and its operation can be seen in the transistor-level simulation.
Microprogramming is still used in modern CPU designs. In some cases, after the microcode is debugged in simulation, logic functions are substituted for the control store. Logic functions are often faster and less expensive than the equivalent microprogram memory.
Benefits.
A processor's microprograms operate on a more primitive, totally different, and much more hardware-oriented architecture than the assembly instructions visible to normal programmers. In coordination with the hardware, the microcode implements the programmer-visible architecture. The underlying hardware need not have a fixed relationship to the visible architecture. This makes it easier to implement a given instruction set architecture on a wide variety of underlying hardware micro-architectures.
The IBM System/360 has a 32-bit architecture with 16 general-purpose registers, but most of the System/360 implementations actually use hardware that implemented a much simpler underlying microarchitecture; for example, the System/360 Model 30 has 8-bit data paths to the arithmetic logic unit (ALU) and main memory and implemented the general-purpose registers in a special unit of higher-speed core memory, and the System/360 Model 40 has 8-bit data paths to the ALU and 16-bit data paths to main memory and also implemented the general-purpose registers in a special unit of higher-speed core memory. The Model 50 has full 32-bit data paths and implements the general-purpose registers in a special unit of higher-speed core memory. The Model 65 through the Model 195 have larger data paths and implement the general-purpose registers in faster transistor circuits. In this way, microprogramming enabled IBM to design many System/360 models with substantially different hardware and spanning a wide range of cost and performance, while making them all architecturally compatible. This dramatically reduces the number of unique system software programs that must be written for each model.
A similar approach was used by Digital Equipment Corporation (DEC) in their VAX family of computers. As a result, different VAX processors use different microarchitectures, yet the programmer-visible architecture does not change.
Microprogramming also reduces the cost of field changes to correct defects (bugs) in the processor; a bug can often be fixed by replacing a portion of the microprogram rather than by changes being made to hardware logic and wiring.
History.
In 1947, the design of the MIT Whirlwind introduced the concept of a control store as a way to simplify computer design and move beyond "ad hoc" methods. The control store was a diode matrix: a two-dimensional lattice, where one dimension accepts "control time pulses" from the CPU's internal clock, and the other connects to control signals on gates and other circuits. A "pulse distributor" takes the pulses generated by the CPU clock and break them up into eight separate time pulses, each of which would activate a different row of the lattice. When the row is activated, it activates the control signals connected to it.
Described another way, the signals transmitted by the control store are being played much like a player piano roll. That is, they are controlled by a sequence of very wide words constructed of bits, and they are "played" sequentially. In a control store, however, the "song" is short and repeated continuously.
In 1951, Maurice Wilkes enhanced this concept by adding "conditional execution", a concept akin to a conditional in computer software. His initial implementation consisted of a pair of matrices: the first one generated signals in the manner of the Whirlwind control store, while the second matrix selected which row of signals (the microprogram instruction word, so to speak) to invoke on the next cycle. Conditionals were implemented by providing a way that a single line in the control store could choose from alternatives in the second matrix. This made the control signals conditional on the detected internal signal. Wilkes coined the term microprogramming to describe this feature and distinguish it from a simple control store.
Implementation.
Each microinstruction in a microprogram provides the bits that control the functional elements that internally compose a CPU. The advantage over a hard-wired CPU is that internal CPU control becomes a specialized form of a computer program. Microcode thus transforms a complex electronic design challenge (the control of a CPU) into a less complex programming challenge. To take advantage of this, a CPU is divided into several parts:
There may also be a memory address register and a memory data register, used to access the main computer storage. Together, these elements form an "execution unit". Most modern CPUs have several execution units. Even simple computers usually have one unit to read and write memory, and another to execute user code. These elements could often be brought together as a single chip. This chip comes in a fixed width that would form a "slice" through the execution unit. These are known as "bit slice" chips. The AMD Am2900 family is one of the best known examples of bit slice elements. The parts of the execution units and the execution units themselves are interconnected by a bundle of wires called a bus.
Programmers develop microprograms, using basic software tools. A microassembler allows a programmer to define the table of bits symbolically. Because of its close relationship to the underlying architecture, "microcode has several properties that make it difficult to generate using a compiler." A simulator program is intended to execute the bits in the same way as the electronics, and allows much more freedom to debug the microprogram. After the microprogram is finalized, and extensively tested, it is sometimes used as the input to a computer program that constructs logic to produce the same data. This program is similar to those used to optimize a programmable logic array. No known computer program can produce optimal logic, but even good logic can vastly reduce the number of transistors from the number required for a ROM control store. This reduces the cost of producing, and the electricity consumed by, a CPU.
Microcode can be characterized as "horizontal" or "vertical", referring primarily to whether each microinstruction controls CPU elements with little or no decoding (horizontal microcode) or requires extensive decoding by combinatorial logic before doing so (vertical microcode). Consequently, each horizontal microinstruction is wider (contains more bits) and occupies more storage space than a vertical microinstruction.
Horizontal microcode.
"Horizontal microcode has several discrete micro-operations that are combined in a single microinstruction for simultaneous operation." Horizontal microcode is typically contained in a fairly wide control store; it is not uncommon for each word to be 108 bits or more. On each tick of a sequencer clock a microcode word is read, decoded, and used to control the functional elements that make up the CPU.
In a typical implementation a horizontal microprogram word comprises fairly tightly defined groups of bits. For example, one simple arrangement might be:
For this type of micromachine to implement a JUMP instruction with the address following the opcode, the microcode might require two clock ticks. The engineer designing it would write microassembler source code looking something like this:
For each tick it is common to find that only some portions of the CPU are used, with the remaining groups of bits in the microinstruction being no-ops. With careful design of hardware and microcode, this property can be exploited to parallelise operations that use different areas of the CPU; for example, in the case above, the ALU is not required during the first tick, so it could potentially be used to complete an earlier arithmetic instruction.
Vertical microcode.
In vertical microcode, each microinstruction is significantly encoded that is, the bit fields generally pass through intermediate combinatory logic that, in turn, generates the actual control and sequencing signals for internal CPU elements (ALU, registers, etc.). This is in contrast with horizontal microcode, in which the bit fields themselves either directly produce the control and sequencing signals or are only minimally encoded. Consequently, vertical microcode requires smaller instruction lengths and less storage, but requires more time to decode, resulting in a slower CPU clock.
Some vertical microcode is just the assembly language of a simple conventional computer that is emulating a more complex computer. Some processors, such as DEC Alpha processors and the CMOS microprocessors on later IBM System/390 mainframes and z/Architecture mainframes, have PALcode (the term used on Alpha processors) or millicode (the term used on IBM mainframe microprocessors). This is a form of machine code, with access to special registers and other hardware resources not available to regular machine code, used to implement some instructions and other functions, such as page table walks on Alpha processors.
Another form of vertical microcode has two fields:
The "field select" selects which part of the CPU will be controlled by this word of the control store. The "field value" actually controls that part of the CPU. With this type of microcode, a designer explicitly chooses to make a slower CPU to save money by reducing the unused bits in the control store; however, the reduced complexity may increase the CPU's clock frequency, which lessens the effect of an increased number of cycles per instruction.
As transistors became cheaper, horizontal microcode came to dominate the design of CPUs using microcode, with vertical microcode being used less often.
When both vertical and horizontal microcode are used, the horizontal microcode may be referred to as "nanocode" or "picocode".
Writable control store.
A few computers were built using "writable microcode". In this design, rather than storing the microcode in ROM or hard-wired logic, the microcode is stored in a RAM called a "Writable Control Store" or "WCS". Such a computer is sometimes called a "Writable Instruction Set Computer" or "WISC".
Many experimental prototype computers use writable control stores; there are also commercial machines that use writable microcode, such as the Burroughs Small Systems, early Xerox workstations, the DEC VAX 8800 ("Nautilus") family, the Symbolics L- and G-machines, a number of IBM System/360 and System/370 implementations, some DEC PDP-10 machines, and the Data General Eclipse MV/8000.
Many more machines offer user-programmable writable control stores as an option, including the HP 2100, DEC PDP-11/60 and Varian Data Machines V-70 series minicomputers. The IBM System/370 includes a facility called "Initial-Microprogram Load" ("IML" or "IMPL") that can be invoked from the console, as part of "Power On Reset" ("POR") or from another processor in a tightly coupled multiprocessor complex.
Some commercial machines, for example IBM 360/85, have both a Read-only storage and a Writable Control Store for microcode.
WCS offers several advantages including the ease of patching the microprogram and, for certain hardware generations, faster access than ROMs can provide. User-programmable WCS allow the user to optimize the machine for specific purposes.
Several Intel CPUs in the x86 architecture family have writable microcode. This, for example, has allowed bugs in the Intel Core 2 and Intel Xeon microcodes to be fixed by patching their microprograms, rather than requiring the entire chips to be replaced.
Comparison to VLIW and RISC.
The design trend toward heavily microcoded processors with complex instructions began in the early 1960s and continued until roughly the mid-1980s. At that point the RISC design philosophy started becoming more prominent.
A CPU that uses microcode generally takes several clock cycles to execute a single instruction, one clock cycle for each step in the microprogram for that instruction. Some CISC processors include instructions that can take a very long time to execute. Such variations interfere with both interrupt latency and, what is far more important in modern systems, pipelining.
When designing a new processor, a hardwired control RISC has the following advantages over microcoded CISC:
There are counterpoints as well:
Many RISC and VLIW processors are designed to execute every instruction (as long as it is in the cache) in a single cycle. This is very similar to the way CPUs with microcode execute one microinstruction per cycle. VLIW processors have instructions that behave similarly to very wide horizontal microcode, although typically without such fine-grained control over the hardware as provided by microcode. RISC instructions are sometimes similar to the narrow vertical microcode.
Microcoding has been popular in application-specific processors such as network processors.

</doc>
<doc id="20003" url="https://en.wikipedia.org/wiki?curid=20003" title="Multitier architecture">
Multitier architecture

In software engineering, multitier architecture (often referred to as "n"-tier architecture) is a client–server architecture in which presentation, application processing, and data management functions are physically separated. The most widespread use of multitier architecture is the three-tier architecture.
"N"-tier application architecture provides a model by which developers can create flexible and reusable applications. By segregating an application into tiers, developers acquire the option of modifying or adding a specific layer, instead of reworking the entire application. A three-tier architecture is typically composed of a "presentation" tier, a "domain logic" tier, and a "data storage" tier.
While the concepts of layer and tier are often used interchangeably, one fairly common point of view is that there is indeed a difference. This view holds that a "layer" is a logical structuring mechanism for the elements that make up the software solution, while a "tier" is a physical structuring mechanism for the system infrastructure.
Three-tier architecture.
Three-tier architecture is a client–server software architecture pattern in which the user interface (presentation), functional process logic ("business rules"), computer data storage and data access are developed and maintained as independent modules, most often on separate platforms. It was developed by John J. Donovan in Open Environment Corporation (OEC), a tools company he founded in Cambridge, Massachusetts.
Apart from the usual advantages of modular software with well-defined interfaces, the three-tier architecture is intended to allow any of the three tiers to be upgraded or replaced independently in response to changes in requirements or technology. For example, a change of operating system in the "presentation tier" would only affect the user interface code.
Typically, the user interface runs on a desktop PC or workstation and uses a standard graphical user interface, functional process logic that may consist of one or more separate modules running on a workstation or application server, and an RDBMS on a database server or mainframe that contains the computer data storage logic. The middle tier may be multitiered itself (in which case the overall architecture is called an ""n"-tier architecture").
Three-tier architecture:
Web development usage.
In the web development field, three-tier is often used to refer to websites, commonly electronic commerce websites, which are built using three tiers:
Other considerations.
Data transfer between tiers is part of the architecture. Protocols involved may include one or more of SNMP, CORBA, Java RMI, .NET Remoting, Windows Communication Foundation, sockets, UDP, web services or other standard or proprietary protocols. Often middleware is used to connect the separate tiers. Separate tiers often (but not necessarily) run on separate physical servers, and each tier may itself run on a cluster.
Traceability.
The end-to-end traceability of data flows through "n"-tier systems is a challenging task which becomes more important when systems increase in complexity. The Application Response Measurement defines concepts and APIs for measuring performance and correlating transactions between tiers. 
Generally, the term "tiers" is used to describe physical distribution of components of a system on separate servers, computers, or networks (processing nodes). A three-tier architecture then will have three processing nodes. The term "layers" refer to a logical grouping of components which may or may not be physically located on one processing node.

</doc>
<doc id="20016" url="https://en.wikipedia.org/wiki?curid=20016" title="Myrinet">
Myrinet

Myrinet, ANSI/VITA 26-1998, is a high-speed local area networking system designed by Myricom to be used as an interconnect between multiple machines to form computer clusters. Myrinet has much lower protocol overhead than standards such as Ethernet, and therefore provides better throughput, less interference, and lower latency while using the host CPU. Although it can be used as a traditional networking system, Myrinet is often used directly by programs that "know" about it, thereby bypassing a call into the operating system.
Myrinet physically consists of two fibre optic cables, upstream and downstream, connected to the host computers with a single connector. Machines are connected via low-overhead routers and switches, as opposed to connecting one machine directly to another. Myrinet includes a number of fault-tolerance features, mostly backed by the switches. These include flow control, error control, and "heartbeat" monitoring on every link. The newest, "fourth-generation" Myrinet, called Myri-10G, supports a 10 Gbit/s data rate and is interoperable with 10 Gigabit Ethernet on PHY, the physical layer (cables, connectors, distances, signaling). Myri-10G started shipping at the end of 2005.
Performance.
Myrinet is a lightweight protocol with little overhead that allows it to operate with throughput close to the basic signaling speed of the physical layer. On the latest 2.0 Gbit/s links, Myrinet often runs at 1.98 Gbit/s of sustained throughput, considerably better than what Ethernet offers, which varies from 0.6 to 1.9 Gbit/s, depending on load. However, for supercomputing, the low latency of Myrinet is even more important than its throughput performance, since, according to Amdahl's law, a high-performance parallel system tends to be bottlenecked by its slowest sequential process, which in all but the most embarrassingly parallel supercomputer workloads is often the latency of message transmission across the network.
Deployment.
According to Myricom, 141 (28.2%) of the June 2005 TOP500 supercomputers used Myrinet technology. In the November 2005 TOP500, the number of supercomputers using Myrinet was down to 101 computers, or 20.2%, in November 2006, 79 (15.8%), and by November 2007, 18 (3.6%), a long way behind gigabit Ethernet at 54% and Infiniband at 24.2%.
In the latest TOP500 list (June 2014), the number of supercomputers using Myrinet interconnect is 1 (0.2%).

</doc>
<doc id="20017" url="https://en.wikipedia.org/wiki?curid=20017" title="Musique concrète">
Musique concrète

Musique concrète (, meaning "concrete music") is a genre of electroacoustic music that is made in part from acousmatic sound, or sound without an apparent originating cause. It can feature sounds derived from recordings of musical instruments, the human voice, and the natural environment as well as those created using synthesizers and computer-based digital signal processing. Compositions in this idiom are not restricted to the normal musical rules of melody, harmony, rhythm, metre, and so on. Originally contrasted with "pure" "elektronische Musik" (based solely on the production and manipulation of electronically produced sounds rather than recorded sounds), the theoretical basis of "musique concrète" as a compositional practice was developed by Pierre Schaeffer, beginning in the early 1940s.
History.
Beginnings.
In 1928 music critic André Cœuroy wrote in his book "Panorama of Contemporary Music" that "perhaps the time is not far off when a composer will be able to represent through recording, music specifically composed for the gramophone" . In the same period the American composer Henry Cowell, in referring to the projects of Nikolai Lopatnikoff, believed that "there was a wide field open for the composition of music for phonographic discs." This sentiment was echoed further in 1930 by Igor Stravinsky, when he stated in the revue "Kultur und Schallplatte" that "there will be a greater interest in creating music in a way that will be peculiar to the gramophone record." The following year, 1931, Boris de Schloezer also expressed the opinion that one could write for the gramophone or for the wireless just as one can for the piano or the violin . Shortly after, German art theorist Rudolf Arnheim discussed the effects of microphonic recording in an essay entitled "Radio", published in 1936. In it the idea of a creative role for the recording medium was introduced and Arnheim stated that: "The rediscovery of the musicality of sound in noise and in language, and the reunification of music, noise and language in order to obtain a unity of material: that is one of the chief artistic tasks of radio" .
Pierre Schaeffer and Studio d'Essai.
In 1942 French composer and theoretician Pierre Schaeffer began his exploration of radiophony when he joined Jacques Copeau and his pupils in the foundation of the Studio d'Essai de la Radiodiffusion nationale. The studio originally functioned as a center for the Resistance movement in French radio, which in August 1944 was responsible for the first broadcasts in liberated Paris. It was here that Schaeffer began to experiment with creative radiophonic techniques using the sound technologies of the time .
The development of Schaeffer’s practice was informed by encounters with voice actors, and microphone usage and radiophonic art played an important part in inspiring and consolidating Schaeffer's conception of sound-based composition . Another important influence on Schaeffer’s practice was cinema, and the techniques of recording and montage, which were originally associated with cinematographic practice, came to "serve as the substrate of musique concrète." Marc Battier notes that, prior to Schaeffer, Jean Epstein drew attention to the manner in which sound recording revealed what was hidden in the act of basic acoustic listening. Epstein's reference to this "phenomenon of an epiphanic being", which appears through the transduction of sound, proved influential on Schaeffer’s concept of reduced listening. Schaeffer would explicitly cite Jean Epstein with reference to his use of extra-musical sound material. Epstein had already imagined that "through the transposition of natural sounds, it becomes possible to create chords and dissonances, melodies and symphonies of noise, which are a new and specifically cinematographic music" .
Halim El-Dabh's tape music.
Contemporaneous to Schaeffer conducting his preliminary experiments into sound manipulation was the activity of Egyptian composer Halim El-Dabh. As a student in Cairo in the early to mid-1940s he began experimenting with "tape music" using a cumbersome wire recorder. He recorded the sounds of an ancient "zaar" ceremony and at the Middle East Radio studios processed the material using reverberation, echo, voltage controls, and re-recording. The resulting tape-based composition, entitled "The Expression of Zaar," was presented in 1944 at an art gallery event in Cairo. El-Dabh has described his initial activities as an attempt to unlock "the inner sound" of the recordings. While his early compositional work was not widely known outside of Egypt at the time, El-Dabh would eventually gain recognition for his influential work at the Columbia-Princeton Electronic Music Center in the late 1950s .
Club d'Essai and Cinq études de bruits.
Following Schaeffer's work with Studio d'Essai at Radiodiffusion Nationale during the early 1940s he was credited with originating the theory and practice of "musique concrète." The Studio d'Essai was renamed Club d'Essai de la Radiodiffusion-Télévision Française in 1946 and in the same year Schaeffer discussed, in writing, the question surrounding the transformation of time perceived through recording. The essay evidenced knowledge of sound manipulation techniques he would further exploit compositionally. In 1948 Schaeffer formally initiated "research in to noises" at the Club d'Essai and on 5 October 1948 the results of his initial experimentation were premiered at a concert given in Paris . Five works for phonograph (known collectively as "Cinq études de bruits"—Five Studies of Noises) including "Etude violette" ("Study in Purple") and "Etude aux chemins de fer" (Study of the Railroads), were presented.
Musique concrète.
By 1949 Schaeffer's compositional work was known publicly as "musique concrète" . Schaeffer stated: "when I proposed the term 'musique concrète,' I intended … to point out an opposition with the way musical work usually goes. Instead of notating musical ideas on paper with the symbols of solfege and entrusting their realization to well-known instruments, the question was to collect concrete sounds, wherever they came from, and to abstract the musical values they were potentially containing" . According to Pierre Henry, "musique concrète was not a study of timbre, it is focused on envelopes, forms. It must be presented by means of non-traditional characteristics, you see … one might say that the origin of this music is also found in the interest in ‘plastifying’ music, of rendering it plastic like sculpture…musique concrète, in my opinion … led to a manner of composing, indeed, a new mental framework of composing" . Schaeffer had developed an aesthetic that was centred upon the use of sound as a primary compositional resource. The aesthetic also emphasised the importance of play ("jeu") in the practice of sound based composition. Schaeffer's use of the word "jeu", from the verb "jouer", carries the same double meaning as the English verb play: 'to enjoy oneself by interacting with one's surroundings', as well as 'to operate a musical instrument' .
Groupe de Recherche de Musique Concrète.
By 1951 the work of Schaeffer, composer-percussionist Pierre Henry, and sound engineer Jacques Poullin had received official recognition and The Groupe de Recherches de Musique Concrète, Club d 'Essai de la Radiodiffusion-Télévision Française was established at RTF in Paris, the ancestor of the ORTF . At RTF the GRMC established the first purpose-built electroacoustic music studio. It quickly attracted many who either were or were later to become notable composers, including Olivier Messiaen, Pierre Boulez, Jean Barraqué, Karlheinz Stockhausen, Edgard Varèse, Iannis Xenakis, Michel Philippot, and Arthur Honegger. Compositional output from 1951 to 1953 comprised "Étude I" (1951) and "Étude II" (1951) by Boulez, "Timbres-durées" (1952) by Messiaen, "Konkrete Etüde" (1952) by Stockhausen, "Le microphone bien tempéré" (1952) and "La voile d’Orphée" (1953) by Henry, "Étude I" (1953) by Philippot, "Étude" (1953) by Barraqué, the mixed pieces "Toute la lyre" (1951) and "Orphée 53" (1953) by Schaeffer/Henry, and the film music "Masquerage" (1952) by Schaeffer and "Astrologie" (1953) by Henry. In 1954 Varèse and Honegger visited to work on the tape parts of "Déserts" and "La rivière endormie" .
In the early and mid 1950s Schaeffer's commitments to RTF included official missions which often required extended absences from the studios. This led him to invest Philippe Arthuys with responsibility for the GRMC in his absence, with Pierre Henry operating as Director of Works. Pierre Henry’s composing talent developed greatly during this period at the GRMC and he worked with experimental filmmakers such as Max de Haas, Jean Gremillon, Enrico Fulchignoni, and Jean Rouch, and with choreographers including Dick Sanders and Maurice Béjart . Schaeffer returned to run the group at the end of 1957, and immediately stated his disapproval of the direction the GRMC had taken. A proposal was then made to "renew completely the spirit, the methods and the personnel of the Group, with a view to undertake research and to offer a much needed welcome to young composers" .
Groupe de Recherches Musicales.
Following the emergence of differences within the GRMC Pierre Henry, Philippe Arthuys, and several of their colleagues, resigned in April 1958. Schaeffer created a new collective, called Groupe de Recherches Musicales (GRM) and set about recruiting new members including Luc Ferrari, François-Bernard Mâche, Iannis Xenakis, Bernard Parmegiani, and Mireille Chamass-Kyrou. Later arrivals included Ivo Malec, Philippe Carson, Romuald Vandelle, Edgardo Canton and François Bayle .
GRM was one of several theoretical and experimental groups working under the umbrella of the Schaeffer-led Service de la Recherche at ORTF (1960–74). Together with the GRM, three other groups existed: the Groupe de Recherches Image GRI, the Groupe de Recherches Technologiques GRT and the Groupe de Recherches which became the Groupe d’Etudes Critiques . Communication was the one theme that unified the various groups, all of which were devoted to production and creation. In terms of the question "who says what to whom?" Schaeffer added "how?", thereby creating a platform for research into audiovisual communication and mass media, audible phenomena and music in general (including non-Western musics) (Beatriz Ferreyra, new preface to Schaeffer and Reibel 1967, reedition of 1998, 9). At the GRM the theoretical teaching remained based on practice and could be summed up in the catch phrase "‘do and listen’" .
Schaeffer kept up a practice established with the GRMC of delegating the functions (though not the title) of Group Director to colleagues. Since 1961 GRM has had six Group Directors: Michel Philippot (1960–61), Luc Ferrari (1962–63), Bernard Baschet and François Vercken (1964–66). From the beginning of 1966, François Bayle took over the direction for the duration of thirty-one years, to 1997. He was then replaced by Daniel Teruggi .
Traité des objets musicaux.
The group continued to refine Schaeffer's ideas and strengthened the concept of "musique acousmatique" . Schaeffer had borrowed the term acousmatic from Pythagoras and defined it as: ""Acousmatic, adjective: referring to a sound that one hears without seeing the causes behind it"" . In 1966 Schaeffer published the book "Traité des objets musicaux" (Treatise on Musical Objects) which represented the culmination of some 20 years of research in the field of "musique concrète". In conjunction with this publication, a set of sound recordings was produced, entitled "Le solfège de l'objet sonore" (Music Theory of the Acoustic Object), to provide examples of concepts dealt with in the treatise.
Technology.
The development of musique concrète was facilitated by the emergence of new music technology in post-war Europe. Access to microphones, phonographs, and later magnetic tape recorders (created in 1939 and acquired by the Schaeffer's Groupe de Recherche de Musique Concrète (Research Group on Concrete Music) in 1952), facilitated by an association with the French national broadcasting organization, at that time the Radiodiffusion-Télévision Française, gave Schaeffer and his colleagues an opportunity to experiment with recording technology and tape manipulation.
Initial tools of musique concrète.
In 1948, a typical radio studio consisted of a series of shellac record players, a shellac record recorder, a mixing desk with rotating potentiometers, mechanical reverberation, filters, and microphones. This technology made a number of limited operations available to a composer (, ):
The application of the above technologies in the creation of musique concrete led to the development of a number of sound manipulation techniques including (, ):
Magnetic tape.
The first tape recorders started arriving at ORTF in 1949; however, their functioning was much less reliable than the shellac players, to the point that the "Symphonie pour un homme seul", which was composed in 1950–51, was mainly composed with records, even if the tape recorder was available . In 1950, when the machines finally functioned correctly, the techniques of musique concrete were expanded. A range of new sound manipulation practices were explored using improved media manipulation methods and operations such as speed variation. A completely new possibility of organising sounds appears with tape editing, which permits tape to be spliced and arranged with an extraordinary new precision. The ‘axe-cut junctions’ were replaced with micrometric junctions and a whole new technique of production, less dependency on performance skills, could be developed. Tape editing brought a new technique called ‘micro-editing’, in which very tiny fragments of sound, representing milliseconds of time, were edited together, thus creating completely new sounds or structures .
Development of novel devices.
During the GRMC period from 1951–1958 time Schaeffer and Jacques Poullin developed a number of novel sound creation tools including a three-track tape recorder, a machine with ten playback heads to replay tape loops in echo (the morphophone), a keyboard-controlled machine to replay tape loops at twenty-four preset speeds (the keyboard, chromatic, or Tolana phonogène), a slide-controlled machine to replay tape loops at a continuously variable range of speeds (the handle, continuous, or Sareg phonogène), and a device to distribute an encoded track across four loudspeakers, including one hanging from the centre of the ceiling (the potentiomètre d’espace) .
The phonogène.
Speed variation was a powerful tool for sound design applications. It had been identified that transformations brought about by varying playback speed lead to modification in the character of the sound material:
The phonogène was a machine capable of modifying sound structure significantly and it provided composers with a means to adapt sound to meet specific compositional contexts. The initial phonogènes were manufactured in 1953 by two subcontractors: the chromatic phonogène by a company called Tolana, and the sliding version by the SAREG Company . A third version was developed later at ORTF. An outline of the unique capabilities of the various phonogènes can be seen here:
The three-head tape recorder.
This original tape recorder was one of the first machines permitting the simultaneous listening of several synchronised sources. Until 1958 musique concrète, radio and the studio machines were monophonic. The three-head tape recorder superposed three magnetic tapes that were dragged by a common motor, each tape having an independent spool. The objective was to keep the three tapes synchronised from a common starting point. Works could then be conceived polyphonically, and thus each head conveyed a part of the information and was listened to through a dedicated loudspeaker. It was an ancestor of the multi-track player (four then eight tracks) that appeared in the 1960s. "Timbres Durées" by Olivier Messiaen with the technical assistance of Pierre Henry was the first work composed for this tape recorder in 1952. A rapid rhythmic polyphony was distributed over the three channels .
The morphophone.
This machine was conceived to build complex forms through repetition, and accumulation of events through delays, filtering and feedback. It consisted of a large rotating disk, 50 cm in diameter, on which was stuck a tape with its magnetic side facing outward. A series of twelve movable magnetic heads (one each recording head and erasing head, and ten playback heads) were positioned around the disk, in contact with the tape. A sound up to four seconds long could be recorded on the looped tape and the ten playback heads would then read the information with different delays, according to their (adjustable) positions around the disk. A separate amplifier and band-pass filter for each head could modify the spectrum of the sound, and additional feedback loops could transmit the information to the recording head. The resulting repetitions of a sound occurred at different time intervals, and could be filtered or modified through feedback. This system was also easily capable of producing artificial reverberation or continuous sounds .
Early sound spatialisation system.
At the premiere of Pierre Schaeffer's "Symphonie pour un homme seul" in 1951, a system that was designed for the spatial control of sound was tested. It was called a "relief desk" ("pupitre de relief", but also referred to as "pupitre d'espace" or "potentiomètre d'espace") and was intended to control the dynamic level of music played from several shellac players. This created a stereophonic effect by controlling the positioning of a monophonic sound source . One of five tracks, provided by a purpose-built tape machine, was controlled by the performer and the other four tracks each supplied a single loudspeaker. This provided a mixture of live and preset sound positions . The placement of loudspeakers in the performance space included two loudspeakers at the front right and left of the audience, one placed at the rear, and in the centre of the space a loudspeaker was placed in a high position above the audience. The sounds could therefore be moved around the audience, rather than just across the front stage. On stage, the control system allowed a performer to position a sound either to the left or right, above or behind the audience, simply by moving a small, hand held transmitter coil towards or away from four somewhat larger receiver coils arranged around the performer in a manner reflecting the loudspeaker positions . A contemporary eyewitness described the "potentiomètre d'espace" in normal use:
One found one’s self sitting in a small studio which was equipped with four loudspeakers—two in front of one—right and left; one behind one and a fourth suspended above. In the front center were four large loops and an “executant” moving a small magnetic unit through the air. The four loops controlled the four speakers, and while all four were giving off sounds all the time, the distance of the unit from the loops determined the volume of sound sent out from each.<br>The music thus came to one at varying intensity from various parts of the room, and this “spatial projection” gave new sense to the rather abstract sequence of sound originally recorded. The central concept underlying this method was the notion that music should be controlled during public presentation in order to create a performance situation; an attitude that has stayed with acousmatic music to the present day .
The Coupigny synthesiser and Studio 54 mixing desk.
After the longstanding rivalry with the "electronic music" of the Cologne studio had subsided, in 1970 the GRM finally created an electronic studio using tools developed by the physicist Enrico Chiarucci, called the Studio 54, which featured the "Coupigny modular synthesiser" and a Moog synthesiser with VCA . The Coupigny synthesiser, named for its designer François Coupigny, director of the Group for Technical Research (Battier 2007, 200), and the Studio 54 mixing desk had a major influence on the evolution of GRM and from the point of their introduction on they brought a new quality to the music . The mixing desk and synthesiser were combined in one unit and were created specifically for the creation of musique concrète.
The design of the desk was influenced by trade union rules at French National Radio that required technicians and production staff to have clearly defined duties. The solitary practice of musique concrète composition did not suit a system that involved three operators: one in charge of the machines, a second controlling the mixing desk, and third to provide guidance to the others. Because of this the synthesiser and desk were combined and organised in a manner that allowed it to be used easily by a composer. Independently of the mixing tracks (twenty-four in total), it had a coupled connection patch that permitted the organisation of the machines within the studio. It also had a number of remote controls for operating tape recorders. The system was easily adaptable to any context, particularly that of interfacing with external equipment .
Before the late 1960s the musique concrète produced at GRM had largely been based on the recording and manipulation of sounds, but synthesised sounds had featured in a number of works prior to the introduction of the Coupigny. Pierre Henry had used oscillators to produce sounds as early as 1955. But a synthesiser with parametrical control was something Pierre Schaeffer was against, since it favoured the preconception of music and therefore deviated from Schaeffer's principal of "‘making through listening’" . Because of Schaeffer's concerns the Coupigny synthesiser was conceived as a sound-event generator with parameters controlled globally, without a means to define values as precisely as some other synthesisers of the day .
The development of the machine was constrained by several factors. It needed to be modular and the modules had to be easily interconnected (so that the synthesiser would have more modules than slots and it would have an easy-to-use patch). It also needed to include all the major functions of a modular synthesiser including oscillators, noise-generators, filters, ring-modulators, but an intermodulation facility was viewed as the primary requirement; to enable complex synthesis processes such as frequency modulation, amplitude modulation, and modulation via an external source. No keyboard was attached to the synthesiser and instead a specific and somewhat complex envelope generator was used to shape sound. This synthesiser was well-adapted to the production of continuous and complex sounds using intermodulation techniques such as cross-synthesis and frequency modulation but was less effective in generating precisely defined frequencies and triggering specific sounds .
The Coupigny synthesiser also served as the model for a smaller, portable unit, which has been used down to the present day .
The Acousmonium.
In 1966 composer and technician François Bayle was placed in charge of the Groupe de Recherches Musicales and in 1975, GRM was integrated with the new Institut national de l'audiovisuel (INA - Audiovisual National Institute) with Bayle as its head. In taking the lead on work that began in the early 1950s, with Jacques Poullin's potentiomètre d’espace, a system designed to move monophonic sound sources across four speakers, Bayle and the engineer Jean-Claude Lallemand created an orchestra of loudspeakers ("un orchestre de haut-parleurs") known as the Acousmonium in 1974 . An inaugural concert took place on 14 February 1974 at the Espace Pierre Cardin in Paris with a presentation of Bayle's "Expérience acoustique" .
The Acousmonium is a specialised sound reinforcement system consisting of between 50 and 100 loudspeakers, depending on the character of the concert, of varying shape and size. The system was designed specifically for the concert presentation of musique-concrète-based works but with the added enhancement of sound spatialisation. Loudspeakers are placed both on stage and at positions throughout the performance space and a mixing console is used to manipulate the placement of acousmatic material across the speaker array, using a performative technique known as "sound diffusion" . Bayle has commented that the purpose of the Acousmonium is to ""substitute a momentary classical disposition of sound making, which diffuses the sound from the circumference towards the centre of the hall, by a group of sound projectors which form an ‘orchestration’ of the acoustic image"" .
As of 2010, the Acousmonium was still performing, with 64 speakers, 35 amplifiers, and 2 consoles .

</doc>
<doc id="20018" url="https://en.wikipedia.org/wiki?curid=20018" title="Metric space">
Metric space

In mathematics, a metric space is a set for which distances between all members of the set are defined. Those distances, taken together, are called a metric on the set. A metric on a space induces topological properties like open and closed sets, which lead to the study of more abstract topological spaces.
The most familiar metric space is 3-dimensional Euclidean space. In fact, a "metric" is the generalization of the Euclidean metric arising from the four long-known properties of the Euclidean distance. The Euclidean metric defines the distance between two points as the length of the straight line segment connecting them. Other metric spaces occur for example in elliptic geometry and hyperbolic geometry, where distance on a sphere measured by angle is a metric, and the hyperboloid model of hyperbolic geometry is used by special relativity as a metric space of velocities.
History.
Maurice Fréchet introduced metric spaces in his work "Sur quelques points du calcul fonctionnel", Rendic. Circ. Mat. Palermo 22 (1906) 1–74.
Definition.
A metric space is an ordered pair formula_1 where formula_2 is a set and formula_3 is a metric on formula_2, i.e., a function
such that for any formula_6, the following holds:
The first condition follows from the other three. Since for any formula_7:
The function formula_3 is also called "distance function" or simply "distance". Often, formula_3 is omitted and one just writes formula_2 for a metric space if it is clear from the context what metric is used.
Ignoring mathematical details, for any system of roads and terrains the distance between two locations can be defined as the length of the shortest route connecting those locations. To be a metric there shouldn't be any one-way roads. The triangle inequality expresses the fact that detours aren't shortcuts. Many of the examples below can be seen as concrete versions of this general idea.
Open and closed sets, topology and convergence.
Every metric space is a topological space in a natural manner, and therefore all definitions and theorems about general topological spaces also apply to all metric spaces.
About any point formula_14 in a metric space formula_2 we define the open ball of radius formula_77 (where formula_78 is a real number) about formula_14 as the set
These open balls form the base for a topology on "M", making it a topological space.
Explicitly, a subset formula_81 of formula_2 is called open if for every formula_14 in formula_81 there exists an formula_77 such that formula_86 is contained in formula_81. The complement of an open set is called closed. A neighborhood of the point formula_14 is any subset of formula_2 that contains an open ball about formula_14 as a subset.
A topological space which can arise in this way from a metric space is called a metrizable space; see the article on metrization theorems for further details.
A sequence (formula_91) in a metric space formula_2 is said to converge to the limit formula_93 iff for every formula_94, there exists a natural number "N" such that formula_95 for all formula_96. Equivalently, one can use the general definition of convergence available in all topological spaces.
A subset formula_68 of the metric space formula_2 is closed iff every sequence in formula_68 that converges to a limit in formula_2 has its limit in formula_68.
Types of metric spaces.
Complete spaces.
A metric space formula_2 is said to be complete if every Cauchy sequence converges in formula_2. That is to say: if formula_104 as both formula_73 and formula_72 independently go to infinity, then there is some formula_107 with formula_108.
Every Euclidean space is complete, as is every closed subset of a complete space. The rational numbers, using the absolute value metric formula_109, are not complete.
Every metric space has a unique (up to isometry) completion, which is a complete space that contains the given space as a dense subset. For example, the real numbers are the completion of the rationals.
If formula_30 is a complete subset of the metric space formula_2, then formula_30 is closed in formula_2. Indeed, a space is complete iff it is closed in any containing metric space.
Every complete metric space is a Baire space.
Bounded and totally bounded spaces.
A metric space "M" is called bounded if there exists some number "r", such that "d"("x","y") ≤ "r" for all "x" and "y" in "M". The smallest possible such "r" is called the diameter of "M". The space "M" is called precompact or totally bounded if for every "r" > 0 there exist finitely many open balls of radius "r" whose union covers "M". Since the set of the centres of these balls is finite, it has finite diameter, from which it follows (using the triangle inequality) that every totally bounded space is bounded. The converse does not hold, since any infinite set can be given the discrete metric (one of the examples above) under which it is bounded and yet not totally bounded.
Note that in the context of intervals in the space of real numbers and occasionally regions in a Euclidean space Rn a bounded set is referred to as "a finite interval" or "finite region". However boundedness should not in general be confused with "finite", which refers to the number of elements, not to how far the set extends; finiteness implies boundedness, but not conversely. Also note that an unbounded subset of Rn may have a finite volume.
Compact spaces.
A metric space "M" is compact if every sequence in "M" has a subsequence that converges to a point in "M". This is known as sequential compactness and, in metric spaces (but not in general topological spaces), is equivalent to the topological notions of countable compactness and compactness defined via open covers.
Examples of compact metric spaces include the closed interval [0,1] with the absolute value metric, all metric spaces with finitely many points, and the Cantor set. Every closed subset of a compact space is itself compact.
A metric space is compact iff it is complete and totally bounded. This is known as the Heine–Borel theorem. Note that compactness depends only on the topology, while boundedness depends on the metric.
Lebesgue's number lemma states that for every open cover of a compact metric space "M", there exists a "Lebesgue number" δ such that every subset of "M" of diameter < δ is contained in some member of the cover.
Every compact metric space is second countable, and is a continuous image of the Cantor set. (The latter result is due to Pavel Alexandrov and Urysohn.)
Locally compact and proper spaces.
A metric space is said to be locally compact if every point has a compact neighborhood. Euclidean spaces are locally compact, but infinite-dimensional Banach spaces are not.
A space is proper if every closed ball {"y" : "d"("x","y") ≤ "r"} is compact. Proper spaces are locally compact, but the converse is not true in general.
Connectedness.
A metric space formula_2 is connected if the only subsets that are both open and closed are the empty set and formula_2 itself.
A metric space formula_2 is path connected if for any two points formula_7 there exists a continuous map formula_118 with formula_119 and formula_120.
Every path connected space is connected, but the converse is not true in general.
There are also local versions of these definitions: locally connected spaces and locally path connected spaces.
Simply connected spaces are those that, in a certain sense, do not have "holes".
Separable spaces.
A metric space is separable space if it has a countable dense subset. Typical examples are the real numbers or any Euclidean space. For metric spaces (but not for general topological spaces) separability is equivalent to second countability and also to the Lindelöf property.
Types of maps between metric spaces.
Suppose ("M"1,"d"1) and ("M"2,"d"2) are two metric spaces.
Continuous maps.
The map "f":"M"1→"M"2 is continuous
if it has one (and therefore all) of the following equivalent properties:
Moreover, "f" is continuous if and only if it is continuous on every compact subset of "M"1.
The image of every compact set under a continuous function is compact, and the image of every connected set under a continuous function is connected.
Uniformly continuous maps.
The map "ƒ" : "M"1 → "M"2 is uniformly continuous if for every "ε" > 0 there exists "δ" > 0 such that
Every uniformly continuous map "ƒ" : "M"1 → "M"2 is continuous. The converse is true if "M"1 is compact (Heine–Cantor theorem).
Uniformly continuous maps turn Cauchy sequences in "M"1 into Cauchy sequences in "M"2. For continuous maps this is generally wrong; for example, a continuous map
from the open interval (0,1) "onto" the real line turns some Cauchy sequences into unbounded sequences.
Lipschitz-continuous maps and contractions.
Given a number "K" > 0, the map "ƒ" : "M"1 → "M"2 is "K"-Lipschitz continuous if
Every Lipschitz-continuous map is uniformly continuous, but the converse is not true in general.
If "K" < 1, then "ƒ" is called a contraction. Suppose "M"2 = "M"1 and "M"1 is complete. If "ƒ" is a contraction, then "ƒ" admits a unique fixed point (Banach fixed point theorem). If "M"1 is compact, the condition can be weakened a bit: "ƒ" admits a unique fixed point if
Isometries.
The map "f":"M"1→"M"2 is an isometry if
Isometries are always injective; the image of a compact or complete set under an isometry is compact or complete, respectively. However, if the isometry is not surjective, then the image of a closed (or open) set need not be closed (or open).
Quasi-isometries.
The map "f" : "M"1 → "M"2 is a quasi-isometry if there exist constants "A" ≥ 1 and "B" ≥ 0 such that
and a constant "C" ≥ 0 such that every point in "M"2 has a distance at most "C" from some point in the image "f"("M"1).
Note that a quasi-isometry is not required to be continuous. Quasi-isometries compare the "large-scale structure" of metric spaces; they find use in geometric group theory in relation to the word metric.
Notions of metric space equivalence.
Given two metric spaces ("M"1, "d"1) and ("M"2, "d"2):
Topological properties.
Metric spaces are paracompact Hausdorff spaces and hence normal (indeed they are perfectly normal). An important consequence is that every metric space admits partitions of unity and that every continuous real-valued function defined on a closed subset of a metric space can be extended to a continuous map on the whole space (Tietze extension theorem). It is also true that every real-valued Lipschitz-continuous map defined on a subset of a metric space can be extended to a Lipschitz-continuous map on the whole space.
Metric spaces are first countable since one can use balls with rational radius as a neighborhood base.
The metric topology on a metric space "M" is the coarsest topology on "M" relative to which the metric "d" is a continuous map from the product of "M" with itself to the non-negative real numbers.
Distance between points and sets; Hausdorff distance and Gromov metric.
A simple way to construct a function separating a point from a closed set (as required for a completely regular space) is to consider the distance between the point and the set. If ("M","d") is a metric space, "S" is a subset of "M" and "x" is a point of "M", we define the distance from "x" to "S" as
Then "d"("x", "S") = 0 if and only if "x" belongs to the closure of "S". Furthermore, we have the following generalization of the triangle inequality:
which in particular shows that the map formula_130 is continuous.
Given two subsets "S" and "T" of "M", we define their Hausdorff distance to be
In general, the Hausdorff distance "d"H("S","T") can be infinite. Two sets are close to each other in the Hausdorff distance if every element of either set is close to some element of the other set.
The Hausdorff distance "d"H turns the set "K"("M") of all non-empty compact subsets of "M" into a metric space. One can show that "K"("M") is complete if "M" is complete.
One can then define the Gromov–Hausdorff distance between any two metric spaces by considering the minimal Hausdorff distance of isometrically embedded versions of the two spaces. Using this distance, the class of all (isometry classes of) compact metric spaces becomes a metric space in its own right.
Product metric spaces.
If formula_133 are metric spaces, and "N" is the Euclidean norm on "Rn", then formula_134 is a metric space, where the product metric is defined by
and the induced topology agrees with the product topology. By the equivalence of norms in finite dimensions, an equivalent metric is obtained if "N" is the taxicab norm, a p-norm, the max norm, or any other norm which is non-decreasing as the coordinates of a positive "n"-tuple increase (yielding the triangle inequality).
Similarly, a countable product of metric spaces can be obtained using the following metric
An uncountable product of metric spaces need not be metrizable. For example, formula_137 is not first-countable and thus isn't metrizable.
Continuity of distance.
In the case of a single space formula_1, the distance map formula_139 (from the definition) is uniformly continuous with respect to any of the above product metrics formula_140, and in particular is continuous with respect to the product topology of formula_141.
Quotient metric spaces.
If "M" is a metric space with metric "d", and "~" is an equivalence relation on "M", then we can endow the quotient set "M/~" with the following (pseudo)metric. Given two equivalence classes ["x"] and ["y"], we define
where the infimum is taken over all finite sequences formula_143 and formula_144 with formula_145. In general this will only define a pseudometric, i.e. formula_146. However, for nice equivalence relations (e.g., those given by gluing together polyhedra along faces), it is a metric.
The quotient metric "d" is characterized by the following universal property. If formula_147 is a metric map between metric spaces (that is, formula_148 for all "x", "y") satisfying "f"("x")="f"("y") whenever formula_149 then the induced function formula_150, given by formula_151, is a metric map formula_152
A topological space is sequential if and only if it is a quotient of a metric space.
Generalizations of metric spaces.
Metric spaces as enriched categories.
The ordered set formula_154 can be seen as a category by requesting exactly one morphism formula_155 if formula_156 and none otherwise. By using formula_157 as the tensor product and formula_23 as the identity, it becomes a monoidal category formula_159.
Every metric space formula_1 can now be viewed as a category formula_161 enriched over formula_159:
See the paper by F.W. Lawvere listed below.
References.
This is reprinted (with author commentary) at Reprints in Theory and Applications of Categories
Also (with an author commentary) in Enriched categories in the logic of geometry and analysis. Repr. Theory Appl. Categ. No. 1 (2002), 1–37.

</doc>
<doc id="20021" url="https://en.wikipedia.org/wiki?curid=20021" title="Marine biology">
Marine biology

Marine biology is the scientific study of organisms in the ocean or other marine bodies of water. Given that in biology many phyla, families and genera have some species that live in the sea and others that live on land, marine biology classifies species based on the environment rather than on taxonomy. Marine biology differs from marine ecology as marine ecology is focused on how organisms interact with each other and the environment, while biology is the study of the organisms themselves.
A large proportion of all life on Earth lives in the ocean. Exactly how large the proportion is unknown, since many ocean species are still to be discovered. The ocean is a complex three-dimensional world covering approximately 71% of the Earth's surface. The habitats studied in marine biology include everything from the tiny layers of surface water in which organisms and abiotic items may be trapped in surface tension between the ocean and atmosphere, to the depths of the oceanic trenches, sometimes 10,000 meters or more beneath the surface of the ocean. Specific habitats include coral reefs, kelp forests, seagrass meadows, the surrounds of seamounts and thermal vents, tidepools, muddy, sandy and rocky bottoms, and the open ocean (pelagic) zone, where solid objects are rare and the surface of the water is the only visible boundary. The organisms studied range from microscopic phytoplankton and zooplankton to huge cetaceans (whales) in length.
Marine life is a vast resource, providing food, medicine, and raw materials, in addition to helping to support recreation and tourism all over the world. At a fundamental level, marine life helps determine the very nature of our planet. Marine organisms contribute significantly to the oxygen cycle, and are involved in the regulation of the Earth's climate. Shorelines are in part shaped and protected by marine life, and some marine organisms even help create new land.
Many species are economically important to humans, including both finfish and shellfish. It is also becoming understood that the well-being of marine organisms and other organisms are linked in fundamental ways. The human body of knowledge regarding the relationship between life in the sea and important cycles is rapidly growing, with new discoveries being made nearly every day. These cycles include those of matter (such as the carbon cycle) and of air (such as Earth's respiration, and movement of energy through ecosystems including the ocean). Large areas beneath the ocean surface still remain effectively unexplored.
History.
Early instances of the study of marine biology trace back to Aristotle (384–322 BC) who made several contributions which laid the foundation for many future discoveries and were the first big step in the early exploration period of the ocean and marine life. In 1768, Samuel Gottlieb Gmelin (1744–1774) published the "Historia Fucorum", the first work dedicated to marine algae and the first book on marine biology to use the then new binomial nomenclature of Linnaeus. It included elaborate illustrations of seaweed and marine algae on folded leaves. The British naturalist Edward Forbes (1815–1854) is generally regarded as the founder of the science of marine biology. The pace of oceanographic and marine biology studies quickly accelerated during the course of the 19th century.
The observations made in the first studies of marine biology fueled the age of discovery and exploration that followed. During this time, a vast amount of knowledge was gained about the life that exists in the oceans of the world. Many voyages contributed significantly to this pool of knowledge. Among the most significant were the voyages of the HMS "Beagle" where Charles Darwin came up with his theories of evolution and on the formation of coral reefs. Another important expedition was undertaken by HMS "Challenger", where findings were made of unexpectedly high species diversity among fauna stimulating much theorizing by population ecologists on how such varieties of life could be maintained in what was thought to be such a hostile environment. This era was important for the history of marine biology but naturalists were still limited in their studies because they lacked technology that would allow them to adequately examine species that lived in deep parts of the oceans.
The creation of marine laboratories was important because it allowed marine biologists to conduct research and process their specimens from expeditions. The oldest marine laboratory in the world, Station biologique de Roscoff, was established in France in 1872. In the United States, the Scripps Institution of Oceanography dates back to 1903, while the prominent Woods Hole Oceanographic Institute was founded in 1930. The development of technology such as sound navigation ranging, scuba diving gear, submersibles and remotely operated vehicles allowed marine biologists to discover and explore life in deep oceans that was once thought to not exist.
Subfields.
The marine ecosystem is large, and thus there are many sub-fields of marine biology. Most involve studying specializations of particular animal groups, such as phycology, invertebrate zoology and ichthyology.
Other subfields study the physical effects of continual immersion in sea water and the ocean in general, adaptation to a salty environment, and the effects of changing various oceanic properties on marine life. A subfield of marine biology studies the relationships between oceans and ocean life, and global warming and environmental issues (such as carbon dioxide displacement).
Recent marine biotechnology has focused largely on marine biomolecules, especially proteins, that may have uses in medicine or engineering. Marine environments are the home to many exotic biological materials that may inspire biomimetic materials.
Related fields.
Marine biology is a branch of biology. It is closely linked to oceanography and may be regarded as a sub-field of marine science. It also encompasses many ideas from ecology. Fisheries science and marine conservation can be considered partial offshoots of marine biology (as well as environmental studies). Marine Chemistry, Physical oceanography and Atmospheric sciences are closely related to this field.
Animals.
Birds.
Birds adapted to living in the marine environment are often called seabirds. Examples include albatross, penguins, gannets, and auks. Although they spend most of their lives in the ocean, species such as gulls can often be found thousands of miles inland.
Fish.
Fish anatomy includes a two-chambered heart, operculum, swim bladder, scales, fins, lips, eyes and secretory cells that produce mucous. Fish breathe by extracting oxygen from water through their gills. Fins propel and stabilize the fish in the water. Many fish fall under two major categories - Elasmobranchii and Teleostei.
A reported 32,700 species of fish have been described (as of December 2013), more than the combined total of all other vertebrates. About 60% of fish species are saltwater fish.
Invertebrates.
As on land, invertebrates make up a huge portion of all life in the sea. Invertebrate sea life includes Cnidaria such as jellyfish and sea anemones; Ctenophora; sea worms including the phyla Platyhelminthes, Nemertea, Annelida, Sipuncula, Echiura, Chaetognatha, and Phoronida; Mollusca including shellfish, squid, octopus; Arthropoda including Chelicerata and Crustacea; Porifera; Bryozoa; Echinodermata including starfish; and Urochordata including sea squirts or tunicates.
Mammals.
There are five main types of marine mammals.
Reptiles.
Reptiles which inhabit or frequent the sea include sea turtles, sea snakes, terrapins, the marine iguana, and the saltwater crocodile. Most extant marine reptiles, except for some sea snakes, are oviparous and need to return to land to lay their eggs. Thus most species, excepting sea turtles, spend most of their lives on or near land rather than in the ocean. Despite their marine adaptations, most sea snakes prefer shallow waters nearby land, around islands, especially waters that are somewhat sheltered, as well as near estuaries. Some extinct marine reptiles, such as ichthyosaurs, evolved to be viviparous and had no requirement to return to land.
Fungi.
Over 1500 species of fungi are known from marine environments. These parasitic marine algae or animals, or are saprobes on algae, corals, protozoan cysts, sea grasses, wood and other substrata, and can also be found in sea foam. Spores of many species have special appendages which facilitate attachment to the substratum. A very diverse range of unusual secondary metabolites is produced by marine fungi.
Plants and algae.
Microscopic algae and plants provide important habitats for life, sometimes acting as hiding and foraging places for larval forms of larger fish and invertebrates.
Algal life is widespread and very diverse under the ocean. Microscopic photosynthetic algae contribute a larger proportion of the world's photosynthetic output than all the terrestrial forests combined. Most of the niche occupied by sub plants on land is actually occupied by macroscopic algae in the ocean, such as "Sargassum" and kelp, which are commonly known as seaweeds that creates kelp forests.
Plants that survive in the sea are often found in shallow waters, such as the seagrasses (examples of which are eelgrass, "Zostera", and turtle grass, "Thalassia"). These plants have adapted to the high salinity of the ocean environment. The intertidal zone is also a good place to find plant life in the sea, where mangroves or cordgrass or beach grass might grow. Microscopic algae and plants provide important habitats for life, sometimes acting as hiding and foraging places for larval forms of larger fish and invertebrates.
Microscopic life.
As inhabitants of the largest environment on Earth, microbial marine systems drive changes in every global system. Microbes are responsible for virtually all the photosynthesis that occurs in the ocean, as well as the cycling of carbon, nitrogen, phosphorus and other nutrients and trace elements.
Microscopic life undersea is incredibly diverse and still poorly understood. For example, the role of viruses in marine ecosystems is barely being explored even in the beginning of the 21st century.
The role of phytoplankton is better understood due to their critical position as the most numerous primary producers on Earth. Phytoplankton are categorized into cyanobacteria (also called blue-green algae/bacteria), various types of algae (red, green, brown, and yellow-green), diatoms, dinoflagellates, euglenoids, coccolithophorids, cryptomonads, chrysophytes, chlorophytes, prasinophytes, and silicoflagellates.
Zooplankton tend to be somewhat larger, and not all are microscopic. Many Protozoa are zooplankton, including dinoflagellates, zooflagellates, foraminiferans, and radiolarians. Some of these (such as dinoflagellates) are also phytoplankton; the distinction between plants and animals often breaks down in very small organisms. Other zooplankton include cnidarians, ctenophores, chaetognaths, molluscs, arthropods, urochordates, and annelids such as polychaetes. Many larger animals begin their life as zooplankton before they become large enough to take their familiar forms. Two examples are fish larvae and sea stars (also called starfish).
Marine habitats.
Marine habitats can be divided into coastal and open ocean habitats. Coastal habitats are found in the area that extends from the shoreline to the edge of the continental shelf. Most marine life is found in coastal habitats, even though the shelf area occupies only seven percent of the total ocean area. Open ocean habitats are found in the deep ocean beyond the edge of the continental shelf
Alternatively, marine habitats can be divided into pelagic and demersal habitats. Pelagic habitats are found near the surface or in the open water column, away from the bottom of the ocean. Demersal habitats are near or on the bottom of the ocean. An organism living in a pelagic habitat is said to be a pelagic organism, as in pelagic fish. Similarly, an organism living in a demersal habitat is said to be a demersal organism, as in demersal fish. Pelagic habitats are intrinsically shifting and ephemeral, depending on what ocean currents are doing.
Marine habitats can be modified by their inhabitants. Some marine organisms, like corals, kelp and seagrasses, are ecosystem engineers which reshape the marine environment to the point where they create further habitat for other organisms.
Intertidal and shore.
Intertidal zones, those areas close to shore, are constantly being exposed and covered by the ocean's tides. A huge array of life lives within this zone.
Shore habitats span from the upper intertidal zones to the area where land vegetation takes prominence. It can be underwater anywhere from daily to very infrequently. Many species here are scavengers, living off of sea life that is washed up on the shore. Many land animals also make much use of the shore and intertidal habitats. A subgroup of organisms in this habitat bores and grinds exposed rock through the process of bioerosion.
Reefs.
Reefs comprise some of the densest and most diverse habitats in the world. The best-known types of reefs are tropical coral reefs which exist in most tropical waters; however, reefs can also exist in cold water. Reefs are built up by corals and other calcium-depositing animals, usually on top of a rocky outcrop on the ocean floor. Reefs can also grow on other surfaces, which has made it possible to create artificial reefs. Coral reefs also support a huge community of life, including the corals themselves, their symbiotic zooxanthellae, tropical fish and many other organisms.
Much attention in marine biology is focused on coral reefs and the El Niño weather phenomenon. In 1998, coral reefs experienced the most severe mass bleaching events on record, when vast expanses of reefs across the world died because sea surface temperatures rose well above normal. Some reefs are recovering, but scientists say that between 50% and 70% of the world's coral reefs are now endangered and predict that global warming could exacerbate this trend.
Open ocean.
The open ocean is relatively unproductive because of a lack of nutrients, yet because it is so vast, in total it produces the most primary productivity. Much of the aphotic zone's energy is supplied by the open ocean in the form of detritus.
Deep sea and trenches.
The deepest recorded oceanic trench measured to date is the Mariana Trench, near the Philippines, in the Pacific Ocean at . At such depths, water pressure is extreme and there is no sunlight, but some life still exists. A white flatfish, a shrimp and a jellyfish were seen by the American crew of the bathyscaphe "Trieste" when it dove to the bottom in 1960.
Other notable oceanic trenches include Monterey Canyon, in the eastern Pacific, the Tonga Trench in the southwest at , the Philippine Trench, the Puerto Rico Trench at , the Romanche Trench at , Fram Basin in the Arctic Ocean at , the Java Trench at , and the South Sandwich Trench at .
In general, the deep sea is considered to start at the aphotic zone, the point where sunlight loses its power of transference through the water. Many life forms that live at these depths have the ability to create their own light known as bio-luminescence.
Marine life also flourishes around seamounts that rise from the depths, where fish and other sea life congregate to spawn and feed. Hydrothermal vents along the mid-ocean ridge spreading centers act as oases, as do their opposites, cold seeps. Such places support unique biomes and many new microbes and other lifeforms have been discovered at these locations .
Distribution factors.
An active research topic in marine biology is to discover and map the life cycles of various species and where they spend their time. Technologies that aid in this discovery include pop-up satellite archival tags, acoustic tags, and a variety of other data loggers. Marine biologists study how the ocean currents, tides and many other oceanic factors affect ocean life forms, including their growth, distribution and well-being. This has only recently become technically feasible with advances in GPS and newer underwater visual devices.
Most ocean life breeds in specific places, nests or not in others, spends time as juveniles in still others, and in maturity in yet others. Scientists know little about where many species spend different parts of their life cycles especially in the infant and juvenile years. For example, it is still largely unknown where juvenile sea turtles and some year-1 sharks travel. Recent advances in underwater tracking devices are illuminating what we know about marine organisms that live at great Ocean depths. The information that pop-up satellite archival tags give aids in certain time of the year fishing closures and development of a marine protected area. This data is important to both scientists and fishermen because they are discovering that by restricting commercial fishing in one small area they can have a large impact in maintaining a healthy fish population in a much larger area.

</doc>
<doc id="20023" url="https://en.wikipedia.org/wiki?curid=20023" title="Microkernel">
Microkernel

In computer science, a microkernel (also known as "μ-kernel") is the near-minimum amount of software that can provide the mechanisms needed to implement an operating system (OS). These mechanisms include low-level address space management, thread management, and inter-process communication (IPC).
If the hardware provides multiple rings or CPU modes, the microkernel may be the only software executing at the most privileged level, which is generally referred to as supervisor or kernel mode. Traditional operating system functions, such as device drivers, protocol stacks and file systems, are typically removed from the microkernel itself and are instead run in user space.
In terms of the source code size, as a general rule microkernels tend to be smaller than monolithic kernels, usually sizing at under 10,000 lines of code. The MINIX 3 microkernel, for example, has fewer than 6,000 lines of code.
History.
Microkernels were developed in the 1980s as a response to changes in the computer world, and to several challenges adapting existing "mono-kernels" to these new systems. New device drivers, protocol stacks, file systems and other low-level systems were being developed all the time. This code was normally located in the monolithic kernel, and thus required considerable work and careful code management to work on. Microkernels were developed with the idea that all of these services would be implemented as user-space programs, like any other, allowing them to be worked on monolithically and started and stopped like any other program. This would not only allow these services to be more easily worked on, but also separated the kernel code to allow it to be finely tuned without worrying about unintended side effects. Moreover, it would allow entirely new operating systems to be "built up" on a common core, aiding OS research.
Microkernels were a very hot topic in the 1980s when the first usable local area networks were being introduced. The same mechanisms that allowed the kernel to be distributed into user space also allowed the system to be distributed across network links. The first microkernels, notably Mach, proved to have disappointing performance, but the inherent advantages appeared so great that it was a major line of research into the late 1990s. However, during this time the speed of computers grew greatly in relation to networking systems, and the disadvantages in performance came to overwhelm the advantages in development terms. Many attempts were made to adapt the existing systems to have better performance, but the overhead was always considerable and most of these efforts required the user-space programs to be moved back into the kernel. By 2000, most large-scale (Mach-like) efforts had ended, although OpenStep used an adapted Mach kernel called XNU, which is used in Darwin, an operating system serving as the open source part of both OS X and iOS. , the Mach-based GNU Hurd is also functional and included in testing versions of Arch Linux and Debian.
Although major work on microkernels had largely ended, experimenters continued development. It has since been shown that many of the performance problems of earlier designs were not a fundamental requirement of the concept, but instead due to the designer's desire to use single-purpose systems to implement as many of these services as possible. Using a more pragmatic approach to the problem, including assembly code and relying on the processor to enforce concepts normally supported in software led to a new series of microkernels with dramatically improved performance.
Microkernels are closely related to exokernels.
They also have much in common with hypervisors,
but the latter make no claim to minimality and are specialized to supporting virtual machines; indeed, the L4 microkernel frequently finds use in a hypervisor capacity.
Introduction.
Early operating system kernels were rather small, partly because computer memory was limited. As the capability of computers grew, the number of devices the kernel had to control also grew. Through the early history of Unix, kernels were generally small, even though those kernels contained various device drivers and file system implementations. When address spaces increased from 16 to 32 bits, kernel design was no longer cramped by the hardware architecture, and kernels began to grow.
The Berkeley Software Distribution (BSD) of Unix began the era of big kernels. In addition to operating a basic system consisting of the CPU, disks and printers, BSD started adding additional file systems, a complete TCP/IP networking system, and a number of "virtual" devices that allowed the existing programs to work invisibly over the network. This growth continued for many years, resulting in kernels with millions of lines of source code. As a result of this growth, kernels were more prone to bugs and became increasingly difficult to maintain.
The microkernel was designed to address the increasing growth of kernels and the difficulties that came with them. In theory, the microkernel design allows for easier management of code due to its division into user space services. This also allows for increased security and stability resulting from the reduced amount of code running in kernel mode. For example, if a networking service crashed due to buffer overflow, only the networking service's memory would be corrupted, leaving the rest of the system still functional.
Inter-process communication.
Inter-process communication (IPC) is any mechanism which allows separate processes to communicate with each other, usually by sending messages. Shared memory is strictly speaking also an inter-process communication mechanism, but the abbreviation IPC usually only refers to message passing, and it is the latter that is particularly relevant to microkernels. IPC allows the operating system to be built from a number of small programs called servers, which are used by other programs on the system, invoked via IPC. Most or all support for peripheral hardware is handled in this fashion, with servers for device drivers, network protocol stacks, file systems, graphics, etc.
IPC can be synchronous or asynchronous. Asynchronous IPC is analogous to network communication: the sender dispatches a message and continues executing. The receiver checks (polls) for the availability of the message by attempting a receive, or is alerted to it via some notification mechanism. Asynchronous IPC requires that the kernel maintains buffers and queues for messages, and deals with buffer overflows; it also requires double copying of messages (sender to kernel and kernel to receiver). In synchronous IPC, the first party (sender or receiver) blocks until the other party is ready to perform the IPC. It does not require buffering or multiple copies, but the implicit rendezvous can make programming tricky. Most programmers prefer asynchronous send and synchronous receive.
First-generation microkernels typically supported synchronous as well as asynchronous IPC, and suffered from poor IPC performance. Jochen Liedtke identified the design and implementation of the IPC mechanisms as the underlying reason for this poor performance. In his L4 microkernel he pioneered methods that lowered IPC costs by an order of magnitude. These include an IPC system call that supports a send as well as a receive operation, making all IPC synchronous, and passing as much data as possible in registers. Furthermore, Liedtke introduced the concept of the "direct process switch", where during an IPC execution an (incomplete) context switch is performed from the sender directly to the receiver. If, as in L4, part or all of the message is passed in registers, this transfers the in-register part of the message without any copying at all. Furthermore, the overhead of invoking the scheduler is avoided; this is especially beneficial in the common case where IPC is used in an RPC-type fashion by a client invoking a server. Another optimization, called "lazy scheduling", avoids traversing scheduling queues during IPC by leaving threads that block during IPC in the ready queue. Once the scheduler is invoked, it moves such threads to the appropriate waiting queue. As in many cases a thread gets unblocked before the next scheduler invocation, this approach saves significant work. Similar approaches have since been adopted by QNX and MINIX 3.
In a client-server system, most communication is essentially synchronous, even if using asynchronous primitives, as the typical operation is a client invoking a server and then waiting for a reply. As it also lends itself to more efficient implementation, modern microkernels generally follow L4's lead and only provide a synchronous IPC primitive. Asynchronous IPC can be implemented on top by using helper threads. However, versions of L4 deployed in commercial products have found it necessary to add an asynchronous notification mechanism to better support asynchronous communication. This signal-like mechanism does not carry data and therefore does not require buffering by the kernel.
As synchronous IPC blocks the first party until the other is ready, unrestricted use could easily lead to deadlocks. Furthermore, a client could easily mount a denial-of-service attack on a server by sending a request and never attempting to receive the reply. Therefore, synchronous IPC must provide a means to prevent indefinite blocking. Many microkernels provide timeouts on IPC calls, which limit the blocking time. In practice, choosing sensible timeout values is difficult, and systems almost inevitably use infinite timeouts for clients and zero timeouts for servers. As a consequence, the trend is towards not providing arbitrary timeouts, but only a flag which indicates that the IPC should fail immediately if the partner is not ready. This approach effectively provides a choice of the two timeout values of zero and infinity. Recent versions of L4 and MINIX have gone down this path (older versions of L4 used timeouts, as does QNX).
Servers.
Microkernel servers are essentially daemon programs like any others, except that the kernel grants some of them privileges to interact with parts of physical memory that are otherwise off limits to most programs. This allows some servers, particularly device drivers, to interact directly with hardware.
A basic set of servers for a general-purpose microkernel includes file system servers, device driver servers, networking servers, display servers, and user interface device servers. This set of servers (drawn from QNX) provides roughly the set of services offered by a Unix monolithic kernel. The necessary servers are started at system startup and provide services, such as file, network, and device access, to ordinary application programs. With such servers running in the environment of a user application, server development is similar to ordinary application development, rather than the build-and-boot process needed for kernel development.
Additionally, many "crashes" can be corrected by simply stopping and restarting the server. However, part of the system state is lost with the failing server, hence this approach requires applications to cope with failure. A good example is a server responsible for TCP/IP connections: If this server is restarted, applications will experience a "lost" connection, a normal occurrence in a networked system. For other services, failure is less expected and may require changes to application code. For QNX, restart capability is offered as the QNX High Availability Toolkit.
To make all servers restartable, some microkernels have concentrated on adding various database-like methods such as transactions, replication and checkpointing to preserve essential state across single server restarts. An example is ChorusOS, which was made for high-availability applications in the telecommunications world. Chorus included features to allow any "properly written" server to be restarted at any time, with clients using those servers being paused while the server brought itself back into its original state. However, such kernel features are incompatible with the minimality principle, and are thus not provided in modern microkernels, which instead rely on appropriate user-level protocols.
Device drivers.
Device drivers frequently perform direct memory access (DMA), and therefore can write to arbitrary locations of physical memory, including various kernel data structures. Such drivers must therefore be trusted. It is a common misconception that this means that they must be part of the kernel. In fact, a driver is not inherently more or less trustworthy by being part of the kernel.
While running a device driver in user space does not necessarily reduce the damage a misbehaving driver can cause, in practice it is beneficial for system stability in the presence of buggy (rather than malicious) drivers: memory-access violations by the driver code itself (as opposed to the device) may still be caught by the memory-management hardware. Furthermore, many devices are not DMA-capable, their drivers can be made untrusted by running them in user space. Recently, an increasing number of computers feature IOMMUs, many of which can be used to restrict a device's access to physical memory. (IBM mainframes have had IO MMUs since the IBM System/360 Model 67 and System/370.) This also allows user-mode drivers to become untrusted.
User-mode drivers actually predate microkernels. The Michigan Terminal System (MTS), in 1967, supported user space drivers (including its file system support), the first operating system to be designed with that capability.
Historically, drivers were less of a problem, as the number of devices was small and trusted anyway, so having them in the kernel simplified the design and avoided potential performance problems. This led to the traditional driver-in-the-kernel style of Unix, Linux, and Windows before Windows XP.
With the proliferation of various kinds of peripherals, the amount of driver code escalated and in modern operating systems dominates the kernel in code size.
Essential components and minimality.
As a microkernel must allow building arbitrary operating system services on top, it must provide some core functionality. At a minimum, this includes:
This minimal design was pioneered by Brinch Hansen's Nucleus and the hypervisor of IBM's VM. It has since been formalised in Liedtke's "minimality principle":
A concept is tolerated inside the microkernel only if moving it outside the kernel, i.e., permitting competing implementations, would prevent the implementation of the system's required functionality.
Everything else can be done in a usermode program, although device drivers implemented as user programs may on some processor architectures require special privileges to access I/O hardware.
Related to the minimality principle, and equally important for microkernel design, is the separation of mechanism and policy, it is what enables the construction of arbitrary systems on top of a minimal kernel. Any policy built into the kernel cannot be overwritten at user level and therefore limits the generality of the microkernel.
Policy implemented in user-level servers can be changed by replacing the servers (or letting the application choose between competing servers offering similar services).
For efficiency, most microkernels contain schedulers and manage timers, in violation of the minimality principle and the principle of policy-mechanism separation.
Start up (booting) of a microkernel-based system requires device drivers, which are not part of the kernel. Typically this means that they are packaged with the kernel in the boot image, and the kernel supports a bootstrap protocol that defines how the drivers are located and started; this is the traditional bootstrap procedure of L4 microkernels. Some microkernels simplify this by placing some key drivers inside the kernel (in violation of the minimality principle), LynxOS and the original Minix are examples. Some even include a file system in the kernel to simplify booting. A microkernel-based system may boot via multiboot compatible boot loader. Such systems usually load statically-linked servers to make an initial bootstrap or mount an OS image to continue bootstrapping.
A key component of a microkernel is a good IPC system and virtual-memory-manager design that allows implementing page-fault handling and swapping in usermode servers in a safe way. Since all services are performed by usermode programs, efficient means of communication between programs are essential, far more so than in monolithic kernels. The design of the IPC system makes or breaks a microkernel. To be effective, the IPC system must not only have low overhead, but also interact well with CPU scheduling.
Performance.
On most mainstream processors, obtaining a service is inherently more expensive in a microkernel-based system than a monolithic system. In the monolithic system, the service is obtained by a single system call, which requires two "mode switches" (changes of the processor's ring or CPU mode). In the microkernel-based system, the service is obtained by sending an IPC message to a server, and obtaining the result in another IPC message from the server. This requires a context switch if the drivers are implemented as processes, or a function call if they are implemented as procedures. In addition, passing actual data to the server and back may incur extra copying overhead, while in a monolithic system the kernel can directly access the data in the client's buffers.
Performance is therefore a potential issue in microkernel systems. Indeed, the experience of first-generation microkernels such as Mach and ChorusOS showed that systems based on them performed very poorly.
However, Jochen Liedtke showed that Mach's performance problems were the result of poor design and implementation, specifically Mach's excessive page cache footprint.
Liedtke demonstrated with his own L4 microkernel that through careful design and implementation, and especially by following the minimality principle, IPC costs could be reduced by more than an order of magnitude compared to Mach. L4's IPC performance is still unbeaten across a range of architectures.
While these results demonstrate that the poor performance of systems based on first-generation microkernels is not representative for second-generation kernels such as L4, this constitutes no proof that microkernel-based systems can be built with good performance. It has been shown that a monolithic Linux server ported to L4 exhibits only a few percent overhead over native Linux.
However, such a single-server system exhibits few, if any, of the advantages microkernels are supposed to provide by structuring operating system functionality into separate servers.
A number of commercial multi-server systems exist, in particular the real-time systems QNX and Integrity. No comprehensive comparison of performance relative to monolithic systems has been published for those multiserver systems. Furthermore, performance does not seem to be the overriding concern for those commercial systems, which instead emphasize reliably quick interrupt handling response times (QNX) and simplicity for the sake of robustness. An attempt to build a high-performance multiserver operating system was the IBM Sawmill Linux project.
However, this project was never completed.
It has been shown in the meantime that user-level device drivers can come close to the performance of in-kernel drivers even for such high-throughput, high-interrupt devices as Gigabit Ethernet. This seems to imply that high-performance multi-server systems are possible.
Security.
The security benefits of microkernels have been frequently discussed. In the context of security the minimality principle of microkernels is, some have argued, a direct consequence of the principle of least privilege, according to which all code should have only the privileges needed to provide required functionality. Minimality requires that a system's trusted computing base (TCB) should be kept minimal. As the kernel (the code that executes in the privileged mode of the hardware) has unvetted access to any data and can thus violate its integrity or confidentiality, the kernel is always part of the TCB. Minimizing it is natural in a security-driven design.
Consequently, microkernel designs have been used for systems designed for high-security applications, including KeyKOS, EROS and military systems. In fact common criteria (CC) at the highest assurance level (Evaluation Assurance Level (EAL) 7) has an explicit requirement that the target of evaluation be "simple", an acknowledgment of the practical impossibility of establishing true trustworthiness for a complex system. Unfortunately, again, the term "simple" is misleading and ill-defined. At least the Department of Defense Trusted Computer System Evaluation Criteria introduced somewhat more precise verbiage at the B3/A1 classes, to wit, "The TCB shall complete, conceptually simple protection mechanisms with precisely defined semantics. Significant system engineering shall be directed toward minimizing the complexity of the TCB, as well as excluding from the TCB those modules that are not protection-critical."
Third generation.
Recent work on microkernels has been focusing on formal specifications of the kernel API, and formal proofs of the API's security properties and implementation correctness. The first example of this is a mathematical proof of the confinement mechanisms in EROS, based on a simplified model of the EROS API. More recently, a comprehensive set of machine-checked proofs has been performed of the properties of the protection model of , a version of L4.
This has led to what is referred to as "third-generation microkernels",
characterised by a security-oriented API with resource access controlled by capabilities, virtualization as a first-class concern, novel approaches to kernel resource management,
and a design goal of suitability for formal analysis, besides the usual goal of high performance. Examples are Coyotos, , Nova,
and Fiasco.OC.
In the case of seL4, complete formal verification of the implementation has been achieved, i.e. a mathematical proof that the kernel's implementation is consistent with its formal specification. This provides a guarantee that the properties proved about the API actually hold for the real kernel, a degree of assurance which goes beyond even CC EAL7. It was followed by proofs of security-enforcement properties of the API, and a proof demonstrating that the executable binary code is a correct translation of the C implementation, taking the compiler out of the TCB. Taken together, these proofs establish an end-to-end proof of security properties of the kernel.
Nanokernel.
The term "nanokernel" or "picokernel" historically referred to:
There is also at least one case where the term nanokernel is used to refer not to a small kernel, but one that supports a nanosecond clock resolution.

</doc>
<doc id="20024" url="https://en.wikipedia.org/wiki?curid=20024" title="Mach">
Mach

Mach may refer to:

</doc>
<doc id="20025" url="https://en.wikipedia.org/wiki?curid=20025" title="Multihull">
Multihull

A multihull is a ship, vessel, craft or boat with more than one hull. Multihull ships (vessels, craft, boats) include multiple types, sizes and applications. Hulls range from two to five, of uniform or diverse types and arrangements
Catamarans are the most common type, with two symmetric hulls. They are used as racing, sailing, tourist and fishing boats. About 70% of fast passenger and car-passenger ferries are catamarans. About 300 semi-submersible drilling and auxiliary platforms operate at sea.
Some ships with outriggers are built, including the experimental ship "Tritone" (UK) and the first and second sister-ships of the series of Littoral Combat Ships (US). About 70 small waterplan area ships whose hulls have a smaller cross section at the waterplane than below the surface exist.
Multihull ships can be classified by the number of hulls, by their arrangement and by their shapes and sizes.
The first multihull vessels were Austronesian canoes. They hollowed out logs to make canoes and stabilized them by attaching outriggers to prevent them from capsizing. This led to the proa, catamaran, and trimaran plus various outriggers throughout the Pacific.
Types.
Terminology.
Individual hulls are connected by an above-water structure called the "platform" or "above-water bridge". The structure can be watertight partially or fully, or can consist of separate connections.
The distance between hulls is called the "transverse clearance" and can be measured between center planes or between the inner boards.
The distance between the design waterplane and the bottom of the above-water platform ("wet deck") is called the "vertical clearance".
The longitudinal distance between a triple-hull ship is called the "longitudinal clearance" and can be measured between the middle of the hulls or between their fore perpendiculars.
Outrigger canoe.
An outrigger is a small side hull attached to the main load-carrying hull by two or more struts.
"Proas" have one hull and one outrigger. Engines are commonly placed on the main hull.
A ship with one hull of conventional shape and two small side hulls (outriggers) is called an "outrigger ship". This type is identified as "trimaran" in English language publications.
Three terms from the Malay and Micronesian language group describe hull components. The terms originated as descriptions of the proa. "Catamarans and trimarans share the same terminology."
Semantically, the catamaran is a pair of "Vaka" held together by "Aka", whereas the trimaran is a central "Vaka", with "Ama" on each side, attached by "Aka".
Two-hull.
Sometimes, the term "catamaran" is applied to any ship or boat consisting of two hulls. However, other twin-hull types include" duplus" and "trisec"
The history of commercial catamarans began in 17th century England. Separate attempts at steam-powered catamarans were carried out by the middle of the 20th century. However, success required better materials and more developed hydrodynamic technologies. During the second half of the 20th century catamaran designs flourished.
Three-hull.
The term "trimaran" is often used for any triple-hull ships or boats. More precisely the term "trimaran" is used for a ship or boat with three identical hulls of traditional shape. The other triple-hull ships are "outrigger "and "tricore".
The trimaran has the widest range of interactions of wave systems generated by hulls at speed. The interactions can be favorable or unfavorable, depending on relative hull arrangement and speed. No authentic trimarans exist. Model test results and corresponding simulations provide estimates on the power of the full-scale ships. The calculations show possible advantages in a defined band of relative speeds.
A new type of super-fast vessel, the "wave-piercing" trimaran (WPT) is known as an air-born unloaded (up to 25% of displacement) vessel, that can achieve twice the speed with a relative power.
Four-hull.
The term "quadrimaran" is used for four-hulled vessels.
Five-hull.
The term "pentamaran" is used for five-hulled vessels. The M80 Stiletto is a pentamaran.
Small waterplane area.
Hulls with beam designs that are narrower at the water surface (waterplane) than below can be classified as hulls with decreased or small waterplane area. More often the term "small waterplane area hull" means a hull with an underwater "gondola" and strut(s) that connect the gondola to the above-water platform. Any ship employing such hulls qualifies.
Any twin-hull SWA ship is called a "small waterplane area twin hull" ("SWATH)". A SWATH with one long strut on each hull is called a "duplus" (named after the first drilling ship of this type). The duplus is the most common type of SWA ships built.
A "trisec" is a SWATH ship with two struts on each gondola. A trisec can have a minimal waterplane area and minimal motions in waves, resulting in more effective motion control.
A triple-hull SWA ship is called a "tricore", regardless of the number of struts. The term applies to ship with three identical SWA hulls. There are no built tricores, but towing tests of models show the possibility of sufficient advantage from a power point of view in defining band of the relative speeds.
Outrigger ships can employ SWA main hull and/or outriggers .
Characteristics.
Many traits differentiate multihulls from monohulls along several axes.
Physical.
Multihulls have a broader variety of hull and payload geometries. They have a relatively large beam, deck area (upper and inner), above-water capacity, shallower draft (allowing operation in shallower water).
Stability.
The Austronesians discovered that round logs tied together into a raft don't roll, or capsize as easily as a single log. Hollowing out the logs increased buoyancy (increasing payload) while preserving stability. However, this requires a lot of work and it has increased drag and weight.
Separating two logs by a pair of cross-members called akas achieved the increased stability at lower weight and less effort. Covering the intervening distance with a platform provides stability similar to a raft.
Multihulls feature reduced roll and yaw (equivalent pitch motion) with transverse stability (depends on transverse clearance) comparable to or greater than longitudinal stability. This reduced motion reduces seasickness and allows for more efficient solar energy collection and radar operation. They offer more effective motion mitigation systems (SWA ships), reduced wave resistance and towing resistance by controlling hull aspect ratio (for twin-hulls) and optimizing the interference between each hull's wave systems (triple-hulls).
The inertia of a (heavier) monohull will drive it after the wind drops, while a (lighter) multihull requires wind. Monohulls can push through waves that a multihull must pass over. Multihulls are more prone towards "hobby horsing" especially when lightly loaded and of short overall length.
Weight stabilized.
Other cultures stabilized their watercraft by filling the bottom with rocks and other ballast. The practice can be traced back to the Romans, Phoenicians, Vikings and others. Modern ocean liners carry tons of ballast. Naval architects enssure that the center of gravity of their designs remains substantially below the metacenter. The low centre of gravity acts as a counterweight as the craft rotates around its centre of buoyancy, creating a restorative force as the craft deviates from its vertical position.
Safety.
Multihulls feature greater seaworthiness vs monohulls with the same displacement. Most production multihulls are officially rated as unsinkable. Watertight above-water platforms with sections protected by water-tight bulkheads can prevent sinking if the hulls fail. They have increased reliability because the engines are on separate hulls. However, capsized monohulls may right themselves, pulled by the ballast. Multihulls do not and larger vessels may require a crane. Their reduced weight and shallow draft make them unsuitable for use in icy waters.
Performance.
Multihulls are substantially faster than monohulls of comparable size, in part because of their reduced weight, reduced draft and reduced waterline profile. However, their relatively greater total wetted area increases frictional resistance and total resistance at low speeds.
Unlike monohulls multihulls can be designed to give very low wake at some speeds.
Applications.
Multihulls have more complicated conventional docking due to their wider beam. The beam requires a wider area for construction and dry dock and a larger haul-out ramp.
Sailboats and small vessels.
Common multihull sailboats and small craft include proas, catamarans and trimarans.
The added space and stability are valued amenities for small boat users.
Multihull powerboats, usually catamarans (never proas) are used for racing and transportation. Speed, maneuverability, and space are the main factors for choosing multihulls.
Popular models.
Multihulls are popular for racing, especially in Europe, New Zealand and Australia, and are somewhat popular for cruising in the Caribbean and South Pacific. They appear less frequently in the United States. Until the 1980s most multihull sailboats (except for beach cats) were built either by their owners or by boat builders. Since then companies have been selling mass-produced boats.
Small sailing catamarans are also called beach catamarans. The most recognized racing classes are the Hobie Cat 16, Formula 18 cats, A-cats, the former Olympic multihull Tornado and New Zealand's Weta trimaran.
Power catamarans are becoming more common in Caribbean and Mediterranean international charter fleets.
Mega or super catamarans are those over 60 feet in length. These often receive substantial customization following the request of the owner. One builder is New Zealand's Pachoud Yachts.
Builders include Corsair Marine (mid-sized trimarans) and Privilege (large catamarans). The Seawind, Perry and Lightwave. The largest manufacturer of large multihulls is Fontaine-Pajot in France.
Powerboats range from small single pilot Formula 1s to large multi-engined or gas turbined power boats that are used in off-shore racing and employ 2 to 4 pilots.
Pioneers of multihull design include James Wharram (UK), Derek Kelsall (UK), Loch Crowther (Aust), Hedly Nicol (Aust), Malcolm Tennant (NZ), Jim Brown (USA), Arthur Piver (USA), Chris White (US), Ian Farrier (NZ) and LOMOcean (NZ).
Performance.
In 1978, 101 years after catamarans like "Amaryllis" were banned from yacht racing they returned the sport. This started with the victory of the trimaran "Olympus Photo", skippered by Mike Birch in the first Route du Rhum. Thereafter, no open ocean race was won by a monohull. Winning times dropped by 70%, since 1978. Olympus Photo's 23 day 6 hr 58' 35" success dropped to Gitana 11's 7d 17h 19'6", in 2006.

</doc>
<doc id="20029" url="https://en.wikipedia.org/wiki?curid=20029" title="Multics Relational Data Store">
Multics Relational Data Store

The Multics Relational Data Store, or MRDS for short, was the first commercial relational database management system. It was written in PL/1 by Honeywell for the Multics operating system and first sold in June 1976. Unlike the SQL systems that emerged in the late 1970s and early 80's, MRDS used a command language only for basic data manipulation, equivalent to the codice_1 or codice_2 statements in SQL. Other operations, like creating a new database, or general file management, required the use of a separate command program.

</doc>
<doc id="20032" url="https://en.wikipedia.org/wiki?curid=20032" title="Mike Oldfield">
Mike Oldfield

Michael Gordon "Mike" Oldfield (born 15 May 1953) is an English musician and composer. His work blends progressive rock with world, folk, classical, electronic, ambient, and new-age music. He is best known for his 1973 album "Tubular Bells"which launched Virgin Records and became a hit after its opening was used as the theme for the film "The Exorcist"and for his 1983 hit single "Moonlight Shadow". He is also known for his rendition of the Christmas piece "In Dulci Jubilo".
Oldfield has released more than 20 albums with the most recent being a rock album titled "Man on the Rocks", released in 2014.
Biography.
Early life.
Mike Oldfield's parents are Raymond Oldfield, a general practitioner, and Maureen Liston, an Irish nurse. His sister Sally and brother Terry are also successful musicians and have appeared on several of Mike's albums. He also had a younger brother, David, who had Down's syndrome and died in infancy.
Oldfield was born in the Battle Hospital in Reading, Berkshire, and he attended St. Joseph's Convent School, Highlands Junior School, St. Edward's preparatory school, and Presentation College in Reading. When he was 13, he moved with his parents to Harold Wood in Essex and attended Hornchurch Grammar School, where, having already begun his career in music, he took just one GCE examination, in English.
Early career.
Oldfield's career began fairly early, playing acoustic guitar in local folk clubs. At this time, he already had two 15-minute instrumental pieces in which he would "go through all sorts of moods", precursors to his landmark 1970s compositions. In his early teens, Oldfield was involved in a beat group playing The Shadows-style music (he has often cited Hank Marvin as a major influence, and would later cover The Shadows' song "Wonderful Land"). In 1967, Oldfield and his sister Sally formed the folk duo The Sallyangie and, after exposure in the local folk scene, were signed to Transatlantic Records. An album, "Children of the Sun", was issued in 1968. After The Sallyangie disbanded, he formed another duo, called Barefoot, with his brother, which took him back to rock music.
In 1970, Oldfield joined The Whole Worldformer Soft Machine vocalist Kevin Ayers's backing groupplaying bass and occasionally lead guitar. He is featured on two Ayers albums, "Whatevershebringswesing" and "Shooting at the Moon". The band also included keyboardist and composer David Bedford, who quickly befriended Oldfield, encouraged him in his composition of an early version of "Tubular Bells" and later arranged and conducted an orchestral version of the "Tubular Bells" album. Oldfield was also the reserve guitarist for the musical "Hair" and played with Alex Harvey.
Having recorded sections of this early version of "Tubular Bells" as demo pieces, Oldfield attempted to persuade record labels to take on the "Tubular Bells" project. Nothing came of his efforts until September 1971, when he attended recording sessions at The Manor Studioowned by a young Richard Branson and run by engineers Tom Newman and Simon Heyworthas bass guitarist for the Arthur Louis Band. Branson already had several business ventures and was about to start his own record label, Virgin Records. Newman and Heyworth heard some of Oldfield's demo music and took it to Branson and Simon Draper, who eventually gave Oldfield one week's worth of recording time at The Manor. During this week, he completed "Part One" of "Tubular Bells"; "Part Two" was then compiled over a number of months.
Virgin years (1973–91).
"Tubular Bells" is Oldfield's most famous work. The instrumental composition was recorded in 1972 and released on 25 May 1973 as the inaugural album of Richard Branson's label Virgin Records. Oldfield played more than twenty different instruments in the multi-layered recording, and its style moved through many diverse musical genres. Its 2,630,000 UK sales put it at No. 34 on the list of the best-selling albums in the country. The title track became a top 10 hit single in the US after the opening was used in "The Exorcist" film. It is today considered to be a forerunner of the new-age music movement. In 1974, Oldfield played guitar on the critically acclaimed album "Rock Bottom" by Robert Wyatt. In late 1974, the follow-up LP, "Hergest Ridge", was No. 1 in the UK for three weeks before being dethroned by "Tubular Bells". Although "Hergest Ridge" was released over a year after "Tubular Bells", it reached No. 1 first. "Tubular Bells" spent 11 weeks (10 of them consecutive) at No. 2 before its one week at the top. In 1979, Oldfield's music was used as the musical score for "The Space Movie", a Virgin movie that celebrated the tenth anniversary of the Apollo 11 mission.
Like "Tubular Bells", "Hergest Ridge" is a two-movement instrumental piece, this time evoking scenes from Oldfield's Herefordshire country retreat. It was followed in 1975 by the pioneering world music piece "Ommadawn" released after the death of his mother Maureen, and in 1978 by "Incantations", which introduced more diverse choral performances from Sally Oldfield, Maddy Prior, and the Queen's College Girls Choir. In 1975, Oldfield recorded a version of the Christmas piece "In Dulci Jubilo" which charted at No. 4 in the UK. Oldfield's 1976 rendition of "Portsmouth" remains his best-performing single on the UK Singles Chart, reaching No. 3.
In 1975, Oldfield received a Grammy award for Best Instrumental Composition in "Tubular Bells – Theme from "The Exorcist"". In 1976, Oldfield and his sister joined his friend and band member Pekka Pohjola to play on his album "Mathematician's Air Display", which was released in 1977. The album was recorded and edited at Oldfield's Througham Slad Manor in Gloucestershire by Oldfield and Paul Lindsay.
Around the time of "Incantations", Oldfield underwent a controversial self-assertiveness therapy course known as Exegesis, which had a significant effect on his personality for some years, he has described, making him more confident and out-going. Possibly as a result, the formerly reclusive musician staged a major Tour of Europe to promote the album, chronicled in his live album "Exposed", much of which was recorded at the National Exhibition Centre near Birmingham. In 1979, he recorded a version of the signature tune of the British children's television programme "Blue Peter", which was used by the show for 10 years. In 1981, Oldfield was asked to compose a piece for the Royal Wedding of Charles, Prince of Wales, and Lady Diana Spencer, titled "Royal Wedding Anthem".
The early 1980s saw Oldfield make a transition to mainstream pop music, beginning with the inclusion of shorter instrumental tracks and contemporary cover versions on "Platinum" and "QE2" (the latter named after the ocean liner). Soon afterwards he turned to songwriting, with a string of collaborations featuring various lead vocalists alongside his characteristic searing guitar solos. The best known of these is "Moonlight Shadow", his 1983 hit with Maggie Reilly. The most successful Oldfield composition on the US pop charts during this period was actually a cover version – Hall & Oates's cover of Oldfield's "Family Man" for their 1982 album "H2O". Released as the album's third single, it hit the Top 10 during the spring of 1983 and was a hugely popular MTV music video.
Oldfield later turned to film and video, writing the score for Roland Joffé's acclaimed film "The Killing Fields" and producing substantial video footage for his album "Islands". "Islands" continued what Oldfield had been doing on the past couple of albums, with an instrumental piece on one side and rock/pop singles on the other. Of these, "Islands", sung by Bonnie Tyler and "Magic Touch", with vocals by Max Bacon (in the US version) and Glasgow vocalist Jim Price (Southside Jimmy) in the rest of the world, were the major hits. In the US "Magic Touch" reached the top 10 on the Billboard album rock charts in 1988. During the 1980s, Oldfield's then-wife, Norwegian singer Anita Hegerland, contributed vocals to many songs including "Pictures in the Dark".
"Earth Moving" was released in July 1989 and was a moderate success. The album was the first to consist solely of rock/pop songs, several of which were released as singles: "Innocent" and "Holy" in Europe, and "Hostage" in the US for album rock stations. This was, however, a time of much friction with his record label. Virgin Records insisted that Oldfield use the title "Tubular Bells 2" for his next instrumental album. Oldfield's rebellious response was "Amarok", an hour-long work featuring rapidly changing themes, unpredictable bursts of noise, and a hidden Morse code insult, stating "Fuck off RB", directed at Richard Branson. It was not a commercial success. His last album for the Virgin label was "Heaven's Open", released under the name 'Michael Oldfield'. The album, notable for being the first time Oldfield had contributed all the lead vocals himself, consisted of songs and the rapidly changing instrumental piece "Music from the Balcony". However the rift was healed some years later. In 2013 Oldfield invited Sir Richard Branson to preside over the opening of the new school hall at St.Andrew's International School of The Bahamas which two of his children attended. This was the occasion of the debut of "Tubular Bells for Schools", a piano solo adaptation of Oldfield's work.
Warner years (1992–2003).
The very first thing Oldfield did when arriving at his new label, Warner Bros., was to write and release "Tubular Bells II", the sequel to his first record on Virgin, as his final insult to his former label. It was premiered at a live concert at Edinburgh Castle. He then continued to embrace new musical styles, with "The Songs of Distant Earth" (based on Arthur C. Clarke's novel of the same name) exhibiting a softer new-age sound. In 1994, he also had an asteroid, 5656 Oldfield, named after him.
In 1995, Oldfield continued to embrace new musical styles by producing the Celtic-themed album "Voyager". In 1992, Oldfield met Luar na Lubre, a Galician Celtic-folk band (from A Coruña, Spain). The band's popularity grew after Oldfield covered their song "O son do ar" ("The sound of the air") on his "Voyager" album.
In 1998, Oldfield produced the third "Tubular Bells" album (also premiered at a concert, this time in Horse Guards Parade, London), drawing from the dance music scene at his then new home on the island of Ibiza. This album was still inspired by themes from "Tubular Bells", but differed in lacking a clear two-part layout.
During 1999, Oldfield released two albums. The first, "Guitars", used guitars as the source for all the sounds on the album, including percussion. The second, "The Millennium Bell", consisted of pastiches of a number of styles of music that represented various historical periods over the past millennium. The work was performed live in Berlin for the city's millennium celebrations in 1999–2000.
He added to his repertoire the MusicVR project, combining his music with a virtual reality-based computer game. His first work on this project is "Tr3s Lunas" launched in 2002, a virtual game where the player can interact with a world full of new music. This project appeared as a double CD, one with the music, and the other with the game.
In 2003, Oldfield released "Tubular Bells 2003", a re-recording of the original "Tubular Bells", on CD, and DVD-Audio. This was done to "fix" many "imperfections" in the original due to the recording technologies of the early 1970s and limitations in time that he could spend in the recording studio. It celebrated the 30th anniversary of "Tubular Bells", Oldfield's 50th birthday and his marriage to Fanny in the same year. At around the same time Virgin released an SACD version containing both the original stereo album and the 1975 quadraphonic mix by Phil Newell. In the 2003 version, the original voice of the 'Master of Ceremonies' (Viv Stanshall) was replaced by the voice of John Cleese, Stanshall having died in the interim.
Mercury years (since 2004).
On 12 April 2004 Oldfield launched his next virtual reality project, "Maestro", which contains music from the "Tubular Bells 2003" album and also some new chillout melodies. The games have since been made available free of charge on Tubular.net. A double album, "Light + Shade", was released on Mercury Records in 2005, with whom Mike had recently signed a three-album deal. The two discs contain music of contrasting moods, one relaxed ("Light") and the other more edgy and moody ("Shade"). Oldfield headlined the pan-European Night of the Proms tour, consisting of 21 concerts in 2006 and 2007.
His autobiography "Changeling" was published in May 2007 by Virgin Books. In March 2008 Oldfield released his first classical album, "Music of the Spheres"; Karl Jenkins assisted with the orchestration. In the first week of release the album topped the UK Classical chart and reached number 9 on the main UK Album Chart. A single "Spheres", featuring a demo version of pieces from the album, was released digitally. The album was nominated for a Classical Brit Award, the NS&I Best Album of 2009.
In 2008, when Oldfield's original 35-year deal with Virgin Records ended, the rights to "Tubular Bells" and his other Virgin releases were returned to him, and then they were transferred to Mercury Records. Mercury issued a press release on 15 April 2009, noting that Oldfield's Virgin albums would be re-released, starting 8 June 2009. These releases include special features from the archives. a further seven albums have been reissued and compilation albums have been released such as "Two Sides".
In March 2010, "Music Week" reported that publishing company Stage Three Music had acquired a 50% stake in the songs of Oldfield's entire recorded output in a seven-figure deal.
In 2008, Oldfield contributed an exclusive song ("Song for Survival") to a charity album called "Songs for Survival", in support of the Survival International. Oldfield's daughter, Molly, played a large part in the project. In 2010 lyricist Don Black said in an interview with "Music Week" that he had been working with Oldfield. In 2012, Oldfield was featured on Terry Oldfield's "Journey into Space" album and on a track called "Islanders" by German producer Torsten Stenzel's York project. In 2013 Oldfield and York released a remix album titled "Tubular Beats".
At the 2012 Summer Olympics opening ceremony, Oldfield performed renditions of "Tubular Bells", "Far Above the Clouds" and "In Dulci Jubilo" during a segment about the National Health Service. This track appears on the "Isles of Wonder" album which contains music from the Danny Boyle-directed show.
In October 2013, the BBC broadcast "Tubular Bells: The Mike Oldfield Story", an hour-long appreciation of Oldfield's life and musical career, filmed on location at his home recording studio in Nassau.
Oldfield's latest rock-themed album of songs, titled "Man on the Rocks", was released on 3 March 2014 by Virgin EMI. The album was produced by Steve Lipson. The album marks a return of Oldfield to a Virgin branded label, through the merger of Mercury Records UK and Virgin Records after Universal Music's purchase of EMI. The track "Nuclear" was used for the E3 trailer of "".
Interviewed by Steve Wright in May 2015 for his BBC Radio 2 show, Oldfield said that he was currently working on a "prequel to "Tubular Bells"" which was being recorded using analogue equipment as much as possible. He suggested that the album might only be released on vinyl. The project is in its infancy and would follow his current reissue campaign. Oldfield suggested that it would be released "in a couple of years".
On 16 October 2015 Oldfield tweeted, via his official Twitter account ""I am continuing to work on ideas for "A New Ommadawn" for the last week or so to see if [...] the idea actually works."" No more is known about the ""Ommadawn II"" project or if it is related to the previously mentioned "Tubular Bells" prequel.
Multi-instrumentalism and instrument choices.
Although Oldfield considers himself primarily a guitarist, he is also one of popular music's most skilled and diverse multi-instrumentalists. His 1970s recordings were characterised by a very broad variety of instrumentation predominantly played by himself, plus assorted guitar sound treatments to suggest other instrumental timbres (such as the bagpipe, mandolin, "Glorfindel" and varispeed guitars on the original "Tubular Bells").
During the 1980s Oldfield became expert in the use of digital synthesizer and sequencers (notably the Fairlight CMI) which began to dominate the sound of his recordings: from the late 1990s onwards, he became a keen user of software synthesizers. He has, however, regularly returned to projects emphasising detailed, manually played and part-acoustic instrumentation (such as 1990's "Amarok", 1996's "Voyager" and 1999's "Guitars").
Oldfield has played over forty distinct and different instruments on record, including:
While generally preferring the sound of guest vocalists, Oldfield has frequently sung both lead and backup parts for his songs and compositions. He has also contributed experimental vocal effects such as fake choirs and the notorious "Piltdown Man" impression on "Tubular Bells".
Although recognised as an extremely skilled guitarist, Oldfield is fairly self-deprecating about his other instrumental skills, citing them as having been achieved out of necessity at the time to perform and record the music he composes. (He has been particularly dismissive of his violin-playing and singing abilities.)
Guitars.
Over the years, Oldfield has used a range of guitars. Among the more notable of these are:
Oldfield used a modified Roland GP8 effects processor in conjunction with his PRS Artist to get many of his heavily overdriven guitar sounds from the "Earth Moving" album onwards. Oldfield has also been using guitar synthesizers since the mid-1980s, using a 1980s Roland GR-300/G-808 type system, then a 1990s Roland GK2 equipped red PRS Custom 24 (sold in 2006) with a Roland VG8, and most recently a Line 6 Variax.
Oldfield has an unusual playing style, using both fingers and fingernails and different ways of creating vibrato: a "very fast side-to-side vibrato" and "violinist's vibrato". Oldfield has stated that his playing style originates from his musical roots playing folk music and the bass guitar.
Keyboards.
Over the years, Oldfield has owned and used a vast number of synthesizers and other keyboard instruments. In the 1980s, he composed the score for the film "The Killing Fields" on a Fairlight CMI. Some examples of keyboard and synthesised instruments which Oldfield has made use of include Sequential Circuits Prophet-5s (notably on "Platinum" and "The Killing Fields"), Roland JV-1080/JV-2080 units (1990s), a Korg M1 (as seen in the "Innocent" video), a Clavia Nord Lead and Steinway pianos. In recent years, he has also made use of software synthesis products, such as Native Instruments.
Lead Vocalists.
As with any other instrument, Mike Oldfield uses to be careful with the sound of vocals in his works. He has only occasionally sung by himself in his records and life performances, sometimes using a vocoder as a resource. It’s not unusual for him to make call to diverse singers and performing selection tests before deciding the most appropriate for a particular song or album. Featured lead vocalists that have collaborated with him include:
Recording.
Oldfield has self-recorded and produced many of his albums, and played the majority of the instruments that feature on them, largely at his home studios. In the 1990s and 2000s he mainly used DAWs such as Apple Logic, Avid Pro Tools and Steinberg Nuendo as recording suites. For composing classical music Oldfield has been quoted as using the software notation program Sibelius running on Apple Macintoshes. He also used the FL Studio DAW on his 2005 double album "Light + Shade". Among the mixing consoles Oldfield has owned are an AMS Neve Capricorn 33238, a Harrison Series X, and a Euphonix System 5-MC.
Personal life.
Oldfield and his siblings were raised as Roman Catholics, their mother's faith. In his early life Oldfield used drugs such as LSD and discussed the mental health effects that it had on him in his autobiography. In the early nineties he underwent a successful course of counselling and therapy which gave him a deep understanding of the mental health problems of other people. He then set up a foundation called Tonic which sponsored people to have valuable counselling and therapy. The trustee was the professor of psychiatry at Guy's Hospital, London and the charity helped a great many people over the years.
In the late 1970s, Oldfield briefly married Diana D'Aubigny (the sister of the Exegesis group leader), but this lasted just a few weeks. Mike Oldfield has had seven children. In the early 1980s, he had three children with Sally Cooper: Molly, Dougal (who died aged 33 in May 2015) and Luke. In the late 1980s, he had two children (Greta and Noah) with Norwegian singer Anita Hegerland. In the 2000s, he married Fanny Vandekerckhove (born 1977), whom he met during his time in Ibiza; they have two sons together (Jake and Eugene). Oldfield and Fanny separated in 2013.
Oldfield is a motorcycle fan and has five bikes. These include a BMW R1200GS, a Suzuki GSX-R750, a Suzuki GSX-R1000, and a Yamaha R1. He says that some of his inspiration for composing comes from riding them. Throughout his life Oldfield has also had a passion for aircraft and building model aircraft. Since 1980, he has been a licensed pilot and has flown fixed wing aircraft (the first of which was a Beechcraft Sierra) and helicopters (including the Agusta Bell 47G, which featured on the sleeve of his cover version of the ABBA song "Arrival" as a parody of their album artwork). He is also interested in cars and has owned a Ferrari and a Bentley which was a gift from Richard Branson as an incentive for him to give his first live performance of "Tubular Bells". He has endorsed the Mercedes-Benz S-Class in the Mercedes UK magazine. Oldfield also considers himself to be a Trekkie (fan of the popular science fiction television series "Star Trek"). He also noted in an interview in 2008 that he had two boats.
In 2007, Oldfield caused a minor stir in the British press by criticising Britain for being too controlling and protective, specifically concentrating on the smoking ban which England and Wales had introduced that year. Oldfield then moved from his Gloucestershire home to Palma de Mallorca, Spain and then to Monaco. He has lived outside the UK in the past, including in Los Angeles and Ibiza in the 1990s and, for tax reasons, Switzerland in the mid-1980s. In 2009, he decided to move to the Bahamas and put his home in Mallorca up for sale.
Discography.
Oldfield has had more than 30 charting albums and 25 charting singles on the British charts and many more around the world.

</doc>
<doc id="20034" url="https://en.wikipedia.org/wiki?curid=20034" title="Mutual recursion">
Mutual recursion

In mathematics and computer science, mutual recursion is a form of recursion where two mathematical or computational objects, such as functions or data types, are defined in terms of each other. Mutual recursion is very common in functional programming and in some problem domains, such as recursive descent parsers, where the data types are naturally mutually recursive, but is uncommon in other domains.
Examples.
Data types.
The most important basic example of a data type that can be defined by mutual recursion is a tree, which can be defined mutually recursively in terms of a forest (a list of trees). Symbolically:
A forest "f" consists of a list of trees, while a tree "t" consists of a pair of a value "v" and a forest "f" (its children). This definition is elegant and easy to work with abstractly (such as when proving theorems about properties of trees), as it expresses a tree in simple terms: a list of one type, and a pair of two types. Further, it matches many algorithms on trees, which consist of doing one thing with the value, and another thing with the children.
This mutually recursive definition can be converted to a singly recursive definition by inlining the definition of a forest:
A tree "t" consists of a pair of a value "v" and a list of trees (its children). This definition is more compact, but somewhat messier: a tree consists of a pair of one type and a list of another, which require disentangling to prove results about.
In Standard ML, the tree and forest data types can be mutually recursively defined as follows, allowing empty trees:
Computer functions.
Just as algorithms on recursive data types can naturally be given by recursive functions, algorithms on mutually recursive data structures can be naturally given by mutually recursive functions. Common examples include algorithms on trees, and recursive descent parsers. As with direct recursion, tail call optimization is necessary if the recursion depth is large or unbounded, such as using mutual recursion for multitasking. Note that tail call optimization in general (when the function called is not the same as the original function, as in tail-recursive calls) may be more difficult to implement than the special case of tail-recursive call optimization, and thus efficient implementation of mutual tail recursion may be absent from languages that only optimize tail-recursive calls. In languages such as Pascal that require declaration before use, mutually recursive functions require forward declaration, as a forward reference cannot be avoided when defining them.
As with directly recursive functions, a wrapper function may be useful, with the mutually recursive functions defined as nested functions within its scope if this is supported. This is particularly useful for sharing state across a set of functions without having to pass parameters between them.
Basic examples.
A standard example of mutual recursion, which is admittedly artificial, is determining whether a non-negative number is even or is odd by having two separate functions and calling each other, decrementing each time. In C:
These functions are based on the observation that the question "is 4 even?" is equivalent to "is 3 odd?", which is in turn equivalent to "is 2 even?", and so on down to 0. This example is mutual single recursion, and could easily be replaced by iteration. In this example, the mutually recursive calls are tail calls, and tail call optimization would be necessary for this to execute in constant stack space; in C this would take "O"("n") stack space, unless rewritten to use jumps instead of calls.
As a more general class of examples, an algorithm on a tree can be decomposed into its behavior on a value and its behavior on children, and can be split up into two mutually recursive functions, one specifying the behavior on a tree, calling the forest function for the forest of children, and one specifying the behavior on a forest, calling the tree function for the tree in the forest. In Python:
In this case the tree function calls the forest function by single recursion, but the forest function calls the tree function by multiple recursion.
Using the Standard ML data type above, the size of a tree (number of nodes) can be computed via the following mutually recursive functions:
A more detailed example in Scheme, counting the leaves of a tree:
These examples reduce easily to a single recursive function by inlining the forest function in the tree function, which is commonly done in practice: directly recursive functions that operate on trees sequentially process the value of the node and recurse on the children within one function, rather than dividing these into two separate functions.
Advanced examples.
A more complicated example is given by recursive descent parsers, which can be naturally implemented by having one function for each production rule of a grammar, which then mutually recurse; this will in general be multiple recursion, as production rules generally combine multiple parts. This can also be done without mutual recursion, for example by still having separate functions for each production rule, but having them called by a single controller function, or by putting all the grammar in a single function.
Mutual recursion can also implement a finite-state machine, with one function for each state, and single recursion in changing state; this requires tail call optimization if the number of state changes is large or unbounded. This can be used as a simple form of cooperative multitasking. A similar approach to multitasking is to instead use coroutines which call each other, where rather than terminating by calling another routine, one coroutine yields to another but does not terminate, and then resumes execution when it is yielded back to. This allows individual coroutines to hold state, without it needing to be passed by parameters or stored in shared variables.
There are also some algorithms which naturally have two phases, such as minimax (min and max), and these can be implemented by having each phase in a separate function with mutual recursion, though they can also be combined into a single function with direct recursion.
Mathematical functions.
In mathematics, the Hofstadter Female and Male sequences are an example of a pair of integer sequences defined in a mutually recursive manner.
Fractals can be computed (up to a given resolution) by recursive functions. This can sometimes be done more elegantly via mutually recursive functions; the Sierpiński curve is a good example.
Prevalence.
Mutual recursion is very common in the functional programming style, and is often used for programs written in LISP, Scheme, ML, and similar languages. In languages such as Prolog, mutual recursion is almost unavoidable.
Some programming styles discourage mutual recursion, claiming that it can be confusing to distinguish the conditions which will return an answer from the conditions that would allow the code to run forever without producing an answer. Peter Norvig points to a design pattern which discourages the use entirely, stating:
Terminology.
Mutual recursion is also known as indirect recursion, by contrast with direct recursion, where a single function calls itself directly. This is simply a difference of emphasis, not a different notion: "indirect recursion" emphasises an individual function, while "mutual recursion" emphasises the set of functions, and does not single out an individual function. For example, if "f" calls itself, that is direct recursion. If instead "f" calls "g" and then "g" calls "f," which in turn calls "g" again, from the point of view of "f" alone, "f" is indirectly recursing, while from the point of view of "g" alone, "g" is indirectly recursing, while from the point of view of both, "f" and "g" are mutually recursing on each other. Similarly a set of three or more functions that call each other can be called a set of mutually recursive functions.
Conversion to direct recursion.
Mathematically, a set of mutually recursive functions are primitive recursive, which can be proven by course-of-values recursion, building a single function "F" that lists the values of the individual recursive function in order: formula_1 and rewriting the mutual recursion as a primitive recursion.
Any mutual recursion between two procedures can be converted to direct recursion by inlining the code of one procedure into the other. If there is only one site where one procedure calls the other, this is straightforward, though if there are several it can involve code duplication. In terms of the call stack, two mutually recursive procedures yield a stack ABABAB..., and inlining B into A yields the direct recursion (AB)(AB)(AB)...
Alternately, any number of procedures can be merged into a single procedure that takes as argument a variant record (or algebraic data type) representing the selection of a procedure and its arguments; the merged procedure then dispatches on its argument to execute the corresponding code and uses direct recursion to call self as appropriate. This can be seen as a limited application of defunctionalization. This translation may be useful when any of the mutually recursive procedures can be called by outside code, so there is no obvious case for inlining one procedure into the other. Such code then needs to be modified so that procedure calls are performed by bundling arguments into a variant record as described; alternately, wrapper procedures may be used for this task.

</doc>
<doc id="20036" url="https://en.wikipedia.org/wiki?curid=20036" title="Metasyntactic variable">
Metasyntactic variable

A metasyntactic variable is a placeholder name used in computer science, a word without meaning intended to be substituted by some objects pertaining to the context where it is used. The word foo as used in IETF Requests for Comments is a good example.
By mathematical analogy, a metasyntactic variable is a word that is a variable for other words, just as in algebra letters are used as variables for numbers. Any symbol or word which does not violate the syntactic rules of the language can be used as a metasyntactic variable. For specifications written in natural language, nonsense words are commonly used as metasyntactic variables.
Metasyntactic variables have a secondary, implied meaning to the reader (often students), which makes them different from normal metavariables. It is understood by those who have studied computer science that certain words are placeholders or examples only and should or must be replaced in a production-level computer program.
In hacker culture, "metasyntactic variable" has come to denote some typical (otherwise meaningless) words used as metavariables in computing; see reification. For example, "The Hacker's Dictionary" (1st ed.) defined FOO as "the first metasyntactic variable" and BAR as "the second metasyntactic variable", explaining that "When you have to invent an arbitrary temporary name for something for the sake of exposition, FOO is usually used. If you need a second one, BAR or BAZ is usually used; there is a slight preference at MIT for bar and at Stanford for baz. Clearly, bar was the original, for the concatenation FOOBAR is widely used also, and this in turn can be traced to the obscene acronym 'FUBAR' that arose in the armed forces during World War II. [...] A hacker avoids using 'foo' as the real name of anything. Indeed, a standard convention is that any file with 'foo' in its name is temporary and can be deleted on sight." The names of these consecrated "metasyntactic variables" are also commonly used as actual identifiers (for variables, functions, etc.) in tutorial programming examples when their purpose is to emphasize syntax; in this usage, "metasyntactic variable" is synonymous with "meaningless word".
Construction.
So "metasyntactic variable" denotes a word that "transcends grammar and can assume a value" or one that is "more comprehensive than suggested by its grammatical arrangement and is likely to vary". It may also denote a word that provides information about the grammatical arrangement of words by being able to assume a value that is expected to vary.
Examples.
RFC 772 (cited in RFC 3092) contains for instance:
Both the IETF RFCs and computer programming languages are rendered in plain text, making it necessary to distinguish metasyntactic variables by a naming convention, more or less obvious from context. If rich text formatting is available, e.g. as in the HTML produced from texinfo sources, then a typographical convention may be used, as done for the example in the GNU Fortran manual:
The above example uses italics to denote metavariables (borrowing from the common convention to use italics for variables in mathematics), although italics are also used in the same text for emphasizing other words. (The documentation for texinfo emphasizes the distinction between metavariables and mere variables used in a programming language being documented in some texinfo file as: "Use the @var command to indicate metasyntactic variables. A metasyntactic variable is something that stands for another piece of text. For example, you should use a metasyntactic variable in the documentation of a function to describe the arguments that are passed to that function. Do not use @var for the names of particular variables in programming languages. These are specific names from a program, so @code is correct for them.") Another point reflected in the above example is the convention that a metavariable is to be uniformly substituted with the same instance in all its appearances in a given schema. This is in contrast with nonterminal symbols in formal grammars where the nonterminals on the right of a production can be substituted by different instances.
A third example of the use of the "metasyntactic variables" foo and bar, this time as actual identifiers in a programming (interview) example is contrasting the following C++ function prototypes for their different argument passing mechanisms:
Words commonly used as metasyntactic variables.
Arabic.
In Arabic, the word "kedha" (كذا) is often used in the same way English speakers use the word "bla" as in, "kedha, kedha, kedha" to mean "this, that, and the other thing" or, "such and such". Similarly, the names "Fullan" (فلان) and "'Allan" (علان) are used to refer to non-specific persons, a practice which has been adopted in other languages (see Portuguese, Spanish, Turkish and Persian below).
Chinese.
A close term in Chinese is '如此这般', but it is more like the phrase 'blah blah blah' instead of just one word.
Czech.
The standard list of Czech metasyntactic variables is "huhu", "lala", "keke", "koko".
English.
A "standard list of metasyntactic variables used in syntax examples" often used in the United States is: foo, bar, baz, qux, quux, corge, grault, garply, waldo, fred, plugh, xyzzy, thud. The word "foo" occurs in over 330 RFCs and "bar" occurs in over 290. "Wibble", "wobble", "wubble", "Fred" and "flob" are often used in the UK.
Due to English being the foundation-language, or lingua franca, of most computer programming languages these variables are also commonly seen even in programs and examples of programs written for other spoken-language audiences.
The typical names may depend however on the subculture that has developed around a given programming language. For example, spam, ham, and eggs are the principal metasyntactic variables used in the Python programming language. This is a reference to the famous comedy sketch "Spam" by Monty Python, the eponym of the language.
The R programming language often adds "norf" to the list.
German.
In German, the words "bla", "blubb" and "blabla" are commonly used as names for metasyntactic variables (comparable with English "blah", "blah-blah").
French.
In French, the words "toto", "titi", "tata", "tutu", "truc", "bidule", "machin" and "azerty" are commonly used (AZERTY being the order of first letters on French keyboards). For representing people, "Untel Unetelle" or the family names "Dupont", "Durand" or "Tartempion" are used.
Hebrew.
In Hebrew, the words "chupchick" and "stam" are commonly used. 
Italian.
In Italian, the word "pippo" is commonly used. Besides being a diminutive of the first names "Giuseppe" (Joseph) and "Filippo" (Philip), "Pippo" is the Italian name of the Disney character Goofy. This name can be typed very quickly on a computer keyboard, as it involves three adjacent keys (, and ). Sometimes the words "pluto" and "paperino" (the Italian name of Donald Duck) can be used as additional terms. In non-technical usage, a list of generic person names is "Tizio", "Caio" and "Sempronio," dating back to the middle Ages; "un tizio" has the universally recognized meaning of "an unspecified or unknown guy". With a simiar meaning, but somewhat out of fashion, is also the name (complete with surname) "Tal dei Tali"; compare with Spanish below. "Bla bla" is also used as a placeholder for a lengthy but unspecified (or uninteresting) speech.
Japanese.
In Japanese, the words "hoge" and "piyo" are commonly used, with other common words and variants being "fuga, hogera," and "hogehoge." Note that "-ra" is a pluralizing ending in Japanese, and reduplication is also used for pluralizing. The origin of "hoge" as a metasyntactic variable is not known, but it is believed to date to the early 1980s. "Hoge" was also used extensively in the lyrics of the theme song for the Dororo animated adaptation, in 1969.
Portuguese.
In Portuguese, the words "fulano", "sicrano" and "beltrano" are commonly used to refer to people. To refer to objects in general, the most common placeholder name is XPTO.
Spanish.
In Spanish, the words "fulano", "mengano" "perengano", and "zutano" are commonly used, often followed by "de tal" ("of such"), mocking a lastname in Spanish form (e.g. "Fulano de Tal"). These words have the constraint that they can only be used to refer to people, as in the case with Portuguese. Also, when referring to an example of some person performing a certain action, "Perico de los Palotes" can also be used as a placeholder for a real name. In place of people or objects (including numbers, etc.) the usual X, Y, Z are used (e.g. "Person X", "Quantity Z").
Turkish.
In Turkish, the words "falan", "filan", "hede", "hödö", "hebele", "hübele" are commonly used.
Persian.
In Persian, the word "folân" is used for Foo and the words "bahmān" and "bisār" used for Bar.

</doc>
<doc id="20038" url="https://en.wikipedia.org/wiki?curid=20038" title="Mondegreen">
Mondegreen

A mondegreen is a mishearing or misinterpretation of a phrase as a result of near-homophony, in a way that gives it a new meaning.
Mondegreens are most often created by a person listening to a poem or a song; the listener, being unable to clearly hear a lyric, substitutes words that sound similar, and make some kind of sense. American writer Sylvia Wright coined the term in her essay "The Death of Lady Mondegreen", published in "Harper's Magazine" in November 1954.
The term was inspired by "...and Lady Mondegreen", a misinterpretation of the line "...and laid him on the green" from the Scottish ballad "The Bonnie Earl O' Moray". "Mondegreen" was included in the 2000 edition of the "Random House Webster's College Dictionary". Merriam-Webster's "Collegiate Dictionary" added the word in 2008. The phenomenon is not limited to English, with examples cited by Fyodor Dostoyevsky, in the Hebrew song "Háva Nagíla" ("Let's Be Happy"), and in Bollywood movies.
A closely related category is a Hobson-Jobson, where a word from a foreign language is homophonically translated into one's own language, e.g. "cockroach" from Spanish "cucaracha". For misheard lyrics this phenomenon is called soramimi. An unintentionally incorrect use of similar-sounding words or phrases in speaking is a malapropism. If there is a connection in meaning, it can be called an eggcorn. If a person stubbornly sticks to a mispronunciation after being corrected, that person has committed a mumpsimus.
Etymology.
In the essay, Wright described how, as a young girl, she misheard the last line of the first stanza from the 17th-century ballad "The Bonnie Earl o' Moray". She wrote:
The actual fourth line is "And "laid him on the green"." Wright explained the need for a new term:
Her essay had already described the bonny Earl holding the beautiful Lady Mondegreen's hand, both bleeding profusely but faithful unto death. She disputed:
Other examples Wright suggested are:
Psychology.
Human beings interpret their environment partially based on experience, and this includes speech perception. People are more likely to notice what they expect than things not part of their everyday experiences, and they may mistake an unfamiliar stimulus for a familiar and more plausible version. For example, to consider a well-known mondegreen in the song "Purple Haze", one would be more likely to hear Hendrix singing that he is about to "kiss this guy" than that he is about to "kiss the sky". Similarly, if a lyric uses words or phrases that the listener is unfamiliar with, they may be misheard as using more familiar terms.
The creation of mondegreens may be driven in part by a phenomenon akin to cognitive dissonance, as the listener may find it psychologically uncomfortable to listen to a song and not be able to make out the words, particularly if the listener is fluent in the language of the lyrics. Steven Connor suggests that mondegreens are the result of the brain's constant attempts to make sense of the world by making assumptions to fill in the gaps when it cannot clearly determine what it is hearing. Connor sees mondegreens as the "wrenchings of nonsense into sense".
On the other hand, Steven Pinker has observed that mondegreen mishearings tend to be "less" plausible than the original lyrics, and that once a listener has "locked in" to a particular misheard interpretation of a song's lyrics, it can remain unquestioned, even when that plausibility becomes strained (for more on this sort of stubbornness, see Mumpsimus). Pinker gives the example of a student "stubbornly" mishearing the chorus to "I'm Your Venus" as "I'm your penis", and being surprised that the song was allowed on the radio.
James Gleick claims that the mondegreen is a distinctly modern phenomenon. Although people have no doubt misconstrued song lyrics for as long as songs have been sung, without improved communication and the language standardization that accompanies it, he believes there would have been no way to recognize and discuss this shared experience. Since time immemorial, songs have been passed on by word of mouth. Just as mondegreens transform songs based on experience, a folk song learned by repetition of heard lyrics is often transformed over time when sung by people in a region where some of the song's references have become obscure. A classic example is "The Golden Vanity", which contains the line "As she sailed upon the lowland sea". English immigrants carried the song to Appalachia, where singers, not knowing what the term "lowland sea" refers to, transformed it over generations from "lowland" to "lonesome".
Examples.
In songs.
The top three mondegreens submitted regularly to mondegreen expert Jon Carroll are:
Both Creedence's John Fogerty and Hendrix eventually acknowledged these mishearings by deliberately singing the "mondegreen" versions of their songs in concert.
The 1963 song "Louie Louie" by The Kingsmen was so difficult to understand, because of how poorly the Kingsmen's version of it was recorded, that many people suspected the song contained obscene lyrics. The FBI was asked to investigate whether or not those involved with the song violated laws against the interstate transportation of obscene material. The most notable misinterpretation of the lyrics presented by the parent who sent the complaint can be found in the verse "Me see Jamaica moon above; / It won't be long me see me love. / Me take her in my arms and then / I tell her I never leave again". which was misheard as "She had a rag on, she moved above. / It won't be long, she'll slip it off. / I held her in my arms and then, / and I told her I'd rather lay her again". No lyrics were ever officially published for the song, and after two years of investigation, the FBI concluded that the lyrics were unintelligible.
Rap and hip hop lyrics may be particularly susceptible to being misheard because they do not necessarily follow standard pronunciations. The delivery of rap lyrics relies heavily upon an often regional pronunciation or non-traditional accenting of words and their phonemes to adhere to the artist's stylizations and the lyrics' written structure. This issue is exemplified in controversies over alleged transcription errors in Yale University Press's 2010 "Anthology of Rap".
"Blinded by the Light", a cover of a Bruce Springsteen song by Manfred Mann's Earth Band, contains what has been called "probably the most misheard lyric of all time". The phrase "revved up like a deuce" (altered from Springsteen's original "cut loose like a deuce", both lyrics referring to the hot rodders slang for a 1932 Ford coupé) is frequently misheard as "wrapped up like a douche". Springsteen himself has joked about the phenomenon, claiming that it was not until Manfred Mann rewrote the song to be about a "feminine hygiene product" that the song became popular.
Sometimes, the modified version of a lyric becomes standard, as is the case with "The Twelve Days of Christmas". The original has "four colly birds" ("colly" means "black"; in "A Midsummer Night's Dream", Shakespeare wrote "Brief as the lighting in the collied night."); sometime around the turn of the twentieth century, these became "calling" birds, which is the lyric used in the 1909 Frederic Austin version.
A number of misheard lyrics have been recorded, turning a mondegreen into a real title. The song "Sea Lion Woman", recorded in 1939 by Christine and Katherine Shipp, was performed by Nina Simone under the title "See Line Woman". According to the liner notes from the compilation "A Treasury of Library of Congress Field Recordings", the actual title of this playground song might also be "See Lyin' Woman" or "C-Line Woman". Jack Lawrence's misinterpretation of the French phrase "pauvre Jean" ("poor John") as the identically pronounced "pauvres gens" ("poor people") led to the translation of "La goualante du pauvre Jean" ("The Ballad of Poor John") as "The Poor People of Paris", a hit song in 1956.
Non-English language.
Ghil'ad Zuckermann cites the Hebrew example "mukhrakhím liyót saméakh" ("we must be happy", with a grammar mistake) instead of (the high-register) "úru 'akhím belév saméakh" ("wake up, brothers, with a happy heart"), from the well-known song "Háva Nagíla" ("Let’s be happy").
The Israeli site dedicated to Hebrew mondegreens has coined the term "avatiach" (Hebrew for watermelon) for "mondegreen", named for a common mishearing of Shlomo Artzi's award-winning 1970 song "Ahavtia" ("I loved her", using a form uncommon in spoken Hebrew).
The title of the film "La Vie en rose" depicting the life of Édith Piaf can be mistaken for "L'Avion rose" (The pink airplane).
The French word "lapalissade", designating a gross truism or platitude, is derived from the name of Jacques II de Chabannes, Seigneur de La Palice, because of a misread mondegreen in a mourning song written just after his heroic death. Reading an "f" as a long "s" (ſ), ""s’il n’était pas mort, il ferait encore envie"" ("if he was not dead, he would still arouse envy") becomes ""il serait encore en vie"" ("he would still be alive"). This truism remains as the first and most well-known "lapalissade" in French.
In literature.
The title of J. D. Salinger's "The Catcher in the Rye" is often mistaken for a mondegreen. The main character, Holden Caulfield, misremembers a sung version of the Robert Burns poem "Comin' Thro' the Rye": the line "Gin a body meet a body / comin' through the rye" is recalled as "Gin a body catch a body / comin' through the rye." This is not a mondegreen, but a result of a confabulation in Holden's psyche in line with the theme of the novel. 
"A Monk Swimming" by author Malachy McCourt is so titled because of a childhood mishearing of a phrase from the Catholic rosary prayer, Hail Mary. "Amongst women" became "a monk swimmin'".
Other examples.
The traditional game Chinese whispers ("Telephone" in the United States) involves mishearing a whispered sentence to produce a mondegreen.
Among schoolchildren in the U.S., daily rote recitation of the Pledge of Allegiance has long provided opportunities for the genesis of mondegreens.
Russian author Fyodor Dostoyevsky, in 1875, cited a line from Fyodor Glinka's song "Troika" (1825) "колокольчик, дар Валдая" ("the bell, gift of Valday") claiming that it is usually understood as "колокольчик, дарвалдая" ("the bell "darvaldaying""—the onomatopoetic verb for ringing).
The Turkish political party, the Democratic Party, changed its logo in 2007 to one of a white horse in front of a red background because rural voters often could not pronounce its Turkish name ("Demokrat"), instead saying "demir kırat" ("iron white-horse").
Reverse mondegreen.
Some nonsensical lyrics can be interpreted homophonically as rational text. A prominent example is "Mairzy Doats", a 1943 novelty song by Milton Drake, Al Hoffman, and Jerry Livingston. The lyrics are a mondegreen and it is up to the listener to figure out what they mean.
The refrain of the song repeats nonsensical sounding lines:
The clue to the meaning is contained in the bridge:
The listener can figure out that the last line of the refrain is "A kid'll eat ivy, too; wouldn't you?", but this line is sung only as a mondegreen.
Other examples include:
Deliberate mondegreen.
Two authors have written books of supposed foreign-language poetry that are actually mondegreens of nursery rhymes in English. Luis van Rooten's pseudo-French "" includes critical, historical, and interpretive apparatus, as does John Hulme's "Mörder Guss Reims", attributed to a fictitious German poet. Both titles sound like the phrase "Mother Goose Rhymes". Both works can also be considered soramimi, which produces different meanings when interpreted in another language. Wolfgang Amadeus Mozart produced a similar effect in his canon "Difficile Lectu" (written c. 1786-87, when he was 30 or 31), which, though ostensibly in Latin, is actually an opportunity for scatological humor in both German and Italian.
Some performers and writers have used deliberate mondegreens to create double entendres. The phrase "if you see Kay" (F-U-C-K) has been employed many times, notably as a line from James Joyce's 1922 novel "Ulysses" and in many songs, including by blues pianist Memphis Slim in 1963, R. Stevie Moore in 1977, April Wine on its 1982 album "Power Play", the Poster Children via their "Daisy Chain Reaction" in 1991, Turbonegro in 2005, Aerosmith in "Devil's Got a New Disguise in 2006, and The Script in their 2008 song "If You See Kay". Britney Spears did the same thing with the song "If U Seek Amy". A similar effect was created in Hindi in the 2011 Bollywood movie "Delhi Belly" in the song "Bhaag D.K. Bose". While "D. K. Bose" appears to be a person's name, it is sung repeatedly in the chorus to form the deliberate mondegreen ""bhosadi ke"" (Hindi: भोसडी के), a Hindi expletive. 
"Mondegreen" is a song by Yeasayer on their 2010 album, "Odd Blood". The lyrics are intentionally obscure (for instance, "Everybody sugar in my bed" and "Perhaps the pollen in the air turns us into a stapler") and spoken hastily to encourage the mondegreen effect.
Notes and references.
Notes
Citations

</doc>
<doc id="20039" url="https://en.wikipedia.org/wiki?curid=20039" title="Merge sort">
Merge sort

In computer science, merge sort (also commonly spelled mergesort) is an efficient, general-purpose, comparison-based sorting algorithm. Most implementations produce a stable sort, which means that the implementation preserves the input order of equal elements in the sorted output. Mergesort is a divide and conquer algorithm that was invented by John von Neumann in 1945. A detailed description and analysis of bottom-up mergesort appeared in a report by Goldstine and Neumann as early as 1948.
Algorithm.
Conceptually, a merge sort works as follows:
Top-down implementation.
Example C-like code using indices for top down merge sort algorithm that recursively splits the list (called "runs" in this example) into sublists until sublist size is 1, then merges those sublists to produce a sorted list. The copy back step could be avoided if the recursion alternated between two functions so that the direction of the merge corresponds with the level of recursion.
Bottom-up implementation.
Example C-like code using indices for bottom up merge sort algorithm which treats the list as an array of "n" sublists (called "runs" in this example) of size 1, and iteratively merges sub-lists back and forth between two buffers:
Top-down implementation using lists.
Pseudocode for top down merge sort algorithm which recursively divides the input list into smaller sublists until the sublists are trivially sorted, and then merges the sublists while returning up the call chain.
In this example, the function merges the left and right sublists.
Bottom-up implementation using lists.
Pseudocode for bottom up merge sort algorithm which uses a small fixed size array of references to nodes, where array is either a reference to a list of size 2 i or 0. "node" is a reference or pointer to a node. The merge() function would be similar to the one shown in the top down merge lists example, it merges two already sorted lists, and handles empty lists. In this case, merge() would use "node" for its input parameters and return value.
Natural merge sort.
A natural merge sort is similar to a bottom up merge sort except that any naturally occurring runs (sorted sequences) in the input are exploited. Both monotonic and bitonic (alternating up/down) runs may be exploited, with lists (or equivalently tapes or files) being convenient data structures (used as FIFO queues or LIFO stacks). In the bottom up merge sort, the starting point assumes each run is one item long. In practice, random input data will have many short runs that just happen to be sorted. In the typical case, the natural merge sort may not need as many passes because there are fewer runs to merge. In the best case, the input is already sorted (i.e., is one run), so the natural merge sort need only make one pass through the data. In many practical cases, long natural runs are present, and for that reason natural merge sort is exploited as the key component of Timsort. Example:
Tournament replacement selection sorts are used to gather the initial runs for external sorting algorithms.
Analysis.
In sorting "n" objects, merge sort has an average and worst-case performance of O("n" log "n"). If the running time of merge sort for a list of length "n" is "T"("n"), then the recurrence "T"("n") = 2"T"("n"/2) + "n" follows from the definition of the algorithm (apply the algorithm to two lists of half the size of the original list, and add the "n" steps taken to merge the resulting two lists). The closed form follows from the master theorem.
In the worst case, the number of comparisons merge sort makes is equal to or slightly smaller than ("n" ⌈lg "n"⌉ - 2⌈lg "n"⌉ + 1), which is between ("n" lg "n" - "n" + 1) and ("n" lg "n" + "n" + O(lg "n")).
For large "n" and a randomly ordered input list, merge sort's expected (average) number of comparisons approaches "α"·"n" fewer than the worst case where formula_1
In the "worst" case, merge sort does about 39% fewer comparisons than quicksort does in the "average" case. In terms of moves, merge sort's worst case complexity is O("n" log "n")—the same complexity as quicksort's best case, and merge sort's best case takes about half as many iterations as the worst case.
Merge sort is more efficient than quicksort for some types of lists if the data to be sorted can only be efficiently accessed sequentially, and is thus popular in languages such as Lisp, where sequentially accessed data structures are very common. Unlike some (efficient) implementations of quicksort, merge sort is a stable sort.
Merge sort's most common implementation does not sort in place ; therefore, the memory size of the input must be allocated for the sorted output to be stored in (see below for versions that need only "n"/2 extra spaces).
Merge sort also has some demerits. One is its use of 2"n" locations; the additional "n" locations are commonly used because merging two sorted sets in place is more complicated and would need more comparisons and move operations. But despite the use of this space the algorithm still does a lot of work: The contents of "m" are first copied into "left" and "right" and later into the list "result" on each invocation of "merge_sort" (variable names according to the pseudocode above).
Variants.
Variants of merge sort are primarily concerned with reducing the space complexity and the cost of copying.
A simple alternative for reducing the space overhead to "n"/2 is to maintain "left" and "right" as a combined structure, copy only the "left" part of "m" into temporary space, and to direct the "merge" routine to place the merged output into "m". With this version it is better to allocate the temporary space outside the "merge" routine, so that only one allocation is needed. The excessive copying mentioned previously is also mitigated, since the last pair of lines before the "return result" statement (function " merge "in the pseudo code above) become superfluous.
One drawback of merge sort, when implemented on arrays, is its working memory requirement. Several in-place variants have been suggested:
An alternative to reduce the copying into multiple lists is to associate a new field of information with each key (the elements in "m" are called keys). This field will be used to link the keys and any associated information together in a sorted list (a key and its related information is called a record). Then the merging of the sorted lists proceeds by changing the link values; no records need to be moved at all. A field which contains only a link will generally be smaller than an entire record so less space will also be used. This is a standard sorting technique, not restricted to merge sort.
Use with tape drives.
An external merge sort is practical to run using disk or tape drives when the data to be sorted is too large to fit into memory. External sorting explains how merge sort is implemented with disk drives. A typical tape drive sort uses four tape drives. All I/O is sequential (except for rewinds at the end of each pass). A minimal implementation can get by with just 2 record buffers and a few program variables.
Naming the four tape drives as A, B, C, D, with the original data on A, and using only 2 record buffers, the algorithm is similar to Bottom-up implementation, using pairs of tape drives instead of arrays in memory. The basic algorithm can be described as follows:
Instead of starting with very short runs, usually a hybrid algorithm is used, where the initial pass will read many records into memory, do an internal sort to create a long run, and then distribute those long runs onto the output set. The step avoids many early passes. For example, an internal sort of 1024 records will save 9 passes. The internal sort is often large because it has such a benefit. In fact, there are techniques that can make the initial runs longer than the available internal memory.
A more sophisticated merge sort that optimizes tape (and disk) drive usage is the polyphase merge sort.
Optimizing merge sort.
On modern computers, locality of reference can be of paramount importance in software optimization, because multilevel memory hierarchies are used. Cache-aware versions of the merge sort algorithm, whose operations have been specifically chosen to minimize the movement of pages in and out of a machine's memory cache, have been proposed. For example, the algorithm stops partitioning subarrays when subarrays of size S are reached, where S is the number of data items fitting into a CPU's cache. Each of these subarrays is sorted with an in-place sorting algorithm such as insertion sort, to discourage memory swaps, and normal merge sort is then completed in the standard recursive fashion. This algorithm has demonstrated better performance on machines that benefit from cache optimization. 
Also, many applications of external sorting use a form of merge sorting where the input get split up to a higher number of sublists, ideally to a number for which merging them still makes the currently processed set of pages fit into main memory.
Parallel merge sort.
Merge sort parallelizes well due to use of the divide-and-conquer method. Several parallel variants are discussed in the third edition of Cormen, Leiserson, Rivest, and Stein's "Introduction to Algorithms". The first of these can be very easily expressed in a pseudocode with fork and join keywords:
This algorithm is a trivial modification from the serial version, but its speedup is not impressive: when executed on an infinite number of processors, it runs in time, which is only a improvement on the serial version. A better result can be obtained by using a parallelized merge algorithm, which gives parallelism , meaning that this type of parallel merge sort runs in
time if enough processors are available. Such a sort can perform well in practice when combined with a fast stable sequential sort, such as insertion sort, and a fast sequential merge as a base case for merging small arrays.
Merge sort was one of the first sorting algorithms where optimal speed up was achieved, with Richard Cole using a clever subsampling algorithm to ensure merge. Other sophisticated parallel sorting algorithms can achieve the same or better time bounds with a lower constant. For example, in 1991 David Powers described a parallelized quicksort (and a related radix sort) that can operate in "O"(log "n") time on a CRCW PRAM with "n" processors by performing partitioning implicitly. Powers further shows that a pipelined version of Batcher's Bitonic Mergesort at "O"(log2"n") time on a butterfly sorting network is in practice actually faster than his "O"(log "n") sorts on a PRAM, and he provides detailed discussion of the hidden overheads in comparison, radix and parallel sorting.
Comparison with other sort algorithms.
Although heapsort has the same time bounds as merge sort, it requires only Θ(1) auxiliary space instead of merge sort's Θ("n"). On typical modern architectures, efficient quicksort implementations generally outperform mergesort for sorting RAM-based arrays. On the other hand, merge sort is a stable sort and is more efficient at handling slow-to-access sequential media. Merge sort is often the best choice for sorting a linked list: in this situation it is relatively easy to implement a merge sort in such a way that it requires only Θ(1) extra space, and the slow random-access performance of a linked list makes some other algorithms (such as quicksort) perform poorly, and others (such as heapsort) completely impossible.
As of Perl 5.8, merge sort is its default sorting algorithm (it was quicksort in previous versions of Perl). In Java, the Arrays.sort() methods use merge sort or a tuned quicksort depending on the datatypes and for implementation efficiency switch to insertion sort when fewer than seven array elements are being sorted. Python uses Timsort, another tuned hybrid of merge sort and insertion sort, that has become the standard sort algorithm in Java SE 7, on the Android platform, and in GNU Octave.

</doc>
<doc id="20040" url="https://en.wikipedia.org/wiki?curid=20040" title="Maule Air">
Maule Air

Maule Air, Inc. is a manufacturer of light, single-engined, short take-off and landing (STOL) aircraft, based in Moultrie, Georgia, USA. The company delivered 2,500 aircraft in its first 50 years of business.
History.
Belford D. Maule (1911–1995) designed his first aircraft, the M-1 starting at age 19. He founded the company Mechanical Products Co. in Jackson, Michigan to market his own starter design. In 1941 the B.D. Maule Co. was founded, and Maule produced tailwheels and fabric testers. In 1953 he began design work, and started aircraft production with the "Bee-Dee" M-4 in 1957.
The company is a family-owned enterprise. Its owner, June Maule, widow of B. D. Maule, remained directly involved with factory production until her death in 2009 at the age of 92.
Products.
The aircraft produced by Maule Air are tube-and-fabric designs and are popular with bush pilots, thanks to their very low stall speed, their tundra tires and oleo strut landing gear. Most Maules are built with tailwheel or amphibious configurations, although the newer MXT models have tricycle gear.

</doc>
<doc id="20041" url="https://en.wikipedia.org/wiki?curid=20041" title="Shoma Morita">
Shoma Morita

This is a page on the advancement of Constructive Living. This is not a valid or reliable scholarly source on Shoma Morita, the founder of Morita therapy.
Morita Masatake (1874-1938), also read as Morita Shoma (森田 正馬), was a contemporary of Sigmund Freud and the founder of Morita therapy, a branch of clinical psychology strongly influenced by Zen Buddhism. In his capacity as the head of psychiatry for a large Tokyo hospital, Morita began developing his methods while working with sufferers of "shinkeishitsu", or anxiety disorders with a hypochondriac base.
Theory and methods.
According to Morita, how a person feels is important as a and as an indicator for the present moment, but is uncontrollable: we don't create feelings, feelings happen to us. Since feelings do not cause our behavior, we can coexist with unpleasant feelings while still taking constructive action.
The essence of Morita's method maybe summarized in three rules: Accept all your feelings, know your purpose(s), and do what needs to be done. When once asked what shy people should do, Morita replied, "Sweat."
Karen Horney, an American psychologist, acknowledged the usefulness of Morita's techniques as did, by extension, Albert Ellis. Perhaps most notable of Morita's followers is David K. Reynolds. Dr. Reynolds synthesized parts of Morita therapy along with the practice of Naikan into Constructive Living, an educational method intended for English speaking Westerners. Constructive Living has since become extremely popular in Japan. Fritz Perls spent a week in a Morita Hospital in Japan.

</doc>
<doc id="20042" url="https://en.wikipedia.org/wiki?curid=20042" title="Montezuma">
Montezuma

Montezuma, Moctezuma, Moteczoma, Motecuhzoma, Moteuczomah, and Mwatazuma are variant spellings of the same word and may refer to:

</doc>
<doc id="20048" url="https://en.wikipedia.org/wiki?curid=20048" title="Mooney">
Mooney

Mooney is a family name, which is probably predominantly derived from the Irish Ó Maonaigh. It can also be spelled Moony, Moonie, Mainey, Mauney, Meaney and Meeney depending on the dialectic pronunciation that was Anglicised. 
Origins.
The origin of the different Moony or Mooney families is lost in antiquity. The name is derived from "maoin" a Gaelic word meaning "wealth" or "treasure of treasure", hence when O'Maonaigh was anglicised to Mooney it meant "the descendant of the wealthy one." 
According to Irish lore, the Mooney family comes from one of the largest and most noble Irish lines. They are said to be descendants of the ancient Irish King Heremon, who, along with his brother Herber, conquered Ireland. Heremon slew his brother shortly after their invasion, took the throne for himself, and fathered a line of kings of Ireland that include Malachi II, and King Niall of the Nine Hostages. 
Baptismal records, parish records, ancient land grants, the Annals of the Four Masters, and books by O'Hart, McLysaght, and O'Brien were all used in researching the history of the Mooney family name. These varied and often ancient records indicate that distant septs of the name arose in several places throughout Ireland. The most known and most numerous sept came from the county of Offaly. The members of this sept were from Chieftain Monach, son of Ailill Mor, Lord of Ulster, who was descended from the Kings of Connacht. These family members gave their name to town lands called Ballymooney both in that county and in the neighboring county of Leix.

</doc>
<doc id="20050" url="https://en.wikipedia.org/wiki?curid=20050" title="Minnesota Twins">
Minnesota Twins

The Minnesota Twins are a Major League Baseball (MLB) team based in Minneapolis, Minnesota. They play in the Central Division of the American League. The team is named after the Twin Cities area comprising Minneapolis and St. Paul. They played in Metropolitan Stadium from 1961 to 1981 and the Hubert H. Humphrey Metrodome from 1982 to 2009. They played their inaugural game at the newly completed Target Field on April 12, 2010.
The team was founded in Washington, D.C. in as one of the eight original teams of the American League, named the Washington Senators or Washington Nationals. Although the Washington team endured long bouts of mediocrity (immortalized in the 1955 Broadway musical "Damn Yankees"), they had a period of prolonged success in the 1920s and 1930s, led by Baseball Hall of Fame members Bucky Harris, Goose Goslin, Sam Rice, Heinie Manush, Joe Cronin, and above all Walter Johnson. Manager Clark Griffith joined the team in 1912 and became the team's owner in 1920. The franchise remained under Griffith family ownership until 1984.
In 1960, Major League Baseball granted the city of Minneapolis an expansion team. Washington owner Calvin Griffith, Clark's nephew and adopted son, requested that he be allowed to move his team to Minneapolis and instead give Washington the expansion team. Upon league approval, the team moved to Minnesota after the 1960 season, setting up shop in Metropolitan Stadium, while Washington fielded a brand new "Washington Senators" (which later became the Texas Rangers prior to the 1972 season).
Success came quickly to the team in Minnesota. Sluggers Harmon Killebrew and Bob Allison, who had already been stars in Washington, were joined by Tony Oliva and Zoilo Versalles, and later second baseman Rod Carew and pitchers Jim Kaat and Jim Perry, winning the American League pennant in 1965. A second wave of success came in the late 1980s and early 1990s under manager Tom Kelly, led by Kent Hrbek, Bert Blyleven, Frank Viola, and Kirby Puckett, winning the franchise's second and third World Series (and first in Minnesota).
Through the 2015 season, the franchise has won three World Series championships (1924, 1987, and 1991), and has fielded 18 American League batting champions.
Team history.
Washington Nationals/Senators: 1901–1960.
The Washington Senators spent the first decade of their existence finishing near the bottom of the American League standings. Their fortunes began to improve with the arrival of 19-year-old pitcher, Walter Johnson, in 1907. Johnson blossomed in 1911 with 25 victories, although the Senators still finished the season in seventh place. In 1912, the Senators improved dramatically, as their pitching staff led the league in team earned run average and in strikeouts. Johnson won 33 games while teammate Bob Groom added another 24 wins to help the Senators finish the season in second place. The Senators continued to perform respectably in 1913 with Johnson posting a career-high 35 victories, as the team once again finished in second place. The Senators then fell into another period of decline for the next decade. After a string of mediocre seasons, a rejuvenated Johnson rebounded in 1924 to win 23 games with the help of his catcher, Muddy Ruel, as the Senators won the American League pennant for the first time in the history of the franchise.
The Senators faced John McGraw's heavily favored New York Giants in the 1924 World Series. The two teams traded wins back and forth until the series reached the seventh and deciding game. The Senators trailed the Giants 3 to 1 in the eighth inning of Game 7, when Bucky Harris hit a routine ground ball to third which hit a pebble and took a bad hop over Giants third baseman Freddie Lindstrom. Two runners scored on the play, tying the score at three. An aging Walter Johnson then came in to pitch the ninth inning, and held the Giants scoreless into extra innings. In the bottom of the twelfth inning with Ruel at bat, he hit a high, foul ball directly over home plate. The Giants' catcher, Hank Gowdy, dropped his protective mask to field the ball but, failing to toss the mask aside, stumbled over it and dropped the ball, thus giving Ruel another chance to bat. On the next pitch, Ruel hit a double and proceeded to score the winning run when Earl McNeely hit a ground ball that took another bad hop over Lindstrom's head. This would mark the only World Series triumph for the franchise during their 60-year tenure in Washington.
The following season they repeated as American League champions but ultimately lost the 1925 World Series to the Pittsburgh Pirates. After Walter Johnson's retirement in 1927, he was hired as manager of the Senators. After enduring a few losing seasons, the team returned to contention in 1930. In 1933, Senators owner Clark Griffith returned to the formula that worked for him nine years prior: 26-year-old shortstop Joe Cronin became player-manager. The Senators posted a 99–53 record and cruised to the pennant seven games ahead of the New York Yankees, but in the 1933 World Series the Giants exacted their revenge winning in five games. Following the loss, the Senators sank all the way to seventh place in 1934 and attendance began to fall. Despite the return of Harris as manager from 1935–42 and again from 1950–54, Washington was mostly a losing ball club for the next 25 years contending for the pennant only during World War II. Washington came to be known as "first in war, first in peace, and last in the American League", with their hard luck being crucial to the plot of the musical and film "Damn Yankees". Cecil Travis, Buddy Myer (1935 A.L. batting champion), Roy Sievers, Mickey Vernon (batting champion in 1946 and 1953), and Eddie Yost were notable Senators players whose careers were spent in obscurity due to the team's lack of success. In 1954, the Senators signed future Hall of Fame member Harmon Killebrew. By 1959 he was the Senators’ regular third baseman and led the league with 42 home runs earning him a starting spot on the American League All-Star team.
After Griffith's death in 1955, his nephew and adopted son Calvin took over the team presidency. Calvin sold Griffith Stadium to the city of Washington and leased it back leading to speculation that the team was planning to move as the Boston Braves, St. Louis Browns and Philadelphia Athletics had all done in the early 1950s. By 1957, after an early flirtation with San Francisco (where the New York Giants would eventually move after that season ended), Griffith began courting Minneapolis–St. Paul, a prolonged process that resulted in his rejecting the Twin Cities' first offer before agreeing to relocate. The American League opposed the move at first, but in 1960 a deal was reached: The Senators would move and would be replaced with an expansion Senators team for 1961. Thus, the old Washington Senators became the Minnesota Twins.
Team nickname.
The Washington franchise was known as both "Senators" and "Nationals" at various times, and sometimes at the same time. In 1905, the team changed its official name to the "Washington Nationals." The name "Nationals" appeared on uniforms for only two seasons, and was then replaced with the "W" logo for the next 52 years. The media often shortened the nickname to "Nats." Many fans and newspapers (especially out-of-town papers) persisted in using the "Senators" nickname, because of potential confusion caused by an American League team using the "Nationals" name. Over time, "Nationals" faded as a nickname, and "Senators" became dominant. Baseball guides listed the club's nickname as "Nationals or Senators", acknowledging the dual-nickname situation.
The team name was officially changed to Washington Senators around the time of Clark Griffith's death. It was not until 1959 that the word "Senators" first appeared on team shirts. "Nats" continued to be used by space-saving headline writers, even for the 1961 expansion team, which was never officially known as "Nationals."
The current "Nationals" and "Nats" names were revived in 2005, when the Montreal Expos relocated to Washington to become the Nationals.
Minnesota Twins: 1961 to present.
The name "Twins" was derived from the popular name of the region, the Twin Cities. The NBA's Minneapolis Lakers had re-located to Los Angeles in 1960 due to poor attendance which was believed to have been caused in part by the reluctance of fans in St. Paul to support the team. Griffith was determined not to alienate fans in either city by naming the team after one city or the other, so his desire was to name the team the "Twin Cities Twins", however MLB objected. Griffith therefore named the team the "Minnesota Twins". However, the team was allowed to keep its original "TC" (for Twin Cities) insignia for its caps. The team's logo shows two men, one in a Minneapolis Millers uniform and one in a St. Paul Saints uniform, shaking hands across the Mississippi River. The "TC" remained on the Twins' caps until 1987, when they adopted their current uniforms. By this time, the team felt it was established enough to put an "M" on its cap without having St. Paul fans think it stood for Minneapolis. The "TC" logo was moved to a sleeve on the jerseys, and occasionally appeared as an alternate cap design. Both the "TC" and "Minnie & Paul" logos remain the team's primary insignia. As of 2010, the "TC" logo has been reinstated on the cap as their logo.
1960s.
The Twins were eagerly greeted in Minnesota when they arrived in 1961. They brought a nucleus of talented players: Harmon Killebrew, Bob Allison, Camilo Pascual, Zoilo Versalles, Jim Kaat, Earl Battey, and Lenny Green. Tony Oliva, who would go on to win American League batting championships in 1964, 1965 and 1971, made his major league debut in 1962. That year, the Twins won 91 games, the most by the franchise since 1933. Behind Mudcat Grant's 21 victories, Versalles' A.L. MVP season and Oliva's batting title, the Twins won 102 games and the American League Pennant in 1965, but they were defeated in the World Series by the Los Angeles Dodgers in seven games (behind the Series MVP, Sandy Koufax, who compiled a 2–1 record, including winning the seventh game).
Heading into the final weekend of the 1967 season, when Rod Carew was named the A.L. Rookie of the Year, the Twins, Boston Red Sox, Chicago White Sox, and Detroit Tigers all had a shot at clinching the American League championship. The Twins and the Red Sox started the weekend tied for 1st place and played against each other in Boston for the final three games of the season. The Red Sox won two out of the three games, seizing their first pennant since 1946 with a 92–70 record. The Twins and Tigers both finished one game back, with 91–71 records, while the White Sox finished three games back, at 89–73. In 1969, the new manager of the Twins, Billy Martin, pushed aggressive base running all-around, and Carew set the all-time Major League record by stealing home seven times in addition to winning the first of seven A.L. batting championships. With Killebrew slugging 49 homers and winning the AL MVP Award, these 1969 Twins won the very first American League Western Division Championship, but they lost three straight games to the Baltimore Orioles, winners of 109 games, in the first American League Championship Series. The Orioles would go on to be upset by the New York Mets in the World Series. Martin was fired after the season following an August fight in Detroit with 20-game winner Dave Boswell and outfielder Bob Allison, in an alley outside the Lindell A.C. bar. However Bill Rigney led the Twins to a repeat division title in 1970, behind the star pitching of Jim Perry (24-12), the A.L. Cy Young Award winner, while the Orioles again won the Eastern Division Championship behind the star pitching of Jim Palmer. Once again, the Orioles won the A.L. Championship Series in a three-game sweep, and this time they would win the World Series.
1970s.
After winning the division again in 1970, the team entered an eight-year dry spell, finishing around the .500 mark. Killebrew departed after 1974. Owner Calvin Griffith faced financial difficulty with the start of free agency, costing the Twins the services of Lyman Bostock and Larry Hisle, who left as free agents after the 1977 season, and Carew, who was traded after the 1978 season. In 1975, Carew won his fourth consecutive AL batting title, having already joined Ty Cobb as the only players to lead the major leagues in batting average for three consecutive seasons. In , Carew batted .388, which was the highest in baseball since Boston's Ted Williams hit .406 in ; he won the 1977 AL MVP Award. He won another batting title in 1978, hitting .333.
1980s–90s.
In 1982, the Twins moved into the Hubert H. Humphrey Metrodome, which they shared with the Minnesota Vikings. After a 16-54 start, the Twins were on the verge on becoming the worst team in MLB history. They turned the season around somewhat, but still lost 102 games which is the worst record in Twins history, despite the .301 average, 23 homers and 92 RBI from rookie Kent Hrbek. In 1984, Griffith sold the Twins to multi-billionaire banker/financier Carl Pohlad. The Metrodome hosted the 1985 Major League Baseball All-Star Game. After several losing seasons, the 1987 team, led by Hrbek, Gary Gaetti, Frank Viola (A.L. Cy Young winner in 1988), Bert Blyleven, Jeff Reardon, Tom Brunansky, Dan Gladden, and rising star Kirby Puckett, returned to the World Series after defeating the favored Detroit Tigers in the ALCS, 4 games to 1. Tom Kelly managed the Twins to World Series victories over the St. Louis Cardinals in 1987 and the Atlanta Braves in 1991. The 1988 Twins were the first team in American League history to draw more than 3 million fans. Twins' pitcher and Minnesota native Jack Morris was the star of the series in 1991, going 2–0 in his three starts with a 1.17 ERA. 1991 also marked the first time that any team that finished in last place in their division would advance to the World Series the following season; both the Twins and the Braves did this in 1991. Contributors to the 1991 Twins' improvement from 74 wins to 95 included Chuck Knoblauch, the A.L. Rookie of the Year; Scott Erickson, 20-game winner; new closer Rick Aguilera and new designated hitter Chili Davis.
The World Series in 1991 is regarded by many as one of the classics of all time. In this Series, four games were won during the teams' final at-bat, and three of these were in extra innings. The Atlanta Braves won all three of their games in Atlanta, and the Twins won all four of their games in Minnesota. The sixth game was a legendary one for Puckett, who tripled in a run, made a sensational leaping catch against the wall, and finally in the 11th inning hit the game-winning home run. The seventh game was tied 0–0 after the regulation nine innings, and marked only the second time that the seventh game of the World Series had ever gone into extra innings. The Twins won on a walk-off RBI single by Gene Larkin in the bottom of the 10th inning, after Morris had pitched ten shutout innings against the Braves. The seventh game of the 1991 World Series is widely regarded as one of the greatest games in the history of professional baseball.
After a winning season in 1992 but falling short of Oakland in the division, the Twins fell into a years-long stretch of mediocrity, posting a losing record each season for the next eight: 71–91 in 1993, 50–63 in 1994, 56–88 in 1995, 78–84 in 1996, 68–94 in 1997, 70–92 in 1998, 63–97 in 1999 and 69–93 in 2000. From 1994 to 1997, a long sequence of retirements and injuries hurt the team badly, and Tom Kelly spent the remainder of his managerial career attempting to rebuild the Twins. In 1997, owner Carl Pohlad almost sold the Twins to North Carolina businessman Don Beaver, who would have moved the team to the Piedmont Triad area.
Puckett after the 1995 season was forced to retire at age 35 due to loss of vision in one eye from a central retinal vein occlusion. The 1989 A.L. batting champion, he retired as the Twins' all-time leader in career hits, runs, doubles, and total bases. At the time of his retirement, his .318 career batting average was the highest by any right-handed American League batter since Joe DiMaggio. Puckett was the fourth baseball player during the 20th century to record 1,000 hits in his first five full calendar years in Major League Baseball, and was the second to record 2,000 hits during his first 10 full calendar years. He was elected to the Baseball Hall of Fame in 2001, his first year of eligibility.
2000s.
The Twins dominated the Central Division in the first decade of the new century, winning the division in six of those ten years ('02, '03, '04, '06, '09 and '10), and nearly winning it in '08 as well. From 2001 to 2006, the Twins compiled the longest streak of consecutive winning seasons since moving to Minnesota.
Threatened with closure by league contraction, the 2002 team battled back to reach the American League Championship Series before being eliminated 4–1 by that year's World Series champion Anaheim Angels.
2006.
In 2006, the Twins won the division on the last day of the regular season (the only day all season they held sole possession of first place) but lost to the Oakland Athletics in the ALDS. Ozzie Guillén coined a nickname for this squad, calling the Twins "little piranhas". The Twins players embraced the label, and in response, the Twins Front office started a "Piranha Night", with piranha finger puppets given out to the first 10,000 fans. Scoreboard operators sometimes played an animated sequence of piranhas munching under that caption in situations where the Twins were scoring runs playing "small ball", and the stadium vendors sold T-shirts and hats advertising "The Little Piranhas".
The Twins also had the AL MVP in Justin Morneau, the AL batting champion in Joe Mauer, and the AL Cy Young Award winner in Johan Santana.
2008.
In 2008, the Twins finished the regular season tied with the White Sox on top of the AL Central, forcing a one-game playoff in Chicago to determine the division champion. The Twins lost that game and missed the playoffs. The game location was determined by rule of a coin flip that was conducted in mid-September. This rule was changed for the start of the 2009 season, making the site for any tiebreaker game to be determined by the winner of the regular season head-to-head record between the teams involved.
2009.
After a mediocre year where the Twins played .500 baseball for most of the season, the team won 17 of their last 21 games to tie the Detroit Tigers for the lead in the Central Division. The Twins were able to use the play-in game rule to their advantage when they won the AL Central at the end of the regular season by way of a 6–5 tiebreaker game that concluded with a 12th-inning walk-off hit by Alexi Casilla. However, they failed to advance to the American League Championship Series as they lost the American League Divisional Series in three straight games to the eventual World Series champion New York Yankees. That year, Joe Mauer became only the second catcher in 33 years to win the AL MVP award. Iván Rodríguez won for the Texas Rangers in 1999, previous to that, the last catcher to win an AL MVP was the New York Yankees Thurman Munson in 1976.
2010.
In their inaugural season played at Target Field, the Twins finished the regular season with a record of 94-68, clinching the AL Central Division title for the 6th time in 9 years under manager Ron Gardenhire. New regular players included rookie Danny Valencia at third base, designated hitter Jim Thome, closer Matt Capps, infielder J. J. Hardy, and infielder Orlando Hudson. In relief pitching roles were late additions Brian Fuentes and Randy Flores. On July 7, the team suffered a major blow when Justin Morneau sustained a concussion, which knocked him out for the rest of the season. In the divisional series, the Twins lost to the Yankees in a three-game sweep for the second consecutive year. Following the season, Ron Gardenhire received AL Manager of the Year honors after finishing as a runner up in several prior years.
2011.
After repeating as AL Central champions in 2010, the Twins entered 2011 with no players on the disabled list, and the team seemed poised for another strong season. During the off-season, the team signed Japanese shortstop Tsuyoshi Nishioka to fill a hole in the middle infield, re-signed Jim Thome, who was in pursuit of career home run number 600, and also re-signed Carl Pavano. However, the season was largely derailed by an extensive list of injuries. Nishioka's broken leg in a collision at second base led the way and was followed by DL stints from Kevin Slowey, Joe Mauer, Jason Repko, Thome, Delmon Young (two stints on the DL), José Mijares, Glen Perkins, Joe Nathan, Francisco Liriano, Jason Kubel, Denard Span (two stints), Justin Morneau, Scott Baker, and Alexi Casilla. The team's low point was arguably on May 1 when the team started 7 players who were batting below .235 in a game against Kansas City. From that day forward, the Twins made a strong push to get as close as five games back of the division lead by the All-Star break. However, the team struggled down the stretch and fell back out of contention. The team failed to reach the playoffs for the first time since 2008 and experienced their first losing season in four years. Despite an AL-worst 63-99 record, the team drew over 3 million fans for the second consecutive year.
Michael Cuddyer served as the Twins representative at the All-Star game, his first appearance. Bert Blyleven's number was retired during the season and he was also inducted into the Baseball Hall of Fame during the month of July. On August 10, Nathan recorded his 255th save, passing Rick Aguilera for first place on the franchise's all-time saves list. On August 15, Thome hit 599th and 600th home run at Comerica Park to become the eighth player in Major League history to hit 600 home runs, joining Babe Ruth, Willie Mays, Hank Aaron, Barry Bonds, Sammy Sosa, Ken Griffey, Jr., and Alex Rodriguez.
2012–2014.
The team started the 2012 season with a league worst 10-24 record. In late May and early June, the team embarked on a hot streak, winning ten out of thirteen games. By mid July, the team found themselves only 10 games out of the division lead. On July 16, the Twins defeated the Baltimore Orioles 19-7, the most runs scored in the short history of Target Field. By the end of August, the Twins were more than 20 games below .500, and last in the American League. On August 29, it was announced that the Twins would host the 2014 All-Star Game. In 2013, the Twins finished in 4th place in the AL Central, with a record of 66-96. In 2014, the team finished with a 70-92 record, last in the division and accumulated the second fewest wins in the American League. As a result, Ron Gardenhire was fired on September 29, 2014.
On November 3, Paul Molitor was announced by the team as the 13th manager in Twins history.
Threatened contraction or relocation of the team.
The quirks of the Hubert H. Humphrey Metrodome, including the turf floor and the white roof, gave the Twins a significant home-field advantage that played into their winning the World Series in both 1987 and 1991, at least in the opinion of their opponents, as the Twins went 12–1 in postseason home games during those two seasons. These were the first two World Series in professional baseball history in which a team won the championship by winning all four home games. (The feat has since been repeated once, by the Arizona Diamondbacks in 2001.) Nevertheless, the Twins argued that the Metrodome was obsolete and that the lack of a dedicated baseball-only ballpark limited team revenue and made it difficult to sustain a top-notch, competitive team (the Twins had been sharing tenancy in stadiums with the NFL's Minnesota Vikings since 1961). The team was rumored to contemplate moving to such places as New Jersey, Las Vegas, Portland, Oregon, the Greensboro/Winston-Salem, North Carolina, area, and elsewhere in search of a more financially competitive market. In 2002, the team was nearly disbanded when Major League Baseball selected the Twins and the Montreal Expos (now the Washington Nationals franchise) for elimination due to their financial weakness relative to other franchises in the league. The impetus for league contraction diminished after a court decision forced the Twins to play out their lease on the Metrodome. However, Twins owner Carl Pohlad continued his efforts to relocate, pursuing litigation against the Metropolitan Stadium Commission and obtaining a state court ruling that his team was not obligated to play in the Metrodome after the 2006 season. This cleared the way for the Twins to either be relocated or disbanded prior to the 2007 season if a new deal was not reached.
Target Field.
In response to the threatened loss of the Twins, the Minnesota private and public sector negotiated and approved a financing package for a replacement stadium— a baseball-only outdoor, natural turf ballpark in the Warehouse District of downtown Minneapolis— owned by a new entity known as the Minnesota Ballpark Authority. Target Field was constructed at a cost of $544.4 million (including site acquisition and infrastructure), utilizing the proceeds of a $392 million public bond offering based on a 0.15% sales tax in Hennepin County and private financing of $185 million provided by the Pohlad family. As part of the deal, the Twins also signed a 30-year lease of the new stadium, effectively guaranteeing the continuation of the team in Minnesota for a long time to come. Construction of the new field began in 2007, and was completed in December 2009, in time for the 2010 season. Commissioner Bud Selig, who earlier had threatened to disband the team, observed that without the new stadium the Twins could not have committed to sign their star player, catcher Joe Mauer, to an 8-year, $184 million contract extension. The first regular season game in Target Field was played against the Boston Red Sox on April 12, 2010, with Mauer driving in two runs and going 3-for-5 to help the Twins defeat the Red Sox, 5–2.
On May 18, 2011, Target Field was named "The Best Place To Shop" by Street and Smith's "SportsBusiness Journal" at the magazine's 2011 Sports Business Awards Ceremony in New York City. It was also named "The Best Sports Stadium in North America" by "ESPN The Magazine" in a ranking that included over 120 different stadiums, ballparks and arenas from around North America.
In July, 2014, Target Field hosted the 85th Major League Baseball All-Star Game and the Home Run Derby.
Current roster.
Minnesota Twins all-time roster: A complete list of players who played in at least one game for the Twins franchise.
Achievements.
Baseball Hall of Famers.
Molitor and Winfield, St. Paul natives and University of Minnesota graduates, came to the team late in their careers and were warmly received as "hometown heroes", but were elected to the Hall on the basis of their tenures with other teams. Both swatted their 3,000th hit with the Twins.
Cronin, Goslin, Griffith, Harris, Johnson, Killebrew and Wynn are listed on the Washington Hall of Stars display at Nationals Park (previously they were listed at Robert F. Kennedy Stadium). So are Ossie Bluege, George Case, Joe Judge, George Selkirk, Roy Sievers, Cecil Travis, Mickey Vernon and Eddie Yost.
Retired numbers.
The Metrodome's upper deck in center and right fields was partly covered by a curtain containing banners of various titles won, and retired numbers. There was no acknowledgment of the Twins' prior championships in Washington and several Senator Hall of Famers, such as Walter Johnson, played in the days prior to numbers being used on uniforms. However Killebrew played seven seasons as a Senator, including two full seasons as a regular prior to the move to Minnesota in 1961.
The numbers that have been retired hang within Target Field in front of the tower that serves as the Twins' executive offices in left field foul territory. The championships banners have been replaced by small pennants that fly on masts at the back of the left field upper deck. Those pennants, along with the flags flying in the plaza behind right field, serve as a visual cue for the players, suggesting the wind direction and speed.
Jackie Robinson's number, 42, was retired by Major League Baseball on April 15, 1997 and formally honored by the Twins on May 23, 1997. Robinson's number was positioned to the left of the Twins numbers in both venues.
Radio and television.
As of 2007, the Twins took the rights to the broadcasts in-house and created the Twins Radio Network (TRN). With that new network in place the Twins secured a new Metro Affiliate flagship radio station in KSTP. It replaced WCCO, which held broadcast rights for the Twins since the team moved to Minneapolis in 1961. For 2013, the Twins moved to FM radio on KTWN-FM "96.3 K-Twin", which is owned by the Pohlad family. The original radio voices of the Twins in 1961 were Ray Scott, Halsey Hall and Bob Wolff. After the first season, Herb Carneal replaced Wolff. Twins TV and radio broadcasts were originally sponsored by the Hamm's Brewing Company. In 2009, Treasure Island Resort & Casino became the first ever naming rights partner for the Twins Radio Network, making the commercial name of TRN the Treasure Island Baseball Network.
Cory Provus is the current radio play by play announcer, taking over in 2012 for longtime Twins voice John Gordon who retired following the 2011 season. Former Twins OF Dan Gladden serves as color commentator.
TRN broadcasts are originated from the studios at Minnesota News Network and Minnesota Farm Networks. Kris Atteberry hosts the pre-game show, the "Lineup Card" and the "Post-game Download" from those studios except when filling in for Provus or Gladden when they are on vacation.
On April 1, 2007, Herb Carneal, the radio voice of the Twins for all but one year of their existence, died at his home in Minnetonka, Minnesota after a long battle with a list of illnesses. Carneal is in the broadcasters wing of the Baseball Hall of Fame.
The television rights are held by Fox Sports North with Dick Bremer as the play-by-play announcer and former Twin, 2011 National Baseball Hall of Fame inductee, Bert Blyleven as color analyst. They are sometimes joined by Ron Coomer and Roy Smalley.
Bob Casey was the Twins first public-address announcer starting in 1961 and continuing until his death in 2005. He was well known for his unique delivery and his signature announcements of "No smoking in the Metrodome, either go outside or quit!" (or "go back to Boston", etc.), "Batting 3rd, the center-fielder, No. 34, Kirby Puckett!!!" and asking fans not to 'throw anything or anybody' onto the field.
Team and franchise traditions.
Fans wave a "Homer Hanky" to rally the team during play-offs and other crucial games. The Homer Hanky was created by Terrie Robbins of the Star Tribune newspaper in the Twin Cities in 1987. It was her idea to originally give away 60,000 inaugural Homer Hankies. That year, over 2.3 million Homer Hankies were distributed.
The party atmosphere of the Twins clubhouse after a win is well-known, the team's players unwinding with loud rock music (usually the choice of the winning pitcher) and video games.
The club has several hazing rituals, such as requiring the most junior relief pitcher on the team to carry water and snacks to the bullpen in a brightly colored small child's backpack (Barbie in 2005, SpongeBob SquarePants in 2006, Hello Kitty in 2007, Disney Princess and Tinkerbell in 2009, Chewbacca and Darth Vader in 2010), and many of its players, both past and present, are notorious pranksters. For example, Bert Blyleven earned the nickname "The Frying Dutchman" for his ability to pull the "hotfoot" – which entails crawling under the bench in the dugout and lighting a teammate's shoelaces on fire.

</doc>
<doc id="20051" url="https://en.wikipedia.org/wiki?curid=20051" title="Mach number">
Mach number

In fluid dynamics, the Mach number (M or Ma) (; ) is a dimensionless quantity representing the ratio of flow velocity past a boundary to the local speed of sound.
where
In the simplest explanation, the speed of Mach 1 is equal to the speed of sound. Therefore, Mach 0.65 is about 65% of the speed of sound (subsonic), and Mach 1.35 is about 35% faster than the speed of sound (supersonic).
The local speed of sound, and thereby the Mach number, depends on the condition of the surrounding medium, in particular the temperature and pressure. The Mach number is primarily used to determine the approximation with which a flow can be treated as an incompressible flow. The medium can be a gas or a liquid. The boundary can be traveling in the medium, or it can be stationary while the medium flows along it, or they can both be moving, with different velocities: what matters is their relative velocity with respect to each other. The boundary can be the boundary of an object immersed in the medium, or of a channel such as a nozzle, diffusers or wind tunnels chaneling the medium. As the Mach number is defined as the ratio of two speeds, it is a dimensionless number. If  < 0.2–0.3 and the flow is quasi-steady and isothermal, compressibility effects will be small and a simplified incompressible flow equations can be used.
The Mach number is named after Austrian physicist and philosopher Ernst Mach, a designation proposed by aeronautical engineer Jakob Ackeret. As the Mach number is a dimensionless quantity rather than a unit of measure, with Mach, the number comes "after" the unit; the second Mach number is "Mach 2" instead of "2 Mach" (or Machs). This is somewhat reminiscent of the early modern ocean sounding unit "mark" (a synonym for fathom), which was also unit-first, and may have influenced the use of the term Mach. In the decade preceding faster-than-sound human flight, aeronautical engineers referred to the speed of sound as "Mach's number", never "Mach 1."
Overview.
At standard sea level conditions (corresponding to a temperature of 15 degrees Celsius), the speed of sound is 340.3 m/s (1225 km/h, or 761.2 mph, or 661.5 knots, or 1116 ft/s) in the Earth's atmosphere. The speed represented by Mach 1 is not a constant; for example, it is mostly dependent on temperature.
Since the speed of sound increases as the ambient temperature increases, the actual speed of an object traveling at Mach 1 will depend on the temperature of the fluid through which the object is passing. Mach number is useful because the fluid behaves in a similar manner at a given Mach number, regardless of other variables. So, an aircraft traveling at Mach 1 at 20°C (68°F) at sea level will experience shock waves just like an aircraft traveling at Mach 1 at 11,000 m (36,000 ft) altitude at −50°C (−58°F), even though the second aircraft is only traveling 86% as fast as the first.
Classification of Mach regimes.
While the terms "subsonic" and "supersonic," in the purest sense, refer to speeds below and above the local speed of sound respectively, aerodynamicists often use the same terms to talk about particular ranges of Mach values. This occurs because of the presence of a "transonic regime" around M = 1 where approximations of the Navier-Stokes equations used for subsonic design actually no longer apply; the simplest explanation is that the flow locally begins to exceed M = 1 even though the freestream Mach number is below this value.
Meanwhile, the "supersonic regime" is usually used to talk about the set of Mach numbers for which linearised theory may be used, where for example the (air) flow is not chemically reacting, and where heat-transfer between air and vehicle may be reasonably neglected in calculations.
In the following table, the "regimes" or "ranges of Mach values" are referred to, and not the "pure" meanings of the words "subsonic" and "supersonic".
Generally, NASA defines "high" hypersonic as any Mach number from 10 to 25, and re-entry speeds as anything greater than Mach 25. Aircraft operating in this regime include the Space Shuttle and various space planes in development.
High-speed flow around objects.
Flight can be roughly classified in six categories:
For comparison: the required speed for low Earth orbit is approximately 7.5 km/s = Mach 25.4 in air at high altitudes. The speed of light in a vacuum corresponds to a Mach number of approximately 880,991.09 (relative to air at sea level).
At transonic speeds, the flow field around the object includes both sub- and supersonic parts. The transonic period begins when first zones of M > 1 flow appear around the object. In case of an airfoil (such as an aircraft's wing), this typically happens above the wing. Supersonic flow can decelerate back to subsonic only in a normal shock; this typically happens before the trailing edge. (Fig.1a)
As the speed increases, the zone of M > 1 flow increases towards both leading and trailing edges. As M = 1 is reached and passed, the normal shock reaches the trailing edge and becomes a weak oblique shock: the flow decelerates over the shock, but remains supersonic. A normal shock is created ahead of the object, and the only subsonic zone in the flow field is a small area around the object's leading edge. (Fig.1b)
Fig. 1. "Mach number in transonic airflow around an airfoil; M < 1 (a) and M > 1 (b)."
When an aircraft exceeds Mach 1 (i.e. the sound barrier), a large pressure difference is created just in front of the aircraft. This abrupt pressure difference, called a shock wave, spreads backward and outward from the aircraft in a cone shape (a so-called Mach cone). It is this shock wave that causes the sonic boom heard as a fast moving aircraft travels overhead. A person inside the aircraft will not hear this. The higher the speed, the more narrow the cone; at just over M = 1 it is hardly a cone at all, but closer to a slightly concave plane.
At fully supersonic speed, the shock wave starts to take its cone shape and flow is either completely supersonic, or (in case of a blunt object), only a very small subsonic flow area remains between the object's nose and the shock wave it creates ahead of itself. (In the case of a sharp object, there is no air between the nose and the shock wave: the shock wave starts from the nose.)
As the Mach number increases, so does the strength of the shock wave and the Mach cone becomes increasingly narrow. As the fluid flow crosses the shock wave, its speed is reduced and temperature, pressure, and density increase. The stronger the shock, the greater the changes. At high enough Mach numbers the temperature increases so much over the shock that ionization and dissociation of gas molecules behind the shock wave begin. Such flows are called hypersonic.
It is clear that any object traveling at hypersonic speeds will likewise be exposed to the same extreme temperatures as the gas behind the nose shock wave, and hence choice of heat-resistant materials becomes important.
High-speed flow in a channel.
As a flow in a channel becomes supersonic, one significant change takes place. The conservation of mass flow rate leads one to expect that contracting the flow channel would increase the flow speed (i.e. making the channel narrower results in faster air flow) and at subsonic speeds this holds true. However, once the flow becomes supersonic, the relationship of flow area and speed is reversed: expanding the channel actually increases the speed.
The obvious result is that in order to accelerate a flow to supersonic, one needs a convergent-divergent nozzle, where the converging section accelerates the flow to sonic speeds, and the diverging section continues the acceleration. Such nozzles are called de Laval nozzles and in extreme cases they are able to reach hypersonic speeds ( at 20°C).
An aircraft Machmeter or electronic flight information system (EFIS) can display Mach number derived from stagnation pressure (pitot tube) and static pressure.
Calculation.
The Mach number at which an aircraft is flying can be calculated by
where:
Note that the dynamic pressure can be found as:
Assuming air to be an ideal gas, the formula to compute Mach number in a subsonic compressible flow is derived from Bernoulli's equation for M < 1:
where:
The formula to compute Mach number in a supersonic compressible flow is derived from the Rayleigh Supersonic Pitot equation:
Calculating Mach Number from Pitot Tube Pressure.
At altitude, for reasons explained, Mach number is a function of temperature.
Aircraft flight instruments, however, operate using pressure differential to compute Mach number, not temperature. The assumption is that a particular pressure represents a particular altitude and, therefore, a standard temperature. Aircraft flight instruments need to operate this way because the stagnation pressure sensed by a Pitot tube is dependent on altitude as well as speed.
Assuming air to be an ideal gas, the formula to compute Mach number in a subsonic compressible flow is found from Bernoulli's equation for M < 1 (above):
The formula to compute Mach number in a supersonic compressible flow can be found from the Rayleigh Supersonic Pitot equation (above) using parameters for air:
where:
As can be seen, M appears on both sides of the equation. The easiest method to solve the supersonic M calculation is to enter both the subsonic and supersonic equations into a computer spreadsheet such as Microsoft Excel, OpenOffice.org Calc, or some equivalent program to solve it numerically. It is first determined whether M is indeed greater than 1.0 by calculating M from the subsonic equation. If M is greater than 1.0 at that point, then the value of M from the subsonic equation is used as the initial condition in the supersonic equation. Then a simple iteration of the supersonic equation is performed, each time using the last computed value of M, until M converges to a value—usually in just a few iterations. Alternatively, Newton's method can also be used.

</doc>
<doc id="20053" url="https://en.wikipedia.org/wiki?curid=20053" title="March 8">
March 8


</doc>
<doc id="20054" url="https://en.wikipedia.org/wiki?curid=20054" title="March 9">
March 9


</doc>
<doc id="20055" url="https://en.wikipedia.org/wiki?curid=20055" title="Moving Picture Experts Group">
Moving Picture Experts Group

The Moving Picture Experts Group (MPEG) is a working group of authorities that was formed by ISO and IEC to set standards for audio and video compression and transmission. It was established in 1988 by the initiative of Hiroshi Yasuda (Nippon Telegraph and Telephone) and Leonardo Chiariglione, group Chair since its inception. The first MPEG meeting was in May 1988 in Ottawa, Canada. As of late 2005, MPEG has grown to include approximately 350 members per meeting from various industries, universities, and research institutions. MPEG's official designation is ISO/IEC JTC 1/SC 29/WG 11 – "Coding of moving pictures and audio" (ISO/IEC Joint Technical Committee 1, Subcommittee 29, Working Group 11).
Sub Groups.
ISO/IEC JTC1/SC29/WG11 – "Coding of moving pictures and audio" has following Sub Groups (SG):
Cooperation with other groups.
Joint Video Team.
Joint Video Team (JVT) is joint project between ITU-T SG16/Q.6 (Study Group 16 / Question 6) – VCEG (Video Coding Experts Group) and ISO/IEC JTC1/SC29/WG11 – MPEG for the development of new video coding recommendation and international standard. It was formed in 2001 and its main result has been H.264/MPEG-4 AVC (MPEG-4 Part 10).
Joint Collaborative Team on Video Coding.
Joint Collaborative Team on Video Coding (JCT-VC) is a group of video coding experts from ITU-T Study Group 16 (VCEG) and ISO/IEC JTC 1/SC 29/WG 11 (MPEG). It was created in 2010 to develop High Efficiency Video Coding, a new generation video coding standard that further reduces (by 50%) the data rate required for high quality video coding, as compared to the current ITU-T H.264 / ISO/IEC 14496-10 standard. JCT-VC is co-chaired by Jens-Rainer Ohm and Gary Sullivan.
Standards.
The MPEG standards consist of different "Parts". Each "part" covers a certain aspect of the whole specification. The standards also specify "Profiles" and "Levels". "Profiles" are intended to define a set of tools that are available, and "Levels" define the range of appropriate values for the properties associated with them. Some of the approved MPEG standards were revised by later amendments and/or new editions. MPEG has standardized the following compression formats and ancillary standards:
MPEG-4 has been chosen as the compression scheme for over-the-air in Brazil (ISDB-TB), based on original digital television from Japan (ISDB-T).
In addition, the following standards, while not sequential advances to the video encoding standard as with MPEG-1 through MPEG-4, are referred to by similar notation:
Moreover, more recently than other standards above, MPEG has started following international standards; each of the standards holds multiple MPEG technologies for a way of application. (For example, MPEG-A includes a number of technologies on multimedia application format.)
Standardization process.
A standard published by ISO/IEC is the last stage of a long process that starts with the proposal of new work within a committee. Here are some abbreviations used for marking a standard with its status:
Other abbreviations:
A proposal of work (New Proposal) is approved at Subcommittee and then at the Technical Committee level (SC29 and JTC1 respectively – in the case of MPEG). When the scope of new work is sufficiently clarified, MPEG usually makes open requests for proposals – known as "Call for proposals". The first document that is produced for audio and video coding standards is called a Verification Model (VM). In the case of MPEG-1 and MPEG-2 this was called Simulation and Test Model, respectively. When a sufficient confidence in the stability of the standard under development is reached, a Working Draft (WD) is produced. This is in the form of a standard but is kept internal to MPEG for revision. When a WD is sufficiently solid, becomes Committee Draft (CD) (usually at the planned time). It is then sent to National Bodies (NB) for ballot. The CD becomes Final Committee Draft (FCD) if the number of positive votes is above the quorum. After a review and comments issued by NBs, FCD is again submitted to NBs for the second ballot. If the FCD is approved, it becomes Final Draft International Standard (FDIS). ISO then holds a ballot with National Bodies, where no technical changes are allowed (yes/no ballot). If approved, the document becomes International Standard (IS).
ISO/IEC Directives allow also the so-called "Fast-track procedure". In this procedure a document is submitted directly for approval as a draft International Standard (DIS) to the ISO member bodies or as a final draft International Standard (FDIS) if the document was developed by an international standardizing body recognized by the ISO Council.

</doc>
<doc id="20056" url="https://en.wikipedia.org/wiki?curid=20056" title="MPEG-1">
MPEG-1

MPEG-1 is a standard for lossy compression of video and audio. It is designed to compress VHS-quality raw digital video and CD audio down to 1.5 Mbit/s (26:1 and 6:1 compression ratios respectively) without excessive quality loss, making video CDs, digital cable/satellite TV and digital audio broadcasting (DAB) possible.
Today, MPEG-1 has become the most widely compatible lossy audio/video format in the world, and is used in a large number of products and technologies. Perhaps the best-known part of the MPEG-1 standard is the MP3 audio format it introduced.
The MPEG-1 standard is published as ISO/IEC 11172 – Information technology—Coding of moving pictures and associated audio for digital storage media at up to about 1.5 Mbit/s. The standard consists of the following five "Parts":
History.
Modeled on the successful collaborative approach and the compression technologies developed by the Joint Photographic Experts Group and CCITT's Experts Group on Telephony (creators of the JPEG image compression standard and the H.261 standard for video conferencing respectively), the Moving Picture Experts Group (MPEG) working group was established in January 1988. MPEG was formed to address the need for standard video and audio formats, and to build on H.261 to get better quality through the use of more complex encoding methods.
Development of the MPEG-1 standard began in May 1988. Fourteen video and fourteen audio codec proposals were submitted by individual companies and institutions for evaluation. The codecs were extensively tested for computational complexity and subjective (human perceived) quality, at data rates of 1.5 Mbit/s. This specific bitrate was chosen for transmission over T-1/E-1 lines and as the approximate data rate of audio CDs. The codecs that excelled in this testing were utilized as the basis for the standard and refined further, with additional features and other improvements being incorporated in the process.
After 20 meetings of the full group in various cities around the world, and 4½ years of development and testing, the final standard (for parts 1–3) was approved in early November 1992 and published a few months later. The reported completion date of the MPEG-1 standard varies greatly: a largely complete draft standard was produced in September 1990, and from that point on, only minor changes were introduced. The draft standard was publicly available for purchase. The standard was finished with the 6 November 1992 meeting. The Berkeley Plateau Multimedia Research Group developed an MPEG-1 decoder in November 1992. In July 1990, before the first draft of the MPEG-1 standard had even been written, work began on a second standard, MPEG-2, intended to extend MPEG-1 technology to provide full broadcast-quality video (as per CCIR 601) at high bitrates (3–15  Mbit/s) and support for interlaced video. Due in part to the similarity between the two codecs, the MPEG-2 standard includes full backwards compatibility with MPEG-1 video, so any MPEG-2 decoder can play MPEG-1 videos.
Notably, the MPEG-1 standard very strictly defines the bitstream, and decoder function, but does not define how MPEG-1 encoding is to be performed, although a reference implementation is provided in ISO/IEC-11172-5. This means that MPEG-1 coding efficiency can drastically vary depending on the encoder used, and generally means that newer encoders perform significantly better than their predecessors. The first three parts (Systems, Video and Audio) of ISO/IEC 11172 were published in August 1993.
Patents.
All widely known patent searches suggest that, due to its age, MPEG-1 video and Layer I/II audio is no longer covered by any patents and can thus be used without obtaining a licence or paying any fees. The ISO patent database lists one patent for ISO 11172, US 4,472,747, which expired in 2003. The near-complete draft of the MPEG-1 standard was publicly available as ISO CD 11172 by December 6, 1991. Neither the July 2008 Kuro5hin article "Patent Status of MPEG-1, H.261 and MPEG-2" nor an August 2008 thread on the gstreamer-devel mailing list were able to list a single unexpired MPEG-1 video and Layer I/II audio patent. A May 2009 discussion on the whatwg mailing list mentioned US 5,214,678 patent as possibly covering MPEG audio layer II. Filed in 1990 and published in 1993, this patent is now expired.
A full MPEG-1 decoder and encoder, with "Layer 3 audio", cannot be implemented royalty free since there are companies that require patent fees for implementations of MPEG-1 Layer 3 Audio as discussed in the MP3 article.
Part 1: Systems.
Part 1 of the MPEG-1 standard covers "systems", and is defined in ISO/IEC-11172-1.
MPEG-1 Systems specifies the logical layout and methods used to store the encoded audio, video, and other data into a standard bitstream, and to maintain synchronization between the different contents. This file format is specifically designed for storage on media, and transmission over data channels, that are considered relatively reliable. Only limited error protection is defined by the standard, and small errors in the bitstream may cause noticeable defects.
This structure was later named an MPEG program stream: "The MPEG-1 Systems design is essentially identical to the MPEG-2 Program Stream structure." This terminology is more popular, precise (differentiates it from an MPEG transport stream) and will be used here.
Elementary streams.
Elementary Streams (ES) are the raw bitstreams of MPEG-1 audio and video encoded data (output from an encoder). These files can be distributed on their own, such as is the case with MP3 files.
Packetized Elementary Streams ("PES") are elementary streams packetized into packets of variable lengths, i.e., divided ES into independent chunks where cyclic redundancy check (CRC) checksum was added to each packet for error detection.
System Clock Reference (SCR) is a timing value stored in a 33-bit header of each PES, at a frequency/precision of 90 kHz, with an extra 9-bit extension that stores additional timing data with a precision of 27 MHz. These are inserted by the encoder, derived from the system time clock (STC). Simultaneously encoded audio and video streams will not have identical SCR values, however, due to buffering, encoding, jitter, and other delay.
Program streams.
Program Streams (PS) are concerned with combining multiple packetized elementary streams (usually just one audio and video PES) into a single stream, ensuring simultaneous delivery, and maintaining synchronization. The PS structure is known as a multiplex, or a container format.
Presentation time stamps (PTS) exist in PS to correct the inevitable disparity between audio and video SCR values (time-base correction). 90 kHz PTS values in the PS header tell the decoder which video SCR values match which audio SCR values. PTS determines when to display a portion of an MPEG program, and is also used by the decoder to determine when data can be discarded from the buffer. Either video or audio will be delayed by the decoder until the corresponding segment of the other arrives and can be decoded.
PTS handling can be problematic. Decoders must accept multiple "program streams" that have been concatenated (joined sequentially). This causes PTS values in the middle of the video to reset to zero, which then begin incrementing again. Such PTS wraparound disparities can cause timing issues that must be specially handled by the decoder.
Decoding Time Stamps (DTS), additionally, are required because of B-frames. With B-frames in the video stream, adjacent frames have to be encoded and decoded out-of-order (re-ordered frames). DTS is quite similar to PTS, but instead of just handling sequential frames, it contains the proper time-stamps to tell the decoder when to decode and display the next B-frame (types of frames explained below), ahead of its anchor (P- or I-) frame. Without B-frames in the video, PTS and DTS values are identical.
Multiplexing.
To generate the PS, the multiplexer will interleave the (two or more) packetized elementary streams. This is done so the packets of the simultaneous streams can be transferred over the same channel and are guaranteed to both arrive at the decoder at precisely the same time. This is a case of time-division multiplexing.
Determining how much data from each stream should be in each interleaved segment (the size of the interleave) is complicated, yet an important requirement. Improper interleaving will result in buffer underflows or overflows, as the receiver gets more of one stream than it can store (e.g. audio), before it gets enough data to decode the other simultaneous stream (e.g. video). The MPEG Video Buffering Verifier (VBV) assists in determining if a multiplexed PS can be decoded by a device with a specified data throughput rate and buffer size. This offers feedback to the muxer and the encoder, so that they can change the mux size or adjust bitrates as needed for compliance.
Part 2: Video.
Part 2 of the MPEG-1 standard covers video and is defined in ISO/IEC-11172-2. The design was heavily influenced by H.261.
MPEG-1 Video exploits perceptual compression methods to significantly reduce the data rate required by a video stream. It reduces or completely discards information in certain frequencies and areas of the picture that the human eye has limited ability to fully perceive. It also exploits temporal (over time) and spatial (across a picture) redundancy common in video to achieve better data compression than would be possible otherwise. (See: Video compression)
Color space.
Before encoding video to MPEG-1, the color-space is transformed to Y'CbCr (Y'=Luma, Cb=Chroma Blue, Cr=Chroma Red). Luma (brightness, resolution) is stored separately from chroma (color, hue, phase) and even further separated into red and blue components. The chroma is also subsampled to , meaning it is reduced by one half vertically and one half horizontally, to just one quarter the resolution of the video.
This software algorithm also has analogies in hardware, such as the output from a Bayer pattern filter, common in digital colour cameras.
Because the human eye is much more sensitive to small changes in brightness (the Y component) than in color (the Cr and Cb components), chroma subsampling is a very effective way to reduce the amount of video data that needs to be compressed. On videos with fine detail (high spatial complexity) this can manifest as chroma aliasing artifacts. Compared to other digital compression artifacts, this issue seems to be very rarely a source of annoyance.
Because of subsampling, Y'CbCr video must always be stored using even dimensions (divisible by 2), otherwise chroma mismatch ("ghosts") will occur, and it will appear as if the color is ahead of, or behind the rest of the video, much like a shadow.
Y'CbCr is often inaccurately called YUV which is only used in the domain of analog video signals. Similarly, the terms luminance and chrominance are often used instead of the (more accurate) terms luma and chroma.
Resolution/bitrate.
MPEG-1 supports resolutions up to 4095×4095 (12-bits), and bitrates up to 100 Mbit/s.
MPEG-1 videos are most commonly seen using Source Input Format (SIF) resolution: 352x240, 352x288, or 320x240. These low resolutions, combined with a bitrate less than 1.5 Mbit/s, make up what is known as a constrained parameters bitstream (CPB), later renamed the "Low Level" (LL) profile in MPEG-2. This is the minimum video specifications any decoder should be able to handle, to be considered MPEG-1 compliant. This was selected to provide a good balance between quality and performance, allowing the use of reasonably inexpensive hardware of the time.
Frame/picture/block types.
MPEG-1 has several frame/picture types that serve different purposes. The most important, yet simplest, is I-frame.
I-frames.
I-frame is an abbreviation for Intra-frame, so-called because they can be decoded independently of any other frames. They may also be known as I-pictures, or keyframes due to their somewhat similar function to the key frames used in animation. I-frames can be considered effectively identical to baseline JPEG images.
High-speed seeking through an MPEG-1 video is only possible to the nearest I-frame. When cutting a video it is not possible to start playback of a segment of video before the first I-frame in the segment (at least not without computationally intensive re-encoding). For this reason, I-frame-only MPEG videos are used in editing applications.
I-frame only compression is very fast, but produces very large file sizes: a factor of 3× (or more) larger than normally encoded MPEG-1 video, depending on how temporally complex a specific video is. I-frame only MPEG-1 video is very similar to MJPEG video. So much so that very high-speed and theoretically lossless (in reality, there are rounding errors) conversion can be made from one format to the other, provided a couple of restrictions (color space and quantization matrix) are followed in the creation of the bitstream.
The length between I-frames is known as the group of pictures (GOP) size. MPEG-1 most commonly uses a GOP size of 15-18. i.e. 1 I-frame for every 14-17 non-I-frames (some combination of P- and B- frames). With more intelligent encoders, GOP size is dynamically chosen, up to some pre-selected maximum limit.
Limits are placed on the maximum number of frames between I-frames due to decoding complexing, decoder buffer size, recovery time after data errors, seeking ability, and accumulation of IDCT errors in low-precision implementations most common in hardware decoders (See: IEEE-1180).
P-frames.
P-frame is an abbreviation for Predicted-frame. They may also be called forward-predicted frames, or inter-frames (B-frames are also inter-frames).
P-frames exist to improve compression by exploiting the temporal (over time) redundancy in a video. P-frames store only the "difference" in image from the frame (either an I-frame or P-frame) immediately preceding it (this reference frame is also called the "anchor frame").
The difference between a P-frame and its anchor frame is calculated using "motion vectors" on each "macroblock" of the frame (see below). Such motion vector data will be embedded in the P-frame for use by the decoder.
A P-frame can contain any number of intra-coded blocks, in addition to any forward-predicted blocks.
If a video drastically changes from one frame to the next (such as a cut), it is more efficient to encode it as an I-frame.
B-frames.
B-frame stands for bidirectional-frame. They may also be known as backwards-predicted frames or B-pictures. B-frames are quite similar to P-frames, except they can make predictions using both the previous and future frames (i.e. two anchor frames).
It is therefore necessary for the player to first decode the next I- or P- anchor frame sequentially after the B-frame, before the B-frame can be decoded and displayed. This means decoding B-frames requires larger data buffers and causes an increased delay on both decoding and during encoding. This also necessitates the decoding time stamps (DTS) feature in the container/system stream (see above). As such, B-frames have long been subject of much controversy, they are often avoided in videos, and are sometimes not fully supported by hardware decoders.
No other frames are predicted from a B-frame. Because of this, a very low bitrate B-frame can be inserted, where needed, to help control the bitrate. If this was done with a P-frame, future P-frames would be predicted from it and would lower the quality of the entire sequence. However, similarly, the future P-frame must still encode all the changes between it and the previous I- or P- anchor frame. B-frames can also be beneficial in videos where the background behind an object is being revealed over several frames, or in fading transitions, such as scene changes.
A B-frame can contain any number of intra-coded blocks and forward-predicted blocks, in addition to backwards-predicted, or bidirectionally predicted blocks.
D-frames.
MPEG-1 has a unique frame type not found in later video standards. D-frames or DC-pictures are independent images (intra-frames) that have been encoded using DC transform coefficients only (AC coefficients are removed when encoding D-frames—see DCT below) and hence are very low quality. D-frames are never referenced by I-, P- or B- frames. D-frames are only used for fast previews of video, for instance when seeking through a video at high speed.
Given moderately higher-performance decoding equipment, fast preview can be accomplished by decoding I-frames instead of D-frames. This provides higher quality previews, since I-frames contain AC coefficients as well as DC coefficients. If the encoder can assume that rapid I-frame decoding capability is available in decoders, it can save bits by not sending D-frames (thus improving compression of the video content). For this reason, D-frames are seldom actually used in MPEG-1 video encoding, and the D-frame feature has not been included in any later video coding standards.
Macroblocks.
MPEG-1 operates on video in a series of 8x8 blocks for quantization. However, because chroma (color) is subsampled by a factor of 4, each pair of (red and blue) chroma blocks corresponds to 4 different luma blocks. This set of 6 blocks, with a resolution of 16x16, is called a macroblock.
A macroblock is the smallest independent unit of (color) video. Motion vectors (see below) operate solely at the macroblock level.
If the height and/or width of the video is not exact multiples of 16, a full row of macroblocks must still be encoded (though not displayed) to store the remainder of the picture (macroblock padding). This wastes a significant amount of data in the bitstream, and is to be strictly avoided.
Some decoders will also improperly handle videos with partial macroblocks, resulting in visible artifacts.
Motion vectors.
To decrease the amount of temporal redundancy in a video, only blocks that change are updated, (up to the maximum GOP size). This is known as conditional replenishment. However, this is not very effective by itself. Movement of the objects, and/or the camera may result in large portions of the frame needing to be updated, even though only the position of the previously encoded objects has changed. Through motion estimation the encoder can compensate for this movement and remove a large amount of redundant information.
The encoder compares the current frame with adjacent parts of the video from the anchor frame (previous I- or P- frame) in a diamond pattern, up to a (encoder-specific) predefined radius limit from the area of the current macroblock. If a match is found, only the direction and distance (i.e. the "vector" of the "motion") from the previous video area to the current macroblock need to be encoded into the inter-frame (P- or B- frame). The reverse of this process, performed by the decoder to reconstruct the picture, is called motion compensation.
A predicted macroblock rarely matches the current picture perfectly, however. The differences between the estimated matching area, and the real frame/macroblock is called the prediction error. The larger the error, the more data must be additionally encoded in the frame. For efficient video compression, it is very important that the encoder is capable of effectively and precisely performing motion estimation.
Motion vectors record the "distance" between two areas on screen based on the number of pixels (called pels). MPEG-1 video uses a motion vector (MV) precision of one half of one pixel, or half-pel. The finer the precision of the MVs, the more accurate the match is likely to be, and the more efficient the compression. There are trade-offs to higher precision, however. Finer MVs result in larger data size, as larger numbers must be stored in the frame for every single MV, increased coding complexity as increasing levels of interpolation on the macroblock are required for both the encoder and decoder, and diminishing returns (minimal gains) with higher precision MVs. Half-pel was chosen as the ideal trade-off. (See: qpel)
Because neighboring macroblocks are likely to have very similar motion vectors, this redundant information can be compressed quite effectively by being stored DPCM-encoded. Only the (smaller) amount of difference between the MVs for each macroblock needs to be stored in the final bitstream.
P-frames have one motion vector per macroblock, relative to the previous anchor frame. B-frames, however, can use two motion vectors; one from the previous anchor frame, and one from the future anchor frame.
Partial macroblocks, and black borders/bars encoded into the video that do not fall exactly on a macroblock boundary, cause havoc with motion prediction. The block padding/border information prevents the macroblock from closely matching with any other area of the video, and so, significantly larger prediction error information must be encoded for every one of the several dozen partial macroblocks along the screen border. DCT encoding and quantization (see below) also isn't nearly as effective when there is large/sharp picture contrast in a block.
An even more serious problem exists with macroblocks that contain significant, random, "edge noise", where the picture transitions to (typically) black. All the above problems also apply to edge noise. In addition, the added randomness is simply impossible to compress significantly. All of these effects will lower the quality (or increase the bitrate) of the video substantially.
DCT.
Each 8x8 block is encoded by first applying a "forward" discrete cosine transform (FDCT) and then a quantization process. The FDCT process (by itself) is theoretically lossless, and can be reversed by applying an "Inverse" DCT (IDCT) to reproduce the original values (in the absence of any quantization and rounding errors). In reality, there are some (sometimes large) rounding errors introduced both by quantization in the encoder (as described in the next section) and by IDCT approximation error in the decoder. The minimum allowed accuracy of a decoder IDCT approximation is defined by ISO/IEC 23002-1. (Prior to 2006, it was specified by IEEE 1180-1990.)
The FDCT process converts the 8x8 block of uncompressed pixel values (brightness or color difference values) into an 8x8 indexed array of "frequency coefficient" values. One of these is the (statistically high in variance) DC coefficient, which represents the average value of the entire 8x8 block. The other 63 coefficients are the statistically smaller AC coefficients, which are positive or negative values each representing sinusoidal deviations from the flat block value represented by the "DC coefficient".
An example of an encoded 8x8 FDCT block: 
Since the DC coefficient value is statistically correlated from one block to the next, it is compressed using DPCM encoding. Only the (smaller) amount of difference between each DC value and the value of the DC coefficient in the block to its left needs to be represented in the final bitstream.
Additionally, the frequency conversion performed by applying the DCT provides a statistical decorrelation function to efficiently concentrate the signal into fewer high-amplitude values prior to applying quantization (see below).
Quantization.
Quantization (of digital data) is, essentially, the process of reducing the accuracy of a signal, by dividing it into some larger step size (i.e. finding the nearest multiple, and discarding the remainder/modulus).
The frame-level quantizer is a number from 0 to 31 (although encoders will usually omit/disable some of the extreme values) which determines how much information will be removed from a given frame. The frame-level quantizer is either dynamically selected by the encoder to maintain a certain user-specified bitrate, or (much less commonly) directly specified by the user.
Contrary to popular belief, a fixed frame-level quantizer (set by the user) does not deliver a constant level of quality. Instead, it is an arbitrary metric that will provide a somewhat varying level of quality, depending on the contents of each frame. Given two files of identical sizes, the one encoded at an average bitrate should look better than the one encoded with a fixed quantizer (variable bitrate). Constant quantizer encoding can be used, however, to accurately determine the minimum and maximum bitrates possible for encoding a given video.
A quantization matrix is a string of 64-numbers (0-255) which tells the encoder how relatively important or unimportant each piece of visual information is. Each number in the matrix corresponds to a certain frequency component of the video image.
An example quantization matrix:
Quantization is performed by taking each of the 64 "frequency" values of the DCT block, dividing them by the frame-level quantizer, then dividing them by their corresponding values in the quantization matrix. Finally, the result is rounded down. This significantly reduces, or completely eliminates, the information in some frequency components of the picture. Typically, high frequency information is less visually important, and so high frequencies are much more "strongly quantized" (drastically reduced). MPEG-1 actually uses two separate quantization matrices, one for intra-blocks (I-blocks) and one for inter-block (P- and B- blocks) so quantization of different block types can be done independently, and so, more effectively.
This quantization process usually reduces a significant number of the "AC coefficients" to zero, (known as sparse data) which can then be more efficiently compressed by entropy coding (lossless compression) in the next step.
An example quantized DCT block:
Quantization eliminates a large amount of data, and is the main lossy processing step in MPEG-1 video encoding. This is also the primary source of most MPEG-1 video compression artifacts, like blockiness, color banding, noise, ringing, discoloration, et al. This happens when video is encoded with an insufficient bitrate, and the encoder is therefore forced to use high frame-level quantizers ("strong quantization") through much of the video.
Entropy coding.
Several steps in the encoding of MPEG-1 video are lossless, meaning they will be reversed upon decoding, to produce exactly the same (original) values. Since these lossless data compression steps don't add noise into, or otherwise change the contents (unlike quantization), it is sometimes referred to as noiseless coding. Since lossless compression aims to remove as much redundancy as possible, it is known as entropy coding in the field of information theory.
The coefficients of quantized DCT blocks tend to zero towards the bottom-right. Maximum compression can be achieved by a zig-zag scanning of the DCT block starting from the top left and using Run-length encoding techniques.
The DC coefficients and motion vectors are DPCM-encoded.
Run-length encoding (RLE) is a very simple method of compressing repetition. A sequential string of characters, no matter how long, can be replaced with a few bytes, noting the value that repeats, and how many times. For example, if someone were to say "five nines", you would know they mean the number: 99999.
RLE is particularly effective after quantization, as a significant number of the AC coefficients are now zero (called sparse data), and can be represented with just a couple of bytes. This is stored in a special 2-dimensional Huffman table that codes the run-length and the run-ending character.
Huffman Coding is a very popular method of entropy coding, and used in MPEG-1 video to reduce the data size. The data is analyzed to find strings that repeat often. Those strings are then put into a special table, with the most frequently repeating data assigned the shortest code. This keeps the data as small as possible with this form of compression. Once the table is constructed, those strings in the data are replaced with their (much smaller) codes, which reference the appropriate entry in the table. The decoder simply reverses this process to produce the original data.
This is the final step in the video encoding process, so the result of Huffman coding is known as the MPEG-1 video "bitstream."
GOP configurations for specific applications.
I-frames store complete frame info within the frame and is therefore suited for random access. P-frames provide compression using motion vectors relative to the previous frame ( I or P ). B-frames provide maximum compression but requires the previous as well as next frame for computation. Therefore, processing of B-frames require more buffer on the decoded side. A configuration of the Group of Pictures (GOP) should be selected based on these factors. I-frame only sequences gives least compression, but is useful for random access, FF/FR and editability. I and P frame sequences give moderate compression but add a certain degree of random access, FF/FR functionality. I,P & B frame sequences give very high compression but also increases the coding/decoding delay significantly. Such configurations are therefore not suited for video-telephony or video-conferencing applications.
The typical data rate of an I-frame is 1 bit per pixel while that of a P-frame is 0.1 bit per pixel and for a B-frame, 0.015 bit per pixel.
Part 3: Audio.
Part 3 of the MPEG-1 standard covers audio and is defined in ISO/IEC-11172-3.
MPEG-1 Audio utilizes psychoacoustics to significantly reduce the data rate required by an audio stream. It reduces or completely discards certain parts of the audio that it deduces that the human ear can't "hear", either because they are in frequencies where the ear has limited sensitivity, or are "masked" by other (typically louder) sounds.
Channel Encoding:
MPEG-1 Audio is divided into 3 layers. Each higher layer is more computationally complex, and generally more efficient at lower bitrates than the previous. The layers are semi backwards compatible as higher layers reuse technologies implemented by the lower layers. A "Full" Layer II decoder can also play Layer I audio, but not Layer III audio, although not all higher level players are "full".
Layer I.
MPEG-1 Layer I is nothing more than a simplified version of Layer II. Layer I uses a smaller 384-sample frame size for very low delay, and finer resolution. This is advantageous for applications like teleconferencing, studio editing, etc. It has lower complexity than Layer II to facilitate real-time encoding on the hardware available circa 1990.
Layer I saw limited adoption in its time, and most notably was used on Philips' defunct Digital Compact Cassette at a bitrate of 384 kbit/s. With the substantial performance improvements in digital processing since its introduction, Layer I quickly became unnecessary and obsolete.
Layer I audio files typically use the extension .mp1 or sometimes .m1a
Layer II.
MPEG-1 Layer II (MP2—often incorrectly called MUSICAM) is a lossy audio format designed to provide high quality at about 192 kbit/s for stereo sound. Decoding MP2 audio is computationally simple, relative to MP3, AAC, etc.
History/MUSICAM.
MPEG-1 Layer II was derived from the MUSICAM ("Masking pattern adapted Universal Subband Integrated Coding And Multiplexing") audio codec, developed by Centre commun d'études de télévision et télécommunications (CCETT), Philips, and Institut für Rundfunktechnik (IRT/CNET) as part of the EUREKA 147 pan-European inter-governmental research and development initiative for the development of digital audio broadcasting.
Most key features of MPEG-1 Audio were directly inherited from MUSICAM, including the filter bank, time-domain processing, audio frame sizes, etc. However, improvements were made, and the actual MUSICAM algorithm was not used in the final MPEG-1 Layer II audio standard. The widespread usage of the term MUSICAM to refer to Layer II is entirely incorrect and discouraged for both technical and legal reasons.
Technical details.
Layer II/MP2 is a time-domain encoder. It uses a low-delay 32 sub-band polyphased filter bank for time-frequency mapping; having overlapping ranges (i.e. polyphased) to prevent aliasing. The psychoacoustic model is based on the principles of auditory masking, simultaneous masking effects, and the absolute threshold of hearing (ATH). The size of a Layer II frame is fixed at 1152-samples (coefficients).
Time domain refers to how analysis and quantization is performed on short, discrete samples/chunks of the audio waveform. This offers low delay as only a small number of samples are analyzed before encoding, as opposed to frequency domain encoding (like MP3) which must analyze many times more samples before it can decide how to transform and output encoded audio. This also offers higher performance on complex, random and transient impulses (such as percussive instruments, and applause), offering avoidance of artifacts like pre-echo.
The 32 sub-band filter bank returns 32 amplitude coefficients, one for each equal-sized frequency band/segment of the audio, which is about 700 Hz wide (depending on the audio's sampling frequency). The encoder then utilizes the psychoacoustic model to determine which sub-bands contain audio information that is less important, and so, where quantization will be inaudible, or at least much less noticeable.
The psychoacoustic model is applied using a 1024-point Fast Fourier Transform (FFT). Of the 1152 samples per frame, 64 samples at the top and bottom of the frequency range are ignored for this analysis. They are presumably not significant enough to change the result. The psychoacoustic model uses an empirically determined masking model to determine which sub-bands contribute more to the masking threshold, and how much quantization noise each can contain without being perceived. Any sounds below the absolute threshold of hearing (ATH) are completely discarded. The available bits are then assigned to each sub-band accordingly.
Typically, sub-bands are less important if they contain quieter sounds (smaller coefficient) than a neighboring (i.e. similar frequency) sub-band with louder sounds (larger coefficient). Also, "noise" components typically have a more significant masking effect than "tonal" components.
Less significant sub-bands are reduced in accuracy by quantization. This basically involves compressing the frequency range (amplitude of the coefficient), i.e. raising the noise floor. Then computing an amplification factor, for the decoder to use to re-expand each sub-band to the proper frequency range.
Layer II can also optionally use intensity stereo coding, a form of joint stereo. This means that the frequencies above 6 kHz of both channels are combined/down-mixed into one single (mono) channel, but the "side channel" information on the relative intensity (volume, amplitude) of each channel is preserved and encoded into the bitstream separately. On playback, the single channel is played through left and right speakers, with the intensity information applied to each channel to give the illusion of stereo sound. This perceptual trick is known as stereo irrelevancy. This can allow further reduction of the audio bitrate without much perceivable loss of fidelity, but is generally not used with higher bitrates as it does not provide very high quality (transparent) audio.
Quality.
Subjective audio testing by experts, in the most critical conditions ever implemented, has shown MP2 to offer transparent audio compression at 256 kbit/s for 16-bit 44.1 kHz CD audio using the earliest reference implementation (more recent encoders should presumably perform even better). That (approximately) 1:6 compression ratio for CD audio is particularly impressive because it is quite close to the estimated upper limit of perceptual entropy, at just over 1:8. Achieving much higher compression is simply not possible without discarding some perceptible information.
MP2 remains a favoured lossy audio coding standard due to its particularly high audio coding performances on important audio material such as castanet, symphonic orchestra, male and female voices and particularly complex and high energy transients (impulses) like percussive sounds: triangle, glockenspiel and audience applause. More recent testing has shown that MPEG Multichannel (based on MP2), despite being compromised by an inferior matrixed mode (for the sake of backwards compatibility) rates just slightly lower than much more recent audio codecs, such as Dolby Digital (AC-3) and Advanced Audio Coding (AAC) (mostly within the margin of error—and substantially superior in some cases, such as audience applause). This is one reason that MP2 audio continues to be used extensively. The MPEG-2 AAC Stereo verification tests reached a vastly different conclusion, however, showing AAC to provide superior performance to MP2 at half the bitrate. The reason for this disparity with both earlier and later tests is not clear, but strangely, a sample of applause is notably absent from the latter test.
Layer II audio files typically use the extension .mp2 or sometimes .m2a
Layer III/MP3.
MPEG-1 Layer III (MP3) is a lossy audio format designed to provide acceptable quality at about 64 kbit/s for monaural audio over single-channel (BRI) ISDN links, and 128 kbit/s for stereo sound.
History/ASPEC.
Layer III/MP3 was derived from the "Adaptive Spectral Perceptual Entropy Coding" (ASPEC) codec developed by Fraunhofer as part of the EUREKA 147 pan-European inter-governmental research and development initiative for the development of digital audio broadcasting. ASPEC was adapted to fit in with the Layer II/MUSICAM model (frame size, filter bank, FFT, etc.), to become Layer III.
ASPEC was itself based on "Multiple adaptive Spectral audio Coding" (MSC) by E. F. Schroeder, "Optimum Coding in the Frequency domain" (OCF) the doctoral thesis by Karlheinz Brandenburg at the University of Erlangen-Nuremberg, "Perceptual Transform Coding" (PXFM) by J. D. Johnston at AT&T Bell Labs, and "Transform coding of audio signals" by Y. Mahieux and J. Petit at Institut für Rundfunktechnik (IRT/CNET).
Technical details.
MP3 is a frequency-domain audio transform encoder. Even though it utilizes some of the lower layer functions, MP3 is quite different from Layer II/MP2.
MP3 works on 1152 samples like Layer II, but needs to take multiple frames for analysis before frequency-domain (MDCT) processing and quantization can be effective. It outputs a variable number of samples, using a bit buffer to enable this variable bitrate (VBR) encoding while maintaining 1152 sample size output frames. This causes a significantly longer delay before output, which has caused MP3 to be considered unsuitable for studio applications where editing or other processing needs to take place.
MP3 does not benefit from the 32 sub-band polyphased filter bank, instead just using an 18-point MDCT transformation on each output to split the data into 576 frequency components, and processing it in the frequency domain. This extra granularity allows MP3 to have a much finer psychoacoustic model, and more carefully apply appropriate quantization to each band, providing much better low-bitrate performance.
Frequency-domain processing imposes some limitations as well, causing a factor of 12 or 36 × worse temporal resolution than Layer II. This causes quantization artifacts, due to transient sounds like percussive events and other high-frequency events that spread over a larger window. This results in audible smearing and pre-echo. MP3 uses pre-echo detection routines, and VBR encoding, which allows it to temporarily increase the bitrate during difficult passages, in an attempt to reduce this effect. It is also able to switch between the normal 36 sample quantization window, and instead using 3× short 12 sample windows instead, to reduce the temporal (time) length of quantization artifacts. And yet in choosing a fairly small window size to make MP3's temporal response adequate enough to avoid the most serious artifacts, MP3 becomes much less efficient in frequency domain compression of stationary, tonal components.
Being forced to use a "hybrid" time domain (filter bank) /frequency domain (MDCT) model to fit in with Layer II simply wastes processing time and compromises quality by introducing aliasing artifacts. MP3 has an aliasing cancellation stage specifically to mask this problem, but which instead produces frequency domain energy which must be encoded in the audio. This is pushed to the top of the frequency range, where most people have limited hearing, in hopes the distortion it causes will be less audible.
Layer II's 1024 point FFT doesn't entirely cover all samples, and would omit several entire MP3 sub-bands, where quantization factors must be determined. MP3 instead uses two passes of FFT analysis for spectral estimation, to calculate the global and individual masking thresholds. This allows it to cover all 1152 samples. Of the two, it utilizes the global masking threshold level from the more critical pass, with the most difficult audio.
In addition to Layer II's intensity encoded joint stereo, MP3 can use middle/side (mid/side, m/s, MS, matrixed) joint stereo. With mid/side stereo, certain frequency ranges of both channels are merged into a single (middle, mid, L+R) mono channel, while the sound difference between the left and right channels is stored as a separate (side, L-R) channel. Unlike intensity stereo, this process does not discard any audio information. When combined with quantization, however, it can exaggerate artifacts.
If the difference between the left and right channels is small, the side channel will be small, which will offer as much as a 50% bitrate savings, and associated quality improvement. If the difference between left and right is large, standard (discrete, left/right) stereo encoding may be preferred, as mid/side joint stereo will not provide any benefits. An MP3 encoder can switch between m/s stereo and full stereo on a frame-by-frame basis.
Unlike Layers I/II, MP3 uses variable-length Huffman coding (after perceptual) to further reduce the bitrate, without any further quality loss.
Quality.
These technical limitations inherently prevent MP3 from providing critically transparent quality at any bitrate. This makes Layer II sound quality actually superior to MP3 audio, when it is used at a high enough bitrate to avoid noticeable artifacts. The term "transparent" often gets misused, however. The quality of MP3 (and other codecs) is sometimes called "transparent," even at impossibly low bitrates, when what is really meant is "good quality on average/non-critical material," or perhaps "exhibiting only non-annoying artifacts."
MP3's more fine-grained and selective quantization does prove notably superior to Layer II/MP2 at lower-bitrates, however. It is able to provide nearly equivalent audio quality to Layer II, at a 15% lower bitrate (approximately). 128 kbit/s is considered the "sweet spot" for MP3; meaning it provides generally acceptable quality stereo sound on most music, and there are diminishing quality improvements from increasing the bitrate further. MP3 is also regarded as exhibiting artifacts that are less annoying than Layer II, when both are used at bitrates that are too low to possibly provide faithful reproduction.
Layer III audio files use the extension .mp3.
MPEG-2 audio extensions.
The MPEG-2 standard includes several extensions to MPEG-1 Audio. These are known as MPEG-2 BC – backwards compatible with MPEG-1 Audio. MPEG-2 Audio is defined in ISO/IEC 13818-3
These sampling rates are exactly half that of those originally defined for MPEG-1 Audio. They were introduced to maintain higher quality sound when encoding audio at lower-bitrates. The even-lower bitrates were introduced because tests showed that MPEG-1 Audio could provide higher quality than any existing (circa 1994) very low bitrate (i.e. speech) audio codecs.
Part 4: Conformance testing.
Part 4 of the MPEG-1 standard covers conformance testing, and is defined in ISO/IEC-11172-4.
Conformance: Procedures for testing conformance.
Provides two sets of guidelines and reference bitstreams for testing the conformance of MPEG-1 audio and video decoders, as well as the bitstreams produced by an encoder.
Part 5: Reference software.
Part 5 of the MPEG-1 standard includes reference software, and is defined in ISO/IEC TR 11172-5.
Simulation: Reference software.
C reference code for encoding and decoding of audio and video, as well as multiplexing and demultiplexing.
This includes the "ISO Dist10" audio encoder code, which LAME and TooLAME were originally based upon.
File extension.
.mpg is one of a number of file extensions for MPEG-1 or MPEG-2 audio and video compression. MPEG-1 Part 2 video is rare nowadays, and this extension typically refers to an MPEG program stream (defined in MPEG-1 and MPEG-2) or MPEG transport stream (defined in MPEG-2). Other suffixes such as .m2ts also exists specifying the precise container, in this case MPEG-2 TS, but this has little relevance to MPEG-1 media.
.mp3 is the most common extension for files containing MPEG-1 Layer 3 audio. An MP3 file is typically an uncontained stream of raw audio; the conventional way to tag MP3 files is by writing data to "garbage" segments of each frame, which preserve the media information but are discarded by the player. This is similar in many respects to how raw .AAC files are tagged (but this is less supported nowadays, e.g. iTunes).
Note that although it would apply, .mpg does not normally append raw AAC or AAC in MPEG-2 Part 7 Containers. The .aac extension normally denotes these audio files.

</doc>
<doc id="20057" url="https://en.wikipedia.org/wiki?curid=20057" title="Mumia Abu-Jamal">
Mumia Abu-Jamal

Mumia Abu-Jamal (born Wesley Cook April 24, 1954) is an American activist and journalist who was convicted and sentenced to death in 1982 for the 1981 murder of Philadelphia police officer Daniel Faulkner.
Abu-Jamal became involved in black nationalism in his youth and was a member of the Black Panther Party until October 1970, after which he became a radio journalist, eventually becoming president of the Philadelphia Association of Black Journalists. On December 9, 1981, Faulkner was fatally shot while conducting a routine traffic stop of Abu-Jamal's brother, William Cook. Abu-Jamal was found at the scene with a bullet wound from Faulkner's gun and his own discharged revolver beside him. He was arrested and charged with Faulkner's murder.
Prosecution witnesses identified Abu-Jamal as the shooter and two testified that he had confessed to shooting Faulkner. A jury convicted Abu-Jamal on all counts and sentenced him to death. He spent the next 30 years on death row. After a succession of all possible appeals by Abu-Jamal were exhausted, his conviction was upheld but his death sentence vacated. He was resentenced to life in prison without parole. District Attorney Seth Williams later stated that no further appeals would be filed in pursuit of the death penalty.
Activists, celebrities, and liberal groups have criticized the fairness of Abu-Jamal's trial, professed his innocence, and opposed his death sentence. The Faulkner family, public authorities, police organizations, and conservative groups have maintained that Abu-Jamal's trial was fair, his guilt undeniable, and his death sentence appropriate. Once described as "perhaps the world's best known death-row inmate" by "The New York Times", during his imprisonment Abu-Jamal has published books and commentaries on social and political issues, including "Live from Death Row" (1995).
Early life and activism.
Abu-Jamal was given the name Mumia in 1968 by his high school teacher, a Kenyan instructing a class on African cultures in which students took African classroom names. According to Abu-Jamal, 'Mumia' means "Prince" and was the name of Kenyan anti-colonial African nationalists who fought against the British before Kenyan independence. He adopted the surname Abu-Jamal ("father of Jamal" in Arabic) after the birth of his son Jamal on July 18, 1971. His first marriage at age 19, to Jamal's mother, Biba, was short-lived. Their daughter, Lateefa, was born shortly after the wedding. Abu-Jamal married his second wife, Marilyn (known as "Peachie"), in 1977. Their son, Mazi, was born in early 1978. By 1981, Abu-Jamal was living with his third and current wife, Wadiya.
Involvement with the Black Panthers.
In his own writings, Abu-Jamal describes his adolescent experience of being "kicked ... into the Black Panther Party" after suffering a beating from "white racists" and a policeman for his efforts to disrupt a George Wallace for President rally in 1968. From the age of 14, he helped form the Philadelphia branch of the Black Panther Party with Defense Captain Reggie Schell, and other Panthers, taking appointment, in his own words, as the chapter's "Lieutenant of Information", exercising a responsibility for writing information and news communications. In one of the interviews he gave at the time he quoted Mao Zedong, saying that "political power grows out of the barrel of a gun". That same year, he dropped out of Benjamin Franklin High School and took up residence in the branch's headquarters. He spent late 1969 in New York City and early 1970 in Oakland, living and working with BPP colleagues in those cities. He was a party member from May 1969 until October 1970 and was subject to Federal Bureau of Investigation COINTELPRO surveillance, with which the Philadelphia police cooperated, from then until about 1974.
Education and journalism career.
After returning to his old high school after his departure from the Panthers, Abu-Jamal was suspended for distributing literature calling for "black revolutionary student power". He also led unsuccessful protests to change the school name to Malcolm X High. After attaining his GED, he studied briefly at Goddard College in rural Vermont.
By 1975 he was pursuing a vocation in radio newscasting, first at Temple University's WRTI and then at commercial enterprises. In 1975, he was employed at radio station WHAT and he became host of a weekly feature program of WCAU-FM in 1978. He was also employed for brief periods at radio station WPEN, and became active in the local chapter of the Marijuana Users Association of America. From 1979 he worked at National Public Radio-affiliate (NPR) WUHY until 1981 when he was asked to submit his resignation after a dispute about the requirements of objective focus in his presentation of news. As a radio journalist he earned the moniker "the voice of the voiceless" and was renowned for identifying with and giving exposure to the MOVE anarcho-primitivist commune in Philadelphia's Powelton Village neighborhood, including reportage of the 1979–80 trial of certain of its members (the "MOVE Nine") convicted of the murder of police officer James Ramp. During his broadcasting career, his high-profile interviews included Julius Erving, Bob Marley and Alex Haley, and he was elected president of the Philadelphia Association of Black Journalists.
At the time of Daniel Faulkner's murder, Abu-Jamal was working as a taxicab driver in Philadelphia two nights a week to supplement his income. He had been working part-time as a reporter for WDAS, then an African-American-oriented and minority-owned radio station.
Arrest for murder and trial.
At 3:55 AM on December 9, 1981, in Philadelphia, close to the intersection at 13th and Locust Streets, Philadelphia Police Department officer Daniel Faulkner conducted a traffic stop on a vehicle belonging to William Cook, Abu-Jamal's younger brother. During the traffic stop, Abu-Jamal's taxi was parked across the street. He ran across the street towards Cook's car, where Faulkner was shot from behind and then in the face. Abu-Jamal was shot by Faulkner in the stomach. Faulkner died at the scene from the gunshot to his head. Police arrived and arrested Abu-Jamal, who was found wearing a shoulder holster. His revolver, which had five spent cartridges, was beside him. Abu-Jamal was taken directly from the scene of the shooting to Thomas Jefferson University Hospital, where he received treatment for his wound.
Abu-Jamal was charged with the first-degree murder of Officer Faulkner. The case went to trial in June 1982 in Philadelphia. Judge Albert F. Sabo initially agreed to Abu-Jamal's request to represent himself, with criminal defense attorney Anthony Jackson acting as his legal advisor. During the first day of the trial, Sabo warned Abu-Jamal that he would forfeit his legal right to self-representation if he kept being intentionally disruptive in a fashion that was unbecoming under the law. Due to Abu-Jamal's continued disruptive behavior, Sabo ruled that Abu-Jamal forfeited his right to self-representation.
Prosecution case at trial.
The prosecution presented four witnesses to the court. Robert Chobert, a cab driver who testified he was parked behind Faulkner, identified Abu-Jamal as the shooter. Cynthia White, a prostitute, testified that Abu-Jamal emerged from a nearby parking lot and shot Faulkner. Michael Scanlan, a motorist, testified that from two car lengths away, he saw a man, matching Abu-Jamal's description, run across the street from a parking lot and shoot Faulkner. Albert Magilton, a pedestrian who did not see the actual murder, testified to witnessing Faulkner pull over Cook's car. At the point of seeing Abu-Jamal start to cross the street toward them from the parking lot, Magilton turned away and lost sight of what happened next.
The prosecution also presented two witnesses who were at the hospital after the shootings. Hospital security guard Priscilla Durham and police officer Garry Bell testified that Abu-Jamal confessed in the hospital by saying, "I shot the motherfucker, and I hope the motherfucker dies."
A .38 caliber Charter Arms revolver, belonging to Abu-Jamal, with five spent cartridges was retrieved beside him at the scene. He was wearing a shoulder holster, and Anthony Paul, the Supervisor of the Philadelphia Police Department's firearms identification unit, testified at trial that the cartridge cases and rifling characteristics of the weapon were consistent with bullet fragments taken from Faulkner's body. Tests to confirm that Abu-Jamal had handled and fired the weapon were not performed, as contact with arresting police and other surfaces at the scene could have compromised the forensic value of such tests.
Defense case at trial.
The defense maintained that Abu-Jamal was innocent and that the prosecution witnesses were unreliable. The defense presented nine character witnesses, including poet Sonia Sanchez, who testified that Abu-Jamal was "viewed by the black community as a creative, articulate, peaceful, genial man". Another defense witness, Dessie Hightower, testified that he saw a man running along the street shortly after the shooting although he did not see the actual shooting itself. His testimony contributed to the development of a "running man theory", based on the possibility that a "running man" may have been the actual shooter. Veronica Jones also testified for the defense, but she did not see anyone running. Other potential defense witnesses refused to appear in court. Abu-Jamal did not testify in his own defense. Nor did his brother, William Cook, who told investigators at the crime scene: "I ain't got nothing to do with this."
Verdict and sentence.
The jury delivered a unanimous guilty verdict after three hours of deliberations.
In the sentencing phase of the trial, Abu-Jamal read to the jury from a prepared statement. He was then cross-examined about issues relevant to the assessment of his character by Joseph McGill, the prosecuting attorney.
In his statement Abu-Jamal criticized his attorney as a "legal trained lawyer" who was imposed on him against his will and who "knew he was inadequate to the task and chose to follow the directions of this black-robed conspirator, Albert Sabo, even if it meant ignoring my directions". He claimed that his rights had been "deceitfully stolen" from him by Sabo, particularly focusing on the denial of his request to receive defense assistance from non-attorney John Africa and being prevented from proceeding "pro se". He quoted remarks of John Africa, and said:
Abu-Jamal was subsequently sentenced to death by the unanimous decision of the jury.
Appeals and review.
State appeals.
Direct appeal of his conviction was considered and denied by the Supreme Court of Pennsylvania on March 6, 1989, subsequently denying rehearing. The Supreme Court of the United States denied his petition for writ of "certiorari" on October 1, 1990, and denied his petition for rehearing twice up to June 10, 1991.
On June 1, 1995, his death warrant was signed by Pennsylvania Governor Tom Ridge. Its execution was suspended while Abu-Jamal pursued state post-conviction review. At the post-conviction review hearings, new witnesses were called. William "Dales" Singletary testified that he saw the shooting and that the gunman was the passenger in Cook's car. Singletary's account contained discrepancies which rendered it "not credible" in the opinion of the court. William Harmon, a convicted fraudster, testified that Faulkner's murderer fled in a car which pulled up at the crime scene, and could not have been Abu-Jamal. However, Robert Harkins testified that he had witnessed a man stand over Faulkner as the latter lay wounded on the ground, who shot him point-blank in the face and then "walked and sat down on the curb".
The six judges of the Supreme Court of Pennsylvania ruled unanimously that all issues raised by Abu-Jamal, including the claim of ineffective assistance of counsel, were without merit. The Supreme Court of the United States denied a petition for "certiorari" against that decision on October 4, 1999, enabling Ridge to sign a second death warrant on October 13, 1999. Its execution in turn was stayed as Abu-Jamal commenced his pursuit of federal "habeas corpus" review.
In 1999, Arnold Beverly claimed that he and an unnamed assailant, not Mumia Abu-Jamal, shot Daniel Faulkner as part of a contract killing because Faulkner was interfering with graft and payoff to corrupt police. The Beverly affidavit became an item of division for Mumia's defense team, as some thought it usable and others rejected Beverly's story as "not credible".
Private investigator George Newman claimed in 2001 that Chobert had recanted his testimony. Commentators also noted that police and news photographs of the crime scene did not show Chobert's taxi, and that Cynthia White, the only witness at the trial to testify to seeing the taxi, had previously provided crime scene descriptions that omitted it. Cynthia White was declared to be dead by the state of New Jersey in 1992 although Pamela Jenkins claimed that she saw White alive as late as 1997. Mumia supporters often claim that White was a police informant and that she falsified her testimony against Abu-Jamal. Priscilla Durham's step-brother, Kenneth Pate, who was imprisoned with Abu-Jamal on other charges, has since claimed that Durham admitted to not hearing the hospital confession. The hospital doctors stated that Abu-Jamal was "on the verge of fainting" when brought in and they did not overhear a confession. In 2008, the Supreme Court of Pennsylvania rejected a further request from Abu-Jamal for a hearing into claims that the trial witnesses perjured themselves on the grounds that he had waited too long before filing the appeal.
On March 26, 2012 the Supreme Court of Pennsylvania rejected his most recent appeal for retrial asserted on the basis that a 2009 report by the National Academy of Science demonstrated that forensic evidence put by the prosecution and accepted into evidence in the original trial was unreliable. It was reported to be the former death row inmate's last legal appeal.
Federal ruling directing resentencing.
Abu-Jamal did not make any public statements about Faulkner's murder until May 2001. In his version of events, he claimed that he was sitting in his cab across the street when he heard shouting, then saw a police vehicle, then heard the sound of gunshots. Upon seeing his brother appearing disoriented across the street, Abu-Jamal ran to him from the parking lot and was shot by a police officer. The driver originally stopped by police officer Faulkner, Abu-Jamal's brother William Cook, did not testify or make any statement until April 29, 2001, when he claimed that he had not seen who had shot Faulkner.
Judge William H. Yohn Jr. of the United States District Court for the Eastern District of Pennsylvania upheld the conviction but vacated the sentence of death on December 18, 2001, citing irregularities in the original process of sentencing. Particularly, He ordered the State of Pennsylvania to commence new sentencing proceedings within 180 days and ruled that it was unconstitutional to require that a jury's finding of circumstances mitigating against determining a sentence of death be unanimous. Eliot Grossman and Marlene Kamish, attorneys for Abu-Jamal, criticized the ruling on the grounds that it denied the possibility of a "trial de novo" at which they could introduce evidence that their client had been framed. Prosecutors also criticized the ruling; Officer Faulkner's widow Maureen described Abu-Jamal as a "remorseless, hate-filled killer" who would "be permitted to enjoy the pleasures that come from simply being alive" on the basis of the judgment. Both parties appealed.
Federal appeal.
On December 6, 2005, the Third Circuit Court admitted four issues for appeal of the ruling of the District Court: 
The Third Circuit Court heard oral arguments in the appeals on May 17, 2007, at the United States Courthouse in Philadelphia. The appeal panel consisted of Chief Judge Anthony Joseph Scirica, Judge Thomas Ambro, and Judge Robert Cowen. The Commonwealth of Pennsylvania sought to reinstate the sentence of death, on the basis that Yohn's ruling was flawed, as he should have deferred to the Pennsylvania Supreme Court which had already ruled on the issue of sentencing, and the "Batson" claim was invalid because Abu-Jamal made no complaints during the original jury selection. Although Abu-Jamal's jury was racially mixed with 2 blacks and 10 whites at the time of his unanimous conviction, his counsel told the Third Circuit Court that Abu-Jamal did not get a fair trial because the jury was racially biased, misinformed, the judge was a racist, and noted that the prosecution used eleven out of fourteen peremptory challenges to eliminate prospective black jurors. Terri Maurer-Carter, a former Philadelphia court stenographer claimed in a 2001 affidavit nearly 20 years after the trial that she overheard Judge Sabo say "Yeah, and I'm going to help them fry the nigger" in the course of a conversation with three people present regarding Abu-Jamal's case. Sabo denied having made any such comment.
On March 27, 2008, the three-judge panel issued a majority 2–1 opinion upholding Yohn's 2001 opinion but rejecting the bias and "Batson" claims, with Judge Ambro dissenting on the "Batson" issue. On July 22, 2008, Abu-Jamal's formal petition seeking reconsideration of the decision by the full Third Circuit panel of 12 judges was denied. On April 6, 2009, the United States Supreme Court also refused to hear Abu-Jamal's appeal, allowing his conviction to stand. On January 19, 2010, the Supreme Court ordered the appeals court to reconsider its decision to rescind the death penalty, with the same three-judge panel convening in Philadelphia on November 9, 2010, to hear oral argument. On April 26, 2011, the Third Circuit Court of Appeals reaffirmed its prior decision to vacate the death sentence on the grounds that the jury instructions and verdict form were ambiguous and confusing. The Supreme Court declined to hear the case in October.
Death penalty dropped.
On December 7, 2011, District Attorney of Philadelphia R. Seth Williams announced that prosecutors, with the support of the victim's family, would no longer seek the death penalty for Abu-Jamal. Faulkner had indicated she did not wish to relive the trauma of another trial, and that it would be extremely difficult to present the case against Abu-Jamal again, after the passage of 30 years and the deaths of several key witnesses. Williams, the prosecutor, said that Abu-Jamal will spend the rest of his life in prison without the possibility of parole, a sentence that was reaffirmed by the Superior Court of Pennsylvania on July 9, 2013. After the press conference, Maureen Faulkner made an emotional statement harshly condemning Abu-Jamal:
Life as a prisoner.
In 1991 Abu-Jamal published an essay in the "Yale Law Journal", on the death penalty and his death row experience. In May 1994, Abu-Jamal was engaged by National Public Radio's "All Things Considered" program to deliver a series of monthly three-minute commentaries on crime and punishment. The broadcast plans and commercial arrangement were canceled following condemnations from, among others, the Fraternal Order of Police and US Senator Bob Dole (Kansas Republican Party). Abu-Jamal sued NPR for not airing his work, but a federal judge dismissed the suit. The commentaries later appeared in print in May 1995 as part of "Live from Death Row".
In 1999, he was invited to record a keynote address for the graduating class at The Evergreen State College. The event was protested by some. In 2000, he recorded a commencement address for Antioch College. The now defunct New College of California School of Law presented him with an honorary degree "for his struggle to resist the death penalty". On October 5, 2014, he gave the commencement speech at Goddard College, via playback of a recording. As before, the choice of Abu-Jamal was controversial.
With occasional interruptions due to prison disciplinary actions, Abu-Jamal has for many years been a regular commentator on an online broadcast, sponsored by Prison Radio, as well as a regular columnist for "Junge Welt", a Marxist newspaper in Germany. In 1995, he was punished with solitary confinement for engaging in entrepreneurship contrary to prison regulations. Subsequent to the airing of the 1996 HBO documentary "", which included footage from visitation interviews conducted with him, the Pennsylvania Department of Corrections acted to ban outsiders from using any recording equipment in state prisons. In litigation before the US Court of Appeals in 1998 he successfully established his right to write for financial gain in prison. The same litigation also established that the Pennsylvania Department of Corrections had illegally opened his mail in an attempt to establish whether he was writing for financial gain. When, for a brief time in August 1999, he began delivering his radio commentaries live on the Pacifica Network's "Democracy Now!" weekday radio newsmagazine, prison staff severed the connecting wires of his telephone from their mounting in mid-performance. He was later allowed to resume his broadcasts, and hundreds of his broadcasts have been aired on Pacifica Radio.
His publications include "Death Blossoms: Reflections from a Prisoner of Conscience", in which he explores religious themes, "All Things Censored", a political critique examining issues of crime and punishment, "Live From Death Row", a diary of life on Pennsylvania's death row, and "We Want Freedom: A Life in the Black Panther Party", which is a history of the Black Panthers drawing on autobiographical material.
At the end of January 2012 he was released into general prison population at State Correctional Institution – Mahanoy. He went into diabetic shock on 30 March 2015 and has been diagnosed with active Hepatitis C. In August 2015 his attorneys filed suit in the US District Court for the Middle District of Pennsylvania upon the allegation that he has not received appropriate medical care for his health conditions.
Popular support and opposition.
Labor unions, politicians, advocates, educators, the NAACP Legal Defense and Educational Fund, and human rights advocacy organizations such as Human Rights Watch and Amnesty International have expressed concern about the impartiality of the trial of Abu-Jamal, though Amnesty International neither takes a position on the guilt or innocence of Abu-Jamal nor classifies him as a political prisoner. They are opposed by the family of Daniel Faulkner, the Commonwealth of Pennsylvania, the City of Philadelphia, Republican politicians, and the Fraternal Order of Police. In August 1999, the Fraternal Order of Police called for an economic boycott against all individuals and organizations that support Abu-Jamal.
Abu-Jamal has been made an honorary citizen of about 25 cities around the world, including Montreal, Palermo, and Paris.
In 2001, he received the sixth biennial Erich Mühsam Prize, named after an anarcho-communist essayist, which recognizes activism in line with that of its namesake. In October 2002, he was made an honorary member of the German political organization Society of People Persecuted by the Nazi Regime – Federation of Anti-Fascists (VVN-BdA) which Germany's Federal Office for the Protection of the Constitution has considered to be influenced by left-wing extremism.
On April 29, 2006, a newly paved road in the Parisian suburb of Saint-Denis was named Rue Mumia Abu-Jamal in his honor. In protest of the street-naming, US Congressman Michael Fitzpatrick and Senator Rick Santorum, both members of the Republican Party of Pennsylvania, introduced resolutions in both Houses of Congress condemning the decision. The House of Representatives voted 368–31 in favor of Fitzpatrick's resolution. In December 2006, the 25th anniversary of the murder, the executive committee of the Republican Party for the 59th Ward of the City of Philadelphia—covering approximately Germantown, Philadelphia—filed two criminal complaints in the French legal system against the city of Paris and the city of Saint-Denis, accusing the municipalities of "glorifying" Abu-Jamal and alleging the offense "apology or denial of crime" in respect of their actions.
In 2007, the widow of Officer Faulkner coauthored a book with Philadelphia radio journalist Michael Smerconish entitled "Murdered by Mumia: A Life Sentence of Pain, Loss, and Injustice." The book was part memoir of Faulkner's widow, part discussion in which they chronicled Abu-Jamal's trial and discussed evidence for his conviction, and part discussion on supporting the death penalty. J. Patrick O'Connor, editor and publisher of crimemagazine.com, argues in his book "The Framing of Mumia Abu-Jamal" that the preponderance of evidence establishes that it was not Abu-Jamal but a passenger in Abu-Jamal's brother's car, Kenneth Freeman, who killed Faulkner, and that the Philadelphia Police Department and District Attorney's Office framed Abu-Jamal. His book was criticized in the "American Thinker" as "replete with selective use of testimony, distortions, unsubstantiated charges, and a theory that has failed Abu-Jamal in the past."
In 2009, Radio host Rush Limbaugh professed no doubt about Abu-Jamal's guilt, calling him a "notorious Philadelphia murderer".
In early 2014, President Barack Obama nominated Debo P. Adegbile, a former lawyer for the NAACP who worked on Abu-Jamal's case, to head the civil rights division of the Justice Department, but the nomination was rejected by the U.S. Senate on a bipartisan basis because of Adegbile's prior public support of Abu-Jamal.
In April 10, 2015 Marylin Zuniga, a teacher at Forest Street Elementary School in Orange, New Jersey, was suspended without pay after asking her students to write letters to Abu-Jamal, who fell ill in prison due to complications from diabetes, without approval from the school or parents. Some parents and police leaders denounced her actions. On the other hand, community members, parents, teachers, and professors expressed their support and condemned Zuniga’s suspension. Scholars and educators nationwide including Noam Chomsky, Chris Hedges and Cornel West among others signed a letter calling for her immediate reinstatement. On May 13, 2015 The Orange Preparatory Academy board voted to dismiss Marylin Zuniga after hearing from her and several of her supporters.
External links.
Video
Supporter websites
Opponent websites

</doc>
<doc id="20059" url="https://en.wikipedia.org/wiki?curid=20059" title="Multiplicative function">
Multiplicative function

In number theory, a multiplicative function is an arithmetic function "f"("n") of the positive integer "n" with the property that "f"(1) = 1 and whenever
"a" and "b" are coprime, then
An arithmetic function "f"("n") is said to be completely multiplicative (or totally multiplicative) if "f"(1) = 1 and "f"("ab") = "f"("a") "f"("b") holds "for all" positive integers "a" and "b", even when they are not coprime.
Examples.
Some multiplicative functions are defined to make formulas easier to write:
Other examples of multiplicative functions include many functions of importance in number theory, such as:
An example of a non-multiplicative function is the arithmetic function "r""2"("n") - the number of representations of "n" as a sum of squares of two integers, positive, negative, or zero, where in counting the number of ways, reversal of order is allowed. For example:
and therefore "r"2(1) = 4 ≠ 1. This shows that the function is not multiplicative. However, "r"2("n")/4 is multiplicative.
In the On-Line Encyclopedia of Integer Sequences, sequences of values of a multiplicative function have the keyword "mult".
See arithmetic function for some other examples of non-multiplicative functions.
Properties.
A multiplicative function is completely determined by its values at the powers of prime numbers, a consequence of the fundamental theorem of arithmetic. Thus, if "n" is a product of powers of distinct primes, say "n" = "p""a" "q""b" ..., then 
"f"("n") = "f"("p""a") "f"("q""b") ...
This property of multiplicative functions significantly reduces the need for computation, as in the following examples for "n" = 144 = 24 · 32:
Similarly, we have:
In general, if "f"("n") is a multiplicative function and "a", "b" are any two positive integers, then
Every completely multiplicative function is a homomorphism of monoids and is completely determined by its restriction to the prime numbers.
Convolution.
If "f" and "g" are two multiplicative functions, one defines a new multiplicative function "f" * "g", the "Dirichlet convolution" of "f" and "g", by
where the sum extends over all positive divisors "d" of "n". 
With this operation, the set of all multiplicative functions turns into an abelian group; the identity element is "ε". Convolution is commutative, associative, and distributive over addition.
Relations among the multiplicative functions discussed above include:
The Dirichlet convolution can be defined for general arithmetic functions, and yields a ring structure, the Dirichlet ring.
Dirichlet series for some multiplicative functions.
More examples are shown in the article on Dirichlet series.
Multiplicative function over.
Let "A"=, the polynomial ring over the finite field with "q" elements. "A" is principal ideal domain and therefore "A" is unique factorization domain.
a complex-valued function formula_14 on "A" is called multiplicative if formula_15, whenever "f" and "g" are relatively prime.
Zeta function and Dirichlet series in.
Let "h" be a polynomial arithmetic function (i.e. a function on set of monic polynomials over "A"). Its corresponding Dirichlet series define to be
formula_16,
where for formula_17, set formula_18 if formula_19, and formula_20 otherwise.
The polynomial zeta function is then
formula_21.
Similar to the situation in , every Dirichlet series of a multiplicative function "h" has a product representation (Euler product):
formula_22,
Where the product runs over all monic irreducible polynomials "P".
For example, the product representation of the zeta function is as for the integers: formula_23.
Unlike the classical zeta function, formula_24 is a simple rational function:
formula_25.
In a similar way, If "ƒ" and "g" are two polynomial arithmetic functions, one defines "ƒ" * "g", the "Dirichlet convolution" of "ƒ" and "g", by
where the sum extends over all monic divisors "d" of "m", or equivalently over all pairs ("a", "b") of monic polynomials whose product is "m".
The identity formula_27 still holds.

</doc>
<doc id="20060" url="https://en.wikipedia.org/wiki?curid=20060" title="MPEG-2">
MPEG-2

MPEG-2 (aka H.222/H.262 as defined by the ITU) is a standard for "the generic coding of moving pictures and ISO/IEC 13818 MPEG-2 at the ISO Store. It describes a combination of lossy video compression and lossy audio data compression methods, which permit storage and transmission of movies using currently available storage media and transmission bandwidth. While MPEG-2 is not as efficient as newer standards such as H.264 and H.265/HEVC, backwards compatibility with existing hardware and software means it is still widely used, for example in over-the-air digital television broadcasting and in the DVD-Video standard.
Main characteristics.
MPEG-2 is widely used as the format of digital television signals that are broadcast by terrestrial (over-the-air), cable, and direct broadcast satellite TV systems. It also specifies the format of movies and other programs that are distributed on DVD and similar discs. TV stations, TV receivers, DVD players, and other equipment are often designed to this standard. MPEG-2 was the second of several standards developed by the Moving Pictures Expert Group (MPEG) and is an international standard (ISO/IEC 13818). Parts 1 and 2 of MPEG-2 were developed in a collaboration with ITU-T, and they have a respective catalog number in the ITU-T Recommendation Series.
While MPEG-2 is the core of most digital television and DVD formats, it does not completely specify them. Regional institutions can adapt it to their needs by restricting and augmenting aspects of the standard. See Video profiles and levels.
Systems.
MPEG-2 includes a Systems section, part 1, that defines two distinct, but related, container formats. One is the "transport stream", a data packet format designed to transmit one data packet in four ATM data packets for streaming digital video and audio over fixed or mobile transmission mediums, where the beginning and the end of the stream may not be identified, such as radio frequency, cable and linear recording mediums, examples of which include ATSC/DVB/ISDB/SBTVD broadcasting, and HDV recording on tape. The other is the "program stream", an extended version of the container format without the extra overhead of previously mentioned "transport stream" designed for random access storage mediums such as hard disk drives, optical discs and flash memory.
"Program stream" exceptions are M2TS, which is used on Blu-ray discs, AVCHD on re-writable DVDs and HDV on compact flash cards all use the unnecessary overhead of a "transport stream". While VOB on DVDs and Enhanced VOB on the short lived HD DVD do not waste storage space and just use the "program stream". M2TS also adds an incompatible private extension of four network ordered bytes to the end of every transport stream packet, which is used as a random access timing reference for faster read times over using the "Program Clock Reference" contained in the adaption section of the primary stream.
MPEG-2 Systems is formally known as ISO/IEC 13818-1 and as ITU-T Rec. H.222.0. ISO authorized the "SMPTE Registration Authority, LLC" as the registration authority for MPEG-2 format identifiers. The registration descriptor of MPEG-2 transport is provided by ISO/IEC 13818-1 in order to enable users of the standard to unambiguously carry data when its format is not necessarily a recognized international standard. This provision will permit the MPEG-2 transport standard to carry all types of data while providing for a method of unambiguous identification of the characteristics of the underlying private data.
Video.
The Video section, part 2 of MPEG-2, is similar to the previous MPEG-1 standard, but also provides support for interlaced video, the format used by analog broadcast TV systems. MPEG-2 video is not optimized for low bit-rates, especially less than 1 Mbit/s at standard definition resolutions. All standards-compliant MPEG-2 Video decoders are fully capable of playing back MPEG-1 Video streams conforming to the Constrained Parameters Bitstream syntax. MPEG-2/Video is formally known as ISO/IEC 13818-2 and as ITU-T Rec. H.262.
With some enhancements, MPEG-2 Video and Systems are also used in some HDTV transmission systems, and is the standard format for over-the-air ATSC digital television.
Audio.
MPEG-2 introduces new audio encoding methods compared to MPEG-1:
MPEG-2 Part 3.
The MPEG-2 Audio section, defined in Part 3 (ISO/IEC 13818-3) of the standard, enhances MPEG-1's audio by allowing the coding of audio programs with more than two channels, up to 5.1 multichannel. This method is backwards-compatible (also known as MPEG-2 BC), allowing MPEG-1 audio decoders to decode the two main stereo components of the presentation. MPEG-2 part 3 also defined additional bit rates and sample rates for MPEG-1 Audio Layer I, II and III.
MPEG-2 BC (backward compatible with MPEG-1 audio formats)
MPEG-2 Part 7.
Part 7 (ISO/IEC 13818-7) of the MPEG-2 standard specifies a rather different, non-backwards-compatible audio format (also known as MPEG-2 NBC). Part 7 is referred to as MPEG-2 AAC. AAC is more efficient than the previous MPEG audio standards, and is in some ways less complicated than its predecessor, MPEG-1 Audio, Layer 3, in that it does not have the hybrid filter bank. It supports from 1 to 48 channels at sampling rates of 8 to 96 kHz, with multichannel, multilingual, and multiprogram capabilities. Advanced Audio is also defined in Part 3 of the MPEG-4 standard.
MPEG-2 NBC (Non-Backward Compatible)
ISO/IEC 13818.
MPEG-2 standards are published as parts of ISO/IEC 13818. Each part covers a certain aspect of the whole specification.
History.
MPEG-2 evolved out of the shortcomings of MPEG-1.
MPEG-1's known weaknesses:
Filename extensions.
.mpg, .mpeg, .m2v, .mp2, mp3 are some of a number of filename extensions used for MPEG-1 or MPEG-2 audio and video file formats.
Applications.
DVD-Video.
The DVD-Video standard uses MPEG-2 video, but imposes some restrictions:
HDV.
HDV is a format for recording and playback of high-definition MPEG-2 video on a DV cassette tape.
MOD and TOD.
MOD and TOD are recording formats for use in consumer digital file-based camcorders.
XDCAM.
XDCAM is a professional file-based video recording format.
DVB.
Application-specific restrictions on MPEG-2 video in the DVB standard:
Allowed resolutions for SDTV:
For HDTV:
ATSC.
The ATSC A/53 standard used in the United States, uses MPEG-2 video at the Main Profile @ High Level (MP@HL), with additional restrictions such as the maximum bitrate of 19.4 Mbit/s for broadcast television and 38.8 Mbit/s for cable television, 4:2:0 chroma subsampling format, and mandatory colorimetry information.
ATSC allows the following video resolutions, aspect ratios, and frame/field rates:
ATSC standard A/63 defines additional resolutions and aspect rates for 50 Hz (PAL) signal.
The ATSC specification and MPEG-2 allow the use of progressive frames, even within an interlaced video sequence. For example, a station that transmits 1080i60 video sequence can use a coding method where those 60 fields are coded with 24 progressive frames and metadata instructs the decoder to interlace them and perform 3:2 pulldown before display. This allows broadcasters to switch between 60 Hz interlaced (news, soap operas) and 24 Hz progressive (prime-time) content without ending the MPEG-2 sequence and introducing a several seconds of delay as the TV switches formats. This is the reason why 1080p30 and 1080p24 sequences allowed by the ATSC specification are not used in practice.
The 1080-line formats are encoded with 1920 × 1088 pixel luma matrices and 960 × 540 chroma matrices, but the last 8 lines are discarded by the MPEG-2 decoding and display process.
ATSC A/72 is the newest revision of ATSC standards for digital television, which allows the use of H.264/AVC video coding format and 1080p60 signal.
MPEG-2 audio was a contender for the ATSC standard during the DTV "Grand Alliance" shootout, but lost out to Dolby AC-3.
ISDB-T.
Technical features of MPEG-2 in ATSC are also valid for ISDB-T, except that in the main TS has aggregated a second program for mobile devices compressed in MPEG-4 H.264 AVC for video and AAC-LC for audio, mainly known as 1seg.
Blu-ray.
Even if a disc isn't MPEG-2 coded the first 10 second long "FBI anti-piracy warning" is MPEG-2 coded.
Patent pool.
MPEG LA, a private patent licensing organization, has acquired rights from over 20 corporations and one university to license a patent pool of approximately 640 worldwide patents, which it claims are the "essential" to use of MPEG-2 technology, although many of the patents have since expired. Where software patentability is upheld, the use of MPEG-2 requires the payment of licensing fees to the patent holders. Other patents are licensed by Audio MPEG, Inc. The development of the standard itself took less time than the patent negotiations. Patent pooling between essential and peripheral patent holders in the MPEG-2 pool is the subject of a study by the University of Wisconsin. Over half of the patents expired in 2012.
According to the MPEG-2 licensing agreement any use of MPEG-2 technology is subject to royalties. MPEG-2 encoders are subject to a royalty of $2.00 per unit, decoders are subject to a royalty of $2.00 per unit, and royalty-based sales of encoders and decoders are subject to different rules and $2.50 per unit. Also, any packaged medium (DVDs/Data Streams) is subject to licence fees according to length of recording/broadcast. 
A criticism of the MPEG-2 patent pool is that even though the number of patents will decrease from 1,048 to 416 by June 2013 the license fee has not decreased with the expiration rate of MPEG-2 patents. Since January 1, 2010, the MPEG-2 patent pool has remained at $2 for a decoding license and $2 for an encoding license. By 2015 more than 90% of the MPEG-2 patents will have expired but as long as there are one or more active patents in the MPEG-2 patent pool in either the country of manufacture or the country of sale the MPEG-2 license agreement requires that licensees pay a license fee that does not change based on the number of patents that have expired.

</doc>
<doc id="20061" url="https://en.wikipedia.org/wiki?curid=20061" title="MPEG-3">
MPEG-3

MPEG-3 is the designation for a group of audio and video coding standards agreed upon by the Moving Picture Experts Group (MPEG) designed to handle HDTV signals at 1080p in the range of 20 to 40 megabits per second. MPEG-3 was launched as an effort to address the need of an HDTV standard while work on MPEG-2 was underway, but it was soon discovered that MPEG-2, at high data rates, would accommodate HDTV. Thus, in 1992 HDTV was included as a separate profile in the MPEG-2 standard and MPEG-3 was rolled into MPEG-2.

</doc>
<doc id="20062" url="https://en.wikipedia.org/wiki?curid=20062" title="Meditation">
Meditation

Meditation is a practice where an individual trains the mind or induces a mode of consciousness, either to realize some benefit or for the mind to simply acknowledge its content without becoming identified with that content, or as an end in itself. 
The term "meditation" refers to a broad variety of practices that includes techniques designed to promote relaxation, build internal energy or life force ("qi", "ki", "prana", etc.) and develop compassion, love, patience, generosity and forgiveness. A particularly ambitious form of meditation aims at effortlessly sustained single-pointed concentration meant to enable its practitioner to enjoy an indestructible sense of well-being while engaging in any life activity.
The word "meditation" carries different meanings in different contexts. Meditation has been practiced since antiquity as a component of numerous religious traditions and beliefs. Meditation often involves an internal effort to self-regulate the mind in some way. Meditation is often used to clear the mind and ease many health concerns, such as high blood pressure, depression, and anxiety. It may be done sitting, or in an active way—for instance, Buddhist monks involve awareness in their day-to-day activities as a form of mind-training. Prayer beads or other ritual objects are commonly used during meditation in order to keep track of or remind the practitioner about some aspect of that training.
Meditation may involve generating an emotional state for the purpose of analyzing that state—such as anger, hatred, etc.—or cultivating a particular mental response to various phenomena, such as compassion. The term "meditation" can refer to the state itself, as well as to practices or techniques employed to cultivate the state. Meditation may also involve repeating a mantra and closing the eyes. The mantra is chosen based on its suitability to the individual meditator. Meditation has a calming effect and directs awareness inward until pure awareness is achieved, described as "being awake inside without being aware of anything except awareness itself." In brief, there are dozens of specific styles of meditation practice, and many different types of activity commonly referred to as meditative practices.
Etymology.
The English "meditation" is derived from the Latin "meditatio", from a verb "meditari", meaning "to think, contemplate, devise, ponder".
In the Old Testament, "hāgâ" (Hebrew: "הגה") means to sigh or murmur, and also, to meditate. When the Hebrew Bible was translated into Greek, "hāgâ" became the Greek "melete". The Latin Bible then translated "hāgâ"/"melete" into "meditatio".
The use of the term "meditatio" as part of a formal, stepwise process of meditation goes back to the 12th-century monk Guigo II.
The Tibetan word for meditation "Gom" means "to become familiar with" and has the strong implication of training the mind to be familiar with states that are beneficial: concentration, compassion, correct understanding, patience, humility, perseverance, etc.
Apart from its historical usage, the term "meditation" was introduced as a translation for Eastern spiritual practices, referred to as "dhyāna" in Buddhism and in Hinduism, which comes from the Sanskrit root "dhyai", meaning to contemplate or meditate. The term "meditation" in English may also refer to practices from Islamic Sufism, or other traditions such as Jewish Kabbalah and Christian Hesychasm.
An edited book about "meditation" published in 2003, for example, included chapter contributions by authors describing Hindu, Buddhist, Taoist, Jewish, Christian and Islamic traditions.
Scholars have noted that "the term 'meditation' as it has entered contemporary usage" is parallel to the term "contemplation" in Christianity, but in many cases, practices similar to modern forms of meditation were simply called 'prayer'. Christian, Judaic and Islamic forms of meditation are typically devotional, scriptural or thematic, while Asian forms of meditation are often more purely technical.
History.
The history of meditation is intimately bound up with the religious context within which it was practiced. Some authors have even suggested the hypothesis that the emergence of the capacity for focused attention, an element of many methods of meditation, may have contributed to the final phases of human biological evolution. Some of the earliest references to meditation are found in the Hindu Vedas of Nepal ad India. Wilson translates the most famous Vedic mantra 'Gayatri' thus : "We meditate on that desirable light of the divine Savitri, who influences our pious rites" (Rgveda : Mandala-3, Sukta-62, Rcha-10). Around the 6th to 5th centuries BCE, other forms of meditation developed in Confucian, and Taoist in China and Hindu, Jain and Buddhist in Nepa and India.
In the west, by 20 BCE Philo of Alexandria had written on some form of "spiritual exercises" involving attention (prosoche) and concentration and by the 3rd century Plotinus had developed meditative techniques.
The Pāli Canon, which dates to 1st century BCE considers Indian Buddhist meditation as a step towards salvation. By the time Buddhism was spreading in China, the "Vimalakirti Sutra" which dates to 100 CE included a number of passages on meditation, clearly pointing to Zen (known as Chan in China, Thiền in Vietnam, and Seon in Korea). The Silk Road transmission of Buddhism introduced meditation to other Asian countries, and in 653 the first meditation hall was opened in Singapore. Returning from China around 1227, Dōgen wrote the instructions for zazen.
The Islamic practice of Dhikr had involved the repetition of the 99 Names of God since the 8th or 9th century. By the 12th century, the practice of Sufism included specific meditative techniques, and its followers practiced breathing controls and the repetition of holy words. Interactions with Indians, Nepalese or the Sufis may have influenced the Eastern Christian meditation approach to hesychasm, but this can not be proved. Between the 10th and 14th centuries, hesychasm was developed, particularly on Mount Athos in Greece, and involves the repetition of the Jesus prayer.
Western Christian meditation contrasts with most other approaches in that it does not involve the repetition of any phrase or action and requires no specific posture. Western Christian meditation progressed from the 6th century practice of Bible reading among Benedictine monks called Lectio Divina, i.e. divine reading. Its four formal steps as a "ladder" were defined by the monk Guigo II in the 12th century with the Latin terms "lectio", "meditatio", "oratio", and "contemplatio" (i.e. read, ponder, pray, contemplate). Western Christian meditation was further developed by saints such as Ignatius of Loyola and Teresa of Avila in the 16th century.
Secular forms of meditation were introduced in India in the 1950s as a Westernized form of Hindu meditative techniques and arrived in the United States and Europe in the 1960s. Rather than focusing on spiritual growth, secular meditation emphasizes stress reduction, relaxation and self-improvement. Both spiritual and secular forms of meditation have been subjects of scientific analyses. Research on meditation began in 1931, with scientific research increasing dramatically during the 1970s and 1980s. Since the beginning of the '70s more than a thousand studies of meditation in English-language have been reported. However, after 60 years of scientific study, the exact mechanism at work in meditation remains unclear.
Modern definitions and Western models.
Definitions and scope.
As early as 1971, Claudio Naranjo noted that "The word 'meditation' has been used to designate a variety of practices that differ enough from one another so that we may find trouble in defining what "meditation" is." There remains no definition of necessary and sufficient criteria for meditation that has achieved universal or widespread acceptance within the modern scientific community, as one study recently noted a "persistent lack of consensus in the literature" and a "seeming intractability of defining "meditation"".
In popular usage, the word "meditation" and the phrase "meditative practice" are often used imprecisely to designate broadly similar practices, or sets of practices, that are found across many cultures and traditions.
Some of the difficulty in precisely defining meditation has been the need to recognize the particularities of the many various traditions. There may be differences between the theories of one tradition of meditation as to what it means to practice meditation. The differences between the various traditions themselves, which have grown up a great distance apart from each other, may be even starker. To accurately define "what is meditation" has caused difficulties for modern scientists. Scientific reviews have proposed that researchers attempt to more clearly define the type of meditation being practiced in order that the results of their studies be made clearer. Taylor noted that to refer only to meditation from a particular faith (e.g., "Hindu" or "Buddhist")
The table shows several definitions of meditation that have been used by influential modern reviews of research on meditation across multiple traditions. Within a specific context, more precise meanings are not uncommonly given the word "meditation". For example, "meditation" is sometimes the translation of "meditatio" in Latin. "Meditatio" is the third of four steps of "Lectio Divina", an ancient form of Christian prayer. "Meditation" also refers to the seventh of the eight steps of Yoga in Patanjali's "Yoga Sutras", a step called "dhyāna" in Sanskrit. Meditation refers to a mental or spiritual "state" that may be attained by such practices, and also refers to the "practice" of that state.
This article mainly focuses on meditation in the broad sense of a type of discipline, found in various forms in many cultures, by which the practitioner attempts to get beyond the reflexive, "thinking" mind (sometimes called "discursive thinking" or "logic") into a deeper, more devout, or more relaxed state. The terms "meditative practice" and "meditation" are mostly used here in this broad sense. However, usage may vary somewhat by context – readers should be aware that in quotations, or in discussions of particular traditions, more specialized meanings of "meditation" may sometimes be used (with meanings made clear by context whenever possible).
Western typologies.
Ornstein noted that "Most techniques of meditation do not exist as solitary practices but are only artificially separable from an entire system of practice and belief." This means that, for instance, while monks engage in meditation as a part of their everyday lives, they also engage the codified rules and live together in monasteries in specific cultural settings that go along with their meditative practices. These meditative practices sometimes have similarities (often noticed by Westerners), for instance concentration on the breath is practiced in Zen, Tibetan and Theravadan contexts, and these similarities or "typologies" are noted here.
Progress on the "intractable" problem of defining meditation was attempted by a recent study of views common to seven experts trained in diverse but empirically highly studied (clinical or Eastern-derived) forms of meditation. The study identified "three main criteria... as essential to any meditation practice: the use of a defined technique, logic relaxation, and a self-induced state/mode. Other criteria deemed important not essential involve a state of psychophysical relaxation, the use of a self-focus skill or anchor, the presence of a state of suspension of logical thought processes, a religious/spiritual/philosophical context, or a state of mental silence." However, the study cautioned, "It is plausible that meditation is best thought of as a natural category of techniques best captured by 'family resemblances'... or by the related 'prototype' model of concepts."
In modern psychological research, meditation has been defined and characterized in a variety of ways; many of these emphasize the role of attention.
In the West, meditation is sometimes thought of in two broad categories: concentrative meditation and mindfulness meditation. These two categories are discussed in the following two paragraphs, with concentrative meditation being used interchangeably with focused attention and mindfulness meditation being used interchangeably with open monitoring,
Direction of mental attention... A practitioner can focus intensively on one particular object (so-called "concentrative meditation"), on all mental events that enter the field of awareness (so-called "mindfulness meditation"), or both specific focal points and the field of awareness.
One style, Focused Attention (FA) meditation, entails the voluntary focusing of attention on a chosen object. The other style, Open Monitoring (OM) meditation, involves non-reactive monitoring of the content of experience from moment to moment.
Other typologies have also been proposed, and some techniques shift among major categories.
Evidence from neuroimaging studies suggests that the categories of meditation, defined by how they direct attention, appear to generate different brainwave patterns. Evidence also suggests that using different focus objects during meditation may generate different brainwave patterns.
Forms of meditation.
Physical postures.
Various postures are taken up in some meditation techniques. Sitting, supine, and standing postures are used. Popular in Buddhism, Jainism and Hinduism are the full-lotus, half-lotus, Burmese, Sieza, and kneeling positions. Meditation is sometimes done while walking, known as kinhin, or while doing a simple task mindfully, known as samu.
Some mantra techniques (as with Transcendental Meditation) do not require learning special positions, only sitting comfortably with eyes closed.
Prayer beads.
Most of the ancient religions of the world have a tradition of using some type of prayer beads as tools in devotional meditation. Most prayer beads and Christian rosaries consist of pearls or beads linked together by a thread. The Roman Catholic rosary is a string of beads containing five sets with ten small beads. Each set of ten is separated by another bead. The Hindu japa mala has 108 beads (the figure 108 in itself having spiritual significance, as well as those used in Jainism and Buddhist prayer beads. Each bead is counted once as a person recites a mantra until the person has gone all the way around the mala. The Muslim mishbaha has 99 beads. Specific meditations of each religion may be different.
Religious and spiritual meditation.
Indian religions.
Jainism.
In Jainism, meditation has been a core spiritual practice, one that Jains believe people have undertaken since the teaching of the Tirthankara, Rishabha. All the twenty-four Tirthankaras practiced deep meditation and attained enlightenment. They are all shown in meditative postures in the images or idols. Mahavira practiced deep meditation for twelve years and attained enlightenment. The Acaranga Sutra dating to 500 BCE, addresses the meditation system of Jainism in detail. Acharya Bhadrabahu of the 4th century BCE practiced deep "Mahaprana" meditation for twelve years. Kundakunda of 1st century BCE, opened new dimensions of meditation in Jain tradition through his books "Samayasāra", "Pravachansar" and others.
Jain meditation and spiritual practices system were referred to as salvation-path. It has three important parts called the "Ratnatraya" "Three Jewels": right perception and faith, right knowledge and right conduct. Meditation in Jainism aims at realizing the self, attaining salvation, take the soul to complete freedom. It aims to reach and to remain in the pure state of soul which is believed to be pure consciousness, beyond any attachment or aversion. The practitioner strives to be just a knower-seer (Gyata-Drashta). Jain meditation can be broadly categorized to "Dharmya Dhyana" and "Shukla Dhyana".
There exists a number of meditation techniques such as "pindāstha-dhyāna, padāstha-dhyāna, rūpāstha-dhyāna, rūpātita-dhyāna, savīrya-dhyāna", etc. In "padāstha dhyāna" one focuses on Mantra. A Mantra could be either a combination of core letters or words on deity or themes. There is a rich tradition of Mantra in Jainism. All Jain followers irrespective of their sect, whether Digambara or Svetambara, practice mantra. Mantra chanting is an important part of daily lives of Jain monks and followers. Mantra chanting can be done either loudly or silently in mind. Yogasana and "Pranayama" has been an important practice undertaken since ages. Pranayama – breathing exercises – are performed to strengthen the five "Pranas" or vital energy. Yogasana and "Pranayama" balances the functioning of neuro-endocrine system of body and helps in achieving good physical, mental and emotional health.
Contemplation is a very old and important meditation technique. The practitioner meditates deeply on subtle facts. In "agnya vichāya", one contemplates on seven facts – life and non-life, the inflow, bondage, stoppage and removal of "karmas", and the final accomplishment of liberation. In "apaya vichāya", one contemplates on the incorrect insights one indulges, which eventually develops right insight. In "vipaka vichāya", one reflects on the eight causes or basic types of "karma". In "sansathan vichāya", one thinks about the vastness of the universe and the loneliness of the soul.
Acharya Mahapragya formulated Preksha meditation in the 1970s and presented a well-organised system of meditation. Asana and "Pranayama", meditation, contemplation, mantra and therapy are its integral parts. Numerous Preksha meditation centers came into existence around the world and numerous meditations camps are being organized to impart training in it.
Buddhism.
Buddhist meditation refers to the meditative practices associated with the religion and philosophy of Buddhism. Core meditation techniques have been preserved in ancient Buddhist texts and have proliferated and diversified through teacher-student transmissions. Buddhists pursue meditation as part of the path toward enlightenment and nirvana. The closest words for meditation in the classical languages of Buddhism are "bhāvanā", "jhāna"/"dhyāna", and "vipassana". According to Manmatha Nath Dutt, there is hardly any difference between mainstream Hinduism's Dhyana, Dharana and Samadhi with the Buddhist Dhyana, Bhavana, Samadhi, especially as both require following the precepts (nayas and niyamas).
Buddhist meditation techniques have become increasingly popular in the wider world, with many non-Buddhists taking them up for a variety of reasons. There is considerable homogeneity across meditative practices – such as breath meditation and various recollections ("anussati") – that are used across Buddhist schools, as well as significant diversity. In the Theravāda tradition alone, there are over fifty methods for developing mindfulness and forty for developing concentration, while in the Tibetan tradition there are thousands of visualization meditations. Most classical and contemporary Buddhist meditation guides are school-specific.
The Buddha is said to have identified two paramount mental qualities that arise from wholesome meditative practice:
Through the meditative development of serenity, one is able to release obscuring hindrances; it is with the release of the hindrances through the meditative development of insight that one gains liberating wisdom.
Hinduism.
There are many schools and styles of meditation within Hinduism.
Traditional.
Yoga is generally done to prepare one for meditation, and meditation is done to realize union of one's self, one's ātman, with the omnipresent and non-dual Brahman. This experience is referred to as moksha by Hindus, and is similar to the concept of nirvana in Buddhism. The earliest clear references to meditation in Hindu literature are in the middle Upanishads and the Mahabharata, the latter of which includes the Bhagavad Gita. According to Gavin Flood, the earlier Brihadaranyaka Upanishad refers to meditation when it states that "having become calm and concentrated, one perceives the self ("ātman") within oneself".
Within Patañjali's Ashtanga yoga practice there are eight limbs leading to kaivalya "aloneness." These are ethical discipline (yamas), rules (niyamas), physical postures (āsanas), breath control (prāṇāyama), withdrawal from the senses (pratyāhāra), one-pointedness of mind (dhāraṇā), meditation (dhyāna), and finally samādhi, which is often described as the realization of the identity of the Self (ātman) with the omnipresent (Brahman), and is the ultimate aim of all Hindu yogis.
New religious movements.
Meditation in Hinduism has expanded beyond Hinduism to the West. Mantra meditation, with the use of a japa mala and especially with focus on the Hare Krishna maha-mantra, is a central practice of the Gaudiya Vaishnava faith tradition and the International Society for Krishna Consciousness (ISKCON), also known as the Hare Krishna movement. Other popular New Religious Movements include the Ramakrishna Mission, Vedanta Society, Divine Light Mission, Chinmaya Mission, Osho, Transcendental Meditation, Oneness University, and Brahma Kumaris.
Sikhism.
In Sikhism, simran (meditation) and good deeds are both necessary to achieve the devotee's Spiritual goals; without good deeds meditation is futile. When Sikhs meditate they aim to feel God's presence and immerge in the divine light. It is only God's divine will or order that allows a devotee to desire to begin to meditate. Guru Nanak in the Japji Sahib daily Sikh scripture explains, ""Visits to temples, penance, compassion and charity gain you but a sesame seed of credit. It is hearkening to His Name, accepting and adoring Him that obtains emancipation by bathing in the shrine of soul. All virtues are Yours, O Lord! I have none; Without good deeds one can't even meditate."" Japji Sahib (Stanza 21).
Nām Japnā involves focusing one's attention on the names or great attributes of God. The practices of Simran and Nām Japnā encourage quiet internal meditation but may be practiced vocally in the sangat (holy congregation). Sikhs believe that there are ten 'gates' to the body, the nine visible holes (nostrils, eyes, ears, mouth, urethra, anus) and the tenth invisible hole. The tenth invisible hole is the topmost energy level and is called the tenth gate or Dasam Duaar. When one reaches this stage through continuous practice meditation becomes a habit that continues whilst walking, talking, eating, awake and even sleeping. There is a distinct taste or flavour when a meditator reaches this lofty stage of meditation, and experiences absolute peace and tranquility inside and outside the body.
Followers of the Sikh religion also believe that love comes through meditation on the lord's name since meditation only conjures up positive emotions in oneself which are portrayed through our actions. The first Guru of the Sikhs, Guru Nanak Dev Ji preached the equality of all humankind and stressed the importance of living a householder's life instead of wandering around jungles meditating, the latter of which being a popular practice at the time. The Guru preached that we can obtain liberation from life and death by living a totally normal family life and by spreading love amongst every human being regardless of religion.
In the Sikh religion, kirtan, otherwise known as singing the hymns of God is seen as one of the most beneficial ways of aiding meditation, and it too in some ways is believed to be a meditation of one kind.
East-Asian religions.
Taoism.
Taoist or Daoist meditation has a long history, and has developed various techniques including concentration, visualization, "qi" cultivation, contemplation, and mindfulness meditations. Traditional Daoist meditative practices were influenced by Chinese Buddhism beginning around the 5th century, and later had influence upon Traditional Chinese medicine and the Chinese martial arts.
Livia Kohn distinguishes three basic types of Daoist meditation: "concentrative", "insight", and "visualization". "Ding" 定 (literally means "decide; settle; stabilize") refers to "deep concentration", "intent contemplation", or "perfect absorption." "Guan" 觀 (lit. "watch; observe; view") meditation seeks to merge and attain unity with the Dao. It was developed by Tang Dynasty (618–907) Daoist masters based upon the "Tiantai" Buddhist practice of "Vipassanā" "insight" or "wisdom" meditation. "Cun" 存 (lit. "exist; be present; survive") has a sense of "to cause to exist; to make present" in the meditation techniques popularized by the Daoist Shangqing and Lingbao Schools. A meditator visualizes or actualizes solar and lunar essences, lights, and deities within his/her body, which supposedly results in health and longevity, even "xian" 仙/仚/僊, "immortality".
The (late 4th century) "Guanzi" essay "Neiye" 內業 "Inward training" is the oldest received writing on the subject of "qi" cultivation and breath-control meditation techniques. For instance, "When you enlarge your mind and let go of it, when you relax your vital breath and expand it, when your body is calm and unmoving: And you can maintain the One and discard the myriad disturbances. ... This is called "revolving the vital breath": Your thoughts and deeds seem heavenly."
The (c. 3rd century BCE) Daoist "Zhuangzi" records "zuowang" or "sitting forgetting" meditation. Confucius asked his disciple Yan Hui to explain what "sit and forget" means: "I slough off my limbs and trunk, dim my intelligence, depart from my form, leave knowledge behind, and become identical with the Transformational Thoroughfare."
Daoist meditation practices are central to Chinese martial arts (and some Japanese martial arts), especially the "qi"-related "neijia" "internal martial arts". Some well-known examples are "daoyin" "guiding and pulling", "qigong" "life-energy exercises", "neigong" "internal exercises", "neidan" "internal alchemy", and "taijiquan" "great ultimate boxing", which is thought of as moving meditation. One common explanation contrasts "movement in stillness" referring to energetic visualization of "qi" circulation in "qigong" and "zuochan" "seated meditation", versus "stillness in movement" referring to a state of meditative calm in "taijiquan" forms.
Iranian religions.
Bahá'í Faith.
In the teachings of the Bahá'í Faith, meditation along with prayer are both primary tools for spiritual development and mainly refer to one's reflection on the words of God. While prayer and meditation are linked, where meditation happens generally in a prayerful attitude, prayer is seen specifically as turning toward God, and meditation is seen as a communion with one's self where one focuses on the divine.
The Bahá'í teachings note that the purpose of meditation is to strengthen one's understanding of the words of God, and to make one's soul more susceptible to their potentially transformative power, more receptive to the need for both prayer and meditation to bring about and maintain a spiritual communion with God.
Bahá'u'lláh, the founder of the religion, never specified any particular form of meditation, and thus each person is free to choose their own form. However, he specifically did state that Bahá'ís should read a passage of the Bahá'í writings twice a day, once in the morning, and once in the evening, and meditate on it. He also encouraged people to reflect on one's actions and worth at the end of each day. During the Nineteen Day Fast, a period of the year during which Bahá'ís adhere to a sunrise-to-sunset fast, they meditate and pray to reinvigorate their spiritual forces.
Abrahamic religions.
Judaism.
There is evidence that Judaism has had meditative practices that go back thousands of years. For instance, in the Torah, the patriarch Isaac is described as going ""לשוח"" ("lasuach") in the field—a term understood by all commentators as some type of meditative practice (Genesis 24:63).
Similarly, there are indications throughout the Tanach (the Hebrew Bible) that meditation was used by the prophets. In the Old Testament, there are two Hebrew words for meditation: "hāgâ" (), which means "to sigh" or "murmur", but also "to meditate", and "sîḥâ" (), which means "to muse", or "rehearse in one's mind".
Some meditative traditions have been encouraged in the school of Judaism known as Kabbalah, and some Jews have described Kabbalah as an inherently meditative field of study. Aryeh Kaplan has argued that, for the Kabbalist, the ultimate purpose of meditative practice is to understand and cleave to the Divine. Classic methods include the mental visualisation of the supernal realms the soul navigates through to achieve certain ends. One of the best known types of meditation in early Jewish mysticism was the work of the Merkabah, from the root /R-K-B/ meaning "chariot" (of God).
Meditation has been of interest to a wide variety of modern Jews. In modern Jewish practice, one of the best known meditative practices is called ""hitbodedut"" ("התבודדות", alternatively transliterated as "hisbodedus"), and is explained in Kabbalistic, Hasidic, and Mussar writings, especially the Hasidic method of Rabbi Nachman of Breslav. The word derives from the Hebrew word "boded" (בודד), meaning the state of being alone. Another Hasidic system is the Habad method of "hisbonenus", related to the Sephirah of "Binah", Hebrew for understanding. This practice is the analytical reflective process of making oneself understand a mystical concept well, that follows and internalises its study in Hasidic writings.
The Musar Movement, founded by Rabbi Israel Salanter in the middle of the nineteenth-century, emphasized meditative practices of introspection and visualization that could help to improve moral character.
Christianity.
Christian meditation is a term for a form of prayer in which a structured attempt is made to get in touch with and deliberately reflect upon the revelations of God. The word meditation comes from the Latin word "meditari", which means to concentrate. Christian meditation is the process of deliberately focusing on specific thoughts (e.g. a biblical scene involving Jesus and the Virgin Mary) and reflecting on their meaning in the context of the love of God.
Christian meditation contrasts with Eastern forms of meditation as radically as the portrayal of God the Father in the Bible contrasts with depictions of Krishna or Brahman in Indian teachings. Unlike Eastern meditations, most styles of Christian meditations do not rely on the repeated use of mantras, and yet are also intended to stimulate thought and deepen meaning. Christian meditation aims to heighten the personal relationship based on the love of God that marks Christian communion.
In "Aspects of Christian meditation", the Catholic Church warned of potential incompatibilities in mixing Christian and Eastern styles of meditation. In 2003, in "A Christian reflection on the New Age" the Vatican announced that the "Church avoids any concept that is close to those of the New Age".
Christian meditation is sometimes taken to mean the middle level in a broad three stage characterization of prayer: it then involves more reflection than first level vocal prayer, but is more structured than the multiple layers of contemplation in Christianity.
Islam.
Remembrance of God in Islam, which is known by the concept Dhikr is interpreted in different meditative techniques in Sufism or Islamic mysticism. This became one of the essential elements of Sufism as it was systematized traditionally. It is juxtaposed with "fikr" (thinking) which leads to knowledge. By the 12th century, the practice of Sufism included specific meditative techniques, and its followers practiced breathing controls and the repetition of holy words.
Numerous Sufi traditions place emphasis upon a meditative procedure which comes from the cognitive aspect to one of the two principal approaches to be found in the Buddhist traditions: that of the concentration technique, involving high-intensity and sharply focused introspection. In the Oveyssi-Shahmaghsoudi Sufi order, for example, this is particularly evident, where muraqaba takes the form of tamarkoz, the latter being a Persian term that means "concentration". Meditative quiescence is said to have a quality of healing, and—in contemporary terminology—enhancing "creativity".
"Tafakkur" or "tadabbur" in Sufism literally means "reflection upon the universe": this is considered to permit access to a form of cognitive and emotional development that can emanate only from the higher level, i.e. from God. The sensation of receiving divine inspiration awakens and liberates both heart and intellect, permitting such inner growth that the apparently mundane actually takes on the quality of the infinite. Muslim teachings embrace life as a test of one's submission to God.
Meditation in the Sufi traditions is largely based on a spectrum of mystical exercises, varying from one lineage to another. Such techniques, particularly the more audacious, can be, and often have been down the ages, a source of controversy among scholars. One broad group of ulema, followers of the great Al-Ghazali, for example, have in general been open to such techniques and forms of devotion.
In recent years, meditation or Muraqaba has been popularized in various parts of the world by Silsila Naqshbandia Mujaddadia under Nazim Al-Haqqani and Silsila Azeemia under Khwaja Shamsuddin Azeemi.
Modern spirituality.
New Age.
New Age meditations are often influenced by Eastern philosophy, mysticism, Yoga, Hinduism and Buddhism, yet may contain some degree of Western influence. In the West, meditation found its mainstream roots through the social revolution of the 1960s and 1970s, when many of the youth of the day rebelled against traditional belief systems as a reaction against what some perceived as the failure of Christianity to provide spiritual and ethical guidance.
New Age meditation as practised by the early hippies is regarded for its techniques of blanking out the mind and releasing oneself from conscious thinking. This is often aided by repetitive chanting of a mantra, or focusing on an object. New Age meditation evolved into a range of purposes and practices, from serenity and balance to access to other realms of consciousness to the concentration of energy in group meditation to the supreme goal of samadhi, as in the ancient yogic practice of meditation.
Pagan and occult religions.
Religions and religious movements which use magic, such as Wicca, Thelema, Neopaganism, occultism etc., often require their adherents to meditate as a preliminary to the magical work. This is because magic is often thought to require a particular state of mind in order to make contact with spirits, or because one has to visualize one's goal or otherwise keep intent focused for a long period during the ritual in order to see the desired outcome. Meditation practice in these religions usually revolves around visualization, absorbing energy from the universe or higher self, directing one's internal energy, and inducing various trance states. Meditation and magic practice often overlap in these religions as meditation is often seen as merely a stepping stone to supernatural power, and the meditation sessions may be peppered with various chants and spells.
Western context.
Dissemination in the west.
Methods of meditation have been cross-culturally disseminated at various times throughout history, such as Buddhism going to East Asia, and Sufi practices going to many Islamic societies. Of special relevance to the modern world is the dissemination of meditative practices since the late 19th century, accompanying increased travel and communication among cultures worldwide. Most prominent has been the transmission of numerous Asian-derived practices to the West. In addition, interest in some Western-based meditative practices has also been revived, and these have been disseminated to a limited extent to Asian countries. Also evident is some extent of influence over Enlightenment thinking through Diderot's Encyclopédie; although he states, "I find that a meditation practitioner is often quite useless and that a contemplation practitioner is always insane".
Ideas about Eastern meditation had begun "seeping into American popular culture even before the American Revolution through the various sects of European occult Christianity," and such ideas "came pouring in America during the era of the transcendentalists, especially between the 1840s and the 1880s." But
The World Parliament of Religions, held in Chicago in 1893, was the landmark event that increased Western awareness of meditation. This was the first time that Western audiences on American soil received Asian spiritual teachings from Asians themselves. Thereafter, Swami Vivekananda... various Vedanta ashrams... Anagarika Dharmapala lectured at Harvard on Theravada Buddhist meditation in 1904; Abdul Baha ... [toured the US teaching the principles of Bahai, and Soyen Shaku toured in 1907 teaching Zen...
In the late 19th century, Theosophists adopted the word "meditation" to refer to various spiritual practices drawn from Hinduism, Buddhism and other Indian religions. Thus the English word "meditation" does not exclusively translate to any single term or concept, and can be used to translate words such as the Sanskrit "dhāraṇā", "dhyāna", "samādhi" and "bhāvanā".
More recently, in the 1960s, another surge in Western interest in meditative practices began. Observers have suggested many types of explanations for this interest in Eastern meditation and revived Western contemplation. Thomas Keating, a founder of Contemplative Outreach, wrote that "the rush to the East is a symptom of what is lacking in the West. There is a deep spiritual hunger that is not being satisfied in the West." Daniel Goleman, a scholar of meditation, suggested that the shift in interest from "established religions" to meditative practices "is caused by the scarcity of the personal experience of these [meditation-derived] transcendental states – the living spirit at the common core of all religions."
Another suggested contributing factor is the rise of communist political power in Asia, which, "set the stage for an influx of Asian spiritual teachers to the West," oftentimes as refugees.
Secular applications.
Meditation may be for a religious purpose, but even before being brought to the West it was used in secular contexts. Beginning with the Theosophists meditation has been employed in the West by a number of religious and spiritual movements, such as Yoga, New Age and the New Thought movement.
Meditation techniques have also been used by Western theories of counseling and psychotherapy. Relaxation training works toward achieving mental and muscle relaxation to reduce daily stresses. Jacobson is credited with developing the initial progressive relaxation procedure. These techniques are used in conjunction with other behavioral techniques. Originally used with systematic desensitization, relaxation techniques are now used with other clinical problems. Meditation, hypnosis and biofeedback-induced relaxation are a few of the techniques used with relaxation training. One of the eight essential phases of EMDR (developed by Francine Shapiro), bringing adequate closure to the end of each session, also entails the use of relaxation techniques, including meditation. Multimodal therapy, a technically eclectic approach to behavioral therapy, also employs the use of meditation as a technique used in individual therapy.
From the point of view of psychology and physiology, meditation can induce an altered state of consciousness. Such altered states of consciousness may correspond to altered neuro-physiologic states.
Today, there are many different types of meditation practiced in western culture. Mindful breathing, progressive muscle relaxation, and loving kindness meditations for instance have been found to provide cognitive benefits such as relaxation and decentering. With training in meditation, depressive rumination can be decreased and overall peace of mind can flourish. Different techniques have shown to work better for different people.
As stated by the National Center for Complementary and Alternative Medicine, a U.S. government entity within the National Institutes of Health that advocates various forms of Alternative Medicine, "Meditation may be practiced for many reasons, such as to increase calmness and physical relaxation, to improve psychological balance, to cope with illness, or to enhance overall health and well-being."
Sound-based meditation.
Herbert Benson of Harvard Medical School conducted a series of clinical tests on meditators from various disciplines, including the Transcendental Meditation technique and Tibetan Buddhism. In 1975, Benson published a book titled "The Relaxation Response" where he outlined his own version of meditation for relaxation. Also in the 1970s, the American psychologist Patricia Carrington developed a similar technique called Clinically Standardized Meditation (CSM). In Norway, another sound-based method called Acem Meditation developed a psychology of meditation and has been the subject of several scientific studies.
Biofeedback has been used by many researchers since the 1950s in an effort to enter deeper states of mind.
Mindfulness.
Over the past 20 years, Mindfulness and mindfulness-based programs have become increasingly important to Westerners and in the Western medical and psychological community as a means of helping people, whether they be clinically sick or healthy. Jon Kabat-Zinn, who founded the Mindfulness-Based Stress Reduction program in 1979, has defined mindfulness as 'moment to moment non-judgmental awareness.' Several methods are used during time set aside specifically for mindfulness meditation, such as body scan techniques or letting thought arise and pass, and also during our daily lives, such as being aware of the taste and texture of the food that we eat. Scientifically demonstrated benefits of mindfulness practice include an increase in the body's ability to heal and a shift from a tendency to use the right prefrontal cortex instead of the left prefrontal cortex, associated with a trend away from depression and anxiety, and towards happiness, relaxation, and emotional balance.
Jacobson's Progressive Muscle Relaxation was developed by American physician Edmund Jacobson in the early 1920s. In this practice one tenses and then relaxes muscle groups in a sequential pattern whilst concentrating on how they feel. The method has been seen to help people with many conditions especially extreme anxiety.
As a result of the popularity in participation of mindfulness, conferences such as Wisdom 2.0 have arisen. Mindfulness has entered the secular world in many ways allowing to reach many different groups of people.
Mental silence.
Sahaja yoga meditation is regarded as a mental silence meditation, and has been shown to correlate with particular brain and brain wave activity. Some studies have led to suggestions that Sahaja meditation involves 'switching off' irrelevant brain networks for the maintenance of focused internalized attention and inhibition of inappropriate information. Sahaja meditators scored above peer group for emotional wellbeing measures on SF-36 ratings.
Research on meditation.
Beneficial effects.
Research on the processes and effects of meditation is a growing subfield of neurological research. Activation of the parasympathetic nervous system and stress relief are thought to play a role in meditation's positive effects on chronic health conditions. Modern scientific techniques and instruments, such as fMRI and EEG, have been used to see what happens in the body of people when they meditate, and how their bodies and brain change after meditating regularly.
Since the 1950s hundreds of studies on meditation have been conducted, though many of the early studies were flawed and thus yielded unreliable results. More recent reviews have pointed out many of these flaws with the hope of guiding current research into a more fruitful path. More reports assessed that further research needs to be directed towards the theoretical grounding and definition of meditation.
There are rare cases of meditation-induced psychosis, primarily in persons with pre-existing psychotic conditions, and also cases where meditation had adverse effects in individuals without psychiatric history.
Negative effects.
Some research has indicated that meditation can have negative effects, often relating to surfacing of pre-existing trauma or depression.
Meditation, religion and drugs.
Many traditions in which meditation is practiced, such as Sahaja Yoga, Transcendental Meditation Buddhism, Hinduism, and other religions, advise members not to consume intoxicants, while others, such as the Rastafarian movements and Native American Church, view drugs as integral to their religious lifestyle.
The fifth of the five precepts of the Pancasila, the ethical code in the Theravada and Mahayana Buddhist traditions, states that adherents must: "abstain from fermented and distilled beverages that cause heedlessness."
On the other hand, the ingestion of psychoactives has been a central feature in the rituals of many religions, in order to produce altered states of consciousness. In several traditional shamanistic ceremonies, drugs are used as agents of ritual. In the Rastafari movement, cannabis is believed to be a gift from Jah and a sacred herb to be used regularly, while alcohol is considered to debase man. Native Americans use peyote, as part of religious ceremony, continuing today. In India, the soma drink has a long history of use alongside prayer and sacrifice, and is mentioned in the Vedas.
During the 1960s, both eastern meditation traditions and psychedelics, such as LSD, became popular in America, and it was suggested that LSD use and meditation were both means to the same spiritual/existential end. Many practitioners of eastern traditions rejected this idea, including many who had tried LSD themselves. In "The Master Game", Robert S de Ropp writes that the "door to full consciousness" can be glimpsed with the aid of substances, but to "pass beyond the door" requires yoga and meditation. Other authors, such as Rick Strassman, believe that the relationship between religious experiences reached by way of meditation and through the use of psychedelic drugs deserves further exploration. Also see Psychedelic psychotherapy.
Popular culture.
Various forms of meditation have been described in popular culture sources. In particular, science fiction stories such as Frank Herbert's "Dune", "Star Trek", "Artemis Fowl", "Star Wars", "Maskman", "Lost Horizon" by James Hilton, and "Stargate SG-1" have featured characters who practice one form of meditation or another. Meditation also appears as overt themes in novels such as Jack Kerouac's "The Dharma Bums" and Herman Hesse's "Siddhartha".

</doc>
<doc id="20063" url="https://en.wikipedia.org/wiki?curid=20063" title="MPEG-4">
MPEG-4

MPEG-4 is a method of defining compression of audio and visual (AV) digital data. It was introduced in late 1998 and designated a standard for a group of audio and video coding formats and related technology agreed upon by the ISO/IEC Moving Picture Experts Group (MPEG) (ISO/IEC JTC1/SC29/WG11) under the formal standard ISO/IEC 14496 – "Coding of audio-visual objects". Uses of MPEG-4 include compression of AV data for web (streaming media) and CD distribution, voice (telephone, videophone) and broadcast television applications.
Background.
MPEG-4 absorbs many of the features of MPEG-1 and MPEG-2 and other related standards, adding new features such as (extended) VRML support for 3D rendering, object-oriented composite files (including audio, video and VRML objects), support for externally specified Digital Rights Management and various types of interactivity. AAC (Advanced Audio Coding) was standardized as an adjunct to MPEG-2 (as Part 7) before MPEG-4 was issued.
MPEG-4 is still an evolving standard and is divided into a number of parts. Companies promoting MPEG-4 compatibility do not always clearly state which "part" level compatibility they are referring to. The key parts to be aware of are MPEG-4 Part 2 (including Advanced Simple Profile, used by codecs such as DivX, Xvid, Nero Digital and 3ivx and by QuickTime 6) and MPEG-4 part 10 (MPEG-4 AVC/H.264 or Advanced Video Coding, used by the x264 encoder, Nero Digital AVC, QuickTime 7, and high-definition video media like Blu-ray Disc).
Most of the features included in MPEG-4 are left to individual developers to decide whether or not to implement. This means that there are probably no complete implementations of the entire MPEG-4 set of standards. To deal with this, the standard includes the concept of "profiles" and "levels", allowing a specific set of capabilities to be defined in a manner appropriate for a subset of applications.
Initially, MPEG-4 was aimed primarily at low bit-rate video communications; however, its scope as a multimedia coding standard was later expanded. MPEG-4 is efficient across a variety of bit-rates ranging from a few kilobits per second to tens of megabits per second. MPEG-4 provides the following functions:
Overview.
MPEG-4 provides a series of technologies for developers, for various service-providers and for end users:
The MPEG-4 format can perform various functions, among which might be the following:
Profiles and Levels.
MPEG-4 provides a large and rich set of tools for encoding.
Subsets of the MPEG-4 tool sets have been provided for use in specific applications.
These subsets, called 'Profiles', limit the size of the tool set a decoder is required to implement. In order to restrict computational complexity, one or more 'Levels' are set for each Profile. A Profile and Level combination allows:
MPEG-4 Parts.
MPEG-4 consists of several standards—termed "parts"—including the following (each part covers a certain aspect of the whole specification):
Profiles are also defined within the individual "parts", so an implementation of a part is ordinarily not an implementation of an entire part.
MPEG-1, MPEG-2, MPEG-7 and MPEG-21 are other suites of MPEG standards.
MPEG-4 Levels.
The low profile levels are part of the MPEG-4 video encoding/decoding constraints and are compatible with the older ITU H.261 standard, also compatible with former analog TV standards for broadcast and records (such as NTSC or PAL video). The ASP profile in its highest level is suitable for most usual DVD medias and players or for many online video sites, but not for Blu-ray records or online HD video contents.
More advanced profiles for HD media have been defined later in the AVC profile, which is functionally identical to the ITU H.264 standard but are now also integrated in MPEG-4 Part 10 (see H.264/MPEG-4 AVC for the list of defined levels in this AVC profile).
Licensing.
MPEG-4 contains patented technologies, the use of which requires licensing in countries that acknowledge software algorithm patents. Over two dozen companies claim to have patents covering MPEG-4. MPEG LA licenses patents required for MPEG-4 Part 2 Visual from a wide range of companies (audio is licensed separately) and lists all of its licensors and licensees on the site. New licenses for MPEG-4 System patents are under development and no new licenses are being offered while holders of its old MPEG-4 Systems license are still covered under the terms of that license for the patents listed (MPEG LA – Patent List).
AT&T is trying to sue companies such as Apple Inc. over alleged MPEG-4 patent infringement. The terms of Apple's QuickTime 7 license for users describes in paragraph 14 the terms under Apple's existing MPEG-4 System Patent Portfolio license from MPEG LA.

</doc>
<doc id="20064" url="https://en.wikipedia.org/wiki?curid=20064" title="Maritime archaeology">
Maritime archaeology

Maritime archaeology (also known as marine archaeology) is a discipline within archaeology as a whole that specifically studies human interaction with the sea, lakes and rivers through the study of associated physical remains, be they vessels, shore side facilities, port-related structures, cargoes, human remains and submerged landscapes. A specialty within maritime archaeology is nautical archaeology, which studies vessel construction and use. As with archaeology as a whole, maritime archaeology can be practised within the historical, industrial, or prehistoric periods. An associated discipline, and again one that lies within archaeology itself, is underwater archaeology, which studies the past through any submerged remains be they of maritime interest or not. An example from the prehistoric era would be the remains submerged settlements or deposits now lying under water despite having been dry land when sea levels were lower. The study of submerged aircraft lost in lakes, rivers or in the sea is an example from the historical, industrial or modern era. Many specialist sub-disciplines within the broader maritime and underwater archaeological categories have emerged in recent years.
Maritime archaeological sites often result from shipwrecks or sometimes seismic catastrophes, and thus represent a moment in time rather than a slow deposition of material accumulated over a period of years, as is the case with port-related structures (such as piers, wharves and jetties) where objects are lost or thrown off structures over extended periods of time. This fact has led to shipwrecks often being described in the media and in popular accounts as 'time capsules'.
Archaeological material in the sea or in other underwater environments is typically subject to different factors than artifacts on land. However, as with terrestrial archaeology what survives to be investigated by modern archaeologists can often be a tiny fraction of the material originally deposited. A feature of maritime archaeology is that despite all the material that is lost, there are occasional rare examples of substantial survival, from which a great deal can be learned, due to the difficulties often experienced in accessing the sites.
There are those in the archaeology community who see maritime archaeology as a separate discipline with its own concerns (such as shipwrecks) and requiring the specialized skills of the underwater archaeologist. Others value an integrated approach, stressing that nautical activity has economic and social links to communities on land and that archaeology is archaeology no matter where the study is conducted. All that is required is the mastering of skills specific to the environment in which the work occurs.
Integrating land and sea.
Before the industrial era, travel by water was often easier than over land. As a result, marine channels, navigable rivers and sea crossings formed the trade routes of historic and ancient civilisations. For example, the Mediterranean Sea was known to the Romans as the inner sea because the Roman empire spread around its coasts. The historic record as well as the remains of harbours, ships and cargoes, testify to the volume of trade that crossed it. Later, nations with a strong maritime culture such as the United Kingdom, the Netherlands, Denmark, Portugal and Spain were able to establish colonies on other continents. Wars were fought at sea over the control of important resources. The material cultural remains that are discovered by maritime archaeologists along former trade routes can be combined with historic documents and material cultural remains found on land to understand the economic, social and political environment of the past. Of late maritime archaeologists have been examining the submerged cultural remains of China, India, Korea and other Asian nations.
Preservation of material underwater.
There are significant differences in the survival of archaeological material depending on whether a site is wet or dry, on the nature of the chemical environment, on the presence of biological organisms and on the dynamic forces present. Thus rocky coastlines, especially in shallow water, are typically inimical to the survival of artifacts, which can be dispersed, smashed or ground by the effect of currents and surf, possibly (but not always) leaving an artifact pattern but little if any wreck structure.
Saltwater is particularly inimical to iron artefacts including metal shipwrecks, and sea organisms will readily consume organic material such as wooden shipwrecks. On the other hand, out of all the thousands of potential archaeological sites destroyed or grossly eroded by such natural processes, occasionally sites survive with exceptional preservation of a related collection of artifacts. An example of such a collection is the "Mary Rose". Survival in this instance is largely due to the remains being buried in sediment
Of the many examples where the sea bed provides an extremely hostile environment for submerged evidence of history, one of the most notable, the "RMS Titanic", though a relatively young wreck and in deep water so calcium-starved that concretion does not occur, appears strong and relatively intact, though indications are that it has already incurred irreversible degradation of her steel and iron hull. As such degradation inevitably continues, data will be forever lost, objects' context will be destroyed and the bulk of the wreck will over centuries completely deteriorate on the floor of the Atlantic Ocean. Comparative evidence shows that all iron and steel ships, especially those in a highly oxygenated environment, continue to degrade and will continue to do so until only their engines and other machinery project much above the sea-floor. Where it remains even after the passage of time, the iron or steel hull is often fragile with no remaining metal within the layer of concretion and corrosion products. The "USS Monitor", having been found in the 1970s, was subjected to a program of attempted "in situ" preservation, for example, but deterioration of the vessel progressed at such a rate that the rescue of her turret was undertaken lest nothing be saved from the wreck.
Some wrecks, lost to natural obstacles to navigation, are at risk of being smashed by subsequent wrecks sunk by the same hazard, or are deliberately destroyed because they present a hazard to navigation. Even in deep water, commercial activities such as pipe-laying operations and deep sea trawling can place a wreck at risk. Such a wreck is the "Mardi Gras" shipwreck sunk in the Gulf of Mexico in 4,000 feet (1220 meters) of water. The shipwreck lay forgotten at the bottom of the sea until it was discovered in 2002 by an oilfield inspection crew working for the Okeanos Gas Gathering Company (OGGC). Large pipelines can crush sites and render some of their remnants inaccessible as pipe is dropped from the ocean surface to the substrate thousands of feet below. Trawl nets snag and tear superstructures and separate artifacts from their context.
The wrecks, and other archaeological sites that have been preserved have generally survived because the dynamic nature of the sea bed can result in artifacts becoming rapidly buried in sediments. These sediments then provide an anaerobic environment which protects from further degradation. Wet environments, whether on land in the form of peat bogs and wells, or underwater are particularly important for the survival of organic material, such as wood, leather, fabric and horn. Cold and absence of light also aid survival of artifacts, because there is little energy available for either organic activity or chemical reactions. Salt water provides for greater organic activity than freshwater, and in particular, the shipworm, terredo navalis, lives only in salt water, so some of the best preservation in the absence of sediments has been found in the cold, dark waters of the Great Lakes in North America and in the (low salinity) Baltic Sea (where the "Vasa" was preserved).
While the land surface is continuously reused by man, the sea bed was largely inaccessible until the advent of submarines, scuba equipment and remotely operated underwater vehicles (ROV's) in the twentieth century. Salvagers have operated in much earlier times, but much of the material was beyond the reach of anyone. Thus the "Mary Rose" was subject to salvage from the sixteenth century and later, but a very large amount of material, buried in the sediments, remained to be found by maritime archaeologists of the twentieth century.
While preservation in situ is not assured, material that has survived underwater and is then recovered to land is typically in an unstable state and can only be preserved using highly specialised conservation processes. While the wooden structure of the "Mary Rose" and the individual artifacts have been undergoing conservation since their recovery, the Holland 1 provides an example of a relatively recent (metal) wreck for which extensive conservation has been necessary to preserve the hull. While the hull remains intact, its machinery remains inoperable. The SS Xantho engine that was recovered in 1985 from a saline environment after over a century underwater is presently considered somewhat anomalous, in that after two decades of treatment it can now be turned over by hand.
A challenge for the modern archaeologist is to consider whether "in-situ" preservation, or recovery and conservation on land is the preferable option; or to face the fact that preservation in any form, other than as an archaeological record is not feasible. A site that has been discovered has typically been subjected to disturbance of the very factors that caused its survival in the first place, for example, when a covering of sediment has been removed by storms or the action of man. Active monitoring and deliberate protection may mitigate further rapid destruction making "in situ" preservation an option, but long-term survival can never be guaranteed. For very many sites, the costs are too great for either active measures to ensure "in situ" preservation or to provide for satisfactory conservation on recovery. Even the cost of proper and complete archaeological investigation may be too great to enable this to occur within a timescale that ensures that an archaeological record is made before data is inevitably lost.
Submerged sites.
Pre-historic landscapes.
Maritime archaeology studies prehistorical objects and sites that are, because of changes in climate and geology, now underwater.
Bodies of water, fresh and saline, have been important sources of food for people for as long as we have existed. It should be no surprise that ancient villages were located at the water's edge. Since the last ice age sea level has risen as much as 400 feet (~120 meters).
Therefore, a great deal of the record of human activity throughout the Ice Age is now to be found under water.
The flooding of the area now known as the Black Sea (when a land bridge, where the Bosporus is now, collapsed under the pressure of rising water in the Mediterranean Sea) submerged a great deal of human activity that had been gathered round what had been an enormous, fresh-water lake.
Significant cave art sites off the coast of western Europe such as the Grotto Cosquer can be reached only by diving, because the cave entrances are underwater, though the upper portions of the caves themselves are not flooded.
Historic sites.
Throughout history, seismic events have at times caused submergence of human settlements. The remains of such catastrophes exist all over the world, and sites such as Alexandria and Port Royal now form important archaeological sites. As with shipwrecks, archaeological research can follow multiple themes, including evidence of the final catastrophe, the structures and landscape before the catastrophe and the culture and economy of which it formed a part. Unlike the wrecking of a ship, the destruction of a town by a seismic event can take place over many years and there may be evidence for several phases of damage, sometimes with rebuilding in between.
Coastal and foreshore.
Not all maritime sites are underwater. There are many structures at the margin of land and water that provide evidence of the human societies of the past. Some are deliberately created for access - such as bridges and walkways. Other structures remain from exploitation of resources, such as dams and fish traps. Nautical remains include early harbours and places where ships were built or repaired. At the end of their life, ships were often beached. Valuable or easily accessed timber has often been salvaged leaving just a few frames and bottom planking.
Archaeological sites can also be found on the foreshore today that would have been on dry land when they were constructed. An example of such a site is Seahenge, a Bronze Age timber circle.
Ships and Shipwrecks.
The archaeology of shipwrecks can be divided into a three-tier hierarchy, of which the first tier considers the wrecking process itself: how does a ship break up, how does a ship sink to the bottom, and how do the remains of the ship, cargo and the surrounding environment evolve over time? The second tier studies the ship as a machine, both in itself and in a military or economic system. The third tier consists of the archaeology of maritime cultures, in which nautical technology, naval warfare, trade and shipboard societies are studied. Some consider this to be the most important tier. Ships and boats are not necessarily wrecked: some are deliberately abandoned, scuttled or beached. Many such abandoned vessels have been extensively salvaged.
Bronze Age.
The earliest boats discovered date from the Bronze Age and are constructed of hollowed out logs or sewn planks. Vessels have been discovered where they have been preserved in sediments underwater or in waterlogged land sites, such as the discovery of a canoe near St Botolphs. Examples of sewn-plank boats include those found at North Ferriby and the Dover Bronze Age Boat which is now displayed at Dover Museum[http://www.dover.gov.uk/museum/boat/]. These may be an evolution from boats made of sewn hides, but it is highly unlikely that hide boats could have survived.
Ships wrecked in the sea have probably not survived, although remains of cargo (particularly bronze material) have been discovered, such as those at the Salcombe B site. A close collection of artefacts on the sea bed may imply that artefacts were from a ship, even if there are no remains of the actual vessel.
Late Bronze Age ships, such as the Uluburun Shipwreck have been discovered in the Mediterranean, constructed of edge joined planks. This shipbuilding technology continued through the classical period.
Maritime archaeology by region.
Mediterranean area.
In the Mediterranean area, maritime archaeologists have investigated several ancient cultures. Notable early Iron Age shipwrecks include two Phoenician ships of c. 750 BC that foundered off Gaza with cargoes of wine in amphoras. The crew of the U.S. Navy deep submergence research submarine NR-1 discovered the sites in 1997. In 1999 a team led by Robert Ballard and Harvard University archaeology Professor Lawrence Stager investigated the wrecks.
Extensive research has been carried out on the Mediterranean and Aegean coastlines of Turkey. Complete excavations have been performed on several wrecks from the Classical, Hellenistic, Byzantine, and Ottoman periods.
Maritime archaeological studies in Italy illuminate the naval and maritime activities of the Etruscans, Greek colonists, and Romans. After the 2nd century BC, the Roman fleet ruled the Mediterranean and actively suppressed piracy. During this Pax Romana, seaborne trade increased significantly throughout the region. Though sailing was the safest, fastest, and most efficient method of transportation in the ancient world, some fractional percentage of voyages ended in shipwreck. With the significantly increased sea traffic during the Roman era came a corresponding increase in shipwrecks. These wrecks and their cargo remains offer glimpses through time of the economy, culture, and politics of the ancient world. Particularly useful to archaeologists are studies of amphoras, the ceramic shipping containers used in the Mediterranean region from the 15th century BC through the Medieval period.
In addition to many discoveries in the sea, some wrecks have been examined in lakes. Most notable are Caligula's pleasure barges in Lake Nemi, Italy. The Nemi ships and other shipwreck sites occasionally yield objects of unique artistic value. For instance, the Antikythera wreck contained a staggering collection of marble and bronze statues including the Antikythera Youth. Discovered in 1900 by Greek sponge divers, the ship probably sank in the 1st century BC and may have been dispatched by the Roman general, Sulla, to carry booty back to Rome. The sponge divers also recovered from the wreck the famous Antikythera mechanism, believed to be an astronomical calculator. Further examples of fabulous works of art recovered from the sea floor are the two "bronzi" found in Riace (Calabria), Italy. In the cases of Antikythera and Riace, however, the artifacts were recovered without the direct participation of maritime archaeologists.
Recent studies in the Sarno river (near Pompeii) show other interesting elements of ancient life. The Sarno projects suggests that on the Tyrrhenian shore there were little towns with palafittes, similar to ancient Venice. In the same area, the submerged town of Puteoli (Pozzuoli, close to Naples) contains the "portus Julius" created by Marcus Vipsanius Agrippa in 37 BC, later sunk due to bradyseism.
The sea floor elsewhere in the Mediterranean holds countless archaeological sites. In Israel, Herod the Great's port at Caesarea Maritima has been extensively studied. Other finds are consistent with some passages of the Bible (like the so-called Jesus boat, which appears to have been in use during the first century AD).
Underwater and Maritime Archaeology in Australia.
Maritime Archaeology emerged in Australia commenced in the 1970s with the advent of Jeremy Green due to concerns expressed by academics and politicians with the rampant destruction of the Dutch and British East India ships lost on the west coast. As Commonwealth legislation was enacted and enforced after 1976 and as States enacted their own legislation the sub-discipline spread throughout Australia concentrating initially on shipwrecks due to on-going funding by both the States and the Commonwealth under their shipwreck legislation. Studies now include as an element of Underwater archaeology, as a whole, the study of submerged indigenous sites. Nautical Archaeology, (the specialised study of boat and ship construction) is also practised in the region. Often the sites or relics studied in Australia as in the rest of the world are not inundated. The study of historic submerged aircraft, better known as a sub-discipline of aviation archaeology, underwater aviation archaeology is also practised in the region. In some states maritime and underwater archaeology is practised out of Museums and in others out of cultural heritage management units and all practitioners operate under the aegis of the Australasian Institute for Maritime Archaeology (AIMA).

</doc>
<doc id="20069" url="https://en.wikipedia.org/wiki?curid=20069" title="Morihei Ueshiba">
Morihei Ueshiba

The son of a landowner from Tanabe, Ueshiba studied a number of martial arts in his youth, and served in the Japanese Army during the Russo-Japanese War. After being discharged in 1907, he moved to Hokkaidō as the head of a pioneer settlement; here he met and studied with Takeda Sokaku, the founder of Daitō-ryū aiki-jūjutsu. On leaving Hokkaido in 1919, Ueshiba joined the Ōmoto-kyō movement, a Shinto sect, in Ayabe, where he served as a martial arts instructor and opened his first dojo. He accompanied the head of the Ōmoto-kyō group, Onisaburo Deguchi, on an expedition to Mongolia in 1924, where they were captured by Chinese troops and returned to Japan. The following year, he experienced a great spiritual enlightenment, stating that, "a golden spirit sprang up from the ground, veiled my body, and changed my body into a golden one." After this experience, his martial arts skill appeared to be greatly increased.
Ueshiba moved to Tokyo in 1926, where he set up the Aikikai Hombu Dojo. In the aftermath of World War II the dojo was closed, but Ueshiba continued training at another dojo he had set up in Iwama. From the end of the war until the 1960s, he worked to promote aikido throughout Japan and abroad. He died from liver cancer in 1969.
Early years.
Morihei Ueshiba was born in Tanabe, Wakayama Prefecture, Japan on December 14, 1883, the fourth child (and only son) born to Yoroku Ueshiba and his wife Yuki.
The young Ueshiba was raised in a somewhat privileged setting. His father was a rich landowner who also traded in lumber and fishing and was politically active. Ueshiba was a rather weak, sickly child and bookish in his inclinations. At a young age his father encouraged him to take up sumo wrestling and swimming and entertained him with stories of his great-grandfather Kichiemon, who was considered a very strong samurai in his era. The need for such strength was further emphasized when the young Ueshiba witnessed his father being attacked by followers of a competing politician.
At the age of six Ueshiba was sent to study at the Jizōderu Temple, but had little interest in the rote learning of Confucian education. However, his schoolmaster was also a priest of Shingon Buddhism, and taught the young Ueshiba some of the esoteric chants and ritual observances of the sect, which Ueshiba found intriguing. He went to Tanage Higher Elementary School and then to Tanabe Prefectural Middle School, but left formal education in his early teens, enrolling instead at the a private abacus academy, the Yoshida Institute, to study accountancy. On graduating from the academy, he worked at a local tax office for a few months, but the job did not suit him and in 1901 he left for Tokyo, funded by his father. Ueshiba Trading, the stationery business which he opened there was short-lived; unhappy with life in the capital, he returned to Tanabe less than a year later after suffering a bout of beri-beri. Shortly thereafter he married his childhood acquaintance Hatsu Itokawa.
In 1903, Ueshiba was called up for military service. He failed the initial physical examination, being shorter than the regulation . To overcome this, he stretched his spine by attaching heavy weights to his legs and suspending himself from tree branches; when he re-took the physical exam he had increased his height by the necessary half-inch to pass. He was assigned to the Osaka Fourth Division, 37th Regiment, and was a corporal by the following year; after serving on the front lines during the Russo-Japanese War he was promoted to sergeant. He was discharged in 1907, and again returned to his father's farm in Tanabe. Here he befriended the writer and philosopher Minakata Kumagusu, becoming involved with Minakata's opposition to the Meiji government's Shrine Consolidation Policy. He and his wife had their first child, a daughter named Matsuko, in 1911.
Ueshiba studied several martial arts during his early life, and was renowned for his physical strength during his youth. His training in Gotō-ha Yagyū-ryu under Masakatsu Nakai was sporadic due to his military service, although he was granted a diploma in the art within a few years. In 1901 he received some instruction from Tozawa Tokusaburōin in Tenjin Shin'yō-ryū jujutsu and he studied judo with Kiyoichi Takagi in Tanabe in 1911.
Hokkaidō.
In 1912, Ueshiba and his wife left Tanabe and moved to Japan's northernmost island, Hokkaidō. At the time, Hokkaidō was still largely unsettled by the Japanese, being occupied primarily by the indigenous Ainu. Ueshiba was the leader of the Kishū Settlement Group, a collective of eighty-five pioneers who intended to settle in the Shirataki district and live as farmers. Poor soil conditions and bad weather led to crop failures during the first three years of the project, but the group still managed to cultivate mint and farm livestock. The burgeoning timber industry provided a boost to the settlement's economy, but a fire in 1917 razed the entire village, leading to the departure of around twenty families. Ueshiba, elected to the village council that year, led the reconstruction efforts. In the summer of 1918, Hatsu gave birth to their first son, Takemori.
In Hokkaidō, the young Ueshiba met Takeda Sokaku, the founder of Daitō-ryū aiki-jūjutsu at the Hisada Inn in Engaru, in March 1915. Ueshiba was deeply impressed with Takeda's martial art. He requested formal instruction and began studying Takeda's style of jūjutsu in earnest, going so far as to construct a dojo at his home and inviting his new teacher to be a permanent house guest. He received a "kyoju dairi" certificate, or teaching license, for the system from Takeda in 1922, when Takeda visited him in Ayabe. He also received a Yagyū Shinkage-ryū sword transmission scroll from Takeda. Ueshiba then became a representative of Daitō-ryū, toured with Takeda as a teaching assistant and taught the system to others.
Onisaburo Deguchi and Ōmoto-kyō.
In November 1919, Ueshiba learned that his father Yoroku was ill, and was not expected to survive. Leaving most of his possessions to Sokaku, Ueshiba left Shirataki with the apparent intention returning to Tanabe to visit his ailing parent. "En route", however, he made a detour to Ayabe, near Kyoto, intending to visit Onisaburo Deguchi, the spiritual leader of the Ōmoto-kyō religion in Ayabe. Having met Deguchi, Ueshiba stayed at the Ōmoto-kyō headquarters for several days. On his return to Tanabe, he found that his father had died. Within a few months, he was back in Ayabe, having decided to become a full-time student of Ōmoto-kyō. In 1920 Deguchi asked Ueshiba to become the group's martial arts instructor, and a dojo—the first of several that Ueshiba was to lead—was constructed on the centre's grounds. Ueshiba also taught Takeda's Daitō-ryū in neighbouring Hyōgo Prefecture during this period. His second son, Kuniharu, was born in 1920 in Ayabe, but died from illness the same year, along with three-year-old Takemori.
In 1921, in an event known as the First Ōmoto-kyō Incident ("Ōmoto jiken"), the Japanese authorities raided the compound, destroying the main buildings on the site and arresting Deguchi on charges of lèse-majesté. Ueshiba's dojo was undamaged, however, and over the following two years he worked closely with Deguchi to reconstruct the group's centre, becoming heavily involved in farming work. His son Kisshomaru Ueshiba was born in the summer of 1921.
Three years later, in 1924, Onisaburo Deguchi led a small group of Ōmoto-kyō disciples, including Ueshiba, on a journey to Mongolia at the invitation of retired naval captain Yutaro Yano and his associates within the ultra-nationalist Black Dragon Society. Deguchi's intent was to establish a new religious kingdom in Mongolia, and to this end he had distributed propaganda suggesting that he was the reincarnation of Genghis Khan. Allied with the Mongolian bandit Lu Zhankui, Deguchi's group were arrested in Tongliao by the Chinese authorities—fortunately for Ueshiba, whilst Lu and his men were executed by firing squad, the Japanese group were released into the custody of the Japanese consul. They were returned under guard to Japan, where Deguchi was imprisoned for breaking the terms of his bail.
After returning to Ayabe, Ueshiba began a regimen of spiritual training, regularly retreating by himself to the mountains or performing "misogi" in the Nachi Falls. As his prowess as a martial artist increased, his fame began to spread. He was challenged by many established martial artists, some of whom subsequently became his students after being defeated by him. In the autumn of 1925 he was asked to give a demonstration of his art in Tokyo, at the behest of Admiral Isamu Takeshita; one of the spectators was Yamamoto Gonnohyōe, who requested that Ueshiba stay in the capital to instruct the Imperial Guard in his martial art. After a couple of weeks, however, Ueshiba took issue with several government officials who voiced concerns about his connections to Deguchi; he cancelled the training and returned to Ayabe.
Ōmoto-kyō priests still oversee the Aiki-jinja Taisai ceremony in Ueshiba's honor every April 29 at the Aiki Shrine in Iwama.
Tokyo.
In 1926 Takeshita invited Ueshiba to visit Tokyo again. Ueshiba relented and returned to the capital, but while residing there was stricken with a serious illness. Deguchi visited his ailing student and, concerned for his health, commanded Ueshiba to return to Ayabe. The appeal of returning increased after Ueshiba was questioned by the police following his meeting with Deguchi; the authorities were keeping the Ōmoto-kyō leader under close surveillance. Angered at the treatment he had received, Ueshiba went back to Ayabe again. Six months later, however, and this time with Deguchi's blessing, he and his family moved permanently to Tokyo. Arriving in October 1927, they set up home in the Shirokane district. The building, however, was too small to house the growing number of aikido students, and so the Ueshibas moved to larger premises, first in Mita district, then in Takanawa, and finally to a purpose-built hall in Shinjuku. This last location, originally named the Kobukan , would eventually become the Aikikai Hombu Dojo. During its construction, Ueshiba rented a property nearby, where he was visited by Jigoro Kano, the founder of judo.
In 1932, Ueshiba's daughter Matsuko was married to the swordsman Kiyoshi Nakakura, who was adopted as Ueshiba's heir under the name Morihiro Ueshiba. The marriage ended after a few years, and Nakakura left the family in 1937.
Between 1940 and 1942 he made several visits to Manchukuo (Japanese occupied Manchuria) where he was the principal martial arts instructor at Kenkoku University.
Iwama.
From 1935 onwards, Ueshiba had been purchasing land in Iwama in Ibaraki Prefecture. In 1942, having acquired around of farmland there, he left Tokyo and moved to Iwama permanently, settling in a small farmer's cottage. Here he founded the Aiki Shuren Dojo, also known as the Iwama dojo. During all this time he traveled extensively in Japan, particularly in the Kansai region teaching his aikido. Despite the prohibition on the teaching of martial arts after World War II, Ueshiba and his students continued to practice in secret at the Iwama dojo; the Hombu dojo in Tokyo was in any case being used as a refugee centre for citizens displaced by the severe firebombing.
The prohibition (on aikido, at least) was lifted in 1948 with the creation of the Aiki Foundation, established by the Japanese Ministry of Education with permission from the Occupation forces. The Hombu dojo re-opened the following year. After the war, however, Ueshiba delegated most of the work of running the Hombu dojo and the Aiki Federation to his son Kisshomaru, choosing to spend much of his time in prayer, meditation, calligraphy and farming. He still travelled extensively to promote aikido, however, even visiting Hawaii in 1961. He also appeared in a television documentary on aikido: NTV's "The Master of Aikido", broadcast in January 1960.
In his later years, he was regarded as very kind and gentle as a rule, but there are also stories of terrifying scoldings delivered to his students. For instance, he once thoroughly chastised students for practicing "jō" (staff) strikes on trees without first covering them in protective padding.
Death.
In 1969, Ueshiba became ill. He led his last training session on March 10, and was subsequently taken to hospital where he was diagnosed with cancer of the liver. He died suddenly on April 26, 1969. Two months later, his wife Hatsu also died.(植芝 はつ; "Ueshiba Hatsu", née "Itokawa Hatsu"; 1881–1969)
Development of aikido.
Aikido—usually translated as the "Way of Unifying Spirit" or the "Way of Spiritual Harmony"—is a fighting system that focuses on throws, pins and joint locks together with some striking techniques. It is unusual among the martial arts for its heavy emphasis on protecting the opponent and on spiritual and social development.
Ueshiba developed aikido after experiencing three instances of spiritual awakening. The first happened in 1925, after Ueshiba had defeated a naval officer's "bokken" (wooden katana) attacks unarmed and without hurting the officer. Ueshiba then walked to his garden and had a spiritual awakening.
His second experience occurred in 1940 when engaged in the ritual purification process of misogi.
His third experience was in 1942 during the worst fighting of World War II, Ueshiba had a vision of the "Great Spirit of Peace".
The technical curriculum of aikido was undoubtedly most greatly influenced by the teachings of Takeda Sokaku. The basic techniques of aikido seem to have their basis in teachings from various points in the Daitō-ryū curriculum. In the earlier years of his teaching, from the 1920s to the mid-1930s, Ueshiba taught the Daitō-ryū "aiki-jūjutsu" system; his early students' documents bear the term "aiki-jūjutsu". Indeed, Ueshiba trained one of the future highest grade earners in Daitō-ryū, Takuma Hisa, in the art before Takeda took charge of Hisa's training.
The early form of training under Ueshiba was noticeably different from later forms of aikido. It had a larger curriculum, increased use of strikes to vital points ("atemi") and a greater use of weapons. The schools of aikido developed by Ueshiba's students from the pre-war period tend to reflect the harder style of the early training. These students included Kenji Tomiki (who founded the Shodokan Aikido sometimes called Tomiki-ryū), Noriaki Inoue (who founded Shin'ei Taidō), Minoru Mochizuki (who founded Yoseikan Budo), Gozo Shioda (who founded Yoshinkan Aikido). Many of these styles are therefore considered "pre-war styles", although some of these teachers continued to train with Ueshiba in the years after World War II.
Later, as Ueshiba seemed to slowly grow away from Takeda, he began to change his art. These changes are reflected in the differing names with which he referred to his system, first as "aiki-jūjutsu", then Ueshiba-ryū, Asahi-ryū, and "aiki budō". In 1942, the martial art that Ueshiba developed finally came to be known as aikido.
As Ueshiba grew older, more skilled, and more spiritual in his outlook, his art also changed and became softer and more circular. Striking techniques became less important and the formal curriculum became simpler. In his own expression of the art there was a greater emphasis on what is referred to as "kokyū-nage", or "breath throws" which are soft and blending, utilizing the opponent's movement in order to throw them. Ueshiba regularly practiced cold water "misogi", as well as other spiritual and religious rites, and viewed his studies of aikido as part of this spiritual training.
Students.
Over the years, Ueshiba trained a large number of students, many of whom have grown into great teachers in their own right.
Some of them were "uchideshi", or live-in students.
There are roughly four generations of students. A partial list follows:

</doc>
<doc id="20070" url="https://en.wikipedia.org/wiki?curid=20070" title="Memory address register">
Memory address register

In a computer, the Memory Address Register (MAR) is a CPU register that either stores the memory address from which data will be fetched to the CPU or the address to which data will be sent and stored.
In other words, MAR holds the memory location of data that needs to be accessed. When reading from memory, data addressed by MAR is fed into the MDR (memory data register) and then used by the CPU.
When writing to memory, the CPU writes data from MDR to the memory location whose address is stored in MAR.
The Memory Address Register is half of a minimal interface between a microprogram and computer storage. The other half is a memory data register.
Far more complex memory interfaces exist, but this is the least that can work perfectly. 
The "Memory Address Register" holds the address of the current instruction being executed. It points to the relevant location in memory where the required instruction is (at this stage the address is simply copied from the Program Counter).
In general MAR is a parallel load register that contains the next memory address to be manipulated. For example, the next address to be read or written.

</doc>
<doc id="20071" url="https://en.wikipedia.org/wiki?curid=20071" title="Memory data register">
Memory data register

The Memory Data Register (MDR) or Memory Buffer Register (MBR) is the register of a computer's control unit that contains the data to be stored in the computer storage (e.g. RAM), or the data after a fetch from the computer storage. It acts like a buffer and holds anything that is copied from the memory ready for the processor to use it.
The MDR is a "two-way register." When data is fetched from memory and placed into the MDR, it is written to go 
in one direction. When there is a write instruction, the data to be written is placed into the MDR from another CPU register, which then puts the data into memory.
The Memory Data Register is half of a minimal interface between a microprogram and computer storage, the other half is a memory address register (MAR).
Far more complex memory interfaces exist, but this is the simplest that can work.
The Memory Data Register (MDR) contains the data value being fetched or stored. It is a common mistake to say that the MDR should be W bits wide, where W is the cell size. However, on most computers the cell size is only 8-bits, and most data values occupy multiple cells. Thus, the size of the MDR is usually a multiple of 8 bits. Typical values of MDR width are 32 and 64 bits, which would allow us to fetch, in a single step, either an integer or a float value.
For example, to retrieve the contents of cell 123, we would load the value 123 (in binary) into the MAR and perform a fetch operation. When the operation is done, a copy of the contents of cell 123 would be in the MDR. To store the value 98 into cell 4, we load a 4 into the MAR and a 98 into the MDR and perform a store. When the operation is completed the contents of cell 4 will have been set to 98, by discarding whatever was there previously.
MDR has two inputs and two outputs. Data may be loaded into MDR either from the memory bus or from the internal processor bus.The data stored in MDR may be placed on either bus.

</doc>
