<doc id="19552" url="https://en.wikipedia.org/wiki?curid=19552" title="Media studies">
Media studies

Media studies throughout the world.
Australia.
Media is studied as a broad subject in most states in Australia, with the states of Victoria being world leaders in curriculum development. Media studies in Australia was first developed as an area of study in Victorian universities in the early 1960s, and in secondary schools in the mid 1960s.
Today, almost all Australian universities teach media studies. According to the Government of Australia's "Excellence in Research for Australia" report, the leading universities in the country for media studies (which were ranked well above World standards by the report's scoring methodology) are Monash University, QUT, RMIT, University of Melbourne, University of Queensland and UTS.
In secondary schools, an early film studies course first began being taught as part of the Victorian junior secondary curriculum during the mid 1960s. And, by the early 1970s, an expanded media studies course was being taught. The course became part of the senior secondary curriculum (later known as the Victorian Certificate of Education or "VCE") in the 1980s. It has since become, and continues to be, a strong component of the VCE. Notable figures in the development of the Victorian secondary school curriculum were the long time Rusden College media teacher Peter Greenaway (not the British film director), Trevor Barr (who authored one of the first media text books "Reflections of Reality") and later John Murray (who authored "The Box in the Corner", "In Focus", and "10 Lessons in Film Appreciation").
Today, Australian states and territories that teach media studies at a secondary level are Australian Capital Territory, Northern Territory, Queensland, South Australia, Victoria and Western Australia. Media studies does not appear to be taught in the state of New South Wales at a secondary level.
In Victoria, the VCE media studies course is structured as: Unit 1 - Representation, Technologies of Representation, and New Media; Unit 2 - Media Production, Australian Media Organisations; Unit 3 - Narrative Texts, Production Planning; and Unit 4 - Media Process, Social Values, and Media Influence. Media studies also form a major part of the primary and junior secondary curriculum, and includes areas such as photography, print media and television.
Victoria also hosts the peak media teaching body known as ATOM which publishes "Metro" and "Screen Education" magazines.
China.
There are two universities in China that specialize in media studies. Communication University of China, formerly known as the Beijing Broadcasting Institute, that dates back to 1954. CUC has 15,307 full-time students, including 9264 undergraduates, 3512 candidates for doctor and master's degrees and 16780 students in programs of continuing education. The other university known for media studies in China is Zhejiang University of Media and Communications (ZUMC) which has campuses in Hangzhou and Tongxiang. Almost 10,000 full-time students are currently studying in over 50 programs at the 13 Colleges and Schools of ZUMC. Both institutions have produced some of China's brightest broadcasting talents for television as well as leading journalists at magazines and newspapers.
France.
One prominent French media critic is the sociologist Pierre Bourdieu who wrote among other books "On Television" (New Press, 1999). Bourdieu's analysis is that television provides far less autonomy, or freedom, than we think. In his view, the market (which implies the hunt for higher advertising revenue) not only imposes uniformity and banality, but also a form of invisible censorship. When, for example, television producers "pre-interview" participants in news and public affairs programs, to insure that they will speak in simple, attention-grabbing terms, and when the search for viewers leads to an emphasis on the sensational and the spectacular, people with complex or nuanced views are not allowed a hearing.
Germany.
In Germany two main branches of media theory or media studies can be identified.
The first major branch of media theory has its roots in the humanities and cultural studies, such as theater studies ("Theaterwissenschaft") and German language and literature studies. This branch has broadened out substantially since the 1990s. And it is on this initial basis that media studies in Germany has primarily developed and established itself.
One of the early publications in this new direction is a volume edited by Helmut Kreuzer, "Literature Studies - Media Studies" ("Literaturwissenschaft – Medienwissenschaft"), which summarizes the presentations given at the Düsseldorfer Germanistentag 1976.
The second branch of media studies in Germany is comparable to Communication Studies. Pioneered by Elisabeth Noelle-Neumann in the 1940s, this branch studies mass media, its institutions and its effects on society and individuals. The German Institute for Media and Communication Policy, founded in 2005 by media scholar Lutz Hachmeister, is one of the few independent research institutions that is dedicated to issues surrounding media and communications policies.
The term "Wissenschaft" cannot be translated straightforwardly as "studies", as it calls to mind both scientific methods and the humanities. Accordingly, German media theory combines philosophy, psychoanalysis, history, and scienctific studies with media-specific research.
"Medienwissenschaften" is currently one of the most popular courses of study at universities in Germany, with many applicants mistakenly assuming that studying it will automatically lead to a career in TV or other media. This has led to widespread disillusionment, with students blaming the universities for offering highly theoretical course content. The universities maintain that practical journalistic training is not the aim of the academic studies they offer.
India.
The media industry is growing in India at the rate of 20 percent per annum. Together, entertainment and media form the country's sixth biggest industry, with 3.5 million people working in it. Within the next 4–5 years, the industry is expected to gross eighty thousand crores (800 billion rupees) annually.
With a view to making the best use of communication facilities for information, publicity and development, the Government of India in 1962-63 sought the advice of the Ford Foundation/UNESCO team of internationally known mass communication specialists who recommended the setting up of a national institute for training, teaching and research in mass communication.
Anna University is the first university to take up the idea of India's University Grants Commission (UGC) to start Master of Science in Electronic Media programmes. It offers a five-year integrated programme and a two-year programme in Electronic Media. The Department of Media Sciences was started in January 2002, branching off from the UGC's Educational Multimedia Research Centre (EMMRC). National Institute of Open Schooling, the world's largest open schooling system, offers Mass Communication as a subject of studies at senior secondary level.
Netherlands.
In the Netherlands, media studies are split into several academic courses such as (applied) communication sciences, communication- and information sciences, communication and media, media and culture or theater, film and television sciences. Whereas communication sciences focuses on the way people communicate, be it mediated or unmediated, media studies tends to narrow the communication down to just mediated communication. However, it would be a mistake to consider media studies a specialism of communication sciences, since media make up just a small portion of the overall course. Indeed, both studies tend to borrow elements from one another.
Communication sciences (or a derivative thereof) can be studied at Erasmus University Rotterdam, Radboud University, Tilburg University, University of Amsterdam, University of Groningen, University of Twente, Roosevelt Academy, University of Utrecht, VU University Amsterdam and Wageningen University and Research Centre.
Media studies (or something similar) can be studied at the University of Amsterdam, VU University Amsterdam, Erasmus University Rotterdam, University of Groningen and the University of Utrecht.
New Zealand.
Media Studies in New Zealand is very healthy, especially due to the NZ film industry and is taught at both secondary and tertiary education institutes. Media Studies in NZ can be regarded as a singular success, with the subject well-established in the tertiary sector (such as Screen and Media Studies at the University of Waikato; Media Studies, Victoria University of Wellington; Film, Television and Media Studies, University of Auckland; Media Studies, Massey University; Communication Studies, University of Otago). Different Media Studies courses can offer students a range of specialisations- such as cultural studies, media theory and analysis, practical film-making, journalism and communications studies.
Pakistan.
Media Studies Program is offered in Pakistan by Karachi University Karachi university, Pakistan this was formerly known as Mass communication. Riphah International University Islamabad is also offering Undergraduate and Postgraduate degree programs in Media Studies, Mass Communications and Media Production. Riphah International University also have its own broadcast facilities such as FM 102.2. Kinnaird College, Lahore and Szabist University also offers undergraduate program Szabist University in various cities across Pakistan, Dubai and UAE.
Switzerland.
In Switzerland, media and communication studies are offered by several higher education institutions including the International University in Geneva, Zurich University of Applied Sciences, University of Lugano, University of Fribourg and others.
UK.
In the UK, media studies developed in the 1960s from the academic study of English, and from literary criticism more broadly. The key date, according to Andrew Crisell, is 1959:
When Joseph Trenaman left the BBC's Further Education Unit to become the first holder of the Granada Research Fellowship in Television at Leeds University. Soon after in 1966, the Centre for Mass Communication Research was founded at Leicester University, and degree programmes in media studies began to sprout at polytechnics and other universities during the 1970s and 1980s.
James Halloran at Leicester University is credited with much influence in the development of media studies and communication studies, as the head of the university's Centre for Mass Communication Research, and founder of the International Association for Media and Communication Research. Media Studies is now taught all over the UK. It is taught at Key Stages 1– 3, Entry Level, GCSE and at A level and the Scottish Qualifications Authority offers formal qualifications at a number of different levels. It is offered through a large area of exam boards including AQA and WJEC.
Much research in the field of news media studies has been led by the Reuters Institute for the Study of Journalism. Details of the research projects and results are published in the RISJ annual report.
United States.
Mass communication, Communication studies or simply 'Communication' may be more popular names than “media studies” for academic departments in the United States. However, the focus of such programs sometimes excludes certain media—film, book publishing, video games, etc. The title “media studies” may be used alone, to designate film studies and rhetorical or critical theory, or it may appear in combinations like “media studies and communication” to join two fields or emphasize a different focus.
In 1999, the MIT Comparative Media Studies program started under the leadership of Henry Jenkins, since growing into a graduate program, MIT's largest humanities major, and, following a 2012 merger with the Writing and Humanistic Studies program, a roster of twenty faculty, including Pulitzer Prize-winning author Junot Diaz, science fiction writer Joe Haldeman, games scholar T. L. Taylor, and media scholars William Uricchio (a CMS co-founder), Edward Schiappa, and Heather Hendershot. Now named Comparative Media Studies/Writing, the department places an emphasis on what Jenkins and colleagues had termed "applied humanities": it hosts several research groups for civic media, digital humanities, games, computational media, documentary, and mobile design, and these groups are used to provide graduate students with research assistantships to cover the cost of tuition and living expenses. The incorporation of Writing and Humanistic Studies also placed MIT's Science Writing program, Writing Across the Curriculum, and Writing and Communications Center under the same roof.
Formerly an interdisciplinary major at the University of Virginia the Department of Media Studies was officially established in 2001 and has quickly grown to wide recognition. This is partly thanks to the acquisition of Professor Siva Vaidhyanathan, a cultural historian and media scholar, as well as the Inaugural Verklin Media Policy and Ethics Conference, endowed by the CEO of Canoe Ventures and UVA alumnus David Verklin. In 2010, a group of undergraduate students in the Media Studies Department established the Movable Type Academic Journal, the first ever undergraduate academic journal of its kind. The department is expanding rapidly and doubled in size in 2011.
Brooklyn College, part of the City University of New York, has been offering graduate studies in television and media since 1961. Currently, the Department of Television and Radio administers an MS in Media Studies, and hosts the Center for the Study of World Television.
The University of Southern California has three distinct centers for media studies: the Center for Visual Anthropology (founded in 1984), the Institute for Media Literacy at the School of Cinematic Arts (founded in 1998) and the Annenberg School for Communication and Journalism (founded in 1971).
University of California, Irvine had in Mark Poster one of the first and foremost theorists of media culture in the US, and can boast a strong Department of Film & Media Studies. University of California, Berkeley has three institutional structures within which media studies can take place: the department of Film and Media (formerly Film Studies Program), including famous theorists as Mary Ann Doane and Linda Williams, the Center for New Media, and a long established interdisciplinary program formerly titled Mass Communications, which recently changed its name to Media Studies, dropping any connotations which accompany the term “Mass” in the former title. Until recently, Radford University in Virginia used the title "media studies" for a department that taught practitioner-oriented major concentrations in journalism, advertising, broadcast production and Web design. In 2008, those programs were combined with a previous department of communication (speech and public relations) to create a School of Communication. (A media studies major at Radford still means someone concentrating on journalism, broadcasting, advertising or Web production.)
The University of Denver has a renowned program for digital media studies. It is an interdisciplinary program combining Communications, Computer Science, and the arts.
In 2002 Bernard Luskin of Fielding Graduate University established an EdD program in Media Studies and a PhD program in Media Psychology with a concentration in Media Studies. Courses in Media Studies were started by Luskin at Touro University Worldwide in 2009.
Canada.
In Canada, media studies and communication studies are incorporated in the same departments and cover a wide range of approaches (from critical theory to organizations to research-creation and political economy, for example). Over time, research developed to employ theories and methods from cultural studies, philosophy, political economy, gender, sexuality and race theory, management, rhetoric, film theory, sociology, and anthropology. Harold Innis and Marshall McLuhan are famous Canadian scholars for their contributions to the fields of media ecology and political economy in the 20th century. They were both important members of the Toronto School of Communication at the time. More recently, the School of Montreal and its founder James R. Taylor significantly contributed to the field of organizational communication by focusing on the ontological processes of organizations.
Carleton University and the University of Western Ontario, 1945 and 1946 prospectively, created Journalism specific programs or schools. A Journalism specific program was also created at Ryerson in 1950. The first communication programs in Canada were started at Ryerson and Concordia Universities. The Radio and Television Arts program at Ryerson were started in the 1950s, while the Film, Media Studies/Media Arts, and Photography programs also originated from programs started in the 1950s. The Communication studies department at Concordia was created in the late 1960s. Ryerson's Radio and Television, Film, Media and Photography programs were renowned by the mid 1970s, and it's programs were being copied by other colleges and universities nationally and Internationally.
Today, most universities offer undergraduate degrees in Media and Communication Studies, and many Canadian scholars actively contribute to the field, among which: Brian Massumi (philosophy, cultural studies), Kim Sawchuk (cultural studies, feminist, ageing studies), Carrie Rentschler (feminist theory), and François Cooren (organizational communication).
In his book “Understanding Media, The Extensions of Man”, media theorist Marshall McLuhan suggested that "the medium is the message", and that all human artefacts and technologies are media. His book introduced the usage of terms such as “media” into our language along with other precepts, among them “global village” and “Age of Information”. A medium is anything that mediates our interaction with the world or other humans. Given this perspective, media study is not restricted to just media of communications but all forms of technology. Media and their users form an ecosystem and the study of this ecosystem is known as media ecology.
McLuhan says that the “technique of fragmentation that is the essence of machine technology” shaped the restructuring of human work and association and “the essence of automation technology is the opposite”. He uses an example of the electric light to make this connection and to explain “the medium is the message”. The electric light is pure information and it is a medium without a message unless it is used to spell out some verbal ad or a name. The characteristic of all media means the “content” of any medium is always another medium. For example, the content of writing is speech, the written word is the content of print, and print is the content of the telegraph. The change that the medium or technology introduces into human affairs is the “message”. If the electric light is used for Friday night football or to light up your desk you could argue that the content of the electric light is these activities. The fact that it is the medium that shapes and controls the form of human association and action makes it’s the message. The electric light is over looked as a communication medium because it doesn’t have any content. It is not until the electric light is used to spell a brand name that it’s recognized as medium. Similar to radio and other mass media electric light eliminates time and space factors in human association creating deeper involvement. McLuhan compared the “content” to a juicy piece of meat being carried by a burglar to distract the “watchdog of the mind”. The effect of the medium is made strong because it’s given another media “content”. The content of a movie is a book, play or maybe even an opera.
McLuhan talks about media being “hot” or “cold” and touches on the principle that distinguishes them from one another. A hot medium (i.e., radio or Movie) extends a single sense in “high definition”. High definition means the state of being well filled with data. A cool medium (i.e., Telephone and TV) is considered “low definition” because a small amount of data/information is given and has to be filled in. Hot media are low in participation and cool media are high in participation. Hot media are low in participation because it is giving most of the information and it excludes. Cool media are high in participation because it gives you information but you have to fill in the blanks and it’s inclusive. He used lecturing as an example for hot media and seminars as an example for low media. If you use a hot medium in a hot or cool culture makes a difference.

</doc>
<doc id="19553" url="https://en.wikipedia.org/wiki?curid=19553" title="Microprocessor">
Microprocessor

A microprocessor is a computer processor which incorporates the functions of a computer's central processing unit (CPU) on a single integrated circuit (IC), or at most a few integrated circuits. The microprocessor is a multipurpose, clock driven, register based, programmable electronic device which accepts digital or binary data as input, processes it according to instructions stored in its memory, and provides results as output. Microprocessors contain both combinational logic and sequential digital logic. Microprocessors operate on numbers and symbols represented in the binary numeral system.
The integration of a whole CPU onto a single chip or on a few chips greatly reduced the cost of processing power. Integrated circuit processors are produced in large numbers by highly automated processes resulting in a low per unit cost. Single-chip processors increase reliability as there are many fewer electrical connections to fail. As microprocessor designs get faster, the cost of manufacturing a chip (with smaller components built on a semiconductor chip the same size) generally stays the same.
Before microprocessors, small computers had been built using racks of circuit boards with many medium- and small-scale integrated circuits. Microprocessors combined this into one or a few large-scale ICs. Continued increases in microprocessor capacity have since rendered other forms of computers almost completely obsolete (see history of computing hardware), with one or more microprocessors used in everything from the smallest embedded systems and handheld devices to the largest mainframes and supercomputers.
Structure.
The internal arrangement of a microprocessor varies depending on the age of the design and the intended purposes of the microprocessor. The complexity of an integrated circuit is bounded by physical limitations of the number of transistors that can be put onto one chip, the number of package terminations that can connect the processor to other parts of the system, the number of interconnections it is possible to make on the chip, and the heat that the chip can dissipate. Advancing technology makes more complex and powerful chips feasible to manufacture.
A minimal hypothetical microprocessor might only include an arithmetic logic unit (ALU) and a control logic section. The ALU performs operations such as addition, subtraction, and operations such as AND or OR. Each operation of the ALU sets one or more flags in a status register, which indicate the results of the last operation (zero value, negative number, overflow, or others). The control logic retrieves instruction codes from memory and initiates the sequence of operations required for the ALU to carry out the instruction. A single operation code might affect many individual data paths, registers, and other elements of the processor.
As integrated circuit technology advanced, it was feasible to manufacture more and more complex processors on a single chip. The size of data objects became larger; allowing more transistors on a chip allowed word sizes to increase from 4- and 8-bit words up to today's 64-bit words. Additional features were added to the processor architecture; more on-chip registers sped up programs, and complex instructions could be used to make more compact programs. Floating-point arithmetic, for example, was often not available on 8-bit microprocessors, but had to be carried out in software. Integration of the floating point unit first as a separate integrated circuit and then as part of the same microprocessor chip, sped up floating point calculations.
Occasionally, physical limitations of integrated circuits made such practices as a bit slice approach necessary. Instead of processing all of a long word on one integrated circuit, multiple circuits in parallel processed subsets of each data word. While this required extra logic to handle, for example, carry and overflow within each slice, the result was a system that could handle, for example, 32-bit words using integrated circuits with a capacity for only four bits each.
With the ability to put large numbers of transistors on one chip, it becomes feasible to integrate memory on the same die as the processor. This CPU cache has the advantage of faster access than off-chip memory, and increases the processing speed of the system for many applications. Processor clock frequency has increased more rapidly than external memory speed, except in the recent past, so cache memory is necessary if the processor is not delayed by slower external memory.
Special-purpose designs.
A microprocessor is a general purpose system. Several specialized processing devices have followed from the technology. Microcontrollers integrate a microprocessor with peripheral devices in embedded systems. A digital signal processor (DSP) is specialized for signal processing. Graphics processing units may have no limited or general programming facilities. For example, GPUs through the 1990s were mostly non-programmable and have only recently gained limited facilities like programmable vertex shaders.
32-bit processors have more digital logic than narrower processors, so 32-bit (and wider) processors produce more digital noise and have higher static consumption than narrower processors.
Reducing digital noise improves ADC conversion results.
So, 8-bit or 16-bit processors are better than 32-bit processors for system on a chip and microcontrollers that require extremely low-power electronics, or are part of a mixed-signal integrated circuit with noise-sensitive on-chip analog electronics such as high-resolution analog to digital converters, or both.
Nevertheless, trade-offs apply: running 32-bit arithmetic on an 8-bit chip could end up using more power, as the chip must execute software with multiple instructions. Modern microprocessors go into low power states when possible, and a 8-bit chip running 32-bit software is active most of the time. This creates a delicate balance between software, hardware and use patterns, plus costs. 
When manufactured on a similar process, 8-bit microprocessors use less power when operating and less power when sleeping than 32-bit microprocessors.
However, some people say a 32-bit microprocessor may use less average power than an 8-bit microprocessor when the application requires certain operations such as floating-point math
that take many more clock cycles on an 8-bit microprocessor than a 32-bit microprocessor so the 8-bit microprocessor spends more time in high-power operating mode.
Embedded applications.
Thousands of items that were traditionally not computer-related include microprocessors. These include large and small household appliances, cars (and their accessory equipment units), car keys, tools and test instruments, toys, light switches/dimmers and electrical circuit breakers, smoke alarms, battery packs, and hi-fi audio/visual components (from DVD players to phonograph turntables). Such products as cellular telephones, DVD video system and HDTV broadcast systems fundamentally require consumer devices with powerful, low-cost, microprocessors. Increasingly stringent pollution control standards effectively require automobile manufacturers to use microprocessor engine management systems, to allow optimal control of emissions over widely varying operating conditions of an automobile. Non-programmable controls would require complex, bulky, or costly implementation to achieve the results possible with a microprocessor.
A microprocessor control program (embedded software) can be easily tailored to different needs of a product line, allowing upgrades in performance with minimal redesign of the product. Different features can be implemented in different models of a product line at negligible production cost.
Microprocessor control of a system can provide control strategies that would be impractical to implement using electromechanical controls or purpose-built electronic controls. For example, an engine control system in an automobile can adjust ignition timing based on engine speed, load on the engine, ambient temperature, and any observed tendency for knocking—allowing an automobile to operate on a range of fuel grades.
History.
The advent of low-cost computers on integrated circuits has transformed modern society. General-purpose microprocessors in personal computers are used for computation, text editing, multimedia display, and communication over the Internet. Many more microprocessors are part of embedded systems, providing digital control over myriad objects from appliances to automobiles to cellular phones and industrial process control.
The first use of the term "microprocessor" is attributed to Viatron Computer Systems describing the custom integrated circuit used in their System 21 small computer system announced in 1968.
By the late-1960s, designers were striving to integrate the central processing unit (CPU) functions of a computer onto a handful of MOS LSI chips, called microprocessor unit (MPU) chip sets. Building on 8-bit arithmetic logic units (3800/3804) he designed earlier at Fairchild, in 1969 Lee Boysel created the Four-Phase Systems Inc. AL-1 an 8-bit CPU slice that was expandable to 32-bits. In 1970, Steve Geller and Ray Holt of Garrett AiResearch designed the MP944 chip set to implement the F-14A Central Air Data Computer on six metal-gate chips fabricated by AMI.
Intel introduced its first 4-bit microprocessor 4004 in 1971 and its 8-bit microprocessor 8008 in 1972. During the 1960s, computer processors were constructed out of small and medium-scale ICs—each containing from tens of transistors to a few hundred. These were placed and soldered onto printed circuit boards, and often multiple boards were interconnected in a chassis. The large number of discrete logic gates used more electrical power—and therefore produced more heat—than a more integrated design with fewer ICs. The distance that signals had to travel between ICs on the boards limited a computer's operating speed.
In the NASA Apollo space missions to the moon in the 1960s and 1970s, all onboard computations for primary guidance, navigation and control were provided by a small custom processor called "The Apollo Guidance Computer". It used wire wrap circuit boards whose only logic elements were three-input NOR gates.
The first microprocessors emerged in the early 1970s and were used for electronic calculators, using binary-coded decimal (BCD) arithmetic on 4-bit words. Other embedded uses of 4-bit and 8-bit microprocessors, such as terminals, printers, various kinds of automation etc., followed soon after. Affordable 8-bit microprocessors with 16-bit addressing also led to the first general-purpose microcomputers from the mid-1970s on.
Since the early 1970s, the increase in capacity of microprocessors has followed Moore's law; this originally suggested that the number of components that can be fitted onto a chip doubles every year. With present technology, it is actually every two years, and as such Moore later changed the period to two years.
First projects.
Three projects delivered a microprocessor at about the same time: Garrett AiResearch's Central Air Data Computer (CADC), Texas Instruments (TI) TMS 1000 (1971 September), and Intel's 4004 (1971 November).
CADC.
In 1968, Garrett AiResearch (which employed designers Ray Holt and Steve Geller) was invited to produce a digital computer to compete with electromechanical systems then under development for the main flight control computer in the US Navy's new F-14 Tomcat fighter. The design was complete by 1970, and used a MOS-based chipset as the core CPU. The design was significantly (approximately 20 times) smaller and much more reliable than the mechanical systems it competed against, and was used in all of the early Tomcat models. This system contained "a 20-bit, pipelined, parallel multi-microprocessor". The Navy refused to allow publication of the design until 1997. For this reason the CADC, and the MP944 chipset it used, are fairly unknown.
Ray Holt graduated from California Polytechnic University in 1968, and began his computer design career with the CADC. From its inception, it was shrouded in secrecy until 1998 when at Holt's request, the US Navy allowed the documents into the public domain. Since then people have debated whether this was the first microprocessor. Holt has stated that no one has compared this microprocessor with those that came later. According to Parab et al. (2007), ""The scientific papers and literature published around 1971 reveal that the MP944 digital processor used for the F-14 Tomcat aircraft of the US Navy qualifies as the first microprocessor. Although interesting, it was not a single-chip processor, as was not the Intel 4004 – they both were more like a set of parallel building blocks you could use to make a general-purpose form. It contains a CPU, RAM, ROM, and two other support chips like the Intel 4004. It was made from the same P-channel technology, operated at military specifications and had larger chips -- an excellent computer engineering design by any standards. Its design indicates a major advance over Intel, and two year earlier. It actually worked and was flying in the F-14 when the Intel 4004 was announced. It indicates that today’s industry theme of converging DSP-microcontroller architectures was started in 1971."" This convergence of DSP and microcontroller architectures is known as a digital signal controller.
Four-Phase Systems AL1.
The Four-Phase Systems AL1 was an 8-bit bit slice chip containing eight registers and an ALU. It was designed by Lee Boysel in 1969. At the time, it formed part of a nine-chip, 24-bit CPU with three AL1s, but it was later called a microprocessor when, in response to 1990s litigation by Texas Instruments, a demonstration system was constructed where a single AL1 formed part of a courtroom demonstration computer system, together with RAM, ROM, and an input-output device.
Pico/General Instrument.
In 1971, Pico Electronics and General Instrument (GI) introduced their first collaboration in ICs, a complete single chip calculator IC for the Monroe/Litton Royal Digital III calculator. This chip could also arguably lay claim to be one of the first microprocessors or microcontrollers having ROM, RAM and a RISC instruction set on-chip. The layout for the four layers of the PMOS process was hand drawn at x500 scale on mylar film, a significant task at the time given the complexity of the chip.
Pico was a spinout by five GI design engineers whose vision was to create single chip calculator ICs. They had significant previous design experience on multiple calculator chipsets with both GI and Marconi-Elliott. The key team members had originally been tasked by Elliott Automation to create an 8-bit computer in MOS and had helped establish a MOS Research Laboratory in Glenrothes, Scotland in 1967.
Calculators were becoming the largest single market for semiconductors so Pico and GI went on to have significant success in this burgeoning market. GI continued to innovate in microprocessors and microcontrollers with products including the CP1600, IOB1680 and PIC1650. In 1987 the GI Microelectronics business was spun out into the Microchip PIC microcontroller business.
Intel 4004.
The Intel 4004 is generally regarded as the first commercially available microprocessor, and cost . The first known advertisement for the 4004 is dated November 15, 1971 and appeared in Electronic News. The project that produced the 4004 originated in 1969, when Busicom, a Japanese calculator manufacturer, asked Intel to build a chipset for high-performance desktop calculators. Busicom's original design called for a programmable chip set consisting of seven different chips. Three of the chips were to make a special-purpose CPU with its program stored in ROM and its data stored in shift register read-write memory. Ted Hoff, the Intel engineer assigned to evaluate the project, believed the Busicom design could be simplified by using dynamic RAM storage for data, rather than shift register memory, and a more traditional general-purpose CPU architecture. Hoff came up with a four-chip architectural proposal: a ROM chip for storing the programs, a dynamic RAM chip for storing data, a simple I/O device and a 4-bit central processing unit (CPU). Although not a chip designer, he felt the CPU could be integrated into a single chip, but as he lacked the technical know-how the idea remained just a wish for the time being.
While the architecture and specifications of the MCS-4 came from the interaction of Hoff with Stanley Mazor, a software engineer reporting to him, and with Busicom engineer Masatoshi Shima, during 1969, Mazor and Hoff moved on to other projects. In April 1970, Intel hired Italian-born engineer Federico Faggin as project leader, a move that ultimately made the single-chip CPU final design a reality (Shima meanwhile designed the Busicom calculator firmware and assisted Faggin during the first six months of the implementation). Faggin, who originally developed the silicon gate technology (SGT) in 1968 at Fairchild Semiconductor and designed the world’s first commercial integrated circuit using SGT, the Fairchild 3708, had the correct background to lead the project into what would become the first commercial general purpose microprocessor. Since SGT was his very own invention, Faggin also used it to create his new methodology for random logic design that made it possible to implement a single-chip CPU with the proper speed, power dissipation and cost. The manager of Intel's MOS Design Department was Leslie L. Vadász at the time of the MCS-4 development but Vadász's attention was completely focused on the mainstream business of semiconductor memories so he left the leadership and the management of the MCS-4 project to Faggin, who was ultimately responsible for leading the 4004 project to its realization. Production units of the 4004 were first delivered to Busicom in March 1971 and shipped to other customers in late 1971.
Gilbert Hyatt.
Gilbert Hyatt was awarded a patent claiming an invention pre-dating both TI and Intel, describing a "microcontroller". The patent was later invalidated, but not before substantial royalties were paid out.
TMS 1000.
The Smithsonian Institution says TI engineers Gary Boone and Michael Cochran succeeded in creating the first microcontroller (also called a microcomputer) and the first single-chip CPU in 1971. The result of their work was the TMS 1000, which went on the market in 1974.
TI stressed the 4-bit TMS 1000 for use in pre-programmed embedded applications, introducing a version called the TMS1802NC on September 17, 1971 that implemented a calculator on a chip.
TI filed for a patent on the microprocessor. Gary Boone was awarded for the single-chip microprocessor architecture on September 4, 1973. In 1971 and again in 1976, Intel and TI entered into broad patent cross-licensing agreements, with Intel paying royalties to TI for the microprocessor patent. A history of these events is contained in court documentation from a legal dispute between Cyrix and Intel, with TI as inventor and owner of the microprocessor patent.
A computer-on-a-chip combines the microprocessor core (CPU), memory, and I/O (input/output) lines onto one chip. The computer-on-a-chip patent, called the "microcomputer patent" at the time, , was awarded to Gary Boone and Michael J. Cochran of TI. Aside from this patent, the standard meaning of microcomputer is a computer using one or more microprocessors as its CPU(s), while the concept defined in the patent is more akin to a microcontroller.
8-bit designs.
The Intel 4004 was followed in 1972 by the Intel 8008, the world's first 8-bit microprocessor. The 8008 was not, however, an extension of the 4004 design, but instead the culmination of a separate design project at Intel, arising from a contract with Computer Terminals Corporation, of San Antonio TX, for a chip for a terminal they were designing, the Datapoint 2200—fundamental aspects of the design came not from Intel but from CTC. In 1968, CTC's Vic Poor and Harry Pyle developed the original design for the instruction set and operation of the processor. In 1969, CTC contracted two companies, Intel and Texas Instruments, to make a single-chip implementation, known as the CTC 1201. In late 1970 or early 1971, TI dropped out being unable to make a reliable part. In 1970, with Intel yet to deliver the part, CTC opted to use their own implementation in the Datapoint 2200, using traditional TTL logic instead (thus the first machine to run "8008 code" was not in fact a microprocessor at all and was delivered a year earlier). Intel's version of the 1201 microprocessor arrived in late 1971, but was too late, slow, and required a number of additional support chips. CTC had no interest in using it. CTC had originally contracted Intel for the chip, and would have owed them for their design work. To avoid paying for a chip they did not want (and could not use), CTC released Intel from their contract and allowed them free use of the design. Intel marketed it as the 8008 in April, 1972, as the world's first 8-bit microprocessor. It was the basis for the famous "Mark-8" computer kit advertised in the magazine "Radio-Electronics" in 1974. This processor had an 8-bit data bus and a 14-bit address bus.
The 8008 was the precursor to the successful Intel 8080 (1974), which offered improved performance over the 8008 and required fewer support chips. Federico Faggin conceived and designed it using high voltage N channel MOS. The Zilog Z80 (1976) was also a Faggin design, using low voltage N channel with depletion load and derivative Intel 8-bit processors: all designed with the methodology Faggin created for the 4004. Motorola released the competing 6800 in August 1974, and the similar MOS Technology 6502 in 1975 (both designed largely by the same people). The 6502 family rivaled the Z80 in popularity during the 1980s.
A low overall cost, small packaging, simple computer bus requirements, and sometimes the integration of extra circuitry (e.g. the Z80's built-in memory refresh circuitry) allowed the home computer "revolution" to accelerate sharply in the early 1980s. This delivered such inexpensive machines as the Sinclair ZX-81, which sold for . A variation of the 6502, the MOS Technology 6510 was used in the Commodore 64 and yet another variant, the 8502, powered the Commodore 128.
The Western Design Center, Inc (WDC) introduced the CMOS 65C02 in 1982 and licensed the design to several firms. It was used as the CPU in the Apple IIe and IIc personal computers as well as in medical implantable grade pacemakers and defibrillators, automotive, industrial and consumer devices. WDC pioneered the licensing of microprocessor designs, later followed by ARM (32-bit) and other microprocessor intellectual property (IP) providers in the 1990s.
Motorola introduced the MC6809 in 1978. It was an ambitious and well thought-through 8-bit design that was source compatible with the 6800, and implemented using purely hard-wired logic (subsequent 16-bit microprocessors typically used microcode to some extent, as CISC design requirements were becoming too complex for pure hard-wired logic).
Another early 8-bit microprocessor was the Signetics 2650, which enjoyed a brief surge of interest due to its innovative and powerful instruction set architecture.
A seminal microprocessor in the world of spaceflight was RCA's RCA 1802 (aka CDP1802, RCA COSMAC) (introduced in 1976), which was used on board the Galileo probe to Jupiter (launched 1989, arrived 1995). RCA COSMAC was the first to implement CMOS technology. The CDP1802 was used because it could be run at very low power, and because a variant was available fabricated using a special production process, silicon on sapphire (SOS), which provided much better protection against cosmic radiation and electrostatic discharge than that of any other processor of the era. Thus, the SOS version of the 1802 was said to be the first radiation-hardened microprocessor.
The RCA 1802 had what is called a static design, meaning that the clock frequency could be made arbitrarily low, even to 0 Hz, a total stop condition. This let the Galileo spacecraft use minimum electric power for long uneventful stretches of a voyage. Timers or sensors would awaken the processor in time for important tasks, such as navigation updates, attitude control, data acquisition, and radio communication. Current versions of the Western Design Center 65C02 and 65C816 have static cores, and thus retain data even when the clock is completely halted.
12-bit designs.
The Intersil 6100 family consisted of a 12-bit microprocessor (the 6100) and a range of peripheral support and memory ICs. The microprocessor recognised the DEC PDP-8 minicomputer instruction set. As such it was sometimes referred to as the CMOS-PDP8. Since it was also produced by Harris Corporation, it was also known as the Harris HM-6100. By virtue of its CMOS technology and associated benefits, the 6100 was being incorporated into some military designs until the early 1980s.
16-bit designs.
The first multi-chip 16-bit microprocessor was the National Semiconductor IMP-16, introduced in early 1973. An 8-bit version of the chipset was introduced in 1974 as the IMP-8.
Other early multi-chip 16-bit microprocessors include one that Digital Equipment Corporation (DEC) used in the LSI-11 OEM board set and the packaged PDP 11/03 minicomputer—and the Fairchild Semiconductor MicroFlame 9440, both introduced in 1975–76. In 1975, National introduced the first 16-bit single-chip microprocessor, the National Semiconductor PACE, which was later followed by an NMOS version, the INS8900.
Another early single-chip 16-bit microprocessor was TI's TMS 9900, which was also compatible with their TI-990 line of minicomputers. The 9900 was used in the TI 990/4 minicomputer, the TI-99/4A home computer, and the TM990 line of OEM microcomputer boards. The chip was packaged in a large ceramic 64-pin DIP package, while most 8-bit microprocessors such as the Intel 8080 used the more common, smaller, and less expensive plastic 40-pin DIP. A follow-on chip, the TMS 9980, was designed to compete with the Intel 8080, had the full TI 990 16-bit instruction set, used a plastic 40-pin package, moved data 8 bits at a time, but could only address 16 KB. A third chip, the TMS 9995, was a new design. The family later expanded to include the 99105 and 99110.
The Western Design Center (WDC) introduced the CMOS 65816 16-bit upgrade of the WDC CMOS 65C02 in 1984. The 65816 16-bit microprocessor was the core of the Apple IIgs and later the Super Nintendo Entertainment System, making it one of the most popular 16-bit designs of all time.
Intel "upsized" their 8080 design into the 16-bit Intel 8086, the first member of the x86 family, which powers most modern PC type computers. Intel introduced the 8086 as a cost-effective way of porting software from the 8080 lines, and succeeded in winning much business on that premise. The 8088, a version of the 8086 that used an 8-bit external data bus, was the microprocessor in the first IBM PC. Intel then released the 80186 and 80188, the 80286 and, in 1985, the 32-bit 80386, cementing their PC market dominance with the processor family's backwards compatibility. The 80186 and 80188 were essentially versions of the 8086 and 8088, enhanced with some onboard peripherals and a few new instructions. Although Intel's 80186 and 80188 were not used in IBM PC type designs, second source versions from NEC, the V20 and V30 frequently were. The 8086 and successors had an innovative but limited method of memory segmentation, while the 80286 introduced a full-featured segmented memory management unit (MMU). The 80386 introduced a flat 32-bit memory model with paged memory management.
The 16-bit Intel x86 processors up to and including the 80386 do not include floating-point units (FPUs). Intel introduced the 8087, 80187, 80287 and 80387 math coprocessors to add hardware floating-point and transcendental function capabilities to the 8086 through 80386 CPUs. The 8087 works with the 8086/8088 and 80186/80188, the 80187 works with the 80186 but not the 80188, the 80287 works with the 80286 and the 80387 works with the 80386. The combination of an x86 CPU and an x87 coprocessor forms a single multi-chip microprocessor; the two chips are programmed as a unit using a single integrated instruction set. The 8087 and 80187 coprocessors are connected in parallel with the data and address buses of their parent processor and directly execute instructions intended for them. The 80287 and 80387 coprocessors are interfaced to the CPU through I/O ports in the CPU's address space, this is transparent to the program, which does not need to know about or access these I/O ports directly; the program accesses the coprocessor and its registers through normal instruction opcodes.
32-bit designs.
16-bit designs had only been on the market briefly when 32-bit implementations started to appear.
The most significant of the 32-bit designs is the Motorola MC68000, introduced in 1979. The 68k, as it was widely known, had 32-bit registers in its programming model but used 16-bit internal data paths, three 16-bit Arithmetic Logic Units, and a 16-bit external data bus (to reduce pin count), and externally supported only 24-bit addresses (internally it worked with full 32 bit addresses). In PC-based IBM-compatible mainframes the MC68000 internal microcode was modified to emulate the 32-bit System/370 IBM mainframe. Motorola generally described it as a 16-bit processor. The combination of high performance, large (16 megabytes or 224 bytes) memory space and fairly low cost made it the most popular CPU design of its class. The Apple Lisa and Macintosh designs made use of the 68000, as did a host of other designs in the mid-1980s, including the Atari ST and Commodore Amiga.
The world's first single-chip fully 32-bit microprocessor, with 32-bit data paths, 32-bit buses, and 32-bit addresses, was the AT&T Bell Labs BELLMAC-32A, with first samples in 1980, and general production in 1982. After the divestiture of AT&T in 1984, it was renamed the WE 32000 (WE for Western Electric), and had two follow-on generations, the WE 32100 and WE 32200. These microprocessors were used in the AT&T 3B5 and 3B15 minicomputers; in the 3B2, the world's first desktop super microcomputer; in the "Companion", the world's first 32-bit laptop computer; and in "Alexander", the world's first book-sized super microcomputer, featuring ROM-pack memory cartridges similar to today's gaming consoles. All these systems ran the UNIX System V operating system.
The first commercial, single chip, fully 32-bit microprocessor available on the market was the HP FOCUS.
Intel's first 32-bit microprocessor was the iAPX 432, which was introduced in 1981, but was not a commercial success. It had an advanced capability-based object-oriented architecture, but poor performance compared to contemporary architectures such as Intel's own 80286 (introduced 1982), which was almost four times as fast on typical benchmark tests. However, the results for the iAPX432 was partly due to a rushed and therefore suboptimal Ada compiler.
Motorola's success with the 68000 led to the MC68010, which added virtual memory support. The MC68020, introduced in 1984 added full 32-bit data and address buses. The 68020 became hugely popular in the Unix supermicrocomputer market, and many small companies (e.g., Altos, Charles River Data Systems, Cromemco) produced desktop-size systems. The MC68030 was introduced next, improving upon the previous design by integrating the MMU into the chip. The continued success led to the MC68040, which included an FPU for better math performance. A 68050 failed to achieve its performance goals and was not released, and the follow-up MC68060 was released into a market saturated by much faster RISC designs. The 68k family faded from use in the early 1990s.
Other large companies designed the 68020 and follow-ons into embedded equipment. At one point, there were more 68020s in embedded equipment than there were Intel Pentiums in PCs. The ColdFire processor cores are derivatives of the venerable 68020.
During this time (early to mid-1980s), National Semiconductor introduced a very similar 16-bit pinout, 32-bit internal microprocessor called the NS 16032 (later renamed 32016), the full 32-bit version named the NS 32032. Later, National Semiconductor produced the NS 32132, which allowed two CPUs to reside on the same memory bus with built in arbitration. The NS32016/32 outperformed the MC68000/10, but the NS32332—which arrived at approximately the same time as the MC68020—did not have enough performance. The third generation chip, the NS32532, was different. It had about double the performance of the MC68030, which was released around the same time. The appearance of RISC processors like the AM29000 and MC88000 (now both dead) influenced the architecture of the final core, the NS32764. Technically advanced—with a superscalar RISC core, 64-bit bus, and internally overclocked—it could still execute Series 32000 instructions through real-time translation.
When National Semiconductor decided to leave the Unix market, the chip was redesigned into the Swordfish Embedded processor with a set of on chip peripherals. The chip turned out to be too expensive for the laser printer market and was killed. The design team went to Intel and there designed the Pentium processor, which is very similar to the NS32764 core internally. The big success of the Series 32000 was in the laser printer market, where the NS32CG16 with microcoded BitBlt instructions had very good price/performance and was adopted by large companies like Canon. By the mid-1980s, Sequent introduced the first SMP server-class computer using the NS 32032. This was one of the design's few wins, and it disappeared in the late 1980s. The MIPS R2000 (1984) and R3000 (1989) were highly successful 32-bit RISC microprocessors. They were used in high-end workstations and servers by SGI, among others. Other designs included the Zilog Z80000, which arrived too late to market to stand a chance and disappeared quickly.
The ARM first appeared in 1985. This is a RISC processor design, which has since come to dominate the 32-bit embedded systems processor space due in large part to its power efficiency, its licensing model, and its wide selection of system development tools. Semiconductor manufacturers generally license cores and integrate them into their own system on a chip products; only a few such vendors are licensed to modify the ARM cores. Most cell phones include an ARM processor, as do a wide variety of other products. There are microcontroller-oriented ARM cores without virtual memory support, as well as symmetric multiprocessor (SMP) applications processors with virtual memory.
From 1993 to 2003, the 32-bit x86 architectures became increasingly dominant in desktop, laptop, and server markets, and these microprocessors became faster and more capable. Intel had licensed early versions of the architecture to other companies, but declined to license the Pentium, so AMD and Cyrix built later versions of the architecture based on their own designs. During this span, these processors increased in complexity (transistor count) and capability (instructions/second) by at least three orders of magnitude. Intel's Pentium line is probably the most famous and recognizable 32-bit processor model, at least with the public at broad.
64-bit designs in personal computers.
While 64-bit microprocessor designs have been in use in several markets since the early 1990s (including the Nintendo 64 gaming console in 1996), the early 2000s saw the introduction of 64-bit microprocessors targeted at the PC market.
With AMD's introduction of a 64-bit architecture backwards-compatible with x86, x86-64 (also called AMD64), in September 2003, followed by Intel's near fully compatible 64-bit extensions (first called IA-32e or EM64T, later renamed Intel 64), the 64-bit desktop era began. Both versions can run 32-bit legacy applications without any performance penalty as well as new 64-bit software. With operating systems Windows XP x64, Windows Vista x64, Windows 7 x64, Linux, BSD, and Mac OS X that run 64-bit native, the software is also geared to fully utilize the capabilities of such processors. The move to 64 bits is more than just an increase in register size from the IA-32 as it also doubles the number of general-purpose registers.
The move to 64 bits by PowerPC processors had been intended since the processors' design in the early 90s and was not a major cause of incompatibility. Existing integer registers are extended as are all related data pathways, but, as was the case with IA-32, both floating point and vector units had been operating at or above 64 bits for several years. Unlike what happened when IA-32 was extended to x86-64, no new general purpose registers were added in 64-bit PowerPC, so any performance gained when using the 64-bit mode for applications making no use of the larger address space is minimal.
In 2011, ARM introduced a new 64-bit ARM architecture.
RISC.
In the mid-1980s to early 1990s, a crop of new high-performance reduced instruction set computer (RISC) microprocessors appeared, influenced by discrete RISC-like CPU designs such as the IBM 801 and others. RISC microprocessors were initially used in special-purpose machines and Unix workstations, but then gained wide acceptance in other roles.
The first commercial RISC microprocessor design was released in 1984 by MIPS Computer Systems, the 32-bit R2000 (the R1000 was not released). In 1986, HP released its first system with a PA-RISC CPU. In 1987 in the non-Unix Acorn computers' 32-bit, then cache-less, ARM2-based Acorn Archimedes became the first commercial success using the ARM architecture, then known as Acorn RISC Machine (ARM); first silicon ARM1 in 1985. The R3000 made the design truly practical, and the R4000 introduced the world's first commercially available 64-bit RISC microprocessor. Competing projects would result in the IBM POWER and Sun SPARC architectures. Soon every major vendor was releasing a RISC design, including the AT&T CRISP, AMD 29000, Intel i860 and Intel i960, Motorola 88000, DEC Alpha.
In the late 1990s, only two 64-bit RISC architectures were still produced in volume for non-embedded applications: SPARC and Power ISA, but as ARM has become increasingly powerful, in the early 2010s, it became the third RISC architecture in the general computing segment.
Multi-core designs.
A different approach to improving a computer's performance is to add extra processors, as in symmetric multiprocessing designs, which have been popular in servers and workstations since the early 1990s. Keeping up with Moore's Law is becoming increasingly challenging as chip-making technologies approach their physical limits. In response, microprocessor manufacturers look for other ways to improve performance so they can maintain the momentum of constant upgrades.
A multi-core processor is a single chip that contains more than one microprocessor core. Each core can simultaneously execute processor instructions in parallel. This effectively multiplies the processor's potential performance by the number of cores, if the software is designed to take advantage of more than one processor core. Some components, such as bus interface and cache, may be shared between cores. Because the cores are physically close to each other, they can communicate with each other much faster than separate (off-chip) processors in a multiprocessor system, which improves overall system performance.
In 2005, AMD released the first native dual-core processor, the Athlon X2. Intel's Pentium D had beaten the X2 to market by a few weeks, but it used two separate CPU dies and was less efficient than AMD's native design. , dual- and quad-core processors are widely used in home PCs and laptops, while quad-, six-, eight-, ten-, twelve-, and sixteen-core processors are common in the professional and enterprise markets with workstations and servers.
Sun Microsystems has released the Niagara and Niagara 2 chips, both of which feature an eight-core design. The Niagara 2 supports more threads and operates at 1.6 GHz.
High-end Intel Xeon processors that are on the LGA 771, LGA 1366, and LGA 2011 sockets and high-end AMD Opteron processors that are on the C32 and G34 sockets are DP (dual processor) capable, as well as the older Intel Core 2 Extreme QX9775 also used in an older Mac Pro by Apple and the Intel Skulltrail motherboard. AMD's G34 motherboards can support up to four CPUs and Intel's LGA 1567 motherboards can support up to eight CPUs.
Modern desktop computers support systems with multiple CPUs, but few applications outside of the professional market can make good use of more than four cores. Both Intel and AMD currently offer fast quad, hex and octa-core desktop CPUs, making multi-CPU systems obsolete for many purposes.
The desktop market has been in a transition towards quad-core CPUs since Intel's Core 2 Quad was released and are now common, although dual-core CPUs are still more prevalent. Older or mobile computers are less likely to have more than two cores than newer desktops. Not all software is optimised for multi-core CPUs, making fewer, more powerful cores preferable.
AMD offers CPUs with more cores for a given amount of money than similarly priced Intel CPUs—but the AMD cores are somewhat slower, so the two trade blows in different applications depending on how well-threaded the programs running are. For example, Intel's cheapest Sandy Bridge quad-core CPUs often cost almost twice as much as AMD's cheapest Athlon II, Phenom II, and FX quad-core CPUs but Intel has dual-core CPUs in the same price ranges as AMD's cheaper quad-core CPUs. In an application that uses one or two threads, the Intel dual-core CPUs outperform AMD's similarly priced quad-core CPUs—and if a program supports three or four threads the cheap AMD quad-core CPUs outperform the similarly priced Intel dual-core CPUs.
Historically, AMD and Intel have switched places as the company with the fastest CPU several times. Intel currently leads on the desktop side of the computer CPU market, with their Sandy Bridge and Ivy Bridge series. In servers, AMD's new Opterons seem to have superior performance for their price point. This means that AMD are currently more competitive in low- to mid-end servers and workstations that more effectively use fewer cores and threads.
Market statistics.
In 1997, about 55% of all CPUs sold in the world are 8-bit microcontrollers, over two billion of which were sold.
In 2002, less than 10% of all the CPUs sold in the world were 32-bit or more. Of all the 32-bit CPUs sold, about 2% are used in desktop or laptop personal computers. Most microprocessors are used in embedded control applications such as household appliances, automobiles, and computer peripherals. Taken as a whole, the average price for a microprocessor, microcontroller, or DSP is just over .
In 2003, about billion worth of microprocessors were manufactured and sold. Although about half of that money was spent on CPUs used in desktop or laptop personal computers, those count for only about 2% of all CPUs sold. The quality-adjusted price of laptop microprocessors improved -25% to -35% per year in 2004–10, and the rate of improvement slowed to -15% to -25% per year in 2010–13.
About ten billion CPUs were manufactured in 2008. About 98% of new CPUs produced each year are embedded.

</doc>
<doc id="19555" url="https://en.wikipedia.org/wiki?curid=19555" title="Molecule">
Molecule

A molecule is an electrically neutral group of two or more atoms held together by chemical bonds. Molecules are distinguished from ions by their lack of electrical charge. However, in quantum physics, organic chemistry, and biochemistry, the term "molecule" is often used less strictly, also being applied to polyatomic ions.
In the kinetic theory of gases, the term "molecule" is often used for any gaseous particle regardless of its composition. According to this definition, noble gas atoms are considered molecules as they are in fact monoatomic molecules.
A molecule may be homonuclear, that is, it consists of atoms of a single chemical element, as with oxygen (O2); or it may be heteronuclear, a chemical compound composed of more than one element, as with water (H2O). Atoms and complexes connected by non-covalent bonds such as hydrogen bonds or ionic bonds are generally not considered single molecules.
Molecules as components of matter are common in organic substances (and therefore biochemistry). They also make up most of the oceans and atmosphere. However, the majority of familiar solid substances on Earth, including most of the minerals that make up the crust, mantle, and core of the Earth, contain many chemical bonds, but are "not" made of identifiable molecules. Also, no typical molecule can be defined for ionic crystals (salts) and covalent crystals (network solids), although these are often composed of repeating unit cells that extend either in a plane (such as in graphene) or three-dimensionally (such as in diamond, quartz, or sodium chloride). The theme of repeated unit-cellular-structure also holds for most condensed phases with metallic bonding, which means that solid metals are also not made of molecules. In glasses (solids that exist in a vitreous disordered state), atoms may also be held together by chemical bonds without presence of any definable molecule, but also without any of the regularity of repeating units that characterizes crystals.
Molecular science.
The science of molecules is called "molecular chemistry" or "molecular physics", depending on whether the focus is on chemistry or physics. Molecular chemistry deals with the laws governing the interaction between molecules that results in the formation and breakage of chemical bonds, while molecular physics deals with the laws governing their structure and properties. In practice, however, this distinction is vague. In molecular sciences, a molecule consists of a stable system (bound state) composed of two or more atoms. Polyatomic ions may sometimes be usefully thought of as electrically charged molecules. The term "unstable molecule" is used for very reactive species, i.e., short-lived assemblies (resonances) of electrons and nuclei, such as radicals, molecular ions, Rydberg molecules, transition states, van der Waals complexes, or systems of colliding atoms as in Bose–Einstein condensate.
History and etymology.
According to Merriam-Webster and the Online Etymology Dictionary, the word "molecule" derives from the Latin "moles" or small unit of mass.
The definition of the molecule has evolved as knowledge of the structure of molecules has increased. Earlier definitions were less precise, defining molecules as the smallest particles of pure chemical substances that still retain their composition and chemical properties. This definition often breaks down since many substances in ordinary experience, such as rocks, salts, and metals, are composed of large crystalline networks of chemically bonded atoms or ions, but are not made of discrete molecules.
Bonding.
Molecules are held together by either Covalent Bonding or Ionic Bonding. Several types of non metal elements exist only as molecules in the environment. For example, Hydrogen only exists as Hydrogen molecule. A molecule of a compound is made out of two or more elements.
Covalent.
A covalent bond is a chemical bond that involves the sharing of electron pairs between atoms. These electron pairs are known as shared pairs or bonding pairs, and the stable balance of attractive and repulsive forces between atoms, when they share electrons, is known as covalent bonding.
Ionic.
Ionic bonding is a type of chemical bond that involves the electrostatic attraction between oppositely charged ions, and is the primary interaction occurring in ionic compounds. The ions are atoms that have lost one or more electrons (known as cations) and atoms that have gained one or more electrons (known as anions). This transfer of electrons is known as electrovalence in contrast to covalence. In the simplest case, the cation is a metal atom and the anion is a nonmetal atom, but these ions can be of a more complex nature, e.g. molecular ions like NH4+ or SO42−. In simpler words, an ionic bond is the transfer of electrons from a metal to a non-metal in order for both atoms to obtain a full valence shell.
Molecular size.
Most molecules are far too small to be seen with the naked eye, but there are exceptions. DNA, a macromolecule, can reach macroscopic sizes, as can molecules of many polymers. Molecules commonly used as building blocks for organic synthesis have a dimension of a few angstroms (Å) to several dozen Å, or around one billionth of a meter. Single molecules cannot usually be observed by light (as noted above), but small molecules and even the outlines of individual atoms may be traced in some circumstances by use of an atomic force microscope. Some of the largest molecules are macromolecules or supermolecules.
The smallest molecule is the diatomic hydrogen (H2), with a bond length of 0.74 Å.
Effective molecular radius is the size a molecule displays in solution.
The table of permselectivity for different substances contains examples.
Molecular formulas.
Chemical formula types.
The chemical formula for a molecule uses a single line of chemical element symbols, numbers, and sometimes also other symbols, such as parentheses, dashes, brackets, and "plus" (+) and "minus" (−) signs. These are limited to a single typographic line of symbols, which may include subscripts and superscripts.
A compound's empirical formula is a very simple type of chemical formula. It is the simplest integer ratio of the chemical elements that constitute it. For example, water is always composed of a 2:1 ratio of hydrogen to oxygen atoms, and ethyl alcohol or ethanol is always composed of carbon, hydrogen, and oxygen in a 2:6:1 ratio. However, this does not determine the kind of molecule uniquely – dimethyl ether has the same ratios as ethanol, for instance. Molecules with the same atoms in different arrangements are called isomers. Also carbohydrates, for example, have the same ratio (carbon:hydrogen:oxygen = 1:2:1) (and thus the same empirical formula) but different total numbers of atoms in the molecule.
The molecular formula reflects the exact number of atoms that compose the molecule and so characterizes different molecules. However different isomers can have the same atomic composition while being different molecules.
The empirical formula is often the same as the molecular formula but not always. For example, the molecule acetylene has molecular formula C2H2, but the simplest integer ratio of elements is CH.
The molecular mass can be calculated from the chemical formula and is expressed in conventional atomic mass units equal to 1/12 of the mass of a neutral carbon-12 (12C isotope) atom. For network solids, the term formula unit is used in stoichiometric calculations.
Structural formula.
For molecules with a complicated 3-dimensional structure, especially involving atoms bonded to four different substituents, a simple molecular formula or even semi-structural chemical formula may not be enough to completely specify the molecule. In this case, a graphical type of formula called a structural formula may be needed. Structural formulas may in turn be represented with a one-dimensional chemical name, but such chemical nomenclature requires many words and terms which are not part of chemical formulas.
Molecular geometry.
Molecules have fixed equilibrium geometries—bond lengths and angles— about which they continuously oscillate through vibrational and rotational motions. A pure substance is composed of molecules with the same average geometrical structure. The chemical formula and the structure of a molecule are the two important factors that determine its properties, particularly its reactivity. Isomers share a chemical formula but normally have very different properties because of their different structures. Stereoisomers, a particular type of isomer, may have very similar physico-chemical properties and at the same time different biochemical activities.
Molecular spectroscopy.
Molecular spectroscopy deals with the response (spectrum) of molecules interacting with probing signals of known energy (or frequency, according to Planck's formula). Molecules have quantized energy levels that can be analyzed by detecting the molecule's energy exchange through absorbance or emission.
Spectroscopy does not generally refer to diffraction studies where particles such as neutrons, electrons, or high energy X-rays interact with a regular arrangement of molecules (as in a crystal).
Microwave spectroscopy commonly measures changes in the rotation of molecules, and can be used to identify molecules in outer space. Infrared spectroscopy measures changes in vibration of molecules, including stretching, bending or twisting motions. It is commonly used to identify the kinds of bonds or functional groups in molecules. Changes in the arrangements of electrons yield absorption or emission lines in ultraviolet, visible or near infrared light, and result in colour. Nuclear resonance spectroscopy actually measures the environment of particular nuclei in the molecule, and can be used to characterise the numbers of atoms in different positions in a molecule.
Theoretical aspects.
The study of molecules by molecular physics and theoretical chemistry is largely based on quantum mechanics and is essential for the understanding of the chemical bond. The simplest of molecules is the hydrogen molecule-ion, H2+, and the simplest of all the chemical bonds is the one-electron bond. H2+ is composed of two positively charged protons and one negatively charged electron, which means that the Schrödinger equation for the system can be solved more easily due to the lack of electron–electron repulsion. With the development of fast digital computers, approximate solutions for more complicated molecules became possible and are one of the main aspects of computational chemistry.
When trying to define rigorously whether an arrangement of atoms is "sufficiently stable" to be considered a molecule, IUPAC suggests that it "must correspond to a depression on the potential energy surface that is deep enough to confine at least one vibrational state". This definition does not depend on the nature of the interaction between the atoms, but only on the strength of the interaction. In fact, it includes weakly bound species that would not traditionally be considered molecules, such as the helium dimer, He2, which has one vibrational bound state and is so loosely bound that it is only likely to be observed at very low temperatures.
Whether or not an arrangement of atoms is "sufficiently stable" to be considered a molecule is inherently an operational definition. Philosophically, therefore, a molecule is not a fundamental entity (in contrast, for instance, to an elementary particle); rather, the concept of a molecule is the chemist's way of making a useful statement about the strengths of atomic-scale interactions in the world that we observe.

</doc>
<doc id="19556" url="https://en.wikipedia.org/wiki?curid=19556" title="Mode (music)">
Mode (music)

In the theory of Western music, mode (from Latin "modus," "measure, standard, manner, way, size, limit of quantity, method") (; OED) generally refers to a type of scale, coupled with a set of characteristic melodic behaviours. This use, still the most common in recent years, reflects a tradition dating to the Middle Ages, itself inspired by the theory of ancient Greek music.
Mode as a general concept.
Regarding the concept of mode as applied to pitch relationships generally, Harold S. Powers proposed mode as a general term but limited for melody types, which were based on the modal interpretation of ancient Greek octave species called "tonos" (τόνος) or "harmonia" (ἁρμονία), with "most of the area between ... being in the domain of mode" . This synthesis between tonus as a church tone and the older meaning associated with an octave species was done by medieval theorists for the Western monodic plainchant tradition (see Hucbald and Aurelian). It is generally assumed that Carolingian theorists imported monastic Octoechos propagated in the patriarchates of Jerusalem (Mar Saba) and Constantinople (Stoudios Monastery) which meant the eight echoi they used for the composition of hymns (e.g., ), though direct adaptations of Byzantine chants in the survived Gregorian repertoire are extremely rare.
Since the end of the eighteenth century, the term "mode" has also applied to pitch structures in non-European musical cultures, sometimes with doubtful compatibility . The concept is also heavily used in regard to the Western polyphony before advent of the so-called common-practice music, as for example "modale Mehrstimmigkeit" by Carl Dahlhaus or "Tonarten" of the 16th and 17th centuries found by Bernhard Meier (; ).
Additional meanings.
The word encompasses several additional meanings, however. Authors from the ninth century until the early eighteenth century (e.g. Guido of Arezzo) sometimes employed the Latin "modus" for interval. In the theory of late-medieval mensural polyphony (e.g. Franco of Cologne), "modus" is a rhythmic relationship between long and short values or a pattern made from them ; in mensural music most often theorists applied it to division of longa into 3 or 2 breves.
Modes and scales.
A "scale" is an ordered series of pitches that, with the key or tonic (first tone) as a reference point, defines that scale's intervals, or steps. The concept of "mode" in Western music theory has three successive stages: in Gregorian chant theory, in Renaissance polyphonic theory, and in tonal harmonic music of the common practice period. In all three contexts, "mode" incorporates the idea of the diatonic scale, but differs from it by also involving an element of melody type. This concerns particular repertories of short musical figures or groups of tones within a certain scale so that, depending on the point of view, mode takes on the meaning of either a "particularized scale" or a "generalized tune". Modern musicological practice has extended the concept of mode to earlier musical systems, such as those of Ancient Greek music, Jewish cantillation, and the Byzantine system of "octoechos", as well as to other non-Western musics (; ).
By the early 19th century, the word "mode" had taken on an additional meaning, in reference to the difference between major and minor keys, specified as "major mode" and "minor mode". At the same time, composers were beginning to conceive of "modality" as something outside of the major/minor system that could be used to evoke religious feelings or to suggest folk-music idioms .
Greek.
Early Greek treatises describe three interrelated concepts that are related to the later, medieval idea of "mode": (1) scales (or "systems"), (2) "tonos"—pl. "tonoi"—(the more usual term used in medieval theory for what later came to be called "mode"), and (3) "harmonia" (harmony)—pl. "harmoniai"—this third term subsuming the corresponding "tonoi" but not necessarily the converse .
Greek scales.
The Greek scales in the Aristoxenian tradition were (; ):
These names are derived from Ancient Greek subgroups (Dorians), one small region in central Greece (Locris), and certain neighboring (non-Greek) peoples from Asia Minor (Lydia, Phrygia). The association of these ethnic names with the octave species appears to precede Aristoxenus, who criticized their application to the "tonoi" by the earlier theorists whom he called the Harmonicists .
Depending on the positioning (spacing) of the interposed tones in the tetrachords, three "genera" of the seven octave species can be recognized. The diatonic genus (composed of tones and semitones), the chromatic genus (semitones and a minor third), and the enharmonic genus (with a major third and two quarter tones or dieses) . The framing interval of the perfect fourth is fixed, while the two internal pitches are movable. Within the basic forms, the intervals of the chromatic and diatonic genera were varied further by three and two "shades" ("chroai"), respectively (; ).
In contrast to the medieval modal system, these scales and their related "tonoi" and "harmoniai" appear to have had no hierarchical relationships amongst the notes that could establish contrasting points of tension and rest, although the "mese" ("middle note") may have had some sort of gravitational function .
"Tonoi".
The term "tonos" (pl. "tonoi") was used in four senses: "as note, interval, region of the voice, and pitch. We use it of the region of the voice whenever we speak of Dorian, or Phrygian, or Lydian, or any of the other tones" . Cleonides attributes thirteen "tonoi" to Aristoxenus, which represent a progressive transposition of the entire system (or scale) by semitone over the range of an octave between the Hypodorian and the Hypermixolydian . Aristoxenus's transpositional "tonoi", according to , were named analogously to the octave species, supplemented with new terms to raise the number of degrees from seven to thirteen. However, according to the interpretation of at least two modern authorities, in these transpositional "tonoi" the Hypodorian is the lowest, and the Mixolydian next-to-highest—the reverse of the case of the octave species (; ), with nominal base pitches as follows (descending order):
Ptolemy, in his "Harmonics", ii.3–11, construed the "tonoi" differently, presenting all seven octave species within a fixed octave, through chromatic inflection of the scale degrees (comparable to the modern conception of building all seven modal scales on a single tonic). In Ptolemy's system, therefore there are only seven "tonoi" (; ). Pythagoras also construed the intervals arithmetically (if somewhat more rigorously, initially allowing for 1:1 = Unison, 2:1 = Octave, 3:2 = Fifth, 4:3 = Fourth and 5:4 = Major Third within the octave). In their diatonic genus, these "tonoi" and corresponding "harmoniai" correspond with the intervals of the familiar modern major and minor scales. See Pythagorean tuning and Pythagorean interval.
"Harmoniai".
In music theory the Greek word "harmonia" can signify the enharmonic genus of tetrachord, the seven octave species, or a style of music associated with one of the ethnic types or the "tonoi" named by them .
Particularly in the earliest surviving writings, "harmonia" is regarded not as a scale, but as the epitome of the stylised singing of a particular district or people or occupation . When the late 6th-century poet Lasus of Hermione referred to the Aeolian "harmonia", for example, he was more likely thinking of a melodic style characteristic of Greeks speaking the Aeolic dialect than of a scale pattern . By the late fifth century BC these regional types are being described in terms of differences in what is called "harmonia"—a word with several senses, but here referring to the pattern of intervals between the notes sounded by the strings of a lyra or a kithara. However, there is no reason to suppose that, at this time, these tuning patterns stood in any straightforward and organised relations to one another. It was only around the year 400 that attempts were made by a group of theorists known as the harmonicists to bring these "harmoniai" into a single system, and to express them as orderly transformations of a single structure. Eratocles was the most prominent of the harmonicists, though his ideas are known only at second hand, through Aristoxenus, from whom we learn they represented the "harmoniai" as cyclic reorderings of a given series of intervals within the octave, producing seven octave species. We also learn that Eratocles confined his descriptions to the enharmonic genus .
In "The Republic", Plato uses the term inclusively to encompass a particular type of scale, range and register, characteristic rhythmic pattern, textual subject, etc. (Mathiesen 2001a, 6(iii)(e)). He held that playing music in a particular "harmonia" would incline one towards specific behaviors associated with it, and suggested that soldiers should listen to music in Dorian or Phrygian "harmoniai" to help make them stronger, but avoid music in Lydian, Mixolydian or Ionian "harmoniai", for fear of being softened. Plato believed that a change in the musical modes of the state would cause a wide-scale social revolution (Plato, Rep. III.10–III.12 = 398C–403C)
The philosophical writings of Plato and Aristotle (c. 350 BC) include sections that describe the effect of different "harmoniai" on mood and character formation. For example, Aristotle in the "Politics" (viii:1340a:40–1340b:5):
Aristotle continues by describing the effects of rhythm, and concludes about the combined effect of rhythm and "harmonia" (viii:1340b:10–13): 
The word "ethos" (ἦθος) in this context means "moral character", and Greek ethos theory concerns the ways in which music can convey, foster, and even generate ethical states .
"Melos".
Some treatises also describe "melic" composition (μελοποιΐα), "the employment of the materials subject to harmonic practice with due regard to the requirements of each of the subjects under consideration" —which, together with the scales, "tonoi", and "harmoniai" resemble elements found in medieval modal theory . According to Aristides Quintilianus ("On Music", i.12), melic composition is subdivided into three classes: dithyrambic, nomic, and tragic. These parallel his three classes of rhythmic composition: systaltic, diastaltic and hesychastic. Each of these broad classes of melic composition may contain various subclasses, such as erotic, comic and panegyric, and any composition might be elevating (diastaltic), depressing (systaltic), or soothing (hesychastic) .
According to Mathiesen, music as a performing art was called melos, which in its perfect form (μέλος τέλειον) comprised not only the melody and the text (including its elements of rhythm and diction) but also stylized dance movement. Melic and rhythmic composition (respectively, μελοποιΐα and ῥυθμοποιΐα) were the processes of selecting and applying the various components of melos and rhythm to create a complete work. Aristides Quintilianus: 
Western Church.
Tonaries, which are lists of chant titles grouped by mode, appear in western sources around the turn of the 9th century. The influence of developments in Byzantium, from Jerusalem and Damascus, for instance the works of Saints John of Damascus (d. 749) and Cosmas of Maiouma (; ), are still not fully understood. The eight-fold division of the Latin modal system, in a four-by-two matrix, was certainly of Eastern provenance, originating probably in Syria or even in Jerusalem, and was transmitted from Byzantine sources to Carolingian practice and theory during the 8th century. However, the earlier Greek model for the Carolingian system was probably ordered like the later Byzantine "oktōēchos", that is, with the four principal (authentic) modes first, then the four plagals, whereas the Latin modes were always grouped the other way, with the authentics and plagals paired .
The 6th century scholar Boethius had translated Greek music theory treatises by Nicomachus and Ptolemy into Latin . Later authors created confusion by applying mode as described by Boethius to explain plainchant modes, which were a wholly different system . In his "De institutione musica", book 4 chapter 15, Boethius, like his Hellenistic sources, twice used the term "harmonia" to describe what would likely correspond to the later notion of "mode", but also used the word "modus"—probably translating the Greek word τρόπος ("tropos"), which he also rendered as Latin "tropus"—in connection with the system of transpositions required to produce seven diatonic octave species , so the term was simply a means of describing transposition and had nothing to do with the church modes .
Later, 9th-century theorists applied Boethius’s terms "tropus" and "modus" (along with "tonus") to the system of church modes. The treatise "De Musica" (or "De harmonica institutione") of Hucbald synthesized the three previously disparate strands of modal theory: chant theory, the Byzantine "oktōēchos" and Boethius's account of Hellenistic theory . The later 9th-century treatise known as the "Alia musica" imposed the seven species of the octave described by Boethius onto the eight church modes . Thus, the names of the modes used today do not actually reflect those used by the Greeks.
The eight church modes, or Gregorian modes, can be divided into four pairs, where each pair shares the "final" note and the four notes above the final, but have different ambituses, or ranges. If the "scale" is completed by adding three higher notes, the mode is termed "authentic", if the scale is completed by adding three lower notes, it is called "plagal" (from Greek πλάγιος, "oblique, sideways"). Otherwise explained: if the melody moves mostly above the final, with an occasional cadence to the sub-final, the mode is authentic. Plagal modes shift range and also explore the fourth below the final as well as the fifth above. In both cases, the strict ambitus of the mode is one octave. A melody that remains confined to the mode's ambitus is called "perfect"; if it falls short of it, "imperfect"; if it exceeds it, "superfluous"; and a melody that combines the ambituses of both the plagal and authentic is said to be in a "mixed mode" .
Although the earlier (Greek) model for the Carolingian system was probably ordered like the Byzantine "oktōēchos", with the four authentic modes first, followed by the four plagals, the earliest extant sources for the Latin system are organized in four pairs of authentic and plagal modes sharing the same final: protus authentic/plagal, deuterus authentic/plagal, tritus authentic/plagal, and tetrardus authentic/plagal .
Each mode has, in addition to its final, a "reciting tone", sometimes called the "dominant" (; ). It is also sometimes called the "tenor", from Latin "tenere" "to hold", meaning the tone around which the melody principally centres . The reciting tones of all authentic modes began a fifth above the final, with those of the plagal modes a third above. However, the reciting tones of modes 3, 4, and 8 rose one step during the tenth and eleventh centuries with 3 and 8 moving from B to C (half step) and that of 4 moving from G to A (whole step) .
After the reciting tone, every mode is distinguished by scale degrees called "mediant" and "participant". The mediant is named from its position between the final and reciting tone. In the authentic modes it is the third of the scale, unless that note should happen to be B, in which case C substitutes for it. In the plagal modes, its position is somewhat irregular. The participant is an auxiliary note, generally adjacent to the mediant in authentic modes and, in the plagal forms, coincident with the reciting tone of the corresponding authentic mode (some modes have a second participant) .
Only one accidental is used commonly in Gregorian chant—B may be lowered by a half-step to B. This usually (but not always) occurs in modes V and VI, as well as in the upper tetrachord of IV, and is optional in other modes except III, VII and VIII .
In 1547, the Swiss theorist Henricus Glareanus published the "Dodecachordon", in which he solidified the concept of the church modes, and added four additional modes: the Aeolian (mode 9), Hypoaeolian (mode 10), Ionian (mode 11), and Hypoionian (mode 12). A little later in the century, the Italian Gioseffo Zarlino at first adopted Glarean's system in 1558, but later (1571 and 1573) revised the numbering and naming conventions in a manner he deemed more logical, resulting in the widespread promulgation of two conflicting systems. Zarlino's system reassigned the six pairs of authentic–plagal mode numbers to finals in the order of the natural hexachord, C D E F G A, and transferred the Greek names as well, so that modes 1 through 8 now became C-authentic to F-plagal, and were now called by the names Dorian to Hypomixolydian. The pair of G modes were numbered 9 and 10 and were named Ionian and Hypoionian, while the pair of A modes retained both the numbers and names (11, Aeolian, and 12 Hypoaeolian) of Glarean's system. While Zarlino's system became popular in France, Italian composers preferred Glarean's scheme because it retained the traditional eight modes, while expanding them. Luzzasco Luzzaschi was an exception in Italy, in that he used Zarlino’s new system .
In the late-eighteenth and nineteenth centuries, some chant reformers (notably the editors of the Mechlin, Pustet-Ratisbon (Regensburg), and Rheims-Cambrai Office-Books, collectively referred to as the Cecilian Movement) renumbered the modes once again, this time retaining the original eight mode numbers and Glareanus's modes 9 and 10, but assigning numbers 11 and 12 to the modes on the final B, which they named Locrian and Hypolocrian (even while rejecting their use in chant). The Ionian and Hypoionian modes (on C) become in this system modes 13 and 14 .
Given the confusion between ancient, medieval, and modern terminology, "today it is more consistent and practical to use the traditional designation of the modes with numbers one to eight" (), using Roman numeral (I–VIII), rather than using the pseudo-Greek naming system. Contemporary terms, also used by scholars, are simply the Greek ordinals ("first", "second", etc.), usually transliterated into the Latin alphabet: protus (πρῶτος), deuterus (δεύτερος), tritus (τρίτος), and tetrardus (τέταρτος), in practice used as: protus authentus / plagalis.
Use.
A mode indicated a primary pitch (a final); the organization of pitches in relation to the final; suggested range; melodic formulas associated with different modes; location and importance of cadences; and affect (i.e., emotional effect/character). Liane Curtis writes that "Modes should not be equated with scales: principles of melodic organization, placement of cadences, and emotional affect are essential parts of modal content" in Medieval and Renaissance music (, in ).
Carl lists "three factors that form the respective starting points for the modal theories of Aurelian of Réôme, Hermannus Contractus, and Guido of Arezzo:
The oldest medieval treatise regarding modes is "Musica disciplina" by Aurelian of Réôme (dating from around 850) while Hermannus Contractus was the first to define modes as partitionings of the octave . However, the earliest Western source using the system of eight modes is the Tonary of St Riquier, dated between about 795 and 800 .
Various interpretations of the "character" imparted by the different modes have been suggested. Three such interpretations, from Guido of Arezzo (995–1050), Adam of Fulda (1445–1505), and Juan de Espinosa Medrano (1632–1688), follow:
Modern.
The modern Western modes consist of seven scales related to the familiar major and minor keys.
Although the names of the modern modes are Greek and some have names used in ancient Greek theory for some of the "harmoniai", the names of the modern modes are conventional and do not indicate a link between them and ancient Greek theory, and they do not present the sequences of intervals found even in the diatonic genus of the Greek octave species sharing the same name.
Modern Western modes use the same set of notes as the major scale, in the same order, but starting from one of its seven degrees in turn as a "tonic", and so present a different sequence of whole and half steps. The interval sequence of the major scale being T-T-s-T-T-T-s, where "s" means a semitone (half step) and "T" means a whole tone (whole step), it is thus possible to generate the following scales:
For the sake of simplicity, the examples shown above are formed by natural notes (also called "white-notes", as they can be played using the white keys of a piano keyboard). However, any transposition of each of these scales is a valid example of the corresponding mode. In other words, transposition preserves mode.
Analysis.
Each mode has characteristic intervals and chords that give it its distinctive sound.
The following is an analysis of each of the seven modern modes. The examples are provided in a key signature with no sharps or flats (scales composed of natural notes).
Ionian (I).
Ionian may arbitrarily be designated the first mode. It is the modern major scale. The example composed of natural notes begins on C, and is also known as the C-major scale: 
Dorian (II).
Dorian is the second mode. The example composed of natural notes begins on D: 
The Dorian mode is very similar to the modern natural minor scale (see Aeolian mode below). The only difference with respect to the natural minor scale is in the sixth scale degree, which is a major sixth (M6) above the tonic, rather than a minor sixth (m6).
Phrygian (III).
Phrygian is the third mode. The example composed of natural notes starts on E: 
The Phrygian mode is very similar to the modern natural minor scale (see Aeolian mode below). The only difference with respect to the natural minor scale is in the second scale degree, which is a minor second (m2) above the tonic, rather than a major second (M2).
Lydian (IV).
Lydian is the fourth mode. The example composed of natural notes starts on F: 
The single tone that differentiates this scale from the major scale (Ionian mode), is its fourth degree, which is an augmented fourth (A4) above the tonic (F), rather than a perfect fourth (P4).
Mixolydian (V).
Mixolydian is the fifth mode. The example composed of natural notes begins on G: 
The single tone that differentiates this scale from the major scale (Ionian mode), is its seventh degree, which is a minor seventh (m7) above the tonic (G), rather than a major seventh (M7).
Aeolian (VI).
Aeolian is the sixth mode. It is also called the natural minor scale. The example composed of natural notes begins on A, and is also known as the A natural-minor scale:
Locrian (VII).
Locrian is the seventh and final mode. The example composed of natural notes begins on B:
The distinctive scale degree here is the diminished fifth (d5). This makes the tonic triad diminished, so this mode is the only one in which the chords built on the tonic and dominant scale degrees have their roots separated by a diminished, rather than perfect, fifth. Similarly the tonic seventh chord is half-diminished.
Summary.
The modes can be arranged in the following sequence, which follows the circle of fifths. In this sequence, each mode has one more lowered interval relative to the tonic than the mode preceding it. Thus taking Lydian as reference, Ionian (major) has a lowered fourth; Mixolydian, a lowered fourth and seventh; Dorian, a lowered fourth, seventh, and third; Aeolian (Natural Minor), a lowered fourth, seventh, third, and sixth; Phrygian, a lowered fourth, seventh, third, sixth, and second; and Locrian, a lowered fourth, seventh, third, sixth, second, and fifth. Put another way, the augmented fourth of the Lydian scale has been reduced to a perfect fourth in Ionian, the major seventh in Ionian, to a minor seventh in Mixolydian, etc.
The tonic of a transposed mode is at the same number of 5ths down (resp. up) from the natural tonic of the mode as there are flats (resp. sharps) in its signature: e.g. the Dorian scale with 3 is F dorian as F is three 5ths down from D (F - C - G - D) and the Dorian scale with 3 is B dorian as B is three 5ths up from D (D - A - E - B). Or equivalently it is at the same interval from the tonic of the major scale with the same signature (its relative major) as that formed by its natural tonic and C: e.g. the Lydian scale with 2 is E Lydian and the Lydian scale with 2 is G Lydian as E forms with B (relative major) and G forms with D (relative major), the same interval as between F and C.
Conversely the signature of a transposed mode has as many sharps (resp. flats) as there are 5ths up (resp. down) between the tonic of the natural mode and the tonic of the transposed mode: e.g. B Dorian's signature is 4 as B is four 5ths down from D (B - F - C - G - D) and A lydian's signature is 4 as A is four 5ths up from F (F - C - G - D - A). Or again equivalently the signature of a transposed mode is the same as that of its relative major. That forms with the tonic of the transposed mode the same interval as C with the tonic of the natural mode: e.g. B Phrygian's signature is 6 as its relative major is G (C is a major 3rd down from E) and C Mixolydian's signature is 6 as its relative major is F (C is a 5th down from G).
For example the modes transposed to a common tonic of C have the following signatures:
The first three modes are sometimes called major, the next three minor, and the last one diminished (Locrian), according to the quality of their tonic triads.
The Locrian mode is traditionally considered theoretical rather than practical because the triad built on the first scale degree is diminished. Diminished triads are not consonant and therefore do not lend themselves to cadential endings. A diminished chord cannot be tonicized according to tonal phrasing practice.
Major modes.
The Ionian mode () corresponds to the major scale. Scales in the Lydian mode () are major scales with the subdominant (fourth scale degree) raised by a semitone. The Mixolydian mode () corresponds to the major scale using the subtonic (the lowered seventh) instead of the leading-tone.
Minor modes.
The Aeolian mode () is identical to the natural minor scale. The Dorian mode () corresponds to the natural minor scale with the submediant (sixth scale degree) raised a semitone. The Phrygian mode () corresponds to the natural minor scale with the supertonic (second scale degree) lowered a semitone.
Diminished mode.
The Locrian () is neither a major nor a minor mode because, although its third scale degree is minor, the fifth degree is diminished instead of perfect. For this reason it is sometimes called a "diminished" scale, though in jazz theory this term is also applied to the octatonic scale. This interval is enharmonically equivalent to the augmented fourth found between scale-degrees 1 and 4 in the Lydian mode and is also referred to as the tritone.
Use.
Use and conception of modes or modality today is different from that in early music. As Jim Samson explains, "Clearly any comparison of medieval and modern modality would recognize that the latter takes place against a background of some three centuries of harmonic tonality, permitting, and in the nineteenth century requiring, a dialogue between modal and diatonic procedure" . Indeed, when 19th-century composers revived the modes, they rendered them more strictly than Renaissance composers had, to make their qualities distinct from the prevailing major-minor system. Renaissance composers routinely sharped leading tones at cadences and lowered the fourth in the Lydian mode .
The Ionian, or Iastian (; ; ; ; ; ; ; ) mode is another name for the major scale used in much Western music. The Aeolian forms the base of the most common Western minor scale; in modern practice the Aeolian mode is differentiated from the minor by using only the seven notes of the Aeolian scale. By contrast, minor mode compositions of the common practice period frequently raise the seventh scale degree by a semitone to strengthen the cadences, and in conjunction also raise the sixth scale degree by a semitone to avoid the awkward interval of an augmented second. This is particularly true of vocal music .
Traditional folk music provides countless examples of modal melodies. For example, Irish traditional music makes extensive usage not only of the major mode, but also the Mixolydian, Dorian, and Aeolian modes . Much Flamenco music is in the Phrygian mode.
Zoltán Kodály, Gustav Holst, Manuel de Falla use modal elements as modifications of a diatonic background, while in the music of Debussy and Béla Bartók modality replaces diatonic tonality (, )
Other types.
While the term "mode" is still most commonly understood to refer to Ionian, Dorian, Phrygian, Lydian, Mixolydian, Aeolian, or Locrian scales, in modern music theory the word is sometimes applied to scales other than the diatonic. This is seen, for example, in "melodic minor" scale harmony, which is based on the seven rotations of the ascending melodic minor scale, yielding some interesting scales as shown below. The "chord" row lists tetrads that can be built from the pitches in the given mode (); see also Avoid note.
The number of possible modes for any intervallic set is dictated by the pattern of intervals in the scale. For scales built of a pattern of intervals that only repeats at the octave (like the diatonic set), the number of modes is equal to the number of notes in the scale. Scales with a recurring interval pattern smaller than an octave, however, have only as many modes as notes within that subdivision: e.g., the diminished scale, which is built of alternating whole and half steps, has only two distinct modes, since all odd-numbered modes are equivalent to the first (starting with a whole step) and all even-numbered modes are equivalent to the second (starting with a half step). The chromatic and whole-tone scales, each containing only steps of uniform size, have only a single mode each, as any rotation of the sequence results in the same sequence. Another general definition excludes these equal-division scales, and defines modal scales as subsets of them: "If we leave out certain steps of aequal-step scale we get a modal construction" (Karlheinz Stockhausen, in ). In "Messiaen's narrow sense, "a mode is any scale" made up from the 'chromatic total,' the twelve tones of the tempered system" .

</doc>
<doc id="19559" url="https://en.wikipedia.org/wiki?curid=19559" title="Mechanics">
Mechanics

Mechanics (Greek ) is an area of science concerned with the behavior of physical bodies when subjected to forces or displacements, and the subsequent effects of the bodies on their environment.
The scientific discipline has its origins in Ancient Greece with the writings of Aristotle and Archimedes (see History of classical mechanics and Timeline of classical mechanics). During the early modern period, scientists such as Khayaam, Galileo, Kepler, and Newton, laid the foundation for what is now known as classical mechanics.
It is a branch of classical physics that deals with particles that are either at rest or are moving with velocities significantly less than the speed of light. 
It can also be defined as a branch of science which deals with the motion of and forces on objects.
Classical versus quantum mechanics.
Historically, classical mechanics came first, while quantum mechanics is a comparatively recent invention. Classical mechanics originated with Isaac Newton's laws of motion in "Principia Mathematica"; Quantum Mechanics was discovered in the early 20th century. Both are commonly held to constitute the most certain knowledge that exists about physical nature. Classical mechanics has especially often been viewed as a model for other so-called exact sciences. Essential in this respect is the relentless use of mathematics in theories, as well as the decisive role played by experiment in generating and testing them.
Quantum mechanics is of a wider scope, as it encompasses classical mechanics as a sub-discipline which applies under certain restricted circumstances. According to the correspondence principle, there is no contradiction or conflict between the two subjects, each simply pertains to specific situations. The correspondence principle states that the behavior of systems described by quantum theories reproduces classical physics in the limit of large quantum numbers. Quantum mechanics has superseded classical mechanics at the foundational level and is indispensable for the explanation and prediction of processes at molecular and (sub)atomic level. However, for macroscopic processes classical mechanics is able to solve problems which are unmanageably difficult in quantum mechanics and hence remains useful and well used.
Modern descriptions of such behavior begin with a careful definition of such quantities as displacement (distance moved), time, velocity, acceleration, mass, and force. Until about 400 years ago, however, motion was explained from a very different point of view. For example, following the ideas of Greek philosopher and scientist Aristotle, scientists reasoned that a cannonball falls down because its natural position is in the Earth; the sun, the moon, and the stars travel in circles around the earth because it is the nature of heavenly objects to travel in perfect circles.
The father of science Galileo brought together the ideas of other great thinkers of his time and began to analyze motion in terms of distance traveled from some starting position and the time that it took. He showed that the speed of falling objects increases steadily during the time of their fall. This acceleration is the same for heavy objects as for light ones, provided air friction (air resistance) is discounted. The English mathematician and physicist Isaac Newton improved this analysis by defining force and mass and relating these to acceleration. For objects traveling at speeds close to the speed of light, Newton’s laws were superseded by Albert Einstein’s theory of relativity. For atomic and subatomic particles, Newton’s laws were superseded by quantum theory. For everyday phenomena, however, Newton’s three laws of motion remain the cornerstone of dynamics, which is the study of what causes motion.
Relativistic versus Newtonian mechanics.
In analogy to the distinction between quantum and classical mechanics, Einstein's general and special theories of relativity have expanded the scope of Newton and Galileo's formulation of mechanics. The differences between relativistic and Newtonian mechanics become significant and even dominant as the velocity of a massive body approaches the speed of light. For instance, in Newtonian mechanics, Newton's laws of motion specify that F = "m"a, whereas in Relativistic mechanics and Lorentz transformations, which were first discovered by Hendrik Lorentz, F = γ"m"a (where γ is the Lorentz factor, which is almost equal to 1 for low speeds).
General relativistic versus quantum.
Relativistic corrections are also needed for quantum mechanics, although general relativity has not been integrated. The two theories remain incompatible, a hurdle which must be overcome in developing a theory of everything.
History.
Antiquity.
The main theory of mechanics in antiquity was Aristotelian mechanics. A later developer in this tradition is Hipparchus.
Medieval age.
In the Middle Ages, Aristotle's theories were criticized and modified by a number of figures, beginning with John Philoponus in the 6th century. A central problem was that of projectile motion, which was discussed by Hipparchus and Philoponus. This led to the development of the theory of impetus by 14th-century French priest Jean Buridan, which developed into the modern theories of inertia, velocity, acceleration and momentum. This work and others was developed in 14th-century England by the Oxford Calculators such as Thomas Bradwardine, who studied and formulated various laws regarding falling bodies.
On the question of a body subject to a constant (uniform) force, the 12th-century Jewish-Arab Nathanel (Iraqi, of Baghdad) stated that constant force imparts constant acceleration, while the main properties are uniformly accelerated motion (as of falling bodies) was worked out by the 14th-century Oxford Calculators.
Early modern age.
Two central figures in the early modern age are Galileo Galilei and Isaac Newton. Galileo's final statement of his mechanics, particularly of falling bodies, is his "Two New Sciences" (1638). Newton's 1687 "Philosophiæ Naturalis Principia Mathematica" provided a detailed mathematical account of mechanics, using the newly developed mathematics of calculus and providing the basis of Newtonian mechanics.
There is some dispute over priority of various ideas: Newton's "Principia" is certainly the seminal work and has been tremendously influential, and the systematic mathematics therein did not and could not have been stated earlier because calculus had not been developed. However, many of the ideas, particularly as pertain to inertia (impetus) and falling bodies had been developed and stated by earlier researchers, both the then-recent Galileo and the less-known medieval predecessors. Precise credit is at times difficult or contentious because scientific language and standards of proof changed, so whether medieval statements are "equivalent" to modern statements or "sufficient" proof, or instead "similar" to modern statements and "hypotheses" is often debatable.
Modern age.
Two main modern developments in mechanics are general relativity of Einstein, and quantum mechanics, both developed in the 20th century based in part on earlier 19th-century ideas. The development in the modern continuum mechanics, particularly in the areas of elasticity, plasticity, fluid dynamics, electrodynamics and thermodynamics of deformable media, started in the second half of the 20th century.
Types of mechanical bodies.
The often-used term body needs to stand for a wide assortment of objects, including particles, projectiles, spacecraft, stars, parts of machinery, parts of solids, parts of fluids (gases and liquids), etc.
Other distinctions between the various sub-disciplines of mechanics, concern the nature of the bodies being described. Particles are bodies with little (known) internal structure, treated as mathematical points in classical mechanics. Rigid bodies have size and shape, but retain a simplicity close to that of the particle, adding just a few so-called degrees of freedom, such as orientation in space.
Otherwise, bodies may be semi-rigid, i.e. elastic, or non-rigid, i.e. fluid. These subjects have both classical and quantum divisions of study.
For instance, the motion of a spacecraft, regarding its orbit and attitude (rotation), is described by the relativistic theory of classical mechanics, while the analogous movements of an atomic nucleus are described by quantum mechanics.
Sub-disciplines in mechanics.
The following are two lists of various subjects that are studied in mechanics.
Note that there is also the "theory of fields" which constitutes a separate discipline in physics, formally treated as distinct from mechanics, whether classical fields or quantum fields. But in actual practice, subjects belonging to mechanics and fields are closely interwoven. Thus, for instance, forces that act on particles are frequently derived from fields (electromagnetic or gravitational), and particles generate fields by acting as sources. In fact, in quantum mechanics, particles themselves are fields, as described theoretically by the wave function.
Classical mechanics.
[Walter Lewin explains Newton's law of gravitation in [http://ocw.mit.edu/courses/physics/8-01-physics-i-classical-mechanics-fall-1999/video-lectures/lecture-11/ MIT course 8.01 ]]
The following are described as forming classical mechanics:
Quantum mechanics.
The following are categorized as being part of quantum mechanics:

</doc>
<doc id="19562" url="https://en.wikipedia.org/wiki?curid=19562" title="Mandelbrot set">
Mandelbrot set

The Mandelbrot set is the set of complex numbers "c" for which the function formula_1 does not diverge when iterated from formula_2, i.e., for which the sequence formula_3, formula_4, etc., remains bounded in absolute value.
The set is closely related to the idea of Julia sets, which produce similarly complex shapes. Its definition and name are due to Adrien Douady, in tribute to the mathematician Benoit Mandelbrot.
Mandelbrot set images are made by sampling complex numbers and determining for each whether the result tends towards infinity when a particular mathematical operation is iterated on it. Treating the real and imaginary parts of each number as image coordinates, pixels are colored according to how rapidly the sequence diverges, if at all.
More precisely, the Mandelbrot set is the set of values of "c" in the complex plane for which the orbit of 0 under iteration of the quadratic map
remains bounded. That is, a complex number "c" is part of the Mandelbrot set if, when starting with "z"0 = 0 and applying the iteration repeatedly, the absolute value of "z""n" remains bounded however large "n" gets. This can also be represented as
For example, letting "c" = 1 gives the sequence 0, 1, 2, 5, 26,…, which tends to infinity. As this sequence is unbounded, 1 is not an element of the Mandelbrot set. On the other hand, "c" = −1 gives the sequence 0, −1, 0, −1, 0,…, which is bounded, and so −1 belongs to the Mandelbrot set.
Images of the Mandelbrot set display an elaborate boundary that reveals progressively ever-finer recursive detail at increasing magnifications. The "style" of this repeating detail depends on the region of the set being examined. The set's boundary also incorporates smaller versions of the main shape, so the fractal property of self-similarity applies to the entire set, and not just to its parts.
The Mandelbrot set has become popular outside mathematics both for its aesthetic appeal and as an example of a complex structure arising from the application of simple rules. It is one of the best-known examples of mathematical visualization.
History.
The Mandelbrot set has its place in complex dynamics, a field first investigated by the French mathematicians Pierre Fatou and Gaston Julia at the beginning of the 20th century. This fractal was first defined and drawn in 1978 by Robert W. Brooks and Peter Matelski as part of a study of Kleinian groups. On 1 March 1980, at IBM's Thomas J. Watson Research Center in Yorktown, Heights, New York, Benoit Mandelbrot first saw a visualization of the set.
Mandelbrot studied the parameter space of quadratic polynomials in an article that appeared in 1980. The mathematical study of the Mandelbrot set really began with work by the mathematicians Adrien Douady and John H. Hubbard, who established many of its fundamental properties and named the set in honor of Mandelbrot.
The mathematicians Heinz-Otto Peitgen and Peter Richter became well known for promoting the set with photographs, books, and an internationally touring exhibit of the German Goethe-Institut.
The cover article of the August 1985 "Scientific American" introduced the algorithm for computing the Mandelbrot set to a wide audience. The cover featured an image created by Peitgen, et al. The Mandelbrot set became prominent in the mid-1980s as a computer graphics demo, when personal computers became powerful enough to plot and display the set in high resolution.
The work of Douady and Hubbard coincided with a huge increase in interest in complex dynamics and abstract mathematics, and the study of the Mandelbrot set has been a centerpiece of this field ever since. An exhaustive list of all the mathematicians who have contributed to the understanding of this set since then is beyond the scope of this article, but such a list would notably include Mikhail Lyubich, Curt McMullen, John Milnor, Mitsuhiro Shishikura, and Jean-Christophe Yoccoz.
Formal definition.
The Mandelbrot set formula_8 is defined by a family of complex quadratic polynomials
given by
where formula_11 is a complex parameter. For each formula_11, one considers the behavior of the sequence
obtained by iterating formula_14 starting at critical point formula_15, which either escapes to infinity or stays within a disk of some finite radius. The Mandelbrot set is defined as the set of all points formula_11 such that the above sequence does "not" escape to infinity.
More formally, if formula_17 denotes the "n"th iterate of formula_14 (i.e. formula_14 composed with itself "n" times), the Mandelbrot set is the subset of the complex plane given by
As explained below, it is in fact possible to simplify this definition by taking formula_21.
Mathematically, the Mandelbrot set is just a set of complex numbers. A given complex number "c" either belongs to "M" or it does not. A picture of the Mandelbrot set can be made by coloring all the points formula_11 that belong to "M" black, and all other points white. The more colorful pictures usually seen are generated by coloring points not in the set according to which term in the sequence formula_23 is the first term with an absolute value greater than a certain cutoff value, usually 2. See the section on computer drawings below for more details.
The Mandelbrot set can also be defined as the connectedness locus of the family of polynomials formula_14. That is, it is the subset of the complex plane consisting of those parameters formula_11 for which the Julia set of formula_26 is connected.
Basic properties.
The Mandelbrot set is a compact set, since it is closed and contained in the closed disk of radius 2 around the origin. More specifically, a point formula_11 belongs to the Mandelbrot set if and only if
In other words, if the absolute value of formula_30 ever becomes larger than 2, the sequence will escape to infinity.
The intersection of formula_8 with the real axis is precisely the interval [−2, 1/4]. The parameters along this interval can be put in one-to-one correspondence with those of the
real logistic family,
The correspondence is given by
In fact, this gives a correspondence between the entire parameter space of the logistic family and that of the Mandelbrot set.
As of October 2012, the area of the Mandelbrot is estimated to be ± .
Douady and Hubbard have shown that the Mandelbrot set is connected. In fact, they constructed an explicit conformal isomorphism between the complement of the Mandelbrot set and the complement of the closed unit disk. Mandelbrot had originally conjectured that the Mandelbrot set is disconnected. This conjecture was based on computer pictures generated by programs that are unable to detect the thin filaments connecting different parts of formula_8. Upon further experiments, he revised his conjecture, deciding that formula_8 should be connected.
The dynamical formula for the uniformisation of the complement of the Mandelbrot set, arising from Douady and Hubbard's proof of the connectedness of formula_8, gives rise to external rays of the Mandelbrot set. These rays can be used to study the Mandelbrot set in combinatorial terms and form the backbone of the Yoccoz parapuzzle.
The boundary of the Mandelbrot set is exactly the bifurcation locus of the quadratic family; that is, the set of parameters formula_11 for which the dynamics changes abruptly under small changes of formula_38 It can be constructed as the limit set of a sequence of plane algebraic curves, the "Mandelbrot curves", of the general type known as polynomial lemniscates. The Mandelbrot curves are defined by setting "p"0 = "z", "p""n"+1 = "p""n"2 + "z", and then interpreting the set of points |"p""n"("z")| = 2 in the complex plane as a curve in the real Cartesian plane of degree 2"n"+1 in "x" and "y".
Other properties.
Main cardioid and period bulbs.
Upon looking at a picture of the Mandelbrot set, one immediately notices the large cardioid-shaped region in the center. This "main cardioid"
is the region of parameters formula_11 for which formula_26 has an attracting fixed point. It consists of all parameters of the form
for some formula_42 in the open unit disk.
To the left of the main cardioid, attached to it at the point formula_43, a circular-shaped bulb is visible. This bulb consists of those parameters formula_11 for which formula_26 has an attracting cycle of period 2. This set of parameters is an actual circle, namely that of radius 1/4 around −1.
There are infinitely many other bulbs tangent to the main cardioid: for every rational number formula_46, with "p" and "q" coprime, there is such a bulb that is tangent at the parameter
This bulb is called the "formula_46-bulb" of the Mandelbrot set. It consists of parameters that have an attracting cycle of period formula_49 and combinatorial rotation number formula_46. More precisely, the formula_49 periodic Fatou components containing the attracting cycle all touch at a common point (commonly called the "formula_52-fixed point"). If we label these components formula_53 in counterclockwise orientation, then formula_26 maps the component formula_55 to the component formula_56.
The change of behavior occurring at formula_57 is known as a bifurcation: the attracting fixed point "collides" with a repelling period "q"-cycle. As we pass through the bifurcation parameter into the formula_46-bulb, the attracting fixed point turns into a repelling fixed point (the formula_52-fixed point), and the period "q"-cycle becomes attracting.
Hyperbolic components.
All the bulbs we encountered in the previous section were interior components of
the Mandelbrot set in which the maps formula_26 have an attracting periodic cycle. Such components are called "hyperbolic components".
It is conjectured that these are the "only" interior regions of formula_8. This problem, known as "density of hyperbolicity", may be the most important open problem in the field of complex dynamics. Hypothetical non-hyperbolic components of the Mandelbrot set are often referred to as "queer" or ghost components.
For "real" quadratic polynomials, this question was answered positively in the 1990s independently by Lyubich and by Graczyk and Świątek. (Note that hyperbolic components intersecting the real axis correspond exactly to periodic windows in the Feigenbaum diagram. So this result states that such windows exist near every parameter in the diagram.)
Not every hyperbolic component can be reached by a sequence of direct bifurcations from the main cardioid of the Mandelbrot set. However, such a component "can" be reached by a sequence of direct bifurcations from the main cardioid of a little Mandelbrot copy (see below).
Each of the hyperbolic components has a "centre", which is a point "c" such that the inner Fatou domain for formula_14 has a super-attracting cycle – that is, that the attraction is infinite (see the image here). This means that the cycle contains the critical point 0, so that 0 is iterated back to itself after some iterations. We therefore have that formula_26nformula_64 for some "n". If we call this polynomial formula_65 (letting it depend on "c" instead of "z"), we have that formula_66 and that the degree of formula_65 is formula_68. We can therefore construct the centres of the hyperbolic components by successively solving the equations formula_69. Note that for each step, we get just as many new centres as we have found so far.
Local connectivity.
It is conjectured that the Mandelbrot set is locally connected. This famous conjecture is known as "MLC" (for "Mandelbrot Locally Connected"). By the work of Adrien Douady and John H. Hubbard, this conjecture would result in a simple abstract "pinched disk" model of the Mandelbrot set. In particular, it would imply the important "hyperbolicity conjecture" mentioned above.
The work of Jean-Christophe Yoccoz established local connectivity of the Mandelbrot set at all finitely renormalizable parameters; that is, roughly speaking those contained only in finitely many small Mandelbrot copies. Since then, local connectivity has been proved at many other points of formula_8, but the full conjecture is still open.
Self-similarity.
The little copies of the Mandelbrot set are all slightly different, mostly because of the thin threads connecting them to the main body of the set.
Further results.
The Hausdorff dimension of the boundary of the Mandelbrot set equals 2 as determined by a result of Mitsuhiro Shishikura. It is not known whether the boundary of the Mandelbrot set has positive planar Lebesgue measure.
In the Blum-Shub-Smale model of real computation, the Mandelbrot set is not computable, but its complement is computably enumerable. However, many simple objects ("e.g.", the graph of exponentiation) are also not computable in the BSS model.
At present, it is unknown whether the Mandelbrot set is computable in models of real computation based on computable analysis, which correspond more closely to the intuitive notion of "plotting the set by a computer". Hertling has shown that the Mandelbrot set is computable in this model if the hyperbolicity conjecture is true.
The occurrence of π in the Mandelbrot set was discovered by David Boll in 1991. He found that when looking at the pinch points of the Mandelbrot set, the number of iterations needed for the point (−3/4, ε) before escaping, multiplied by ε, was equal to π. Based on this initial finding, Aaron Klebanoff developed a further test near another pinch point (1/4 + ε, 0) in the Mandelbrot set and found that the number of iterations times the square root of ε was equal to π.
Relationship with Julia sets.
As a consequence of the definition of the Mandelbrot set, there is a close correspondence between the geometry of the Mandelbrot set at a given point and the structure of the corresponding Julia set. For instance, a point is in the Mandelbrot set exactly when the corresponding Julia set is connected.
This principle is exploited in virtually all deep results on the Mandelbrot set. For example, Shishikura proves that, for a dense set of parameters in the boundary of the Mandelbrot set, the Julia set has Hausdorff dimension two, and then transfers this information to the parameter plane. Similarly, Yoccoz first proved the local connectivity of Julia sets, before establishing it for the Mandelbrot set at the corresponding parameters. Adrien Douady phrases this principle as:
Geometry.
For every rational number formula_46, where "p" and "q" are relatively prime, a hyperbolic component of period "q" bifurcates from the main cardioid. The part of the Mandelbrot set connected to the main cardioid at this bifurcation point is called the "p"/"q"-limb. Computer experiments suggest that the diameter of the limb tends to zero like formula_72. The best current estimate known is the "Yoccoz-inequality", which states that the size tends to zero like formula_73.
A period-"q" limb will have "q" − 1 "antennae" at the top of its limb. We can thus determine the period of a given bulb by counting these antennas.
In an attempt to demonstrate that the thickness of the "p"/"q"-limb is zero, David Boll carried out a computer experiment in 1991, where he computed the number of iterations required for the series to converge for z = formula_74 (formula_75 being the location thereof). As the series doesn't converge for the exact value of z = formula_75, the number of iterations required increases with a small ε. It turns out that multiplying the value of ε with the number of iterations required yields an approximation of π that becomes better for smaller ε. For example, for ε = 0.0000001 the number of iterations is 31415928 and the product is 3.1415928.
Image gallery of a zoom sequence.
The Mandelbrot set shows more intricate detail the closer one looks or magnifies the image, usually called "zooming in". The following example of an image sequence zooming to a selected "c" value gives an impression of the infinite richness of different geometrical structures and explains some of their typical rules.
The magnification of the last image relative to the first one is about 1010 to 1. Relating to an ordinary monitor, it represents a section of a Mandelbrot set with a diameter of 4 million kilometres. Its border would show an astronomical number of different fractal structures.
The seahorse "body" is composed by 25 "spokes" consisting of two groups of 12 "spokes" each and one "spoke" connecting to the main cardioid. These two groups can be attributed by some kind of metamorphosis to the two "fingers" of the "upper hand" of the Mandelbrot set; therefore, the number of "spokes" increases from one "seahorse" to the next by 2; the "hub" is a so-called Misiurewicz point. Between the "upper part of the body" and the "tail" a distorted small copy of the Mandelbrot set called satellite may be recognized.
The islands above seem to consist of infinitely many parts like Cantor sets, as is actually the case for the corresponding Julia set "Jc". However, they are connected by tiny structures, so that the whole represents a simply connected set. The tiny structures meet each other at a satellite in the center that is too small to be recognized at this magnification. The value of "c" for the corresponding "Jc" is not that of the image center but, relative to the main body of the Mandelbrot set, has the same position as the center of this image relative to the satellite shown in the 6th zoom step.
Generalizations.
Multibrot sets are bounded sets found in the complex plane for members of the general monic univariate polynomial family of recursions
For integer d, these sets are connectedness loci for the Julia sets built from the same formula. The full cubic connectedness map has also been studied; here one considers the two-parameter recursion formula_78, whose two critical points are the complex square roots of the parameter "k". A point is in the map if either critical point is stable.
For general families of holomorphic functions, the "boundary" of the Mandelbrot set generalizes to the bifurcation locus, which is a natural object to study even when the connectedness locus is not useful.
Other, non-analytic, mappings.
Of particular interest is the
tricorn fractal, the connectedness locus of the anti-holomorphic family
The tricorn (also sometimes called the "Mandelbar set") was encountered by Milnor in his study of parameter slices of real cubic polynomials. It is "not" locally connected. This property is inherited by the connectedness locus of real cubic polynomials.
Another non-analytic generalization is the Burning Ship fractal, which is obtained by iterating the mapping
The Multibrot set is obtained by varying the value of the exponent "d". The article has a video that shows the development from "d" = 0 to 7 at which point there are 6 i.e. ("d" − 1) lobes around the perimeter. A similar development with negative exponents results in (1 − "d") clefts on the inside of a ring.
Computer drawings.
There are many programs used to generate the Mandelbrot set and other fractals, some of which are described in fractal-generating software. These programs use a variety of algorithms to determine the color of individual pixels and achieve efficient computation.
Escape time algorithm.
The simplest algorithm for generating a representation of the Mandelbrot set is known as the "escape time" algorithm. A repeating calculation is performed for each "x", "y" point in the plot area and based on the behavior of that calculation, a color is chosen for that pixel.
The "x" and "y" locations of each point are used as starting values in a repeating, or iterating calculation (described in detail below). The result of each iteration is used as the starting values for the next. The values are checked during each iteration to see whether they have reached a critical "escape" condition, or "bailout". If that condition is reached, the calculation is stopped, the pixel is drawn, and the next "x", "y" point is examined. For some starting values, escape occurs quickly, after only a small number of iterations. For starting values very close to but not in the set, it may take hundreds or thousands of iterations to escape. For values within the Mandelbrot set, escape will never occur. The programmer or user must choose how much iteration, or "depth", they wish to examine. The higher the maximal number of iterations, the more detail and subtlety emerge in the final image, but the longer time it will take to calculate the fractal image.
Escape conditions can be simple or complex. Because no complex number with a real or imaginary part greater than 2 can be part of the set, a common bailout is to escape when either coefficient exceeds 2. A more computationally complex method that detects escapes sooner, is to compute distance from the origin using the Pythagorean theorem, i.e., to determine the absolute value, or "modulus", of the complex number. If this value exceeds 2, the point has reached escape. More computationally intensive rendering variations include the Buddhabrot method, which finds escaping points and plots their iterated coordinates.
The color of each point represents how quickly the values reached the escape point. Often black is used to show values that fail to escape before the iteration limit, and gradually brighter colors are used for points that escape. This gives a visual representation of how many cycles were required before reaching the escape condition.
To render such an image, the region of the complex plane we are considering is subdivided into a certain number of pixels. To color any such pixel, let formula_11 be the midpoint of that pixel. We now iterate the critical point 0 under formula_26, checking at each step whether the orbit point has modulus larger than 2. When this is the case, we know that formula_11 does not belong to the Mandelbrot set, and we color our pixel according to the number of iterations used to find out. Otherwise, we keep iterating up to a fixed number of steps, after which we decide that our parameter is "probably" in the Mandelbrot set, or at least very close to it, and color the pixel black.
In pseudocode, this algorithm would look as follows. The algorithm does not use complex numbers and manually simulates complex-number operations using two real numbers, for those who do not have a complex data type. The program may be simplified if the programming language includes complex-data-type operations.
0, "y"0), not ("x","y"). -->
Here, relating the pseudocode to formula_11, formula_85 and formula_26:
and so, as can be seen in the pseudocode in the computation of "x" and "y":
To get colorful images of the set, the assignment of a color to each value of the number of executed iterations can be made using one of a variety of functions (linear, exponential, etc.). One practical way, without slowing down calculations, is to use the number of executed iterations as an entry to a look-up color palette table initialized at startup. If the color table has, for instance, 500 entries, then the color selection is "n" mod 500, where "n" is the number of iterations.
Histogram coloring.
A more accurate coloring method involves using a histogram, which keeps track of how many pixels reached each iteration number, from 1 to "n". This method will equally distribute colors to the same overall area, and, importantly, is independent of the maximal number of iterations chosen.
First, create an array of size "n". For each pixel, which took "i" iterations, find the "i"th element and increment it. This creates the histogram during computation of the image. Then, when finished, perform a second "rendering" pass over each pixel, utilizing the completed histogram. If you had a continuous color palette ranging from 0 to 1, you could find the normalized color of each pixel as follows, using the variables from above.
This method may be combined with the smooth coloring method below for more aesthetically pleasing images.
Continuous (smooth) coloring.
The escape time algorithm is popular for its simplicity. However, it creates bands of color, which, as a type of aliasing, can detract from an image's aesthetic value. This can be improved using an algorithm known as "normalized iteration count", which provides a smooth transition of colors between iterations. The algorithm associates a real number formula_92 with each value of "z" by using the connection of the iteration number with the potential function. This function is given by
where "z""n" is the value after "n" iterations and "P" is the power for which "z" is raised to in the Mandelbrot set equation ("z""n"+1 = "z"n"""P" + "c", "P" is generally 2).
If we choose a large bailout radius "N" (e.g., 10100), we have that
for some real number formula_95, and this is
and as "n" is the first iteration number such that |"z""n"| > "N", the number we subtract from "n" is in the interval [0, 1).
For the coloring we must have a cyclic scale of colors (constructed mathematically, for instance) and containing "H" colors numbered from 0 to "H" − 1 ("H" = 500, for instance). We multiply the real number formula_95 by a fixed real number determining the density of the colors in the picture, take the integral part of this number modulo "H", and use it to look up the corresponding color in the color table.
For example, modifying the above pseudocode and also using the concept of linear interpolation would yield
Distance estimates.
One can compute the distance from point "c" (in exterior or interior) to nearest point on the boundary of the Mandelbrot set.
Exterior distance estimation.
The proof of the connectedness of the Mandelbrot set in fact gives a formula for the uniformizing map of the complement of formula_8 (and the derivative of this map). By the Koebe 1/4 theorem, one can then estimate the distance between the midpoint of our pixel and the Mandelbrot set up to a factor of 4.
In other words, provided that the maximal number of iterations is sufficiently high, one obtains a picture of the Mandelbrot set with the following properties:
The distance estimate "b" of a pixel "c" (a complex number) from the Mandelbrot set is given by
where 
The idea behind this formula is simple: When the equipotential lines for the potential function formula_111 lie close, the number formula_112 is large, and conversely, therefore the equipotential lines for the function formula_113 should lie approximately regularly.
From a mathematician's point of view, this formula only works in limit where "n" goes to infinity, but very reasonable estimates can be found with just a few additional iterations after the main loop exits.
Once "b" is found, by the Koebe 1/4-theorem, we know there's no point of the Mandelbrot set with distance from "c" smaller than "b/4".
The distance estimation can be used for drawing of the boundary of the Mandelbrot set, see the article Julia set.
Interior distance estimation.
It is also possible to estimate the distance of a limitly periodic (i.e., inner) point to the boundary of the Mandelbrot set. The estimate is given by
where
Analogous to the exterior case, once "b" is found, we know that all points within the distance of "b"/4 from "c" are inside the Mandelbrot set.
There are two practical problems with the interior distance estimate: first, we need to find formula_123 precisely, and second, we need to find formula_115 precisely.
The problem with formula_123 is that the convergence to formula_123 by iterating formula_14 requires, theoretically, an infinite number of operations.
The problem with any given formula_115 is that, sometimes, due to rounding errors, a period is falsely identified to be an integer multiple of the real period (e.g., a period of 86 is detected, while the real period is only 43=86/2). In such case, the distance is overestimated, i.e., the reported radius could contain points outside the Mandelbrot set.
Optimizations.
Cardioid / bulb checking.
One way to improve calculations is to find out beforehand whether the given point lies within the cardioid or in the period-2 bulb. Before passing the complex value through the escape time algorithm, first check that:
where "x" represents the real value of the point and "y" the imaginary value. The first two equations determine that the point is within the cardioid, the last the period-2 bulb.
The cardioid test can equivalently be performed without the square root:
3rd- and higher-order buds do not have equivalent tests, because they are not perfectly circular. However, it is possible to find whether the points are within circles inscribed within these higher-order bulbs, preventing many, though not all, of the points in the bulb from being iterated.
Periodicity checking.
To prevent having to do huge numbers of iterations for points in the set, one can perform periodicity checking. Check whether a point reached in iterating a pixel has been reached before. If so, the pixel cannot diverge and must be in the set.
Periodicity checking is, of course, a trade-off. The need to remember points costs memory and "data management" instructions, whereas it saves "computational" instructions.
However, checking against only one previous iteration can detect many periods with little performance overhead. For example, within the while loop of the pseudocode above, make the following modifications.
Border tracing / edge checking.
It can be shown that if a solid shape can be drawn on the Mandelbrot set, with all the border colors being the same, then the shape can be filled in with that color. This is a result of the Mandelbrot set being simply connected. Boundary tracing works by following the lemniscates of the various iteration levels (colored bands) all around the set, and then filling the entire band at once. This can be a good speed increase, because it means that large numbers of points can be skipped.
A similar method operating on the same principle uses rectangles instead of arbitrary border shapes. It is usually faster than boundary tracing because it requires fewer calculations to work out the rectangle. It is inefficient, however, because boundaries are not rectangular, and so some areas can be missed. This issue can be minimized by created a recursive algorithm that, if a rectangle border fails, will subdivide it into four smaller rectangles and test those, and either fill each or subdivide again and repeat the process.
However, this only works using discrete colors in the escape time algorithm. It will not work for smooth/continuous coloring.
Perturbation theory and series approximation.
Very highly magnified images require more than the standard 64–128 or so bits of precision that most hardware floating-point units provide, requiring renderers to use slow "bignum" or "arbitrary-precision" math libraries to calculate. However, this can be sped up by the exploitation of perturbation theory. Given
as the iteration, and a small epsilon, it is the case that
or
so if one defines
one can calculate a single point (e.g. the center of an image) using high-precision arithmetic ("z"), giving a "reference orbit", and then compute many points around it in terms of various initial offsets epsilon-zero plus the above iteration for epsilon. For most iterations, epsilon does not need more than 16 significant figures, and consequently hardware floating-point may be used to get a mostly accurate image. There will often be some areas where the orbits of points diverge enough from the reference orbit that extra precision is needed on those points, or else additional local high-precision-calculated reference orbits are needed. By measuring the orbit distance between the reference point and the point calculated with low precision, it can be detected that it is not possible to calculate the point correctly, and the calculation can be stopped. These incorrect points can later be re-calculated e.g. from another closer reference point.
Further, it is possible to approximate the starting values for the low-precision points with a truncated Taylor series, which often enables a significant amount of iterations to be skipped.
Renderers implementing these techniques are publicly available and offer speedups for highly magnified images by around two orders of magnitude.

</doc>
<doc id="19565" url="https://en.wikipedia.org/wiki?curid=19565" title="Michael Mann (director)">
Michael Mann (director)

Michael Kenneth Mann (born February 5, 1943) is an American film director, screenwriter, and producer.
For his work, he has received nominations from international organizations and juries, including those at the British Academy of Film and Television Arts, Cannes and the Academy of Motion Picture Arts and Sciences. His major films include the period epic "The Last of the Mohicans" (1992), the drama "The Insider" (1999), the biopic "Ali" (2001), and the crime films "Heat" (1995) "Collateral" (2004) and "Public Enemies" (2009).
"Total Film" ranked Mann No. 28 on its list of the 100 Greatest Directors Ever, "Sight and Sound" ranked him No. 5 on their list of the 10 Best Directors of the Last 25 Years, and "Entertainment Weekly" ranked Mann No. 8 on their 25 Greatest Active Film Directors list.
Early life and education.
Mann was born on February 5, 1943 in Chicago, Illinois, of Jewish ancestry, the son of grocers Esther and Jack Mann.
He received a B.A. in English at the University of Wisconsin–Madison where he developed interests in history, philosophy and architecture. It was at this time that he first saw Stanley Kubrick's "Dr. Strangelove" and fell in love with movies. In a recent "L.A. Weekly" interview, he describes the film's impact on him: "It said to my whole generation of filmmakers that you could make an individual statement of high integrity and have that film be successfully seen by a mass audience all at the same time. In other words, you didn't have to be making "Seven Brides for Seven Brothers" if you wanted to work in the mainstream film industry, or be reduced to niche filmmaking if you wanted to be serious about cinema. So that's what Kubrick meant, aside from the fact that "Strangelove" was a revelation." His daughter Ami Canaan Mann is also a film director and producer.
Career.
1960s–1970s.
Mann later moved to London in the mid 1960s to go to graduate school in cinema. He went on to receive a graduate degree at the London Film School. He spent seven years in the United Kingdom going to film school and then working on commercials along with contemporaries Alan Parker, Ridley Scott and Adrian Lyne. In 1968, footage he shot of the Paris student revolt for a documentary, "Insurrection", aired on NBC's "First Tuesday" news program and he developed his '68 experiences into the short film "Jaunpuri" which won the Jury Prize at Cannes in 1970.
Mann returned to United States after divorcing his first wife in 1971. He went on to direct a road trip documentary, "17 Days Down the Line". Three years later, "Hawaii Five-O" veteran Robert Lewin gave Mann a shot and a crash course on television writing and story structure. Mann wrote four episodes of "Starsky and Hutch" (three in the first series and one in the second ) and the pilot episode for "Vega$". Around this time, he worked on a show called "Police Story" with cop-turned-novelist Joseph Wambaugh. "Police Story" concentrated on the detailed realism of a real cop's life and taught Mann that first-hand research was essential to bring authenticity to his work. His first feature movie was a television special called "The Jericho Mile", which was released theatrically in Europe. It won the Emmy for best MOW in 1979 and the DGA Best Director award.
1980s.
His television work also includes being the executive producer on "Miami Vice" and "Crime Story". Contrary to popular belief, he was not the creator of these shows, but the executive producer and showrunner. They were produced by his production company and his cinematic influence is felt throughout each show in terms of casting and style. Mann is now known primarily as a feature film director. He has a distinctive style that is reflected in his works: his trademarks are intricate scene setups, during "Miami Vice" to such an extent that a whole scene was completely color-coordinated, from props to backgrounds to actors' wardrobes, as well as powerfully-lit night scenes and combining exterior filming in such a way that shots of completely unrelated filming locations can appear as being of the same building or landmark. In terms of sound, he is known for unusual scores, such as Tangerine Dream in "Thief" or the new-age score to "Manhunter". Dante Spinotti is a frequent cinematographer of Mann's pictures.
Mann's first cinema feature as director was "Thief" (1981) starring James Caan. His next film "The Keep" (1983), a supernatural thriller set in Nazi-occupied Romania, was an uncharacteristic choice. Though it was a commercial flop, the film has since attained cult status amongst fans.
In 1986, Mann was the first to bring Thomas Harris's character of serial killer Hannibal Lecter to the screen with "Manhunter", his adaptation of the novel "Red Dragon", which starred Brian Cox as a more down-to-earth Hannibal. The story was remade less than 20 years after it came out by Brett Ratner because Anthony Hopkins reprisal of the role in Ridley Scott's "Hannibal" had made the character a highly lucrative property. In an interview on the "Manhunter" DVD, star William Petersen comments that because Mann is so focused on his creations, it takes several years for him to complete a film; Petersen believes that this is why Mann does not make films very often.
1990s.
He gained widespread recognition in 1992 for his film adaptation of James Fenimore Cooper's novel into the epic film "Last of the Mohicans". His biggest critical successes in the 1990s began with the release of "Heat" in 1995 and "The Insider" in 1999. The films, which featured Al Pacino with Robert De Niro in "Heat" and Russell Crowe in "The Insider", showcased Mann's cinematic style and adeptness at creating rich, complex storylines as well as directing actors. "The Insider" was nominated for seven Academy Awards as a result, including a nomination for Mann's direction.
2000s.
With his next film "Ali" starring Will Smith in 2001, he started experimenting with digital cameras. The film helped catapult Will Smith to greater fame, and he was nominated for an Academy Award for his performance. For his crime film "Collateral", which cast Tom Cruise against type by giving him the role of a hitman, Mann shot all of the exterior scenes digitally so that he could achieve more depth and detail during the night scenes while shooting most of the interiors on film stock. In 2004, Mann produced "The Aviator", based on the life of Howard Hughes, which he had developed with Leonardo DiCaprio. "The Aviator" was nominated for an Academy Award for Best Picture but lost to "Million Dollar Baby". After "Collateral", Mann directed the film adaptation of "Miami Vice" which he also executive produced. It stars a completely new cast with Colin Farrell as Don Johnson's character Sonny Crockett, and Jamie Foxx filling Philip Michael Thomas' shoes.
Mann served as a producer and Peter Berg as director for "The Kingdom" and "Hancock". "Hancock" stars Will Smith as a hard-drinking superhero who has fallen out of favor with the public and who begins to have a relationship with the wife (Charlize Theron) of a public relations expert (Jason Bateman), who is helping him to repair his image. Mann also makes a cameo appearance in the film as an executive. In the fall of 2007, Mann directed two commercials for Nike. The ad campaign "Leave Nothing" features football action scenes with current NFL players Shawne Merriman and Steven Jackson.
In 2009, Mann wrote and directed "Public Enemies" for Universal Pictures, about the Depression-era crime wave, based on Brian Burrough's nonfiction book, "Public Enemies: America's Greatest Crime Wave and the Birth of the FBI, 1933–34". It starred Johnny Depp and Christian Bale. Depp played John Dillinger in the film, and Bale played Melvin Purvis, the FBI agent in charge of capturing Dillinger.
2010s.
In January 2010 it was reported by "Variety" that Mann, alongside David Milch, would serve as co-executive producer of new TV series "Luck". The series was an hour-long HBO production, and Mann directed the series' pilot. Although initially renewed for a second season after the airing of the pilot, it was eventually cancelled due to the death of three horses during production.
On February 14, 2013, it was announced that Mann had been developing an untitled thriller film with screenwriter Morgan Davis Foehl for over a year, for Legendary Pictures. In May 2013, Mann started filming the thriller, named "Blackhat", in Los Angeles, Kuala Lumpur, Hong Kong and Jakarta. The film, starring Chris Hemsworth as a hacker who gets released from prison to pursue a cyberterrorist across the globe, was released on January 16, 2015. It was a critical and commercial disaster, although many critics included it in their year-end "best-of" lists.
In January 2015, it was reported in "The New Yorker" that Mann is developing a film about Ferrari founder Enzo Ferrari.Christian Bale was originally cast to play the title character, but later dropped out citing health concerns over the weight gain required to play the role. Financing is coming from Vendian Entertainment in the US and China's Bliss Media. Distribution rights have already been purchased by Paramount for the US and Bliss Media for China.
Filming style.
Mann's films often contain one to four deadpan male protagonists who are expert in some profession. Importantly, his films often involve a tragic rather than a happy ending, such as in "Miami Vice", when of the two undercover police officers, one has his girlfriend (also an undercover officer) come out of a coma and the other tearfully separates from his romantic interest. Mann's films contain fast-paced, artful scenes that strongly depend on powerful music, where often two opposing sides intermix, such as undercover policework and undercover drug trafficking, so that it is hard to distinguish between the two. For example, in "Heat", the police detective invites the criminal to meet for coffee, where they discuss their affairs like old business partners. Often it is hard to distinguish between opposing sides (police vs. criminals, etc.), where the actions, dress, and mannerisms of the characters are extremely similar. Also, Mann's work often involves landscapes and modes where the heroic protagonists occupy a somewhat secret world, away from ordinary concerns (law, life and death, money, daily-life survival duties, family duties, and so on), where the secret world may or may not coincide with ordinary reality. Protagonists often find impassioned romantic interests which are severed under tragic situations near the end of the film ("Last of the Mohicans", "Heat", "Collateral", "Miami Vice, Public Enemies"). Overall, Mann's films mix artistry (via music, stylishness and emotional intensity) with sexuality, strong violence, humorless noir-like stoicism, and complex plot twists.
Advertising.
Mann directed the 2002 "Lucky Star" advertisement for Mercedes-Benz, which took the form of a film trailer for a purported thriller featuring Benicio del Toro. Mann also directed the 2008 promotional video for Ferrari's California sports car. In 2009 Mann also directed a commercial for Nike that featured several stylistic cues, most notably the use of "Promontory" from the soundtrack of "The Last of the Mohicans".
Reception.
Awards and honors.
Mann received an Emmy in 1979 for Outstanding Writing in a Limited Series or a Special for "The Jericho Mile". The following year he was honored by the Directors Guild of America for Outstanding Directorial Achievement for "The Jericho Mile". In 1990, he won another Emmy for Outstanding Miniseries for "". Mann was the recipient of the Humanitas Prize and the Writers Guild of America's Paul Selvin Award in 2000 for "The Insider". In 2005, he received the BAFTA Film Award for co-producing "The Aviator".
To date he has received four Academy Award nominations: in 2000, the Best Adapted Screenplay, Best Director and Best Motion Picture of the Year all for "The Insider", in 2005 Mann received nomination for production of Scorsese's "The Aviator".

</doc>
<doc id="19566" url="https://en.wikipedia.org/wiki?curid=19566" title="Main-group element">
Main-group element

In chemistry and atomic physics, the main group is the group of elements whose lightest members are represented by helium, lithium, beryllium, boron, carbon, nitrogen, oxygen, fluorine, and neon as arranged in the periodic table of the elements. The main group includes the elements (except hydrogen) in groups 1 and 2 (s-block), and groups 13 to 18 (p-block). Group 12 elements are usually considered to be transition metals; however, zinc (Zn), cadmium (Cd), and mercury (Hg) share some properties of both groups, and some scientists believe they should be included in the main group.
In older nomenclature the main-group elements are groups IA and IIA, and groups IIIB to 0 (CAS groups IIIA to VIIIA). Group 12 is labelled as group IIB in both systems.
Main-group elements (with some of the lighter transition metals) are the most abundant elements on Earth, in the Solar System, and in the Universe. They are sometimes also called the representative elements.

</doc>
<doc id="19567" url="https://en.wikipedia.org/wiki?curid=19567" title="Microscopy">
Microscopy

Microscopy is the technical field of using microscopes to view objects and areas of objects that cannot be seen with the naked eye (objects that are not within the resolution range of the normal eye). There are three well-known branches of microscopy: optical, electron, and scanning probe microscopy.
Optical and electron microscopy involve the diffraction, reflection, or refraction of electromagnetic radiation/electron beams interacting with the specimen, and the collection of the scattered radiation or another signal in order to create an image. This process may be carried out by wide-field irradiation of the sample (for example standard light microscopy and transmission electron microscopy) or by scanning of a fine beam over the sample (for example confocal laser scanning microscopy and scanning electron microscopy). Scanning probe microscopy involves the interaction of a scanning probe with the surface of the object of interest. The development of microscopy revolutionized biology, gave rise to the field of histology and so remains an essential technique in the life and physical sciences.
Optical microscopy.
Optical or light microscopy involves passing visible light transmitted through or reflected from the sample through a single or multiple lenses to allow a magnified view of the sample. The resulting image can be detected directly by the eye, imaged on a photographic plate or captured digitally. The single lens with its attachments, or the system of lenses and imaging equipment, along with the appropriate lighting equipment, sample stage and support, makes up the basic light microscope. The most recent development is the digital microscope, which uses a CCD camera to focus on the exhibit of interest. The image is shown on a computer screen, so eye-pieces are unnecessary.
Limitations.
Limitations of standard optical microscopy (bright field microscopy) lie in three areas;
Live cells in particular generally lack sufficient contrast to be studied successfully, since the internal structures of the cell are colourless and transparent. The most common way to increase contrast is to stain the different structures with selective dyes, but this often involves killing and fixing the sample. Staining may also introduce artifacts, apparent structural details that are caused by the processing of the specimen and are thus not legitimate features of the specimen. In general, these techniques make use of differences in the refractive index of cell structures. It is comparable to looking through a glass window: you (bright field microscopy) don't see the glass but merely the dirt on the glass. There is a difference, as glass is a denser material, and this creates a difference in phase of the light passing through. The human eye is not sensitive to this difference in phase, but clever optical solutions have been thought out to change this difference in phase into a difference in amplitude (light intensity).
Techniques.
In order to improve specimen contrast or highlight certain structures in a sample special techniques must be used. A huge selection of microscopy techniques are available to increase contrast or label a sample.
Bright field.
Bright field microscopy is the simplest of all the light microscopy techniques. Sample illumination is via transmitted white light, i.e. illuminated from below and observed from above. Limitations include low contrast of most biological samples and low apparent resolution due to the blur of out of focus material. The simplicity of the technique and the minimal sample preparation required are significant advantages.
Oblique illumination.
The use of oblique (from the side) illumination gives the image a 3-dimensional appearance and can highlight otherwise invisible features. A more recent technique based on this method is "Hoffmann's modulation contrast", a system found on inverted microscopes for use in cell culture. Oblique illumination suffers from the same limitations as bright field microscopy (low contrast of many biological samples; low apparent resolution due to out of focus objects).
Dark field.
Dark field microscopy is a technique for improving the contrast of unstained, transparent specimens. Dark field illumination uses a carefully aligned light source to minimize the quantity of directly transmitted (unscattered) light entering the image plane, collecting only the light scattered by the sample. Dark field can dramatically improve image contrast – especially of transparent objects – while requiring little equipment setup or sample preparation. However, the technique suffers from low light intensity in final image of many biological samples, and continues to be affected by low apparent resolution.
"Rheinberg illumination" is a special variant of dark field illumination in which transparent, colored filters are inserted just before the condenser so that light rays at high aperture are differently colored than those at low aperture (i.e. the background to the specimen may be blue while the object appears self-luminous red). Other color combinations are possible but their effectiveness is quite variable.
Dispersion staining.
Dispersion staining is an optical technique that results in a colored image of a colorless object. This is an optical staining technique and requires no stains or dyes to produce a color effect. There are five different microscope configurations used in the broader technique of dispersion staining. They include brightfield Becke line, oblique, darkfield, phase contrast, and objective stop dispersion staining.
Phase contrast.
More sophisticated techniques will show proportional differences in optical density. Phase contrast is a widely used technique that shows differences in refractive index as difference in contrast. It was developed by the Dutch physicist Frits Zernike in the 1930s (for which he was awarded the Nobel Prize in 1953). The nucleus in a cell for example will show up darkly against the surrounding cytoplasm. Contrast is excellent; however it is not for use with thick objects. Frequently, a halo is formed even around small objects, which obscures detail. The system consists of a circular annulus in the condenser, which produces a cone of light. This cone is superimposed on a similar sized ring within the phase-objective. Every objective has a different size ring, so for every objective another condenser setting has to be chosen. The ring in the objective has special optical properties: it, first of all, reduces the direct light in intensity, but more importantly, it creates an artificial phase difference of about a quarter wavelength. As the physical properties of this direct light have changed, interference with the diffracted light occurs, resulting in the phase contrast image. One disadvantage of phase-contrast microscopy is halo formation (halo-light ring).
Differential interference contrast.
Superior and much more expensive is the use of interference contrast. Differences in optical density will show up as differences in relief. A nucleus within a cell will actually show up as a globule in the most often used differential interference contrast system according to Georges Nomarski. However, it has to be kept in mind that this is an "optical effect", and the relief does not necessarily resemble the true shape. Contrast is very good and the condenser aperture can be used fully open, thereby reducing the depth of field and maximizing resolution.
The system consists of a special prism (Nomarski prism, Wollaston prism) in the condenser that splits light in an ordinary and an extraordinary beam. The spatial difference between the two beams is minimal (less than the maximum resolution of the objective). After passage through the specimen, the beams are reunited by a similar prism in the objective.
In a homogeneous specimen, there is no difference between the two beams, and no contrast is being generated. However, near a refractive boundary (say a nucleus within the cytoplasm), the difference between the ordinary and the extraordinary beam will generate a relief in the image. Differential interference contrast requires a polarized light source to function; two polarizing filters have to be fitted in the light path, one below the condenser (the polarizer), and the other above the objective (the analyzer).
Note: In cases where the optical design of a microscope produces an appreciable lateral separation of the two beams we have the case of classical interference microscopy, which does not result in relief images, but can nevertheless be used for the quantitative determination of mass-thicknesses of microscopic objects.
Interference reflection.
An additional technique using interference is interference reflection microscopy (also known as reflected interference contrast, or RIC). It relies on cell adhesion to the slide to produce an interference signal. If there is no cell attached to the glass, there will be no interference.
Interference reflection microscopy can be obtained by using the same elements used by DIC, but without the prisms. Also, the light that is being detected is reflected and not transmitted as it is when DIC is employed.
Fluorescence.
When certain compounds are illuminated with high energy light, they emit light of a lower frequency. This effect is known as fluorescence. Often specimens show their characteristic autofluorescence image, based on their chemical makeup.
This method is of critical importance in the modern life sciences, as it can be extremely sensitive, allowing the detection of single molecules. Many different fluorescent dyes can be used to stain different structures or chemical compounds. One particularly powerful method is the combination of antibodies coupled to a fluorophore as in immunostaining. Examples of commonly used fluorophores are fluorescein or rhodamine.
The antibodies can be tailor-made for a chemical compound. For example, one strategy often in use is the artificial production of proteins, based on the genetic code (DNA). These proteins can then be used to immunize rabbits, forming antibodies which bind to the protein. The antibodies are then coupled chemically to a fluorophore and used to trace the proteins in the cells under study.
Highly efficient fluorescent proteins such as the green fluorescent protein (GFP) have been developed using the molecular biology technique of gene fusion, a process that links the expression of the fluorescent compound to that of the target protein. This combined fluorescent protein is, in general, non-toxic to the organism and rarely interferes with the function of the protein under study. Genetically modified cells or organisms directly express the fluorescently tagged proteins, which enables the study of the function of the original protein in vivo.
Growth of protein crystals results in both protein and salt crystals. Both are colorless and microscopic. Recovery of the protein crystals requires imaging which can be done by the intrinsic fluorescence of the protein or by using transmission microscopy. Both methods require an ultraviolet microscope as protein absorbs light at 280 nm. Protein will also fluorescence at approximately 353 nm when excited with 280 nm light.
Since fluorescence emission differs in wavelength (color) from the excitation light, an ideal fluorescent image shows only the structure of interest that was labeled with the fluorescent dye. This high specificity led to the widespread use of fluorescence light microscopy in biomedical research. Different fluorescent dyes can be used to stain different biological structures, which can then be detected simultaneously, while still being specific due to the individual color of the dye.
To block the excitation light from reaching the observer or the detector, filter sets of high quality are needed. These typically consist of an excitation filter selecting the range of excitation wavelengths, a dichroic mirror, and an emission filter blocking the excitation light. Most fluorescence microscopes are operated in the Epi-illumination mode (illumination and detection from one side of the sample) to further decrease the amount of excitation light entering the detector.
An example of fluorescence microscopy today is two-photon or multi-photon imaging. Two photon imaging allows imaging of living tissues up to a very high depth by enabling greater excitation light penetration and reduced background emission signal. A recent development using this technique is called Superpenetration Multi Photon Microscopy, which allows imaging at greater depths than two-photon or multi-photon imaging would by implementing adaptive optics into the system. Pioneered by the Cui Lab at Howard Hughes Medical Center and recently reported by Boston University on focusing light through static and dynamic strongly scattering media. By utilizing adaptive optics,it has allowed the optical wavelength control needed for transformative impacts on deep tissue imaging.
See also:
total internal reflection fluorescence microscope
Neuroscience
Confocal.
Confocal microscopy uses a scanning point of light and a pinhole to prevent out of focus light from reaching the detector. Compared to full sample illumination, confocal microscopy gives slightly higher resolution, and significantly improves optical sectioning. Confocal microscopy is, therefore, commonly used where 3D structure is important.
Single plane illumination microscopy and light sheet fluorescence microscopy.
Using a plane of light formed by focusing light through a cylindrical lens at a narrow angle or by scanning a line of light in a plane perpendicular to the axis of objective, high resolution optical sections can be taken. Single plane illumination, or light sheet illumination, is also accomplished using beam shaping techniques incorporating multiple-prism beam expanders. The images are captured by CCDs. These variants allow very fast and high signal to noise ratio image capture.
Wide-field multiphoton microscopy.
Wide-field multiphoton microscopy refers to an optical non-linear imaging technique tailored for ultrafast imaging in which a large area of the object is illuminated and imaged without the need for scanning. High intensities are required to induce non-linear optical processes such as two-photon fluorescence or second harmonic generation. In scanning multiphoton microscopes the high intensities are achieved by tightly focusing the light, and the image is obtained by stage- or beam-scanning the sample. In wide-field multiphoton microscopy the high intensities are best achieved using an optically amplified pulsed laser source to attain a large field of view (~100 µm). The image in this case is obtained as a single frame with a CCD without the need of scanning, making the technique particularly useful to visualize dynamic processes simultaneously across the object of interest. With wide-field multiphoton microscopy the frame rate can be increased up to a 1000-fold compared to multiphoton scanning microscopy.
Deconvolution.
Fluorescence microscopy is a powerful technique to show specifically labeled structures within a complex environment and to provide three-dimensional information of biological structures. However, this information is blurred by the fact that, upon illumination, all fluorescently labeled structures emit light, irrespective of whether they are in focus or not. So an image of a certain structure is always blurred by the contribution of light from structures that are out of focus. This phenomenon results in a loss of contrast especially when using objectives with a high resolving power, typically oil immersion objectives with a high numerical aperture.
However, blurring is not caused by random processes, such as light scattering, but can be well defined by the optical properties of the image formation in the microscope imaging system. If one considers a small fluorescent light source (essentially a bright spot), light coming from this spot spreads out further from our perspective as the spot becomes more out of focus. Under ideal conditions, this produces an "hourglass" shape of this point source in the third (axial) dimension. This shape is called the point spread function (PSF) of the microscope imaging system. Since any fluorescence image is made up of a large number of such small fluorescent light sources, the image is said to be "convolved by the point spread function".
Knowing this point spread function means that it is possible to reverse this process to a certain extent by computer-based methods commonly known as deconvolution microscopy. There are various algorithms available for 2D or 3D deconvolution. They can be roughly classified in "nonrestorative" and "restorative" methods. While the nonrestorative methods can improve contrast by removing out-of-focus light from focal planes, only the restorative methods can actually reassign light to its proper place of origin. Processing fluorescent images in this manner can be an advantage over directly acquiring images without out-of-focus light, such as images from confocal microscopy, because light signals otherwise eliminated become useful information. For 3D deconvolution, one typically provides a series of images taken from different focal planes (called a Z-stack) plus the knowledge of the PSF, which can be derived either experimentally or theoretically from knowing all contributing parameters of the microscope.
Sub-diffraction techniques.
A multitude of super-resolution microscopy techniques have been developed in recent times which circumvent the diffraction barrier.
This is mostly achieved by imaging a sufficiently static sample multiple times and either modifying the excitation light or observing stochastic changes in the image.
Knowledge of and chemical control over fluorophore photophysics is at the core of these techniques, by which resolutions of ~20 nanometers are regularly obtained.
Serial time-encoded amplified microscopy.
Serial time encoded amplified microscopy (STEAM) is an imaging method that provides ultrafast shutter speed and frame rate, by using optical image amplification to circumvent the fundamental trade-off between sensitivity and speed, and a single-pixel photodetector to eliminate the need for a detector array and readout time limitations The method is at least 1000 times faster than the state-of-the-art CCD and CMOS cameras. Consequently, it is potentially useful for a broad range of scientific, industrial, and biomedical applications that require high image acquisition rates, including real-time diagnosis and evaluation of shockwaves, microfluidics, MEMS, and laser surgery. 
Extensions.
Most modern instruments provide simple solutions for micro-photography and image recording electronically. However such capabilities are not always present and the more experienced microscopist will, in many cases, still prefer a hand drawn image to a photograph. This is because a microscopist with knowledge of the subject can accurately convert a three-dimensional image into a precise two-dimensional drawing. In a photograph or other image capture system however, only one thin plane is ever in good focus.
The creation of careful and accurate micrographs requires a microscopical technique using a monocular eyepiece. It is essential that both eyes are open and that the eye that is not observing down the microscope is instead concentrated on a sheet of paper on the bench besides the microscope. With practice, and without moving the head or eyes, it is possible to accurately record the observed details by tracing round the observed shapes by simultaneously "seeing" the pencil point in the microscopical image.
Practicing this technique also establishes good general microscopical technique. It is always less tiring to observe with the microscope focused so that the image is seen at infinity and with both eyes open at all times.
Other enhancements.
Microspectroscopy:spectroscopy with a microscope
X-ray.
As resolution depends on the wavelength of the light. Electron microscopy has been developed since the 1930s that use electron beams instead of light. Because of the much smaller wavelength of the electron beam, resolution is far higher.
Though less common, X-ray microscopy has also been developed since the late 1940s. The resolution of X-ray microscopy lies between that of light microscopy and electron microscopy.
Electron microscopy.
Until the invention of sub-diffraction microscopy, the wavelength of the light limited the resolution of traditional microscopy to around 0.2 micrometers. In order to gain higher resolution, the use of an electron beam with a far smaller wavelength is used in electron microscopes.
Electron microscopes equipped for X-ray spectroscopy can provide qualitative and quantitative elemental analysis.
Scanning probe microscopy.
This is a sub-diffraction technique. Examples of scanning probe microscopes are the atomic force microscope (AFM), the Scanning tunneling microscope, the photonic force microscope and the recurrence tracking microscope. All such methods use the physical contact of a solid probe tip to scan the surface of an object, which is supposed to be almost flat.
Ultrasonic force.
Ultrasonic Force Microscopy (UFM) has been developed in order to improve the details and image contrast on "flat" areas of interest where AFM images are limited in contrast. The combination of AFM-UFM allows a near field acoustic microscopic image to be generated. The AFM tip is used to detect the ultrasonic waves and overcomes the limitation of wavelength that occurs in acoustic microscopy. By using the elastic changes under the AFM tip, an image of much greater detail than the AFM topography can be generated.
Ultrasonic force microscopy allows the local mapping of elasticity in atomic force microscopy by the application of ultrasonic vibration to the cantilever or sample. In an attempt to analyze the results of ultrasonic force microscopy in a quantitative fashion, a force-distance curve measurement is done with ultrasonic vibration applied to the cantilever base, and the results are compared with a model of the cantilever dynamics and tip-sample interaction based on the finite-difference technique.
Ultraviolet microscopy.
Ultraviolet microscopes have two main purposes. The first is to utilize the shorter wavelength of ultraviolet electromagnetic energy to improve the image resolution beyond that of the diffraction limit of standard optical microscopes. This technique is used for non-destructive inspection of devices with very small features such as those found in modern semiconductors. The second application for UV microscopes is contrast enhancement where the response of individual samples is enhanced, relative to their surrounding, due to the interaction of light with the molecules within the sample itself. One example is in the growth of protein crystals. Protein crystals are formed in salt solutions. As salt and protein crystals are both formed in the growth process, and both are commonly transparent to the human eye, they cannot be differentiated with a standard optical microscope. As the tryptophan of protein absorbs light at 280 nm, imaging with a UV microscope with 280 nm bandpass filters makes it simple to differentiate between the two types of crystals. The protein crystals appear dark while the salt crystals are transparent.
Infrared microscopy.
The term "infrared microscopy" refers to microscopy performed at infrared wavelengths. In the typical instrument configuration a Fourier Transform Infrared Spectrometer (FTIR) is combined with an optical microscope and an infrared detector. The infrared detector can be a single point detector, a linear array or a 2D focal plane array. The FTIR provides the ability to perform chemical analysis via infrared spectroscopy and the microscope and point or array detector enable this chemical analysis to be spatially resolved, i.e. performed at different regions of the sample. As such, the technique is also called infrared microspectroscopy [http://onlinelibrary.wiley.com/doi/10.1002/9780470027318.a5609.pub2/abstract] This technique is frequently used for infrared chemical imaging, where the image contrast is determined by the response of individual sample regions to particular IR wavelengths selected by the user, usually specific IR absorption bands and associated molecular resonances . A key limitation of conventional infrared microspectroscopy is that the spatial resolution is diffraction-limited. Specifically the spatial resolution is limited to a figure related to the wavelength of the light. For practical IR microscopes, the spatial resolution is limited to 1-3X the wavelength, depending on the specific technique and instrument used. For mid-IR wavelengths, this sets a practical spatial resolution limit of ~3-30 μm.
IR versions of sub-diffraction microscopy (see above) also exist [http://onlinelibrary.wiley.com/doi/10.1002/9780470027318.a5609.pub2/abstract]. These include IR NSOM, photothermal microspectroscopy, and atomic force microscope based infrared spectroscopy (AFM-IR).
Digital holographic microscopy.
In digital holographic microscopy (DHM), interfering wave fronts from a coherent (monochromatic) light-source are recorded on a sensor. The image is digitally reconstructed by a computer from the recorded hologram. Besides the ordinary bright field image, a phase shift image is created.
DHM can operate both in reflection and transmission mode. In reflection mode, the phase shift image provides a relative distance measurement and thus represents a topography map of the reflecting surface. In transmission mode, the phase shift image provides a label-free quantitative measurement of the optical thickness of the specimen. Phase shift images of biological cells are very similar to images of stained cells and have successfully been analyzed by high content analysis software.
A unique feature of DHM is the ability to adjust focus after the image is recorded, since all focus planes are recorded simultaneously by the hologram. This feature makes it possible to image moving particles in a volume or to rapidly scan a surface. Another attractive feature is DHM’s ability to use low cost optics by correcting optical aberrations by software.
Digital pathology (virtual microscopy).
Digital pathology is an image-based information environment enabled by computer technology that allows for the management of information generated from a digital slide. Digital pathology is enabled in part by virtual microscopy, which is the practice of converting glass slides into digital slides that can be viewed, managed, and analyzed.
Laser microscopy.
Laser microscopy is a rapidly growing field that uses laser illumination sources in various forms of microscopy. For instance, laser microscopy focused on biological applications uses ultrashort pulse lasers, in a number of techniques labeled as nonlinear microscopy, saturation microscopy, and two-photon excitation microscopy.
High-intensity, short-pulse laboratory x-ray lasers have been under development for several years. When this technology comes to fruition, it will be possible to obtain magnified three-dimensional images of elementary biological structures in the living state at a precisely defined instant. For optimum contrast between water and protein and for best sensitivity and resolution, the laser should be tuned near the nitrogen line at about 0.3 nanometers. Resolution will be limited mainly by the hydrodynamic expansion that occurs while the necessary number of photons is being registered. Thus, while the specimen is destroyed by the exposure, its configuration can be captured before it explodes.
Scientists have been working on practical designs and prototypes for x-ray holographic microscopes, despite the prolonged development of the appropriate laser.
Amateur microscopy.
"Amateur Microscopy" is the investigation and observation of biological and non-biological specimens for recreational purposes. Collectors of minerals, insects, seashells, and plants may use microscopes as tools to uncover features that help them classify their collected items. Other amateurs may be interested in observing the life found in pond water and of other samples. Microscopes may also prove useful for the water quality assessment for people that keep a home aquarium. Photographic documentation and drawing of the microscopic images are additional tasks that augment the spectrum of tasks of the amateur. There are even competitions for photomicrograph art. Participants of this pastime may either use commercially prepared microscopic slides or engage in the task of specimen preparation.
While microscopy is a central tool in the documentation of biological specimens, it is, in general, insufficient to justify the description of a new species based on microscopic investigations alone. Often genetic and biochemical tests are necessary to confirm the discovery of a new species. A laboratory and access to academic literature is a necessity, which is specialized and, in general, not available to amateurs. There is, however, one huge advantage that amateurs have above professionals: time to explore their surroundings. Often, advanced amateurs team up with professionals to validate their findings and (possibly) describe new species.
In the late 1800s, amateur microscopy became a popular hobby in the United States and Europe. Several 'professional amateurs' were being paid for their sampling trips and microscopic explorations by philanthropists, to keep them amused on the Sunday afternoon (e.g., the diatom specialist A. Grunow, being paid by (among others) a Belgian industrialist). Professor John Phin published "Practical Hints on the Selection and Use of the Microscope (Second Edition, 1878)," and was also the editor of the "American Journal of Microscopy."
Examples of amateur microscopy images:

</doc>
<doc id="19568" url="https://en.wikipedia.org/wiki?curid=19568" title="Microscope">
Microscope

A microscope (from the , "mikrós", "small" and , "skopeîn", "to look" or "see") is an instrument used to see objects that are too small for the naked eye. The science of investigating small objects using such an instrument is called microscopy. Microscopic means invisible to the eye unless aided by a microscope.
There are many types of microscopes. The most common (and the first to be invented) is the optical microscope, which uses light to image the sample. Other major types of microscopes are the electron microscope (both the transmission electron microscope and the scanning electron microscope), the ultramicroscope, and the various types of scanning probe microscope.
History.
The first microscope to be developed was the optical microscope, although the original inventor is not easy to identify. Evidence points to the first compound microscope appearing in the Netherlands by the 1620s, with a likely inventor being Cornelis Drebbel. Counter claims include it being invented by Hans Lippershey (who obtained the first telescope patent) and what may be a dubious claim by Zacharias Janssen's son that his father invented the microscope and telescope. Giovanni Faber coined the name microscope for Galileo Galilei's compound microscope in 1625 (Galileo had called it the ""occhiolino"" or ""little eye"").
Rise of modern light microscopy.
The first detailed account of the interior construction of living tissue based on the use of a microscope did not appear until 1644, in Giambattista Odierna's "L'occhio della mosca", or "The Fly's Eye".
It was not until the 1660s and 1670s that the microscope was used extensively for research in Italy, the Netherlands and England. Marcelo Malpighi in Italy began the analysis of biological structures beginning with the lungs. Robert Hooke's "Micrographia" had a huge impact, largely because of its impressive illustrations. The greatest contribution came from Antonie van Leeuwenhoek who discovered red blood cells and spermatozoa and helped popularise microscopy as a technique. On 9 October 1676, Van Leeuwenhoek reported the discovery of micro-organisms.
The performance of light microscopy depends as much on how the sample is illuminated as on how it is observed. Early instruments were limited until this principle was fully appreciated and developed, and until electric lamps were available as light sources. The first piece of fiction to involve the microcosm was probably Fitz-James O'Brien's "The Diamond Lens," which tells the story of a scientist who invents a powerful microscope and discovers a beautiful woman living in a microscopic world inside a drop of water. In 1893 August Köhler developed a key principle of sample illumination, Köhler illumination, which is central to achieving the theoretical limits of light microscopy. This method of sample illumination produces even lighting and overcomes the limited contrast and resolution imposed by early techniques of sample illumination. Further developments in sample illumination came from the discovery of phase contrast by Frits Zernike in 1953, and differential interference contrast illumination by Georges Nomarski in 1955; both of which allow imaging of unstained, transparent samples.
Electron microscopy.
In the early 20th century a significant alternative to light microscopy was developed, using electrons rather than light to generate the image. Ernst Ruska started development of the first electron microscope in 1931 which was the transmission electron microscope (TEM). The transmission electron microscope works on the same principle as an optical microscope but uses electrons in the place of light and electromagnets in the place of glass lenses. Use of electrons instead of light allows a much higher resolution.
Development of the transmission electron microscope was quickly followed in 1935 by the development of the scanning electron microscope by Max Knoll.
Electron microscopes quickly became popular following the Second World War. Ernst Ruska, working at Siemens, developed the first commercial transmission electron microscope and major scientific conferences on electron microscopy started being held in the 1950s. In 1965 the first commercial scanning electron microscope was developed by Professor Sir Charles Oatley and his postgraduate student Gary Stewart and marketed by the Cambridge Instrument Company as the "Stereoscan".
Scanning probe microscopy.
The 1980s saw the development of the first scanning probe microscopes. The first was the scanning tunneling microscope in 1981, developed by Gerd Binnig and Heinrich Rohrer. This was closely followed in 1986 with Gerd Binnig, Quate, and Gerber's invention of the atomic force microscope.
Fluorescence and light microscopy.
The most recent developments in light microscope largely centre on the rise of fluorescence microscopy in biology. During the last decades of the 20th century, particularly in the post-genomic era, many techniques for fluorescent labeling of cellular structures were developed. The main groups of techniques are small chemical staining of cellular structures, for example DAPI to label DNA, use of antibodies conjugated to fluorescent reporters, see
immunofluorescence, and fluorescent proteins, such as green fluorescent protein. These techniques use these different fluorophores for analysis of cell structure at a molecular level in both live and fixed samples.
The rise of fluorescence microscopy drove the development of a major modern microscope design, the confocal microscope. The principle was patented in 1957 by Marvin Minsky, although laser technology limited practical application of the technique. It was not until 1978 when Thomas and Christoph Cremer developed the first practical confocal laser scanning microscope and the technique rapidly gained popularity through the 1980s.
Much current research (in the early 21st century) on optical microscope techniques is focused on development of superresolution analysis of fluorescently labeled samples. Structured illumination can improve resolution by around two to four times and techniques like stimulated Emission Depletion microscopy are approaching the resolution of electron microscopes.
Types.
Microscopes can be separated into several different classes. One grouping is based on what interacts with the sample to generate the image, i.e., light or photons (optical microscopes), electrons (electron microscopes) or a probe (scanning probe microscopes). Alternatively, microscopes can be classed on whether they analyse the sample via a scanning point (confocal optical microscopes, scanning electron microscopes and scanning probe microscopes) or analyse the sample all at once (wide field optical microscope and transmission electron microscopes).
Wide field optical microscopes and transmission electron microscopes both use the theory of lenses (optics for light microscopes and electromagnet lenses for electron microscopes) in order to magnify the image generated by the passage of a wave transmitted through the sample, or reflected by the sample. The waves used are electromagnetic (in optical microscopes) or electron beams (in electron microscopes). Resolution in these microscopes is limited by the wavelength of the radiation used to image the sample, where shorter wavelengths allow for a higher resolution.
Scanning optical and electron microscopes, like the confocal microscope and scanning electron microscope, use lenses to focus a spot of light or electrons onto the sample then analyze the reflected or transmitted waves. The point is then scanned over the sample to analyze a rectangular region. Magnification of the image is achieved by displaying the data from scanning a physically small sample area on a relatively large screen. These microscopes have the same resolution limit as wide field optical, probe, and electron microscopes.
Scanning probe microscopes also analyze a single point in the sample and then scan the probe over a rectangular sample region to build up an image. As these microscopes do not use electromagnetic or electron radiation for imaging they are not subject to the same resolution limit as the optical and electron microscopes described above.
Optical.
The most common type of microscope (and the first invented) is the optical microscope. This is an optical instrument containing one or more lenses producing an enlarged image of a sample placed in the focal plane. Optical microscopes have refractive glass and occasionally of plastic or quartz, to focus light into the eye or another light detector. Mirror-based optical microscopes operate in the same manner. Typical magnification of a light microscope, assuming visible range light, is up to 1250x with a theoretical resolution limit of around 0.250 micrometres or 250 nanometres. This limits the practical magnification limit to ~1500x. Specialized techniques (e.g., scanning confocal microscopy, Vertico SMI) may exceed this magnification but the resolution is diffraction limited. The use of shorter wavelengths of light, such as the ultraviolet, is one way to improve the spatial resolution of the optical microscope, as are devices such as the near-field scanning optical microscope.Sarfus, a recent optical technique increases the sensitivity of standard optical microscope to a point it becomes possible to directly visualize nanometric films (down to 0.3 nanometre) and isolated nano-objects (down to 2 nm-diameter). The technique is based on the use of non-reflecting substrates for cross-polarized reflected light microscopy.
Ultraviolet light enables the resolution of microscopic features, as well as to image samples that are transparent to the eye. Near infrared light can be used to visualize circuitry embedded in bonded silicon devices, since silicon is transparent in this region of wavelengths.
In fluorescence microscopy, many wavelengths of light, ranging from the ultraviolet to the visible can be used to cause samples to fluoresce to allow viewing by eye or with the use of specifically sensitive cameras.
Phase contrast microscopy is an optical microscopy illumination technique in which small phase shifts in the light passing through a transparent specimen are converted into amplitude or contrast changes in the image.
The use of phase contrast does not require staining to view the slide. This microscope technique made it possible to study the cell cycle in live cells.
The traditional optical microscope has more recently evolved into the digital microscope. In addition to, or instead of, directly viewing the object through the eyepieces, a type of sensor similar to those used in a digital camera is used to obtain an image, which is then displayed on a computer monitor. These sensors may use CMOS or charge-coupled device (CCD) technology, depending on the application.
Electron.
Three major types of electron microscopes exist:
Scanning probe.
Of these techniques AFM and STM are the most commonly used.
Other types.
Scanning acoustic microscopes use sound waves to measure variations in acoustic impedance. Similar to Sonar in principle, they are used for such jobs as detecting defects in the subsurfaces of materials including those found in integrated circuits. On February 4, 2013, Australian engineers built a "quantum microscope" which provides unparalleled precision.

</doc>
<doc id="19570" url="https://en.wikipedia.org/wiki?curid=19570" title="Midrash">
Midrash

In Judaism, the Midrash (; ; pl. מדרשים "Midrashim") is the body of exegesis of Torah texts along with homiletic stories as taught by Chazal (Rabbinical Jewish sages of the post-Temple era) that provide an intrinsic analysis to passages in the Tanakh.
Midrash is a method of interpreting biblical stories that goes beyond simple distillation of religious, legal, or moral teachings. It fills in gaps left in the biblical narrative regarding events and personalities that are only hinted at.
The purpose of midrash was to resolve problems in the interpretation of difficult passages of the text of the Hebrew Bible, using Rabbinic principles of hermeneutics and philology to align them with the religious and ethical values of religious teachers.
Etymology.
Gesenius ascribes the etymology of "midrash" to the Qal of the common Hebrew verb "darash" (דָּרַשׁ) "to seek, study, inquire". The word "midrash" occurs twice in the Hebrew Bible: 2 Chronicles 13:22 "in the midrash of the prophet Iddo", and 24:27 "in the midrash of the Book of the Kings".
Methodology.
According to the PaRDeS approaches to exegesis, interpretation of Biblical texts in Judaism is realized through "peshat" (literal or plain meaning, lit. "plain" or "simple"), "remez" (deep meaning, lit. "hints"), "derash" (comparative meaning, from Hebrew "darash"—"to inquire" or "to seek") and "sod" (hidden meaning or philosophy, lit. "secret" or "mystery"). The Midrash concentrates somewhat on "remez" but mostly on "derash" (Some thinkers divide PaRDeS into pshat, remez, "din" (law) and sod. In this understanding, midrash aggada deals with remez and midrash halakha deals with din).
Many different exegetical methods are employed to derive deeper meaning from a text. This is not limited to the traditional thirteen textual tools attributed to the Tanna Rabbi Ishmael, which are used in the interpretation of "halakha" (Jewish law). Presence of apparently superfluous words or letters, chronology of events, parallel narratives or other textual anomalies are often a springboard for interpretation of segments of Biblical text. In many cases, a dialogue is expanded manifold: handfuls of lines in the Biblical narrative may become long philosophical discussions. It is unclear whether the midrash assumes these dialogues took place in reality or if this refers only to subtext or religious implication.
Many midrashim start off with a seemingly unrelated sentence from the Biblical books of Psalms, Proverbs or the Prophets. This sentence later turns out to metaphorically reflect the content of the rabbinical interpretation offered. This strategy is used particularly in a subgenre of midrash known as the "Petikhta".
Some Midrash discussions are highly metaphorical, and many Jewish authors stress that they are not intended to be taken literally. Rather, other midrashic sources may sometimes serve as a key to particularly esoteric discussions. Later authors maintain that this was done to make this material less accessible to the casual reader and prevent its abuse by detractors.
Forms of Midrashic literature.
In general the midrash is focused on either halakha (legal) or Aggadic (non-legal and chiefly homiletical) subject matter. Both kinds of midrashim were at first preserved only orally; but their writing down commenced in the 2nd century, and they now exist in the shape chiefly of exegetical or homiletical commentaries on Tanakh (the Hebrew Bible). Midrashic literature is worthwhile reading not only for its insights into Judaism and the history of Jewish thought, but also for the more incidental data it provides to historians, philologists, philosophers, and scholars of either historical-critical Bible study or comparative religion.
Halakhic midrashim.
"Midrash halakha" are the works in which the sources in the Tanakh (Hebrew Bible) of the traditionally received laws are identified. These Midrashim often predate the Mishnah. The Midrash linking a verse to a "halakha" will often function as a proof of a law's authenticity; a correct elucidation of the Torah carries with it the support of the "halakhah", and often the reason for the rule's existence (although many rabbinical laws have no direct Biblical source). The term is applied also to the derivation of new laws, either by means of a correct interpretation of the obvious meaning of scriptural words themselves or by the application of certain hermeneutic rules.
Origins.
After the return of Jewish refugees from their exile in Babylon, some argue that the Torah was central to Jewish life at home and abroad. This is certainly the case in some strains of Judaism, although scholars agree the period was marked by wide diversity, so the centrality of Torah would vary greatly for different groups. A significant concern of Jewish authorities was to ensure compliance with the Torah's commandments, the enactments of the Mosaic Law; yet, as these laws had been written in circumstances of the past, they seemed to call for adaptation or explication if they were to fit the circumstances of contemporary life. Explanations of the terms of the Mosaic legislation are legal, or halakhic midrashim. Relatedly, the Mishnah does not generally cite a scriptural basis for its laws; connecting the Mishnaic law with the Torah law is also undertaken by the later midrash (and Talmuds).
Aggadic midrashim.
Homiletic midrashim embraces the interpretation of the non-legal portions of the Hebrew Bible. These midrashim are sometimes referred to as "aggadah" or "haggadah", a loosely defined term that may refer to all non-legal discourse in classical rabbinic literature.
Aggadic explanations of the non-legal parts of Scripture are characterized by a much greater freedom of exposition than the halakhic Midrashim (midrashim on Jewish law.) Aggadic expositors availed themselves of various techniques, including sayings of prominent rabbis. These aggadic explanations could be philosophical or mystical disquisitions concerning angels, demons, paradise, hell, the messiah, Satan, feasts and fasts, parables, legends, satirical assaults on those who practice idolatry, etc.
Some of these midrashim entail mystical teachings. The presentation is such that the Midrash is a simple lesson to the uninitiated, and a direct allusion, or analogy, to a Mystical teaching for those educated in this area.
An example of a Midrashic interpretation:
Contemporary Midrash.
A wealth of literature and artwork has been created in the 20th and 21st centuries by people aspiring to create "Contemporary Midrash". Forms include poetry, prose, Bibliodrama (the acting out of Bible stories), murals, masks, and music, among others. The Institute for Contemporary Midrash was formed to facilitate these reinterpretations of sacred texts. The institute hosted several week-long intensives between 1995 and 2004, and published eight issues of "Living Text: The Journal of Contemporary Midrash" from 1997 to 2000.

</doc>
<doc id="19571" url="https://en.wikipedia.org/wiki?curid=19571" title="Missouri">
Missouri

Missouri (see pronunciations) is a state located in the Midwestern United States. It is the 21st most extensive, and the 18th most populous of the fifty states. The state comprises 114 counties and the independent city of St. Louis.
As defined by the 2010 US census, the four largest urban areas in order of population are St. Louis, Kansas City, Springfield, and Columbia. The mean center of the United States population at the 2010 census was in the town of Plato in Texas County.
The state's capital is Jefferson City. The land that is now Missouri was acquired from France as part of the Louisiana Purchase and became known as the Missouri Territory. Part of this territory was admitted into the union as the 24th state on August 10, 1821.
Missouri's geography is highly varied. The northern part of the state lies in dissected till plains and the southern portion lies in the Ozark Mountains (a dissected plateau), with the Missouri River dividing the regions. The state lies at the intersection of the three greatest rivers of the United States, with the confluence of the Mississippi and Missouri Rivers near St. Louis, and the confluence of the Ohio River with the Mississippi north of the Bootheel. The starting points for the Pony Express, Santa Fe Trail, and Oregon Trail were all located in Missouri as well.
Etymology and pronunciation.
The state is named for the Missouri River, which was named after the indigenous Missouri Indians, a Siouan-language tribe. They were called the "ouemessourita" ("wimihsoorita"), meaning "those who have dugout canoes", by the Miami-Illinois language speakers. As the Illini were the first natives encountered by Europeans in the region, the latter adopted the Illini name for the Missouri people.
The name "Missouri" has several different pronunciations even among its present-day natives, the two most common being and . This situation of differing pronunciations has existed since the late-1600s. Further pronunciations also exist in Missouri or elsewhere in the United States, involving the realization of the first syllable as either or ; the medial consonant as either or ; the stressed second syllable as either or ; and the third syllable as , , centralized (]), or even ∅ (in other words, a non-existent third syllable). Any combination of these phonetic realizations may be observed coming from speakers of American English.
The linguistic history was treated definitively by Donald M. Lance, who acknowledged that the question is sociologically complex, but that no pronunciation could be declared "correct," nor could any be clearly defined as native or outsider, rural or urban, southern or northern, educated or otherwise. Politicians often employ multiple pronunciations, even during a single speech, to appeal to a greater number of listeners. Often, informal respellings of the state's name, such as "Missour-"ee"" or "Missour-"uh"", are used informally to phonetically distinguish pronunciations.
Nicknames.
There is no official state nickname. However, Missouri's unofficial nickname is the "Show Me State", which appears on its license plates. This phrase has several origins. One is popularly ascribed to a speech by Congressman Willard Vandiver in 1899, who declared that "I come from a state that raises corn and cotton, cockleburs and Democrats, and frothy eloquence neither convinces nor satisfies me. I'm from Missouri, and you have got to show me." This is in keeping with the saying "I'm from Missouri" which means "I'm skeptical of the matter and not easily convinced." However, according to researchers, the phrase "show me" was already in use before the 1890s. Another one states that it is a reference to Missouri miners who were taken to Leadville, Colorado to replace striking workers. Since the new men were unfamiliar with the mining methods, they required frequent instruction.
Other nicknames for Missouri include "The Lead State", "The Bullion State", "The Ozark State", "The Mother of the West", "The Iron Mountain State", and "Pennsylvania of the West". It is also known as the "Cave State" because there are more than 6000 recorded caves in the state (second to Tennessee). Perry County is the county with the largest number of caves and the single longest cave.
The official state motto is , which means "Let the welfare of the people be the supreme law."
Geography.
Missouri is landlocked and borders eight different states as does its neighbor, Tennessee. No state in the U.S. touches more than eight. Missouri is bounded by Iowa on the north; by Illinois, Kentucky, and Tennessee across the Mississippi River on the east; on the south by Arkansas; and by Oklahoma, Kansas, and Nebraska (the last across the Missouri River) on the west. The two largest rivers are the Mississippi (which defines the eastern boundary of the state) and the Missouri River (which flows from west to east through the state) essentially connecting the two largest metros of Kansas City and St. Louis.
Although it is usually today considered part of the Midwest, Missouri was historically considered by many to be a border state, chiefly because of the settlement of migrants from the South and its status as a slave state before the Civil War, balanced by the influence of the St. Louis. The counties that made up "Little Dixie" were those along the Missouri River in the center of the state, settled by Southern migrants who held the greatest concentration of slaves.
In 2005, Missouri received 16,695,000 visitors to its national parks and other recreational areas totaling , giving it $7.41 million in annual revenues, 26.6% of its operating expenditures.
Topography.
North of, and in some cases just south of, the Missouri River lie the Northern Plains that stretch into Iowa, Nebraska, and Kansas. Here, rolling hills remain from the glaciation that once extended from the Canadian Shield to the Missouri River. Missouri has many large river bluffs along the Mississippi, Missouri, and Meramec Rivers. Southern Missouri rises to the Ozark Mountains, a dissected plateau surrounding the Precambrian igneous St. Francois Mountains. This region also hosts karst topography characterized by high limestone content with the formation of sinkholes and caves.
The southeastern part of the state is known as the Bootheel region, which is part of the Mississippi Alluvial Plain or Mississippi embayment. This region is the lowest, flattest, warmest, and wettest part of the state. It is also among the poorest, as the economy there is mostly agricultural. It is also the most fertile, with cotton and rice crops predominant. The Bootheel was the epicenter of the four New Madrid Earthquakes of 1811 and 1812.
Climate.
Missouri generally has a humid continental climate with cold snowy winters and hot, humid, and wet summers. In the southern part of the state, particularly in the Bootheel, the climate becomes humid subtropical. Located in the interior United States, Missouri often experiences extremes in temperatures. Without high mountains or oceans nearby to moderate temperature, its climate is alternately influenced by air from the cold Arctic and the hot and humid Gulf of Mexico. Missouri's highest recorded temperature is at Warsaw and Union on July 14, 1954 while the lowest recorded temperature is also at Warsaw on February 13, 1905.
Located in Tornado Alley, Missouri also receives extreme weather in the form of severe thunderstorms and tornadoes. The most recent tornado in the state to cause damage and casualties was the 2011 Joplin tornado, which destroyed roughly one-third of the city of Joplin. The tornado caused an estimated $1–3 billion in damages, killed 159 (+1 non-tornadic), and injured over 1,000 people. It was the first EF5 to hit the state since 1957 and the deadliest in the U.S. since 1947, making it the seventh deadliest tornado in American history and 27th deadliest in the world. St. Louis and its suburbs also have a history of experiencing particularly severe tornadoes, the most recent memorable one being an EF4 tornado that damaged Lambert-St. Louis International Airport on April 22, 2011. One of the worst tornadoes in American history struck St. Louis on May 27, 1896.
History.
Indigenous peoples inhabited Missouri for thousands of years before European exploration and settlement. Archaeological excavations along the rivers have shown continuous habitation for more than 7,000 years. Beginning before 1000 CE, there arose the complex Mississippian culture, whose people created regional political centers at present-day St. Louis and across the Mississippi River at Cahokia, near present-day Collinsville, Illinois. Their large cities included thousands of individual residences, but they are known for their surviving massive earthwork mounds, built for religious, political and social reasons, in platform, ridgetop and conical shapes. Cahokia was the center of a regional trading network that reached from the Great Lakes to the Gulf of Mexico. The civilization declined by 1400 CE, and most descendants left the area long before the arrival of Europeans. St. Louis was at one time known as Mound City by the European Americans, because of the numerous surviving prehistoric mounds, since lost to urban development. The Mississippian culture left mounds throughout the middle Mississippi and Ohio river valleys, extending into the southeast as well as the upper river.
The first European settlers were mostly ethnic French Canadians, who created their first settlement in Missouri at present-day Ste. Genevieve, about an hour south of St. Louis. They had migrated about 1750 from the Illinois Country. They came from colonial villages on the east side of the Mississippi River, where soils were becoming exhausted and there was insufficient river bottom land for the growing population. Sainte-Geneviève became a thriving agricultural center, producing enough surplus wheat, corn and tobacco to ship tons of grain annually downriver to Lower Louisiana for trade. Grain production in the Illinois Country was critical to the survival of Lower Louisiana and especially the city of New Orleans.
St. Louis was founded soon after by French from New Orleans in 1764. From 1764 to 1803, European control of the area west of the Mississippi to the northernmost part of the Missouri River basin, called Louisiana, was assumed by the Spanish as part of the Viceroyalty of New Spain, due to Treaty of Fontainebleau (in order to have Spain join with France in the war against England). The arrival of the Spanish in St. Louis was in September 1767.
St. Louis became the center of a regional fur trade with Native American tribes that extended up the Missouri and Mississippi rivers, which dominated the regional economy for decades. Trading partners of major firms shipped their furs from St. Louis by river down to New Orleans for export to Europe. They provided a variety of goods to traders, for sale and trade with their Native American clients. The fur trade and associated businesses made St. Louis an early financial center and provided the wealth for some to build fine houses and import luxury items. Its location near the confluence of the Illinois River meant it also handled produce from the agricultural areas. River traffic and trade along the Mississippi were integral to the state's economy, and as the area's first major city, St. Louis expanded greatly after the invention of the steamboat and the increased river trade.
Early nineteenth century.
Napoleon Bonaparte had gained Louisiana for French ownership from Spain in 1800 under the Treaty of San Ildefonso, after it had been a Spanish colony since 1762. But the treaty was kept secret. Louisiana remained nominally under Spanish control until a transfer of power to France on November 30, 1803, just three weeks before the cession to the United States.
Part of the 1803 Louisiana Purchase by the United States, Missouri earned the nickname "Gateway to the West" because it served as a major departure point for expeditions and settlers heading to the West during the 19th century. St. Charles, just west of St. Louis, was the starting point and the return destination of the Lewis and Clark Expedition, which ascended the Missouri River in 1804, in order to explore the western lands to the Pacific Ocean. St. Louis was a major supply point for decades, for parties of settlers heading west.
As many of the early settlers in western Missouri migrated from the Upper South, they brought enslaved African Americans as agricultural laborers, and they desired to continue their culture and the institution of slavery. They settled predominantly in 17 counties along the Missouri River, in an area of flatlands that enabled plantation agriculture and became known as "Little Dixie." In 1821 the former Missouri Territory was admitted as a slave state, in accordance with the Missouri Compromise, and with a temporary state capital in St. Charles. In 1826, the capital was shifted to its current, permanent location of Jefferson City, also on the Missouri River.
The state was rocked by the 1811–12 New Madrid earthquakes. Casualties were few due to the sparse population.
Originally the state's western border was a straight line, defined as the meridian passing through the Kawsmouth, the point where the Kansas River enters the Missouri River. The river has moved since this designation. This line is known as the Osage Boundary. In 1836 the Platte Purchase was added to the northwest corner of the state after purchase of the land from the native tribes, making the Missouri River the border north of the Kansas River. This addition increased the land area of what was already the largest state in the Union at the time (about to Virginia's 65,000 square miles, which then included West Virginia).
In the early 1830s, Mormon migrants from northern states and Canada began settling near Independence and areas just north of there. Conflicts over religion and slavery arose between the 'old settlers' (mainly from the South) and the Mormons (mainly from the North). The Mormon War erupted in 1838. By 1839, with the help of an "Extermination Order" by Governor Lilburn Boggs, the old settlers forcefully expelled the Mormons from Missouri and confiscated their lands.
Conflicts over slavery exacerbated border tensions among the states and territories. From 1838 to 1839, a border dispute with Iowa over the so-called Honey Lands resulted in both states' calling-up of militias along the border.
With increasing migration, from the 1830s to the 1860s Missouri's population almost doubled with every decade. Most of the newcomers were American-born, but many Irish and German immigrants arrived in the late 1840s and 1850s. As a majority were Catholic, they set up their own religious institutions in the state, which had been mostly Protestant. Having fled famine and oppression in Ireland, and revolutionary upheaval in Germany, the immigrants were not sympathetic to slavery. Many settled in cities, where they created a regional and then state network of Catholic churches and schools. Nineteenth-century German immigrants created the wine industry along the Missouri River and the beer industry in St. Louis.
Most Missouri farmers practiced subsistence farming before the American Civil War. The majority of those who held slaves had fewer than five each. Planters, defined by some historians as those holding twenty slaves or more, were concentrated in the counties known as "Little Dixie", in the central part of the state along the Missouri River. The tensions over slavery chiefly had to do with the future of the state and nation. In 1860, enslaved African Americans made up less than 10% of the state's population of 1,182,012. In order to control the flooding of farmland and low-lying villages along the Mississippi, the state had completed construction of of levees along the river by 1860.
American Civil War.
After the secession of Southern states began in 1861, the Missouri legislature called for the election of a special convention on secession. The convention voted decisively to remain within the Union. Pro-Southern Governor Claiborne F. Jackson ordered the mobilization of several hundred members of the state militia who had gathered in a camp in St. Louis for training. Alarmed at this action, Union General Nathaniel Lyon struck first, encircling the camp and forcing the state troops to surrender. Lyon directed his soldiers, largely non-English-speaking German immigrants, to march the prisoners through the streets, and they opened fire on the largely hostile crowds of civilians who gathered around them. Soldiers killed unarmed prisoners as well as men, women and children of St. Louis in the incident that became known as the "St. Louis Massacre".
These events heightened Confederate support within the state. Governor Jackson appointed Sterling Price, president of the convention on secession, as head of the new Missouri State Guard. In the face of Union General Lyon's rapid advance through the state, Jackson and Price were forced to flee the capital of Jefferson City on June 14, 1861. In the town of Neosho, Missouri, Jackson called the state legislature into session. They enacted a secession ordinance. However, even under the Southern view of secession, only the state convention had the power to secede. Since the convention was dominated by unionists, and the state was more pro-Union than pro-Confederate in any event, the ordinance of secession adopted by the legislature is generally given little credence. The Confederacy nonetheless recognized it on October 30, 1861.
With the elected governor absent from the capital and the legislators largely dispersed, the state convention was reassembled with most of its members present, save 20 that fled south with Jackson's forces. The convention declared all offices vacant, and installed Hamilton Gamble as the new governor of Missouri. President Lincoln's administration immediately recognized Gamble's government as the legal Missouri government. The federal government's decision enabled raising pro-Union militia forces for service within the state as well as volunteer regiments for the Union Army.
Fighting ensued between Union forces and a combined army of General Price's Missouri State Guard and Confederate troops from Arkansas and Texas under General Ben McCulloch. After winning victories at the battle of Wilson's Creek and the siege of Lexington, Missouri and suffering losses elsewhere, the Confederate forces retreated to Arkansas and later Marshall, Texas, in the face of a largely reinforced Union Army.
Though regular Confederate troops staged some large-scale raids into Missouri, the fighting in the state for the next three years consisted chiefly of guerrilla warfare. "Citizen soldiers" or insurgents such as Captain William Quantrill, Frank and Jesse James, the Younger brothers, and William T. Anderson made use of quick, small-unit tactics. Pioneered by the Missouri Partisan Rangers, such insurgencies also arose in portions of the Confederacy occupied by the Union during the Civil War. Historians have portrayed stories of the James brothers' outlaw years as an American "Robin Hood" myth. The vigilante activities of the Bald Knobbers of the Ozarks in the 1880s were an unofficial continuation of insurgent mentality long after the official end of the war, and they are a favorite theme in Branson's self-image.
20th century to present.
The Progressive Era (1890s to 1920s) saw numerous prominent leaders from Missouri trying to end corruption and modernize politics, government and society. Joseph "Holy Joe" Folk was a key leader who made a strong appeal to middle class and rural evangelical Protestants. Folk was elected governor as a progressive reformer and Democrat in the 1904 election. He promoted what he called "the Missouri Idea", the concept of Missouri as a leader in public morality through popular control of law and strict enforcement. He successfully conducted antitrust prosecutions, ended free railroad passes for state officials, extended bribery statutes, improved election laws, required formal registration for lobbyists, made racetrack gambling illegal, and enforced the Sunday-closing law. He helped enact Progressive legislation, including an initiative and referendum provision, regulation of elections, education, employment and child labor, railroads, food, business, and public utilities. A number of efficiency-oriented examiner boards and commissions were established during Folk's administration, including many agricultural boards and the Missouri library commission.
Between the Civil War and the end of World War II, Missouri transitioned from a rural economy to a hybrid industrial-service-agricultural economy as the Midwest rapidly industrialized. The expansion of railroads to the West transformed Kansas City into a major transportation hub within the nation. The growth of the Texas cattle industry along with this increased rail infrastructure and the invention of the refrigerated boxcar also made Kansas City a major meatpacking center, as large cattle drives from Texas brought herds of cattle to Dodge City and other Kansas towns. There, the cattle were loaded onto trains destined for Kansas City, where they were butchered and distributed to the eastern markets. The first half of the twentieth century was the height of Kansas City's prominence and its downtown became a showcase for stylish Art Deco skyscrapers as construction boomed.
In 1930, there was a diphtheria epidemic in the area around Springfield, which killed approximately 100 people. Serum was rushed to the area, and medical personnel stopped the epidemic.
During the mid-1950s and 1960s, St. Louis and Kansas City suffered deindustrialization and loss of jobs in railroads and manufacturing, as did other Midwestern industrial cities. In 1956 St. Charles claims to be the site of the first interstate highway project. Such highway construction made it easy for middle-class residents to leave the city for newer housing developed in the suburbs, often former farmland where land was available at lower prices. These major cities have gone through decades of readjustment to develop different economies and adjust to demographic changes. Suburban areas have developed separate job markets, both in knowledge industries and services, such as major retail malls.
Demographics.
The United States Census Bureau estimates that the population of Missouri was 6,083,672 on July 1, 2015, a 1.58% increase since the 2010 United States Census.
Missouri had a population of 5,988,927, according to the 2010 Census; an increase of 392,369 (7.0 percent) since the year 2000. From 2000 to 2007, this includes a natural increase of 137,564 people since the last census (480,763 births less 343,199 deaths), and an increase of 88,088 people due to net migration into the state. Immigration from outside the United States resulted in a net increase of 50,450 people, and migration within the country produced a net increase of 37,638 people. Over half of Missourians (3,294,936 people, or 55.0%) live within the state's two largest metropolitan areas–St. Louis and Kansas City. The state's population density 86.9 in 2009, is also closer to the national average (86.8 in 2009) than any other state.
In 2011, the racial composition of the state was:
In 2011, 3.7% of the total population was of Hispanic or Latino origin (they may be of any race). In 2011, 28.1% of Missouri's population younger than age 1 were minorities.
The U.S. Census of 2000 found that the population center of the United States is in Phelps County, Missouri. The center of population of Missouri itself is located in Osage County, in the city of Westphalia.
In 2004, the population included 194,000 foreign-born (3.4 percent of the state population).
The five largest ancestry groups in Missouri are: German (27.4 percent), Irish (14.8 percent), English (10.2 percent), American (8.5 percent) and French (3.7 percent).
German Americans are an ancestry group present throughout Missouri. African Americans are a substantial part of the population in St. Louis (56.6% of African Americans in the state lived in St. Louis or St. Louis County as of the 2010 census), Kansas City, Boone County and in the southeastern Bootheel and some parts of the Missouri River Valley, where plantation agriculture was once important. Missouri Creoles of French ancestry are concentrated in the Mississippi River Valley south of St. Louis (see Missouri French). Kansas City is home to large and growing immigrant communities from Latin America esp. Mexico and Colombia, Africa (i.e. Sudan, Somalia and Nigeria), and Southeast Asia including China and the Philippines; and Europe like the former Yugoslavia (see Bosnian American). A notable Cherokee Indian population exists in Missouri.
In 2004, 6.6 percent of the state's population was reported as younger than 5 years old, 25.5 percent younger than 18, and 13.5 percent was 65 or older. Females were approximately 51.4 percent of the population. 81.3 percent of Missouri residents were high school graduates (more than the national average), and 21.6 percent had a bachelor's degree or higher. 3.4 percent of Missourians were foreign-born, and 5.1 percent reported speaking a language other than English at home.
In 2010, there were 2,349,955 households in Missouri, with 2.45 people per household. The home ownership rate was 70.0 percent, and the median value of an owner-occupied housing unit was $137,700. The median household income for 2010 was $46,262, or $24,724 per capita. There were 14.0 percent (1,018,118) Missourians living below the poverty line in 2010.
The mean commute time to work was 23.8 minutes.
Language.
The vast majority of people in Missouri speak English. Approximately 5.1% of the population reported speaking a language other than English at home. The Spanish language is spoken in small Latino communities in the St. Louis and Kansas City Metro areas.
Missouri is home to an endangered dialect of the French language known as Missouri French. Speakers of the dialect, who call themselves "Créoles", are descendants of the French pioneers who settled the area then known as the Illinois Country beginning in the late 17th century. It developed in isolation from French speakers in Canada and Louisiana, becoming quite distinct from the varieties of Canadian French and Louisiana Creole French. Once widely spoken throughout the area, Missouri French is now nearly extinct, with only a few elderly speakers able to use it.
Religion.
According to a Pew Research study conducted in 2014, 80% of Missourians identify with a religion. 77% affiliate with Christianity and its various denominations, and the other 3% are adherents of non-Christian religions. The remaining 20% have no religion, with 2% specifically identifying as atheists and 3% identifying as agnostics (the other 15% do not identify as "anything as particular").
Broken down, the religious demographics of Missouri are as follows:
The largest denominations by number of adherents in 2010 were the Southern Baptist Convention with 749,685; the Roman Catholic Church with 724,315; and the United Methodist Church with 226,409.
Among the other denominations there are approximately 93,000 Mormons in 253 congregations, 25,000 Jewish adherents in 21 temples, 12,000 Muslims in 39 masjids, 7,000 Buddhists in 34 temples, 7,000 Hindus in 17 temples, 2,500 Unitarians in 9 congregations, 2,000 Baha'i in 17 temples, 5 Sikh temples, a Zorastrian temple, a Jain temple and an uncounted number of neopagans.
Several religious organizations have headquarters in Missouri, including the Lutheran Church–Missouri Synod, which has its headquarters in Kirkwood, as well as the United Pentecostal Church International in Hazelwood, both outside St. Louis.
Independence, near Kansas City, is the headquarters for the Community of Christ (formerly the Reorganized Church of Jesus Christ of Latter Day Saints), the Church of Christ (Temple Lot) and the group Remnant Church of Jesus Christ of Latter Day Saints. This area and other parts of Missouri are also of significant religious and historical importance to The Church of Jesus Christ of Latter-day Saints (LDS Church), which maintains several sites and visitors centers.
Springfield is the headquarters of the Assemblies of God USA and the Baptist Bible Fellowship International. The General Association of General Baptists has its headquarters in Poplar Bluff. The Unity Church is headquartered in Unity Village.
Economy.
The Bureau of Economic Analysis estimates that Missouri's total state product in 2006 was $225.9 billion. Per capita personal income in 2006 was $32,705, ranking 26th in the nation. Major industries include aerospace, transportation equipment, food processing, chemicals, printing/publishing, electrical equipment, light manufacturing, and beer.
The agriculture products of the state are beef, soybeans, pork, dairy products, hay, corn, poultry, sorghum, cotton, rice, and eggs. Missouri is ranked 6th in the nation for the production of hogs and 7th for cattle. Missouri is ranked in the top five states in the nation for production of soy beans, and it is ranked fourth in the nation for the production of rice. In 2001, there were 108,000 farms, the second-largest number in any state after Texas. Missouri actively promotes its rapidly growing wine industry.
Missouri has vast quantities of limestone. Other resources mined are lead, coal, and crushed stone. Missouri produces the most lead of all of the states. Most of the lead mines are in the central eastern portion of the state. Missouri also ranks first or near first in the production of lime, a key ingredient in Portland cement.
Missouri also has a growing science and biotechnology field. Monsanto, one of the largest gene companies in America, is based in St. Louis.
Tourism, services and wholesale/retail trade follow manufacturing in importance.
Missouri is the only state in the Union to have two Federal Reserve Banks: one in Kansas City (serving western Missouri, Kansas, Nebraska, Oklahoma, Colorado, northern New Mexico, and Wyoming) and one in St. Louis (serving eastern Missouri, southern Illinois, southern Indiana, western Kentucky, western Tennessee, northern Mississippi, and all of Arkansas).
As of November 2014, the state's unemployment rate was 4.8%, while the nation overall was 5.5%.
Taxation.
Personal income is taxed in ten different earning brackets, ranging from 1.5% to 6.0%. Missouri's sales tax rate for most items is 4.225% with some additional local levies. More than 2,500 Missouri local governments rely on property taxes levied on real property (real estate) and personal property.
Most personal property is exempt, except for motorized vehicles. Exempt real estate includes property owned by governments and property used as nonprofit cemeteries, exclusively for religious worship, for schools and colleges and for purely charitable purposes. There is no inheritance tax and limited Missouri estate tax related to federal estate tax collection.
Energy.
In 2012, Missouri had roughly 22,000 MW of installed electricity generation capacity. In 2011, 82% of Missouri's electricity was generated by coal. 10% was generated from the state's only nuclear power plant, the Callaway Plant in Callaway County, northeast of Jefferson City. 5% was generated by natural gas. 1% was generated by hydroelectric sources, such as the dams for Truman Lake and Lake of the Ozarks. Missouri has a small but growing amount of wind and solar power—wind capacity increased from 309 MW in 2009 to 459 MW in 2011, while photovoltaics have increased from 0.2 MW to 1.3 MW over the same period.
Oil wells in Missouri produced 120,000 barrels of crude oil in fiscal 2012. There are no oil refineries in Missouri.
Transportation.
Airports.
Missouri has two major airport hubs: Lambert–St. Louis International Airport and Kansas City International Airport. Residents of Mid-Missouri use Columbia Regional Airport (COU) to fly to either Chicago (ORD) or Dallas (DFW). Southern Missouri has the Springfield-Branson National Airport (SFG) with multiple non-stop destinations.
Rail.
Two of the nation's three busiest rail centers are located in Missouri. Kansas City is a major railroad hub for BNSF Railway, Norfolk Southern Railway, Kansas City Southern Railway, and Union Pacific Railroad. Kansas City is the second largest freight rail center in the US (but is first in the amount of tonnage handled). Like Kansas City, St. Louis is a major destination for train freight. Springfield remains an operational hub for BNSF Railway.
Amtrak passenger trains serve Kansas City, La Plata, Jefferson City, St. Louis, Lee's Summit, Independence, Warrensburg, Hermann, Washington, Kirkwood, Sedalia, and Poplar Bluff. A proposed high-speed rail route in Missouri as part of the Chicago Hub Network has received $31 million in funding.
The only urban light rail/subway system operating in Missouri is MetroLink, which connects the city of St. Louis with suburbs in Illinois and St. Louis County. It is one of the largest systems (by track mileage) in the United States. A streetcar line in downtown Kansas City is scheduled to open in 2015.
The Gateway Multimodal Transportation Center in St. Louis is the largest active multi-use transportation center in the state. It is located in downtown St. Louis, next to the historic Union Station complex. It serves as a hub center/station for MetroLink, the MetroBus regional bus system, Greyhound, Amtrak, and taxi services.
Bus.
Many cities have regular fixed-route systems, and many rural counties have rural public transit services. Greyhound and Trailways provide inter-city bus service in Missouri. Megabus serves St. Louis, but discontinued service to Columbia and Kansas City in 2015.
Rivers.
The Mississippi River and Missouri River are commercially navigable over their entire lengths in Missouri. The Missouri was channelized through dredging and jettys and the Mississippi was given a series of locks and dams to avoid rocks and deepen the river. St. Louis is a major destination for barge traffic on the Mississippi.
Roads.
Several highways, detailed below, traverse the state.
Following the passage of Amendment 3 in late 2004, the Missouri Department of Transportation (MoDOT) began its Smoother, Safer, Sooner road-building program with a goal of bringing of highways up to good condition by December 2007. From 2006–2010 traffic deaths have decreased annually from 1,257 in 2005, to 1,096 in 2006, to 992 for 2007, to 960 for 2008, to 878 in 2009, to 821 in 2010.
Interstate freeways.
The only section of freeway in Missouri to have High-Occupancy Vehicle Lane (HOV) is Interstate 55 from Ste. Genevieve, Missouri to Interstate 270-255 Interchange in St. Louis County. They were striped, registered, and opened on February 10, 2013. HOV Lanes are also being striped on Interstate 70 in St. Charles County through Interstate 270 in Saint Louis County, and on the North-South corridor of Interstate 270 in central St. Louis County.
United States Routes.
"North-south routes"
"East-west routes"
Law and government.
Framework.
The current Constitution of Missouri, the fourth constitution for the state, was adopted in 1945. It provides for three branches of government: the legislative, judicial, and executive branches. The legislative branch consists of two bodies: the House of Representatives and the Senate. These bodies comprise the Missouri General Assembly.
The House of Representatives has 163 members who are apportioned based on the last decennial census. The Senate consists of 34 members from districts of approximately equal populations. The judicial department comprises the Supreme Court of Missouri, which has seven judges, the Missouri Court of Appeals (an intermediate appellate court divided into three districts), sitting in Kansas City, St. Louis, and Springfield, and 45 Circuit Courts which function as local trial courts. The executive branch is headed by the Governor of Missouri and includes five other statewide elected offices. Following the death of Tom Schweich in 2015, all but one of Missouri's statewide elected offices are held by Democrats.
Harry S Truman (1884–1972), the 33rd President of the United States (Democrat, 1945–1953), was born in Lamar. He was a judge in Jackson County and then represented the state in the United States Senate for ten years, before being elected Vice-President in 1944. He lived in Independence after retiring.
Status as a political bellwether.
Missouri is widely regarded as a bellwether in American politics, often making it a swing state. The state had a longer stretch of supporting the winning presidential candidate than any other state, having voted with the nation in every election from 1904 to 2004 with a single exception: 1956, when Democratic candidate Adlai Stevenson of neighboring Illinois lost the election despite carrying Missouri. The state's status as a bellwether has been questioned in recent years, as Missouri twice voted against Democrat Barack Obama, who nonetheless widely prevailed in the 2008 and 2012 elections. Missouri's nearly 10% margin in favor of the losing Mitt Romney in 2012 suggests the state is starting to trend more Republican in presidential contests.
On October 24, 2012, there were 4,190,936 registered voters. At the state level, both Democratic Senator Claire McCaskill and Democratic Governor Jay Nixon were re-elected.
Laissez-faire alcohol and tobacco laws.
Missouri has been known for its population's generally "stalwart, conservative, noncredulous" attitude toward regulatory regimes, which is one of the origins of the state's unofficial nickname, the "Show-Me State." As a result, and combined with the fact that Missouri is one of America's leading alcohol states, regulation of alcohol and tobacco in Missouri is among the most laissez-faire in America. For 2013, the annual "Freedom in the 50 States" study prepared by the Mercatus Center at George Mason University ranked Missouri as #3 in America for alcohol freedom and #1 for tobacco freedom (#7 for freedom overall). The study notes that Missouri's "alcohol regime is one of the least restrictive in the United States, with no blue laws and taxes well below average," and that "Missouri ranks best in the nation on tobacco freedom."
Missouri law makes it "an improper employment practice" for an employer to refuse to hire, to fire, or otherwise to disadvantage any person because that person lawfully uses alcohol and/or tobacco products when he or she is not at work.
Alcohol.
With a large German immigrant population and the development of a brewing industry, Missouri always has had among the most permissive alcohol laws in the United States. It never enacted statewide prohibition. Missouri voters rejected prohibition in three separate referenda in 1910, 1912, and 1918. Alcohol regulation did not begin in Missouri until 1934.
Today, alcohol laws are controlled by the state government, and local jurisdictions are prohibited from going beyond those state laws. Missouri has no statewide open container law or prohibition on drinking in public, no alcohol-related blue laws, no local option, no precise locations for selling liquor by the package (allowing even drug stores and gas stations to sell any kind of liquor), and no differentiation of laws based on alcohol percentage. State law protects persons from arrest or criminal penalty for public intoxication.
Missouri law expressly prohibits any jurisdiction from going dry. Missouri law also expressly allows parents and guardians to serve alcohol to their children. The Power & Light District in Kansas City is one of the few places in the United States where a state law explicitly allows persons over the age of 21 to possess and consume open containers of alcohol in the street (as long as the beverage is in a plastic cup).
Tobacco.
As for tobacco (as of June 2014), Missouri has the lowest cigarette excise taxes in the United States, at 17 cents per pack, and the state electorate voted in 2002, 2006, and 2012 to keep it that way. In 2007, "Forbes" named Missouri's largest metropolitan area, St. Louis, America's "best city for smokers."
According to the Centers for Disease Control and Prevention, in 2008 Missouri had the fourth highest percentage of adult smokers among U.S states, at 24.5%. Although Missouri's minimum age for purchase and distribution of tobacco products is 18, tobacco products can be distributed to persons under 18 by family members on private property.
No statewide smoking ban ever has been seriously entertained before the Missouri General Assembly, and in October 2008, a statewide survey by the Missouri Department of Health and Senior Services found that only 27.5% of Missourians support a statewide ban on smoking in all bars and restaurants. Missouri state law permits restaurants seating less than 50 people, bars, bowling alleys, and billiard parlors to decide their own smoking policies, without limitation.
Counties.
Missouri has 114 counties and one independent city (St. Louis).
The largest county by size is Texas County (1,179 sq. miles) and Shannon County is second (1,004 sq. miles). Worth County is the smallest (266 sq. miles). The independent city of St. Louis has only of area. St. Louis City is the most densely populated area (5,140.1 per sq. mi.) in Missouri.
The largest county by population (2012 estimate) is St. Louis County (1,000,438 residents), with Jackson County second (677,377 residents), St. Charles third (368,666), and St. Louis fourth (318,172). Worth County is the least populous with 2,171 (2010 census) residents.
Major cities.
Jefferson City is the capital of Missouri.
The five largest cities in Missouri are Kansas City, St. Louis, Springfield, Independence, and Columbia.
St. Louis is the principal city of the largest metropolitan area in Missouri, composed of 17 counties and the independent city of St. Louis; eight of those counties lie in Illinois. As of 2009, St. Louis was the 18th largest metropolitan area in the nation with 2.83 million people. However, if ranked using Combined Statistical Area, it is 15th largest with 2.89 million people. Some of the major cities making up the St. Louis Metro area in Missouri are St. Charles, St. Peters, Florissant, Chesterfield, Creve Coeur, Wildwood, Maryland Heights, O'Fallon, Clayton, Ballwin, and University City.
Kansas City is Missouri's largest city and the principal city of the fifteen-county Kansas City Metropolitan Statistical Area, including six counties in the state of Kansas. As of 2009, it was the 29th largest metropolitan area in the nation, with 2.068 million people. Some of the other major cities comprising the Kansas City metro area in Missouri include Independence, Lee's Summit, Blue Springs, Raytown, Liberty, and Gladstone.
Branson is a major tourist attraction in the Ozarks of southwestern Missouri.
Education.
Missouri State Board of education.
The Missouri State Board of Education has general authority over all public education in the state of Missouri. It is made up of eight citizens appointed by the governor and confirmed by the Missouri Senate.
Primary and secondary schools.
Education is compulsory from ages seven to seventeen, and it is required that any parent, guardian or other person with custody of a child between the ages of seven and seventeen the compulsory attendance age for the district, must ensure that the child is enrolled in and regularly attends public, private, parochial school, home school or a combination of schools for the full term of the school year. Compulsory attendance also ends when children complete sixteen credits in high school..
Children in Missouri between the ages of five and seven are not required to be enrolled in school. However, if they are enrolled in a public school their parent, guardian or custodian must ensure that they regularly attend.
Missouri schools are commonly but not exclusively divided into three tiers of primary and secondary education: elementary school, middle school or junior high school and high school. The public schools system includes kindergarten to 12th grade. District territories are often complex in structure. In some cases, elementary, middle and junior high schools of a single district feed into high schools in another district. High school athletics and competitions are governed by the Missouri State High School Activities Association (MSHSAA).
Homeschooling is legal in Missouri and is an option to meet the compulsory education requirement. It is neither monitored nor regulated by the state's Department of Elementary and Secondary Education
A supplemental education program, the Missouri Scholars Academy, provides an extracurricular learning experience for gifted high school students in the state of Missouri. The official MSA website describes the goals of the Academy to be as such: "The academy reflects Missouri's desire to strive for excellence in education at all levels. The program is based on the premise that Missouri's gifted youth must be provided with special opportunities for learning and personal development in order for them to realize their full potential."
Another gifted school is the Missouri Academy of Science, Mathematics and Computing, which is located at the Northwest Missouri State University.
Colleges and universities.
The University of Missouri System is Missouri's statewide public university system. The flagship institution and largest university in the state is the University of Missouri in Columbia. The others in the system are University of Missouri–Kansas City, University of Missouri–St. Louis, and Missouri University of Science and Technology in Rolla.
During the late nineteenth and early twentieth century the state established a series of normal schools in each region of the state, originally named after the geographic districts: Northeast Missouri State University (now Truman State University) (1867), Central Missouri State University (now the University of Central Missouri) (1871), Southeast Missouri State University (1873), Southwest Missouri State University (now Missouri State University) (1905), Northwest Missouri State University (1905), Missouri Western State University (1915), and Missouri Southern State University (1937). Lincoln University and Harris–Stowe State University were established in the mid-nineteenth century and are historically black colleges and universities.
Among private institutions Washington University in St. Louis and Saint Louis University are two top ranked schools in the US. There are numerous junior colleges, trade schools, church universities and other private universities in the state. A.T. Still University was the first osteopathic medical school in the world. Hannibal–LaGrange University in Hannibal, Missouri, was one of the first colleges west of the Mississippi (founded 1858 in LaGrange, Missouri, and moved to Hannibal in 1928).
The state funds a $2000, renewable merit-based scholarship, Bright Flight, given to the top three percent of Missouri high school graduates who attend a university in-state.
The 19th century border wars between Missouri and Kansas have continued as a sports rivalry between the University of Missouri and University of Kansas. The rivalry is chiefly expressed through football and basketball games between the two universities. It is the oldest college rivalry west of the Mississippi River and the second oldest in the nation. Each year when the universities meet to play, the game is coined "Border War." An exchange occurs following the game where the winner gets to take a historic Indian War Drum, which has been passed back and forth for decades.
Culture and entertainment.
Music.
Many well-known musicians were born or have lived in Missouri. These include guitarist and rock pioneer Chuck Berry, singer and actress Josephine Baker, "Queen of Rock" Tina Turner, pop singer-songwriter Sheryl Crow, Michael McDonald of the Doobie Brothers, and rappers Nelly, Chingy and Akon, all of whom are either current or former residents of St. Louis.
Country singers from Missouri include New Franklin native Sara Evans, Cantwell native Ferlin Husky, West Plains native Porter Wagoner, Tyler Farr of Garden City, and Mora native Leroy Van Dyke, along with bluegrass musician Rhonda Vincent, a native of Greentop.
Rapper Eminem was born in St. Joseph and also lived in Savannah and Kansas City.
Ragtime composer Scott Joplin lived in St. Louis and Sedalia.
Jazz saxophonist Charlie Parker lived in Kansas City.
Rock and Roll singer Steve Walsh of the group Kansas was born in St. Louis and grew up in St. Joseph.
The Kansas City Symphony and the St. Louis Symphony Orchestra are the state's major orchestras. The latter is the nation's second-oldest symphony orchestra and achieved prominence in recent years under conductor Leonard Slatkin.
Branson is well known for its music theaters, most of which bear the name of a star performer or musical group. These facilities have made Branson one of America's most popular tourist destinations..
Literature.
Missouri is the native state of Mark Twain. His novels "The Adventures of Tom Sawyer" and "The Adventures of Huckleberry Finn" are set in his boyhood hometown of Hannibal.
Kansas City-born writer William Least Heat-Moon currently resides in Rocheport. He is best known for "Blue Highways", a chronicle of his travels to small towns across America. The book was on the New York Times Bestseller list for nearly a year in 1982-1983.
Famed authors Kate Chopin, T. S. Eliot and Tennessee Williams were all from St. Louis.
Film.
Filmmaker, animator, and businessman Walt Disney spent part of his childhood in the Linn County town of Marceline before moving to Kansas City, Missouri. Disney began his artistic career in Kansas City, where he founded the Laugh-O-Gram Studio.
Several Film versions of Mark Twain's novels "The Adventures of Tom Sawyer" and "The Adventures of Huckleberry Finn" have been made.
"Meet Me in St. Louis", a musical involving the 1904 St. Louis World's Fair, starred Judy Garland.
Part of the 1983 road movie National Lampoon's Vacation was shot on location in Missouri, for the Griswold's trip from Chicago to Los Angeles.
The Thanksgiving holiday film Planes, Trains, and Automobiles was partially shot at Lambert–St. Louis International Airport.
"White Palace" was filmed in St. Louis.
The award-winning 2010 film "Winter's Bone" was shot in the Ozarks of Missouri.
Up in the Air starring George Clooney was filmed in St. Louis.
John Carpernter's "Escape from New York" was filmed in Saint Louis in the early eighties, due to the high number of abandoned buildings in the city.
Part of the 1973 movie, "Paper Moon", which starred Ryan and Tatum O'Neal, was filmed in St. Joseph.
Most of HBO's film "Truman" were filmed in Kansas City, Independence, and the surrounding area. Gary Sinise won an Emmy for his portrayal of Harry Truman in the 1995 film.
"Ride With the Devil" starring Jewel and Tobey Maguire were also filmed in the countryside of Jackson County (also where the historic events of the film took place).
"Gone Girl", A 2014 film starring Ben Affleck, Rosamund Pike, Neil Patrick Harris, and Tyler Perry was filmed in "Cape Girardeau".
Sports.
Missouri hosted the 1904 Summer Olympics at St. Louis, the first time the games were hosted in the United States.
Naval vessels.
Four US Navy vessels have been named after the state.
Wildlife.
Missouri is home to a diversity of both flora and fauna. There is a large amount of fresh water present due to the Mississippi River, Missouri River, and Lake of the Ozarks, with numerous smaller tributary rivers, streams, and lakes. North of the Missouri River, the state is primarily rolling hills of the Great Plains, whereas south of the Missouri River, the state is dominated by the Oak-Hickory Central U.S. hardwood forest.
Some of the native species found in Missouri include:
Mammals.
Within historic times, pronghorn, gray wolf, and brown bear were all found in Missouri, but have since been eliminated. Wapiti and American bison were formerly common, but are currently confined to private farms and parks.
Birds.
Year-round:
Summer/breeders: 
Winter residents: 
Within historic times, the passenger pigeon, the carolina parakeet, and the ivory-billed woodpecker were all found in Missouri, but they have since been eliminated.
Reptiles and amphibians.
Reptiles: 
Amphibians:
Trees and shrubs.
The trees and shrubs growing in Missouri include the following:
Insect migrations.
There has also been a migration of insects from the south to Missouri. One example of this is the wasp "Polistes exclamans".
Famous Missourians.
See entire collection at List of people from Missouri.

</doc>
<doc id="19574" url="https://en.wikipedia.org/wiki?curid=19574" title="Monitor">
Monitor

Monitor or monitor may refer to:

</doc>
<doc id="19577" url="https://en.wikipedia.org/wiki?curid=19577" title="Moses">
Moses

Moses (; , Modern ' Tiberian ' ISO 259-3 '; "Moushe"; '; "" in both the Septuagint and the New Testament) is a prophet in Abrahamic religions. According to the Hebrew Bible, he was a former Egyptian prince who later in life became a religious leader and lawgiver, to whom the authorship of the Torah is traditionally attributed. Also called "Moshe Rabbenu" in Hebrew (, "lit." "Moses our Teacher"), he is the most important prophet in Judaism. He is also an important prophet in Christianity, Islam, Baha'ism as well as a number of other faiths.
According to the Book of Exodus, Moses was born in a time when his people, the Israelites, an enslaved minority, were increasing in numbers and the Egyptian Pharaoh was worried that they might ally with Egypt's enemies. Moses' Hebrew mother, Jochebed, secretly hid him when the Pharaoh ordered all newborn Hebrew boys to be killed in order to reduce the population of the Israelites. Through the Pharaoh's daughter (identified as Queen Bithia in the Midrash), the child was adopted as a foundling from the Nile river and grew up with the Egyptian royal family. After killing an Egyptian slavemaster (because the slavemaster was smiting a Hebrew to death), Moses fled across the Red Sea to Midian, where he encountered the God of Israel speaking to him from within a "burning bush which was not consumed by the fire" on Mount Horeb (which he regarded as the Mountain of God).
God sent Moses back to Egypt to demand the release of the Israelites from slavery. Moses said that he could not speak with assurance or eloquence, so God allowed Aaron, his brother, to become his spokesperson. After the Ten Plagues, Moses led the Exodus of the Israelites out of Egypt and across the Red Sea, after which they based themselves at Mount Sinai, where Moses received the Ten Commandments. After 40 years of wandering in the desert, Moses died within sight of the Promised Land.
Rabbinical Judaism calculated a lifespan of Moses corresponding to 1391–1271 (120 years) BCE; Jerome gives 1592 BCE, and James Ussher 1571 BCE as his birth year. Moses may have flourished during c. 1400 BCE-C.1201 BCE.
The current scholarly consensus is that Moses is a legendary figure and not a historical person. 
Name.
The Biblical account of Moses' birth provides him with a folk etymology to explain the ostensible meaning of his name. He is said to have received it from Pharaoh's daughter: "he became her son. She named him Moses (Moshe), saying, "I drew him out ("meshitihu") of the water." This explanation links it to a verb, "mashah", meaning "to draw out", which makes the Pharaoh's daughter's declaration a play on words. The princess made a grammatical mistake which is prophetic of his role in the future life, as someone who will "draw the people of Israel out of Egypt through the waters of the Red Sea."
Several etymologies have been proposed. An Egyptian root "msy", 'child of', has been considered as a possible etymology, arguably an abbreviation of a theophoric name, as for example in Egyptian names like Thutmoses (Thoth created him) and Ramesses (Ra created him), with the god's name omitted. Another hypothesis would link it to an Egyptian word "to conceive". Abraham Yahuda, based on the spelling given in the Tanakh, argues that it combines 'water' or 'seed' and 'pond, expanse of water', thus yielding the sense of 'child of the Nile' ("mw-še").
The pun on his name in the Tanakh led to speculation in later Jewish tradition. The Hebrew etymology in the Biblical story may reflect an attempt to cancel out traces of Moses's Egyptian origins. The Egyptian character of his name was recognized as such by ancient Jewish writers like Philo of Alexandria and Josephus. Philo linked Mōēsēs (Μωησής) to the Egyptian(Coptic) word for water ("mou"/μῶυ), while Josephus, in his Antiquities of the Jews claimed that the second element, "-esês", meant 'those who are saved'. The problem of how an Egyptian princess, known to Josephus as Thermutis (identified as Tharmuth) and in later Jewish tradition as Bithiah, could have known Hebrew puzzled medieval Jewish commentater like Abraham ibn Ezra and Hezekiah ben Manoah, known also as Hizkuni. Hizkuni suggested she either converted or took a tip from Jochebed.
Biblical narrative.
Prophet and deliverer of Israel.
The Israelites had settled in the Land of Goshen in the time of Joseph and Jacob, but a new pharaoh arose who oppressed the children of Israel. At this time Moses was born to his father Amram, son of Kohath the Levite, who entered Egypt with Jacob's household; his mother was Jochebed (also Yocheved), who was kin to Kohath. Moses had one older (by seven years) sister, Miriam, and one older (by three years) brother, Aaron.
Pharaoh had commanded that all male Hebrew children born be drowned in the river Nile, but Moses' mother placed him in an ark and concealed the ark in the bulrushes by the riverbank, where the baby was discovered and adopted by Pharaoh's daughter. One day after Moses had reached adulthood he killed an Egyptian who was beating a Hebrew. Moses, in order to escape Pharaoh's death penalty, fled to Midian (a desert country south of Judah).
There, on Mount Horeb, God revealed to Moses his name YHWH (probably pronounced Yahweh) and commanded him to return to Egypt and bring his Chosen People (Israel) out of bondage and into the Promised Land (Canaan). Moses returned to carry out God's command, but God caused Pharaoh to refuse, and only after God had subjected Egypt to ten plagues did Pharaoh relent. Moses led the Israelites to the border of Egypt, but there God hardened Pharaoh's heart once more, so that he could destroy Pharaoh and his army at the Red Sea Crossing as a sign of his power to Israel and the nations.
From Egypt, Moses led the Israelites to Mount Sinai, where he was given ten commandments from God, written on stone tablets. However, since Moses remained a long time on the mountain, some of the people feared that he might be dead, so they made a golden statue of a calf and worshipped it, thus disobeying and angering God and Moses. Moses, out of anger, broke the tablets, and later ordered the elimination of those who had worshipped the golden statue, which was melted down and fed to the idolaters. He also wrote the ten commandments on a new set of tablets. Later at Mount Sinai, Moses and the elders entered into a covenant, by which Israel would become the people of YHWH, obeying his laws, and YHWH would be their god. Moses delivered laws of God to Israel, instituted the priesthood under the sons of Moses' brother Aaron, and destroyed those Israelites who fell away from his worship. In his final act at Sinai, God gave Moses instructions for the Tabernacle, the mobile shrine by which he would travel with Israel to the Promised Land.
From Sinai, Moses led the Israelites to the Desert of Paran on the border of Canaan. From there he sent twelve spies into the land. The spies returned with samples of the land's fertility, but warned that its inhabitants were giants. The people were afraid and wanted to return to Egypt, and some rebelled against Moses and against God. Moses told the Israelites that they were not worthy to inherit the land, and would wander the wilderness for forty years until the generation who had refused to enter Canaan had died, so that it would be their children who would possess the land.
When the forty years had passed, Moses led the Israelites east around the Dead Sea to the territories of Edom and Moab. There they escaped the temptation of idolatry, received God's blessing through Balaam the prophet, and massacred the Midianites, who by the end of the Exodus journey had become the enemies of the Israelites. Moses was twice given notice that he would die before entry to the Promised Land: in Numbers 27:13, once he had seen the Promised Land from a viewpoint on Mount Abarim, and again in Numbers 31:1 once battle with the Midianites had been won.
On the banks of the Jordan, in sight of the land, Moses assembled the tribes. After recalling their wanderings he delivered God's laws by which they must live in the land, sang a song of praise and pronounced a blessing on the people, and passed his authority to Joshua, under whom they would possess the land. Moses then went up Mount Nebo to the top of Pisgah, looked over the promised land of Israel spread out before him, and died, at the age of one hundred and twenty. More humble than any other man (Num. 12:3), "there hath not arisen a prophet since in Israel like unto Moses, whom YHWH knew face to face" (Deuteronomy 34:10). The New Testament states that after Moses' death, Michael the Archangel and the devil disputed over his body (Jude 1:9).
Lawgiver of Israel.
Moses is honoured among Jews today as the "lawgiver of Israel", and he delivers several sets of laws in the course of the four books. The first is the Covenant code, Exodus 19–24, the terms of the covenant which God offers to Israel at the foot of Sinai. Embedded in the covenant are the Decalogue (the Ten Commandments, Exodus 20:1–17) and the Book of the Covenant (Exodus 20:22–23:19). The entire Book of Leviticus constitutes a second body of law, the Book of Numbers begins with yet another set, and the Book of Deuteronomy another.
Moses has traditionally been regarded as the author of those four books and the Book of Genesis, which together comprise the Torah, the first and most revered section of the Jewish Bible.
Historicity.
The current scholarly consensus is that Moses is a figure of legend, not of history. Some scholars, like Frank Cross, consider it possible that a "Moses group" might have made a transit along the route from Egypt to Edom around the 13th-12th centuries.
No Egyptian sources mention Moses or the events of Exodus-Deuteronomy, nor has any archaeological evidence been discovered in Egypt or the Sinai wilderness to support the story in which he is the central figure. The story of his being placed in a wicker basket covered with tar and pitch and left among reeds on the waters of the Nile (Exodus 2:3) picks up a familiar motif in Near Eastern mythological accounts of the ruler who rises from humble origins. Thus Sargon of Akkad's Sumerian account of his origins runs;
"My mother, the high priestess, conceived; in secret she bore me"
"She set me in a basket of rushes, with bitumen she sealed my lid" 
"She cast me into the river which rose over me."
The tradition of Moses as a lawgiver and culture hero of the Israelites may go back to the 7th-century sources of the Deuteronomist, which might conserve earlier traditions. Kenneth Kitchen, a solitary voice among British Egyptologists, argues that there is an historic core behind the Exodus, with Egyptian corvée labour exacted from Hebrews during the imperialist control exercised by the Egyptian Empire over Canaan from the time of the Thutmosides down to the revolt against Merenptah and Rameses III. William Albright believed in the essential historicity of the biblical tales of Moses and the Exodus, accepting however that the core narrative had been overlaid by legendary accretions. Biblical minimalists such as Philip R. Davies and Niels Peter Lemche regard all biblical books, and the stories of an Exodus, united monarchy, exile and return as fictions composed by a social elite in Yehud in the Persian period or even later, the purpose being to legitimize a return to indigenous roots.
Despite the imposing fame associated with Moses, no source mentions him until he emerges in texts associated with the Babylonian exile. A theory developed by Cornelius Tiele in 1872, which had proved influential, and still held in regard by modern scholars, argued that Yahweh was a Midianite god, introduced to the Israelites by Moses, whose father-in-law Jethro was a Midianite priest. It was to such a Moses that Yahweh reveals his real name, hidden from the Patriarchs who knew him only as El Shaddai, Against this view is the modern consensus that most of the Israelites were native to Palestine. Martin Noth argued that the Pentateuch uses the figure of Moses, originally linked to legends of a Transjordan conquest, as a narrative bracket or late redactional device to weld together 4 of the 5, originally independent, themes of that work.
Manfred Görg, and Rolf Krauss the latter in a somewhat sensationalist manner, have suggested that the Moses story is a distortion or transmogrification of the historical pharaoh Amenmose (ca. 1200 BCE), who was dismissed from office and whose name was later simplified to "msy" (Mose). Aidan Dodson regards this hypothesis as "intriguing, but beyond proof."
The Exodus narrative, which in traditional chronology begins with the impossible date of 1496 BCE, itself has resisted numerous attempts to verify it or ground it in archaeological digs, which have been abandoned as a "fruitless pursuit," since the evidence points to an indigenous origin for Israelites. Attempts to locate the "yam sūp" ("Reed sea"/Red Sea) as described in Exodus have failed. The figure of 600,000 adult males described in Exodus 12:37, or 603,550 at Exodus 38:26, would imply a total population of Israelites in flight through the desert for 40 years of 2 to 2.5 million people, when the total population of Egypt at the time was 3 to 4.5 million. Had such a catastrophic demographic outflow taken place, it would have been recorded in Egyptian writings.
The name King Mesha of Moab has been linked to that of Moses:Mesha also is associated with narratives of an exodus and a conquest, and several motifs in stories about him are shared with the Exodus tale and that regarding Israel's war with Moab (2 Kings:3). Moab rebels against oppression, like Moses, leads his people out of Israel, as Moses does from Egypt, and his first-born son is slaughtered at the wall of Kir-hareseth as the firstborn of Israel are condemned to slaughter in the Exodus story, "an infernal passover that delivers Mesha while wrath burns against his enemies".
An Egyptian version of the tale that crosses over with the Moses story is found in Manetho who, according to the summary in Josephus, wrote that a certain Osarseph, Heliopolitan priest, became overseer of a band of lepers, when Amenophis, following indications by Amenhotep, son of Hapu, had all the lepers in Egypt quarantined in order to cleanse the land so that he might see the gods. The lepers are bundled into Avaris, the former capital of the Hyksos, where Osarseph prescribes for them everything forbidden in Egypt, while proscribing everything permitted in Egypt. They invite the Hyksos to reinvade Egypt, rule with them for 13 years – Osarseph then assumes the name Moses - and are then driven out.
Sources.
Apart from a few scattered references elsewhere in the Jewish scriptures, all that is known about Moses comes from the books of Exodus, Leviticus, Numbers and Deuteronomy. The majority of scholars consider these books to go back to the Persian period, 538–332 BCE.
Moses in Hellenistic literature.
Non-biblical writings about Jews, with references to the role of Moses, first appear at the beginning of the Hellenistic period, from 323 BCE to about 146 BCE. Shmuel notes that "a characteristic of this literature is the high honour in which it holds the peoples of the East in general and some specific groups among these peoples."
In addition to the Judeo-Roman or Judeo-Hellenic historians Artapanus, Eupolemus, Josephus, and Philo, a few non-Jewish historians including Hecataeus of Abdera (quoted by Diodorus Siculus), Alexander Polyhistor, Manetho, Apion, Chaeremon of Alexandria, Tacitus and Porphyry also make reference to him. The extent to which any of these accounts rely on earlier sources is unknown. Moses also appears in other religious texts such as the Mishnah (c. 200 CE), Midrash (200–1200 CE), and the Qur'an (c. 610–53).
The figure of Osarseph in Hellenistic historiography is a renegade Egyptian priest who leads an army of lepers against the pharaoh and is finally expelled from Egypt, changing his name to Moses.
In Hecataeus.
The earliest existing reference to Moses in Greek literature occurs in the Egyptian history of Hecataeus of Abdera (4th century BCE). All that remains of his description of Moses are two references made by Diodorus Siculus, wherein, writes historian Arthur Droge, "he describes Moses as a wise and courageous leader who left Egypt and colonized Judaea." Among the many accomplishments described by Hecataeus, Moses had founded cities, established a temple and religious cult, and issued laws:
Droge also points out that this statement by Hecataeus was similar to statements made subsequently by Eupolemus.
In Artapanus.
The Jewish historian Artapanus of Alexandria (2nd century BCE), portrayed Moses as a cultural hero, alien to the Pharaonic court. According to theologian John Barclay, the Moses of Artapanus "clearly bears the destiny of the Jews, and in his personal, cultural and military splendor, brings credit to the whole Jewish people."
Artapanus goes on to relate how Moses returns to Egypt with Aaron, and is imprisoned, but miraculously escapes through the name of YHWH in order to lead the Exodus. This account further testifies that all Egyptian temples of Isis thereafter contained a rod, in remembrance of that used for Moses' miracles. He describes Moses as 80 years old, "tall and ruddy, with long white hair, and dignified."
Some historians, however, point out the "apologetic nature of much of Artapanus' work," with his addition extra-biblical details, as with references to Jethro: the non-Jewish Jethro expresses admiration for Moses' gallantry in helping his daughters, and chooses to adopt Moses as his son.
In Strabo.
Strabo, a Greek historian, geographer and philosopher, in his "Geography" (c. 24 CE), wrote in detail about Moses, whom he considered to be an Egyptian who deplored the situation in his homeland, and thereby attracted many followers who respected the deity. He writes, for example, that Moses opposed the picturing of the deity in the form of man or animal, and was convinced that the deity was an entity which encompassed everything – land and sea:
In Strabo's writings of the history of Judaism as he understood it, he describes various stages in its development: from the first stage, including Moses and his direct heirs; to the final stage where "the Temple of Jerusalem continued to be surrounded by an aura of sanctity." Strabo's "positive and unequivocal appreciation of Moses' personality is among the most sympathetic in all ancient literature." His portrayal of Moses is said to be similar to the writing of Hecataeus who "described Moses as a man who excelled in wisdom and courage."
Egyptologist Jan Assmann concludes that Strabo was the historian "who came closest to a construction of Moses' religion as monotheism and as a pronounced counter-religion." It recognized "only one divine being whom no image can represent... the only way to approach this god is to live in virtue and in justice."
In Tacitus.
The Roman historian Tacitus (c. 56–120 CE) refers to Moses by noting that the Jewish religion was monotheistic and without a clear image. His primary work, wherein he describes Jewish philosophy, is his "Histories" (c. 100), where, according to Murphy, as a result of the Jewish worship of one God, "pagan mythology fell into contempt." Tacitus states that, despite various opinions current in his day regarding the Jews' ethnicity, most of his sources are in agreement that there was an Exodus from Egypt. By his account, the Pharaoh Bocchoris, suffering from a plague, banished the Jews in response to an oracle of the god Zeus-Amun.
In this version, Moses and the Jews wander through the desert for only six days, capturing the Holy Land on the seventh.
In Longinus.
The Septuagint, the Greek version of the Hebrew Bible, influenced Longinus, who may have been the author of the great book of literary criticism, "On the Sublime". The date of composition is unknown, but it is commonly assigned to the late Ist century C.E.
The writer quotes Genesis in a "style which presents the nature of the deity in a manner suitable to his pure and great being," however he does not mention Moses by name, calling him 'no chance person' (οὐχ ὁ τυχὼν ἀνήρ ) but "the Lawgiver" (θεσμοθέτης) of the Jews," a term that puts him on a par with Lycurgus and Minos. Aside from a reference to Cicero, Moses is the only non-Greek writer quoted in the work, contextually he is put on a par with Homer, and he is described "with far more admiration than even Greek writers who treated Moses with respect, such as Hecataeus and Strabo.
In Josephus.
In Josephus' (37 – c. 100 CE) "Antiquities of the Jews", Moses is mentioned throughout. For example Book VIII Ch. IV, describes Solomon's Temple, also known as the First Temple, at the time the Ark of the Covenant was first moved into the newly built temple:
According to Feldman, Josephus also attaches particular significance to Moses' possession of the "cardinal virtues of wisdom, courage, temperance, and justice." He also includes piety as an added fifth virtue. In addition, he "stresses Moses' willingness to undergo toil and his careful avoidance of bribery. Like Plato's philosopher-king, Moses excels as an educator."
In Numenius.
Numenius, a Greek philosopher who was a native of Apamea, in Syria, wrote during the latter half of the 2nd century CE. Historian Kennieth Guthrie writes that "Numenius is perhaps the only recognized Greek philosopher who explicitly studied Moses, the prophets, and the life of Jesus..." He describes his background:
In Justin Martyr.
The Christian saint and religious philosopher Justin Martyr (103–165 CE) drew the same conclusion as Numenius, according to other experts. Theologian Paul Blackham notes that Justin considered Moses to be "more trustworthy, profound and truthful because he is "older" than the Greek philosophers." He quotes him:
Moses in Abrahamic religions.
Judaism.
There is a wealth of stories and additional information about Moses in the Jewish apocrypha and in the genre of rabbinical exegesis known as Midrash, as well as in the primary works of the Jewish oral law, the Mishnah and the Talmud. Moses is also given a number of bynames in Jewish tradition. The Midrash identifies Moses as one of seven biblical personalities who were called by various names. Moses' other names were: Jekuthiel (by his mother), Heber (by his father), Jered (by Miriam), Avi Zanoah (by Aaron), Avi Gedor (by Kohath), Avi Soco (by his wet-nurse), Shemaiah ben Nethanel (by people of Israel). Moses is also attributed the names Toviah (as a first name), and Levi (as a family name) (Vayikra Rabbah 1:3), Heman, Mechoqeiq (lawgiver) and Ehl Gav Ish (Numbers 12:3).
Jewish historians who lived at Alexandria, such as Eupolemus, attributed to Moses the feat of having taught the Phoenicians their alphabet, similar to legends of Thoth. Artapanus of Alexandria explicitly identified Moses not only with Thoth/Hermes, but also with the Greek figure Musaeus (whom he called "the teacher of Orpheus"), and ascribed to him the division of Egypt into 36 districts, each with its own liturgy. He named the princess who adopted Moses as Merris, wife of Pharaoh Chenephres.
Ancient sources mention an Assumption of Moses and a Testimony of Moses. A Latin text was found in Milan in the 19th century by Antonio Ceriani who called it the Assumption of Moses, even though it does not refer to an assumption of Moses or contain portions of the Assumption which are cited by ancient authors, and it is apparently actually the Testimony. The incident which the ancient authors cite is also mentioned in the Epistle of Jude.
To Orthodox Jews, Moses is called "Moshe Rabbenu, `Eved HaShem, Avi haNeviim zya"a": "Our Leader Moshe, Servant of God, Father of all the Prophets (may his merit shield us, amen)". In the orthodox view, Moses received not only the Torah, but also the revealed (written and oral) and the hidden (the "`hokhmat nistar" teachings, which gave Judaism the Zohar of the Rashbi, the Torah of the Ari haQadosh and all that is discussed in the Heavenly Yeshiva between the Ramhal and his masters). He is also considered the greatest prophet.
Arising in part from his age, but also because 120 is elsewhere stated as the maximum age for Noah's descendants (one interpretation of ), "may you live to 120" has become a common blessing among Jews.
Christianity.
For Christians, Moses—mentioned more often in the New Testament than any other Old Testament figure—is often a symbol of God's law, as reinforced and expounded on in the teachings of Jesus. New Testament writers often compared Jesus' words and deeds with Moses' to explain Jesus' mission. In Acts 7:39–43, 51–53, for example, the rejection of Moses by the Jews who worshiped the golden calf is likened to the rejection of Jesus by the Jews that continued in traditional Judaism.
Moses also figures in several of Jesus' messages. When he met the Pharisee Nicodemus at night in the third chapter of the Gospel of John, he compared Moses' lifting up of the bronze serpent in the wilderness, which any Israelite could look at and be healed, to his own lifting up (by his death and resurrection) for the people to look at and be healed. In the sixth chapter, Jesus responded to the people's claim that Moses provided them "manna" in the wilderness by saying that it was not Moses, but God, who provided. Calling himself the "bread of life", Jesus stated that He was provided to feed God's people.
Moses, along with Elijah, is presented as meeting with Jesus in all three Gospel accounts of the Transfiguration of Jesus in Matthew 17, Mark 9, and Luke 9, respectively. Later Christians found numerous other parallels between the life of Moses and Jesus to the extent that Jesus was likened to a "second Moses." For instance, Jesus' escape from the slaughter by Herod in Bethlehem is compared to Moses' escape from Pharaoh's designs to kill Hebrew infants. Such parallels, unlike those mentioned above, are not pointed out in Scripture. See the article on typology.
His relevance to modern Christianity has not diminished. Moses is considered to be a saint by several churches; and is commemorated as a prophet in the respective Calendars of Saints of the Eastern Orthodox Church, Roman Catholic Church, and Lutheran churches on September 4. The Orthodox Church also commemorates him on the Sunday of the Forefathers, two Sundays before the Nativity.
The Armenian Apostolic Church commemorates him as one of the Holy Forefathers in their Calendar of Saints on July 30.
Mormonism.
Members of The Church of Jesus Christ of Latter-day Saints (colloquially called Mormons) generally view Moses in the same way that other Christians do. However, in addition to accepting the biblical account of Moses, Mormons include Selections from the Book of Moses as part of their scriptural canon. This book is believed to be the translated writings of Moses, and is included in the Pearl of Great Price.
Latter-day Saints are also unique in believing that Moses was taken to heaven without having tasted death (translated). In addition, Joseph Smith and Oliver Cowdery stated that on April 3, 1836, Moses appeared to them in the Kirtland Temple in a glorified, immortal, physical form and bestowed upon them the "keys of the gathering of Israel from the four parts of the earth, and the leading of the ten tribes from the land of the north."
Islam.
Moses is mentioned more in the Quran than any other individual and his life is narrated and recounted more than that of any other Islamic prophet. In general, Moses is described in ways which parallel the Islamic prophet Muhammad, and "his character exhibits some of the main themes of Islamic theology," including the "moral injunction that we are to submit ourselves to God."
Moses is defined in the Qur'an as both prophet ("nabi") and messenger ("rasul"), the latter term indicating that he was one of those prophets who brought a scripture and law to his people.
Moses is mentioned 502 times in the Qur'an; passages mentioning Moses include 2.49–61, 7.103–160, 10.75–93, 17.101–104, 20.9–97, 26.10–66, 27.7–14, 28.3–46, 40.23–30, 43.46–55, 44.17–31, and 79.15–25. and many others. Most of the key events in Moses' life which are narrated in the Bible are to be found dispersed through the different Surahs of Qur'an, with a story about meeting Khidr which is not found in the Bible.
In the Moses story related by the Qur'an, Jochebed is commanded by God to place Moses in an ark and cast him on the waters of the Nile, thus abandoning him completely to God's protection. Pharaoh's wife Asiya, not his daughter, found Moses floating in the waters of the Nile. She convinced Pharaoh to keep him as their son because they were not blessed with any children.
The Qur'an's account has emphasized Moses' mission to invite the Pharaoh to accept God's divine message as well as give salvation to the Israelites. According to the Qur'an, Moses encourages the Israelites to enter Canaan, but they are unwilling to fight the Canaanites, fearing certain defeat. Moses responds by pleading to Allah that he and his brother Aaron be separated from the rebellious Israelites. After which the Israelites are made to wander for 40 years.
According to Islamic tradition, Moses is buried at Maqam El-Nabi Musa, Jericho.
Baha'i Faith.
Moses is one of the most important prophets in the Baha'i Faith. He is considered to be a messenger from God who is equally authentic as those sent in other eras. An epithet of Moses in Baha'i scriptures is "Interlocutor of God," or alternatively the One Who Conversed with God.
Important figures in the Baha’i religion, such as Abdul’l-Baha, have highlighted the fact that Moses, like Abraham, had none of the makings of a great man of history, but through God's assistance he was able achieve many great things. He is described as having been "for a long time a shepherd in the wilderness," of having had a stammer, and of being "much hated and detested" by the Pharaoh and the ancient Egyptians of his time. He is said to have been raised in an oppressive household, and to have been known, in Egypt, as a man who had committed murder – though he had done so in order to prevent an act of cruelty.
Nevertheless, like Abraham, through the assistance of God, he achieved great things and gained renown even beyond the Levant. Chief among these achievements was the freeing of his people, the Hebrews, from bondage in Egypt and leading "them to the Holy Land." He is viewed as the one who bestowed on Israel 'the religious and the civil law' which gave them "honour among all nations," and which spread their fame to different parts of the world.
Furthermore, through the law, Moses is believed to have led the Hebrews 'to the highest possible degree of civilization at that period.’ Abdul’l-Baha asserts that the ancient Greek philosophers regarded "the illustrious men of Israel as models of perfection." Chief among these philosophers, he says, was Socrates who "visited Syria, and took from the children of Israel the teachings of the Unity of God and of the immortality of the soul."
Moses is further described as paving the way for Baha'ullah and his ultimate revelation, and as a teacher of truth, whose teachings were in line with the customs of his time.
Modern reception.
Politics and law.
In a metaphorical sense in the Christian tradition, a "Moses" has been referred to as the leader who delivers the people from a terrible situation. Among the presidents known to have used the symbolism of Moses were Harry S. Truman, Jimmy Carter, Ronald Reagan, Bill Clinton, George W. Bush and Barack Obama, who referred to his supporters as "the Moses generation."
Winston Churchill, in his essay called "Moses—the Leader of a People", published in 1931, used the story of Moses to convince the British population of its need for strong leadership, and that "human success depends on the favor of God." He saw Moses as more than a metaphor, however, rejecting as "myth" the assertions that Moses was only a legendary figure.
He described him as "the supreme law-giver, who received from God that remarkable code upon which the religious, moral, and social life of the nation was so securely founded… one of the greatest human beings with the most decisive leap forward ever discernable in the human story." Churchill also noted the relevance of the story of Moses to modern Britain: "We may believe that they happened to a people not so very different from ourselves..."
In his essay, Churchill implied that the Ten Commandments were a primary set of laws, "Here Sinai Moses received from the tables of those fundamental laws which were henceforth to be followed, with occasional lapses, by the highest forms of human society."
In subsequent years, theologians linked the Ten Commandments with the formation of early democracy. Scottish theologian William Barclay described them as "the universal foundation of all things… the law without which nationhood is impossible. …Our society is founded upon it. Pope Francis addressed the U.S. Congress in 2015 stating that all people need to "keep alive their sense of unity by means of just legislation... the figure of Moses leads us directly to God and thus to the transcendent dignity of the human being.
American history.
Pilgrims.
References to Moses were used by the Puritans, who relied on the story of Moses to give meaning and hope to the lives of Pilgrims seeking religious and personal freedom in America. John Carver was the first governor of Plymouth colony and first signer of the Mayflower Compact, which he wrote in 1620 during the ship's three-month voyage. He inspired the Pilgrims with a "sense of earthly grandeur and divine purpose," notes historian Jon Meacham, and was called the "Moses of the Pilgrims." Early American writer James Russell Lowell noted the similarity of the founding of America by the Pilgrims to that of ancient Israel by Moses:
Following Carver's death the following year, William Bradford was made governor. He feared that the remaining Pilgrims would not survive the hardships of the new land, with half their people having already died within months of arriving. Bradford evoked the symbol of Moses to the weakened and desperate Pilgrims to help calm them and give them hope: "Violence will break all. Where is the meek and humble spirit of Moses?" William G. Dever explains the attitude of the Pilgrims: "We considered ourselves the 'New Israel,' particularly we in America. And for that reason we knew who we were, what we believed in and valued, and what our 'manifest destiny' was."
Founding fathers.
On July 4, 1776, immediately after the Declaration of Independence was officially passed, the Continental Congress asked John Adams, Thomas Jefferson, and Benjamin Franklin to design a seal that would clearly represent a symbol for the new United States. They chose the symbol of Moses leading the Israelites to freedom. The founding fathers inscribed the words of Moses on the Liberty Bell: "Proclaim Liberty thro' all the Land to all the Inhabitants thereof." (Levit. 25)
Upon the death of George Washington in 1799, two thirds of his eulogies referred to him as "America's Moses," with one orator saying that "Washington has been the same to us as Moses was to the Children of Israel."
Benjamin Franklin, in 1788, saw the difficulties that some of the newly independent American states were having in forming a government, and proposed that until a new code of laws could be agreed to, they should be governed by "the laws of Moses," as contained in the Old Testament. He justified his proposal by explaining that the laws had worked in biblical times: "The Supreme Being… having rescued them from bondage by many miracles, performed by his servant Moses, he personally delivered to that chosen servant, in the presence of the whole nation, a constitution and code of laws for their observance.
John Adams, America's 2nd president, stated why he relied on the laws of Moses over Greek philosophy for establishing the Constitution: "As much as I love, esteem, and admire the Greeks, I believe the Hebrews have done more to enlighten and civilize the world. Moses did more than all their legislators and philosophers. Swedish historian Hugo Valentin credited Moses as the "first to proclaim the rights of man."
Slavery and civil rights.
Historian Gladys L. Knight describes how leaders who emerged during slavery time and after often personified the Moses symbol. "The symbol of Moses was empowering in that it served to amplify a need for freedom." Therefore, when Abraham Lincoln was assassinated in 1865 after freeing the slaves, black Americans said they had lost "their Moses". Lincoln biographer Charles Carleton Coffin writes, "The millions whom Abraham Lincoln delivered from slavery will ever liken him to Moses, the deliverer of Israel." Similarly, Harriet Tubman, who rescued approximately seventy enslaved family and friends, was also described as the "Moses" of her people.
In the 1960s, a leading figure in the civil rights movement was Martin Luther King, Jr., who was called "a modern Moses," and often referred to Moses in his speeches: "The struggle of Moses, the struggle of his devoted followers as they sought to get out of Egypt. This is something of the story of every people struggling for freedom."
Literature.
Thomas Mann's novella "The Tables of the Law" is a retelling of the story of the exodus from Egypt, with Moses as its main character.
In Freud.
Sigmund Freud, in his last book, "Moses and Monotheism" in 1939, postulated that Moses was an Egyptian nobleman who adhered to the monotheism of Akhenaten. Following a theory proposed by a contemporary biblical critic, Freud believed that Moses was murdered in the wilderness, producing a collective sense of patricidal guilt that has been at the heart of Judaism ever since. "Judaism had been a religion of the father, Christianity became a religion of the son", he wrote. The possible Egyptian origin of Moses and of his message has received significant scholarly attention.
Opponents of this view observe that the religion of the Torah seems different from Atenism in everything except the central feature of devotion to a single god, although this has been countered by a variety of arguments, e.g. pointing out the similarities between the Hymn to Aten and Psalm 104. Freud's interpretation of the historical Moses is not well accepted among historians, and is considered pseudohistory by many.
Criticism of Moses.
Moses' prominence in religious literature has made him a popular target for biblical critics, most of whom question his reputation as a just and compassionate leader, drawing attention to certain passages in which he appears to display a more brutal and unforgiving side. Given his holy status in the minds of Jews, Christians and Muslims, criticism of Moses' life and teachings has been for the most part by deists, agnostics and atheists.
Thomas Paine and Numbers 31:13-18.
In the late eighteenth century, the deist Thomas Paine commented at length on Moses' Laws in "The Age of Reason". Paine considered Moses to be a "detestable villain", and cited as an example of his "unexampled atrocities". In the passage, the Jewish army had returned from conquering the Midianites, and Moses has gone down to meet it:
The prominent atheist Richard Dawkins also made reference to these verses in his 2006 book, "The God Delusion", concluding that Moses was "not a great role model for modern moralists".
However, some Jewish sources defend Moses' role. The Chasam Sofer emphasizes that this war was not fought at Moses' behest, but was commanded by God as an act of revenge against the Midianite women, who, according to the Biblical account, had seduced the Israelites and led them to sin. Rabbi Joel Grossman argued that the story is a "powerful fable of lust and betrayal", and that Moses' execution of the women was a symbolic condemnation of those who seek to turn sex and desire to evil purposes. Alan Levin, an educational specialist with the Reform movement, has similarly suggested that the story should be taken as a cautionary tale, to "warn successive generations of Jews to watch their own idolatrous behavior".
Figurative art.
Moses is depicted in several U.S. government buildings because of his legacy as a lawgiver. In the Library of Congress stands a large statue of Moses alongside a statue of the Apostle Paul. Moses is one of the 23 lawgivers depicted in marble bas-reliefs in the chamber of the U.S. House of Representatives in the United States Capitol. The plaque's overview states: "Moses (c. 1350–1250 B.C.) Hebrew prophet and lawgiver; transformed a wandering people into a nation; received the Ten Commandments."
The other twenty-two figures have their profiles turned to Moses, which is the only forward-facing bas-relief.
Moses appears eight times in carvings that ring the Supreme Court Great Hall ceiling. His face is presented along with other ancient figures such as Solomon, the Greek god Zeus and the Roman goddess of wisdom, Minerva. The Supreme Court building's east pediment depicts Moses holding two tablets. Tablets representing the Ten Commandments can be found carved in the oak courtroom doors, on the support frame of the courtroom's bronze gates and in the library woodwork. A controversial image is one that sits directly above the chief justice's head. In the center of the 40-foot-long Spanish marble carving is a tablet displaying Roman numerals I through X, with some numbers partially hidden.
Michelangelo's statue.
Michelangelo's statue of Moses in the Church of San Pietro in Vincoli, Rome, is one of the most familiar masterpieces in the world. The horns the sculptor included on Moses' head are the result of a mistranslation of the Hebrew Bible into the Latin Vulgate Bible with which Michelangelo was familiar. The Hebrew word taken from "Exodus" means either a "horn" or an "irradiation." Experts at the Archaeological Institute of America show that the term was used when Moses "returned to his people after seeing as much of the Glory of the Lord as human eye could stand," and his face "reflected radiance." In early Jewish art, moreover, Moses is often "shown with rays coming out of his head."
Another author explains, "When Saint Jerome translated the Old Testament into Latin, he thought no one but Christ should glow with rays of light — so he advanced the secondary translation. However, writer J. Stephen Lang points out that Jerome's version actually described Moses as "giving off hornlike rays," and he "rather clumsily translated it to mean 'having horns.'" It has also been noted that he had Moses seated on a throne, yet Moses was never given the title of a King nor ever sat on such thrones.
Film and television.
Moses was portrayed by Theodore Roberts in Cecil B. DeMille's 1923 silent film "The Ten Commandments". Moses appeared as the central character in the 1956 DeMille movie, also called "The Ten Commandments", in which he was portrayed by Charlton Heston. A television remake was produced in 2006.
Burt Lancaster played "Moses" in the 1975 television miniseries "Moses the Lawgiver".
In the 1981 comedy film "History of the World, Part I", Moses was portrayed by Mel Brooks.
Sir Ben Kingsley was the narrator of the 2007 animated film, "The Ten Commandments".
Moses appeared as the central character in the 1998 DreamWorks Pictures' animated movie, "The Prince of Egypt". He was voiced by Val Kilmer.
Christian Bale portrayed Moses in Ridley Scott's 2014 film "" which portrayed Moses and Rameses II as being raised by Seti I as cousins.

</doc>
<doc id="19579" url="https://en.wikipedia.org/wiki?curid=19579" title="Mississippi River">
Mississippi River

The Mississippi River is the chief river of the largest drainage system on the North American continent. Flowing entirely in the United States (although its drainage basin reaches into Canada), it rises in northern Minnesota and meanders slowly southwards for to the Mississippi River Delta at the Gulf of Mexico. With its many tributaries, the Mississippi's watershed drains all or parts of 31 US states and 2 Canadian provinces between the Rocky and Appalachian Mountains. The Mississippi ranks as the fourth longest and ninth largest river in the world by discharge. The river either borders or passes through the states of Minnesota, Wisconsin, Iowa, Illinois, Missouri, Kentucky, Tennessee, Arkansas, Mississippi, and Louisiana.
Native Americans long lived along the Mississippi River and its tributaries. Most were hunter-gatherers, but some, such as the Mound builders, formed prolific agricultural societies. The arrival of Europeans in the 16th century changed the native way of life as first explorers, then settlers, ventured into the basin in increasing numbers. The river served first as a barrier – forming borders for New Spain, New France, and the early United States – then as a vital transportation artery and communications link. In the 19th century, during the height of the ideology of Manifest Destiny, the Mississippi and several western tributaries, most notably the Missouri, formed pathways for the western expansion of the United States.
Formed from thick layers of the river's silt deposits, the Mississippi Embayment is one of the most fertile agricultural regions of the country, which resulted in the river's storied steamboat era. During the American Civil War, the Mississippi's capture by Union forces marked a turning point towards victory because of the river's importance as a route of trade and travel, not least to the Confederacy. Because of substantial growth of cities and the larger ships and barges that supplanted riverboats, the first decades of the 20th century saw the construction of massive engineering works such as levees, locks and dams, often built in combination.
Since modern development of the basin began, the Mississippi has also seen its share of pollution and environmental problems – most notably large volumes of agricultural runoff, which has led to the Gulf of Mexico dead zone off the Delta. In recent years, the river has shown a steady shift towards the Atchafalaya River channel in the Delta; a course change would be an economic disaster for the port city of New Orleans.
Name.
The word itself comes from "Messipi", the French rendering of the Anishinaabe (Ojibwe or Algonquin) name for the river, "Misi-ziibi" (Great River). See below in History section for additional information.
In addition to historical traditions shown by names, there are at least two other measures of a river's identity, one being the largest branch (by water volume), and the other being the longest branch. Using the largest-branch criterion, the Ohio (not the Middle and Upper Mississippi) would be the main branch of the Lower Mississippi. Using the longest-branch criterion, the Middle Mississippi-Missouri-Jefferson-Beaverhead-Red Rock-Hellroaring Creek River would be the main branch. According to either school of thought, the Upper Mississippi from Lake Itasca, Minnesota to St. Louis, despite its name, would only be a secondary tributary of the final river flowing from Cairo, Illinois to the Gulf of Mexico. 
While the Missouri River, flowing from the confluence of the Jefferson, Madison and Gallatin Rivers to the Mississippi, is the longest continuously named river in the United States, the serially named river known sequentially as Hellroaring Creek, Red Rock, Beaverhead, Jefferson, Missouri, Middle Mississippi, and Lower Mississippi, as one continuous waterway, is the longest river in North America and the fourth longest river in the world. Its length of at least is exceeded only by the Nile, the Amazon, and perhaps the Yangtze River among the longest rivers in the world. The source of this waterway is at Brower's Spring, above sea level in southwestern Montana, along the Continental Divide outside Yellowstone National Park. 
In the 18th century, the river was the primary western boundary of the young United States, and since the country's expansion westward, the Mississippi River has been widely considered a convenient if approximate dividing line between the Eastern, Southern, and Midwestern United States, and the Western United States. This is exemplified by the Gateway Arch in St. Louis, and the phrase "Trans-Mississippi" as used in the name of the Trans-Mississippi Exposition. It is common to qualify a regionally superlative landmark in relation to it, such as "the highest peak east of the Mississippi" or "the oldest city west of the Mississippi".
Physical geography.
The geographical setting of the Mississippi River includes considerations of the course of the river itself, its watershed, its outflow, its prehistoric and historic course changes, and possibilities of future course changes. The New Madrid Seismic Zone along the river is also noteworthy. These various basic geographical aspects of the river in turn underlie its human history and present uses of the waterway and its adjacent lands.
Divisions.
The Mississippi River can be divided into three sections: the Upper Mississippi, the river from its headwaters to the confluence with the Missouri River; the Middle Mississippi, which is downriver from the Missouri to the Ohio River; and the Lower Mississippi, which flows from the Ohio to the Gulf of Mexico.
Upper Mississippi.
The Upper Mississippi runs from its headwaters to its confluence with the Missouri River at St. Louis, Missouri. The Upper Mississippi is divided into two sections:
The source of the Upper Mississippi branch is traditionally accepted as Lake Itasca, above sea level in Itasca State Park in Clearwater County, Minnesota. The name "Itasca" was chosen to designate the "true head" of the Mississippi River as a combination of the last four letters of the Latin word for truth ("veritas") and the first two letters of the Latin word for head ("caput"). However, the lake is in turn fed by a number of smaller streams.
From its origin at Lake Itasca to St. Louis, Missouri, the waterway's flow is moderated by 43 dams. Fourteen of these dams are located above Minneapolis in the headwaters region and serve multiple purposes, including power generation and recreation. The remaining 29 dams, beginning in downtown Minneapolis, all contain locks and were constructed to improve commercial navigation of the upper river. Taken as a whole, these 43 dams significantly shape the geography and influence the ecology of the upper river. Beginning just below Saint Paul, Minnesota, and continuing throughout the upper and lower river, the Mississippi is further controlled by thousands of wing dikes that moderate the river's flow in order to maintain an open navigation channel and prevent the river from eroding its banks.
The head of navigation on the Mississippi is the Coon Rapids Dam in Coon Rapids, Minnesota. Before it was built in 1913, steamboats could occasionally go upstream as far as Saint Cloud, Minnesota, depending on river conditions.
The uppermost lock and dam on the Upper Mississippi River is the Upper St. Anthony Falls Lock and Dam in Minneapolis. Above the dam, the river's elevation is . Below the dam, the river's elevation is . This drop is the largest of all the Mississippi River locks and dams. The origin of the dramatic drop is a waterfall preserved adjacent to the lock under an apron of concrete. Saint Anthony Falls is the only true waterfall on the entire Mississippi River. The water elevation continues to drop steeply as it passes through the gorge carved by the waterfall.
After the completion of the St. Anthony Falls Lock and Dam in 1963, the river's head of navigation moved upstream, to the Coon Rapids Dam. However, the Locks were closed in 2015 to control the spread of invasive Asian carp, making Minneapolis once again the site of the head of navigation of the river.
The Upper Mississippi features various natural and artificial lakes, with its widest point being Lake Winnibigoshish, near Grand Rapids, Minnesota, over across. Also of note is Lake Onalaska (created by Lock and Dam No. 7), near La Crosse, Wisconsin, over wide. On the other hand, Lake Pepin is natural, formed due to the delta formed by the Chippewa River of Wisconsin as it enters the Upper Mississippi; it is more than wide.
By the time the Upper Mississippi reaches Saint Paul, Minnesota, below Lock and Dam No. 1, it has dropped more than half its original elevation and is above sea level. From St. Paul to St. Louis, Missouri, the river elevation falls much more slowly, and is controlled and managed as a series of pools created by 26 locks and dams.
The Upper Mississippi River is joined by the Minnesota River at Fort Snelling in the Twin Cities; the St. Croix River near Prescott, Wisconsin; the Cannon River near Red Wing, Minnesota; the Zumbro River at Wabasha, Minnesota; the Black, La Crosse, and Root rivers in La Crosse, Wisconsin; the Wisconsin River at Prairie du Chien, Wisconsin; the Rock River at the Quad Cities; the Iowa River near Wapello, Iowa; the Skunk River south of Burlington, Iowa; and the Des Moines River at Keokuk, Iowa. Other major tributaries of the Upper Mississippi include the Crow River in Minnesota, the Chippewa River in Wisconsin, the Maquoketa River and the Wapsipinicon River in Iowa, and the Illinois River in Illinois.
The Upper Mississippi is largely a multi-thread stream with many bars and islands. From its confluence with the St. Croix River downstream to Dubuque, Iowa, the river is entrenched, with high bedrock bluffs lying on either side. The height of these bluffs decreases to the south of Dubuque, though they are still significant through Savanna, Illinois. This topography contrasts strongly with the Lower Mississippi, which is a meandering river in a broad, flat area, only rarely flowing alongside a bluff (as at Vicksburg, Mississippi).
The Upper Mississippi River is home to over 119 species of fish. Some fish include; walleye, sauger, large mouth bass, small mouth bass, and white bass. Northern pike, bluegill and crappie also reside in the Upper Mississippi River. Other fish like channel catfish, flathead catfish, carp, the common shiner, freshwater drum, paddlefish and shovelnose sturgeon also live in these upper Mississippi waters. The Minnesota Department of Natural Resources has designated much of the Mississippi River in the state as infested waters by the exotic species zebra mussels and Eurasian watermilfoil.
Middle Mississippi.
The Mississippi River is known as the Middle Mississippi from the Upper Mississippi River's confluence with the Missouri River at St. Louis, Missouri, for to its confluence with the Ohio River at Cairo, Illinois.
The Middle Mississippi is relatively free-flowing. From St. Louis to the Ohio River confluence, the Middle Mississippi falls over for an average rate of . At its confluence with the Ohio River, the Middle Mississippi is above sea level. Apart from the Missouri and Meramec rivers of Missouri and the Kaskaskia River of Illinois, no major tributaries enter the Middle Mississippi River.
Lower Mississippi.
The Mississippi River is called the Lower Mississippi River from its confluence with the Ohio River to its mouth at the Gulf of Mexico, a distance of about . At the confluence of the Ohio and the Middle Mississippi, the long-term mean discharge of the Ohio at Cairo, Illinois is , while the long-term mean discharge of the Mississippi at Thebes, Illinois (just upriver from Cairo) is . Thus, by volume, the main branch of the Mississippi River system at Cairo can be considered to be the Ohio River (and the Allegheny River further upstream), rather than the Middle Mississippi.
In addition to the Ohio River, the major tributaries of the Lower Mississippi River are the White River, flowing in at the White River National Wildlife Refuge in east central Arkansas; the Arkansas River, joining the Mississippi at Arkansas Post; the Big Black River in Mississippi; the Yazoo River, meeting the Mississippi at Vicksburg, Mississippi; and the Red River in Louisiana. The widest point of the Mississippi River is in the Lower Mississippi portion where it exceeds in width in several places.
Deliberate water diversion at the Old River Control Structure in Louisiana allows the Atchafalaya River in Louisiana to be a major distributary of the Mississippi River, with 30% of the Mississippi flowing to the Gulf of Mexico by this route, rather than continuing down the Mississippi's current channel past Baton Rouge and New Orleans on a longer route to the Gulf.
Watershed.
The Mississippi River has the world's fourth largest drainage basin ("watershed" or "catchment"). The basin covers more than , including all or parts of 31 U.S. states and two Canadian provinces. The drainage basin empties into the Gulf of Mexico, part of the Atlantic Ocean. The total catchment of the Mississippi River covers nearly 40% of the landmass of the continental United States. The highest point within the watershed is also the highest point of the Rocky Mountains, Mount Elbert at .
In the United States, the Mississippi River drains the majority of the area between crest of the Rocky Mountains and the crest of the Appalachian Mountains, except for various regions drained to Hudson Bay by the Red River of the North; to the Atlantic Ocean by the Great Lakes and the Saint Lawrence River; and to the Gulf of Mexico by the Rio Grande, the Alabama and Tombigbee rivers, the Chattahoochee and Appalachicola rivers, and various smaller coastal waterways along the Gulf.
The Mississippi River empties into the Gulf of Mexico about downstream from New Orleans. Measurements of the length of the Mississippi from Lake Itasca to the Gulf of Mexico vary somewhat, but the United States Geological Survey's number is . The retention time from Lake Itasca to the Gulf is typically about 90 days.
Outflow.
The Mississippi River discharges at an annual average rate of between 200 and 700 thousand cubic feet per second (7,000–20,000 m3/s). Although it is the 5th largest river in the world by volume, this flow is a small fraction of the output of the Amazon, which moves nearly 7 million cubic feet per second (200,000 m3/s) during wet seasons. On average, the Mississippi has only 8% the flow of the Amazon River.
Fresh river water flowing from the Mississippi into the Gulf of Mexico does not mix into the salt water immediately. The images from NASA's MODIS (to the right) show a large plume of fresh water, which appears as a dark ribbon against the lighter-blue surrounding waters. These images demonstrate that the plume did not mix with the surrounding sea water immediately. Instead, it stayed intact as it flowed through the Gulf of Mexico, into the Straits of Florida, and entered the Gulf Stream. The Mississippi River water rounded the tip of Florida and traveled up the southeast coast to the latitude of Georgia before finally mixing in so thoroughly with the ocean that it could no longer be detected by MODIS.
Before 1900, the Mississippi River transported an estimated 400 million metric tons of sediment per year from the interior of the United States to coastal Louisiana and the Gulf of Mexico. During the last two decades, this number was only 145 million metric tons per year. The reduction in sediment transported down the Mississippi River is the result of engineering modification of the Mississippi, Missouri, and Ohio rivers and their tributaries by dams, meander cutoffs, river-training structures, and bank revetments and soil erosion control programs in the areas drained by them.
Course changes.
Over geologic time, the Mississippi River has experienced numerous large and small changes to its main course, as well as additions, deletions, and other changes among its numerous tributaries, and the lower Mississippi River has used different pathways as its main channel to the Gulf of Mexico across the delta region.
Through a natural process known as avulsion or delta switching, the lower Mississippi River has shifted its final course to the mouth of the Gulf of Mexico every thousand years or so. This occurs because the deposits of silt and sediment begin to clog its channel, raising the river's level and causing it to eventually find a steeper, more direct route to the Gulf of Mexico. The abandoned distributaries diminish in volume and form what are known as bayous. This process has, over the past 5,000 years, caused the coastline of south Louisiana to advance toward the Gulf from . The currently active delta lobe is called the Birdfoot Delta, after its shape, or the Balize Delta, after La Balize, Louisiana, the first French settlement at the mouth of the Mississippi.
Prehistoric courses.
The current form of the Mississippi River basin was largely shaped by the Laurentide Ice Sheet of the most recent Ice Age. The southernmost extent of this enormous glaciation extended well into the present-day United States and Mississippi basin. When the ice sheet began to recede, hundreds of feet of rich sediment were deposited, creating the flat and fertile landscape of the Mississippi Valley. During the melt, giant glacial rivers found drainage paths into the Mississippi watershed, creating such features as the Minnesota River, James River, and Milk River valleys. When the ice sheet completely retreated, many of these "temporary" rivers found paths to Hudson Bay or the Arctic Ocean, leaving the Mississippi Basin with many features "oversized" for the existing rivers to have carved in the same time period.
Ice sheets during the Illinoian Stage about 300,000 to 132,000 years before present, blocked the Mississippi near Rock Island, Illinois, diverting it to its present channel farther to the west, the current western border of Illinois. The Hennepin Canal roughly follows the ancient channel of the Mississippi downstream from Rock Island to Hennepin, Illinois. South of Hennepin, to Alton, Illinois, the current Illinois River follows the ancient channel used by the Mississippi River before the Illinoian Stage.
Timeline of outflow course changes
Historic course changes.
In March 1876, the Mississippi suddenly changed course near the settlement of Reverie, Tennessee, leaving a small part of Tipton County, Tennessee, attached to Arkansas and separated from the rest of Tennessee by the new river channel. Since this event was an avulsion, rather than the effect of incremental erosion and deposition, the state line still follows the old channel.
New Madrid Seismic Zone.
The New Madrid Seismic Zone, along the Mississippi River near New Madrid, Missouri, between Memphis and St. Louis, is related to an aulacogen (failed rift) that formed at the same time as the Gulf of Mexico. This area is still quite active seismically. Four great earthquakes in 1811 and 1812, estimated at approximately 8 on the Richter magnitude scale, had tremendous local effects in the then sparsely settled area, and were felt in many other places in the midwestern and eastern U.S. These earthquakes created Reelfoot Lake in Tennessee from the altered landscape near the river.
Cultural geography.
State boundaries.
The Mississippi River runs through or along 10 states, from Minnesota to Louisiana, and was used to define portions of these states' borders, with Wisconsin, Illinois, Kentucky, Tennessee, and Mississippi along the east side of the river, and Iowa, Missouri, and Arkansas along its west side. Substantial parts of both Minnesota and Louisiana are on either side of the river, although the Mississippi defines part of the boundary of each of these states.
In all of these cases, the middle of the riverbed at the time the borders were established was used as the line to define the borders between adjacent states. In various areas, the river has since shifted, but the state borders have not changed, still following the former bed of the Mississippi River as of their establishment, leaving several small isolated areas of one state across the new river channel, contiguous with the adjacent state. Also, due to a meander in the river, a small part of western Kentucky is contiguous with Tennessee, but isolated from the rest of its state.
Communities along the river.
Many of the communities along the Mississippi River are listed below; most have either historic significance or cultural lore connecting them to the river. They are sequenced from the source of the river to its end.
Bridge crossings.
The first bridge across the Mississippi River was built in 1855. It spanned the river in Minneapolis, Minnesota where the current Hennepin Avenue Bridge is located. No highway or railroad tunnels cross under the Mississippi River.
The first railroad bridge across the Mississippi was built in 1856. It spanned the river between the Rock Island Arsenal in Illinois and Davenport, Iowa. Steamboat captains of the day, fearful of competition from the railroads, considered the new bridge a hazard to navigation. Two weeks after the bridge opened, the steamboat "Effie Afton" rammed part of the bridge, setting it on fire. Legal proceedings ensued, with Abraham Lincoln defending the railroad. The lawsuit went to the Supreme Court of the United States, which ruled in favor of the railroad.
Below is a general overview of selected Mississippi bridges which have notable engineering or landmark significance, with their cities or locations. They are sequenced from the Upper Mississippi's source to the Lower Mississippi's mouth.
Navigation and flood control.
A clear channel is needed for the barges and other vessels that make the main stem Mississippi one of the great commercial waterways of the world. The task of maintaining a navigation channel is the responsibility of the United States Army Corps of Engineers, which was established in 1802. Earlier projects began as early as 1829 to remove snags, close off secondary channels and excavate rocks and sandbars.
Steamboats entered trade in the 1820s, so the period 18301850 became the golden age of steamboats. As there were few roads or rails in the lands of the Louisiana Purchase, river traffic was an ideal solution. Cotton, timber and food came down the river, as did Appalachian coal. The port of New Orleans boomed as it was the trans-shipment point to deep sea ocean vessels. As a result, the image of the twin stacked, wedding cake Mississippi steamer entered into American mythology. Steamers worked the entire route from the trickles of Montana, to the Ohio river; down the Missouri and Tennessee, to the main channel of the Mississippi. Only with the arrival of the railroads in the 1880s did steamboat traffic diminish. Steamboats remained a feature until the 1920s. Most have been superseded by pusher tugs. A few survive as icons—the Delta Queen and the River Queen for instance.
A series of 29 locks and dams on the upper Mississippi, most of which were built in the 1930s, is designed primarily to maintain a deep channel for commercial barge traffic. The lakes formed are also used for recreational boating and fishing. The dams make the river deeper and wider but do not stop it. No flood control is intended. During periods of high flow, the gates, some of which are submersible, are completely opened and the dams simply cease to function. Below St. Louis, the Mississippi is relatively free-flowing, although it is constrained by numerous levees and directed by numerous wing dams.
On the lower Mississippi, from Baton Rouge to the mouth of the Mississippi, the navigation depth is 45 feet, allowing container ships and cruise ships to dock at the Port of New Orleans and bulk cargo ships shorter than 150 foot air draft that fit under the Huey P. Long Bridge to traverse the Mississippi to Baton Rouge. There is a feasibility study to dredge this portion of the river to 50 feet to allow New Panamax ship depths.
19th century.
In 1829, there were surveys of the two major obstacles on the upper Mississippi, the Des Moines Rapids and the Rock Island Rapids, where the river was shallow and the riverbed was rock. The Des Moines Rapids were about 11 mi (18 km) long and just above the mouth of the Des Moines River at Keokuk, Iowa. The Rock Island Rapids were between Rock Island and Moline, Illinois. Both rapids were considered virtually impassable.
In 1848, the Illinois and Michigan Canal was built to connect the Mississippi River to Lake Michigan via the Illinois River near Peru, Illinois. The canal allowed shipping between these important waterways. In 1900, the canal was replaced by the Chicago Sanitary and Ship Canal. The second canal, in addition to shipping, also allowed Chicago to address specific health issues (typhoid fever, cholera and other waterborne diseases) by sending its waste down the Illinois and Mississippi river systems rather than polluting its water source of Lake Michigan.
The Corps of Engineers recommended the excavation of a -deep channel at the Des Moines Rapids, but work did not begin until after Lieutenant Robert E. Lee endorsed the project in 1837. The Corps later also began excavating the Rock Island Rapids. By 1866, it had become evident that excavation was impractical, and it was decided to build a canal around the Des Moines Rapids. The canal opened in 1877, but the Rock Island Rapids remained an obstacle. In 1878, Congress authorized the Corps to establish a -deep channel to be obtained by building wing dams which direct the river to a narrow channel causing it to cut a deeper channel, by closing secondary channels and by dredging. The channel project was complete when the Moline Lock, which bypassed the Rock Island Rapids, opened in 1907.
To improve navigation between St. Paul, Minnesota, and Prairie du Chien, Wisconsin, the Corps constructed several dams on lakes in the headwaters area, including Lake Winnibigoshish and Lake Pokegama. The dams, which were built beginning in the 1880s, stored spring run-off which was released during low water to help maintain channel depth.
20th century.
In 1907, Congress authorized a deep channel project on the Mississippi, which was not complete when it was abandoned in the late 1920s in favor of the deep channel project.
In 1913, construction was complete on Lock and Dam No. 19 at Keokuk, Iowa, the first dam below St. Anthony Falls. Built by a private power company (Union Electric Company of St. Louis) to generate electricity (originally for Streetcars in St. Louis), the Keokuk dam was one of the largest hydro-electric plants in the world at the time. The dam also eliminated the Des Moines Rapids. Lock and Dam No. 1 was completed in Minneapolis, Minnesota in 1917. Lock and Dam No. 2, near Hastings, Minnesota was completed in 1930.
Before the Great Mississippi Flood of 1927, the Corps's primary strategy was to close off as many side channels as possible to increase the flow in the main river. It was thought that the river's velocity would scour off bottom sediments, deepening the river and decreasing the possibility of flooding. The 1927 flood proved this to be so wrong that communities threatened by the flood began to create their own levee breaks to relieve the force of the rising river.
The Rivers and Harbors Act of 1930 authorized the channel project, which called for a navigation channel 9 feet deep and wide to accommodate multiple-barge tows. This was achieved by a series of locks and dams, and by dredging. Twenty-three new locks and dams were built on the upper Mississippi in the 1930s in addition to the three already in existence.
Until the 1950s, there was no dam below Lock and Dam 26 at Alton, Illinois. Chain of Rocks Lock (Lock and Dam No. 27), which consists of a low-water dam and an long canal, was added in 1953, just below the confluence with the Missouri River, primarily to bypass a series of rock ledges at St. Louis. It also serves to protect the St. Louis city water intakes during times of low water.
U.S. government scientists determined in the 1950s that the Mississippi River was starting to switch to the Atchafalaya River channel because of its much steeper path to the Gulf of Mexico. Eventually the Atchafalaya River would capture the Mississippi River and become its main channel to the Gulf of Mexico, leaving New Orleans on a side channel. As a result, the U.S. Congress authorized a project called the Old River Control Structure, which has prevented the Mississippi River from leaving its current channel that drains into the Gulf via New Orleans.
Because the large scale of high-energy water flow threatened to damage the structure, an auxiliary flow control station was built adjacent to the standing control station. This US$300 million project was completed in 1986 by the U.S. Army Corps Of Engineers. Beginning in the 1970s, the Corps applied hydrological transport models to analyze flood flow and water quality of the Mississippi. Dam 26 at Alton, Illinois, which had structural problems, was replaced by the Mel Price Lock and Dam in 1990. The original Lock and Dam 26 was demolished.
21st century.
The Corps now actively creates and maintains spillways and floodways to divert periodic water surges into backwater channels and lakes, as well as route part of the Mississippi's flow into the Atchafalaya Basin and from there to the Gulf of Mexico, bypassing Baton Rouge and New Orleans. The main structures are the Birds Point-New Madrid Floodway in Missouri; the Old River Control Structure and the Morganza Spillway in Louisiana, which direct excess water down the west and east sides (respectively) of the Atchafalaya River; and the Bonnet Carré Spillway, also in Louisiana, which directs floodwaters to Lake Pontchartrain (see diagram). Some experts blame urban sprawl for increases in both the risk and frequency of flooding on the Mississippi River.
Some of the pre-1927 strategy is still in use today, with the Corps actively cutting the necks of horseshoe bends, allowing the water to move faster and reducing flood heights.
History.
Native Americans.
The area of the Mississippi River basin was first settled by hunting and gathering Native American peoples and is considered one of the few independent centers of plant domestication in human history. Evidence of early cultivation of sunflower, a goosefoot, a marsh elder and an indigenous squash dates to the 4th millennium BCE. The lifestyle gradually became more settled after around 1000 BCE during what is now called the Woodland period, with increasing evidence of shelter construction, pottery, weaving and other practices. A network of trade routes referred to as the Hopewell interaction sphere was active along the waterways between about 200 and 500 CE, spreading common cultural practices over the entire area between the Gulf of Mexico and the Great Lakes. A period of more isolated communities followed, and agriculture introduced from Mesoamerica based on the Three Sisters (maize, beans and squash) gradually came to dominate. After around 800 CE there arose an advanced agricultural society today referred to as the Mississippian culture, with evidence of highly stratified complex chiefdoms and large population centers. The most prominent of these, now called Cahokia, was occupied between about 600 and 1400 CE and at its peak numbered between 8,000 and 40,000 inhabitants, larger than London, England of that time. At the time of first contact with Europeans, Cahokia and many other Mississippian cities had dispersed, and archaeological finds attest to increased social stress.
Modern American Indian nations inhabiting the Mississippi basin include Cheyenne, Sioux, Ojibwe, Potawatomi, Ho-Chunk, Fox, Kickapoo, Tamaroa, Moingwena, Quapaw and Chickasaw.
The word "Mississippi" itself comes from "Messipi", the French rendering of the Anishinaabe (Ojibwe or Algonquin) name for the river, "Misi-ziibi" (Great River). The Ojibwe called Lake Itasca "Omashkoozo-zaaga'igan" (Elk Lake) and the river flowing out of it "Omashkoozo-ziibi" (Elk River). After flowing into Lake Bemidji, the Ojibwe called the river "Bemijigamaag-ziibi" (River from the Traversing Lake). After flowing into Cass Lake, the name of the river changes to "Gaa-miskwaawaakokaag-ziibi" (Red Cedar River) and then out of Lake Winnibigoshish as "Wiinibiigoonzhish-ziibi" (Miserable Wretched Dirty Water River), "Gichi-ziibi" (Big River) after the confluence with the Leech Lake River, then finally as "Misi-ziibi" (Great River) after the confluence with the Crow Wing River. After the expeditions by Giacomo Beltrami and Henry Schoolcraft, the longest stream above the juncture of the Crow Wing River and "Gichi-ziibi" was named "Mississippi River". The Mississippi River Band of Chippewa Indians, known as the "Gichi-ziibiwininiwag", are named after the stretch of the Mississippi River known as the "Gichi-ziibi". The Cheyenne, one of the earliest inhabitants of the upper Mississippi River, called it the "Máʼxe-éʼometaaʼe" (Big Greasy River) in the Cheyenne language. The Arapaho name for the river is Beesniicíe. The Pawnee name is "Kickaátit".
European exploration.
On May 8, 1541, Spanish explorer Hernando de Soto became the first recorded European to reach the Mississippi River, which he called "Río del Espíritu Santo" ("River of the Holy Spirit"), in the area of what is now Mississippi. In Spanish, the river is called "Río Mississippi".
French explorers Louis Jolliet and Jacques Marquette began exploring the Mississippi in the 17th century. Marquette traveled with a Sioux Indian who named it "Ne Tongo" ("Big river" in Sioux language) in 1673. Marquette proposed calling it the "River of the Immaculate Conception".
When Louis Jolliet explored the Mississippi Valley in the 17th century, natives guided him to a quicker way to return to French Canada via the Illinois River. When he found the Chicago Portage, he remarked that a canal of "only half a league" (less than 2 miles (3.2 km), 3 km) would join the Mississippi and the Great Lakes. In 1848, the continental divide separating the waters of the Great Lakes and the Mississippi Valley was breached by the Illinois and Michigan canal via the Chicago River. This both accelerated the development, and forever changed the ecology of the Mississippi Valley and the Great Lakes.
In 1682, René-Robert Cavelier, Sieur de La Salle and Henri de Tonti claimed the entire Mississippi River Valley for France, calling the river "Colbert River" after Jean-Baptiste Colbert and the region "La Louisiane", for King Louis XIV. On March 2, 1699, Pierre Le Moyne d'Iberville rediscovered the mouth of the Mississippi, following the death of La Salle. The French built the small fort of La Balise there to control passage.
In 1718, about upriver, New Orleans was established along the river crescent by Jean-Baptiste Le Moyne, Sieur de Bienville, with construction patterned after the 1711 resettlement on Mobile Bay of Mobile, the capital of French Louisiana at the time.
In 1762 the entire region is part of the Spanish Louisiana from southern Canada to the Gulf of Mexico until 1802.
Colonization.
Following Britain's victory in the Seven Years War the Mississippi became the border between the British and Spanish Empires. The Treaty of Paris (1763) gave Great Britain rights to all land east of the Mississippi and Spain rights to land west of the Mississippi. Spain also ceded Florida to Britain to regain Cuba, which the British occupied during the war. Britain then divided the territory into East and West Florida.
Article 8 of the Treaty of Paris (1783) states, "The navigation of the river Mississippi, from its source to the ocean, shall forever remain free and open to the subjects of Great Britain and the citizens of the United States". With this treaty, which ended the American Revolutionary War, Britain also ceded West Florida back to Spain to regain the Bahamas, which Spain had occupied during the war. In 1800, under duress from Napoleon of France, Spain ceded an undefined portion of West Florida to France. When France then sold the Louisiana Territory to the U.S. in 1803, a dispute arose again between Spain and the U.S. on which parts of West Florida exactly had Spain ceded to France, which would in turn decide which parts of West Florida were now U.S. property versus Spanish property. These aspirations ended when Spain was pressured into signing Pinckney's Treaty in 1795.
France reacquired 'Louisiana' from Spain in the secret Treaty of San Ildefonso in 1800. The United States then bought the territory from France in the Louisiana Purchase of 1803. In 1815, the U.S. defeated Britain at the Battle of New Orleans, part of the War of 1812, securing American control of the river. So many settlers traveled westward through the Mississippi river basin, as well as settled in it, that Zadok Cramer wrote a guide book called "The Navigator", detailing the features and dangers and navigable waterways of the area. It was so popular that he updated and expanded it through 12 editions over a period of 25 years.
The colonization of the area was barely slowed by the three earthquakes in 1811 and 1812, estimated at approximately 8 on the Richter magnitude scale, that were centered near New Madrid, Missouri.
Steamboat era.
Mark Twain's book, "Life on the Mississippi", covered the steamboat commerce which took place from 1830 to 1870 on the river before more modern ships replaced the steamer. The book was published first in serial form in "Harper's Weekly" in seven parts in 1875. The full version, including a passage from the then unfinished "Adventures of Huckleberry Finn" and works from other authors, was published by James R. Osgood & Company in 1885.
The first steamboat to travel the full length of the Lower Mississippi from the Ohio River to New Orleans was the "New Orleans" in December 1811. Its maiden voyage occurred during the series of New Madrid earthquakes in 1811–12.The Upper Mississippi was treacherous, unpredictable and to make traveling worse, the area was not properly mapped out or surveyed. Until the 1840s only two trips a year to the Twin Cities landings were made by steamboats which suggests it was not very profitable.
Steamboat transport remained a viable industry, both in terms of passengers and freight until the end of the first decade of the 20th century. Among the several Mississippi River system steamboat companies was the noted Anchor Line, which, from 1859 to 1898, operated a luxurious fleet of steamers between St. Louis and New Orleans.
Italian explorer Giacomo Beltrami, wrote about his journey on the Virginia, which was the first steam boat to make it to Fort St.Anthony in Minnesota. He referred to his voyage as a promenade that was once a journey on the Mississippi.The steamboat era changed the economic and political life of the Mississippi, as well as the nature of travel itself. The Mississippi was completely changed by the steamboat era as it transformed into a flourishing tourists trade.
Civil War.
Control of the river was a strategic objective of both sides in the American Civil War. In 1862 Union forces coming down the river successfully cleared Confederate defenses at Island Number 10 and Memphis, Tennessee, while Naval forces coming upriver from the Gulf of Mexico captured New Orleans, Louisiana. The remaining major Confederate stronghold was on the heights overlooking the river at Vicksburg, Mississippi, and the Union's Vicksburg Campaign (December 1862 to July 1863), and the fall of Port Hudson, completed control of the lower Mississippi River. The Union victory ending the Siege of Vicksburg on July 4, 1863, was pivotal to the Union's final victory of the Civil War.
20th and 21st centuries.
The "Big Freeze" of 1918–19 blocked river traffic north of Memphis, Tennessee, preventing transportation of coal from southern Illinois. This resulted in widespread shortages, high prices, and rationing of coal in January and February.
In the spring of 1927, the river broke out of its banks in 145 places, during the Great Mississippi Flood of 1927 and inundated to a depth of up to .
In 1962 and 1963, industrial accidents spilled of soybean oil into the Mississippi and Minnesota rivers. The oil covered the Mississippi River from St. Paul to Lake Pepin, creating an ecological disaster and a demand to control water pollution.
On October 20, 1976, the automobile ferry, "MV George Prince", was struck by a ship traveling upstream as the ferry attempted to cross from Destrehan, Louisiana, to Luling, Louisiana. Seventy-eight passengers and crew died; only eighteen survived the accident.
In 1988, record low water levels provided an opportunity and obligation to examine the climax of the wooden-hulled age. The Mississippi fell to below zero on the Memphis gauge. Water craft remains were exposed in an area of on the bottom of the Mississippi River at West Memphis, Arkansas. They dated to the late 19th to early 20th centuries. The State of Arkansas, the Arkansas Archeological Survey, and the Arkansas Archeological Society responded with a two-month data recovery effort. The fieldwork received national media attention as good news in the middle of a drought.
The Great Flood of 1993 was another significant flood, primarily affecting the Mississippi above its confluence with the Ohio River at Cairo, Illinois.
Two portions of the Mississippi were designated as American Heritage Rivers in 1997: the lower portion around Louisiana and Tennessee, and the upper portion around Iowa, Illinois, Minnesota and Missouri. The Nature Conservancy's project called "America's Rivershed Initiative" announced a 'report card' assessment of the entire basin in October 2015 and gave the grade of D+. The assessment noted the aging navigation and flood control infrastructure along with multiple environmental problems.
In 2002, Slovenian long-distance swimmer Martin Strel swam the entire length of the river, from Minnesota to Louisiana, over the course of 68 days. In 2005, the Source to Sea Expedition paddled the Mississippi and Atchafalaya Rivers to benefit the Audubon Society's Upper Mississippi River Campaign.
Future.
Geologists believe that the lower Mississippi could take a new course to the Gulf. Either of two new routes – through the Atchafalaya Basin or through Lake Pontchartrain — might become the Mississippi's main channel if flood-control structures are overtopped or heavily damaged during a severe flood.
Failure of the Old River Control Structure, the Morganza Spillway, or nearby levees would likely re-route the main channel of the Mississippi through Louisiana's Atchafalaya Basin and down the Atchafalaya River to reach the Gulf of Mexico south of Morgan City in southern Louisiana. This route provides a more direct path to the Gulf of Mexico than the present Mississippi River channel through Baton Rouge and New Orleans. While the risk of such a diversion is present during any major flood event, such a change has so far been prevented by active human intervention involving the construction, maintenance, and operation of various levees, spillways, and other control structures by the U.S. Army Corps of Engineers.
The Old River Control Structure, between the present Mississippi River channel and the Atchafalaya Basin, sits at the normal water elevation and is ordinarily used to divert 30% of the Mississippi's flow to the Atchafalaya River. There is a steep drop here away from the Mississippi's main channel into the Atchafalaya Basin. If this facility were to fail during a major flood, there is a strong concern the water would scour and erode the river bottom enough to capture the Mississippi's main channel. The structure was nearly lost during the 1973 flood, but repairs and improvements were made after engineers studied the forces at play. In particular, the Corps of Engineers made many improvements and constructed additional facilities for routing water through the vicinity. These additional facilities give the Corps much more flexibility and potential flow capacity than they had in 1973, which further reduces the risk of a catastrophic failure in this area during other major floods, such as that of 2011.
Because the Morganza Spillway is located at slightly higher elevation well back from the river, it is normally dry on both sides. Even if this structure were to fail at the crest during a severe flood, the flood waters would have to cause a significant amount of erosion, down to normal water levels, before the Mississippi could permanently jump channel at this location. During the 2011 floods, the Corps of Engineers decided to open the Morganza Spillway to 1/4 of its capacity to allow 150,000 ft3/sec of water to flood the Morganza and Atchafalaya floodways and continue directly to the Gulf of Mexico, bypassing Baton Rouge and New Orleans. In addition to reducing the Mississippi River crest downstream, this diversion reduced the chances of a channel change by reducing stress on the other elements of the control system.
Some geologists have noted that the possibility for course change into the Atchafalaya also exists in the area immediately north of the Old River Control Structure. Army Corps of Engineers geologist Fred Smith once stated, "The Mississippi wants to go west. 1973 was a forty-year flood. The big one lies out there somewhere—when the structures can't release all the floodwaters and the levee is going to have to give way. That is when the river's going to jump its banks and try to break through."
Another possible course change for the Mississippi River is a diversion into Lake Pontchartrain near New Orleans. This route is controlled by the Bonnet Carré Spillway, built to reduce flooding in New Orleans. This spillway and an imperfect natural levee about 4–6 meters (12 to 20 feet) high are all that prevents the Mississippi from taking a new, shorter course through Lake Pontchartrain to the Gulf of Mexico. Diversion of the Mississippi's main channel through Lake Pontchartrain would have consequences similar to an Atchafalaya diversion, but to a lesser extent, since the present river channel would remain in use past Baton Rouge and into the New Orleans area.
Recreation.
The sport of water skiing was invented on the river in a wide region between Minnesota and Wisconsin known as Lake Pepin. Ralph Samuelson of Lake City, Minnesota, created and refined his skiing technique in late June and early July 1922. He later performed the first water ski jump in 1925 and was pulled along at by a Curtiss flying boat later that year.
There are seven National Park Service sites along the Mississippi River. The Mississippi National River and Recreation Area is the National Park Service site dedicated to protecting and interpreting the Mississippi River itself. The other six National Park Service sites along the river are (listed from north to south):

</doc>
<doc id="19581" url="https://en.wikipedia.org/wiki?curid=19581" title="Men in black">
Men in black

In popular culture and UFO conspiracy theories, men in black (MIB) are men dressed in black suits who claim to be government agents and who harass or threaten UFO witnesses to keep them quiet about what they have seen. It is sometimes implied that they may be aliens themselves. The term is also frequently used to describe mysterious men working for unknown organizations, as well as various branches of government allegedly designed to protect secrets or perform other strange activities. The term is generic, used for any unusual, threatening or strangely behaved individual whose appearance on the scene can be linked in some fashion with a UFO sighting.
Folklore.
Folklorist Peter Rojcewicz compares accounts of men in black to tales of people encountering the devil and speculates that they can be considered a kind of "psychological drama".
Ufologists.
Men in black figure prominently in ufology and UFO folklore. In 1947, Harold Dahl claimed to have been warned not to talk about his alleged UFO sighting on Maury Island by a man in a dark suit. In the mid 1950s, the ufologist Albert K. Bender claimed he was visited by men in dark suits who threatened and warned him not to continue investigating UFOs. Bender believed the men in black were secret government agents tasked with suppressing evidence of UFOs. The ufologist John Keel claimed to have had encounters with men in black and referred to them as "demonic supernaturals" with "dark skin and/or “exotic” facial features". According to the ufologist Jerome Clark, reports of men in black represent "experiences" that "don’t seem to have occurred in the world of consensus reality".
Hoax.
In his article, "Gray Barker: My Friend, the Myth-Maker," John C. Sherwood claims that, in the late 1960s, at the age of 18, he cooperated when Gray Barker urged him to develop a hoax – which Barker subsequently published – about what Barker called "blackmen", three mysterious UFO inhabitants who silenced Sherwood's pseudonymous identity, "Dr. Richard H. Pratt".

</doc>
<doc id="19582" url="https://en.wikipedia.org/wiki?curid=19582" title="May 7">
May 7


</doc>
<doc id="19583" url="https://en.wikipedia.org/wiki?curid=19583" title="Monomer">
Monomer

A monomer ( ) ("mono-", "one" + "-mer", "part") is a molecule that may bind chemically or supramolecularly to other molecules to form a (supramolecular) polymer. The process by which monomers combine end to end to form a polymer is called polymerization. Molecules made of a small number of monomer units (up to a few dozen) are called oligomers. The term "monomeric protein" may also be used to describe one of the proteins making up a multiprotein complex.
Polymer groupings, and the types of monomers that create them: 
Examples: The most common natural monomer is glucose, which is linked by glycosidic bonds into polymers such as cellulose, starch, and glycogen. Most often the term "monomer" refers to the organic molecules which form synthetic polymers, such as vinyl chloride, which is used to produce the polymer polyvinyl chloride (PVC).
Natural monomers.
Amino acids are natural monomers that polymerize at ribosomes to form proteins. Nucleotides, monomers found in the cell nucleus, polymerize to form nucleic acids – DNA and RNA. Glucose monomers can polymerize to form starches, glycogen or cellulose; xylose monomers can polymerise to form xylan. In all these cases and is thus not pliable, a hydrogen atom and a hydroxyl (-OH) group are lost to form H2O, and an oxygen atom links each monomer unit. Due to the formation of water as one of the products, these reactions are known as dehydration.
Isoprene is a natural monomer and polymerizes to form natural rubber, most often "cis-"1,4-polyisoprene, but also "trans-"1,4-polymer
Molecular weight.
The lower molecular weight compounds built from monomers are also referred to as dimers, trimers, tetramers, pentamers, hexamers, heptamers, octamers, nonamers, decamers, dodecamers, eicosamers, etc. if they have 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, or 20 monomer units, respectively. Any number of these monomer units may be indicated by the appropriate Greek prefix. Larger numbers are often stated in English or numbers instead of Greek; e.g., a "20-mer" is formed from 20 monomers. Molecules made of a small number of monomer units, up to a few dozen, are called oligomers.
Industrial use.
In the light of the current tight monomers market, particularly in propylene, and of the benefits of membrane-based recovery processes, major polyolefin producers around the world already employ such recovery processes in new state-of-the-art plants. In order to enhance the competitiveness of older plants, the use of a recovery solution has started to become mandatory.

</doc>
<doc id="19588" url="https://en.wikipedia.org/wiki?curid=19588" title="Mitochondrion">
Mitochondrion

The mitochondrion (plural mitochondria) is a double membrane-bound organelle found in most eukaryotic cells. The word mitochondrion comes from the Greek , , i.e. "thread", and , , i.e. "granule" or "grain-like".
Mitochondria are commonly between 0.75 and 3μm in diameter but vary considerably in size and structure. Unless specifically stained, they are not visible. Mitochondria have been described as "the powerhouse of the cell" because they generate most of the cell's supply of adenosine triphosphate (ATP), used as a source of chemical energy. In addition to supplying cellular energy, mitochondria are involved in other tasks, such as signaling, cellular differentiation, and cell death, as well as maintaining control of the cell cycle and cell growth. Mitochondria have been implicated in several human diseases, including mitochondrial disorders, cardiac dysfunction, and heart failure. In 2014, a study including ten children diagnosed with severe autism suggests that autism may be correlated with mitochondrial defects.
The number of mitochondria in a cell can vary widely by organism, tissue, and cell type. For instance, red blood cells have no mitochondria, whereas liver cells can have more than 2000. The organelle is composed of compartments that carry out specialized functions. These compartments or regions include the outer membrane, the intermembrane space, the inner membrane, and the cristae and matrix. Mitochondrial proteins vary depending on the tissue and the species. In humans, 615 distinct types of protein have been identified from cardiac mitochondria, whereas in rats, 940 proteins have been reported. The mitochondrial proteome is thought to be dynamically regulated. Although most of a cell's DNA is contained in the cell nucleus, the mitochondrion has its own independent genome that shows substantial similarity to bacterial genomes.
History.
The first observations of intracellular structures that probably represented mitochondria were published in the 1840s. Richard Altmann, in 1894, established them as cell organelles and called them "bioblasts". The term "mitochondria" was coined by Carl Benda in 1898. Leonor Michaelis discovered that Janus green can be used as a supravital stain for mitochondria in 1900. In 1904, Friedrich Meves, made the first recorded observation of mitochondria in plants in cells of the white waterlily, "Nymphaea alba" and in 1908, along with Claudius Regaud, suggested that they contain proteins and lipids. Benjamin F. Kingsbury, in 1912, first related them with cell respiration, but almost exclusively based on morphological observations. In 1913, particles from extracts of guinea-pig liver were linked to respiration by Otto Heinrich Warburg, which he called "grana". Warburg and Heinrich Otto Wieland, who had also postulated a similar particle mechanism, disagreed on the chemical nature of the respiration. It was not until 1925, when David Keilin discovered cytochromes, that the respiratory chain was described.
In 1939, experiments using minced muscle cells demonstrated that cellular respiration using one oxygen atom can form two adenosine triphosphate (ATP) molecules, and, in 1941, the concept of the phosphate bonds of ATP being a form of energy in cellular metabolism was developed by Fritz Albert Lipmann. In the following years, the mechanism behind cellular respiration was further elaborated, although its link to the mitochondria was not known. The introduction of tissue fractionation by Albert Claude allowed mitochondria to be isolated from other cell fractions and biochemical analysis to be conducted on them alone. In 1946, he concluded that cytochrome oxidase and other enzymes responsible for the respiratory chain were isolated to the mitchondria. Eugene Kennedy and Albert Lehninger discovered in 1948 that mitochondria are the site of oxidative phosphorylation in eukaryotes. Over time, the fractionation method was further developed, improving the quality of the mitochondria isolated, and other elements of cell respiration were determined to occur in the mitochondria.
The first high-resolution electron micrographs appeared in 1952, replacing the Janus Green stains as the preferred way of visualising the mitochondria. This led to a more detailed analysis of the structure of the mitochondria, including confirmation that they were surrounded by a membrane. It also showed a second membrane inside the mitochondria that folded up in ridges dividing up the inner chamber and that the size and shape of the mitochondria varied from cell to cell.
The popular term "powerhouse of the cell" was coined by Philip Siekevitz in 1957.
In 1967, it was discovered that mitochondria contained ribosomes. In 1968, methods were developed for mapping the mitochondrial genes, with the genetic and physical map of yeast mitochondrial DNA being completed in 1976.
Origin.
There are two hypotheses about the origin of mitochondria: endosymbiotic and autogenous. The endosymbiotic hypothesis suggests that mitochondria were originally prokaryotic cells, capable of implementing oxidative mechanisms that were not possible for eukaryotic cells; they became endosymbionts living inside the eukaryote. In the autogenous hypothesis, mitochondria were born by splitting off a portion of DNA from the nucleus of the eukaryotic cell at the time of divergence with the prokaryotes; this DNA portion would have been enclosed by membranes, which could not be crossed by proteins. Since mitochondria have many features in common with bacteria, the most accredited theory at present is endosymbiosis.
A mitochondrion contains DNA, which is organized as several copies of a single, circular chromosome. This mitochondrial chromosome contains genes for redox proteins, such as those of the respiratory chain. The CoRR hypothesis proposes that this co-location is required for redox regulation. The mitochondrial genome codes for some RNAs of ribosomes, and the 22 tRNAs necessary for the translation of messenger RNAs into protein. The circular structure is also found in prokaryotes. The proto-mitochondrion was probably closely related to the "Rickettsia". However, the exact relationship of the ancestor of mitochondria to the alphaproteobacteria and whether the mitochondrion was formed at the same time or after the nucleus, remains controversial.
A recent study by researchers of the University of Hawaii at Manoa and the Oregon State University indicates that the SAR11 clade of bacteria shares a relatively recent common ancestor with the mitochondria existing in most eukaryotic cells.
The ribosomes coded for by the mitochondrial DNA are similar to those from bacteria in size and structure. They closely resemble the bacterial 70S ribosome and not the 80S cytoplasmic ribosomes, which are coded for by nuclear DNA.
The endosymbiotic relationship of mitochondria with their host cells was popularized by Lynn Margulis. The endosymbiotic hypothesis suggests that mitochondria descended from bacteria that somehow survived endocytosis by another cell, and became incorporated into the cytoplasm. The ability of these bacteria to conduct respiration in host cells that had relied on glycolysis and fermentation would have provided a considerable evolutionary advantage. This symbiotic relationship probably developed 1.7 to 2 billion years ago.
A few groups of unicellular eukaryotes lack mitochondria: the microsporidians, metamonads, and archamoebae. These groups appear as the most primitive eukaryotes on phylogenetic trees constructed using rRNA information, which once suggested that they appeared before the origin of mitochondria. However, this is now known to be an artifact of long-branch attraction—they are derived groups and retain genes or organelles derived from mitochondria (e.g., mitosomes and hydrogenosomes).
Structure.
A mitochondrion contains outer and inner membranes composed of phospholipid bilayers and proteins. The two membranes have different properties. Because of this double-membraned organization, there are five distinct parts to a mitochondrion. They are:
Mitochondria stripped of their outer membrane are called mitoplasts.
Outer membrane.
The outer mitochondrial membrane, which encloses the entire organelle, is 60 to 75 angstroms (Å) thick. It has a protein-to-phospholipid ratio similar to that of the eukaryotic plasma membrane (about 1:1 by weight). It contains large numbers of integral membrane proteins called porins. These porins form channels that allow molecules of 5000 daltons or less in molecular weight to freely diffuse from one side of the membrane to the other. Larger proteins can enter the mitochondrion if a signaling sequence at their N-terminus binds to a large multisubunit protein called translocase of the outer membrane, which then actively moves them across the membrane. Mitochondrial pro-proteins are imported through specialised translocation complexes. The outer membrane also contains enzymes involved in such diverse activities as the elongation of fatty acids, oxidation of epinephrine, and the degradation of tryptophan. These enzymes include monoamine oxidase, rotenone-insensitive NADH-cytochrome c-reductase, kynurenine hydroxylase and fatty acid Co-A ligase. Disruption of the outer membrane permits proteins in the intermembrane space to leak into the cytosol, leading to certain cell death. The mitochondrial outer membrane can associate with the endoplasmic reticulum (ER) membrane, in a structure called MAM (mitochondria-associated ER-membrane). This is important in the ER-mitochondria calcium signaling and is involved in the transfer of lipids between the ER and mitochondria.
Intermembrane space.
The intermembrane space is the space between the outer membrane and the inner membrane. It is also known as perimitochondrial space. Because the outer membrane is freely permeable to small molecules, the concentrations of small molecules, such as ions and sugars, in the intermembrane space is the same as in the cytosol. However, large proteins must have a specific signaling sequence to be transported across the outer membrane, so the protein composition of this space is different from the protein composition of the cytosol. One protein that is localized to the intermembrane space in this way is cytochrome c.
Inner membrane.
The inner mitochondrial membrane contains proteins with five types of functions:
It contains more than 151 different polypeptides, and has a very high protein-to-phospholipid ratio (more than 3:1 by weight, which is about 1 protein for 15 phospholipids). The inner membrane is home to around 1/5 of the total protein in a mitochondrion. In addition, the inner membrane is rich in an unusual phospholipid, cardiolipin. This phospholipid was originally discovered in cow hearts in 1942, and is usually characteristic of mitochondrial and bacterial plasma membranes. Cardiolipin contains four fatty acids rather than two, and may help to make the inner membrane impermeable. Unlike the outer membrane, the inner membrane doesn't contain porins, and is highly impermeable to all molecules. Almost all ions and molecules require special membrane transporters to enter or exit the matrix. Proteins are ferried into the matrix via the translocase of the inner membrane (TIM) complex or via Oxa1. In addition, there is a membrane potential across the inner membrane, formed by the action of the enzymes of the electron transport chain.
Cristae.
The inner mitochondrial membrane is compartmentalized into numerous cristae, which expand the surface area of the inner mitochondrial membrane, enhancing its ability to produce ATP. For typical liver mitochondria, the area of the inner membrane is about five times as large as the outer membrane. This ratio is variable and mitochondria from cells that have a greater demand for ATP, such as muscle cells, contain even more cristae. These folds are studded with small round bodies known as F1 particles or oxysomes. These are not simple random folds but rather invaginations of the inner membrane, which can affect overall chemiosmotic function.
One recent mathematical modeling study has suggested that the optical properties of the cristae in filamentous mitochondria may affect the generation and propagation of light within the tissue.
Matrix.
The matrix is the space enclosed by the inner membrane. It contains about 2/3 of the total protein in a mitochondrion. The matrix is important in the production of ATP with the aid of the ATP synthase contained in the inner membrane. The matrix contains a highly concentrated mixture of hundreds of enzymes, special mitochondrial ribosomes, tRNA, and several copies of the mitochondrial DNA genome. Of the enzymes, the major functions include oxidation of pyruvate and fatty acids, and the citric acid cycle.
Mitochondria have their own genetic material, and the machinery to manufacture their own RNAs and proteins ("see: protein biosynthesis"). A published human mitochondrial DNA sequence revealed 16,569 base pairs encoding 37 genes: 22 tRNA, 2 rRNA, and 13 peptide genes. The 13 mitochondrial peptides in humans are integrated into the inner mitochondrial membrane, along with proteins encoded by genes that reside in the host cell's nucleus.
Mitochondria-associated ER membrane (MAM).
The mitochondria-associated ER membrane (MAM) is another structural element that is increasingly recognized for its critical role in cellular physiology and homeostasis. Once considered a technical snag in cell fractionation techniques, the alleged ER vesicle contaminants that invariably appeared in the mitochondrial fraction have been re-identified as membranous structures derived from the MAM—the interface between mitochondria and the ER. Physical coupling between these two organelles had previously been observed in electron micrographs and has more recently been probed with fluorescence microscopy. Such studies estimate that at the MAM, which may comprise up to 20% of the mitochondrial outer membrane, the ER and mitochondria are separated by a mere 10–25 nm and held together by protein tethering complexes.
Purified MAM from subcellular fractionation has been shown to be enriched in enzymes involved in phospholipid exchange, in addition to channels associated with Ca2+ signaling. These hints of a prominent role for the MAM in the regulation of cellular lipid stores and signal transduction have been borne out, with significant implications for mitochondrial-associated cellular phenomena, as discussed below. Not only has the MAM provided insight into the mechanistic basis underlying such physiological processes as intrinsic apoptosis and the propagation of calcium signaling, but it also favors a more refined view of the mitochondria. Though often seen as static, isolated 'powerhouses' hijacked for cellular metabolism through an ancient endosymbiotic event, the evolution of the MAM underscores the extent to which mitochondria have been integrated into overall cellular physiology, with intimate physical and functional coupling to the endomembrane system.
Phospholipid transfer.
The MAM is enriched in enzymes involved in lipid biosynthesis, such as phosphatidylserine synthase on the ER face and phosphatidylserine decarboxylase on the mitochondrial face. Because mitochondria are dynamic organelles constantly undergoing fission and fusion events, they require a constant and well-regulated supply of phospholipids for membrane integrity. But mitochondria are not only a destination for the phospholipids they finish synthesis of; rather, this organelle also plays a role in inter-organelle trafficking of the intermediates and products of phospholipid biosynthetic pathways, ceramide and cholesterol metabolism, and glycosphingolipid anabolism.
Such trafficking capacity depends on the MAM, which has been shown to facilitate transfer of lipid intermediates between organelles. In contrast to the standard vesicular mechanism of lipid transfer, evidence indicates that the physical proximity of the ER and mitochondrial membranes at the MAM allows for lipid flipping between opposed bilayers. Despite this unusual and seemingly energetically unfavorable mechanism, such transport does not require ATP. Instead, in yeast, it has been shown to be dependent on a multiprotein tethering structure termed the ER-mitochondria encounter structure, or ERMES, although it remains unclear whether this structure directly mediates lipid transfer or is required to keep the membranes in sufficiently close proximity to lower the energy barrier for lipid flipping.
The MAM may also be part of the secretory pathway, in addition to its role in intracellular lipid trafficking. In particular, the MAM appears to be an intermediate destination between the rough ER and the Golgi in the pathway that leads to very-low-density lipoprotein, or VLDL, assembly and secretion. The MAM thus serves as a critical metabolic and trafficking hub in lipid metabolism.
Calcium signaling.
A critical role for the ER in calcium signaling was acknowledged before such a role for the mitochondria was widely accepted, in part because the low affinity of Ca2+ channels localized to the outer mitochondrial membrane seemed to fly in the face of this organelle's purported responsiveness to changes in intracellular Ca2+ flux. But the presence of the MAM resolves this apparent contradiction: the close physical association between the two organelles results in Ca2+ microdomains at contact points that facilitate efficient Ca2+ transmission from the ER to the mitochondria. Transmission occurs in response to so-called "Ca2+ puffs" generated by spontaneous clustering and activation of IP3R, a canonical ER membrane Ca2+ channel.
The fate of these puffs—in particular, whether they remain restricted to isolated locales or integrated into Ca2+ waves for propagation throughout the cell—is determined in large part by MAM dynamics. Although reuptake of Ca2+ by the ER (concomitant with its release) modulates the intensity of the puffs, thus insulating mitochondria to a certain degree from high Ca2+ exposure, the MAM often serves as a firewall that essentially buffers Ca2+ puffs by acting as a sink into which free ions released into the cytosol can be funneled. This Ca2+ tunneling occurs through the low-affinity Ca2+ receptor VDAC1, which recently has been shown to be physically tethered to the IP3R clusters on the ER membrane and enriched at the MAM. The ability of mitochondria to serve as a Ca2+ sink is a result of the electrochemical gradient generated during oxidative phosphorylation, which makes tunneling of the cation an exergonic process. Normally, mild calcium influx from cytosol into the mitochondrial matrix causes transient depolarization that is corrected by pumping out protons.
But transmission of Ca2+ is not unidirectional; rather, it is a two-way street. The properties of the Ca2+ pump SERCA and the channel IP3R present on the ER membrane facilitate feedback regulation coordinated by MAM function. In particular, the clearance of Ca2+ by the MAM allows for spatio-temporal patterning of Ca2+ signaling because Ca2+ alters IP3R activity in a biphasic manner. SERCA is likewise affected by mitochondrial feedback: uptake of Ca2+ by the MAM stimulates ATP production, thus providing energy that enables SERCA to reload the ER with Ca2+ for continued Ca2+ efflux at the MAM. Thus, the MAM is not a passive buffer for Ca2+ puffs; rather it helps modulate further Ca2+ signaling through feedback loops that affect ER dynamics.
Regulating ER release of Ca2+ at the MAM is especially critical because only a certain window of Ca2+ uptake sustains the mitochondria, and consequently the cell, at homeostasis. Sufficient intraorganelle Ca2+ signaling is required to stimulate metabolism by activating dehydrogenase enzymes critical to flux through the citric acid cycle. However, once Ca2+ signaling in the mitochondria passes a certain threshold, it stimulates the intrinsic pathway of apoptosis in part by collapsing the mitochondrial membrane potential required for metabolism. Studies examining the role of pro- and anti-apoptotic factors support this model; for example, the anti-apoptotic factor Bcl-2 has been shown to interact with IP3Rs to reduce Ca2+ filling of the ER, leading to reduced efflux at the MAM and preventing collapse of the mitochondrial membrane potential post-apoptotic stimuli. Given the need for such fine regulation of Ca2+ signaling, it is perhaps unsurprising that dysregulated mitochondrial Ca2+ has been implicated in several neurodegenerative diseases, while the catalogue of tumor suppressors includes a few that are enriched at the MAM.
Molecular basis for tethering.
Recent advances in the identification of the tethers between the mitochondrial and ER membranes suggest that the scaffolding function of the molecular elements involved is secondary to other, non-structural functions. In yeast, ERMES, a multiprotein complex of interacting ER- and mitochondrial-resident membrane proteins, is required for lipid transfer at the MAM and exemplifies this principle. One of its components, for example, is also a constituent of the protein complex required for insertion of transmembrane beta-barrel proteins into the lipid bilayer. However, a homologue of the ERMES complex has not yet been identified in mammalian cells. Other proteins implicated in scaffolding likewise have functions independent of structural tethering at the MAM; for example, ER-resident and mitochondrial-resident mitofusins form heterocomplexes that regulate the number of inter-organelle contact sites, although mitofusins were first identified for their role in fission and fusion events between individual mitochondria. Glucose-related protein 75 (grp75) is another dual-function protein. In addition to the matrix pool of grp75, a portion serves as a chaperone that physically links the mitochondrial and ER Ca2+ channels VDAC and IP3R for efficient Ca2+ transmission at the MAM. Another potential tether is Sigma-1R, a non-opioid receptor whose stabilization of ER-resident IP3R may preserve communication at the MAM during the metabolic stress response.
Perspective.
The MAM is a critical signaling, metabolic, and trafficking hub in the cell that allows for the integration of ER and mitochondrial physiology. Coupling between these organelles is not simply structural but functional as well and critical for overall cellular physiology and homeostasis. The MAM thus offers a perspective on mitochondria that diverges from the traditional view of this organelle as a static, isolated unit appropriated for its metabolic capacity by the cell. Instead, this mitochondrial-ER interface emphasizes the integration of the mitochondria, the product of an endosymbiotic event, into diverse cellular processes.
Organization and distribution.
Mitochondria are found in nearly all eukaryotes. Although commonly depicted as bean-like structures they form a highly dynamic network in the majority of cells where they constantly undergo fission and fusion. Mitochondria vary in number and location according to cell type. A single mitochondrion is often found in unicellular organisms. Conversely, numerous mitochondria are found in human liver cells, with about 1000–2000 mitochondria per cell, making up 1/5 of the cell volume. The mitochondrial content of otherwise similar cells can vary substantially in size and membrane potential, with differences arising from sources including uneven partitioning at cell divisions, leading to extrinsic differences in ATP levels and downstream cellular processes. The mitochondria can be found nestled between myofibrils of muscle or wrapped around the sperm flagellum. Often, they form a complex 3D branching network inside the cell with the cytoskeleton. The association with the cytoskeleton determines mitochondrial shape, which can affect the function as well: different structures of the mitochondrial network may afford the population a variety of physical, chemical, and signalling advantages or disadvantages. Mitochondria in cells are always distributed along microtubules and the distribution of these organelles is also correlated with the endoplasmic reticulum. Recent evidence suggests that vimentin, one of the components of the cytoskeleton, is also critical to the association with the cytoskeleton.
Function.
The most prominent roles of mitochondria are to produce the energy currency of the cell, ATP (i.e., phosphorylation of ADP), through respiration, and to regulate cellular metabolism. The central set of reactions involved in ATP production are collectively known as the citric acid cycle, or the Krebs cycle. However, the mitochondrion has many other functions in addition to the production of ATP.
Energy conversion.
A dominant role for the mitochondria is the production of ATP, as reflected by the large number of proteins in the inner membrane for this task. This is done by oxidizing the major products of glucose: pyruvate, and NADH, which are produced in the cytosol. This type of cellular respiration known as aerobic respiration, is dependent on the presence of oxygen. When oxygen is limited, the glycolytic products will be metabolized by anaerobic fermentation, a process that is independent of the mitochondria. The production of ATP from glucose has an approximately 13-times higher yield during aerobic respiration compared to fermentation. Recently it has been shown that plant mitochondria can produce a limited amount of ATP without oxygen by using the alternate substrate nitrite. ATP crosses out through the inner membrane with the help of a specific protein, and across the outer membrane via porins. ADP returns via the same route.
Pyruvate and the citric acid cycle.
Pyruvate molecules produced by glycolysis are actively transported across the inner mitochondrial membrane, and into the matrix where they can either be oxidized and combined with coenzyme A to form CO2, acetyl-CoA, and NADH, or they can be carboxylated (by pyruvate carboxylase) to form oxaloacetate. This latter reaction ”fills up” the amount of oxaloacetate in the citric acid cycle, and is therefore an anaplerotic reaction, increasing the cycle’s capacity to metabolize acetyl-CoA when the tissue's energy needs (e.g. in muscle) are suddenly increased by activity.
In the citric acid cycle, all the intermediates (e.g. citrate, iso-citrate, alpha-ketoglutarate, succinate, fumarate, malate and oxaloacetate) are regenerated during each turn of the cycle. Adding more of any of these intermediates to the mitochondrion therefore means that the additional amount is retained within the cycle, increasing all the other intermediates as one is converted into the other. Hence, the addition of any one of them to the cycle has an anaplerotic effect, and its removal has a cataplerotic effect. These anaplerotic and cataplerotic reactions will, during the course of the cycle, increase or decrease the amount of oxaloacetate available to combine with acetyl-CoA to form citric acid. This in turn increases or decreases the rate of ATP production by the mitochondrion, and thus the availability of ATP to the cell.
Acetyl-CoA, on the other hand, derived from pyruvate oxidation, or from the beta-oxidation of fatty acids, is the only fuel to enter the citric acid cycle. With each turn of the cycle one molecule of acetyl-CoA is consumed for every molecule of oxaloacetate present in the mitochondrial matrix, and is never regenerated. It is the oxidation of the acetate portion of acetyl-CoA that produces CO2 and water, with the energy thus released captured in the form of ATP.
In the liver, the carboxylation of cytosolic pyruvate into intra-mitochondrial oxaloacetate is an early step in the gluconeogenic pathway, which converts lactate and de-aminated alanine into glucose, under the influence of high levels of glucagon and/or epinephrine in the blood. Here, the addition of oxaloacetate to the mitochondrion does not have a net anaplerotic effect, as another citric acid cycle intermediate (malate) is immediately removed from the mitochondrion to be converted into cytosolic oxaloacetate, which is ultimately converted into glucose, in a process that is almost the reverse of glycolysis.
The enzymes of the citric acid cycle are located in the mitochondrial matrix, with the exception of succinate dehydrogenase, which is bound to the inner mitochondrial membrane as part of Complex II. The citric acid cycle oxidizes the acetyl-CoA to carbon dioxide, and, in the process, produces reduced cofactors (three molecules of NADH and one molecule of FADH2) that are a source of electrons for the "electron transport chain", and a molecule of GTP (that is readily converted to an ATP).
NADH and FADH2: the electron transport chain.
The redox energy from NADH and FADH2 is transferred to oxygen (O2) in several steps via the electron transport chain. These energy-rich molecules are produced within the matrix via the citric acid cycle but are also produced in the cytoplasm by glycolysis. Reducing equivalents from the cytoplasm can be imported via the malate-aspartate shuttle system of antiporter proteins or feed into the electron transport chain using a glycerol phosphate shuttle. Protein complexes in the inner membrane (NADH dehydrogenase (ubiquinone), cytochrome c reductase, and cytochrome c oxidase) perform the transfer and the incremental release of energy is used to pump protons (H+) into the intermembrane space. This process is efficient, but a small percentage of electrons may prematurely reduce oxygen, forming reactive oxygen species such as superoxide. This can cause oxidative stress in the mitochondria and may contribute to the decline in mitochondrial function associated with the aging process.
As the proton concentration increases in the intermembrane space, a strong electrochemical gradient is established across the inner membrane. The protons can return to the matrix through the ATP synthase complex, and their potential energy is used to synthesize ATP from ADP and inorganic phosphate (Pi). This process is called chemiosmosis, and was first described by Peter Mitchell who was awarded the 1978 Nobel Prize in Chemistry for his work. Later, part of the 1997 Nobel Prize in Chemistry was awarded to Paul D. Boyer and John E. Walker for their clarification of the working mechanism of ATP synthase.
Heat production.
Under certain conditions, protons can re-enter the mitochondrial matrix without contributing to ATP synthesis. This process is known as "proton leak" or "mitochondrial uncoupling" and is due to the facilitated diffusion of protons into the matrix. The process results in the unharnessed potential energy of the proton electrochemical gradient being released as heat. The process is mediated by a proton channel called thermogenin, or UCP1. Thermogenin is a 33 kDa protein first discovered in 1973. Thermogenin is primarily found in brown adipose tissue, or brown fat, and is responsible for non-shivering thermogenesis. Brown adipose tissue is found in mammals, and is at its highest levels in early life and in hibernating animals. In humans, brown adipose tissue is present at birth and decreases with age.
Storage of calcium ions.
The concentrations of free calcium in the cell can regulate an array of reactions and is important for signal transduction in the cell. Mitochondria can transiently store calcium, a contributing process for the cell's homeostasis of calcium. In fact, their ability to rapidly take in calcium for later release makes them very good "cytosolic buffers" for calcium. The endoplasmic reticulum (ER) is the most significant storage site of calcium, and there is a significant interplay between the mitochondrion and ER with regard to calcium. The calcium is taken up into the matrix by the mitochondrial calcium uniporter on the inner mitochondrial membrane. It is primarily driven by the mitochondrial membrane potential. Release of this calcium back into the cell's interior can occur via a sodium-calcium exchange protein or via "calcium-induced-calcium-release" pathways. This can initiate calcium spikes or calcium waves with large changes in the membrane potential. These can activate a series of second messenger system proteins that can coordinate processes such as neurotransmitter release in nerve cells and release of hormones in endocrine cells.
Ca2+ influx to the mitochondrial matrix has recently been implicated as a mechanism to regulate respiratory bioenergetics by allowing the electrochemical potential across the membrane to transiently "pulse" from ΔΨ-dominated to pH-dominated, facilitating a reduction of oxidative stress. In neurons, concomitant increases in cytosolic and mitochondrial calcium act to synchronize neuronal activity with mitochondrial energy metabolism. Mitochondrial matrix calcium levels can reach the tens of micromolar levels, which is necessary for the activation of isocitrate dehydrogenase, one of the key regulatory enzymes of the Kreb's cycle.
Additional functions.
Mitochondria play a central role in many other metabolic tasks, such as:
Some mitochondrial functions are performed only in specific types of cells. For example, mitochondria in liver cells contain enzymes that allow them to detoxify ammonia, a waste product of protein metabolism. A mutation in the genes regulating any of these functions can result in mitochondrial diseases.
Cellular proliferation regulation.
The relationship between cellular proliferation and mitochondria has been investigated using cervical cancer HeLa cells. Tumor cells require an ample amount of ATP (Adenosine triphosphate) in order to synthesize bioactive compounds such as lipids, proteins, and nucleotides for rapid cell proliferation. The majority of ATP in tumor cells is generated via the oxidative phosphorylation pathway (OxPhos). Interference with OxPhos have shown to cause cell cycle arrest suggesting that mitochondria play a role in cell proliferation. Mitochondrial ATP production is also vital for cell division in addition to other basic functions in the cell including the regulation of cell volume, solute concentration, and cellular architecture. ATP levels differ at various stages of the cell cycle suggesting that there is a relationship between the abundance of ATP and the cell's ability to enter a new cell cycle. ATP's role in the basic functions of the cell make the cell cycle sensitive to changes in the availability of mitochondrial derived ATP. The variation in ATP levels at different stages of the cell cycle support the hypothesis that mitochondria play an important role in cell cycle regulation. Although the specific mechanisms between mitochondria and the cell cycle regulation is not well understood, studies have shown that low energy cell cycle checkpoints monitor the energy capability before committing to another round of cell division.
Genome.
The human mitochondrial genome is a circular DNA molecule of about 16 kilobases. It encodes 37 genes: 13 for subunits of respiratory complexes I, III, IV and V, 22 for mitochondrial tRNA (for the 20 standard amino acids, plus an extra gene for leucine and serine), and 2 for rRNA. One mitochondrion can contain two to ten copies of its DNA.
As in prokaryotes, there is a very high proportion of coding DNA and an absence of repeats. Mitochondrial genes are transcribed as multigenic transcripts, which are cleaved and polyadenylated to yield mature mRNAs. Not all proteins necessary for mitochondrial function are encoded by the mitochondrial genome; most are coded by genes in the cell nucleus and the corresponding proteins are imported into the mitochondrion. The exact number of genes encoded by the nucleus and the mitochondrial genome differs between species. Most mitochondrial genomes are circular, although exceptions have been reported. In general, mitochondrial DNA lacks introns, as is the case in the human mitochondrial genome; however, introns have been observed in some eukaryotic mitochondrial DNA, such as that of yeast and protists, including "Dictyostelium discoideum". Between protein-coding regions, tRNAs are present. During transcription, the tRNAs acquire their characteristic L-shape that gets recognized and cleaved by specific enzymes. Mitochondrial tRNA genes have different sequences from the nuclear tRNAs but lookalikes of mitochondrial tRNAs have been found in the nuclear chromosomes with high sequence similarity.
In animals, the mitochondrial genome is typically a single circular chromosome that is approximately 16 kb long and has 37 genes. The genes, while highly conserved, may vary in location. Curiously, this pattern is not found in the human body louse ("Pediculus humanus"). Instead, this mitochondrial genome is arranged in 18 minicircular chromosomes, each of which is 3–4 kb long and has one to three genes. This pattern is also found in other sucking lice, but not in chewing lice. Recombination has been shown to occur between the minichromosomes. The reason for this difference is not known.
While slight variations on the standard code had been predicted earlier, none was discovered until 1979, when researchers studying human mitochondrial genes determined that they used an alternative code. Although, the mitochondria of many other eukaryotes, including most plants, use the standard code. Many slight variants have been discovered since, including various alternative mitochondrial codes. Further, the AUA, AUC, and AUU codons are all allowable start codons.
Some of these differences should be regarded as pseudo-changes in the genetic code due to the phenomenon of RNA editing, which is common in mitochondria. In higher plants, it was thought that CGG encoded for tryptophan and not arginine; however, the codon in the processed RNA was discovered to be the UGG codon, consistent with the standard genetic code for tryptophan. Of note, the arthropod mitochondrial genetic code has undergone parallel evolution within a phylum, with some organisms uniquely translating AGG to lysine.
Mitochondrial genomes have far fewer genes than the bacteria from which they are thought to be descended. Although some have been lost altogether, many have been transferred to the nucleus, such as the respiratory complex II protein subunits. This is thought to be relatively common over evolutionary time. A few organisms, such as the "Cryptosporidium", actually have mitochondria that lack any DNA, presumably because all their genes have been lost or transferred. In "Cryptosporidium", the mitochondria have an altered ATP generation system that renders the parasite resistant to many classical mitochondrial inhibitors such as cyanide, azide, and atovaquone.
Replication and inheritance.
Mitochondria divide by binary fission, similar to bacterial cell division. The regulation of this division differs between eukaryotes. In many single-celled eukaryotes, their growth and division is linked to the cell cycle. For example, a single mitochondrion may divide synchronously with the nucleus. This division and segregation process must be tightly controlled so that each daughter cell receives at least one mitochondrion. In other eukaryotes (in mammals for example), mitochondria may replicate their DNA and divide mainly in response to the energy needs of the cell, rather than in phase with the cell cycle. When the energy needs of a cell are high, mitochondria grow and divide. When the energy use is low, mitochondria are destroyed or become inactive. In such examples, and in contrast to the situation in many single celled eukaryotes, mitochondria are apparently randomly distributed to the daughter cells during the division of the cytoplasm. Understanding of mitochondrial dynamics, which is described as the balance between mitochondrial fusion and fission, has revealed that functional and structural alterations in mitochondrial morphology are important factors in pathologies associated with several disease conditions.
The hypothesis of mitochondrial binary fission has relied on the visualization by fluorescence microscopy and conventional transmission electron microscopy (TEM). The resolution of fluorescence microscopy(~200 nm) is insufficient to distinguish structural details, such as double mitochondrial membrane in mitochondrial division or even to distinguish individual mitochondria when several are close together. Conventional TEM has also some technical limitations in verifying mitochondrial division. Cryo-electron tomography was recently used to visualize mitochondrial division in frozen hydrated intact cells. It revealed that mitochondria divide by budding.
An individual's mitochondrial genes are not inherited by the same mechanism as nuclear genes. Typically, the mitochondria are inherited from one parent only. In humans, when an egg cell is fertilized by a sperm, the egg nucleus and sperm nucleus each contribute equally to the genetic makeup of the zygote nucleus. In contrast, the mitochondria, and therefore the mitochondrial DNA, usually come from the egg only. The sperm's mitochondria enter the egg, but do not contribute genetic information to the embryo. Instead, paternal mitochondria are marked with ubiquitin to select them for later destruction inside the embryo. The egg cell contains relatively few mitochondria, but it is these mitochondria that survive and divide to populate the cells of the adult organism. Mitochondria are, therefore, in most cases inherited only from mothers, a pattern known as maternal inheritance. This mode is seen in most organisms, including the majority of animals. However, mitochondria in some species can sometimes be inherited paternally. This is the norm among certain coniferous plants, although not in pine trees and yews. For Mytilids, paternal inheritance only occurs within males of the species. It has been suggested that it occurs at a very low level in humans. There is a recent suggestion that mitochondria that shorten male lifespan stay in the system because they are inherited only through the mother. By contrast, natural selection weeds out mitochondria that reduce female survival as such mitochondria are less likely to be passed on to the next generation. Therefore, it is suggested that human females and female animals tend to live longer than males. The authors claim that this is a partial explanation.
Uniparental inheritance leads to little opportunity for genetic recombination between different lineages of mitochondria, although a single mitochondrion can contain 2–10 copies of its DNA. For this reason, mitochondrial DNA is usually thought to reproduce by binary fission. What recombination does take place maintains genetic integrity rather than maintaining diversity. However, there are studies showing evidence of recombination in mitochondrial DNA. It is clear that the enzymes necessary for recombination are present in mammalian cells. Further, evidence suggests that animal mitochondria can undergo recombination. The data are a bit more controversial in humans, although indirect evidence of recombination exists. If recombination does not occur, the whole mitochondrial DNA sequence represents a single haplotype, which makes it useful for studying the evolutionary history of populations.
Entities undergoing uniparental inheritance and with little to no recombination may be expected to be subject to Muller's ratchet, the inexorable accumulation of deleterious mutations until functionality is lost. Animal populations of mitochondria avoid this buildup through a developmental process known as the mtDNA bottleneck. The bottleneck exploits stochastic processes in the cell to increase in the cell-to-cell variability in mutant load as an organism develops: a single egg cell with some proportion of mutant mtDNA thus produces an embryo where different cells have different mutant loads. Cell-level selection may then act to remove those cells with more mutant mtDNA, leading to a stabilisation or reduction in mutant load between generations. The mechanism underlying the bottleneck is debated, with a recent mathematical and experimental metastudy providing evidence for a combination of random partitioning of mtDNAs at cell divisions and random turnover of mtDNA molecules within the cell. 
Population genetic studies.
The near-absence of genetic recombination in mitochondrial DNA makes it a useful source of information for scientists involved in population genetics and evolutionary biology. Because all the mitochondrial DNA is inherited as a single unit, or haplotype, the relationships between mitochondrial DNA from different individuals can be represented as a gene tree. Patterns in these gene trees can be used to infer the evolutionary history of populations. The classic example of this is in human evolutionary genetics, where the molecular clock can be used to provide a recent date for mitochondrial Eve. This is often interpreted as strong support for a recent modern human expansion out of Africa. Another human example is the sequencing of mitochondrial DNA from Neanderthal bones. The relatively large evolutionary distance between the mitochondrial DNA sequences of Neanderthals and living humans has been interpreted as evidence for the lack of interbreeding between Neanderthals and anatomically modern humans.
However, mitochondrial DNA reflects only the history of the females in a population and so may not represent the history of the population as a whole. This can be partially overcome by the use of paternal genetic sequences, such as the non-recombining region of the Y-chromosome. In a broader sense, only studies that also include nuclear DNA can provide a comprehensive evolutionary history of a population.
Recent measurements of the molecular clock for mitochondrial DNA reported a value of 1 mutation every 7884 years dating back to the most recent common ancestor of humans and apes, which is consistent with estimates of mutation rates of autosomal DNA (10−8 per base per generation).
Dysfunction and disease.
Mitochondrial diseases.
Damage and subsequent dysfunction in mitochondria is an important factor in a range of human diseases due to their influence in cell metabolism. Mitochondrial disorders often present themselves as neurological disorders, including autism. They can also manifest as myopathy, diabetes, multiple endocrinopathy, and a variety of other systemic disorders. Diseases caused by mutation in the mtDNA include Kearns-Sayre syndrome, MELAS syndrome and Leber's hereditary optic neuropathy. In the vast majority of cases, these diseases are transmitted by a female to her children, as the zygote derives its mitochondria and hence its mtDNA from the ovum. Diseases such as Kearns-Sayre syndrome, Pearson syndrome, and progressive external ophthalmoplegia are thought to be due to large-scale mtDNA rearrangements, whereas other diseases such as MELAS syndrome, Leber's hereditary optic neuropathy, myoclonic epilepsy with ragged red fibers (MERRF), and others are due to point mutations in mtDNA.
In other diseases, defects in nuclear genes lead to dysfunction of mitochondrial proteins. This is the case in Friedreich's ataxia, hereditary spastic paraplegia, and Wilson's disease. These diseases are inherited in a dominance relationship, as applies to most other genetic diseases. A variety of disorders can be caused by nuclear mutations of oxidative phosphorylation enzymes, such as coenzyme Q10 deficiency and Barth syndrome. Environmental influences may interact with hereditary predispositions and cause mitochondrial disease. For example, there may be a link between pesticide exposure and the later onset of Parkinson's disease. Other pathologies with etiology involving mitochondrial dysfunction include schizophrenia, bipolar disorder, dementia, Alzheimer's disease, Parkinson's disease, epilepsy, stroke, cardiovascular disease, chronic fatigue syndrome, retinitis pigmentosa, and diabetes mellitus.
Mitochondria-mediated oxidative stress plays a role in cardiomyopathy in Type 2 diabetics. Increased fatty acid delivery to the heart increases fatty acid uptake by cardiomyocytes, resulting in increased fatty acid oxidation in these cells. This process increases the reducing equivalents available to the electron transport chain of the mitochondria, ultimately increasing reactive oxygen species (ROS) production. ROS increases uncoupling proteins (UCPs) and potentiate proton leakage through the adenine nucleotide translocator (ANT), the combination of which uncouples the mitochondria. Uncoupling then increases oxygen consumption by the mitochondria, compounding the increase in fatty acid oxidation. This creates a vicious cycle of uncoupling; furthermore, even though oxygen consumption increases, ATP synthesis does not increase proportionally because the mitochondria is uncoupled. Less ATP availability ultimately results in an energy deficit presenting as reduced cardiac efficiency and contractile dysfunction. To compound the problem, impaired sarcoplasmic reticulum calcium release and reduced mitochondrial reuptake limits peak cytosolic levels of the important signaling ion during muscle contraction. The decreased intra-mitochondrial calcium concentration increases dehydrogenase activation and ATP synthesis. So in addition to lower ATP synthesis due to fatty acid oxidation, ATP synthesis is impaired by poor calcium signaling as well, causing cardiac problems for diabetics.
Possible relationships to aging.
Given the role of mitochondria as the cell's powerhouse, there may be some leakage of the high-energy electrons in the respiratory chain to form reactive oxygen species. This was thought to result in significant oxidative stress in the mitochondria with high mutation rates of mitochondrial DNA (mtDNA). Hypothesized links between aging and oxidative stress are not new and were proposed in 1956, which was later refined into the mitochondrial free radical theory of aging. A vicious cycle was thought to occur, as oxidative stress leads to mitochondrial DNA mutations, which can lead to enzymatic abnormalities and further oxidative stress.
A number of changes can occur to mitochondria during the aging process. Tissues from elderly patients show a decrease in enzymatic activity of the proteins of the respiratory chain. However, mutated mtDNA can only be found in about 0.2% of very old cells. Large deletions in the mitochondrial genome have been hypothesized to lead to high levels of oxidative stress and neuronal death in Parkinson's disease.
References.
General

</doc>
<doc id="19589" url="https://en.wikipedia.org/wiki?curid=19589" title="Minimax">
Minimax

Minimax (sometimes MinMax or MM) is a decision rule used in decision theory, game theory, statistics and philosophy for "mini"mizing the possible loss for a worst case ("max"imum loss) scenario. Originally formulated for two-player zero-sum game theory, covering both the cases where players take alternate moves and those where they make simultaneous moves, it has also been extended to more complex games and to general decision-making in the presence of uncertainty.
Game theory.
In general games.
The maximin value of a player is the largest value that the player can be sure to get without knowing the actions of the other players. Its formal definition is:
Where:
Calculating the maximin value of a player is done in a worst-case approach: for each possible action of the player, we check all possible actions of the other players and determine the worst possible combination of actions - the one that gives player formula_2 the smallest value. Then, we determine which action player formula_2 can take in order to make sure that this smallest value is the largest possible.
For example, consider the following game for two players, where the first player ("row player") may choose any of three moves, labelled T, M, or B, and the second player ("column" player) may choose either of two moves, L or R. The result of the combination of both moves is expressed in a payoff table:
(where the first number in each cell is the pay-out of the row player and the second number is the pay-out of the column player).
For the sake of example, we consider only pure strategies. Check each player in turn:
If both players play their maximin strategies (T,L), the payoff vector is (3,1). In contrast, the only Nash equilibrium in this game is (B,R), which leads to a payoff vector of (4,4).
The minimax value of a player is the smallest value that the other players can force the player to receive, without knowing his actions. Equivalently, it is the largest value the player can be sure to get when he "knows" the actions of the other players. Its formal definition is:
The definition is very similar to that of the maximin value - only the order of the maximum and minimum operators is inverse.
For every player , the maximin is at most the minimax:
Intuitively, in maximin the maximization comes before the minimization, so player tries to maximize his value before knowing what the others will do; in minimax the maximization comes after the minimization, so player is in a much better position - he maximizes his value knowing what the others did.
Usually, the maximin is strictly smaller than the minimax. Consider the game in the above example:
In zero-sum games.
In zero-sum games, the minimax solution is the same as the Nash equilibrium.
In the context of zero-sum games, the minimax theorem is equivalent to:
For every two-person, zero-sum game with finitely many strategies, there exists a value V and a mixed strategy for each player, such that
Equivalently, Player 1's strategy guarantees him a payoff of V regardless of Player 2's strategy, and similarly Player 2 can guarantee himself a payoff of −V. The name minimax arises because each player minimizes the maximum payoff possible for the other—since the game is zero-sum, he/she also minimizes his/their own maximum loss (i.e. maximize his/her minimum payoff).
See also example of a game without a value.
Example.
The following example of a zero-sum game, where A and B make simultaneous moves, illustrates "minimax" solutions. Suppose each player has three choices and consider the payoff matrix for A displayed on the right. Assume the payoff matrix for B is the same matrix with the signs reversed (i.e. if the choices are A1 and B1 then B pays 3 to A). Then, the minimax choice for A is A2 since the worst possible result is then having to pay 1, while the simple minimax choice for B is B2 since the worst possible result is then no payment. However, this solution is not stable, since if B believes A will choose A2 then B will choose B1 to gain 1; then if A believes B will choose B1 then A will choose A1 to gain 3; and then B will choose B2; and eventually both players will realize the difficulty of making a choice. So a more stable strategy is needed.
Some choices are "dominated" by others and can be eliminated: A will not choose A3 since either A1 or A2 will produce a better result, no matter what B chooses; B will not choose B3 since some mixtures of B1 and B2 will produce a better result, no matter what A chooses.
A can avoid having to make an expected payment of more than 1∕3 by choosing A1 with probability 1∕6 and A2 with probability 5∕6: The expected payoff for A would be 3 × (1∕6) − 1 × (5∕6) = −1∕3 in case B chose B1 and −2 × (1∕6) + 0 × (5∕6) = −1/3 in case B chose B2. Similarly, B can ensure an expected gain of at least 1/3, no matter what A chooses, by using a randomized strategy of choosing B1 with probability 1∕3 and B2 with probability 2∕3. These mixed minimax strategies are now stable and cannot be improved.
Maximin.
Frequently, in game theory, maximin is distinct from minimax. Minimax is used in zero-sum games to denote minimizing the opponent's maximum payoff. In a zero-sum game, this is identical to minimizing one's own maximum loss, and to maximizing one's own minimum gain.
"Maximin" is a term commonly used for non-zero-sum games to describe the strategy which maximizes one's own minimum payoff. In non-zero-sum games, this is not generally the same as minimizing the opponent's maximum gain, nor the same as the Nash equilibrium strategy.
In repeated games.
The minimax values are very important in the theory of repeated games. One of the central theorems in this theory, the folk theorem, relies on the minimax values.
Combinatorial game theory.
In combinatorial game theory, there is a minimax algorithm for game solutions.
A simple version of the minimax "algorithm", stated below, deals with games such as tic-tac-toe, where each player can win, lose, or draw.
If player A "can" win in one move, his best move is that winning move.
If player B knows that one move will lead to the situation where player A "can" win in one move, while another move will lead to the situation where player A can, at best, draw, then player B's best move is the one leading to a draw.
Late in the game, it's easy to see what the "best" move is.
The Minimax algorithm helps find the best move, by working backwards from the end of the game. At each step it assumes that player A is trying to maximize the chances of A winning, while on the next turn player B is trying to minimize the chances of A winning (i.e., to maximize B's own chances of winning).
Minimax algorithm with alternate moves.
A minimax algorithm is a recursive algorithm for choosing the next move in an n-player game, usually a two-player game. A value is associated with each position or state of the game. This value is computed by means of a position evaluation function and it indicates how good it would be for a player to reach that position. The player then makes the move that maximizes the minimum value of the position resulting from the opponent's possible following moves. If it is A's turn to move, A gives a value to each of his legal moves.
A possible allocation method consists in assigning a certain win for A as +1 and for B as −1. This leads to combinatorial game theory as developed by John Horton Conway. An alternative is using a rule that if the result of a move is an immediate win for A it is assigned positive infinity and, if it is an immediate win for B, negative infinity. The value to A of any other move is the minimum of the values resulting from each of B's possible replies. For this reason, A is called the "maximizing player" and B is called the "minimizing player", hence the name "minimax algorithm". The above algorithm will assign a value of positive or negative infinity to any position since the value of every position will be the value of some final winning or losing position. Often this is generally only possible at the very end of complicated games such as chess or go, since it is not computationally feasible to look ahead as far as the completion of the game, except towards the end, and instead positions are given finite values as estimates of the degree of belief that they will lead to a win for one player or another.
This can be extended if we can supply a heuristic evaluation function which gives values to non-final game states without considering all possible following complete sequences. We can then limit the minimax algorithm to look only at a certain number of moves ahead. This number is called the "look-ahead", measured in "plies". For example, the chess computer Deep Blue (the first one to beat a reigning world champion, Garry Kasparov at that time) looked ahead at least 12 plies, then applied a heuristic evaluation function.
The algorithm can be thought of as exploring the nodes of a "game tree". The "effective branching factor" of the tree is the average number of children of each node (i.e., the average number of legal moves in a position). The number of nodes to be explored usually increases exponentially with the number of plies (it is less than exponential if evaluating forced moves or repeated positions). The number of nodes to be explored for the analysis of a game is therefore approximately the branching factor raised to the power of the number of plies. It is therefore impractical to completely analyze games such as chess using the minimax algorithm.
The performance of the naïve minimax algorithm may be improved dramatically, without affecting the result, by the use of alpha-beta pruning.
Other heuristic pruning methods can also be used, but not all of them are guaranteed to give the same result as the un-pruned search.
A naïve minimax algorithm may be trivially modified to additionally return an entire Principal Variation along with a minimax score.
Pseudocode.
The pseudocode for the depth limited minimax algorithm is given below.
The minimax function returns a heuristic value for leaf nodes (terminal nodes and nodes at the maximum search depth).
Non leaf nodes inherit their value, "bestValue", from a descendant leaf node.
The heuristic value is a score measuring the favorability of the node for the maximizing player.
Hence nodes resulting in a favorable outcome, such as a win, for the maximizing player have higher scores than nodes more favorable for the minimizing player.
The heuristic value for terminal (game ending) leaf nodes are scores corresponding to win, loss, or draw, for the maximizing player.
For non terminal leaf nodes at the maximum search depth, an evaluation function estimates a heuristic value for the node.
The quality of this estimate and the search depth determine the quality and accuracy of the final minimax result.
Minimax treats the two players (the maximizing player and the minimizing player) separately in its code. Based on the observation that formula_18, minimax may often be simplified into the negamax algorithm.
Example.
Suppose the game being played only has a maximum of two possible moves per player each turn. The algorithm generates the tree on the right, where the circles represent the moves of the player running the algorithm ("maximizing player"), and squares represent the moves of the opponent ("minimizing player"). Because of the limitation of computation resources, as explained above, the tree is limited to a "look-ahead" of 4 moves.
The algorithm evaluates each "leaf node" using a heuristic evaluation function, obtaining the values shown. The moves where the "maximizing player" wins are assigned with positive infinity, while the moves that lead to a win of the "minimizing player" are assigned with negative infinity. At level 3, the algorithm will choose, for each node, the smallest of the "child node" values, and assign it to that same node (e.g. the node on the left will choose the minimum between "10" and "+∞", therefore assigning the value "10" to itself). The next step, in level 2, consists of choosing for each node the largest of the "child node" values. Once again, the values are assigned to each "parent node". The algorithm continues evaluating the maximum and minimum values of the child nodes alternately until it reaches the "root node", where it chooses the move with the largest value (represented in the figure with a blue arrow). This is the move that the player should make in order to "minimize" the "maximum" possible loss.
Minimax for individual decisions.
Minimax in the face of uncertainty.
Minimax theory has been extended to decisions where there is no other player, but where the consequences of decisions depend on unknown facts. For example, deciding to prospect for minerals entails a cost which will be wasted if the minerals are not present, but will bring major rewards if they are. One approach is to treat this as a game against "nature" (see move by nature), and using a similar mindset as Murphy's law or resistentialism, take an approach which minimizes the maximum expected loss, using the same techniques as in the two-person zero-sum games.
In addition, expectiminimax trees have been developed, for two-player games in which chance (for example, dice) is a factor.
Minimax criterion in statistical decision theory.
In classical statistical decision theory, we have an estimator formula_19 that is used to estimate a parameter formula_20. We also assume a risk function formula_21, usually specified as the integral of a loss function. In this framework, formula_22 is called minimax if it satisfies
An alternative criterion in the decision theoretic framework is the Bayes estimator in the presence of a prior distribution formula_24. An estimator is Bayes if it minimizes the "average" risk
Non-probabilistic decision theory.
A key feature of minimax decision making is being non-probabilistic: in contrast to decisions using expected value or expected utility, it makes no assumptions about the probabilities of various outcomes, just scenario analysis of what the possible outcomes are. It is thus robust to changes in the assumptions, as these other decision techniques are not. Various extensions of this non-probabilistic approach exist, notably minimax regret and Info-gap decision theory.
Further, minimax only requires ordinal measurement (that outcomes be compared and ranked), not "interval" measurements (that outcomes include "how much better or worse"), and returns ordinal data, using only the modeled outcomes: the conclusion of a minimax analysis is: "this strategy is minimax, as the worst case is (outcome), which is less bad than any other strategy". Compare to expected value analysis, whose conclusion is of the form: "this strategy yields E("X")="n."" Minimax thus can be used on ordinal data, and can be more transparent.
Maximin in philosophy.
In philosophy, the term "maximin" is often used in the context of John Rawls's "A Theory of Justice," where he refers to it (Rawls (1971, p. 152)) in the context of The Difference Principle.
Rawls defined this principle as the rule which states that social and economic inequalities should be arranged so that "they are to be of the greatest benefit to the least-advantaged members of society".

</doc>
<doc id="19590" url="https://en.wikipedia.org/wiki?curid=19590" title="Minnesota">
Minnesota

Minnesota (; locally ) is a state in the Midwestern United States. Minnesota was admitted as the 32nd state on May 11, 1858, created from the eastern half of the Minnesota Territory. The name comes from the Dakota word for "clear blue water". Owing to its large number of lakes, the state is informally known as the "Land of 10,000 Lakes". Its official motto is "L'Étoile du Nord ("French:" Star of the North)." 
Minnesota is the 12th largest in area and the 21st most populous of the U.S. States; nearly 60 percent of its residents live in the Minneapolis–Saint Paul metropolitan area (known as the "Twin Cities"), the center of transportation, business, industry, education, and government and home to an internationally known arts community. The remainder of the state consists of western prairies now given over to intensive agriculture; deciduous forests in the southeast, now partially cleared, farmed and settled; and the less populated North Woods, used for mining, forestry, and recreation.
Minnesota is known for its progressive political orientation and its high rate of civic participation and voter turnout. Until European settlement, Minnesota was inhabited by the Dakota and Ojibwe/Anishinaabe. The large majority of the original European settlers emigrated from Scandinavia and Germany, and the state remains a center of Scandinavian American and German American culture. In recent decades, immigration from Asia, the Horn of Africa, and Latin America has broadened its historic demographic and cultural composition. Minnesota's standard of living index is among the highest in the United States, and the state is also among the best-educated and wealthiest in the nation.
Etymology.
The word "Minnesota" comes from the Dakota name for the Minnesota River: "Mnisota". The root "mni" (also spelled "mini" or "minne") means "water" and "tō" ("ta") means "blue". "Mnisota" can be translated as "clear blue water" or "clouded blue water" depending on pronunciation. Native Americans demonstrated the name to early settlers by dropping milk into water and calling it "mnisota". Many places in the state have similar names, such as Minnehaha Falls ("laughing water" (waterfall)), Minneiska ("white water"), Minneota ("much water"), Minnetonka ("big water"), Minnetrista ("crooked water"), and Minneapolis, a combination of "mni" and "polis", the Greek word for "city".
Geography.
Minnesota is the second northernmost U.S. state (after Alaska). Its isolated Northwest Angle in Lake of the Woods county is the only part of the 48 contiguous states lying north of the 49th parallel. The state is part of the U.S. region known as the Upper Midwest and part of North America's Great Lakes Region. It shares a Lake Superior water border with Michigan and a land and water border with Wisconsin to the east. Iowa is to the south, North Dakota and South Dakota are to the west, and the Canadian provinces of Ontario and Manitoba are to the north. With , or approximately 2.25 percent of the United States, Minnesota is the 12th-largest state.
Geology.
Minnesota contains some of the oldest rocks found on earth. Gneisses are about 3.6 billion years old (80 percent as old as the planet). About 2.7 billion years ago, basaltic lava poured out of cracks in the floor of the primordial ocean; the remains of this volcanic rock formed the Canadian Shield in northeast Minnesota. The roots of these volcanic mountains and the action of Precambrian seas formed the Iron Range of northern Minnesota. Following a period of volcanism 1.1 billion years ago, Minnesota's geological activity has been more subdued, with no volcanism or mountain formation, but with repeated incursions of the sea, which left behind multiple strata of sedimentary rock.
In more recent times, massive ice sheets at least one kilometer thick ravaged the landscape of the state and sculpted its current terrain. The Wisconsin glaciation left 12,000 years ago. These glaciers covered all of Minnesota except the far southeast, an area characterized by steep hills and streams that cut into the bedrock. This area is known as the Driftless Zone for its absence of glacial drift. Much of the remainder of the state outside the northeast has 50 feet (15 m) or more of glacial till left behind as the last glaciers retreated. Gigantic Lake Agassiz formed in the northwest 13,000 years ago. Its bed created the fertile Red River valley, and its outflow, glacial River Warren, carved the valley of the Minnesota River and the Upper Mississippi downstream from Fort Snelling. Minnesota is geologically quiet today; it experiences earthquakes infrequently, and most of them are minor.
The state's high point is Eagle Mountain at 2,301 feet (701 m), which is only away from the low of 601 feet (183 m) at the shore of Lake Superior. Notwithstanding dramatic local differences in elevation, much of the state is a gently rolling peneplain.
Two major drainage divides meet in the northeastern part of Minnesota in rural Hibbing, forming a triple watershed. Precipitation can follow the Mississippi River south to the Gulf of Mexico, the Saint Lawrence Seaway east to the Atlantic Ocean, or the Hudson Bay watershed to the Arctic Ocean.
The state's nickname, the "Land of 10,000 Lakes", is apt, as there are 11,842 Minnesota lakes over in size. The Minnesota portion of Lake Superior is the largest at and deepest (at ) body of water in the state. Minnesota has 6,564 natural rivers and streams that cumulatively flow for . The Mississippi River begins its journey from its headwaters at Lake Itasca and crosses the Iowa border downstream. It is joined by the Minnesota River at Fort Snelling, by the St. Croix River near Hastings, by the Chippewa River at Wabasha, and by many smaller streams. The Red River, in the bed of glacial Lake Agassiz, drains the northwest part of the state northward toward Canada's Hudson Bay. Approximately of wetlands are contained within Minnesota's borders, the most of any state except Alaska.
Flora and fauna.
Minnesota has four ecological provinces: Prairie Parkland, in the southwestern and western parts of the state; the Eastern Broadleaf Forest (Big Woods) in the southeast, extending in a narrowing strip to the northwestern part of the state, where it transitions into Tallgrass Aspen Parkland; and the northern Laurentian Mixed Forest, a transitional forest between the northern boreal forest and the broadleaf forests to the south. These northern forests are a vast wilderness of pine and spruce trees mixed with patchy stands of birch and poplar.
Much of Minnesota's northern forest underwent logging at some time, leaving only a few patches of old growth forest today in areas such as in the Chippewa National Forest and the Superior National Forest where the Boundary Waters Canoe Area Wilderness has some of unlogged land. Although logging continues, regrowth and replanting keeps about one third of the state forested. Nearly all of Minnesota's prairies and oak savannas have been fragmented by farming, grazing, logging, and suburban development.
While loss of habitat has affected native animals such as the pine marten, elk, woodland caribou, and bison, others like whitetail deer and bobcat thrive. The state has the nation's largest population of timber wolves outside Alaska, and supports healthy populations of black bears, moose, and gophers. Located on the Mississippi Flyway, Minnesota hosts migratory waterfowl such as geese and ducks, and game birds such as grouse, pheasants, and turkeys. It is home to birds of prey including the largest number of breeding pairs of bald eagles in the lower 48 states as of 2007, red-tailed hawks, and snowy owls. The lakes teem with sport fish such as walleye, bass, muskellunge, and northern pike, and streams in the southeast and northeast are populated by brook, brown, and rainbow trout.
Climate.
Minnesota experiences temperature extremes characteristic of its continental climate, with cold winters and hot summers. The record high and low span is , from at Tower on February 2, 1996, to at Moorhead on July 6, 1936. Meteorological events include rain, snow, blizzards, thunderstorms, hail, derechos, tornadoes, and high-velocity straight-line winds. The growing season varies from 90 days per year in the Iron Range to 160 days in southeast Minnesota near the Mississippi River, and average temperatures range from . Average summer dew points range from about in the south to about in the north. Average annual precipitation ranges from , and droughts occur every 10 to 50 years.
Protected lands.
Minnesota's first state park, Itasca State Park, was established in 1891, and is the source of the Mississippi River. Today Minnesota has 72 state parks and recreation areas, 58 state forests covering about four million acres (16,000 km²), and numerous state wildlife preserves, all managed by the Minnesota Department of Natural Resources. There are in the Chippewa and [National Forest|Superior national forest]s. The Superior National Forest in the northeast contains the Boundary Waters Canoe Area Wilderness, which encompasses over a million acres (4,000 km²) and a thousand lakes. To its west is Voyageurs National Park. The Mississippi National River and Recreation Area (MNRRA), is a corridor along the Mississippi River through the Minneapolis–St. Paul Metropolitan Area connecting a variety of sites of historic, cultural, and geologic interest.
History.
Before European settlement of North America, Minnesota was populated by the Dakota people. As Europeans settled the east coast, Native American movement away from them caused migration of the Anishinaabe and other Native Americans into the Minnesota area. The first Europeans in the area were French fur traders who arrived in the 17th century. Late that century, Anishinaabe, also known as Ojibwe Indians migrated westward to Minnesota, causing tensions with the Dakota people. Explorers such as Daniel Greysolon, Sieur du Lhut, Father Louis Hennepin, Jonathan Carver, Henry Schoolcraft, and Joseph Nicollet mapped out the state.
In 1762 the region became part of Spanish Louisiana until 1802. The portion of the state east of the Mississippi River became part of the United States at the end of the American Revolutionary War, when the Second Treaty of Paris was signed. Land west of the Mississippi River was acquired with the Louisiana Purchase, although a portion of the Red River Valley was disputed until the Treaty of 1818. In 1805, Zebulon Pike bargained with Native Americans to acquire land at the confluence of the Minnesota and Mississippi rivers. The construction of Fort Snelling followed between 1819 and 1825. Its soldiers built a grist mill and a sawmill at Saint Anthony Falls, the first of the water-powered industries around which the city of Minneapolis later grew. Meanwhile, squatters, government officials, and tourists had settled near the fort. In 1839, the army forced them to move downriver and they settled in the area that became St. Paul. Minnesota Territory was formed on March 3, 1849. The first territorial legislature (held September 2, 1849) was dominated by men from New England or of New England ancestry. Thousands of people had come to build farms and cut timber, and Minnesota became the 32nd U.S. state on May 11, 1858. The founding population was so overwhelmingly of New England origins that the state was dubbed "the New England of the West".
Treaties between European settlers and the Dakota and Ojibwe gradually forced the natives off their lands and on to smaller reservations. In 1861, residents of Mankato formed the Knights of the Forest, with a goal of eliminating all Indians from Minnesota. As conditions deteriorated for the Dakota, tensions rose, leading to the Dakota War of 1862. The result of the six-week war was the execution of 38 Dakota and the exile of most of the rest of the Dakota to the Crow Creek Reservation in Dakota Territory. As many as 800 white settlers died during the war.
Logging and farming were mainstays of Minnesota's early economy. The sawmills at Saint Anthony Falls, and logging centers like Marine on St. Croix, Stillwater, and Winona, processed high volumes of lumber. These cities were situated on rivers that were ideal for transportation. Later, Saint Anthony Falls was tapped to provide power for flour mills. Innovations by Minneapolis millers led to the production of Minnesota "patent" flour, which commanded almost double the price of "bakers'" or "clear" flour, which it replaced. By 1900, Minnesota mills, led by Pillsbury, Northwestern and the Washburn-Crosby Company (a forerunner of General Mills), were grinding 14.1 percent of the nation's grain.
The state's iron-mining industry was established with the discovery of iron in the Vermilion Range and the Mesabi Range in the 1880s, and in the Cuyuna Range in the early 20th century. The ore was shipped by rail to Duluth and Two Harbors, then loaded onto ships and transported eastward over the Great Lakes.
Industrial development and the rise of manufacturing caused the population to shift gradually from rural areas to cities during the early 20th century. Nevertheless, farming remained prevalent. Minnesota's economy was hard-hit by the Great Depression, resulting in lower prices for farmers, layoffs among iron miners, and labor unrest. Compounding the adversity, western Minnesota and the Dakotas were hit by drought from 1931 to 1935. New Deal programs provided some economic turnaround. The Civilian Conservation Corps and other programs around the state established some jobs for Indians on their reservations, and the Indian Reorganization Act of 1934 provided the tribes with a mechanism of self-government. This provided natives a greater voice within the state, and promoted more respect for tribal customs because religious ceremonies and native languages were no longer suppressed.
After World War II, industrial development quickened. New technology increased farm productivity through automation of feedlots for hogs and cattle, machine milking at dairy farms, and raising chickens in large buildings. Planting became more specialized with hybridization of corn and wheat, and the use of farm machinery such as tractors and combines became the norm. University of Minnesota professor Norman Borlaug contributed to these developments as part of the Green Revolution. Suburban development accelerated due to increased postwar housing demand and convenient transportation. Increased mobility, in turn, enabled more specialized jobs.
Minnesota became a center of technology after World War II. Engineering Research Associates was formed in 1946 to develop computers for the United States Navy. It later merged with Remington Rand, and then became Sperry Rand. William Norris left Sperry in 1957 to form Control Data Corporation (CDC). Cray Research was formed when Seymour Cray left CDC to form his own company. Medical device maker Medtronic also started business in the Twin Cities in 1949.
Cities and towns.
Saint Paul, located in east-central Minnesota along the banks of the Mississippi River, has been Minnesota's capital city since 1849, first as capital of the Territory of Minnesota, and then as state capital since 1858.
Saint Paul is adjacent to Minnesota's most populous city, Minneapolis; they and their suburbs are known collectively as the Twin Cities metropolitan area, the 13th-largest metropolitan area in the United States and home to about 60 percent of the state's population. The remainder of the state is known as "Greater Minnesota" or "Outstate Minnesota".
The state has 17 cities with populations above 50,000 (as of the 2010 census). In descending order of population, they are Minneapolis, Saint Paul, Rochester, Duluth, Bloomington, Brooklyn Park, Plymouth, Saint Cloud, Woodbury, Eagan, Maple Grove, Coon Rapids, Eden Prairie, Minnetonka, Burnsville, Apple Valley, Blaine and Lakeville. Of these only Rochester, Duluth, and Saint Cloud are outside the Twin Cities metropolitan area.
Minnesota's population continues to grow, primarily in the urban centers. The populations of metropolitan Sherburne and Scott counties doubled between 1980 and 2000, while 40 of the state's 87 counties lost residents over the same period.
Demographics.
Population.
From fewer than 6,120 people in 1850, Minnesota's population grew to over 1.7 million by 1900. Each of the next six decades saw a 15 percent increase in population, reaching 3.4 million in 1960. Growth then slowed, rising 11 percent to 3.8 million in 1970, and an average of 9 percent over the next three decades to 4.9 million in the 2000 Census. The United States Census Bureau estimates that the population of Minnesota was 5,489,594 on July 1, 2015, a 3.5 percent increase since the 2010 United States Census. The rate of population change, and age and gender distributions, approximate the national average. Minnesota's center of population is in Hennepin County.
Race and ancestry.
The state's estimated racial composition in the 2011 American Census Bureau estimate was:
Hispanics or Latinos made up 4.7 percent of the population.
In 2011, non-Hispanic whites were involved in 72.3 percent of all the births. Minnesota's growing minority groups, however, still form a smaller percentage of the population than in the nation as a whole.
The principal ancestries of Minnesota's residents in 2010 were surveyed to be the following:
Ancestries claimed by less than 3 percent of the population include American, Czech, and Dutch, each between 2 and 3 percent; Sub-Saharan African and East African, Scottish, French Canadian, Scotch-Irish and Mexican, each between 1 and 1.9 percent; and less than 1 percent each for Russian, Welsh, Bosnian, Croatian, Serbian, Swiss, Arab, Hungarian, Ukrainian, Greek, Slovak, Lithuanian, Portuguese, and West Indian.
Religion.
The majority of Minnesotans are Protestants, including a significant Lutheran contingent, owing to the state's largely Northern European ethnic makeup, but Roman Catholics (of largely German, Irish, and Slavic descent) make up the largest single Christian denomination. A 2010 survey by the Pew Forum on Religion and Public Life showed that 32 percent of Minnesotans were affiliated with Mainline Protestant traditions, 21 percent were Evangelical Protestants, 28 percent were Roman Catholic, 1 percent each were Jewish, Muslim, Buddhist, and Black Protestant, and smaller amounts were of other faiths, with 13 percent unaffiliated. According to the Association of Religion Data Archives, the denominations with the most adherents in 2010 were the Roman Catholic Church with 1,150,367; the Evangelical Lutheran Church in America with 737,537; and the Lutheran Church Missouri Synod with 182,439. This is broadly consistent with the results of the 2001 American Religious Identification Survey, which also gives detailed percentages for many individual denominations. Although Christianity is dominant, Minnesota has a long history with non-Christian faiths. Ashkenazi Jewish pioneers set up Saint Paul's first synagogue in 1856. Minnesota is home to over 30 mosques, mostly in the Twin Cities metro area. The Temple of ECK, the spiritual home of Eckankar, is based in Minnesota, and there are tens of thousands of Eckists in the state.
Economy.
Once primarily a producer of raw materials, Minnesota's economy has transformed to emphasize finished products and services. Perhaps the most significant characteristic of the economy is its diversity; the relative outputs of its business sectors closely match the United States as a whole. The economy of Minnesota had a gross domestic product of $262 billion in 2008. In 2008, thirty-three of the United States' top 1,000 publicly traded companies (by revenue) were headquartered in Minnesota, including Target, UnitedHealth Group, 3M, General Mills, U.S. Bancorp, Ameriprise, Hormel, Land O' Lakes, SuperValu, Best Buy and Valspar. Private companies based in Minnesota include Cargill, the largest privately owned company in the United States, and Carlson Companies, the parent company of Radisson Hotels.
The per capita personal income in 2008 was $42,772, the tenth-highest in the nation. The three-year median household income from 2002 to 2004 was $55,914, ranking fifth in the U.S. and first among the 36 states not on the Atlantic coast.
As of January 2015, the state's unemployment rate was 3.7 percent.
Industry and commerce.
Minnesota's earliest industries were fur trading and agriculture. The city of Minneapolis grew around the flour mills powered by St. Anthony Falls. Although less than one percent of the population is now employed in the agricultural sector, it remains a major part of the state's economy, ranking sixth in the nation in the value of products sold. The state is the U.S.'s largest producer of sugar beets, sweet corn, and green peas for processing, and farm-raised turkeys. Minnesota is also a large producer of corn and soybeans. Minnesota has the most food cooperatives per capita in the United States. Forestry remains strong, including logging, pulpwood processing and paper production, and forest products manufacturing. Minnesota was famous for its soft-ore mines, which produced a significant portion of the world's iron ore for over a century. Although the high-grade ore is now depleted, taconite mining continues, using processes developed locally to save the industry. In 2004, the state produced 75 percent of the country's usable iron ore. The mining boom created the port of Duluth which continues to be important for shipping ore, coal, and agricultural products. The manufacturing sector now includes technology and biomedical firms in addition to the older food processors and heavy industry. The nation's first indoor shopping mall was Edina's Southdale Center and its largest is Bloomington's Mall of America.
Minnesota is one of 42 U.S. states with its own lottery; its games include Powerball, Mega Millions, Hot Lotto (all three multi-state), Northstar Cash and Gopher 5.
Energy use and production.
Minnesota produces ethanol fuel and is the first to mandate its use, a ten percent mix (E10). In 2005 there were more than 310 service stations supplying E85 fuel, comprising 85 percent ethanol and 15 percent gasoline. A two percent biodiesel blend has been required in diesel fuel since 2005. As of December 2006 the state was the country's fourth-largest producer of wind power, with 895 megawatts installed and another 200 megawatts planned, much of it on the windy Buffalo Ridge in the southwest part of the state.
State taxes.
Minnesota has a progressive income tax structure; the four brackets of state income tax rates are 5.35, 7.05, 7.85 and 9.85 percent. As of 2008, Minnesota was ranked 12th in the nation in per capita total state and local taxes. In 2008, Minnesotans paid 10.2 percent of their income in state and local taxes; the U.S. average was 9.7 percent. The state sales tax in Minnesota is 6.875 percent, but there is no sales tax on clothing, prescription drug medications, some services, or food items for home consumption. The state legislature may allow municipalities to institute local sales taxes and special local taxes, such as the 0.5 percent supplemental sales tax in Minneapolis. Excise taxes are levied on alcohol, tobacco, and motor fuel. The state imposes a use tax on items purchased elsewhere but used within Minnesota. Owners of real property in Minnesota pay property tax to their county, municipality, school district, and special taxing districts.
Culture.
Literature.
The rigors and rewards of pioneer life on the prairie are the subject of "Giants in the Earth" by Ole Rolvaag and the "Little House" series of children's books by Laura Ingalls Wilder. Small-town life is portrayed grimly by Sinclair Lewis in the novel "Main Street", and more gently and affectionately by Garrison Keillor in his tales of Lake Wobegon. St. Paul native F. Scott Fitzgerald writes of the social insecurities and aspirations of the young city in stories such as "Winter Dreams" and "The Ice Palace" (published in "Flappers and Philosophers"). Henry Wadsworth Longfellow's epic poem "The Song of Hiawatha" was inspired by Minnesota and names many of the state's places and bodies of water.
Entertainment.
Minnesota musicians include Bob Dylan, Eddie Cochran, The Andrews Sisters, The Castaways, The Trashmen, Prince, Soul Asylum, David Ellefson, Hüsker Dü, and The Replacements. Minnesotans helped shape the history of music through popular American culture: the Andrews Sisters' "Boogie Woogie Bugle Boy" was an iconic tune of World War II, while the Trashmen's "Surfin' Bird" and Bob Dylan epitomize two sides of the 1960s. In the 1980s, influential hit radio groups and musicians included Prince, The Original 7ven, Jimmy Jam & Terry Lewis, The Jets, Lipps Inc., and Information Society.
Minnesotans have also made significant contributions to comedy, theater, media, and film. The comic strip "Peanuts" was created by St. Paul native Charles M. Schulz. Garrison Keillor resurrected old-style radio comedy with "A Prairie Home Companion", which has aired since 1974. The cult shows "Mystery Science Theater 3000" and "Let's Bowl" originated in the Twin Cities, and Lizz Winstead and Craig Kilborn helped create the increasingly influential Comedy Central program "The Daily Show".
Joel and Ethan Coen, Terry Gilliam, Bill Pohlad, and Mike Todd contributed to the art of filmmaking as writers, directors, and producers. Actors from Minnesota include Loni Anderson, Richard Dean Anderson, James Arness, Jessica Biel, Rachael Leigh Cook, Julia Duffy, Mike Farrell, Judy Garland, Peter Graves, Josh Hartnett, Garrett Hedlund, Tippi Hedren, Jessica Lange, Kelly Lynch, E.G. Marshall, Chris Pratt, Prince, Jane Russell, Winona Ryder, Seann William Scott, Kevin Sorbo, Lea Thompson, Vince Vaughn, Jesse Ventura, and Steve Zahn.
Popular culture.
Stereotypical traits of Minnesotans include "Minnesota nice", Lutheranism, a strong sense of community and shared culture, and a distinctive brand of North Central American English sprinkled with Scandinavian expressions. Potlucks, usually with a variety of hotdishes, are popular small-town church activities. A small segment of the Scandinavian population attend a traditional lutefisk dinner to celebrate Christmas. Many of these Scandinavian cultural characteristics and personality traits are satirized on the nationally-syndicated public radio program "A Prairie Home Companion". Life in Minnesota is depicted in movies such as "Fargo", "Grumpy Old Men", "Grumpier Old Men", "Juno", "Drop Dead Gorgeous", "Young Adult", "A Serious Man", "New in Town", and in famous television series like "Little House on the Prairie", "The Mary Tyler Moore Show", "The Golden Girls", "Coach", "The Rocky and Bullwinkle Show", and "Fargo". Major movies that were shot on location in Minnesota include "That Was Then... This Is Now", "Purple Rain", "Airport", "Beautiful Girls", "North Country", Untamed Heart", "Feeling Minnesota", "Jingle All The Way", "A Simple Plan" and "The Mighty Ducks films".
The Minnesota State Fair, advertised as "The Great Minnesota Get-Together", is an icon of state culture. In a state of 5.4 million people, there were over 1.8 million visitors to the fair in 2014, setting a new attendance record. The fair covers the variety of Minnesotan life, including fine art, science, agriculture, food preparation, 4-H displays, music, the midway, and corporate merchandising. It is known for its displays of seed art, butter sculptures of dairy princesses, the birthing barn, and the "fattest pig" competition. One can also find dozens of varieties of food on a stick, such as Pronto Pups, cheese curds, and deep-fried candy bars. On a smaller scale, many of these attractions are offered at numerous county fairs.
Other large annual festivals include the Saint Paul Winter Carnival, the Minnesota Renaissance Festival, Minneapolis' Aquatennial and Mill City Music Festival, Moondance Jam in Walker, Sonshine Christian music festival in Willmar, the Judy Garland Festival in Grand Rapids, the Eelpout Festival on Leech Lake, and the WE Fest in Detroit Lakes.
Health.
Minnesotans have low rates of premature death, infant mortality, cardiovascular disease, and occupational fatalities. They have long life expectancies, and high rates of health insurance and regular exercise. These and other measures have led two groups to rank Minnesota as the healthiest state in the nation; however, in one of these rankings, Minnesota descended from first to sixth in the nation between 2005 and 2009 because of low levels of public health funding and the prevalence of binge drinking.
On October 1, 2007, Minnesota became the 17th state to enact the Freedom to Breathe Act, a statewide smoking ban in restaurants and bars.
Medical care in the state is provided by a comprehensive network of hospitals and clinics headed by two institutions with international reputations. The University of Minnesota Medical School is a high-rated teaching institution that has made a number of breakthroughs in treatment, and its research activities contribute significantly to the state's growing biotechnology industry. The Mayo Clinic, a world-renowned hospital based in Rochester, was founded by William Worrall Mayo, an immigrant from England.
"U.S. News and World Report" 2014–2015 survey ranked 4,743 hospitals in the United States in 16 specialized fields of care, and placed the Mayo Clinic in the top four in all fields except psychiatry, where it ranked seventh. The hospital ranked #1 in eight fields and #2 in three others. The Mayo Clinic and the University of Minnesota are partners in the Minnesota Partnership for Biotechnology and Medical Genomics, a state-funded program that conducts research into cancer, Alzheimer's disease, heart health, obesity, and other areas.
Education.
One of the Minnesota Legislature's first acts when it opened in 1858 was the creation of a normal school in Winona. Minnesota's commitment to education has contributed to a literate and well-educated populace. In 2009, according to the U.S. Census Bureau, Minnesota had the second-highest proportion of high school graduates, with 91.5% of people 25 and older holding a diploma, and the tenth-highest proportion of people with bachelor's degrees. In a 2013 study conducted by the National Center for Educational Statistics comparing the performance of eighth-grade students internationally in math and science, Minnesota ranked eighth in the world and third in the United States, behind Massachusetts and Vermont. In 2014, Minnesota students earned the tenth-highest average composite score in the nation on the ACT exam. While Minnesota has chosen not to implement school vouchers, it is home to the first charter school.
The state supports a network of public universities and colleges, including 32 institutions in the Minnesota State Colleges and Universities System, and five major campuses of the University of Minnesota. It is also home to more than 20 private colleges and universities, six of which rank among the nation's top 100 liberal arts colleges, according to U.S. News & World Report.
Transportation.
Transportation in Minnesota is overseen by the Minnesota Department of Transportation (MnDOT for short and used in the local news media). Principal transportation corridors radiate from the Minneapolis–St. Paul metropolitan area and Duluth. The major Interstate highways are Interstate 35 (I-35), I-90, and I-94, with I-35 and I-94 passing through the Minneapolis–St. Paul metropolitan area, and I-90 traveling east-west along the southern edge of the state. In 2006, a constitutional amendment was passed that required sales and use taxes on motor vehicles to fund transportation, with at least 40 percent dedicated to public transit. There are nearly two dozen rail corridors in Minnesota, most of which go through Minneapolis–St. Paul or Duluth. There is water transportation along the Mississippi River system and from the ports of Lake Superior.
Minnesota's principal airport is Minneapolis–St. Paul International Airport (MSP), a major passenger and freight hub for Delta Air Lines and Sun Country Airlines. Most other domestic carriers serve the airport. Large commercial jet service is provided at Duluth and Rochester, with scheduled commuter service to four smaller cities via Delta Connection carriers SkyWest Airlines, Compass Airlines, and Endeavor Air.
Amtrak's daily "Empire Builder" (Chicago–Seattle/Portland) train runs through Minnesota, calling at the Saint Paul Union Depot and five other stations. Intercity bus providers include Jefferson Lines, Greyhound, and Megabus. Local public transit is provided by bus networks in the larger cities and by two rail services. The Northstar Line commuter rail service runs from Big Lake to the Target Field station in downtown Minneapolis. From there, light rail runs to Saint Paul Union Depot on the Green Line, and to the MSP airport and the Mall of America via the Blue Line.
Law and government.
As with the federal government of the United States, power in Minnesota is divided into three branches: executive, legislative, and judicial.
Executive.
The executive branch is headed by the governor. Governor Mark Dayton, DFL (Democratic Farmer Labor), took office on January 3, 2011, to become the first DFL governor to hold the seat in two decades. The governor has a cabinet consisting of the leaders of various state government agencies, called commissioners. The other elected constitutional offices are secretary of state, attorney general, and state auditor.
Legislature.
The Minnesota Legislature is a bicameral body consisting of the Senate and the House of Representatives. The state has sixty-seven districts, each covering about sixty thousand people. Each district has one senator and two representatives (each district being divided into "A" and "B" sections). Senators serve for four years and representatives for two years. In the November 2010 election, the Minnesota Republican Party gained twenty-five house seats, giving them control of the House of Representatives by a 72-62 margin. The 2010 election also saw Minnesota voters elect a Republican majority in the Senate for the first time since 1972. In 2012, the Democrats regained the House of Representatives by a margin of 73-61, picking up 11 seats; the Democrats also regained the Minnesota Senate.
Judiciary.
Minnesota's court system has three levels. Most cases start in the district courts, which are courts of general jurisdiction. There are 279 district court judgeships in ten judicial districts. Appeals from the trial courts and challenges to certain governmental decisions are heard by the Minnesota Court of Appeals, consisting of nineteen judges who typically sit in three-judge panels. The seven-justice Minnesota Supreme Court hears all appeals from the tax court, the workers' compensation court of appeals, first-degree murder convictions, and discretionary appeals from the court of appeals; it also has original jurisdiction over election disputes.
Two specialized courts within administrative agencies have been established: the workers' compensation court of appeals, and the tax court, which deals with non-criminal tax cases.
Regional.
In addition to the city and county levels of government found in the United States, Minnesota has other entities that provide governmental oversight and planning. Some actions in the Twin Cities metropolitan area are coordinated by the Metropolitan Council, and many lakes and rivers are overseen by watershed districts and soil and water conservation districts.
Federal.
Minnesota's United States senators are Democrat Amy Klobuchar and Democrat Al Franken. The outcome of the 2008 U.S. Senate election in Minnesota was contested until June 30 the next year; when the Minnesota Supreme Court ruled in favor of Franken, Republican Norm Coleman conceded defeat, and the vacant seat was filled by Franken. The state has eight congressional districts; they are represented by Tim Walz (1st district; DFL), John Kline (2nd; R), Erik Paulsen (3rd; R), Betty McCollum (4th; DFL), Keith Ellison (5th; DFL), Tom Emmer (6th; R), Collin Peterson (7th; DFL), and Rick Nolan (8th; DFL).
Federal court cases are heard in the United States District Court for the District of Minnesota, which holds court in Minneapolis, St. Paul, Duluth, and Fergus Falls. Appeals are heard by the Eighth Circuit Court of Appeals, which is based in St. Louis, Missouri and routinely also hears cases in St. Paul.
Tribal.
The State of Minnesota was created by the US out of the homelands of the Dakota and Anishinaabe native peoples. Today the remaining native governments are divided into 11 semi-autonomous reservations that negotiate with the US and the state on a peer nation-to-nation basis:
Four Dakota Mdewakanton communities: 
Seven Anishinaabe reservations: 
The first six of the Anishinaabe bands compose the Minnesota Chippewa Tribe, the collective federally recognized tribal government of the Bois Forte, Fond du Lac, Grand Portage, Leech Lake, Mille Lacs, and White Earth reservations.
Politics.
Minnesota is known for a politically active citizenry, and populism has been a longstanding force among the state's political parties. Minnesota has a consistently high voter turnout (due in part to its liberal voter registration laws) with virtually no evidence of unlawful voting. In the 2008 U.S. presidential election, 78.2 percent of eligible Minnesotans voted—the highest percentage of any U.S. state—versus the national average of 61.2 percent. Previously unregistered voters can register on election day at their polls with evidence of residency.
Hubert Humphrey brought national attention to the state with his address at the 1948 Democratic National Convention. Minnesotans have consistently cast their Electoral College votes for Democratic presidential candidates since 1976, longer than any other state. Minnesota is the only state in the nation that did not vote for Ronald Reagan in either of his presidential runs. Minnesota has gone to the Democratic Party in every presidential election since 1960, with the exception of 1972, when it was carried by Richard Nixon and the Republican Party.
Both the Democratic and Republican parties have major party status in Minnesota, but its state-level "Democratic" party is actually a separate party, officially known as the Minnesota Democratic-Farmer-Labor Party (DFL). Formed out of a 1944 alliance of the Minnesota Democratic and Farmer-Labor parties, its distinction from the national Democratic Party, while still official, is now but a technicality.
The state has had active third party movements. The Reform Party, now the Independence Party, was able to elect former mayor of Brooklyn Park and professional wrestler Jesse Ventura to the governorship in 1998. The Independence Party has received enough support to keep major party status. The Green Party, while no longer having major party status, has a large presence in municipal government, notably in Minneapolis and Duluth, where it competes directly with the DFL party for local offices. Official "Major party" status in Minnesota (which grants state funding for elections) is reserved to parties whose candidates receive five percent or more of the vote in any statewide election (e.g., Governor, Secretary of State, U.S. President).
The state's U.S. Senate seats have generally been split since the early 1990s, and in the 108th and 109th Congresses, Minnesota's congressional delegation was split, with four representatives and one senator from each party. In the 2006 midterm election, Democrats were elected to all state offices except for governor and lieutenant governor, where Republicans Tim Pawlenty and Carol Molnau narrowly won reelection. The DFL also posted double-digit gains in both houses of the legislature, elected Amy Klobuchar to the U.S. Senate, and increased the party's U.S. House caucus by one. Keith Ellison (DFL) was elected as the first African American U.S. Representative from Minnesota as well as the first Muslim elected to Congress nationwide. In 2008 DFLer and former comedian and radio talk show host Al Franken beat incumbent Republican Norm Coleman in the United States Senate race by 312 votes out of 3 million cast.
In the election of 2010, Republicans took control of both chambers of the Minnesota legislature for the first time in 38 years, and with Mark Dayton's election the Democratic-Farmer-Labor party took the governor's office for the first time in 20 years. Two years later, the DFL regained control of both houses, and with Governor Dayton in office, the party has same-party control of both the legislative and executive branches for the first time since 1990. Two years later, the Republicans regained control of the Minnesota House in the 2014 election.
Media.
The Twin Cities area is the fifteenth largest media market in the United States as ranked by Nielsen Media Research. The state's other top markets are Fargo–Moorhead (118th nationally), Duluth–Superior (137th), Rochester–Mason City–Austin (152nd), and Mankato (200th).
Broadcast television in Minnesota and the Upper Midwest started on April 27, 1948, when KSTP-TV began broadcasting. Hubbard Broadcasting, which owns KSTP, is now the only locally owned television company in Minnesota. There are currently 39 analog broadcast stations and 23 digital channels broadcast over Minnesota.
The four largest daily newspapers are the "Star Tribune" in Minneapolis, the "Pioneer Press" in Saint Paul, the "Duluth News Tribune" in Duluth and the "Post-Bulletin" in Rochester. "The Minnesota Daily" is the largest student-run newspaper in the U.S. Sites offering daily news on the Web include "The UpTake", "MinnPost", the Twin Cities "Daily Planet", business news site "Finance and Commerce" and Washington D.C.-based "Minnesota Independent". Weeklies including "City Pages" and monthly publications such as "Minnesota Monthly" are available.
Two of the largest public radio networks, Minnesota Public Radio (MPR) and Public Radio International (PRI), are based in the state. MPR has the largest audience of any regional public radio network in the nation, broadcasting on 37 radio stations. PRI weekly provides more than 400 hours of programming to almost 800 affiliates. The state's oldest radio station, KUOM-AM, was launched in 1922 and is among the 10 –oldest radio stations in the United States. The University of Minnesota-owned station is still on the air, and since 1993 broadcasts a college rock format.
Sports, recreation and tourism.
Minnesota has a very active program of organized amateur and professional sports. Tourism has become an important industry, especially in the Lake region. In the North Country, what had been an industrial area focused on mining and timber has largely been transformed into a vacation destination. Popular interest in the environment and environmentalism, added to traditional interests in hunting and fishing, has attracted a large urban audience within driving range.
Organized sports.
Minnesota has professional men's teams in all major sports. The Hubert H. Humphrey Metrodome was home to the Minnesota Vikings of the National Football League through the 2013 season; it has been torn down and a new stadium is being constructed. The Dome also hosted the Minnesota Twins of Major League Baseball, winners of the 1987 and 1991 World Series, until 2010, when they began playing at Target Field. The Minnesota Timberwolves of the National Basketball Association play in the Target Center.
The National Hockey League's Minnesota Wild play in St. Paul's Xcel Energy Center and reached 300 consecutive sold-out games on January 16, 2008. Previously, the Minnesota North Stars competed in NHL from 1967 to 1993, which played the 1981 and 1991 Stanley Cup Finals.
Minnesota also has minor-league professional sports. NASL Minnesota United FC replaced the Minnesota Thunder in 2010 and plays at the National Sports Center in Blaine. They will eventually join Major League Soccer in 2017 or 2018.
The Minnesota Swarm play at the Xcel Energy Center and play in the NLL (National Lacrosse League). 
Minor league baseball is represented both by major league-sponsored teams and independent teams such as the St. Paul Saints.
Professional women's sports include the Minnesota Lynx of the Women's National Basketball Association, winners of the 2011, 2013, and 2015 WNBA Championships, the Minnesota Lightning of the United Soccer Leagues W-League, the Minnesota Vixen of the Independent Women's Football League, the Minnesota Valkyrie of the Legends Football League, and the Minnesota Whitecaps of the National Women's Hockey League.
The Twin Cities campus of the University of Minnesota is a National Collegiate Athletic Association (NCAA) Division I school competing in the Big Ten Conference. Four additional schools in the state compete in NCAA Division I ice hockey: the University of Minnesota Duluth; Minnesota State University, Mankato; St. Cloud State University and Bemidji State University. There are nine NCAA Division II colleges in the Northern Sun Intercollegiate Conference, and nineteen NCAA Division III colleges in the Minnesota Intercollegiate Athletic Conference and Upper Midwest Athletic Conference.
The Hazeltine National Golf Club has hosted the U.S. Open, U.S. Women's Open, U.S. Senior Open and PGA Championship. The course will also host the Ryder Cup in the fall of 2016, when it will become one of two courses in the U.S. to host all major golf competitions.
Interlachen Country Club has hosted the U.S. Open, U.S. Women's Open, and Solheim Cup.
Winter Olympic Games medallists from the state include twelve of the twenty members of the gold medal 1980 ice hockey team (coached by Minnesota native Herb Brooks) and the bronze medallist U.S. men's curling team in the 2006 Winter Olympics. Swimmer Tom Malchow won an Olympic gold medal in the 2000 Summer games and a silver medal in 1996.
Grandma's Marathon is run every summer along the scenic North Shore of Lake Superior, and the Twin Cities Marathon winds around lakes and the Mississippi River during the peak of the fall color season. Farther north, Eveleth is the location of the United States Hockey Hall of Fame.
Outdoor recreation.
Minnesotans participate in high levels of physical activity, and many of these activities are outdoors. The strong interest of Minnesotans in environmentalism has been attributed to the popularity of these pursuits.
In the warmer months, these activities often involve water. Weekend and longer trips to family cabins on Minnesota's numerous lakes are a way of life for many residents. Activities include water sports such as water skiing, which originated in the state, boating, canoeing, and fishing. More than 36 percent of Minnesotans fish, second only to Alaska.
Fishing does not cease when the lakes freeze; ice fishing has been around since the arrival of early Scandinavian immigrants. Minnesotans have learned to embrace their long, harsh winters in ice sports such as skating, hockey, curling, and broomball, and snow sports such as cross-country skiing, alpine skiing, snowshoeing, and snowmobiling. Minnesota is the only U.S. state where bandy is played.
State and national forests and the seventy-two state parks are used year-round for hunting, camping, and hiking. There are almost of snowmobile trails statewide. Minnesota has more miles of bike trails than any other state, and a growing network of hiking trails, including the Superior Hiking Trail in the northeast. Many hiking and bike trails are used for cross-country skiing during the winter.

</doc>
