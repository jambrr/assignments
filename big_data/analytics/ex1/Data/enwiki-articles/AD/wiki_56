<doc id="21791" url="https://en.wikipedia.org/wiki?curid=21791" title="Nanjing">
Nanjing

Nanjing (; , literally "Southern Capital") is a city situated in the heartland of the lower Yangtze River region in China, and which has long been a major centre of culture, education, research, politics, economy, transport networks and tourism. It is the capital city of Jiangsu province of People's Republic of China and the second largest city in East China, with a total population of 8,216,100, and legally the capital of Republic of China which lost the mainland during the civil war. The city has a prominent place in Chinese history and culture, having served as the capitals of various Chinese dynasties, kingdoms and republican governments dating from the 3rd century AD to 1949. Prior to the advent of pinyin romanization, Nanjing's city name was spelled as Nanking or Nankin. Nanjing has a number of other names, and some historical names are now used as names of districts of the city, and among them there is the name Jiangning (江寧), whose former character Jiang (江, River) is the former part of the name Jiangsu and latter character Ning (寧, simplified form 宁, Peace) is the short name of Nanjing. When being the capital of a state, for instance, ROC, Jing (京) is adopted as the abbreviation of Nanjing. Although as a city located in southern part of China becoming Chinese national capital as early as in Jin dynasty, the name Nanjing was designated to the city in Ming dynasty, about a thousand years later. Nanjing is particularly known as Jinling or Ginling (金陵, literally "Gold Mountain") and the old name has been used since the Warring States Period in Zhou Dynasty.
Located in Yangtze River Delta area and the center of East China, Nanjing is home to one of the world's largest inland ports. Nanjing is also one of the fifteen sub-provincial cities in the People's Republic of China's administrative structure, enjoying jurisdictional and economic autonomy only slightly less than that of a province. Nanjing has been ranked seventh in the evaluation of "Cities with Strongest Comprehensive Strength" issued by the National Statistics Bureau, and second in the evaluation of cities with most sustainable development potential in the Yangtze River Delta. It has also been awarded the title of 2008 Habitat Scroll of Honour of China, Special UN Habitat Scroll of Honour Award and National Civilized City. Nanjing boasts many high-quality universities and research institutes, with the number of universities listed in 100 National Key Universities ranking third, including Nanjing University. The ratio of college students to total population ranks No.1 among large cities nationwide. Nanjing is one of the three Chinese top research centres according to Nature Index.
Nanjing, one of the nation's most important cities for over a thousand years, is recognized as one of the Four Great Ancient Capitals of China, and had been the world's largest city aggregately for hundreds of years, enjoyed peace and prosperity and beared wars and disasters. Nanjing served as the capital of Eastern Wu, one of the three major states in the Three Kingdoms period (211-280); the Eastern Jin and each of the Southern Dynasties (Liu Song, Southern Qi, Liang and Chen), which successively ruled southern China from 317-589; the Southern Tang, one of the Ten Kingdoms (937-76); the Ming dynasty when, for the first time, all of China was ruled from the city (1368-1421); and the Republic of China (1927–37, 1945–49) prior to its flight to Taiwan during the Chinese Civil War. The city also served as the seat of the rebel Taiping Heavenly Kingdom (1851–64) and the Japanese puppet regime of Wang Jingwei (1940–45) during the Second Sino-Japanese War, and suffered appalling atrocities in both conflicts, including the Nanjing Massacre. It has been serving as the capital city of Jiangsu province after the China was established, and is still the nominal capital of Republic of China that accommodates many of its important heritage sites, including the Presidential Palace and Sun Yat-sen Mausoleum. Nanjing is famous for human historical landscapes, mountains and waters such as Fuzimiao, Ming Palace, Chaotian Palace, Porcelain Tower, Drum Tower, Stone City, City Wall, Qinhuai River, Xuanwu Lake and Purple Mountain. Key cultural facilities include Nanjing Library, Nanjing Museum and Art Museum.
Early history.
Archaeological discovery shows that "Nanjing Man" lived in more than 500 thousand years ago. Zun, a kind of wine vessel, was found to exist in Beiyinyangying culture of Nanjing in about 5000 years ago. According to legend, Fuchai, King of the State of Wu, founded a fort named Yecheng () in today's Nanjing area in 495 BC. Later in 473 BC, the State of Yue conquered Wu and constructed the fort of Yuecheng () on the outskirts of the present-day Zhonghua Gate. In 333 BC, after eliminating the State of Yue, the State of Chu built Jinling Yi () in the western part of present-day Nanjing. It was renamed Moling () during reign of Qin Shi Huang. Since then, the city experienced destruction and renewal many times. The area was successively part of Kuaiji, Zhang and Danyang prefectures in Qin and Han dynasty, and part of Yangzhou region which was established as the nation's 13 supervisory and administrative regions in the 5th year of Yuanfeng in Han dynasty (106 BC). Nanjing was later the capital city of Danyang Prefecture, and had been the capital city of Yangzhou for about 400 years from late Han to early Tang.
Capital of the Six Dynasties.
Nanjing first became a state capital in 229 AD, when the state of Eastern Wu founded by Sun Quan during the Three Kingdoms period relocated its capital to Jianye (), the city extended on the basis of Jinling Yi in 211 AD. Although conquered by the Western Jin dynasty in 280, Nanjing and its neighbouring areas had been well cultivated and developed into one of the commercial, cultural and political centers of China during the rule of East Wu. This city would soon play a vital role in the following centuries.
Shortly after the unification of the region, the Western Jin dynasty collapsed. First the rebellions by eight Jin princes for the throne and later rebellions and invasion from Xiongnu and other nomadic peoples that destroyed the rule of the Jin dynasty in the north. In 317, remnants of the Jin court, as well as nobles and wealthy families, fled from the north to the south and reestablished the Jin court in Nanjing, which was then called Jiankang (), replacing Luoyang. It's the first time that the capital of the nation moved to southern part.
During the period of North–South division, Nanjing remained the capital of the Southern dynasties for more than two and a half centuries. During this time, Nanjing was the international hub of East Asia. Based on historical documents, the city had 280,000 registered households. Assuming an average Nanjing household had about 5.1 people at that time, the city had more than 1.4 million residents.
A number of sculptural ensembles of that era, erected at the tombs of royals and other dignitaries, have survived (in various degrees of preservation) in Nanjing's northeastern and eastern suburbs, primarily in Qixia and Jiangning District. Possibly the best preserved of them is the ensemble of the Tomb of Xiao Xiu (475–518), a brother of Emperor Wu of Liang. The period of division ended when the Sui Dynasty reunified China and almost destroyed the entire city, turning it into a small town.
Sui dynasty to Yuan dynasty.
The city of Nanjing was razed after the Sui dynasty took over it. It renamed Shengzhou (昇州) in Tang dynasty and resuscitated during the late Tang. It was chosen as the capital and called Jinling () during the Southern Tang (937–976), a state that succeeded Wu state. It renamed Jiangning (江寧) in Northern Song dynasty and renamed Jiankang in Southern Song dynasty. Jiankang's textile industry burgeoned and thrived during the Song dynasty despite the constant threat of foreign invasions from the north by the Jurchen-led Jin dynasty. The court of Da Chu, a short-lived puppet state established by the Jurchens, and the court of Song were once in the city. Song was eventually exterminated by the Mongol empire under the name Yuan and in Yuan dynasty the city's status as a hub of the textile industry was further consolidated.
Capital of the early Ming dynasty.
The first emperor of the Ming dynasty, Zhu Yuanzhang (the Hongwu Emperor), who overthrew the Yuan dynasty, renamed the city Yingtian, rebuilt it, and made it the dynastic capital in 1368. He constructed a long city wall around Yingtian, as well as a new Ming Palace complex, and government halls. It took 200,000 laborers 21 years to finish the project. The present-day City Wall of Nanjing was mainly built during that time and today it remains in good condition and has been well preserved. It is among the longest surviving city walls in China. The Jianwen Emperor ruled from 1398 to 1402.
It is believed that Nanjing was the largest city in the world from 1358 to 1425 with a population of 487,000 in 1400. Nanjing remained the capital of the Ming Empire until 1421, when the third emperor of the Ming dynasty, the Yongle Emperor, relocated the capital to Beijing.
Besides the city wall, other famous Ming-era structures in the city included the famous Ming Xiaoling Mausoleum and Porcelain Tower, although the latter was destroyed by the Taipings in the 19th century either in order to prevent a hostile faction from using it to observe and shell the city or from superstitious fear of its geomantic properties.
A monument to the huge human cost of some of the gigantic construction projects of the early Ming dynasty is the Yangshan Quarry (located some east of the walled city and Ming Xiaoling mausoleum), where a gigantic stele, cut on the orders of the Yongle Emperor, lies abandoned, just as it was left 600 years ago when it was understood it was impossible to move or complete it.
As the center of the empire, early-Ming Nanjing had worldwide connections. It was home of the admiral Zheng He, who went to sail the Pacific and Indian Oceans, and it was visited by foreign dignitaries, such as a king from Borneo (Boni 渤泥), who died during his visit to China in 1408. The Tomb of the King of Boni, with a spirit way and "a "tortoise stele, was discovered in Yuhuatai District (south of the walled city) in 1958, and has been restored."
Southern Ming dynasty.
Over two centuries after the removal of the capital to Beijing, Nanjing was destined to become the capital of a Ming emperor one more time. After the fall of Beijing to Li Zicheng's rebel forces and then to the Manchu-led Qing dynasty in the spring of 1644, the Ming prince Zhu Yousong was enthroned in Nanjing in June 1644 as the Hongguang Emperor. His short reign was described by later historians as the first reign of the so-called Southern Ming dynasty.
Zhu Yousong, however, fared a lot worse than his ancestor Zhu Yuanzhang three centuries earlier. Beset by factional conflicts, his regime could not offer effective resistance to Qing forces, when the Qing army, led by the Manchu prince Dodo approached Jiangnan the next spring. Days after Yangzhou fell to the Manchus in late May 1645, the Hongguang Emperor fled Nanjing, and the imperial Ming Palace was looted by local residents. On June 6, Dodo's troops approached Nanjing, and the commander of the city's garrison, Zhao the Earl of Xincheng, promptly surrendered the city to them. The Manchus soon ordered all male residents of the city to shave their heads in the Manchu queue way. They requisitioned a large section of the city for the bannermen's cantonment, and destroyed the former imperial Ming Palace, but otherwise the city was spared the mass murders and destruction that befell Yangzhou.
Qing dynasty.
Under the Qing dynasty (1644–1911), the Nanjing area was known as Jiangning () and served as the seat of government for the Viceroy of Liangjiang. It had been visited by the Kangxi and Qianlong emperors a number of times on their tours of the southern provinces. Nanjing was invaded by British troops during the close of the First Opium War, which was ended by the Treaty of Nanjing in 1842. As the capital of the brief-lived rebel Taiping Heavenly Kingdom (founded by the Taiping rebels in the mid-19th century, Nanjing was known as Tianjing (, "Heavenly Capital" or "Capital of Heaven").
Both the Qing viceroy and the Taiping king resided in buildings that would later be known as the Presidential Palace. When Qing forces led by Zeng Guofan retook the city in 1864, a massive slaughter occurred in the city with over 100,000 estimated to have committed suicide or fought to the death. Since the Taiping Rebellion began, Qing forces allowed no rebels speaking its dialect to surrender. This policy of mass murder of civilians occurred in Nanjing.
Capital of the Republic of China.
The Xinhai Revolution led to the founding of the Republic of China in January 1912 with Sun Yat-sen as the first provisional president and Nanking was selected as its new capital. However, the Qing Empire controlled large regions to the north, so revolutionaries asked Yuan Shikai to replace Sun as president in exchange for the abdication of Puyi, the Last Emperor. Yuan demanded the capital be Beijing (closer to his power base).
In 1927, the Kuomintang (KMT; Nationalist Party) under Generalissimo Chiang Kai-shek again established Nanjing as the capital of the Republic of China, and this became internationally recognized once KMT forces took Beijing in 1928. The following decade is known as the Nanking decade.
In 1937, the Empire of Japan started a full-scale invasion of China after invading Manchuria in 1931, beginning the Second Sino-Japanese War (often considered a theatre of World War II). Their troops occupied Nanjing in December and carried out the systematic and brutal Nanking Massacre (the "Rape of Nanking"). Even children, the elderly, and nuns are reported to have suffered at the hands of the Imperial Japanese Army. The total death toll, including estimates made by the International Military Tribunal for the Far East and the Nanjing War Crimes Tribunal, was between 300,000 and 350,000. The city itself was also severely damaged during the massacre. The Nanjing Massacre Memorial Hall was built in 1985 to commemorate this event.
A few days before the fall of the city, the National Government of China was relocated to the southwestern city Chungking (Chongqing) and resumed Chinese resistance. In 1940, a Japanese-collaborationist government known as the "Nanjing Regime" or "Reorganized National Government of China" led by Wang Jingwei was established in Nanjing as a rival to Chiang Kai-shek's government in Chongqing. In 1946, after the Surrender of Japan, the KMT relocated its central government back to Nanjing.
People's Republic of China.
On 21 April, Communist forces crossed the Yangtze River. On April 23, 1949, the Communist People's Liberation Army (PLA) captured Nanjing. The KMT government retreated to Canton (Guangzhou) until October 15, Chongqing until November 25, and then Chengdu before retreating to Taiwan on December 10. By late 1949, the PLA was pursuing remnants of KMT forces southwards in southern China, and only Tibet was left. After the establishment of the People's Republic of China in October 1949, Nanjing was initially a province-level municipality, but it was soon merged into Jiangsu province and again became the provincial capital by replacing Zhenjiang which was transferred in 1928, and retains that status to this day.
Geography.
Nanjing, with a total land area of , is situated in the heartland of drainage area of lower reaches of Yangtze River, and in Yangtze River Delta, one of the largest economic zones of China. The Yangtze River flows past the west side and then north side of Nanjing City, while the Ningzheng Ridge surrounds the north, east and south side of the city. The city is west-northwest of Shanghai, south-southeast of Beijing, and east-northeast of Chongqing. The downstream Yangtze River flows from Jiujiang, Jiangxi, through Anhui and Jiangsu to East Sea, north to drainage basin of downstream Yangtze is Huai River basin and south to it is Zhe River basin, and they are connected by the Grand Canal east to Nanjing. The area around Nanjing is called Hsiajiang (下江, Downstream River) region, with Jianghuai (江淮) stressing northern part and Jiangzhe (江浙) stressing southern part. The region is also known as Dongnan (東南, South East, the Southeast) and Jiangnan (江南, River South, south of Yangtze).
Nanjing borders Yangzhou to the northeast, one town downstream when following the north bank of the Yangtze, Zhenjiang to the east, one town downstream when following the south bank of the Yangtze, and Changzhou to the southeast. On its western boundary is Anhui province, where Nanjing borders five prefecture-level cities, Chuzhou to the northwest, Wuhu, Chaohu and Maanshan to the west and Xuancheng to the southwest.
Nanjing is the intersection of Yangtze River, an east-west water transport artery, and Nanjing–Beijing railway, a south-north land transport artery, hence the name “door of the east and west, throat of the south and north”. Furthermore, the west part of the Ningzhen range is in Nanjing; the Loong-like Zhong Mountain is curling in the east of the city; the tiger-like Stone Mountain is crouching in the west of the city, hence the name “the Zhong Mountain, a dragon curling, and the Stone Mountain, a tiger crouching”. Mr. Sun Yet-sen spoke highly of Nanjing in the “Constructive Scheme for Our Country”, “The position of Nanjing is wonderful since mountains, lakes and plains all integrated in it. It is hard to find another city like this.”
Climate and environment.
Nanjing has a humid subtropical climate (Köppen "Cfa") and is under the influence of the East Asian monsoon. The four seasons are distinct, with damp conditions seen throughout the year, very hot and muggy summers, cold, damp winters, and in between, spring and autumn are of reasonable length. Along with Chongqing and Wuhan, Nanjing is traditionally referred to as one of the "Three Furnacelike Cities" along the Yangtze River () for the perennially high temperatures in the summertime. However, the time from mid-June to the end of July is the plum blossom blooming season in which the "meiyu" (rainy season of East Asia; literally "plum rain") occurs, during which the city experiences a period of mild rain as well as dampness. Typhoons are uncommon but possible in the late stages of summer and early part of autumn. The annual mean temperature is around , with the monthly 24-hour average temperature ranging from in January to in July. Extremes since 1951 have ranged from on 6 January 1955 to on 22 August 1959. On average precipitation falls 115 days out of the year, and the average annual rainfall is . With monthly percent possible sunshine ranging from 37 percent in March to 52 percent in August, the city receives 1,983 hours of bright sunshine annually.
Nanjing is endowed with rich natural resources, which include more than 40 kinds of minerals. Among them, iron and sulfur reserves make up 40 percent of those of Jiangsu province. Its reserves of strontium rank first in East Asia and the South East Asia region. Nanjing also possesses abundant water resources, both from the Yangtze River and groundwater. In addition, it has several natural hot springs such as Tangshan Hot Spring in Jiangning and Tangquan Hot Spring in Pukou.
Surrounded by the Yangtze River and mountains, Nanjing also enjoys beautiful natural scenery. Natural lakes such as Xuanwu Lake and Mochou Lake are located in the centre of the city and are easily accessible to the public, while hills like Purple Mountain are covered with evergreens and oaks and host various historical and cultural sites. Sun Quan relocated his capital to Nanjing after Liu Bei's suggestion as Liu Bei was impressed by Nanjing's impeccable geographic position when negotiating an alliance with Sun Quan. Sun Quan then renamed the city from Moling () to Jianye () shortly thereafter.
Environmental issues.
Air pollution in 2013.
A dense wave of smog began in the Central and Eastern part of China on 2 December 2013 across a distance of around , including Tianjin, Hebei, Shandong, Jiangsu, Anhui, Shanghai and Zhejiang. A lack of cold air flow, combined with slow-moving air masses carrying industrial emissions, collected airborne pollutants to form a thick layer of smog over the region. The heavy smog heavily polluted central and southern Jiangsu Province, especially in and around Nanjing, with its AQI pollution Index at "severely polluted" for five straight days and "heavily polluted" for nine. On 3 December 2013, levels of PM2.5 particulate matter average over 943 micrograms per cubic metre, falling to over 338 micrograms per cubic metre on 4 December 2013. Between 3:00 pm, 3 December and 2:00pm, 4 December local time, several expressways from Nanjing to other Jiangsu cities were closed, stranding dozens of passenger buses in Zhongyangmen bus station. From 5 to 6 December, Nanjing issued a red alert for air pollution and closed down all kindergarten through middle schools. Children's Hospital outpatient services increased by 33 percent; general incidence of bronchitis, pneumonia, upper respiratory tract infections significantly increased. The smog dissipated 12 December. Officials blamed the dense pollution on lack of wind, automobile exhaust emissions under low air pressure, and coal-powered district heating system in North China region. Prevailing winds blew low-hanging air masses of factory emissions (mostly SO2) towards China's east coast.
Government.
At present, the full name of the government of Nanjing is "People's Government of Nanjing City" and the city is under the one-party rule of the CPC, with the CPC Nanjing Committee Secretary as the "de facto" governor of the city and the mayor as the executive head of the government working under the secretary.
Administrative divisions.
The sub-provincial city of Nanjing is divided into 11 districts.
Demographics.
According to the "Sixth China Census", the total population of the City of Nanjing reached 8.005 million in 2010. The statistics in 2011 estimated the total population to be 8.11 million. The birth rate was 8.86 percent and the death rate was 6.88 percent. The urban area had a population of 6.47 million people. The sex ratio of the city population was 107.31 males to 100 females.
As in most of eastern China the ethnic makeup of Nanjing is predominantly Han nationality (98.56 percent), with 50 other minority nationalities. In 1999, 77,394 residents belonged to minority nationalities, among which the vast majority (64,832) were Hui nationalities, contributing 83.76 percent to the minority population. The second and third largest minority groups were Manchu (2,311) and Zhuang (533) nationalities. Most of the minority nationalities resided in Jianye District, comprising 9.13 percent of the district's population.
Economy.
Earlier development.
Since the Three Kingdoms period, Nanjing has been an industrial centre for textiles and minting owing to its strategic geographical location and convenient transportation. During the Ming dynasty, Nanjing's industry was further expanded, and the city became one of the most prosperous cities in China and the world. It led in textiles, minting, printing, shipbuilding and many other industries, and was the busiest business center in East Asia. Textiles boomed particularly in Qing dynasty, the industry created around 200 thousand jobs and there were about 50 thousand satin machines in the city in 18th and 19th century.
Modern times.
Into the first half of the twentieth century after the establishment of ROC, Nanjing gradually shifted from being a production hub towards being a heavy consumption city, mainly because of the rapid expansion of its wealthy population after Nanjing once again regained the political spotlight of China. A number of huge department stores such as Zhongyang Shangchang sprouted up, attracting merchants from all over China to sell their products in Nanjing. In 1933, the revenue generated by the food and entertainment industry in the city exceeded the sum of the output of the manufacturing and agriculture industry. One third of the city population worked in the service industry, .
In the 1950s after PRC was established by CPC, the government invested heavily in the city to build a series of state-owned heavy industries, as part of the national plan of rapid industrialization, converting it into a heavy industry production centre of East China. Overenthusiastic in building a “world-class” industrial city, the government also made many disastrous mistakes during development, such as spending hundreds of millions of yuan to mine for non-existent coal, resulting in negative economic growth in the late 1960s. From 1960s to 1980s there were Five Pillar Industries, namely, electronics, cars, petrochemical, iron and steel, and power, each with big state-owned firms. After the Reform and Opening recovering market economy, the state-owned enterprises found themselves incapable of competing with efficient multinational firms and local private firms, hence were either mired in heavy debt or forced into bankruptcy or privatization and this resulted in large numbers of layoff workers who were technically not unemployed but effectively jobless.
Today.
The current economy of the city is basically newly developed based on the past. Service industries are dominating, accounting for about 60 percent of the GDP of the city, and financial industry, culture industry and tourism industry are top 3 of them. Industries of information technology, energy saving and environmental protection, new energy, smart power grid and intelligent equipment manufacturing have become pillar industries. Big private firms include Suning, Yurun, Sanpower, Fuzhong, Hiteker, 5stars, Jinpu, Tiandi, CTTQ Pharmaceutical and Simcere Pharmaceutical. Big state-owned firms include Panda Electronics, Yangzi Petrochemical, Jinling Petrochemical, Nanjing Chemical, Nanjing Steel, Jincheng Motors, Jinling Pharmaceutical, Chenguang and NARI. The city has also attracted foreign investment, multinational firms such as Siemens, Ericsson, Volkswagen, Iveco, A.O. Smith, and Sharp have established their lines, and a number of multinationals such as Ford, IBM, Lucent, Samsung and SAP established research center there. Many China-based leading firms such as Huawei, ZTE and Lenovo have key R & D institutes in the city. Nanjing is an industrial technology research and development hub, hosting many R & D centers and institutions, especially in areas of electronics technology, information technology, computer software, biotechnology and pharmaceutical technology and new material technology.
In recent years, Nanjing has been developing its economy, commerce, industry, as well as city construction. In 2013 the city's GDP was RMB 801 billion (3rd in Jiangsu), and GDP per capita(current price) was RMB 98,174(US$16041), a 11 percent increase from 2012. The average urban resident's disposable income was RMB 36,200, while the average rural resident's net income was RMB 14,513. The registered urban unemployment rate was 3.02 percent, lower than the national average (4.3 percent). Nanjing's Gross Domestic Product ranked 12th in 2013 in China, and its overall competence ranked 6th in mainland and 8th including Taiwan and Hong Kong in 2009.
There are a number of industrial zones in Nanjing.
Transportation.
Nanjing is the transportation hub in eastern China and the downstream Yangtze River area. Different means of transportation constitute a three-dimensional transport system that includes land, water and air. As in most other Chinese cities, public transportation is the dominant mode of travel of the majority of the citizens. As of October 2014, Nanjing had five bridges and two tunnels over the Yangtze River, which are tying districts north of the river with the city centre on the south bank.
Rail.
Nanjing is an important railway hub in eastern China. It serves as rail junction for the Beijing-Shanghai (Jinghu) (which is itself composed of the old Jinpu and Huning Railways), Nanjing–Tongling Railway (Ningtong), Nanjing–Qidong (Ningqi), and the Nanjing-Xian (Ningxi) which encompasses the Hefei–Nanjing Railway.
Nanjing is connected to the national high-speed railway network by Beijing–Shanghai High-Speed Railway and Shanghai–Wuhan–Chengdu Passenger Dedicated Line, with several more high-speed rail lines under construction.
Among all 17 railway stations in Nanjing, passenger rail service is mainly provided by Nanjing Railway Station and Nanjing South Railway Station, while other stations like Nanjing West Railway Station, Zhonghuamen Railway Station and Xianlin Railway Station serve minor roles. Nanjing Railway Station was first built in 1968. In 1999, On November 12, 1999, the station was burnt in a serious fire. Reconstruction of the station was finished on September 1, 2005. Nanjing South Railway Station, which is one of the 5 hub stations on Beijing–Shanghai High-Speed Railway, has officially been claimed as the largest railway station in Asia and the second largest in the world in terms of GFA (Gross Floor Area). Construction of Nanjing South Station began on 10 January 2008. The station was opened for public service in 2011.
Road.
As an important regional hub in the Yangtze River Delta, Nanjing is well-connected by over 60 state and provincial highways to all parts of China.
Express highways such as Hu–Ning, Ning–He, Ning–Hang enable commuters to travel to Shanghai, Hefei, Hangzhou, and other important cities quickly and conveniently. Inside the city of Nanjing, there are of highways, with a highway coverage density of 3.38 kilometres per hundred square kilometrs (5.44 mi/100 sq mi). The total road coverage density of the city is 112.56 kilometres per hundred square kilometres (181.15 mi/100 sq mi). The two artery roads in Nanjing are Zhongshan Road and Hanzhong. The two roads cross in the city centre, Xinjiekou.
Expressways:
National Highway (GXXX):
Public transportation.
The city also boasts an efficient network of public transportation, which mainly consists of bus, taxi and metro systems. The bus network, which is currently run by three companies since 2011, provides more than 370 routes covering all parts of the city and suburban areas. Nanjing Metro Line 1, started service on September 3, 2005, with 16 stations and a length of 21.72 km. Line 2 and the 24.5 km-long south extension of Line 1 officially opened to passenger service on May 28, 2010. At present, Nanjing has a metro system with a grand total of 223.6 kilometers (138.9 mi) of route and 121 stations. They are Line 1, Line 2, Line 3, Line 10, Line S1 and Line S8. The city is planning to complete a 17-line Metro and light-rail system by 2030. The expansion of the Metro network will greatly facilitate the intracity transportation and reduce the currently heavy traffic congestion.
Air.
Nanjing's airport, Lukou International Airport, serves both national and international flights. In 2013, Nanjing airport handled 15,011,792 passengers and 255,788.6 tonnes of freight. The airport currently has 85 routes to national and international destinations, which include Japan, Korea, Thailand, Malaysia, Singapore, USA and Germany. The airport is connected by a 29-kilometre (18 mi) highway directly to the city center, and is also linked to various intercity highways, making it accessible to the passengers from the surrounding cities. A railway Ninggao Intercity Line is being built to link the airport with Nanjing South Railway Station. Lukou Airport was opened on 28 June 1997, replacing Nanjing Dajiaochang Airport as the main airport serving Nanjing. Dajiaochang Airport is still used as a military air base.
Water.
Port of Nanjing is the largest inland port in China, with annual cargo tonnage reached 191,970,000 t in 2012. The port area is in length and has 64 berths including 16 berths for ships with a tonnage of more than 10,000. Nanjing is also the biggest container port along the Yangtze River; in March 2004, the one million container-capacity base, Longtan Containers Port Area opened, further consolidating Nanjing as the leading port in the region. , it operated six public ports and three industrial ports.
Yangtze River crossings.
In the 1960s, the first Nanjing Yangtze River Bridge was completed, and served as the only bridge crossing over the Lower Yangtze in eastern China at that time. The bridge was a source of pride and an important symbol of modern China, having been built and designed by the Chinese themselves following failed surveys by other nations and the reliance on and then rejection of Soviet expertise. Begun in 1960 and opened to traffic in 1968, the bridge is a two-tiered road and rail design spanning 4,600 metres on the upper deck, with approximately 1,580 metres spanning the river itself. Since then four more bridges and two tunnels have been built. Going in the downstream direction, the Yangtze crossings in Nanjing are: Dashengguan Bridge, Line 10 Metro Tunnel, Third Bridge, Nanjing Yangtze River Tunnel, First Bridge, Second Bridge and Fourth Bridge.
Culture and art.
Being one of the four ancient capitals of China, Nanjing has always been a cultural centre attracting intellectuals from all over the country. In the Tang and Song dynasties, Nanjing was a place where poets gathered and composed poems reminiscent of its luxurious past; during the Ming and Qing dynasties, the city was the official imperial examination centre (Jiangnan Examination Hall) for the Jiangnan region, again acting as a hub where different thoughts and opinions converged and thrived.
Today, with a long cultural tradition and strong support from local educational institutions, Nanjing is commonly viewed as a “city of culture” and one of the more pleasant cities to live in China.
Art.
Some of the leading art groups of China are based in Nanjing; they include the Qianxian Dance Company, Nanjing Dance Company, Jiangsu Peking Opera Institute and Nanjing Xiaohonghua Art Company among others.
Jiangsu Province Kun Opera is one of the best theatres for Kunqu, China's oldest stage art.
It is considered a conservative and traditional troupe. Nanjing also has professional opera troupes for the Yang, Yue (shaoxing), Xi and Jing (Chinese opera varieties) as well as Suzhou pingtan, spoken theatre and puppet theatre.
Jiangsu Art Gallery is the largest gallery in Jiangsu Province, presenting some of the best traditional and contemporary art pieces of China; many other smaller-scale galleries, such as Red Chamber Art Garden and Jinling Stone Gallery, also have their own special exhibitions.
Festivals.
Many traditional festivals and customs were observed in the old times, which included climbing the City Wall on January 16, bathing in Qing Xi on March 3, hill hiking on September 9 and others (the dates are in Chinese lunar calendar). Almost none of them, however, are still celebrated by modern Nanjingese.
Instead, Nanjing, as a popular tourist destination, hosts a series of government-organised events throughout the year. The annual International Plum Blossom Festival held in Plum Blossom Hill, the largest plum collection in China, attracts thousands of tourists both domestically and internationally. Other events include Nanjing Baima Peach Blossom and Kite Festival, Jiangxin Zhou Fruit Festival and Linggu Temple Sweet Osmanthus Festival.
Libraries.
Nanjing Library, founded in 1907, houses more than 10 million volumes of printed materials and is the third largest library in China, after the National Library in Beijing and Shanghai Library. Other libraries, such as city-owned Jinling Library and various district libraries, also provide considerable amount of information to citizens. Nanjing University Library is the second largest university libraries in China after Peking University Library, and the fifth largest nationwide, especially in the number of precious collections.
Museums.
Nanjing has some of the oldest and finest museums in China. Nanjing Museum, formerly known as National Central Museum during ROC period, is the first modern museum and remains as one of the leading museums in China having 400,000 items in its permanent collection. The museum is notable for enormous collections of Ming and Qing imperial porcelain, which is among the largest in the world.
Other museums include the City Museum of Nanjing in the Chaotian Palace, the Oriental Metropolitan Museum, the China Modern History Museum in the Presidential Palace, the Nanjing Massacre Memorial Hall, the Taiping Kingdom History Museum, Jiangning Imperial Silk Manufacturing Museum, Nanjing Yunjin Museum, Nanjing City Wall Cultural Museum, Nanjing Customs Museum in Ganxi House, Nanjing Astronomical History Museum, Nanjing Paleontological Museum, Nanjing Geological Museum, Nanjing Riverstones Museum, and other museums and memorials such Zheng He Memorial, Jinling Four Modern Calligraphers Memorial.
Theatre.
Most of Nanjing's major theatres are multi-purpose, used as convention halls, cinemas, musical halls and theatres on different occasions. The major theatres include the People's Convention Hall and the Nanjing Arts and Culture Center. The Capital Theatre well known in the past is now a museum in theatre/film.
Night life.
Traditionally Nanjing's nightlife was mostly centered around Nanjing Fuzimiao (Confucius Temple) area along the Qinhuai River, where night markets, restaurants and pubs thrived. Boating at night in the river was a main attraction of the city. Thus, one can see the statues of the famous teachers and educators of the past not too far from those of the courtesans who educated the young men in the other arts.
In the past 20 years, several commercial streets have been developed, hence the nightlife has become more diverse: there are shopping malls opening late in the Xinjiekou CBD and Hunan Road. The well-established "Nanjing 1912" district hosts a wide variety of recreational facilities ranging from traditional restaurants and western pubs to dance clubs. There are two major areas where bars are densely located; one is in 1912 block; the other is along Shanghai road and its neighbourhood. Both are popular with international residents of the city.
These days, the most comprehensive source of nightlife information (in English) can be found on HelloNanjing.net and NanjingExpat.com.
Local people still very much enjoy street food, such as Turkish Kebab. As elsewhere in Asia, Karaoke is popular with both young and old crowds.
Food and symbolism.
Many of the city's local favorite dishes are based on ducks, including nanjing salted duck, duck blood and vermicelli soup, and duck oil pancake.
The radish is also a typical food representing people of Nanjing, which has been spread through word of mouth as an interesting fact for many years in China. According to Nanjing.GOV.cn, "There is a long history of growing radish in Nanjing especially the southern suburb. In the spring, the radish tastes very juicy and sweet. It is well-known that people in Nanjing like eating radish. And the people are even addressed as 'Nanjing big radish', which means they are unsophisticated, passionate and conservative. From health perspective, eating radish can help to offset the stodgy food that people take during the Spring Festival".
Sports and stadiums.
Nanjing's planned 20,000 seat Youth Olympic Sports Park Gymnasium will be one of the venues for the 2019 FIBA Basketball World Cup.
As a major Chinese city, Nanjing is home to many professional sports teams. Jiangsu Sainty, the football club currently staying in Chinese Super League, is a long-term tenant of Nanjing Olympic Sports Center. Jiangsu Nangang Basketball Club is a competitive team which has long been one of the major clubs fighting for the title in China top level league, CBA. Jiangsu Volleyball men and women teams are also traditionally considered as at top level in China volleyball league.
There are two major sports centers in Nanjing, Wutaishan Sports Center and Nanjing Olympic Sports Center. Both of these two are comprehensive sports centers, including stadium, gymnasium, natatorium, tennis court, etc. Wutaishan Sports Center was established in 1952 and it was one of the oldest and most advanced stadiums in early time of People's Republic of China.
Nanjing hosted the 10th National Games of P.R.C. in 2005 and hosted the 2nd summer Youth Olympic Games in 2014.
In 2005, in order to host The 10th National Game of People's Republic of China, there was a new stadium, Nanjing Olympic Sports Center, constructed in Nanjing. Compared to Wutaishan Sports Center, which the major stadium's capacity is 18,500, Nanjing Olympic Sports Center has a more advanced stadium which is big enough to seat 60,000 spectators. Its gymnasium has capacity of 13,000, and natatorium of capacity 3,000.
On 10 February 2010, the 122nd IOC session at Vancouver announced Nanjing as the host city for the 2nd Summer Youth Olympic Games. The slogan of the 2014 Youth Olympic Games was “Share the Games, Share our Dreams”. The Nanjing 2014 Youth Olympic Games featured all 28 sports on the Olympic programme and were held from 16 to 28 August. The Nanjing Youth Olympic Games Organising Committee (NYOGOC) worked together with the International Olympic Committee (IOC) to attract the best young athletes from around the world to compete at the highest level. Off the competition fields, an integrated culture and education programme focused on discussions about education, Olympic values, social challenges, and cultural diversity. The YOG aims to spread the Olympic spirit and encourage sports participation.
Tourism.
Nanjing is one of the most beautiful cities of mainland China with lush green parks, natural scenic lakes, small mountains, historical buildings and monuments, relics and much more, which attracts thousands of tourists every year.
Buildings and monuments.
Republic of China period.
Because it was designated as the national capital, many structures were built around that time. Even today, some of them still remain which are open to tourists.
Education.
Nanjing has been the educational centre in southern China for more than 1700 years. There are 75 institutions of higher learning till 2013. The number of National key laboratories, National key disciplines and the academicians of Chinese Academy of Sciences and Chinese Academy of Engineering all rank third in the nation. It boasts some of the most prominent educational institutions in the region, some of which are listed as follows:
National universities and colleges.
Operated by Ministry of Education
Operated by Ministry of Industry and Information Technology
Operated by the joint Commission of the State Forest Administration and Public Order Ministry
Operated by the general sport Administration

</doc>
<doc id="21793" url="https://en.wikipedia.org/wiki?curid=21793" title="Ninth Fort">
Ninth Fort

The Ninth Fort () is a stronghold in the northern part of Šilainiai elderate, Kaunas, Lithuania. It is a part of the Kaunas Fortress, which was constructed in the late 19th century. During the occupation of Kaunas and the rest of Lithuania by the Soviet Union, the fort was used as a prison and way-station for prisoners being transported to labour camps. After the occupation of Lithuania by Nazi Germany, the fort was used as a place of execution for Jews, captured Soviets, and others.
History.
At the end of the 19th century the city of Kaunas was fortified and by 1890 was encircled by eight forts and nine gun batteries. Construction of the Ninth Fort (its numerical designation having become its name) began in 1902 and was completed on the eve of World War I. From 1924 on, the Ninth Fort was used as the Kaunas City prison.
During the years of Soviet occupation, 1940–1941, the Ninth Fort was used by the NKVD to house political prisoners pending transfer to Gulag forced labor camps.
During the years of Nazi occupation, the Ninth Fort was put to use as a place of mass murder. At least 10,000 Jews, most from Kaunas and largely taken from the Kovno Ghetto, were transported to the Ninth Fort and killed by Nazis with the collaboration of some Lithuanians in what became known as the Kaunas massacre.
Notable among the victims was Rabbi Elchonon Wasserman of Baranovitch. In addition, Jews from as far as France, Austria and Germany were brought to Kaunas during the course of Nazi occupation and executed in the Ninth Fort. In 1943, the Germans operated special Jewish squads to dig mass graves and burn the remaining corpses. One squad of 62 people managed to escape the fortress on the eve of 1944. That year, as the Soviets moved in, the Germans liquidated the ghetto and what had by then come to be known as the "Fort of Death". The prisoners were dispersed to other camps. After World War II, the Soviets again used the Ninth Fort as a prison for several years. From 1948 to 1958, farm organizations were managed from the Ninth Fort.
In 1958, a museum was established in the Ninth Fort. In 1959, an exhibition was prepared in four cells, telling of the Nazi war crimes carried out in Lithuania. In 1960, the discovery, cataloguing, and forensic investigation of local mass murder sites began in an effort to gain knowledge regarding the scope of these crimes.
Museum.
The Ninth Fort museum contains collections of historical artifacts related both to Soviet atrocities and the Nazi genocide, as well as materials related to the earlier history of Kaunas and Ninth Fort. Most exhibits are labelled in English.
The memorial to the victims of Nazism at the Ninth Fort in Kaunas, Lithuania, was designed by sculptor A. Ambraziunas. Erected in 1984, the monument is 105 feet (32 m) high. The mass burial place of the victims of the massacres carried out in the fort is a grass field, marked by a simple yet frankly worded memorial written in several languages. It reads, "This is the place where Nazis and their assistants killed more than 30,000 Jews from Lithuania and other European countries."
On April 11, 2011, the memorial to the victims of Nazism was vandalized — the memorial tombstones were knocked down, and white swastikas were spray-painted on the memorial. On the adjacent sidewalk, the words “Juden raus” (German: Jews Out) were inscribed.

</doc>
<doc id="21794" url="https://en.wikipedia.org/wiki?curid=21794" title="Nostratic languages">
Nostratic languages

Nostratic is a macrofamily, or hypothetical large-scale language family, that includes many of the indigenous language families of Eurasia, although its exact composition and structure vary among proponents. In its more restricted, current form, it includes the Indo-European, Uralic, Altaic and Kartvelian languages. Often also included are the Afroasiatic languages native to North Africa, the Horn of Africa, the Arabian Peninsula and the Near East, and the Dravidian languages of the Indian Subcontinent (sometimes also Elamo-Dravidian, which connects India and the Iranian Plateau).
The hypothetical ancestral language of the Nostratic family is called Proto-Nostratic. Proto-Nostratic would have been spoken between 15,000 and 12,000 BCE, in the Epipaleolithic period, close to the end of the last glacial period.
The Nostratic hypothesis originates with Holger Pedersen in the early 20th century. The name "Nostratic" is due to Pedersen (1903), derived from the Latin "nostrates" "fellow countrymen". The hypothesis was significantly expanded in the 1960s by Soviet linguists, notably Vladislav Illich-Svitych and Aharon Dolgopolsky, termed the "Moscovite school" by Bomhard (2008, 2011, and 2014), and it has received renewed attention in English-speaking academia since the 1990s.
The hypothesis is controversial and has varying degrees of acceptance amongst linguists worldwide. In Russia, it is endorsed by a minority of linguists, such as Vladimir Dybo, but is not a generally accepted hypothesis. Allan Bomhard is a supporter, Lyle Campbell a critic. Some linguists take an agnostic view. Eurasiatic, a similar but not identical grouping, was proposed by Joseph Greenberg (2000) and endorsed by Merritt Ruhlen: it is taken as a subfamily of Nostratic by Bomhard (2008).
History of research.
Origin of the Nostratic hypothesis.
The last quarter of the 19th century saw various linguists putting forward proposals linking the Indo-European languages to other language families, such as Finno-Ugric and Altaic.
These proposals were taken much further in 1903 when Holger Pedersen proposed "Nostratic", a common ancestor for the Indo-European, Finno-Ugric, Samoyed, Turkish, Mongolian, Manchu, Yukaghir, Eskimo, Semitic, and Hamitic languages, with the door left open to the eventual inclusion of others.
The name "Nostratic" derives from the Latin word "nostrās", meaning 'our fellow-countryman' (plural: "nostrates") and has been defined, since Pedersen, as consisting of those language families that are related to Indo-European. Merritt Ruhlen notes that this definition is not properly taxonomic but amorphous, since there are broader and narrower degrees of relatedness, and moreover, some linguists who broadly accept the concept (such as Greenberg and Ruhlen himself) have criticised the name as reflecting the ethnocentrism frequent among Europeans at the time. Martin Bernal has described the term as distasteful because it implies that speakers of other language families are excluded from academic discussion. Even so, the concept arguably transcends ethnocentric associations. (Indeed, Pedersen's older contemporary Henry Sweet attributed some of the resistance by Indo-European specialists to hypotheses of wider genetic relationships as "prejudice against dethroning [Indo-European] from its proud isolation and affiliating it to the languages of yellow races".) Proposed alternative names such as "Mitian", formed from the characteristic Nostratic first- and second-person pronouns "mi" 'I' and "ti" 'you' (exactly 'thou'), have not attained the same currency.
An early supporter was the French linguist Albert Cuny—better known for his role in the development of the laryngeal theory—who published his "Recherches sur le vocalisme, le consonantisme et la formation des racines en « nostratique », ancêtre de l'indo-européen et du chamito-sémitique" ('Researches on the Vocalism, Consonantism, and Formation of Roots in "Nostratic", Ancestor of Indo-European and Hamito-Semitic') in 1943. Although Cuny enjoyed a high reputation as a linguist, the work was coldly received.
Muscovite school.
While Pedersen's Nostratic hypothesis did not make much headway in the West, it became quite popular in what was then the Soviet Union. Working independently at first, Vladislav Illich-Svitych and Aharon Dolgopolsky elaborated the first version of the contemporary form of the hypothesis during the 1960s. They expanded it to include additional language families. Illich-Svitych also prepared the first dictionary of the hypothetical language.
A principal source for the items in Illich-Svitych’s dictionary was the earlier work of Alfredo Trombetti (1866–1929), an Italian linguist who had developed a classification scheme for all the world’s languages, widely reviled at the time and subsequently ignored by almost all linguists. In Trombetti’s time, a widely held view on classifying languages was that similarity in inflections is the surest proof of genetic relationship. In the interim, the view had taken hold that the comparative method—previously used as a means of studying languages already known to be related and without any thought of classification—is the most effective means to establish genetic relationship, eventually hardening into the conviction that it is the only legitimate means to do so. This view was basic to the outlook of the new Nostraticists. Although Illich-Svitych adopted many of Trombetti’s etymologies, he sought to validate them by a systematic comparison of the sound systems of the languages concerned.
21st century.
The chief events in Nostratic studies in 2008 were the posting online of the latest version of Dolgopolsky's "Nostratic Dictionary" and the publication of Allan Bomhard's comprehensive treatment of the subject, "Reconstructing Proto-Nostratic", in 2 volumes. 2008 also saw the opening of a website, "Nostratica", devoted to providing important texts in Nostratic studies online, which is now offline. Also significant was Bomhard's partly critical review of Dolgopolsky's dictionary, in which he argued that only those Nostratic etymologies that are strongest should be included, in contrast to Dolgopolsky's more expansive approach, which includes many etymologies that are possible but not secure.
In early 2014, Allan Bomhard published his latest monograph on Nostratic, "A Comprehensive Introduction to Nostratic Comparative Linguistics".
Constituent language families.
The language families proposed for inclusion in Nostratic vary, but all Nostraticists agree on a common core of language families, with differences of opinion appearing over the inclusion of additional families.
The three groups universally accepted among Nostraticists are Indo-European, Uralic, and Altaic; the validity of the Altaic family, while itself controversial, is taken for granted by Nostraticists. Nearly all also include the Dravidian and Kartvelian language families.
Following Pedersen, Illich-Svitych, and Dolgopolsky, most advocates of the theory have included Afroasiatic, though criticisms by Joseph Greenberg and others from the late 1980s onward suggested a reassessment of this position.
A fairly representative grouping, arranged in rough geographical order (and probable order of phylogenetic branching, following Starostin), would include:
The Sumerian and Etruscan languages, usually regarded as language isolates, are thought by some to be Nostratic languages as well. Others, however, consider one or both to be members of another macrofamily called Dené–Caucasian. Another notional isolate, the Elamite language, also figures in a number of Nostratic classifications. It is frequently grouped with Dravidian as Elamo-Dravidian.
In 1987 Joseph Greenberg proposed a similar macrofamily which he called Eurasiatic. It included the same "Euraltaic" core (Indo-European, Uralic, and Altaic), but excluded some of the above-listed families, most notably Afroasiatic. At about this time Russian Nostraticists, notably Sergei Starostin, constructed a revised version of Nostratic which was slightly broader than Greenberg's grouping but which similarly left out Afroasiatic.
Recently, a consensus has been emerging among proponents of the Nostratic hypothesis. Greenberg basically agreed with the Nostratic concept, though he stressed a deep internal division between its northern 'tier' (his Eurasiatic) and a southern 'tier' (principally Afroasiatic and Dravidian). The American Nostraticist Allan Bomhard considers Eurasiatic a branch of Nostratic alongside other branches: Afroasiatic, Elamo-Dravidian, and Kartvelian. Similarly, Georgiy Starostin (2002) arrives at a tripartite overall grouping: he considers Afroasiatic, Nostratic and Elamite to be roughly equidistant and more closely related to each other than to anything else. Sergei Starostin's school has now re-included Afroasiatic in a broadly defined Nostratic, while reserving the term Eurasiatic to designate the narrower subgrouping which comprises the rest of the macrofamily. Recent proposals thus differ mainly on the precise placement of Dravidian and Kartvelian.
According to Greenberg, Eurasiatic and Amerind form a genetic node, being more closely related to each other than either is to "the other families of the Old World". There are a number of hypotheses incorporating Nostratic into an even broader linguistic 'mega-phylum', sometimes called Borean, which would also include at least the Dené–Caucasian and perhaps the Amerind and Austric superfamilies. The term SCAN has been used for a group that would include Sino-Caucasian, Amerind, and Nostratic.
Urheimat and differentiation.
Allan Bomhard and Colin Renfrew are in broad agreement with the earlier conclusions of Illich-Svitych and Dolgopolsky in seeking the Nostratic Urheimat (original homeland) within the Mesolithic (or Epipaleolithic) in the Fertile Crescent, the stage which directly preceded the Neolithic and was transitional to it.
Looking at the cultural assemblages of this period, two sequences in particular stand out as possible archeological correlates of the earliest Nostratians or their immediate precursors. Both hypotheses place Proto-Nostratic within the Fertile Crescent at around the end of the last glacial period.
It has been proposed that the broad spectrum revolution of Kent Flannery (1969), associated with microliths, the use of the bow and arrow, and the domestication of the dog, all of which are associated with these cultures, may have been the cultural "motor" that led to their expansion.
Certainly cultures which appeared at Franchthi Cave in the Aegean and Lepenski Vir in the Balkans, and the Murzak-Koba (9100–8000 BCE) and Grebenki (8500–7000 BCE) cultures of the Ukrainian steppe, all displayed these adaptations.
Bomhard (2008) suggests a differentiation of Proto-Nostratic by 8,000 BCE, the beginning of the Neolithic Revolution in the Levant, over a territory spanning the entire Fertile Crescent and beyond into the Caucasus (Proto-Kartvelian), Egypt and along the Red Sea to the Horn of Africa (Proto-Afroasiatic), the Iranian Plateau (Proto-Elamo-Dravidian) and into Central Asia (Proto-Eurasiatic, to be further subdivided by 5,000 BCE into Proto-Indo-European, Proto-Uralic and Proto-Altaic).
According to scholarly opinion the Kebaran is derived from the Levantine Upper Palaeolithic in which the microlithic component originated.
Ouchtata retouch is also a characteristic of the Late Ahmarian Upper Palaeolithic culture of the Levant and does not indicate African influence.
Varga Csaba, a Hungarian linguist believes Hungary to be the Urheimat of Nostratic.
Reconstruction of Proto-Nostratic.
The following data is taken from Kaiser and Shevoroshkin (1988) and Bengtson (1998) and transcribed into the .
Phonology.
The phonemes tabulated below are commonly reconstructed for the Proto-Nostratic language (Kaiser and Shevoroshkin 1988). Allan Bomhard (2008), who relies more heavily on Afroasiatic and Dravidian than on Uralic, as do members of the "Moscow School", reconstructs a different vowel system, with three pairs of vowels represented as: , as well as independent /i/, /o/, and /u/. In the first three pairs of vowels, Bomhard is attempting to specify the subphonemic variation involved, inasmuch as that variation led to some of the vowel gradation (ablaut) and vowel harmony patterning found in various daughter languages.
Consonants.
The reconstructed consonants of Nostratic are shown in the table below. Every distinction is supposed to be contrastive by the Nostraticists who reconstruct them.
Sound correspondences.
The following table is compiled from data given by Kaiser and Shevoroshkin (1988) and Starostin. They follow Illich-Svitych's correspondences in which Nostratic voiceless stops give (traditional) PIE voiced ones, and Nostratic glottalized stops give (traditional) PIE voiceless stops, in contradiction with the PIE glottalic theory, which makes traditional PIE voiced stops appeared like glottalized ones. To correct this anomaly, linguists such as Manaster Ramer and Bomhard have proposed to correlate Nostratic voiceless and glottalized stops with PIE ones, so this is done in the table.
Because linguists working on Proto-Indo-European, Proto-Uralic, and Proto-Dravidian do not usually use the IPA, the transcriptions used in those fields are also given where the letters differ from the IPA symbols. The IPA symbols are between slashes because this is a phonemic transcription. The exact values of the phoneme "
Note that, due to lack of research, there are at present several different mutually incompatible reconstructions of Proto-Afroasiatic (see [http://www.tufs.ac.jp/ts/personal/ratcliffe/comp%20&%20method-Ratcliffe.pdf] for two recent ones). The one used here has been said to be based too strongly on Proto-Semitic (Yakubovich 1998).
Similarly, the paper by Kaiser and Shevoroshkin is much older than the newest "Altaic Etymological Dictionary" (2003; see Altaic languages article) and therefore assumes a somewhat different phonological system for Proto-Altaic. 
Morphology.
Because grammar is less easily borrowed than words, grammar is usually considered stronger evidence for language relationships than vocabulary. The following correspondences (slightly modified to account for the reconstruction of Proto-Altaic by Starostin et al. ) have been suggested by Kaiser and Shevoroshkin (1988). /N/ could be any nasal consonant. /V/ could be any vowel. (The above cautionary notes on Afroasiatic and Dravidian apply.)
In addition, Kaiser and Shevoroshkin write the following about Proto-Nostratic grammar (two asterisks are used for reconstructions based on reconstructions; citation format changed):
The verb stood at the end of the sentence (SV and SOV type). The 1st pwas formed by adding the 1st ps. pronoun **mi to the verb; similarly, the 2nd ps. was formed by adding **ti. There were no endings for the 3rd ps. present ["or at least none can be reconstructed", while the 3rd ps. preterit ending was **-di (Illich-Svitych 1971, pp. 218–19). Verbs could be active and passive, causative, desiderative, and reflective; and there were special markers for most of these categories. Nouns could be animate or inanimate, and plural markers differed for each category. There were subject and object markers, locative and lative enclitic particles, etc. Pronouns distinguished direct and oblique forms, animate and inanimate categories, notions of the type 'near':'far', inclusive:exclusive […], etc. Apparently there were no prefixes. Nostratic words were either equal to roots or built by adding endings or suffixes. There are some cases of word composition...
Lexicon.
According to Dolgopolsky Proto-Nostratic language had analytic structure, which he argues by diverging of post- and prepositions of auxiliary words in descendant languages.
Dolgopolsky states three lexical categories to be in Proto-Nostratic language:
Word order was subject–object–verb when the subject was a noun, and object–verb–subject when it was a pronoun. Attributive (expressed by a lexical word) preceded its head. Pronominal attributive ('my', 'this') might follow the noun. Auxiliary words are considered to be postpositions.
Core vocabulary.
The list of etymologies of lexical words reconstructed by Dolgopolsky that are considered by Bomhard to be strong is as follows:
Personal pronouns.
Personal pronouns are seldom borrowed between languages. Therefore the many correspondences between Nostratic pronouns are rather strong evidence for the existence of a Proto-Nostratic language. The difficulty of finding Afroasiatic cognates is, however, taken by some as evidence that Nostratic has two or three branches, Afroasiatic and Eurasiatic (and possibly Dravidian), and that most or all of the pronouns in the following table can only be traced to Proto-Eurasiatic.
Nivkh is a living (if moribund) language with an orthography, which is given here. /V/ means that it is not clear which vowel should be reconstructed.
For space reasons, Etruscan is not included, but the fact that it had /mi/ 'I' and /mini/ 'me' seems to fit the pattern reconstructed for Proto-Nostratic ideally, leading some to argue that the Aegean or Tyrsenian languages were yet another Nostratic branch.
There is no reconstruction of Proto-Eskimo–Aleut, although the existence of the Eskimo–Aleut family is generally accepted.
Other words.
Below are selected reconstructed etymologies from Kaiser and Shevoroshkin (1988) and Bengtson (1998). Reconstructed ( = unattested) forms are marked with an asterisk. /V/ means that it is not clear which vowel should be reconstructed. Likewise, /E/ could have been any front vowel and /N/ any nasal consonant. Only the consonants are given of Proto-Afroasiatic roots (see above).
Sample text.
Vladislav Illich-Svitych using his version of Proto-Nostratic composed a brief poem. (Compare Schleicher's fable for similar attempts with several different reconstructions of Proto-Indo-European.)
The value of K̥ or is uncertain—it could be or . H could similarly be at least or . V or is an uncertain vowel.
Status within comparative linguistics.
While the Nostratic hypothesis is not endorsed by the mainstream of comparative linguistics, Nostratic studies by nature of being based on the comparative method remain within the mainstream of contemporary linguistics from a methodological point of view; it is the scope with which the comparative method is applied rather than the methodology itself that raises eyebrows.
Nostraticists tend to refuse to include in their schema language families for which no proto-language has yet been reconstructed. This approach was criticized by Joseph Greenberg on the ground that genetic classification is necessarily prior to linguistic reconstruction but this criticism has so far had no effect on Nostraticist theory and practice.
Certain critiques have pointed out that the data from individual, established language families that is cited in Nostratic comparisons often involves a high degree of errors; Campbell (1998) demonstrates this for Uralic data. Defenders of the Nostratic theory argue that were this to be true, it would remain that in classifying languages genetically, positives count for vastly more than negatives (Ruhlen 1994). The reason for this is that, above a certain threshold, resemblances in sound/meaning correspondences are highly improbable mathematically.
The technique of comparing grammatical structures (as opposed to words) has suggested to some that the Nostratic candidates lack interrelatedness. However, Pedersen's original Nostratic proposal synthesized earlier macrofamilies, some of which, including Indo-Uralic, involved extensive comparison of inflections. It is true the Russian Nostraticists and Bomhard initially emphasized lexical comparisons. Bomhard recognized the necessity to explore morphological comparisons and has since published extensive work in this area (see especially Bomhard 2008:1.273–386). According to him the breakthrough came with the publication of the first volume of Joseph Greenberg's Eurasiatic work, which provided a massive list of possible morphemic correspondences that has proved fruitful to explore. Other important contributions on Nostratic morphology have been published by John C. Kerns and Vladimir Dybo.
Critics argue that were one to collect all the words from the various known Indo-European languages and dialects which have at least one of any 4 meanings, one could easily form a list that would cover any conceivable combination of two consonants and a vowel (of which there are only about 20*20*5=2000). Nostraticists respond that they do not compare isolated lexical items but reconstructed proto-languages. To include a word for a proto-language it must be found in a number of languages and the forms must be relatable by regular sound changes. In addition, many languages have restrictions on root structure, reducing the number of possible root-forms far below its mathematical maximum. These languages include, among others, Indo-European, Uralic, and Altaic—all the core languages of the Nostratic hypothesis. To understand how the root structures of one language relate to those of another has long been a focus of Nostratic studies. For a highly critical assessment of the work of the Moscow School, especially the work of Illich-Svitych, cf. Campbell and Poser 2008:243-264.
It has also been argued that Nostratic comparisons mistake Wanderwörter and cross-borrowings between branches for true cognates.

</doc>
<doc id="21796" url="https://en.wikipedia.org/wiki?curid=21796" title="Namespace">
Namespace

In computing, a namespace is a set of symbols that are used to organize objects of various kinds, so that these objects may be referred to by name. Prominent examples include:
Namespaces are commonly structured as hierarchies to allow reuse of names in different contexts. As an analogy, consider a system of naming of people where each person has a proper name, as well as a family name shared with their relatives. If, in each family, the names of family members are unique, then each person can be uniquely identified by the combination of first name and family name; there is only one Jane Doe, though there may be many Janes. Within the namespace of the Doe family, just "Jane" suffices to unambiguously designate this person, while within the "global" namespace of all people, the full name must be used.
In a similar way, hierarchical file systems organize files in directories. Each directory is a separate namespace, so that the directories "letters" and "invoices" may both contain a file "to_jane".
In computer programming, namespaces are typically employed for the purpose of grouping symbols and identifiers around a particular functionality and to avoid name collisions between multiple identifiers that share the same name.
In networking, the Domain Name System organizes websites (and other resources) into namespaces. For example, "org" is a namespace for non-profit organizations, "wikipedia.org" is a sub-namespace assigned to the Wikimedia Foundation, and "en.wikipedia.org" is the name of the English-language Wikipedia within this space.
Name conflicts.
Element names are defined by the developer. This often results in a conflict when trying to mix XML documents from different XML applications.
This XML carries HTML table information:
This XML carries information about a table (i.e. a piece of furniture):
If these XML fragments were added together, there would be a name conflict. Both contain a <table> element, but the elements have different content and meaning.
An XML parser will not know how to handle these differences.
Solution via prefix.
Name conflicts in XML can easily be avoided using a name prefix.
The following XML distinguishes between information about the HTML table and furniture by prefixing "h" and "f" at the beginning xml/xml_namespaces.asp
Naming system.
A name in a namespace consists of a namespace identifier and a local name. The namespace name is usually applied as a prefix to the local name.
In Augmented Backus-Naur Form:
When local names are used by themselves, name resolution is used to decide which (if any) particular item is alluded to by some particular local name.
Delegation.
Delegation of responsibilities between parties is important in real-world applications, such as the structure of the World Wide Web. Namespaces allow delegation of identifier assignment to multiple name issuing organisations whilst retaining global uniqueness. A central Registration authority registers the assigned namespace identifiers allocated. Each namespace identifier is allocated to an organisation which is subsequently responsible for the assignment of names in their allocated namespace. This organisation may be a name issuing organisation that assign the names themselves, or another Registration authority which further delegates parts of their namespace to different organisations.
Hierarchy.
A naming scheme that allows subdelegation of namespaces to third parties is a hierarchical namespace
A hierarchy is recursive if the syntax for the namespace identifiers is the same for each subdelegation. An example of a recursive hierarchy is the Domain name system.
An example of a non-recursive hierarchy are Uniform resource name representing an Internet Assigned Numbers Authority (IANA) number.
Namespace versus scope.
A namespace identifier may provide context ("Scope" in computer science) to a name, and the terms are sometimes used interchangeably. However, the context of a name may also be provided by other factors, such as the location where it occurs or the syntax of the name.
In programming languages.
For many programming languages, namespace is a context for their identifiers. In an operating system, an example of namespace is a directory. Each name in a directory uniquely identifies one file or subdirectory, but one file may have the same name multiple times.
As a rule, names in a namespace cannot have more than one meaning; that is, different meanings cannot share the same name in the same namespace. A namespace is also called a context, because the same name in different namespaces can have different meanings, each one appropriate for its namespace.
Following are other characteristics of namespaces:
As well as its abstract language technical usage as described above, some languages have a specific keyword used for explicit namespace control, amongst other uses. Below is an example of a namespace in C++:
Computer science considerations.
A namespace in computer science (sometimes also called a name scope), is an abstract container or environment created to hold a logical grouping of unique identifiers or symbols (i.e. names). An identifier defined in a namespace is associated only with that namespace. The same identifier can be independently defined in multiple namespaces. That is, an identifier defined in one namespace may or may not have the same meaning as the same identifier defined in another namespace. Languages that support namespaces specify the rules that determine to which namespace an identifier (not its definition) belongs.
This concept can be illustrated with an analogy. Imagine that two companies, X and Y, each assign ID numbers to their employees. X should not have two employees with the same ID number, and likewise for Y; but it is not a problem for the same ID number to be used at both companies. For example, if Bill works for company X and Jane works for company Y, then it is not a problem for each of them to be employee #123. In this analogy, the ID number is the identifier, and the company serves as the namespace. It does not cause problems for the same identifier to identify a different person in each namespace.
In large computer programs or documents it is not uncommon to have hundreds or thousands of identifiers. Namespaces (or a similar technique, see Emulating namespaces) provide a mechanism for hiding local identifiers. They provide a means of grouping logically related identifiers into corresponding namespaces, thereby making the system more modular.
Data storage devices and many modern programming languages support namespaces. Storage devices use directories (or folders) as namespaces. This allows two files with the same name to be stored on the device so long as they are stored in different directories. In some programming languages (e.g. C++, Python), the identifiers naming namespaces are themselves associated with an enclosing namespace. Thus, in these languages namespaces can nest, forming a namespace tree. At the root of this tree is the unnamed global namespace.
Use in common languages.
In C++, a namespace is defined with a namespace block.
Within this block, identifiers can be used exactly as they are declared. Outside of this block, the namespace specifier must be prefixed. For example, outside of codice_1, codice_2 must be written codice_3 to be accessed. C++ includes another construct that makes this verbosity unnecessary. By adding the line
to a piece of code, the prefix codice_4 is no longer needed.
Code that is not explicitly declared within a namespace is considered to be in the global namespace.
Namespace resolution in C++ is hierarchical. This means that within the hypothetical namespace codice_5, the identifier codice_6 refers to codice_7. If codice_7 doesn't exist, it then refers to codice_9. If neither codice_7 nor codice_9 exist, codice_6 refers to codice_13, an identifier in the global namespace.
Namespaces in C++ are most often used to avoid naming collisions. Although namespaces are used extensively in recent C++ code, most older code does not use this facility because it did not exist in early versions of the language. For example, the entire C++ standard library is defined within codice_14, but before standardization many components were originally in the global namespace. A programmer can insert the codice_15 directive to bypass namespace resolution requirements and obtain backwards compatibility with older code that expects all identifiers to be in the global namespace. However, use of the codice_15 directive for reasons other than backwards compatibility (e.g., convenience), it is considered to be against good code practices.
In Java, the idea of a namespace is embodied in Java packages. All code belongs to a package, although that package need not be explicitly named. Code from other packages is accessed by prefixing the package name before the appropriate identifier, for example codice_17 in codice_18 can be referred to as codice_19 (this is known as the fully qualified class name). Like C++, Java offers a construct that makes it unnecessary to type the package name (codice_20). However, certain features (such as reflection) require the programmer to use the fully qualified name.
Unlike C++, namespaces in Java are not hierarchical as far as the syntax of the language is concerned. However, packages are named in a hierarchical manner. For example, all packages beginning with codice_21 are a part of the Java platform—the package contains classes core to the language, and contains core classes specifically relating to reflection.
In Java (and Ada, C#, and others), namespaces/packages express semantic categories of code. For example, in C#, codice_22 contains code provided by the system (the .NET Framework). How specific these categories are and how deep the hierarchies go differ from language to language.
Function and class scopes can be viewed as implicit namespaces that are inextricably linked with visibility, accessibility, and object lifetime.
Namespaces are heavily used in C# language. All .NET Framework classes are organized in namespaces, to be used more clearly and to avoid chaos. Furthermore, custom namespaces are extensively used by programmers, both to organize their work and to avoid naming collisions.
When referencing a class, one should specify either its fully qualified name, which means namespace followed by the class name,
or add a using statement. This, eliminates the need to mention the complete name of all classes in that namespace.
In the above examples, System is a namespace, and Console and Convert are classes defined within System.
In Python, namespaces are defined by the individual modules, and since modules can be contained in hierarchical packages, then name spaces are hierarchical too.
In general when a module is imported then the names defined in the module are defined via that module's name space, and are accessed in from the calling modules by using the fully qualified name.
The "from ... import ..." can be used to insert the relevant names directly into the calling module's namespace, and those names can be accessed from the calling module without the qualified name :
Since this directly imports names (without qualification) it can overwrite existing names with no warnings.
A special form is "from ... import *", which imports all names defined in the named package directly in the calling modules namespace. Use of this form of import, although supported within the language, is generally discouraged as it pollutes the namespace of the calling module and will cause already defined names to be overwritten in the case of name clashes.
Python also supports "import x as y" as a way of providing an alias or alternative name for use by the calling module:
In XML, the XML namespace specification enables the names of elements and attributes in an XML document to be unique, similar to the role of namespaces in programming languages. Using XML namespaces, XML documents may contain element or attribute names from more than one XML vocabulary.
Namespaces were introduced into PHP from version 5.3 onwards. Naming collision of classes, functions and variables can be avoided.
In PHP, a namespace is defined with a namespace block.
We can reference a PHP namespace with the following different ways:
Emulating namespaces.
In programming languages lacking language support for namespaces, namespaces can be emulated to some extent by using an identifier naming convention. For example, C libraries such as Libpng often use a fixed prefix for all functions and variables that are part of their exposed interface. Libpng exposes identifiers such as:
This naming convention provides reasonable assurance that the identifiers are unique and can therefore be used in larger programs without fear of identifier naming collisions. Likewise, many packages originally written in Fortran (e.g., BLAS, LAPACK) reserve the first few letters of a function's name to indicate which group it belongs to.
Unfortunately, this technique has several drawbacks:

</doc>
<doc id="21797" url="https://en.wikipedia.org/wiki?curid=21797" title="Nahum">
Nahum

Nahum ( or ; ) was a minor prophet whose prophecy is recorded in the Hebrew Bible. His book comes in chronological order between Micah and Habakkuk in the Bible. He wrote about the end of the Assyrian Empire, and its capital city, Nineveh, in a vivid poetic style.
Life.
Little is known about Nahum's personal history. His name means "comforter," and he was from the town of Alqosh, (Nah 1:1) which scholars have attempted to identify with several cities, including the modern Alqosh of Assyria and Capharnaum of northern Galilee. He was a very nationalistic Hebrew, however, and lived amongst the Elkoshites in peace. Nahum, called "the Elkoshite," is the seventh in order of the minor prophets.
Works.
Nahum's writings could be taken as prophecy or as history. One account suggests that his writings are a prophecy written in about 615 BC, just before the downfall of Assyria, while another account suggests that he wrote this passage as liturgy just after its downfall in 612 BC.
The book was introduced in Calvin's Commentary as a complete and finished poem:
Nahum, taking words from Moses himself, has shown in a general way what sort of "Being God is". The Reformation theologian Calvin argued, Nahum painted God by which his nature must be seen, and "it is from that most memorable vision, when God appeared to Moses after the breaking of the tables."
Tomb.
The tomb of Nahum is supposedly inside the synagogue at Alqosh, although there are other places outside Iraq that lay claim also to being the original "Elkosh" from which Nahum hailed. Alquosh was abandoned by its Jewish population in 1948, when they were expelled, and the synagogue that purportedly houses the tomb is in a poor structural state, to the extent that the tomb itself is in danger of destruction. The tomb underwent basic repairs in 1796. When all Jews were compelled to flee Alqosh in 1948, the iron keys to the tomb were handed to an Assyrian man by the name of Sami Jajouhana. Few Jews visit the historic site, yet Jajouhana continues to keep the promise he made with his Jewish friends, and looks after the tomb. A team of US/UK construction engineers, led by Huw Thomas, is currently planning ways to save the building and the tomb. Money had been allocated for proposed renovation in 2008. The tomb is currently in disrepair and may be threatened by the rise of ISIS in Iraq.
There are however two other possible burial sites mentioned in historical accounts: Elkesi, near Ramah in the Galilee and Elcesei in the West Bank.
Liturgical commemoration.
The Prophet Nahum is venerated as a saint in Eastern Christianity. On the Eastern Orthodox liturgical calendar, his feast day is December 1 (for those churches which follow the traditional Julian Calendar, December 1 currently falls on December 14 of the modern Gregorian Calendar). He is commemorated with the other minor prophets in the Calendar of saints of the Armenian Apostolic Church on July 31.

</doc>
<doc id="21798" url="https://en.wikipedia.org/wiki?curid=21798" title="November 17">
November 17


</doc>
<doc id="21803" url="https://en.wikipedia.org/wiki?curid=21803" title="Newfoundland English">
Newfoundland English

Newfoundland English is a name for several accents and dialects of English found in the province of Newfoundland and Labrador. Most of these differ substantially from the English commonly spoken elsewhere in neighbouring Canada and the North Atlantic. Many Newfoundland dialects are similar to the West Country dialects of the West Country in England, particularly the city of Bristol and counties Cornwall, Devon, Dorset, Hampshire and Somerset, while others resemble dialects of Ireland's southeast, particularly Waterford, Wexford, Kilkenny and Cork. Still others blend elements of both and there is also a Scottish influence on the dialects - while the Scottish came in smaller numbers than the English and Irish, they had a large influence on Newfoundland society. One estimate claims 80 to 85 percent of Newfoundland's English heritage came from the southwest of the country.
The dialects that comprise Newfoundland English developed because of Newfoundland's history as well as its geography. Newfoundland was one of the first areas settled by England in North America, beginning in small numbers in the early 17th century before peaking in the early 19th century. Newfoundland was a British colony until 1907 when it became an independent Dominion within the British Empire. It became a part of Canada in 1949. Newfoundland is an island in the Atlantic Ocean, separated by the Strait of Belle Isle from Labrador, the sparsely populated mainland part of the province. Most of the population remained rather isolated on the island, allowing the dialects time to develop independently of those on the North American continent. Today, some words from Newfoundland English have been adopted through popular culture in other places in Canada (especially Ontario and eastward).
Historically, Newfoundland English was first recognized as a separate dialect by the late 18th century when George Cartwright published a glossary of Newfoundland words.
Other names for Newfoundland English.
Newfoundland English is often humorously called "Newfinese". The term "Newfie" is also sometimes used though it is sometimes considered pejorative when used by people from outside of Newfoundland.
Other languages and dialects that have influenced Newfoundland English.
There is also a dialect of French centred mainly on the Port au Port Peninsula on the west coast of the island which has had an impact on the syntax of English in the area. One example of these constructs found in Newfoundland is "Throw grandpa down the stairs his hat", a dative construction in which the hat makes the trip, not the grandfather. Another is the use of French reflexive constructions in sentences such as the reply to a question like "Where are you going?", reply: "Me I'm goin' downtown" (this reflexive form of grammar also exists in Irish Gaelic and Jerriais).
Newfoundland French was deliberately discouraged by the Newfoundland government through the public schools during the mid-20th-century, and only a small handful of mainly elderly people are still fluent in the French-Newfoundland dialect. In the last couple of decades, many parents in the region have demanded and obtained French education for their children, but this would be Standard French education and does not represent a continuation of the old dialect per se. Some people living in the Codroy Valley on the south-west tip of the island are also ancestrally Francophone, but represent Acadian settlers from the Maritime Provinces of Canada who arrived during the 19th century. This population has also lost the French language.
The greatest distinction between Newfoundland English and General Canadian English is its vocabulary. It includes some Inuit and First Nations words (for example "tabanask", a kind of sled), preserved archaic English words no longer found in other English dialects (for example "pook", a mound of hay), Irish language survivals like "sleveen" and "angishore", compound words created from English words to describe things unique to Newfoundland (for example "stun breeze", a wind of at least 20 knots (37 km/h)), English words which have undergone a semantic shift (for example "rind", the bark of a tree), and unique words whose origins are unknown (for example "diddies", a nightmare).
Newfoundland English expressions.
In recent years, the most commonly noted Newfoundland English expression might be "Whadd'ya at?" ("What are you at?"), loosely translated to "How's it going?" or "What are you doing?" Coming in a close second might be "You're stunned as me arse, b'y," inferring incredible stupidity or foolishness to the person being spoken to.
Other local expressions include:
Also of note is the widespread use of the term "b'y" as a common form of address. It is shorthand for "boy", (and is a turn of phrase particularly pronounced with the Waterford dialect of Hiberno-Irish) but is used variably to address members of either sex. Another term of endearment, often spoken by older generations, is "me ducky", used when addressing a female in an informal manner, and usually placed at the end of a sentence which is often a question (Example: "How's she goin', me ducky?") -- a phrase also found in East Midlands British English. Also pervasive as a sentence ending is "right" used in the same manner as the Canadian "eh" or the American "huh" or "y'know". Even if the sentence would otherwise be a non-question, the pronunciation of "right" can sometimes make it seem like affirmation is being requested.
Certain words have also gained prominence amongst the speakers of Newfoundland English. For instance, a large body of water that may be referred to as a "lake" elsewhere, can often (but not uniformly) be referred to as a pond. In addition, a large landmass that rises high out of the ground, regardless of elevation, is referred to unwaveringly as a "hill". Yet there is a difference between a hill and a big hill.
Another major characteristic of some variants of Newfoundland English is adding the letter 'h' to words that begin with vowel sounds, or removing 'h' from words that begin with it. In some districts, the term house commonly is referred to as the "ouse," for example, while "even" might be said "h'even." The idiom "'E drops 'is h in 'Olyrood and picks en up in H'Avondale." is often used to describe this using the eastern towns Holyrood and Avondale as examples. There are many different variations of the Newfoundland dialect depending on geographical location within the province. It is also important to note that Labrador has a very distinct culture and dialect within its region.
Other.
Although it is referred to as "Newfoundland English" or "Newfinese", Newfoundland is not the only place which uses this dialect. The southern coast of Labrador (the nearest point of Labrador to Newfoundland) and an area near the Labrador border, the Basse-Côte-Nord of Quebec, also use this form of speaking. Younger generations of this area have adapted the way of speaking, and created some of their own expressions. Some older generations speak Newfoundland English, but it is more commonly used by the younger generations. "B'y" is one of the most common terms used in this area.
It is also common to hear Newfoundland English in Yellowknife, Southern Alberta and Fort McMurray, Alberta, places to which many Newfoundlanders have moved or commute regularly for employment.
Newfoundland English is also used frequently in the city of Cambridge ON. This is due the high Newfoundland (mostly from Bell Island) population. There are even counties in the Southern Appalachian Mountains of Tennessee (of Scottish-Irish-English descendants), and also in Kentucky who have similar dialects, and also in both Prince Edward Island and Nova Scotia as well, but these dialects are dying out fast.
Similarities to Australian English.
Some of the features of Newfoundland English here can be or were also found in Australian English, especially among speakers of the "Broad Australian" variant and in rural areas.
Such features can be seen in older popular literature, such as C. J. Dennis's "The Sentimental Bloke" and Henry Lawson's writings.
These include forms that have their origins in Irish-English and Irish-Gaelic..

</doc>
<doc id="21804" url="https://en.wikipedia.org/wiki?curid=21804" title="National flag">
National flag

A national flag is a flag that symbolises a country. The flag is flown by the government, but usually can also be flown by citizens of the country.
Both public and private buildings such as schools and courthouses may fly the national flag. In some countries, the national flags are only flown from non-military buildings on certain flag days.
History.
Historically, flags originate as military standards, used as field signs. The practice of flying flags indicating the country of origin outside of the context of warfare became common with the maritime flag, introduced during the age of sail, in the early 17th century. The Danish flags however, which is the eldest, dates back to 1208/1219 and was first depicted as national banner in 1397. The origins of the Union Jack flag date back to 1603, when James VI of Scotland inherited the English and Irish thrones (as James I), thereby uniting the crowns of England, Scotland and Ireland in a personal union (which remained separate states). On 12 April 1606, a new flag to represent this regal union between England and Scotland was specified in a royal decree, according to which the flag of England (a red cross on a white background, known as St George's Cross), and the flag of Scotland (a white saltire on a blue background, known as the Saltire or St Andrew's Cross), would be joined together, forming the flag of Great Britain and first Union Flag.
With the emergence of nationalist sentiment from the late 18th century the desire was felt to display national flags also in civilian contexts, notably the US flag, in origin adopted as a naval ensign in 1777, which after the American Revolution began to be displayed as a generic symbol of the United States, and the French Tricolore which became a symbol of the Republic in the 1790s.
Most countries of Europe adopted a national flag in the course of the 19th and early 20th centuries, often based on older (medieval) war flags. The specifications of the flag of Denmark were codified in 1748, based on a 14th-century design. The flag of Switzerland was introduced in 1889, also based on medieval war flags. The Netherlands introduced two national flags in 1813 (either an orange-white-blue or a red-white-blue tricolour; the final decision in favour of red was made in 1937).
The Ottoman flag (now the flag of Turkey) was adopted in 1844. Other non-European powers followed the trend in the late 19th century, the flag of Japan being introduced in 1870, that of Qing China in 1890. 
Also in the 19th century, most countries of South America introduced a flag as they became independent (Peru in 1820, Bolivia in 1851, Colombia in 1860, Brazil in 1822, etc.)
Process of adoption.
The national flag is often, but not always, mentioned or described in a country's constitution, but its detailed description may be delegated to a flag law passed by the legislative, or even secondary legislation or in monarchies a decree.
Thus, the national flag is mentioned briefly in the Basic Law for the Federal Republic of Germany of 1949 "the federal flag is black-red-gold" (art. 22.2 "Die Bundesflagge ist schwarz-rot-gold"), but its proportions were regulated in a document passed by the government in the following year. The Flag of the United States is not defined in the constitution but rather in a separate Flag Resolution passed in 1777.
Minor design changes of national flags are often passed on a legislative or executive level, while substantial changes have constitutional character. The design of the flag of Serbia omitting the communist star of the flag of Yugoslavia was a decision made in the 1992 Serbian constitutional referendum, but the adoption of a coat of arms within the flag was based on a government "recommendation" in 2003, adopted legislatively in 2009 and again subject ot a minor design change in 2010. The Flag of the United States underwent numerous changes because the number of stars represents the number of states, proactively defined in a Flag Act of 1818 to the effect that "on the admission of every new state into the Union, one star be added to the union of the flag"; it was changed for the last time in 1960 with the accession of Hawaii.
A change in national flag is often due to a change of regime, especially following a civil war or revolution. In such cases, the military origins of the national flag and its connection to political ideology (form of government, monarchy vs. republic vs. theocracy, etc.) remains visible. In such cases national flags acquire the status of a political symbols.
The flag of Germany, for instance, was a tricolour of black-white-red under the German Empire, inherited from the North German Confederation (1866). The Weimar Republic that followed adopted a black-red-gold tricolour. Nazi Germany went back to black-white-red in 1933, and black-red-gold was reinstituted by the two successor states, West Germany and East Germany following World War II. Similarly the flag of Libya introduced with the creation of the Kingdom of Libya in 1951 was abandoned in 1969 with the coup d'état led by Muammar Gaddafi. It was used again by National Transitional Council and by anti-Gaddafi forces during the Libyan Civil War in 2011 and officially adopted by the Libyan interim Constitutional Declaration.
Usage.
There are three distinct types of national flag for use on land, and three for use at sea, though many countries use identical designs for several (and sometimes all) of these types of flag.
On land.
On land, there is a distinction between civil flags (FIAV symbol ), state flags (), and war or military flags (). State flags are those used officially by government agencies, whereas civil flags may be flown by anyone regardless of whether they are linked to government. War flags (also called military flags) are used by military organisations such as Armies, Marine Corps, or Air Forces.
In practice, many countries (such as the United States and the United Kingdom) have identical flags for these three purposes; national flag is sometimes used as a vexillological term to refer to such a three-purpose flag (). In a number of countries, however, and notably those in Latin America, there is a distinct difference between civil and state flags. In most cases, the civil flag is a simplified version of the state flag, with the difference often being the presence of a coat of arms on the state flag that is absent from the civil flag.
Very few countries use a war flag that differs from the state flag. The People's Republic of China, the Republic of China (Taiwan), and Japan are notable examples of this. Swallow-tailed flags are used as war flags and naval ensigns in Nordic countries and charged versions as presidential or royal standards. The Philippines does not have a distinctive war flag in this usual sense, but the flag of the Philippines is legally unique in that it is flown with the red stripe on top when the country is in a state of war, rather than the conventional blue.
At sea.
The flag that indicates nationality on a ship is called an ensign. As with the national flags, there are three varieties: the civil ensign (), flown by private vessels; state ensigns (also called government ensigns; ), flown by government ships; and war ensigns (also called naval ensigns; ), flown by naval vessels. The ensign is flown from an ensign-staff at the stern of the ship, or from a gaff when underway. Both these positions are superior to any other on the ship, even though the masthead is higher. In the absence of a gaff the ensign may be flown from the yardarm. (See Maritime flags.) National flags may also be flown by aircraft and the land vehicles of important officials. In the case of aircraft, those flags are usually painted on, and those are usually to be painted on in the position as if they were blowing in the wind.
In some countries, such as the United States and Canada (except for the Royal Canadian Navy's Ensign), the national ensign is identical to the national flag, while in others, such as the United Kingdom and Japan, there are specific ensigns for maritime use. Most countries do not have a separate state ensign, although the United Kingdom is a rare exception, in having a red ensign for civil use, a white ensign as its naval ensign, and a blue ensign for government non-military vessels.
Protocol.
There is a great deal of protocol involved in the proper display of national flags. A general rule is that the national flag should be flown in the position of honour, and not in an inferior position to any other flag (although some countries make an exception for royal standards). The following rules are typical of the conventions when flags are flown on land:
Hanging a flag vertically.
Most flags are hung vertically by rotating the flag pole. However, some countries have specific protocols for this purpose or even have special flags for vertical hanging; usually rotating some elements of the flag — such as the coat of arms — so that they are seen in an upright position.
Examples of countries that have special protocol for vertical hanging are: Canada, Czech Republic, Greece, Israel, the Philippines, Saudi Arabia, South Africa, and the United States (reverse always showing); and the United Kingdom (obverse always showing).
Examples of countries that have special designs for vertical hanging are: Austria, Germany, Hungary, Mexico, Poland, Montenegro, and Slovakia (coat of arms must be rotated to normal position); Cambodia (coat of arms must be rotated and blue strips are narrowed); Dominica (coat of arms must be rotated and reverse always showing); Liechtenstein (crown must be rotated).
Design.
The art and practice of designing flags is known as vexillography. The design of national flags has seen a number of customs become apparent.
All national flags are rectangular, except for the flag of Nepal. The ratios of height to width vary among national flags, but none is taller than it is wide, again except for the flag of Nepal. The flags of Switzerland and the Vatican City are the only national flags which are exact squares.
The obverse and reverse of all national flags are either identical or mirrored, except for the flag of Paraguay and the partially recognized Sahrawi Arab Democratic Republic. See Flags whose reverse differs from the obverse for a list of exceptions including non-national flags.
As of 2011, all national flags consist of at least two different colours. In many cases, the different colours are presented in either horizontal or vertical bands. It is particularly common for colours to be presented in bands of three.
It is common for many flags to feature national symbols, such as coats of arms. National patterns are present in some flags. Variations in design within a national flag can be common in the flag's upper left quarter, or canton.
Similarities.
Although the national flag is meant to be a unique symbol for a country, many pairs of countries have highly similar flags. Examples include the flags of Monaco and Indonesia, which differ only slightly in proportion; the flags of the Netherlands and Luxembourg, which differ in proportion as well as in the tint of blue used; and the flags of Romania and Chad, which differ only in the tint of blue.
The flags of Ireland and Côte d'Ivoire and the flags of Mali and Guinea are (aside from shade or ratio differences) vertically mirrored versions from each other. This means that the reverse of one flag matches the obverse of the other. Other than horizontal mirrored flags (like Poland and Indonesia) the direction in which these flags fly are crucial to identify them.
There are three colour combinations that are used on several flags in certain regions. Blue, white, and red is a common combination in Slavic countries such as the Czech Republic, Slovakia, Russia, Serbia, Slovenia, and Croatia as well as among Western nations including Australia, France, Iceland, Norway, New Zealand, the United Kingdom, the Netherlands and the United States of America. Many African nations use the Pan-African colours of red, yellow, and green, including Ghana, Cameroon, Mali and Senegal. Flags containing red, white, and black (a subset of the Pan-Arab colours) can be found particularly among the Arab nations such as Egypt, Iraq and Yemen.
While some similarities are coincidental, others are rooted in shared histories. For example, the flags of Colombia, of Ecuador, and of Venezuela all use variants of the flag of Gran Colombia, the country they composed upon their independence from Spain, created by the Venezuelan independence hero Francisco de Miranda; and the flags of Kuwait, of Jordan, and of Palestine are all highly similar variants of the flag of the Arab revolt of 1916–1918. The flags of Romania and Moldova are virtually the same, because of the common history and heritage. Moldova adopted the Romanian flag during the declaration of independence from the USSR in 1991 (and was used in various demonstrations and revolts by the population) and later the Moldovan coat of arms (which is part of the Romanian coat of arms) was placed in the centre of the flag. The Nordic countries all use the Nordic Cross design (Iceland, Denmark, Norway, Sweden, Finland, in addition to the autonomous regions of the Faroe Islands and Åland), a horizontal cross shifted to the left on a single-coloured background. The United States and United Kingdom both have red, white, and blue. This similarity is due to the fact that the first 13 states of the U.S. were former colonies of the United Kingdom. Also, Australia and New Zealand share a very similar flag, which stems from their joint British heritage. Both of these flags feature the Union Jack in one corner, both have royal blue background, and both have the Southern Cross as a prominent feature. The only differences between these flags is that the Australian flag has the Commonwealth Star below the canton, and that on the New Zealand flag, just four stars in the Southern Cross are presented, and they are five-pointed red stars with white borders. On the other hand, all five stars of the Southern Cross are presented on the Australian flag, and they are white with seven points, except for the additional smaller fifth star in the Southern Cross which has only five points on this flag. Some similarities to the United States flag with the red and white stripes are noted as well such as the flag of Malaysia and the flag of Liberia, the latter of which was an American resettlement colony.
Many other similarities may be found among current national flags, particularly if inversions of colour schemes are considered, e.g., compare the flag of Senegal to that of Cameroon and Indonesia to Poland. Also the Flag of Italy and the Flag of Hungary uses the same colours, in the same order, only the direction differs (the Italian flag is vertical and the Hungarian flag is horizontal)

</doc>
<doc id="21805" url="https://en.wikipedia.org/wiki?curid=21805" title="November 4">
November 4


</doc>
<doc id="21806" url="https://en.wikipedia.org/wiki?curid=21806" title="November 23">
November 23


</doc>
<doc id="21809" url="https://en.wikipedia.org/wiki?curid=21809" title="National Hockey League">
National Hockey League

The National Hockey League (NHL; ) is a professional ice hockey league composed of 30 member clubs: 23 in the United States and 7 in Canada. Headquartered in New York City, the NHL is considered to be the premier professional ice hockey league in the world, and one of the major professional sports leagues in the United States and Canada. The Stanley Cup, the oldest professional sports trophy in North America, is awarded annually to the league playoff champion at the end of each season.
The National Hockey League was organized on November 26, 1917, in Montreal, Quebec, after the suspension of operations of its predecessor organization, the National Hockey Association (NHA), which had been founded in 1909 in Renfrew, Ontario. The NHL immediately took the NHA's place as one of the leagues that contested for the Stanley Cup in an annual interleague competition before a series of league mergers and folds left the NHL as the only league left competing for the Stanley Cup in 1926. At its inception, the NHL had four teams—all in Canada, thus the adjective "National" in the league's name. The league expanded to the United States in 1924, when the Boston Bruins joined, and has since consisted of American and Canadian teams. After a labour-management dispute that led to the cancellation of the entire 2004–05 season, the league resumed play under a new collective agreement that included a salary cap. In 2009, the NHL enjoyed record highs in terms of sponsorships, attendance, and television audiences.
The league draws many highly skilled players from all over the world and currently has players from approximately 20 different countries. Canadians have historically constituted the majority of the players in the league, with an increasing percentage of American and European players in recent seasons.
History.
Early years.
The National Hockey League was established in 1917 as the successor to the National Hockey Association (NHA). Founded in 1909, the NHA began play one year later with seven teams in Ontario and Quebec, and was one of the first major leagues in professional ice hockey. But by the NHA's eighth season, a series of disputes with Toronto Blueshirts owner Eddie Livingstone led the other team owners, representing the Montreal Canadiens, Montreal Wanderers, Ottawa Senators, and Quebec Bulldogs to meet about the league's future. Realizing the NHA constitution left them unable to force Livingstone out, the four teams voted instead to suspend the NHA, and on November 26, 1917, formed the National Hockey League. Frank Calder was chosen as its first president, serving until his death in 1943.
The Bulldogs were unable to play, and the remaining owners created a new team in Toronto, the Arenas, to compete with the Canadiens, Wanderers and Senators. The first games were played on December 19, 1917. The Montreal Arena burned down in January 1918, causing the Wanderers to cease operations, and the NHL continued on as a three-team league until the Bulldogs returned in 1919.
The NHL replaced the NHA as one of the leagues that competed for the Stanley Cup, which was an interleague competition back then. Toronto won the first NHL title, the 1918 Stanley Cup. The Canadiens won the league title in 1919; however their Stanley Cup Final against the Seattle Metropolitans was abandoned as a result of the Spanish Flu epidemic. Montreal in 1924 won their first Stanley Cup as a member of the NHL. The Hamilton Tigers, won the regular season title in 1924–25 but refused to play in the championship series unless they were given a C$200 bonus. The league refused and declared the Canadiens the league champion after they defeated the Toronto St. Patricks (formerly the Arenas) in the semi-final. Montreal was then defeated by the Victoria Cougars for the 1925 Stanley Cup. It was the last time a non-NHL team won the trophy, as the Stanley Cup became the "de facto" NHL championship in 1926 after the WCHL ceased operation.
The National Hockey League embarked on rapid expansion in the 1920s, adding the Montreal Maroons and Boston Bruins in 1924. The Bruins were the first American team in the league. The New York Americans began play in 1925 after purchasing the assets of the Hamilton Tigers, and were joined by the Pittsburgh Pirates. The New York Rangers were added in 1926. The Chicago Black Hawks and Detroit Cougars (later Red Wings) were also added after the league purchased the assets of the defunct WCHL. A group purchased the Toronto St. Patricks in 1927 and immediately renamed them the Maple Leafs.
The Original Six.
The first NHL All-Star Game was held in 1934 to benefit Ace Bailey, whose career ended on a vicious hit by Eddie Shore. The second was held in 1937 in support of Howie Morenz's family when he died of a coronary embolism after breaking his leg during a game.
The Great Depression and the onset of World War II took a toll on the league. The Pirates became the Philadelphia Quakers in 1930, then folded one year later. The Senators likewise became the St. Louis Eagles in 1934, also lasting only one year. The Maroons did not survive, as they suspended operations in 1938. The Americans were suspended in 1942 due to a lack of players, and never revived.
The league was reduced to six teams for the 1942–43 NHL season: the Boston Bruins, Chicago Black Hawks, Detroit Red Wings, Montreal Canadiens, New York Rangers and Toronto Maple Leafs. These six teams remained constant for 25 years, a period known as the Original Six. The league reached an agreement with the Stanley Cup trustees in 1947 to take full control of the trophy, allowing the NHL to reject challenges from other leagues that wished to play for the Cup.
Maurice "Rocket" Richard became the first player to score 50 goals, doing so in a 50-game season. Richard later led the Canadiens to five consecutive titles between 1956 and 1960, a record no team has matched. Willie O'Ree broke the league's colour barrier on January 18, 1958 when he made his debut with the Boston Bruins and became the first black player in league history.
Post-Original Six expansion.
By the mid-1960s, the desire for a network television contract in the U.S., and concerns that the Western Hockey League was planning to declare itself a major league and challenge for the Stanley Cup, spurred the league to undertake its first expansion since the 1920s. The league doubled in size for the 1967–68 season, adding the Los Angeles Kings, Minnesota North Stars, Philadelphia Flyers, Pittsburgh Penguins, California Seals and St. Louis Blues. Canadian fans were outraged that all six teams were placed in the United States, and the league responded by adding the Vancouver Canucks in 1970 along with the Buffalo Sabres, who are located on the U.S.-Canadian border. Two years later, the emergence of the newly founded World Hockey Association (WHA) led the league to add the New York Islanders and Atlanta Flames to keep the rival league out of those markets. In 1974, the Washington Capitals and Kansas City Scouts were added, bringing the league up to 18 teams.
The National Hockey League fought the WHA for players, losing 67 to the new league in its first season of 1972–73, including Bobby Hull, who signed a ten-year, $2.5 million contract with the Winnipeg Jets, the largest in hockey history at the time. The league attempted to block the defections in court, but a counter-suit by the WHA led to a Philadelphia judge ruling the NHL's reserve clause to be illegal, thus eliminating the elder league's monopoly over the players. Seven years of battling for players and markets financially damaged both leagues, leading to a 1979 merger agreement that saw the WHA cease operations while the NHL absorbed the Winnipeg Jets, Edmonton Oilers, Hartford Whalers and Quebec Nordiques. The owners initially rejected this merger agreement by one vote, but a massive boycott of Molson Brewery products by fans in Canada caused the Montreal Canadiens, which was owned by Molson, to reverse its position, along with the Vancouver Canucks. In a second vote the plan was approved.
Wayne Gretzky played one season in the WHA for the Indianapolis Racers (eight games) and the Edmonton Oilers (72 games) before the Oilers joined the National Hockey League for the 1979–80 season. Gretzky went on to lead the Oilers to four Stanley Cup championships in 1984, 1985, 1987 and 1988, and set single season records for goals (92 in 1981–82), assists (163 in 1985–86) and points (215 in 1985–86), as well as career records for goals (894), assists (1,963) and points (2,857). He was traded to the Kings in 1988, a deal that dramatically improved the league's popularity in the United States, and provided the impetus for the 1990s expansion cycles that saw the addition of nine teams: the San Jose Sharks, Tampa Bay Lightning, Ottawa Senators, Mighty Ducks of Anaheim, Florida Panthers, Nashville Predators, Atlanta Thrashers, and in 2000 the Minnesota Wild and Columbus Blue Jackets. On July 21, 2015, the NHL confirmed that it had received applications from prospective ownership groups in Quebec City and Las Vegas for possible expansion teams.
Labour issues.
There have been four league-wide work stoppages in league history, all happening since 1992.
The first was a strike by the National Hockey League Players' Association in April 1992 which lasted for ten days, but the strike was settled quickly and all affected games were rescheduled.
A lockout at the start of the 1994–95 season forced the league to reduce the schedule from 84 games to just 48, with the teams playing only intra-conference games during the reduced season. The resulting collective bargaining agreement (CBA) was set for renegotiation in 1998 and extended to September 15, 2004.
With no new agreement in hand when the contract expired on September 15, 2004, league commissioner Gary Bettman announced a lockout of the players union and closed the league's head office. The league vowed to install what it dubbed "cost certainty" for its teams, but the Players' Association countered that the move was little more than a euphemism for a salary cap, which the union initially said it would not accept. The lockout shut down the league for 310 days, the longest in sports history. The NHL became the first professional sports league to lose an entire season. A new collective bargaining agreement was eventually ratified in July 2005, including a salary cap. The agreement had a term of six years with an option of extending the collective bargaining agreement for an additional year at the end of the term, allowing the league to resume as of the 2005–06 season.
On October 5, 2005, the first post-lockout season took to the ice with all 30 teams. The NHL received record attendance in the 2005–06 season: an average of 16,955 per game. After losing a season to a labour dispute in 2005, the League's TV audience was slower to rebound because of American cable broadcaster ESPN's decision to drop the sport. The league's post-lockout agreement with NBC gave the league a share of revenue from each game's advertising sales, rather than the usual lump sum paid up front for game rights. The league's annual revenues were estimated at approximately $2.27 billion.
At midnight September 16, 2012, the labour pact expired, and the league again locked out the players. The owners proposed reducing the players' share of hockey-related revenues from 57 percent to 47 percent. All games were cancelled up to January 14, 2013, as well as the 2013 NHL Winter Classic and the 2013 NHL All-Star Weekend. A tentative agreement was reached on January 6, 2013, on a ten-year deal. On January 12, the league and the Players' Association signed a memorandum of understanding on the new deal, allowing teams to begin their training camps on January 13, with a shortened 48-game season schedule that began on January 19.
Player safety issues.
Player safety has become a major issue within the past five years and concussions, which result from a hard hit to the head, have been the biggest cause. With recent studies showing how concussions can affect retired players and how it has decreased their quality of life after retirement, concussions have become a very important topic of debate when it comes to player safety issues. This had significant effects on the league as elite players were being taken out of the game, such as Sidney Crosby being sidelined for approximately 10 and a half months, which adversely affected the league's marketability. As a result, in December 2009, Brendan Shanahan was hired to replace Colin Campbell and given the role of Senior Vice-President of Player Safety. Shanahan began to hand out suspensions on high profile perpetrators responsible for dangerous hits, such as Raffi Torres receiving 25 games for his hit on Marian Hossa.
To aid with removing high speed collisions on icing, which had led to several potential career ending injuries such as Hurricanes' Defencemen Joni Pitkanen, the league mandated hybrid no-touch icing for the 2013–14 NHL season.
On November 25, 2013, ten former players, Gary Leeman, Rick Vaive, Brad Aitken, Darren Banks, Curt Bennett, Richie Dunn, Warren Holmes, Bob Manno, Blair Stewart and Morris Titanic sued the league for negligence on protecting players from concussions. The suit came three months after the NFL agreed to pay former players US$765 million due to a player safety lawsuit.
Organizational structure.
The Board of Governors is the ruling and governing body of the league. In this context, each team is a member of the league, and each member appoints a Governor (usually the owner of the club), and two alternates to the Board. The current chairman of the Board is Boston Bruins owner, Jeremy Jacobs. The Board of Governors exists to establish the policies of the league, and to uphold its constitution. Some of the responsibilities of the Board of Governors include:
The Board of Governors meets twice per year, in the months of June and December, with the exact date and place to be fixed by the Commissioner.
Executives.
The chief executive of the league is Commissioner Gary Bettman. Some of the principal decision makers who serve under the authority of the commissioner include:
Teams.
The NHL consists of 30 teams, 23 of which are based in the United States and seven in Canada. The NHL divides the 30 teams into two conferences: the Eastern Conference and the Western Conference. Each conference is split into two divisions: the Eastern Conference contains 16 teams (eight per division), while the Western Conference has 14 teams (seven per division). The current alignment has existed since the 2013–14 season.
The number of NHL teams has held constant at 30 teams since the 2000–01 season when the Minnesota Wild and the Columbus Blue Jackets joined the league as expansion teams. That expansion capped a period in the 1990s of rapid expansion and relocation when the NHL added 9 teams to grow from 21 to 30 teams, and relocated four teams mostly from smaller northern cities (e.g., Hartford, Quebec) to larger warmer metropolitan areas (e.g., Dallas, Phoenix). The league has not contracted any teams since the Cleveland Barons folded in 1978.
Game.
Each National Hockey League regulation game is 60 minutes long. The game is composed of three 20-minute periods with an intermission between periods. 
At the end of regulation time, the team with the most goals wins the game. If a game is tied after regulation time, overtime ensues. During the regular season, overtime is a five-minute, three-on-three sudden-death period, in which whoever scores a goal first will win the game.
If the game is still tied at the end of overtime, the game enters a shootout. Three players for each team in turn take a penalty shot. The team with the most goals during the three-round shootout wins the game. If the game is still tied after the three shootout rounds, the shootout continues but becomes sudden-death. Whichever team ultimately wins the shootout is awarded a goal in the game score and thus awarded two points in the standings. The losing team in overtime or shootout is awarded only one. Shootout goals and saves are not tracked in hockey statistics; shootout statistics are tracked separately.
There are no shootouts during the Playoffs. Instead, multiple sudden-death, 20-minute five-on-five periods are played until one team scores. Two games have reached six overtime periods, but none have gone beyond six. During playoff overtime periods, the only break is to clean the loose ice at the first stoppage after the period is halfway finished.
Hockey rink.
National Hockey League games are played on a rectangular hockey rink with rounded corners surrounded by walls and Plexiglas. It measures by in the NHL, approximately the same length but much narrower than International Ice Hockey Federation standards. The centre line divides the ice in half, and is used to judge icing violations. There are two blue lines that divide the rink roughly into thirds, delineating one neutral and two attacking zones. Near the end of both ends of the rink, there is a thin red "goal line" spanning the width of the ice, which is used to judge goals and icing calls.
A trapezoidal area behind each goal net has been introduced. The goaltender can play the puck only within the trapezoid or in front of the goal line; if the goaltender plays the puck behind the goal line and outside the trapezoidal area, a two-minute minor penalty for delay of game is assessed. The rule is unofficially nicknamed the "Martin Brodeur rule".
Since the 2013–14 season, the league trimmed the goal frames by on each side and reduced the size of the goalies' leg pads.
Rules.
The National Hockey League's rules are one of the two standard sets of professional ice hockey rules in the world. The rules themselves have evolved directly from the first organized indoor ice hockey game in Montreal in 1875, updated by subsequent leagues up to 1917, when the NHL adopted the existing NHA set of rules. The NHL's rules are the basis for rules governing most professional and major junior ice hockey leagues in North America. Infractions of the rules, such as offside and icing, lead to a stoppage of play and subsequent face-offs, while more serious infractions leading to penalties to the offending teams. The league also determines the specifications for playing equipment used in its games.
The league has regularly modified its rules to counter perceived imperfections in the game. The penalty shot was adopted from the Pacific Coast Hockey Association to ensure players were not being blocked from opportunities to score. For the 2005–06 season, the league changed some of the rules regarding being offside. First, the league removed the "offside pass" or "two-line pass" rule, which required a stoppage in play if a pass originating from inside a team's defending zone was completed on the offensive side of the centre line, unless the puck crossed the line before the player. Furthermore, the league reinstated the "tag-up offside" which allows an attacking player a chance to get back onside by returning to the neutral zone. The changes to the offside rule were among several rule changes intended to increase overall scoring, which had been in decline since the expansion years of the mid-nineties and the increased prevalence of the neutral zone trap. Since 2005, when a team is guilty of icing the puck they are not allowed to make a line change or skater substitution of any sort before the following face-off (except to replace an injured player or re-install a pulled goaltender). Since 2013, the league has used "hybrid icing", where a linesman stops play due to icing if a defending player (other than the goaltender) crosses the imaginary line that connects the two face-off dots in their defensive zone before an attacking player is able to. This was done to counter a trend of player injury in races to the puck.
The league's rules differ from the rules of the International Ice Hockey Federation (IIHF), as used in tournaments such as the Olympics, which were themselves derived from the Canadian amateur ice hockey rules of the early 20th century. In the NHL, fighting leads to "major penalties" while IIHF rules, and most amateur rules, call for the ejection of fighting players. Usually a penalized team cannot replace a player that is penalized on the ice and is thus short-handed for the duration of the penalty, but if the penalties are coincidental, for example when two players fight, both teams remain at full strength. Also, unlike minor penalties, major penalties must be served to their full completion, regardless of number of goals scored during the power play. The NHL and IIHF differ also in playing rules, such as icing, the areas of play for goaltenders, helmet rules, officiating rules, timeouts and play reviews.
The league also imposes a conduct policy on its players. Players are banned from gambling and criminal activities have led to the suspension of players. The league and the Players' Association agreed to a stringent anti-doping policy in the 2005 bargaining agreement. The policy provides for a twenty-game suspension for a first positive test, a sixty-game suspension for a second positive test, and a lifetime suspension for a third positive test.
Season structure.
The National Hockey League season is divided into a regular season (from early October through early to mid April) and a postseason (the Stanley Cup playoffs).
During the regular season, clubs play each other in a predefined schedule. In the regular season, each team plays 82 games: 41 games each of home and road. Eastern teams play 30 games in their own geographic division—four or five against each of their seven other divisional opponents—and 24 games against the eight remaining non-divisional intra-conference opponents—three games against every team in the other division of its conference. Western teams play 28 or 29 games in their own geographic division-four or five against each of their six other divisional opponents-and 21 or 22 games against the seven remaining non-divisional intra-conference opponents-three games against every team in the other division of its conference, with one cross-division intra-conference match-up occurring in four games. All teams play every team in the other conference twice-home and road.
The league's regular season standings are based on a point system. Two points are awarded for a win, one point for losing in overtime or a shootout, and zero points for a loss in regulation. At the end of the regular season, the team that finishes with the most points in each division is crowned the division champion, and the league's overall leader is awarded the Presidents' Trophy.
The Stanley Cup playoffs, which go from April to the beginning of June, is an elimination tournament where two teams play against each other to win a best-of-seven series in order to advance to the next round. The final remaining team is crowned the Stanley Cup champion. Eight teams from each conference qualify for the playoffs: the top three teams in each division plus the two conference teams with the next highest number of points. The Stanley Cup playoffs are an elimination tournament where the teams are grouped in pairs to play best-of-seven series and the winners moving on to the next round. The two conference champions proceed to the Stanley Cup Final. In all rounds, the higher-ranked team is awarded home-ice advantage, with four of the seven games played at this team's home venue. In the Stanley Cup Final, the team with the most points during the regular season has home-ice advantage.
Entry Draft.
The annual NHL Entry Draft consists of a seven-round off-season draft held in late June. Amateur players from junior, collegiate, or European leagues are eligible to enter the Entry Draft. The selection order is determined by a combination of the standings at the end of the regular season, playoff results, and a draft lottery. The 14 teams that did not qualify for the playoffs are entered in a weighted lottery to determine the initial draft picks in the first round, with the 30th-place team having the best chance of winning the lottery. Once the lottery determines the initial draft picks, the order for the remaining non-playoff teams is determined by the standings at the end of the regular season. For those teams that did qualify for the playoffs, the draft order is then determined by the order in which they were eliminated, with the Stanley Cup winner getting the 30th and last pick, and the runner-up is given the 29th pick.
Trophies and awards.
The National Hockey League presents a number of trophies each year.
Teams.
The most prestigious team award is the Stanley Cup, which is awarded to the league champion at the end of the Stanley Cup playoffs. The team that has the most points in the regular season is awarded the Presidents' Trophy.
The Montreal Canadiens are the most successful franchise in the league. Since the formation of the league in 1917, they have 25 NHL championships (three between 1917 and 1925 when the Stanley Cup was still contested in an interleague competition, twenty-two since 1926 after the Stanley Cup became the NHL's championship trophy). They also lead all teams with 24 Stanley Cup championships (one as an NHA team, twenty-three as an NHL team). Of the four major professional sports leagues in North America, the Montreal Canadiens are surpassed in the number of championships only by the New York Yankees of Major League Baseball, who have three more. 
The longest streak of winning the Stanley Cup in consecutive years is five, held by the Montreal Canadiens from 1955–56 to 1959–60. The 1977 edition of the Montreal Canadiens, the second of four straight Stanley Cup champions, was named by ESPN as the second greatest sports team of all-time. Montreal, however, has not won a Stanley Cup since 1993.
The next most successful NHL franchise is the Toronto Maple Leafs with 13 Stanley Cup championships, but they have not won one since 1967. The Detroit Red Wings, with 11 Stanley Cup championships, are the most successful American franchise.
The same trophy is reused every year for each of its awards. The Stanley Cup, much like its CFL counterpart, is unique in this aspect, as opposed to the Vince Lombardi Trophy, Larry O'Brien Trophy, and Commissioner's Trophy, which have new ones made every year for that year's champion. Despite only one trophy being used, the names of the teams winning and the players are engraved every year on the Stanley Cup. The same can also be said for the other trophies reissued every year.
Players.
There are numerous trophies that are awarded to players based on their statistics during the regular season; they include, among others, the Art Ross Trophy for the league scoring champion (goals and assists), the Maurice "Rocket" Richard Trophy for the goal-scoring leader, and the William M. Jennings Trophy for the goaltender(s) for the team with the fewest goals against them.
The other player trophies are voted on by the Professional Hockey Writers' Association or the team general managers. These individual awards are presented at a formal ceremony held in late June after the playoffs have concluded. The most prestigious individual award is the Hart Memorial Trophy which is awarded annually to the Most Valuable Player; the voting is conducted by members of the Professional Hockey Writers Association to judge the player who is the most valuable to his team during the regular season. The Vezina Trophy is awarded annually to the person deemed the best goaltender as voted on by the general managers of the teams in the NHL. The James Norris Memorial Trophy is awarded annually to the National Hockey League's top defenceman, the Calder Memorial Trophy is awarded annually to the top rookie, and the Lady Byng Memorial Trophy is awarded to the player deemed to combine the highest degree of skill and sportsmanship; all three of these awards are voted on by members of the Professional Hockey Writers Association.
In addition to the regular season awards, the Conn Smythe Trophy is awarded annually to the most valuable player during the NHL's Stanley Cup playoffs. Furthermore, the top coach in the league wins the Jack Adams Award as selected by a poll of the National Hockey League Broadcasters Association. The National Hockey League publishes the names of the top three vote getters for all awards, and then names the award winner during the NHL Awards Ceremony.
Players, coaches, officials, and team builders who have had notable careers are eligible to be voted into the Hockey Hall of Fame. Players cannot enter until three years have passed since their last professional game, the shortest such time period of any major sport. One unique consequence has been Hall of Fame members (specifically, Gordie Howe, Guy Lafleur, and Mario Lemieux) coming out of retirement to play once more. If a player was deemed significant enough, the three-year wait would be waived; only ten individuals have been honoured in this manner. In 1999, Wayne Gretzky joined the Hall and became the last player to have the three-year restriction waived. After his induction, the Hall of Fame announced that Gretzky would be the last to have the waiting period waived.
Origin of players.
In addition to Canadian and American born and trained players, who have historically composed a large majority of NHL rosters, the NHL also draws players from an expanding pool of other nations where organized and professional hockey is played. Since the collapse of the Soviet Bloc, political/ideological restrictions on the movement of hockey players from this region have disappeared, leading to a large influx of players mostly from Czech Republic, Slovakia and Russia into the NHL. Swedes, Finns, and other Western Europeans, who were always free to move to North America, came to the league in greater numbers than before.
Many of the league's top players today come from these European countries, including Daniel Alfredsson, Erik Karlsson, Henrik Sedin, Daniel Sedin, Henrik Lundqvist, Jaromir Jagr, Patrik Elias, Zdeno Chara, Pavel Datsyuk, Evgeni Malkin, and Alexander Ovechkin. European players were drafted and signed by NHL teams in an effort to bring in more "skilled offensive players", although recently there has been a decline in European players as more American players enter the league. The addition of European players changed the style of play in the NHL and European style hockey has been integrated into the NHL game.
Since 1998, during Winter Olympic years the NHL has suspended its all-star game and expanded the traditional all-star break to allow NHL players to represent their countries. Conversely, the IIHF World Championships are held at the same time as the Stanley Cup Playoffs. Thus, NHL players generally only join their respective country's team in the World Championships if their respective NHL team has been eliminated from Stanley Cup contention, or did not make the playoffs.
The NHL has players from 18 different countries, with over 50% coming from Canada and over 20% from the United States. The following table shows the six countries make up the vast majority of NHL players. The table follows the Hockey Hall of Fame convention of classifying players by the currently existing countries in which their birthplaces are located, without regard to their citizenship or where they were trained.
Television and radio.
Canada.
Broadcasting rights in Canada have historically included the CBC's "Hockey Night in Canada" ("HNIC"), a Canadian tradition dating to 1952, and even prior to that on radio since the 1920s.
The current national television and digital rightsholder is Rogers Communications, under a 12-year deal valued at C$5.2 billion which began in the 2014–15 season, as the national broadcast and cable television rightsholders. National English-language coverage of the NHL is carried primarily by Rogers' Sportsnet group of specialty channels; Sportsnet holds national windows on Wednesday and Sunday nights. "Hockey Night in Canada" was maintained and expanded under the deal, airing up to seven games nationally on Saturday nights throughout the regular season. CBC maintains Rogers-produced NHL coverage during the regular season and playoffs. Sportsnet's networks also air occasional games involving all-U.S. matchups.
Quebecor Media holds national French-language rights to the NHL, with all coverage airing on its specialty channel TVA Sports.
Games that are not broadcast as part of the national rights deal are broadcast by Sportsnet's regional feeds, TSN's regional feeds, and RDS. Regional games are subject to blackout for viewers outside of each team's designated market.
United States.
Historically, the NHL has never fared well on American television in comparison to the other American professional leagues. The league's American broadcast partners have been in flux for decades, ranging from such networks as CBS, SportsChannel America, the USA Network, Fox, ABC, and ESPN.
National U.S. television rights are currently held by NBC Sports; its current 10-year, US$2 billion contract, which began in the 2011-12 season, extended and unified rights deals that were first established in the 2005-06 season, when Comcast acquired cable rights to replace ESPN, and NBC acquired broadcast television rights under a revenue-sharing agreement to replace ABC. NBC Sports Network and the company negotiated a new, 10-year, unified rights deal worth nearly US$2 billion. Under this contract, NBCSN usually airs at least two regular season games per week, while NBC airs afternoon games on selected weekends. NBCUniversal holds exclusive rights to Wednesday night games, all games televised by the NBC network, and every game in the Stanley Cup Playoffs beginning in the second round. Coverage of the playoffs and the Finals is split between the two networks, with other games shown on CNBC, USA Network, and NHL Network.
As in Canada, games not broadcast nationally are aired regionally within a team's home market, and are subject to blackout outside of them. These broadcasters include regional sports network chains. Certain national telecasts on NBCSN are non-exclusive, and may also air in tandem with telecasts of the game by local broadcasters. However, national telecasts of these games are blacked out in the participating teams' markets to protect the local broadcaster.
NHL Network.
The league co-owns the NHL Network, a television specialty channel devoted to the NHL. Its signature show is "NHL Tonight". The NHL Network also airs live games, but primarily simulcasts of one of the team's regional broadcasters. 
The U.S. version simulcasts selected regular season games nationally that are not aired by NBC Sports, as well as be used as an overflow channel during the playoffs.
Out-of-market packages.
The NHL operates two subscription-based services allowing access to live, out-of-market games. NHL Centre Ice in Canada and NHL Center Ice in the United States offer access to out-of-market feeds of games through a cable or satellite television provider.
The league also offers "NHL.tv" (branded as "Rogers NHL GameCentre Live" in Canada), which allows the streaming of out-of-market games over the internet and is coordinated by MLB Advanced Media as of February 2016. In the United States, NHL.tv does not carry national games or in-market games.
International.
Outside of Canada and the United States, NHL games are broadcast across Europe, in the Middle East, in Australia, and in the Americas across Mexico, Central America, Dominican Republic, Caribbean, South America and Brazil, among others.
NHL.tv is also available for people outside Canada and the United States to watch games online.
International competitions.
The National Hockey League has occasionally participated in international club competitions. Most of these competitions were arranged by the NHL or NHLPA. The first international club competition was held in 1976, with eight NHL teams playing against the Soviet Championship League's HC CSKA Moscow, and Krylya Sovetov Moscow. Between 1976 and 1991, the NHL, and the Soviet Championship League would hold a number of exhibition games between the two leagues known as the Super Series. No NHL club had played a Russian club from the end of the Super Series in 1991 to 2008, when the New York Rangers faced Metallurg Magnitogorsk in the 2008 Victoria Cup.
In addition to the Russian clubs, NHL clubs had participated in a number of international club competitions with various European leagues. In the 2000s the NHL had organized four NHL Challenge series between NHL, and European clubs. From 2007 to 2011, the NHL organized exhibition games prior to the beginning of the season, known as the NHL Premier, between NHL clubs and teams from a number of European leagues. The 2011 NHL Premiere was the last NHL-organized club competition involving European teams. NHL clubs have also participated in IIHF-organized club tournaments. The most recent IIHF-organized event including a NHL club was the 2009 Victoria Cup, between the Swiss National League A's ZSC Lions, and the Chicago Blackhawks.
Popularity.
The NHL is considered one of the four major professional sports leagues in North America, along with Major League Baseball, the National Football League, and the National Basketball Association. The league is very prominent in Canada, where hockey is the most popular of these four major sports as alongside CFL. Overall, hockey has the smallest total fan base of the four leagues, the smallest revenue from television, and the least sponsorship.
The NHL holds one of the most affluent fan bases. Studies by the Sports Marketing Group conducted from 1998 to 2004 show that the NHL's fan base is much more affluent than that of the PGA Tour. A study done by the Stanford Graduate School of Business in 2004, found that NHL fans in America were the most educated and affluent of the four major leagues. Further it noted that season-ticket sales were more prominent in the NHL than the other three because of the financial ability of the NHL fan to purchase them. According to Reuters in 2010, the largest demographic of NHL fans was highly sought after group males aged 18–34.
The NHL estimates that half of its fan base roots for teams in outside markets. Beginning in 2008, the NHL began a shift toward using digital technology to market to fans to capitalize on this.
The debut of the Winter Classic, an outdoor regular season NHL game held on New Year's Day 2008, was a major success for the league. The game has since become an annual staple of the NHL schedule. This, along with the transition to a national "Game of the Week" and an annual "Hockey Day in America" regional coverage, all televised on NBC, has helped increase the NHL's regular season television viewership in the United States. These improvements led NBC and the cable channel Versus to sign a ten-year broadcast deal, paying US$200 million per year for both American cable and broadcast rights; the deal will lead to further increases in television coverage on the NBC channels.
This television contract has boosted viewership metrics for the NHL. The 2010 Stanley Cup playoffs saw the largest audience in the history of the sport "after a regular season that saw record-breaking business success, propelled in large part by the NHL's strategy of engaging fans through big events and robust digital offerings." This success has resulted in a 66 percent rise in NHL advertising and sponsorship revenue. Merchandise sales were up 22 percent and the number of unique visitors on the NHL.com website were up 17 percent during the playoffs after rising 29 percent in the regular season.

</doc>
<doc id="21810" url="https://en.wikipedia.org/wiki?curid=21810" title="Northern Michigan University">
Northern Michigan University

Northern Michigan University (NMU) is a four-year college public university established in 1899 and located in Marquette, in the Upper Peninsula of the U.S. state of Michigan. With enrollment of about 9,000 undergraduate and graduate students, Northern Michigan University is the Upper Peninsula's largest university. The university is known for its extensive wireless system that covers not only the campus, but the city of Marquette and the surrounding communities and its laptop program that issues laptops to all full-time students and faculty members.
History.
Northern Michigan University was established in 1899 by the Michigan Legislature as Northern State Normal School with the original purpose of providing teacher preparation programs in Michigan's then-wild and sparsely populated Upper Peninsula. When it opened its doors in 1899, NMU enrolled thirty-two students who were taught by six faculty members utilizing rented rooms in Marquette city hall. The original campus-site at the corner of Presque Isle and Kaye Avenues was on land donated by local businessman and philanthropist John M. Longyear, whose namesake academic building, Longyear Hall, opened its doors to students in 1900.
Throughout the school's first half-century, education and teacher training was the primary focus of the small regional school. During this time, the school built the native sandstone buildings Kaye and Peter White Halls, as well as a manual training school adjacent to the campus buildings, J.D. Pierce School. Modest increases in enrollment resulted in several name changes throughout the years:
In 1963, through the adoption of a new state constitution in Michigan, Northern Michigan was designated a comprehensive university serving the diverse educational needs of Upper Michigan. During this time, enrollment at the small state school swelled (due in large part to the 1957 opening of the Mackinac Bridge, linking vehicle traffic between the Upper and Lower Peninsulas); and as a result, the campus expanded rapidly, roughly to the size it remains to this day. Accredited undergraduate and graduate degree programs are offered by the College of Arts and Sciences, the College of Business, the College of Health Sciences and Professional Studies.
Graduate education was inaugurated in 1928 when courses at the master's degree level were offered in cooperation with the University of Michigan.
Academic profile.
NMU has four academic divisions:
Within these four academic divisions 180 undergraduate and graduate degree programs are offered.
Placement Data
Facilities.
NMU is a tobacco-free campus.
Instructional Spaces
In the 10 buildings where classes are held, there are at least 210 instructional spaces, each having a Wi-Fi signal strong enough to accommodate not only the instructor(s) but every student. 112 of these rooms seat at least 30 students. There are 63 general use classrooms which can be scheduled for multiple disciplines. All but 4 general-purpose rooms are smart classrooms fitted with technology for projecting images and sound from one’s laptop computer. There are 14 tiered classrooms, 10 of which are considered lecture halls with a seat-count of at least 90. The largest lecture hall, Jamrich 102, seats 501. There are 58 labs covering the gamut of arts and sciences. There are 28 departmental classrooms, 16 of which are “smart”. There are 3 distance learning facilities, the largest of which is Mead Auditorium which seats 100.
Art and Design
Berry Events Center
Cohodas Hall
Forest Roberts Theatre
Gries Hall
CB Hedgcock Building
Jamrich Hall
Lydia M. Olson Library
McClintock Hall
Physical Education Instructional Facility
Seaborg Science Complex
Superior Dome
The Jacobetti Center
Whitman Hall
Accreditation.
Northern Michigan University is accredited by the Commission on Institutions of Higher Education of the North Central Association of Colleges and Secondary Schools.
The Higher Learning Commission of the North Central Association of Colleges and Secondary Schools
All education programs are accredited by the Teacher Education Accreditation Council (TEAC). Other accreditations include the Accreditation Board for Engineering and Technology; American Alliance for Health, Physical Education, Recreation and Dance; American Chemical Society; American Society of Cytology; Commission on Accreditation of Allied Health Education Professionals (Surgical Technology); Committee on Accreditation for Respiratory Care of the Commission on Accreditation of Allied Health Education Programs; Council on Social Work Education; Department of Transportation Federal Aviation Administration Certification; International Association of Counseling Services, Inc.; Joint Review Committee on Education in Radiologic Technology; Michigan Department of Licensing and Regulation, State Board of Nursing; National Accrediting Agency for Clinical Laboratory Sciences; and the National Association of Schools of Music.
In addition, the nursing programs (practical nursing, baccalaureate, and master's degrees) are fully approved by the Michigan Department of Licensing and Regulation, State Board of Nursing and the baccalaureate and master's degrees are fully accredited by the Commission on Collegiate Nursing Education (CCNE).
The baccalaureate degree programs of the Walker L. Cisler College of Business are accredited by the Association to Advance Collegiate Schools of Business.
Technology.
The Teaching, Learning, and Communication (TLC) initiative places a notebook computer in the hands of every full-time undergraduate student and faculty. This initiative makes NMU one of the largest public university laptop programs in the world. Laptop program participants receive a new notebook computer every four years. Northern’s campus-wide effort for technological mastery helps NMU students compete in the high-tech global marketplace after they graduate. The university has national and international awards for its innovative work in the area of technology in higher education.
Vision of technology initiative.
Northern Michigan University's vision for education in the 21st century is a learning environment that embraces technology to enhance student access, promote the development of independent learners and encourage greater student-faculty communication and collaboration. To help achieve this vision, the university implemented a laptop program in the fall of 2000 that ensures students and faculty have a standard set of tools (hardware and software) that meet a majority of their computing and telecommunications needs, promotes communication and enables quality support. NMU is the first public university in Michigan — but one of many nationwide — to pursue the idea of a "laptop" campus.
Since 2002, most of the campus and surrounding city is covered by a wireless network. Although electronic documents are encouraged, networked printers are installed in various campus locations for hard copy documents.
In the fall of 2009 the university initiated a WiMAX connection initiative. This far-reaching technology has brought Internet access to students off and on campus. It was the first educational facility to create such an initiative and an example of Northern's vision for the future. Because of its popularity and recognition, the campus was visited by President Barack Obama on February 10, 2011, where he praised the development in wireless technology and promoted a National Wireless Initiative to bring high-speed Internet to 98% of the U.S. by 2016.
In 2015, the university began work on their LTE network. Northern has plans to mirror the WiMAX network coverage with LTE, and hopes to finish work by the end of the winter semester in 2016. The WiMAX network is scheduled to be turned off at the start of the fall 2017 semester.
The university has a help desk and walk-in service center to handle laptop maintenance problems.
Cost to students.
NMU leases the laptop computers and issues them to full-time students on a three-year replacement cycle (a student will never have a computer more than three years old). Continuing students who pre-register for the following fall will be able to use the laptop through the summer at no additional charge.
Part-time students have the option to participate in the program. For a fee, part-time students may also check out the laptops from the library on a daily basis.
Additional aspects.
NMU continues to support and improve "specialty labs" as a function of need and resource availability. These are labs designed to meet the needs of specific academic programs that have special equipment and software needs (e.g., graphic design, computer science, GIS, CAD among others). The Center for Instructional Technology in Education (CITE) in the LRC supports faculty use of technology in instruction.
Athletics.
NMU’s Wildcats compete in the NCAA's Division II Great Lakes Intercollegiate Athletic Conference in basketball, football, golf, skiing, cross country, soccer, volleyball, track & field, and swimming/diving. The hockey program competes in Division I as a member of the Western Collegiate Hockey Association. The Division II football team plays in the world's largest wooden dome, the Superior Dome. Lloyd Carr, former head coach at the University of Michigan, former NFL coach Jerry Glanville, and Steve Mariucci, former head coach of the Detroit Lions and San Francisco 49ers, played football for NMU, and current Michigan State coach Tom Izzo played basketball at NMU. Northern Michigan's rivals in sports action are the two other major schools in the upper peninsula: Michigan Technological University, and Lake Superior State University.
The winner of the annual football game between NMU and Michigan Tech is awarded the Miner's Cup.
OTS.
The United States Olympic Training Site on the campus of Northern Michigan University is one of 16 Olympic training sites in the country. The NMU-OTS provides secondary and post-secondary educational opportunities for athletes while offering world-class training.
With more than 70 resident athletes and coaches, the NMU-OTS is the second-largest Olympic training center in the United States, in terms of residents, behind Colorado Springs. The USOEC has more residential athletes than the Lake Placid and Chula Vista sites combined. Over the years, it has grown into a major contributor to the U.S. Olympic movement.
Current resident training programs include Greco-Roman wrestling, weightlifting and women’s freestyle wrestling. Athletes must be approved by the NMU-OTS, their national governing body and NMU to be admitted into the program.
NMU-OTS athletes attend NMU or Marquette Senior High School, Marquette, Michigan while training in their respective sports. The student athletes receive free or reduced room and board, access to world-class training facilities as well as sports medicine and sports science services, academic tutoring, and a waiver of out-of-state tuition fees by NMU. Although athletes are responsible for tuition at the in-state rate, they may receive the B.J. Stupak Scholarship to help cover expenses.
On-campus NMU-OTS athletes live in NMU’s Meyland Hall, eat in campus dining halls, and train at the university’s Berry Events Center and Superior Dome.
The NMU-OTS also offers a variety of short-term training camps; regional, national, and international competitions; coaches and officials education clinics; and an educational program for retired Olympians.
Student life.
Residential life.
Residence hall government is an important facet of student life and NMU. Ten to twenty students from each of the ten residence halls are elected and/or appointed to meet with the staff from their hall on a weekly basis. They represent their peers on a variety of matters pertaining to their residence hall community and campus life.
Students who participate in hall government have the option of participating in various leadership training activities.
One student from up campus (2 halls) and two from down campus (8 halls) are elected to serve on ASNMU, NMU's Student Government.
The 10 residence halls are:
In addition to the residence halls, NMU operates and maintains seven apartment buildings on campus.
The apartments are 
Many halls that have been listed above contain "houses", smaller communities within each residence hall, which participate in campus events and socialize.Many have long-running traditions. For instance, Arctic House in Hunt Hall takes a swim in Lake Superior in the middle of winter. This is known as the Arctic plunge. [http://www.nmu.edu/Housing Many houses in Payne Hall are noted for their volunteer involvement in projects during the school year. Northern Michigan Hall traditions are numerous and involve the students, letting them bond as a community.
Groups and activities.
Student organizations.
NMU hosts a large number of student organizations which are governmental, academic, programming, social, religious, and athletic, as well as residence hall-related, in nature. There are over 300 registered student organizations that provide programs and activities for the campus community. [http://www.nmu.edu/CSE]
Army ROTC.
NMU is the proud host of the United States Army Cadet Command's "Wildcat Battalion". Roughly 70 Cadets train to earn their commissions as United States Army Officers in both the Active Duty and Reserve components. NMU ROTC also trains a specially selected group of Cadets to compete in the annual Ranger Challenge competition held in Fort McCoy, Wisc.
Greek life.
Fraternities
Sororities
Student Leader Fellowship Program.
The Student Leader Fellowship Program (SLFP) is committed to developing competent, ethical, and community-centered leaders. Over a two-year period, students participate in six component areas (Fall Retreat, Mentors, Leadership Theory and Practice Course, Skill Builders! Leadership Workshops, Community Service Internship, and Special Occasions) focusing on self-development and community development.
The Volunteer Center.
The NMU Volunteer Center is designed to assist students, both individuals and in student organizations, as well as faculty and staff at the university with finding ways in which they can contribute to the Marquette community.
Superior Edge.
Unique to Northern, this citizen-leader development program is open to all NMU students, regardless of GPA, major or year in school. Participants can work on any or all of the edges; citizenship, diversity awareness, leadership and real-world experience. Students log a minimum of 100 hours of volunteer, contact, classroom or work time for each edge and write a reflection paper. Achievement of edges is recorded on a student development transcript that is issued alongside a student's academic transcript.
The Superior Edge was developed in 2004-05 by a task force that included students, faculty, and staff. The Superior Edge encompasses a wide range of in- and out-of-classroom experiences that will provide Northern Michigan University students with a distinct advantage by better preparing them for careers, lifelong learning, graduate school, and life as engaged citizens.
Honors Program.
The Honors Program provides talented undergraduates the opportunity to take rigorous coursework that leads to the designation of Lower Division Honors, Upper Division Honors, or Full Honors on their academic transcript. For Full Honors, students must complete two years (16–20 credits) of lower division honors courses, two years of a foreign language, mathematics at the pre-calculus level or higher, 12 credits of upper division coursework in their major or minor that have been "honorized", and a capstone project in the final semester before graduation. To qualify for acceptance to the program, students must have a recalculated GPA of 3.5 or higher (on a 4.0 scale), an ACT score of 27 or above and submit two letters of recommendation. About 40 freshmen are admitted to the program annually.
The North Wind.
The North Wind began in 1972 as Northern Michigan University's second independent, student newspaper. The university's first newspaper was The Northern News, which was shut down due to published articles throughout the 1960s that painted the school in an unflattering manner. Coincidentally in 2015, a controversy arose between the school's administration and members of the North Wind staff, which reached federal court on claims of first amendment violations before the case was dismissed. The weekly paper covers news from the university and community alike and prints on most Wednesdays during the school year. Ray Bressette serves as the current editor-in-chief of the publication. 
WUPX.
WUPX is Northern Michigan University's non-commercial, student run, radio station broadcasting at 91.5 FM. WUPX provides NMU Students and the Marquette area with a wide variety of music, event announcements, and activities.
Student government.
The Associated Students of Northern Michigan University (ASNMU) is made up of three distinct branches: Executive, Legislative and Judicial. Representatives elected to represent Student Affairs groups and Academic Affairs comprise the Legislative Branch with a member of the Legislative Branch elected as Chair of the Assembly. The All Student Judiciary (ASJ), the judicial branch of ASNMU, is a panel composed of 16 students who hear cases involving students who violate the regulations of the University Student Code. The Student Finance Committee (SFC) a sub-committee oversees the collection and disbursement of Student Activity Fee and govern the disbursement of funds to registered student organizations. [http://asnmu.nmu.edu/sfc][http://asnmu.nmu.edu]
Charter schools.
NMU operates seven charter schools throughout Michigan.
As of July 1, 2014, NMU will add three more charter schools: Frances Reh Academy in Saginaw, George Crockett Academy in Detroit, and Universal Leadership Academy in Port Huron

</doc>
<doc id="21811" url="https://en.wikipedia.org/wiki?curid=21811" title="Nemo">
Nemo

Nemo, a Latin word meaning "no man" or "no one", may refer to:

</doc>
<doc id="21813" url="https://en.wikipedia.org/wiki?curid=21813" title="Naked News">
Naked News

Naked News, billing itself as "the program with nothing to hide", is a subscription website featuring a real television newscast. The show is prepared in Toronto and runs daily, with 25-minute episodes 6 days per week. The female anchors read the news fully nude or strip as they present their news segments. Naked News TV is its offshoot pay-per-view or subscription service. Naked News also aired briefly as a late night television series on Citytv Toronto.
Alongside the English language version, there is also Naked News Japan. An Italian version existed but closed after a few years. Naked News en Español was briefly trialled. A male version in English was also launched but ceased production on 31 October 2007.
Most of the show's announcers have been recruited through classified ads in alternative newspapers in Toronto. As such, most of the show's crew comes from the Toronto area. The show features occasional on-the-street interviews by topless newscasters, which are made possible by Ontario's Topfree equality laws. Since the show's inception in 1999, there has been much turnover among the newscasters, and many guest anchors. The female announcers have been featured in almost every medium including television ("CBS Sunday Morning", "The Today Show", "The View", "Sally Jessy Raphaël", and numerous appearances on "Entertainment Tonight" and "ET Insider") newspapers and magazines, ("TV Guide", "Playboy") and as guests on multiple radio shows including Howard Stern.
History.
Naked News was conceived by Fernando Pereira and Kirby Stasyna and debuted in December 1999 as a web-based news service featuring an all-female cast. It began with only one anchor, Victoria Sinclair (who left the program in 2014), and has currently grown to eight female anchors, plus guest anchors. The website was popularized entirely by word of mouth, and quickly became a popular web destination. During the height of its popularity, the website was receiving over 6 million hits per month. Part of the large amounts of web traffic in the site's early days was because the entire newscast could be viewed for free and supported by advertising. By 2002, after the crash of Internet advertising, only one news segment could be viewed freely, and by 2004, no free content remained on the website. Beginning in 2005, a nudity-free version of Naked News was available to non-subscribers. Beginning in June 2008, two news segments could be viewed freely. However, this ended in December 2009. The British channel Sumo TV briefly showed episodes of Naked News, while the free-to-view Playboy One broadcast the show at 9:30pm Mondays-Fridays until its closure in 2008.
A male version of the show was created in 2001 to parallel the female version, but has ceased production as it did not enjoy the female version's popularity and fame. Although it was originally targeted towards female viewers (at one point said to be 30% of the website's audience), the male show later promoted itself as news from a gay perspective.
Similar shows.
A comedic "precursor" to this concept occurred in an episode of "Monty Python's Flying Circus", in which Terry Jones began performing a striptease while giving a fast-paced rundown of economic news.
In the late 1990s, British cable television channel L!VE TV broadcast "Tiffany's Big City Tips", in which model Tiffany Banister gave the financial news while stripping to her underwear.

</doc>
<doc id="21814" url="https://en.wikipedia.org/wiki?curid=21814" title="Nitrogen Oxide Protocol">
Nitrogen Oxide Protocol

Protocol to the 1979 Convention on Long-Range Transboundary Air Pollution Concerning the Control of Emissions of Nitrogen Oxides or Their Transboundary Fluxes, opened for signature on 31 October 1988 and entered into force on 14 February 1991, was to provide for the control or reduction of nitrogen oxides and their transboundary fluxes. It was concluded in Sofia, Bulgaria.
Parties (as of May 2013): (34) Albania, Austria, Belarus, Belgium, Bulgaria, Canada, Croatia, Cyprus, Czech Republic, Denmark, Estonia, European Union, Finland, France, Germany, Greece, Hungary, Ireland, Italy, Liechtenstein, Lithuania, Luxembourg, Republic of Macedonia, Netherlands, Norway, Russia, Slovakia, Slovenia. Spain, Sweden, Switzerland, Ukraine, United Kingdom, United States.
Countries that have signed the protocol but not yet ratified it: Poland.

</doc>
<doc id="21815" url="https://en.wikipedia.org/wiki?curid=21815" title="Noble Eightfold Path">
Noble Eightfold Path

The Noble Eightfold Path (, ) is one of the principal teachings of Śrāvakayāna. It is used to develop insight into the true nature of phenomena (or reality) and to eradicate greed, hatred, and delusion. The Noble Eightfold Path is the fourth of the Buddha's Four Noble Truths; the first element of the Noble Eightfold Path is, in turn, an understanding of the Four Noble Truths. It is also known as the "Middle Path" or "Middle Way." Its goal is Arhatship. The Noble Eightfold Path is contrasted with the Bodhisattva path of Mahayana which culminates in Buddhahood.
All eight elements of the Path begin with the word "right," which translates the word "samyañc" (in Sanskrit) or "sammā" (in Pāli). These denote completion, togetherness, and coherence, and can also suggest the senses of "perfect" or "ideal." 'Samma' is also translated as "wholesome," "wise" and "skillful."
In Buddhist symbolism, the Noble Eightfold Path is often represented by means of the dharma wheel (dharmachakra), whose eight spokes represent the eight elements of the path.
Origin.
According to the Buddhist tradition.
According to discourses found in both the Theravada school's Pali canon, and some of the Āgamas in the Chinese Buddhist canon, the Noble Eightfold Path was rediscovered by Gautama Buddha during his quest for enlightenment. The scriptures describe an ancient path which has been followed and practiced by all the previous Buddhas. The Noble Eightfold Path is a practice said to lead its practitioner toward self-awakening and liberation. The path was taught by the Buddha to his disciples so that they, too, could follow it.
Additionally, some sources give alternate definitions for the Noble Eightfold Path. The Ekottara Āgama in particular contains variant teachings of basic doctrines such as the Noble Eightfold Path, which are different from those found in the Pali Canon.
Historical.
According to Vetter, the description of the Buddhist path may initially have been as simple as the term "the middle way". In time, this short description was elaborated, resulting in the description of the eightfold path. Vetter and Bucknell both note that longer descriptions of "the path" can be found, which can be condensed into the eightfold path. One of those longer sequences, from the "CulaHatthipadopama-sutta", the "Lesser Discourse on the Simile of the Elephant's Footprints", is as follows:
Threefold division.
The Noble Eightfold Path is sometimes divided into three basic divisions, as follows:
This presentation is called the "Three Higher Trainings" in Mahāyāna Buddhism: higher moral discipline, higher concentration and higher wisdom. "Higher" here refers to the fact that these trainings that lead to liberation and enlightenment are engaged in with the motivation of renunciation and bodhicitta.
Practice.
According to the "bhikkhu" (monk) and scholar Walpola Rahula, the divisions of the noble eightfold path "are to be developed more or less simultaneously, as far as possible according to the capacity of each individual. They are all linked together and each helps the cultivation of the others." Bhikkhu Bodhi explains that "with a certain degree of progress all eight factors can be present simultaneously, each supporting the others. However, until that point is reached, some sequence in the unfolding of the path is inevitable."
According to the discourses in the Pali and Chinese canons, right view, right intention, right speech, right action, right livelihood, right effort, and right mindfulness are used as the support and requisite conditions for the practice of right concentration. Understanding of the right view is the preliminary role, and is also the forerunner of the entire Noble Eightfold Path. The practitioner should first try to understand the concepts of right view. Once right view has been understood, it will inspire and encourage the arising of right intention within the practitioner. Right intention will lead to the arising of right speech. Right speech will lead to the arising of right action. Right action will lead to the arising of right livelihood. Right livelihood will lead to the arising of right effort. Right effort will lead to the arising of right mindfulness. The practitioner must make the right effort to abandon the wrong view and to enter into the right view. Right mindfulness is used to constantly remain in the right view. This will help the practitioner restrain greed, hatred and delusion.
Once these support and requisite conditions have been established, a practitioner can then practice right concentration more easily. During the practice of right concentration, one will need to use right effort and right mindfulness to aid concentration practice. In the state of concentration, one will need to investigate and verify his or her understanding of right view. This will then result in the arising of right knowledge, which will eliminate greed, hatred and delusion. The last and final factor to arise is right liberation.
Wisdom.
"Wisdom" ("prajñā" / "paññā"), sometimes translated as "discernment" at its preparatory role, provides the sense of direction with its conceptual understanding of reality. It is designed to awaken the faculty of penetrative understanding to see things as they really are. At a later stage, when the mind has been refined by training in moral discipline and concentration, and with the gradual arising of right knowledge, it will arrive at a superior right view and right intention.
Right view.
Right view (' / ') can also be translated as "right perspective", "right outlook" or "right understanding".
According to Paul Fuller, right-view is a way of seeing which transcends all views. It is a detached way of seeing, different from the attitude of holding to any view, wrong or right.
According to contemporary Theravada Buddhism, it is the right way of looking at life, nature, and the world as they really are for us. It is to understand how our reality works. It acts as the reasoning with which someone starts practicing the path. It explains the reasons for our human existence, suffering, sickness, aging, death, the existence of greed, hatred, and delusion. Right view gives direction and efficacy to the other seven path factors. It begins with concepts and propositional knowledge, but through the practice of right concentration, it gradually becomes transmuted into wisdom, which can eradicate the fetters of the mind. An understanding of right view will inspire the person to lead a virtuous life in line with right view. In the Pāli and Chinese canons, it is explained thus:
There are two types of right view:
Right view has many facets; its elementary form is suitable for lay followers, while the other form, which requires deeper understanding, is suitable for monastics. Usually, it involves understanding the following reality:
Right view for monastics is explained in detail in the "Sammaditthi Sutta" ("Right View Discourse"), in which Ven. Sariputta instructs that right view can alternately be attained by the thorough understanding of the unwholesome and the wholesome, the four nutriments, the twelve "nidanas" or the three taints.
"Wrong view" arising from ignorance ("avijja"), is the precondition for wrong intention, wrong speech, wrong action, wrong livelihood, wrong effort, wrong mindfulness and wrong concentration. The practitioner should use right effort to abandon the wrong view and to enter into right view. Right mindfulness is used to constantly remain in right view.
The purpose of right view is to clear one's path of the majority of confusion, misunderstanding, and deluded thinking. It is a means to gain right understanding of reality. Right view should be held with a flexible, open mind, without clinging to that view as a dogmatic position. In this way, right view becomes a route to liberation rather than an obstacle.
Right intention.
Right intention ("samyak-saṃkalpa" / "sammā sankappa") can also be known as "right thought", "right resolve", "right conception", "right aspiration" or "the exertion of our own will to change". In this factor, the practitioner constantly aspires to rid themselves of whatever qualities they know to be wrong and immoral. Correct understanding of right view will help the practitioner to discern the differences between right intention and wrong intention. In the Chinese and Pali Canon, it is explained thus:
It means the renunciation of the worldly things and an accordant greater commitment to the spiritual path; good will; and a commitment to non-violence, or harmlessness, towards other living beings.
Ethical conduct.
For the mind to be unified in concentration, it is necessary to refrain from unwholesome deeds of body and speech to prevent the faculties of bodily action and speech from becoming tools of the defilements. Ethical conduct ("Śīla" / "Sīla") is used primarily to facilitate mental purification.
Right speech.
Right speech ("samyag-vāc" / "sammā-vācā"), deals with the way in which a Buddhist practitioner would best make use of their words. In the Pali Canon, it is explained thus:
The "Samaññaphala Sutta", "Kevatta Sutta" and "Cunda Kammaraputta Sutta" elaborate:
The "Abhaya Sutta" elaborates:
In every case, if it is not true, beneficial nor timely, one is not to say it. The Buddha followed this, for example, when asked questions of a purely "meta"physical nature, unrelated to the goal, path or discipline that he taught. When asked a question such as "Is the universe eternal?", the Buddha dismissed the topic with the response: "It does not further." (or: "The personal possibilities (goals) assigned you are not furthered by an answer to an ultimate question about the universe's fate.")
Right action.
Right action ("samyak-karmānta" / "sammā-kammanta") can also be translated as "right conduct". As such, the practitioner should train oneself to be morally upright in one's activities, not acting in ways that would be corrupt or bring harm to oneself or to others. In the Chinese and Pali Canon, it is explained as:
For the lay follower, the "Cunda Kammaraputta Sutta" elaborates:
For the monastic, the "Samaññaphala Sutta" adds:
Right livelihood.
The words "Sammā-ājīva" ("samyag-ājīva" / "sammā-ājīva") translates as "Right way of life" (often interpreted as right livelihood). This is taken to mean living in a way that will not elongate samsara, or living with the mindset that rejects the journey of samsara. Specifically, this means distancing oneself from the 10 immoral actions - "Dasa Akusal" and cultivating their opposites, the 10 moral actions - "Dasa Kusal". Right livelihood is a way of implementing this. Right livelihood means that practitioners ought not to engage in trades or occupations which, either directly or indirectly, result in harm for other living beings. In the Chinese and Pali Canon, it is explained thus:
More concretely today interpretations include "work and career need to be integrated into life as a Buddhist," it is also an ethical livelihood, "wealth obtained through rightful means" (Bhikku Basnagoda Rahula) - that means being honest and ethical in business dealings, not to cheat, lie or steal. As people are spending most of their time at work, it’s important to assess how our work affects our mind and heart. So important questions include "How can work become meaningful? How can it be a support, not a hindrance, to spiritual practice — a place to deepen our awareness and kindness?"
The five types of businesses that should not be undertaken:
Concentration.
Concentration ("samadhi") is achieved through concentrating the attention on a single meditation object. This brings the calm and collectedness needed to develop true wisdom by direct experience.
Right effort.
Right effort ("samyag-vyāyāma" / "sammā-vāyāma") can also be translated as "right endeavor" or "right diligence". In this factor, the practitioners should make a persisting effort to abandon all the wrong and harmful thoughts, words, and deeds. The practitioner should instead be persisting in giving rise to what would be good and useful to themselves and others in their thoughts, words, and deeds, without a thought for the difficulty or weariness involved. In both the Chinese and the Pali Canon, it is explained thus:
Although the above instruction is given to the male monastic order, it is also meant for the female monastic order and can be practiced by lay followers of both genders.
The above four phases of right effort mean to:
Right mindfulness.
Right mindfulness ("" / "sammā-sati"), also translated as "right memory", "right awareness" or "right attention". Here, practitioners should constantly keep their minds alert to phenomena that affect the body and mind. They should be mindful and deliberate, making sure not to act or speak due to inattention or forgetfulness. In the Pali Canon, it is explained thus:
Although the above instruction is given to the male monastic order, it is also meant for the female monastic order and can be practiced by lay followers of all genders.
Bhikkhu Bodhi, a monk of the Theravada tradition, further explains the concept of mindfulness as follows:
The Maha Satipatthana Sutta also teaches that by mindfully observing these phenomena, we begin to discern its arising and subsiding and the Three Characteristics of Dharma in direct experience, which leads to the arising of insight and the qualities of dispassion, non-clinging, and release.
Right concentration.
Right concentration ("samyak-samādhi" / "sammā-samādhi"), as its Sanskrit and Pali names indicate, is the practice of concentration ("samadhi"). It is also known as right meditation. As such, the practitioner concentrates on an object of attention until reaching full concentration and a state of meditative absorption ("jhana"). Traditionally, the practice of samadhi can be developed through mindfulness of breathing ("anapanasati"), through visual objects ("kasina"), and through repetition of phrases ("mantra"). Jhana is used to suppress the five hindrances in order to enter into Samadhi. Jhana is an instrument used for developing wisdom by cultivating insight and using it to examine true nature of phenomena with direct cognition. This leads to cutting off the defilements, realizing the dhamma and, finally, self-awakening.
During the practice of right concentration, the practitioner will need to investigate and verify their right view. In the process right knowledge will arise, followed by right liberation. In the Pali Canon, it is explained thus:
Although this instruction is given to the male monastic order, it is also meant for the female monastic order and can be practiced by lay followers from both genders.
According to the Pali and Chinese canon, right concentration is dependent on the development of preceding path factors:
Acquired factors.
In the "Mahācattārīsaka Sutta" which appears in the Chinese and Pali canons, the Buddha explains that cultivation of the noble eightfold path leads to the development of two further factors, which are right knowledge, or insight ("sammā-ñāṇa"), and right liberation, or release ("sammā-vimutti"). These two factors fall under the category of wisdom ("paññā").
Right knowledge and right liberation.
Right knowledge is seeing things as they really are by direct experience, not as they appear to be, nor as the practitioner wants them to be, but as they truly are. A result of Right Knowledge is the tenth factor - Right liberation.
These two factors are the end result of correctly practicing the noble eightfold path, which arise during the practice of right concentration. The first to arise is right knowledge: this is where deep insight into the ultimate reality arises. The last to arise is right liberation: this is where self-awakening occurs and the practitioner has reached the pinnacle of their practice.
Cognitive psychology.
In the essay "Buddhism Meets Western Science", Gay Watson explains:
The noble eightfold path is, from this psychological viewpoint, an attempt to change patterns of thought and behavior. It is for this reason that the first element of the path is right understanding ('), which is how one's mind views the world. Under the wisdom ("paññā") subdivision of the noble eightfold path, this worldview is intimately connected with the second element, right thought ('), which concerns the patterns of thought and intention that controls one's actions. These elements can be seen at work, for example, in the opening verses of the "Dhammapada": The noble eightfold path is also the fourth noble truth.
Thus, by altering one's distorted worldview, bringing out "tranquil perception" in the place of "perception polluted", one is able to ease suffering. Watson points this out from a psychological standpoint:
Research has shown that repeated action, learning, and memory can actually change the nervous system physically, altering both synaptic strength and connections. Such changes may be brought about by cultivated change in emotion and action; they will, in turn, change subsequent experience.

</doc>
<doc id="21818" url="https://en.wikipedia.org/wiki?curid=21818" title="National park">
National park

A national park is a park in use for conservation purposes. Often it is a reserve of natural, semi-natural, or developed land that a sovereign state declares or owns. Although individual nations designate their own national parks differently, there is a common idea: the conservation of 'wild nature' for posterity and as a symbol of national pride. An international organization, the International Union for Conservation of Nature (IUCN), and its World Commission on Protected Areas, has defined "National Park" as its "Category II" type of protected areas.
While this type of national park had been proposed previously, the United States established the first "public park or pleasuring-ground for the benefit and enjoyment of the people", Yellowstone National Park, in 1872. Although Yellowstone was not officially termed a "national park" in its establishing law, it was always termed such in practice and is widely held to be the first and oldest national park in the world. The first area to use "national park" in its creation legislation was the US's Mackinac Island, in 1875. Australia's Royal National Park, established in 1879, was the world's third official national park. In 1895 ownership of Mackinac Island was transferred to the State of Michigan as a state park and national park status was consequently lost. As a result, Australia's Royal National Park is by some considerations the second oldest national park now in existence.
The largest national park in the world meeting the IUCN definition is the Northeast Greenland National Park, which was established in 1974. According to the IUCN, 6,555 national parks worldwide met its criteria in 2006. IUCN is still discussing the parameters of defining a national park.
National parks are almost always open to visitors. Most national parks provide outdoor recreation and camping opportunities as well as classes designed to educate the public on the importance of conservation and the natural wonders of the land in which the national park is located.
Definitions.
In 1969, the IUCN declared a national park to be a relatively large area with the following defining characteristics:
In 1971, these criteria were further expanded upon leading to more clear and defined benchmarks to evaluate a national park. These include:
While the term national park is now defined by the IUCN, many protected areas in many countries are called national park even when they correspond to other categories of the IUCN Protected Area Management Definition, for example:
While national parks are generally understood to be administered by national governments (hence the name), in Australia national parks are run by state governments and predate the Federation of Australia; similarly, national parks in the Netherlands are administered by the provinces.
In many countries, including Indonesia, the Netherlands, and the United Kingdom, national parks do not adhere to the IUCN definition, while some areas which adhere to the IUCN definition are not designated as national parks.
History.
In 1810, the English poet William Wordsworth described the Lake District as a The painter George Catlin, in his travels through the American West, wrote during the 1830s that the Native Americans in the United States might be preserved 
The first effort by the Federal government to set aside such protected lands was on April 20, 1832, when President Andrew Jackson signed legislation that the 22nd United States Congress had enacted to set aside four sections of land around what is now Hot Springs, Arkansas, to protect the natural, thermal springs and adjoining mountainsides for the future disposal of the U.S. government. It was known as Hot Springs Reservation, but no legal authority was established. Federal control of the area was not clearly established until 1877.
John Muir is today referred to as the "Father of the National Parks" due to his work in Yosemite. He published two influential articles in The Century Magazine, which formed the base for the subsequent legislation.
President Abraham Lincoln signed an Act of Congress on July 1, 1864, ceding the Yosemite Valley and the Mariposa Grove of Giant Sequoias (later becoming Yosemite National Park) to the state of California. According to this bill, private ownership of the land in this area was no longer possible. The state of California was designated to manage the park for "public use, resort, and recreation". Leases were permitted for up to ten years and the proceeds were to be used for conservation and improvement. A public discussion followed this first legislation of its kind and there was a heated debate over whether the government had the right to create parks. The perceived mismanagement of Yosemite by the Californian state was the reason why Yellowstone at its establishment six years later was put under national control.
In 1872, Yellowstone National Park was established as the United States' first national park, being also the world's first national park. In some European countries, however, national protection and nature reserves already existed, such as Drachenfels (Germany, 1822) and a part of Forest of Fontainebleau (France, 1861).
Yellowstone was part of a federally governed territory. With no state government that could assume stewardship of the land so the federal government took on direct responsibility for the park, the official first national park of the United States. The combined effort and interest of conservationists, politicians and the Northern Pacific Railroad ensured the passage of enabling legislation by the United States Congress to create Yellowstone National Park. Theodore Roosevelt, already an active campaigner and so influential, as good stump speakers were highly necessary in the pre-telecommunications era, was highly influential in convincing fellow Republicans and big business to back the bill.
American Pulitzer Prize-winning author Wallace Stegner wrote:
National parks are the best idea we ever had. Absolutely American, absolutely democratic, they reflect us at our best rather than our worst.
In his book "Dispossessing the Wilderness: Indian Removal and the Making of the National Parks", Mark David Spence made the point that in order to create these uninhabited spaces, the United States first had to disposess the Indians who were living in them.
Even with the creation of Yellowstone, Yosemite, and nearly 37 other national parks and monuments, another 44 years passed before an agency was created in the United States to administer these units in a comprehensive way – the U.S. National Park Service (NPS). The 64th United States Congress passed the National Park Service Organic Act, which President Woodrow Wilson signed into law on August 25, 1916. Of the sites managed by the National Park Service of the United States, only 59 carry the designation of National Park. 
Following the idea established in Yellowstone, there soon followed parks in other nations. In Australia, the Royal National Park was established just south of Sydney on April 26, 1879, becoming the world's second official national park (actually the 3rd: Mackinac National Park in Michigan was created in 1875 as a national park but was later transferred to the state's authority in 1895, thus losing its official "national park" status). Rocky Mountain National Park became Canada's first national park in 1885. Argentina became the third country in the Americas to create a national park system, with the creation of the Nahuel Huapi National Park in 1934, through the initiative of Francisco Moreno. New Zealand established Tongariro National Park in 1887. In Europe, the first national parks were a set of nine parks in Sweden in 1909, followed by the Swiss National Park in 1914. Europe has some 359 national parks as of 2010. Africa's first national park was established in 1925 when Albert I of Belgium designated an area of what is now Democratic Republic of Congo centred on the Virunga Mountains as the Albert National Park (since renamed Virunga National Park). In 1973, Mount Kilimanjaro was classified as a National Park and was opened to public access in 1977. In 1926, the government of South Africa designated Kruger National Park as the nation's first national park, although it was an expansion of the earlier Sabie Game Reserve established in 1898 by President Paul Kruger of the old South African Republic, after whom the park was named. After World War II, national parks were founded all over the world. The Vanoise National Park in the Alps was the first French national park, created in 1963 after public mobilization against a touristic project.
The world's first national park service was established May 19, 1911, in Canada. The Dominion Forest Reserves and Parks Act placed the dominion parks under the administration of the Dominion Park Branch (now Parks Canada). The branch was established to "protect sites of natural wonder" to provide a recreational experience, centered on the idea of the natural world providing rest and spiritual renewal from the urban setting. Canada now has the largest protected area in the world with 377,000 km² of national park space. In 1989, the Qomolangma National Nature Preserve (QNNP) was created to protect 3.381 million hectares on the north slope of Mount Everest in the Tibet Autonomous Region of China. This national park is the first major global park to have no separate warden and protection staff—all of its management being done through existing local authorities, allowing a lower cost basis and a larger geographical coverage (in 1989 when created, it was the largest protected area in Asia). It includes four of the six highest mountains Everest, Lhotse, Makalu, and Cho Oyu. The QNNP is contiguous to four Nepali national parks, creating a transborder conservation area equal in size to Switzerland.
Economic ramifications.
Countries with a large nature-based tourism industry, such as Costa Rica, often experience a huge economic effect on park management as well as the economy of the country as a whole.
Tourism.
Tourism to national parks has increased considerably over time. In Costa Rica for example, a megadiverse country, tourism to parks has increased by 400% from 1985 to 1999. The term "national park" is perceived as a brand name that is associated with nature-based tourism and it symbolizes "high quality natural environment and well-design tourism infrastructure".
Staff.
The duties of a park ranger are to supervise, manage, and/or perform work in the conservation and use of Federal park resources. This involves functions such as park conservation; natural, historical, and cultural resource management; and the development and operation of interpretive and recreational programs for the benefit of the visiting public. Park rangers also have fire fighting responsibilities and execute search and rescue missions. Activities also include heritage interpretation to disseminate information to visitors of general, historical, or scientific information. Management of resources such as wildlife, lakeshores, seashores, forests, historic buildings, battlefields, archeological properties, and recreation areas are also part of the job of a park ranger. Since the establishment of the National Park Service in the US in 1916, the role of the park ranger has shifted from merely being a custodian of natural resources to include several activities that are associated with law enforcement. They control traffic and investigate violations, complaints, trespass/encroachment, and accidents.

</doc>
<doc id="21819" url="https://en.wikipedia.org/wiki?curid=21819" title="Papal diplomacy">
Papal diplomacy

Nuncio (officially known as an Apostolic nuncio and also known as a papal nuncio) is the title for an ecclesiastical diplomat, being an envoy or permanent diplomatic representative of the Holy See to a state or international organization. A nuncio is appointed by and represents the Holy See, and is the head of the diplomatic mission, called an Apostolic Nunciature, which is the equivalent of an embassy. The Holy See is legally distinct from the Vatican City or the Catholic Church. A nuncio is usually an archbishop.
A papal nuncio is generally equivalent in rank to that of ambassador extraordinary and plenipotentiary, although in Catholic countries the nuncio often ranks above ambassadors in diplomatic protocol. A nuncio performs the same functions as an ambassador and has the same diplomatic privileges. Under the 1961 Vienna Convention on Diplomatic Relations, to which the Holy See is a party, a nuncio is an ambassador like those from any other country. The Vienna Convention allows the host state to grant seniority of precedence to the nuncio over others of ambassadorial rank accredited to the same country, and may grant the deanship of that country's diplomatic corps to the nuncio regardless of seniority. The representative of the Holy See in some situations is called a Delegate or, in the case of the United Nations, Permanent Observer. In the Holy See hierarchy, these usually rank equally to a nuncio, but they do not have formal diplomatic status, though in some countries they have some diplomatic privileges.
In addition, the nuncio serves as the liaison between the Holy See and the Church in that particular nation, supervising the diocesan episcopate (usually a national conference of bishops which has its own chairman, usually the highest-ranking bishop or archbishop, especially if his seat carries the title of primate or he has individually been created a cardinal) and has an important role in the selection of bishops.
Terminology and history.
The name nuncio is derived from the ancient Latin word, "nuntius", meaning "envoy" or "messenger".
Formerly, the title (Apostolic) Internuncio denoted a papal diplomatic representative of the second class, corresponding to Envoy Extraordinary and Minister Plenipotentiary as a title for diplomatic representatives of states (cf. Article 14, par. 2 of the Vienna Convention). Before 1829, Internuncio was the title applied instead to the "ad interim" head of a mission when one Nuncio had left office and his replacement had not yet assumed it.
A legate a latere is a temporary papal representative or a representative for a special purpose.
Historically, the most important type of apocrisiary (a title also applying to representatives exchanged by a high prelate with a Patriarch) was the equivalent of a nuncio, sent by the Pope to the Byzantine Empire; during the fifth and sixth centuries, when much of Italy remained under Byzantine control, several Popes were former apocrisiaries.
Pro-nuncio was a term used from 1965 to 1991 for a papal diplomatic representative of full ambassadorial rank accredited to a country that did not accord him precedence over other ambassadors and "de jure" deanship of the Diplomatic Corps. In those countries, the papal representative's precedence within the corps is exactly on a par with that of the other members of ambassadorial rank, so that he becomes dean only on becoming the senior member of the corps.
In countries with whom the Holy See does not have diplomatic ties, an Apostolic Delegate may be sent to act as a liaison with the Roman Catholic Church in that country, though not accredited to its government. Apostolic delegates have the same ecclesiastical rank as nuncios, but have no formal diplomatic status, though in some countries they have some diplomatic privileges. For example, an apostolic delegate served as the Holy See's "de facto" diplomatic representative to the United States and the United Kingdom, until both major Anglo-Saxon states with a predominantly Protestant tradition established full-fledged relations with the Holy See in the late twentieth century, allowing for the appointment of a Papal Nuncio (see the list of British Ambassadors to the Holy See). Archbishop Pio Laghi, for example, was first apostolic delegate, then pro-nuncio, to the United States during the Jimmy Carter, Ronald Reagan, and George H. W. Bush presidencies.
Apostolic delegates are also sent to regions such as the West Indies and the islands of the Pacific. These delegates are also appointed nuncio to at least some of the many states covered by their delegation, but the area entrusted to them also contains one or more territories that either are not independent states or are states that do not have diplomatic relations with the Holy See.
In accordance with this article, many states (even not predominantly Catholic ones such as Germany and Switzerland and including the great majority in central and western Europe and in the Americas) give precedence to the Nuncio over other diplomatic representatives, according him the position of Dean of the Diplomatic Corps reserved in other countries for the longest-serving resident ambassador.
Multilateral.
A Holy See Representative is accredited to an international organisation (mostly UN-related or regional) where other states dispatch a Permanent Representative, usually at ambassadorial level (Titular Archbishop, equivalent a papal Nuncio).

</doc>
<doc id="21820" url="https://en.wikipedia.org/wiki?curid=21820" title="Newlyn School">
Newlyn School

The Newlyn School was an art colony of artists based in or near Newlyn, a fishing village adjacent to Penzance, Cornwall, from the 1880s until the early twentieth century. The establishment of the Newlyn School was reminiscent of the Barbizon School in France, where artists fled Paris to paint in a more pure setting emphasizing natural light. These schools along with a related California movement were also known as En plein air.
Newlyn had a number of things guaranteed to attract artists: fantastic light, cheap living, and the availability of inexpensive models. The artists were fascinated by the fishermen's working life at sea and the everyday life in the harbour and nearby villages. Some paintings showed the hazards and tragedy of the community's life, such as women anxiously looking out to sea as the boats go out, or a young woman crying on hearing news of a disaster. Lamorna Birch was the prime mover behind the colony and the work done there. The later Forbes School of Painting, founded by Stanhope Forbes and his wife Elizabeth in 1899, promoted the study of figure painting. A present day Newlyn School of Art was formed in 2011 with Arts Council funding providing art courses taught by many of the best-known artists working in Cornwall today.
In the late nineteenth and early twentieth centuries, Lamorna, a nearby fishing village to the south, became popular with artists of the Newlyn School and is particularly associated with the artist S. J. "Lamorna" Birch who lived there from 1908.
Member artists.
Newlyn School painters include:
For a full list see: George Bednar. "Every Corner was a Picture: A checklist compiled for the West Cornwall Art Archive of 50 artists from the early Newlyn School painters through to the present." ISBN 1-872229-36-0

</doc>
<doc id="21822" url="https://en.wikipedia.org/wiki?curid=21822" title="Natural Law Party">
Natural Law Party

The Natural Law Party (NLP) was a transnational party founded in 1992 on "the principles of Transcendental Meditation", the laws of nature, and their application to all levels of government. It was active in up to 74 countries. It continues in India and some parts of the United States. The party defined "natural law" as the organizing intelligence which governs the natural universe. George Harrison performed a benefit concert in support of the party in 1992. Electoral success was achieved by the Ajeya Bharat Party in India, which elected a legislator to the state assembly, and the Croatian NLP, which elected a member of their regional assembly in 1993. In the USA its organization was reported to rival that of other "established third parties".
History and platform.
According to the Maharishi, the Natural Law Party (NLP) was first founded in the United Kingdom in March 1992 and was later established in the United States, France, Austria, Germany, Croatia, Israel, Japan, Spain, the Netherlands, Italy, Australia, Norway, Sweden, New Zealand, Chile, Thailand and Canada. The American branch of the party was founded later that year in Fairfield, Iowa U.S.A. by educators, business leaders, lawyers and other supporters of the Transcendental Meditation movement. The party was active in many countries and delegates from 60 countries attended an international convention in Bonn, Germany in 1998. The party became largely inactive in the United States in 2004 and was discontinued in the Netherlands in 2007.
The party had its foundation in the principles of Transcendental Meditation and was committed to "prevention oriented government and conflict free politics" through holistic health programs and the practice of the Transcendental Meditation technique. In Scotland and Wales, party advertisements proclaimed that "natural law which silently governs the whole universe in perfect order and without a problem." The party contested several federal and state elections between 1990 and 1997.
Canada.
The Natural Law Party was active in the Canadian federal elections of 1993, 1997 and 2000 and in provincial elections in Ontario and Quebec during this period, before it was deregistered in 2003.
India.
The Natural Law Party in India is known as the Ajeya Bharat Party (AJBP) or Invincible India Party. It promotes a Vedic way of life. It was formed in late 1998 as the political wing of the Maharishi Vedic Vishwa Prashasan (MVVP (Maharishi Global Administration Through Natural Law)), which had nominated thirty-four candidates in the February 1998 parliamentary election from Madhya Pradesh. The Maharishi was said to be "keenly interested" in building a political base in his native province. The MVVP received 0.28% of the vote in its first election. Mukesh Nayak left the cabinet and the Congress Party to assume the leadership of the Madhya Pradesh MVVP. For the November 1998 election, the Ajeya Bharat had a list of 100 candidates for the Assembly. It received 0.5% of the vote and won one seat in the 320-member state assembly. The following year, that member switched parties, leaving the Ajeya Bharat with no representation. In 2008, Nayak left the party to rejoin the Congress Party. In 2009, the Ajeya Bharat Party president, Ambati Krishnamurthy, filed a complaint against another party for using a flag similar to its own.
Ireland.
The Natural Law Party became active in Ireland in 1994 and was based in Dublin. The party leader was John Burns, who was one of nine Natural Law Party candidates in the 1997 general election. In addition, there were four candidates in the European elections of 1999. Burns endorsed the alternative health system of Maharishi Vedic Approach to Health and the five European candidates gained about 0.5% of first-preference votes cast. Burns, who also contested the 1999 Dublin South Central by-election, spent only £163 on his campaign. After 1999, the party ceased to field candidates in Ireland. The amount of corporate political donations in 2000 was nil.
Israel.
The Natural Law Party of Israel (, "Mifleget Hok HaTeva Shel Yisrael") was a minor political party in Israel. Its leader was Amihai Rokah. In the 1992 elections the Natural Law Party won 1,734 votes (0.06%), and in the 1999 elections, won 2,924 votes (0.09%), both below the then 1.5% electoral threshold required to enter the Knesset. It has not run in an election since and its website states it has ceased political activity, but as of 2011 it is still registered as a party in Israel.
Italy.
The Natural Law Party in Italy ("Partito della Legge Naturale", PLN) took place to several (both general and local) elections in the nineties. In the 1994 general elections it won 24,897 votes (0.06%) for the Chamber of Deputies and 86,588 votes (0.26%) for the Senate. The list was on ballot in a few constituencies only. In the 1996 general elections the Natural Law Party ran candidates only in the Trentino-Alto Adige/Südtirol region, who won 8,298 votes for the Chamber of Deputies and 5,842 for the Senate (about 1% on a regional basis, 0.2% in the whole country).
New Zealand.
The Natural Law Party of New Zealand was formed in 1995. The Natural Law Party never won any seats in Parliament, and was removed from the register of official political parties in February 2001.
Trinidad and Tobago.
The Natural Law Party in Trinidad and Tobago contested the 1995 general elections. It received 1,590 votes, but failed to win a seat.
United Kingdom.
The Natural Law Party was founded in the United Kingdom in March 1992. Geoffrey Clements was its leader.
The UK manifesto, as published on its website, listed 5 key aspects of a successful government including 
In the 1992 general election, held on 9 April, the NLP contested 310 seats in the UK, garnering 0.19% of the vote, with every candidate losing their deposit for failing to receive at least 5% of the vote. The group announced that they had budgeted nearly 1 million pounds for the campaign. A significant number of constituencies were contested by nationals of countries outside the UK, including Canada, Australia, New Zealand, and India, as British electoral law allows any member of a Commonwealth country to stand for Parliament. Among them was Canadian-born magician Doug Henning. Despite the "dismal" amount of votes, an article in the The Herald of Scotland reported that it could be considered a "reasonable return for a campaign which began only three weeks before polling day." In addition the NLP "notched up" a "headline-grabbing record" when it put forward candidates for all 87 British seats in the 1994 European Parliament – the first party to do so.
George Harrison performed a fund-raising concert at the Royal Albert Hall in London for the NLP on 6 April 1992, his first full concert in the UK since 1969. According to Harrison, a week before the general election, Maharishi Mahesh Yogi suggested to Harrison that he, Paul McCartney and Ringo Starr stand for election as MPs for Liverpool as NLP candidates, but they declined.
In the 1997 general election, the NLP ran 197 candidates for Parliament in the UK, garnering 0.10% of the vote, with every candidate losing their deposit.
The NLP ran 16 candidates in the 20 by-elections held between 1992 and 1997, with every candidate losing their deposit. The NLP ran 8 candidates for the 16 by-elections held between 1997 and 2001, averaging 0.10% of the vote, with every candidate losing their deposit. The NLP did not run any candidates for Parliament in the 2001 general election or in the succeeding by-elections. The party, along with its Northern Ireland wing, voluntarily deregistered with the Electoral Commission at the end of 2003.
Northern Ireland.
According to the NLP, they prepared a 70-page report in response to the "1996 Framework Document of the British and Irish governments." The report was presented to leaders in Ireland, Northern Ireland and the U.S. Afterwards, NLP representatives participated in the "special elections to the Northern Ireland Forum", but withdrew before the election.
United States.
The Natural Law Party (United States) ran John Hagelin as its presidential candidate in 1992, 1996, and 2000. It attempted to merge with the Reform Party in 2000. The NLP in the United States was largely disbanded in 2004. However, some state affiliates, such as Michigan, have kept their ballot positions and allied with other small parties.

</doc>
<doc id="21826" url="https://en.wikipedia.org/wiki?curid=21826" title="Naturalistic fallacy">
Naturalistic fallacy

In philosophical ethics, the term "naturalistic fallacy" was introduced by British philosopher G. E. Moore in his 1903 book "Principia Ethica". Moore argues it would be fallacious to explain that which is "good" reductively in terms of natural properties such as "pleasant" or "desirable".
Moore's naturalistic fallacy is closely related to the is–ought problem, which comes from David Hume's "A Treatise of Human Nature" (1738–40). However, unlike Hume's view of the is–ought problem, Moore (and other proponents of ethical non-naturalism) did not consider the naturalistic fallacy to be at odds with moral realism.
Different common uses.
The is–ought problem.
The term "naturalistic fallacy" is sometimes used to describe the deduction of an "ought" from an "is" (the is–ought problem).
In using his categorical imperative Kant deduced that experience was necessary for their application. But experience on its own or the imperative on its own could not possibly identify an act as being moral or immoral. We can have no certain knowledge of morality from them, being incapable of deducing how things ought to be from the fact that they happen to be arranged in a particular manner in experience.
Bentham, in discussing the relations of law and morality, found that when people discuss problems and issues they talk about how they wish it would be as opposed to how it actually is. This can be seen in discussions of natural law and positive law. Bentham criticized natural law theory because in his view it was a naturalistic fallacy, claiming that it described how things ought to be instead of how things are.
Moore's discussion.
According to G. E. Moore's "Principia Ethica", when philosophers try to define "good" reductively in terms of natural properties like "pleasant" or "desirable", they are committing the naturalistic fallacy.
In defense of ethical non-naturalism, Moore's argument is concerned with the semantic and metaphysical underpinnings of ethics. In general, opponents of ethical naturalism reject ethical conclusions drawn from natural facts.
Moore argues that good, in the sense of intrinsic value, is simply ineffable: it cannot be defined because it is not a natural property, being "one of those innumerable objects of thought which are themselves incapable of definition, because they are the ultimate terms by reference to which whatever "is" capable of definition must be defined". On the other hand, ethical naturalists eschew such principles in favor of a more empirically accessible analysis of what it means to be good: for example, in terms of pleasure in the context of hedonism.
In §7, Moore argues that a property is either a complex of simple properties, or else it is irreducibly simple. Complex properties can be defined in terms of their constituent parts but a simple property has no parts. In addition to "good" and "pleasure", Moore suggests that colour qualia are undefined: if one wants to understand yellow, one must see examples of it. It will do no good to read the dictionary and learn that "yellow" names the colour of egg yolks and ripe lemons, or that "yellow" names the primary colour between green and orange on the spectrum, or that the perception of yellow is stimulated by electromagnetic radiation with a wavelength of between 570 and 590 nanometers, because yellow is all that and more, by the open question argument.
Bernard Williams called Moore's use of the term 'naturalistic fallacy' a "spectacular misnomer", the question being metaphysical, as opposed to rational.
Appeal to nature.
Some people use the phrase "naturalistic fallacy" or "appeal to nature" to characterize inferences of the form "Something is natural; therefore, it is morally acceptable" or "This property is unnatural; therefore, this property is undesireable." Such inferences are common in discussions of homosexuality, environmentalism, and veganism.
Criticism.
Some philosophers reject the naturalistic fallacy and/or suggest solutions for the proposed is–ought problem.
Sam Harris argues that it is possible to derive "ought" from "is", and even that it has already been done to some extent. He sees morality as a budding science. This view is critical of Moore's "simple indefinable terms" (which amount to qualia), arguing instead that such terms actually can be broken down into constituents.
Ralph McInerny suggests that "ought" is already bound up in "is", in so far as the very nature of things have ends/goals within them. For example, a clock is a device used to keep time. When one understands the function of a clock, then a standard of evaluation is implicit in the very description of the clock, i.e., because it "is" a clock, it "ought" to keep the time. Thus, if one cannot pick a good clock from a bad clock, then one does not really know what a clock is. In like manner, if one cannot determine good human action from bad, then one does not really know what the human person is.
Certain uses of the naturalistic fallacy refutation (a scheme of reasoning that declares an inference invalid because it incorporates an instance of the naturalistic fallacy) have been criticized as lacking rational bases, and labelled anti-naturalistic fallacy. For instance, Alex Walter wrote:
The refutations from naturalistic fallacy defined as inferring evaluative conclusions from purely factual premises do assert, implicitly, that there is no connection between the facts and the norms (in particular, between the facts and the mental process that led to adoption of the norms).

</doc>
<doc id="21828" url="https://en.wikipedia.org/wiki?curid=21828" title="Neapolitan ice cream">
Neapolitan ice cream

Neapolitan ice cream, sometimes known as harlequin ice cream, is made up of blocks of vanilla, chocolate, and strawberry ice cream side by side in the same container (typically with no packaging in between). Some brands intermix the flavors more, though the separate flavors are still clearly visible. 
Neapolitan ice cream was named in the late 19th century as a reflection of its presumed origins in the cuisine of the Italian city of Naples, and the many Neapolitan immigrants who brought their expertise in frozen desserts with them to the United States. Spumoni was introduced to the United States in the 1870s as Neapolitan-style ice cream. Early recipes used a variety of flavors; however, the number of three molded together was a common denominator, to resemble the Italian flag (cf. insalata tricolore). More than likely, chocolate, vanilla, and strawberry became the standard for the reason that they were the most popular flavors in the United States at the time of introduction.
Quotes from food historians.
"Cosmopolitan slice. A slice of ice-cream cake made with mousse mixture and ordinary ice cream, presented in a small pleated paper case. Neapolitan ice cream consists of three layers, each of a different colour and flavour (chocolate, strawberry, and vanilla), moulded into a block and cut into slices.
Neapolitan ice-cream makers were famous in Paris at the beginning of the 19th century, especially Tortoni, creator of numerous ice-cream cakes."
"Eighteenth century... confectioners' shops very often run by Italians. Consequently ice creams were often called "Italian ice creams" or "Neapolitan ice creams" throughout the nineteenth century, and the purveying of such confections became associated with Italian immigrants."
"Neapolitan ice cream, different flavoured layers frozen together... being first being talked about in the 1870s."
A cultural reference from "The New York Times" in 1887:"...in a dress of pink and white stripes, strongly resembling Neapolitan ice cream."
19th century descriptions.
1885 – "Neapolitan box" 
"You must have a Neapolitan box for this ice and fill it up in three or four layers with different coloured and flavoured ice creams (a water ice may be used with the custards); for instance, lemon, vanilla, chocolate and pistachio. Mould in the patent ice cave for about 1½ to 2 hours, turn it out, cut it in slices, and arrange neatly on the dish, on a napkin or dish-paper."
1894 – "Neapolitan Icey Cones"
"These are prepared by putting ices of various kinds and colors into a mold known as a Neapolitan ice box, which, when set and turned out, is cut into slices suitable for serving. However small the pieces, the block should be cut so that each person gets some of each kind. They are generally laid on a lace paper on an ice plate. Four or five kinds are usually put in the mold, though three sorts will do. The following will serve as a guide in arranging: First, vanilla cream, then raspberry or cherry or currant water; coffee or chocolate in the middle; the strawberry cream, with lemon or orange or pineapple water to finish. A cream ice flavored with any liqueur, a brown bread cream flavored with brandy, with a couple of bright-colored water ices, form another agreeable mixture. Tea cream may be introduced into almost any combination unless coffee were used. Banana cream, pistachio, or almond cream with cherry water and damson or strawberry water are other options.
The Neapolitan Ice Spoon has a double use; ice bowl is for putting the mixture into the mold, and the handle is for leveling it. The boxes may be made of tin, which is less expensive than pewter. They are generally sold small enough to make single ices, but these are much more troublesome to prepare. After filling the molds, if there is no cave, "bed" the ice in the usual way.
Cake.
In Australia there is a popular cake known as Neapolitan cake or marble cake, made with the same three colors of Neapolitan ice cream swirled through in a marble pattern, usually topped with pink icing.

</doc>
<doc id="21830" url="https://en.wikipedia.org/wiki?curid=21830" title="Nature">
Nature

Nature, in the broadest sense, is the natural, physical, or material world or universe. "Nature" can refer to the phenomena of the physical world, and also to life in general. The study of nature is a large part of science. Although humans are part of nature, human activity is often understood as a separate category from other natural phenomena.
The word "nature" is derived from the Latin word "natura", or "essential qualities, innate disposition", and in ancient times, literally meant "birth". "Natura" is a Latin translation of the Greek word "physis" (φύσις), which originally related to the intrinsic characteristics that plants, animals, and other features of the world develop of their own accord. The concept of nature as a whole, the physical universe, is one of several expansions of the original notion; it began with certain core applications of the word φύσις by pre-Socratic philosophers, and has steadily gained currency ever since. This usage continued during the advent of modern scientific method in the last several centuries.
Within the various uses of the word today, "nature" often refers to geology and wildlife. Nature can refer to the general realm of living plants and animals, and in some cases to the processes associated with inanimate objects – the way that particular types of things exist and change of their own accord, such as the weather and geology of the Earth. It is often taken to mean the "natural environment" or wilderness–wild animals, rocks, forest, and in general those things that have not been substantially altered by human intervention, or which persist despite human intervention. For example, manufactured objects and human interaction generally are not considered part of nature, unless qualified as, for example, "human nature" or "the whole of nature". This more traditional concept of natural things which can still be found today implies a distinction between the natural and the artificial, with the artificial being understood as that which has been brought into being by a human consciousness or a human mind. Depending on the particular context, the term "natural" might also be distinguished from the or the supernatural.
Earth.
Earth is the only planet known to support life, and its natural features are the subject of many fields of scientific research. Within the solar system, it is third closest to the sun; it is the largest terrestrial planet and the fifth largest overall. Its most prominent climatic features are its two large polar regions, two relatively narrow temperate zones, and a wide equatorial tropical to subtropical region. Precipitation varies widely with location, from several metres of water per year to less than a millimetre. 71 percent of the Earth's surface is covered by salt-water oceans. The remainder consists of continents and islands, with most of the inhabited land in the Northern Hemisphere.
Earth has evolved through geological and biological processes that have left traces of the original conditions. The outer surface is divided into several gradually migrating tectonic plates. The interior remains active, with a thick layer of plastic mantle and an iron-filled core that generates a magnetic field. This iron core is composed of a solid inner phase, and a fluid outer phase. It is the rotation of the outer, fluid iron core that generates an electric current through dynamo action, which in turn generates a strong magnetic field.
The atmospheric conditions have been significantly altered from the original conditions by the presence of life-forms, which create an ecological balance that stabilizes the surface conditions. Despite the wide regional variations in climate by latitude and other geographic factors, the long-term average global climate is quite stable during interglacial periods, and variations of a degree or two of average global temperature have historically had major effects on the ecological balance, and on the actual geography of the Earth.
Geology.
Geology is the science and study of the solid and liquid matter that constitutes the Earth. The field of geology encompasses the study of the composition, structure, physical properties, dynamics, and history of Earth materials, and the processes by which they are formed, moved, and changed. The field is a major academic discipline, and is also important for mineral and hydrocarbon extraction, knowledge about and mitigation of natural hazards, some Geotechnical engineering fields, and understanding past climates and environments.
Geological evolution.
The geology of an area evolves through time as rock units are deposited and inserted and deformational processes change their shapes and locations.
Rock units are first emplaced either by deposition onto the surface or intrude into the overlying rock. Deposition can occur when sediments settle onto the surface of the Earth and later lithify into sedimentary rock, or when as volcanic material such as volcanic ash or lava flows, blanket the surface. Igneous intrusions such as batholiths, laccoliths, dikes, and sills, push upwards into the overlying rock, and crystallize as they intrude.
After the initial sequence of rocks has been deposited, the rock units can be deformed and/or metamorphosed. Deformation typically occurs as a result of horizontal shortening, horizontal extension, or side-to-side (strike-slip) motion. These structural regimes broadly relate to convergent boundaries, divergent boundaries, and transform boundaries, respectively, between tectonic plates.
Historical perspective.
Earth is estimated to have formed 4.54 billion years ago from the solar nebula, along with the Sun and other planets. The moon formed roughly 20 million years later. Initially molten, the outer layer of the Earth cooled, resulting in the solid crust. Outgassing and volcanic activity produced the primordial atmosphere. Condensing water vapor, most or all of which came from ice delivered by comets, produced the oceans and other water sources. The highly energetic chemistry is believed to have produced a self-replicating molecule around 4 billion years ago.
Continents formed, then broke up and reformed as the surface of Earth reshaped over hundreds of millions of years, occasionally combining to make a supercontinent. Roughly 750 million years ago, the earliest known supercontinent Rodinia, began to break apart. The continents later recombined to form Pannotia which broke apart about 540 million years ago, then finally Pangaea, which broke apart about 180 million years ago.
During the Neoproterozoic era covered much of the Earth in glaciers and ice sheets. This hypothesis has been termed the "Snowball Earth", and it is of particular interest as it precedes the Cambrian explosion in which multicellular life forms began to proliferate about 530–540 million years ago.
Since the Cambrian explosion there have been five distinctly identifiable mass extinctions. The last mass extinction occurred some 66 million years ago, when a meteorite collision probably triggered the extinction of the non-avian dinosaurs and other large reptiles, but spared small animals such as mammals. Over the past 66 million years, mammalian life diversified.
Several million years ago, a species of small African ape gained the ability to stand upright. The subsequent advent of human life, and the development of agriculture and further civilization allowed humans to affect the Earth more rapidly than any previous life form, affecting both the nature and quantity of other organisms as well as global climate. By comparison, the Great Oxygenation Event, produced by the proliferation of algae during the Siderian period, required about 300 million years to culminate.
The present era is classified as part of a mass extinction event, the Holocene extinction event, the fastest ever to have occurred. Some, such as E. O. Wilson of Harvard University, predict that human destruction of the biosphere could cause the extinction of one-half of all species in the next 100 years. The extent of the current extinction event is still being researched, debated and calculated by biologists.
Atmosphere, climate, and weather.
The Earth's atmosphere is a key factor in sustaining the ecosystem. The thin layer of gases that envelops the Earth is held in place by gravity. Air is mostly nitrogen, oxygen, water vapor, with much smaller amounts of carbon dioxide, argon, etc. The atmospheric pressure declines steadily with altitude. The ozone layer plays an important role in depleting the amount of ultraviolet (UV) radiation that reaches the surface. As DNA is readily damaged by UV light, this serves to protect life at the surface. The atmosphere also retains heat during the night, thereby reducing the daily temperature extremes.
Terrestrial weather occurs almost exclusively in the lower part of the atmosphere, and serves as a convective system for redistributing heat. Ocean currents are another important factor in determining climate, particularly the major underwater thermohaline circulation which distributes heat energy from the equatorial oceans to the polar regions. These currents help to moderate the differences in temperature between winter and summer in the temperate zones. Also, without the redistributions of heat energy by the ocean currents and atmosphere, the tropics would be much hotter, and the polar regions much colder.
Weather can have both beneficial and harmful effects. Extremes in weather, such as tornadoes or hurricanes and cyclones, can expend large amounts of energy along their paths, and produce devastation. Surface vegetation has evolved a dependence on the seasonal variation of the weather, and sudden changes lasting only a few years can have a dramatic effect, both on the vegetation and on the animals which depend on its growth for their food.
Climate is a measure of the long-term trends in the weather. Various factors are known to influence the climate, including ocean currents, surface albedo, greenhouse gases, variations in the solar luminosity, and changes to the Earth's orbit. Based on historical records, the Earth is known to have undergone drastic climate changes in the past, including ice ages.
The climate of a region depends on a number of factors, especially latitude. A latitudinal band of the surface with similar climatic attributes forms a climate region. There are a number of such regions, ranging from the tropical climate at the equator to the polar climate in the northern and southern extremes. Weather is also influenced by the seasons, which result from the Earth's axis being tilted relative to its orbital plane. Thus, at any given time during the summer or winter, one part of the Earth is more directly exposed to the rays of the sun. This exposure alternates as the Earth revolves in its orbit. At any given time, regardless of season, the northern and southern hemispheres experience opposite seasons.
Weather is a chaotic system that is readily modified by small changes to the environment, so accurate weather forecasting is limited to only a few days. Overall, two things are happening worldwide: (1) temperature is increasing on the average; and (2) regional climates have been undergoing noticeable changes.
Water on Earth.
Water is a chemical substance that is composed of hydrogen and oxygen and is vital for all known forms of life. In typical usage, "water" refers only to its liquid form or state, but the substance also has a solid state, ice, and a gaseous state, water vapor, or steam. 21 kilograms) of water-->Water covers 71% of the Earth's surface. On Earth, it is found mostly in oceans and other large water bodies, with 1.6% of water below ground in aquifers and 0.001% in the air as vapor, clouds, and precipitation. Oceans hold 97% of surface water, glaciers, and polar ice caps 2.4%, and other land surface water such as rivers, lakes, and ponds 0.6%. Additionally, a minute amount of the Earth's water is contained within biological bodies and manufactured products.
Oceans.
An ocean is a major body of saline water, and a principal component of the hydrosphere. Approximately 71% of the Earth's surface (an area of some 361 million square kilometers) is covered by ocean, a continuous body of water that is customarily divided into several principal oceans and smaller seas. More than half of this area is over deep. Average oceanic salinity is around 35 parts per thousand (ppt) (3.5%), and nearly all seawater has a salinity in the range of 30 to 38 ppt. Though generally recognized as several 'separate' oceans, these waters comprise one global, interconnected body of salt water often referred to as the World Ocean or global ocean. This concept of a global ocean as a continuous body of water with relatively free interchange among its parts is of fundamental importance to oceanography.
The major oceanic divisions are defined in part by the continents, various archipelagos, and other criteria: these divisions are (in descending order of size) the Pacific Ocean, the Atlantic Ocean, the Indian Ocean, the Southern Ocean, and the Arctic Ocean. Smaller regions of the oceans are called seas, gulfs, bays and other names. There are also salt lakes, which are smaller bodies of landlocked saltwater that are not interconnected with the World Ocean. Two notable examples of salt lakes are the Aral Sea and the Great Salt Lake.
Lakes.
A lake (from Latin "lacus") is a terrain feature (or physical feature), a body of liquid on the surface of a world that is localized to the bottom of basin (another type of landform or terrain feature; that is, it is not global) and moves slowly if it moves at all. On Earth, a body of water is considered a lake when it is inland, not part of the ocean, is larger and deeper than a pond, and is fed by a river. The only world other than Earth known to harbor lakes is Titan, Saturn's largest moon, which has lakes of ethane, most likely mixed with methane. It is not known if Titan's lakes are fed by rivers, though Titan's surface is carved by numerous river beds. Natural lakes on Earth are generally found in mountainous areas, rift zones, and areas with ongoing or recent glaciation. Other lakes are found in endorheic basins or along the courses of mature rivers. In some parts of the world, there are many lakes because of chaotic drainage patterns left over from the last Ice Age. All lakes are temporary over geologic time scales, as they will slowly fill in with sediments or spill out of the basin containing them.
Ponds.
A pond is a body of standing water, either natural or man-made, that is usually smaller than a lake. A wide variety of man-made bodies of water are classified as ponds, including water gardens designed for aesthetic ornamentation, fish ponds designed for commercial fish breeding, and solar ponds designed to store thermal energy. Ponds and lakes are distinguished from streams via current speed. While currents in streams are easily observed, ponds and lakes possess thermally driven micro-currents and moderate wind driven currents. These features distinguish a pond from many other aquatic terrain features, such as stream pools and tide pools.
Rivers.
A river is a natural watercourse, usually freshwater, flowing toward an ocean, a lake, a sea or another river. In a few cases, a river simply flows into the ground or dries up completely before reaching another body of water. Small rivers may also be called by several other names, including stream, creek, brook, rivulet, and rill; there is no general rule that defines what can be called a river. Many names for small rivers are specific to geographic location; one example is "Burn" in Scotland and North-east England. Sometimes a river is said to be larger than a creek, but this is not always the case, due to vagueness in the language. A river is part of the hydrological cycle. Water within a river is generally collected from precipitation through surface runoff, groundwater recharge, springs, and the release of stored water in natural ice and snowpacks (i.e., from glaciers).
Streams.
A stream is a flowing body of water with a current, confined within a bed and stream banks. In the United States, a stream is classified as a watercourse less than wide. Streams are important as conduits in the water cycle, instruments in groundwater recharge, and they serve as corridors for fish and wildlife migration. The biological habitat in the immediate vicinity of a stream is called a riparian zone. Given the status of the ongoing Holocene extinction, streams play an important corridor role in connecting fragmented habitats and thus in conserving biodiversity. The study of streams and waterways in general involves many branches of inter-disciplinary natural science and engineering, including hydrology, fluvial geomorphology, aquatic ecology, fish biology, riparian ecology, and others.
Ecosystems.
Ecosystems are composed of a variety of abiotic and biotic components that function in an interrelated way. The structure and composition is determined by various environmental factors that are interrelated. Variations of these factors will initiate dynamic modifications to the ecosystem. Some of the more important components are: soil, atmosphere, radiation from the sun, water, and living organisms.
Central to the ecosystem concept is the idea that living organisms interact with every other element in their local environment. Eugene Odum, a founder of ecology, stated: "Any unit that includes all of the organisms (ie: the "community") in a given area interacting with the physical environment so that a flow of energy leads to clearly defined trophic structure, biotic diversity, and material cycles (i.e.: exchange of materials between living and nonliving parts) within the system is an ecosystem." Within the ecosystem, species are connected and dependent upon one another in the food chain, and exchange energy and matter between themselves as well as with their environment. The human ecosystem concept is grounded in the deconstruction of the human/nature dichotomy and the premise that all species are ecologically integrated with each other, as well as with the abiotic constituents of their biotope.
A smaller unit of size is called a microecosystem. For example, a microsystem can be a stone and all the life under it. A "macroecosystem" might involve a whole ecoregion, with its drainage basin.
Wilderness.
Wilderness is generally defined as areas that have not been significantly modified by human activity. Wilderness areas can be found in preserves, estates, farms, conservation preserves, ranches, national forests, national parks, and even in urban areas along rivers, gulches, or otherwise undeveloped areas. Wilderness areas and protected parks are considered important for the survival of certain species, ecological studies, conservation, solitude, and recreation. Some nature writers believe wilderness areas are vital for the human spirit and creativity, and some Ecologists consider wilderness areas to be an integral part of the Earth's self-sustaining natural ecosystem (the biosphere). They may also preserve historic genetic traits and that they provide habitat for wild flora and fauna that may be difficult to recreate in zoos, arboretums, or laboratories.
Life.
Although there is no universal agreement on the definition of life, scientists generally accept that the biological manifestation of life is characterized by organization, metabolism, growth, adaptation, and response to stimuli and reproduction. Life may also be said to be simply the characteristic state of organisms.
Properties common to terrestrial organisms (plants, animals, fungi, protists, archaea, and bacteria) are that they are cellular, carbon-and-water-based with complex organization, having a metabolism, a capacity to grow, respond to stimuli, and reproduce. An entity with these properties is generally considered life. However, not every definition of life considers all of these properties to be essential. Human-made analogs of life may also be considered to be life.
The biosphere is the part of Earth's outer shell – including land, surface rocks, water, air and the atmosphere – within which life occurs, and which biotic processes in turn alter or transform. From the broadest geophysiological point of view, the biosphere is the global ecological system integrating all living beings and their relationships, including their interaction with the elements of the lithosphere (rocks), hydrosphere (water), and atmosphere (air). The entire Earth contains over 75 billion tons (150 "trillion" pounds or about 6.8×1013 kilograms) of biomass (life), which lives within various environments within the biosphere.
Over nine-tenths of the total biomass on Earth is plant life, on which animal life depends very heavily for its existence. More than 2 million species of plant and animal life have been identified to date, and estimates of the actual number of existing species range from several million to well over 50 million. The number of individual species of life is constantly in some degree of flux, with new species appearing and others ceasing to exist on a continual basis. The total number of species is in rapid decline.
Evolution.
The origin of life on Earth is not well understood, but it is known to have occurred at least 3.5 billion years ago, during the hadean or archean eons on a primordial Earth that had a substantially different environment than is found at present. These life forms possessed the basic traits of self-replication and inheritable traits. Once life had appeared, the process of evolution by natural selection resulted in the development of ever-more diverse life forms.
Species that were unable to adapt to the changing environment and competition from other life forms became extinct. However, the fossil record retains evidence of many of these older species. Current fossil and DNA evidence shows that all existing species can trace a continual ancestry back to the first primitive life forms.
The advent of photosynthesis in very basic forms of plant life worldwide allowed the sun's energy to be harvested to create conditions allowing for more complex life. The resultant oxygen accumulated in the atmosphere and gave rise to the ozone layer. The incorporation of smaller cells within larger ones resulted in the development of yet more complex cells called eukaryotes. Cells within colonies became increasingly specialized, resulting in true multicellular organisms. With the ozone layer absorbing harmful ultraviolet radiation, life colonized the surface of Earth.
Microbes.
The first form of life to develop on the Earth were microbes, and they remained the only form of life until about a billion years ago when multi-cellular organisms began to appear. Microorganisms are single-celled organisms that are generally microscopic, and smaller than the human eye can see. They include Bacteria, Fungi, Archaea, and Protista.
These life forms are found in almost every location on the Earth where there is liquid water, including in the Earth's interior.
Their reproduction is both rapid and profuse. The combination of a high mutation rate and a horizontal gene transfer ability makes them highly adaptable, and able to survive in new environments, including outer space. They form an essential part of the planetary ecosystem. However, some microorganisms are pathogenic and can post health risk to other organisms.
Plants and Animals.
Originally Aristotle divided all living things between plants, which generally do not move fast enough for humans to notice, and animals. In Linnaeus' system, these became the kingdoms Vegetabilia (later Plantae) and Animalia. Since then, it has become clear that the Plantae as originally defined included several unrelated groups, and the fungi and several groups of algae were removed to new kingdoms. However, these are still often considered plants in many contexts. Bacterial life is sometimes included in flora, and some classifications use the term "bacterial flora" separately from "plant flora".
Among the many ways of classifying plants are by regional floras, which, depending on the purpose of study, can also include "fossil flora", remnants<br> of plant life from a previous era. People in many regions and countries take great pride in their individual arrays of characteristic flora, which can vary widely across the globe due to differences in climate and terrain.
Regional floras commonly are divided into categories such as "native flora" and "agricultural and garden flora", the lastly mentioned of which are intentionally grown and cultivated. Some types of "native flora" actually have been introduced centuries ago by people migrating from one region or continent to another, and become an integral part of the native, or natural flora of the place to which they were introduced. This is an example of how human interaction with nature can blur the boundary of what is considered nature.
Another category of plant has historically been carved out for "weeds". Though the term has fallen into disfavor among botanists as a formal way to categorize "useless" plants, the informal use of the word "weeds" to describe those plants that are deemed worthy of elimination is illustrative of the general tendency of people and societies to seek to alter or shape the course of nature. Similarly, animals are often categorized in ways such as "domestic", "farm animals", "wild animals", "pests", etc. according to their relationship to human life.
Animals as a category have several characteristics that generally set them apart from other living things. Animals are eukaryotic and usually multicellular (although see Myxozoa), which separates them from bacteria, archaea, and most protists. They are heterotrophic, generally digesting food in an internal chamber, which separates them from plants and algae. They are also distinguished from plants, algae, and fungi by lacking cell walls.
With a few exceptions, most notably the sponges (Phylum Porifera), animals have bodies differentiated into separate tissues. These include muscles, which are able to contract and control locomotion, and a nervous system, which sends and processes signals. There is also typically an internal digestive chamber. The eukaryotic cells possessed by all animals are surrounded by a characteristic extracellular matrix composed of collagen and elastic glycoproteins. This may be calcified to form structures like shells, bones, and spicules, a framework upon which cells can move about and be reorganized during development and maturation, and which supports the complex anatomy required for mobility.
Human interrelationship.
Although humans comprise only a minuscule proportion of the total living biomass on Earth, the human effect on nature is disproportionately large. Because of the extent of human influence, the boundaries between what humans regard as nature and "made environments" is not clear cut except at the extremes. Even at the extremes, the amount of natural environment that is free of discernible human influence is diminishing at an increasingly rapid pace.
The development of technology by the human race has allowed the greater exploitation of natural resources and has helped to alleviate some of the risk from natural hazards. In spite of this progress, however, the fate of human civilization remains closely linked to changes in the environment. There exists a highly complex feedback loop between the use of advanced technology and changes to the environment that are only slowly becoming understood. Man-made threats to the Earth's natural environment include pollution, deforestation, and disasters such as oil spills. Humans have contributed to the extinction of many plants and animals.
Humans employ nature for both leisure and economic activities. The acquisition of natural resources for industrial use remains the primary component of the world's economic system. Some activities, such as hunting and fishing, are used for both sustenance and leisure, often by different people. Agriculture was first adopted around the 9th millennium BCE. Ranging from food production to energy, nature influences economic wealth.
Although early humans gathered uncultivated plant materials for food and employed the medicinal properties of vegetation for healing, most modern human use of plants is through agriculture. The clearance of large tracts of land for crop growth has led to a significant reduction in the amount available of forestation and wetlands, resulting in the loss of habitat for many plant and animal species as well as increased erosion.
Aesthetics and beauty.
Beauty in nature has historically been a prevalent theme in art and books, filling large sections of libraries and bookstores. That nature has been depicted and celebrated by so much art, photography, poetry, and other literature shows the strength with which many people associate nature and beauty. Reasons why this association exists, and what the association consists of, are studied by the branch of philosophy called aesthetics. Beyond certain basic characteristics that many philosophers agree about to explain what is seen as beautiful, the opinions are virtually endless. Nature and wildness have been important subjects in various eras of world history. An early tradition of landscape art began in China during the Tang Dynasty (618–907). The tradition of representing nature "as it is" became one of the aims of Chinese painting and was a significant influence in Asian art.
Although natural wonders are celebrated in the Psalms and the Book of Job, wilderness portrayals in art became more prevalent in the 1800s, especially in the works of the Romantic movement. British artists John Constable and J. M. W. Turner turned their attention to capturing the beauty of the natural world in their paintings. Before that, paintings had been primarily of religious scenes or of human beings. William Wordsworth's poetry described the wonder of the natural world, which had formerly been viewed as a threatening place. Increasingly the valuing of nature became an aspect of Western culture. This artistic movement also coincided with the Transcendentalist movement in the Western world. A common classical idea of beautiful art involves the word mimesis, the imitation of nature. Also in the realm of ideas about beauty in nature is that the perfect is implied through perfect mathematical forms and more generally by patterns in nature. As David Rothenburg writes, "The beautiful is the root of science and the goal of art, the highest possibility that humanity can ever hope to see".
Matter and energy.
Some fields of science see nature as matter in motion, obeying certain laws of nature which science seeks to understand. For this reason the most fundamental science is generally understood to be "physics" – the name for which is still recognizable as meaning that it is the study of nature.
Matter is commonly defined as the substance of which physical objects are composed. It constitutes the observable universe. The visible components of the universe are now believed to compose only 4.9 percent of the total mass. The remainder is believed to consist of 26.8 percent cold dark matter and 68.3 percent dark energy. The exact arrangement of these components is still unknown and is under intensive investigation by physicists.
The behavior of matter and energy throughout the observable universe appears to follow well-defined physical laws. These laws have been employed to produce cosmological models that successfully explain the structure and the evolution of the universe we can observe. The mathematical expressions of the laws of physics employ a set of twenty physical constants that appear to be static across the observable universe. The values of these constants have been carefully measured, but the reason for their specific values remains a mystery.
Beyond Earth.
Outer space, also simply called "space", refers to the relatively empty regions of the universe outside the atmospheres of celestial bodies. "Outer" space is used to distinguish it from airspace (and terrestrial locations). There is no discrete boundary between the Earth's atmosphere and space, as the atmosphere gradually attenuates with increasing altitude. Outer space within the Solar System is called interplanetary space, which passes over into interstellar space at what is known as the heliopause.
Outer space is sparsely filled with several dozen types of organic molecules discovered to date by microwave spectroscopy, blackbody radiation left over from the big bang and the origin of the universe, and cosmic rays, which include ionized atomic nuclei and various subatomic particles. There is also some gas, plasma and dust, and small meteors. Additionally, there are signs of human life in outer space today, such as material left over from previous manned and unmanned launches which are a potential hazard to spacecraft. Some of this debris re-enters the atmosphere periodically.
Although the Earth is the only body within the solar system known to support life, evidence suggests that in the distant past the planet Mars possessed bodies of liquid water on the surface. For a brief period in Mars' history, it may have also been capable of forming life. At present though, most of the water remaining on Mars is frozen.
If life exists at all on Mars, it is most likely to be located underground where liquid water can still exist.
Conditions on the other terrestrial planets, Mercury and Venus, appear to be too harsh to support life as we know it. But it has been conjectured that Europa, the fourth-largest moon of Jupiter, may possess a sub-surface ocean of liquid water and could potentially host life.
Astronomers have started to discover extrasolar Earth analogs – planets that lie in the habitable zone of space surrounding a star, and therefore could possibly host life as we know it.
See also.
Media:
Organizations:
Science:
Philosophy:

</doc>
<doc id="21833" url="https://en.wikipedia.org/wiki?curid=21833" title="New moon">
New moon

In astronomy, new moon is the first phase of the Moon, when it orbits as seen from the Earth, the moment when the Moon and the Sun have the same ecliptical longitude.
The Moon is not visible at this time except when it is seen in silhouette during a solar eclipse when it is illuminated by earthshine. See the article on phases of the Moon for further details.
A lunation or synodic month is the mean (average) time from one new moon to the next. 
In the J2000.0 epoch, the average length of a lunation is 29.530588 days (or 29 days, 12 hours, 44 minutes, and 3 seconds). However, the length of any one synodic month can vary from 29.26 to 29.80 days due to the perturbing effects of the Sun's gravity on the Moon's eccentric orbit. 
In a lunar calendar, each month corresponds to a lunation. Each lunar cycle can be assigned a unique Lunation Number to identify it.
Determining new moons: an approximate formula.
The length of a lunation is about 29.53 days. Its precise duration is linked to many phenomena in nature, such as the variation between (the extreme highest and lowest tides, respectively).
An approximate formula to compute the mean moments of new moon (conjunction between Sun and Moon) for successive months is:
where N is an integer, starting with 0 for the first new moon in the year 2000, and that is incremented by 1 for each successive synodic month; and the result d is the number of days (and fractions) since 2000-01-01 00:00:00 reckoned in the time scale known as Terrestrial Time (TT) used in ephemerides.
To obtain this moment expressed in Universal Time (UT, world clock time), add the result of following approximate correction to the result d obtained above:
Periodic perturbations change the time of true conjunction from these mean values. For all new moons between 1601 and 2401, the maximum difference is 0.592 days = 14h13m in either direction. The duration of a lunation ("i.e." the time from new moon to the next new moon) varies in this period between 29.272 and 29.833 days, i.e. −0.259d = 6h12m shorter, or +0.302d = 7h15m longer than average. This range is smaller than the difference between mean and true conjunction, because during one lunation the periodic terms cannot all change to their maximum opposite value.
See the article on the full moon cycle for a fairly simple method to compute the moment of new moon more accurately.
The long-term error of the formula is approximately: 1 cy2 seconds in TT, and 11 cy2 seconds in UT ("cy" is centuries since 2000; see section "Explanation of the formulae" for details.)
Explanation of the formula.
The moment of mean conjunction can easily be computed from an expression for the mean ecliptical longitude of the Moon minus the mean ecliptical longitude of the Sun (Delauney parameter D). Jean Meeus gave formulae to compute this in his popular "Astronomical Formulae for Calculators" based on the ephemerides of Brown and Newcomb (ca. 1900); and in his 1st edition of "Astronomical Algorithms" based on the ELP2000-85 (the 2nd edition uses ELP2000-82 with improved expressions from Chapront "et al." in 1998). These are now outdated: Chapront "et al." (2002) published improved parameters. Also Meeus's formula uses a fractional variable to allow computation of the four main phases, and uses a second variable for the secular terms. For the convenience of the reader, the formula given above is based on Chapront's latest parameters and expressed with a single integer variable, and the following additional terms have been added:
constant term:
quadratic term:
The theoretical tidal contribution to ΔT is about +42 s/cy2 the smaller observed value is thought to be mostly due to changes in the shape of the Earth. Because the discrepancy is not fully explained, uncertainty of our prediction of UT (rotation angle of the Earth) may be as large as the difference between these values: 11 s/cy2. The error in the position of the Moon itself is only maybe 0.5"/cy2, or (because the apparent mean angular velocity of the Moon is about 0.5"/s), 1 s/cy2 in the time of conjunction with the Sun.
Lunation Number.
The Lunation Number or Lunation Cycle is a number given to each lunation beginning from a certain one in history. Several conventions are in use.
The most commonly used is the Brown Lunation Number (BLN), which defines lunation 1 as beginning at the first new moon of 1923, the year when Ernest William Brown's lunar theory was introduced in the major national astronomical almanacs. Lunation 1 occurred at approximately 02:41 UTC, January 17, 1923. New moons occur on Julian Dates
formula_3, with the given uncertainty due to varying torques from the sun.
Another increasingly popular lunation number (simply called the Lunation Number), introduced by Jean Meeus, defines lunation 0 as beginning on the first new moon of 2000 (this occurred at approximately 18:14 UTC, January 6, 2000). The formula relating this Lunation Number with the Brown Lunation Number is: BLN = LN + 953.
The Goldstine Lunation Number refers to the lunation numbering used by Herman Goldstine in his 1973 book "New and Full Moons: 1001 B.C. to A.D. 1651", with lunation 0 beginning on January 11, 1001 BC, and can be calculated using GLN = LN + 37105.
The Hebrew Lunation Number is the count of lunations in the Hebrew calendar with lunation 1 beginning on October 7, 3761 BC. It can be calculated using HLN = LN + 71234.
The Islamic Lunation Number is the count of lunations in the Islamic calendar with lunation 1 as beginning on July 16, 622. It can be calculated using ILN = LN + 17038.
The Thai Lunation Number is called "มาสเกณฑ์" (Maasa-Kendha), defines lunation 0 as beginning of the SouthEast-Asian Calendar on Sunday March 22, 638 (Julian Calendar). It can be calculated using TLN = LN + 16843.
Lunar calendars.
In non-astronomical contexts, "new moon" refers to the first visible crescent of the Moon, after conjunction with the Sun. This takes place over the western horizon in a brief period between sunset and moonset, and therefore the precise time and even the date of the appearance of the new moon by this definition will be influenced by the geographical location of the observer. The astronomical new moon, sometimes known as the "dark moon" to avoid confusion, occurs by definition at the moment of conjunction in ecliptical longitude with the Sun, when the Moon is invisible from the Earth. This moment is unique and does not depend on location, and in certain circumstances it coincides with a solar eclipse.
In the above meaning , the first crescent marks the beginning of the month in lunar calendars such as the Muslim calendar, and in lunisolar calendars such as the Hebrew calendar, Hindu calendars, and Buddhist calendar. In the Chinese calendar, the beginning of the month is marked by the dark moon. The new moon is also important in astrology, as is the full moon.
Hindu calendar.
The new moon is significant in the Hindu calendar. People generally wait for the new moon to begin projects, as the waxing period of the moon is considered to be favorable for new work. 
There are fifteen moon dates for each of the waxing and waning periods. These fifteen dates divided evenly into five categories: Nanda, Bhadra, Jaya, Rikta, and Purna, which are cycled through in that order. 
Nanda dates are considered to be favorable for auspicious works; Bhadra dates for works related with community, social, family, friends; and Jaya dates for dealing with conflict. Rikta dates are considered beneficial only for works related to cruelty. Purna dates are considered to be favorable for all work. 
The first day of the Lunar Hindu calendar starts the day after the new moon (Amavasya), which is considered a powerful force for good or evil. The Hindu epic Mahabharatha states that the Kurukshetra War started this day, which was also a Tuesday (Mangalvaar, day of the week named after Mars).
Islamic calendar.
The Islamic calendar has retained an observational definition of the new moon, marking the new month when the first crescent moon is actually seen, and making it impossible to be certain in advance of when a specific month will begin (in particular, the exact date on which Ramadan will begin is not known in advance). In Saudi Arabia, the new King Abdullah Centre for Crescent Observations and Astronomy in Makkah has a clock for addressing this as an international scientific project. In Pakistan, there is a "Central Ruet-e-Hilal Committee" whose head is Mufti Muneeb-ur-Rehman, assisted by 150 observatories of the Pakistan Meteorological Department, which announces the sighting of the new moon. Since its creation in 1974, the status of the Central Ruet-e-Hilal Committee has been controversial as it refused the "Witnesses" (Shahadats) from other sects. In Iran a special committee receives observations of every new moon to determine the beginning of each month. This committee uses one hundred groups of observers.
An attempt to unify Muslims on a scientifically calculated worldwide calendar was adopted by both the Fiqh Council of North America and the European Council for Fatwa and Research in 2007. The new calculation requires that conjunction occur before sunset in Mecca, Saudi Arabia and that moonset on the following day must take place after sunset. These can be precisely calculated and therefore a unified calendar is imminent if it becomes adopted worldwide.
Chinese calendar.
The new moon is the beginning of the month in the Chinese calendar. Some Buddhist Chinese keep a vegetarian diet on the new moon and full moon each month. A new moon is when the sun comes out.
Jewish calendar.
The new moon signifies the start of every Jewish month, and is considered an important date and minor holiday in the Hebrew calendar. The modern form of the calendar is a rule-based lunisolar calendar, akin to the Chinese calendar, measuring months defined in lunar cycles as well as years measured in solar cycles, and distinct from the purely lunar Islamic calendar and the almost entirely solar Gregorian calendar. According to Jewish tradition, the Jewish calendar is calculated based on mathematical rules handed down from God to Moses at the moment the command was given to make sure that Passover always falls in the springtime. More likely, this fixed lunisolar calendar was introduced by Hillel II. This calculation makes use of a mean lunation length used by Ptolemy and handed down from Babylonians (see Lunar theory#Babylon), which is still very accurate: ca. 29.530594 days vs. a present value (see ) of 29.530589 days. This difference of only 0.0000005, or five millionths of a day, adds up to about only four hours since Babylonian times.
The messianic Pentecostal group, the New Israelites of Peru, keeps the new moon as a Sabbath of rest. As an evangelical church, it follows the Bible's teachings that God sanctified the seventh-day Sabbath, and the new moons in addition to it. See Ezekiel 46:1, 3. No work may be done from dusk until dusk, and the services run for 11 hours, although a large number spend 24 hours within the gates of the temples, sleeping and singing praises throughout the night.
Bahá'í calendar.
In the Bahá'í Faith, effective from 2015 onwards, the "Twin Holy Birthdays", referring to two successive holy days in the Bahá'í calendar (the birth of the Báb and the birth of Bahá'u'lláh), will be observed on the first and the second day following the occurrence of the eighth new moon after Naw-Rúz (Bahá'í New Year), as determined in advance by astronomical tables using Tehran as the point of reference. This will result in the observance of the Twin Birthdays moving, year to year, from mid-October to mid-November according to the Gregorian calendar.

</doc>
<doc id="21834" url="https://en.wikipedia.org/wiki?curid=21834" title="B-spline">
B-spline

In the mathematical subfield of numerical analysis, a B-spline, or basis spline, is a spline function that has minimal support with respect to a given degree, smoothness, and domain partition. Any spline function of given degree can be expressed as a linear combination of B-splines of that degree. Cardinal B-splines have knots that are equidistant from each other. B-splines can be used for curve-fitting and numerical differentiation of experimental data.
In the computer-aided design and computer graphics, spline functions are constructed as linear combinations of B-splines with a set of control points.
Introduction.
The term "B-spline" was coined by Isaac Jacob Schoenberg and is short for basis spline. A spline function is a piecewise polynomial function of degree <"k" in a variable "x". The places where the pieces meet are known as knots. The number of internal knots must be equal to, or greater than "k"-1. Thus the spline function has limited support. The key property of spline functions is that they are continuous at the knots. Some derivatives of the spline function may also be continuous, depending on whether the knots are distinct or not. A fundamental theorem states that every spline function of a given degree, smoothness, and domain partition, can be uniquely represented as a linear combination of B-splines of that same degree and smoothness, and over that same partition.
Definition.
A B-spline of order "n" is a piecewise polynomial function of degree <"n" in a variable "x". It is defined over a domain "t" 0 ≤ "x" ≤ "t"m, m = n. The points where "x" = "t" j are known as knots or break-points. The number of internal knots is equal to the degree of the polynomial if there are no knot multiplicities. The knots must be in ascending order. The number of knots is the minimum for the degree of the B-spline, which has a non-zero value only in the range between the first and last knot. Each piece of the function is a polynomial of degree "<n" between and including adjacent knots. A B-spline is a continuous function at the knots. When all internal knots are distinct its derivatives are also continuous up to the derivative of degree "n"-1. If internal knots are coincident at a given value of "x", the continuity of derivative order is reduced by 1 for each additional knot.
For any given set of knots, the B-spline is unique, hence the name, B being short for Basis. The usefulness of B-splines lies in the fact that any spline function of order "n" on a given set of knots can be expressed as a linear combination of B-splines:
This follows from the fact that all pieces have the same continuity properties, within their individual range of support, at the knots.
Expressions for the polynomial pieces can be derived by means of the Cox-de Boor recursion formula
That is, formula_4 is piecewise constant one or zero indicating which knot span "x" is in (zero if knot span "j" is repeated). The recursion equation is in two parts: 
ramps from zero to one as "x" goes from formula_6 to formula_7 and
ramps from one to zero as "x" goes from formula_9 to formula_10. The corresponding "B"s are zero outside those respective ranges. For example, formula_11 is a triangular function that is zero below formula_12, ramps to one at formula_13 and back to zero at and beyond formula_14. However, because B-spline basis functions have local support, B-splines are typically computed by algorithms that do not need to evaluate basis functions where they are zero, such as de Boor's algorithm.
This relation leads directly to the FORTRAN-coded algorithm BSPLV which generates values of the B-splines of order "n" at "x". The following scheme illustrates how each piece of degree "n" is a linear combination of the pieces of B-splines of degree "n"-1 to its left.
Application of the recursion formula with the knots at 0, 1, 2, and 3 gives the pieces of the uniform B-spline of degree 2 
These pieces are shown in the diagram. The continuity property of a quadratic spline function and its first derivative at the internal knots are illustrated, as follows
The second derivative of a B-spline of degree 2 is discontinuous at the knots:
Faster variants of the de Boor algorithm have been proposed but they suffer from comparatively lower stability.
Cardinal B-spline.
A cardinal B-spline has a constant separation, "h", between knots. The cardinal B-splines for a given degree "n" are just shifted copies of each other. They can be obtained from the simpler definition.
The "placeholder" notation is used to indicate that the "n"th divided difference of the function formula_23 of the two variables "t" and "x" is to be taken by fixing "x" and considering formula_24 as a function of "t" alone.
A cardinal B-spline has uniform spaced knots, therefore interpolation between the knots equals convolution with a smoothing kernel.
Example, if we want to interpolate three values inbetween B-spline nodes (formula_25), we can write the signal as:
formula_26
Convolution of the signal formula_27 with a rectangle function formula_28 gives first order interpolated b-spline values. Second-order B-spline interpolation is convolution with a rectangle function twiceformula_29, by iterative filtering with a rectangle function higher order interpolation is obtained.
Fast b-spline interpolation on a uniform sample domain can be done by iterative mean-filtering. Alternative, a rectangle function equals Sinc in Fourier domain. Therefore cubic spline interpolation equals multiplying the signal in Fourier domain with Sinc^4.
See Irwin–Hall distribution#Special cases for algebraic expressions for the cardinal B-splines of degree 1-4.
P-spline.
The term P-spline stands for "penalized B-spline". It refers to using the B-spline representation where the coefficients are determined partly by the data to be fitted, and partly by an additional penalty function that aims to impose smoothness to avoid overfitting.
Derivative expressions.
The derivative of a B-spline of degree "k" is simply a function of B-splines of degree "k"-1.
This implies that
which shows that there is a simple relationship between the derivative of a spline function and the B-splines of degree one less.
Relationship to piecewise/composite Bézier.
A piecewise/composite Bézier curve is a series of Bézier curves joined with at least C0 continuity (the last point of one curve coincides with the starting point of the next curve). Depending on the application, additional smoothness requirements (such as C1 or C2 continuity) may be added. C1 continuous curves have identical tangents at the breakpoint (where the two curves meet). C2 continuous curves have identical curvature at the breakpoint.
To gain C2 continuity the Bézier Curve loses local control, because to enforce C2 continuity the control points are dependent on each other. If a single control point moves, the whole spline needs to be re-evaluated. B-Splines have both C2 continuity and local control, but they lose the interpolation property of a piecewise Bézier.
Curve fitting.
Usually in curve fitting, a set of data points is fitted with a curve defined by some mathematical function. For example common types of curve fitting use a polynomial or a set of exponential functions. When there is no theoretical basis for choosing a fitting function, the curve may be fitted with a spline function composed of a sum of B-splines, using the method of least squares. Thus, the objective function for least squares minimization is, for a spline function of degree "k",
"W(x)" is a weight and "y(x)" is the datum value at "x". The coefficients formula_33 are the parameters to be determined. The knot values may be fixed or they too may be treated as parameters.
The main difficulty in applying this process is in determining the number of knots to use and where they should be placed. de Boor suggests various strategies to address this problem. For instance, the spacing between knots is decreased in proportion to the curvature (2nd. derivative) of the data. A few applications have been published. For instance, the use of B-splines for fitting single Lorentzian and Gaussian curves has been investigated. Optimal spline functions of degrees 3-7 inclusive, based on symmetric arrangements of 5, 6, and 7 knots, have been computed and the method was applied for smoothing and differentiation of spectroscopic curves. In a comparable study, the two-dimensional version of the Savitzky-Golay filtering and the spline method produced better results than moving average or Chebyshev filtering.
NURBS.
In computer aided design, computer aided manufacturing, and computer graphics, a powerful extension of B-splines is non-uniform rational B-splines (NURBS). NURBS are essentially B-splines in homogeneous coordinates. Like B-splines, they are defined by their order, and a knot vector, and a set of control points, but unlike simple B-splines, the control points each have a weight. When the weight is equal to 1, a NURBS is simply a B-spline and as such NURBS generalizes both B-splines and Bézier curves and surfaces, the primary difference being the weighting of the control points which makes NURBS curves "rational".
By evaluating a NURBS at various values of the parameters, the curve can be traced through space; likewise, by evaluating a NURBS surface at various values of the two parameters, the surface can be represented in Cartesian space.
Like B-splines, NURBS control points determine the shape of the curve. Each point of the curve is computed by taking a weighted sum of a number of control points. The weight of each point varies according to the governing parameter. For a curve of degree "d", the influence of any control point is only nonzero in "d"+1 intervals (knot spans) of the parameter space. Within those intervals, the weight changes according to a polynomial function (basis functions) of degree "d". At the boundaries of the intervals, the basis functions go smoothly to zero, the smoothness being determined by the degree of the polynomial.
The knot vector is a sequence of parameter values that determines where and how the control points affect the NURBS curve. The number of knots is always equal to the number of control points plus curve degree plus one. Each time the parameter value enters a new knot span, a new control point becomes active, while an old control point is discarded.
A NURBS curve takes the following form:
Here the notation is as follows. "u" is the independent variable (instead of "x"), "k" is the number of control points, "N" is a B-spline (used instead of "B"), "n" is the polynomial degree, "P" is a control point and "w" is a weight. The denominator is a normalizing factor that evaluates to one if all weights are one.
It is customary to write this as
in which the functions
are known as the rational basis functions.
A NURBS surface is obtained as the tensor product of two NURBS curves, thus using two independent parameters "u" and "v" (with indices "i" and "j" respectively):
with
as rational basis functions.

</doc>
<doc id="21836" url="https://en.wikipedia.org/wiki?curid=21836" title="North Pole">
North Pole

The North Pole, also known as the Geographic North Pole or Terrestrial North Pole, is (subject to the caveats explained below) defined as the point in the Northern Hemisphere where the Earth's axis of rotation meets its surface. It should not be confused with the North Magnetic Pole.
The North Pole is the northernmost point on the Earth, lying diametrically opposite the South Pole. It defines geodetic latitude 90° North, as well as the direction of true north. At the North Pole all directions point south; all lines of longitude converge there, so its longitude can be defined as any degree value. Along tight latitude circles, counterclockwise is east and clockwise is west.
While the South Pole lies on a continental land mass, the North Pole is located in the middle of the Arctic Ocean amid waters that are almost permanently covered with constantly shifting sea ice. This makes it impractical to construct a permanent station at the North Pole (unlike the South Pole). However, the Soviet Union, and later Russia, constructed a number of manned drifting stations on a generally annual basis since 1937, some of which have passed over or very close to the Pole. Since 2002, the Russians have also annually established a base, Barneo, close to the Pole. This operates for a few weeks during early spring. Studies in the 2000s predicted that the North Pole may become seasonally ice-free because of Arctic ice shrinkage, with timescales varying from 2016 to the late 21st century or later.
The sea depth at the North Pole has been measured at by the Russian Mir submersible in 2007 and at 4,087 m (13,410 ft) by USS "Nautilus" in 1958. The nearest land is usually said to be Kaffeklubben Island, off the northern coast of Greenland about away, though some perhaps non-permanent gravel banks lie slightly closer. The nearest permanently inhabited place is Alert in the Qikiqtaaluk Region, Nunavut, Canada, which is located from the Pole.
Precise definition.
The Earth's axis of rotation – and hence the position of the North Pole – was commonly believed to be fixed (relative to the surface of the Earth) until, in the 18th century, the mathematician Leonhard Euler predicted that the axis might "wobble" slightly. Around the beginning of the 20th century astronomers noticed a small apparent "variation of latitude," as determined for a fixed point on Earth from the observation of stars. Part of this variation could be attributed to a wandering of the Pole across the Earth's surface, by a range of a few metres. The wandering has several periodic components and an irregular component. The component with a period of about 435 days is identified with the 8 month wandering predicted by Euler and is now called the Chandler wobble after its discoverer. The exact point of intersection of the Earth's axis and the Earth's surface, at any given moment, is called the "instantaneous pole", but because of the "wobble" this cannot be used as a definition of a fixed North Pole (or South Pole) when metre-scale precision is required.
It is desirable to tie the system of Earth coordinates (latitude, longitude, and elevations or orography) to fixed landforms. Of course, given plate tectonics and isostasy, there is no system in which all geographic features are fixed. Yet the International Earth Rotation and Reference Systems Service and the International Astronomical Union have defined a framework called the International Terrestrial Reference System.
Exploration.
Pre-1900.
As early as the 16th century, many eminent people correctly believed that the North Pole was in a sea, which in the 19th century was called the Polynya or Open Polar Sea. It was therefore hoped that passage could be found through ice floes at favorable times of the year. Several expeditions set out to find the way, generally with whaling ships, already commonly used in the cold northern latitudes.
One of the earliest expeditions to set out with the explicit intention of reaching the North Pole was that of British naval officer William Edward Parry, who in 1827 reached latitude 82°45′ North. In 1871 the Polaris expedition, a US attempt on the Pole led by Charles Francis Hall, ended in disaster. Another British Royal Navy attempt on the pole, part of the British Arctic Expedition, by Commander Albert H. Markham reached a then-record 83°20'26" North in May 1876 before turning back. An 1879–1881 expedition commanded by US naval officer George W. DeLong ended tragically when their ship, the USS "Jeanette", was crushed by ice. Over half the crew, including DeLong, were lost.
In April 1895 the Norwegian explorers Fridtjof Nansen and Hjalmar Johansen struck out for the Pole on skis after leaving Nansen's icebound ship "Fram". The pair reached latitude 86°14′ North before they abandoned the attempt and turned southwards, eventually reaching Franz Josef Land.
In 1897 Swedish engineer Salomon August Andrée and two companions tried to reach the North Pole in the hydrogen balloon "Örnen" ("Eagle"), but came down north of Kvitøya, the northeasternmost part of the Svalbard archipelago. They trekked to Kvitøya but died there three months later. In 1930 the remains of this expedition were found by the Norwegian Bratvaag Expedition.
The Italian explorer Luigi Amedeo, Duke of the Abruzzi and Captain Umberto Cagni of the Italian Royal Navy (Regia Marina) sailed the converted whaler "Stella Polare" ("Pole Star") from Norway in 1899. On 11 March 1900 Cagni led a party over the ice and reached latitude 86° 34’ on 25 April, setting a new record by beating Nansen's result of 1895 by . Cagni barely managed to return to the camp, remaining there until 23 June. On 16 August the "Stella Polare" left Rudolf Island heading south and the expedition returned to Norway.
1900–1940.
The US explorer Frederick Cook claimed to have reached the North Pole on 21 April 1908 with two Inuit men, Ahwelah and Etukishook, but he was unable to produce convincing proof and his claim is not widely accepted.
The conquest of the North Pole was for many years credited to US Navy engineer Robert Peary, who claimed to have reached the Pole on 6 April 1909, accompanied by Matthew Henson and four Inuit men, Ootah, Seeglo, Egingwah, and Ooqueah. However, Peary's claim remains highly disputed and controversial. Those who accompanied Peary on the final stage of the journey were not trained in navigation, and thus could not independently confirm his navigational work, which some claim to have been particularly sloppy as he approached the Pole.
The distances and speeds that Peary claimed to have achieved once the last support party turned back seem incredible to many people, almost three times that which he had accomplished up to that point. Peary's account of a journey to the Pole and back while traveling along the direct line – the only strategy that is consistent with the time constraints that he was facing – is contradicted by Henson's account of tortuous detours to avoid pressure ridges and open leads.
The British explorer Wally Herbert, initially a supporter of Peary, researched Peary's records in 1989 and found that there were significant discrepancies in the explorer's navigational records. He concluded that Peary had not reached the Pole. Support for Peary came again in 2005, however, when British explorer Tom Avery and four companions recreated the outward portion of Peary's journey with replica wooden sleds and Canadian Eskimo Dog teams, reaching the North Pole in 36 days, 22 hours – nearly five hours faster than Peary. However, Avery's fastest 5-day march was 90 nautical miles, significantly short of the 135 claimed by Peary. Avery writes on his web site that "The admiration and respect which I hold for Robert Peary, Matthew Henson and the four Inuit men who ventured North in 1909, has grown enormously since we set out from Cape Columbia. Having now seen for myself how he travelled across the pack ice, I am more convinced than ever that Peary did indeed discover the North Pole."
Another rejection of Peary's claim arrived in 2009, when E. Myles Standish of the California Institute of Technology, an experienced referee of scientific claims, reported numerous alleged lacunae and inconsistencies.
The first claimed flight over the Pole was made on 9 May 1926 by US naval officer Richard E. Byrd and pilot Floyd Bennett in a Fokker tri-motor aircraft. Although verified at the time by a committee of the National Geographic Society, this claim has since been undermined by the 1996 revelation that Byrd's long-hidden diary's solar sextant data (which the NGS never checked) consistently contradict his June 1926 report's parallel data by over . The secret report's alleged en-route solar sextant data were inadvertently so impossibly overprecise that he excised all these alleged raw solar observations out of the version of the report finally sent to geographical societies five months later (while the original version was hidden for 70 years), a realization first published in 2000 by the University of Cambridge after scrupulous refereeing.
According to Standish, "Anyone who is acquainted with the facts and has any amount of logical reasoning can not avoid the conclusion that neither Cook, nor Peary, nor Byrd reached the North Pole; and they all knew it."
The first consistent, verified, and scientifically convincing attainment of the Pole was on 12 May 1926, by Norwegian explorer Roald Amundsen and his US sponsor Lincoln Ellsworth from the airship "Norge". "Norge", though Norwegian-owned, was designed and piloted by the Italian Umberto Nobile. The flight started from Svalbard in Norway, and crossed the Arctic Ocean to Alaska. Nobile, with several scientists and crew from the "Norge", overflew the Pole a second time on 24 May 1928, in the airship "Italia". The "Italia" crashed on its return from the Pole, with the loss of half the crew.
In May 1937 the world's first North Pole ice station, North Pole-1, was established by Soviet scientists by air 20 kilometres (13 mi) from the North Pole. The expedition members: oceanographer Pyotr Shirshov, meteorologist Yevgeny Fyodorov, radio operator Ernst Krenkel, and the leader Ivan Papanin conducted scientific research at the station for the next nine months. By 19 February 1938, when the group was picked up by the ice breakers "Taimyr" and "Murman", their station had drifted 2850 km to the eastern coast of Greenland.
1940–2000.
In May 1945 an RAF Lancaster of the "Aries" expedition became the first Commonwealth aircraft to overfly the North Geographic and North Magnetic Poles. The plane was piloted by David Cecil McKinley of the Royal Air Force. It carried an 11-man crew, with Kenneth C. Maclure of the Royal Canadian Air Force in charge of all scientific observations. In 2006, Maclure was honoured with a spot in Canada's Aviation Hall of Fame.
Discounting Peary's disputed claim, the first men to set foot at the North Pole were a Soviet party including geophysicists Mikhail Ostrekin and Pavel Senko, oceanographers Mikhail Somov and Pavel Gordienko, and other scientists and flight crew (24 people in total) of Aleksandr Kuznetsov's "Sever-2" expedition (March–May 1948). It was organized by the Chief Directorate of the Northern Sea Route. The party flew on three planes (pilots Ivan Cherevichnyy, Vitaly Maslennikov and Ilya Kotov) from Kotelny Island to the North Pole and landed there at 4:44pm (Moscow Time, ) on 23 April 1948. They established a temporary camp and for the next two days conducted scientific observations. On 26 April the expedition flew back to the continent.
Next year, on 9 May 1949, two other Soviet scientists (Vitali Volovich and Andrei Medvedev) became the first people to parachute onto the North Pole. They jumped from a Douglas C-47 Skytrain, registered CCCP H-369.
On 3 May 1952, U.S. Air Force Lieutenant Colonel Joseph O. Fletcher and Lieutenant William Pershing Benedict, along with scientist Albert P. Crary, landed a modified Douglas C-47 Skytrain at the North Pole. Some Western sources considered this to be the first landing at the Pole until the Soviet landings became widely known.
The United States Navy submarine "USS Nautilus" (SSN-571) crossed the North Pole on 3 August 1958. On 17 March 1959 "USS Skate" (SSN-578) surfaced at the Pole, breaking through the ice above it, becoming the first naval vessel to do so.
Setting aside Peary's claim, the first confirmed surface conquest of the North Pole was that of Ralph Plaisted, Walt Pederson, Gerry Pitzl and Jean Luc Bombardier, who traveled over the ice by snowmobile and arrived on 19 April 1968. The United States Air Force independently confirmed their position.
On 6 April 1969, Wally Herbert and companions Allan Gill, Roy Koerner and Kenneth Hedges of the British Trans-Arctic Expedition became the first men to reach the North Pole on foot (albeit with the aid of dog teams and airdrops). They continued on to complete the first surface crossing of the Arctic Ocean – and by its longest axis, Barrow, Alaska to Svalbard – a feat that has never been repeated. Because of suggestions (later proven false) of Plaisted's use of air transport, some sources classify Herbert's expedition as the first confirmed to reach the North Pole over the ice surface by any means. In the 1980s, Plaisted's pilots Weldy Phipps and Ken Lee signed affidavits asserting that no such airlift was provided. It is also said that Herbert was the first person to reach the pole of inaccessibility.
On 17 August 1977, the Soviet nuclear-powered icebreaker "Arktika" completed the first surface vessel journey to the North Pole.
In 1982 Ranulph Fiennes and Charles R. Burton became the first people to cross the Arctic Ocean in a single season. They departed from Cape Crozier, Ellesmere Island, on 17 February 1982 and arrived at the geographic North Pole on 10 April 1982. They travelled on foot and snowmobile. From the Pole, they travelled towards Svalbard but, due to the unstable nature of the ice, ended their crossing at the ice edge after drifting south on an ice floe for 99 days. They were eventually able to walk to their expedition ship "MV Benjamin Bowring" and boarded it on 4 August 1982 at position 80:31N 00:59W. As a result of this journey, which formed a section of the three-year Transglobe Expedition 1979–1982, Fiennes and Burton became the first people to complete a circumnavigation of the world via both North and South Poles, by surface travel alone. This achievement remains unchallenged to this day.
In 1985, Sir Edmund Hillary (the first man to stand on the summit of Mount Everest) and Neil Armstrong (the first man to stand on the moon) landed at the North Pole in a small twin-engined ski plane. Hillary thus became the first man to stand at both poles and on the summit of Everest.
In 1986, Will Steger, with seven teammates, became the first to be confirmed as reaching the Pole by dogsled and without resupply.
On 6 May 1986, USS "Archerfish" (SSN 678), USS "Ray" (SSN 653) and USS Hawkbill (SSN 666) surfaced at the North Pole, the first tri-submarine surfacing at the North Pole.
On 21 April 1987, Shinji Kazama of Japan became the first person to reach the North Pole on a motorcycle.
On 18 May 1987, USS "Billfish" (SSN 676), USS "Sea Devil" (SSN 664) and HMS "Superb" (S 109) surfaced at the North Pole, the first international surfacing at the North Pole.
In 1988, a 13-man strong team (9 Soviets, 4 Canadians) skied across the arctic from Siberia to northern Canada. One of the Canadians, Richard Weber became the first person to reach the Pole from both sides of the Arctic Ocean.
On 4 May 1990, Børge Ousland and Erling Kagge became the first explorers ever to reach the North Pole unsupported, after a 58-day ski trek from Ellesmere Island in Canada, a distance of 800 km.
On 7 September 1991, the German research vessel "Polarstern" and the Swedish icebreaker "Oden" reached the North Pole as the first conventional powered vessels. Both scientific parties and crew took oceanographic and geological samples and had a common tug of war and a football game on an ice floe. Polarstern again reached the pole exactly 10 years later with the "Healy".
In 1998, 1999, and 2000 Lada Niva Marshs (special very large wheeled versions made by BRONTO, Lada/Vaz's experimental product division) were driven to the North Pole. The 1998 expedition was dropped by parachute and completed the track to the North Pole. The 2000 expedition departed from a Russian research base around 114 km from the Pole and claimed an average speed of 20–15 km/h in an average temperature of −30 degrees.
21st century.
In recent years, journeys to the North Pole by air (landing by helicopter or on a runway prepared on the ice) or by icebreaker have become relatively routine, and are even available to small groups of tourists through adventure holiday companies. Parachute jumps have frequently been made onto the North Pole in recent years. The temporary seasonal Russian camp of Barneo has been established by air a short distance from the Pole annually since 2002, and caters for scientific researchers as well as tourist parties. Trips from the camp to the Pole itself may be arranged overland or by helicopter.
The first attempt at underwater exploration of the North Pole was made on 22 April 1998 by Russian firefighter and diver Andrei Rozhkov with the support of the Diving Club of Moscow State University, but ended in fatality. The next attempted dive at the North Pole was organized the next year by the same diving club, and ended in success on 24 April 1999. The divers were Michael Wolff (Austria), Brett Cormick (UK), and Bob Wass (USA).
In 2005, the United States Navy submarine USS "Charlotte" (SSN-766) surfaced through of ice at the North Pole and spent 18 hours there.
In July 2007, British endurance swimmer Lewis Gordon Pugh completed a swim at the North Pole. His feat, undertaken to highlight the effects of global warming, took place in clear water that had opened up between the ice floes. His later attempt to paddle a kayak to the North Pole in late 2008, following the erroneous prediction of clear water to the Pole, was stymied when his expedition found itself stuck in thick ice after only three days. The expedition was then abandoned.
By September 2007 the North Pole had been visited 66 times by different surface ships: 54 times by Soviet and Russian icebreakers, 4 times by Swedish "Oden", 3 times by German "Polarstern", 3 times by USCGC "Healy" and USCGC "Polar Sea", and once by CCGS "Louis S. St-Laurent" and by Swedish "Vidar Viking".
2007 descent to the North Pole seabed.
On 2 August 2007, a Russian scientific expedition Arktika 2007 made the first ever manned descent to the ocean floor at the North Pole, to a depth of , as part of the research programme in support of Russia's 2001 extended continental shelf claim to a large swathe of the Arctic Ocean floor. The descent took place in two MIR submersibles and was led by Soviet and Russian polar explorer Artur Chilingarov. In a symbolic act of visitation, the Russian flag was placed on the ocean floor exactly at the Pole.
The expedition was the latest in a series of efforts intended to give Russia a dominant influence in the Arctic according to the New York Times. The warming Arctic climate and summer shrinkage of the iced area has attracted the attention of many countries, such as China and the United States, toward the top of the world, where resources and shipping routes may soon be exploitable.
MLAE 2009 Expedition.
In 2009, the Russian Marine Live-Ice Automobile Expedition—MLAE 2009 (Vasily Elagin as a leader, and a team of Sergey Larin, Afanasy Makovnev, Vladimir Obikhod, Alexey Ushakov, Alexey Shkrabkin, and Nikolay Nikulshin) reached the North Pole on two custom-built 6 x 6 low-pressure-tire ATVs—Yemelya 1 and Yemelya 2—designed by Vasily Elagin, a known Russian mountain climber, explorer, and engineer. The vehicles reached the North Pole on 26 April 2009, 17:30 (Moscow time). The expedition was supported by the Russian Geographical Society. The Russian Book of Records recognized it as the first successful vehicle trip to the Geographical North Pole.
MLAE 2013 Expedition.
On 1 March 2013, the Russian Marine Live-Ice Automobile Expedition — MLAE 2013 (Vasily Elagin as a leader, and a team of Andrey Vankov, Sergey Isayev, Nikolay Kozlov, Afanasy Makovnev, Vladimir Obikhod, and Alexey Shkrabkin) on two custom-built 6 x 6 low-pressure-tire ATVs—Yemelya 3 and Yemelya 4—started from Golomyanny Island (the Severnaya Zemlya Archipelago) to the North Pole across drifting ice of the Arctic Ocean. The vehicles reached the Pole on 6 April and then continued to the Canadian coast. The coast was reached on 30 April 2013 (83°08N, 075°59W), and on 5 May 2013, the expedition finished in Resolute Bay, NU. The way between the Russian borderland (Machtovyi Island of the Severnaya Zemlya Archipelago, 80°15N, 097°27E) and the Canadian coast (83°08N, 075°59W) took 55 days; it was ~2300 km across drifting ice and about 4000 km in total. The expedition was totally self-dependent and used no external supplies. The expedition was supported by the Russian Geographical Society.
Day and night.
The sun at the North Pole is continuously above the horizon during the summer and continuously below the horizon during the winter. Sunrise is just before the March equinox (around 20 March); the sun then takes three months to reach its highest point of near 23½° elevation at the summer solstice (around 21 June), after which time it begins to sink, reaching sunset just after the September equinox (around 23 September). When the sun is visible in the polar sky, it appears to move in a horizontal circle above the horizon. This circle gradually rises from near the horizon just after the vernal equinox to its maximum elevation (in degrees) above the horizon at summer solstice and then sinks back toward the horizon before sinking below it at the autumnal equinox.
A civil twilight period of about two weeks occurs before sunrise and after sunset, a nautical twilight period of about five weeks occurs before sunrise and after sunset and an astronomical twilight period of about seven weeks occurs before sunrise and after sunset.
These effects are caused by a combination of the Earth's axial tilt and its revolution around the sun. The direction of the Earth's axial tilt, as well as its angle relative to the plane of the Earth's orbit around the sun, remains very nearly constant over the course of a year (both change very slowly over long time periods). At northern midsummer the North Pole is facing towards the sun to its maximum extent. As the year progresses and the Earth moves around the sun, the North Pole gradually turns away from the sun until at midwinter it is facing away from the Sun to its maximum extent. A similar sequence is observed at the South Pole, with a six-month time difference.
Time.
In most places on Earth, local time is determined by longitude, such that the time of day is more-or-less synchronised to the position of the sun in the sky (for example, at midday the sun is roughly at its highest). This line of reasoning fails at the North Pole, where the sun rises and sets only once per year, and all lines of longitude, and hence all time zones, converge. There is no permanent human presence at the North Pole and no particular time zone has been assigned. Polar expeditions may use any time zone that is convenient, such as Greenwich Mean Time, or the time zone of the country from which they departed.
Climate.
The North Pole is substantially warmer than the South Pole because it lies at sea level in the middle of an ocean (which acts as a reservoir of heat), rather than at altitude on a continental land mass.
Winter temperatures at the North Pole can range from about , averaging around . However, a freak storm caused the temperature to reach for a time at a World Meteorological Organization buoy, located at 87.45°N, on December 30, 2015. It was estimated that the temperature at the North Pole was between during the storm. Summer temperatures (June, July, and August) average around the freezing point (). The highest temperature yet recorded is , much warmer than the South Pole's record high of only .
The sea ice at the North Pole is typically around thick, although ice thickness, its spatial extent, and the fraction of open water within the ice pack can vary rapidly and profoundly in response to weather and climate. Studies have shown that the average ice thickness has decreased in recent years. It is likely that global warming has contributed to this, but it is not possible to attribute the recent abrupt decrease in thickness entirely to the observed warming in the Arctic. Reports have also predicted that within a few decades the Arctic Ocean will be entirely free of ice in the summer. This may have significant commercial implications; see "Territorial Claims," below.
The retreat of the Arctic sea ice will accelerate global warming, as less ice cover reflects less solar radiation, and may have serious climate implications by contributing to Arctic cyclone generation.
Flora and fauna.
Polar bears are believed rarely to travel beyond about 82° North owing to the scarcity of food, though tracks have been seen in the vicinity of the North Pole, and a 2006 expedition reported sighting a polar bear just from the Pole. The ringed seal has also been seen at the Pole, and Arctic foxes have been observed less than away at 89°40′ N.
Birds seen at or very near the Pole include the snow bunting, northern fulmar and black-legged kittiwake, though some bird sightings may be distorted by the tendency of birds to follow ships and expeditions.
Fish have been seen in the waters at the North Pole, but these are probably few in number. A member of the Russian team that descended to the North Pole seabed in August 2007 reported seeing no sea creatures living there. However, it was later reported that a sea anemone had been scooped up from the seabed mud by the Russian team and that video footage from the dive showed unidentified shrimps and amphipods.
Territorial claims to the North Pole and Arctic regions.
Currently, under international law, no country owns the North Pole or the region of the Arctic Ocean surrounding it. The five surrounding Arctic countries, Russian Federation (the biggest country), Canada, Norway, Denmark (via Greenland), and the United States (via Alaska), are limited to a exclusive economic zone around their coasts, and the area beyond that is administered by the International Seabed Authority.
Upon ratification of the United Nations Convention on the Law of the Sea, a country has 10 years to make claims to an extended continental shelf beyond its 200-mile exclusive economic zone. If validated, such a claim gives the claimant state rights to what may be on or beneath the sea bottom within the claimed zone. Norway (ratified the convention in 1996), Russia (ratified in 1997), Canada (ratified in 2003) and Denmark (ratified in 2004) have all launched projects to base claims that certain areas of Arctic continental shelves should be subject to their sole sovereign exploitation.
In 1907 Canada invoked a "sector principle" to claim sovereignty over a sector stretching from its coasts to the North Pole. This claim has not been relinquished, but was not consistently pressed until 2013.
Cultural associations.
In some children's Western cultures, the geographic North Pole is described as the location of Santa Claus' workshop and residence, although the depictions have been inconsistent between the geographic and magnetic North Pole. Canada Post has assigned postal code H0H 0H0 to the North Pole (referring to Santa's traditional exclamation of "Ho ho ho!").
This association reflects an age-old esoteric mythology of Hyperborea that posits the North Pole, the otherworldly world-axis, as the abode of God and superhuman beings. The popular figure of the pole-dwelling Santa Claus thus functions as an archetype of spiritual purity and transcendence.
As Henry Corbin has documented, the North Pole plays a key part in the cultural worldview of Sufism and Iranian mysticism. "The Orient sought by the mystic, the Orient that cannot be located on our maps, is in the direction of the north, beyond the north."
Owing to its remoteness, the Pole is sometimes identified with a mysterious mountain of ancient Iranian tradition called Mount Qaf (Jabal Qaf), the "farthest point of the earth". According to certain authors, the Jabal Qaf of Muslim cosmology is a version of Rupes Nigra, a mountain whose ascent, like Dante's climbing of the Mountain of Purgatory, represents the pilgrim's progress through spiritual states. In Iranian theosophy, the heavenly Pole, the focal point of the spiritual ascent, acts as a magnet to draw beings to its "palaces ablaze with immaterial matter." 

</doc>
<doc id="21837" url="https://en.wikipedia.org/wiki?curid=21837" title="Nanometre">
Nanometre

The nanometre (International spelling as used by the International Bureau of Weights and Measures; SI symbol: nm) or nanometer (American spelling) is a unit of length in the metric system, equal to one billionth of a metre ( m) . The name combines the SI prefix "nano-" (from the Ancient Greek , ', "dwarf") with the parent unit name "metre" (from Greek , ', "unit of measurement"). It can be written in scientific notation as , in engineering notation as , and is simply . One nanometre equals ten ångströms.
Use.
The nanometre is often used to express dimensions on an atomic scale: the diameter of a helium atom, for example, is about 0.1 nm, and that of a ribosome is about 20 nm. The nanometre is also commonly used to specify the wavelength of electromagnetic radiation near the visible part of the spectrum: visible light ranges from around 400 to 700 nm. The angstrom, which is equal to 0.1 nm, was formerly used for these purposes, but is still used in other fields.
History.
The nanometre was formerly known as the millimicrometre – or, more commonly, the millimicron for short – since it is 1/1000 of a micron (micrometre), and was often denoted by the symbol mµ or (more rarely) µµ. In 1960, the U.S. National Bureau of Standards adopted the prefix "nano-" for "a billionth" (1/1000³). The nanometre is often associated with the field of nanotechnology. Since the late 1980s, it has also been used to describe generations of the manufacturing technology in the semiconductor industry.

</doc>
<doc id="21840" url="https://en.wikipedia.org/wiki?curid=21840" title="National Transportation Safety Board">
National Transportation Safety Board

The National Transportation Safety Board (NTSB) is an independent U.S. government investigative agency responsible for civil transportation accident investigation. In this role, the NTSB investigates and reports on aviation accidents and incidents, certain types of highway crashes, ship and marine accidents, pipeline incidents and railroad accidents. When requested, the NTSB will assist the military and foreign governments with accident investigation. The NTSB is also in charge of investigating cases of hazardous materials releases that occur during transportation. The agency is based in Washington, D.C. , it has four regional offices located in Anchorage, Alaska, Denver, Colorado, Ashburn, Virginia, and Seattle, Washington. The agency also operates a national training center at its Ashburn facility.
History.
The origin of the NTSB was in the Air Commerce Act of 1926, which assigned the United States Department of Commerce responsibility for investigating domestic aviation accidents. In 1940, this authority was transferred to the Civil Aeronautics Board's newly formed Bureau of Aviation Safety.
In 1967, Congress created a separate cabinet-level Department of Transportation, which among other things established the Federal Aviation Administration as agency under the DOT. At the same time, the NTSB was established as an independent agency which absorbed the Bureau of Aviation Safety's responsibilities. However, from 1967 to 1975, the NTSB reported to the DOT for administrative purposes, while conducting investigations into the Federal Aviation Administration, also a DOT agency.
To avoid any conflict, Congress passed the Independent Safety Board Act, and on April 1, 1975 the NTSB became a fully independent federal agency. , the NTSB has investigated over 140,000 aviation incidents and several thousand surface transportation incidents.
Organization.
Formally, the "National Transportation Safety Board" refers to a five-manager investigative board whose five members are nominated by the President and confirmed by the Senate for five-year terms. No more than three of the five members may be from the same political party. One of the five board members is nominated as the Chairman by the President and then approved by the Senate for a fixed 2-year term; another is designated as vice-chairman and becomes acting chairman when there is no formal chairman. This board is authorized by Congress under Chapter 11, Title 49 of the United States Code to investigate civil aviation, highway, marine, pipeline, and railroad accidents and incidents. This five-member board is authorized to establish and manage separate sub-offices for highway, marine, aviation, railroad, pipeline, and hazardous materials investigations. Collectively, "National Transportation Safety Board", the "Safety Board" or "NTSB" is used to refer to the entire investigative agency established and managed by this five-member board. , Christopher A. Hart is Chairman of the NTSB, and T. Bella Dinh-Zarr is Vice-Chairman. The board also includes Robert L. Sumwalt and Earl F. Weener, and has one vacant seat.
Since its creation, the NTSB's primary mission has been "to determine the probable cause of transportation accidents and incidents and to formulate safety recommendations to improve transportation safety." Based on the results of investigations within its jurisdiction, the NTSB issues formal safety recommendations to agencies and institutions with the power to implement those recommendations. The NTSB considers safety recommendations to be its primary tool for preventing future civil transportation accidents. However, the NTSB does not have the authority to enforce its safety recommendations.
Investigations.
The NTSB is the lead agency in the investigation of a civil transportation accident or incident within its sphere. An investigation of a major accident within the United States typically starts with the creation of a "go team," composed of specialists in fields relating to the incident who are rapidly deployed to the incident location. The "go team" can have as few as 3-4 people or as many as a dozen, depending on the nature of the incident. Following the investigation, the agency may then choose to hold public hearings on the issue. Ultimately, it will publish a final report which may include safety recommendations based on its findings. The NTSB has no legal authority to implement or impose its recommendations, which must be implemented by regulators at either the federal or state level or individual transportation companies.
Significant investigations conducted by the NTSB in all modes of transportation in recent years include the collapse of the I-35W highway bridge in Minneapolis, Minnesota; the collision between two transit trains in Washington, D.C.; the pipeline explosion that destroyed much of a neighborhood in San Bruno, California; the sinking of an amphibious vessel in Philadelphia; and the crash of a regional airliner near Buffalo, New York.
Recommendations.
, the NTSB has issued about 14,000 safety recommendations in its history, 73 percent of which have been adopted in whole or in part by the entities to which they were directed. Starting in 1990, the NTSB has annually published a "Most Wanted List" which highlights safety recommendations that the NTSB believes would provide the most significant — and sometimes immediate — benefit to the traveling public.
Among transportation safety improvements brought about or inspired by NTSB recommendations:
Other responsibilities.
A little-known responsibility of the NTSB is that it serves as a court of appeals for airmen, aircraft mechanics, certificated aviation-related companies and mariners who have their licenses suspended or revoked by the FAA or the Coast Guard. The NTSB employs administrative law judges which initially hear all appeals, and the administrative law judge's ruling may be appealed to the five-member Board. The Board's determinations may be appealed to the federal court system by the losing party, whether it is the individual or company, on the one hand, or the FAA or the Coast Guard, on the other. However, the NTSB's determinations will not be overturned by the federal courts unless the NTSB abused its discretion, or its determination is wholly unsupported by the evidence.
The Safety Board maintains a training academy in Ashburn, Virginia, where it conducts courses for its employees and professionals in other government agencies, foreign governments or private companies, in areas such as general accident investigation, specific elements of investigations like survival factors or human performance, or related matters like family affairs or media relations. The facility houses for training purposes the reconstruction of more than 90 feet of the TWA Flight 800 Boeing 747, which was recovered from the Atlantic Ocean after it crashed on July 17, 1996, following a fuel tank explosion.

</doc>
<doc id="21843" url="https://en.wikipedia.org/wiki?curid=21843" title="Nucleosome">
Nucleosome

A nucleosome is a basic unit of DNA packaging in eukaryotes, consisting of a segment of DNA wound in sequence around eight histone protein cores. This structure is often compared to thread wrapped around a spool.
Nucleosomes form the fundamental repeating units of eukaryotic chromatin, which is used to pack the large eukaryotic genomes into the nucleus while still ensuring appropriate access to it (in mammalian cells approximately 2 m of linear DNA have to be packed into a nucleus of roughly 10 µm diameter). Nucleosomes are folded through a series of successively higher order structures to eventually form a chromosome; this both compacts DNA and creates an added layer of regulatory control, which ensures correct gene expression. Nucleosomes are thought to carry epigenetically inherited information in the form of covalent modifications of their core histones.
Nucleosomes were observed as particles in the electron microscope by Don and Ada Olins and their existence and structure (as histone octamers surrounded by approximately 200 base pairs of DNA) were proposed by Roger Kornberg. The role of the nucleosome as a general gene repressor was demonstrated by Lorch et al. in vitro and by Han and Grunstein in vivo.
The nucleosome core particle consists of approximately 147 base pairs of DNA wrapped in 1.67 left-handed superhelical turns around a histone octamer consisting of 2 copies each of the core histones H2A, H2B, H3, and H4. Core particles are connected by stretches of "linker DNA", which can be up to about 80 bp long. Technically, a nucleosome is defined as the core particle plus one of these linker regions; however the word is often synonymous with the core particle. Genome-wide nucleosome positioning maps are now available for many model organisms including mouse liver and brain.
Linker histones such as H1 and its isoforms are involved in chromatin compaction and sit at the base of the nucleosome near the DNA entry and exit binding to the linker region of the DNA. Non-condensed nucleosomes without the linker histone resemble "beads on a string of DNA" under an electron microscope.
In contrast to most eukaryotic cells, mature sperm cells largely use protamines to package their genomic DNA, most likely to achieve an even higher packaging ratio. Histone equivalents and a simplified chromatin structure have also been found in Archea, suggesting that eukaryotes are not the only organisms that use nucleosomes.
Structure.
Structure of the core particle.
Overview.
Pioneering structural studies in the 1980s by Aaron Klug's group provided the first evidence that an octamer of histone proteins wraps DNA around itself in about two turns of a left-handed superhelix. In 1997 the first near atomic resolution crystal structure of the nucleosome was solved by the Richmond group, showing the most important details of the particle. The human alpha-satellite palindromic DNA critical to achieving the 1997 nucleosome crystal structure was developed by the Bunick group at Oak Ridge National Laboratory in Tennessee. The structures of over 20 different nucleosome core particles have been solved to date, including those containing histone variants and histones from different species. The structure of the nucleosome core particle is remarkably conserved, and even a change of over 100 residues between frog and yeast histones results in electron density maps with an overall root mean square deviation of only 1.6Å.
The nucleosome core particle (NCP).
The nucleosome core particle (shown in the figure) consists of about 146 bp of DNA wrapped in 1.67 left-handed superhelical turns around the histone octamer, consisting of 2 copies each of the core histones H2A, H2B, H3, and H4. Adjacent nucleosomes are joined by a stretch of free DNA termed "linker DNA" (which varies from 10 - 80 bp in length depending on species and tissue type).
Nucleosome core particles are observed when chromatin in interphase is treated to cause the chromatin to unfold partially. The resulting image, via an electron microscope, is "beads on a string". The string is the DNA, while each bead in the nucleosome is a core particle. The nucleosome core particle is composed of DNA and histone proteins.
Partial DNAse digestion of chromatin reveals its nucleosome structure. Because DNA portions of nucleosome core particles are less accessible for DNAse than linking sections, DNA gets digested into fragments of lengths equal to multiplicity of distance between nucleosomes (180, 360, 540 base pairs etc.). Hence a very characteristic pattern similar to a ladder is visible during gel electrophoresis of that DNA. Such digestion can occur also under natural conditions during apoptosis ("cell suicide" or programmed cell death), because autodestruction of DNA typically is its role.
Protein interactions within the nucleosome.
The core histone proteins contains a characteristic structural motif termed the "histone fold," which consists of three alpha-helices (α1-3) separated by two loops (L1-2). In solution, the histones form H2A-H2B heterodimers and H3-H4 heterotetramers. Histones dimerise about their long α2 helices in an anti-parallel orientation, and, in the case of H3 and H4, two such dimers form a 4-helix bundle stabilised by extensive H3-H3’ interaction. The H2A/H2B dimer binds onto the H3/H4 tetramer due to interactions between H4 and H2B, which include the formation of a hydrophobic cluster. 
The histone octamer is formed by a central H3/H4 tetramer sandwiched between two H2A/H2B dimers. Due to the highly basic charge of all four core histones, the histone octamer is stable only in the presence of DNA or very high salt concentrations.
Histone - DNA interactions.
The nucleosome contains over 120 direct protein-DNA interactions and several hundred water-mediated ones. Direct protein - DNA interactions are not spread evenly about the octamer surface but rather located at discrete sites. These are due to the formation of two types of DNA binding sites within the octamer; the α1α1 site, which uses the α1 helix from two adjacent histones, and the L1L2 site formed by the L1 and L2 loops. Salt links and hydrogen bonding between both side-chain basic and hydroxyl groups and main-chain amides with the DNA backbone phosphates form the bulk of interactions with the DNA. This is important, given that the ubiquitous distribution of nucleosomes along genomes requires it to be a non-sequence-specific DNA-binding factor. Although nucleosomes tend to prefer some DNA sequences over others, they are capable of binding practically to any sequence, which is thought to be due to the flexibility in the formation of these water-mediated interactions. In addition, non-polar interactions are made between protein side-chains and the deoxyribose groups, and an arginine side-chain intercalates into the DNA minor groove at all 14 sites where it faces the octamer surface.
The distribution and strength of DNA-binding sites about the octamer surface distorts the DNA within the nucleosome core. The DNA is non-uniformly bent and also contains twist defects. The twist of free B-form DNA in solution is 10.5 bp per turn. However, the overall twist of nucleosomal DNA is only 10.2 bp per turn, varying from a value of 9.4 to 10.9 bp per turn.
Histone tail domains.
The histone tail extensions constitute up to 30% by mass of histones, but are not visible in the crystal structures of nucleosomes due to their high intrinsic flexibility, and have been thought to be largely unstructured. The N-terminal tails of histones H3 and H2B pass through a channel formed by the minor grooves of the two DNA strands, protruding from the DNA every 20 bp. The N-terminal tail of histone H4, on the other hand, has a region of highly basic amino acids (16-25), which, in the crystal structure, forms an interaction with the highly acidic surface region of a H2A-H2B dimer of another nucleosome, being potentially relevant for the higher-order structure of nucleosomes. This interaction is thought to occur under physiological conditions also, and suggests that acetylation of the H4 tail distorts the higher-order structure of chromatin.
Higher order structure.
The organization of the DNA that is achieved by the nucleosome cannot fully explain the packaging of DNA observed in the cell nucleus. Further compaction of chromatin into the cell nucleus is necessary, but is not yet well understood. The current understanding is that repeating nucleosomes with intervening "linker" DNA form a "10-nm-fiber", described as "beads on a string", and have a packing ratio of about five to ten. A chain of nucleosomes can be arranged in a "30 nm fiber", a compacted structure with a packing ratio of ~50 and whose formation is dependent on the presence of the H1 histone.
A crystal structure of a tetranucleosome has been presented and used to build up a proposed structure of the 30 nm fiber as a two-start helix. 
There is still a certain amount of contention regarding this model, as it is incompatible with recent electron microscopy data. Beyond this, the structure of chromatin is poorly understood, but it is classically suggested that the 30 nm fiber is arranged into loops along a central protein scaffold to form transcriptionally active euchromatin. Further compaction leads to transcriptionally inactive heterochromatin.
Nucleosome dynamics.
Although the nucleosome is a very stable protein-DNA complex, it is not static and has been shown to undergo a number of different structural re-arrangements including nucleosome sliding and DNA site exposure. Depending on the context, nucleosomes can inhibit or facilitate transcription factor binding. Nucleosome positions are controlled by three major contributions: First, the intrinsic binding affinity of the histone octamer depends on the DNA sequence. Second, the nucleosome can be displaced or recruited by the competitive or cooperative binding of other protein factors. Third, the nucleosome may be actively translocated by ATP-dependent remodeling complexes.
Nucleosome sliding.
Work performed in the Bradbury laboratory showed that nucleosomes reconstituted onto the 5S DNA positioning sequence were able to reposition themselves translationally onto adjacent sequences when incubated thermally. Later work showed that this repositioning did not require disruption of the histone octamer but was consistent with nucleosomes being able to “slide” along the DNA "in cis". In 2008, It was further revealed that CTCF binding sites act as nucleosome positioning anchors so that, when used to align various genomic signals, multiple flanking nucleosomes can be readily identified. Although nucleosomes are intrinsically mobile, eukaryotes have evolved a large family of ATP-dependent chromatin remodelling enzymes to alter chromatin structure, many of which do so via nucleosome sliding. In 2012, Beena Pillai's laboratory has demonstrated that nucleosome sliding is one of the possible mechanism for large scale tissue specific expression of genes. The work shows that the transcription start site for genes expressed in a particular tissue, are nucleosome depleted while, the same set of genes in other tissue where they are not expressed, are nucleosome bound.
DNA site exposure.
Work from the Widom laboratory has shown that nucleosomal DNA is in equilibrium between a wrapped and unwrapped state. Measurements of these rates using time-resolved FRET revealed that DNA within the nucleosome remains fully wrapped for only 250 ms before it is unwrapped for 10-50 ms and then rapidly rewrapped. This implies that DNA does not need to be actively dissociated from the nucleosome but that there is a significant fraction of time during which it is fully accessible. Indeed, this can be extended to the observation that introducing a DNA-binding sequence within the nucleosome increases the accessibility of adjacent regions of DNA when bound. This propensity for DNA within the nucleosome to “breathe” is predicted to have important functional consequences for all DNA-binding proteins that operate in a chromatin environment.
Modulating nucleosome structure.
Eukaryotic genomes are ubiquitously associated into chromatin; however, cells must spatially and temporally regulate specific loci independently of bulk chromatin. In order to achieve the high level of control required to co-ordinate nuclear processes such as DNA replication, repair, and transcription, cells have developed a variety of means to locally and specifically modulate chromatin structure and function. This can involve covalent modification of histones, the incorporation of histone variants, and non-covalent remodelling by ATP-dependent remodeling enzymes.
Histone post-translational modifications.
Since they were discovered in the mid-1960s, histone modifications have been predicted to affect transcription. The fact that most of the early post-translational modifications found were concentrated within the tail extensions that protrude from the nucleosome core lead to two main theories regarding the mechanism of histone modification. The first of the theories suggested that they may affect electrostatic interactions between the histone tails and DNA to “loosen” chromatin structure. Later it was proposed that combinations of these modifications may create binding epitopes with which to recruit other proteins. Recently, given that more modifications have been found in the structured regions of histones, it has been put forward that these modifications may affect histone-DNA and histone-histone interactions within the nucleosome core. Modifications (such as acetylation or phosphorylation) that lower the charge of the globular histone core are predicted to "loosen" core-DNA association; the strength of the effect depends on location of the modification within the core.
Some modifications have been shown to be correlated with gene silencing; others seem to be correlated with gene activation. Common modifications include acetylation, methylation, or ubiquitination of lysine; methylation of arginine; and phosphorylation of serine. The information stored in this way is considered epigenetic, since it is not encoded in the DNA but is still inherited to daughter cells. The maintenance of a repressed or activated status of a gene is often necessary for cellular differentiation.
Histone variants.
Although histones are remarkably conserved throughout evolution, several variant forms have been identified. It is interesting to note that this diversification of histone function is restricted to H2A and H3, with H2B and H4 being mostly invariant. H2A can be replaced by H2AZ (which leads to reduced nucleosome stability) or H2AX (which is associated with DNA repair and T cell differentiation), whereas the inactive X chromosomes in mammals are enriched in macroH2A. H3 can be replaced by H3.3 (which correlates with activate genes and regulatory elements) and in centromeres H3 is replaced by CENPA.
ATP-dependent nucleosome remodeling.
A number of distinct reactions are associated with the term ATP-dependent chromatin remodeling. Remodeling enzymes have been shown to slide nucleosomes along DNA, disrupt histone-DNA contacts to the extent of destabilising the H2A/H2B dimer and to generate negative superhelical torsion in DNA and chromatin. Recently, the Swr1 remodeling enzyme has been shown to introduce the variant histone H2A.Z into nucleosomes. At present, it is not clear if all of these represent distinct reactions or merely alternative outcomes of a common mechanism. What is shared between all, and indeed the hallmark of ATP-dependent chromatin remodeling, is that they all result in altered DNA accessibility. 
Studies looking at gene activation "in vivo" and, more astonishingly, remodelling "in vitro" have revealed that chromatin remodeling events and transcription-factor binding are cyclical and periodic in nature. While the consequences of this for the reaction mechanism of chromatin remodeling are not known, the dynamic nature of the system may allow it to respond faster to external stimuli. A recent study indicates that nucleosome positions change significantly during mouse embryonic stem cell development, and these changes are related to binding of developmental transcription factors.
Dynamic nucleosome remodelling across the Yeast genome.
Studies in 2007 have catalogued nucleosome positions in yeast and shown that nucleosomes are depleted in promoter regions and origins of replication.
About 80% of the yeast genome appears to be covered by nucleosomes and the pattern of nucleosome positioning clearly relates to DNA regions that regulate transcription, regions that are transcribed and regions that initiate DNA replication. Most recently, a new study examined ‘’dynamic changes’’ in nucleosome repositioning during a global transcriptional reprogramming event to elucidate the effects on nucleosome displacement during genome-wide transcriptional changes in yeast ("Saccharomyces cerevisiae"). The results suggested that nucleosomes that were localized to promoter regions are displaced in response to stress (like heat shock). In addition, the removal of nucleosomes usually corresponded to transcriptional activation and the replacement of nucleosomes usually corresponded to transcriptional repression, presumably because transcription factor binding sites became more or less accessible, respectively. In general, only one or two nucleosomes were repositioned at the promoter to effect these transcriptional changes. However, even in chromosomal regions that were not associated with transcriptional changes, nucleosome repositioning was observed, suggesting that the covering and uncovering of transcriptional DNA does not necessarily produce a transcriptional event.
Nucleosome assembly "in vitro".
Nucleosomes can be assembled "in vitro" by either using purified native or recombinant histones. One standard technique of loading the DNA around the histones involves the use of salt dialysis. A reaction consisting of the histone octamers and a naked DNA template can be incubated together at a salt concentration of 2 M. By steadily decreasing the salt concentration, the DNA will equilibrate to a position where it is wrapped around the histone octamers, forming nucleosomes. In appropriate conditions, this reconstitution process allows for the nucleosome positioning affinity of a given sequence to be mapped experimentally.
Gallery.
The crystal structure of the nucleosome core particle (PDB ID: 1EQZ ) - different views showing details of histone folding and organization. Histones , , , and are coloured.

</doc>
<doc id="21847" url="https://en.wikipedia.org/wiki?curid=21847" title="Nordic">
Nordic

Nordic commonly refers to:
Nordic may also refer to:

</doc>
<doc id="21848" url="https://en.wikipedia.org/wiki?curid=21848" title="Neurosurgery">
Neurosurgery

Neurosurgery (or neurological surgery) is the medical specialty concerned with the prevention, diagnosis, treatment, and rehabilitation of disorders which affect any portion of the nervous system including the brain, spinal cord, peripheral nerves, and extra-cranial cerebrovascular system.
Education and context.
In different countries, there are different requirements for an individual to legally practice neurosurgery, and there are varying methods through which they must be educated. In most countries, neurosurgeon training requires a minimum period of seven years after graduating from medical school.
United States.
In the United States, a neurosurgeon must generally complete four years of undergraduate education, four years of medical school, and seven years of residency (PGY-1-7). Most, but not all, residency programs have some component of basic science or clinical research. Neurosurgeons may pursue additional training in the form of a fellowship, after residency or in some cases, as a senior resident. These fellowships include pediatric neurosurgery, trauma/neurocritical care, functional and stereotactic surgery, surgical neuro-oncology, radiosurgery, neurovascular surgery, skull-base surgery, peripheral nerve and spine surgery. In the U.S., neurosurgery is considered a highly competitive specialty composed of 0.6% of all practicing physicians.
United Kingdom.
In the United Kingdom, students must gain entry into medical school. MBBS qualification (Bachelor of Medicine, Bachelor of Surgery) takes four to six years depending on the student's route. The newly qualified physician must then complete foundation training lasting two years; this is a paid training program in a hospital or clinical setting covering a range of medical specialties including surgery. Junior doctors then apply to enter the neurosurgical pathway. Unlike most other surgical specialties, it currently has its own independent training pathway which takes around eight years (ST1-8); before being able to sit for consultant exams with sufficient amounts of experience and practice behind them. Neurosurgery remains consistently amongst the most competitive medical specialties in which to obtain entry.
History.
Neurosurgery, or the premeditated incision of ones head for pain relief, has been around for thousands of years. But, notable advancements in neurosurgery have only came within the last hundred years.
Ancient.
Around 7000 years ago the beginning stages of neurosurgery were starting to develop. A procedure known as trepanation, or burrowing, was used to cure people of an "affliction", where cutting and removing sections of the skull would relieve intracranial pressure. The reasons to initiate such a procedure are up to debate and vary among the different cultures who practiced. The procedure is still practiced today in parts of Africa, South America, and Melanesia.
Modern.
There was not much advancement in neurosurgery until late 19th early 20th century, when electrodes were placed on the brain and superficial tumors were removed.
History of electrodes in the brain: In 1878 Richard Canton discovered that electrical signals transmitted through an animal's brain. In 1950 Dr. Jose Delgado invented the first electrode that was implanted in an animal's brain, using it to make it run and change direction. In 1972 the cochlear implant, a neurological prosthetic that allowed deaf people to hear was marketed for commercial use. In 1998 researcher Philip Kennedy implanted the first Brain Computer Interface (BCI) into a human subject.
History of tumor removal: In 1879 after locating it via neurological signs alone, Scottish surgeon William Macewen (1848-1924) performed the first successful brain tumor removal. On November 25, 1884 after English physician Alexander Hughes Bennett (1848-1901) used Macewen's technique to locate it, English surgeon Rickman Godlee (1849-1925) performed the first primary brain tumor removal, which differs from Macewen's operation in that Bennett operated on the exposed brain, whereas Macewen operated outside of the "brain proper" via trepanation. On March 16, 1907 Austrian surgeon Hermann Schloffer became the first to successfully remove a pituitary tumor.
Modern Surgical Instruments.
The main advancements in neurosurgery came about as a result of highly crafted tools. Modern neurosurgical tools, or instruments, include chisels, curettes, dissectors, distractors, elevators, forcepts, hooks, impactors, probes, suction tubes, power tools, and robots. Most of these modern tools, like chisels, elevators, forcepts, hooks, impactors, and probes, have been in medical practice for a relatively long time. The main difference of these tools, pre and post advancement in neurosurgery, were the precision in which they were crafted. These tools are crafted with edges that are within a millimeter of desired accuracy. Other tools such as hand held power saws and robots have only recently been commonly used inside of a neurological operating room. Robotics have played an instrumental role in making neurosurgery safer, more precise, and less invasive. In 1985 Programmable Universal Machine for Assembly (PUMA) was the first robot to assist in neurological surgery. PUMA was able to guide incisions into the brain based off an x,y,z coordinate frame placed around the patients head. In 1991 Minerva was the first robot to provide real time imagery of the brain as it moved during the procedure. This allowed the surgeon to change their incision trajectory as the procedure progressed. In 1995 The Steady Hand System allowed surgeons to operate on patients and minimize slight tremors caused by the hand of the surgeon.
Main divisions of neurosurgery.
General neurosurgery involves most neurosurgical conditions including neuro-trauma and other neuro-emergencies such as intracranial hemorrhage. Most level 1 hospitals have this kind of practice.
Specialized branches have developed to cater to special and difficult conditions. These specialized branches co-exist with general neurosurgery in more sophisticated hospitals. To practice advanced specialization within neurosurgery, additional higher fellowship training of one to two years is expected from the neurosurgeon.
Some of these divisions of neurosurgery are:
Neuropathology.
Neuropathology is a specialty within the study of Pathology focused on the disease of the brain, spinal cord, and neural tissue. This includes the central nervous system and the peripheral nervous system. Tissue analysis comes from either surgical biopsies or post mortem autopsies. Common tissue samples include muscle fibers and nervous tissue. Common applications of neuropathology include studying samples of tissue in patients who have Parkinson's disease, Alzheimer's disease, Dementia, Huntington's disease, Amyotrophic Lateral Sclerosis, Mitochondria Disease, and any disorder that has neural deterioration in the brain or spinal cord.
History.
Pathology has been studied ever since men have decided to cut each other open and see what is inside, thousands of years. But, only within the last few hundred years has medicine focused on a tissue and organ based approach to tissue disease. In 1810 Dr. Thomas Hodgkin started to look at the damaged tissue for the cause, not the gods. This was conjoined with the emergence of microscopy and started the current understanding of how we study the tissue of the human body.
Neuroanesthesia.
Neuroanesthesia is a field of anesthesiology which focuses on neurosurgery. Anesthesia is not used during the middle of an "awake" brain surgery. Awake brain surgery is where the patient is conscious for the middle of the procedure and sedated for the beginning and end. This procedure is used when the tumor does not have clear boundaries and the surgeon wants to know if they are invading on critical regions of the brain which involve functions like talking, cognition, vision, and hearing. It will also be conducted for procedures which the surgeon is trying to combat epileptic seizures
History.
Early forms of neuroanesthesia were found during procedures of trepanning in Southern America, like Peru. In these procedures coca leaves and datura plants were used to manage pain as the person had dull primitive tools cut open their skull. In 400 BC The physician Hippocrates made accounts of using different wines to sedate patients while trepanning. In 60 AD Dioscorides, a physician, pharmacologist, and botanist, detailed how mandrake, henbane, opium, and alcohol were used to put patients to sleep during trepanning. In 972 AD two brother surgeons, in modern-day India, used "samohine" to sedate a patient while removing a small tumor and awoke the patient by pouring onion and vinegar in the patients mouth. Since then, multiple cocktails have been derived in order to sedate a patient during a brain surgery. The most recent form of nueroanesthesia is the combination of carbon dioxide, hydrogen, and nitrogen. This was discovered in the 18th century by Sir Humphry Davy and brought into the operating room by Sir Astley Cooper.
Neurosurgery methods.
Neuroradiology plays a key role not only in diagnosis but also in the operative phase of neurosurgery.
Neuroradiology methods are used in modern neurosurgery diagnosis and treatment. They include computer assisted imaging computed tomography (CT), magnetic resonance imaging (MRI), positron emission tomography (PET), magnetoencephalography (MEG), and stereotactic radiosurgery. Some neurosurgery procedures involve the use of intra-operative MRI and functional MRI.
In "conventional open surgery" the neurosurgeon opens the skull, creating a large opening to access the brain. Techniques involving smaller openings with the aid of microscopes and endoscopes are now being used as well. Methods that utilize small craniotomies in conjunction with high-clarity microscopic visualization of neural tissue offer excellent results. However, the open methods are still traditionally used in trauma or emergency situations.
"Microsurgery" is utilized in many aspects of neurological surgery. Microvascular techniques are used in EC-IC bypass surgery and in restoration carotid endarterectomy. The clipping of an aneurysm is performed under microscopic vision. minimally-invasive spine surgery utilizes microscopes or endoscopes. Procedures such as microdiscectomy, laminectomy, and artificial disc replacement rely on microsurgery.
Using "stereotaxy" neurosurgeons can approach a minute target in the brain through a minimal opening. This is used in functional neurosurgery where electrodes are implanted or gene therapy is instituted with high level of accuracy as in the case of Parkinson's disease or Alzheimer's disease. Using the combination method of open and stereotactic surgery, intraventricular hemorrhages can potentially be evacuated successfully. Conventional surgery using image guidance technologies is also becoming common and is referred to as surgical navigation, computer assisted surgery, navigated surgery, stereotactic navigation. Similar to a car or mobile Global Positioning System (GPS), image guided surgery systems, like Curve Image Guided Surgery and StealthStation, use cameras or electromagnetic fields to capture and relay the patient’s anatomy and the surgeon’s precise movements in relation to the patient, to computer monitors in the operating room. These sophisticated computerized systems are used before and during surgery to help orient the surgeon with three-dimensional images of the patient’s anatomy including the tumor.
Minimally invasive "endoscopic surgery" is commonly utilized by neurosurgeons when appropriate. Techniques such as endoscopic endonasal surgery are used in pituitary tumors, craniopharyngiomas, chordomas, and the repair of cerebrospinal fluid leaks. Ventricular endoscopy is used in the treatment of intraventricular bleeds, hydrocephalus, colloid cyst and neurocysticercosis. Endonasal endoscopy is at times carried out with neurosurgeons and ENT surgeons working together as a team.
Repair of craniofacial disorders and disturbance of cerebrospinal fluid circulation is done by neurosurgeons who also occasionally team up with maxillofacial and plastic surgeons. Cranioplasty for craniosynostosis is performed by pediatric neurosurgeons with or without plastic surgeons.
Neurosurgeons are involved in "stereotactic radiosurgery" along with radiation oncologists in tumor and AVM treatment. Radiosurgical methods such as Gamma knife, Cyberknife and Novalis Radiosurgery are used as well.
"Endovascular Neurosurgery" utilize endovascular image guided procedures for the treatment of aneurysms, AVMs, carotid stenosis, strokes, and spinal malformations, and vasospasms. Techniques such as angioplasty, stenting, clot retrieval, embolization, and diagnostic angiography are endovascular procedures.
A common procedure performed in neurosurgery is the placement of Ventriculo-Peritoneal Shunt (VP Shunt). In pediatric practice this is often implemented in cases of congenital hydrocephalus. The most common indication for this procedure in adults is Normal Pressure Hydrocephalus (NPH).
"Neurosurgery of the spine" covers the cervical, thoracic and lumbar spine. Some indications for spine surgery include spinal cord compression resulting from trauma, arthritis of the spinal discs, or spondylosis. In cervical cord compression, patients may have difficulty with gait, balance issues, and/or numbness and tingling in the hands or feet. Spondylosis is the condition of spinal disc degeneration and arthritis that may compress the spinal canal. This condition can often result in bone-spurring and disc herniation. Power drills and special instruments are often used to correct any compression problems of the spinal canal. Disc herniations of spinal vertebral discs are removed with special rongeurs. This procedure is known as a "discectomy". Generally once a disc is removed it is replaced by an implant which will create a bony fusion between vertebral bodies above and below. Instead, a mobile disc could be implanted into the disc space to maintain mobility. This is commonly used in cervical disc surgery. At times instead of disc removal a Laser discectomy could be used to decompress a nerve root. This method is mainly used for lumbar discs. "Laminectomy" is the removal of the Lamina portion of the vertebrae of the spine in order to make room for the compressed nerve tissue.
Radiology assisted spine surgery uses minimally-invasive procedures. They include the techniques of "vertebroplasty" and "kyphoplasty" in which certain types of spinal fractures are managed.
Potentially unstable spines will need "spine fusions". At present these procedures include complex instrumentation. Spine fusions could be performed as open surgery or as minimally invasive surgery. Anterior cervical diskectomy and fusion is a common surgery that is performed for disc disease of cervical spine.
However, each method described above may not work in all patients. Therefore, careful selection of patients for each procedure is important. It has to be noted that if there is prior permanent neural tissue damage spinal surgery may not take away the symptoms.
Surgery for chronic pain is a sub branch of functional neurosurgery. Some of the techniques include implantation of deep brain stimulators, spinal cord stimulators, peripheral stimulators and pain pumps.
Surgery of the peripheral nervous system is also possible, and includes the very common procedures of carpal tunnel decompression and peripheral nerve transposition. Numerous other types of nerve entrapment conditions and other problems with the peripheral nervous system are treated as well.
Conditions.
Other conditions treated by neurosurgeons include:

</doc>
<doc id="21849" url="https://en.wikipedia.org/wiki?curid=21849" title="Nintendo 64">
Nintendo 64

The , stylized as NINTENDO64 and often referred to as N64, is Nintendo's third home video game console for the international market. Named for its 64-bit central processing unit, it was released in June 1996 in Japan, September 1996 in North America, March 1997 in Europe and Australia, September 1997 in France and December 1997 in Brazil. It is the industry's last major home console to use the cartridge as its primary storage format, although current handheld systems (such as the PlayStation Vita and Nintendo 3DS) also use cartridges. While the Nintendo 64 was succeeded by Nintendo's MiniDVD-based GameCube in November 2001, the consoles remained available until the system was retired in late 2003.
Code named Project Reality, the console's design was mostly finalized by mid-1995, though Nintendo 64's launch was delayed until 1996. As part of the fifth generation of gaming, the system competed primarily with the PlayStation and the Sega Saturn. The Nintendo 64 was launched with three games: "Super Mario 64" and "Pilotwings 64", released worldwide; and "Saikyō Habu Shōgi", released only in Japan. The Nintendo 64's suggested retail price at launch was and it was later marketed with the slogan "Get N, or get Out!". With 32.93 million units worldwide, the console was ultimately released in a range of different colors and designs, and an assortment of limited-edition controllers were sold or used as contest prizes during the system's lifespan. IGN named it the 9th greatest video game console of all time; and in 1996, "Time Magazine" named it Machine of the Year.
History.
Development.
At the beginning of the 1990s, Nintendo led the video game industry with its Nintendo Entertainment System (NES). Although the NES follow-up console, the Super NES (SNES), was successful, sales took a hit from the Japanese recession. Competition from long-time rival Sega, and relative newcomer Sony, emphasized Nintendo's need to develop a successor for the SNES, or risk losing market dominance to its rivals. Further complicating matters, Nintendo also faced a backlash from third-party developers unhappy with Nintendo's strict licensing policies.
Silicon Graphics, Inc. (SGI), a long-time leader in graphics visualization and supercomputing, was interested in expanding its business by adapting its technology into the higher volume realm of consumer products, starting with the video game market. Based upon its MIPS R4000 family of supercomputing and workstation CPUs, SGI developed a CPU requiring a fraction of the resources—consuming only 0.5 watts of power instead of 1.5 to 2 watts, with an estimated target price of instead of –200. The company created a design proposal for a video game system, seeking an already well established partner in that market. James H. Clark, founder of SGI, initially offered the proposal to Tom Kalinske, who was the CEO of Sega of America. The next candidate was Nintendo.
The historical details of these preliminary negotiations were controversial between the two competing suitors. Tom Kalinske said that he and Joe Miller of Sega of America were "quite impressed" with SGI's prototype, inviting their hardware team to travel from Japan to meet with SGI. The engineers from Sega Enterprises claimed that their evaluation of the early prototype had uncovered several unresolved hardware issues and deficiencies. Those were subsequently resolved, but Sega had already decided against SGI's design. Nintendo resisted that summary conclusion, arguing that the reason for SGI's ultimate choice of partner is due to Nintendo having been a more appealing business partner than Sega. While Sega demanded exclusive rights to the chip, Nintendo was willing to license the technology on a non-exclusive basis. Michael Slater, publisher of "Microprocessor Report" said, "The mere fact of a business relationship there is significant because of Nintendo's phenomenal ability to drive volume. If it works at all, it could bring MIPS to levels of volume never dreamed of."
James Clark met with Nintendo CEO Hiroshi Yamauchi in early 1993, thus initiating Project Reality. On August 23, 1993, the two companies announced a global joint partnership and licensing agreement surrounding Project Reality, projecting that the yet unnamed eventual product would be "developed specifically for Nintendo, will be unveiled in arcades in 1994, and will be available for home use by late 1995 ... below $250." This announcement coincided with Nintendo's August 1993 Shoshinkai trade show.
As with most of the computing industry, Nintendo had limited experience with 3D graphics, and worked with several outside companies to develop the technology comprising the console. Some chip technology was provided by NEC, Toshiba, and Sharp. SGI had recently acquired MIPS Computer Systems, and the two worked together toward a low-cost realtime 3D graphics hardware system. SGI and its subsidiary MIPS Technologies were responsible for the R4300i microprocessor and the 3D graphics hardware used in the Nintendo 64.
The initial Project Reality software development platform was developed and sold by SGI in the form of its – Onyx supercomputer loaded with the namesake RealityEngine2 graphics boards and four 150 MHz R4400 CPUs, and with early Project Reality application and emulation APIs. By purchasing and developing upon this graphics supercomputing platform, Nintendo and its select game developer partners could fully prototype their games according to SGI's estimated console performance, prior to the finalization of the console hardware specifications. That software-based console prototype platform was later supplanted by a workstation-hosted console simulation board, representing the finalized console hardware. SGI's performance estimates based upon their RealityEngine supercomputing platform were ultimately reported to be fairly accurate to the final consumer console product.
The console's design was publicly revealed for the first time in late Q2 1994. Images of the console displayed the Nintendo Ultra 64 logo, a ROM cartridge, but no controller. This prototype console's form factor would be retained by the product eventually launched as Nintendo 64 due to a legal issue with Konami. Having initially indicated the possibility of utilizing the increasingly popular CD-ROM if the medium's endemic performance problems were solved, the company now announced a much faster but space-limited cartridge-based system, which prompted open analysis by the gaming press. The system was frequently marketed as the world's first 64-bit gaming system, often stating the console was more powerful than the first moon landing computers. Atari had already claimed to have made the first 64-bit game console with their Atari Jaguar, but the Jaguar only uses a general 64-bit architecture in conjunction with two 32-bit RISC processors and a 16/32-bit Motorola 68000.
Later in Q2 1994, Nintendo signed a licensing agreement with Midway's parent company which enabled Midway to develop and market arcade games using the Project Reality hardware and formed a joint venture company called Williams/Nintendo to market Nintendo-exclusive home conversions of these games. The result is two arcade games, "Killer Instinct" and "Cruis'n USA", which boasted their upcoming release on the arcade branch of the Nintendo Ultra 64 platform. Compared to the console branch of Ultra 64, the arcade branch uses a different MIPS CPU, has no Reality Coprocessor, and uses onboard ROM chips and a hard drive instead of a cartridge. "Killer Instinct" features pre-rendered character artwork, and CG movie backgrounds that are streamed off the hard drive and animated as the characters move horizontally.
The completed Nintendo 64 was fully unveiled to the public in a playable form on November 24, 1995, at the 7th Annual Shoshinkai Software Exhibition in Japan. Nintendo's next-generation console was introduced as the "Nintendo 64" (a name given by Shigesato Itoi), contrary to speculation that it would be called "Nintendo Ultra 64". Photos of the event were disseminated on the web by "Game Zero" magazine two days later. Official coverage by Nintendo followed later via the "Nintendo Power" website and print magazine.
In the lead up to the console's release, Nintendo had adopted a new global branding strategy, assigning the console the same name for all markets: Nintendo 64. Previously, the plan had been to release the console as the Ultra Famicom in Japan and as the Nintendo Ultra 64 in other markets. Nintendo said that trademark issues were not a factor, and the sole reason for the name change was to establish a single worldwide brand and logo for the console. The prefix for the model numbering scheme for hardware and software across the Nintendo 64 platform is "NUS-", a reference to the console's original name, "Nintendo Ultra Sixty-four".
The console was originally slated for release by Christmas of 1995. In May 1995, Nintendo pushed back the release to April 1996. The prospect of a release the following year at a lower price than the competition lowered sales of competing Sega and Sony consoles during the important Christmas shopping season. "Electronic Gaming Monthly" editor Ed Semrad even suggested that Nintendo may have announced the April 1996 release date with this end in mind, knowing in advance that the system would not be ready by that date.
In its explanation of the delay, Nintendo claimed it needed more time for Nintendo 64 software to mature, and for third-party developers to produce games. Adrian Sfarti, a former engineer for SGI, attributed the delay to hardware problems; he claimed that the chips underperformed in testing and were being redesigned. In 1996, the Nintendo 64's software development kit was redesigned as the Partner-N64 system, by Kyoto Microcomputer, Co. Ltd. of Japan.
To counter the possibility that gamers would grow impatient with the wait for the Nintendo 64 and purchase one of the several competing consoles already on the market, Nintendo ran ads for the system well in advance of its announced release dates, with slogans like "Wait for it..." and "Is it worth the wait? Only if you want the best!"
Release.
"Popular Electronics" called the launch a "much hyped, long-anticipated moment."
The console was first released in Japan on June 23, 1996. The Nintendo 64 was first sold in North America on September 26, 1996, despite having been advertised for the 29th. It was launched with just two games in the United States, "Pilotwings 64" and "Super Mario 64". In 1994, prior to the launch, Nintendo of America chairman Howard Lincoln emphasized the quality of first-party games, saying "... we're convinced that a few great games at launch are more important than great games mixed in with a lot of dogs." The PAL version of the console was released in Europe on March 1, 1997.
Originally intended to be priced at , the console was ultimately launched at to make it competitive with Sony and Sega offerings (Both the Saturn and PlayStation had been lowered to $199.99 earlier that summer). Nintendo priced the console as an impulse purchase, a strategy from the toy industry. The price of the console in the United States was further reduced in August 1998.
Promotion.
To boost sales during the slow post-Christmas season, Nintendo and General Mills worked together on a promotional campaign that appeared in early 1999. A television advertising campaign cost $5 million. The advertisement by Saatchi and Saatchi, New York began on January 25 and encouraged children to buy Fruit by the Foot snacks for tips to help them with their Nintendo 64 games. Ninety different tips were available, with three variations of thirty tips each.
Nintendo advertised its Funtastic Series of peripherals with a $10 million print and television campaign from February 28 to April 30, 2000. Leo Burnett, Chicago, was in charge.
Reception.
Critical reception.
The Nintendo 64 received generally positive reviews from critics. Reviewers generally praised the console's advanced 3D graphics and gameplay, while criticizing the lack of games. On G4techTV's (now G4's) "Filter", the Nintendo 64 was voted up to No. 1 by registered users.
In February 1996, "Next Generation" magazine called the Nintendo Ultra 64 the "best kept secret in videogames" and the "world's most powerful game machine". It called the system's November 24, 1995 unveiling at Shoshinkai "the most anticipated videogaming event of the 1990s, possibly of all time." Previewing the Nintendo 64 shortly prior to its launch, "Time Magazine" praised the realistic movement and gameplay provided by the combination of fast graphics processing, pressure-sensitive controller, and the "Super Mario 64" game. The review praised the "fastest, smoothest game action yet attainable via joystick at the service of equally virtuoso motion", where "or once, the movement on the screen feels real".
At launch, the "Los Angeles Times" called the system "quite simply, the fastest, most graceful game machine on the market". Its form factor was described as small, light, and "built for heavy play by kids" unlike the "relatively fragile Sega Saturn". Showing concern for a major console product launch during a sharp, several-year long, decline in the game console market, the review said that the long-delayed Nintendo 64 was "worth the wait" in the company's pursuit of quality. Nintendo's "penchant for perfection" in game quality control was praised, though with concerns about having only two launch titles at retail and twelve expected by Christmas. Describing the quality control incentives associated with cartridge-based development, the "Times" cited Nintendo's position that cartridge game developers tend to "place a premium on substance over flash", and noted that the launch titles lack the "poorly acted live-action sequences or half-baked musical overtures" which it says tend to be found on CD-ROM games. Praising Nintendo's controversial choice of the cartridge medium with its "nonexistent" load times and "continuous, fast-paced action CD-ROMs simply cannot deliver", the review concluded that "the cartridge-based Nintendo 64 delivers blistering speed and tack-sharp graphics that are unheard of on personal computers and make competing 32-bit, disc-based consoles from Sega and Sony seem downright sluggish".
"Time Magazine" named it their 1996 Machine of the Year, saying the machine had "done to video-gaming what the 707 did to air travel." The magazine said the console achieved "the most realistic and compelling three-dimensional experience ever presented by a computer." "Time" credited the Nintendo 64 with revitalizing the video game market, "rescuing this industry from the dustbin of entertainment history." The magazine suggested that the Nintendo 64 would play a major role in introducing children to digital technology in the final years of the 20th century. The article concluded by saying the console had already provided "the first glimpse of a future where immensely powerful computing will be as common and easy to use as our televisions."
"Popular Electronics" complimented the system's hardware, calling its specifications "quite impressive." It found the controller "comfortable to hold, and the controls to be accurate and responsive."
Developer Factor 5, who created some of the system's most technologically advanced games along with the system's audio development tools for Nintendo, said, "N64 is really sexy because it combines the performance of an SGI machine with a cartridge. We're big arcade fans, and cartridges are still the best for arcade games or perhaps a really fast CD-ROM. But there's no such thing for consoles yet [as of 1998."
Sales.
The Nintendo 64 was in heavy demand upon its release. David Cole, industry analyst, said "You have people fighting to get it from stores." "Time Magazine" called the purchasing interest "that rare and glorious middle-class Cabbage Patch-doll frenzy." The magazine said celebrities Matthew Perry, Steven Spielberg's office, and some Chicago Bulls players called Nintendo to ask for special treatment to get their hands on the console.
During the system's first three days on the market, retailers sold 350,000 of 500,000 available console units. During its first four months, the console yielded 500,000 unit sales in North America. Nintendo successfully outsold Sony and Sega early in 1997 in the US; and by the end of its first full year, 3.6 million units were sold in the United States. "BusinessWire" reported that the Nintendo 64 was responsible for Nintendo's sales having increased by 156% by 1997.
After a strong launch year, the decision to use the cartridge format is said to have contributed to the diminished release pace and higher price of games compared to the competition, and thus Nintendo was unable to maintain their lead in America. The company would continue to outsell the Sega Saturn throughout the generation, but would trail behind the PlayStation.
In Japan, the console was not as successful, failing to outsell the PlayStation and even the Sega Saturn. Benimaru Itō, a developer for "EarthBound 64" and friend of Shigeru Miyamoto, speculated in 1997 that the Nintendo 64's lower popularity in Japan was due to the lack of role-playing video games.
Nintendo reported that the system's vintage hardware and software sales had ceased by 2004, three years after the GameCube's launch; as of December 31, 2009, the Nintendo 64 had yielded a lifetime total of 5.54 million system units sold in Japan, 20.63 million in the Americas, and 6.75 million in other regions, for a total of 32.93 million units.
Legacy.
The Nintendo 64 remains one of the most recognized video game systems in history and its games still have impact on the games industry. Designed in tandem with the controller, "Super Mario 64" and "" are widely considered by critics and the public to be some of the greatest and most influential games of all time. "GoldenEye 007" is one of the most influential games for the shooter genre.
The Aleck 64 is a Nintendo 64 design in arcade form, designed by Seta in cooperation with Nintendo, and sold from 1998 to 2003 only in Japan.
Games.
A total of 388 games were released for the console, though there were a few that were exclusively sold in Japan. For comparison, rivals PlayStation and the Sega Saturn received around 1,100 games and 600 games respectively, while previous Nintendo consoles such as the NES and SNES had 768 and 725 games released in the USA. However, the Nintendo 64 game library included a high number of critically acclaimed and widely sold games. "Super Mario 64" is the best selling game of the generation, with 11 million units sold and beating the PlayStation's "Gran Turismo" (at 10.85 million) and "Final Fantasy VII" (at 9.72 million) in sales. The game also received much praise from critics and helping to pioneer three-dimensional control schemes. "GoldenEye 007" was important in the evolution of the first-person shooter, and has been named one of the greatest in the genre. "" set the standard for future 3D action-adventure games and is considered by many to be one of the greatest games ever made.
Graphics.
The most graphically demanding Nintendo 64 games that arrived on larger 32 or 64 MB cartridges are the most advanced and detailed of the 32-bit/64-bit generation. In order to maximize use of the Nintendo 64 hardware developers had to create their own alternate bespoke custom microcode. Nintendo 64 games running on custom microcode benefited from much higher polygon counts in tandem with more advanced lighting, animation, physics and AI routines than its 32-bit competition. "Conker's Bad Fur Day" is arguably the pinnacle of its generation combining multicolored real-time lighting that illuminates each area to real-time shadowing and detailed texturing replete with a full in game facial animation system. The Nintendo 64's graphics chip is capable of executing many more advanced and complex rendering techniques than its competitors. It is the first home console to feature trilinear filtering, which allowed textures to look very smooth. This contrasted with the Saturn and PlayStation, which used nearest-neighbor interpolation and produced more pixelated textures. Overall however the results of the Nintendo cartridge system were mixed and this was tied primarily to its storage medium.
The smaller storage size of ROM cartridges limited the number of available textures. As a result, many games which utilized much smaller 8 or 12 MB cartridges are forced to stretch textures over larger surfaces. Compounded by a limit of 4,096-bytes allocated for texture storage, the end-result is often a distorted, out-of-proportion appearance. Many titles that feature larger 32 or 64 MB cartridges avoided this issue entirely, notable games include "Resident Evil 2", "Sin and Punishment: Successor of the Earth", and "Conker's Bad Fur Day" as they feature more ROM space, allowing for more detailed graphics by utilizing multiple, multi-layered textures across all surfaces.
Game Paks.
Nintendo 64 games are ROM cartridge based. Cartridge size varies from 4 to 64 MB. ROM cartridges are expensive and time-consuming to manufacture. Many cartridges include the ability to save games internally.
Nintendo cited several advantages for making the Nintendo 64 cartridge-based. Primarily cited was the ROM cartridges' very fast load times in comparison to disc-based games. While loading screens appear in many PlayStation games, they are rare on the Nintendo 64. Although vulnerable to long-term environmental damage the cartridges are far more resistant to physical damage than compact discs. Nintendo also cited the fact that cartridges are more difficult to pirate than CDs.
On the downside, cartridges took longer to manufacture than CDs, with each production run (from order to delivery) taking two weeks or more. This meant that publishers of Nintendo 64 games had to attempt to predict demand for a game ahead of its release. They risked being left with a surplus of expensive cartridges for a failed game or a weeks-long shortage of product if they underestimated a game's popularity. The cost of producing a Nintendo 64 cartridge was also far higher than for a CD. Publishers passed these expenses onto the consumer. Nintendo 64 games cost an average of $10 more when compared to games produced for rival consoles.
As fifth generation games became more complex in content, sound and graphics, games began to exceed the limits of cartridge storage capacity. Nintendo 64 cartridges had a maximum of 64 MB of data, whereas CDs held over 650 MB. Software ported from other platforms was often heavily compressed or redesigned with the storage limits of a cartridge in mind. Due to the cartridge's space limitations, full motion video was not usually feasible for use in cutscenes. When it was present, it was heavily compressed to fit on the cartridge and usually of very brief length.
The era's competing systems from Sony and Sega (the PlayStation and Saturn, respectively) used CD-ROM discs to store their games. As a result, game developers who had traditionally supported Nintendo game consoles were now developing games for the competition. Some third-party developers, such as Square and Enix, whose "Final Fantasy VII" and "Dragon Warrior VII" were initially planned for the Nintendo 64, switched to the PlayStation, citing the insufficient storage capacity of the N64 cartridges. Some who remained released fewer games to the Nintendo 64; Konami released fifty PlayStation games, but only thirteen for the Nintendo 64. New Nintendo 64 game releases were infrequent while new games were coming out rapidly for the PlayStation.
Through the difficulties with third parties, the Nintendo 64 supported popular games such as "GoldenEye 007", giving it a long market life. Additionally, Nintendo's strong first-party franchises such as "Mario" had strong name brand appeal. Second-parties of Nintendo, such as Rare, helped.
Nintendo's controversial selection of the cartridge medium for the Nintendo 64 has been cited as a key factor in Nintendo losing its dominant position in the gaming market. Some of the cartridge's advantages are difficult for developers to manifest prominently, requiring innovative solutions which only came late in the console's life cycle.
One of its technical drawbacks is a limited texture cache, which can hold textures of limited dimensions and reduced color depth, which must be stretched to cover larger in-game surfaces. Its vintage ROM cartridges are constrained by small capacity and high production expenses, compared to the compact disc format used by its chief competitors. Some third-party publishers that supported Nintendo's previous consoles reduced their output or stopped publishing for the console; the Nintendo 64's most successful games came from first-party or second-party studios.
Emulation.
Several Nintendo 64 games have been released for the Wii's and Wii U's Virtual Console service and are playable with either the Classic Controller, Nintendo GameCube controller, or Wii U GamePad. There are some differences between these versions and the original cartridge versions. For example, the games run in a higher resolution and at a more consistent framerate than their Nintendo 64 counterparts. However, some features, such as Rumble Pak functionality, are not available in the Wii versions. Some features are also altered for the Virtual Console releases. For example, the VC version of "Pokémon Snap" allows players to send photos through the Wii's message service, while "Wave Race 64"'s in-game content was altered due to the expiration of the Kawasaki license. Several games from Rare have seen release on Microsoft's Xbox Live Arcade service, including "Banjo-Kazooie", "Banjo-Tooie" and "Perfect Dark", the reason being that Rareware was purchased by Microsoft in 2002. However one exception was Donkey Kong 64, which was released in April 2015 on the Wii U Virtual Console since Nintendo owns the rights to that game.
Several unofficial emulation systems have been developed in order to execute Nintendo 64 titles on multiple platforms, such as PCs.
Accessories.
A number of accessories were produced for the Nintendo 64, including the Rumble Pak and the Transfer Pak.
The controller was shaped like an "M", employing a joystick in the center. "Popular Electronics" called its shape "evocative of some alien space ship." While noting that the three handles could be confusing, the magazine said "the separate grips allow different hand positions for various game types."
64DD.
Nintendo released a peripheral platform called 64DD, where "DD" stands for "Disk Drive". Connecting to the expansion slot at the bottom of the system, the 64DD turns the Nintendo 64 console into an Internet appliance, a multimedia workstation, and an expanded gaming platform. This large peripheral allows players to play Nintendo 64 disk-based games, capture images from an external video source, and it allowed players to connect to the now-defunct Japanese Randnet online service. Not long after its limited mail-order release, the peripheral was discontinued. Only nine games were released, including the four "Mario Artist" games ("Paint Studio", "Talent Studio", "Communication Kit", and "Polygon Studio"). Many more planned games were eventually released in cartridge format or on other game consoles. The 64DD and the accompanying Randnet online service were released only in Japan, despite always being announced for America and Europe.
To illustrate the fundamental significance of the 64DD to all game development at Nintendo, lead designer Shigesato Itoi said, "I came up with a lot of ideas because of the 64DD. All things start with the 64DD. There are so many ideas I wouldn’t have been allowed to come up with if we didn’t have the 64DD." Shigeru Miyamoto concluded, "Almost every new project for the N64 is based on the 64DD. ... we’ll make the game on a cartridge first, then add the technology we’ve cultivated to finish it up as a full-out 64DD game."
Technical specifications.
Hardware.
The Nintendo 64's central processing unit (CPU) is the NEC VR4300. "Popular Electronics" said it had power similar to the Pentium processors found in desktop computers. Except for its narrower 32-bit system bus, the VR4300 retained the computational abilities of the more powerful 64-bit MIPS R4300i, though software rarely took advantage of 64-bit data precision operations. Nintendo 64 games generally used faster (and more compact) 32-bit data-operations, as these were sufficient to generate 3D-scene data for the console's RSP (Reality Signal Processor) unit. In addition, 32-bit code executes faster and requires less storage space (which is at a premium on the Nintendo 64's cartridges).
In terms of its random-access memory, or RAM, the Nintendo 64 is one of the first modern consoles to implement a unified memory subsystem, instead of having separate banks of memory for CPU, audio, and video, for example. The memory itself consists of 4 megabytes of RDRAM, made by Rambus. The RAM is expandable to 8 MB with the Expansion Pak. Rambus was quite new at the time and offered Nintendo a way to provide a large amount of bandwidth for a relatively low cost.
The system allows for video output in two formats: composite video and S-Video. The composite and S-Video cables are the same as those used with the earlier SNES and later GameCube systems.
The Nintendo 64 supports 16.8 million colors. The system can display resolutions from 320 × 240 up to 640 × 480 pixels. Most games that made use of the systems higher resolution 640x480 mode required use of the Expansion Pak RAM upgrade; there were a number however which did not, such as Acclaim's NFL Quarterback Club series and EA Sports 2nd generation Madden, FIFA, Supercross, and NHL games which arrived on the system. The majority of games used the system's low resolution 320 × 240 mode. A number of games also support a video display ratio of up to using either Anamorphic widescreen or Letterboxing.
Color variants.
The Nintendo 64 comes in several colors. The standard Nintendo 64 is dark gray, nearly black, and the controller is light gray (later releases in the U.S. included a bonus second controller in Atomic Purple). Various colorations and special editions were released.
Most Nintendo 64 game cartridges are gray in color, but some games have a colored cartridge. Fourteen games have black cartridges, and other colors (such as green, blue, red, yellow and gold) were each used for six or fewer games. Several games, such as "", were released both in standard gray and in colored, limited edition versions.
Programming characteristics.
The programming characteristics of the Nintendo 64 present unique challenges, with distinct potential advantages. "The Economist" described effective programming for the Nintendo 64 as being "horrendously complex". As with many other game consoles and other types of embedded systems, the Nintendo 64's architectural optimizations are uniquely acute, due to a combination of oversight on the part of the hardware designers, limitations on 3D technology of the time, and manufacturing capabilities.
As the Nintendo 64 reached the end of its lifecycle, hardware development chief Genyo Takeda repeatedly referred to the programming challenges using the word "hansei" ( "reflective regret"). Looking back, Takeda said "When we made Nintendo 64, we thought it was logical that if you want to make advanced games, it becomes technically more difficult. We were wrong. We now understand it's the cruising speed that matters, not the momentary flash of peak power."

</doc>
<doc id="21850" url="https://en.wikipedia.org/wiki?curid=21850" title="GNU nano">
GNU nano

nano is a text editor for Unix-like computing systems or operating environments using a command line interface. It emulates the Pico text editor, part of the Pine email client, and also provides additional functionality.
In contrast to Pico, nano is licensed under the GNU General Public License (GPL). Released as free software by Chris Allegretta in 1999, nano became part of the GNU Project in 2001.
History.
nano was first created in 1999 with the name "TIP" ("This Isn't Pico"), by Chris Allegretta. His motivation was to create a free software replacement for Pico, which was not distributed under a free software license. The name was changed to nano on January 10, 2000 to avoid a naming conflict with the existing Unix utility "tip". The name comes from the system of SI prefixes, in which nano is 1000 times larger than pico. In February 2001, nano became a part of the GNU Project.
nano implements some features that Pico lacks, including colored text, regular expression search and replace, smooth scrolling, multiple buffers, rebindable key support, and undoing and redoing of edit changes.
On August 11, 2003, Chris Allegretta officially handed the source code maintenance for nano to David Lawrence Ramsey. On December 20, 2007, Ramsey stepped down as nano's maintainer.
Control keys.
nano, like Pico, is keyboard-oriented, controlled with control keys. For example, saves the current file; goes to the search menu. Nano puts a two-line "shortcut bar" at the bottom of the screen, listing many of the commands available in the current context. For a complete list, gets the help screen.
Unlike Pico, nano uses meta keys to toggle its behavior. For example, toggles smooth scrolling mode on and off. Almost all features that can be selected from the command line can be dynamically toggled. On keyboards without the meta key it is often mapped to the escape key, , such that in order to simulate, say, one has to press the key, then release it, and then press the key.
Nano can also use pointer devices, such as a mouse, to activate functions that are on the shortcut bar, as well as position the cursor.

</doc>
<doc id="21851" url="https://en.wikipedia.org/wiki?curid=21851" title="Nieuwe Waterweg">
Nieuwe Waterweg

The Nieuwe Waterweg ("New Waterway") is a ship canal in the Netherlands from het Scheur (a branch of the Rhine-Meuse-Scheldt delta) west of the town of Maassluis to the North Sea at Hook of Holland: the Maasmond, where the Nieuwe Waterweg connects to the Maasgeul. It is the artificial mouth of the river Rhine.
The Nieuwe Waterweg, which opened in 1872 and has a length of approximately , was constructed to keep the city and port of Rotterdam accessible to seafaring vessels as the natural Meuse-Rhine branches silted up. The Waterway is a busy shipping route since it is the primary access to one of the busiest ports in the world, the Europoort of Rotterdam. At the entrance to the sea, a flood protection system called Maeslantkering has been installed (completed in 1997). There are no bridges or tunnels across the Nieuwe Waterweg.
History.
By the middle of the 19th century, Rotterdam was already one of the largest port cities in the world, mainly because of transshipment of goods from Germany to Great Britain. The increase in shipping traffic created a capacity problem: there were too many branches in the river delta, making the port difficult to reach.
In 1863, a law was passed that allowed for the provision of a new canal for large ocean-going ships from Rotterdam to the North Sea. Hydraulic engineer Pieter Caland was commissioned to design a canal cutting through the "Hook of Holland” and to extend the Mouth of Rhine to the sea. The designs for this were already done back in 1731 by Nicolaas Samuelsz Cruquius but the implementation could no longer be postponed to prevent the decline of the harbour of Rotterdam.
Construction began on October 31, 1863. The first phase consisted of the expropriation of farm lands from Rozenburg to Hoek van Holland.
During the second phase two dikes were built parallel to each other, which took 2 years. Caland proposed to extend the dikes 2 km into the sea to disrupt the coastal sea currents and decrease silt deposits in the shipping lane.
Upon the completion of the dikes, the third phase began by the digging of the actual waterway. This began on October 31, 1866, and was completed three years later. The large amounts of removed soil were in turn used to reinforce other dams and dikes.
The last phase consisted of the removal of the dam separating the new waterway from the sea and river. In 1872, the Nieuwe Waterweg was completed and Rotterdam was easily accessible.
Because of the currents and erosion, the shipping lane has been widened somewhat. Yet because of the draft of today's supertankers, it needs to be dredged constantly.
In 1997, the last part of the Delta Works, the Maeslantkering, was put in operation near the mouth of the Nieuwe Waterweg. This storm surge barrier protects Rotterdam against north westerly Beaufort Force 10 to 12 storms.
Current situation.
The Nieuwe Waterweg gives the Port of Rotterdam its deep-water access to the North Sea. From Hook of Holland it stretches for approximately where the waterway continues as the Nieuwe Maas. The very first Nieuwe Waterweg—a breach through the dunes at Hook of Holland—was only long, but in around 1877 the channel was made much larger and wider and the current Nieuwe Waterweg was created. Currently the width of the channel is between and it is dredged to a depth of below Amsterdam Ordnance Datum.
It is this channel, together with the dredged channels in the North Sea, Maasgeul and Eurogeul, that allows ships like the MS "Berge Stahl" and MV "Vale Rio de Janeiro" (both with a draught of 23 meters) to enter Europoort.
The Dutch government agency Rijkswaterstaat is responsible for maintaining the channel.
Maasmond.
The point where the Nieuwe Waterweg enters into the North Sea, between Hook of Holland on the north bank and the Maasvlakte to the south, is called the Maasmond. It is marked with two navigation light-towers called the Paddestoelen ("mushrooms"). The Nieuwe Waterweg connects, in the North Sea, to the Maasgeul. This dredged channel in the North Sea is being widened to to facilitate the largest container vessels for the new Maasvlakte 2 that will be opened in 2012.

</doc>
<doc id="21853" url="https://en.wikipedia.org/wiki?curid=21853" title="Neijia">
Neijia

Neijia is a term in Chinese martial arts, grouping those styles that practice "neijing", usually translated as internal martial arts, occupied with spiritual, mental or qi-related aspects, as opposed to an "external" approach focused on physiological aspects. The distinction dates to the 17th century, but its modern application is due to publications by Sun Lutang, dating to the period of 1915 to 1928. Neijing is developed by using "neigong", or "internal exercises," as opposed to "external exercises" (wàigōng 外功), 
Wudangquan is a more specific grouping of internal martial arts named for their association with the Taoist monasteries of the Wudang Mountains, Hubei in Chinese popular legend. These styles were enumerated by Sun Lutang as Taijiquan, Xingyiquan and Baguazhang, but most also include Bajiquan and the legendary Wudang Sword.
Some other Chinese arts, not in the Wudangquan group, such as Qigong, Liuhebafa, Bak Mei Pai, Zi Ran Men (Nature Boxing), Bok Foo Pai and Yiquan are frequently classified (or classify themselves) as "internal".
History.
Qing China.
The term "neijia" and the distinction between internal and external martial arts first appears in Huang Zongxi's 1669 "Epitaph for Wang Zhengnan". Stanley Henning proposes that the "Epitaph"'s identification of the internal martial arts with the Taoism indigenous to China and of the external martial arts with the foreign Buddhism of Shaolin—and the Manchu Qing Dynasty to which Huang Zongxi was opposed—was an act of political defiance rather than one of technical classification.
In 1676 Huang Zongxi's son, Huang Baijia, who learned martial arts from Wang Zhengnan, compiled the earliest extant manual of internal martial arts, the "Nèijiā quánfǎ".
Republic of China.
Beginning in 1914, Sun Lutang together with Yang Shao-hou, Yang Chengfu and Wu Chien-ch'uan taught t'ai chi to the public at the Beijing Physical Education Research Institute. Sun taught there until 1928, a seminal period in the development of modern Yang, Wu and Sun-style t'ai chi ch'uan. Sun Lutang from 1915 also published martial arts texts.
In 1928, Kuomintang generals Li Jing Lin, Zhang Zi Jiang, and Fung Zu Ziang organized a national martial arts tournament in China; they did so to screen the best martial artists in order to begin building the Central Martial Arts Academy (Zhongyang Guoshuguan). The generals separated the participants of the tournament into Shaolin and Wudang. Wudang participants were recognized as having "internal" skills. These participants were generally practitioners of t'ai chi ch'uan, Xingyiquan and Baguazhang. All other participants competed under the classification of Shaolin. One of the winners in the "internal" category was the Baguazhang master Fu Chen Sung.
Sun Lutang.
Sun Lutang identified the following as the criteria that distinguish an internal martial art:
Sun Lutang's eponymous style of t'ai chi ch'uan fuses principles from all three arts he named as neijia. Similarities applying classical principles between taiji, xingyi, and baquazhang include: Loosening (song) the soft tissue, opening shoulder and hip gates or gua, cultivating qi or intrinsic energy, issuing various jin or compounded energies. Taijiquan is characterized by an ever present peng jin or expanding energy. Xingyiquan is characterized by its solely forward moving pressing ji jin energy. Baguazhang is characterized by its “dragon body” circular movements. Some Chinese martial arts other than the ones Sun named also teach what are termed internal practices, despite being generally classified as external (e.g. Wing Chun that also is internal). Some non-Chinese martial arts also claim to be internal, for example Aikido and Kito Ryu. Many martial artists, especially outside of China, disregard the distinction entirely. Some neijia schools refer to their arts as "soft style" martial arts.
Training.
Internal styles focus on awareness of the spirit, mind, qi ("energy") and the use of relaxed ("" ) leverage rather than muscular tension. Pushing hands is a training method commonly used in neijia arts to develop sensitivity and softness.
Much time may nevertheless be spent on basic physical training, such as stance training ("zhan zhuang"), stretching and strengthening of muscles, as well as on empty hand and weapon forms which can be quite demanding.
Some forms in internal styles are performed slowly, although some include sudden outbursts of explosive movements (fa jin), such as those the Chen style of Taijiquan is famous for teaching earlier than some other styles (e.g. Yang and Wu). The reason for the generally slow pace is to improve coordination and balance by increasing the work load, and to require the student to pay minute attention to their whole body and its weight as they perform a technique. At an advanced level, and in actual fighting, internal styles are performed quickly, but the goal is to learn to involve the entire body in every motion, to stay relaxed, with deep, controlled breathing, and to coordinate the motions of the body and the breathing accurately according to the dictates of the forms while maintaining perfect balance.
Characteristics.
The reason for the label "internal," according to most schools, is that there is a focus on the internal aspects earlier in the training, once these internal relationships are apprehended (the theory goes) they are then applied to the external applications of the styles in question.
External styles are characterized by fast and explosive movements and a focus on physical strength and agility. External styles include both the traditional styles focusing on application and fighting, as well as the modern styles adapted for competition and exercise. Examples of external styles are Shaolinquan, with its direct explosive attacks and many Wushu forms that have spectacular aerial techniques. External styles begin with a training focus on muscular power, speed and application, and generally integrate their qigong aspects in advanced training, after their desired "hard" physical level has been reached.
Some say that there is no differentiation between the so-called internal and external systems of the Chinese martial arts, while other well known teachers have expressed differing opinions. For example, the Taijiquan teacher Wu Jianquan:
Those who practice Shaolinquan leap about with strength and force; people not proficient at this kind of training soon lose their breath and are exhausted. Taijiquan is unlike this. Strive for quiescence of body, mind and intention.
Current practice.
Many internal schools teach forms that are practised for health benefits only. Thus, T'ai chi ch'uan in spite of its roots in martial arts has become similar in scope to Qigong, the purely meditative practice based on notions of circulation of qi. With purely a health emphasis, T'ai chi classes have become popular in hospitals, clinics, community and senior centers in the last twenty years or so, as baby boomers age and the art's reputation as a low stress training for seniors became better known.
Traditionalists feel that a school not teaching martial aspects somewhere in their syllabus cannot be said to be actually teaching the art itself, that they have accredited themselves prematurely. Traditional teachers also believe that understanding the core theoretical principles of neijia and the ability to apply them are a necessary gateway to health benefits.
Fiction.
Internal styles have been associated in legend and in much popular fiction with the Taoist monasteries of the Wudang Mountains in central China.
Neijia are a common theme in Chinese Wuxia novels and films, and are usually represented as originating in Wudang or similar mythologies. Often, genuine internal practices are highly exaggerated to the point of making them seem miraculous, as in "Crouching Tiger Hidden Dragon" or "Tai Chi Master". Internal concepts have also been a source of comedy, such as in the films "Shaolin Soccer" and "Kung Fu Hustle".
In Naruto series, Neji Hyūga's name and techniques was based on Neijia.

</doc>
<doc id="21854" url="https://en.wikipedia.org/wiki?curid=21854" title="Navigation">
Navigation

Navigation is a field of study that focuses on the process of monitoring and controlling the movement of a craft or vehicle from one place to another. The field of navigation includes four general categories: land navigation, marine navigation, aeronautic navigation, and space navigation.
It is also the term of art used for the specialized knowledge used by navigators to perform navigation tasks. All navigational techniques involve locating the navigator's position compared to known locations or patterns.
Navigation, in a broader sense, can refer to any skill or study that involves the determination of position and direction. In this sense, navigation includes orienteering and pedestrian navigation. For information about different navigation strategies that people use, visit human navigation.
History.
In the European medieval period, navigation was considered part of the set of "seven mechanical arts", none of which were used for long voyages across open ocean. Polynesian navigation is probably the earliest form of open ocean navigation, though it was based on memory and observation rather than on scientific methods or instruments. Early Pacific Polynesians used the motion of stars, weather, the position of certain wildlife species, or the size of waves to find the path from one island to another.
Maritime navigation using scientific instruments such as the mariner's astrolabe first occurred in the Mediterranean during the Middle Ages. Although land astrolabes were invented in the Hellenistic period and existed in classical antiquity and the Islamic Golden Age, the oldest record of a sea astrolabe is that of Majorcan astronomer Ramon Llull dating from 1295. The perfecting of this navigation instrument is attributed to Portuguese navigators during early Portuguese discoveries in the Age of Discovery. The earliest known description of how to make and use a sea astrolabe comes from Spanish cosmographer Melvin Mel Pros Cespedes's "Arte de Navegar" ("The Art of Navigation") published in 1551, based on the principle of the archipendulum used in constructing the Egyptian pyramids.
Open-seas navigation using the astrolabe and the compass started during the Age of Discovery in the 15th century. The Portuguese began systematically exploring the Atlantic coast of Africa from 1418, under the sponsorship of Prince Henry. In 1488 Bartolomeu Dias reached the Indian Ocean by this route. In 1492 the Spanish monarchs funded Christopher Columbus's expedition to sail west to reach the Indies by crossing the Atlantic, which resulted in the Discovery of America. In 1498, a Portuguese expedition commanded by Vasco da Gama reached India by sailing around Africa, opening up direct trade with Asia. Soon, the Portuguese sailed further eastward, to the Spice Islands in 1512, landing in China one year later.
The first circumnavigation of the earth was completed in 1522 with the Magellan-Elcano expedition, a Spanish voyage of discovery led by Portuguese explorer Ferdinand Magellan and completed by Spanish navigator Juan Sebastián Elcano after the former's death in the Philippines in 1521. The fleet of seven ships sailed from Sanlúcar de Barrameda in Southern Spain in 1519, crossed the Atlantic Ocean and after several stopovers rounded the southern tip of South America. Some ships were lost, but the remaining fleet continued across the Pacific making a number of discoveries including Guam and the Philippines. By then, only two galleons were left from the original seven. The "Victoria" led by Elcano sailed across the Indian Ocean and north along the coast of Africa, to finally arrive in Spain in 1522, three years after its departure. The "Trinidad" sailed east from the Philippines, trying to find a maritime path back to the Americas, but was unsuccessful. The eastward route across the Pacific, also known as the "tornaviaje" (return trip) was only discovered forty years later, when Spanish cosmographer Andrés de Urdaneta sailed from the Philippines, north to parallel 39°, and hit the eastward Kuroshio Current which took its galleon across the Pacific. He arrived in Acapulco on October 8, 1565.
Etymology.
The term stems from 1530s, from Latin "navigationem" (nom. "navigatio"), from "navigatus", pp. of "navigare" "to sail, sail over, go by sea, steer a ship," from "navis" "ship" and the root of "agere" "to drive".
Basic concepts.
Latitude.
Roughly, the latitude of a place on Earth is its angular distance north or south of the equator. Latitude is usually expressed in degrees (marked with °) ranging from 0° at the Equator to 90° at the North and South poles. The latitude of the North Pole is 90° N, and the latitude of the South Pole is 90° S. Mariners calculated latitude in the Northern Hemisphere by sighting the North Star Polaris with a sextant and using sight reduction tables to correct for height of eye and atmospheric refraction. The height of Polaris in degrees above the horizon is the latitude of the observer, within a degree or so.
Longitude.
Similar to latitude, the longitude of a place on Earth is the angular distance east or west of the prime meridian or Greenwich meridian. Longitude is usually expressed in degrees (marked with °) ranging from 0° at the Greenwich meridian to 180° east and west. Sydney, for example, has a longitude of about 151° east. New York City has a longitude of 74° west. For most of history, mariners struggled to determine longitude. Longitude can be calculated if the precise time of a sighting is known. Lacking that, one can use a sextant to take a lunar distance (also called "the lunar observation", or "lunar" for short) that, with a nautical almanac, can be used to calculate the time at zero longitude (see Greenwich Mean Time). Reliable marine chronometers were unavailable until the late 18th century and not affordable until the 19th century. For about a hundred years, from about 1767 until about 1850, mariners lacking a chronometer used the method of lunar distances to determine Greenwich time to find their longitude. A mariner with a chronometer could check its reading using a lunar determination of Greenwich time.
Loxodrome.
In navigation, a rhumb line (or loxodrome) is a line crossing all meridians of longitude at the same angle, i.e. a path derived from a defined initial bearing. That is, upon taking an initial bearing, one proceeds along the same bearing, without changing the direction as measured relative to true or magnetic north.
Modern technique.
Most modern navigation relies primarily on positions determined electronically by receivers collecting information from satellites. Most other modern techniques rely on crossing lines of position or LOP. A line of position can refer to two different things: a line on a chart and a line between the observer and an object in real life. A bearing is a measure of the direction to an object. If the navigator measures the direction in real life, the angle can then be drawn on a nautical chart and the navigator will be on that line on the chart.
In addition to bearings, navigators also often measure distances to objects. On the chart, a distance produces a circle or arc of position. Circles, arcs, and hyperbolae of positions are often referred to as lines of position.
If the navigator draws two lines of position, and they intersect he must be at that position. A fix is the intersection of two or more LOPs.
If only one line of position is available, this may be evaluated against the Dead reckoning position to establish an estimated position.
Lines (or circles) of position can be derived from a variety of sources:
There are some methods seldom used today such as "dipping a light" to calculate the geographic range from observer to lighthouse
Methods of navigation have changed through history. Each new method has enhanced the mariner's ability to complete his voyage. One of the most important judgments the navigator must make is the best method to use. Some types of navigation are depicted in the table.
The practice of navigation usually involves a combination of these different methods.
Mental navigation checks.
By mental navigation checks, a pilot or a navigator estimates tracks, distances, and altitudes which then will help him or her avoid gross navigation errors.
Piloting.
Piloting (also called pilotage) involves navigating an aircraft by visual reference to landmarks, or a water vessel in restricted waters and fixing its position as precisely as possible at frequent intervals. More so than in other phases of navigation, proper preparation and attention to detail are important. Procedures vary from vessel to vessel, and between military, commercial, and private vessels.
A military navigation team will nearly always consist of several people. A military navigator might have bearing takers stationed at the gyro repeaters on the bridge wings for taking simultaneous bearings, while the civilian navigator must often take and plot them himself. While the military navigator will have a bearing book and someone to record entries for each fix, the civilian navigator will simply pilot the bearings on the chart as they are taken and not record them at all.
If the ship is equipped with an ECDIS, it is reasonable for the navigator to simply monitor the progress of the ship along the chosen track, visually ensuring that the ship is proceeding as desired, checking the compass, sounder and other indicators only occasionally. If a pilot is aboard, as is often the case in the most restricted of waters, his judgement can generally be relied upon, further easing the workload. But should the ECDIS fail, the navigator will have to rely on his skill in the manual and time-tested procedures.
Celestial navigation.
Celestial navigation systems are based on observation of the positions of the Sun, Moon, Planets and navigational stars. Such systems are in use as well for terrestrial navigating as for interstellar navigating. By knowing which point on the rotating earth a celestial object is above and measuring its height above the observer's horizon, the navigator can determine his distance from that subpoint. A nautical almanac and a marine chronometer are used to compute the subpoint on earth a celestial body is over, and a sextant is used to measure the body's angular height above the horizon. That height can then be used to compute distance from the subpoint to create a circular line of position. A navigator shoots a number of stars in succession to give a series of overlapping lines of position. Where they intersect is the celestial fix. The moon and sun may also be used. The sun can also be used by itself to shoot a succession of lines of position (best done around local noon) to determine a position.
Marine chronometer.
In order to accurately measure longitude, the precise time of a sextant sighting (down to the second, if possible) must be recorded. Each second of error is equivalent to 15 seconds of longitude error, which at the equator is a position error of .25 of a nautical mile, about the accuracy limit of manual celestial navigation.
The spring-driven marine chronometer is a precision timepiece used aboard ship to provide accurate time for celestial observations. A chronometer differs from a spring-driven watch principally in that it contains a variable lever device to maintain even pressure on the mainspring, and a special balance designed to compensate for temperature variations.
A spring-driven chronometer is set approximately to Greenwich mean time (GMT) and is not reset until the instrument is overhauled and cleaned, usually at three-year intervals. The difference between GMT and chronometer time is carefully determined and applied as a correction to all chronometer readings. Spring-driven chronometers must be wound at about the same time each day.
Quartz crystal marine chronometers have replaced spring-driven chronometers aboard many ships because of their greater accuracy. They are maintained on GMT directly from radio time signals. This eliminates chronometer error and watch error corrections. Should the second hand be in error by a readable amount, it can be reset electrically.
The basic element for time generation is a quartz crystal oscillator. The quartz crystal is temperature compensated and is hermetically sealed in an evacuated envelope. A calibrated adjustment capability is provided to adjust for the aging of the crystal.
The chronometer is designed to operate for a minimum of 1 year on a single set of batteries. Observations may be timed and ship's clocks set with a comparing watch, which is set to chronometer time and taken to the bridge wing for recording sight times. In practice, a wrist watch coordinated to the nearest second with the chronometer will be adequate.
A stop watch, either spring wound or digital, may also be used for celestial observations. In this case, the watch is started at a known GMT by chronometer, and the elapsed time of each sight added to this to obtain GMT of the sight.
All chronometers and watches should be checked regularly with a radio time signal. Times and frequencies of radio time signals are listed in publications such as Radio Navigational Aids.
The marine sextant.
The second critical component of celestial navigation is to measure the angle formed at the observer's eye between the celestial body and the sensible horizon. The sextant, an optical instrument, is used to perform this function. The sextant consists of two primary assemblies. The frame is a rigid triangular structure with a pivot at the top and a graduated segment of a circle, referred to as the "arc", at the bottom. The second component is the index arm, which is attached to the pivot at the top of the frame. At the bottom is an endless vernier which clamps into teeth on the bottom of the "arc". The optical system consists of two mirrors and, generally, a low power telescope. One mirror, referred to as the "index mirror" is fixed to the top of the index arm, over the pivot. As the index arm is moved, this mirror rotates, and the graduated scale on the arc indicates the measured angle ("altitude").
The second mirror, referred to as the "horizon glass", is fixed to the front of the frame. One half of the horizon glass is silvered and the other half is clear. Light from the celestial body strikes the index mirror and is reflected to the silvered portion of the horizon glass, then back to the observer's eye through the telescope. The observer manipulates the index arm so the reflected image of the body in the horizon glass is just resting on the visual horizon, seen through the clear side of the horizon glass.
Adjustment of the sextant consists of checking and aligning all the optical elements to eliminate "index correction". Index correction should be checked, using the horizon or more preferably a star, each time the sextant is used. The practice of taking celestial observations from the deck of a rolling ship, often through cloud cover and with a hazy horizon, is by far the most challenging part of celestial navigation.
Inertial navigation.
Inertial navigation system is a dead reckoning type of navigation system that computes its position based on motion sensors. Once the initial latitude and longitude is established, the system receives impulses from motion detectors that measure the acceleration along three or more axes enabling it to continually and accurately calculate the current latitude and longitude. Its advantages over other navigation systems are that, once the starting position is set, it does not require outside information, it is not affected by adverse weather conditions and it cannot be detected or jammed. Its disadvantage is that since the current position is calculated solely from previous positions, its errors are cumulative, increasing at a rate roughly proportional to the time since the initial position was input. Inertial navigation systems must therefore be frequently corrected with a location 'fix' from some other type of navigation system. The US Navy developed a Ships Inertial Navigation System (SINS) during the Polaris missile program to ensure a safe, reliable and accurate navigation system for its missile submarines. Inertial navigation systems were in wide use until satellite navigation systems (GPS) became available. Inertial Navigation Systems are still in common use on submarines, since GPS reception or other fix sources are not possible while submerged.
Electronic navigation.
Radio navigation.
A radio direction finder or RDF is a device for finding the direction to a radio source. Due to radio's ability to travel very long distances "over the horizon", it makes a particularly good navigation system for ships and aircraft that might be flying at a distance from land.
RDFs works by rotating a directional antenna and listening for the direction in which the signal from a known station comes through most strongly. This sort of system was widely used in the 1930s and 1940s. RDF antennas are easy to spot on German World War II aircraft, as loops under the rear section of the fuselage, whereas most US aircraft enclosed the antenna in a small teardrop-shaped fairing.
In navigational applications, RDF signals are provided in the form of "radio beacons", the radio version of a lighthouse. The signal is typically a simple AM broadcast of a morse code series of letters, which the RDF can tune in to see if the beacon is "on the air". Most modern detectors can also tune in any commercial radio stations, which is particularly useful due to their high power and location near major cities.
Decca, OMEGA, and LORAN-C are three similar hyperbolic navigation systems. Decca was a hyperbolic low frequency radio navigation system (also known as multilateration) that was first deployed during World War II when the Allied forces needed a system which could be used to achieve accurate landings. As was the case with Loran C, its primary use was for ship navigation in coastal waters. Fishing vessels were major post-war users, but it was also used on aircraft, including a very early (1949) application of moving-map displays. The system was deployed in the North Sea and was used by helicopters operating to oil platforms.
The OMEGA Navigation System was the first truly global radio navigation system for aircraft, operated by the United States in cooperation with six partner nations. OMEGA was developed by the United States Navy for military aviation users. It was approved for development in 1968 and promised a true worldwide oceanic coverage capability with only eight transmitters and the ability to achieve a four-mile (6 km) accuracy when fixing a position. Initially, the system was to be used for navigating nuclear bombers across the North Pole to Russia. Later, it was found useful for submarines.[http://www.jproc.ca/hyperbolic/omega.html] Due to the success of the Global Positioning System the use of Omega declined during the 1990s, to a point where the cost of operating Omega could no longer be justified. Omega was terminated on September 30, 1997 and all stations ceased operation.
LORAN is a terrestrial navigation system using low frequency radio transmitters that use the time interval between radio signals received from three or more stations to determine the position of a ship or aircraft. The current version of LORAN in common use is LORAN-C, which operates in the low frequency portion of the EM spectrum from 90 to 110 kHz. Many nations are users of the system, including the United States, Japan, and several European countries. Russia uses a nearly exact system in the same frequency range, called CHAYKA. LORAN use is in steep decline, with GPS being the primary replacement. However, there are attempts to enhance and re-popularize LORAN. LORAN signals are less susceptible to interference and can penetrate better into foliage and buildings than GPS signals.
Radar navigation.
When a vessel is within radar range of land or special radar aids to navigation, the navigator can take distances and angular bearings to charted objects and use these to establish arcs of position and lines of position on a chart. A fix consisting of only radar information is called a radar fix.
Types of radar fixes include "range and bearing to a single object," "two or more bearings," "tangent bearings," and "two or more ranges."
Parallel indexing is a technique defined by William Burger in the 1957 book "The Radar Observer's Handbook". This technique involves creating a line on the screen that is parallel to the ship's course, but offset to the left or right by some distance. This parallel line allows the navigator to maintain a given distance away from hazards.
Some techniques have been developed for special situations. One, known as the "contour method," involves marking a transparent plastic template on the radar screen and moving it to the chart to fix a position.
Another special technique, known as the Franklin Continuous Radar Plot Technique, involves drawing the path a radar object should follow on the radar display if the ship stays on its planned course. During the transit, the navigator can check that the ship is on track by checking that the pip lies on the drawn line.
Satellite navigation.
Global Navigation Satellite System or GNSS is the term for satellite navigation systems that provide positioning with global coverage. A GNSS allow small electronic receivers to determine their location (longitude, latitude, and altitude) to within a few metres using time signals transmitted along a line of sight by radio from satellites. Receivers on the ground with a fixed position can also be used to calculate the precise time as a reference for scientific experiments.
As of October 2011, only the United States NAVSTAR Global Positioning System (GPS) and the Russian GLONASS are fully globally operational GNSSs. The European Union's Galileo positioning system is a next generation GNSS in the initial deployment phase, scheduled to be operational by 2013. China has indicated it may expand its regional Beidou navigation system into a global system.
More than two dozen GPS satellites are in medium Earth orbit, transmitting signals allowing GPS receivers to determine the receiver's location, speed and direction.
Since the first experimental satellite was launched in 1978, GPS has become an indispensable aid to navigation around the world, and an important tool for map-making and land surveying. GPS also provides a precise time reference used in many applications including scientific study of earthquakes, and synchronization of telecommunications networks.
Developed by the United States Department of Defense, GPS is officially named NAVSTAR GPS (NAVigation Satellite Timing And Ranging Global Positioning System). The satellite constellation is managed by the United States Air Force 50th Space Wing. The cost of maintaining the system is approximately US$750 million per year, including the replacement of aging satellites, and research and development. Despite this fact, GPS is free for civilian use as a public good.
Navigation processes.
Day's work in navigation.
The Day's work in navigation is a minimal set of tasks consistent with prudent navigation. The definition will vary on military and civilian vessels, and from ship to ship, but takes a form resembling:
Passage planning.
Studies show that human error is a factor in 80 percent of navigational accidents and that in many cases the human making the error had access to information that could have prevented the accident. The practice of voyage planning has evolved from penciling lines on nautical charts to a process of risk management.
Passage planning consists of four stages: appraisal, planning, execution, and monitoring, which are specified in "International Maritime Organization Resolution A.893(21), Guidelines For Voyage Planning," and these guidelines are reflected in the local laws of IMO signatory countries (for example, Title 33 of the U.S. Code of Federal Regulations), and a number of professional books or publications. There are some fifty elements of a comprehensive passage plan depending on the size and type of vessel.
The appraisal stage deals with the collection of information relevant to the proposed voyage as well as ascertaining risks and assessing the key features of the voyage. In the next stage, the written plan is created. The third stage is the execution of the finalised voyage plan, taking into account any special circumstances which may arise such as changes in the weather, which may require the plan to be reviewed or altered. The final stage of passage planning consists of monitoring the vessel's progress in relation to the plan and responding to deviations and unforeseen circumstances.
Integrated bridge systems.
Electronic integrated bridge concepts are driving future navigation system planning. Integrated systems take inputs from various ship sensors, electronically display positioning information, and provide control signals required to maintain a vessel on a preset course. The navigator becomes a system manager, choosing system presets, interpreting system output, and monitoring vessel response.

</doc>
<doc id="21857" url="https://en.wikipedia.org/wiki?curid=21857" title="Non-fiction">
Non-fiction

Nonfiction or non-fiction is content (often, in the form of a story) whose creator, in good faith, assumes responsibility for the truth or accuracy of the events, people, and/or information presented. In contrast, a story whose creator explicitly leaves open if and how the work refers to reality is usually classified as fiction. Nonfiction, which may be presented either objectively or subjectively, is traditionally one of the two main divisions of narratives (and, specifically, prose writing), the other traditional division being fiction, which contrasts with nonfiction by dealing in information, events, and characters expected to be partly or largely imaginary. 
Nonfiction's specific factual assertions and descriptions may or may not be accurate, and can give either a true or a false account of the subject in question. However, authors of such accounts genuinely believe or claim them to be truthful at the time of their composition or, at least, pose them to a convinced audience as historically or empirically factual. Reporting the beliefs of others in a nonfiction format is not necessarily an endorsement of the ultimate veracity of those beliefs, it is simply saying it is true that people believe them (for such topics as mythology). Nonfiction can also be written about fiction, typically known as literary criticism, giving information and analysis on these other works. Nonfiction need not necessarily be written text, since pictures and film can also purport to present a factual account of a subject.
Distinctions.
The numerous literary and creative devices used within fiction are generally thought inappropriate for use in nonfiction. They are still present particularly in older works but they are often muted so as not to overshadow the information within the work. Simplicity, clarity and directness are some of the most important considerations when producing nonfiction. Audience is important in any artistic or descriptive endeavor, but it is perhaps most important in nonfiction. In fiction, the writer believes that readers will make an effort to follow and interpret an indirectly or abstractly presented progression of theme, whereas the production of nonfiction has more to do with the direct provision of information. Understanding of the potential readers' use for the work and their existing knowledge of a subject are both fundamental for effective nonfiction. Despite the truth of nonfiction, it is often necessary to persuade the reader to agree with the ideas and so a balanced, coherent and informed argument is vital. However, the boundaries between fiction and nonfiction are continually blurred and argued upon, especially in the field of biography; as Virginia Woolf said: "if we think of truth as something of granite-like solidity and of personality as something of rainbow-like intangibility and reflect that the aim of biography is to weld these two into one seamless whole, we shall admit that the problem is a stiff one and that we need not wonder if biographers, for the most part failed to solve it."
Semi-fiction is fiction implementing a great deal of nonfiction, e.g. a fictional description based on a true story.
Major types.
Common literary examples of nonfiction include expository, argumentative, functional, and opinion pieces; essays on art or literature; biographies; memoirs; journalism; and historical, scientific, technical, or economic writings (including electronic ones).
Journals, photographs, textbooks, travel books, blueprints, and diagrams are also often considered non-fictional. Including information that the author knows to be untrue within any of these works is usually regarded as dishonest. Other works can legitimately be either fiction or nonfiction, such as journals of self-expression, letters, magazine articles, and other expressions of imagination. Though they are mostly either one or the other, it is possible for there to be a blend of both. Some fiction may include nonfictional elements. Some nonfiction may include elements of unverified supposition, deduction, or imagination for the purpose of smoothing out a narrative, but the inclusion of open falsehoods would discredit it as a work of nonfiction. The publishing and bookselling business sometimes uses the phrase "literary nonfiction" to distinguish works with a more literary or intellectual bent, as opposed to the greater collection of nonfiction subjects.

</doc>
<doc id="21861" url="https://en.wikipedia.org/wiki?curid=21861" title="Cryptonomicon">
Cryptonomicon

Cryptonomicon is a 1999 novel by American author Neal Stephenson, set in two different time periods. One group of characters are World War II-era Allied codebreakers and tactical-deception operatives affiliated with the Government Code and Cypher School at Bletchley Park(U.K.), and disillusioned Axis military and intelligence figures. The second narrative is set in the late 1990s, with characters that are (in part) descendants of those of the earlier time period, who employ cryptologic, telecom and computer technology to build an underground data haven in the fictional Sultanate of Kinakuta. Their goal is to facilitate anonymous Internet banking using electronic money and (later) digital gold currency, with a long-term objective to distribute Holocaust Education and Avoidance Pod (HEAP) media for instructing genocide-target populations on defensive warfare.
Genre and subject matter.
"Cryptonomicon" is closer to the genres of historical fiction and contemporary techno-thriller than to the science fiction of Stephenson's two previous novels, "Snow Crash" and "Diamond Age". It features fictionalized characterizations of such historical figures as Alan Turing, Albert Einstein, Douglas MacArthur, Winston Churchill, Isoroku Yamamoto, Karl Dönitz, Hermann Göring, and Ronald Reagan, as well as some highly technical and detailed descriptions of modern cryptography and information security, with discussions of prime numbers, modular arithmetic, and Van Eck phreaking.
Title.
According to Stephenson:
The title is a play on "Necronomicon", the title of a book mentioned in the stories of horror writer H. P. Lovecraft:
The novel's Cryptonomicon, described as a "cryptographer's bible", is a fictional book summarizing America's knowledge of cryptography and cryptanalysis. Begun by John Wilkins (the Cryptonomicon is mentioned in "Quicksilver") and amended over time by William Friedman, Lawrence Waterhouse, and others, the Cryptonomicon is described by Katherine Hayles as "a kind of Kabala created by a Brotherhood of Code that stretches across centuries. To know its contents is to qualify as a Morlock among the Eloi, and the elite among the elite are those gifted enough actually to contribute to it."
Plot.
The action takes place in two periods — World War II and the late 1990s, during the Internet boom and Asian financial crisis.
In 1942, Lawrence Pritchard Waterhouse, a young United States Navy code breaker and mathematical genius, is assigned to the newly formed joint British and American Detachment 2702. This ultra-secret unit's role is to hide the fact that Allied intelligence has cracked the German Enigma code. The detachment stages events, often behind enemy lines, that provide alternative explanations for the Allied intelligence successes. United States Marine sergeant Bobby Shaftoe, a veteran of China and Guadalcanal, serves in unit 2702, carrying out Waterhouse's plans. At the same time, Japanese soldiers, including mining engineer Goto Dengo, an old friend of Shaftoe's, are assigned to build a mysterious bunker in the mountains in the Philippines as part of what turns out to be a literal suicide mission.
Circa 1997, Randy Waterhouse (Lawrence's grandson) joins his old role-playing game companion Avi Halaby in a new startup, providing Pinoy-grams (inexpensive, non-real-time video messages) to migrant Filipinos via new fiber-optic cables. The Epiphyte Corporation uses this income stream to fund the creation of a data haven in the nearby fictional Sultanate of Kinakuta. Vietnam veteran Doug Shaftoe, the son of Bobby Shaftoe, and his daughter Amy do the undersea surveying for the cables and engineering work on the haven is overseen by Goto Furudenendu, heir-apparent to Goto Engineering. Complications arise as figures from the past reappear seeking gold or revenge.
Characters.
World War II storyline.
Historical figures.
Fictionalized versions of several historical figures appear in the World War II storyline:
1990s storyline.
The precise date of this storyline is not established, but the ages of characters, the technologies described, and certain date-specific references suggest that it is set in the late 1990s, at the time of the internet boom and the Asian financial crisis.
Technical content.
Portions of "Cryptonomicon" are notably complex and may be considered somewhat difficult by the non-technical reader. Several pages are spent explaining in detail some of the concepts behind cryptography and data storage security, including a description of Van Eck phreaking.
Stephenson also includes a precise description of (and even Perl script for) the Solitaire (or Pontifex) cipher, a cryptographic algorithm developed by Bruce Schneier for use with a deck of playing cards, as part of the plot.
He also describes computers using a fictional operating system, Finux. The name is a thinly veiled reference to Linux, a kernel originally written by the Finnish native Linus Torvalds. Stephenson changed the name so as not to be creatively constrained by the technical details of Linux-based operating systems.
Allusions/references from other works.
An excerpt from "Cryptonomicon" was originally published in the short story collection "Disco 2000", edited by Sarah Champion and published in 1998.
Stephenson's subsequent work, "The Baroque Cycle", provides part of the backstory to the characters and events featured in "Cryptonomicon". An excerpt of "Quicksilver", Volume One of "The Baroque Cycle", is included in later prints of the Mass Market Paperback edition.
"The Baroque Cycle", set in the late 17th and early 18th centuries, features ancestors of several characters in "Cryptonomicon", as well as events and items which affect the action of the later-set book. The subtext implies the existence of secret societies or conspiracies, and familial tendencies and groupings found within those darker worlds.
The short story "Jipi and the Paranoid Chip" appears to take place some time after the events of "Cryptonomicon". In the story, the construction of the Crypt has triggered economic growth in Manila and Kinakuta, in which Goto Engineering, and Homa/Homer Goto, a Goto family heir, are involved. The IDTRO ("Black Chamber") is also mentioned.
Peter Thiel states in his book "Zero to One" that "Cryptonomicon" was required reading during the early days of PayPal.
Literary significance and criticism.
According to critic Jay Clayton, the book is written for a technical or geek audience. Despite the technical detail, the book drew praise from both Stephenson's science fiction fan base and literary critics and buyers. In his book "Charles Dickens in Cyberspace: The Afterlife of the Nineteenth Century in Postmodern Culture" (2003), Jay Clayton calls Stephenson’s book the “ultimate geek novel” and draws attention to the “literary-scientific-engineering-military-industrial-intelligence alliance” that produced discoveries in two eras separated by fifty years, World War II and the Internet age. In July 2012, io9 included the book on its list of "10 Science Fiction Novels You Pretend to Have Read".

</doc>
<doc id="21862" url="https://en.wikipedia.org/wiki?curid=21862" title="In the Beginning... Was the Command Line">
In the Beginning... Was the Command Line

In the Beginning... Was the Command Line is an essay by Neal Stephenson which was originally published online in 1999 and later made available in book form (November 1999, ISBN 978-0380815937). The essay is a commentary on why the proprietary operating systems business is unlikely to remain profitable in the future because of competition from free software. It also analyzes the corporate/collective culture of the Microsoft, Apple, and free software communities.
Themes.
Stephenson explores the GUI as a metaphor in terms of the increasing interposition of abstractions between humans and the actual workings of devices (in a similar manner to "Zen and the Art of Motorcycle Maintenance") and explains the beauty hackers feel in good-quality tools. He does this with a car analogy. He compares four operating systems, Mac OS by Apple Computer to a luxury European car, Windows by Microsoft to a station wagon, Linux to a free tank, and BeOS to a batmobile. Stephenson argues that people continue to buy the station wagon despite free tanks being given away, because people do not want to learn how to operate a tank; they know that the station wagon dealership has a machine shop that they can take their car to when it breaks down. Because of this attitude, Stephenson argues that Microsoft is not really a monopoly, as evidenced by the free availability of other choice OSes, but rather has simply accrued enough mindshare among the people to have them coming back. He compares Microsoft to Disney, in that both are selling a vision to their customers, who in turn "want to believe" in that vision.
Stephenson relays his experience with the Debian bug tracking system (#6518). He then contrasts it with Microsoft's approach. Debian developers responded from around the world within a day. He was completely frustrated with his initial attempt to achieve the same response from Microsoft, but he concedes that his subsequent experience was satisfactory. The difference he notes is that Debian developers are personally accessible and transparently own up to defects in their OS distribution, while Microsoft pretends errors don't exist.
Later developments.
The essay was written before the advent of Mac OS X. A recurring theme is the full power of the command line compared with easier to learn graphical user interfaces (GUIs) which are described as broken mixed metaphors for 'power users'. He then mentions GUIs which allow traditional terminal windows to be used. In a Slashdot interview in 2004, in response to the question:
... have you embraced the new UNIX based MacOS X as the OS you want to use when you "Just want to go to Disneyland"?
he replied:
I embraced OS X as soon as it was available and have never looked back. So a lot of "In the Beginning...was the Command Line" is now obsolete. I keep meaning to update it, but if I'm honest with myself, I have to say this is unlikely.
With Neal Stephenson's permission, Garrett Birkel responded to "In the Beginning...was the Command Line" in 2004, bringing it up to date and critically discussing Stephenson's argument. Birkel's response is interspersed throughout the original text, which remains untouched.

</doc>
<doc id="21863" url="https://en.wikipedia.org/wiki?curid=21863" title="Netscape Navigator">
Netscape Navigator

Netscape Navigator is a discontinued proprietary web browser, and the original browser of the Netscape line, from versions 1 to 4.08, and 9.x. It was the flagship product of the Netscape Communications Corp and was the dominant web browser in terms of usage share in the 1990s, but by 2002 its usage had almost disappeared. This was primarily due to the increased usage of Microsoft's Internet Explorer web browser software, and partly because the Netscape Corporation (later purchased by AOL) did not sustain Netscape Navigator's technical innovation after the late 1990s.
The business demise of Netscape was a central premise of Microsoft's antitrust trial, wherein the Court ruled that Microsoft Corporation's bundling of Internet Explorer with the Windows operating system was a monopolistic and illegal business practice. The decision came too late for Netscape, however, as Internet Explorer had by then become the dominant web browser in Windows.
The Netscape Navigator web browser was succeeded by the Netscape Communicator suite in 1997. Netscape Communicator's 4.x source code was the base for the Netscape-developed Mozilla Application Suite, which was later renamed SeaMonkey. Netscape's Mozilla Suite also served as the base for a browser-only spinoff called Mozilla Firefox.
The Netscape Navigator name returned in 2007 when AOL announced version 9 of the Netscape series of browsers, Netscape Navigator 9. On 28 December 2007, AOL canceled its development but continued supporting the web browser with security updates until 1 March 2008. AOL allows downloading of archived versions of the Netscape Navigator web browser family. AOL maintains the Netscape website as an Internet portal.
History and development.
Origin.
Netscape Navigator was based on the Mosaic web browser, which was co-written by Marc Andreessen, a part-time employee of the National Center for Supercomputing Applications and a student at the University of Illinois. After Andreessen graduated in 1993, he moved to California and there met Jim Clark, the recently departed founder of Silicon Graphics. Clark believed that the Mosaic browser had great commercial possibilities and provided the seed money. Soon Mosaic Communications Corporation was in business in Mountain View, California, with Andreessen as a vice-president. Since the University of Illinois was unhappy with the company's use of the Mosaic name, the company changed its name to Netscape Communications (thought up by Product Manager Greg Sands) and named its flagship web browser Netscape Navigator.
Netscape announced in its first press release (13 October 1994) that it would make Navigator available without charge to all non-commercial users, and beta versions of version 1.0 and 1.1 were indeed freely downloadable in November 1994 and March 1995, with the full version 1.0 available in December 1994. Netscape's initial corporate policy regarding Navigator is interesting, as it claimed that it would make Navigator freely available for non-commercial use in accordance with the notion that Internet software should be distributed for free.
However, within two months of that press release, Netscape apparently reversed its policy on who could freely obtain and use version 1.0 by only mentioning that educational and non-profit institutions could use version 1.0 at no charge.
The reversal was complete with the availability of version 1.1 beta on 6 March 1995, in which a press release states that the final 1.1 release would be available at no cost only for academic and non-profit organizational use. Gone was the notion expressed in the first press release that Navigator would be freely available in the spirit of Internet software.
Some security experts and cryptographers found out that all released Netscape versions had major security problems with crashing the browser with long URLs and 40 bits encryption keys.
The first few releases of the product were made available in “commercial” and “evaluation” versions; for example, version “1.0” and version “1.0N”. The “N” evaluation versions were completely identical to the commercial versions; the letter was there to remind people to pay for the browser once they felt they had tried it long enough and were satisfied with it. This distinction was formally dropped within a year of the initial release, and the full version of the browser continued to be made available for free online, with boxed versions available on floppy disks (and later CDs) in stores along with a period of phone support. During this era, "Internet Starter Kit" books were popular, and usually included a floppy disk or CD containing internet software, and this was a popular means of obtaining Netscape's and other browsers. Email support was initially free, and remained so for a year or two until the volume of support requests grew too high.
During development, the Netscape browser was known by the code name "Mozilla", which became the name of a Godzilla-like cartoon dragon mascot used prominently on the company's web site. The Mozilla name was also used as the User-Agent in HTTP requests by the browser. Other web browsers claimed to be compatible with Netscape's extensions to HTML, and therefore used the same name in their User-Agent identifiers so that web servers would send them the same pages as were sent to Netscape browsers. Mozilla is now a generic name for matters related to the open source successor to Netscape Communicator.
Rise of Netscape.
When the consumer Internet revolution arrived in the mid-to-late 1990s, Netscape was well-positioned to take advantage of it. With a good mix of features and an attractive licensing scheme that allowed free use for non-commercial purposes, the Netscape browser soon became the de facto standard, particularly on the Windows platform. Internet service providers and computer magazine publishers helped make Navigator readily available.
An important innovation that Netscape introduced in 1994 was the on-the-fly display of web pages, where text and graphics appeared on the screen as the web page downloaded. Earlier web browsers would not display a page until all graphics on it had been loaded over the network connection; this often made a user stare at a blank page for as long as several minutes. With Netscape, people using dial-up connections could begin reading the text of a web page within seconds of entering a web address, even before the rest of the text and graphics had finished downloading. This made the web much more tolerable to the average user.
Through the late 1990s, Netscape made sure that Navigator remained the technical leader among web browsers. Important new features included cookies, frames, proxy auto-config, and JavaScript (in version 2.0). Although those and other innovations eventually became open standards of the W3C and ECMA and were emulated by other browsers, they were often viewed as controversial. Netscape, according to critics, was more interested in bending the web to its own de facto "standards" (bypassing standards committees and thus marginalizing the commercial competition) than it was in fixing bugs in its products. Consumer rights advocates were particularly critical of cookies and of commercial web sites using them to invade individual privacy.
In the marketplace, however, these concerns made little difference. Netscape Navigator remained the market leader with more than 50% usage share. The browser software was available for a wide range of operating systems, including Windows (3.1, 95, 98, NT), Macintosh, Linux, OS/2, and many versions of Unix including DEC, Sun Solaris, BSD/OS, IRIX, AIX, and HP-UX, and looked and worked nearly identically on every one of them. Netscape began to experiment with prototypes of a web-based system, known internally as “Constellation”, which would allow a user to access and edit his or her files anywhere across a network no matter what computer or operating system he or she happened to be using.
Industry observers confidently forecast the dawn of a new era of connected computing. The underlying operating system, it was believed, would become an unimportant consideration; future applications would run within a web browser. This was seen by Netscape as a clear opportunity to entrench Navigator at the heart of the next generation of computing, and thus gain the opportunity to expand into all manner of other software and service markets.
Decline.
With the success of Netscape showing the importance of the web (more people were using the Internet due in part to the ease of using Netscape), Internet browsing began to be seen as a potentially profitable market. Following Netscape's lead, Microsoft started a campaign to enter the web browser software market. Like Netscape before them, Microsoft licensed the Mosaic source code from Spyglass, Inc. (which in turn licensed code from University of Illinois). Using this basic code, Microsoft created Internet Explorer (IE).
The competition between Microsoft and Netscape dominated the Browser Wars. Internet Explorer, Version 1.0 (shipped in the Internet Jumpstart Kit in Microsoft Plus! For Windows 95) and IE, Version 2.0 (the first cross-platform version of the web browser, supporting both Windows and Mac OS) were thought by many to be inferior and primitive when compared to contemporary versions of Netscape Navigator. With the release of IE version 3.0 (1996) Microsoft was able to catch up with Netscape competitively, with IE Version 4.0 (1997) further improving in terms of market share. IE 5.0 (1999) improved stability and took significant market share from Netscape Navigator for the first time.
There were two versions of Netscape Navigator 3.0; the Standard Edition and the Gold Edition. The latter consisted of the Navigator browser with e-mail, news readers, and a WYSIWYG web page compositor; however, these extra functions enlarged and slowed the software, rendering it prone to crashing.
This Gold Edition was renamed Netscape Communicator starting with version 4.0; the name change diluted its name-recognition and confused users. Netscape CEO James L. Barksdale insisted on the name change because Communicator was a general-purpose "client" application, which contained the Navigator "browser".
The aging Netscape Communicator 4.x was slower than Internet Explorer 5.0. Typical web pages had become heavily illustrated, often JavaScript-intensive, and encoded with HTML features designed for specific purposes but now employed as global layout tools (HTML tables, the most obvious example of this, were especially difficult for Communicator to render). The Netscape browser, once a solid product, became crash-prone and buggy; for example, some versions re-downloaded an entire web page to re-render it when the browser window was re-sized (a nuisance to dial-up users), and the browser would usually crash when the page contained simple Cascading Style Sheets. Moreover, Netscape Communicator's browser interface design appeared dated in comparison to Internet Explorer and interface changes in Microsoft and Apple's operating systems.
At decade's end, Netscape's web browser had lost dominance over the Windows platform, and the August 1997 Microsoft financial agreement to invest one hundred and fifty million dollars in Apple required that Apple make Internet Explorer the default web browser in new Mac OS distributions. The latest IE Mac release at that time was Internet Explorer version 3.0 for Macintosh, but Internet Explorer 4 was released later that year.
Microsoft succeeded in having ISPs and PC vendors distribute Internet Explorer to their customers instead of Netscape Navigator, mostly due to Microsoft using its leverage from Windows OEM licenses, and partly aided by Microsoft's investment in making IE brandable, such that a customized version of IE could be offered. Also, web developers used proprietary, browser-specific extensions in web pages. Both Microsoft and Netscape did this, having added many proprietary HTML tags to their browsers, which forced users to choose between two competing and almost incompatible web browsers.
In March 1998, Netscape released most of the development code base for Netscape Communicator under an open source license. Only pre-alpha versions of Netscape 5 were released before the open source community decided to scrap the Netscape Navigator codebase entirely and build a new web browser around the Gecko layout engine which Netscape had been developing but which had not yet incorporated. The community-developed open source project was named "Mozilla", Netscape Navigator's original code name. America Online bought Netscape; Netscape programmers took a pre-beta-quality form of the Mozilla codebase, gave it a new GUI, and released it as Netscape 6. This did nothing to win back users, who continued to migrate to Internet Explorer. After the release of Netscape 7 and a long public beta test, Mozilla 1.0 was released on 5 June 2002. The same code-base, notably the Gecko layout engine, became the basis of independent applications, including Firefox and Thunderbird.
On 28 December 2007, the Netscape developers announced that AOL had canceled development of Netscape Navigator, leaving it unsupported as of 1 March 2008. Despite this, archived and unsupported versions of the browser remain available for download. Firefox would go on to win back market share from Internet Explorer in the next round of the browser wars.
Legacy.
Netscape's contributions to the web include JavaScript, which was submitted as a new standard to Ecma International. The resultant ECMAScript specification allowed JavaScript support by multiple web browsers and its use as a cross-browser scripting language, long after Netscape Navigator itself has dropped in popularity. Another example is the FRAME tag, that is widely supported today, and that has been incorporated into official web standards such as the "HTML 4.01 Frameset" specification.
In a 2007 "PC World" column, the original Netscape Navigator was considered the "best tech product of all time" due to its impact on the Internet.

</doc>
<doc id="21865" url="https://en.wikipedia.org/wiki?curid=21865" title="Neurotransmitter">
Neurotransmitter

Neurotransmitters, also known as chemical messengers, are endogenous chemicals that enable neurotransmission. They transmit signals across a chemical synapse, such as a neuromuscular junction, from one neuron (nerve cell) to another "target" neuron, muscle cell, or gland cell. Neurotransmitters are released from synaptic vesicles in synapses into the synaptic cleft, where they are received by receptors on the target cells. Many neurotransmitters are synthesized from simple and plentiful precursors such as amino acids, which are readily available from the diet and only require a small number of biosynthetic steps for conversion. Neurotransmitters play a major role in shaping everyday life and functions. Their exact numbers are unknown, but more than 100 chemical messengers have been uniquely identified.
Mechanism.
Neurotransmitters are stored in a synapse in synaptic vesicles, clustered beneath the membrane in the axon terminal located at the presynaptic side of the synapse. Neurotransmitters are released into and diffused across the synaptic cleft, where they bind to specific receptors in the membrane on the postsynaptic side of the synapse.
Most neurotransmitters are about the size of a single amino acid, however, some neurotransmitters may be the size of larger proteins or peptides. A released neurotransmitter is typically available in the synaptic cleft for a short time before it is metabolized by enzymes, pulled back into the presynaptic neuron through reuptake, or bound to a postsynaptic receptor. Nevertheless, short-term exposure of the receptor to a neurotransmitter is typically sufficient for causing a postsynaptic response by way of synaptic transmission.
In response to a threshold action potential or graded electrical potential, a neurotransmitter is released at the presynaptic terminal. Low level "baseline" release also occurs without electrical stimulation. The released neurotransmitter may then move across the synapse to be detected by and bind with receptors in the postsynaptic neuron. Binding of neurotransmitters may influence the postsynaptic neuron in either an inhibitory or excitatory way. This neuron may be connected to many more neurons, and if the total of excitatory influences are greater than those of inhibitory influences, the neuron will also "fire". Ultimately it will create a new action potential at its axon hillock to release neurotransmitters and pass on the information to yet another neighboring neuron.
Discovery.
Until the early 20th century, scientists assumed that the majority of synaptic communication in the brain was electrical. However, through the careful histological examinations by Ramón y Cajal (1852–1934), a 20 to 40 nm gap between neurons, known today as the synaptic cleft, was discovered. The presence of such a gap suggested communication via chemical messengers traversing the synaptic cleft, and in 1921 German pharmacologist Otto Loewi (1873–1961) confirmed that neurons can communicate by releasing chemicals. Through a series of experiments involving the vagus nerves of frogs, Loewi was able to manually slow the heart rate of frogs by controlling the amount of saline solution present around the vagus nerve. Upon completion of this experiment, Loewi asserted that sympathetic regulation of cardiac function can be mediated through changes in chemical concentrations. Furthermore, Otto Loewi is credited with discovering acetylcholine (ACh)—the first known neurotransmitter. Some neurons do, however, communicate via electrical synapses through the use of gap junctions, which allow specific ions to pass directly from one cell to another.
Identification.
There are four main criteria for identifying neurotransmitters:
However, given advances in pharmacology, genetics, and chemical neuroanatomy, the term "neurotransmitter" can be applied to chemicals that:
The anatomical localization of neurotransmitters is typically determined using immunocytochemical techniques, which identify either the location of either the transmitter substances themselves, or of the enzymes that are involved in their synthesis. Immunocytochemical techniques have also revealed that many transmitters, particularly the neuropeptides, are co-localized, that is, one neuron may release more than one transmitter from its synaptic terminal. Various techniques and experiments such as staining, stimulating, and collecting can be used to identify neurotransmitters throughout the central nervous system.
Types.
There are many different ways to classify neurotransmitters. Dividing them into amino acids, peptides, and monoamines is sufficient for some classification purposes.
Major neurotransmitters:
In addition, over 50 neuroactive peptides have been found, and new ones are discovered regularly. Many of these are "co-released" along with a small-molecule transmitter. Nevertheless, in some cases a peptide is the primary transmitter at a synapse. β-endorphin is a relatively well known example of a peptide neurotransmitter because it engages in highly specific interactions with opioid receptors in the central nervous system.
Single ions (such as synaptically released zinc) are also considered neurotransmitters by some, as well as some gaseous molecules such as nitric oxide (NO), carbon monoxide (CO), and hydrogen sulfide (H2S). The gases are produced in the neural cytoplasm and are immediately diffused through the cell membrane into the extracellular fluid and into nearby cells to stimulate production of second messengers. Soluble gas neurotransmitters are difficult to study, as they act rapidly and are immediately broken down, existing for only a few seconds.
The most prevalent transmitter is glutamate, which is excitatory at well over 90% of the synapses in the human brain. The next most prevalent is Gamma-Aminobutyric Acid, or GABA, which is inhibitory at more than 90% of the synapses that do not use glutamate. Although other transmitters are used in fewer synapses, they may be very important functionally: the great majority of psychoactive drugs exert their effects by altering the actions of some neurotransmitter systems, often acting through transmitters other than glutamate or GABA. Addictive drugs such as cocaine and amphetamines exert their effects primarily on the dopamine system. The addictive opiate drugs exert their effects primarily as functional analogs of opioid peptides, which, in turn, regulate dopamine levels.
Actions.
Neurons form elaborate networks through which nerve impulses—action potentials—travel. Each neuron has as many as 15,000 connections with neighboring neurons.
Neurons do not touch each other (except in the case of an electrical synapse through a gap junction); instead, neurons interact at contact points called synapses: a junction within two nerve cells, consisting of a miniature gap which impulses pass by a neurotransmitter. A neuron transports its information by way of a nerve impulse called an action potential. When an action potential arrives at the synapse's presynaptic terminal button, it may stimulate the release of neurotransmitters. These neurotransmitters are released into the synaptic cleft to bind onto the receptors of the postsynaptic membrane and influence another cell, either in an inhibitory or excitatory way. The next neuron may be connected to many more neurons, and if the total of excitatory influences is greater than that of inhibitory influences, it will also "fire". That is to say, it will create a new action potential at its axon hillock, releasing neurotransmitters and passing on the information to yet another neighboring neuron.
Excitatory and inhibitory.
A neurotransmitter can influence the function of a neuron through a remarkable number of mechanisms. In its direct actions in influencing a neuron’s electrical excitability, however, a neurotransmitter acts in only one of two ways: excitatory or inhibitory. A neurotransmitter influences trans-membrane ion flow either to increase (excitatory) or to decrease (inhibitory) the probability that the cell with which it comes in contact will produce an action potential. Thus, despite the wide variety of synapses, they all convey messages of only these two types, and they are labeled as such. Type I synapses are excitatory in their actions, whereas type II synapses are inhibitory. Each type has a different appearance and is located on different parts of the neurons under its influence. Each neuron receives thousands of excitatory and inhibitory signals every second.
Type I (excitatory) synapses are typically located on the shafts or the spines of dendrites, whereas type II (inhibitory) synapses are typically located on a cell body. In addition, Type I synapses have round synaptic vesicles, whereas the vesicles of type II synapses are flattened. The material on the presynaptic and post-synaptic membranes is denser in a Type I synapse than it is in a type II, and the type I synaptic cleft is wider. Finally, the active zone on a Type I synapse is larger than that on a Type II synapse.
The different locations of type I and type II synapses divide a neuron into two zones: an excitatory dendritic tree and an inhibitory cell body. From an inhibitory perspective, excitation comes in over the dendrites and spreads to the axon hillock to trigger an action potential. If the message is to be stopped, it is best stopped by applying inhibition on the cell body, close to the axon hillock where the action potential originates. Another way to conceptualize excitatory–inhibitory interaction is to picture excitation overcoming inhibition. If the cell body is normally in an inhibited state, the only way to generate an action potential at the axon hillock is to reduce the cell body’s inhibition. In this “open the gates” strategy, the excitatory message is like a racehorse ready to run down the track, but first the inhibitory starting gate must be removed.
Examples of important neurotransmitter actions.
As explained above, the only direct action of a neurotransmitter is to activate a receptor. Therefore, the effects of a neurotransmitter system depend on the connections of the neurons that use the transmitter, and the chemical properties of the receptors that the transmitter binds to.
Here are a few examples of important neurotransmitter actions:
Brain neurotransmitter systems.
Neurons expressing certain types of neurotransmitters sometimes form distinct systems, where activation of the system affects large volumes of the brain, called volume transmission. Major neurotransmitter systems include the noradrenaline (norepinephrine) system, the dopamine system, the serotonin system, and the cholinergic system, among others. It should be noted that trace amines, primarily via TAAR1 activation, have a very significant impact on neurotransmission in monoamine pathways (i.e., dopamine, histamine, norepinephrine, and serotonin pathways) throughout the brain. A brief comparison of these systems follows:
Drug effects.
Understanding the effects of drugs on neurotransmitters comprises a significant portion of research initiatives in the field of neuroscience. Most neuroscientists involved in this field of research believe that such efforts may further advance our understanding of the circuits responsible for various neurological diseases and disorders, as well as ways to effectively treat and someday possibly prevent or cure such illnesses.
Drugs can influence behavior by altering neurotransmitter activity. For instance, drugs can decrease the rate of synthesis of neurotransmitters by affecting the synthetic enzyme(s) for that neurotransmitter. When neurotransmitter syntheses are blocked, the amount of neurotransmitters available for release becomes substantially lower, resulting in a decrease in neurotransmitter activity. Some drugs block or stimulate the release of specific neurotransmitters. Alternatively, drugs can prevent neurotransmitter storage in synaptic vesicles by causing the synaptic vesicle membranes to leak. Drugs that prevent a neurotransmitter from binding to its receptor are called receptor antagonists. For example, drugs used to treat patients with schizophrenia such as haloperidol, chlorpromazine, and clozapine are antagonists at receptors in the brain for dopamine. Other drugs act by binding to a receptor and mimicking the normal neurotransmitter. Such drugs are called receptor agonists. An example of a receptor agonist is Valium, a benzodiazepine that mimics effects of the endogenous neurotransmitter gamma-aminobutyric acid (GABA) to decrease anxiety. Other drugs interfere with the deactivation of a neurotransmitter after it has been released, thereby prolonging the action of a neurotransmitter. This can be accomplished by blocking re-uptake or inhibiting degradative enzymes. Lastly, drugs can also prevent an action potential from occurring, blocking neuronal activity throughout the central and peripheral nervous system. Drugs such as tetrodotoxin that block neural activity are typically lethal.
Drugs targeting the neurotransmitter of major systems affect the whole system, which can explain the complexity of action of some drugs. Cocaine, for example, blocks the re-uptake of dopamine back into the presynaptic neuron, leaving the neurotransmitter molecules in the synaptic gap for an extended period of time. Since the dopamine remains in the synapse longer, the neurotransmitter continues to bind to the receptors on the postsynaptic neuron, eliciting a pleasurable emotional response. Physical addiction to cocaine may result from prolonged exposure to excess dopamine in the synapses, which leads to the downregulation of some post-synaptic receptors. After the effects of the drug wear off, an individual can become depressed due to decreased probability of the neurotransmitter binding to a receptor. Fluoxetine is a selective serotonin re-uptake inhibitor (SSRI), which blocks re-uptake of serotonin by the presynaptic cell which increases the amount of serotonin present at the synapse and furthermore allows it to remain there longer, providing potential for the effect of naturally released serotonin. AMPT prevents the conversion of tyrosine to L-DOPA, the precursor to dopamine; reserpine prevents dopamine storage within vesicles; and deprenyl inhibits monoamine oxidase (MAO)-B and thus increases dopamine levels.
Agonists.
An agonist is a chemical capable of binding to a receptor, such as a neurotransmitter receptor, and initiating the same reaction typically produced by the binding of the endogenous substance. An agonist of a neurotransmitter will thus initiate the same receptor response as the transmitter. In neurons, an agonist drug may activate neurotransmitter receptors either directly or indirectly. Direct-binding agonists can be further characterized as full agonists, partial agonists, inverse agonists.
Direct agonists act similar to a neurotransmitter by binding directly to its associated receptor site(s), which may be located on the presynaptic neuron or postsynaptic neuron, or both. Typically, neurotransmitter receptors are located on the postsynaptic neuron, while neurotransmitter autoreceptors are located on the presynaptic neuron, as is the case for monoamine neurotransmitters; in some cases, a neurotransmitter utilizes retrograde neurotransmission, a type of feedback signaling in neurons where the neurotransmitter is released postsynaptically and binds to target receptors located on the presynaptic neuron. Nicotine, a compound found in tobacco, is a direct agonist of most nicotinic acetylcholine receptors, mainly located in cholinergic neurons. Opiates, such as morphine, heroin, hydrocodone, oxycodone, codeine, and methadone, are μ-opioid receptor agonists; this action mediates their euphoriant and pain relieving properties.
Indirect agonists increase the binding of neurotransmitters at their target receptors by stimulating the release or preventing the reuptake of neurotransmitters. Some indirect agonists trigger neurotransmitter release and prevent neurotransmitter reuptake. Amphetamine, for example, is an indirect agonist of postsynaptic dopamine, norepinephrine, and serotonin receptors in each their respective neurons; it produces both neurotransmitter release into the presynaptic neuron and subsequently the synaptic cleft and prevents their reuptake from the synaptic cleft by activating TAAR1, a presynaptic G protein-coupled receptor, and binding to a site on VMAT2, a type of monoamine transporter located on synaptic vesicles within monoamine neurons.
Antagonists.
An antagonist is a chemical that acts within the body to reduce the physiological activity of another chemical substance (as an opiate); especially one that opposes the action on the nervous system of a drug or a substance occurring naturally in the body by combining with and blocking its nervous receptor.
There are two main types of antagonist: direct-acting Antagonist and indirect-acting Antagonists:
Drug antagonists.
An antagonist drug is one that attaches (or binds) to a site called a receptor without activating that receptor to produce a biological response. It is therefore said to have no intrinsic activity. An antagonist may also be called a receptor "blocker" because they block the effect of an agonist at the site. The pharmacological effects of an antagonist therefore result in preventing the corresponding receptor site's agonists (e.g., drugs, hormones, neurotransmitters) from binding to and activating it. Antagonists may be "competitive" or "irreversible".
A competitive antagonist competes with an agonist for binding to the receptor. As the concentration of antagonist increases, the binding of the agonist is progressively inhibited, resulting in a decrease in the physiological response. High concentration of an antagonist can completely inhibit the response. This inhibition can be reversed, however, by an increase of the concentration of the agonist, since the agonist and antagonist compete for binding to the receptor. Competitive antagonists, therefore, can be characterized as shifting the dose-response relationship for the agonist to the right. In the presence of a competitive antagonist, it takes an increased concentration of the agonist to produce the same response observed in the absence of the antagonist.
An irreversible antagonist binds so strongly to the receptor as to render the receptor unavailable for binding to the agonist. Irreversible antagonists may even form covalent chemical bonds with the receptor. In either case, if the concentration of the irreversible antagonist is high enough, the number of unbound receptors remaining for agonist binding may be so low that even high concentrations of the agonist do not produce the maximum biological response.
Precursors.
While intake of neurotransmitter precursors does increase neurotransmitter synthesis, evidence is mixed as to whether neurotransmitter release and postsynaptic receptor firing is increased. Even with increased neurotransmitter release, it is unclear whether this will result in a long-term increase in neurotransmitter signal strength, since the nervous system can adapt to changes such as increased neurotransmitter synthesis and may therefore maintain constant firing. Some neurotransmitters may have a role in depression and there is some evidence to suggest that intake of precursors of these neurotransmitters may be useful in the treatment of mild and moderate depression.
Catecholamine and trace amine precursors.
L-DOPA, a precursor of dopamine that crosses the blood–brain barrier, is used in the treatment of Parkinson's disease. For depressed patients where low activity of the neurotransmitter norepinephrine is implicated, there is only little evidence for benefit of neurotransmitter precursor administration. L-phenylalanine and L-tyrosine are both precursors for dopamine, norepinephrine, and epinephrine. These conversions require vitamin B6, vitamin C, and S-adenosylmethionine. A few studies suggest potential antidepressant effects of L-phenylalanine and L-tyrosine, but there is much room for further research in this area.
Serotonin precursors.
Administration of L-tryptophan, a precursor for serotonin, is seen to double the production of serotonin in the brain. It is significantly more effective than a placebo in the treatment of mild and moderate depression. This conversion requires vitamin C. 5-hydroxytryptophan (5-HTP), also a precursor for serotonin, is more effective than a placebo.
Diseases and disorders.
Diseases and disorders may also affect specific neurotransmitter systems. For example, problems in producing dopamine can result in Parkinson's disease, a disorder that affects a person's ability to move as they want to, resulting in stiffness, tremors or shaking, and other symptoms. Some studies suggest that having too little dopamine or problems using dopamine in the thinking and feeling regions of the brain may play a role in disorders like schizophrenia or attention deficit hyperactivity disorder (ADHD). Moreover, research shows that people diagnosed with depression often have lower than normal levels of serotonin. The types of medications most commonly prescribed to treat depression act by blocking the recycling, or reuptake, of serotonin by the sending neuron. As a result, more serotonin stays in the synapse for the receiving neuron to bind onto, leading to more normal mood functioning. Furthermore, problems in making or using glutamate have been linked to many mental disorders, including autism, obsessive compulsive disorder (OCD), schizophrenia, and depression.
Elimination of neurotransmitters.
A neurotransmitter must be broken down once it reaches the post-synaptic cell to prevent further excitatory or inhibitory signal transduction. This allows new signals to be produced from the adjacent nerve cells. When the neurotransmitter has been secreted into the synaptic cleft, it binds to specific receptors on the postsynaptic cell, thereby generating a postsynaptic electrical signal. The transmitter must then be removed rapidly to enable the postsynaptic cell to engage in another cycle of neurotransmitter release, binding, and signal generation. Neurotransmitters are terminated in three different ways:
For example, choline is taken up and recycled by the pre-synaptic neuron to synthesize more ACh. Other neurotransmitters such as dopamine are able to diffuse away from their targeted synaptic junctions and are eliminated from the body via the kidneys, or destroyed in the liver. Each neurotransmitter has very specific degradation pathways at regulatory points, which may be targeted by the body's regulatory system or by recreational drugs.
Neurotransmitter imbalance.
Neurotransmitter imbalances have been connected to the cause of many diseases. These include Parkinson's, depression, insomnia, Attention Deficit Hyperactivity Disorder (ADHD), anxiety, memory loss, dramatic changes in weight and addictions. They all involve amino acids which form neurotransmitters. The acids are made up of protein and without a sufficient amount of this then cells are not structured properly; therefore not functioning properly. Chronic stress is the primary contributor to neurotransmitter imbalance. Physical and emotional stress from a job or a relationship causes neurons to use up large amounts of neurotransmitters in order to cope with the ongoing stress. Over time the stress wears out the nervous system and depletes neurotransmitter supply. Genetics play a part in correlating with neurotransmitter imbalance. Some people are already born with neurotransmitter deficiencies or excesses. Scientists are trying to supplement medication by changing the diets of some patients instead; adding amino acids into the body. Medications that directly react with serotonin and norepinephrine are prescribed to patients with diseases such as depression and anxiety disorders.

</doc>
<doc id="21868" url="https://en.wikipedia.org/wiki?curid=21868" title="Neutronium">
Neutronium

Neutronium (sometimes shortened to neutrium) is a proposed name for a substance composed purely of neutrons. The word was coined by scientist Andreas von Antropoff in 1926 (before the discovery of the neutron) for the conjectured "element of atomic number zero" that he placed at the head of the periodic table. However, the meaning of the term has changed over time, and from the last half of the 20th century onward it has been also used legitimately to refer to extremely dense substances resembling the neutron-degenerate matter theorized to exist in the cores of neutron stars; henceforth ""degenerate" neutronium" will refer to this. Science fiction and popular literature frequently use the term "neutronium" to refer to a highly dense phase of matter composed primarily of neutrons.
Neutronium and neutron stars.
Neutronium is used in popular literature to refer to the material present in the cores of neutron stars (stars which are too massive to be supported by electron degeneracy pressure and which collapse into a denser phase of matter). This term is very rarely used in scientific literature, for three reasons:
When neutron star core material is presumed to consist mostly of free neutrons, it is typically referred to as neutron-degenerate matter in scientific literature.
Neutronium and the periodic table.
The term "neutronium" was coined in 1926 by Andreas von Antropoff for a conjectured form of matter made up of neutrons with no protons or electrons, which he placed as the chemical element of atomic number zero at the head of his new version of the periodic table. It was subsequently placed in the middle of several spiral representations of the periodic system for classifying the chemical elements, such as those of Charles Janet (1928), E. I. Emerson (1944), John D. Clark (1950) and in Philip Stewart's Chemical Galaxy (2005).
Although the term is not used in the scientific literature either for a condensed form of matter, or as an element, there have been reports that, besides the free neutron, there may exist two bound forms of neutrons without protons. If neutronium were considered to be an element, then these neutron clusters could be considered to be the isotopes of that element. However, these reports have not been further substantiated.
Although not called "neutronium", the National Nuclear Data Center's "Nuclear Wallet Cards" lists as its first "isotope" an "element" with the symbol n and atomic number "Z" = 0 and mass number "A" = 1. This isotope is described as decaying to element H with a half life of .
Properties.
Due to beta (β−) decay of mononeutron and extreme instability of aforementioned heavier "isotopes", degenerate neutronium is not expected to be stable under ordinary pressures. Free neutrons decay with a half-life of 10 minutes, 11 seconds. A teaspoon of degenerate neutronium gas would have a mass of two billion tonnes, and if moved to standard temperature and pressure, would emit 57 billion joules of β− decay energy in the first half-life (average of 95 MW of power). This energy may be absorbed as the neutronium gas expands. Though, in the presence of atomic matter compressed to the state of electron degeneracy, the β− decay may be inhibited due to Pauli exclusion principle, thus making free neutrons stable. Also, elevated pressures should make neutrons degenerate themselves. Compared to ordinary elements, neutronium should be more compressible due to the absence of electrically charged protons and electrons. This makes neutronium more energetically favorable than (positive-"Z") atomic nuclei and leads to their conversion to (degenerate) neutronium through electron capture, a process which is believed to occur in stellar cores in the final seconds of the lifetime of massive stars, where it is facilitated by cooling via emission. As a result, degenerate neutronium can have a density of , roughly 13 magnitudes denser than the densest known ordinary substances. It was theorized that extreme pressures may deform the neutrons into a cubic symmetry, allowing tighter packing of neutrons, or cause a strange matter formation.
In fiction.
The term "neutronium" has been popular in science fiction since at least the middle of the 20th century. It typically refers to an extremely dense, incredibly strong form of matter. While presumably inspired by the concept of neutron-degenerate matter in the cores of neutron stars, the material used in fiction bears at most only a superficial resemblance, usually depicted as an extremely strong solid under Earth-like conditions, or possessing exotic properties such as the ability to manipulate time and space. In contrast, all proposed forms of neutron star core material are fluids and are extremely unstable at pressures lower than that found in stellar cores. According to one analysis, a neutron star with a mass below about 0.2 solar masses will explode.
Noteworthy appearances of neutronium in fiction include the following:

</doc>
