<doc id="23287" url="https://en.wikipedia.org/wiki?curid=23287" title="Passport">
Passport

A passport is a travel document, usually issued by a country's government, that certifies the identity and nationality of its holder for the purpose of international travel. Standard passports contain the holder's name, place and date of birth, photograph, signature, and other identifying information. Passports are moving towards including biometric information in a microchip embedded in the document, making them machine-readable and difficult to counterfeit.
A passport specifies nationality, but not necessarily citizenship or the place of residence of the passport holder. A passport holder is normally entitled to enter the country that issued the passport, though some people entitled to a passport may not be full citizens with right of abode. A passport is a document certifying identity and nationality; having the document does not of itself grant any rights, such as protection by the consulate of the issuing country, although it may indicate that the holder has such rights. Some passports attest to status as a diplomat or other official, entitled to rights and privileges such as immunity from arrest or prosecution, arising from international treaties.
Many countries normally allow entry to holders of passports of other countries, sometimes requiring a visa also to be held, but this is not an automatic right. Many other additional conditions, such as not being likely to become a public charge for financial or other reasons, and the holder not having been convicted of a crime, may be applicable. Where a country does not recognise another, or is in dispute with it, it may prohibit the use of their passport for travel to that other country, or may prohibit entry to holders of that other country's passports, and sometimes to others who have, for example, visited the other country.
Some countries and international organisations issue travel documents which are not standard passports, but enable the holder to travel internationally to countries that recognise the documents. For example, stateless persons are not normally issued a national passport, but may be able to obtain a refugee travel document or the earlier "Nansen passport" which enables them to travel to countries which recognise them, and sometimes to return to the issuing country. A country may issue a passport to any person, including non-nationals.
A passport is often accepted, in its country of issue and elsewhere, as reliable proof of identity, unrelated to travel.
History.
One of the earliest known references to paperwork that served in a role similar to that of a passport is found in the Hebrew Bible. , dating from approximately 450 BC, states that Nehemiah, an official serving King Artaxerxes I of Persia, asked permission to travel to Judea; the king granted leave and gave him a letter "to the governors beyond the river" requesting safe passage for him as he traveled through their lands.
In the medieval Islamic Caliphate, a form of passport was the "bara'a", a receipt for taxes paid. Only people who paid their "zakah" (for Muslims) or "jizya" (for dhimmis) taxes were permitted to travel to different regions of the Caliphate; thus, the "bara'a" receipt was a "traveler's basic passport."
Etymological sources show that the term "passport" is from a medieval document that was required in order to pass through the gate (or "porte") of a city wall or to pass through a territory. In medieval Europe, such documents were issued to travelers by local authorities, and generally contained a list of towns and cities the document holder was permitted to enter or pass through. On the whole, documents were not required for travel to sea ports, which were considered open trading points, but documents were required to travel inland from sea ports.
King Henry V of England is credited with having invented what some consider the first true passport, as a means of helping his subjects prove who they were in foreign lands. The earliest reference to these documents is found in a 1414 Act of Parliament. In 1540, granting travel documents in England became a role of the Privy Council of England, and it was around this time that the term "passport" was used. In 1794, issuing British passports became the job of the Office of the Secretary of State. The 1548 Imperial Diet of Augsburg required the public to hold imperial documents for travel, at the risk of permanent exile.
A rapid expansion of rail travel and wealth in Europe beginning in the mid-nineteenth century led to a weakening of the passport system for approximately thirty years prior to World War I. The speed of trains, as well as the number of passengers that crossed multiple borders, made enforcement of passport laws difficult. The general reaction was the relaxation of passport requirements. In the later part of the nineteenth century and up to World War I, passports were not required, on the whole, for travel within Europe, and crossing a border was a relatively straightforward procedure. Consequently, comparatively few people held passports.
During World War I, European governments introduced border passport requirements for security reasons, and to control the emigration of people with useful skills. These controls remained in place after the war, becoming a standard, though controversial, procedure. British tourists of the 1920s complained, especially about attached photographs and physical descriptions, which they considered led to a "nasty dehumanization".
In 1920, the League of Nations held a conference on passports, the Paris Conference on Passports & Customs Formalities and Through Tickets. Passport guidelines and a general booklet design resulted from the conference, which was followed up by conferences in 1926 and 1927.
While the United Nations held a travel conference in 1963, no passport guidelines resulted from it. Passport standardization came about in 1980, under the auspices of the International Civil Aviation Organization (ICAO). ICAO standards include those for machine-readable passports. Such passports have an area where some of the information otherwise written in textual form is written as strings of alphanumeric characters, printed in a manner suitable for optical character recognition. This enables border controllers and other law enforcement agents to process these passports more quickly, without having to input the information manually into a computer. ICAO publishes Doc 9303 "Machine Readable Travel Documents", the technical standard for machine-readable passports. A more recent standard is for biometric passports. These contain biometrics to authenticate the identity of travellers. The passport's critical information is stored on a tiny RFID computer chip, much like information stored on smartcards. Like some smartcards, the passport booklet design calls for an embedded contactless chip that is able to hold digital signature data to ensure the integrity of the passport and the biometric data.
Issuance.
Historically, legal authority to issue passports is founded on the exercise of each nation’s executive discretion (or Crown prerogative). Certain legal tenets follow, namely: first, passports are issued in the name of the state; second, no person has a legal right to be issued a passport; third, each nation’s government, in exercising its executive discretion, has complete and unfettered discretion to refuse to issue or to revoke a passport; and fourth, that the latter discretion is not subject to judicial review. However, legal scholars like A.J. Arkelian have argued that evolutions in both the constitutional law of democratic nations and the international law applicable to all nations now render those historical tenets both obsolete and unlawful.
Under some circumstances some countries allow people to hold more than one passport document; the UK is one example. This applies usually to people who travel a lot on business, and may need to have, say, a passport to travel on while another is awaiting a visa for another country. Reasons and supporting documentation (such as a letter from an employer) must be provided to apply for a second UK document.
National conditions.
Many countries issue only one passport to each national (an exception is the Family Passport, see below under "Types"). When passport holders apply for a new passport (commonly, due to expiration of an old passport or lack of blank pages), they may be required to surrender the old passport for invalidation. In some circumstances an expired passport is not required to be surrendered or invalidated (for example, if it contains an unexpired visa).
Under the law of most countries, passports are government property, and may be limited or revoked at any time, usually on specified grounds, and possibly subject to judicial review. In many countries, surrender of the passport is a condition of granting bail in lieu of imprisonment for a pending criminal trial.
Each country sets its own conditions for the issue of passports. For example, Pakistan requires applicants to be interviewed before a Pakistani passport will be granted. When applying for a passport or a national ID card, all Pakistanis are required to sign an oath declaring Mirza Ghulam Ahmad to be an impostor prophet and all Ahmadis to be non-Muslims.
Some countries limit the issuance of passports, where incoming and outgoing international travels are highly regulated, such as North Korea, where general use passports are the privilege of a very small number of people trusted by the government. Other countries put requirements on some citizens in order to be granted passports, such as Finland, where male citizens aged 18–30 years must prove that they have completed, or are exempt from, their obligatory military service to be granted an unrestricted passport; otherwise a passport is issued valid only until the end of their 28th year, to ensure that they return to carry out military service. Other countries with obligatory military service, such as Syria, have similar requirements.
National status.
Passports contain a statement of the nationality of the holder. In most countries, only one class of nationality exists, and only one type of ordinary passport is issued. However, several types of exceptions exist:
Multiple classes of nationality in a single country.
The United Kingdom has a number of classes of United Kingdom nationality due to its colonial history. As a result, the UK issues various passports which are similar in appearance but representative of different nationality statuses which, in turn, has caused foreign governments to subject holders of different UK passports to different entry requirements.
Multiple types of passports, one nationality.
The People's Republic of China (PRC) authorizes its Special Administrative Regions of Hong Kong and Macau to issue passports to their permanent residents with Chinese nationality under the "one country, two systems" arrangement. Visa policies imposed by foreign authorities on Hong Kong and Macau permanent residents holding such passports are different from those holding ordinary passports of the People's Republic of China. A Hong Kong Special Administrative Region passport (HKSAR passport) permits visa-free access to many more countries than ordinary PRC passports.
The three constituent countries of the Danish Realm have a common nationality. Denmark proper is a member of the European Union, but Greenland and Faroe Islands are not. Danish citizens residing in Greenland or Faroe Islands can choose between holding a Danish EU passport and a Greenlandic or Faroese non-EU Danish passport.
Special nationality class through investment.
In rare instances a nationality is available through investment. Some investors have been described in Tongan passports as 'a Tongan protected person', a status which does not necessarily carry with it the right of abode in Tonga.
Passports without sovereign territory.
Several entities without a sovereign territory issue documents described as passports, most notably Iroquois League, the Aboriginal Provisional Government in Australia and the Sovereign Military Order of Malta. Such documents are not necessarily accepted for entry into a country.
Validity.
Passports have a limited validity, usually between 5 and 10 years.
Many countries require a remaining passport validity of no less than six months on arrival, as well as having at least one or two blank pages. These countries include Afghanistan, Algeria, Bhutan, Botswana, Brunei, Cambodia, Comoros, Ecuador, Egypt, El Salvador, Fiji, Guyana, Indonesia, Iran, Iraq (except when arriving at Basra, Erbil or Sulaimaniyah, which only require three months validity on arrival), India, Israel, Ivory Coast, Kenya, Laos, Madagascar, Malaysia, Marshall Islands, Mozambique, Myanmar, Namibia, Nicaragua, Nigeria, Oman, Palau, Papua New Guinea, Philippines, Rwanda, Saint Lucia, Samoa, Saudi Arabia, Singapore, Solomon Islands, Sri Lanka, Suriname, Taiwan, Tanzania, Timor-Leste, Tonga, Turkey, Tuvalu, Uganda, Vanuatu, Venezuela, and Vietnam.
Countries requiring remaining validity of at least four months on arrival include Micronesia and Zambia.
Countries requiring remaining validity of at least three months on arrival include European Union countries (except Denmark, Ireland and the United Kingdom and except between each other), Georgia, Honduras, Iceland, Jordan, Kuwait, Lebanon, Liechtenstein, Moldova, Nauru, New Zealand, Panama, Switzerland, and the United Arab Emirates.
Countries requiring remaining validity of at least 1 month on arrival include Eritrea, Hong Kong, Macau, and South Africa. Other countries require either a passport valid on arrival or passport valid throughout the period of intended stay.
Types.
A rough standardization exists in types of passports throughout the world, although passport types, number of pages and definitions can vary by country.
Non-citizen passports.
Latvia and Estonia.
Non-citizens in Latvia and Estonia are individuals, primarily of Russian or Ukrainian ethnicity, who are not citizens of Latvia or Estonia but whose families have resided in the area since the Soviet era, and thus have the right to a non-citizen passport issued by the Latvian government as well as other specific rights. Approximately two thirds of them are ethnic Russians, followed by ethnic Belarussians, ethnic Ukrainians, ethnic Poles and ethnic Lithuanians.
Non-citizens in the two countries are issued special non-citizen passports as opposed to regular passports issued by the Estonian and Latvian authorities to citizens. This form of legal discrimination is often labelled as xenophobic.
American Samoa.
Although all U.S. citizens are also U.S. nationals, the reverse is not true. As specified in , a person whose only connection to the U.S. is through birth in an outlying possession (which is defined in as American Samoa and Swains Island (which is administered as part of American Samoa)), or through descent from a person so born, acquires U.S. nationality but not U.S. citizenship. This was formerly the case in only four other current or former U.S. overseas possessions.
The U.S. passport issued to non-citizen nationals contains the endorsement code 9 which states: "THE BEARER IS A UNITED STATES NATIONAL AND NOT A UNITED STATES CITIZEN." on the annotations page.
Non-citizen U.S. nationals may reside and work in the United States without restrictions, and may apply for citizenship under the same rules as resident aliens. Like resident aliens, they are not presently allowed by any U.S. state to vote in federal or state elections, although, as with resident aliens, there is no constitutional prohibition against their doing so.
Intra-sovereign territory travel that requires passports.
For some countries, passports are required for some types of travel between their sovereign territories. Two examples of this are:
Internal passports are issued by some countries as an identity document. An example is the internal passport of Russia or certain other post-Soviet countries dating back to imperial times. Some countries use internal passports for controlling migration within a country.
Designs and format.
International Civil Aviation Organization standards.
The International Civil Aviation Organization (ICAO) issues passport standards which are treated as recommendations to national governments. The size of passport booklets normally complies with the ISO/IEC 7810 ID-3 standard, which specifies a size of 125 × 88 mm (4.921 × 3.465 in). This size is the B7 format. Passport cards are issued to the ID-1 (credit card sized) standard.
Common designs.
Passport booklets from almost all countries around the world display the national coat of arms of the issuing country on the front cover. The United Nations keeps a record of national coats of arms.
There are several groups of countries, who through mutual agreement, have adopted common designs for the passports of their respective countries:
Request page.
Passports often contain a message, usually near the front, requesting that the passport's bearer be allowed to pass freely, and further requesting that, in the event of need, the bearer be granted assistance. The message is sometimes made in the name of the government or the head of state, and may be written in more than one language, depending on the language policies of the issuing authority. There are countries, such as Switzerland, Finland and Austria, on whose passports such messages are absent.
Languages.
In 1920, an international conference on passports and through tickets held by the League of Nations recommended that passports be issued in French, historically the language of diplomacy, and one other language. Currently, the ICAO recommends that passports be issued in English and French, or in the national language of the issuing country and in either English or French. Many European countries use their national language, along with English and French.
Some unusual language combinations are:
Immigration stamps.
For immigration control, officials of many countries use entry and exit stamps. Depending on the country, a stamp can serve different purposes. For example, in the United Kingdom, an immigration stamp in a passport includes the formal leave to enter granted to a person subject to entry control. In other countries, a stamp activates or acknowledges the continuing leave conferred in the passport bearer's entry clearance.
Under the Schengen system, a foreign passport is stamped with a date stamp which does not indicate any duration of stay. This means that the person is deemed to have permission to remain either for three months or for the period shown on his visa (whichever is shorter).
Visas often take the form of an inked stamp, although some countries use adhesive stickers that incorporate security features to discourage forgery.
Member states of the European Union are not permitted to place a stamp in the passport of a person who is not subject to immigration control. Stamping is prohibited because it is an imposition of a control that the person is not subject to.
Countries usually have different styles of stamps for entries and exits, to make it easier to identify the movements of people. Other ways to easily determine information. Ink color might be used to designate mode of transportation (air, land or sea), such as in Hong Kong prior to 1997; while border styles did the same thing in Macau. Other variations include changing the size of the stamp to indicate length of stay, as in Singapore.
Immigration stamps are a useful reminder of travels. Some travellers "collect" immigration stamps in passports, and will choose to enter or exit countries via different means (for example, land, sea or air) in order to have different stamps in their passports. Some countries, such as Liechtenstein, that do not stamp passports may provide a passport stamp on request for such "memory" purposes. However, such memorial stamps can preclude the passport bearer from travelling to certain countries. For example, Finland consistently rejects what they call 'falsified passports', where travelers have been refused visas or entry due to memorial stamps and are required to renew their passports.
Limitations on use.
A passport is merely an identity document that is widely recognised for international travel purposes, and the possession of a passport does not in itself entitle a traveller to enter any country other than the country that issued it, and sometimes not even then. Many countries normally require visitors to obtain a visa. Each country has different requirements or conditions for the grant of visas, such as for the visitor not being likely to become a public charge for financial, health, family, or other reasons, and the holder not having been convicted of a crime or considered likely to commit one.
Where a country does not recognise another, or is in dispute with it, entry may be prohibited to holders of passports of the other party to the dispute, and sometimes to others who have, for example, visited the other country; examples are listed below. A country that issues a passport may also restrict its validity or use in specified circumstances, such as use for travel to certain countries for political, security or health reasons.
International travel without passports.
International travel is possible without passports in some circumstances. Nonetheless, a document stating citizenship, such as a national identity card or an Enhanced Drivers License, is usually required.

</doc>
<doc id="23288" url="https://en.wikipedia.org/wiki?curid=23288" title="Point-of-view shot">
Point-of-view shot

A point of view shot (also known as POV shot or a subjective camera) is a short film scene that shows what a character (the subject) is looking at (represented through the camera). It is usually established by being positioned between a shot of a character looking at something, and a shot showing the character's reaction (see shot reverse shot). The technique of POV is one of the foundations of film editing.
A POV shot need not be the strict point-of-view of an actual single character in a film. Sometimes the point-of-view shot is taken over the shoulder of the character (third person), who remains visible on the screen. Sometimes a POV shot is "shared" ("dual" or "triple"), i.e. it represents the joint POV of two (or more) characters.
Point-of-view, or simply p.o.v., camera angles record the scene from a particular player's viewpoint. The point-of-view is an objective angle, but since it falls between the objective and subjective angle, it should be placed in a separate category and given special consideration. A point-of-view shot is as close as an objective shot can approach a subjective shot—and still remain objective. The camera is positioned at the side of a subjective player—whose viewpoint is being depicted—so that the audience is given the impression they are standing cheek-to-cheek with the off-screen player. The viewer does not see the event through the player's eyes, as in a subjective shot in which the camera trades places with the screen player. He sees the event from the player's viewpoint, as if standing alongside him. Thus, the camera angle remains objective, since it is an unseen observer not involved in the action." 
Supporting narrative elements are required to indicate the shot to the viewer as a POV shot. These may include shot sequencing, sound effects, visual effects and acting.
Leading actor POV.
When the leading actor is the subject of the POV it is known as the subjective viewpoint. The audience sees events through the leading actor's eyes, as if they were experiencing the events themselves. Some films are partially or totally shot using this technique, for example the 1947 film noir "Lady in the Lake", which is shot entirely through the subjective POV of its central character in an attempt to replicate the first-person narrative style of the Raymond Chandler novel upon which the film is based.
Technology.
POV footage has existed since the first cameras were mounted in early airplanes and cars, anywhere a film’s creator intended to take viewers inside the action with the psychological purpose of giving viewers a feel of "What he or she is going through", he or she being a participant in the subject matter. Cameras were increasingly introduced into more difficult experiences.
Dick Barrymore, an early action filmmaker akin to Warren Miller, experimented with film cameras and counter weights mounted to a helmet. Barrymore could ski unencumbered while capturing footage of scenery and other skiers. Though the unit was heavy relative to its manner of use, it was considered hands-free, and worked.
Numerous companies have developed successful POV designs, from laparoscopic video equipment used inside the body during medical procedures, to high tech film and digital cameras mounted to jets and employed during flight. On professional levels, the equipment is well defined, expensive, and requires intensive training and support.
However the race for hands-free POV cameras for use on a consumer level has always faced problems. The technology has had issues with usability, combining lenses with microphones with batteries with recording units; all connected using spidery cables, which proved cumbersome in use when compared to the quality of the end content.
Notable examples.
In making 1927's "Napoléon", director Abel Gance wrapped a camera and much of the lens in sponge padding so that it could be punched by other actors to portray the leading character's point of view during a fist fight, part of a larger snowball fight between schoolboys including young Napoleon. Gance wrote in the technical scenario that the camera "defends itself as if it were Bonaparte himself. It is in the fortress and fights back. It clambers on the wall of snow and jumps down, as if it were human. A punch in the lens. Arms at the side of the camera as if the camera itself had arms. Camera K falls on the ground, struggles, gets up." In the scenario, "Camera K" refers to Gance's main photographer, Jules Kruger, who wore the camera mounted to a breastplate strapped to his chest for these shots.
POV shots were used extensively by Alfred Hitchock for various narrative effects.

</doc>
<doc id="23289" url="https://en.wikipedia.org/wiki?curid=23289" title="Persistence of vision">
Persistence of vision

Persistence of vision refers to the optical illusion whereby multiple discrete images blend into a single image in the human mind and believed to be the explanation for motion perception in cinema and animated films. Like other illusions of visual perception, it is produced by certain characteristics of the visual system.
History.
Narrowly defined, the theory of persistence of vision is the belief that human perception of motion (brain centered) is the result of persistence of vision (eye centered). That version of the theory was disproved in 1912 by Wertheimer but persists in many citations in many classic and modern film-theory texts. A more plausible theory to explain motion perception (at least on a descriptive level) are two distinct perceptual illusions: phi phenomenon and beta movement.
A visual form of memory known as iconic memory has been described as the cause of this phenomenon. Although psychologists and physiologists have rejected the relevance of this theory to film viewership, film academics and theorists generally have not. Some scientists nowadays consider the entire theory a myth.
In contrasting persistence of vision theory with phi phenomena, a critical part of understanding that emerges with these visual perception phenomena is that the eye "is not a camera" and does not see in frames per second. In other words, vision is not as simple as light registering on a medium, since the brain has to make sense of the visual data the eye provides and construct a coherent picture of reality. Joseph Anderson and Barbara Fisher argue that the phi phenomena privileges a more constructionist approach to the cinema (David Bordwell, Noël Carroll, Kirsten Thompson), whereas the persistence of vision privileges a realist approach (André Bazin, Christian Metz, Jean-Louis Baudry).
The discovery of persistence of vision is attributed to the Roman poet Lucretius, although he only mentions it in connection
with images seen in a dream. In the modern era, some stroboscopic experiments performed by Peter Mark Roget in 1824 were also cited as the basis for the theory.
Film systems.
Persistence of vision is still the accepted term for this phenomenon in the realm of cinema history and theory. Early practitioners tried different frame rates, and chose a rate of 16 frames per second (frame/s) as high enough to cause the mind to stop seeing flashing images. Audiences still interpret motion at rates as low as ten frames per second or slower (as in a flipbook), but the flicker caused by the shutter of a film projector is distracting below the 16-frame threshold.
Modern theatrical film runs at 24 frames a second. This is the case for both physical film and digital cinema systems.
It is important to distinguish between the "frame" rate and the "flicker" rate, which are not necessarily the same. In physical film systems, it is necessary to pull down the film frame, and this pulling-down needs to be obscured by a shutter to avoid the appearance of blurring; therefore, there needs to be at least one flicker per frame in film. To reduce the appearance of flicker, virtually all modern projector shutters are designed to add additional flicker periods, typically doubling the flicker rate to 48 Hz (single-bladed shutters make two rotations per frame – double-bladed shutters make one rotation per frame), which is less visible. (Some three-bladed projector shutters even triple it to 72 Hz.)
In digital film systems, the scan rate may be decoupled from the image update rate. In some systems, such as the Digital Light Processing (DLP) system, there is no flying spot or raster scan at all, so there is no flicker other than that generated by the temporal aliasing of the film image capture.
The new film system MaxiVision 48 films at 48 frames per second, which, according to film critic Roger Ebert, offers even a strobeless tracking shot past picket fences. The lack of strobe (as opposed to flicker) is due to the higher sampling rate of the camera relative to the speed of movement of the image across the film plane. This ultra-smooth imaging is called high motion.
Computer monitors.
Aside from some configurations used until the early 1990s, computer monitors do not use interlacing. They may sometimes be seen to flicker, often in a brightly lit room, and at close viewing distances. The greater flickering in close-up viewing is due to more of the screen being in the viewer's peripheral vision, which has more sensitivity to flickering. Generally, a refresh rate of 85 Hz or above (as found in most modern CRT monitors) is sufficient to minimize flicker in close viewing, and all recent computer monitors are capable of at least that rate. 
Flat-panel liquid crystal display (LCD) monitors do not suffer from flicker even if their refresh rate is 60 Hz or lower. This is because an LCD pixel generates a continuous stream of light as long as that part of the image is supposed to be lit (see also ghosting). With each scan, the monitor determines whether a pixel should be light or dark and changes the state of the pixel accordingly. In a CRT, by comparison, each pixel generates a temporary burst of light, then darkening, in each periodic scan. The monitor activates a phosphor on the screen during each scan if the pixel is supposed to be light, but the phosphor fades before the next scan.
Cartoon animation.
In drawn animation, moving characters are often shot "on twos", that is to say, one drawing is shown for every two frames of film (which usually runs at 24 frames per second), meaning there are only 12 drawings per second. Even though the image update rate is low, the fluidity is satisfactory for most subjects. However, when a character is required to perform a quick movement, it is usually necessary to revert to animating "on ones", as "twos" are too slow to convey the motion adequately. A blend of the two techniques keeps the eye fooled without unnecessary production cost.
Animation for most "Saturday morning cartoons" is produced as cheaply as possible, and is most often shot on "threes", or even "fours", i.e. three or four frames per drawing. This translates to only 8 or 6 drawings per second, respectively.
Printed media.
Flip books use this principle. If the book is flipped at a fast enough speed, the illusion of smooth motion is created.
Sparkler's trail effect.
The sparkler's trail effect occurs when one waves around a lit sparkler, creating a trail of light. Although it appears that this trail is created by the light left from the sparkler as it is waved through the air, there is, in fact, no light along this trail. The lighted trail is a creation of the mind, which retains a perception of the sparkler's light for a fraction of a second in sensory memory.
Persistence of vision displays.
A class of display device described as "POV" is one that composes an image by displaying one spatial portion at a time in rapid succession (for example, one column of pixels every few milliseconds). A two-dimensional POV display is often accomplished by means of rapidly moving a single row of LEDs along a linear or circular path. The effect is that the image is perceived as a whole by the viewer as long as the entire path is completed during the visual persistence time of the human eye. A further effect is often to give the illusion of the image floating in mid-air. A three-dimensional POV display is often constructed using a 2D grid of LEDs which is swept or rotated through a volume. POV display devices can be used in combination with long camera exposures to produce light writing.
A common example of this can be seen in the use of bicycle wheel lights that produce patterns.

</doc>
<doc id="23290" url="https://en.wikipedia.org/wiki?curid=23290" title="Propaganda film">
Propaganda film

A propaganda film is a film that involves some form of propaganda. Propaganda films may be packaged in numerous ways, but are most often documentary-style productions or fictional screenplays, that are produced to convince the viewer of a specific political point or influence the opinions or behavior of the viewer, often by providing subjective content that may be deliberately misleading.
Propaganda can be defined as the ability "to produce and spread fertile messages that, once sown, will germinate in large human cultures.” However, in the 20th century, a “new” propaganda emerged, which revolved around political organizations and their need to communicate messages that would “sway relevant groups of people in order to accommodate their agendas”. First developed by the Lumiere brothers in 1896, film provided a unique means of accessing large audiences at once. Film was the first universal mass medium in that it could simultaneously influence viewers as individuals and members of a crowd, which led to it quickly becoming a tool for governments and non-state organizations to project a desired ideological message. As Nancy Snow stated in her book, "Information War: American Propaganda, Free Speech and Opinion Control Since 9-11", propaganda "begins where critical thinking ends." 
Film as a propaganda tool.
Film is a unique medium in that it reproduces images, movement, and sound in a lifelike manner as it fuses meaning with evolvement as time passes in the story depicted. Unlike many other art forms, film produces a sense of immediacy. Film’s ability to create the illusion of life and reality, opening up new, unknown perspectives on the world, is why films, especially those of unknown cultures or places, are taken to be accurate depictions of life.
Some film academics have noted film’s great illusory abilities. Dziga Vertov claimed in his 1924 manifesto, “The Birth of Kino-Eye” that “the cinema-eye is cinema-truth.” To paraphrase Hilmar Hoffman, this means that in film, only what the camera ‘sees’ exists, and the viewer, lacking alternative perspectives, conventionally takes the image for reality.
Films are effective propaganda tools because they establish visual icons of historical reality and consciousness, define public attitudes of the time they’re depicting or that at which they were filmed, mobilize people for a common cause, or bring attention to an unknown cause. Political and historical films represent, influence, and create historical consciousness and are able to distort events making it a persuasive and possibly untrustworthy medium.

</doc>
<doc id="23291" url="https://en.wikipedia.org/wiki?curid=23291" title="Pliocene">
Pliocene

The Pliocene (; also Pleiocene) Epoch (symbol PO) is the epoch in the geologic timescale that extends from 5.333 million to 2.58 million years BP. It is the second and youngest epoch of the Neogene Period in the Cenozoic Era. The Pliocene follows the Miocene Epoch and is followed by the Pleistocene Epoch. Prior to the 2009 revision of the geologic time scale, which placed the four most recent major glaciations entirely within the Pleistocene, the Pliocene also included the Gelasian stage, which lasted from 2.588 to 1.806 million years ago, and is now included in the Pleistocene.
As with other older geologic periods, the geological strata that define the start and end are well identified but the exact dates of the start and end of the epoch are slightly uncertain. The boundaries defining the Pliocene are not set at an easily identified worldwide event but rather at regional boundaries between the warmer Miocene and the relatively cooler Pliocene. The upper boundary was set at the start of the Pleistocene glaciations.
Etymology.
The Pliocene was named by Sir Charles Lyell. The name comes from the Greek words ("pleion", "more") and ("kainos", "new") and means roughly "continuation of the recent", referring to the essentially modern marine mollusc faunas. H.W. Fowler called the term (along with other examples such as "pleistocene" and "miocene") a "regrettable barbarism" and an indication that even "a good classical scholar" such as Lyell should have requested a philologist's help when coining words.
Subdivisions.
In the official timescale of the ICS, the Pliocene is subdivided into two stages. From youngest to oldest they are:
The Piacenzian is sometimes referred to as the Late Pliocene, whereas the Zanclean is referred to as the Early Pliocene.
In the system of
In the Paratethys area (central Europe and parts of western Asia) the Pliocene contains the Dacian (roughly equal to the Zanclean) and Romanian (roughly equal to the Piacenzian and Gelasian together) stages. As usual in stratigraphy, there are many other regional and local subdivisions in use.
In Britain the Pliocene is divided into the following stages (old to young): Gedgravian, Waltonian, Pre-Ludhamian, Ludhamian, Thurnian, Bramertonian or Antian, Pre-Pastonian or Baventian, Pastonian and Beestonian. In the Netherlands the Pliocene is divided into these stages (old to young): Brunssumian C, Reuverian A, Reuverian B, Reuverian C, Praetiglian, Tiglian A, Tiglian B, Tiglian C1-4b, Tiglian C4c, Tiglian C5, Tiglian C6 and Eburonian. The exact correlations between these local stages and the ICS stages is still a matter of detail.
Climate.
The global average temperature in the mid-Pliocene (3.3–3 mya) was 2–3 °C higher than today, and the Northern hemisphere ice sheet was ephemeral before the onset of extensive glaciation over Greenland that occurred in the late Pliocene around 3 Ma.
The formation of an Arctic ice cap is signaled by an abrupt shift in oxygen isotope ratios and ice-rafted cobbles in the North Atlantic and North Pacific ocean beds. Mid-latitude glaciation was probably underway before the end of the epoch. The global cooling that occurred during the Pliocene may have spurred on the disappearance of forests and the spread of grasslands and savannas.
Paleogeography.
Continents continued to drift, moving from positions possibly as far as 250 km from their present locations to positions only 70 km from their current locations. South America became linked to North America through the Isthmus of Panama during the Pliocene, making possible the Great American Interchange and bringing a nearly complete end to South America's distinctive large marsupial predator and native ungulate faunas. The formation of the Isthmus had major consequences on global temperatures, since warm equatorial ocean currents were cut off and an Atlantic cooling cycle began, with cold Arctic and Antarctic waters dropping temperatures in the now-isolated Atlantic Ocean.
Africa's collision with Europe formed the Mediterranean Sea, cutting off the remnants of the Tethys Ocean. The border between the Miocene and the Pliocene is also the time of the Messinian salinity crisis.
Sea level changes exposed the land-bridge between Alaska and Asia.
Pliocene marine rocks are well exposed in the Mediterranean, India, and China. Elsewhere, they are exposed largely near shores.
Flora.
The change to a cooler, dry, seasonal climate had considerable impacts on Pliocene vegetation, reducing tropical species worldwide. Deciduous forests proliferated, coniferous forests and tundra covered much of the north, and grasslands spread on all continents (except Antarctica). Tropical forests were limited to a tight band around the equator, and in addition to dry savannahs, deserts appeared in Asia and Africa.
Fauna.
Both marine and continental faunas were essentially modern, although continental faunas were a bit more primitive than today. The first recognizable hominins, the australopithecines, appeared in the Pliocene.
The land mass collisions meant great migration and mixing of previously isolated species, such as in the Great American Interchange. Herbivores got bigger, as did specialized predators.
Mammals.
In North America, rodents, large mastodons and gomphotheres, and opossums continued successfully, while hoofed animals (ungulates) declined, with camel, deer and horse all seeing populations recede. Rhinos, three toed horses ("Nannippus"), oreodonts, protoceratids, and chalicotheres became extinct. Borophagine dogs and "Agriotherium" became extinct, but other carnivores including the weasel family diversified, and dogs and fast-running hunting bears did well. Ground sloths, huge glyptodonts, and armadillos came north with the formation of the Isthmus of Panama.
In Eurasia rodents did well, while primate distribution declined. Elephants, gomphotheres and stegodonts were successful in Asia, and hyraxes migrated north from Africa. Horse diversity declined, while tapirs and rhinos did fairly well. Cows and antelopes were successful, and some camel species crossed into Asia from North America. Hyenas and early saber-toothed cats appeared, joining other predators including dogs, bears and weasels.
Africa was dominated by hoofed animals, and primates continued their evolution, with australopithecines (some of the first hominins) appearing in the late Pliocene. Rodents were successful, and elephant populations increased. Cows and antelopes continued diversification and overtaking pigs in numbers of species. Early giraffes appeared, and camels migrated via Asia from North America. Horses and modern rhinos came onto the scene. Bears, dogs and weasels (originally from North America) joined cats, hyenas and civets as the African predators, forcing hyenas to adapt as specialized scavengers.
South America was invaded by North American species for the first time since the Cretaceous, with North American rodents and primates mixing with southern forms.
Litopterns and the notoungulates, South American natives, were mostly wiped out, except for the macrauchenids and toxodonts, which managed to survive. Small weasel-like carnivorous mustelids, coatis and short faced bears migrated from the north. Grazing glyptodonts, browsing giant ground sloths and smaller caviomorph rodents, pampatheres, and armadillos did the opposite, migrating to the north and thriving there.
The marsupials remained the dominant Australian mammals, with herbivore forms including wombats and kangaroos, and the huge diprotodon. Carnivorous marsupials continued hunting in the Pliocene, including dasyurids, the dog-like thylacine and cat-like "Thylacoleo". The first rodents arrived in Australia. The modern platypus, a monotreme, appeared.
Birds.
The predatory South American phorusrhacids were rare in this time; among the last was "Titanis", a large phorusrhacid that migrated to North America and rivaled mammals as top predator. Other birds probably evolved at this time, some modern, some now extinct.
Reptiles and amphibians.
Alligators and crocodiles died out in Europe as the climate cooled. Venomous snake genera continued to increase as more rodents and birds evolved. Rattlesnakes first appeared in the Pliocene. The modern species "Alligator mississippiensis", having evolved in the Miocene, continued into the Pliocene, except with a more northern range; specimens have been found in very late Miocene deposits of Tennessee. Giant tortoises still thrived in North America, with genera like "Hesperotestudo". Madtsoid snakes were still present in Australia. The amphibian order Allocaudata became extinct.
Oceans.
Oceans continued to be relatively warm during the Pliocene, though they continued cooling. The Arctic ice cap formed, drying the climate and increasing cool shallow currents in the North Atlantic. Deep cold currents flowed from the Antarctic.
The formation of the Isthmus of Panama about 3.5 million years ago cut off the final remnant of what was once essentially a circum-equatorial current that had existed since the Cretaceous and the early Cenozoic. This may have contributed to further cooling of the oceans worldwide.
The Pliocene seas were alive with sea cows, seals and sea lions.
Supernovae.
In 2002, Narciso Benítez "et al." calculated that roughly 2 million years ago, around the end of the Pliocene epoch, a group of bright O and B stars called the Scorpius-Centaurus OB association passed within 130 light-years of Earth and that one or more supernova explosions gave rise to a feature known as the Local Bubble. Such a close explosion could have damaged the Earth's ozone layer and caused the extinction of some ocean life (at its peak, a supernova of this size could have the same absolute magnitude as an entire galaxy of 200 billion stars).

</doc>
<doc id="23294" url="https://en.wikipedia.org/wiki?curid=23294" title="Pharaoh">
Pharaoh

Pharaoh (, or ) is the common title of the monarchs of Ancient Egypt from the First Dynasty (c. 3150) until the Macedonian conquest in 305 BCE, although the actual term "Pharaoh" was not used contemporaneously for a ruler until circa 1200 BCE.
Etymology.
The word "pharaoh" ultimately was derived from a compound word represented as ', written with the two biliteral hieroglyphs ' "house" and "" "column". It was used only in larger phrases such as "smr pr-aa" 'Courtier of the High House', with specific reference to the buildings of the court or palace. From the twelfth dynasty onward, the word appears in a wish formula 'Great House, may it live, prosper, and be in health', but again only with reference to the royal palace and not the person.
During the reign of Thutmose III ("circa" 1479–1425 BC) in the New Kingdom, after the foreign rule of the Hyksos during the Second Intermediate Period, "pharaoh" became the form of address for a person who was king.
The earliest instance where "pr-aa" is used specifically to address the ruler is in a letter to Amenhotep IV (Akhenaten), who reigned "circa" 1353–1336 BC, which is addressed to 'Pharaoh, "all life, prosperity, and health!". During the eighteenth dynasty (16th to 14th centuries BC) the title pharaoh was employed as a reverential designation of the ruler. About the late twenty-first dynasty (10th century BC), however, instead of being used alone as before, it began to be added to the other titles before the ruler's name, and from the twenty-fifth dynasty (eighth to seventh centuries BC) it was, at least in ordinary usage, the only epithet prefixed to the royal appellative.
From the nineteenth dynasty onward "pr-ꜥꜣ" on its own was used as regularly as "hm.f", 'Majesty'. The term, therefore, evolved from a word specifically referring to a building to a respectful designation for the ruler, particularly by the twenty-second dynasty and twenty-third dynasty.
For instance, the first dated appearance of the title pharaoh being attached to a ruler's name occurs in Year 17 of Siamun on a fragment from the Karnak Priestly Annals. Here, an induction of an individual to the Amun priesthood is dated specifically to the reign of Pharaoh Siamun. This new practice was continued under his successor Psusennes II and the twenty-second dynasty kings. For instance, the Large Dakhla stela is specifically dated to Year 5 of king 'Pharaoh Shoshenk, beloved of Amun' whom all Egyptologists concur was Shoshenq I—the founder of the Twenty-second dynasty—including Alan Gardiner in his original 1933 publication of this stela. Shoshenq I was the second successor of Siamun. Meanwhile, the old custom of referring to the sovereign simply as "pr-aa" continued in traditional Egyptian narratives.
By this time, the Late Egyptian word is reconstructed to have been pronounced whence comes Ancient Greek φαραώ "pharaō" and then Late Latin "pharaō". From the latter, English obtained the word "Pharaoh". In the Bible, the title also occurs as פרעה []. Over time, evolved into Sahidic Coptic "prro" and then "rro" (by mistaking "p-" as the definite article prefix "the" from Ancient Egyptian "pꜣ").
Regalia.
Scepters and staves.
Scepters and staves were a general sign of authority in Ancient Egypt. One of the earliest royal scepters was discovered in the tomb of Khasekhemwy in Abydos. Kings were also known to carry a staff, and Pharaoh Anedjib is shown on stone vessels carrying a so-called "mks"-staff. The scepter with the longest history seems to be the "heqa"-scepter, sometimes described as the shepherd's crook. The earliest examples of this piece of regalia dates to pre-dynastic times. A scepter was found in a tomb at Abydos that dates to the late Naqada period.
Another scepter associated with the king is the "was"-scepter. This is a long staff mounted with an animal head. The earliest known depictions of the "was"-scepter date to the first dynasty. The "was"-scepter is shown in the hands of both kings and deities.
The flail later was closely related to the "heqa"-scepter (the crook and flail), but in early representations the king was also depicted solely with the flail, as shown in a late pre-dynastic knife handle which is now in the Metropolitan museum, and on the Narmer Macehead.
The Uraeus.
The earliest evidence we have of the use of the Uraeus—a rearing cobra—is from the reign of Den from the first dynasty. The cobra supposedly protected the pharaoh by spitting fire at its enemies.
Crowns and headdresses.
The red crown of Lower Egypt, the Deshret crown, dates back to pre-dynastic times. A red crown has been found on a pottery shard from Naqada, and later, king Narmer is shown wearing the red crown on both the Narmer macehead and the Narmer palette.
The white crown of Upper Egypt, the Hedjet crown, is shown on the Qustul incense burner which dates to the pre-dynastic period. Later, King Scorpion was depicted wearing the white crown, as was Narmer.
The combination of red and white crown into the double crown, or Pschent crown, is first documented in the middle of the first dynasty. The earliest depiction may date to the reign of Djet, and is otherwise surely attested during the reign of Den.
Khat and nemes headdresses.
The "khat" headdress consists of a kind of "kerchief" whose end is tied similarly to a ponytail. The earliest depictions of the "khat" headdress comes from the reign of Den, but is not found again until the reign of Djoser.
The Nemes headdress dates from the time of Djoser. The statue from his Serdab in Saqqara shows the king wearing the "nemes" headdress.
Physical evidence.
Egyptologist Bob Brier has noted that despite its widespread depiction in royal portraits, no ancient Egyptian crown ever has been discovered. Tutankhamun's tomb, discovered largely intact, did contain such regalia as his crook and flail, but no crown was found, however, among the funerary equipment. Diadems have been discovered.
It is presumed that crowns would have been believed to have magical properties. Brier's speculation is that crowns were religious or state items, so a dead pharaoh likely could not retain a crown as a personal possession. The crowns may have been passed along to the successor.
Titles.
During the early dynastic period kings had as many as three titles. The "Horus name" is the oldest and dates to the late pre-dynastic period. The "Nesw Bity" name was added during the first dynasty. The "Nebty" name was first introduced toward the end of the first dynasty. The Golden falcon ("bik-nbw") name is not well understood. The prenomen and nomen were introduced later and are traditionally enclosed in a cartouche. By the Middle Kingdom, the official titulary of the ruler consisted of five names; Horus, nebty, golden Horus, nomen, and prenomen for some rulers, only one or two of them may be known.
"Nesu Bity" name.
The "Nesu Bity" name, also known as Prenomen, was one of the new developments from the reign of Den. The name would follow the glyphs for the "Sedge and the Bee". The title is usually translated as king of Upper and Lower Egypt. The "nsw bity" name may have been the birth name of the king. It was often the name by which kings were recorded in the later annals and king lists.
Horus name.
The Horus name was adopted by the king, when taking the throne. The name was written within a square frame representing the palace, named a serekh. The earliest known example of a serekh dates to the reign of king Ka, before the first dynasty. The Horus name of several early kings expresses a relationship with Horus. Aha refers to "Horus the fighter", Djer refers to "Horus the strong", etc. Later kings express ideals of kingship in their Horus names. Khasekhemwy refers to "Horus: the two powers are at peace", while Nebra refers to "Horus, Lord of the Sun".
"Nebty" name.
The earliest example of a "nebty" name comes from the reign of king Aha from the first dynasty. The title links the king with the goddesses of Upper and Lower Egypt Nekhbet and Wadjet. The title is preceded by the vulture (Nekhbet) and the cobra (Wadjet) standing on a basket (the neb sign).
Golden Horus.
The Golden Horus or Golden Falcon name was preceded by a falcon on a gold or "nbw" sign. The title may have represented the divine status of the king. The Horus associated with gold may be referring to the idea that the bodies of the deities were made of gold and the pyramids and obelisks are representations of (golden) sun-rays. The gold sign may also be a reference to Nubt, the city of Set. This would suggest that the iconography represents Horus conquering Set.
Nomen and prenomen.
The prenomen and nomen were contained in a cartouche. The prenomen often followed the King of Upper and Lower Egypt ("nsw bity") or Lord of the Two Lands ("nebtawy") title. The prenomen often incorporated the name of Re. The nomen often followed the title Son of Re ("sa-ra") or the title Lord of Appearances ("neb-kha").

</doc>
<doc id="23295" url="https://en.wikipedia.org/wiki?curid=23295" title="Printing press">
Printing press

A printing press is a device for applying pressure to an inked surface resting upon a print medium (such as paper or cloth), thereby transferring the ink. Typically used for texts, the invention and spread of the printing press are widely regarded as the most influential events in the second millennium revolutionizing the way people conceive and describe the world they live in, and ushering in the period of modernity.
The printing press was invented in the Holy Roman Empire by the German Johannes Gutenberg around 1440, based on existing screw presses. Gutenberg, a goldsmith by profession, developed a complete printing system, which perfected the printing process through all of its stages by adapting existing technologies to the printing purposes, as well as making groundbreaking inventions of his own. His newly devised hand mould made for the first time possible the precise and rapid creation of metal movable type in large quantities, a key element in the profitability of the whole printing enterprise. 
The mechanization of bookmaking led to the first mass production of books in history in assembly line-style. A single Renaissance printing press could produce 3,600 pages per workday, compared to forty by typographic hand-printing and a few by hand-copying. Books of bestselling authors like Luther or Erasmus were sold by the hundreds of thousands in their lifetime.
From a single point of origin, Mainz, Germany, printing spread within several decades to over two hundred cities in a dozen European countries. By 1500, printing presses in operation throughout Western Europe had already produced more than twenty million volumes. In the 16th century, with presses spreading further afield, their output rose tenfold to an estimated 150 to 200 million copies. The operation of a press became so synonymous with the enterprise of printing that it lent its name to an entire new branch of media, the press. As early as 1620, the English statesman and philosopher Francis Bacon could write that typographical printing has "changed the whole face and state of things throughout the world".
From its beginnings, printing was practiced also as a true art form, setting a high aesthetic and artistic standard, such as in the famous 42-line Bible. Today, incunables, that is books printed before 1501, are among the most prized possessions of modern libraries.
The unprecedented impact of Gutenberg-style printing on the long-term development of modern European and then world history is difficult to capture in its entirety. Attempts at analysing its manifold effects include the notion of a proper Printing Revolution and the creation of the Gutenberg Galaxy. The ready availability and affordability of the printed word to the general public boosted the democratization of knowledge and laid the material basis for the modern knowledge-based economy.
In Renaissance Europe, the arrival of mechanical movable type printing introduced the era of mass communication which permanently altered the structure of society: The relatively unrestricted circulation of information and (revolutionary) ideas transcended borders, captured the masses in the Reformation and threatened the power of political and religious authorities; the sharp increase in literacy broke the monopoly of the literate elite on education and learning and bolstered the emerging middle class. Across Europe, the increasing cultural self-awareness of its peoples led to the rise of proto-nationalism, accelerated by the flowering of the European vernacular languages to the detriment of Latin's status as lingua franca.
In the 19th century, the replacement of the hand-operated Gutenberg-style press by steam-powered rotary presses allowed printing on an industrial scale, while Western-style printing was adopted all over the world, becoming practically the sole medium for modern bulk printing.
History.
Economic conditions and intellectual climate.
The rapid economic and socio-cultural development of late medieval society in Europe created favorable intellectual and technological conditions for Gutenberg's invention: the entrepreneurial spirit of emerging capitalism increasingly made its impact on medieval modes of production, fostering economic thinking and improving the efficiency of traditional work-processes. The sharp rise of medieval learning and literacy amongst the middle class led to an increased demand for books which the time-consuming hand-copying method fell far short of accommodating.
Technological factors.
At the same time, a number of medieval products and technological processes had reached a level of maturity which allowed their potential use for printing purposes. Gutenberg took up these far-flung strands, combined them into one complete and functioning system, and perfected the printing process through all its stages by adding a number of inventions and innovations of his own:
The screw press which allowed direct pressure to be applied on flat-plane was already of great antiquity in Gutenberg's time and was used for a wide range of tasks. Introduced in the 1st century AD by the Romans, it was commonly employed in agricultural production for pressing wine grapes and (olive) oil fruit, both of which formed an integral part of the mediterranean and medieval diet. The device was also used from very early on in urban contexts as a cloth press for printing patterns. Gutenberg may have also been inspired by the paper presses which had spread through the German lands since the late 14th century and which worked on the same mechanical principles.
Gutenberg adopted the basic design, thereby mechanizing the printing process. Printing, however, put a demand on the machine quite different from pressing. Gutenberg adapted the construction so that the pressing power exerted by the platen on the paper was now applied both evenly and with the required sudden elasticity. To speed up the printing process, he introduced a movable undertable with a plane surface on which the sheets could be swiftly changed.
The concept of movable type was not new in the 15th century; movable type printing had been invented in China during the Song dynasty, and was later used in Korea during the Goryeo Dynasty, where metal movable-type printing technology was developed in 1234. In Europe, sporadic evidence that the typographical principle, the idea of creating a text by reusing individual characters, was well understood and employed in pre-Gutenberg Europe had been cropping up since the 12th century and possibly before. The known examples range from Germany (Prüfening inscription) to England (letter tiles) to Italy. However, the various techniques employed (imprinting, punching and assembling individual letters) did not have the refinement and efficiency needed to become widely accepted.
Gutenberg greatly improved the process by treating typesetting and printing as two separate work steps. A goldsmith by profession, he created his type pieces from a lead-based alloy which suited printing purposes so well that it is still used today. The mass production of metal letters was achieved by his key invention of a special hand mould, the matrix. The Latin alphabet proved to be an enormous advantage in the process because, in contrast to logographic writing systems, it allowed the type-setter to represent any text with a theoretical minimum of only around two dozen different letters.
Another factor conducive to printing arose from the book existing in the format of the codex, which had originated in the Roman period. Considered the most important advance in the history of the book prior to printing itself, the codex had completely replaced the ancient scroll at the onset of the Middle Ages (500 AD). The codex holds considerable practical advantages over the scroll format; it is more convenient to read (by turning pages), is more compact, less costly, and, in particular, unlike the scroll, both recto and verso could be used for writing − and printing.
A fourth development was the early success of medieval papermakers at mechanizing paper manufacture. The introduction of water-powered paper mills, the first certain evidence of which dates to 1282, allowed for a massive expansion of production and replaced the laborious handcraft characteristic of both Chinese and Muslim papermaking. Papermaking centres began to multiply in the late 13th century in Italy, reducing the price of paper to one sixth of parchment and then falling further; papermaking centers reached Germany a century later.
Despite this it appears that the final breakthrough of paper depended just as much on the rapid spread of movable-type printing. It is notable that codices of parchment, which in terms of quality is superior to any other writing material, still had a substantial share in Gutenberg's edition of the 42-line Bible. After much experimentation, Gutenberg managed to overcome the difficulties which traditional water-based inks caused by soaking the paper, and found the formula for an oil-based ink suitable for high-quality printing with metal type.
Function and approach.
A printing press, in its classical form, is a standing mechanism, ranging from long, wide, and tall. Type, or small metal letters that have a raised letter on each end, is arranged into pages and placed in a frame to make a forme, which itself is placed onto a flat stone, 'bed,' or 'coffin.' The text is inked using two pads mounted on handles. These pads were stuffed with sheep's wool and were inked. This ink was then applied to the text evenly. One damp piece of paper was then taken from a heap of paper and placed on the tympan. The paper was damp as this lets the type 'bite' into the paper better. Small pins hold the paper in place. The paper is now held between a frisket and tympan (two frames covered with paper or parchment).
These are folded down, so that the paper lies on the surface of the inked type. The bed is rolled under the platen, using a windlass mechanism. A small rotating handle is used called the 'rounce' to do this, and the impression is made with a screw that transmits pressure through the platen. To turn the screw the long handle attached to it is turned. This is known as the bar or 'Devil's Tail.' Then the screw is reversed, the windlass turned again to move the bed back to its original position, the tympan and frisket raised and opened, and the printed sheet removed. Such presses were always worked by hand. After around 1800, iron presses were developed, some of which could be operated by steam power.
Gutenberg's press.
Johannes Gutenberg's work on the printing press began in approximately 1436 when he partnered with Andreas Dritzehn—a man who had previously instructed in gem-cutting—and Andreas Heilmann, owner of a paper mill. However, it was not until a 1439 lawsuit against Gutenberg that an official record existed; witnesses' testimony discussed Gutenberg's types, an inventory of metals (including lead), and his type molds.
Having previously worked as a professional goldsmith, Gutenberg made skillful use of the knowledge of metals he had learned as a craftsman. He was the first to make type from an alloy of lead, tin, and antimony, which was critical for producing durable type that produced high-quality printed books and proved to be much better suited for printing than all other known materials. To create these lead types, Gutenberg used what is considered one of his most ingenious inventions, a special matrix enabling the quick and precise molding of new type blocks from a uniform template. His type case is estimated to have contained around 290 separate letter boxes, most of which were required for special characters, ligatures, punctuation marks, and so forth.
Gutenberg is also credited with the introduction of an oil-based ink which was more durable than the previously used water-based inks. As printing material he used both paper and vellum (high-quality parchment). In the Gutenberg Bible, Gutenberg made a trial of coloured printing for a few of the page headings, present only in some copies. A later work, the Mainz Psalter of 1453, presumably designed by Gutenberg but published under the imprint of his successors Johann Fust and Peter Schöffer, had elaborate red and blue printed initials.
The new era in print ushered in by the Internet is a distant mirror to Gutenberg's work which similarly revolutionized the printing process.
The Printing Revolution.
The Printing Revolution occurred when the spread of the printing press facilitated the wide circulation of information and ideas, acting as an "agent of change" through the societies that it reached. (Eisenstein (1980))
Mass production and spread of printed books.
The invention of mechanical movable type printing led to a huge increase of printing activities across Europe within only a few decades. From a single print shop in Mainz, Germany, printing had spread to no less than around 270 cities in Central, Western and Eastern Europe by the end of the 15th century. As early as 1480, there were printers active in 110 different places in Germany, Italy, France, Spain, the Netherlands, Belgium, Switzerland, England, Bohemia and Poland. From that time on, it is assumed that "the printed book was in universal use in Europe". 
In Italy, a center of early printing, print shops had been established in 77 cities and towns by 1500. At the end of the following century, 151 locations in Italy had seen at one time printing activities, with a total of nearly three thousand printers known to be active. Despite this proliferation, printing centres soon emerged; thus, one third of the Italian printers published in Venice. 
By 1500, the printing presses in operation throughout Western Europe had already produced more than twenty million copies. In the following century, their output rose tenfold to an estimated 150 to 200 million copies. 
European printing presses of around 1600 were capable of producing 3,600 impressions per workday. By comparison, movable type printing in East Asia, which did not know presses and was solely done by manually rubbing the back of the paper to the page, did not exceed an output of forty pages per day. 
The vast printing capacities meant that individual authors could now become true bestsellers: Of Erasmus's work, at least 750,000 copies were sold during his lifetime alone (1469–1536). In the early days of the Reformation, the revolutionary potential of bulk printing took princes and papacy alike by surprise. In the period from 1518 to 1524, the publication of books in Germany alone skyrocketed sevenfold; between 1518 and 1520, Luther's tracts were distributed in 300,000 printed copies.
The rapidity of typographical text production, as well as the sharp fall in unit costs, led to the issuing of the first newspapers (see "Relation") which opened up an entirely new field for conveying up-to-date information to the public.
A lasting legacy are the prized incunable, surviving pre-16th century print works which are collected by many of the most prestigious libraries in Europe and North America.
Circulation of information and ideas.
The printing press was also a factor in the establishment of a community of scientists who could easily communicate their discoveries through the establishment of widely disseminated scholarly journals, helping to bring on the scientific revolution. Because of the printing press, authorship became more meaningful and profitable. It was suddenly important who had said or written what, and what the precise formulation and time of composition was. This allowed the exact citing of references, producing the rule, "One Author, one work (title), one piece of information" (Giesecke, 1989; 325). Before, the author was less important, since a copy of Aristotle made in Paris would not be exactly identical to one made in Bologna. For many works prior to the printing press, the name of the author has been entirely lost.
Because the printing process ensured that the same information fell on the same pages, page numbering, tables of contents, and indices became common, though they previously had not been unknown. The process of reading also changed, gradually moving over several centuries from oral readings to silent, private reading. The wider availability of printed materials also led to a dramatic rise in the adult literacy rate throughout Europe.
The printing press was an important step towards the democratization of knowledge. Within 50 or 60 years of the invention of the printing press, the entire classical canon had been reprinted and widely promulgated throughout Europe (Eisenstein, 1969; 52). Now that more people had access to knowledge both new and old, more people could discuss these works. Furthermore, now that book production was a more commercial enterprise, the first copyright laws were passed to protect what we now would call intellectual property rights. On the other hand, the printing press was criticized for allowing the dissemination of information which may have been incorrect.
A second outgrowth of this popularization of knowledge was the decline of Latin as the language of most published works, to be replaced by the vernacular language of each area, increasing the variety of published works. The printed word also helped to unify and standardize the spelling and syntax of these vernaculars, in effect 'decreasing' their variability. This rise in importance of national languages as opposed to pan-European Latin is cited as one of the causes of the rise of nationalism in Europe.
Book printing as art form.
For years, book printing was considered a true art form. Typesetting, or the placement of the characters on the page, including the use of ligatures, was passed down from master to apprentice. In Germany, the art of typesetting was termed the "black art", in allusion to the ink-covered printers. It has largely been replaced by computer typesetting programs, which make it easy to get similar results more quickly and with less physical labor. Some practitioners continue to print books the way Gutenberg did. For example, there is a yearly convention of traditional book printers in Mainz, Germany.
Some theorists, such as McLuhan, Eisenstein, Kittler, and Giesecke, see an "alphabetic monopoly" as having developed from printing, removing the role of the image from society. Other authors stress that printed works themselves are a visual medium. Certainly, modern developments in printing have revitalized the role of illustrations.
Industrial printing presses.
At the dawn of the Industrial Revolution, the mechanics of the hand-operated Gutenberg-style press were still essentially unchanged, although new materials in its construction, amongst other innovations, had gradually improved its printing efficiency. By 1800, Lord Stanhope had built a press completely from cast iron which reduced the force required by 90%, while doubling the size of the printed area. With a capacity of 480 pages per hour, it doubled the output of the old style press. Nonetheless, the limitations inherent to the traditional method of printing became obvious.
Two ideas altered the design of the printing press radically: First, the use of steam power for running the machinery, and second the replacement of the printing flatbed with the rotary motion of cylinders. Both elements were for the first time successfully implemented by the German printer Friedrich Koenig in a series of press designs devised between 1802 and 1818. Having moved to London in 1804, Koenig soon met Thomas Bensley and secured financial support for his project in 1807. Patented in 1810, Koenig had designed a steam press "much like a hand press connected to a steam engine." The first production trial of this model occurred in April 1811. He produced his machine with assistance from German engineer Andreas Friedrich Bauer.
Koenig and Bauer sold two of their first models to "The Times" in London in 1814, capable of 1,100 impressions per hour. The first edition so printed was on 28 November 1814. They went on to perfect the early model so that it could print on both sides of a sheet at once. This began the long process of making newspapers available to a mass audience (which in turn helped spread literacy), and from the 1820s changed the nature of book production, forcing a greater standardization in titles and other metadata. Their company Koenig & Bauer AG is still one of the world's largest manufacturers of printing presses today.
The steam powered rotary printing press, invented in 1843 in the United States by Richard M. Hoe, allowed millions of copies of a page in a single day. Mass production of printed works flourished after the transition to rolled paper, as continuous feed allowed the presses to run at a much faster pace.
Also, in the middle of the 19th century, there was a separate development of jobbing presses, small presses capable of printing small-format pieces such as billheads, letterheads, business cards, and envelopes. Jobbing presses were capable of quick set-up (average setup time for a small job was under 15 minutes) and quick production (even on treadle-powered jobbing presses it was considered normal to get 1,000 impressions per hour with one pressman, with speeds of 1,500 iph often attained on simple envelope work). Job printing emerged as a reasonably cost-effective duplicating solution for commerce at this time.
By the late 1930s or early 1940s, printing presses had increased substantially in efficiency: a model by Platen Printing Press was capable of performing 2,500 to 3,000 impressions per hour.
Printing capacity.
The table lists the maximum number of pages which various press designs could print "per hour".
References.
On the effects of the printing press
Technology of printing

</doc>
<doc id="23297" url="https://en.wikipedia.org/wiki?curid=23297" title="Pat Rafter">
Pat Rafter

Patrick Michael "Pat" Rafter (born 28 December 1972) is an Australian former World No. 1 tennis player. He twice won the men's singles title at the US Open and was twice the runner-up at Wimbledon. He was known for his natural serve-and-volley style of play. He became the first man in the Open Era to win Montreal/Toronto, Cincinnati and the US Open in the same year (1998); this achievement has been dubbed the American Summer Slam.
Tennis career.
1990s.
Rafter turned professional in 1991 and won his first career singles title in 1994 in Manchester. Prior to 1997, this was the only ATP singles title he had won.
Rafter's breakthrough came in 1997. At that year's French Open he reached the semifinals, falling in four sets to Sergi Bruguera. Then he surprised many by winning the US Open, defeating Greg Rusedski in a four-set final and Andre Agassi and Michael Chang, among others, in earlier rounds; he was the first non-American to win the title since Stefan Edberg in 1992. This was his first Grand Slam title, and catapulted him ahead of Chang to finish the year ranked #2 in the world, behind only Pete Sampras. The unexpected nature of his U.S. Open title led many, including Hall-of-famer and four-time U.S. Open champion John McEnroe to criticise Rafter as a "one-slam wonder".
1998 was a particularly strong year for Rafter, who won the Canadian Open and Cincinnati in a row (only Andre Agassi, in 1995, Andy Roddick, in 2003, and Rafael Nadal, in 2013 also have won both these tournaments in the same year). Rafter defeated ninth ranked Richard Krajicek in the Toronto final and second ranked Pete Sampras in the Cincinnati final. When asked about the difference between himself and Rafter following titles, Sampras stated "10 grand slams", and that a player has to come back and win a Grand Slam again in order to be considered great.
Following his title at Cincinnati, Rafter won a US Open warm-up tournament in Long Island, New York. Entering the US Open as the defending champion, he reached the final again, defeating Sampras in a five-set semifinal. Rafter pointedly took issue with Sampras' refusal to show him respect in defeat: "That is what really upsets me about him", Rafter said, "and the reason why I try to piss him off as much as I can."
Rafter then defended his U.S. Open title by defeating fellow Australian player Mark Philippoussis in four sets, committing only five unforced errors throughout the match. When asked about Sampras' earlier comments about having to win another Grand Slam in order to be considered great, Rafter replied: "Maybe you can ask him that question, if he thinks that now. For me, I won another Slam, and it hasn't sunk in yet. It's very, very exciting for me, especially to repeat it". Altogether, Rafter won six tournaments in 1998, finishing the year #4 in the world.
At the 1999 French Open, Rafter drew future World No. 1 and 17-time Grand Slam champion Roger Federer in the first round, making him the first ever opponent of Federer in the main draw of a Grand Slam tournament. Rafter defeated him in four sets, after losing the first set. Rafter then reached the Wimbledon semifinals for the first time in 1999, where he lost in straight sets to Agassi, the first of three consecutive years that the two met in the Wimbledon semifinals. July 1999 saw Rafter holding the world No. 1 men's singles ranking for one week, making him the shortest-reigning world No. 1 in ATP tour history. As the two-time defending US Open champion, Rafter lost in the first round of the tournament, retiring in the fifth set against Cédric Pioline after succumbing to shoulder tendinitis. Rafter's shoulder injury wound up being serious enough to necessitate surgery. He won the Australian Open men's doubles title in 1999 (partnering Jonas Björkman), making him one of few players in the modern era to win both a singles and doubles Grand Slam title during their career (fellow countryman Lleyton Hewitt would later achieve this feat in 2001). He and Björkman also won doubles titles at the ATP Masters Series events in Canada (1999) and Indian Wells (1998).
2000s.
His ranking had fallen to No. 21 by the time he reached the Wimbledon final in July 2000. In the semifinals, Rafter defeated Agassi 7–5, 4–6, 7–5, 4–6, 6–3. The match was hailed as a classic, particularly because of their contrasting playing styles, with Agassi playing primarily from the baseline and Rafter attacking the net. Rafter faced Sampras in the final, who was gunning for a record-breaking seventh Wimbledon title overall (and seven in the past eight years). While Rafter made a strong start to the match and took the first set, after the match he would claim that he had "choked" part way through the second set, and was then not able to get back into his game. Sampras won in four sets.
In 2001, Rafter reached the semifinals of the Australian Open, but despite holding a two sets to one lead and having the support of the home crowd, Rafter lost the match to Agassi in five sets. Later in the year, Rafter again reached the Wimbledon final. For the third straight year, he faced Agassi in the semifinals and won in yet another five-setter, 2–6, 6–3, 3–6, 6–2, 8–6. Much like the previous year's semifinal, this match also received praise for the quality of play that the two men displayed. In the final, he squared off against Goran Ivanišević, who had reached the Wimbledon final three times before but had slid down the rankings to World No. 125 following injury problems. In a five-set struggle that lasted just over three hours, Ivanišević prevailed.
Rafter was on the Australian Davis Cup Team that lost in the final in 2000 (to Spain) and 2001 (to France). He was unable to play in the 1999 Davis Cup final – where Australia beat France to win the cup – because of injury (though he won important matches in the earlier rounds to help the team qualify).
Rafter was on the Australian teams that won the World Team Cup in 1999 and 2001.
Rafter is one of only two tennis players, along with Sergi Bruguera, to have always won against Roger Federer, having defeated him thrice. He is also the only player who has a winning record with the 17 time Grand Slam winner on all the three main tennis surfaces: hard, clay and grass.
He retired from the professional tour at the end of 2002 after winning a total of 11 singles titles and 10 doubles titles. He returns to the courts annually to play World Team Tennis for the Philadelphia Freedoms.
Rafter did return at the beginning of the 2004 season to play doubles at two tournaments only; the 2004 Australian Open and the 2004 AAPT Championships (in Adelaide). However, he lost in round one of both events, playing alongside Joshua Eagle.
On Australia Day 2008, Pat Rafter was inducted into the Australian Open Hall of Fame.
2010s.
On January 12, 2014 Rafter, aged 41, announced that he would be partnering current Australian number one Lleyton Hewitt in the doubles draw of the 2014 Australian Open. The comeback, however, was short-lived as the pair went down in straight sets to Eric Butorac and Raven Klaasen in the first round.
ATP Champions Tour.
At the 2009 AEGON Masters Tennis, Rafter lost his opening round robin match against the 1987 Wimbledon Champion Pat Cash 2–6, 6–2, 10–6. In a much anticipated match and reply of the 2001 Wimbledon final, Rafter faced Goran Ivanišević. Rafter won the match when Ivanisevic retired while serving for the opening set, 3–5. Despite his performance, the retirement was enough to push Rafter into the final against Stefan Edberg. In what is described as a spell-binding serve-and-volley showdown, Rafter won the match 6–7, 6–4, 11–9. This represented the first time that Rafter was able to defeat Edberg.
Equipment.
Rafter, while professional, used Prince Sports racquet and Reebok clothes. Since the beginning of 2011, he began using Dunlop Sport racquet, continuing with Reebok clothes.
Personal life.
Rafter was born in Mount Isa, Queensland, and is third-youngest in a family of nine children. He began playing tennis at the age of five with his father and three older brothers.
In April 2004, Rafter married his girlfriend Lara Feltham (with whom he had a son, Joshua) at a resort in Fiji. Their daughter, India, was born in May 2005.
Rafter donated half of the prize money from his 1997 and 1998 US Open wins to the Starlight Children's Foundation; he attempted to do so anonymously in 1997 but was unsuccessful. He has created his own charity organisation that raises funds for children's causes each year. Rafter also supports animal rights and the work of animal liberation groups such as makeitpossible.com.
He has occasionally played reserve grade Australian rules in the Sydney AFL for the North Shore Bombers.
Since his retirement, Rafter has gone on to become an underwear model for Bonds, a brand ambassador for the Mantra Group of hotels and a successful businessman.
In October 2010 he was announced as Australia's Davis Cup captain. Rafter stood down as Australia's Davis Cup captain on 29 January 2015. He was succeeded by Wally Masur.
Honours.
In honour of Patrick Rafter the 5,500 seat centre court of the Queensland Tennis Centre in Brisbane, Australia was named "Pat Rafter Arena". In 2002, he won the Australian of the Year award. This created some controversy as he had spent much of his career residing in Bermuda for tax purposes. He was elected to the International Tennis Hall of Fame and inducted into the Sport Australia Hall of Fame in 2006.
Performance timeline.
Singles.
LQ = lost in qualifying draw

</doc>
<doc id="23298" url="https://en.wikipedia.org/wiki?curid=23298" title="Proportional representation">
Proportional representation

Proportional representation ("PR") characterizes electoral systems by which divisions in an electorate are reflected proportionately in the elected body. If 30% of the electorate support a particular political party, then roughly 30% of seats will be won by that party. The essence of such systems is that all votes contribute to the result: not just a plurality, or a bare majority, of them. Proportional representation requires the use of multiple-member voting districts (also called super-districts); it is not possible using single-member districts alone.
There are two PR voting systems:(Direct Party and Representative Voting (DPR Voting)), party list PR and the single transferable vote (STV). Mixed member proportional representation (MMP), also known as the Additional Member System, is a hybrid Mixed Electoral System that uses party list PR as its proportional component. MMP has the potential to be proportional or semi-proportional depending on a number of factors such as the ratio of FPTP seats to PR seats, the existence or nonexistence of compensatory seats to make up for overhang seats, and election thresholds. 
With party list PR, political parties define candidate lists and voters vote for a list. The relative vote for each list determines how many candidates from each list are actually elected. Lists can be "closed" or "open"; open lists allow voters to indicate individual candidate preferences and vote for independent candidates. Voting districts can be small (as low as 2 in Chile) or as large as a province or an entire nation.
The single transferable vote uses small districts, with voters ranking individual candidates in order of preference. During the count, as candidates are elected or eliminated, surplus or discarded votes that would otherwise be wasted are transferred to other candidates according to the preferences. STV enables voters to vote across party lines and to elect independent candidates.
Mixed member proportional representation (MMP), also called the additional member system (AMS), is a hybrid, two-tier, mixed member system combining a non-proportional Plurality/Majoritarian election and a compensatory regional or national party list PR one. Voters have two votes, one for their single-member district and one for the party list, the party list vote determining the balance of the parties in the elected body. Biproportional apportionment, first used in Zurich in 2006, is a two-tier method for adjusting an election's result to achieve overall proportionality. A further system produces near perfect proportionality by dispensing with voting altogether: the random selection of representatives from the populace, known as sortition. It was used in ancient Athens and the Venetian Republic and is still used today in the summons to jury duty. In recent years, it has received increasing academic attention.
Some form of proportional representation is used for national lower house elections in 94 countries, party list PR being the most widely used (85). MMP is used in seven lower houses, and STV, despite long being advocated by political scientists, is used in only two: Ireland, since independence in 1922, and Malta, since 1921; in the United States, party bosses were generally opposed to it because it transferred more power to the electorate when selecting independent candidates to put forward.
As with all electoral systems, there are overlapping and contentious claims in terms of its advantages and disadvantages.
Advantages and disadvantages.
The case for proportional representation was made by John Stuart Mill in his 1861 essay "Considerations on Representative Government":
Most academic political theorists agree with Mill, that in a representative democracy the representatives should represent all segments of society.
Fairness.
PR tries to resolve the unfairness of Plurality/Majoritarian systems, where the largest parties receive an "unfair" "seat bonus" and smaller parties are disadvantaged and have difficulty winning any representation at all (Duverger's law). The established parties in UK elections can win formal control of the parliament with as little as 35% of votes (UK general election, 2005). In Canada, majority governments are regularly formed by parties with the support of under 40% of votes cast. Coupled with turnout levels in the electorate of less than 60%, this can lead to a party forming a majority government by convincing as few as one quarter of the electorate to vote for it. In the 2005 UK general election, for example, the Labour Party under Tony Blair won a comfortable parliamentary majority with the votes of only 21.6% of the total electorate. But PR systems with a high electoral threshold, or other features that reduce proportionality, are not necessarily much fairer: in the Turkish general election, 2002, using an open list system with a 10% threshold, 46% of votes were wasted.
Plurality/Majoritarian systems can also disproportionately benefit regional parties that can win districts where they have a strong following, while other parties with national support but no strongholds, like the Greens, win few or no seats. An example is the Bloc Québécois in Canada that won 52 seats in the 1993 federal election, all in Quebec, on 18% of the national vote, while the Progressive Conservatives collapsed to two seats on 16% spread nationally. Similarly, in the 2015 UK General Election, the Scottish National Party gained 56 seats, all in Scotland, with a 4.7% share of the vote while the UK Independence Party, with 12.6% of the national vote, gained only a single seat.
Election of minor parties.
The use of multiple-member districts enables a greater variety of candidates to be elected. The more representatives per district and the lower the minimum threshold of votes required for election the more minor parties can gain representation. In emerging democracies, inclusion of minorities in the legislature can be essential for social stability and to consolidate the democratic process.
Critics, on the other hand, claim this can give extreme parties a foothold in parliament, sometimes cited as a cause for the collapse of the Weimar government. With very low thresholds, very small parties can act as "king-makers", holding larger parties to ransom during coalition discussions. The example of Israel is often quoted, but these problems can be limited, as in the modern German Bundestag, by the introduction of higher threshold limits for a party to gain parliamentary representation.
Another criticism is that the dominant parties in plurality systems, often looked on as "coalitions" or as "broad churches", can fragment under PR as the election of candidates from smaller groups becomes possible. Israel, again, and Brazil and Italy are examples. However, research shows, in general, there is only a marginal increase in the number of parties in parliament.
Open list systems and STV, the only PR system which does not require political parties, enable independent candidates to be elected. In Ireland, on average, about six independent candidates have been elected each parliament.
Coalitions.
The election of smaller parties gives rise to the principal objection to PR systems, that they almost always result in coalition governments.
Supporters of PR see coalitions as an advantage, forcing compromise between parties to form a coalition at the centre of the political spectrum, and so leading to continuity and stability. Opponents counter that with many policies compromise is not possible (for example funding a new stealth bomber, or leaving the EU). Neither can many policies be easily positioned on the left-right spectrum (for example, the environment). So policies are horse-traded during coalition formation, with the consequence that voters have no way of knowing which policies will be pursued by the government they elect; voters have less influence on governments. Also, coalitions do not necessarily form at the centre, and small parties can have excessive influence, supplying a coalition with a majority only on condition that a policy or policies favoured by few voters is adopted. Most importantly, the ability of voters to vote a party in disfavour out of power is curtailed.
All these disadvantages, the PR opponents contend, are avoided by two-party plurality systems. Coalitions are rare; the two dominant parties necessarily compete at the centre for votes, so that governments are more reliably moderate; the strong opposition necessary for proper scrutiny of government is assured; and governments remain sensitive to public sentiment because they can be, and are, regularly voted out of power. However, the US experience shows that this is not necessarily so, and that a two-party system can result in a "drift to extremes", hollowing out the centre, or, at least, in one party drifting to an extreme.
Nevertheless, on average, compared to countries using plurality systems, governments elected with PR accord more closely with the median voter and the citizens are more content with democracy.
Voter participation.
Plurality systems usually result in single-party government because relatively few votes in the most finely balanced districts, the "swing seats", can transfer sufficient seats to the opposition to swing the election. More partisan districts remain invulnerable to swings of political mood. In the UK, for example, about half the constituencies have always elected the same party since 1945; in the 2012 US House elections 45 districts (10% of all districts) were uncontested by one of the two dominant parties. Voters who know their preferred candidate cannot win have little incentive to vote, and if they do their vote has no effect, it is "wasted".
With PR, there are no "swing seats", most votes contribute to the election of a candidate so parties need to campaign in all districts, not just those where their support is strongest or where they perceive most advantage. This fact in turn encourages parties to be more responsive to voters, producing a more "balanced" ticket by nominating more women and minority candidates. On average about 8% more women are elected.
Since most votes count, there are fewer "wasted votes", so voters, aware that their vote can make a difference, are more likely to make the effort to vote, and less likely to vote tactically. Compared to countries with plurality voting systems, voter turnout improves and the population is more involved in the political process.
Gerrymandering.
To ensure approximately equal representation, plurality systems are dependent on the drawing of arbitrary boundaries of their single-member districts, a process vulnerable to political interference, to gerrymandering. To compound the problem, boundaries have to be periodically re-drawn to accommodate population changes. Even apolitically drawn boundaries can unintentionally produce the effect of gerrymandering, reflecting naturally occurring concentrations. PR systems with their multiple-member districts are less prone to this research suggests five-seat districts are immune to gerrymandering. The district boundaries are less critical and so can be aligned with historical boundaries such as cities, counties, states, or provinces; population changes can be accommodated by simply adjusting the number of representatives elected. For example, Professor Mollison in his 2010 plan for STV for the UK set an upper limit of 100,000 electors per MP so that a constituency of 500,000 electors would have five seats (1:100,000) but one of 500,001 six seats (1:83,000). His district boundaries follow historical county and local authority boundaries, yet he achieves more uniform representation than does the Boundary Commission, the body responsible for balancing the UK's first-past-the-post constituency sizes.
Mixed member systems are susceptible to gerrymandering for the local seats that remain a part of such systems. Under MMM, there is no compensation for the effects that such gerrymandering might have. Under MMP, the use of compensatory list seats makes gerrymandering less of an issue. However, its effectiveness in this regard depends upon the features of the system, including the size of the regional districts, the relative share of list seats in the total, and opportunities for collusion that might exist. A striking example of how the compensatory mechanism can be undermined can be seen in the 2014 Hungarian parliamentary election, where the leading party, Fidesz, combined gerrymandering and a number of other tricks to contrive a two-thirds parliamentary majority from a 45% vote.
Link between constituent and representative.
It is generally accepted that a particular advantage of plurality of majoritarian voting systems, such as first-past-the-post, is the geographic link between representatives and their constituents. PR is criticized because, as its multiple member districts are larger, this link is weakened if not completely lost. Party list PR systems with large districts, especially those without delineated districts such as the Netherlands and Israel, are vulnerable to this criticism. With smaller districts, in particular with STV, there are counter-arguments: about 90% of voters can consult a representative they voted for, someone whom they might think more sympathetic to their problem - one could say: with whom they have a closer link; constituents have a choice of representative so they can consult one with particular expertise in the topic at issue. With multiple member districts, prominent candidates are more able to be elected in their home constituencies, which they know and can represent authoritatively, so there is less need to parachute them into constituencies in which they are strangers and thus less than ideal representatives. Mixed member PR systems incorporate single-member districts to preserve the link, although, because up to half the parliamentary seats are list rather than district seats, the districts are necessarily up to twice as large as with a single member system.
Wider benefits to society.
Wider benefits from PR have been identified in societies using it as compared to those using FPTP, including higher scores on the UN Human Development Index, a measure of health, education, and personal security, higher economic growth, lower deficits or larger surpluses, less inequality, and better environmental protection.
Attributes of PR systems.
District magnitude.
Academics agree that the most important influence on proportionality is an electoral district's magnitude, the number of representatives elected from the district. Proportionality improves as the magnitude increases. Scholars recommend voting districts of between three and seven members.
At one extreme, the Chilean binomial voting system, a nominally proportional open-list system, uses two-member districts resulting in the election of one candidate from each of the two dominant political blocks, and so cannot be considered proportional.
At the other, where the district encompasses the entire country, and with a low minimum threshold, highly proportionate representation of political parties can result, and parties gain by broadening their appeal by nominating more minority and women candidates.
After the introduction of STV in Ireland in 1921 magnitude slowly diminished as more and more three-member constituencies were defined, benefiting the dominant Fianna Fáil, until 1979 when an independent boundary commission was established reversing the trend. In 2010, a parliamentary constitutional committee recommended a minimum magnitude of four. Nonetheless, despite relatively low magnitudes Ireland has generally experienced highly proportional results.
In the FairVote plan for STV (which FairVote calls ) for the US House of Representatives, three- to five-member super-districts are proposed. In Professor Mollison's plan for STV in the UK, four- and five-member districts are used, with three and six as necessary to fit existing boundaries.
Minimum threshold.
The minimum threshold is the minimum vote required to win a seat. The lower the threshold the higher the proportion of votes contributing to the election of representatives and the fewer the votes wasted.
All electoral systems have thresholds, either formally defined or as a mathematical consequence of the parameters of the election.
A formal threshold usually requires parties to win a certain percentage of the vote in order to be awarded seats from the party lists. In Germany and New Zealand (both MMP), the threshold is 5% of the national vote but both define an alternate threshold of constituency seats won, three in Germany, one in New Zealand. Turkey defines a threshold of 10%, the Netherlands 0.67%. Israel has raised its threshold from 1% (before 1992) to 1.5% (up to 2004), 2% (in 2006) and 3.25% in 2014.
In STV elections, winning the quota (ballots/(seats+1)) of first preference votes assures election. However, well regarded candidates who attract good second (and third, etc.) preference support can hope to win election with only half the quota of first preference votes. Thus, in a six-seat district the effective threshold would be 7.14% of first preference votes (100/(6+1)/2). The need to attract second preferences tends to promote consensus and disadvantage extremes.
Party magnitude.
Party magnitude is the number of candidates elected from one party in one district. As party magnitude increases a more balanced ticket will be more successful encouraging parties to nominate women and minority candidates for election.
But under STV too many candidates can be counter-productive, splitting the vote and so losing seats. An example of this was identified in a ward in the 2007 Scottish local elections where Labour, putting up three candidates, won only one seat but where they would have won two had a candidate particularly favoured by Labour voters not stood. The same effect may have contributed to the collapse of Fianna Fáil in the 2011 Irish general election.
Others.
Other aspects of PR can influence proportionality such as the size of the elected body, the choice of open or closed lists, ballot design, and vote counting methods.
Measuring proportionality.
A number of ways of measuring proportionality have been proposed, including the Loosemore-Hanby Index, the Sainte-Laguë Index and the Gallagher Index. None of these fully support ranked voting.
Deviation from proportionality denotes the proportion of malapointment in a democratic process. The deviation is a mathematical relationship between the percentage of votes obtained by a political party and the percentage of parliamentary seats allocated to it. It is calculated by subtracting each party's vote share from its seat share, adding up the absolute values (ignoring any negative signs), and dividing by two.
PR electoral systems.
Party list PR.
This system is used in many countries, including Finland (local list), Latvia (open list), Sweden (open list), Israel (national closed list), Brazil (open list), the Netherlands (open list), Russia (closed list), South Africa (closed list), Democratic Republic of the Congo (open list). For elections to the European Parliament, most member states use open lists; but most large EU countries use closed lists, so that the majority of EP seats are distributed by those. Local lists were used to elect the Italian Senate during the second half of the 20th century.
Closed list PR.
The parties each list their candidates according to that party's determination of priorities. In closed list systems, voters vote for a list of candidates, with the party choosing the order of candidates on the list and thus, in effect, their probability of being elected. Each party is allocated seats in proportion to the number of votes it receives, using the party-determined ranking order. 
There is an intermediate system in countries like Uruguay, where each party presents several closed lists, each representing a faction. Seats are distributed between parties according to the number of votes, and then between the factions within each party.
Open List PR.
In an open list, voters may vote, depending on the model, for one person, or for two, or indicate their order of preference within the list – nevertheless the number of candidates elected from the list is determined by the number of votes the list receives.
Local List PR.
In a local list system, parties divide their candidates in single member-like constituencies, which are ranked inside each general party list depending by their percentages. This method allows electors to judge every single candidate as in an FPTP system.
Single transferable vote.
The single transferable vote (STV), also called "choice voting", is a preferential voting system: voters rank candidates in order of preference. Voting districts usually elect three to seven representatives. The count is cyclic, electing or eliminating candidates and transferring votes until all seats are filled. A candidate is elected whose tally reaches a quota, the minimum vote that guarantees election. The candidate's surplus votes (those in excess of the quota) are transferred to other candidates at a fraction of their value proportionate to the surplus, according to the votes' preferences. If no candidates reach the quota, the candidate with the fewest votes is eliminated, those votes being transferred to their next preference at full value, and the count continues. There are many methods for transferring votes. Some early, manual, methods transferred surplus votes according to a randomly selected sample, or transferred only a "batch" of the surplus, other more recent methods transfer all votes at a fraction of their value (the surplus divided by the candidate's tally) but may need the use of a computer. Some methods may not produce exactly the same result when the count is repeated. There are also different ways of treating transfers to already elected or eliminated candidates, and these, too, can require a computer.
In effect, the method produces groups of voters of equal size that reflect the diversity of the electorate, each group having a representative the group voted for. Some 90% of voters have a representative to whom they gave their first preference. Voters can choose candidates using any criteria they wish, the proportionality is implicit. Political parties are not necessary; all other PR voting systems presume that parties reflect voters wishes, and so give power to parties. STV satisfies the voting system criterion "proportionality for solid coalitions" a solid coalition for a set of candidates is the group of voters that rank all those candidates above all others and is therefore considered a system of proportional representation. However, the small district magnitude used in STV elections has been criticized as impairing proportionality, especially when more parties compete than there are seats available, and STV has, for this reason, sometimes been labelled "quasi proportional". While this may be true when considering districts in isolation, results are proportional. In Ireland, with particularly small magnitudes, results are "highly proportional". In 1997, the average magnitude was 4.0 but eight parties gained representation, four of them with less than 3% of first preference votes nationally. Six independent candidates also won election. STV has also been described as the proportional system. The system tends to handicap extreme candidates because, to gain preferences and so improve their chance of election, candidates need to canvass voters beyond their own circle of supporters, and so need to moderate their views. Conversely, widely respected candidates can win election with relatively few first preferences by benefitting from strong subordinate preference support.
STV is used for two national lower houses of parliament, Ireland, since independence (as the Irish Free State) in 1922, and Malta, since 1921, long before independence in 1966. It is also used for all other elections in Ireland except for that of the presidency, for the Northern Irish assembly and European and local authorities, Scottish local authorities, some New Zealand and Australian local authorities, the Tasmanian (since 1907) and Australian Capital Territory assemblies, where the method is known as "Hare-Clark", and the city council in Cambridge, Massachusetts, (since 1941).
Australian Senate STV.
The term in Australia refers to the Senate voting system, a variant of "Hare-Clark" characterized by the "above the line" group voting ticket, a party list option. It is used in the Australian upper house, the Senate, and some state upper houses. Due to the number of preferences that are compulsory if a vote for candidates (below-the-line) is to be valid for the Senate a minimum of 90% of candidates must be scored, in 2013 in New South Wales that meant writing 99 preferences on the ballot 95% and more of voters use the above-the-line option, making the system, in all but name, a party list system. Parties determine the order in which candidates are elected and also control transfers to other lists and this has led to anomalies: preference deals between parties, and "micro parties" which rely entirely on these deals. Additionally, independent candidates are unelectable unless they form, or join, a group above-the-line. Concerning the development of STV in Australia researchers have observed: "... we see real evidence of the extent to which Australian politicians, particularly at national levels, are prone to fiddle with the electoral system". A parliamentary commission investigating the 2013 election, has proposed abolishing the group voting ticket, prohibiting preference transfers between lists but allowing voters to rank above-the-line party lists, and easing compulsory preference voting.
Mixed Member Systems.
Mixed member systems, also known as hybrid systems two levels of vote apportionment. The most prominent example is mixed member proportional representation (MMP), as used in Germany since 1949, a hybrid system combining single-winner districts with a compensating party list PR vote at the national level. Straight party list PR systems, too, can use two-tier apportionment, with party lists at the district level but apportioning seats at the district and national levels. Another two-tier system is biproportional apportionment, a method of proportionately balancing elections that has been adopted in some Swiss cities and cantons.
Mixed member proportional representation.
Mixed member proportional representation (MMP) combines a single-district vote, usually first-past-the-post, with a compensatory regional or nationwide party list proportional vote. The system aims to combine the local district representation of FPTP and the proportionality of a national party list system. It was invented for the German Bundestag after the second world war and has spread to Lesotho, Mexico, Bolivia and New Zealand. The system is also used for the Welsh and Scottish assemblies where it is called the additional member system. Voters have two votes, one for their district representative and one for the party list, the list vote determining the relative strength of parties in parliament. After the district winners have been determined, sufficient candidates from each party list are elected to "top-up" each party to the overall number of parliamentary seats due to it according to its overall list vote. Before apportioning list seats, all list votes for parties which failed to reach the minimum threshold are discarded, the proportions for remaining parties improve. Also, any direct seats won by independent candidates are subtracted from the parliamentary total used to apportion list seats.
The system has the potential to produce proportional results, but proportionality can be compromised if the ratio of list to district seats is too low, it may then not be possible to completely compensate district seat disproportionality. Another problem can be how overhang seats are handled, district seats that a party wins in excess of the number due to it under the list vote. To preserve proportionality, other parties need to be awarded "balance seats", increasing the size of parliament by twice the number of overhang seats, but this is not always done. Until recently, Germany increased the size of parliament by the number of overhang seats but did not use the increased size for apportioning list seats. This was changed for the 2013 national election after the constitutional court rejected the previous law, not compensating for overhang seats had resulted in a negative vote weight effect. Lesotho, Scotland and Wales don't increase the size of parliament at all, and, in 2012, a New Zealand parliamentary commission also proposed abandoning compensation for overhang seats, and so fixing the size of parliament. At the same time, it would abolish the single-seat threshold any such seats would then be overhang seats and would otherwise have increased the size of parliament further and reduce the vote threshold from 5% to 4%. Proportionality would not suffer.
Two-tier party list systems.
Some party list proportional systems with open lists use a two-tier compensatory system, as in Denmark, Norway, and Sweden. In Denmark, for example, the country is divided into ten multiple-member voting districts arranged in three regions, electing 135 representatives. In addition, 40 compensatory seats are elected. Voters have one vote which can be cast for an individual candidate or for a party list on the district ballot. To determine district winners, candidates are apportioned their share of their party's district list vote plus their individual votes. The compensatory seats are apportioned to the regions according to the party votes aggregated nationally, and then to the districts where the compensatory representatives are determined. In the 2007 general election, the district magnitudes, including compensatory representatives, varied between 14 and 28. The basic design of the system has remained unchanged since its introduction in 1920.
Biproportional apportionment.
Biproportional apportionment is a two-tier mathematical method (iterative proportional fitting) for modifying an election result to achieve proportionality. It was proposed for elections by the mathematician Michel Balinski in 1989, and first used by the city of Zurich for its council elections in February 2006, in a modified form called "new Zurich apportionment" ("Neue Zürcher Zuteilungsverfahren"). Zurich had had to modify its party list PR system after the Swiss Federal Court ruled that its smallest wards, as a result of population changes over many years, unconstitutionally disadvantaged smaller political parties. With biproportional apportionment, the method of election, the use of open party lists, hasn't changed, but the way winning candidates are determined has. The proportion of seats due to each party is calculated according to their overall, city-wide, vote, and then the district winners are adjusted to conform to these proportions. This means that some candidates, who would otherwise have been successful, can be denied seats in favor of initially unsuccessful candidates, in order to improve the relative proportions of their respective parties overall. This peculiarity is accepted by the Zurich electorate because the resulting city council is proportional and all votes, regardless of district magnitude, now have equal weight. The system has since been adopted by other Swiss cities and cantons.
Balinski has proposed another variant, "fair majority voting" (FMV), for Plurality/Majoritarian voting systems, specifically for the US House of Representatives, introducing proportionality while changing neither the plurality method of election nor the, possibly gerrymandered, district boundaries. The upper apportionment tier would be at the state level. In another proposal, for the UK parliament, whose elections are contested by many more parties, the authors note that parameters can be tuned to adopt any degree of proportionality deemed acceptable to the electorate. In order to elect smaller parties, a number of constituencies would be awarded to candidates placed fourth or even fifth in the constituency unlikely to be acceptable to the electorate, the authors concede but this effect could be substantially reduced by incorporating a third, regional, apportionment tier, or by specifying minimum thresholds.
Sortition.
Sortition is the random selection of political representatives from a population. Within statistical tolerances, the selection proportionately represents the population perfectly, in all its variety; entrepreneurs, teachers, plumbers, women, ethnic minorities, retirees, liberals, socialists, conservatives etc., are all represented in proportion to their presence in society. It was used in ancient Athens where selection by lot was considered a characteristic of democracy. It was later used in the Venetian Republic and is still used today in the summons to jury duty. In the 21st century, sortition has been used to select members for special citizens' assemblies in Ontario, British Columbia, Iceland, Belgium and elsewhere, and the concept has received academic attention.
History.
One of the earliest proposals of proportionality in an assembly was by John Adams in his influential pamphlet "Thoughts on Government", written in 1776 during the American Revolution:
Mirabeau, speaking to the Assembly of Provence on January 30, 1789, was also an early proponent of a proportionally representative assembly: 
In February 1793, the Marquis de Condorcet led the drafting of the Girondist constitution which proposed a limited voting scheme with proportional aspects. Before that could be voted on, the Montagnards took over the National Convention and produced their own constitution. On June 24, Saint-Just proposed the single non-transferable vote, which can be proportional, for national elections but the constitution was passed on the same day specifying first-past-the-post voting.
Already in 1787, James Wilson, like Adams a US Founding Father, understood the importance of multiple-member districts: "Bad elections proceed from the smallness of the districts which give an opportunity to bad men to intrigue themselves into office", and again, in 1791, in his Lectures on Law: "It may, I believe, be assumed as a general maxim, of no small importance in democratical governments, that the more extensive the district of election is, the choice will be the more wise and enlightened". The 1790 Constitution of Pennsylvania specified multiple-member districts for the state Senate and required their boundaries to follow county lines.
STV, or, more precisely, an election method where voters have one transferable vote, was first invented in 1819 by an English schoolmaster, Thomas Wright Hill, who devised a "plan of election" for the committee of the "Society for Literary and Scientific Improvement" in Birmingham that used not only transfers of surplus votes from winners but also from losers, a refinement that later both Andræ and Hare initially omitted. But the procedure was unsuitable for a public election and wasn't publicised. In 1839, Hill's son, Rowland Hill, recommended the concept for public elections in Adelaide, and a simple process was used in which voters formed as many groups as there were representatives to be elected, each group electing one representative.
The first practical PR election method, a list method, was conceived by Thomas Gilpin, a retired paper-mill owner, in a paper he read to the American Philosophical Society in Philadelphia in 1844: "On the representation of minorities of electors to act with the majority in elected assemblies". But the paper appears not to have excited any interest.
A practical election using a single transferable vote was devised in Denmark by Carl Andræ, a mathematician, and first used there in 1855, making it the oldest PR system, but the system never really spread. It was re-invented (apparently independently) in the UK in 1857 by Thomas Hare, a London barrister, in his pamphlet "The Machinery of Representation" and expanded on in his 1859 "Treatise on the Election of Representatives". The scheme was enthusiastically taken up by John Stuart Mill, ensuring international interest. The 1865 edition of the book included the transfer of preferences from dropped candidates and the STV method was essentially complete. Mill proposed it to the House of Commons in 1867, but the British parliament rejected it. The name evolved from "Mr.Hare's scheme" to "proportional representation", then "proportional representation with the single transferable vote", and finally, by the end of the 19th century, to "the single transferable vote".
A party list proportional representation system was devised and described in 1878 by Victor D'Hondt in Belgium. D'Hondt's method of seat allocation, the D'Hondt method, is still widely used. Victor Considerant, a utopian socialist, devised a similar system in an 1892 book. Some Swiss cantons (beginning with Ticino in 1890) used the system before Belgium, which was first to adopt list PR in 1900 for its national parliament. Many European countries adopted similar systems during or after World War I. List PR was favoured on the Continent because the use of lists in elections, the scrutin de liste, was already widespread. STV was preferred in the English-speaking world because its tradition was the election of individuals.
In the last Irish elections to the UK Parliament in 1919, STV was used in the University of Dublin constituency; two Independent Unionists were elected. STV has been in use since Irish independence. This has led to a series of coalition governments; there has not been a single-party government since after the 1989 election, and the last time a majority single party government was formed was in 1977. A mainly centrist party, Fianna Fáil, typically received 30%–50% of the vote while opposition parties, traditionally the centre-right Fine Gael and the centre-left Labour Party, were comparatively weak. However, after the 2011 election this changed significantly with Fianna Fáil losing over half the proportion of the vote they received in the 2007 election, losing their position as largest party in the parliament for the first time since the 1932 election, becoming the third largest party.
STV was used in Tasmania in 1907.
PR is used by more nations than Plurality/Majoritarian systems. Among the world's 35 most robust democracies with populations of at least two million people, only six use winner-take-all systems for elections to the legislative assembly (plurality, runoff or instant runoff); four use parallel systems; and 25 use PR.
PR dominates Europe, including Germany and most of northern and eastern Europe; it is also used for European Parliament elections. France adopted PR at the end of World War II, but discarded it in 1958; it was used for parliament elections in 1986. Switzerland has the most widespread use of proportional representation, which is the system used to elect not only national legislatures and local councils, but also all local executives. PR is less common in the English-speaking world; New Zealand adopted MMP in 1993, but the UK, Canada, India and Australia all use winner-take-all systems for legislative elections.
In Canada, STV was used by the cities of Edmonton and Calgary in Alberta from 1926 to 1955, and by Winnipeg in Manitoba from 1920 to 1953. In both provinces the alternative vote (AV) was used in rural areas. First-past-the-post was re-adopted in Alberta by the dominant party for reasons of political advantage, in Manitoba a principal reason was the underrepresentation of Winnipeg in the provincial legislature.
STV has some history in the United States. Many cities, including New York City, once used it to break up the Democratic Party monopolies on elective office. Cincinnati, Ohio, adopted STV in 1925 to get rid of a Republican Party monopoly, but the Republicans returned the city to FPTP in 1957. From 1870 to 1980, Illinois used a semi-proportional cumulative voting system to elect its House of Representatives. Each district across the state elected both Republicans and Democrats year-after-year. Cambridge, Massachusetts, (STV) and Peoria, Illinois, (cumulative voting) continue to use PR. San Francisco had city-wide elections in which people would cast votes for five or six candidates simultaneously, delivering some of the benefits of proportional representation.
List of countries using proportional representation.
Detailed information on voting systems applying to the first chamber of the legislature is maintained by the ACE Electoral Knowledge Network. This includes both a map and a detailed table by country. What follows is a more summary presentation on countries using proportional representation.

</doc>
<doc id="23299" url="https://en.wikipedia.org/wiki?curid=23299" title="Provisional Irish Republican Army">
Provisional Irish Republican Army

The Provisional Irish Republican Army (IRA or PIRA) was an Irish republican paramilitary organisation that sought to remove Northern Ireland from the United Kingdom and bring about an independent republic encompassing all of Ireland. It was the biggest and most active republican paramilitary group during the Troubles. It saw itself as the successor to the original IRA and called itself simply the Irish Republican Army, or "Óglaigh na hÉireann" in Irish. It was also widely referred to as such by others.
The Provisional IRA emerged in December 1969, following a split in the republican movement. The Troubles had begun a year before, when a largely Catholic, nonviolent civil rights campaign was met with violence from both Ulster loyalists and the Royal Ulster Constabulary, culminating in the August 1969 riots and deployment of British troops. The IRA initially focused on defence, but it began an offensive campaign in 1971 (see timeline). The IRA's primary goal was to force the British to negotiate a withdrawal from Northern Ireland. It used guerrilla tactics against the British Army and RUC in both rural and urban areas. It also carried out a bombing campaign in Northern Ireland and England against what it saw as political and economic targets. The IRA called a final ceasefire in July 1997, after Sinn Féin was re-admitted into the Northern Ireland peace talks. It supported the 1998 Good Friday Agreement and in 2005 it disarmed under international supervision.
Overview of strategies.
The IRA's initial strategy was to use force to cause the collapse of the government of Northern Ireland and to inflict enough casualties on British forces that the British government would be forced by public opinion to withdraw from the region. This policy involved recruitment of volunteers, increasing after the 1972 Bloody Sunday incident in which the British armed forces killed unarmed protesters, and launching attacks against British military and economic targets. The campaign was supported by arms and funding from Libya and from some Irish American groups.
The IRA agreed to a ceasefire in February 1975, which lasted nearly a year before the IRA concluded that the British were drawing them away from military action without offering any guarantees in relation to the IRA's goals (as well as launching an intelligence offensive), and hopes of a quick victory receded. As a result, the IRA launched a new strategy known as "the Long War". This saw them conduct a war of attrition against the British and increased emphasis on political activity, via the political party Sinn Féin.
The success of the 1981 Irish hunger strike in mobilising support and winning elections led to the Armalite and ballot box strategy, with more time and resources devoted to political activity. The abortive attempt at an escalation of the military part of that strategy led republican leaders increasingly to look for a political compromise to end the conflict, with a broadening dissociation of Sinn Féin from the IRA. Following negotiations with the Social Democratic and Labour Party (SDLP) and secret talks with representatives of both the British and Irish governments, the IRA ultimately called a ceasefire in 1994 on the understanding that Sinn Féin would be included in political negotiations for a settlement. When the British government, dependent on Ulster Unionist Party votes at Westminster, then demanded the disarmament of the IRA before it allowed Sinn Féin into multiparty talks, the IRA called off its ceasefire in February 1996. The British demand was quickly dropped after the May 1997 general election in the UK. The IRA ceasefire was then reinstated in July 1997 and Sinn Féin was admitted into all-party talks, which produced the Good Friday Agreement of 1998.
The IRA's armed campaign, primarily in Northern Ireland but also in England and mainland Europe, caused the deaths of approximately 1,800 people. The dead included around 1,100 members of the British security forces, and about 640 civilians. The IRA itself lost 275–300 members and an estimated 10,000 imprisoned at various times over the 30-year period.
On 28 July 2005, the IRA Army Council announced an end to its armed campaign, stating that it would work to achieve its aims using "purely political and democratic programmes through exclusively peaceful means", and shortly afterwards completed decommissioning. In September 2008, the nineteenth report of the Independent Monitoring Commission stated that the IRA was "committed to the political path" and no longer represented "a threat to peace or to democratic politics", and that the Army Council was "no longer operational or functional". The organisation remains classified as a proscribed terrorist group in the UK and as an illegal organisation in the Republic of Ireland. Two small groups split from the Provisional IRA, the Continuity IRA in 1986, and the Real IRA in 1997. Both reject the Good Friday Agreement and continue to engage in paramilitary activity.
On 26 July 2012, it was announced that some former members of the Provisional Irish Republican Army were merging with the Real Irish Republican Army, other independent republican paramilitary groups and the vigilante group Republican Action Against Drugs (but not with the Continuity Irish Republican Army) into a unified formation known simply as the "Irish Republican Army". This new IRA group is estimated by Police Service of Northern Ireland intelligence sources to have between 250 and 300 active militants and many more supporting associates.
Organisation.
The Provisional IRA was organised hierarchically. At the top of the organisation was the IRA Army Council, headed by the IRA Chief of Staff.
Leadership.
All levels of the organisation were entitled to send delegates to IRA General Army Conventions (GACs). The GAC was the IRA's supreme decision-making authority. Before 1969, GACs met regularly. Since 1969, there have only been three, in 1970, 1986, and 2005, owing to the difficulty in organising such a large gathering of an illegal organisation in secret.
The GAC in turn elected a 12-member IRA Executive, which selected seven volunteers to form the IRA Army Council. For day-to-day purposes, authority was vested in the Army Council which, as well as directing policy and taking major tactical decisions, appointed a Chief of Staff from one of its number or, less often, from outside its ranks.
The Chief of Staff would appoint an adjutant general as well as a General Headquarters (GHQ), which consisted of heads of the following departments:
Regional command.
The IRA was divided into a Northern Command, which operated in the nine Ulster counties as well as the border counties of Leitrim and Louth, and a Southern Command, operating in the rest of Ireland. The Provisional IRA was originally commanded by a leadership based in Dublin. However, in 1977, parallel to the introduction of cell structures at local level, command of the "war-zone" was given to the Northern Command. According to Ed Moloney, these moves at re-organisation were the idea of Ivor Bell, Gerry Adams and Brian Keenan.
Brigades.
The IRA referred to its ordinary members as volunteers (or "óglaigh" in Irish). Up until the late 1970s, IRA volunteers were organised in units based on conventional military structures. Volunteers living in one area formed a company as part of a battalion, which could be part of a brigade, although many battalions were not attached to a brigade.
IRA brigades usually followed county lines, which were sometimes subdivided (especially when they included major urban areas). The Belfast Brigade had three battalions, in the west, north and east of the city. In the early years of the Troubles, the IRA in Belfast expanded rapidly; in August 1969, the Belfast Brigade had just 50 active members - by the end of 1971, it had 1,200 members, giving it a large but loosely controlled structure.
The Derry Battalion became the Provisional IRA Derry Brigade in 1972 after a rapid increase in membership following Bloody Sunday when 1 PARA killed 13 unarmed demonstrators at a civil rights march. The Derry Brigade also controlled parts of northern County Londonderry and northeast County Donegal.
County Armagh had three battalions, two very active ones in South Armagh and a less active unit in North Armagh. For this reason the Armagh IRA unit is often referred to as the South Armagh Brigade. Similarly, the Tyrone/Monaghan Brigade, which operated from around the Border of Northern Ireland and the Republic of Ireland, is often called the East Tyrone Brigade. The leadership structure at battalion and company level was the same: each had its own commanding officer, quartermaster, explosives officer and intelligence officer. There was sometimes a training officer or finance officer.
Active service units.
From 1973, the IRA started to move away from the larger conventional military organisational principle owing to its security vulnerability. A system of two parallel types of unit within an IRA brigade was introduced in place of the battalion structures. Firstly, the old "company" structures were used for tasks such as "policing" nationalist areas, intelligence-gathering, and hiding weapons. These were essential support activities. However, the bulk of actual attacks were the responsibility of a second type of unit, the active service unit (ASU). To improve security and operational capacity, these ASUs were smaller, tight-knit cells, usually consisting of five to eight members. The ASU's weapons were controlled by a brigade's quartermaster. It was estimated that in the late 1980s the IRA had roughly 300 members in ASUs and about another 450 serving in supporting roles.
The exception to this reorganisation was the South Armagh Brigade, which retained its traditional hierarchy and battalion structure and used relatively large numbers of volunteers in its actions. South Armagh didn't have the same problems with security that other brigades had.
The IRA's Southern Command, located in the Republic of Ireland, consists of a Dublin Brigade and a number of smaller units in rural areas. These were charged mainly with the importation and storage of arms for the Northern units and with raising finances through robberies and other means.
History.
Origins.
In August 1969, a confrontation between Catholic residents of the Bogside and the Royal Ulster Constabulary (RUC) in Derry following an Apprentice Boys of Derry march led to a large communal riot now referred to as the Battle of the Bogside – three days of fighting between rioters throwing stones and petrol bombs on one side, and police who saturated the area with CS gas and other unionists on the other.
Protests organised by the Northern Ireland Civil Rights Association in support of the Bogsiders were held elsewhere in the region, sparking retaliation by Protestant mobs; the subsequent burning, damage to property and intimidation, largely against the minority community, forced 1,800 people (mostly Catholics) from their homes in Belfast in what became known as the Northern Ireland riots of August 1969, with over 200 Catholic homes being destroyed or requiring major repairs. A number of people were killed on both sides, some by the police. The Irish Republican Army had been poorly armed and its defence of Catholic-majority areas from Ulster loyalists, which had been considered one of its traditional roles since the 1920s, was seen by many as inadequate.
Veteran republicans were critical of the IRA's Dublin leadership which, for political reasons, had refused to prepare for aggressive action in advance of the violence. On 24 August Joe Cahill, Seamus Twomey, Dáithí Ó Conaill, Billy McKee and several other future Provisional leaders came together in Belfast intending to remove the Belfast leadership and turn back to traditional militant republicanism. Although the pro-Goulding commander Billy McMillen stayed in command, he was told it was only for three months and he was not to have any communication with the IRA's Dublin based leadership.
Traditional republicans formed the "Provisional" Army Council in December 1969, after an IRA convention was held at Knockvicar House in Boyle, County Roscommon. The two main issues were the acceptance of the "National Liberation Strategy" and a motion to end abstentionism and to recognise the British, Irish and Northern Ireland parliaments. While the motion on the "National Liberation Strategy" was passed unanimously the motion on abstentionism was only passed by 28 votes to 12. Opponents of this change argued strongly against the ending of abstentionism, and when the vote took place, Seán Mac Stíofáin, present as IRA Director of Intelligence, announced that he no longer considered that the IRA leadership represented republican goals. However, there was no walkout. Those opposed, who included Mac Stíofáin and Ruairí Ó Brádaigh, refused to go forward for election to the new IRA Executive.
While others canvassed support throughout Ireland, Mac Stíofáin was a key person making a connection with the Belfast IRA under Billy McKee and Joe Cahill, who had refused to take orders from the IRA's Dublin leadership since September 1969, in protest at their failure to defend Catholic areas in August. Nine out of thirteen IRA units in Belfast sided with the Provisionals in December 1969, roughly 120 activists and 500 supporters. The first "Provisional" Army Council was composed of Seán Mac Stíofáin, Ruairí Ó Brádaigh, Paddy Mulcahy, Sean Tracey, Leo Martin, and Joe Cahill. They issued their first public statement on 28 December 1969, stating:
We declare our allegiance to the 32 county Irish republic, proclaimed at Easter 1916, established by the first Dáil Éireann in 1919, overthrown by forces of arms in 1922 and suppressed to this day by the existing British-imposed six-county and twenty-six-county partition states.
The Provisional IRA issued all its public statements under the pseudonym "P. O'Neill" of the "Irish Republican Publicity Bureau, Dublin". According to Ruairí Ó Brádaigh, it was Seán Mac Stiofáin, as chief of staff of the IRA, who invented the name. However, under his usage, the name was written and pronounced according to Irish orthography and pronunciation as "P. Ó Néill". According to Danny Morrison, the pseudonym "S. O'Neill" was used during the 1940s.
The Sinn Féin party split along the same lines on 11 January 1970, when a third of the delegates walked out of the Ard Fheis in protest at the party leadership's attempt to force through the ending of abstentionism, despite its failure to achieve a two-thirds majority vote of delegates required to change the policy. Despite the declared support of that faction of Sinn Féin, the early Provisional IRA was extremely suspicious of political activity, arguing rather for the primacy of armed struggle.
What would become the Provisional IRA received arms and funding from the Fianna Fáil-led Irish government in 1969, resulting in the 1970 Arms Crisis in which criminal charges were pursued against two former government ministers and others. Roughly £100,000 was donated by the Irish government to "Defence Committees" in Catholic areas and, according to historian Richard English, "there is now no doubt that some money did go from the Dublin government to the proto-Provisionals".
The Provisionals maintained the principles of the pre-1969 IRA; they considered both British rule in Northern Ireland and the government of the Republic of Ireland to be illegitimate, insisting that the Provisional IRA's Army Council was the only valid government, as head of an all-island Irish Republic. This belief was based on a series of perceived political inheritances which constructed a legal continuity from the Second Dáil (see also Irish republican legitimatism).
By 1971, the Provisionals had inherited most of the existing IRA organisation in the north, as well as the more militant IRA members in the rest of Ireland. In addition, they recruited many young nationalists from the north, who had not been involved in the IRA before but had been radicalised by the violence that broke out in 1969. These people were known in republican parlance as "sixty niners", having joined after 1969. The Provisional IRA adopted the phoenix as symbol of the Irish republican rebirth in 1969. One of its slogans was "out of the ashes rose the Provisionals".
Initial phase.
Following the violence of August 1969, the IRA began to arm and train to protect nationalist areas from further attack. After the Provisionals' split from the Official IRA, the Provisional IRA began planning for offensive action against what it viewed as British occupation.
The Official IRA were opposed to such a campaign because they felt it would lead to sectarian conflict, which would defeat their strategy of uniting the workers from both sides of the sectarian divide. The IRA Border Campaign in the 1950s had avoided actions in urban centres of Northern Ireland to avoid civilian casualties and probable resulting sectarian violence. The Provisional IRA, by contrast, was primarily an urban organisation, based originally in Belfast and Derry.
The Provisional IRA's strategy was to use force to cause the collapse of the government of Northern Ireland and to inflict such casualties on the British forces that the British government would be forced by public opinion to withdraw from Ireland. According to journalist Brendan O'Brien, "the thinking was that the war would be short and successful. Chief of Staff Seán Mac Stíofáin decided they would 'escalate, escalate and escalate' until the British agreed to go". This policy involved recruitment of volunteers and carrying out attacks on British forces, as well as mounting a bombing campaign against economic targets. In the early years of the conflict, IRA slogans spoke of, "Victory 1972" and then "Victory 1974". Its inspiration was the success of the "Old IRA" in the Irish War of Independence (1919–1922), which also relied on British public opinion to achieve its aims. In their assessment of the IRA campaign, the British Army would describe the years 1970–72 as the "insurgency phase".
The British government held secret talks with the IRA leadership in 1972 to try and secure a ceasefire based on a compromise settlement, after the events of Bloody Sunday led to an increase in IRA recruitment and support. The IRA agreed to a temporary ceasefire from 26 June to 9 July. In July 1972, Seán Mac Stíofáin, Dáithí Ó Conaill, Ivor Bell, Seamus Twomey, Gerry Adams and Martin McGuinness met a British delegation led by William Whitelaw. The republicans refused to consider a peace settlement that did not include a commitment to British withdrawal, a retreat of the British Army to its barracks, and a release of republican prisoners. The British refused and the talks broke up.
At this time the IRA brought out the "Éire Nua" (New Ireland) policy, which advocated an all-Ireland federal republic, with decentralised governments and parliaments for each of the four historic provinces of Ireland.
1975 ceasefire.
By the mid-1970s, the hopes of the IRA leadership for a quick military victory were receding and the British military was unsure of when it would see any substantial success against the IRA. Secret meetings between Provisional IRA leaders Ruairí Ó Brádaigh and Billy McKee with British Secretary of State for Northern Ireland Merlyn Rees secured an IRA ceasefire which began in February 1975. The IRA initially believed that this was the start of a long-term process of British withdrawal, but later came to the conclusion that the British were unwilling and/or unable to make concessions in areas they deemed crucial. Critics of the IRA leadership, most notably Gerry Adams, felt that the ceasefire was disastrous for the IRA, leading to infiltration by British informers, the arrest of many activists and a breakdown in IRA discipline resulting in sectarian killings and a feud with fellow republicans in the Official IRA. At this time, the IRA leadership, short of money, weapons and members, was on the brink of calling off the campaign. However, the ceasefire was ended in January 1976 instead.
The "Long War".
Thereafter, the IRA evolved a new strategy which they called the "Long War". This underpinned IRA strategy for the rest of the Troubles and involved the re-organisation of the IRA into small cells, an acceptance that their campaign would last many years before being successful and an increased emphasis on political activity through Sinn Féin. A republican document of the early 1980s states: "Both Sinn Féin and the IRA play different but converging roles in the war of national liberation. The Irish Republican Army wages an armed campaign... Sinn Féin maintains the propaganda war and is the public and political voice of the movement". The 1977 edition of the Green Book, an induction and training manual used by the IRA, describes the strategy of the "Long War" in these terms:
Confidential documents released on 30 December 2008 from the British state archives show that the IRA leadership proposed a ceasefire and peace talks to the British government in 1978. The British refused the offer. Prime Minister James Callaghan decided that there should be "positive rejection" of the approach on the basis that the republicans were not serious and "see their campaign as a long haul". Republic of Ireland state documents from the same period say that the IRA had made a similar offer to the British the previous year. An Irish Defence Forces document, dated 15 February 1977, states that "It is now known that feelers were sent out at Christmas by the top IRA leadership to interest the British authorities in another long ceasefire." The "Éire Nua" policy was discontinued by the Army Council in 1979 but remained Sinn Féin policy until 1982, reflecting the sequence in which the old leadership of the republican movement were being sidelined.
IRA prisoners convicted after March 1976 did not have Special Category Status applied in prison. In response, more than 500 prisoners refused to wear prison clothes. This activity culminated in the 1981 Irish hunger strike, when seven IRA and three Irish National Liberation Army members starved themselves to death in pursuit of political status. The hunger strike leader Bobby Sands and Anti H-Block activist Owen Carron were elected to the British Parliament, and two other protesting prisoners were elected to the Dáil. In addition, there were work stoppages and large demonstrations all over Ireland in sympathy with the hunger strikers. More than 100,000 people attended the funeral of Sands, the first hunger striker to die.
After the success of IRA hunger strikers in mobilising support and winning elections on an Anti H-Block platform in 1981, republicans increasingly devoted time and resources to electoral politics, through the Sinn Féin party. Danny Morrison summed up this policy at a 1981 Sinn Féin Ard Fheis (annual meeting) as a "ballot paper in this hand and an Armalite in the other".
Peace process.
The success of the 1981 Irish hunger strike in mobilising support and winning elections led to what was referred to by Danny Morrison as, "the Armalite and ballot box strategy" with more time and resources devoted to political activity. The perceived stalemate along with British government's hints of a compromise and secret approaches in the early 1990s led republican leaders increasingly to look for a political agreement to end the conflict, with a broadening dissociation of Sinn Féin from the IRA. Following negotiations with the Social Democratic and Labour Party (SDLP) and secret talks with British civil servants, the IRA ultimately called a ceasefire in 1994 on the understanding that Sinn Féin would be included in political talks for a settlement. When the British government then demanded the disarmament of the IRA before it allowed Sinn Féin into multiparty talks, the organisation called off its ceasefire in February 1996. The renewed bombings caused severe economic damage, with the Manchester bombing and the Docklands bombing causing approximately £800 million in combined damage. After the ceasefire was reinstated in July 1997, Sinn Féin was admitted into all-party talks, which produced the Good Friday Agreement of 1998. The IRA's armed campaign, primarily in Northern Ireland but also in England and mainland Europe, caused the deaths of approximately 1,800 people. The dead included around 1,100 members of the British security forces, and about 630 civilians. The IRA itself lost 275–300 members, of an estimated 10,000 total over the 30-year period. Between 1970 and 2005, the IRA had detonated 19,000 improvised explosive devices (IEDs) in the United Kingdom, an average of one every 17 hours for three and a half decades, arguably making it "the biggest terrorist bombing campaign in history".
According to author Ed Moloney, the IRA made an attempt to escalate the conflict with the so-called "Tet Offensive" in the 1980s, which was reluctantly approved by the Army Council and did not prove successful. On the other hand, public speeches from two Northern Ireland Secretaries of State, Peter Brooke and Patrick Mayhew hint that, given the cessation of violence, a political compromise with the IRA was possible. Gerry Adams entered talks with John Hume, the leader of the moderate nationalist Social Democratic and Labour Party (SDLP) in 1993, and secret talks were also conducted since 1991 between Martin McGuinness and a senior MI6 officer, Michael Oatley. Thereafter, Adams increasingly tried to disassociate Sinn Féin from the IRA, claiming they were separate organisations and refusing to speak on behalf of the IRA. Within the Republican Movement (the IRA and Sinn Féin), the new strategy was described by the acronym "TUAS", meaning either "Tactical Use of Armed Struggle" or "Totally Unarmed Strategy".
The IRA ceasefire in 1997 formed part of a process that led to the 1998 Belfast (Good Friday) Agreement. One aim of the Agreement is that all paramilitary groups in Northern Ireland cease their activities and disarm by May 2000. In October 1997, the US stopped designating the IRA as a terrorist organisation.
Calls from Sinn Féin led the IRA to commence disarming in a process that was monitored by Canadian General John de Chastelain's decommissioning body in October 2001. However, following the collapse of the Stormont power-sharing government in 2002, which was partly triggered by allegations that republican spies were operating within Parliament Buildings and the Civil Service, the IRA temporarily broke off contact with General de Chastelain.
In December 2004, attempts to persuade the IRA to disarm entirely collapsed when the Democratic Unionist Party, under Ian Paisley, insisted on photographic evidence. Justice Minister Michael McDowell (in public, and often) insisted that there would need to be a complete end to IRA activity.
At the beginning of February 2005, the IRA declared that it was withdrawing from the disarmament process, but in July 2005 it declared that its campaign of violence was over, and that transparent mechanisms would be used, under the de Chastelain process, to satisfy the Northern Ireland communities that it was disarming totally.
End of the armed campaign.
On 28 July 2005, the IRA Army Council announced an end to the armed campaign, stating that it would work to achieve its aims solely by peaceful political means. The Army Council stated that it had ordered volunteers to dump all weapons and to end all paramilitary activity. It also announced that the IRA would complete the process of disarmament as quickly as possible.
This was not the first time that an organization calling itself the IRA had issued orders to dump arms. After its defeat in the Irish Civil War in 1924 and at the end of its unsuccessful Border Campaign in 1962, the IRA Army Council issued similar orders. However, this was the first time that an IRA had voluntarily decided to dispose of its arms. Some authors, like Patrick McCarthy, Peter Taylor and Brendan O'Brien concluded that, unlike previous IRA campaigns, the provisionals were not defeated.
In September 2005, international weapons inspectors supervised the full disarmament of the IRA. On 26 September, the Independent International Commission on Decommissioning (IICD) announced that "the totality of the IRA's arsenal" had been decommissioned. The IRA invited two independent witnesses to view the secret disarmament work: Catholic priest Father Alec Reid and Protestant minister Reverend Harold Good. Ian Paisley, leader of the Democratic Unionist Party (DUP), complained that the witnesses were not unbiased, because they were appointed by the IRA itself. Nationalists saw his comments as reflecting his opposition to nationalists in government.
Since then, there have been occasional claims in the media that the IRA had not decommissioned all of its weaponry. In response to such claims, the Independent Monitoring Commission (IMC) stated in its tenth report that the IRA had decommissioned all weaponry "under its control". It said that if any weapons had been kept, they would have been kept by individuals and against IRA orders.
There have also been claims that the IRA is still active and has carried out punishment shootings (see Chronology of Provisional Irish Republican Army actions (2000–09)). In August 2008, the "The Sunday Times" quoted a "senior Garda intelligence officer" as saying that the IRA was being maintained "in shadow form"; that it had recruited in recent years, still had weapons and was still capable of carrying out attacks. PSNI Assistant Chief Constable Peter Sheridan, said it was unlikely the IRA as a whole would formally disband in the foreseeable future.
In September 2008, the Independent Monitoring Commission stated in its nineteenth report that the IRA was "committed to the political path" and was no longer "a threat to peace or to democratic politics". It concluded that the IRA as an organization was being allowed to wither away and was "beyond recall": it had disbanded its military departments, stopped recruiting or training members, lost its military capability, and the Army Council was "no longer operational". The report also said that the IRA is not involved in any criminal activity, but that some members have engaged in criminal activity without the sanction or support of the organization.
Sinn Féin President Gerry Adams said in 2011: "The war is over. The IRA is gone. The IRA embraced, facilitated and supported the peace process. When a democratic and peaceful alternative to armed struggle was created the IRA left the stage." In 2014 Adams said: "The IRA is gone. It is finished". However, the Assessment on Paramilitary Groups in Northern Ireland in October 2015 concluded that the Provisional IRA, while committed to peace, continues to exist in a reduced form.
In February 2015, the Garda Commissioner stated that Gardaí have no evidence that the IRA's military structure remains or that the IRA is engaged in crime.
In August 2015, the PSNI Chief Constable stated that the IRA no longer exists as a paramilitary organization. He said that some of its structure remains, but that the group is committed to following a peaceful political path and is not engaged in criminal activity or directing violence. However, he added that some members have engaged in criminal activity or violence for their own ends. The statement was in response to the recent killings of two former IRA members. In May, former IRA commander Gerard Davison was shot dead in Belfast. He had been involved in Direct Action Against Drugs and it is believed he was killed by an organized crime gang. Three months later, former IRA member Kevin McGuigan was also shot dead in Belfast. It is believed he was killed by the group Action Against Drugs, in revenge for the Davison killing. The Chief Constable believed that IRA members collaborated with Action Against Drugs, but without the sanction of the IRA. In response, the UK government commissioned the Independent Assessment of Paramilitary Organisations.
The organization remains classified as a proscribed terrorist group in the UK and as an illegal organisation in the Republic of Ireland. Two small groups split from the IRA, the Continuity IRA in 1986, and the Real IRA in 1997. Both reject the Good Friday Agreement and continue to engage in paramilitary activity.
Weaponry and operations.
In the early days of the Troubles the IRA was very poorly armed, mainly with old World War II weaponry such as M1 Garands and Thompson submachine guns, but starting in the early 1970s it procured large amounts of modern weaponry from such sources as supporters in the United States, Libyan leader Colonel Muammar Gaddafi, and arms dealers in Europe, America, the Middle East and elsewhere. The Libyans supplied the IRA with the RPG-7.
In the first years of the conflict, the IRA's main activities were providing firepower to support nationalist rioters and defending nationalist areas from attacks. The IRA gained much of its support from these activities, as they were widely perceived within the nationalist community as being defenders of Irish nationalist and Roman Catholic people against aggression.
From 1971–1994, the IRA launched a sustained offensive armed campaign that mainly targeted the British Army, the Royal Ulster Constabulary (RUC), the Ulster Defence Regiment (UDR), and economic targets in Northern Ireland. In addition, some IRA members carried out attacks against Protestant civilians.
The IRA was chiefly active in Northern Ireland, although it took its campaign to England and mainland Europe. The IRA also targeted certain British government officials, politicians, judges, establishment figures, British Army and police officers in England, and in other areas such as the Republic of Ireland, West Germany and the Netherlands. By the early 1990s, the bulk of the IRA activity was carried out by the South Armagh Brigade, well known through its sniping operations and attacks on British Army helicopters. The bombing campaign principally targeted political, economic and military targets, and approximately 60 civilians were killed by the IRA in England during the conflict.
It has been argued that this bombing campaign helped convince the British government (who had hoped to contain the conflict to Northern Ireland with its Ulsterisation policy) to negotiate with Sinn Féin after the IRA ceasefires of August 1994 and July 1997.
On 31 August 1994, the IRA declared an indefinite ceasefire. However, from February 1996 until July 1997, the IRA called off its 1994 ceasefire because of its dissatisfaction with the state of negotiations. They re-instated the ceasefire in July 1997, and it has been in operation since then.
The IRA decommissioned all of its remaining arms between July and September 2005. The decommissioning of its weaponry was supervised by the Independent International Commission on Decommissioning (IICD). Among the weaponry estimated (by Jane's Information Group) to have been destroyed as part of this process were:
Having compared the weapons destroyed with the British security forces' estimates of the IRA weaponry, and because of the IRA's full involvement in the process of destroying the weapons, the IICD arrived at their conclusion that all IRA weaponry has been destroyed.
Since the process of decommissioning was completed, unnamed sources in MI5 and the Police Service of Northern Ireland (PSNI) have reported to the press that not all IRA arms were destroyed during the process. This claim remains unsubstantiated so far. In its report dated April 2006 the Independent Monitoring Commission (IMC) stated that it had no reason to disbelieve the IRA or to suspect that it had not fully decommissioned. It believed that any weaponry that had not been handed in had been retained locally and against the wishes of the IRA leadership. The Russian and British Intelligence services alleged that during the decommissioning process the IRA secretly purchased a consignment of 20 Russian special forces AN-94 rifles in Moscow.
In mid-July 2013, the Gardaí displayed arms and explosives (Semtex) recently recovered from dissident republicans in the Dublin area. The Gardaí believe this Semtex to have come from the Libyan connection back in the 1980s and therefore should have been decommissioned.
Other activities.
Apart from its armed campaign, the IRA has also been involved in many other activities.
Sectarian attacks.
The IRA publicly condemned sectarianism and sectarian attacks. However, some IRA members were involved in sectarian tit-for-tat violence. Of those killed by the IRA, Sutton classifies 130 (about 7%) of them as sectarian killings of Protestants. Unlike loyalists, the IRA denied responsibility for sectarian attacks and the members involved used covernames, such as "Republican Action Force". They claimed that their attacks on Protestants were "retaliation" for attacks on Catholics. Many in the IRA opposed these sectarian attacks, but others deemed them effective in preventing sectarian attacks on Catholics.
Some unionists allege that the IRA took part in "ethnic cleansing" of the Protestant minority in rural border areas, such as Fermanagh. Many local Protestants allegedly believed that the IRA tried to force them into leaving. However, most Protestants killed by the IRA in these areas were members of the security forces, and there was no exodus of Protestants.
Alleged involvement in crime.
To fund its campaign, the IRA was allegedly involved in criminal activities such as robberies, counterfeiting, protection rackets, kidnapping for ransom, fuel laundering and cigarette smuggling. The IRA also raised funds through donations and by running legitimate businesses such as taxi firms, social clubs, pubs and restaurants. It is estimated that, by the 1990s, the IRA needed £10.5 million a year to operate.
IRA supporters argue that the IRA's "securing of funds by extralegal methods is justified as a means to achieve a political goal. Unlike crimes committed for personal gain, IRA operations are considered strategic attacks against an oppressive state". However, this activity allowed the British Government to portray the IRA as no more than a criminal gang.
It was estimated that the IRA carried out 1,000 armed robberies in Northern Ireland, mostly of banks and post offices. It was accused of involvement in the biggest bank raid in Irish history—the 2004 Northern Bank robbery—when £26.5 million was stolen from the Northern Bank in Belfast city centre. The PSNI, the Independent Monitoring Commission and the British and Irish governments all accused the IRA of involvement. It is suggested that the IRA needed the money to pay pensions to its volunteers, and to ensure that hardliners stuck with the peace strategy. The IRA denied involvement, however.
Generally, the IRA was against drug dealing and prostitution, "both for 'moral' reasons and because it would be unpopular within its own communities". The Chief of the RUC's Drugs Squad, Kevin Sheehy, said "the Provisional IRA did its best to stop volunteers from becoming directly involved drugs" and noted one occasion when an IRA member caught with a small amount of cannabis was "disowned and humiliated" in his local area. The IRA often targeted drug dealers. Many were given punishment shootings or banished, and some were killed. However, there are claims the IRA "licensed" certain dealers to operate and forced them to pay protection money.
Speaking in 2005, Gerry Adams said "There is no place in republicanism for anyone involved in criminality". However, he went on to say "we refuse to criminalise those who break the law in pursuit of legitimate political objectives".
In 2008, the Independent Monitoring Commission stated that the IRA was no longer involved in criminality, but that some members have engaged in criminality for their own ends, without the sanction or support of the IRA.
Vigilantism.
During the conflict, the IRA took on the role of policing in some Catholic/nationalist areas of Northern Ireland. Many Catholics/nationalists did not trust the official police force—the Royal Ulster Constabulary—and saw it as biased against their community. The RUC found it difficult to operate in certain nationalist neighbourhoods and only entered in armoured convoys, due to the threat of attack from rioters and the IRA. In these neighbourhoods, many residents expected the IRA to act as a policing force, and such policing "provided the IRA a certain propaganda value". The IRA also sought to minimize contact between residents and the RUC, because residents might pass on information or be forced to become a police informer. The IRA set up arbitration panels that would adjudicate and investigate complaints from locals about criminal or 'anti-social' activities. Those responsible for minor offences would be given a warning, be made to compensate the offendee, or be made to do community work. Those responsible for more serious and repeat offences could be given a punishment beating or kneecapping, or be banished from the community.
The IRA's vigilantism has been repeatedly condemned as "summary justice". However, on several occasions the British authorities have recognized the IRA's policing role. In January 1971, the IRA and British Army held secret talks aimed at stopping persistent rioting in Ballymurphy. It was agreed that the IRA would be responsible for policing there, but the agreement was short-lived. During the 1975 ceasefire, the government agreed to the setting up of 'incident centres' in nationalist areas. They were staffed by Sinn Féin members and were to deal with incidents that might endanger the truce. Residents went there to report crime as well as to make complaints about the security forces. The incident centres were seen by locals as 'IRA police stations' and gave some legitimacy to the IRA as a policing force.
The IRA also dealt with sex crimes. The offenders were usually given a punishment shooting and banished under threat of execution. In recent years, there have been complaints over the IRA's handling of sex crimes, especially those by its own members. Political opponents accused the IRA and Sinn Féin of a coverup. The Sinn Féin leader apologized to victims who were "let down" by the IRA and admitted it "was ill-equipped to deal with such matters".
Killing of alleged informers.
The IRA took a hard line against anyone believed to be an "informer" – i.e. secretly passing information about IRA acvitity to British forces. The IRA regarded them as traitors and a threat to the organization and lives of its members. Suspected informers were dealt with by the IRA's Internal Security Unit (ISU). It carried out an investigation, interrogated the suspect and passed judgement. IRA members who confessed to being informers were "executed" with a shot to the head. Civilian informers were regarded as "collaborators" and were usually either killed or exiled. The IRA killed 59 alleged informers, about half of them IRA members and half of them Catholic civilians. The bodies of alleged informers were usually left in public as a warning to others. Twelve, however, were secretly buried and became known as "the Disappeared".
One particularly controversial killing of an alleged informer was that of Jean McConville. A Catholic civilian and widowed mother-of-ten, her body was secretly buried and not found until thirty years later. The IRA has since issued a general apology, saying it "regrets the suffering of all the families whose loved ones were killed and buried by the IRA".
The original IRA, as well as loyalist paramilitaries, also had a policy of killing alleged informers.
Conflict with other republican paramilitaries.
The IRA has also feuded with other republican paramilitary groups such as the Official IRA in the 1970s and the Irish People's Liberation Organisation in the 1990s.
Leading Real Irish Republican Army (RIRA) member Joseph O'Connor was shot dead in Ballymurphy, west Belfast on 11 October 2000. Claims have been made by O'Connor's family and people associated with the RIRA that he was killed by the IRA as the result of a feud between the organisations, but Sinn Féin denied the claims. No-one has been charged with his killing.
Casualties.
"This is a summary. For a detailed breakdown of casualties caused by and inflicted on the IRA see Provisional IRA campaign 1969-1997#Casualties"
The IRA was responsible for more deaths than any other organisation during the Troubles. Two detailed studies of deaths in the Troubles, the Conflict Archive on the Internet (CAIN), and the book "Lost Lives", differ slightly on the numbers killed by the IRA and the total number of conflict deaths. According to CAIN, the IRA was responsible for at least 1,707 deaths, about 48% of the total conflict deaths. Of these, at least 1,009 (about 59%) were members or former members of the British security forces, while at least 508 (about 29%) were civilians. According to "Lost Lives" (2006 edition), the IRA was responsible for 1,768 deaths, about 47% of the total conflict deaths. Of these, 934 (about 52%) were members of the British security forces, while 639 (about 36%) were civilians (including 61 former members of the security forces). The civilian figure also includes civilians employed by British forces, politicians, members of the judiciary, and alleged criminals and informers. Most of the remainder were loyalist or republican paramilitary members; including over 100 IRA members accidentally killed by their own bombs or shot for being security force agents or informers. Overall, the IRA was responsible for 87–90% of the total British security force deaths, and 27–30% of the total civilian deaths in the conflict.
A little under 300 IRA members were killed in the Troubles. In addition, roughly 50–60 members of Sinn Féin were killed. However, many more IRA volunteers were imprisoned than killed. Journalists Eamonn Mallie and Patrick Bishop estimate in their book "The Provisional IRA" that roughly 8,000 people passed through the ranks of the IRA in the first 20 years of its existence, many of them leaving after arrest (senior officers are required to surrender their post after being arrested), retiring from the armed campaign or "disillusionment". They give 10,000 as the total number of past and present IRA members at that time.
Categorisation.
The IRA is a proscribed organisation in the United Kingdom under the Terrorism Act 2000 and an unlawful organisation in the Republic of Ireland under the Offences Against the State Acts. Harold Wilson's secret 1971 meeting with IRA leaders with the help of John O'Connell angered the Irish government; Garret FitzGerald wrote 30 years later that "the strength of the feelings of our democratic leaders ... was not, however, publicly ventilated at the time" because Wilson was a former and possible future British prime minister. Members of IRA are tried in the Republic of Ireland in the Special Criminal Court. In Northern Ireland, the IRA are referred to as terrorists by the Ulster Unionist Party, the Democratic Unionist Party, the Progressive Unionist Party, the Alliance Party of Northern Ireland, and the Social Democratic and Labour Party. On the island of Ireland, the largest political party to state that the IRA is not a terrorist organisation is Sinn Féin. Sinn Féin is widely regarded as the political wing of the IRA, but the party insists that the two organisations are separate.
Peter Mandelson, a former Secretary of State for Northern Ireland, contrasted the post-1997 activities of the IRA with those of Al-Qaeda, describing the latter as "terrorists" and the former as "freedom fighters" (though Mandelson subsequently denied this sentiment). The IRA prefer the terms freedom fighter, soldier, or volunteer. The US Department of State lists them in the category 'other selected terrorist groups also deemed of
relevance in the global war on terrorism.' The organisation has also been described as a "private army" by a number of commentators and politicians.
The IRA described its actions throughout "The Troubles" as a military campaign waged against the British Army, the RUC, other security forces, judiciary, loyalist politicians and loyalist paramilitaries in Northern Ireland, England and Europe. The IRA considers these groups to be all part of the same apparatus. As noted above, the IRA seeks to draw a direct descendancy from the original IRA and those who engaged in the Irish War of Independence. The IRA sees the previous conflict as a guerrilla war which accomplished some of its aims, with some remaining "unfinished business".
A process called "Criminalisation" was begun in the mid-1970s as part of a British strategy of "Criminalisation, Ulsterisation, and Normalisation". The policy was outlined in a 1975 British strategy paper titled "The Way Ahead", which was not published but was referred to by Labour's first Secretary of State for Northern Ireland, Merlyn Rees, and came to be the dominant British political theme in the conflict as it raged into the 1980s.
Another categorisation avoids the terms "guerrilla" or "terrorist" but does view the conflict in military terms. The phrase originated with the British military strategist Frank Kitson who was active in Northern Ireland during the early 1970s. In Kitson's view, the violence of the IRA represented an "insurrection" situation, with the enveloping atmosphere of belligerence representing a "low intensity conflict" – a conflict where the forces involved in fighting operate at a greatly reduced tempo, with fewer combatants, at a reduced range of tactical equipment and limited scope to operate in a military manner.
Membership of the IRA remains illegal in both the UK and the Republic of Ireland, but IRA prisoners convicted of offences committed before 1998 have been granted conditional early release as part of the Good Friday Agreement. In the United Kingdom a person convicted of membership of a "proscribed organisation", such as the IRA, still nominally faces imprisonment for up to 10 years.
Strength and support.
Numerical strength.
In the early to mid-1970s, the numbers recruited by the IRA may have reached several thousand, but these were reduced when the IRA re-organised its structures from 1977 onwards. An RUC report of 1986 estimated that the IRA had 300 or so members in Active Service Units and up to 750 active members in total in Northern Ireland. This does not take into consideration the IRA units in the Republic of Ireland or those in Britain, continental Europe, and throughout the world. In 2005, the then Irish Minister for Justice, Equality and Law Reform, Michael McDowell told the Dáil that the organisation had "between 1,000 and 1,500" active members.
According to the book "The Provisional IRA" (by Eamon Mallie and Patrick Bishop), roughly 8,000 people passed through the ranks of the IRA in the first 20 years of its existence, many of them leaving after arrest, "retirement" or disillusionment. In later years, the IRA's strength has been somewhat weakened by members leaving the organisation to join hardline splinter groups such as the Continuity IRA and the Real IRA. According to former Irish Minister for Justice Michael McDowell, these organisations have little more than 150 members each.
Electoral and popular support.
The popular support for the IRA's campaign in the Troubles is hard to gauge, given that Sinn Féin, the IRA's political wing, did not stand in elections until the early 1980s. Most nationalists in Northern Ireland voted for the moderate Social Democratic and Labour Party (SDLP) until 2001. After the 1981 hunger strike, Sinn Féin mobilised large electoral support and won 105,000 votes, or 43% of the nationalist vote in Northern Ireland, in the United Kingdom general election, 1983, only 34,000 votes behind the SDLP. However, by the 1992 UK General Election, the SDLP won 184,445 votes and four seats to Sinn Féin's 78,291 votes and no seats. In the 1993 Local District Council Elections in Northern Ireland, the SDLP won 136,760 votes to Sinn Féin's 77,600 votes.
Few Protestant voters voted for Sinn Féin. In 1992, many of them voted for SDLP West Belfast candidate Joe Hendron rather than a unionist candidate to make sure Gerry Adams of Sinn Féin lost his seat in the constituency.
The IRA enjoyed some popular support in the Republic of Ireland in the early 70s. However, the movement's appeal was hurt badly by bombings such as the killing of civilians attending a Remembrance Day ceremony at the cenotaph in Enniskillen in 1987 (Remembrance Day bombing), and the death of two children when a bomb exploded in Warrington, which led to tens of thousands of people demonstrating on O'Connell Street in Dublin to call for an end to the IRA's campaign. In the 1987 Irish General Election, they won only 1.7% of the votes cast. They did not make significant electoral gains in the Republic until after the IRA ceasefires and the Good Friday Agreement of 1998. By the 2011 Irish general election Sinn Féin's proportion of the popular vote had reached 9.9 percent.
Sinn Féin now has 29 members of the Northern Ireland Assembly (out of 108), four Westminster MPs (out of 18 from Northern Ireland) and 23 Republic of Ireland TDs (out of 166).
Support from other countries and organisations.
The IRA have had contacts with foreign governments and other illegal armed organisations.
Libya has been the biggest single supplier of arms and funds to the IRA, donating large amounts: three shipments of arms in the early 1970s and another three in the mid-1980s, the latter reputedly enough to arm two regular infantry battalions.
The IRA has also received weapons and logistical support from Irish Americans in the United States. Apart from the Libyan aid, this has been the main source of overseas IRA support. American support has been weakened by the War against Terrorism, and the fallout from the events of 11 September 2001.
In the United States in November 1982, five men were acquitted of smuggling arms to the IRA after they claimed the Central Intelligence Agency had approved the shipment, although the CIA denied this. There are allegations of contact with the East German Stasi, based on the testimony of a Soviet defector to British intelligence Vasili Mitrokhin. Mitrokhin revealed that although the Soviet KGB gave some weapons to the Marxist Official IRA, it had little sympathy with the Provisionals. The IRA has received some training and support from the Palestine Liberation Organization (PLO). In 1977, the Provisionals received a 'sizeable' arms shipment from the PLO, including small arms, rocket launchers and explosives, but this was intercepted at Antwerp after the Israeli intelligence alerted its European counterparts. According to Dr Mir Ali Montazam, one-time first secretary at the Iranian embassy, Iran played a key part in funding the IRA during the 1980s. Iranian officials deposited £4 million into a secret Jersey bank account, funded by the sale of artwork from the Iranian Embassy in London. Hadi Ghaffari, the "machinegun mullah", was sent to Belfast and organised the distribution of the money via sympathetic Irish businessmen.
It has been alleged that the IRA had a co-operative relationship with Basque militant group ETA since the early 1970s. In 1973 it was accused of providing explosives for the assassination of Luis Carrero Blanco in Madrid. In the 1970s, ETA also exchanged a quantity of handguns for training in explosives with the IRA. In addition, the leaders of the political wings of the respective Irish republican and Basque separatist movements have exchanged visits on several occasions to express solidarity with each other's cause. Prominent former IRA prisoners such as Brendan McFarlane and Brendan Hughes have campaigned for the release of ETA prisoners. In the mid-1990s after the IRA ceasefire, Basque media outlets followed the process carefully, sending a team to follow the families of those killed on Bloody Sunday as they campaigned for apology.
In May 1996, the Federal Security Service (FSB), Russia's internal security service, publicly accused Estonia of arms smuggling, and claimed that the IRA had contacted representatives of Estonia's volunteer defence force, Kaitseliit, and some non-government groups to buy weapons. In 2001, three Irish men, who later became known as the Colombia Three, were arrested after allegedly training Colombian guerrillas, the Revolutionary Armed Forces of Colombia (FARC), in bomb making and urban warfare techniques. The US House of Representatives Committee on International Relations in its report of 24 April 2002 concluded "Neither committee investigators nor the Colombians can find credible explanations for the increased, more sophisticated capacity for these specific terror tactics now being employed by the FARC, other than IRA training".
In December 2013 the report of the Smithwick Tribunal concluded that "on the balance of probability" collusion took place between the IRA and members of the Garda Síochána in the 1989 killing of two RUC officers; however, the report could not conclusively prove this.
Informers.
Throughout the Troubles, some members of the IRA passed information to the security forces. Members of the IRA suspected of being informants were usually killed. In the 1980s, many more IRA members were imprisoned on the testimony of former IRA members known as "supergrasses" such as Raymond Gilmour. A Belfast newspaper has claimed that secret documents show that half of the IRA's top men were also British informers.
There have been some high profile allegations of senior IRA figures having been British informers. In May 2003, a number of newspapers named Freddie Scappaticci as the alleged identity of the British Force Research Unit's most senior informer within the IRA, code-named "Stakeknife", who is thought to have been head of the IRA's internal security force, charged with rooting out and executing informers. Scappaticci denies that this is the case and, in 2003, failed in a legal bid to force the then NIO Minister, Jane Kennedy, to state he was not an informer. She has refused to do so, and since then Scappaticci has not launched any libel actions against the media making the allegations.
On 16 December 2005, senior Sinn Féin member Denis Donaldson appeared before TV cameras in Dublin and confessed to being a British spy for twenty years. He was expelled from Sinn Féin and was said to have been debriefed by the party. Donaldson was a former IRA volunteer and subsequently highly placed Sinn Féin party member. Donaldson had been entrusted by Gerry Adams with the running of Sinn Féin's operations in the US in the early 1990s. On 4 April 2006, Donaldson was found shot dead at his retreat near Glenties in County Donegal. When asked whether he felt Donaldson's role as an informer in Sinn Féin was significant, the IRA double agent using the pseudonym "Kevin Fulton" described Donaldson's role as a spy within Sinn Féin as "the tip of the iceberg". The Real IRA claimed responsibility for his assassination on 12 April 2009.
On 8 February 2008, Roy McShane was taken into police protection after being unmasked as an informant. McShane, a former IRA member, had been Gerry Adams' personal driver for many years. Adams said he was "too philosophical" to feel betrayed.

</doc>
<doc id="23300" url="https://en.wikipedia.org/wiki?curid=23300" title="Paula Abdul">
Paula Abdul

Paula Julie Abdul (; born June 19, 1962) is an American singer-songwriter, dancer, choreographer, actress and television personality. She began her career as a cheerleader for the Los Angeles Lakers at the age of 18 before rising to prominence in the 1980s as a highly sought-after choreographer at the height of the music video era. Abdul later scored a string of pop music hits in the late 1980s and early 1990s. Her six number one singles on the "Billboard" Hot 100 tie her with Diana Ross for seventh among the female solo performers who have topped the chart. She won a Grammy for "Best Music Video – Short Form" for "Opposites Attract" and twice won the "Primetime Emmy Award for Outstanding Choreography".
After her initial period of success, Abdul suffered a series of setbacks in her professional and personal life. She saw renewed fame and success as an original judge on "American Idol" in the 2000s, which she left after the eighth season. She went on to star on CBS's short-lived television series "Live to Dance", which lasted one season in 2011, and was subsequently a judge on the first season of the American version of "The X Factor" along with her former "American Idol" co-judge Simon Cowell, the creator and producer of the show. She was also a guest judge on the All-Stars edition of "Dancing with the Stars" in 2012 and the tenth season of "So You Think You Can Dance" in 2013, and more recently became a permanent judge for that series along with the Australian version of the show for its 2014 revival.
Early life.
Abdul was born in San Fernando, California, to Jewish parents. Abdul's father, Harry Abdul, was born into the Syrian Jewish community in Aleppo, Syria, was raised in Brazil, and subsequently emigrated to the United States. Her mother, the concert pianist Lorraine M. Rykiss, grew up in one of the two Jewish families in Minnedosa, Manitoba in Canada, and has Ashkenazi Jewish ancestors from Ukraine. Abdul holds dual citizenship in the United States and Canada. She has a sister named Wendy. In 1980, she graduated from Van Nuys High School.
An avid dancer, Abdul was inspired towards a show business career by Gene Kelly in the classic film "Singin' in the Rain".
Abdul began taking dance lessons at an early age in ballet, jazz, and tap. She attended Van Nuys High School, where she was a cheerleader and an honor student. At 15, she received a scholarship to a dance camp near Palm Springs, and in 1980 appeared in a low-budget Independent musical film, "Junior High School".
Abdul studied broadcasting at the California State University, Northridge. During her freshman year, she was selected from a pool of 700 candidates for the cheerleading squad of the Los Angeles Lakers NBA basketball team—the famed Laker Girls. Within a year, she became head choreographer.
Career.
1982–1986: Dance and choreography era.
Abdul was discovered by The Jacksons, after a few of the band members had watched her while attending a Los Angeles Lakers game. She was signed to do the choreography for the video to their single "Torture". "My only problem was how to tell the Jacksons how to dance," Abdul later recalled. "Imagine me telling them what routines to do. I was young, I was scared. I'm not quite sure how I got through that." The success of the choreography in the video led to Abdul's career of choreographer in music videos. It was also due to the success of the video that Abdul was chosen to be the choreographer for the Jacksons' "Victory" tour.
Abdul choreographed videos for several singers throughout the 1980s, including many videos for Janet Jackson during her "Control" era. In 1995, Abdul released a dance workout video entitled "Paula Abdul's Get Up and Dance!" (re-released on DVD in 2003), a fast-paced, hip-hop style workout. In 1998, she released a second video called "Cardio Dance" (re-released on DVD in 2000). In December 2005, Abdul launched a cheerleading/fitness/dance DVD series called "Cardio Cheer," which is marketed to children and teenage girls involved with cheerleading and dance.
In film, Abdul choreographed sequences for the giant keyboard scene involving Tom Hanks's character in "Big". Other credits include appearances in "Coming to America", "Action Jackson", "Jerry Maguire", "The Running Man", "American Beauty", "Can't Buy Me Love", and Oliver Stone's "The Doors". Her television appearances include "The Tracey Ullman Show," American Music Awards, the Academy Awards, and several commercials, such as The King's touchdown celebration, as seen in a string of popular Burger King television commercials that aired during the 2005–06 NFL season.
1987–1994: "Forever Your Girl" and "Spellbound".
In 1987, Abdul used her savings to make a singing demo. Although she had appeared in the musical film "Junior High School" years earlier, her singing voice was relatively untrained, but her exceptional dancing proved marketable to the visually oriented, MTV-driven, pop music industry. She was a tireless worker, and relied on input from her vocal coach and producers to improve her sound.
In 1988, Abdul released her pop debut album, "Forever Your Girl". The album took 64 weeks to hit No. 1 on the "Billboard" 200 album sales chart—the longest an album has been on the market before hitting No. 1—and spent 10 weeks there. The album eventually became multi-platinum in the spring and summer of 1989, and it spawned five American Top Three singles, four of them No. 1s (three in 1989 and one in 1990): "Straight Up", "Forever Your Girl", "Cold Hearted", and "Opposites Attract". A remix album, "", was also released and reached No. 7 on Billboard's album chart, becoming one of the most successful remix albums to date. The Grammy award-winning video for "Opposites Attract" featured an animated cat named MC Skat Kat.
At the 33rd Grammy Awards, Abdul won her first Grammy for Best Music Video for "Opposites Attract", She was also nominated for Best Female Pop Vocal Performance for her song "Straight Up" but lost out to Bonnie Raitt's "Nick of Time".
In early 1991, Yvette Marine, backing vocalist on "Forever Your Girl", claimed that she sang "co-lead vocals" on the album and sued Abdul and Virgin Records for compensation. After one month of court proceedings, Abdul and Virgin won the case.
Abdul's follow-up album, 1991's "Spellbound," contained another string of hits, and sold 7 million copies worldwide. The first single from "Spellbound" was the ballad, "Rush, Rush", which topped the "Billboard" Hot 100 chart for five consecutive weeks, and was noted for its music video and "Rebel Without a Cause" motif featuring Keanu Reeves in the James Dean role. "Promise of a New Day", the second release from the album, also hit No. 1, and was followed by the Top 10 hit "Blowing Kisses in the Wind" and two Top 20 hits: "Vibeology" and "Will You Marry Me?". Many of these songs were written by Peter Lord, Jeffrey Smith and Sandra St. Victor, who were members of The Family Stand. The album, "Spellbound", retained much of the dance-oriented formula heard on her debut album. The track "U" was written for Abdul by Prince.
Abdul promoted the album through the "Under My Spell Tour", which was named by an MTV contest for fans. This tour was nearly cancelled due to an accident during rehearsals. The tour began on schedule and ran from October 1991 to the summer of 1992. In 1991, Abdul embraced advertising and starred in a popular Diet Coke commercial in which she danced with a digital image of her idol, a young Gene Kelly.
Abdul was honoured with a star on the Hollywood Walk of Fame in December 1991.
1995–2001: "Head over Heels".
By 1995, Paula Abdul had recovered from her battle with the eating disorder bulimia nervosa and prepared to return to the spotlight with her new album "Head over Heels". The first single off the album, "My Love Is for Real" featured a fusion of R&B and traditional Middle Eastern instruments, and was performed with Yemeni-Israeli singer Ofra Haza. Its accompanying "Lawrence of Arabia"-inspired music video was played in theaters across the world as a prologue to the film "Clueless". The single was a hit in the clubs (peaking at No. 1 on Billboard's Hot Dance Music/Club Play chart), and made it to No. 28 on Billboard's Hot 100 chart.
The second single, "Crazy Cool", peaked at No. 13 on the dance charts. "Ain't Never Gonna Give You Up" was the album's third and final single. To date, "Head over Heels" has sold 3 million copies worldwide.
In January 1997, Abdul starred in the ABC television movie "Touched By Evil", playing a businesswoman who discovers that her boyfriend is a serial rapist. Also that year, Abdul co-wrote the song "Spinning Around" with songwriter and producer Kara DioGuardi, who became a fellow judge on "American Idol" in 2009. "Spinning Around" was a dance-pop track intended to be the lead single on Abdul's follow-up album to "Head over Heels". But the album never materialized, and "Spinning Around" was instead given to Kylie Minogue. The song was highly successful, reaching No. 1 in numerous countries.
While Abdul took a break from the music industry, she remained busy behind the scenes. Abdul served as the choreographer for several film and theater productions, including the 1998 musical "Reefer Madness" and the cheerleading scenes in the 1999 film "American Beauty" (she had previously also choreographed the 1991 film "The Doors"). Abdul also co-produced a 2001 pilot episode of "Skirts", a dramatic series that would have aired on MTV about a high-school cheerleading squad; Abdul was also set to appear as the head coach. The pilot never aired.
In 2000, Abdul's "" CD was released by Virgin Records (with whom Abdul was already no longer affiliated). It included all her hit singles and other noteworthy tracks. The song "Bend Time Back 'Round" had previously been heard only on the 1992 soundtrack for the hit television series "Beverly Hills, 90210".
2002–2009: "American Idol".
In 2002, Abdul appeared as one of three judges for the reality television music competition show "American Idol". Abdul, along with fellow judges Simon Cowell and Randy Jackson, evaluated thousands of amateur contestants in their ability to sing. Abdul won praise as a sympathetic and compassionate judge. She seemed especially kind compared to fellow judge Simon Cowell, who was often blunt in his appraisals of the contestants' performances. When she realized that Cowell's over-the-top judging style was heartbreaking for many young contestants, Abdul was so horrified that she considered leaving the show. Although their differences often resulted in extremely heated on-air exchanges and confrontations, Cowell says he played a major role in convincing Abdul not to leave the show.
While serving as a judge on "American Idol", Abdul accepted a second assignment as reporter for "Entertainment Tonight".
In March 2006, Fox announced that Abdul had signed to stay on "American Idol" as a judge for at least three more years. Later that year, fellow "American Idol" judge Simon Cowell invited her to be a guest judge at some of the early auditions for the third series of his similar UK talent show "The X Factor". Abdul was present at the initial audition of the eventual winner, Leona Lewis.
Paula Abdul's second greatest-hits CD, "", was released by Virgin Records on May 8, 2007. Virgin Records also released the music videos to all six of Abdul's No. 1 singles to iTunes. Meanwhile, Bravo began airing the reality television series "Hey Paula," which followed Abdul through her day-to-day life. The series was produced by Scott Sternberg Productions and debuted on June 28, 2007. Abdul's behavior as depicted on the show was described as "erratic" by comedian Rosie O'Donnell and decried by numerous fans and critics. The show aired for only one season.
In 2007, Paula Abdul Jewelry launched its nationwide consumer debut on QVC, with the tagline "fashion jewelry designed with heart and soul." Abdul's first QVC appearance resulted in 15 sellouts of her first jewelry collection involving more than 34,000 pieces.
In January 2008, Abdul returned to the music charts for the first time in nearly 13 years with the single "Dance Like There's No Tomorrow," the first track on the album "Randy Jackson's Music Club Vol 1". The song debuted on "On Air with Ryan Seacrest"., and Abdul performed it during the pre-game show for Super Bowl XLII. "Dance Like There's No Tomorrow" was a modest comeback hit for Abdul, peaking at No. 62 on the "Billboard" Hot 100, No. 11 on iTunes and No. 2 on the Billboard Hot Dance Club Play chart. Abdul has reported other songs that she is working on such as, "Boom Box." Abdul also made a brief guest appearance in season 3 episode 1 of the British television Comedy-Drama "Hotel Babylon", which aired in the United Kingdom on February 19, 2008.
In February 2008, it was reported she was to be working on a new album.
In January 2009, Abdul hosted "RAH!," a 90-minute cheerleading competition on MTV. "RAH!" featured five collegiate squads competing in a series of challenges with Abdul crowning one the winner. In May 2009, Abdul debuted her latest song "I'm Just Here for the Music" (originally an unreleased song from Kylie Minogue's ninth album Body Language) on the Ryan Seacrest Radio KIIS-FM show and performed the single on the "American Idol" results show. "I'm Just Here for the Music" reached No. 87 on the "Billboard" Hot 100, becoming Abdul's 15th song to appear on the chart.
In an interview with the "Los Angeles Times" on July 18, 2009, Abdul's manager David Sonenberg told the newspaper that, "Very sadly, it does not appear that she's going to be back on 'Idol'." This came about as a result of stalled negotiations between Abdul and the show. On August 4, 2009, after numerous contract negotiations, Abdul confirmed that she would not be returning to "Idol" for its ninth season. "The Times" cited reports Abdul had been earning as much as $5 million per season and that she was reportedly seeking as much as $20 million to return. Abdul was replaced by Ellen DeGeneres.
On August 18, 2009, it was reported that Abdul was negotiating to return to "Idol" after not taking part in season nine of "Dancing with the Stars". Two days later, Abdul's manager said that there were not any talks with Fox, but they were not ruling anything out.
Abdul claimed her departure from "Idol" was not about money, but that she had to stand on principle.
2010–present: "Live to Dance", "The X Factor" and "So You Think You Can Dance Australia & America".
In January 2010, Abdul presented a Lifetime Achievement Award to choreographer Julie McDonald at the 11th Anniversary show of . In November, Abdul launched and co-founded AuditionBooth.com, a website that allows aspiring talents to connect with casting directors, producers, and managers.
Abdul kicked off 2011 by serving as lead judge, executive producer, creative partner, mentor and coach on CBS' new dancing competition, "Live to Dance" (formerly "Got to Dance") Abdul said that unlike "American Idol", her new show is less about "competition" and more about "celebration." After its first season of seven weekly shows, it was canceled by CBS.
On May 8, 2011, it was announced that Abdul would rejoin Simon Cowell on the first season of the American version of "The X Factor". The judging panel consisted of Abdul, Cowell, music executive L.A. Reid and Girls Aloud singer Cheryl Cole. However, Cole was axed after two audition cites and replaced by Pussycat Dolls lead singer Nicole Scherzinger who served as co-host alongside Steve Jones. The series premièred on September 21, 2011. She was the mentor for the "Groups" category. She was called by the show's producers about the news of her category whilst at home in Los Angeles, California. During the Judges' Houses stage of the competition, Abdul was aided by guest judge Pharrell Williams in Santa Barbara, California. Abdul's contestants were all eliminated from the competition, her final act "Lakoda Rayne" were eliminated by the public vote on week five of the Live Shows. They were the Groups category's most successful act.
In January 2012, Abdul announced that she would not be returning as a judge for the show's second season. Fellow season one judge Nicole Scherzinger and host Steve Jones were also axed from the show that month. Abdul was replaced by Demi Lovato. On October 15, 2012, Abdul served as a guest judge during week four of the "All-Stars version" of "Dancing with the Stars". In week seven, she performed the "Dream Medley", a compilation of four of her No. 1 singles: "Straight Up", "Cold Hearted", "Forever Your Girl" and "Opposites Attract". The medley was a re-recording with live instruments. On April 18, 2013, Abdul appeared on the Top 5 results show of season 12 of "American Idol" to compliment contestant Candice Glover on her performance of Straight Up.
In January 30, 2013, The Carnival: Choreographers Ball 14th Year Anniversary, Abdul was honored with Lifetime Achievement Award for her contributions and works in the choreography world.
On July 9, 2013, Abdul was a guest judge on "So You Think You Can Dance" (season ten). In October 2013, Abdul was named as a judge on the revamped "So You Think You Can Dance Australia", which aired on Australia's Network Ten from February 9 through May 1, 2014. Abdul was present as a permanent member of the judge's panel for all episodes of this season. Details about the likelihood of the show's return for a fifth season, and of Abdul's continued involvement with it, have yet to be released. She later became a permanent judge on the twelfth season of the American version.
In April 2014, Abdul was a guest judge on "RuPaul's Drag Race" (season 6). The episode put Abdul face to face with previous Idol competitor, Adore Delano. In June 2014, Abdul made a cameo appearance on the Australian soap opera "Neighbours" and shared scenes with established character Karl Kennedy (Alan Fletcher).
On 2015 Abdul made a guest appearance on the comedy sitcom Real Husbands of Hollywood on its Season 4 premiere. She was shown having a conversation with Arsenio Hall. On the show Abdul was trying to kick Hall out of her pool house. The duo made a comedic scene.
In November 16, 2015, Abdul along with Charles "Chucky" Klapow, Renee Richie and Nakul Dev Mahajan won the World Choreography Award for Outstanding Choreography Digital Format for the video 'Check Yourself'.
In November 22, 2015, Paula Abdul and Donnie Wahlberg presented Favorite Female Artist – Pop/Rock at the 43rd American Music Awards; the award Abdul won in 1990 AMA's, presented to her by Wahlberg.
Personal life.
Marriages and relationships.
Abdul was married to Emilio Estevez from 1992 to 1994. She married clothing designer Brad Beckerman in 1996 at the New England Carousel Museum in Bristol, Connecticut. They divorced in 1998, citing irreconcilable differences.
On Valentine's Day 2006, Abdul appeared on "Dr. Phil" as part of a prime time special on love and relationships. She was set up on two dates, and Phil McGraw gave her advice.
In mid-July 2007, Abdul announced that she had begun dating J. T. Torregiani, a restaurant owner 12 years her junior. She told "Access Hollywood": "He is a good guy. Things are looking upwards. It's looking good right now. I wasn't even looking for someone and that's what usually happens." Abdul and Torregiani broke up in June 2008, citing their hectic work schedules.
Religion.
Abdul is observant in her Jewish faith, and is proud of her heritage. She once stated, "My father is a Syrian Jew whose family immigrated to Brazil. My mother is Canadian with Jewish roots. My dream is to go to Israel for a real holiday." In November 2006, Israeli Tourist Minister Isaac Herzog invited her to Israel, Abdul responding with a hug, adding, "I will come; you have helped me make a dream come true." In 2013, at the age of 51, Abdul had her bat mitzvah in Safed, Israel, at a Kabbalah center.
In 2003, Abdul was reported as a practitioner of Transcendental Meditation.
Legal issues.
On December 20, 2004, Abdul was driving her Mercedes on a Los Angeles-area freeway when she changed lanes and hit another vehicle. The driver and passenger took a photograph with a cell phone camera and wrote down the license plate number of the car, which was traced to Abdul. On March 24, 2005, Abdul was fined US$900 and given 24 months of informal probation after pleading no contest to misdemeanor hit-and-run driving. She was ordered to pay US$775 for damage to the other car.
On April 4, 2006, Abdul filed a report at a Hollywood police station claiming she had been a victim of battery at a private party at about 1am on April 2, according to LAPD spokesman Lt. Paul Vernon. "According to Abdul, the man at the party argued with her, grabbed her by the arm and threw her against a wall," Vernon said. "She said she had sustained a concussion and spinal injuries."
Health.
In April 2005, Abdul said that she suffers from a neurological disorder, reflex sympathetic dystrophy, that causes chronic pain.
Activism.
Abdul is a dog lover who raised awareness about National Guide Dog Month in May 2009, and she teamed up with Dick Van Patten to help people with blindness to have more independence through the help of guide dogs. She does not wear real fur.
Controversies.
Corey Clark.
In May 2005, ABC's news magazine "Primetime Live" reported claims by season two "American Idol" contestant Corey Clark that he and Abdul had an affair during that season, and that she had coached him on how to succeed in the competition. The fact that Clark came forward at a time when he was marketing a CD and trying to get a book deal was seen as suspicious by some, but Clark maintains that his career was being prejudiced because of his relationship with Abdul and that is why he came forward with the information to clear his name. For the most part, Abdul refused to comment on Clark's allegations. Simon Cowell came to Abdul's defense, calling Corey Clark a creep and stating "It was just somebody using her to get a lot of publicity for an appalling record, full stop."
At the height of the débâcle, Abdul appeared in a "Saturday Night Live" skit, making light of the situation. While Fox launched an investigation, Abdul received numerous calls of support from celebrities, including Oprah Winfrey and Kelly Ripa; Barbara Walters addressed the camera during an episode of ABC's "The View" to say she was sad to be part of an operation that would report Clark's flimsy tabloid claims under the guise of a news story. In August 2005, the Fox network confirmed that she would be returning to the show, as the investigation had found "insufficient evidence that the communications between Mr. Clark and Ms. Abdul in any way aided his performance."
Substance abuse allegations.
Substance abuse allegations arose as the result of what some described as "erratic behavior" by Abdul during episodes of "American Idol." After reading these allegations on message boards, Abdul told "People" in April 2005 that she suffered from chronic pain for years following a "cheerleading accident" at age 17 and was diagnosed with reflex sympathetic dystrophy (RSD) in November 2004. Abdul says she is now pain-free following treatment, including the anti-inflammatory medication Enbrel.
Allegations arose again in January 2007 when videos circulated on the Internet of Abdul appearing to sway in her chair and slur her speech during a set of interviews. Abdul's publicist attributed this to fatigue and technical difficulties during the recording of the interviews. It was revealed on the Bravo show "Hey Paula", which had followed Abdul with a video camera prior to the interviews, that Abdul had not been sleeping, perhaps suffering from some mild form of insomnia.
In February 2007, Abdul told "Us Weekly" that she had never been drunk or used illegal drugs and called the allegations "lies".
In a March 2007 appearance on the "Late Show with David Letterman", Abdul joked that her scrutinized behavior was caused by her being "abducted by aliens".
In several interviews given in the late 2000s, Abdul said she had been left in debilitating pain after a 1992 car accident and a 1993 plane crash that required 15 spinal surgeries and which left her dependent on pain medication for years. However, there are multiple inconsistencies in this story; there is no record of any such plane crash having occurred according to NTSB records; nor are there any references to her being in a plane crash in any media at the time.
In May 2009, "Ladies' Home Journal" posted an article on its website that said that Abdul told them she stayed at the La Costa Resort and Spa in Carlsbad, California for three days the previous year to recover from physical dependence on prescription pain medications. The medications, prescribed due to injuries and her RSD diagnosis, included a pain patch, nerve medication, and a muscle relaxant. According to the article, Abdul said the medications made her "get weird" at times and that she suffered from physical withdrawal symptoms during her recovery.
Later that same week, in an interview with Detroit radio station WKQI, Abdul rejected the article's accuracy. She told the radio station she never checked into a rehab clinic and never had a drug abuse problem.

</doc>
<doc id="23301" url="https://en.wikipedia.org/wiki?curid=23301" title="Project Gutenberg">
Project Gutenberg

Project Gutenberg (PG) is a volunteer effort to digitize and archive cultural works, to "encourage the creation and distribution of eBooks". It was founded in 1971 by Michael S. Hart and is the oldest digital library. Most of the items in its collection are the full texts of public domain books. The project tries to make these as free as possible, in long-lasting, open formats that can be used on almost any computer. , Project Gutenberg reached 50,000 items in its collection.
The releases are available in plain text but, wherever possible, other formats are included, such as HTML, PDF, EPUB, MOBI, and Plucker. Most releases are in the English language, but many non-English works are also available. There are multiple affiliated projects that are providing additional content, including regional and language-specific works. Project Gutenberg is also closely affiliated with Distributed Proofreaders, an Internet-based community for proofreading scanned texts.
History.
Project Gutenberg was started by Michael Hart in 1971 with the digitization of the United States Declaration of Independence. Hart, a student at the University of Illinois, obtained access to a Xerox Sigma V mainframe computer in the university's Materials Research Lab. Through friendly operators, he received an account with a virtually unlimited amount of computer time; its value at that time has since been variously estimated at $100,000 or $100,000,000. Hart has said he wanted to "give back" this gift by doing something that could be considered to be of great value. His initial goal was to make the 10,000 most consulted books available to the public at little or no charge, and to do so by the end of the 20th century.
This particular computer was one of the 15 nodes on ARPANET, the computer network that would become the Internet. Hart believed that computers would one day be accessible to the general public and decided to make works of literature available in electronic form for free. He used a copy of the United States Declaration of Independence in his backpack, and this became the first Project Gutenberg e-text.
He named the project after Johannes Gutenberg, the fifteenth century German printer who propelled the movable type printing press revolution.
By the mid-1990s, Hart was running Project Gutenberg from Illinois Benedictine College. More volunteers had joined the effort. All of the text was entered manually until 1989 when image scanners and optical character recognition software improved and became more widely available, which made book scanning more feasible. Hart later came to an arrangement with Carnegie Mellon University, which agreed to administer Project Gutenberg's finances. As the volume of e-texts increased, volunteers began to take over the project's day-to-day operations that Hart had run.
Starting in 2004, an improved online catalog made Project Gutenberg content easier to browse, access and hyperlink. Project Gutenberg is now hosted by ibiblio at the University of North Carolina at Chapel Hill.
Italian volunteer Pietro Di Miceli developed and administered the first Project Gutenberg website and started the development of the Project online Catalog. In his ten years in this role (1994–2004), the Project web pages won a number of awards, often being featured in "best of the Web" listings, and contributing to the project's popularity.
Hart died on 6 September 2011 at his home in Urbana, Illinois at the age of 64.
Affiliated organizations.
In 2000, a non-profit corporation, the Project Gutenberg Literary Archive Foundation, Inc. was chartered in Mississippi to handle the project's legal needs. Donations to it are tax-deductible. Long-time Project Gutenberg volunteer Gregory Newby became the foundation's first CEO.
Also in 2000, Charles Franks founded Distributed Proofreaders (DP), which allowed the proofreading of scanned texts to be distributed among many volunteers over the Internet. This effort greatly increased the number and variety of texts being added to Project Gutenberg, as well as making it easier for new volunteers to start contributing. DP became officially affiliated with Project Gutenberg in 2002. , the 10,000+ DP-contributed books comprised almost a third of the nearly books in Project Gutenberg.
Kindle Store Controversy.
There have been many instances of Gutenberg books being sold for profit in the Kindle Store, one being the reselling of the 1906 book "Fox Trapping". The books may not have been altered save the stripping of the Project Gutenberg ("PG") terms & conditions, which is specified by PG when content is used elsewhere, and possibly slight formatting changes. There is no legal impediment to the reselling of works in the public domain, but there exists a debate as to the appropriateness of simply reusing content that is created by volunteers, without sufficient modification. The debate questions sellers who take the PG editions without reformatting to include a linked table of contents, or without repackaging, re-editing, or reinterpretation of the data. Factors in the debate include whether or not the features of the Amazon platform render it accessible to a larger community of readers on a greater variety of devices, or if the users of the Amazon platform are essentially "locked in" by the Kindle's platform-specific content.
CD and DVD project.
In August 2003, Project Gutenberg created a CD containing approximately 600 of the "best" e-books from the collection. The CD is available for download as an ISO image. When users are unable to download the CD, they can request to have a copy sent to them, free of charge.
In December 2003, a DVD was created containing nearly 10,000 items. At the time, this almost represented the entire collection. In early 2004, the DVD also became available by mail.
In July 2007, a new edition of the DVD was released containing over 17,000 books, and in April 2010, a dual-layer DVD was released, containing nearly 30,000 items.
The majority of the DVDs, and all of the CDs mailed by the project, were recorded on recordable media by volunteers. However, the new dual layer DVDs were manufactured, as it proved more economical than having volunteers burn them. , the project has mailed approximately 40,000 discs.
Scope of collection.
, Project Gutenberg claimed over items in its collection, with an average of over fifty new e-books being added each week. These are primarily works of literature from the Western cultural tradition. In addition to literature such as novels, poetry, short stories and drama, Project Gutenberg also has cookbooks, reference works and issues of periodicals. The Project Gutenberg collection also has a few non-text items such as audio files and music notation files.
Most releases are in English, but there are also significant numbers in many other languages. , the non-English languages most represented are: French, German, Finnish, Dutch, Italian, and Portuguese.
Whenever possible, Gutenberg releases are available in plain text, mainly using US-ASCII character encoding but frequently extended to ISO-8859-1 (needed to represent accented characters in French and Scharfes s in German, for example). Besides being copyright-free, the requirement for a Latin (character set) text version of the release has been a criterion of Michael Hart's since the founding of Project Gutenberg, as he believes this is the format most likely to be readable in the extended future. Out of necessity, this criterion has had to be extended further for the sizable collection of texts in East Asian languages such as Chinese and Japanese now in the collection, where UTF-8 is used instead.
Other formats may be released as well when submitted by volunteers. The most common non-ASCII format is HTML, which allows markup and illustrations to be included. Some project members and users have requested more advanced formats, believing them to be much easier to read. But some formats that are not easily editable, such as PDF, are generally not considered to fit in with the goals of Project Gutenberg. Also Project Gutenberg has two options for master formats which can be submitted (from which all other files are generated), customized versions of the Text Encoding Initiative standard since 2005, and reStructuredText, since 2011.
Beginning in 2009 the Project Gutenberg catalog began offering auto-generated alternate file formats, including HTML (when not already provided), EPUB and plucker.
Ideals.
Michael Hart said in 2004, "The mission of Project Gutenberg is simple: 'To encourage the creation and distribution of ebooks'". His goal was, "to provide as many e-books in as many formats as possible for the entire world to read in as many languages as possible". Likewise, a project slogan is to "break down the bars of ignorance and illiteracy", because its volunteers aim to continue spreading public literacy and appreciation for the literary heritage just as public libraries began to do in the late 19th century.
Project Gutenberg is intentionally decentralized. For example, there is no selection policy dictating what texts to add. Instead, individual volunteers work on what they are interested in, or have available. The Project Gutenberg collection is intended to preserve items for the long term, so they cannot be lost by any one localized accident. In an effort to ensure this, the entire collection is backed-up regularly and mirrored on servers in many different locations.
Copyright.
Project Gutenberg is careful to verify the status of its ebooks according to U.S. copyright law. Material is added to the Project Gutenberg archive only after it has received a copyright clearance, and records of these clearances are saved for future reference. Project Gutenberg does not claim new copyright on titles it publishes. Instead, it encourages their free reproduction and distribution.
Most books in the Project Gutenberg collection are distributed as public domain under U.S. copyright law. The licensing included with each ebook puts some restrictions on what can be done with the texts (such as distributing them in modified form, or for commercial purposes) as long as the Project Gutenberg trademark is used. If the header is stripped and the trademark not used, then the public domain texts can be reused without any restrictions.
There are also a few copyrighted texts, like of science fiction author Cory Doctorow, that Project Gutenberg distributes with permission. These are subject to further restrictions as specified by the copyright holder, although they generally tend to be licensed under Creative Commons.
Criticism.
The text files use the legacy format of plain ASCII, wrapped at 65–70 characters, with paragraphs separated by a double-line break. In recent decades the resulting relatively bland appearance and the lack of a markup possibility have often been perceived as a drawback of this format.
Project Gutenberg attempts to address this by making many texts available in HTML, ePub, and PDF versions as well, but faithful to the mission of offering data which is easy to handle with computer code, plain ASCII text remains the most important format, and the ePub version still contains extra line breaks between paragraphs.
In December 1994, Project Gutenberg was criticized by the Text Encoding Initiative for failing to include apparatus (documentation) of the decisions unavoidable in preparing a text, or in some cases, documenting which of several (conflicting) versions of a text has been the one digitized.
The selection of works (and editions) available has been determined by popularity, ease of scanning, being out of copyright, and other factors; this would be difficult to avoid in any crowd-sourced project.
In March 2004, a new initiative was begun by Michael Hart and John S. Guagliardo to provide low-cost intellectual properties. The initial name for this project was "Project Gutenberg 2" (PG II), which created controversy among PG volunteers because of the re-use of the project's trademarked name for a commercial venture.
Affiliated projects.
All affiliated projects are independent organizations which share the same ideals, and have been given permission to use the "Project Gutenberg" trademark. They often have a particular national, or linguistic focus.

</doc>
<doc id="23303" url="https://en.wikipedia.org/wiki?curid=23303" title="Personal area network">
Personal area network

A personal area network (PAN) is a computer network used for data transmission amongst devices such as computers, telephones, tablets and personal digital assistants. PANs can be used for communication amongst the personal devices themselves (interpersonal communication), or for connecting to a higher level network and the Internet (an uplink) where one "master" device takes up the role as internet router.
A wireless personal area network (WPAN) is a low-powered PAN carried over a short-distance wireless network technology such as:
The reach of a WPAN varies from a few centimeters to a few meters. A PAN may also be carried over wired computer buses such as USB and FireWire.
Although a (secured) Wi-Fi tethering connection could be used by only one single user it is not considered to be a PAN.
Wired PAN connection.
The data cable is an example of the above PAN. This is also a Personal Area Network because that connection is for the users personal use. PAN is used for personal use only.
Wireless Personal Area Network.
A wireless personal area network (WPAN) is a personal area network — a network for interconnecting devices centered on an individual person's workspace — in which the connections are wireless. Wireless PAN is based on the standard IEEE 802.15. The two kinds of wireless technologies used for WPAN are Bluetooth and Infrared Data Association.
A WPAN could serve to interconnect all the ordinary computing and communicating devices that many people have on their desk or carry with them today; or it could serve a more specialized purpose such as allowing the surgeon and other team members to communicate during an operation.
A key concept in WPAN technology is known as "plugging in". In the ideal scenario, when any two WPAN-equipped devices come into close proximity (within several meters of each other) or within a few kilometers of a central server, they can communicate as if connected by a cable. Another important feature is the ability of each device to lock out other devices selectively, preventing needless interference or unauthorized access to information.
The technology for WPANs is in its infancy and is undergoing rapid development. Proposed operating frequencies are around 2.4 GHz in digital modes. The objective is to facilitate seamless operation among home or business devices and systems. Every device in a WPAN will be able to plug into any other device in the same WPAN, provided they are within physical range of one another. In addition, WPANs worldwide will be interconnected. Thus, for example, an archeologist on site in Greece might use a PDA to directly access databases at the University of Minnesota in Minneapolis, and to transmit findings to that database.
Bluetooth.
Bluetooth uses short-range radio waves over distances up to approximately 10 metres. For example, Bluetooth devices such as a keyboards, pointing devices, audio head sets, printers may connect to personal digital assistants (PDAs), cell phones, or computers wirelessly.
A Bluetooth PAN is also called a "piconet" (combination of the prefix "pico," meaning very small or one trillionth, and network), and is composed of up to 8 active devices in a master-slave relationship (a very large number of devices can be connected in "parked" mode). The first Bluetooth device in the piconet is the master, and all other devices are slaves that communicate with the master. A piconet typically has a range of , although ranges of up to can be reached under ideal circumstances.
Infrared Data Association.
Infrared Data Association (IrDA) uses infrared light, which has a frequency below the human eye's sensitivity. Infrared in general is used, for instance, in TV remotes. Typical WPAN devices that use IrDA include printers, keyboards, and other serial data interfaces.

</doc>
<doc id="23304" url="https://en.wikipedia.org/wiki?curid=23304" title="Personal digital assistant">
Personal digital assistant

A personal digital assistant (PDA), also known as a handheld PC, or personal data assistant, is a mobile device that functions as a personal information manager. The term evolved from Personal Desktop Assistant, a software term for an application that prompts or prods the user of a computer with suggestions or provides quick reference to contacts and other lists. PDAs were largely discontinued in the early 2010s after the widespread adoption of highly capable smartphones, in particular those based on iOS and Android.
Nearly all PDAs have the ability to connect to the Internet. A PDA has an electronic visual display, enabling it to include a web browser, all models also have audio capabilities enabling use as a portable media player, and also enabling most of them to be used as mobile phones. Most PDAs can access the Internet, intranets or extranets via Wi-Fi or Wireless Wide Area Networks. Most PDAs employ touchscreen technology.
The first PDA was released in 1984 by Psion, the Organizer. Followed by Psion's Series 3, in 1991, which began to resemble the more familiar PDA style. It also had a full keyboard. The term "PDA" was first used on January 7, 1992 by Apple Computer CEO John Sculley at the Consumer Electronics Show in Las Vegas, Nevada, referring to the Apple Newton. In 1994, IBM introduced the first PDA with full mobile phone functionality, the IBM Simon, which can also be considered the first smartphone. Then in 1996, Nokia introduced a PDA with full mobile phone functionality, the 9000 Communicator, which became the world's best-selling PDA. The Communicator spawned a new category of PDAs: the "PDA phone", now called "smartphone". Another early entrant in this market was Palm, with a line of PDA products which began in March 1996. The terms "personal digital assistant" and "PDA" apply to smartphones but are not used in marketing, media, or general conversation to refer to devices such as the BlackBerry, iPad, iPhone or Android devices.
Typical features.
A typical PDA has a touchscreen for entering data, a memory card slot for data storage, and IrDA, Bluetooth and/or Wi-Fi. However, some PDAs may not have a touch screen, using softkeys, a directional pad, and a numeric keypad or a thumb keyboard for input; this is typically seen on telephones that are incidentally PDAs.
In order to have the functions expected of a PDA, a device's software typically includes an appointment calendar, a to-do list, an address book for contacts, a calculator, and some sort of memo (or "note") program. PDAs with wireless data connections also typically include an email client and a Web browser.
Touch screen.
Many of the original PDAs, such as the Apple Newton and Palm Pilot, featured a touchscreen for user interaction, having only a few buttons—usually reserved for shortcuts to often-used programs. Some touchscreen PDAs, including Windows Mobile devices, had a detachable stylus to facilitate making selections. The user interacts with the device by tapping the screen to select buttons or issue commands, or by dragging a finger (or the stylus) on the screen to make selections or scroll.
Typical methods of entering text on touchscreen PDAs include:
Despite rigorous research and development projects, end-users experience mixed results with handwriting recognition systems. Some find it frustrating and inaccurate, while others are satisfied with the quality of the recognition.
Touchscreen PDAs intended for business use, such as the BlackBerry and Palm Treo, usually also offer full keyboards and scroll wheels or thumbwheels to facilitate data entry and navigation.
Many touchscreen PDAs support some form of external keyboard as well. Specialized folding keyboards, which offer a full-sized keyboard but collapse into a compact size for transport, are available for many models. External keyboards may attach to the PDA directly, using a cable, or may use wireless technology such as infrared or Bluetooth to connect to the PDA.
Newer PDAs, such as the HTC HD2, Apple iPhone, Apple iPod Touch, and Palm Pre, Palm Pre Plus, Palm Pixi, Palm Pixi Plus, Google Android (operating system) include more advanced forms of touchscreen that can register multiple touches simultaneously. These "multi-touch" displays allow for more sophisticated interfaces using various gestures entered with one or more fingers.
Memory cards.
Although many early PDAs did not have memory card slots, now most have either some form of Secure Digital (SD) slot, a CompactFlash slot or a combination of the two. Although designed for memory, Secure Digital Input/Output (SDIO) and CompactFlash cards are available that provide accessories like Wi-Fi or digital cameras, if the device can support them. Some PDAs also have a USB port, mainly for USB flash drives. Some PDAs use microSD cards, which are electronically compatible with SD cards, but have a much smaller physical size.
Wired connectivity.
While early PDAs connected to a user's personal computer via serial ports or another proprietary connection, many today connect via a USB cable. Older PDAs were unable to connect to each other via USB, as their implementations of USB didn't support acting as the "host".
Some early PDAs were able to connect to the Internet indirectly by means of an external modem connected via the PDA's serial port or "sync" connector, or directly by using an expansion card that provided an Ethernet port.
Wireless connectivity.
Most modern PDAs have Bluetooth, a popular wireless protocol for mobile devices. Bluetooth can be used to connect keyboards, headsets, GPS receivers, and other nearby accessories. It's also possible to transfer files between PDAs that have Bluetooth.
Many modern PDAs have Wi-Fi wireless network connectivity and can connect to Wi-Fi hotspots. All smartphones, and some other modern PDAs, can connect to Wireless Wide Area Networks, such as those provided by cellular telecommunications companies.
Older PDAs from the 90s to 2006 typically had an IrDA (infrared) port allowing short-range, line-of-sight wireless communication. Few current models use this technology, as it has been supplanted by Bluetooth and Wi-Fi. IrDA allows communication between two PDAs, or between a PDA and any device with an IrDA port or adapter. Some printers have IrDA receivers, allowing IrDA-equipped PDAs to print to them, if the PDA's operating system supports it. Universal PDA keyboards designed for these older PDAs use infrared technology. Infrared technology is low-cost and has the advantage of being allowed aboard.
Synchronization.
Most PDAs can synchronize their data with applications on a user's computer. This allows the user to update contact, schedule, or other information on their computer, using software such as Microsoft Outlook or ACT!, and have that same data transferred to PDA—or transfer updated information from the PDA back to the computer. This eliminates the need for the user to update their data in two places.
Synchronization also prevents the loss of information stored on the device if it is lost, stolen, or destroyed. When the PDA is repaired or replaced, it can be "re-synced" with the computer, restoring the user's data.
Some users find that data input is quicker on their computer than on their PDA, since text input via a touchscreen or small-scale keyboard is slower than a full-size keyboard. Transferring data to a PDA via the computer is therefore a lot quicker than having to manually input all data on the handheld device.
Most PDAs come with the ability to synchronize to a computer. This is done through "synchronization software" provided with the handheld, or sometime with the computer's operating system. Examples of synchronization software include:
These programs allow the PDA to be synchronized with a personal information manager, which may be part of the computer's operating system, provided with the PDA, or sold separately by a third party. For example, the RIM BlackBerry comes with RIM's "Desktop Manager" program, which can synchronize to both Microsoft Outlook and ACT!.
Other PDAs come only with their own proprietary software. For example, some early Palm OS PDAs came only with Palm Desktop, while later Palm PDAs—such as the Treo 650—have the ability to sync to Palm Desktop or Microsoft Outlook. Microsoft's ActiveSync and Windows Mobile Device Center only synchronize with Microsoft Outlook or a Microsoft Exchange server.
Third-party synchronization software is also available for some PDAs from companies like CommonTime and CompanionLink. Third-party software can be used to synchronize PDAs to other personal information managers that are not supported by the PDA manufacturers (for example, GoldMine and IBM Lotus Notes).
Wireless synchronization.
Some PDAs can synchronize some or all of their data using their wireless networking capabilities, rather than having to be directly connected to a personal computer via a cable.
Apple iOS devices, like the iPhone, iPod Touch, and iPad, can use Apple's iCloud service (formerly MobileMe) to synchronize calendar, address book, mail account, Internet bookmark, and other data with one or more Macintosh or Windows computers using Wi-Fi or cellular data connections.
Devices running Palm's webOS or Google's Android operating system primarily sync with the cloud. For example, if Gmail is used, information in contacts, email, and calendar can be synchronized between the phone and Google's servers.
RIM sells BlackBerry Enterprise Server to corporations so that corporate BlackBerry users can wirelessly synchronize their PDAs with the company's Microsoft Exchange Server, IBM Lotus Domino, or Novell GroupWise servers. Email, calendar entries, contacts, tasks, and memos kept on the company's server are automatically synchronized with the BlackBerry.
Operating systems of PDAs.
The most common operating systems pre-installed on PDAs are:-
Other, rarely used operating systems:
Automobile navigation.
Some PDAs include Global Positioning System (GPS) receivers; this is particularly true of smartphones. Other PDAs are compatible with external GPS-receiver add-ons that use the PDA's processor and screen to display location information.
PDAs with GPS functionality can be used for automotive navigation. PDAs are increasingly being fitted as standard on new cars.
PDA-based GPS can also display traffic conditions, perform dynamic routing, and show known locations of roadside mobile radar guns. TomTom, Garmin, and iGO offer GPS navigation software for PDAs.
Ruggedized PDAs.
Some businesses and government organizations rely upon rugged PDAs, sometimes known as enterprise digital assistants (EDAs) or mobile computers, for mobile data applications. EDAs often have extra features for data capture, such as barcode readers, radio-frequency identification (RFID) readers, magnetic stripe card readers, or smart card readers.
Typical applications include:
Medical and scientific uses.
Many companies have developed PDA products aimed at the medical profession's unique needs, such as drug databases, treatment information, and medical news. Services such as AvantGo translate medical journals into PDA-readable formats. WardWatch organizes medical records, providing reminders of information such as the treatment regimens of patients to doctors making ward rounds. Pendragon and Syware provide tools for conducting research with, allowing the user to enter data into a centralized database using their PDA. Microsoft Visual Studio and Sun Java also provide programming tools for developing survey instruments on the handheld. These development tools allow for integration with SQL databases that are stored on the handheld and can be synchronized with a desktop- or server-based database.
PDAs have been shown to aid diagnosis and drug selection and some studies have concluded that when patients use PDAs to record their symptoms, they communicate more effectively with hospitals during follow-up visits.
The development of Sensor Web technology may lead to wearable bodily sensors to monitor ongoing conditions, like diabetes or epilepsy, which would alert patients and doctors when treatment is required using wireless communication and PDAs.
Educational uses.
PDAs and handheld devices are allowed in many classrooms for digital note-taking. Students can spell-check, modify, and amend their class notes on a PDA. Some educators distribute course material through the Internet or infrared file-sharing functions of the PDA. Textbook publishers have begun to release e-books, which can be uploaded directly to a PDA, reducing the number of textbooks students must carry.
Software companies have developed PDA programs to meet the instructional needs of educational institutions, such as dictionaries, thesauri, word processing software, encyclopedias, webinar and digital lesson planners.
Recreational uses.
PDAs may be used by music enthusiasts to play a variety of music file formats. Many PDAs include the functionality of an MP3 player.
Road rally enthusiasts can use PDAs to calculate distance, speed, and time. This information may be used for navigation, or the PDA's GPS functions can be used for navigation.
Underwater divers can use PDAs to plan breathing gas mixtures and decompression schedules using software such as "V-Planner".

</doc>
<doc id="23305" url="https://en.wikipedia.org/wiki?curid=23305" title="POSIX">
POSIX

The Portable Operating System Interface (POSIX) is a family of standards specified by the IEEE Computer Society for maintaining compatibility between operating systems. POSIX defines the application programming interface (API), along with command line shells and utility interfaces, for software compatibility with variants of Unix and other operating systems.
Name.
Originally, the name "POSIX" referred to IEEE Std 1003.1-1988, released in 1988. The family of POSIX standards is formally designated as IEEE 1003 and the international standard name is ISO/IEC 9945.
The standards emerged from a project that began circa 1985. Richard Stallman suggested the name "POSIX" to the IEEE instead of former "IEEE-IX". The committee found it more easily pronounceable and memorable, and thus adopted it.
Overview.
Unix was selected as the basis for a standard system interface partly because it was "manufacturer-neutral." However, several major versions of Unix existed so there was a need to develop a common denominator system. The POSIX specifications for Unix-like operating systems originally consisted of a single document for the core programming interface, but eventually grew to 19 separate documents (POSIX.1, POSIX.2, etc.). The standardized user command line and scripting interface were based on the UNIX System V shell. Many user-level programs, services, and utilities including awk, echo, ed were also standardized, along with required program-level services including basic I/O (file, terminal, and network) services. POSIX also defines a standard threading library API which is supported by most modern operating systems. Nowadays, most of POSIX parts are combined into a single standard, "IEEE Std 1003.1-2008", also known as "POSIX.1-2008".
, POSIX documentation is divided in two parts:
The development of the POSIX standard takes place in the Austin Group, a joint working group linking the IEEE, The Open Group and the ISO/IEC JTC 1 organizations.
Versions.
Parts before 1997.
Before 1997, POSIX comprised several standards:
Versions after 1997.
After 1997, the Austin Group developed the POSIX revisions. The specifications are known under the name Single UNIX Specification, before they become a POSIX standard when formally approved by the ISO.
POSIX.1-2001.
"POSIX.1-2001" or IEEE Std 1003.1-2001 equates to the "Single UNIX Specification version 3"
This standard consisted of:
POSIX.1-2004 (with two TCs).
IEEE Std 1003.1-2004 involved a minor update of POSIX.1-2001. It incorporated two minor updates or errata referred to as "Technical Corrigenda". Its contents are available on the web.
POSIX.1-2008 (with one TC).
, "Base Specifications, Issue 7" or "IEEE Std 1003.1", 2013 edition represents the current version. A free online copy is available.
This standard consists of:
Controversies.
512- vs 1024-byte blocks.
POSIX mandates 512-byte block sizes for the df and du utilities, reflecting the default size of blocks on disks. When Richard Stallman and the GNU team were implementing POSIX for the GNU operating system, they objected to this on the grounds that most people think in terms of 1024 byte (or 1 KiB) blocks. The environment variable "POSIXLY_CORRECT" was introduced to allow the user to force the standards-compliant behaviour. The variable name "POSIX_ME_HARDER" was also discussed. The variable "POSIXLY_CORRECT" is now also used for a number of other behaviour quirks, where "POSIX and common sense disagree".
POSIX-oriented operating systems.
Depending upon the degree of compliance with the standards, one can classify operating systems as fully or partly POSIX compatible. Certified products can be found at the IEEE's website.
POSIX-certified.
Some versions of the following operating systems have been certified to conform to one or more of the various POSIX standards. This means that they passed the automated conformance tests.
Mostly POSIX-compliant.
The following, while not officially certified as POSIX compatible, comply in large part:
POSIX for OS/2.
Mostly POSIX compliant environments for OS/2:
POSIX for DOS.
Partially POSIX compliant environments for DOS include:
Compliant via compatibility feature.
The following are not officially certified as POSIX compatible, but they conform in large part to the standards by implementing POSIX support via some sort of compatibility feature, usually translation libraries, or a layer atop the kernel. Without these features, they are usually noncompliant.

</doc>
<doc id="23306" url="https://en.wikipedia.org/wiki?curid=23306" title="Personal Telco">
Personal Telco

The Personal Telco Project (or PTP) is a wireless community network project in Portland, Oregon. It was founded by Adam Shand in November 2000 after he read a Slashdot article about the Consume The Net project in London.
PTP has wireless hotspots offering free Internet access at locations around Portland including Pioneer Courthouse Square, three public parks, and several restaurants and coffee shops using Wi-Fi.
In 2005 PTP was involved in a grant-funded project to bring free wireless Internet to an entire neighborhood in North Portland, along N. Mississippi Avenue.
PTP is a non-profit organization in the state of Oregon, and received its 501(c)(3) status (a federal tax exemption granted to charitable organizations) in early 2003.
PTP made US news in August 2002 when their hotspot at Pioneer Courthouse Square was blocked by a for-fee hotspot at a nearby Starbucks coffee shop. The problem was resolved amicably when the for-fee hotspot moved their connection to another channel.
On March 4, 2003, a study was published stating Portland had more wireless hotspots per capita than any other U.S. city. This was due in part to PTP's advocacy.

</doc>
<doc id="23307" url="https://en.wikipedia.org/wiki?curid=23307" title="Prince Paul (producer)">
Prince Paul (producer)

Paul Edward Huston, (born April 2, 1967) better known by his stage name Prince Paul, is an American disc jockey, record producer and recording artist from Amityville, New York. Paul began his career as a DJ for Stetsasonic. Since then he has worked on albums by Boogie Down Productions, MC Lyte, Big Daddy Kane and 3rd Bass, among others. Major recognition for Prince Paul came when he produced De La Soul's debut album "3 Feet High and Rising" (1989), in which he pioneered new approaches to hip hop production, mixing and sampling, as well as by adding comedy sketches.
In 1994, Paul joined RZA, Frukwan and Too Poetic in Gravediggaz, a project that debuted with "6 Feet Deep". His first solo album, "", came out in 1997, followed by a second album, "A Prince Among Thieves", in 1999. Later that year Prince Paul formed Handsome Boy Modeling School with Dan the Automator and they released the album "So...How's Your Girl?".
Life and career.
Prince Paul was one of the original members of Stetsasonic. He joined the group in 1984 after he impressed founding group member Daddy-O with his routine in the "Brevoit Day Celebration" DJ battle in Brooklyn. Daddy-O was struck by Paul's energy, stating that he was performed his routine with Liquid Liquid's song "Cavern", "like he was mad at the turntables."
He produced tracks on 3rd Bass' 1989 debut "The Cactus Album" and De La Soul's first three albums, "3 Feet High and Rising" (1989), "De La Soul is Dead" (1991), and "Buhloone Mindstate" (1993). "De La Soul is Dead" received the coveted five mic album review from "The Source (magazine)".
According to Paul, "3 Feet High and Rising" (1989) had a budget of about $20,000 dollars and took a month and a half to make. In Brian Coleman's book "Check the Technique" (2007) Paul reflected on his work with De La Soul by saying, "“If there was ever a sign of the existence of God, De La Soul would be that proof to me. I’ve never had such a perfect fit in any other production situation.”
In 1990, Russell Simmons gave Prince Paul an imprint under his Def Jam label, however the only album, "It Takes a Nation of Suckers to Let Us In" by Resident Alien, was never officially released.
He, along with Frukwan of Stetsasonic, Too Poetic of Brothers Grimm, and The RZA of Wu-Tang Clan compose Gravediggaz. The group was formed after Paul's relationship with Rush Management fell apart between 1992 and 1993. The group had difficulty shopping their first album, "Niggamortis/6 Feet Deep" (1994), but were eventually picked up by Gee Street records.
In 1995, Prince Paul co-produced guitarist Vernon Reid's solo debut with Teo Macero.
In 1996, he appeared on the Red Hot Organization's compilation CD, America is Dying Slowly, alongside Wu-Tang Clan, Coolio, and Fat Joe. The CD, meant to raise awareness of the AIDS epidemic among African American men, was heralded as a masterpiece by "The Source" magazine.
In 1996, during the early recording sessions for "Stakes Is High", De La Soul and Prince Paul decided to part ways. Although Paul was not involved in the making of the album, he has praised it in several interviews, once stating, "I was going through a serious transition period in my life when that album dropped. I was trying to figure out the next thing I was going to do; I was going through a custody case for my son, and I was running out of money. There were a lot of things going on at the time and in a sense, that album pulled me through everything."
After this he released two solo albums: "" and "A Prince Among Thieves", featuring Big Daddy Kane, Xzibit, Kool Keith, and Everlast.
Prince Paul formed Handsome Boy Modeling School with Dan the Automator and their album "So... How's Your Girl?" featured Sean Lennon, Del tha Funkee Homosapien, Alec Empire, and Don Novello. He was also paired up with Deltron 3030 on their self-titled album for the song "The Fantabulous Rap Extravaganza". In 2000, Prince Paul produced MC Paul Barman's début EP "It's Very Stimulating", followed by another Handsome Boy Modeling School album, "White People", with guests including the RZA, Linkin Park, Tim Meadows, and John Oates.
Prince Paul's 2003 album "Politics of the Business" again featured many guests such as Chuck D, Ice-T, the Beatnuts, and Wordsworth. The latter also collaborated on a track Paul composed for the "SpongeBob SquarePants Movie" soundtrack.
Several songs by Paul have been featured only on compilations such as Om Records's "Deep Concentration" and Bill Laswell's "Altered Beats".
His most recent release is the album "Itstrumental". It encompasses a range of genres, relying heavily on past samples, especially from "A Prince Among Thieves", and combining it with lighthearted skits about his depression. He also produced the album "The Art of Picking Up Women" by the Dix, which combines some of hip-hop's misogyny and boasting with 1960s-style R&B.
Paul was the host of XM radio's "The Ill Out Show", until the station was dropped following its merger with Sirius. The show presented news, classic songs, and interviews with various hip-hop artists.
One of Prince Paul's more recent projects is Baby Elephant, a collaboration with Parliament and Talking Heads keyboardist Bernie Worrell, and longtime Paul associate Don Newkirk. An album "Turn My Teeth Up!", released in September 2007, features George Clinton, Shock G, Yellowman, Reggie Watts, Nona Hendryx, David Byrne, and Gabby La La.
In 2008–2009, Paul collaborated with Oakland rap group Souls of Mischief on the album "Montezuma's Revenge" (2009). The idea for the album came about during a Handsome Boy Modeling School tour when Paul told Souls of Mischief member Opio, "Tell the guys that I want to produce the next album.” The album was recorded in a rented house and many of the beats were made with vintage equipment, including a Emu SP-12, Ensoniq ASR-10, Akai MPC-60 and an Akai MPC-2000. Paul stated in an interview that his two favorite songs from the album are "Proper Aim" and "Morgan Freeman Skit".
Style and influences.
Paul credits Public Enemy as being a significant influence on early De La Soul production, saying, "Early Public Enemy production used layers upon layers and layers, and their arrangements were always super duper incredible to me. We were kind of like students to what they did."
He is known for sampling from a wide range of genres. Rapper Biz Markie once said of Paul's production style, "Prince Paul's contribution to hip-hop is that you could use records that weren't by James Brown or just break-beats." While Paul continues to utilize samples, he has expanded his production to include live instrumentation, including guitar, bass guitar, and several analog keyboards. To combine elements of sampling and live instrumentation, Paul now re-plays some of his samples with instruments. In a 2010 interview he described the process by saying, "I’ve gotten to the point now where I’ll re-play samples with instruments. I learned how to interpolate, change the sound, and dust it out so that when I’m re-playing certain samples, it sounds like a direct sample from a record." Paul has credited the process of working with Dan the Automator on the Handsome Boy Modeling School project as helping him learn a great deal about production.
Paul has talked about his fondness for vintage equipment in several interviews. One of his favorite pieces of vintage equipment is the Akai S900, which he credits for having a unique sound, saying, "Even though it's big and bulky, nothing sounds like that. It’s pretty flexible, it's easy to work, and it's easy to truncate your sample and get things tight...When you look at all this new technology, everything sounds very sterile. Everything is clean and super quiet. It kind of lacks something. When I plug that in, it's like, 'Wow, this is hip hop.'"

</doc>
<doc id="23309" url="https://en.wikipedia.org/wiki?curid=23309" title="Paul Vixie">
Paul Vixie

Paul Vixie is an American Internet pioneer, the author of several RFCs and some Unix software.
Vixie attended George Washington High School in San Francisco, California. He received a Ph.D in computer science from Keio University in 2011.
He authored the standard UNIX system programs "SENDS", "proxynet", "rtty" and Vixie cron. At one point he ran his own consulting business, Vixie Enterprises.
Career.
After he left Digital Equipment Corporation (DEC) in 1994, he founded Internet Software Consortium (ISC) together with Rick Adams and Carl Malamud to support BIND and other software for the Internet. The activities of ISC were assumed by a new company, Internet Systems Consortium in 2004. Although ISC operates the F root server, Vixie at one point joined the Open Root Server Network (ORSN) project and operated their L root server.
In 1995 he cofounded the Palo Alto Internet Exchange (PAIX) and, after Metromedia Fiber Network (MFN) bought it in 1999, served as the chief technology officer to MFN / AboveNet and later as the president of PAIX.
In 1998 he cofounded Mail Abuse Prevention System (MAPS), a California non-profit company with the goal of stopping email abuse.
Vixie stated in 2002 that he "now hold the record for 'most CERT advisories due to a single author.'"
In 2008, Vixie served as a judge for the Mozilla Foundation's "Download Day", an attempt to set a Guinness World Record for most downloads in a single day for a new piece of software.
Vixie served on the Board of Trustees of the American Registry for Internet Numbers (ARIN) from 2005 to 2013, and served as chairman in 2009 and 2010.
Vixie also serves on the Security and Stability Advisory Committee of ICANN.
In 2013, after nearly 20 years at ISC, he founded a new company, Farsight Security, Inc. spinning off the Security Business Unit from ISC.
In 2014, Vixie was inducted into the Internet Hall of Fame as an Innovator.

</doc>
<doc id="23310" url="https://en.wikipedia.org/wiki?curid=23310" title="Pleistocene">
Pleistocene

The Pleistocene (; symbol PS) is the geological epoch which lasted from about 2,588,000 to 11,700 years ago, spanning the world's recent period of repeated glaciations. The end of the Pleistocene corresponds with the end of the last glacial period and also with the end of the Paleolithic age used in archaeology. 
Charles Lyell introduced this term in 1839 to describe strata in Sicily that had at least 70% of their molluscan fauna still living today. This distinguished it from the older Pliocene Epoch, which Lyell had originally thought to be the youngest fossil rock layer. He constructed the name "Pleistocene" ("Most New" or "Newest") from the Greek πλεῖστος, "pleīstos", "most", and καινός, "kainós" (latinized as "cænus"), "new"; this contrasting with the immediately preceding Pleiocene ("More New" or "Newer", from πλείων, "pleíōn", "more", and "kainós"; usual spelling: Pliocene), and the immediately subsequent Holocene ("wholly new" or "entirely new", from ὅλος, "hólos", "whole", and "kainós") epoch, which extends to the present time.
The Pleistocene is the first epoch of the Quaternary Period or sixth epoch of the Cenozoic Era. In the ICS timescale, the Pleistocene is divided into four stages or ages, the Gelasian, Calabrian, Ionian and Tarantian. All of these stages were defined in southern Europe. In addition to this international subdivision, various regional subdivisions are often used.
Before a change finally confirmed in 2009 by the International Union of Geological Sciences, the time boundary between the Pleistocene and the preceding Pliocene was regarded as being at 1.806 million years before the present, as opposed to the currently accepted 2.588 million years BP: publications from the preceding years may use either definition of the period.
Dating.
The Pleistocene has been dated from 2.588 million (±.005) to 11,700 years before present (BP), with the end date expressed in radiocarbon years as 10,000 carbon-14 years BP. It covers most of the latest period of repeated glaciation, up to and including the Younger Dryas cold spell. The end of the Younger Dryas has been dated to about 9640 BC (11,654 calendar years BP). It was not until after the development of radiocarbon dating, however, that Pleistocene archaeological excavations shifted to stratified caves and rock-shelters as opposed to open-air river-terrace sites.
In 2009 the International Union of Geological Sciences (IUGS) confirmed a change in time period for the Pleistocene, changing the start date from 1.806 to 2.588 million years BP, and accepted the base of the Gelasian as the base of the Pleistocene, namely the base of the Monte San Nicola GSSP. The IUGS has yet to approve a type section, Global Boundary Stratotype Section and Point (GSSP), for the upper Pleistocene/Holocene boundary ("i.e." the upper boundary). The proposed section is the "North Greenland Ice Core Project" ice core 75° 06' N 42° 18' W. The lower boundary of the Pleistocene Series is formally defined magnetostratigraphically as the base of the Matuyama (C2r) chronozone, isotopic stage 103. Above this point there are notable extinctions of the calcareous nanofossils: "Discoaster pentaradiatus" and "Discoaster surculus".
The Pleistocene covers the recent period of repeated glaciations. The name Plio-Pleistocene has, in the past, been used to mean the last ice age. The revised definition of the Quaternary, by pushing back the start date of the Pleistocene to 2.58 Ma, results in the inclusion of all the recent repeated glaciations within the Pleistocene.
Paleogeography and climate.
The modern continents were essentially at their present positions during the Pleistocene, the plates upon which they sit probably having moved no more than 100 km relative to each other since the beginning of the period.
According to Mark Lynas (through collected data), the Pleistocene's overall climate could be characterized as a continuous El Niño with trade winds in the south Pacific weakening or heading east, warm air rising near Peru, warm water spreading from the west Pacific and the Indian Ocean to the east Pacific, and other El Niño markers.
Glacial features.
Pleistocene climate was marked by repeated glacial cycles in which continental glaciers pushed to the 40th parallel in some places. It is estimated that, at maximum glacial extent, 30% of the Earth's surface was covered by ice. In addition, a zone of permafrost stretched southward from the edge of the glacial sheet, a few hundred kilometres in North America, and several hundred in Eurasia. The mean annual temperature at the edge of the ice was ; at the edge of the permafrost, .
Each glacial advance tied up huge volumes of water in continental ice sheets thick, resulting in temporary sea-level drops of or more over the entire surface of the Earth. During interglacial times, such as at present, drowned coastlines were common, mitigated by isostatic or other emergent motion of some regions.
The effects of glaciation were global. Antarctica was ice-bound throughout the Pleistocene as well as the preceding Pliocene. The Andes were covered in the south by the Patagonian ice cap. There were glaciers in New Zealand and Tasmania. The current decaying glaciers of Mount Kenya, Mount Kilimanjaro, and the Ruwenzori Range in east and central Africa were larger. Glaciers existed in the mountains of Ethiopia and to the west in the Atlas mountains.
In the northern hemisphere, many glaciers fused into one. The Cordilleran ice sheet covered the North American northwest; the east was covered by the Laurentide. The Fenno-Scandian ice sheet rested on northern Europe, including Great Britain; the Alpine ice sheet on the Alps. Scattered domes stretched across Siberia and the Arctic shelf. The northern seas were ice-covered.
South of the ice sheets large lakes accumulated because outlets were blocked and the cooler air slowed evaporation. When the Laurentide ice sheet retreated, north central North America was totally covered by Lake Agassiz. Over a hundred basins, now dry or nearly so, were overflowing in the North American west. Lake Bonneville, for example, stood where Great Salt Lake now does. In Eurasia, large lakes developed as a result of the runoff from the glaciers. Rivers were larger, had a more copious flow, and were braided. African lakes were fuller, apparently from decreased evaporation. Deserts on the other hand were drier and more extensive. Rainfall was lower because of the decrease in oceanic and other evaporation.
It has been estimated that during the Pleistocene, the East Antarctic Ice Sheet thinned by at least 500 meters, and that thinning since the Last Glacial Maximum is less than 50 meters and probably started after ca 14 ka.
Major events.
Over 11 major glacial events have been identified, as well as many minor glacial events. A major glacial event is a general glacial excursion, termed a "glacial." Glacials are separated by "interglacials". During a glacial, the glacier experiences minor advances and retreats. The minor excursion is a "stadial"; times between stadials are "interstadials".
These events are defined differently in different regions of the glacial range, which have their own glacial history depending on latitude, terrain and climate. There is a general correspondence between glacials in different regions. Investigators often interchange the names if the glacial geology of a region is in the process of being defined. However, it is generally incorrect to apply the name of a glacial in one region to another.
For most of the 20th century only a few regions had been studied and the names were relatively few. Today the geologists of different nations are taking more of an interest in Pleistocene glaciology. As a consequence, the number of names is expanding rapidly and will continue to expand. Many of the advances and stadials remain unnamed. Also, the terrestrial evidence for some of them has been erased or obscured by larger ones, but evidence remains from the study of cyclical climate changes.
The glacials in the following tables show "historical" usages, are a simplification of a much more complex cycle of variation in climate and terrain, and are generally no longer used. These names have been abandoned in favor of numeric data because many of the correlations were found to be either inexact or incorrect and more than four major glacials have been recognized since the historical terminology was established.
Corresponding to the terms glacial and interglacial, the terms pluvial and interpluvial are in use (Latin: "pluvia", rain). A pluvial is a warmer period of increased rainfall; an interpluvial, of decreased rainfall. Formerly a pluvial was thought to correspond to a glacial in regions not iced, and in some cases it does. Rainfall is cyclical also. Pluvials and interpluvials are widespread.
There is no systematic correspondence of pluvials to glacials, however. Moreover, regional pluvials do not correspond to each other globally. For example, some have used the term "Riss pluvial" in Egyptian contexts. Any coincidence is an accident of regional factors. Only a few of the names for pluvials in restricted regions have been strategraphically defined.
Palaeocycles.
The sum of transient factors acting at the Earth's surface is cyclical: climate, ocean currents and other movements, wind currents, temperature, etc. The waveform response comes from the underlying cyclical motions of the planet, which eventually drag all the transients into harmony with them. The repeated glaciations of the Pleistocene were caused by the same factors.
Milankovitch cycles.
Glaciation in the Pleistocene was a series of glacials and interglacials, stadials and interstadials, mirroring periodic changes in climate. The main factor at work in climate cycling is now believed to be Milankovitch cycles. These are periodic variations in regional and planetary solar radiation reaching the Earth caused by several repeating changes in the Earth's motion.
Milankovitch cycles cannot be the sole factor responsible for the variations in climate since they explain neither the long term cooling trend over the Plio-Pleistocene, nor the millennial variations in the Greenland Ice Cores. Milankovitch pacing seems to best explain glaciation events with periodicity of 100,000, 40,000, and 20,000 years. Such a pattern seems to fit the information on climate change found in oxygen isotope cores. The timing of our present interglacial interval (known as the Holocene, Postglacial, or the Present Interglacial) to that of the previous interglacial, beginning about 130,000 years ago (The Eemian Interglacial), suggests that the next glacial might begin in about 3,000 years.
Oxygen isotope ratio cycles.
In oxygen isotope ratio analysis, variations in the ratio of to (two isotopes of oxygen) by mass (measured by a mass spectrometer) present in the calcite of oceanic core samples is used as a diagnostic of ancient ocean temperature change and therefore of climate change. Cold oceans are richer in , which is included in the tests of the microorganisms (foraminifera) contributing the calcite.
A more recent version of the sampling process makes use of modern glacial ice cores. Although less rich in than sea water, the snow that fell on the glacier year by year nevertheless contained and in a ratio that depended on the mean annual temperature.
Temperature and climate change are cyclical when plotted on a graph of temperature versus time. Temperature coordinates are given in the form of a deviation from today's annual mean temperature, taken as zero. This sort of graph is based on another of isotope ratio versus time. Ratios are converted to a percentage difference from the ratio found in standard mean ocean water (SMOW).
The graph in either form appears as a waveform with overtones. One half of a period is a Marine isotopic stage (MIS). It indicates a glacial (below zero) or an interglacial (above zero). Overtones are stadials or interstadials.
According to this evidence, Earth experienced 102 MIS stages beginning at about 2.588 Ma BP in the Early Pleistocene Gelasian. Early Pleistocene stages were shallow and frequent. The latest were the most intense and most widely spaced.
By convention, stages are numbered from the Holocene, which is MIS1. Glacials receive an even number; interglacials, odd. The first major glacial was MIS2-4 at about 85–11 ka BP. The largest glacials were 2, 6, 12, and 16; the warmest interglacials, 1, 5, 9 and 11. For matching of MIS numbers to named stages, see under the articles for those names.
Fauna.
Both marine and continental faunas were essentially modern.
The severe climatic changes during the ice age had major impacts on the fauna and flora. With each advance of the ice, large areas of the continents became totally depopulated, and plants and animals retreating southward in front of the advancing glacier faced tremendous stress. The most severe stress resulted from drastic climatic changes, reduced living space, and curtailed food supply. A major extinction event of large mammals (megafauna), which included mammoths, mastodons, saber-toothed cats, glyptodons, ground sloths, Irish elk, cave bears, and short-faced bears, began late in the Pleistocene and continued into the Holocene. Neanderthals also became extinct during this period. At the end of the last ice age, cold-blooded animals, smaller mammals like wood mice, migratory birds, and swifter animals like whitetail deer had replaced the megafauna and migrated north.
The extinctions were especially severe in North America where native horses and camels were eliminated.
Humans.
The evolution of anatomically modern humans took place during the Pleistocene. In the beginning of the Pleistocene "Paranthropus" species are still present, as well as early human ancestors, but during the lower Palaeolithic they disappeared, and the only hominin species found in fossilic records is "Homo erectus" for much of the Pleistocene. Acheulean lithics
appear along with "Homo erectus", some 1.8 million years ago, replacing the more primitive Oldowan industry used by "A. garhi" and by the earliest species of "Homo".
The Middle Paleolithic saw more varied speciation within "Homo", including the appearance of "Homo sapiens" about 200,000 years ago.
According to mitochondrial timing techniques, modern humans migrated from Africa after the Riss glaciation in the Middle Palaeolithic during the Eemian Stage, spreading all over the ice-free world during the late Pleistocene. A 2005 study posits that humans in this migration interbred with archaic human forms already outside of Africa by the late Pleistocene, incorporating archaic human genetic material into the modern human gene pool.
Deposits.
Pleistocene non-marine sediments are found primarily in fluvial deposits, lakebeds, slope and loess deposits as well as in the large amounts of material moved about by glaciers. Less common are cave deposits, travertines and volcanic deposits (lavas, ashes). Pleistocene marine deposits are found primarily in shallow marine basins mostly (but with important exceptions) in areas within a few tens of kilometers of the modern shoreline. In a few geologically active areas such as the Southern California coast, Pleistocene marine deposits may be found at elevations of several hundred meters.

</doc>
<doc id="23311" url="https://en.wikipedia.org/wiki?curid=23311" title="Pasteurization">
Pasteurization

Pasteurization (American English), also spelled pasteurisation (British English), is a process that kills bacteria in liquid food.
It was invented by French scientist Louis Pasteur during the nineteenth century. In 1864, Pasteur discovered that heating beer and wine was enough to kill most of the bacteria that caused spoilage, preventing these beverages from turning sour. The process achieves this by eliminating pathogenic microbes and lowering microbial numbers to prolong the quality of the beverage. Today, pasteurisation is used widely in the dairy and food industries for microbial control and preservation of food.
Unlike sterilization, pasteurization is not intended to kill all micro-organisms in the food. Instead, it aims to reduce the number of viable pathogens so they are unlikely to cause disease (assuming the pasteurized product is stored as indicated and is consumed before its expiration date). Commercial-scale sterilization of food is not common because it adversely affects the taste and quality of the product. Certain foods, such as dairy products, may be superheated to ensure pathogenic microbes are destroyed.
Alcoholic beverages.
The process of heating wine for preservation purposes has been known in China since 1117, and was documented in Japan in the diary "Tamonin-nikki", written by a series of monks between 1478 and 1618.
Much later, in 1768, an Italian priest and scientist Lazzaro Spallanzani proved experimentally that heat killed bacteria, and that they do not re-appear if the product is hermetically sealed. In 1795, a Parisian chef and confectioner named Nicolas Appert began experimenting with ways to preserve foodstuffs, succeeding with soups, vegetables, juices, dairy products, jellies, jams, and syrups. He placed the food in glass jars, sealed them with cork and sealing wax and placed them in boiling water. In that same year, the French military offered a cash prize of 12,000 francs for a new method to preserve food. After some 14 or 15 years of experimenting, Appert submitted his invention and won the prize in January 1810. Later that year, Appert published "L'Art de conserver les substances animales et végétales" (or "The Art of Preserving Animal and Vegetable Substances"). This was the first cookbook of its kind on modern food preservation methods.
"La Maison Appert" (), in the town of Massy, near Paris, became the first food-bottling factory in the world, preserving a variety of food in sealed bottles. Appert's method was to fill thick, large-mouthed glass bottles with produce of every description, ranging from beef and fowl to eggs, milk and prepared dishes. His greatest success for publicity was an entire sheep. He left air space at the top of the bottle, and the cork would then be sealed firmly in the jar by using a vise. The bottle was then wrapped in canvas to protect it, while it was dunked into boiling water and then boiled for as much time as Appert deemed appropriate for cooking the contents thoroughly. Appert patented his method, sometimes called in his honor "appertisation".
Appert's method was so simple and workable that it quickly became widespread. In 1810, British inventor and merchant Peter Durand, also of French origin, patented his own method, but this time in a tin can, so creating the modern-day process of canning foods. In 1812, Englishmen Bryan Donkin and John Hall purchased both patents and began producing preserves. Just a decade later, Appert's method of canning had made its way to America. Tin can production was not common until the beginning of the 20th century, partly because a hammer and chisel were needed to open cans until the invention of a can opener by an inventor named Yates in 1855.
Appert's preservation by boiling involved heating the food to an unnecessarily high temperature, and for an unnecessarily long time, which could destroy some of the flavor of the preserved food.
A less aggressive method was developed by the French chemist Louis Pasteur during an 1864 summer holiday in Arbois. To remedy the frequent acidity of the local wines, he found out experimentally that it is sufficient to heat a young wine to only about for a brief time to kill the microbes, and that the wine could subsequently be aged without sacrificing the final quality. In honour of Pasteur, the process became known as "pasteurization" Pasteurization was originally used as a way of preventing wine and beer from souring, and it would be many years before milk was pasteurized. In the United States in the 1870s, it was common for milk to contain substances intended to mask spoilage before milk was regulated.
Milk.
Milk is an excellent medium for microbial growth, and when stored at ambient temperature bacteria and other pathogens soon proliferate.
The US Centers for Disease Control (CDC) says improperly handled raw milk is responsible for nearly three times more hospitalizations than any other food-borne disease outbreak, making it one of the world's most dangerous food products. Diseases prevented by pasteurization can include tuberculosis, brucellosis, diphtheria, scarlet fever, and Q-fever; it also kills the harmful bacteria "Salmonella", "Listeria", "Yersinia", "Campylobacter", "Staphylococcus aureus", and "Escherichia coli" O157:H7, among others.
Pasteurization is the reason for milk's extended shelf life. High-temperature, short-time (HTST) pasteurized milk typically has a refrigerated shelf life of two to three weeks, whereas ultra-pasteurized milk can last much longer, sometimes two to three months. When ultra-heat treatment (UHT) is combined with sterile handling and container technology (such as aseptic packaging), it can even be stored unrefrigerated for up to 9 months.
History.
Before the widespread urban growth caused by industrialization, people kept dairy cows even in urban areas and the short time period between production and consumption minimized the disease risk of drinking raw milk. As urban densities increased and supply chains lengthened to the distance from country to city, raw milk (often days old) became recognised as a source of disease. For example, between 1912 and 1937 some 65,000 people died of tuberculosis contracted from consuming milk in England and Wales alone.
Developed countries adopted milk pasteurization to prevent such disease and loss of life, and as a result milk is now widely considered one of the safest foods. A traditional form of pasteurization by scalding and straining of cream to increase the keeping qualities of butter was practiced in England before 1773 and was introduced to Boston in the US by 1773, although it was not widely practiced in the United States for the next 20 years. It was still being referred to as a "new" process in American newspapers as late as 1802. Pasteurization of milk was suggested by Franz von Soxhlet in 1886. In the early 20th century, Milton Joseph Rosenau, established the standards (i.e. low temperature, slow heating at 60 °C (140 °F) for 20 minutes) for the pasteurization of milk, while at the United States Marine Hospital Service, notably in his publication of The Milk Question (1912).
Process.
Older pasteurization methods used temperatures below boiling, since at very high temperatures, micelles of the milk protein casein irreversibly aggregate, or "curdle". Newer methods use higher temperature, but shorten the time. Among the pasteurization methods listed below, the two main types of pasteurization used today are high-temperature, short-time (HTST, also known as "flash") and extended shelf life (ESL):
Pasteurization methods are usually standardized and controlled by national food safety agencies (such as the USDA in the United States and the Food Standards Agency in the United Kingdom). These agencies require that milk be HTST pasteurized to qualify for the pasteurized label. Dairy product standards differ, depending on fat content and intended usage. For example, pasteurization standards for cream differ from standards for fluid milk, and standards for pasteurizing cheese are designed to preserve the enzyme phosphatase, which aids cutting. In Canada, all milk produced at a processor and intended for consumption must be pasteurized, which legally requires that it be heated to at least 72 °C for at least 16 seconds, then cooling it to 4 °C to ensure any harmful bacteria are destroyed. The UK Dairy Products Hygiene Regulations 1995 requires that milk be heat treated for 15 seconds at 71.7 °C or other effective time/temperature combination.
A process similar to pasteurization is thermization, which uses lower temperatures to kill bacteria in milk. It allows a milk product, such as cheese, to retain more of the original taste, but thermized foods are not considered pasteurized by food regulators.
Microwave volumetric heating.
Microwave volumetric heating (MVH) is the newest available pasteurization technology. It uses microwaves to heat liquids, suspensions, or semi-solids in a continuous flow. Because MVH delivers energy evenly and deeply into the whole body of a flowing product, it allows for gentler and shorter heating, so that almost all heat-sensitive substances in the milk are preserved.
Efficiency.
The HTST pasteurization standard was designed to achieve a five-log reduction, killing 99.999% of the number of viable micro-organisms in milk. This is considered adequate for destroying almost all yeasts, molds, and common spoilage bacteria and also to ensure adequate destruction of common pathogenic, heat-resistant organisms (including "Mycobacterium tuberculosis", which causes tuberculosis, but not "Coxiella burnetii," which causes Q fever). As a precaution, modern equipment tests and identifies bacteria in milk being processed. HTST pasteurization processes must be designed so the milk is heated evenly, and no part of the milk is subject to a shorter time or a lower temperature.
Even pasteurization without quality control can be effective, though this is generally not permitted for human consumption; a study of farms feeding calves on pasteurized waste milk using a mixture of pasteurization technologies (none of which were routinely monitored for performance) found the resulting pasteurized milk to meet safety requirements at least 92% of the time.
An effect of the heating of pasteurization is that some vitamin, mineral, and beneficial (or probiotic) bacteria is lost. Soluble calcium and phosphorus levels decrease by 5%, thiamine (vitamin B1) and vitamin B12 (cobalamin) levels by 10%, and vitamin C levels by 20%. These losses are not significant nutritionally.
Verification.
Direct microbiological techniques are the ultimate measurement of pathogen contamination, but these are costly and time-consuming (24–48 hours), which means that products are able to spoil by the time pasteurization is verified.
As a result of the unsuitability of microbiological techniques, milk pasteurization efficacy is typically monitored by checking for the presence of alkaline phosphatase, which is denatured by pasteurization. B. tuberculosis, the bacterium that requires the highest temperature to be killed of all milk pathogens is killed at ranges of temperature and time similar to those that denature alkaline phosphatase. For this reason, presence of alkaline phosphatase is an ideal indicator of pasteurization efficacy.
Phosphatase denaturing was originally monitored using a phenol-phosphate substrate. When hydrolysed by the enzyme these compounds liberate phenols, which were then reacted with dibromoquinonechlorimide to give a colour change, which itself was measured by checking absorption at 610 nm (spectrophotometry). Some of the phenols used were inherently coloured (phenolpthalein, nitrophenol) and were simply assayed unreacted. Spectrophotometric analysis is satisfactory but is of relatively low accuracy because many natural products are coloured. For this reason, modern systems (since 1990) use fluorometry which is able to detect much lower levels of raw milk contamination.
Unpasteurized milk.
According to the United States Centers for Disease Control between 1998 and 2011 79% of the dairy related outbreaks were due to raw milk or cheese products. They report 148 outbreaks, 2,384 illnesses (284 requiring hospitalizations) as well as 2 deaths due to raw milk or cheese products during the same time period.
Consumer acceptance.
As pasteurization is a very old and traditional way of preservation, it is well known and accepted by consumers. Nearly every label of milk products contains the word "pasteurization" and it is associated by consumers with good quality attributes and safety. In the consumer studies of Hightech Europe, consumers mentioned more positive than negative associations for this technology, showing that these products are well accepted.

</doc>
<doc id="23312" url="https://en.wikipedia.org/wiki?curid=23312" title="Penicillin">
Penicillin

Penicillin (PCN or pen) is a group of antibiotics which include penicillin G (intravenous use), penicillin V (oral use), procaine penicillin, and benzathine penicillin (intramuscular use). Penicillin antibiotics were among the first medications to be effective against many bacterial infections caused by staphylococci and streptococci. Penicillins are still widely used today, though many types of bacteria have developed resistance following extensive use.
About 10% of people report that they are allergic to penicillin; however, up to 90% of this group may not actually be allergic. Serious allergies only occur in about 0.03%. All penicillins are β-lactam antibiotics.
Penicillin was discovered in 1928 by Scottish scientist Alexander Fleming. People began using it to treat infections in 1942. There are several enhanced penicillin families which are effective against additional bacteria; these include the antistaphylococcal penicillins, aminopenicillins and the antipseudomonal penicillins. They are derived from "Penicillium" fungi.
Medical uses.
The term "penicillin" is often used generically to refer to benzylpenicillin (penicillin G, the original penicillin found in 1928), procaine benzylpenicillin (procaine penicillin), benzathine benzylpenicillin (benzathine penicillin), and phenoxymethylpenicillin (penicillin V). Procaine penicillin and benzathine penicillin have the same antibacterial activity as benzylpenicillin but act for a longer period of time. Phenoxymethylpenicillin is less active against gram-negative bacteria than benzylpenicillin. Benzylpenicillin, procaine penicillin and benzathine penicillin are given by injection (parenterally), but phenoxymethylpenicillin is given orally.
Susceptibility.
While the number of penicillin-resistant bacteria is increasing, penicillin can still be used to treat a wide range of infections caused by certain susceptible bacteria, including Streptococci, Staphylococci, Clostridium, and Listeria genera. The following list illustrates minimum inhibitory concentration susceptibility data for a few medically significant bacteria:
Adverse effects.
Common adverse drug reactions (≥ 1% of people) associated with use of the penicillins include diarrhoea, hypersensitivity, nausea, rash, neurotoxicity, urticaria, and superinfection (including candidiasis). Infrequent adverse effects (0.1–1% of people) include fever, vomiting, erythema, dermatitis, angioedema, seizures (especially in people with epilepsy), and pseudomembranous colitis.
About 10% of people report that they are allergic to penicillin; however, 90% of this group are not actually allergic. Serious allergies only occur in about 0.03%.
Pain and inflammation at the injection site is also common for parenterally administered benzathine benzylpenicillin, benzylpenicillin, and, to a lesser extent, procaine benzylpenicillin.
Although penicillin is still the most commonly reported allergy, less than 20% of people who believe that they have a penicillin allergy are truly allergic to penicillin; nevertheless, penicillin is still the most common cause of severe allergic drug reactions. Significantly, there is an immunologic reaction to Streptolysin S, a toxin released by certain killed bacteria and associated with Penicillin injection, that can cause fatal cardiac syncope.
Allergic reactions to any β-lactam antibiotic may occur in up to 1% of patients receiving that agent. The allergic reaction is a Type I hypersensitivity reaction. Anaphylaxis will occur in approximately 0.01% of patients. It has previously been accepted that there was up to a 10% cross-sensitivity between penicillin-derivatives, cephalosporins, and carbapenems, due to the sharing of the β-lactam ring. Assessments in 2006 found no more risk for cross-allergy for second-generation or later cephalosporins than the first generation. However, as a general risk, research shows that all beta lactams have the intrinsic hazard of very serious hazardous reactions in susceptible patients. Only the frequency of these reactions vary, based on the structure.
Papers in 2006 showed that a major feature in determining frequency of immunological reactions is the similarity of the side chains (e.g., first generation cephalosporins are similar to penicillins); this is why the β-lactams are associated with different frequencies of serious reactions (e.g., anaphylaxis).
Mechanism of action.
Bacteria constantly remodel their peptidoglycan cell walls, simultaneously building and breaking down portions of the cell wall as they grow and divide. "β"-Lactam antibiotics inhibit the formation of peptidoglycan cross-links in the bacterial cell wall; this is achieved through binding of the four-membered "β"-lactam ring of penicillin to the enzyme DD-transpeptidase. As a consequence, DD-transpeptidase cannot catalyze formation of these cross-links, and an imbalance between cell wall production and degradation develops, causing the cell to rapidly die.
The enzymes that hydrolyze the peptidoglycan cross-links continue to function, even while those that form such cross-links do not. This weakens the cell wall of the bacterium, and osmotic pressure becomes increasingly uncompensated—eventually causing cell death (cytolysis). In addition, the build-up of peptidoglycan precursors triggers the activation of bacterial cell wall hydrolases and autolysins, which further digest the cell wall's peptidoglycans. The small size of the penicillins increases their potency, by allowing them to penetrate the entire depth of the cell wall. This is in contrast to the glycopeptide antibiotics vancomycin and teicoplanin, which are both much larger than the penicillins.
Gram-positive bacteria are called protoplasts when they lose their cell walls. Gram-negative bacteria do not lose their cell walls completely and are called spheroplasts after treatment with penicillin.
Penicillin shows a synergistic effect with aminoglycosides, since the inhibition of peptidoglycan synthesis allows aminoglycosides to penetrate the bacterial cell wall more easily, allowing their disruption of bacterial protein synthesis within the cell. This results in a lowered MBC for susceptible organisms.
Penicillins, like other "β"-lactam antibiotics, block not only the division of bacteria, including cyanobacteria, but also the division of cyanelles, the photosynthetic organelles of the glaucophytes, and the division of chloroplasts of bryophytes. In contrast, they have no effect on the plastids of the highly developed vascular plants. This supports the endosymbiotic theory of the evolution of plastid division in land plants.
The chemical structure of penicillin is triggered with a very precise, pH-dependent directed mechanism, effected by a unique spatial assembly of molecular components, which can activate by protonation. It can travel through bodily fluids, targeting and inactivating enzymes responsible for cell-wall synthesis in gram-positive bacteria, meanwhile avoiding the surrounding non-targets. Penicillin can protect itself from spontaneous hydrolysis in the body in its anionic form, while storing its potential as a strong acylating agent, activated only upon approach to the target transpeptidase enzyme and protonated in the active centre. This targeted protonation neutralizes the carboxylic acid moiety, which is weakening of the β-lactam ring N–C(=O) bond, resulting in a self-activation. Specific structural requirements are equated to constructing the perfect mouse trap for catching targeted prey.
Structure.
The term "penam" is used to describe the common core skeleton of a member of the penicillins. This core has the molecular formula R-C9H11N2O4S, where R is the variable side chain that differentiates the penicillins from one another. The penam core has a molecular weight of 243 g/mol, with larger penicillins having molecular weights near 450—for example, cloxacillin has a molecular weight of 436 g/mol. The key structural feature of the penicillins is the four-membered "β"-lactam ring; this structural moiety is essential for penicillin's antibacterial activity. The "β"-lactam ring is itself fused to a five-membered thiazolidine ring. The fusion of these two rings causes the "β"-lactam ring to be more reactive than monocyclic "β"-lactams because the two fused rings distort the "β"-lactam amide bond and therefore remove the resonance stabilisation normally found in these chemical bonds.
History.
Discovery.
In 1897 a French physician, Ernest Duchesne at École du Service de Santé Militaire in Lyon, published a medical thesis entitled "Contribution à l'étude de la concurrence vitale chez les micro-organismes : antagonisme entre les moisissures et les microbes" (Contribution to the study of the vital competition in micro-organisms: antagonism between molds and microbes) in which he specifically studied the interaction between "Escherichia coli" and "Penicillium glaucum". He independently discovered healing properties of "P. glaucum", even curing infected guinea pigs from typhoid. It is not known, however, whether the active chemical in these studies was in fact penicillin. "E. coli" and the causative agent of typhoid are both Gram-negative bacteria and are therefore significantly less susceptible to penicillin than other, Gram-positive, bacteria. In 1895, Italian physician Vincenzo Tiberio from the University of Naples published a study of molds that he found in a well near his house in Arzano, Italy ; he showed that certain molds — among them "Penicillium glaucum" — produce water-soluble substances that kill bacteria.
The discovery of penicillin is attributed to Scottish scientist and Nobel laureate Alexander Fleming in 1928. He showed that, if "Penicillium rubens" were grown in the appropriate substrate, it would exude a substance with antibiotic properties, which he dubbed penicillin. This serendipitous observation began the modern era of antibiotic discovery. The development of penicillin for use as a medicine is attributed to the Australian Nobel laureate Howard Walter Florey, together with the German Nobel laureate Ernst Chain and the English biochemist Norman Heatley.
Fleming recounted that the date of his discovery of penicillin was on the morning of Friday, September 28, 1928. The traditional version of this story describes the discovery as a fortuitous accident: in his laboratory in the basement of St Mary's Hospital in London (now part of Imperial College), Fleming noticed a Petri dish containing Staphylococcus that had been mistakenly left open, was contaminated by blue-green mould from an open window, which formed a visible growth. There was a halo of inhibited bacterial growth around the mould. Fleming concluded that the mould released a substance that repressed the growth and caused lysing of the bacteria.
Once Fleming made his discovery he grew a pure culture and discovered it was a "Penicillium" mould, now known to be "Penicillium notatum". Fleming coined the term "penicillin" to describe the filtrate of a broth culture of the "Penicillium" mould. Fleming asked C. J. La Touche to help identify the mould, which he incorrectly identified as "Penicillium rubrum" (later corrected by Charles Thom). He expressed initial optimism that penicillin would be a useful disinfectant, because of its high potency and minimal toxicity in comparison to antiseptics of the day, and noted its laboratory value in the isolation of "Bacillus influenzae" (now called "Haemophilus influenzae").
Fleming was a famously poor communicator and orator, which meant his findings were not initially given much attention. He was unable to convince a true chemist to help him extract and stabilize the antibacterial compound found in the broth filtrate. Despite the lack of a true chemist, he remained interested in the potential use of penicillin and presented a paper entitled "A Medium for the Isolation of Pfeiffer's Bacillus" to the Medical Research Club of London, which was met with little interest and even less enthusiasm by his peers. Had Fleming been more successful at making other scientists interested in his work, penicillin for medicinal use would possibly have been developed years earlier.
Despite the lack of interest of his fellow scientists, he did conduct several experiments on the antibiotic substance he discovered. The most important result proved it was nontoxic in humans by first performing toxicity tests in animals and then on humans. His following experiments on penicillin's response to heat and pH allowed Fleming to increase the stability of the compound. The one test that modern scientists would find missing from his work was the test of penicillin on an infected animal, the results of which would likely have sparked great interest in penicillin and sped its development by almost a decade.
Medical application.
In 1930, Cecil George Paine, a pathologist at the Royal Infirmary in Sheffield, attempted to use penicillin to treat sycosis barbae, eruptions in beard follicles, but was unsuccessful. Moving on to ophthalmia neonatorum, a gonococcal infection in infants, he achieved the first recorded cure with penicillin, on November 25, 1930. He then cured four additional patients (one adult and three infants) of eye infections, and failed to cure a fifth.
In 1939, Australian scientist Howard Florey (later Baron Florey) and a team of researchers (Ernst Boris Chain, Arthur Duncan Gardner, Norman Heatley, M. Jennings, J. Orr-Ewing and G. Sanders) at the Sir William Dunn School of Pathology, University of Oxford made progress in showing the "in vivo" bactericidal action of penicillin. In 1940 they showed that penicillin effectively cured bacterial infection in mice. In 1941 they treated a policeman, Albert Alexander, with a severe face infection; his condition improved, but then supplies of penicillin ran out and he died. Subsequently, several other patients were treated successfully.
Mass production.
By late 1940, the Oxford team under Howard Florey had devised a method of mass-producing the drug, but yields remained low. In 1941, Florey and Heatley traveled to the U.S. in order to interest pharmaceutical companies in producing the drug and inform them about their process.
Florey and Chain shared the 1945 Nobel Prize in Medicine with Fleming for their work.
The challenge of mass-producing this drug was daunting. On March 14, 1942, the first patient was treated for streptococcal septicemia with US-made penicillin produced by Merck & Co. Half of the total supply produced at the time was used on that one patient. By June 1942, just enough US penicillin was available to treat ten patients. In July 1943, the War Production Board drew up a plan for the mass distribution of penicillin stocks to Allied troops fighting in Europe. The results of fermentation research on corn steep liquor at the Northern Regional Research Laboratory at Peoria, Illinois, allowed the United States to produce 2.3 million doses in time for the invasion of Normandy in the spring of 1944. After a worldwide search in 1943, a mouldy cantaloupe in a Peoria, Illinois market was found to contain the best strain of mould for production using the corn steep liquor process.
Large-scale production resulted from the development of deep-tank fermentation by chemical engineer Margaret Hutchinson Rousseau. As a direct result of the war and the War Production Board, by June 1945, over 646 billion units per year were being produced.
G. Raymond Rettew made a significant contribution to the American war effort by his techniques to produce commercial quantities of penicillin.
During World War II, penicillin made a major difference in the number of deaths and amputations caused by infected wounds among Allied forces, saving an estimated 12%–15% of lives. Availability was severely limited, however, by the difficulty of manufacturing large quantities of penicillin and by the rapid renal clearance of the drug, necessitating frequent dosing. Methods for mass production of penicillin were patented by Andrew Jackson Moyer in 1945. Florey had not patented penicillin, having been advised by Sir Henry Dale that doing so would be unethical.
Penicillin is actively excreted, and about 80% of a penicillin dose is cleared from the body within three to four hours of administration. Indeed, during the early penicillin era, the drug was so scarce and so highly valued that it became common to collect the urine from patients being treated, so that the penicillin in the urine could be isolated and reused. This was not a satisfactory solution, so researchers looked for a way to slow penicillin excretion. They hoped to find a molecule that could compete with penicillin for the organic acid transporter responsible for excretion, such that the transporter would preferentially excrete the competing molecule and the penicillin would be retained. The uricosuric agent probenecid proved to be suitable. When probenecid and penicillin are administered together, probenecid competitively inhibits the excretion of penicillin, increasing penicillin's concentration and prolonging its activity. Eventually, the advent of mass-production techniques and semi-synthetic penicillins resolved the supply issues, so this use of probenecid declined. Probenecid is still useful, however, for certain infections requiring particularly high concentrations of penicillins.
After World War II, Australia was the first country to make the drug available for civilian use. In the U.S., penicillin was made available to the general public on March 15, 1945.
Structure determination and total synthesis.
In 1945 the chemical structure of penicillin was determined using X-ray crystallography by Dorothy Crowfoot Hodgkin, who was also working at Oxford. She later received the Nobel prize for this and other structure determinations.
Chemist John C. Sheehan at the Massachusetts Institute of Technology (MIT) completed the first chemical synthesis of penicillin in 1957. Sheehan had started his studies into penicillin synthesis in 1948, and during these investigations developed new methods for the synthesis of peptides, as well as new protecting groups—groups that mask the reactivity of certain functional groups. Although the initial synthesis developed by Sheehan was not appropriate for mass production of penicillins, one of the intermediate compounds in Sheehan's synthesis was 6-aminopenicillanic acid (6-APA), the nucleus of penicillin. Attaching different groups to the 6-APA 'nucleus' of penicillin allowed the creation of new forms of penicillin.
Developments from penicillin.
The narrow range of treatable diseases or "spectrum of activity" of the penicillins, along with the poor activity of the orally active phenoxymethylpenicillin, led to the search for derivatives of penicillin that could treat a wider range of infections. The isolation of 6-APA, the nucleus of penicillin, allowed for the preparation of semisynthetic penicillins, with various improvements over benzylpenicillin (bioavailability, spectrum, stability, tolerance).
The first major development was ampicillin in 1961. It offered a broader spectrum of activity than either of the original penicillins. Further development yielded β-lactamase-resistant penicillins, including flucloxacillin, dicloxacillin, and methicillin. These were significant for their activity against β-lactamase-producing bacterial species, but were ineffective against the methicillin-resistant "Staphylococcus aureus" (MRSA) strains that subsequently emerged.
Another development of the line of true penicillins was the antipseudomonal penicillins, such as carbenicillin, ticarcillin, and piperacillin, useful for their activity against Gram-negative bacteria. However, the usefulness of the β-lactam ring was such that related antibiotics, including the mecillinams, the carbapenems and, most important, the cephalosporins, still retain it at the center of their structures.
Production.
Penicillin is a secondary metabolite of certain species of "Penicillium" and is produced when growth of the fungus is inhibited by stress. It is not produced during active growth. Production is also limited by feedback in the synthesis pathway of penicillin.
The by-product, -lysine, inhibits the production of homocitrate, so the presence of exogenous lysine should be avoided in penicillin production.
The "Penicillium" cells are grown using a technique called fed-batch culture, in which the cells are constantly subject to stress, which is required for induction of penicillin production. The available carbon sources are also important: Glucose inhibits penicillin production, whereas lactose does not. The pH and the levels of nitrogen, lysine, phosphate, and oxygen of the batches must also be carefully controlled.
The biotechnological method of directed evolution has been applied to produce by mutation a large number of "Penicillium" strains. These techniques include error-prone PCR, DNA shuffling, , and strand-overlap PCR.
Semisynthetic penicillins are prepared starting from the penicillin nucleus 6-APA.
Biosynthesis.
Overall, there are three main and important steps to the biosynthesis of penicillin G (benzylpenicillin).

</doc>
<doc id="23313" url="https://en.wikipedia.org/wiki?curid=23313" title="Piri Reis">
Piri Reis

Ahmed Muhiddin Piri (1465/70–1553), better known as Piri Reis ( or "Hacı Ahmed Muhiddin Pîrî Bey"), was an Ottoman admiral, geographer, and cartographer.
He is primarily known today for his maps and charts collected in his "Kitab-ı Bahriye" ("Book of Navigation"), a book that contains detailed information on navigation, as well as very accurate charts (for their time) describing the important ports and cities of the Mediterranean Sea. He gained fame as a cartographer when a small part of his first world map (prepared in 1513) was discovered in 1929 at the Topkapı Palace in Istanbul. His world map is the oldest known Turkish atlas showing the New World, and one of the oldest maps of America still in existence anywhere (the oldest known map of America that is still in existence is the map drawn by Juan de la Cosa in 1500). Piri Reis' map is centered on the Sahara at the latitude of the Tropic of Cancer.
In 1528, Piri Reis drew a second world map, of which a small fragment (showing Greenland and North America from Labrador and Newfoundland in the north to Florida, Cuba, Hispaniola, Jamaica and parts of Central America in the south) still survives. According to his imprinting text, he had drawn his maps using about 20 foreign charts and mappae mundi (Arab, Spanish, Portuguese, Chinese, Indian and Greek) including one by Christopher Columbus. He was executed in 1553.
Biography.
For many years, little was known about the identity of Piri Reis. His name means "Captain Piri".
Today, based on the Ottoman archives, it is known that his full name was "Hacı Ahmed Muhiddin Piri" and that he was born either in Gelibolu (Gallipoli) on the European part of the Ottoman Empire (in present-day Turkish Thrace), or in Karaman (his father's birthplace) in central Anatolia, then the capital of the Beylik of Karaman (annexed by the Ottoman Empire in 1487). The exact date of his birth is unknown. His father's name was Hacı Mehmed Piri. The honorary and informal Islamic title "Hadji" (Turkish: "Hacı") in Piri's and his father's names indicate that they both had completed the Hajj (Islamic pilgrimage) by going to Mecca during the dedicated period.
Piri began engaging in government-supported privateering (a common practice in the Mediterranean Sea among both the Muslim and Christian states of the 15th and 16th centuries) when he was young, in 1481, following his uncle Kemal Reis, a well-known corsair and seafarer of the time, who later became a famous admiral of the Ottoman Navy. During this period, together with his uncle, he took part in many naval wars of the Ottoman Empire against Spain, the Republic of Genoa and the Republic of Venice, including the First Battle of Lepanto (Battle of Zonchio) in 1499 and Second Battle of Lepanto (Battle of Modon) in 1500. When his uncle Kemal Reis died in 1511 (his ship was wrecked by a storm in the Mediterranean Sea, while he was heading to Egypt), Piri returned to Gelibolu, where he started working on his studies about navigation.
By 1516, he was again at sea as a ship's captain in the Ottoman fleet. He took part in the 1516–17 Ottoman conquest of Egypt. In 1522 he participated in the Siege of Rhodes against the Knights of St. John, which ended with the island's surrender to the Ottomans on 25 December 1522 and the permanent departure of the Knights from Rhodes on 1 January 1523 (the Knights relocated briefly to Sicily and later permanently to Malta). In 1524 he captained the ship that took the Ottoman Grand Vizier Pargalı İbrahim Pasha to Egypt.
In 1547, Piri had risen to the rank of Reis (admiral) as the Commander of the Ottoman Fleet in the Indian Ocean and Admiral of the Fleet in Egypt, headquartered in Suez. On 26 February 1548 he recaptured Aden from the Portuguese, followed in 1552 by the capture of Muscat, which Portugal had occupied since 1507, and the strategically important island of Kish. Turning further east, Piri Reis captured the island of Hormuz in the Strait of Hormuz, at the entrance of the Persian Gulf. When the Portuguese turned their attention to the Persian Gulf, Piri Reis occupied the Qatar peninsula and the island of Bahrain to deprive the Portuguese of suitable bases on the Arabian coast.
He then returned to Egypt, an old man approaching the age of 90. When he refused to support the Ottoman Vali (Governor) of Basra, Kubad Pasha, in another campaign against the Portuguese in the northern Persian Gulf, Piri Reis was beheaded in 1553.
Several warships and submarines of the Turkish Navy have been named after Piri Reis.
"Kitab-ı Bahriye".
Piri Reis is the author of the "Kitab-ı Bahriye", or "Book of Navigation", one of the most famous cartographical works of the period.
The work was first published in 1521, and it was revised in 1524-1525 with additional information and better-crafted charts in order to be presented as a gift to Suleiman I. The revised edition had a total of 434 pages containing 290 maps.
Sources.
Although he was not an explorer and never sailed to the Atlantic, he compiled over twenty maps of Arab, Spanish, Portuguese, Chinese, Indian and older Greek origins into a comprehensive representation of the known world of his era. This work included the recently explored shores of both the African and American continents; on his first World Map of 1513, he imprinted the description "these lands and islands are drawn from the map of Columbus." 
In his text, he also wrote that he used the "maps drawn in the time of Alexander the Great" as a source, but most likely he had mistakenly confused the 2nd-century Greek geographer Ptolemy with Alexander's general of the same name (of four and a half centuries before), since his map is similar with the Jan of Stobnica famous reproduction map of Ptolemy, printed in 1512. 
Ptolemy's "Geographia" had been translated in Turkish after a personal order of Mehmed II some decades before. 
It can be seen that the Atlantic part of the map originates with Columbus because of the errors it contains (such as Columbus’ belief that Cuba was a continental peninsula) since at the time the manuscript was produced, the Spaniards had already been in Mexico for two years.
Contents.
Apart from the maps, the book also contained detailed information on the major ports, bays, gulfs, capes, peninsulas, islands, straits and ideal shelters of the Mediterranean Sea, as well as techniques of navigation and navigation-related information on astronomy, together with information about the local people of each country and city and the curious aspects of their culture. There are thirty legends around the world map, twenty-nine in Turkish and one in Arabic; the latter gives the date as the month Muharrem of AH 919 AH (i.e. the spring of 1513) but most studies have identified the more probable date of completion as 1521.
The "Kitab-ı Bahriye" has two main sections, with the first section dedicated to information about the types of storms; techniques of using a compass; portolan charts with detailed information on ports and coastlines; methods of finding direction using the stars; and characteristics of the major oceans and the lands around them. Special emphasis is given to the discoveries in the New World by Christopher Columbus and those of Vasco da Gama and the other Portuguese seamen on their way to India and the rest of Asia.
The second section is entirely composed of portolan charts and cruise guides. Each topic contains the map of an island or coastline. In the first book (1521), this section has a total of 132 portolan charts, while the second book (1525) has a total of 210 portolan charts. The second section starts with the description of the Dardanelles Strait and continues with the islands and coastlines of the Aegean Sea, Ionian Sea, Adriatic Sea, Tyrrhenian Sea, Ligurian Sea, the French Riviera, the Balearic Islands, the coasts of Spain, the Strait of Gibraltar, the Canary Islands, the coasts of North Africa, Egypt and the River Nile, the Levant and the coastline of Anatolia. This section also includes descriptions and drawings of the famous monuments and buildings in every city, as well as biographic information about Piri Reis who also explains the reasons why he preferred to collect these charts in a book instead of drawing a single map, which would not be able to contain so much information and detail.
A century after Piri's death and during the second half of the 17th century, a third version of his book was produced, which left the text of the second version unaffected while enriching the cartographical part of the manuscript. 
It included additional new large-scale maps, mostly copies of the Italian (from Battista Agnese and Jacopo Gastaldi) and Dutch (Abraham Ortelius) works of the previous century. 
These maps were much more accurate and depict the Black Sea, which was not included in the original.
Manuscripts.
Copies of the Kitab-ı Bahriye are found in various libraries in Istanbul and in some of the major libraries in Europe, besides one copy known to be held privately in the USA (Walters Art Museum).
Copies of the first edition (1521):
Copies of the second edition (1525):
In popular culture.
Piri Reis is mentioned and debuts in the video games and respectively.
In Brotherhood, a group of Italian Assassins sent from Rome to Constantinople by Ezio Auditore da Firenze infiltrated Piri Reis' shop to steal some of his maps detailing the New World, in order to match the Templars' expansion into the new lands.
By Revelations, despite his earlier conflict with the Assassins, Piri joined the Ottoman Assassin Brotherhood in 1506 to serve as a scholar and technician, and even eventually progressed to the rank of Master Assassin. Later, he acts as a pyrotechnician, scholar and cartographer for the Ottoman Assassin Brotherhood. He meets Ezio later in the game, and they start discussing about bombs.
Further reading.
Editions of "Kitab-ı Bahriye"
1513 map:

</doc>
<doc id="23315" url="https://en.wikipedia.org/wiki?curid=23315" title="Physician">
Physician

A physician or medical doctor is a professional who practices medicine, which is concerned with promoting, maintaining, or restoring human health through the study, diagnosis, and treatment of disease, injury, and other physical and mental impairments. Physicians may focus their practice on certain disease categories, types of patients, or methods of treatment—known as specialist medical practitioners—or assume responsibility for the provision of continuing and comprehensive medical care to individuals, families, and communities—known as general practitioners. Medical practice properly requires both a detailed knowledge of the academic disciplines (such as anatomy and physiology) underlying diseases and their treatment—the "science" of medicine—and also a decent competence in its applied practice—the art or "craft" of medicine.
Both the role of the physician and the meaning of the word itself vary around the world. Degrees and other qualifications vary widely, but there are some common elements, such as medical ethics requiring that physicians show consideration, compassion, and benevolence for their patients.
Modern meanings.
Specialist in internal medicine.
Around the world the term physician refers to a specialist in internal medicine or one of its many sub-specialties (especially as opposed to a specialist in surgery). This meaning of physician conveys a sense of expertise in treatment by drugs or medications, rather than by the procedures of surgeons.
This term is at least nine hundred years old in English: physicians and surgeons were once members of separate professions, and traditionally were rivals. The Shorter Oxford English Dictionary, third edition, gives a Middle English quotation making this contrast, from as early as 1400: "O Lord, whi is it so greet difference betwixe a cirugian and a physician."
Henry VIII granted a charter to the London Royal College of Physicians in 1518. It was not until 1540 that he granted the Company of Barber/Surgeons (ancestor of the Royal College of Surgeons) its separate charter. In the same year, the English monarch established the Regius Professorship of Physic at the University of Cambridge. Newer universities would probably describe such an academic as a professor of internal medicine. Hence, in the 16th century, "physic" meant roughly what internal medicine does now.
Currently, a specialist physician in the United States may be described as an "internist". Another term, "hospitalist", was introduced in 1996, to describe US specialists in internal medicine who work largely or exclusively in hospitals. Such 'hospitalists' now make up about 19% of all US "general internists", who are often called "general physicians" in Commonwealth countries.
This original use, as distinct from surgeon, is common in most of the world including the United Kingdom and other Commonwealth countries (such as Australia, Bangladesh, India, New Zealand, Pakistan, South Africa, Sri Lanka, Zimbabwe), as well as in places as diverse as Brazil, Hong Kong, Indonesia, Japan, Ireland, and Taiwan. In such places, the more general English terms "doctor" or "medical practitioner" are prevalent, describing any practitioner of medicine (whom an American would likely call a physician, in the broad sense). In Commonwealth countries, specialist pediatricians and geriatricians are also described as specialist physicians who have sub-specialized by age of patient rather than by organ system.
Physician and surgeon.
Around the world, the combined term "Physician and Surgeon" is used to describe either a general practitioner or any medical practitioner irrespective of specialty. This usage still shows the original meaning of physician and preserves the old difference between a physician, as a practitioner of "physic", and a surgeon. The term may be used by state medical boards in the United States of America, and by equivalent bodies in provinces of Canada, to describe any medical practitioner.
North America.
In modern English, the term "physician" is used in two main ways, with relatively broad and narrow meanings respectively. This is the result of history and is often confusing. These meanings and variations are explained below.
In the United States and Canada, the term "physician" describes all medical practitioners holding a professional medical degree. The American Medical Association, established in 1847, as well as the American Osteopathic Association, founded in 1897, both currently use the term "physician" to describe members. However, the American College of Physicians, established in 1915, does not: its title uses "physician" in its original sense.
American physicians.
A physician trained in the United States has either a Doctor of Medicine degree, and uses the initials "M.D." or has a Doctor of Osteopathic Medicine degree and uses the initials "D.O." After finishing their medical school education both a Doctor of Medicine (MD) and Doctor of Osteopathic Medicine (DO), with licensure, have the same practicing rights in the specialties and subspecialties of medicine. United States osteopathic medical schools have a curriculum very similar to MD schools with the addition of osteopathic manipulative medicine, which focuses on extra instruction in the musculoskeletal system.
All boards of certification now require that physicians demonstrate, by examination, continuing mastery of the core knowledge and skills for a chosen specialty. Recertification varies by particular specialty between every seven and every ten years.
Graduates of osteopathic medical schools in the United States (osteopathic physicians) should not be confused with osteopaths, who are trained in the European and Commonwealth nations. Osteopaths (the term used for non-American-trained practitioners who practice osteopathic manipulation) are not physicians. Their training is similar to physical therapy and they are not licensed to prescribe medications or perform surgeries.
Podiatric physicians.
Also in the United States, the American Podiatric Medical Association (APMA) defines podiatrists as physicians and surgeons that fall under the department of surgery in hospitals. They undergo training with the Doctor of Podiatric Medicine (DPM) degree. This degree is also available at one Canadian university, namely the Université du Québec à Trois-Rivières. Students are typically required to complete an internship in New York prior to the obtention of their professional degree.
Shortage.
Many countries in the developing world have the problem of too few physicians. A shortage of doctors can lead to diseases spreading out of control as seen in the Ebola virus epidemic in West Africa. In 2015, the Association of American Medical Colleges warned that the US will face a doctor shortage of as many as 90,000 by 2025.
Social role and world view.
Biomedicine.
Within Western culture and over recent centuries, medicine has become increasingly based on scientific reductionism and materialism. This style of medicine is now dominant throughout the industrialized world, and is often termed "biomedicine" by medical anthropologists. Biomedicine "formulates the human body and disease in a culturally distinctive pattern", and is a world view learnt by medical students. Within this tradition, the medical model is a term for the complete "set of procedures in which all doctors are trained" (R. D. Laing, 1972), including mental attitudes. A particularly clear expression of this world view, currently dominant among conventional physicians, is evidence-based medicine. Within conventional medicine, most physicians still pay heed to their ancient traditions:
— Sir William Osler, "Chauvanism in Medicine" (1902)
In this Western tradition, physicians are considered to be members of a learned profession, and enjoy high social status, often combined with expectations of a high and stable income and job security. However, medical practitioners often work long and inflexible hours, with shifts at unsociable times. Their high status is partly from their extensive training requirements, and also because of their occupation's special ethical and legal duties. The term traditionally used by physicians to describe a person seeking their help is the word "patient" (although one who visits a physician for a routine check-up may also be so described). This word patient is an ancient reminder of medical duty, as it originally meant 'one who suffers'. The English noun comes from the Latin word "patiens", the present participle of the deponent verb, patior, meaning 'I am suffering,' and akin to the Greek verb πάσχειν (= "paskhein", to suffer) and its cognate noun πάθος (= pathos).
Physicians in the original, narrow sense (specialist physicians or internists, see above) are commonly members or fellows of professional organizations, such as the American College of Physicians or the Royal College of Physicians in the United Kingdom, and such hard-won membership is itself a mark of status. 
Alternative medicine.
While contemporary biomedicine has distanced itself from its ancient roots in religion and magic, many forms of traditional medicine and alternative medicine continue to espouse vitalism in various guises: 'As long as life had its own secret properties, it was possible to have sciences and medicines based on those properties' (Grossinger 1980). The US National Center for Complementary and Alternative Medicine (NCCAM) classifies CAM therapies into five categories or domains, including: alternative medical systems, or complete systems of therapy and practice; mind-body interventions, or techniques designed to facilitate the mind's effect on bodily functions and symptoms; biologically based systems including herbalism; and manipulative and body-based methods such as chiropractic and massage therapy.
In considering these alternate traditions that differ from biomedicine (see above), medical anthropologists emphasize that "all" ways of thinking about health and disease have a significant cultural content, including conventional western medicine.
Ayurveda, Unani medicine and homeopathy are popular types of alternative medicine. They are included in national system of medicines in countries such as India. In general, the practitioners of these medicine in these countries are referred to as Ved, Hakim and homeopathic doctor/homeopath/homeopathic physician, respectively.
Physicians' own health.
Some commentators have argued that physicians have duties to serve as role models for the general public in matters of health, for example by not smoking cigarettes. Indeed, in most western nations relatively few physicians smoke, and their professional knowledge does appear to have a beneficial effect on their health and lifestyle. According to a study of male physicians, life expectancy is slightly higher for physicians (73.0 years for white and 68.7 for black) than lawyers or many other highly educated professionals. Causes of death less likely in physicians than the general population include respiratory disease (including pneumonia, pneumoconioses, COPD, but excluding emphysema and other chronic airway obstruction), alcohol-related deaths, rectosigmoidal and anal cancers, and bacterial diseases.
Physicians do experience exposure to occupational hazards, and there is a well-known aphorism that "doctors make the worst patients". Causes of death that are shown to be higher in the physician population include suicide among doctors and self-inflicted injury, drug-related causes, traffic accidents, and cerebrovascular and ischaemic heart disease.
Education and training.
Medical education and career pathways for doctors vary considerably across the world.
All medical practitioners.
In all developed countries, entry-level medical education programs are tertiary-level courses, undertaken at a medical school attached to a university. Depending on jurisdiction and university, entry may follow directly from secondary school or require pre-requisite undergraduate education. The former commonly takes five or six years to complete. Programs that require previous undergraduate education (typically a three- or four-year degree, often in Science) are usually four or five years in length. Hence, gaining a basic medical degree may typically take from five to eight years, depending on jurisdiction and university.
Following completion of entry-level training, newly graduated medical practitioners are often required to undertake a period of supervised practice before full registration is granted, typically one or two years. This may be referred to as an "internship", as the "foundation" years in the UK, or as "conditional registration". Some jurisdictions, including the United States, require residencies for practice.
Medical practitioners hold a medical degree specific to the university from which they graduated. This degree qualifies the medical practitioner to become licensed or registered under the laws of that particular country, and sometimes of several countries, subject to requirements for internship or conditional registration.
Specialists in internal medicine.
Specialty training is begun immediately following completion of entry-level training, or even before. In other jurisdictions, junior medical doctors must undertake generalist (un-streamed) training for one or more years before commencing specialization. Hence, depending on jurisdiction, a specialist physician (internist) often does not achieve recognition as a specialist until twelve or more years after commencing basic medical training—five to eight years at university to obtain a basic medical qualification, and up to another nine years to become a specialist.
Regulation.
In most jurisdictions, physicians (in either sense of the word) need government permission to practice. Such permission is intended to promote public safety, and often to protect the public purse, as medical care is commonly subsidized by national governments.
In some jurisdictions (e.g., Singapore), it is common for physicians to inflate their qualifications with the title "Dr" in correspondence or namecards, even if their qualifications are limited to a basic (e.g., bachelor level) degree. In other countries (e.g., Germany), only physicians holding an academic doctorate may call themselves doctor.
All medical practitioners.
Among the English-speaking countries, this process is known either as licensure as in the United States, or as registration in the United Kingdom, other Commonwealth countries, and Ireland. Synonyms in use elsewhere include "colegiación" in Spain, "ishi menkyo" in Japan, "autorisasjon" in Norway, "Approbation" in Germany, and "άδεια εργασίας" in Greece. In France, Italy and Portugal, civilian physicians must be members of the Order of Physicians to practice medicine.
In some countries, including the United Kingdom and Ireland, the profession largely regulates itself, with the government affirming the regulating body's authority. The best known example of this is probably the General Medical Council of Britain. In all countries, the regulating authorities will revoke permission to practice in cases of malpractice or serious misconduct.
In the large English-speaking federations (United States, Canada, Australia), the licensing or registration of medical practitioners is done at a state or provincial level or nationally as in New Zealand. Australian states usually have a "Medical Board," which has now been replaced by the Australian Health Practitioner Regulatory Authority (AHPRA) in most states, while Canadian provinces usually have a "College of Physicians and Surgeons." All American states have an agency that is usually called the "Medical Board", although there are alternate names such as "Board of Medicine," "Board of Medical Examiners", "Board of Medical Licensure", "Board of Healing Arts" or some other variation. After graduating from a first-professional school, physicians who wish to practice in the U.S. usually take standardized exams, such as the USMLE for MDs and DOs or the COMLEX-USA for DOs, which is not available to MDs (although most DOs in the U.S. also take the USMLE exams and undergo the same residency training as MDs).
Specialists in internal medicine.
Most countries have some method of officially recognizing specialist qualifications in all branches of medicine, including internal medicine. Sometimes, this aims to promote public safety by restricting the use of hazardous treatments. Other reasons for regulating specialists may include standardization of recognition for hospital employment and restriction on which practitioners are entitled to receive higher insurance payments for specialist services.
Performance and professionalism supervision.
The issue of medical errors, drug abuse, and other issues in physician professional behavior received significant attention across the world, in particular following a critical 2000 report which "arguably launched" the patient-safety movement. In the U.S., as of 2006 there were few organizations that systematically monitored performance. In the U.S. only the Department of Veterans Affairs randomly drug tests, in contrast to drug testing practices for other professions that have a major impact on public welfare. Licensing boards at the U.S. state level depend upon continuing education to maintain competence. Through the utilization of the National Practitioner Data Bank, Federation of State Medical Boards Disciplinary Report, and American Medical Association Physician Profile Service, the 67 State Medical Boards (MD/DO) continually self-report any Adverse/Disciplinary Actions taken against a licensed Physician in order that the other Medical Boards in which the Physician holds or is applying for a medical license will be properly notified so that corrective, reciprocal action can be taken against the offending physician. In Europe, as of 2009 the health systems are governed according to various national laws, and can also vary according to regional differences similar to the United States.
Related occupations and divisions of labor.
Chiropractors.
Chiropractors use the physician title in some countries. In the United States, practitioners with a Doctor of Chiropractic (DC) have been added to the list of recognized physicians by the Joint Commission on Accreditation of Healthcare Organizations. This change does not affect or alter any health care practitioner’s license or scope of practice. Some medical organizations have criticized the addition of chiropractic to the definition of physician.
In Switzerland, students since 2008 have the option of studying in the University of Zurich medical school earning a Bachelor of Medicine (with a focus on chiropractic) and a Masters in Chiropractic Medicine. By attending medical school, they become "physicians" in the more traditional sense. Swiss chiropractors have been found to treat conditions in a similar way to their international counterparts while enjoying a greater number of medical specialist referrals.
Nurse practitioners.
Nurse practitioners (NPs) in the United States are advanced practice registered nurses holding a post-graduate degree such as a Doctor of Nursing Practice. In Canada, nurse practitioners typically have a Master of nursing degree as well as substantial experience they have accumulated throughout the years. Nurse practitioners are not considered physicians but may practice in similar fields as physicians with an emphasis on primary care. Nurse practitioners are educated in nursing theory and nursing practice rather than pure and applied medical practice. The scope of practice for a nurse practitioner in the United States is defined by regulatory boards of nursing, as opposed to boards of medicine that regulate medical doctors.

</doc>
<doc id="23316" url="https://en.wikipedia.org/wiki?curid=23316" title="Pound (mass)">
Pound (mass)

The pound or pound-mass (abbreviations: lb, lb"m", lbm, ℔) is a unit of mass 
used in the imperial, United States customary and other systems of measurement. A number of different definitions have been used, the most common today being the international avoirdupois pound which is legally defined as exactly , and which is divided into 16 avoirdupois ounces.
The unit is descended from the Roman "libra" (hence the abbreviation "lb"); the name "pound" is a Germanic adaptation of the Latin phrase "libra pondo", "a pound by weight".
Usage of the unqualified term "pound" reflects the historical conflation of mass and weight. This accounts for the modern distinguishing terms "pound-mass" and "pound-force".
Current use.
The United States and countries of the Commonwealth of Nations agreed upon common definitions for the pound and the yard. Since 1 July 1959, the international avoirdupois pound has been defined as exactly .
In the United Kingdom, the use of the international pound was implemented in the Weights and Measures Act 1963.
An avoirdupois pound is equal to 16 avoirdupois ounces and to exactly 7,000 grains. The conversion factor between the kilogram and the international pound was therefore chosen to be divisible by 7, and an (international) grain is thus equal to exactly .
In the UK, the process of metrication and European units of measurement directives were expected to eliminate the use of the pound and ounce, but in 2007 the European Commission abandoned the requirement for metric-only labelling on packaged goods there, and allowed for dual metric–imperial marking to continue indefinitely. When used as a measurement of body weight the UK practice remains to use the stone of 14 avoirdupois pounds as the primary measure e.g. "11 stone 4 pounds", rather than "158 pounds" (as done in the US), or "72 kilograms" as used elsewhere.
The US has not adopted the metric system despite many efforts to do so, and the pound remains widely used as one of the key United States customary units.
Historic use.
Historically, in different parts of the world, at different points in time, and for different applications, the pound (or its translation) has referred to broadly similar but not identical standards of mass or force.
Roman "libra".
The libra (Latin for "scales / balance") is an ancient Roman unit of mass that was equivalent to approximately 328.9 grams. It was divided into 12 "unciae" (singular: "uncia"), or ounces. The "libra" is the origin of the abbreviation for pound, "lb".
In Britain.
A number of different definitions of the pound have historically been used in Britain. Amongst these were the avoirdupois pound and the obsolete tower, merchant's and London pounds. Troy pounds and ounces remain in use only for the weight of certain precious metals, especially in the trade; these are normally quoted just in ounces (e.g. "500 ounces") and, when the type of ounce is not explicitly stated, the troy system is assumed.
Historically, the pound sterling was a tower pound of silver. In 1528, the standard was changed to the Troy pound.
Avoirdupois pound.
The avoirdupois pound, also known as the wool pound, first came into general use c. 1300. It was initially equal to 6992 troy grains. The pound avoirdupois was divided into 16 ounces. During the reign of Queen Elizabeth, the avoirdupois pound was redefined as 7,000 troy grains. Since then, the grain has often been an integral part of the avoirdupois system. By 1758, two Elizabethan Exchequer standard weights for the avoirdupois pound existed, and when measured in troy grains they were found to be of 7,002 grains and 6,999 grains.
Imperial Standard Pound.
In the United Kingdom, weights and measures have been defined by a long series of Acts of Parliament, the intention of which has been to regulate the sale of commodities. Materials traded in the marketplace are quantified according to accepted units and standards in order to avoid fraud. The standards themselves are legally defined so as to facilitate the resolution of disputes brought to the courts; only legally defined measures will be recognised by the courts. Quantifying devices used by traders (weights, weighing machines, containers of volumes, measures of length) are subject to official inspection, and penalties apply if they are fraudulent.
The Weights and Measures Act of 1878 marked a major overhaul of the British system of weights and measures, and the definition of the pound given there remained in force until the 1960s. The pound was defined thus (Section 4) "The ... platinum weight ... deposited in the Standards department of the Board of Trade ... shall continue to be the imperial standard of ... weight ... and the said platinum weight shall continue to be the Imperial Standard for determining the Imperial Standard Pound for the United Kingdom". Paragraph 13 states that the weight "in vacuo" of this standard shall be called the Imperial Standard Pound, and that all other weights mentioned in the act and permissible for commerce shall be ascertained from it alone. The First Schedule of the Act gave more details of the standard pound: it is a platinum cylinder nearly high, and diameter, and the edges are carefully rounded off. It has a groove about from the top, to allow the cylinder to be lifted using an ivory fork. It was constructed following the destruction of the Houses of Parliament by fire in 1834, and is stamped P.S. 1844, 1 lb (P.S. stands for "Parliamentary Standard"). This definition of the Imperial pound remains unchanged.
Relationship to the kilogram.
The 1878 Act said that contracts worded in terms of metric units would be deemed by the courts to be made according to the Imperial units defined in the Act, and a table of metric equivalents was supplied so that the Imperial equivalents could be legally calculated. This defined, in UK law, metric units in terms of Imperial ones. The equivalence for the pound was given as 1 lb = or 0.45359 kg, which made the kilogram equivalent to about . In 1883, it was determined jointly by the Standards Department of the Board of Trade and the Bureau International that was a better approximation, and this figure, rounded to was given legal status by an Order in Council in May 1898.
However, in 1963, a new Weights and Measures Act reversed this relationship and the pound was defined for the first time as a mass equal to to match the definition of the international pound agreed in 1959.
Troy pound.
A troy pound is equal to 12 troy ounces and to 5,760 grains, that is exactly grams. Troy weights were used in England by jewellers. Apothecaries also used the troy pound and ounce, but added the drachms and scruples unit in the Apothecaries' system of weights.
Troy weight may take its name from the French market town of Troyes in France where English merchants traded at least as early as the early 9th century.
The troy pound is no longer in general use or a legal unit for trade (it was abolished in the United Kingdom on 6 January 1879 by the Weights and Measures Act of 1878), but the troy ounce, of a troy pound, is still used for measurements of gems such as opals, and precious metals such as silver, platinum and particularly gold.
Tower pound.
The system called tower weight was the more general name for King Offa's pound. This dates to 757 AD and was based on the silver penny. This in turn was struck over Arabic dirhams (2d). The pound was based on the weight of 120 Arabic silver dirhams, which have been found in Offa's Dyke. The same coin weight was used throughout the Hanseatic League.
The mercantile pound (1304) of 6750 troy grains, or 9600 tower grains, derives from this pound, as 25 shilling-weights or 15 tower ounces, for general commercial use. Multiple pounds based on the same ounce were quite common. In much of Europe, the apothecaries' and commercial pounds were different numbers of the same ounce. 
The tower system was referenced to a standard prototype found in the Tower of London and ran concurrently with the avoirdupois and troy systems until the reign of Henry VIII, when a royal proclamation dated 1526 required that the Troy pound to be used for mint purposes instead of the Tower pound.
The tower pound was equivalent to about 350 grams.
Merchants' pound.
The merchants' pound ("mercantile pound", "libra mercantoria", or "commercial pound") was considered to be composed of 25 rather than 20 Tower shillings of 12 pence. It was equal to 9,600 wheat grains (15 tower ounces or 6,750 grains) and was used in England until the 14th century for goods other than money and medicine ("electuaries").
London pound.
The London pound is that of the Hansa, as used in their various trading places. This is based on 16 tower ounces, each ounce divided as the tower ounce. It never became a legal standard in England; the use of this pound waxed and waned with the influence of the Hansa itself.
A London pound was equal to 7,200 troy grains (16 tower ounces or, equivalently, 15 troy ounces).
In the United States.
In the United States, the avoirdupois pound as a unit of mass has been officially defined in terms of the kilogram since the Mendenhall Order of 1893. That Order defined the pound to be pounds to a kilogram. The following year, this relationship was refined as pounds to a kilogram, following a determination of the British pound.
According to a 1959 NIST publication, the United States 1894 pound differed from the international pound by approximately one part in 10 million. The difference is so insignificant that it can be ignored for almost all practical purposes.
Byzantine litra.
The Byzantines used a series of measurements known as pounds (, , "litra"). The most common was the "logarikē litra" (λογαρική λίτρα, "pound of account"), established by Constantine the Great in 309/310. It formed the basis of the Byzantine monetary system, with one "litra" of gold equivalent to 72 "solidi". A hundred "litrai" were known as a "kentēnarion" (κεντηνάριον, "hundredweight"). Its weight seems to have decreased gradually from the original 324 grams to 319. Due to its association with gold, it was also known as the "chrysaphikē litra" (χρυσαφική λίτρα, "gold pound") or "thalassia litra" (θαλάσσια λίτρα, "maritime pound"), but it could also be used as a measure of land, equalling a fortieth of the "thalassios modios".
The "soualia litra" was specifically used for weighing olive oil or wood, and corresponded to 4/5 of the "logarikē", i.e. 256 g. Some outlying regions, especially in later times, adopted various local measures, based on Italian, Arab or Turkish measures. The most important of these was the "argyrikē litra" (αργυρική λίτρα, "silver pound") of 333 g, found in Trebizond and Cyprus, and probably of Arab origin.
French livre.
Since the Middle Ages, various pounds ("livre") have been used in France. Since the 19th century, a "livre" has referred to the "metric pound", 500g.
The "livre esterlin" was equivalent to about and was used between the late 9th century and the mid-14th century.
The "livre poids de marc" or "livre de Paris" was equivalent to about 489.5 grams (7,555 gr) and was used between the 1350s and the late 18th century. It was introduced by the government of John II.
The "livre métrique" was set equal to the kilogram by the decree of "13 Brumaire an IX" between 1800 and 1812. This was a form of official metric pound.
The "livre usuelle" (customary unit) was defined as 500 grams by the decree of 28 March 1812. It was abolished as a unit of mass effective 1 January 1840 by a decree of 4 July 1837, but is still used informally.
German and Austrian Pfund.
Originally derived from the Roman libra, the definition varied throughout Germany in the Middle Ages and onward. The measures and weights of the Habsburg monarchy were reformed in 1761 by Empress Maria Theresia of Austria. The unusually heavy Habsburg (civil) pound of 16 ounces was later defined in terms of 560.012 grams. Bavarian reforms in 1809 and 1811 adopted essentially the same standard pound. In Prussia, a reform in 1816 defined a uniform civil pound in terms of the Prussian foot and distilled water, resulting in a Prussian pound of 467.711 grams.
Between 1803 and 1815, all German regions west of the River Rhine were French, organised in the departements: Roer, Sarre, Rhin-et-Moselle, and Mont-Tonnerre. As a result of the Congress of Vienna, these became part of various German states. However, many of these regions retained the metric system and adopted a metric pound of precisely 500 grams. In 1854, the pound of 500 grams also became the official mass standard of the German Customs Union, but local pounds continued to co-exist with the Zollverein pound for some time in some German states. Nowadays, the term "Pfund" is still in common use and universally refers to a pound of 500 grams.
Russian funt.
The Russian pound (Фунт, funt) is an obsolete Russian unit of measurement of mass. It is equal to 409.51718 grams. In 1899, the Russian pound was the basic unit of weight and all other units of weight were formed from it.
Skålpund.
The Skålpund was a Scandinavian measurement that varied in weight between regions. From the 17th century onward, it was equal to 425.076 grams in Sweden but was abandoned in 1889 when Sweden switched to the metric system.
In Norway, the same name was used for a weight of 498.1 grams. In Denmark, it equalled 471 grams.
In the 19th century, Denmark followed Germany's lead and redefined the pound as 500 grams.
Jersey pound.
A Jersey pound is an obsolete unit of mass used on the island of Jersey from the 14th century to the 19th century. It was equivalent to about 7,561 grains (490 grams). It may have been derived from the French livre poids de marc.
Trone pound.
The trone pound is one of a number of obsolete Scottish units of measurement. It was equivalent to between 21 and 28 avoirdupois ounces (about 600-800 grams).
Metric pounds.
In many countries, upon the introduction of a metric system, the pound (or its translation) became an informal term for 500 grams. In German, the term is "Pfund", in French "livre", in Dutch "pond", in Spanish and Portuguese "libra", in Italian "libbra", and in Danish and Swedish "pund".
Though not from the same linguistic origin, the Chinese "jīn" (, also known a "catty") has a modern definition of exactly 500 grams, divided into 10 "liǎng" (). Traditionally about 605 grams, the "jin" has been in use for more than two thousand years, serving the same purpose as "pound" for the common-use measure of weight.
Hundreds of older pounds were replaced in this way. Examples of the older pounds are one of around 459 to 460 grams in Spain, Portugal, and Latin America; one of 498.1 grams in Norway; and several different ones in what is now Germany.
Although the use of the pound as an informal term persists in these countries to a varying degree, scales and measuring devices are denominated only in grams and kilograms. A pound of product must be determined by weighing the product in grams as the use of the "pound" is not sanctioned for trade within the European Union.
Use in weaponry.
Smoothbore cannon and carronades are designated by the weight in imperial pounds of round solid iron shot of diameter to fit the barrel. A cannon that fires a six-pound ball, for example, is called a "six-pounder". Standard sizes are 6, 12, 18, 24, 32 and 42 pounds; 68-pounders also exist, and other nonstandard weapons use the same scheme. See carronade.
A similar definition, using lead balls, exists for determining the gauge of shotguns.

</doc>
<doc id="23317" url="https://en.wikipedia.org/wiki?curid=23317" title="Proton">
Proton

A proton is a subatomic particle, symbol or , with a positive electric charge of +1e elementary charge and mass slightly less than that of a neutron. Protons and neutrons, each with masses of approximately one atomic mass unit, are collectively referred to as "nucleons". One or more protons are present in the nucleus of every atom. The number of protons in the nucleus is the defining property of an element, and is referred to as the atomic number (represented by the symbol Z). Since each element has a unique number of protons, each element has its own unique atomic number. The word "proton" is Greek for "first", and this name was given to the hydrogen nucleus by Ernest Rutherford in 1920. In previous years Rutherford had discovered that the hydrogen nucleus (known to be the lightest nucleus) could be extracted from the nuclei of nitrogen by collision. Protons were therefore a candidate to be a fundamental particle and a building block of nitrogen and all other heavier atomic nuclei.
In the modern Standard Model of particle physics, protons are hadrons, and like neutrons, the other nucleon (particle present in atomic nuclei), are composed of three quarks. Although protons were originally considered fundamental or elementary particles, they are now known to be composed of three valence quarks: two up quarks and one down quark. The rest masses of quarks contribute only about 1% of a proton's mass, however. The remainder of a proton's mass is due to the kinetic energy of the quarks and to the energy of the gluon fields that bind the quarks together. Because protons are not fundamental particles, they possess a physical size; the radius of a proton is about 0.84–0.87 fm.
At sufficiently low temperatures, free protons will bind to electrons. However, the character of such bound protons does not change, and they remain protons. A fast proton moving through matter will slow by interactions with electrons and nuclei, until it is captured by the electron cloud of an atom. The result is a protonated atom, which is a chemical compound of hydrogen. In vacuum, when free electrons are present, a sufficiently slow proton may pick up a single free electron, becoming a neutral hydrogen atom, which is chemically a free radical. Such "free hydrogen atoms" tend to react chemically with many other types of atoms at sufficiently low energies. When free hydrogen atoms react with each other, they form neutral hydrogen molecules (H2), which are the most common molecular component of molecular clouds in interstellar space. Such molecules of hydrogen on Earth may then serve (among many other uses) as a convenient source of protons for accelerators (as used in proton therapy) and other hadron particle physics experiments that require protons to accelerate, with the most powerful and noted example being the Large Hadron Collider.
Description.
Protons are spin-½ fermions and are composed of three valence quarks, making them baryons (a sub-type of hadrons). The two up quarks and one down quark of a proton are held together by the strong force, mediated by gluons.A modern perspective has a proton composed of the valence quarks (up, up, down), the gluons, and transitory pairs of sea quarks. Protons have an approximately exponentially decaying positive charge distribution with a mean square radius of about 0.8 fm.
Protons and neutrons are both nucleons, which may be bound together by the nuclear force to form atomic nuclei. The nucleus of the most common isotope of the hydrogen atom (with the chemical symbol "H") is a lone proton. The nuclei of the heavy hydrogen isotopes deuterium and tritium contain one proton bound to one and two neutrons, respectively. All other types of atomic nuclei are composed of two or more protons and various numbers of neutrons.
History.
The concept of a hydrogen-like particle as a constituent of other atoms was developed over a long period. As early as 1815, William Prout proposed that all atoms are composed of hydrogen atoms (which he called "protyles"), based on a simplistic interpretation of early values of atomic weights (see Prout's hypothesis), which was disproved when more accurate values were measured.
In 1886, Eugen Goldstein discovered canal rays (also known as anode rays) and showed that they were positively charged particles (ions) produced from gases. However, since particles from different gases had different values of charge-to-mass ratio (e/m), they could not be identified with a single particle, unlike the negative electrons discovered by J. J. Thomson.
Following the discovery of the atomic nucleus by Ernest Rutherford in 1911, Antonius van den Broek proposed that the place of each element in the periodic table (its atomic number) is equal to its nuclear charge. This was confirmed experimentally by Henry Moseley in 1913 using X-ray spectra.
In 1917 (in experiments reported in 1919), Rutherford proved that the hydrogen nucleus is present in other nuclei, a result usually described as the discovery of protons. Rutherford had earlier learned to produce hydrogen nuclei as a type of radiation produced as a product of the impact of alpha particles on nitrogen gas, and recognize them by their unique penetration signature in air and their appearance in scintillation detectors. These experiments were begun when Rutherford had noticed that, when alpha particles were shot into air (mostly nitrogen), his scintillation detectors showed the signatures of typical hydrogen nuclei as a product. After experimentation Rutherford traced the reaction to the nitrogen in air, and found that when alphas were produced into pure nitrogen gas, the effect was larger. Rutherford determined that this hydrogen could have come only from the nitrogen, and therefore nitrogen must contain hydrogen nuclei. One hydrogen nucleus was being knocked off by the impact of the alpha particle, producing oxygen-17 in the process. This was the first reported nuclear reaction, 14N + α → 17O + p. (This reaction would later be observed happening directly in a cloud chamber in 1925).
Rutherford knew hydrogen to be the simplest and lightest element and was influenced by Prout's hypothesis that hydrogen was the building block of all elements. Discovery that the hydrogen nucleus is present in all other nuclei as an elementary particle, led Rutherford to give the hydrogen nucleus a special name as a particle, since he suspected that hydrogen, the lightest element, contained only one of these particles. He named this new fundamental building block of the nucleus the "proton," after the neuter singular of the Greek word for "first", πρῶτον. However, Rutherford also had in mind the word "protyle" as used by Prout. Rutherford spoke at the British Association for the Advancement of Science at its Cardiff meeting beginning 24 August 1920. Rutherford was asked by Oliver Lodge for a new name for the positive hydrogen nucleus to avoid confusion with the neutral hydrogen atom. He initially suggested both "proton" and "prouton" (after Prout). Rutherford later reported that the meeting had accepted his suggestion that the hydrogen nucleus be named the "proton", following Prout's word "protyle". The first use of the word "proton" in the scientific literature appeared in 1920.
Stability.
The free proton (a proton not bound to nucleons or electrons) is a stable particle that has not been observed to break down spontaneously to other particles. Free protons are found naturally in a number of situations in which energies or temperatures are high enough to separate them from electrons, for which they have some affinity. Free protons exist in plasmas in which temperatures are too high to allow them to combine with electrons. Free protons of high energy and velocity make up 90% of cosmic rays, which propagate in vacuum for interstellar distances. Free protons are emitted directly from atomic nuclei in some rare types of radioactive decay. Protons also result (along with electrons and antineutrinos) from the radioactive decay of free neutrons, which are unstable.
The spontaneous decay of free protons has never been observed, and protons are therefore considered stable particles. However, some grand unified theories of particle physics predict that proton decay should take place with lifetimes of the order of , and experimental searches have established lower bounds on the mean lifetime of a proton for various assumed decay products.
Experiments at the Super-Kamiokande detector in Japan gave lower limits for proton mean lifetime of for decay to an antimuon and a neutral pion, and for decay to a positron and a neutral pion.
Another experiment at the Sudbury Neutrino Observatory in Canada searched for gamma rays resulting from residual nuclei resulting from the decay of a proton from oxygen-16. This experiment was designed to detect decay to any product, and established a lower limit to a proton lifetime of .
However, protons are known to transform into neutrons through the process of electron capture (also called inverse beta decay). For free protons, this process does not occur spontaneously but only when energy is supplied. The equation is:
The process is reversible; neutrons can convert back to protons through beta decay, a common form of radioactive decay. In fact, a free neutron decays this way, with a mean lifetime of about 15 minutes.
Quarks and the mass of a proton.
In quantum chromodynamics, the modern theory of the nuclear force, most of the mass of protons and neutrons is explained by special relativity. The mass of a proton is about 80–100 times greater than the sum of the rest masses of the quarks that make it up, while the gluons have zero rest mass. The extra energy of the quarks and gluons in a region within a proton, as compared to the rest energy of the quarks alone in the QCD vacuum, accounts for almost 99% of the mass. The rest mass of a proton is, thus, the invariant mass of the system of moving quarks and gluons that make up the particle, and, in such systems, even the energy of massless particles is still measured as part of the rest mass of the system.
Two terms are used in referring to the mass of the quarks that make up protons: "current quark mass" refers to the mass of a quark by itself, while "constituent quark mass" refers to the current quark mass plus the mass of the gluon particle field surrounding the quark. These masses typically have very different values. As noted, most of a proton's mass comes from the gluons that bind the current quarks together, rather than from the quarks themselves. While gluons are inherently massless, they possess energy—to be more specific, quantum chromodynamics binding energy (QCBE)—and it is this that contributes so greatly to the overall mass of protons (see mass in special relativity). A proton has a mass of approximately 938 MeV/c2, of which the rest mass of its three valence quarks contributes only about 9.4 MeV/c2; much of the remainder can be attributed to the gluons' QCBE.
The internal dynamics of protons are complicated, because they are determined by the quarks' exchanging gluons, and interacting with various vacuum condensates. Lattice QCD provides a way of calculating the mass of a proton directly from the theory to any accuracy, in principle. The most recent calculations claim that the mass is determined to better than 4% accuracy, even to 1% accuracy (see Figure S5 in Dürr "et al."). These claims are still controversial, because the calculations cannot yet be done with quarks as light as they are in the real world. This means that the predictions are found by a process of extrapolation, which can introduce systematic errors. It is hard to tell whether these errors are controlled properly, because the quantities that are compared to experiment are the masses of the hadrons, which are known in advance.
These recent calculations are performed by massive supercomputers, and, as noted by Boffi and Pasquini: "a detailed description of the nucleon structure is still missing because ... long-distance behavior requires a nonperturbative and/or numerical treatment..."
More conceptual approaches to the structure of protons are: the topological soliton approach originally due to Tony Skyrme and the more accurate AdS/QCD approach that extends it to include a string theory of gluons, various QCD-inspired models like the bag model and the constituent quark model, which were popular in the 1980s, and the SVZ sum rules, which allow for rough approximate mass calculations. These methods do not have the same accuracy as the more brute-force lattice QCD methods, at least not yet.
Charge radius.
The problem of defining a radius for an atomic nucleus (proton) is similar to the problem of atomic radius, in that neither atoms nor their nuclei have definite boundaries. However, the nucleus can be modeled as a sphere of positive charge for the interpretation of electron scattering experiments: because there is no definite boundary to the nucleus, the electrons "see" a range of cross-sections, for which a mean can be taken. The qualification of "rms" (for "root mean square") arises because it is the nuclear cross-section, proportional to the square of the radius, which is determining for electron scattering.
The internationally accepted value of a proton's charge radius is (see orders of magnitude for comparison to other sizes). This value is based on measurements involving a proton and an electron (namely, electron scattering measurements and complex calculation involving scattering cross section based on Rosenbluth equation for momentum-transfer cross section), and studies of the atomic energy levels of hydrogen and deuterium).
However, in 2010 an international research team published a proton charge radius measurement via the Lamb shift in muonic hydrogen (an exotic atom made of a proton and a negatively charged muon). Their measurement of the root-mean-square charge radius of a proton is ", which differs by 5.0 standard deviations from the CODATA value of ". In January 2013, an updated value for the charge radius of a proton——was published. The precision was improved by 1.7 times, but the difference with CODATA value persisted at 7σ significance.
The international research team that obtained this result at the Paul Scherrer Institut (PSI) in Villigen (Switzerland) includes scientists from the Max Planck Institute of Quantum Optics (MPQ) in Garching, the Ludwig-Maximilians-Universität (LMU) Munich and the Institut für Strahlwerkzeuge (IFWS) of the Universität Stuttgart (both from Germany), and the University of Coimbra, Portugal.
They are now attempting to explain the discrepancy, and re-examining the results of both previous high-precision measurements and complex calculations involving scattering cross section. If no errors are found in the measurements or calculations, it could be necessary to re-examine the world's most precise and best-tested fundamental theory: quantum electrodynamics.
The proton radius remains a puzzle as of early 2015. Perhaps the discrepancy is due to new physics or the explanation is an ordinary physics effect
that has been missed.
The radius is linked to the form factor and momentum transfer cross section. The atomic form factor G modifies the cross section corresponding to point-like proton.
The atomic form factor is related to the wave function density of the target:
The form factor can be split in electric and magnetic form factors. These can be further written as linear combinations of Dirac and Pauli form factors.
Interaction of free protons with ordinary matter.
Although protons have affinity for oppositely charged electrons, free protons must lose sufficient velocity (and kinetic energy) in order to become closely associated and bound to electrons, since this is a relatively low-energy interaction. High energy protons, in traversing ordinary matter, lose energy by collisions with atomic nuclei, and by ionization of atoms (removing electrons) until they are slowed sufficiently to be captured by the electron cloud in a normal atom.
However, in such an association with an electron, the character of the bound proton is not changed, and it remains a proton. The attraction of low-energy free protons to any electrons present in normal matter (such as the electrons in normal atoms) causes free protons to stop and to form a new chemical bond with an atom. Such a bond happens at any sufficiently "cold" temperature (i.e., comparable to temperatures at the surface of the Sun) and with any type of atom. Thus, in interaction with any type of normal (non-plasma) matter, low-velocity free protons are attracted to electrons in any atom or molecule with which they come in contact, causing the proton and molecule to combine. Such molecules are then said to be "protonated", and chemically they often, as a result, become so-called Bronsted acids.
Proton in chemistry.
Atomic number.
In chemistry, the number of protons in the nucleus of an atom is known as the atomic number, which determines the chemical element to which the atom belongs. For example, the atomic number of chlorine is 17; this means that each chlorine atom has 17 protons and that all atoms with 17 protons are chlorine atoms. The chemical properties of each atom are determined by the number of (negatively charged) electrons, which for neutral atoms is equal to the number of (positive) protons so that the total charge is zero. For example, a neutral chlorine atom has 17 protons and 17 electrons, whereas a Cl− anion has 17 protons and 18 electrons for a total charge of −1.
All atoms of a given element are not necessarily identical, however, as the number of neutrons may vary to form different isotopes, and energy levels may differ forming different nuclear isomers. For example, there are two stable isotopes of chlorine: with 35 − 17 = 18 neutrons and with 37 − 17 = 20 neutrons.
Hydrogen ion.
In chemistry, the term proton refers to the hydrogen ion, . Since the atomic number of hydrogen is 1, a hydrogen ion has no electrons and corresponds to a bare nucleus, consisting of a proton (and 0 neutrons for the most abundant isotope "protium" ). The proton is a "bare charge" with only about 1/64,000 of the radius of a hydrogen atom, and so is extremely reactive chemically. The free proton, thus, has an extremely short lifetime in chemical systems such as liquids and it reacts immediately with the electron cloud of any available molecule. In aqueous solution, it forms the hydronium ion, H3O+, which in turn is further solvated by water molecules in clusters such as and [H9O4+.
The transfer of in an acid–base reaction is usually referred to as "proton transfer". The acid is referred to as a proton donor and the base as a proton acceptor. Likewise, biochemical terms such as proton pump and proton channel refer to the movement of hydrated ions.
The ion produced by removing the electron from a deuterium atom is known as a deuteron, not a proton. Likewise, removing an electron from a tritium atom produces a triton.
Proton nuclear magnetic resonance (NMR).
Also in chemistry, the term "proton NMR" refers to the observation of hydrogen-1 nuclei in (mostly organic) molecules by nuclear magnetic resonance. This method uses the spin of the proton, which has the value one-half. The name refers to examination of protons as they occur in protium (hydrogen-1 atoms) in compounds, and does not imply that free protons exist in the compound being studied.
Human exposure.
The Apollo Lunar Surface Experiments Packages (ALSEP) determined that more than 95% of the particles in the solar wind are electrons and protons, in approximately equal numbers.
Protons also have extrasolar origin from galactic cosmic rays, where they make up about 90% of the total particle flux. These protons often have higher energy than solar wind protons, and their intensity is far more uniform and less variable than protons coming from the Sun, the production of which is heavily affected by solar proton events such as coronal mass ejections.
Research has been performed on the dose-rate effects of protons, as typically found in space travel, on human health. To be more specific, there are hopes to identify what specific chromosomes are damaged, and to define the damage, during cancer development from proton exposure. Another study looks into determining "the effects of exposure to proton irradiation on neurochemical and behavioral endpoints, including dopaminergic functioning, amphetamine-induced conditioned taste aversion learning, and spatial learning and memory as measured by the Morris water maze. Electrical charging of a spacecraft due to interplanetary proton bombardment has also been proposed for study. There are many more studies that pertain to space travel, including galactic cosmic rays and their possible health effects, and solar proton event exposure.
The American Biostack and Soviet Biorack space travel experiments have demonstrated the severity of molecular damage induced by heavy ions on micro organisms including Artemia cysts.
Antiproton.
CPT-symmetry puts strong constraints on the relative properties of particles and antiparticles and, therefore, is open to stringent tests. For example, the charges of a proton and antiproton must sum to exactly zero. This equality has been tested to one part in . The equality of their masses has also been tested to better than one part in . By holding antiprotons in a Penning trap, the equality of the charge to mass ratio of protons and antiprotons has been tested to one part in . The magnetic moment of antiprotons has been measured with error of nuclear Bohr magnetons, and is found to be equal and opposite to that of a proton.

</doc>
<doc id="23318" url="https://en.wikipedia.org/wiki?curid=23318" title="Phosphorus">
Phosphorus

Phosphorus is a chemical element with symbol P and atomic number 15. As an element, phosphorus exists in two major forms—white phosphorus and red phosphorus—but due to its high reactivity, phosphorus is never found as a free element on Earth. With few exceptions, phosphorus bearing minerals are in the maximally oxidised state as inorganic phosphate rocks.
The first form of elemental phosphorus to be produced (white phosphorus, in 1669) emits a faint glow upon exposure to oxygen – hence its name given from Greek mythology, meaning "light-bearer" (Latin "Lucifer"), referring to the "Morning Star", the planet Venus (or Mercury). The term "phosphorescence", meaning glow after illumination, originally derives from this property of phosphorus, although this word has since been used for a different physical process that produces a glow. The glow of phosphorus itself originates from oxidation of the white (but not red) phosphorus — a process now termed chemiluminescence. Together with nitrogen, arsenic, and antimony, phosphorus is classified as a pnictogen.
Phosphorus is essential for life. Phosphates (compounds containing the phosphate ion, PO4−3) are a component of DNA, RNA, ATP, and the phospholipids, which form all cell membranes. Demonstrating the link between phosphorus and life, elemental phosphorus was first isolated from human urine, and bone ash was an important early phosphate source. Phosphate minerals are fossils. Low phosphate levels are an important limit to growth in some aquatic systems. The vast majority of phosphorus compounds produced are consumed as fertilisers. Phosphate is needed to replace the phosphorus that plants remove from the soil, and its annual demand is rising nearly twice as fast as the growth of the human population. Other applications include the role of organophosphorus compounds in detergents, pesticides, and nerve agents. At 0.099%, phosphorus is the most abundant pnictogen in the Earth's crust.
Characteristics.
Physical.
Phosphorus exists as several forms (allotropes) that exhibit strikingly different properties. The two most common allotropes are white phosphorus and red phosphorus.
White phosphorus and related molecular forms.
From the perspective of applications and chemical literature, the most important form of elemental phosphorus is white phosphorus, often abbreviated as WP. It consists of tetrahedral molecules, in which each atom is bound to the other three atoms by a single bond. This tetrahedron is also present in liquid and gaseous phosphorus up to the temperature of when it starts decomposing to molecules. Solid white exists in two forms. At low-temperatures, the β form is stable. At high-temperatures the α form is predominant. These forms differ in terms of the relative orientations of the constituent P4 tetrahedra.
White phosphorus is the least stable, the most reactive, the most volatile, the least dense, and the most toxic of the allotropes. White phosphorus gradually changes to red phosphorus. This transformation is accelerated by light and heat, and samples of white phosphorus almost always contain some red phosphorus and accordingly appear yellow. For this reason, white phosphorus that is aged or otherwise impure (e.g. weapons-grade, not lab-grade WP) is also called yellow phosphorus. When exposed to oxygen, white phosphorus glows in the dark with a very faint tinge of green and blue. It is highly flammable and pyrophoric (self-igniting) upon contact with air. Owing to its pyrophoricity, white phosphorus is used as an additive in napalm. The odour of combustion of this form has a characteristic garlic smell, and samples are commonly coated with white "phosphorus pentoxide", which consists of tetrahedra with oxygen inserted between the phosphorus atoms and at their vertices. White phosphorus is insoluble in water but soluble in carbon disulfide.
White phoshporus is toxic, causing severe liver damage on ingestion.
Thermolysis (cracking) of P4 at 1100 kelvin gives diphosphorus, P2. This species is not stable as a solid or liquid. The dimeric unit contains a triple bond and is analogous to N2. It can also be generated as a transient intermediate in solution by thermolysis of organophosphorus precursor reagents. At still higher temperatures, P2 dissociates into atomic P.
Although the term phosphorescence is derived from phosphorus, the reaction that gives phosphorus its glow is properly called chemiluminescence (glowing due to a cold chemical reaction), not phosphorescence (re-emitting light that previously fell onto a substance and excited it).
Red phosphorus.
Red phosphorus is polymeric in structure. It can be viewed as a derivative of P4 wherein one P-P bond is broken, and one additional bond is formed with the neighbouring tetrahedron resulting in a chain-like structure. Red phosphorus may be formed by heating white phosphorus to 250 °C (482 °F) or by exposing white phosphorus to sunlight. Phosphorus after this treatment is amorphous. Upon further heating, this material crystallises. In this sense, red phosphorus is not an allotrope, but rather an intermediate phase between the white and violet phosphorus, and most of its properties have a range of values. For example, freshly prepared, bright red phosphorus is highly reactive and ignites at about , though it is more stable than white phosphorus, which ignites at about . After prolonged heating or storage, the color darkens (see infobox images); the resulting product is more stable and does not spontaneously ignite in air.
Violet phosphorus.
Violet phosphorus is a form of phosphorus that can be produced by day-long annealing of red phosphorus above 550 °C. In 1865, Hittorf discovered that when phosphorus was recrystallized from molten lead, a red/purple form is obtained. Therefore, this form is sometimes known as "Hittorf's phosphorus" (or violet or α-metallic phosphorus).
Black phosphorus.
Black phosphorus is the least reactive allotrope and the thermodynamically stable form below . It is also known as β-metallic phosphorus and has a structure somewhat resembling that of graphite.
Black phosphorus is obtained by heating white phosphorus under high pressures (about ). In appearance, properties, and structure, it resembles graphite, being black and flaky, a conductor of electricity, and has puckered sheets of linked atoms.
High pressures are usually required to produce black phosphorus, but it can also be produced at ambient conditions using metal salts as catalysts.
Other forms.
Another form, scarlet phosphorus, is obtained by allowing a solution of white phosphorus in carbon disulfide to evaporate in sunlight.
Another allotrope is diphosphorus; it contains a phosphorus dimer as a structural unit and is highly reactive.
Isotopes.
Twenty-three isotopes of phosphorus are known, including all possibilities from up to . Only is stable and is therefore present at 100% abundance. The half-integer nuclear spin and high abundance of 31P make phosphorus-31 NMR spectroscopy a very useful analytical tool in studies of phosphorus-containing samples.
Two radioactive isotopes of phosphorus have half lives suitable for biological scientific experiments. These are:
The high energy beta particles from penetrate skin and corneas and any ingested, inhaled, or absorbed is readily incorporated into bone and nucleic acids. For these reasons, Occupational Safety and Health Administration in the United States, and similar institutions in other developed countries require personnel working with to wear lab coats, disposable gloves, and safety glasses or goggles to protect the eyes, and avoid working directly over open containers. Monitoring personal, clothing, and surface contamination is also required. Shielding requires special consideration. The high energy of the beta particles gives rise to secondary emission of X-rays via Bremsstrahlung (braking radiation) in dense shielding materials such as lead. Therefore, the radiation must be shielded with low density materials such as acrylic or other plastic, water, or (when transparency is not required), even wood.
Occurrence.
Minerals (phosphate rock).
Phosphorus is not found free in nature, but it is widely distributed in many minerals, mainly phosphates. Inorganic phosphate rock, which is partially made of apatite (an impure tri-calcium phosphate mineral), is today the chief commercial source of this element. According to the US Geological Survey (USGS), about 50 percent of the global phosphorus reserves are in the Arab nations. Large deposits of apatite are located in China, Russia, Morocco, Florida, Idaho, Tennessee, Utah, and elsewhere. Albright and Wilson in the UK and their Niagara Falls plant, for instance, were using phosphate rock in the 1890s and 1900s from Tennessee, Florida, and the Îles du Connétable (guano island sources of phosphate); by 1950 they were using phosphate rock mainly from Tennessee and North Africa. In the early 1990s Albright and Wilson's purified wet phosphoric acid business was being adversely affected by phosphate rock sales by China and the entry of their long-standing Moroccan phosphate suppliers into the purified wet phosphoric acid business.
In 2012, the USGS estimated 71 billion tons of world reserves, where reserve figures refer to the amount assumed recoverable at current market prices; 0.19 billion tons were mined in 2011. Critical to contemporary agriculture, its annual demand is rising nearly twice as fast as the growth of the human population.
Recent reports suggest that production of phosphorus may have peaked, leading to the possibility of global shortages by 2040. In 2007, at the rate of consumption, the supply of phosphorus was estimated to run out in 345 years. However, some scientists now believe that a "peak phosphorus" will occur in 30 years and that "At current rates, reserves will be depleted in the next 50 to 100 years." Cofounder of Boston-based investment firm and environmental foundation Jeremy Grantham wrote in "Nature" in November 2012, that consumption of the element "must be drastically reduced in the next 20-40 years or we will begin to starve." According to N.N. Greenwood and A. Earnshaw, authors of the textbook, "Chemistry of the Elements," however, phosphorus comprises about 0.1% by mass of the average rock, and consequently the Earth's supply is vast, although dilute.
Urine.
Urine contains most (94% according to Wolgast) of the NPK nutrients excreted by the human body. The more general limitations to using urine as fertilizer depend mainly on the potential for buildup of excess nitrogen (due to the high ratio of that macronutrient), and inorganic salts such as sodium chloride, which are also part of the wastes excreted by the renal system. The degree to which these factors impact the effectiveness depends on the term of use, salinity tolerance of the plant, soil composition, addition of other fertilizing compounds, and quantity of rainfall or other irrigation.
Urine typically contributes 70% of the nitrogen and more than half the phosphorus and potassium found in urban waste water flows, while making up less than 1% of the overall volume. Thus far, source separation, or urine diversion and on-site treatment, has been implemented in South Africa, China, and Sweden among other countries, with the Bill and Melinda Gates Foundation provided some of the funding implementations.
"Urine management" is a relatively new way to view closing the cycle of agricultural nutrient flows and reducing sewage treatment costs and ecological consequences such as eutrophication resulting from the influx of nutrient rich effluent into aquatic or marine ecosystems. Proponents of urine as a natural source of agricultural fertilizer claim the risks to be negligible or acceptable.
Other organic sources.
Historically-important but limited commercial sources were organic, such as bone ash and (in the latter 19th century) guano.
Supernovae remnants.
In 2013, astronomers detected phosphorus in Cassiopeia A, which confirmed that this element is produced in supernovae through supernova nucleosynthesis. The phosphorus-to-iron ratio in material from the supernova remnant could be up to 100 times higher than in the Milky Way in general.
Production.
Most of the phosphorus-bearing material produced is for agriculture fertilisers. For this purpose, phosphate minerals are converted to phosphoric acid. Two distinct routes are employed, the main one being treatment of phosphate minerals with sulfuric acid. The other process utilises white phosphorus, which may be produced by reaction and distillation from very low grade phosphate sources. The white phosphorus is then oxidised to phosphoric acid and subsequently neutralised with base to give phosphate salts. Phosphoric acid produced from white phosphorus is relatively pure and is the main route for the production of phosphates for all purposes, including detergent.
Elemental phosphorus.
Presently, about of elemental phosphorus is produced annually. Calcium phosphate (phosphate rock), mostly mined in Florida and North Africa, can be heated to 1,200–1,500 °C with sand, which is mostly , and coke (refined coal) to produce vaporized . The product is subsequently condensed into a white powder under water to prevent oxidation by air. Even under water, white phosphorus is slowly converted to the more stable red phosphorus allotrope. The chemical equation for this process when starting with fluoroapatite, a common phosphate mineral, is:
Side products from this process include ferrophosphorus, a crude form of Fe2P, resulting from iron impurities in the mineral precursors. The silicate slag is a useful construction material. The fluoride is sometimes recovered for use in water fluoridation. More problematic is a "mud" containing significant amounts of white phosphorus. Production of white phosphorus is conducted in large facilities in part because it is energy intensive. The white phosphorus is transported in molten form. Some major accidents have occurred during transportation; train derailments at Brownston, Nebraska and Miamisburg, Ohio led to large fires. The worst incident in recent times was an environmental contamination in 1968 when the sea was polluted from spillage and/or inadequately treated sewage from a white phosphorus plant at Placentia Bay, Newfoundland.
Another process by which elemental phosphorus is extracted includes applying at high temperatures (1500 °C):
Historically, before the development of mineral-based extractions, white phosphorus was isolated on an industrial scale from bone ash. In this process, the tricalcium phosphate in bone ash is converted to monocalcium phosphate with sulfuric acid:
Monocalcium phosphate is then dehydrated to the corresponding metaphosphate:
When ignited to a white heat with charcoal, calcium metaphosphate yields two-thirds of its weight of white phosphorus while one-third of the phosphorus remains in the residue as calcium orthophosphate:
Compounds.
Oxoacids of phosphorus.
Phosphorous oxoacids are extensive, often commercially important, and sometimes structurally complicated. They all have acidic protons bound to oxygen atoms, some have nonacidic protons that are bonded directly to phosphorus and some contain phosphorus - phosphorus bonds. Although many oxoacids of phosphorus are formed, only nine are important, and three of them, hypophosphorous acid, phosphorous acid, and phosphoric acid, are particularly important.
Phosphorus(V) compounds.
Oxides.
The most prevalent compounds of phosphorus are derivatives of phosphate (PO43−), a tetrahedral anion. Phosphate is the conjugate base of phosphoric acid, which is produced on a massive scale for use in fertilisers. Being triprotic, phosphoric acid converts stepwise to three conjugate bases:
Phosphate exhibits the tendency to form chains and rings with P-O-P bonds. Many polyphosphates are known, including ATP. Polyphosphates arise by dehydration of hydrogen phosphates such as HPO42− and H2PO4−. For example, the industrially important trisodium triphosphate (also known as sodium tripolyphosphate, STPP) is produced industrially on by the megatonne by this condensation reaction:
Phosphorus pentoxide (P4O10) is the acid anhydride of phosphoric acid, but several intermediates between the two are known. This waxy white solid reacts vigorously with water.
With metal cations, phosphate forms a variety of salts. These solids are polymeric, featuring P-O-M linkages. When the metal cation has a charge of 2+ or 3+, the salts are generally insoluble, hence they exist as common minerals. Many phosphate salts are derived from hydrogen phosphate (HPO42−).
PCl5 and PF5 are common compounds. PF5 is a colourless gas and the molecules have trigonal bypramidal geometry. PCl5 is a colourless solid which has an ionic formulation of PCl4+ PCl6−, but adopts the trigonal bypramidal geometry when molten or in the vapour phase. PBr5 is an unstable solid formulated as PBr4+Br−and PI5 is not known. The pentachloride and pentafluoride are Lewis acids. With fluoride, PF5 forms PF6−, an anion that is isoelectronic with SF6. The most important oxyhalide is phosphorus oxychloride, (POCl3), which is approximately tetrahedral.
Before extensive computer calculations were feasible, it was thought that bonding in phosphorus(V) compounds involved "d" orbitals. Computer modeling of molecular orbital theory indicates that this bonding involves only s- and p-orbitals.
Nitrides.
The PN molecule is considered unstable, but is a product of crystalline Phosphorus nitride decomposition at 1100 K. Similarly, H2PN is considered unstable, and phosphorus nitride halogens like F2PN, Cl2PN, Br2PN, and I2PN oligomerize into cyclic Polyphosphazenes. For example, compounds of the formula (PNCl2)n exist mainly as rings such as the trimer hexachlorophosphazene. The phosphazenes arise by treatment of phosphorus pentachloride with ammonium chloride:PCl5 + NH4Cl → 1/"n" (NPCl2)"n" + 4 HClWhen the chloride groups are replaced by alkoxide (RO−), a family of polymers is produced with potentially useful properties.
Sulfides.
Phosphorus forms a wide range of sulfides, where the phosphorus can be in P(V), P(III) or other oxidation states. The most famous is the three-fold symmetric P4S3 which is used in strike-anywhere matches. P4S10 and P4O10 have analogous structures.
Phosphorus(III) compounds.
All four symmetrical trihalides are well known: gaseous PF3, the yellowish liquids PCl3 and PBr3, and the solid PI3. These materials are moisture sensitive, hydrolysing to give phosphorous acid. The trichloride, a common reagent, is produced by chlorination of white phosphorus:
The trifluoride is produced from the trichloride by halide exchange. PF3 is toxic because it binds to haemoglobin.
Phosphorus(III) oxide, P4O6 (also called tetraphosphorus hexoxide) is the anhydride of P(OH)3, the minor tautomer of phosphorous acid. The structure of P4O6 is like that of P4O10 without the terminal oxide groups.
Mixed oxyhalides and oxyhydrides of phosphorus(III) are almost unknown.
Organophosphorus compounds.
Compounds with P-C and P-O-C bonds are often classified as organophosphorus compounds. They are widely used commercially. The PCl3 serves as a source of P3+ in routes to organophosphorus(III) compounds. For example, it is the precursor to triphenylphosphine:
Treatment of phosphorus trihalides with alcohols and phenols gives phosphites, e.g. triphenylphosphite:
Similar reactions occur for phosphorus oxychloride, affording triphenylphosphate:
Phosphorus(I) and phosphorus(II) compounds.
These compounds generally feature P-P bonds. Examples include catenated derivatives of phosphine and organophosphines. Compounds containing P=P double bonds have also been observed, although they are rare.
Phosphines.
Phosphine (PH3) and its organic derivatives (PR3) are structural analogues with ammonia (NH3) but the bond angles at phosphorus are closer to 90° for phosphine and its organic derivatives. It is an ill-smelling, toxic compound. Phosphorus has an oxidation number of -3 in phosphine. Phosphine is produced by hydrolysis of calcium phosphide, Ca3P2. Unlike ammonia, phosphine is oxidised by air. Phosphine is also far less basic than ammonia. Other phophines are known which contain chains of up to nine phosphorus atoms and have the formula PnHn+2. The highly flammable gas diphosphine (P2H4) is an analogue of hydrazine.
Phosphides.
Phosphides arise by reaction of metals with red phosphorus. The alkali metals (group 1) and alkaline earth metals can form ionic compounds containing the phosphide ion, P3−. These compounds react with water to form phosphine. Other phosphides, for example Na3P7, are known for these reactive metals. With the transition metals as well as the monophosphides there are metal rich phosphides, which are generally hard refractory compounds with a metallic lustre, and phosphorus rich phosphides which are less stable and include semiconductors. Schreibersite is a naturally occurring metal rich phosphide found in meteorites. The structures of the metal rich and phosphorus rich phosphides can be structurally complex.
Spelling and etymology.
The name "Phosphorus" in Ancient Greece was the name for the planet Venus and is derived from the Greek words (φῶς = light, φέρω = carry), which roughly translates as light-bringer or light carrier. (In Greek mythology and tradition, Augerinus (Αυγερινός = morning star, still in use today), Hesperus or Hesperinus (΄Εσπερος or Εσπερινός or Αποσπερίτης = evening star, still in use today) and Eosphorus (Εωσφόρος = dawnbearer, not in use for the planet after Christianity) are close homologues, and also associated with Phosphorus-the-planet).
According to the Oxford English Dictionary, the correct spelling of the element is phosphorus. The word phosphorous is the adjectival form of the P3+ valence: so, just as sulfur forms sulfurous and sulfuric compounds, phosphorus forms phosphorous compounds (e.g., phosphorous acid) and P5+ valence phosphoric compounds (e.g., phosphoric acids and phosphates).
History and discovery.
Phosphorus was the 13th element to be discovered. For this reason, and also due to its use in explosives, poisons and nerve agents, it is sometimes referred to as "the Devil's element". It was the first element to be discovered that was not known since ancient times. The discovery of phosphorus is credited to the German alchemist Hennig Brand in 1669, although other chemists might have discovered phosphorus around the same time. Brand experimented with urine, which contains considerable quantities of dissolved phosphates from normal metabolism. Working in Hamburg, Brand attempted to create the fabled philosopher's stone through the distillation of some salts by evaporating urine, and in the process produced a white material that glowed in the dark and burned brilliantly. It was named "phosphorus mirabilis" ("miraculous bearer of light"). His process originally involved letting urine stand for days until it gave off a terrible smell. Then he boiled it down to a paste, heated this paste to a high temperature, and led the vapours through water, where he hoped they would condense to gold. Instead, he obtained a white, waxy substance that glowed in the dark. Brand had discovered phosphorus. We now know that Brand produced ammonium sodium hydrogen phosphate, . While the quantities were essentially correct (it took about of urine to make about 60 g of phosphorus), it was unnecessary to allow the urine to rot. Later scientists discovered that fresh urine yielded the same amount of phosphorus.
Brand at first tried to keep the method secret, but later sold the recipe for 200 thalers to D Krafft from Dresden, who could now make it as well, and toured much of Europe with it, including England, where he met with Robert Boyle. The secret that it was made from urine leaked out and first Johann Kunckel (1630–1703) in Sweden (1678) and later Boyle in London (1680) also managed to make phosphorus, possibly with the aid of his assistant, Ambrose Godfrey-Hanckwitz, who later made a business of the manufacture of phosphorus. Boyle states that Krafft gave him no information as to the preparation of phosphorus other than that it was derived from "somewhat that belonged to the body of man". This gave Boyle a valuable clue, so that he, too, managed to make phosphorus, and published the method of its manufacture. Later he improved Brand's process by using sand in the reaction (still using urine as base material),
Robert Boyle was the first to use phosphorus to ignite sulfur-tipped wooden splints, forerunners of our modern matches, in 1680.
In 1769 Johan Gottlieb Gahn and Carl Wilhelm Scheele showed that calcium phosphate () is found in bones, and they obtained elemental phosphorus from bone ash. Antoine Lavoisier recognized phosphorus as an element in 1777. Bone ash was the major source of phosphorus until the 1840s. The method started by roasting bones, then employed the use of clay retorts encased in a very hot brick furnace to distill out the highly toxic elemental phosphorus product. Alternately, precipitated phosphates could be made from ground-up bones that had been de-greased and treated with strong acids. White phosphorus could then be made by heating the precipitated phosphates, mixed with ground coal or charcoal in an iron pot, and distilling off phosphorus vapour in a retort. Carbon monoxide and other flammable gases produced during the reduction process were burnt off in a flare stack.
In the 1840s, world phosphate production turned to the mining of tropical island deposits formed from bird and bat guano (see also Guano Islands Act). These became an important source of phosphates for fertilizer in the latter half of the 19th century.
Phosphate rock, a mineral containing calcium phosphate, was first used in 1850 to make phosphorus, and following the introduction of the electric arc furnace in 1890, elemental phosphorus production switched from the bone-ash heating, to electric arc production from phosphate rock. After the depletion of world guano sources about the same time, mineral phosphates became the major source of phosphate fertilizer production. Phosphate rock production greatly increased after World War II, and remains the primary global source of phosphorus and phosphorus chemicals today. See the article on peak phosphorus for more information on the history and present state of phosphate mining. Phosphate rock remains a feedstock in the fertilizer industry, where it is treated with sulfuric acid to produce various "superphosphate" fertilizer products.
White phosphorus was first made commercially in the 19th century for the match industry. This used bone ash for a phosphate source, as described above. The bone-ash process became obsolete when the submerged-arc furnace for phosphorus production was introduced to reduce phosphate rock. The electric furnace method allowed production to increase to the point where phosphorus could be used in weapons of war. In World War I it was used in incendiaries, smoke screens and tracer bullets. A special incendiary bullet was developed to shoot at hydrogen-filled Zeppelins over Britain (hydrogen being highly flammable). During World War II, Molotov cocktails made of phosphorus dissolved in petrol were distributed in Britain to specially selected civilians within the British resistance operation, for defence; and phosphorus incendiary bombs were used in war on a large scale. Burning phosphorus is difficult to extinguish and if it splashes onto human skin it has horrific effects.
Early matches used white phosphorus in their composition, which was dangerous due to its toxicity. Murders, suicides and accidental poisonings resulted from its use. (An apocryphal tale tells of a woman attempting to murder her husband with white phosphorus in his food, which was detected by the stew's giving off luminous steam). In addition, exposure to the vapours gave match workers a severe necrosis of the bones of the jaw, the infamous "phossy jaw". When a safe process for manufacturing red phosphorus was discovered, with its far lower flammability and toxicity, laws were enacted, under the Berne Convention (1906), requiring its adoption as a safer alternative for match manufacture. The toxicity of white phosphorus led to discontinuation of its use in matches. The Allies used phosphorus incendiary bombs in World War II to destroy Hamburg, the place where the "miraculous bearer of light" was first discovered.
Glow.
It was known from early times that the green glow emanating from white phosphorus would persist for a time in a stoppered jar, but then cease. Robert Boyle in the 1680s ascribed it to "debilitation" of the air; in fact, it is oxygen being consumed. By the 18th century, it was known that in pure oxygen, phosphorus does not glow at all; there is only a range of partial pressure at which it does. Heat can be applied to drive the reaction at higher pressures.
In 1974, the glow was explained by R. J. van Zee and A. U. Khan. A reaction with oxygen takes place at the surface of the solid (or liquid) phosphorus, forming the short-lived molecules HPO and that both emit visible light. The reaction is slow and only very little of the intermediates are required to produce the luminescence, hence the extended time the glow continues in a stoppered jar.
Since that time, "phosphors" and "phosphorescence" were used loosely to describe substances that shine in the dark without burning. Although the term phosphorescence was originally coined as a term by analogy with the glow from oxidation of elemental phosphorus, it is now reserved for another fundamentally different process—re-emission of light after illumination.
Applications.
Fertiliser.
Phosphorus is an essential plant nutrient (often the limiting nutrient), and the bulk of all phosphorus production is in concentrated phosphoric acids for agriculture fertilisers, containing as much as 70% to 75% P2O5. Global demand for fertilisers led to large increase in phosphate (PO43−) production in the second half of the 20th century. Artificial phosphate fertilisation is necessary because phosphorus is essential to all life organisms, because natural phosphorus-bearing compounds are mostly insoluble and inaccessible to plants, and because the natural cycle of phosphorus is very slow. Fertiliser is often in the form of superphosphate of lime, a mixture of calcium dihydrogen phosphate (Ca(H2PO4)2), and calcium sulfate dihydrate (CaSO4·2H2O) produced reacting sulfuric acid and water with calcium phosphate.
Processing phosphate minerals with sulfuric acid for obtain fertiliser is so important to the global economy that this is the primary industrial market for sulfuric acid and the greatest industrial use of elemental sulfur.
Organophosphorus compounds.
White phosphorus is widely used to make organophosphorus compounds through intermediate phosphorus chlorides and two phosphorus sulfides, phosphorus pentasulfide, and phosphorus sesquisulfide. Organophosphorus compounds have many applications, including in plasticizers, flame retardants, pesticides, extraction agents, and water treatment.
Metallurgical aspects.
Phosphorus is also an important component in steel production, in the making of phosphor bronze, and in many other related products. Phosphorus is added to metallic copper during its smelting process to react with oxygen present as an impurity in copper and to produce phosphorus-containing copper (CuOFP) alloys with a higher hydrogen embrittlement resistance than normal copper.
Matches.
The first striking match with a phosphorus head was invented by Charles Sauria in 1830.] These matches (and subsequent modifications) were made with heads of white phosphorus, an oxygen-releasing compound (potassium chlorate, lead dioxide, or sometimes nitrate), and a binder. They were poisonous to the workers in manufacture, sensitive to storage conditions, toxic if ingested, and hazardous when accidentally ignited on a rough surface. Production in several countries was banned between 1872 and 1925. The international Berne Convention, ratified in 1906, prohibited the use of white phosphorus in matches.
In consequence, the 'strike-anywhere' matches were gradually replaced by 'safety matches', wherein the white phosphorus was replaced by phosphorus sesquisulfide (P4S3), sulfur, or antimony sulfide. Such matches are difficult to ignite on any surface other than a special strip. The strip contains red phosphorus that heats up upon striking, reacts with the oxygen-releasing compound in the head, and ignites the flammable material of the head.
Water softening.
Sodium tripolyphosphate made from phosphoric acid is used in laundry detergents in some countries, but banned for this use in others. This compound softens the water to enhance the performance of the detergents and to prevent pipe/boiler tube corrosion.
Biological role.
Inorganic phosphorus in the form of the phosphate is required for all known forms of life. Phosphorus plays a major role in the structural framework of DNA and RNA. Living cells use phosphate to transport cellular energy with adenosine triphosphate (ATP), necessary for every cellular process that uses energy. ATP is also important for phosphorylation, a key regulatory event in cells. Phospholipids are the main structural components of all cellular membranes. Calcium phosphatesalts assist in stiffening bones.
Every living cell is encased in a membrane that separates it from its surroundings. Cellular membranes are composed of a phospholipid matrix and proteins, typically in the form of a bilayer. Phospholipids are derived from glycerol with two of the glycerol hydroxyl (OH) protons replaced by fatty acids as an ester, and the third hydroxyl proton has been replaced with phosphate bonded to another alcohol.
An average adult human contains about 0.7 kg of phosphorus, about 85–90% in bones and teeth in the form of apatite, and the remainder in soft tissues and extracellular fluids (~1%). The phosphorus content increases from about 0.5 weight% in infancy to 0.65–1.1 weight% in adults. Average phosphorus concentration in the blood is about 0.4 g/L, about 70% of that is organic and 30% inorganic phosphates. A well-fed adult in the industrialized world consumes and excretes about 1–3 grams of phosphorus per day, with consumption in the form of inorganic phosphate and phosphorus-containing biomolecules such as nucleic acids and phospholipids; and excretion almost exclusively in the form of phosphate ions such as and . Only about 0.1% of body phosphate circulates in the blood, paralleling the amount of phosphate available to soft tissue cells.
Bone and teeth enamel.
The main component of bone is hydroxyapatite as well as amorphous forms of calcium phosphate, possibly including carbonate. Hydroxyapatite is the main component of tooth enamel. Water fluoridation enhances the resistance of teeth to decay by the partial conversion of this mineral to the still harder material called fluoroapatite:
Phosphorus deficiency.
In medicine, phosphate deficiency syndrome may be caused by malnutrition, by failure to absorb phosphate, and by metabolic syndromes that draw phosphate from the blood (such as re-feeding after malnutrition) or pass too much of it into the urine. All are characterized by hypophosphatemia, which is a condition of low levels of soluble phosphate levels in the blood serum and inside the cells. Symptoms of hypophosphatemia include neurological dysfunction and disruption of muscle and blood cells due to lack of ATP. Too much phosphate can lead to diarrhoea and calcification (hardening) of organs and soft tissue, and can interfere with the body's ability to use iron, calcium, magnesium, and zinc.
Phosphorus is an essential macromineral for plants, which is studied extensively in edaphology to understand plant uptake from soil systems. Phosphorus is a limiting factor in many ecosystems; that is, the scarcity of phosphorus limits the rate of organism growth. An excess of phosphorus can also be problematic, especially in aquatic systems where eutrophication sometimes leads to algal blooms.
Food sources.
The main food sources for phosphorus are the same as those containing protein, although proteins do not contain phosphorus. For example, milk, meat, and soya typically also have phosphorus. As a rule, if a diet has sufficient protein and calcium, the amount of phosphorus is probably sufficient.
Precautions.
Organic compounds of phosphorus form a wide class of materials; many are required for life, but some are extremely toxic. Fluorophosphate esters are among the most potent neurotoxins known. A wide range of organophosphorus compounds are used for their toxicity as pesticides (herbicides, insecticides, fungicides, etc.) and weaponised as nerve agents against enemy humans. Most inorganic phosphates are relatively nontoxic and essential nutrients.
The white phosphorus allotrope presents a significant hazard because it ignites in air and produces phosphoric acid residue. Chronic white phosphorus poisoning leads to necrosis of the jaw called "phossy jaw". Ingestion of white phosphorus may cause the medical condition known as "Smoking Stool Syndrome".
In the past, external exposure to elemental phosphorus was treated by washing the affected area with 2% copper sulfate solution to form harmless compounds that are then washed away. According to the recent "US Navy's Treatment of Chemical Agent Casualties and Conventional Military Chemical Injuries: FM8-285: Part 2 Conventional Military Chemical Injuries", "Cupric (copper(II)) sulfate has been used by U.S. personnel in the past and is still being used by some nations. However, copper sulfate is toxic and its use will be discontinued. Copper sulfate may produce kidney and cerebral toxicity as well as intravascular hemolysis."
The manual suggests instead ""a bicarbonate solution to neutralize phosphoric acid, which will then allow removal of visible white phosphorus. Particles often can be located by their emission of smoke when air strikes them, or by their phosphorescence in the dark. In dark surroundings, fragments are seen as luminescent spots. Promptly debride the burn if the patient's condition will permit removal of bits of WP (white phosphorus) that might be absorbed later and possibly produce systemic poisoning. DO NOT apply oily-based ointments until it is certain that all WP has been removed. Following complete removal of the particles, treat the lesions as thermal burns."" As white phosphorus readily mixes with oils, any oily substances or ointments are not recommended until the area is thoroughly cleaned and all white phosphorus removed.
People can be exposed to phosphorus in the workplace by inhalation, injestion, skin contact, and eye contact. The Occupational Safety and Health Administration (OSHA) has set the phosphorus exposure limit (Permissible exposure limit) in the workplace at 0.1 mg/m3 over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set a Recommended exposure limit (REL) of 0.1 mg/m3 over an 8-hour workday. At levels of 5 mg/m3, phosphorus is immediately dangerous to life and health.
US DEA List I status.
Phosphorus can reduce elemental iodine to hydroiodic acid, which is a reagent effective for reducing ephedrine or pseudoephedrine to methamphetamine. For this reason, red and white phosphorus were designated by the United States Drug Enforcement Administration as List I precursor chemicals under 21 CFR 1310.02 effective on November 17, 2001. In the United States, handlers of red or white phosphorus are subject to stringent regulatory controls.

</doc>
<doc id="23319" url="https://en.wikipedia.org/wiki?curid=23319" title="Palladium">
Palladium

Palladium is a chemical element with symbol Pd and atomic number 46. It is a rare and lustrous silvery-white metal discovered in 1803 by William Hyde Wollaston. He named it after the asteroid Pallas, which was itself named after the epithet of the Greek goddess Athena, acquired by her when she slew Pallas. Palladium, platinum, rhodium, ruthenium, iridium and osmium form a group of elements referred to as the platinum group metals (PGMs). These have similar chemical properties, but palladium has the lowest melting point and is the least dense of them.
Over half of the supply of palladium and its congener platinum goes into catalytic converters, which convert up to 90% of harmful gases from auto exhaust (hydrocarbons, carbon monoxide, and nitrogen dioxide) into less-harmful substances (nitrogen, carbon dioxide and water vapor). Palladium is also used in electronics, dentistry, medicine, hydrogen purification, chemical applications, groundwater treatment and jewelry. Palladium plays a key role in the technology used for fuel cells, which combine hydrogen and oxygen to produce electricity, heat, and water.
Ore deposits of palladium and other PGMs are rare, and the most extensive deposits have been found in the norite belt of the Bushveld Igneous Complex covering the Transvaal Basin in South Africa, the Stillwater Complex in Montana, United States, the Thunder Bay District of Ontario, Canada, and the Norilsk Complex in Russia. Recycling is also a source of palladium, mostly from scrapped catalytic converters. The numerous applications and limited supply sources of palladium result in the metal attracting considerable investment interest.
Characteristics.
Palladium belongs to group 10 in the periodic table, but has a very atypical configuration in its outermost electron shells compared to the other members of group 10 (see also niobium (41), ruthenium (44), and rhodium (45)), having fewer filled electron shells than the elements directly preceding it (a phenomenon unique to palladium). This gives its valence shell eighteen electrons – ten more than the eight found in the valence shells of the noble gases from neon onward.
Palladium is a soft silver-white metal that resembles platinum. It is the least dense and has the lowest melting point of the platinum group metals. It is soft and ductile when annealed and greatly increases its strength and hardness when it is cold-worked. Palladium dissolves slowly in concentrated nitric acid, in hot, concentrated sulfuric acid, and, when finely divided, in hydrochloric acid.
Common oxidation states of palladium are 0, +1, +2 and +4. There are relatively few known compounds with palladium unambiguously in the +3 oxidation state, though such compounds have been proposed as intermediates in many palladium-catalyzed cross-coupling reactions. In 2002, palladium(VI) was first reported.
Palladium films with defects produced by alpha particle bombardment at low temperature exhibit superconductivity having Tc=3.2 K.
Isotopes.
Naturally occurring palladium is composed of seven isotopes, which includes six stable isotopes. The most stable radioisotopes are 107Pd with a half-life of 6.5 million years (found in nature), 103Pd with a half-life of 17 days, and 100Pd with a half-life of 3.63 days. Eighteen other radioisotopes have been characterized with atomic weights ranging from 90.94948(64) u (91Pd) to 122.93426(64) u (123Pd). Most of these have half-lives that are less than thirty minutes, except 101Pd (half-life: 8.47 hours), 109Pd (half-life: 13.7 hours), and 112Pd (half-life: 21 hours).
For isotopes with atomic mass unit values less than that of the most abundant stable isotope, 106Pd, the primary decay mode is electron capture with the primary decay product being rhodium. The primary mode of decay for those isotopes of Pd with atomic mass greater than 106 is beta decay with the primary product of this decay being silver.
Radiogenic 107Ag is a decay product of 107Pd and was first discovered in 1978 in the Santa Clara meteorite of 1976. The discoverers suggest that the coalescence and differentiation of iron-cored small planets may have occurred 10 million years after a nucleosynthetic event. 107Pd versus Ag correlations observed in bodies, which have been melted since accretion of the solar system, must reflect the presence of short-lived nuclides in the early solar system.
Compounds.
Palladium does not react with oxygen at normal temperatures (and thus does not tarnish in air). Palladium heated to 800 °C will produce a layer of palladium(II) oxide (PdO). It tarnishes lightly in a moist atmosphere containing sulfur. Palladium primarily exists in the 0, +2, and +4 oxidation states; the +4 oxidation state is comparatively rare. One major example of palladium(IV) is hexachloropalladate(IV), [PdCl6]2−.
Elemental palladium reacts with chlorine to give palladium(II) chloride; it dissolves in nitric acid and precipitates palladium(II) acetate on addition of acetic acid. These two compounds and the bromide are reactive and relatively inexpensive, making them convenient entry points to palladium chemistry. All three are not monomeric; the chloride and bromide often must be refluxed in acetonitrile to obtain the more reactive acetonitrile complex monomers, for example:
Palladium(II) chloride is the principal starting material for many other palladium catalysts. It is used to prepare heterogeneous palladium catalysts: palladium on barium sulfate, palladium on carbon, and palladium chloride on carbon. It reacts with triphenylphosphine in coordinating solvents to give bis(triphenylphosphine)palladium(II) dichloride, a useful catalyst. Where desired, the catalyst may be formed "in situ".
Reduction of this phosphine complex with hydrazine with more phosphine gives tetrakis(triphenylphosphine)palladium(0), one of the two major palladium(0) complexes:
The other major palladium(0) complex, tris(dibenzylideneacetone)dipalladium(0) (Pd2(dba)3), is prepared by reducing sodium tetrachloropalladate in the presence of dibenzylideneacetone.
Mixed valence palladium complex of Pd4(CO)4(OAc)4Pd(acac)2 forms an infinite Pd chain structure, with alternatively interconnected Pd4(CO)4(OAc)4 and Pd(acac)2 units.
The great many reactions in which palladium compounds serve as catalysts are collectively known as palladium-catalyzed coupling reactions. Prominent examples include the Heck, Suzuki and Stille reactions. Palladium(II) acetate, tetrakis(triphenylphosphine)palladium(0) (Pd(PPh3)4, and tris(dibenzylideneacetone)dipalladium(0) (Pd2(dba)3) are useful in this regard, either as catalysts or as starting points to catalysts. 
History.
William Hyde Wollaston noted the discovery of a new noble metal in July 1802 in his lab-book and named it palladium in August of the same year. Wollaston purified enough of the material and offered it, without naming the discoverer, in a small shop in Soho in April 1803. After harsh criticism that palladium is an alloy of platinum and mercury by Richard Chenevix, Wollaston anonymously offered a reward of 20 British pounds for 20 grains of synthetic palladium "alloy". Chenevix received the Copley Medal in 1803 after he published his experiments on palladium. Wollaston published the discovery of rhodium in 1804 and mentions some of his work on palladium. He disclosed that he was the discoverer of palladium in a publication in 1805.
It was named by Wollaston in 1802 after the asteroid Pallas, which had been discovered two months earlier. Wollaston found palladium in crude platinum ore from South America by dissolving the ore in aqua regia, neutralizing the solution with sodium hydroxide, and precipitating platinum as ammonium chloroplatinate with ammonium chloride. He added mercuric cyanide to form the compound palladium(II) cyanide, which was heated to extract palladium metal.
Palladium chloride was at one time prescribed as a tuberculosis treatment at the rate of 0.065 g per day (approximately one milligram per kilogram of body weight). This treatment had many negative side-effects, and was later replaced by more effective drugs.
In the run up to 2000, the Russian supply of palladium to the global market was repeatedly delayed and disrupted because the export quota was not granted on time, for political reasons. The ensuing market panic drove the price to an all-time high of $1100 per troy ounce in January 2001. Around this time, the Ford Motor Company, fearing auto vehicle production disruption due to a possible palladium shortage, stockpiled large amounts of the metal purchased near the price high. When prices fell in early 2001, Ford lost nearly US$1 billion. World demand for palladium increased from 100 tons in 1990 to nearly 300 tons in 2000. The global production of palladium from mines was 222 tonnes in 2006 according to the United States Geological Survey. Most palladium is used for catalytic converters in the automobile industry. There are currently concerns about a steady supply of palladium in the wake of Russia's military maneuvers in Ukraine, partly as sanctions could hamper Russian palladium exports; any restrictions on Russian palladium exports would exacerbate what is already expected to be a large palladium deficit in 2014.
Occurrence.
In 2007, Russia was the top producer of palladium, with a 44% world share, followed by South Africa with 40%. Canada with 6% and the U.S. with 5% are the only other substantial producers of palladium.
Palladium can be found as a free metal alloyed with gold and other platinum-group metals in placer deposits of the Ural Mountains, Australia, Ethiopia, North and South America. For the production of palladium these deposits play only a minor role. The most important commercial sources are nickel-copper deposits found in the Sudbury Basin, Ontario, and the Norilsk–Talnakh deposits in Siberia. The other large deposit is the Merensky Reef platinum group metals deposit within the Bushveld Igneous Complex South Africa. The Stillwater igneous complex of Montana and the Roby zone ore body of the Lac des Îles igneous complex of Ontario are the two other sources of palladium in Canada and the United States. Palladium is found in the rare minerals cooperite and polarite.
Palladium is also produced in nuclear fission reactors and can be extracted from spent nuclear fuel (see synthesis of precious metals) though this source for palladium is not used. None of the existing nuclear reprocessing facilities are equipped to extract palladium from the high-level radioactive waste.
Applications.
The largest use of palladium today is in catalytic converters. Palladium is also used in jewelry, dentistry, watch making, blood sugar test strips, aircraft spark plugs and in the production of surgical instruments and electrical contacts. Palladium is also used to make professional transverse flutes. As a commodity, palladium bullion has ISO currency codes of XPD and 964. Palladium is one of only four metals to have such codes, the others being gold, silver and platinum. Because of its ability to absorb hydrogen, palladium is a key component of the controversial cold fusion experiments that began in 1989.
Catalysis.
When it is finely divided, such as in palladium on carbon, palladium forms a versatile catalyst and speeds up hydrogenation and dehydrogenation reactions, as well as in petroleum cracking. A large number of carbon–carbon bond forming reactions in organic chemistry (such as the Heck reaction and Suzuki coupling) are facilitated by catalysis with palladium compounds. (See Palladium Compounds and palladium-catalyzed coupling reactions.) In addition, palladium, when dispersed on conductive materials, proves to be an excellent electrocatalyst for oxidation of primary alcohols in alkaline media. In 2010, palladium-catalysed organic reactions were recognised by the Nobel Prize in Chemistry. Palladium is also a versatile metal for homogeneous catalysis. It is used in combination with a broad variety of ligands for highly selective chemical transformations. A 2008 study showed that palladium is an effective catalyst for making carbon-fluoride bonds. Palladium is found in the Lindlar catalyst, also called Lindlar's Palladium.
Electronics.
The second-biggest application of palladium in electronics is in the manufacture of multilayer ceramic capacitors, in which palladium (and palladium-silver alloys) are used as electrodes. Palladium (sometimes alloyed with nickel) is used in connector platings in consumer electronics.
It is also used in plating of electronic components and in soldering materials. The electronic sector consumed 1.07 million troy ounces (33.2 tonnes) of palladium in 2006, according to a Johnson Matthey report.
Technology.
Hydrogen easily diffuses through heated palladium; thus, it provides a means of purifying the gas. Membrane reactors with Pd membranes are therefore used for the production of high purity hydrogen. Palladium is a part of the palladium-hydrogen electrode in electrochemical studies. Palladium(II) chloride can oxidize large amounts of carbon monoxide gas, and is used in carbon monoxide detectors.
Hydrogen storage.
Palladium readily absorbs hydrogen at room temperatures forming palladium hydride PdHx with x below 1. While this property is common to many transition metals, palladium is unique by the high absorption capacity and by that it does not lose its ductility until high x values. This property has been investigated for designing an efficient, yet inexpensive hydrogen storage material (palladium itself is prohibitively expensive for this purpose).
The content of hydrogen in palladium can be linked to the magnetic susceptibility, which decreases with the increase of hydrogen content. The susceptibility becomes zero for PdH0.62. At higher ratio the solid solution becomes diamagnetic.
Dentistry.
Palladium is used in small amount (about 0.5%) some alloys of dental amalgam in order to decrease corrosion and increase the metallic lustre of the final restoration.
Jewelry.
Palladium has been used as a precious metal in jewelry since 1939, as an alternative to platinum for making white gold. This use resulted from the naturally white color of palladium, which required no rhodium plating. Palladium is much less dense than platinum. Similar to gold, palladium can be beaten into a thin leaf form as thin as 100 nm ( in). Unlike platinum, palladium may discolor upon heating to above ; it is relatively brittle.
Palladium is one of the three most popular metals used to make white gold alloys (nickel and silver can also be used). Palladium-gold is a more expensive alloy than nickel-gold, but seldom causes allergic reactions (though certain cross-allergies with nickel may occur).
When platinum was declared a strategic government resource during World War II, many jewelry bands were made out of palladium. As recently as September 2001, palladium was more expensive than platinum and rarely used in jewelry also due to the technical obstacle of casting. However, the casting problem has been resolved and its use in jewelry has increased because of a large spike in the price of platinum and a drop in the price of palladium.
Prior to 2004, the principal use of palladium in jewelry was the manufacture of white gold. In early 2004, when gold and platinum prices rose steeply, China began fabricating significant volumes of palladium jewelry and used 37 tonnes of palladium for this purpose in 2005. Changes of the relative price between palladium and platinum after 2008 lowered demand for palladium to 17.4 tonnes in 2009.
In January 2010, hallmarks for palladium were introduced by assay offices in the United Kingdom, and it became a legal requirement to hallmark all articles of jewellery described as being wholly or partly made of palladium. Articles can be marked as containing a minimum of either 500, 950, or 999 parts per thousand of palladium.
Fountain pen nibs made from gold are sometimes plated with palladium when a silver, rather than gold, appearance is desired. Sheaffer has used palladium plating for many decades, either as an accent on otherwise gold nibs or to cover the gold completely.
Photography.
With the platinotype printing process photographers make fine-art black-and-white prints using platinum or palladium salts. Often used with platinum, palladium provides an alternative to silver.
Toxicity.
Palladium is a metal with low toxicity. It is poorly absorbed by human body when digested. Plants such as the water hyacinth are killed by low levels of palladium salts. Most other plants tolerate it, although tests show that at levels above 0.0003% growth is affected. High doses of palladium could be poisonous; tests on rodents suggest it may be carcinogenic, but there is no clear evidence that the element has any adverse effects on humans.
Precautions.
Finely divided palladium metal can be pyrophoric. As a platinum-group metal, the bulk material is quite inert. Although contact dermatitis has been reported, the amount of data on the effects of exposure to palladium is limited. It has been shown that people with an allergic reaction to palladium also react to nickel, making it advisable to avoid the use of dental alloys containing palladium on those so allergic.
A considerable amount of palladium is distributed by the exhausts of cars with catalytic converters. Between 4 and 108 ng/km of palladium particulate is released by such cars. Its total uptake from food is estimated to be lower than 2 µg per person a day. The second possible source for palladium is alloys for dental restoration; there the possible uptake of palladium is estimated to be lower than 15 µg per person per day. People working with palladium or its compounds might have a considerably higher uptake. For soluble compounds such as palladium chloride, 99% is eliminated from the body within 3 days.
The median lethal dose (LD50) of soluble palladium compounds in mice is 200 mg/kg for oral and 5 mg/kg for intravenous administration.

</doc>
<doc id="23321" url="https://en.wikipedia.org/wiki?curid=23321" title="Promethium">
Promethium

Promethium, originally prometheum, is a chemical element with the symbol Pm and atomic number 61. All of its isotopes are radioactive; it is one of only two such elements that are followed in the periodic table by elements with stable forms, a distinction shared with technetium. Chemically, promethium is a lanthanide, which forms salts when combined with other elements. Promethium shows only one stable oxidation state of +3; however, a few +2 compounds may exist.
In 1902, Bohuslav Brauner suggested there was an element with properties intermediate between those of the known elements neodymium (60) and samarium (62); this was confirmed in 1914 by Henry Moseley who, having measured the atomic numbers of all the elements then known, found there was no element with atomic number 61. In 1926, an Italian and an American group claimed to have isolated a sample of element 61; both "discoveries" were soon proven to be false. In 1938, during a nuclear experiment conducted at Ohio State University, a few radioactive nuclides were produced that certainly were not radioisotopes of neodymium or samarium, but there was a lack of chemical proof that element 61 was produced, and the discovery was not generally recognized. Promethium was first produced and characterized at Oak Ridge National Laboratory in 1945 by the separation and analysis of the fission products of uranium fuel irradiated in a graphite reactor. The discoverers proposed the name "prometheum" (the spelling was subsequently changed), derived from Prometheus, the Titan in Greek mythology who stole fire from Mount Olympus and brought it down to humans, to symbolize "both the daring and the possible misuse of mankind's intellect." However, a sample of the metal was made only in 1963.
There are two possible sources for natural promethium: rare decays of natural europium-151 (producing promethium-147), and uranium (various isotopes). Practical applications exist only for chemical compounds of promethium-147, which are used in luminous paint, atomic batteries and thickness measurement devices, even though promethium-145 is the most stable promethium isotope. Because natural promethium is exceedingly scarce, it is typically synthesized by bombarding uranium-235 (enriched uranium) with thermal neutrons to produce promethium-147.
Properties.
Physical properties.
A promethium atom has 61 electrons, arranged in the configuration [Xe]4f56s2. In forming compounds, the atom loses its two outermost electrons and one of the 4f-electrons, which belongs to an open subshell. The element's atomic radius is the third largest among all the lanthanides but is only slightly greater than those of the neighboring elements. It is the only exception to the general trend of the contraction of the atoms with increase of atomic radius (caused by the lanthanide contraction) that is not caused by the filled (or half-filled) 4f-subshell.
Many properties of promethium rely on its position among lanthanides and are intermediate between those of neodymium and samarium. For example, the melting point, the first three ionization energies, and the hydration energy are greater than those of neodymium and lower than those of samarium; similarly, the estimate for the boiling point, ionic (Pm3+) radius, and standard heat of formation of monatomic gas are greater than those of samarium and less those of neodymium.
Promethium has a double hexagonal close packed (dhcp) structure and a hardness of 63 kg/mm2. This low-temperature alpha form converts into a beta, body-centered cubic (bcc) phase upon heating to 890 °C.
Chemical properties and compounds.
Promethium belongs to the cerium group of lanthanides and is chemically very similar to the neighboring elements. Because of its instability, chemical studies of promethium are incomplete. Even though a few compounds have been synthesized, they are not fully studied; in general, they tend to be pink or red in color. Treatment of acidic solutions containing Pm3+ ions with ammonia results in a gelatinous light-brown sediment of hydroxide, Pm(OH)3, which is insoluble in water. When dissolved in hydrochloric acid, a water-soluble yellow salt, PmCl3, is produced; similarly, when dissolved in nitric acid, a nitrate results, Pm(NO3)3. The latter is also well-soluble; when dried, it forms pink crystals, similar to Nd(NO3)3. The electron configuration for Pm3+ is 4f4, and the color of the ion is pink. The ground state term symbol is 5I4. The sulfate is slightly soluble, like the other cerium group sulfates. Cell parameters have been calculated for its octahydrate; they lead to conclusion that the density of Pm2(SO4)3·8 H2O is 2.86 g/cm3. The oxalate, Pm2(C2O4)3·10 H2O, has the lowest solubility of all lanthanide oxalates.
Unlike the nitrate, the oxide is similar to the corresponding samarium salt and not the neodymium salt. As-synthesized, e.g. by heating the oxalate, it is a white or lavender-colored powder with disordered structure. This powder crystallizes in a cubic lattice upon heating to 600 °C. Further annealing at 800 °C and then at 1750 °C irreversibly transforms it to a monoclinic and hexagonal phases, respectively, and the last two phases can be interconverted by adjusting the annealing time and temperature.
Promethium forms only one stable oxidation state, +3, in the form of ions; this is in line with other lanthanides. According to its position in the periodic table, the element cannot be expected to form stable +4 or +2 oxidation states; treating chemical compounds containing Pm3+ ions with strong oxidizing or reducing agents showed that the ion is not easily oxidized or reduced.
Isotopes.
Promethium is the only lanthanide and one of only two elements among the first 83 that has no stable (or even long-lived) isotopes. This is a result of a rarely occurring effect of the liquid drop model and stabilities of neighbor element isotopes; it is also the least stable element of the first 84. The primary decay products are neodymium and samarium isotopes (promethium-146 decays to both, the lighter isotopes generally to neodymium via positron decay and electron capture, and the heavier isotopes to samarium via beta decay). Promethium nuclear isomers may decay to other promethium isotopes and one isotope (145Pm) has a very rare alpha decay mode to praseodymium.
The most stable isotope of the element is promethium-145, which has a specific activity of /g and a half-life of 17.7 years via electron capture. Because it has 84 neutrons (two more than 82, which is a magic number which corresponds to a stable neutron configuration), it may emit an alpha particle (which has 2 neutrons) to form praseodymium-141 with 82 neutrons. Thus it is the only promethium isotope with an experimentally observed alpha decay. Its partial half-life for alpha decay is about 6.3 years, and the relative probability for a 145Pm nucleus to decay in this way is 2.8%. Several other Pm isotopes (144Pm, 146Pm, 147Pm etc.) also have a positive energy release for alpha decay; their alpha decays are predicted to occur but have not been observed.
The element also has 18 nuclear isomers, with mass numbers of 133 to 142, 144, 148, 149, 152, and 154 (some mass numbers have more than one isomer). The most stable of them is promethium-148m, with a half-life of 43.1 days; this is longer than the half-lives of the ground states of all promethium isotopes, except only for promethium-143 to 147 (note that promethium-148m has a longer half-life than the ground state, promethium-148).
Occurrence.
In 1934, Willard Libby found weak beta activity in pure neodymium, which was attributed to a half-life over 1012 years. Almost 20 years later, it was claimed that the element occurs in natural neodymium in equilibrium in quantities below 10−20 grams of promethium per one gram of neodymium. However, these observations were disproved by newer investigations, because for all seven naturally occurring neodymium isotopes, any single beta decays (which can produce promethium nuclides) are forbidden by energy conservation. In particular, careful measurements of atomic masses show that the mass difference 150Nd-150Pm is negative (−87 keV), which absolutely prevents the single beta decay of 150Nd to 150Pm.
Both isotopes of natural europium have larger mass excesses than sums of those of their potential alpha daughters plus that of an alpha particle; therefore, they (stable in practice) may alpha decay. Research at Laboratori Nazionali del Gran Sasso showed that europium-151 experimentally decays to promethium-147 with the half-life of 5 years. It has been shown that europium is "responsible" for about 12 grams of promethium in the Earth's crust. Alpha decays for europium-153 have not been found yet, and its theoretically calculated half-life is so high (due to low energy of decay) that this process will probably never be observed.
Finally, promethium can be formed in nature as a product of spontaneous fission of uranium-238. Only trace amounts can be found in naturally occurring ores: a sample of pitchblende has been found to contain promethium at a concentration of four parts per quintillion (1018) by mass. Uranium is thus "responsible" for 560 g promethium in Earth's crust.
Promethium has also been identified in the spectrum of the star HR 465 in Andromeda; it also has been found in HD 101065 (Przybylski's star) and HD 965. Because of the short half-life of promethium isotopes, they should be formed near the surface of those stars.
History.
Searches for element 61.
In 1902, Czech chemist Bohuslav Brauner found out that the difference between neodymium and samarium is the largest of all neighboring lanthanides pairs; as a conclusion, he suggested there was an element with intermediate properties between them. This prediction was supported in 1914 by Henry Moseley who, having discovered that atomic number was an experimentally measurable property of elements, found a few atomic numbers had no element to correspond: the gaps were 43, 61, 72, 75, 85, and 87. With the knowledge of a gap in the periodic table several groups started to search for the predicted element among other rare earths in the natural environment.
The first claim of a discovery was published by Luigi Rolla and Lorenzo Fernandes of Florence, Italy. After separating a mixture of a few rare earth elements nitrate concentrate from the Brazilian mineral monazite by fractionated crystallization, they yielded a solution containing mostly samarium. This solution gave x-ray spectra attributed to samarium and element 61. In honor of their city, they named element 61 "florentium." The results were published in 1926, but the scientists claimed that the experiments were done in 1924. Also in 1926, a group of scientists from the University of Illinois at Urbana-Champaign, Smith Hopkins and Len Yntema published the discovery of element 61. They named it "illinium," after the university. Both of these reported discoveries were shown to be erroneous because the spectrum line that "corresponded" to element 61 was identical to that of didymium; the lines thought to belong to element 61 turned out to belong to a few impurities (barium, chromium, and platinum).
In 1934, Josef Mattauch finally formulated the isobar rule. One of the indirect consequences of this rule was that element 61 was unable to form stable isotopes. In 1938, a nuclear experiment was conducted by H. B. Law "et al." at Ohio State University. The nuclides produced certainly were not radioisotopes of neodymium or samarium, and the name "cyclonium" was proposed, but there was a lack of chemical proof that element 61 was produced and the discovery not largely recognized.
Discovery and synthesis of promethium metal.
Promethium was first produced and characterized at Oak Ridge National Laboratory (Clinton Laboratories at that time) in 1945 by Jacob A. Marinsky, Lawrence E. Glendenin and Charles D. Coryell by separation and analysis of the fission products of uranium fuel irradiated in the graphite reactor; however, being too busy with military-related research during World War II, they did not announce their discovery until 1947. The original proposed name was "clintonium", after the laboratory where the work was conducted; however, the name "prometheum" was suggested by Grace Mary Coryell, the wife of one of the discoverers. It is derived from Prometheus, the Titan in Greek mythology who stole fire from Mount Olympus and brought it down to humans and symbolizes "both the daring and the possible misuse of the mankind intellect." The spelling was then changed to "promethium," as this was in closer in accordance with other metals.
In 1963, promethium(III) fluoride was used to make promethium metal. Provisionally purified from impurities of samarium, neodymium, and americium, it was put into a tantalum crucible which was located in another tantalum crucible; the outer crucible contained lithium metal (10 times excess compared to promethium). After creating a vacuum, the chemicals were mixed to produce promethium metal:
The promethium sample produced was used to measure a few of the metal's properties, such as its melting point.
In 1963, ion-exchange methods were used at ORNL to prepare about ten grams of promethium from nuclear reactor fuel processing wastes.
Today, promethium is still recovered from the byproducts of uranium fission; it can also be produced by bombarding 146Nd with neutrons, turning it into 147Nd which decays into 147Pm through beta decay with a half-life of 11 days.
Production.
The production methods for different isotopes vary, and only that for promethium-147 is given because it is the only isotope with industrial applications. Promethium-147 is produced in large quantities (compared to other isotopes) by bombarding uranium-235 with thermal neutrons. The output is relatively high, at 2.6% of the total product. Another way to produce promethium-147 is via neodymium-147, which decays to promethium-147 with a short half-life. Neodymium-147 can be obtained either by bombarding enriched neodymium-146 with thermal neutrons or by bombarding a uranium carbide target with energetic protons in a particle accelerator. Another method is to bombard uranium-238 with fast neutrons to cause fast fission, which, among multiple reaction products, creates promethium-147.
As early as the 1960s, Oak Ridge National Laboratory could produce 650 grams of promethium per year and was the world's only large-volume synthesis facility. Gram-scale production of promethium has been discontinued in the U.S. in the early 1980s, but will possibly be resumed after 2010 at the High Flux Isotope Reactor. Currently, Russia is the only country producing promethium-147 on a relatively large scale.
Applications.
Most promethium is used only for research purposes, except for promethium-147, which can be found outside laboratories. It is obtained as the oxide or chloride, in milligram quantities. This isotope does not emit gamma rays, and its radiation has a relatively small penetration depth in matter and a relatively long half-life.
Some signal lights use a luminous paint, containing a phosphor that absorbs the beta radiation emitted by promethium-147 and emits light. This isotope does not cause aging of the phosphor, as alpha emitters do, and therefore the light emission is stable for a few years. Originally, radium-226 was used for the purpose, but it was later replaced by promethium-147 and tritium (hydrogen-3). Promethium may be favored over tritium for safety reasons.
In atomic batteries, the beta particles emitted by promethium-147 are converted into electric current by sandwiching a small Pm source between two semiconductor plates. These batteries have a useful lifetime of about five years. The first promethium-based battery was assembled in 1964 and generated "a few milliwatts of power from a volume of about 2 cubic inches, including shielding".
Promethium is also used to measure the thickness of materials by evaluating the amount of radiation from a promethium source that passes through the sample. It has possible future uses in portable X-ray sources, and as auxiliary heat or power sources for space probes and satellites (although the alpha emitter plutonium-238 has become standard for most space-exploration-related uses).
Precautions.
The element, like other lanthanides, has no biological role. Promethium-147 can emit X-rays during its beta decay, which are dangerous for all lifeforms. Interactions with tiny quantities of promethium-147 are not hazardous if certain precautions are observed. In general, gloves, footwear covers, safety glasses, and an outer layer of easily removed protective clothing should be used.
It is not known what human organs are affected by interaction with promethium; a possible candidate is the bone tissues. Sealed promethium-147 is not dangerous. However, if the packaging is damaged, then promethium becomes dangerous to the environment and humans. If radioactive contamination is found, the contaminated area should be washed with water and soap, but, even though promethium mainly affects the skin, the skin should not be abraded. If a promethium leak is found, the area should be identified as hazardous and evacuated, and emergency services must be contacted. No dangers from promethium aside from the radioactivity are known.

</doc>
<doc id="23322" url="https://en.wikipedia.org/wiki?curid=23322" title="Protactinium">
Protactinium

Protactinium or protoactinium (former name) is a chemical element with symbol Pa and atomic number 91. It is a dense, silvery-gray metal which readily reacts with oxygen, water vapor and inorganic acids. It forms various chemical compounds where protactinium is usually present in the oxidation state +5, but can also assume +4 and even +2 or +3 states. The average concentrations of protactinium in the Earth's crust is typically on the order of a few parts per trillion, but may reach up to a few parts per million in some uraninite ore deposits. Because of its scarcity, high radioactivity and high toxicity, there are currently no uses for protactinium outside of scientific research, and for this purpose, protactinium is mostly extracted from spent nuclear fuel.
Protactinium was first identified in 1913 by Kasimir Fajans and Oswald Helmuth Göhring and named "brevium" because of the short half-life of the specific isotope studied, namely protactinium-234. A more stable isotope (231Pa) of protactinium was discovered in 1917/18 by Otto Hahn and Lise Meitner, and they chose the name proto-actinium, but then the IUPAC named it finally protactinium in 1949 and confirmed Hahn and Meitner as discoverers. The new name meant "parent of actinium" and reflected the fact that actinium is a product of radioactive decay of protactinium. It is noted that John Arnold Cranston (working with Frederick Soddy) is also credited with discovering the stable isotope in 1915 but delayed his announcement due to being called up for service in the First World War.
The longest-lived and most abundant (nearly 100%) naturally occurring isotope of protactinium, protactinium-231, has a half-life of 32,760 years and is a decay product of uranium-235. Much smaller trace amounts of the short-lived nuclear isomer protactinium-234m occur in the decay chain of uranium-238. Protactinium-233 results from the decay of thorium-233 as part of the chain of events used to produce uranium-233 by neutron irradiation of thorium-232. It is an undesired intermediate product in thorium-based nuclear reactors and is therefore removed from the active zone of the reactor during the breeding process. Analysis of the relative concentrations of various uranium, thorium and protactinium isotopes in water and minerals is used in radiometric dating of sediments which are up to 175,000 years old and in modeling of various geological processes.
History.
In 1871, Dmitri Mendeleev predicted the existence of an element between thorium and uranium. The actinide element group was unknown at the time. Therefore, uranium was positioned below tungsten, and thorium below zirconium, leaving the space below tantalum empty and, until the 1950s, periodic tables were published with this structure. For a long time chemists searched for eka-tantalum as an element with similar chemical properties to tantalum, making a discovery of protactinium nearly impossible. Tantalum's heavier analogue was later found to be the transuranic element dubnium.
In 1900, William Crookes isolated protactinium as an intensely radioactive material from uranium; however, he could not characterize it as a new chemical element and thus named it uranium-X (UX). Crookes dissolved uranium nitrate in ether, the residual aqueous phase contains most of the and . His method was still used in the 1950s to isolate and from uranium compounds. Protactinium was first identified in 1913, when Kasimir Fajans and Oswald Helmuth Göhring encountered the isotope 234Pa during their studies of the decay chains of uranium-238: → → → . They named the new element "brevium" (from the Latin word, "brevis", meaning brief or short) because of its short half-life, 6.7 hours for . In 1917/18, two groups of scientists, Otto Hahn and Lise Meitner of Germany and Frederick Soddy and John Cranston of Great Britain, independently discovered another isotope of protactinium, 231Pa having much longer half-life of about 32,000 years. Thus the name "brevium" was changed to "protoactinium" as the new element was part of the decay chain of uranium-235 before the actinium (from = "protos" meaning "first", "before"). For ease of pronunciation, the name was shortened to "protactinium" by the IUPAC in 1949. The discovery of protactinium completed one of the last gaps in the early versions of the periodic table, proposed by Mendeleev in 1869, and it brought to fame the involved scientists.
Aristid von Grosse produced 2 milligrams of Pa2O5 in 1927, and in 1934 first isolated elemental protactinium from 0.1 milligrams of Pa2O5. He used two different procedures: in the first one, protactinium oxide was irradiated by 35 keV electrons in vacuum. In another method, called the van Arkel–de Boer process, the oxide was chemically converted to a halide (chloride, bromide or iodide) and then reduced in a vacuum with an electrically heated metallic filament:
In 1961, the United Kingdom Atomic Energy Authority (UKAEA) produced 125 grams of 99.9% pure protactinium by processing 60 tonnes of waste material in a 12-stage process, at a cost of about 500,000 USD. For many years, this was the world's only significant supply of protactinium, which was provided to various laboratories for scientific studies. Oak Ridge National Laboratory in the US provided protactinium at a cost of about 280 USD/gram.
Isotopes.
Twenty-nine radioisotopes of protactinium have been discovered, the most stable being 231Pa with a half-life of 32,760 years, 233Pa with a half-life of 27 days, and 230Pa with a half-life of 17.4 days. All of the remaining isotopes have half-lives shorter than 1.6 days, and the majority of these have half-lives less than 1.8 seconds. Protactinium also has two nuclear isomers, 217mPa (half-life 1.2 milliseconds) and 234mPa (half-life 1.17 minutes).
The primary decay mode for isotopes of protactinium lighter than (and including) the most stable isotope 231Pa (i.e., 212Pa to 231Pa) is alpha decay and the primary mode for the heavier isotopes (i.e., 232Pa to 240Pa) is beta decay. The primary decay products of isotopes of protactinium lighter than (and including) 231Pa are actinium isotopes and the primary decay products for the heavier isotopes of protactinium are uranium isotopes.
Occurrence.
Protactinium is one of the rarest and most expensive naturally occurring elements. It is found in the form of two isotopes – 231Pa and 234Pa, with the isotope 234Pa occurring in two different energy states. Nearly all natural protactinium is protactinium-231. It is an alpha emitter and is formed by the decay of uranium-235, whereas the beta radiating protactinium-234 is produced as a result of uranium-238 decay. Nearly all uranium-238 (99.8%) decays first to the 234mPa isomer.
Protactinium occurs in uraninite (pitchblende) at concentrations of about 0.3-3 parts 231Pa per million parts (ppm) of ore. Whereas the usual content is closer to 0.3 ppm (e.g. in Jáchymov, Czech Republic), some ores from the Democratic Republic of the Congo have about 3 ppm. Protactinium is homogeneously dispersed in most natural materials and in water, but at much lower concentrations on the order of one part per trillion, that corresponds to the radioactivity of 0.1 picocuries (pCi)/g. There is about 500 times more protactinium in sandy soil particles than in water, even the water present in the same sample of soil. Much higher ratios of 2,000 and above are measured in loam soils and clays, such as bentonite.
In nuclear reactors.
Two major protactinium isotopes, 231Pa and 233Pa, are produced from thorium in nuclear reactors; both are undesirable and are usually removed, thereby adding complexity to the reactor design and operation. In particular, 232Th via ("n",2"n") reactions produces 231Th which quickly (half-life 25.5 hours) decays to 231Pa. The last isotope, while not a transuranic waste, has a long half-life of 32,760 years and is a major contributor to the long term radiotoxicity of spent nuclear fuel.
Protactinium-233 is formed upon neutron capture by 232Th. It further either decays to uranium-233 or captures another neutron and converts into the non-fissile uranium-234. 233Pa has a relatively long half-life of 27 days and high cross section for neutron capture (the so-called "neutron poison"). Thus instead of rapidly decaying to the useful 233U, a significant fraction of 233Pa converts to non-fissile isotopes and consumes neutrons, degrading the reactor efficiency. To avoid this, 233Pa is extracted from the active zone of thorium molten salt reactors, during their operation, so that it only decays to 233U. This is achieved using several meters tall columns of molten bismuth with lithium dissolved in it. In a simplified scenario, lithium selectively reduces protactinium salts to protactinium metal which is then extracted from the molten-salt cycle, and bismuth is merely a carrier. It is chosen because of its low melting point (271 °C), low vapor pressure, good solubility for lithium and actinides, and immiscibility with molten halides.
Preparation.
Before the advent of nuclear reactors, protactinium was separated for scientific experiments from uranium ores. Nowadays, it is mostly produced as an intermediate product of nuclear fission in thorium high-temperature reactors:
Protactinium metal can be prepared by reduction of its fluoride with calcium fluoride, lithium or barium at a temperature of 1300–1400 °C.
Physical and chemical properties.
Protactinium is an actinide which is positioned in the periodic table to the left of uranium and to the right of thorium, and many of its physical properties are intermediate between those two actinides. So, protactinium is more dense and rigid than thorium but is lighter than uranium, and its melting point is lower than that of thorium and higher than that of uranium. The thermal expansion, electrical and thermal conductivities of these three elements are comparable and are typical of post-transition metals. The estimated shear modulus of protactinium is similar to that of titanium. Protactinium is a metal with silvery-gray luster that is preserved for some time in air. Protactinium easily reacts with oxygen, water vapor and acids, but not with alkalis.
At room temperature, protactinium crystallizes in body-centered tetragonal structure which can be regarded as distorted body-centered cubic lattice; this structure does not change upon compression up to 53 GPa. The structure changes to face-centered cubic ("fcc") upon cooling from high temperature, at about 1200 °C. The thermal expansion coefficient of the tetragonal phase between room temperature and 700 °C is 9.9/°C.
Protactinium is paramagnetic and no magnetic transitions are known for it at any temperature. It becomes superconductive at temperatures below 1.4 K. Protactinium tetrachloride is paramagnetic at room temperature but turns ferromagnetic upon cooling to 182 K.
Protactinium exists in two major oxidation states, +4 and +5, both in solids and solutions, and the +3 and +2 states were observed in some solid phases. As the electron configuration of the neutral atom is 5f26d17s2, the +5 oxidation state corresponds to the low-energy (and thus favored) 5f0 configuration. Both +4 and +5 states easily form hydroxides in water with the predominant ions being Pa(OH)3+, , and Pa(OH)4, all colorless. Other known protactinium ions include , , PaF3+, , , and .
Chemical compounds.
Here "a", "b" and "c" are lattice constants in picometers, No is space group number and "Z" is the number of formula units per unit cell; "fcc" stands for the face-centered cubic symmetry. Density was not measured directly but calculated from the lattice parameters.
Oxides and oxygen-containing salts.
Protactinium oxides are known for the metal oxidation states +2, +4 and +5. The most stable is white pentoxide Pa2O5, which can be produced by igniting protactinium(V) hydroxide in air at a temperature of 500 °C. Its crystal structure is cubic, and the chemical composition is often non-stoichiometric, described as PaO2.25. Another phase of this oxide with orthorhombic symmetry has also been reported. The black dioxide PaO2 is obtained from the pentoxide by reducing it at 1550 °C with hydrogen. It is not readily soluble in either dilute or concentrated nitric, hydrochloric or sulfuric acids, but easily dissolves in hydrofluoric acid. The dioxide can be converted back to pentoxide by heating in oxygen-containing atmosphere to 1100 °C. The monoxide PaO has only been observed as a thin coating on protactinium metal, but not in an isolated bulk form.
Protactinium forms mixed binary oxides with various metals. With alkali metals "A", the crystals have a chemical formula APaO3 and perovskite structure, or A3PaO4 and distorted rock-salt structure, or A7PaO6 where oxygen atoms for a hexagonal close-packed lattice. In all these materials, protactinium ions are octahedrally coordinated. The pentoxide Pa2O5 combines with rare-earth metal oxides R2O3 to form various nonstoichiometric mixed-oxides, also of perovskite structure.
Protactinium oxides are basic; they easily convert to hydroxides and can form various salts, such as sulfates, phosphates, nitrates, etc. The nitrate is usually white but can be brown due to radiolytic decomposition. Heating the nitrate in air at 400 °C converts it to the white protactinium pentoxide. The polytrioxophosphate Pa(PO3)4 can be produced by reacting difluoride sulfate PaF2SO4 with phosphoric acid (H3PO4) under inert gas atmosphere. Heating the product to about 900 °C eliminates the reaction by-products such as hydrofluoric acid, sulfur trioxide and phosphoric anhydride. Heating to higher temperatures in an inert atmosphere decomposes Pa(PO3)4 into the diphosphate PaP2O7, which is analogous to diphosphates of other actinides. In the diphosphate, the PO3 groups form pyramids of C2v symmetry. Heating PaP2O7 in air to 1400 °C decomposes it into the pentoxides of phosphorus and protactinium.
Halides.
Protactinium(V) fluoride forms white crystals where protactinium ions are arranged in pentagonal bipyramids and coordinated by 7 other ions. The coordination is the same in protactinium(V) chloride, but the color is yellow. The coordination changes to octahedral in the brown protactinium(V) bromide and is unknown for protactinium(V) iodide. The protactinium coordination in all its tetrahalides is 8, but the arrangement is square antiprismatic in protactinium(IV) fluoride and dodecahedral in the chloride and bromide. Brown-colored protactinium(III) iodide has been reported where protactinium ions are 8-coordinated in a bicapped trigonal prismatic arrangement.
Protactinium(V) fluoride and protactinium(V) chloride have a polymeric structure of monoclinic symmetry. There, within one polymeric chain, all the halide atoms lie in one graphite-like plane and form planar pentagons around the protactinium ions. The coordination 7 of protactinium originates from the 5 halide atoms and two bonds to protactinium atoms belonging to the nearby chains. These compounds easily hydrolyze in water. The pentachloride melts at 300 °C and sublimates at even lower temperatures.
Protactinium(V) fluoride can be prepared by reacting protactinium oxide with either bromine pentafluoride or bromine trifluoride at about 600 °C, and protactinium(IV) fluoride is obtained from the oxide and a mixture of hydrogen and hydrogen fluoride at 600 °C; a large excess of hydrogen is required to remove atmospheric oxygen leaks into the reaction.
Protactinium(V) chloride is prepared by reacting protactinium oxide with carbon tetrachloride at temperature of 200–300 °C. The by-products (such as PaOCl3) are removed by fractional sublimation. Reduction of protactinium(V) chloride with hydrogen at about 800 °C yields protactinium(IV) chloride – a yellow-green solid which sublimes in vacuum at 400 °C; it can also be obtained directly from protactinium dioxide by treating it with carbon tetrachloride at 400 °C.
Protactinium bromides are produced by the action of aluminium bromide, hydrogen bromide, carbon tetrabromide or a mixture of hydrogen bromide and thionyl bromide on protactinium oxide. An alternative reaction is between protactinium pentachloride and hydrogen bromide or thionyl bromide. Protactinium(V) bromide has two similar monoclinic forms, one is obtained by sublimation at 400–410 °C and another by sublimation at slightly lower temperature of 390–400 °C.
Protactinium iodides result from the oxides and aluminium iodide or ammonium iodide heated to 600 °C. Protactinium(III) iodide was obtained by heating protactinium(V) iodide in vacuum. As with oxides, protactinium forms mixed halides with alkali metals. Among those, most remarkable is Na3PaF8 where protactinium ion is symmetrically surrounded by 8 F− ions which form a nearly perfect cube.
More complex protactinium fluorides are also known such as Pa2F9 and ternary fluorides of the types MPaF6 (M = Li, Na, K, Rb, Cs or NH4), M2PaF7 (M = K, Rb, Cs or NH4) and M3PaF8 (M = Li, Na, Rb, Cs), all being white crystalline solids. The MPaF6 formula can be represented as a combination of MF and PaF5. These compounds can be obtained by evaporating a hydrofluoric acid solution containing these both complexes. For the small alkali cations like Na, the crystal structure is tetragonal, whereas it lowers to orthorphombic for larger cations K+, Rb+, Cs+ or NH4+. A similar variation was observed for the M2PaF7 fluorides, namely the crystal symmetry was dependent on the cation and differed for Cs2PaF7 and M2PaF7 (M = K, Rb or NH4).
Other inorganic compounds.
Oxyhalides and oxysulfides of protactinium are known. PaOBr3 has a monoclinic structure composed of double-chain units where protactinium has coordination 7 and is arranged into pentagonal bipyramids. The chains are interconnected through oxygen and bromine atoms, and each oxygen atom is related to three protactinium atoms. PaOS is a light-yellow non-volatile solid with a cubic crystal lattice isostructural to that of other actinide oxysulfides. It is obtained by reacting protactinium(V) chloride with a mixture of hydrogen sulfide and carbon disulfide at 900 °C.
In hydrides and nitrides, protactinium has a low oxidation state of about +3. The hydride is obtained by direct action of hydrogen on the metal at 250 °C, and the nitride is a product of ammonia and protactinium tetrachloride or pentachloride. This bright yellow solid is stable to heating to 800 °C in vacuum. Protactinium carbide PaC is formed by reduction of protactinium tetrafluoride with barium in a carbon crucible at a temperature of about 1400 °C. Protactinium forms borohydrides which include Pa(BH4)4. It has an unusual polymeric structure with helical chains where the protactinium atom has coordination number of 12 and is surrounded by six BH4− ions.
Organometallic compounds.
Protactinium(IV) forms a tetrahedral complex tetrakis(cyclopentadienyl)protactinium(IV) (or Pa(C5H5)4) with four cyclopentadienyl rings, which can be synthesized by reacting protactinium(IV) chloride with molten Be(C5H5)2. One ring can be substituted with a halide atom. Another organometallic complex is golden-yellow bis(π-cyclooctatetraene) protactinium, or protactinocene, Pa(C8H8)2, which is analogous in structure to uranocene. There, the metal atom is sandwiched between two cyclooctatetraene ligands. Similar to uranocene, it can be prepared by reacting protactinium tetrachloride with dipotassium cyclooctatetraenide, K2C8H8, in tetrahydrofuran.
Applications.
Although protactinium is located in the periodic table between uranium and thorium, which both have numerous applications, owing to its scarcity, high radioactivity and high toxicity, there are currently no uses for protactinium outside of scientific research.
Protactinium-231 arises from the decay of uranium-235 formed in nuclear reactors, and by the reaction 232Th + n → 231Th + 2n and subsequent beta decay. It was once thought to be able to support a nuclear chain reaction, which could in principle be used to build nuclear weapons: the physicist once estimated the associated critical mass as . However, the possibility of criticality of 231Pa has been ruled out since then.
With the advent of highly sensitive mass spectrometers, an application of 231Pa as a tracer in geology and paleoceanography has become possible. So, the ratio of protactinium-231 to thorium-230 is used for radiometric dating of sediments which are up to 175,000 years old and in modeling of the formation of minerals. In particular, its evaluation in oceanic sediments allowed to reconstruct the movements of North Atlantic water bodies during the last melting of Ice Age glaciers. Some of the protactinium-related dating variations rely on the analysis of the relative concentrations for several long-living members of the uranium decay chain – uranium, thorium and protactinium, for example. These elements have 6, 5 and 4 f-electrons in the outer shell and thus favor +6, +5 and +4 oxidation states, respectively, and show different physical and chemical properties. So, thorium and protactinium, but not uranium compounds are poorly soluble in aqueous solutions, and precipitate into sediments; the precipitation rate is faster for thorium than for protactinium. Besides, the concentration analysis for both protactinium-231 (half-life 32,760 years) and thorium-230 (half-life 75,380 years) allows to improve the accuracy compared to when only one isotope is measured; this double-isotope method is also weakly sensitive to inhomogeneities in the spatial distribution of the isotopes and to variations in their precipitation rate.
Precautions.
Protactinium is both toxic and highly radioactive and thus all manipulations with it are performed in a sealed glove box. Its major isotope 231Pa has a specific activity of per gram and primarily emits alpha-particles with an energy of 5 MeV, which can be stopped by a thin layer of any material. However, it slowly decays, with a half-life of 32,760 years, into 227Ac, which has a specific activity of per gram, emits both alpha and beta radiation, and has a much shorter half-life of 22 years. 227Ac, in turn, decays into lighter isotopes with even shorter half-lives and much greater specific activities (SA), as summarized in the table below showing the decay chain of protactinium-231.
As protactinium is present in small amounts in most natural products and materials, it is ingested with food or water and inhaled with air. Only about 0.05% of ingested protactinium is absorbed into the blood and the remainder is excreted. From the blood, about 40% of the protactinium
deposits in the bones, about 15% goes to the liver, 2% to the kidneys, and the rest leaves the body. The biological half-life of protactinium is about 50 years in the bones, whereas in other organs the kinetics has a fast and slow component. So in the liver 70% of protactinium have a half-life of 10 days and 30% remain for 60 days. The corresponding values for kidneys are 20% (10 days) and 80% (60 days). In all these organs, protactinium promotes cancer via its radioactivity. The maximum safe dose of Pa in the human body is , which corresponds to 0.5 micrograms of 231Pa. This isotope is 2.5 times more toxic than hydrocyanic acid. The maximum allowed concentrations of 231Pa in the air in Germany is .

</doc>
<doc id="23324" url="https://en.wikipedia.org/wiki?curid=23324" title="Platinum">
Platinum

Platinum is a member of the platinum group of elements and group 10 of the periodic table of elements. It has six naturally occurring isotopes. It is one of the rarer elements in Earth's crust with an average abundance of approximately 5 μg/kg. It occurs in some nickel and copper ores along with some native deposits, mostly in South Africa, which accounts for 80% of the world production. Because of its scarcity in Earth's crust, only a few hundred tonnes are produced annually, and given its important uses, it is highly valuable and is a major precious metal commodity.
Platinum is one of the least reactive metals. It has remarkable resistance to corrosion, even at high temperatures, and is therefore considered a noble metal. Consequently, platinum is often found chemically uncombined as native platinum. Because it occurs naturally in the alluvial sands of various rivers, it was first used by pre-Columbian South American natives to produce artifacts. It was referenced in European writings as early as 16th century, but it was not until Antonio de Ulloa published a report on a new metal of Colombian origin in 1748 that it became investigated by scientists.
Platinum is used in catalytic converters, laboratory equipment, electrical contacts and electrodes, platinum resistance thermometers, dentistry equipment, and jewelry. Being a heavy metal, it leads to health issues upon exposure to its salts, but due to its corrosion resistance, it is not as toxic as some metals. Compounds containing platinum, such as cisplatin, oxaliplatin and carboplatin, are applied in chemotherapy against certain types of cancer.
Characteristics.
Physical.
Pure platinum is a lustrous, ductile, and malleable, silver-white metal. Platinum is more ductile than gold, silver or copper, thus being the most ductile of pure metals, but it is less malleable than gold. The metal has excellent resistance to corrosion, is stable at high temperatures and has stable electrical properties. Platinum reacts with oxygen slowly at very high temperatures. It reacts vigorously with fluorine at to form tetrafluoride. It is also attacked by chlorine, bromine, iodine, and sulfur. Platinum is insoluble in hydrochloric and nitric acid, but dissolves in hot "aqua regia" to form chloroplatinic acid, H2PtCl6.
Its physical characteristics and chemical stability make it useful for industrial applications. Its resistance to wear and tarnish is well suited to use in fine jewelry.
Chemical.
The most common oxidation states of platinum are +2 and +4. The +1 and +3 oxidation states are less common, and are often stabilized by metal bonding in bimetallic (or polymetallic) species. As is expected, tetracoordinate platinum(II) compounds tend to adopt 16-electron square planar geometries. Although elemental platinum is generally unreactive, it dissolves in hot "aqua regia" to give aqueous chloroplatinic acid (H2PtCl6):
As a soft acid, platinum has a great affinity for sulfur, such as on dimethyl sulfoxide (DMSO); numerous DMSO complexes have been reported and care should be taken in the choice of reaction solvent.
Isotopes.
Platinum has six naturally occurring isotopes: 190Pt, 192Pt, 194Pt, 195Pt, 196Pt, and 198Pt. The most abundant of these is 195Pt, comprising 33.83% of all platinum. It is the only stable isotope with a non-zero spin; with a spin of 1/2, 195Pt satellite peaks are often observed in 1H and 31P NMR spectroscopy (i.e., Pt-phosphine and Pt-alkyl complexes). 190Pt is the least abundant at only 0.01%. Of the naturally occurring isotopes, only 190Pt is unstable, though it decays with a half-life of 6.5 years, causing an activity of 15 Bq/kg of natural platinum. 198Pt can undergo alpha decay, but its decay has never been observed (the half-life is known to be longer than 3.2 years); therefore, it is considered stable. Platinum also has 31 synthetic isotopes ranging in atomic mass from 166 to 202, making the total number of known isotopes 37. The least stable of these is 166Pt, with a half-life of 300 µs, whereas the most stable is 193Pt with a half-life of 50 years. Most platinum isotopes decay by some combination of beta decay and alpha decay. 188Pt, 191Pt, and 193Pt decay primarily by electron capture. 190Pt and 198Pt have double beta decay paths.
Occurrence.
Platinum is an extremely rare metal, occurring at a concentration of only 0.005 ppm in Earth's crust. It is sometimes mistaken for silver (Ag). Platinum is often found chemically uncombined as native platinum and as alloy with the other platinum-group metals and iron mostly. Most often the native platinum is found in secondary deposits in alluvial deposits. The alluvial deposits used by pre-Columbian people in the Chocó Department, Colombia are still a source for platinum-group metals. Another large alluvial deposit is in the Ural Mountains, Russia, and it is still mined.
In nickel and copper deposits, platinum-group metals occur as sulfides (e.g. (Pt,Pd)S), tellurides (e.g. PtBiTe), antimonides (PdSb), and arsenides (e.g. PtAs2), and as end alloys with nickel or copper. Platinum arsenide, sperrylite (PtAs2), is a major source of platinum associated with nickel ores in the Sudbury Basin deposit in Ontario, Canada. At Platinum, Alaska, about had been mined between 1927 and 1975. The mine ceased operations in 1990. The rare sulfide mineral cooperite, (Pt,Pd,Ni)S, contains platinum along with palladium and nickel. Cooperite occurs in the Merensky Reef within the Bushveld complex, Gauteng, South Africa.
In 1865, chromites were identified in the Bushveld region of South Africa, followed by the discovery of platinum in 1906. The largest known primary reserves are in the Bushveld complex in South Africa. The large copper–nickel deposits near Norilsk in Russia, and the Sudbury Basin, Canada, are the two other large deposits. In the Sudbury Basin, the huge quantities of nickel ore processed make up for the fact platinum is present as only 0.5 ppm in the ore. Smaller reserves can be found in the United States, for example in the Absaroka Range in Montana. In 2010, South Africa was the top producer of platinum, with an almost 77% share, followed by Russia at 13%; world production in 2010 was .
Platinum deposits are present in the state of Tamil Nadu, India. and a MOU has been signed between Geological Survey of India with TAMIN – Tamil Nadu Minerals Ltd.
Platinum exists in higher abundances on the Moon and in meteorites. Correspondingly, platinum is found in slightly higher abundances at sites of bolide impact on Earth that are associated with resulting post-impact volcanism, and can be mined economically; the Sudbury Basin is one such example.
Compounds.
Halides.
Hexachloroplatinic acid mentioned above is probably the most important platinum compound, as it serves as the precursor for many other platinum compounds. By itself, it has various applications in photography, zinc etchings, indelible ink, plating, mirrors, porcelain coloring, and as a catalyst.
Treatment of hexachloroplatinic acid with an ammonium salt, such as ammonium chloride, gives ammonium hexachloroplatinate, which is relatively insoluble in ammonium solutions. Heating this ammonium salt in the presence of hydrogen reduces it to elemental platinum. Potassium hexachloroplatinate is similarly insoluble, and hexachloroplatinic acid has been used in the determination of potassium ions by gravimetry.
When hexachloroplatinic acid is heated, it decomposes through platinum(IV) chloride and platinum(II) chloride to elemental platinum, although the reactions do not occur stepwise:
All three reactions are reversible. Platinum(II) and platinum(IV) bromides are known as well. Platinum hexafluoride is a strong oxidizer capable of oxidizing oxygen.
Oxides.
Platinum(IV) oxide, PtO2, also known as Adams' catalyst, is a black powder that is soluble in KOH solutions and concentrated acids. PtO2 and the less common PtO both decompose upon heating. Platinum(II,IV) oxide, Pt3O4, is formed in the following reaction:
Other compounds.
Unlike palladium acetate, platinum(II) acetate is not commercially available. Where a base is desired, the halides have been used in conjunction with sodium acetate. The use of platinum(II) acetylacetonate has also been reported.
Several barium platinides have been synthesized in which platinum exhibits negative oxidation states ranging from −1 to −2. These include BaPt, , and . Caesium platinide, , a dark-red transparent crystalline compound has been shown to contain Pt anions. Platinum also exhibits negative oxidation states at surfaces reduced electrochemically. The negative oxidation states exhibited by platinum are unusual for metallic elements, and they are attributed to the relativistic stabilization of the 6s orbitals.
Zeise's salt, containing an ethylene ligand, was one of the first organometallic compounds discovered. Dichloro(cycloocta-1,5-diene)platinum(II) is a commercially available olefin complex, which contains easily displaceable cod ligands ("cod" being an abbreviation of 1,5-cyclooctadiene). The cod complex and the halides are convenient starting points to platinum chemistry.
Cisplatin, or "cis"-diamminedichloroplatinum(II) is the first of a series of square planar platinum(II)-containing chemotherapy drugs, including carboplatin and oxaliplatin. These compounds are capable of crosslinking DNA, and kill cells by similar pathways to alkylating chemotherapeutic agents.
History.
Early uses.
Archaeologists have discovered traces of platinum in the gold used in ancient Egyptian tombs and hieroglyphics as early as 1200 BC. However, the extent of early Egyptians' knowledge of the metal is unclear. It is quite possible they did not recognize there was platinum in their gold.
The metal was used by pre-Columbian Americans near modern-day Esmeraldas, Ecuador to produce artifacts of a white gold-platinum alloy. They employed a relatively sophisticated system of powder metallurgy. The platinum used in such objects was not the pure element, but rather a naturally occurring mixture of the platinum group metals, with small amounts of palladium, rhodium, and iridium.
European discovery.
The first European reference to platinum appears in 1557 in the writings of the Italian humanist Julius Caesar Scaliger as a description of an unknown noble metal found between Darién and Mexico, "which no fire nor any Spanish artifice has yet been able to liquefy". From their first encounters with platinum, the Spanish generally saw the metal as a kind of impurity in gold, and it was treated as such. It was often simply thrown away, and there was an official decree forbidding the adulteration of gold with platinum impurities.
In 1741, Charles Wood, a British metallurgist, found various samples of Colombian platinum in Jamaica, which he sent to William Brownrigg for further investigation. Antonio de Ulloa, also credited with the discovery of platinum, returned to Spain from the French Geodesic Mission in 1746 after having been there for eight years. His historical account of the expedition included a description of platinum as being neither separable nor calcinable. Ulloa also anticipated the discovery of platinum mines. After publishing the report in 1748, Ulloa did not continue to investigate the new metal. In 1758, he was sent to superintend mercury mining operations in Huancavelica.
In 1750, after studying the platinum sent to him by Wood, Brownrigg presented a detailed account of the metal to the Royal Society, stating that he had seen no mention of it in any previous accounts of known minerals. Brownrigg also made note of platinum's extremely high melting point and refractoriness toward borax. Other chemists across Europe soon began studying platinum, including Andreas Sigismund Marggraf, Torbern Bergman, Jöns Jakob Berzelius, William Lewis, and Pierre Macquer. In 1752, Henrik Scheffer published a detailed scientific description of the metal, which he referred to as "white gold", including an account of how he succeeded in fusing platinum ore with the aid of arsenic. Scheffer described platinum as being less pliable than gold, but with similar resistance to corrosion.
Means of malleability.
Carl von Sickingen researched platinum extensively in 1772. He succeeded in making malleable platinum by alloying it with gold, dissolving the alloy in hot "aqua regia", precipitating the platinum with ammonium chloride, igniting the ammonium chloroplatinate, and hammering the resulting finely divided platinum to make it cohere. Franz Karl Achard made the first platinum crucible in 1784. He worked with the platinum by fusing it with arsenic, then later volatilizing the arsenic.
Because the other platinum-family members were not discovered yet (platinum was the first in the list), Scheffer and Sickingen made the false assumption that due to its hardness—which is slightly more than for pure iron—platinum would be a relatively non-pliable material, even brittle at times, when in fact its ductility and malleability are close to that of gold. Their assumptions could not be avoided because the platinum they experimented with was highly contaminated with minute amounts of platinum-family elements such as osmium and iridium, amongst others, which embrittled the platinum alloy. Alloying this impure platinum residue called "plyoxen" with gold was the only solution at the time to obtain a pliable compound, but nowadays, very pure platinum is available and extremely long wires can be drawn from pure platinum, very easily, due to its crystalline structure, which is similar to that of many soft metals.
In 1786, Charles III of Spain provided a library and laboratory to Pierre-François Chabaneau to aid in his research of platinum. Chabaneau succeeded in removing various impurities from the ore, including gold, mercury, lead, copper, and iron. This led him to believe he was working with a single metal, but in truth the ore still contained the yet-undiscovered platinum-group metals. This led to inconsistent results in his experiments. At times, the platinum seemed malleable, but when it was alloyed with iridium, it would be much more brittle. Sometimes the metal was entirely incombustible, but when alloyed with osmium, it would volatilize. After several months, Chabaneau succeeded in producing 23 kilograms of pure, malleable platinum by hammering and compressing the sponge form while white-hot. Chabeneau realized the infusibility of platinum would lend value to objects made of it, and so started a business with Joaquín Cabezas producing platinum ingots and utensils. This started what is known as the "platinum age" in Spain.
In 2007, Gerhard Ertl won the Nobel Prize in Chemistry for determining the detailed molecular mechanisms of the catalytic oxidation of carbon monoxide over platinum (catalytic converter).
Production.
Platinum, along with the rest of the platinum-group metals, is obtained commercially as a by-product from nickel and copper mining and processing. During electrorefining of copper, noble metals such as silver, gold and the platinum-group metals as well as selenium and tellurium settle to the bottom of the cell as "anode mud", which forms the starting point for the extraction of the platinum-group metals.
If pure platinum is found in placer deposits or other ores, it is isolated from them by various methods of subtracting impurities. Because platinum is significantly denser than many of its impurities, the lighter impurities can be removed by simply floating them away in a liquid. Platinum is paramagnetic, whereas nickel and iron are both ferromagnetic. These two impurities are thus removed by running an electromagnet over the mixture. Because platinum has a higher melting point than most other substances, many impurities can be burned or melted away without melting the platinum. Finally, platinum is resistant to hydrochloric and sulfuric acids, whereas other substances are readily attacked by them. Metal impurities can be removed by stirring the mixture in either of the two acids and recovering the remaining platinum.
One suitable method for purification for the raw platinum, which contains platinum, gold, and the other platinum-group metals, is to process it with "aqua regia", in which palladium, gold and platinum are dissolved, whereas osmium, iridium, ruthenium and rhodium stay unreacted. The gold is precipitated by the addition of iron(II) chloride and after filtering off the gold, the platinum is precipitated as ammonium chloroplatinate by the addition of ammonium chloride. Ammonium chloroplatinate can be converted to platinum by heating. Unprecipitated hexachloroplatinate(IV) may be reduced with elemental zinc, and a similar method is suitable for small scale recovery of platinum from laboratory residues.
Applications.
Of the 224 tonnes of platinum sold in 2013, 101 tonnes were used for vehicle emissions control devices (45%), 66.3 tonnes for jewelry (30%), 19.4 tonnes for chemical production and petroleum refining (8.7%), and 5.88 tonnes for electrical applications such as hard disk drives (2.6%). The remaining 31.4 tonnes went to various other minor applications, such as medicine and biomedicine, glassmaking equipment, investment, electrodes, anticancer drugs, oxygen sensors, spark plugs and turbine engines.
Catalyst.
The most common use of platinum is as a catalyst in chemical reactions, often as platinum black. It has been employed as a catalyst since the early 19th century, when platinum powder was used to catalyze the ignition of hydrogen. Its most important application is in automobiles as a catalytic converter, which allows the complete combustion of low concentrations of unburned hydrocarbons from the exhaust into carbon dioxide and water vapor. Platinum is also used in the petroleum industry as a catalyst in a number of separate processes, but especially in catalytic reforming of straight-run naphthas into higher-octane gasoline that becomes rich in aromatic compounds. PtO2, also known as Adams' catalyst, is used as a hydrogenation catalyst, specifically for vegetable oils. Platinum also strongly catalyzes the decomposition of hydrogen peroxide into water and oxygen and it is used in fuel cells as a catalyst for the reduction of oxygen.
Standard.
From 1889 to 1960, the meter was defined as the length of a platinum-iridium (90:10) alloy bar, known as the International Prototype Meter bar. The previous bar was made of platinum in 1799. The International Prototype Kilogram remains defined by a cylinder of the same platinum-iridium alloy made in 1879.
The standard hydrogen electrode also uses a platinized platinum electrode due to its corrosion resistance, and other attributes.
As an investment.
Platinum is a precious metal commodity; its bullion has the ISO currency code of XPT. Coins, bars, and ingots are traded or collected. Platinum finds use in jewellery, usually as a 90–95% alloy, due to its inertness. It is used for this purpose for its prestige and inherent bullion value. Jewellery trade publications advise jewellers to present minute surface scratches (which they term patina) as a desirable feature in attempt to enhance value of platinum products.
In watchmaking, Vacheron Constantin, Patek Philippe, Rolex, Breitling, and other companies use platinum for producing their limited edition watch series. Watchmakers appreciate the unique properties of platinum, as it neither tarnishes nor wears out (the latter quality relative to gold).
The price of platinum, like other industrial commodities, is more volatile than that of gold. In 2008, the price of platinum dropped from $2,252 to $774 per oz, a loss of nearly 2/3 of its value. By contrast, the price of gold dropped from ~$1,000 to ~$700/oz during the same time frame, a loss of only 1/3 of its value.
During periods of sustained economic stability and growth, the price of platinum tends to be as much as twice the price of gold, whereas during periods of economic uncertainty, the price of platinum tends to decrease due to reduced industrial demand, falling below the price of gold. Gold prices are more stable in slow economic times, as gold is considered a safe haven. Although gold is used in industrial applications, its demand is not so driven by industrial uses. In the 18th century, platinum's rarity made King Louis XV of France declare it the only metal fit for a king.
Other uses.
In the laboratory, platinum wire is used for electrodes; platinum pans and supports are used in thermogravimetric analysis because of the stringent requirements of chemical inertness upon heating to high temperatures (~1000 °C). Platinum is used as an alloying agent for various metal products, including fine wires, noncorrosive laboratory containers, medical instruments, dental prostheses, electrical contacts, and thermocouples. Platinum-cobalt, an alloy of roughly three parts platinum and one part cobalt, is used to make relatively strong permanent magnets. Platinum-based anodes are used in ships, pipelines, and steel piers.
Symbol of prestige.
Platinum's rarity as a metal has caused advertisers to associate it with exclusivity and wealth. "Platinum" debit and credit cards have greater privileges than "gold" cards. "Platinum awards" are the second highest possible, ranking above "gold", "silver" and "bronze", but below diamond. For example, in the United States, a musical album that has sold more than 1 million copies, will be credited as "platinum", whereas an album that sold more than 10 million copies will be certified as "diamond". Some products, such as blenders and vehicles, with a silvery-white color are identified as "platinum". Platinum is considered a precious metal, although its use is not as common as the use of gold or silver. The frame of the Crown of Queen Elizabeth The Queen Mother, manufactured for her coronation as Consort of King George VI, is made of platinum. It was the first British crown to be made of this particular metal.
Health issues.
According to the Centers for Disease Control and Prevention, short-term exposure to platinum salts may cause irritation of the eyes, nose, and throat, and long-term exposure may cause both respiratory and skin allergies. The current OSHA standard is 2 micrograms per cubic meter of air averaged over an 8-hour work shift. The National Institute for Occupational Safety and Health has set a recommended exposure limit (REL) for platinum as 1 mg/m3 over an 8-hour workday.
Platinum-based antineoplastic agents are used in chemotherapy, and show good activity against some tumors.
As platinum is a catalyst in the manufacture of the silicone rubber and gel components of several types of medical implants (breast implants, joint replacement prosthetics, artificial lumbar discs, vascular access ports, etc.), the possibility that platinum could enter the body and cause adverse effects has merited study. The Food and Drug Administration and other institutions have reviewed the issue and found no evidence to suggest toxicity in vivo.

</doc>
