<doc id="20423" url="https://en.wikipedia.org/wiki?curid=20423" title="Malaria">
Malaria

Malaria is a mosquito-borne infectious disease of humans and other animals caused by parasitic protozoans (a group of single-celled microorganisms) belonging to the "Plasmodium" type. Malaria causes symptoms that typically include fever, fatigue, vomiting, and headaches. --> In severe cases it can cause yellow skin, seizures, coma, or death. Symptoms usually begin ten to fifteen days after being bitten. --> If not properly treated, people may have recurrences of the disease months later. In those who have recently survived an infection, reinfection usually causes milder symptoms. --> This partial resistance disappears over months to years if the person has no continuing exposure to malaria.
The disease is most commonly transmitted by an infected female "Anopheles" mosquito. --> The mosquito bite introduces the parasites from the mosquito's saliva into a person's blood. The parasites travel to the liver where they mature and reproduce. --> Five species of "Plasmodium" can infect and be spread by humans. Most deaths are caused by "P. falciparum" because "P. vivax", "P. ovale", and "P. malariae" generally cause a milder form of malaria. The species "P. knowlesi" rarely causes disease in humans. Malaria is typically diagnosed by the microscopic examination of blood using blood films, or with antigen-based rapid diagnostic tests. Methods that use the polymerase chain reaction to detect the parasite's DNA have been developed, but are not widely used in areas where malaria is common due to their cost and complexity.
The risk of disease can be reduced by preventing mosquito bites by using mosquito nets and insect repellents, or with mosquito-control measures such as spraying insecticides and draining standing water. Several medications are available to prevent malaria in travellers to areas where the disease is common. --> Occasional doses of the medication sulfadoxine/pyrimethamine are recommended in infants and after the first trimester of pregnancy in areas with high rates of malaria. --> Despite a need, no effective vaccine exists, although efforts to develop one are ongoing. The recommended treatment for malaria is a combination of antimalarial medications that includes an artemisinin. The second medication may be either mefloquine, lumefantrine, or sulfadoxine/pyrimethamine. Quinine along with doxycycline may be used if an artemisinin is not available. It is recommended that in areas where the disease is common, malaria is confirmed if possible before treatment is started due to concerns of increasing drug resistance. --> Resistance among the parasites has developed to several antimalarial medications; for example, chloroquine-resistant "P. falciparum" has spread to most malarial areas, and resistance to artemisinin has become a problem in some parts of Southeast Asia.
The disease is widespread in the tropical and subtropical regions that exist in a broad band around the equator. This includes much of Sub-Saharan Africa, Asia, and Latin America. --> In 2015, there were 214 million cases of malaria worldwide. This resulted in an estimated 438,000 deaths, 90% of which occurred in Africa. Rates of disease have decreased from 2000 to 2015 by 37%, but increased from 2014 during which there were 198 million cases. Malaria is commonly associated with poverty and has a major negative effect on economic development. In Africa, it is estimated to result in losses of US$12 billion a year due to increased healthcare costs, lost ability to work, and negative effects on tourism.
Signs and symptoms.
The signs and symptoms of malaria typically begin 8–25 days following infection; however, symptoms may occur later in those who have taken antimalarial medications as prevention. Initial manifestations of the disease—common to all malaria species—are similar to flu-like symptoms, and can resemble other conditions such as sepsis, gastroenteritis, and viral diseases. The presentation may include headache, fever, shivering, joint pain, vomiting, hemolytic anemia, jaundice, hemoglobin in the urine, retinal damage, and convulsions.
The classic symptom of malaria is paroxysm—a cyclical occurrence of sudden coldness followed by shivering and then fever and sweating, occurring every two days (tertian fever) in "P. vivax" and "P. ovale" infections, and every three days (quartan fever) for "P. malariae". "P. falciparum" infection can cause recurrent fever every 36–48 hours, or a less pronounced and almost continuous fever.
Severe malaria is usually caused by "P. falciparum" (often referred to as falciparum malaria). Symptoms of falciparum malaria arise 9–30 days after infection. Individuals with cerebral malaria frequently exhibit neurological symptoms, including abnormal posturing, nystagmus, conjugate gaze palsy (failure of the eyes to turn together in the same direction), opisthotonus, seizures, or coma.
Complications.
Malaria has several serious complications. Among these is the development of respiratory distress, which occurs in up to 25% of adults and 40% of children with severe "P. falciparum" malaria. Possible causes include respiratory compensation of metabolic acidosis, noncardiogenic pulmonary oedema, concomitant pneumonia, and severe anaemia. Although rare in young children with severe malaria, acute respiratory distress syndrome occurs in 5–25% of adults and up to 29% of pregnant women. Coinfection of HIV with malaria increases mortality. Renal failure is a feature of blackwater fever, where hemoglobin from lysed red blood cells leaks into the urine.
Infection with "P. falciparum" may result in cerebral malaria, a form of severe malaria that involves encephalopathy. It is associated with retinal whitening, which may be a useful clinical sign in distinguishing malaria from other causes of fever. Enlarged spleen, enlarged liver or both of these, severe headache, low blood sugar, and hemoglobin in the urine with renal failure may occur. Complications may include spontaneous bleeding, coagulopathy, and shock.
Malaria in pregnant women is an important cause of stillbirths, infant mortality, abortion and low birth weight, particularly in "P. falciparum" infection, but also with "P. vivax".
Cause.
Malaria parasites belong to the genus "Plasmodium" (phylum Apicomplexa). In humans, malaria is caused by "P. falciparum", "P. malariae", "P. ovale", "P. vivax" and "P. knowlesi". Among those infected, "P. falciparum" is the most common species identified (~75%) followed by "P. vivax" (~20%). Although "P. falciparum" traditionally accounts for the majority of deaths, recent evidence suggests that "P. vivax" malaria is associated with potentially life-threatening conditions about as often as with a diagnosis of "P. falciparum" infection. "P. vivax " proportionally is more common outside Africa. There have been documented human infections with several species of "Plasmodium" from higher apes; however, except for "P. knowlesi"—a zoonotic species that causes malaria in macaques—these are mostly of limited public health importance.
Global warming is likely to affect malaria transmission, but the severity and geographic distribution of such effects is uncertain.
Life cycle.
In the life cycle of "Plasmodium", a female "Anopheles" mosquito (the definitive host) transmits a motile infective form (called the sporozoite) to a vertebrate host such as a human (the secondary host), thus acting as a transmission vector. A sporozoite travels through the blood vessels to liver cells (hepatocytes), where it reproduces asexually (tissue schizogony), producing thousands of merozoites. These infect new red blood cells and initiate a series of asexual multiplication cycles (blood schizogony) that produce 8 to 24 new infective merozoites, at which point the cells burst and the infective cycle begins anew.
Other merozoites develop into immature gametocytes, which are the precursors of male and female gametes. When a fertilised mosquito bites an infected person, gametocytes are taken up with the blood and mature in the mosquito gut. The male and female gametocytes fuse and form an ookinete—a fertilized, motile zygote. Ookinetes develop into new sporozoites that migrate to the insect's salivary glands, ready to infect a new vertebrate host. The sporozoites are injected into the skin, in the saliva, when the mosquito takes a subsequent blood meal.
Only female mosquitoes feed on blood; male mosquitoes feed on plant nectar, and do not transmit the disease. The females of the "Anopheles" genus of mosquito prefer to feed at night. They usually start searching for a meal at dusk, and will continue throughout the night until taking a meal. Malaria parasites can also be transmitted by blood transfusions, although this is rare.
Recurrent malaria.
Symptoms of malaria can recur after varying symptom-free periods. Depending upon the cause, recurrence can be classified as either recrudescence, relapse, or reinfection. Recrudescence is when symptoms return after a symptom-free period. It is caused by parasites surviving in the blood as a result of inadequate or ineffective treatment. Relapse is when symptoms reappear after the parasites have been eliminated from blood but persist as dormant hypnozoites in liver cells. Relapse commonly occurs between 8–24 weeks and is commonly seen with "P. vivax" and "P. ovale" infections. "P. vivax" malaria cases in temperate areas often involve overwintering by hypnozoites, with relapses beginning the year after the mosquito bite. Reinfection means the parasite that caused the past infection was eliminated from the body but a new parasite was introduced. Reinfection cannot readily be distinguished from recrudescence, although recurrence of infection within two weeks of treatment for the initial infection is typically attributed to treatment failure. People may develop some immunity when exposed to frequent infections.
Pathophysiology.
Malaria infection develops via two phases: one that involves the liver (exoerythrocytic phase), and one that involves red blood cells, or erythrocytes (erythrocytic phase). When an infected mosquito pierces a person's skin to take a blood meal, sporozoites in the mosquito's saliva enter the bloodstream and migrate to the liver where they infect hepatocytes, multiplying asexually and asymptomatically for a period of 8–30 days.
After a potential dormant period in the liver, these organisms differentiate to yield thousands of merozoites, which, following rupture of their host cells, escape into the blood and infect red blood cells to begin the erythrocytic stage of the life cycle. The parasite escapes from the liver undetected by wrapping itself in the cell membrane of the infected host liver cell.
Within the red blood cells, the parasites multiply further, again asexually, periodically breaking out of their host cells to invade fresh red blood cells. Several such amplification cycles occur. Thus, classical descriptions of waves of fever arise from simultaneous waves of merozoites escaping and infecting red blood cells.
Some "P. vivax" sporozoites do not immediately develop into exoerythrocytic-phase merozoites, but instead produce hypnozoites that remain dormant for periods ranging from several months (7–10 months is typical) to several years. After a period of dormancy, they reactivate and produce merozoites. Hypnozoites are responsible for long incubation and late relapses in "P. vivax" infections, although their existence in "P. ovale" is uncertain.
The parasite is relatively protected from attack by the body's immune system because for most of its human life cycle it resides within the liver and blood cells and is relatively invisible to immune surveillance. However, circulating infected blood cells are destroyed in the spleen. To avoid this fate, the "P. falciparum" parasite displays adhesive proteins on the surface of the infected blood cells, causing the blood cells to stick to the walls of small blood vessels, thereby sequestering the parasite from passage through the general circulation and the spleen. The blockage of the microvasculature causes symptoms such as in placental malaria. Sequestered red blood cells can breach the blood–brain barrier and cause cerebral malaria.
Genetic resistance.
According to a 2005 review, due to the high levels of mortality and morbidity caused by malaria—especially the "P. falciparum" species—it has placed the greatest selective pressure on the human genome in recent history. Several genetic factors provide some resistance to it including sickle cell trait, thalassaemia traits, glucose-6-phosphate dehydrogenase deficiency, and the absence of Duffy antigens on red blood cells.
The impact of sickle cell trait on malaria immunity illustrates some evolutionary trade-offs that have occurred because of endemic malaria. Sickle cell trait causes a change in the hemoglobin molecule in the blood. Normally, red blood cells have a very flexible, biconcave shape that allows them to move through narrow capillaries; however, when the modified hemoglobin S molecules are exposed to low amounts of oxygen, or crowd together due to dehydration, they can stick together forming strands that cause the cell to sickle or distort into a curved shape. In these strands the molecule is not as effective in taking or releasing oxygen, and the cell is not flexible enough to circulate freely. In the early stages of malaria, the parasite can cause infected red cells to sickle, and so they are removed from circulation sooner. This reduces the frequency with which malaria parasites complete their life cycle in the cell. Individuals who are homozygous (with two copies of the abnormal hemoglobin beta allele) have sickle-cell anaemia, while those who are heterozygous (with one abnormal allele and one normal allele) experience resistance to malaria without severe anemia. Although the shorter life expectancy for those with the homozygous condition would tend to disfavor the trait's survival, the trait is preserved in malaria-prone regions because of the benefits provided by the heterozygous form.
Liver dysfunction.
Liver dysfunction as a result of malaria is uncommon and usually only occurs in those with another liver condition such as viral hepatitis or chronic liver disease. The syndrome is sometimes called "malarial hepatitis". While it has been considered a rare occurrence, malarial hepatopathy has seen an increase, particularly in Southeast Asia and India. Liver compromise in people with malaria correlates with a greater likelihood of complications and death.
Diagnosis.
Owing to the non-specific nature of the presentation of symptoms, diagnosis of malaria in non-endemic areas requires a high degree of suspicion, which might be elicited by any of the following: recent travel history, enlarged spleen, fever, low number of platelets in the blood, and higher-than-normal levels of bilirubin in the blood combined with a normal level of white blood cells.
Malaria is usually confirmed by the microscopic examination of blood films or by antigen-based rapid diagnostic tests (RDT). Microscopy is the most commonly used method to detect the malarial parasite—about 165 million blood films were examined for malaria in 2010. Despite its widespread usage, diagnosis by microscopy suffers from two main drawbacks: many settings (especially rural) are not equipped to perform the test, and the accuracy of the results depends on both the skill of the person examining the blood film and the levels of the parasite in the blood. The sensitivity of blood films ranges from 75–90% in optimum conditions, to as low as 50%. Commercially available RDTs are often more accurate than blood films at predicting the presence of malaria parasites, but they are widely variable in diagnostic sensitivity and specificity depending on manufacturer, and are unable to tell how many parasites are present.
In regions where laboratory tests are readily available, malaria should be suspected, and tested for, in any unwell person who has been in an area where malaria is endemic. In areas that cannot afford laboratory diagnostic tests, it has become common to use only a history of fever as the indication to treat for malaria—thus the common teaching "fever equals malaria unless proven otherwise". A drawback of this practice is overdiagnosis of malaria and mismanagement of non-malarial fever, which wastes limited resources, erodes confidence in the health care system, and contributes to drug resistance. Although polymerase chain reaction-based tests have been developed, they are not widely used in areas where malaria is common as of 2012, due to their complexity.
Classification.
Malaria is classified into either "severe" or "uncomplicated" by the World Health Organization (WHO). It is deemed severe when "any" of the following criteria are present, otherwise it is considered uncomplicated.
Cerebral malaria is defined as a severe "P. falciparum"-malaria presenting with neurological symptoms, including coma (with a Glasgow coma scale less than 11, or a Blantyre coma scale greater than 3), or with a coma that lasts longer than 30 minutes after a seizure.
Various types of malaria have been called by the names below:
Prevention.
Methods used to prevent malaria include medications, mosquito elimination and the prevention of bites. There is no vaccine for malaria. The presence of malaria in an area requires a combination of high human population density, high anopheles mosquito population density and high rates of transmission from humans to mosquitoes and from mosquitoes to humans. If any of these is lowered sufficiently, the parasite will eventually disappear from that area, as happened in North America, Europe and parts of the Middle East. However, unless the parasite is eliminated from the whole world, it could become re-established if conditions revert to a combination that favours the parasite's reproduction. Furthermore, the cost per person of eliminating anopheles mosquitoes rises with decreasing population density, making it economically unfeasible in some areas.
Prevention of malaria may be more cost-effective than treatment of the disease in the long run, but the initial costs required are out of reach of many of the world's poorest people. There is a wide difference in the costs of control (i.e. maintenance of low endemicity) and elimination programs between countries. For example, in China—whose government in 2010 announced a strategy to pursue malaria elimination in the Chinese provinces—the required investment is a small proportion of public expenditure on health. In contrast, a similar program in Tanzania would cost an estimated one-fifth of the public health budget.
In areas where malaria is common, children under five years old often have anemia which is sometimes due to malaria. Giving children with anemia in these areas preventive antimalarial medication improves red blood cell levels slightly but did not affect the risk of death or need for hospitalization.
Mosquito control.
Vector control refers to methods used to decrease malaria by reducing the levels of transmission by mosquitoes. For individual protection, the most effective insect repellents are based on DEET or picaridin. Insecticide-treated mosquito nets (ITNs) and indoor residual spraying (IRS) have been shown to be highly effective in preventing malaria among children in areas where malaria is common. Prompt treatment of confirmed cases with artemisinin-based combination therapies (ACTs) may also reduce transmission.
Mosquito nets help keep mosquitoes away from people and reduce infection rates and transmission of malaria. Nets are not a perfect barrier and are often treated with an insecticide designed to kill the mosquito before it has time to find a way past the net. Insecticide-treated nets are estimated to be twice as effective as untreated nets and offer greater than 70% protection compared with no net. Between 2000 and 2008, the use of ITNs saved the lives of an estimated 250,000 infants in Sub-Saharan Africa. About 13% of households in Sub-Saharan countries owned ITNs in 2007 and 31% of African households were estimated to own at least one ITN in 2008. In 2000, 1.7 million (1.8%) African children living in areas of the world where malaria is common were protected by an ITN. That number increased to 20.3 million (18.5%) African children using ITNs in 2007, leaving 89.6 million children unprotected and to 68% African children using mosquito nets in 2015. Most nets are impregnated with pyrethroids, a class of insecticides with low toxicity. They are most effective when used from dusk to dawn. It is recommended to hang a large "bed net" above the center of a bed and either tuck the edges under the mattress or make sure it is large enough such that it touches the ground.
Indoor residual spraying is the spraying of insecticides on the walls inside a home. After feeding, many mosquitoes rest on a nearby surface while digesting the bloodmeal, so if the walls of houses have been coated with insecticides, the resting mosquitoes can be killed before they can bite another person and transfer the malaria parasite. As of 2006, the World Health Organization recommends 12 insecticides in IRS operations, including DDT and the pyrethroids cyfluthrin and deltamethrin. This public health use of small amounts of DDT is permitted under the Stockholm Convention, which prohibits its agricultural use. One problem with all forms of IRS is insecticide resistance. Mosquitoes affected by IRS tend to rest and live indoors, and due to the irritation caused by spraying, their descendants tend to rest and live outdoors, meaning that they are less affected by the IRS.
There are a number of other methods to reduce mosquito bites and slow the spread of malaria. Efforts to decrease mosquito larva by decreasing the availability of open water in which they develop or by adding substances to decrease their development is effective in some locations. Electronic mosquito repellent devices which make very high frequency sounds that are supposed to keep female mosquitoes away, do not have supporting evidence.
Other methods.
Community participation and health education strategies promoting awareness of malaria and the importance of control measures have been successfully used to reduce the incidence of malaria in some areas of the developing world. Recognizing the disease in the early stages can stop the disease from becoming fatal. Education can also inform people to cover over areas of stagnant, still water, such as water tanks that are ideal breeding grounds for the parasite and mosquito, thus cutting down the risk of the transmission between people. This is generally used in urban areas where there are large centers of population in a confined space and transmission would be most likely in these areas. Intermittent preventive therapy is another intervention that has been used successfully to control malaria in pregnant women and infants, and in preschool children where transmission is seasonal.
Medications.
There are a number of drugs that can help prevent or interrupt malaria in travelers to places where infection is common. Many of these drugs are also used in treatment. Chloroquine may be used where chloroquine-resistant parasites are not common. In places where "Plasmodium" is resistant to one or more medications, three medications—mefloquine ("Lariam"), doxycycline (available generically), or the combination of atovaquone and proguanil hydrochloride ("Malarone")—are frequently used when prophylaxis is needed. Doxycycline and the atovaquone plus proguanil combination are the best tolerated; mefloquine is associated with death, suicide, and neurological and psychiatric symptoms.
The protective effect does not begin immediately, and people visiting areas where malaria exists usually start taking the drugs one to two weeks before arriving and continue taking them for four weeks after leaving (except for atovaquone/proguanil, which only needs to be started two days before and continued for seven days afterward). The use of preventative drugs is often not practical for those who live in areas where malaria exists, and their use is usually only in pregnant women and short-term visitors. This is due to the cost of the drugs, side effects from long-term use, and the difficulty in obtaining anti-malarial drugs outside of wealthy nations. During pregnancy, medication to prevent malaria has been found to improve the weight of the baby at birth and decrease the risk of anemia in the mother. The use of preventative drugs where malaria-bearing mosquitoes are present may encourage the development of partial resistance.
Treatment.
Malaria is treated with antimalarial medications; the ones used depends on the type and severity of the disease. While medications against fever are commonly used, their effects on outcomes are not clear.
Simple or uncomplicated malaria may be treated with oral medications. The most effective treatment for "P. falciparum" infection is the use of artemisinins in combination with other antimalarials (known as artemisinin-combination therapy, or ACT), which decreases resistance to any single drug component. These additional antimalarials include: amodiaquine, lumefantrine, mefloquine or sulfadoxine/pyrimethamine. Another recommended combination is dihydroartemisinin and piperaquine. ACT is about 90% effective when used to treat uncomplicated malaria. To treat malaria during pregnancy, the WHO recommends the use of quinine plus clindamycin early in the pregnancy (1st trimester), and ACT in later stages (2nd and 3rd trimesters). In the 2000s (decade), malaria with partial resistance to artemisins emerged in Southeast Asia. Infection with "P. vivax", "P. ovale" or "P. malariae" usually do not require hospitalization. Treatment of "P. vivax" requires both treatment of blood stages (with chloroquine or ACT) and clearance of liver forms with primaquine.
Severe and complicated malaria are almost always caused by infection with "P. falciparum". The other species usually cause only febrile disease. Severe and complicated malaria are medical emergencies since mortality rates are high (10% to 50%). Cerebral malaria is the form of severe and complicated malaria with the worst neurological symptoms.
Recommended treatment for severe malaria is the intravenous use of antimalarial drugs. For severe malaria, parenteral artesunate was superior to quinine in both children and adults. In another systematic review, artemisinin derivatives (artemether and arteether) were as efficacious as quinine in the treatment of cerebral malaria in children. Treatment of severe malaria involves supportive measures that are best done in a critical care unit. This includes the management of high fevers and the seizures that may result from it. It also includes monitoring for poor breathing effort, low blood sugar, and low blood potassium.
Resistance.
Drug resistance poses a growing problem in 21st-century malaria treatment. Resistance is now common against all classes of antimalarial drugs apart from artemisinins. Treatment of resistant strains became increasingly dependent on this class of drugs. The cost of artemisinins limits their use in the developing world. Malaria strains found on the Cambodia–Thailand border are resistant to combination therapies that include artemisinins, and may therefore be untreatable. Exposure of the parasite population to artemisinin monotherapies in subtherapeutic doses for over 30 years and the availability of substandard artemisinins likely drove the selection of the resistant phenotype. Resistance to artemisinin has been detected in Cambodia, Myanmar, Thailand, and Vietnam, and there has been emerging resistance in Laos.
Prognosis.
[[File:Malaria world map - DALY - WHO2004.svg|thumb|upright=1.2|Disability-adjusted life year for malaria per 100,000 inhabitants in 2004
]]
When properly treated, people with malaria can usually expect a complete recovery. However, severe malaria can progress extremely rapidly and cause death within hours or days. In the most severe cases of the disease, fatality rates can reach 20%, even with intensive care and treatment. Over the longer term, developmental impairments have been documented in children who have suffered episodes of severe malaria. Chronic infection without severe disease can occur in an immune-deficiency syndrome associated with a decreased responsiveness to "Salmonella" bacteria and the Epstein–Barr virus.
During childhood, malaria causes anemia during a period of rapid brain development, and also direct brain damage resulting from cerebral malaria. Some survivors of cerebral malaria have an increased risk of neurological and cognitive deficits, behavioural disorders, and epilepsy. Malaria prophylaxis was shown to improve cognitive function and school performance in clinical trials when compared to placebo groups.
Epidemiology.
The WHO estimates that in 2010 there were 219 million cases of malaria resulting in 660,000 deaths. Others have estimated the number of cases at between 350 and 550 million for falciparum malaria and deaths in 2010 at 1.24 million up from 1.0 million deaths in 1990. The majority of cases (65%) occur in children under 15 years old. About 125 million pregnant women are at risk of infection each year; in Sub-Saharan Africa, maternal malaria is associated with up to 200,000 estimated infant deaths yearly. There are about 10,000 malaria cases per year in Western Europe, and 1300–1500 in the United States. About 900 people died from the disease in Europe between 1993 and 2003. Both the global incidence of disease and resulting mortality have declined in recent years. According to the WHO and UNICEF, deaths attributable to malaria in 2015 were reduced by 60% from a 2000 estimate of 985,000, largely due to the widespread use of insecticide-treated nets and artemisinin-based combination therapies. In 2012, there were 207 million cases of malaria. --> That year, the disease is estimated to have killed between 473,000 and 789,000 people, many of whom were children in Africa. Efforts at decreasing the disease in Africa since the turn of millennium have been partially effective, with rates of the disease dropping by an estimated forty percent on the continent.
Malaria is presently endemic in a broad band around the equator, in areas of the Americas, many parts of Asia, and much of Africa; in Sub-Saharan Africa, 85–90% of malaria fatalities occur. An estimate for 2009 reported that countries with the highest death rate per 100,000 of population were Ivory Coast (86.15), Angola (56.93) and Burkina Faso (50.66). A 2010 estimate indicated the deadliest countries per population were Burkina Faso, Mozambique and Mali. The Malaria Atlas Project aims to map global endemic levels of malaria, providing a means with which to determine the global spatial limits of the disease and to assess disease burden. This effort led to the publication of a map of "P. falciparum" endemicity in 2010. As of 2010, about 100 countries have endemic malaria. Every year, 125 million international travellers visit these countries, and more than 30,000 contract the disease.
The geographic distribution of malaria within large regions is complex, and malaria-afflicted and malaria-free areas are often found close to each other. Malaria is prevalent in tropical and subtropical regions because of rainfall, consistent high temperatures and high humidity, along with stagnant waters in which mosquito larvae readily mature, providing them with the environment they need for continuous breeding. In drier areas, outbreaks of malaria have been predicted with reasonable accuracy by mapping rainfall. Malaria is more common in rural areas than in cities. For example, several cities in the Greater Mekong Subregion of Southeast Asia are essentially malaria-free, but the disease is prevalent in many rural regions, including along international borders and forest fringes. In contrast, malaria in Africa is present in both rural and urban areas, though the risk is lower in the larger cities.
History.
Although the parasite responsible for "P. falciparum" malaria has been in existence for 50,000–100,000 years, the population size of the parasite did not increase until about 10,000 years ago, concurrently with advances in agriculture and the development of human settlements. Close relatives of the human malaria parasites remain common in chimpanzees. Some evidence suggests that the "P. falciparum" malaria may have originated in gorillas.
References to the unique periodic fevers of malaria are found throughout recorded history, beginning in 2700 BC in China. Hippocrates described periodic fevers, labelling them tertian, quartan, subtertian and quotidian. The Roman Columella associated the disease with insects from swamps. Malaria may have contributed to the decline of the Roman Empire, and was so pervasive in Rome that it was known as the "Roman fever". Several regions in ancient Rome were considered at-risk for the disease because of the favourable conditions present for malaria vectors. This included areas such as southern Italy, the island of Sardinia, the Pontine Marshes, the lower regions of coastal Etruria and the city of Rome along the Tiber River. The presence of stagnant water in these places was preferred by mosquitoes for breeding grounds. Irrigated gardens, swamp-like grounds, runoff from agriculture, and drainage problems from road construction led to the increase of standing water.
The term malaria originates from Medieval —"bad air"; the disease was formerly called "ague" or "marsh fever" due to its association with swamps and marshland. The term first appeared in the English literature about 1829. Malaria was once common in most of Europe and North America, where it is no longer endemic, though imported cases do occur.
Scientific studies on malaria made their first significant advance in 1880, when Charles Louis Alphonse Laveran—a French army doctor working in the military hospital of Constantine in Algeria—observed parasites inside the red blood cells of infected people for the first time. He therefore proposed that malaria is caused by this organism, the first time a protist was identified as causing disease. For this and later discoveries, he was awarded the 1907 Nobel Prize for Physiology or Medicine. A year later, Carlos Finlay, a Cuban doctor treating people with yellow fever in Havana, provided strong evidence that mosquitoes were transmitting disease to and from humans. This work followed earlier suggestions by Josiah C. Nott, and work by Sir Patrick Manson, the "father of tropical medicine", on the transmission of filariasis.
In April 1894, a Scottish physician Sir Ronald Ross visited Sir Patrick Manson at his house on Queen Anne Street, London. This visit was the start of four years of collaboration and fervent research that culminated in 1898 when Ross, who was working in the Presidency General Hospital in Calcutta, proved the complete life-cycle of the malaria parasite in mosquitoes. He thus proved that the mosquito was the vector for malaria in humans by showing that certain mosquito species transmit malaria to birds. He isolated malaria parasites from the salivary glands of mosquitoes that had fed on infected birds. For this work, Ross received the 1902 Nobel Prize in Medicine. After resigning from the Indian Medical Service, Ross worked at the newly established Liverpool School of Tropical Medicine and directed malaria-control efforts in Egypt, Panama, Greece and Mauritius. The findings of Finlay and Ross were later confirmed by a medical board headed by Walter Reed in 1900. Its recommendations were implemented by William C. Gorgas in the health measures undertaken during construction of the Panama Canal. This public-health work saved the lives of thousands of workers and helped develop the methods used in future public-health campaigns against the disease.
The first effective treatment for malaria came from the bark of cinchona tree, which contains quinine. This tree grows on the slopes of the Andes, mainly in Peru. The indigenous peoples of Peru made a tincture of cinchona to control fever. Its effectiveness against malaria was found and the Jesuits introduced the treatment to Europe around 1640; by 1677, it was included in the London Pharmacopoeia as an antimalarial treatment. It was not until 1820 that the active ingredient, quinine, was extracted from the bark, isolated and named by the French chemists Pierre Joseph Pelletier and Joseph Bienaimé Caventou.
Quinine became the predominant malarial medication until the 1920s, when other medications began to be developed. In the 1940s, chloroquine replaced quinine as the treatment of both uncomplicated and severe malaria until resistance supervened, first in Southeast Asia and South America in the 1950s and then globally in the 1980s.
The medicinal value of Artemisia annua has been used by Chinese herbalists in traditional Chinese medicines for 2,000 years. In 1596, Li Shizhen recommended tea made from qinghao specifically to treat malaria symptoms in his "Compendium of Materia Medica". Artemisinins, discovered by Chinese scientist Tu Youyou and colleagues in the 1970s from the plant "Artemisia annua", became the recommended treatment for "P. falciparum" malaria, administered in combination with other antimalarials as well as in severe disease. Tu says she was influenced by a traditional Chinese herbal medicine source, "The Handbook of Prescriptions for Emergency Treatments", written in 340 by Ge Hong For her work on malaria, Tu Youyou received the 2015 Nobel Prize in Physiology or Medicine
"Plasmodium vivax" was used between 1917 and the 1940s for malariotherapy—deliberate injection of malaria parasites to induce fever to combat certain diseases such as tertiary syphilis. In 1927, the inventor of this technique, Julius Wagner-Jauregg, received the Nobel Prize in Physiology or Medicine for his discoveries. The technique was dangerous, killing about 15% of patients, so it is no longer in use.
The first pesticide used for indoor residual spraying was DDT. Although it was initially used exclusively to combat malaria, its use quickly spread to agriculture. In time, pest control, rather than disease control, came to dominate DDT use, and this large-scale agricultural use led to the evolution of resistant mosquitoes in many regions. The DDT resistance shown by "Anopheles" mosquitoes can be compared to antibiotic resistance shown by bacteria. During the 1960s, awareness of the negative consequences of its indiscriminate use increased, ultimately leading to bans on agricultural applications of DDT in many countries in the 1970s. Before DDT, malaria was successfully eliminated or controlled in tropical areas like Brazil and Egypt by removing or poisoning the breeding grounds of the mosquitoes or the aquatic habitats of the larva stages, for example by applying the highly toxic arsenic compound Paris Green to places with standing water.
Malaria vaccines have been an elusive goal of research. The first promising studies demonstrating the potential for a malaria vaccine were performed in 1967 by immunizing mice with live, radiation-attenuated sporozoites, which provided significant protection to the mice upon subsequent injection with normal, viable sporozoites. Since the 1970s, there has been a considerable effort to develop similar vaccination strategies for humans.
Society and culture.
Economic impact.
Malaria is not just a disease commonly associated with poverty: some evidence suggests that it is also a cause of poverty and a major hindrance to economic development. Although tropical regions are most affected, malaria's furthest influence reaches into some temperate zones that have extreme seasonal changes. The disease has been associated with major negative economic effects on regions where it is widespread. During the late 19th and early 20th centuries, it was a major factor in the slow economic development of the American southern states.
A comparison of average per capita GDP in 1995, adjusted for parity of purchasing power, between countries with malaria and countries without malaria gives a fivefold difference ($1,526 USD versus $8,268 USD). In the period 1965 to 1990, countries where malaria was common had an average per capita GDP that increased only 0.4% per year, compared to 2.4% per year in other countries.
Poverty can increase the risk of malaria, since those in poverty do not have the financial capacities to prevent or treat the disease. In its entirety, the economic impact of malaria has been estimated to cost Africa US$12 billion every year. The economic impact includes costs of health care, working days lost due to sickness, days lost in education, decreased productivity due to brain damage from cerebral malaria, and loss of investment and tourism. The disease has a heavy burden in some countries, where it may be responsible for 30–50% of hospital admissions, up to 50% of outpatient visits, and up to 40% of public health spending.
Cerebral malaria is one of the leading causes of neurological disabilities in African children. Studies comparing cognitive functions before and after treatment for severe malarial illness continued to show significantly impaired school performance and cognitive abilities even after recovery. Consequently, severe and cerebral malaria have far-reaching socioeconomic consequences that extend beyond the immediate effects of the disease.
Counterfeit and substandard drugs.
Sophisticated counterfeits have been found in several Asian countries such as Cambodia, China, Indonesia, Laos, Thailand, and Vietnam, and are an important cause of avoidable death in those countries. The WHO said that studies indicate that up to 40% of artesunate-based malaria medications are counterfeit, especially in the Greater Mekong region and have established a rapid alert system to enable information about counterfeit drugs to be rapidly reported to the relevant authorities in participating countries. There is no reliable way for doctors or lay people to detect counterfeit drugs without help from a laboratory. Companies are attempting to combat the persistence of counterfeit drugs by using new technology to provide security from source to distribution.
Another clinical and public health concern is the proliferation of substandard antimalarial medicines resulting from inappropriate concentration of ingredients, contamination with other drugs or toxic impurities, poor quality ingredients, poor stability and inadequate packaging. A 2012 study demonstrated that roughly one-third of antimalarial medications in Southeast Asia and Sub-Saharan Africa failed chemical analysis, packaging analysis, or were falsified.
War.
Throughout history, the contraction of malaria has played a prominent role in the fates of government rulers, nation-states, military personnel, and military actions. In 1910, Nobel Prize in Medicine-winner Ronald Ross (himself a malaria survivor), published a book titled "The Prevention of Malaria" that included a chapter titled "The Prevention of Malaria in War." The chapter's author, Colonel C. H. Melville, Professor of Hygiene at Royal Army Medical College in London, addressed the prominent role that malaria has historically played during wars: "The history of malaria in war might almost be taken to be the history of war itself, certainly the history of war in the Christian era. ... It is probably the case that many of the so-called camp fevers, and probably also a considerable proportion of the camp dysentery, of the wars of the sixteenth, seventeenth and eighteenth centuries were malarial in origin."
Malaria was the most important health hazard encountered by U.S. troops in the South Pacific during World War II, where about 500,000 men were infected. According to Joseph Patrick Byrne, "Sixty thousand American soldiers died of malaria during the African and South Pacific campaigns."
Significant financial investments have been made to procure existing and create new anti-malarial agents. During World War I and World War II, inconsistent supplies of the natural anti-malaria drugs cinchona bark and quinine prompted substantial funding into research and development of other drugs and vaccines. American military organizations conducting such research initiatives include the Navy Medical Research Center, Walter Reed Army Institute of Research, and the U.S. Army Medical Research Institute of Infectious Diseases of the US Armed Forces.
Additionally, initiatives have been founded such as Malaria Control in War Areas (MCWA), established in 1942, and its successor, the Communicable Disease Center (now known as the Centers for Disease Control and Prevention, or CDC) established in 1946. According to the CDC, MCWA "was established to control malaria around military training bases in the southern United States and its territories, where malaria was still problematic".
Eradication efforts.
Several notable attempts are being made to eliminate the parasite from sections of the world, or to eradicate it worldwide. In 2006, the organization Malaria No More set a public goal of eliminating malaria from Africa by 2015, and the organization plans to dissolve if that goal is accomplished. Several malaria vaccines are in clinical trials, which are intended to provide protection for children in endemic areas and reduce the speed of transmission of the disease. , The Global Fund to Fight AIDS, Tuberculosis and Malaria has distributed 230 million insecticide-treated nets intended to stop mosquito-borne transmission of malaria. The U.S.-based Clinton Foundation has worked to manage demand and stabilize prices in the artemisinin market. Other efforts, such as the Malaria Atlas Project, focus on analysing climate and weather information required to accurately predict the spread of malaria based on the availability of habitat of malaria-carrying parasites. The Malaria Policy Advisory Committee (MPAC) of the World Health Organization (WHO) was formed in 2012, "to provide strategic advice and technical input to WHO on all aspects of malaria control and elimination". In November 2013, WHO and the malaria vaccine funders group set a goal to develop vaccines designed to interrupt malaria transmission with the long-term goal of malaria eradication.
Malaria has been successfully eliminated or greatly reduced in certain areas. Malaria was once common in the United States and southern Europe, but vector control programs, in conjunction with the monitoring and treatment of infected humans, eliminated it from those regions. Several factors contributed, such as the draining of wetland breeding grounds for agriculture and other changes in water management practices, and advances in sanitation, including greater use of glass windows and screens in dwellings. Malaria was eliminated from most parts of the USA in the early 20th century by such methods, and the use of the pesticide DDT and other means eliminated it from the remaining pockets in the South in the 1950s. (see National Malaria Eradication Program) In Suriname, the disease has been cleared from its capital city and coastal areas through a three-pronged approach initiated by the Global Malaria Eradication program in 1955, involving: vector control through the use of DDT and IRS; regular collection of blood smears from the population to identify existing malaria cases; and providing chemotherapy to all affected individuals. Bhutan is pursuing an aggressive malaria elimination strategy, and has achieved a 98.7% decline in microscopy-confirmed cases from 1994 to 2010. In addition to vector control techniques such as IRS in high-risk areas and thorough distribution of long-lasting ITNs, factors such as economic development and increasing access to health services have contributed to Bhutan's successes in reducing malaria incidence. The UK's Department for International Development and the Bill and Melinda Gates Foundation will spend $4.5bn over five years from 2016 in an effort to end deaths caused by the disease.
Research.
The Malaria Eradication Research Agenda (malERA) initiative was a consultative process to identify which areas of research and development (R&D) needed to be addressed for the worldwide eradication of malaria.
Vaccine.
Immunity (or, more accurately, tolerance) to "P. falciparum" malaria does occur naturally, but only in response to years of repeated infection. An individual can be protected from a "P. falciparum" infection if they receive about a thousand bites from mosquitoes that carry a version of the parasite rendered non-infective by a dose of X-ray irradiation. An effective vaccine is not yet available for malaria, although several are under development. The highly polymorphic nature of many "P. falciparum" proteins results in significant challenges to vaccine design. Vaccine candidates that target antigens on gametes, zygotes, or ookinetes in the mosquito midgut aim to block the transmission of malaria. These transmission-blocking vaccines induce antibodies in the human blood; when a mosquito takes a blood meal from a protected individual, these antibodies prevent the parasite from completing its development in the mosquito. Other vaccine candidates, targeting the blood-stage of the parasite's life cycle, have been inadequate on their own. For example, SPf66 was tested extensively in areas where the disease is common in the 1990s, but trials showed it to be insufficiently effective. Several potential vaccines targeting the pre-erythrocytic stage of the parasite's life cycle are being developed, with RTS,S as a leading candidate; it is expected to be licensed in 2015. A US biotech company, Sanaria, is developing a pre-erythrocytic attenuated vaccine called PfSPZ that uses whole sporozoites to induce an immune response. In 2006, the Malaria Vaccine Advisory Committee to the WHO outlined a "Malaria Vaccine Technology Roadmap" that has as one of its landmark objectives to "develop and license a first-generation malaria vaccine that has a protective efficacy of more than 50% against severe disease and death and lasts longer than one year" by 2015.
Medications.
Malaria parasites contain apicoplasts, organelles usually found in plants, complete with their own genomes. These apicoplasts are thought to have originated through the endosymbiosis of algae and play a crucial role in various aspects of parasite metabolism, such as fatty acid biosynthesis. Over 400 proteins have been found to be produced by apicoplasts and these are now being investigated as possible targets for novel anti-malarial drugs.
With the onset of drug-resistant "Plasmodium" parasites, new strategies are being developed to combat the widespread disease. One such approach lies in the introduction of synthetic pyridoxal-amino acid adducts, which are taken up by the parasite and ultimately interfere with its ability to create several essential B vitamins. Antimalarial drugs using synthetic metal-based complexes are attracting research interest.
Other.
A non-chemical vector control strategy involves genetic manipulation of malaria mosquitoes. Advances in genetic engineering technologies make it possible to introduce foreign DNA into the mosquito genome and either decrease the lifespan of the mosquito, or make it more resistant to the malaria parasite. Sterile insect technique is a genetic control method whereby large numbers of sterile male mosquitoes are reared and released. Mating with wild females reduces the wild population in the subsequent generation; repeated releases eventually eliminate the target population.
Genomics is central to malaria research. With the sequencing of "P. falciparum", one of its vectors "Anopheles gambiae", and the human genome, the genetics of all three organisms in the malaria lifecycle can be studied. Another new application of genetic technology is the ability to produce genetically modified mosquitoes that do not transmit malaria, potentially allowing biological control of malaria transmission.
In one study, a genetically-modified strain of "Anopheles stephensi" was created that no longer supported malaria transmission, and this resistance was passed down to mosquito offspring.
Other animals.
Nearly 200 parasitic "Plasmodium" species have been identified that infect birds, reptiles, and other mammals, and about 30 species naturally infect non-human primates. Some malaria parasites that affect non-human primates (NHP) serve as model organisms for human malarial parasites, such as "P. coatneyi" (a model for "P. falciparum") and "P. cynomolgi" ("P. vivax"). Diagnostic techniques used to detect parasites in NHP are similar to those employed for humans. Malaria parasites that infect rodents are widely used as models in research, such as "P. berghei". Avian malaria primarily affects species of the order Passeriformes, and poses a substantial threat to birds of Hawaii, the Galapagos, and other archipelagoes. The parasite "P. relictum" is known to play a role in limiting the distribution and abundance of endemic Hawaiian birds. Global warming is expected to increase the prevalence and global distribution of avian malaria, as elevated temperatures provide optimal conditions for parasite reproduction.

</doc>
<doc id="20424" url="https://en.wikipedia.org/wiki?curid=20424" title="Lunar phase">
Lunar phase

The lunar phase or phase of the moon is the shape of the illuminated (sunlit) portion of the Moon as seen by an observer on Earth. The lunar phases change cyclically as the Moon orbits the Earth, according to the changing positions of the Moon and Sun relative to the Earth. The Moon's rotation is tidally locked by the Earth's gravity, therefore the same lunar surface always faces Earth. This face is variously sunlit depending on the position of the Moon in its orbit. Therefore, the portion of this hemisphere that is visible to an observer on Earth can vary from about 100% (full moon) to 0% (new moon). The lunar terminator is the boundary between the illuminated and darkened hemispheres. Each of the four "intermediate" lunar phases (see below) is roughly seven days (~7.4 days) but this varies slightly due to the elliptical shape of the Moon's orbit. Aside from some craters near the lunar poles such as Shoemaker, all parts of the Moon see around 14.77 days of sunlight, followed by 14.77 days of "night". (The side of the Moon facing away from the Earth is sometimes called the "dark side", which is a misnomer. It receives just as much sunlight as the Earth-facing side − actually slightly more, since the Earth never obscures the Sun from it.)
Phases of the moon.
In Western culture, the four principal lunar phases are new moon, first quarter, full moon, and third quarter (also known as last quarter). These are the instants when the Moon's apparent geocentric celestial longitude minus the Sun's apparent geocentric celestial longitude is 0°, 90°, 180° and 270°, respectively. Each of these phases is instantaneous, lasting theoretically zero time, although they occur at slightly different times when viewed from different points on the Earth. During the intervals between principal phases, the Moon appears crescent-shaped or gibbous. These shapes, and the periods of time when the Moon shows them, are called the intermediate phases. They last, on average, one-quarter of a synodic month, roughly 7.38 days, but their durations vary slightly because the Moon's orbit is slightly elliptical, and thus its speed in orbit is not constant. The descriptor "waxing" is used for an intermediate phase when the Moon's apparent size is increasing, from new moon toward full moon, and "waning" when the size is decreasing.
The eight principal and intermediate phases are given the following names, in sequential order:
When the Sun and Moon are aligned on the same side of the Earth, the moon is "new", and the side of the Moon facing Earth is not illuminated by the Sun. As the moon "waxes" (the amount of illuminated surface as seen from Earth is increasing), the lunar phases progress through new moon, crescent moon, first-quarter moon, gibbous moon, and full moon. The moon is then said to "wane" as it passes through the gibbous moon, third-quarter moon, crescent moon and back to new moon. The terms "old moon" and "new moon" are not interchangeable. The "old moon" is a waning sliver (which eventually becomes undetectable to the naked eye) until the moment it aligns with the sun and begins to wax, at which point it becomes new again. Half moon is often used to mean the first- and third-quarter moons, while the term 'quarter' refers to the extent of the moon's cycle around the Earth, not its shape.
When a sphere is illuminated on one hemisphere and viewed from a different angle, the portion of the illuminated area that is visible will have a two-dimensional shape defined by the intersection of an ellipse and circle (where the major axis of the ellipse coincides with a diameter of the circle). If the half-ellipse is convex with respect to the half-circle, then the shape will be gibbous (bulging outwards, Origin: 1350–1400; Middle English < Latin gibbōsus humped, equivalent to gibb (a) hump + -ōsus -ous), whereas if the half-ellipse is concave with respect to the half-circle, then the shape will be a crescent. When a crescent Moon occurs, the phenomenon of earthshine may be apparent, where the night side of the Moon faintly reflects light from the Earth.
In the northern hemisphere, if the left side of the Moon is dark then the light part is growing, and the Moon is referred to as waxing (moving toward a full moon). If the right side of the Moon is dark then the light part is shrinking, and the Moon is referred to as waning (past full and moving toward a new moon). Assuming that the viewer is in the northern hemisphere, the right portion of the Moon is the part that is always growing (i.e., if the right side is dark, the Moon is growing darker; if the right side is lit, the Moon is growing lighter). In the southern hemisphere the Moon is observed from a perspective inverted to that of the northern hemisphere, so the opposite sides appear to grow (wax) and shrink (wane).
The above descriptions of the lunar phases apply for observers at temperate or high latitudes on the Earth. Observers in tropical latitudes see the Moon with its terminator apparently horizontal during the morning and evening. The crescent Moon can open upward or downward, with the "horns" of the crescent pointing up or down, respectively. When the Sun appears above the Moon in the sky, the crescent opens downward; when the Moon is above the Sun, the crescent opens upward. The crescent Moon is most clearly and brightly visible when the Sun is below the horizon, which implies that the Moon must be above the Sun, and the crescent must open upward. This is therefore the orientation in which the crescent Moon is most often seen from the Earth's tropics. The waxing and waning crescents look very similar. The waxing crescent appears in the western sky in the evening, and the waning crescent in the east, in the morning.
When the Moon, as seen from Earth, is a narrow crescent, the Earth as seen from the Moon is almost fully lit by the Sun. Often, the part of the Moon that is not directly lit by the Sun is sufficiently brightly lit by light reflected from the Earth to be easily visible from Earth. This phenomenon is called "earthshine", and is sometimes picturesquely described as "the old moon in the new moon's arms" or, as pictured here, "the new moon in the old moon's arms".
Non-western cultures may use a different number of Moon phases, for example traditional Hawaiian culture has a total of 30 different Moon phases.
Calendar.
The average calendar month, which is of a tropical year, is about 30.44 days, while the Moon's phase (synodic) cycle repeats on average every 29.53 days. Therefore, the timing of the Moon's phases shifts by an average of almost one day for each successive month. (Lunar year=354 days.)
Photographing the Moon's phase every day for a month, starting in the evening after sunset, and repeating approximately fifty minutes later each successive day, ending in the morning before sunrise, would create a composite image like the example calendar from May 8, 2005, to June 6, 2005 here in the left. There is no picture on May 20 since a picture would be taken before midnight on May 19, and after midnight on May 21. Similarly, on a calendar listing moon rise or set times, some days will appear to be skipped. When the Moon rises just before midnight one night it will rise just after midnight the next (so too with setting). The 'skipped day' is just a feature of the moon's eastward movement in relation to the sun, which at most latitudes causes the moon to rise later each day. The moon has a predictable orbit every month.
Calculating phase.
Each of the 4 lunar phases lasts approximately 7 days (~7.4 days), but varies slightly due to lunar apogee and perigee.
The approximate age of the moon, and hence the approximate phase, can be calculated for any date by calculating the number of days since a known new moon (such as January 1, 1900 or August 11, 1999) and reducing this modulo 29.530588853 (the length of a synodic month). The difference between two dates can be calculated by subtracting the Julian Day Number of one from that of the other, or there are simpler formulae giving (for instance) the number of days since December 31, 1899. However, this calculation assumes a perfectly circular orbit and makes no allowance for the time of day at which the new moon happened, therefore may be incorrect by several hours (it also becomes less accurate the larger the difference between the required date and the reference date); it is accurate enough to use in a novelty clock application showing moon phase, but specialist usage taking account of lunar apogee and perigee requires a more elaborate calculation. 
Effect of parallax.
The Earth subtends an angle of about two degrees, when seen from the Moon. This means that an observer on Earth who sees the Moon when it is close to the eastern horizon sees it from an angle that is about two degrees different from the line of sight of an observer who sees the Moon on the western horizon. The Moon moves about 12 degrees around its orbit per day, so, if these observers were stationary, they would see the phases of the Moon at times that differ by about one-sixth of a day, or four hours. But in reality the observers are on the surface of the rotating Earth, so someone who sees the Moon on the eastern horizon at one moment sees it on the western horizon about 12 hours later. This adds an oscillation to the apparent progression of the lunar phases. They appear to occur more slowly when the Moon is high in the sky than when it is below the horizon. The Moon appears to move jerkily, and the phases do the same. The amplitude of this oscillation is never more than about four hours, which is a small fraction of a month. It does not have any obvious effect on the appearance of the Moon. However, it does affect accurate calculations of the times of lunar phases.
Misconceptions.
It might be expected that once every month, when the Moon passes between Earth and the Sun during a new moon, its shadow would fall on Earth causing a solar eclipse, but this does not happen every month. Nor is it true that during every full moon, the Earth's shadow falls on the Moon, causing a lunar eclipse. Solar and lunar eclipses are not observed "every" month because the plane of the Moon's orbit around the Earth is tilted by about five degrees with respect to the plane of Earth's orbit around the Sun (the plane of the ecliptic). Thus, when new and full moons occur, the Moon usually lies to the north or south of a direct line through the Earth and Sun. Although an eclipse can only occur when the Moon is either new (solar) or full (lunar), it must also be positioned very near the intersection of Earth's orbit plane about the Sun and the Moon's orbit plane about the Earth (that is, at one of its nodes). This happens about twice per year, and so there are between four and seven eclipses in a calendar year. Most of these events are quite insignificant; major eclipses of the Moon or Sun are less frequent.

</doc>
<doc id="20426" url="https://en.wikipedia.org/wiki?curid=20426" title="Metonic cycle">
Metonic cycle

For astronomy and calendar studies, the Metonic cycle or Enneadecaeteris (from , "nineteen years") is a period of very close to 19 years that is remarkable for being nearly a common multiple of the solar year and the synodic (lunar) month. The Greek astronomer Meton of Athens (fifth century BC) observed that a period of 19 years is almost exactly equal to 235 synodic months and, rounded to full days, counts 6,940 days. The difference between the two periods (of 19 years and 235 synodic months) is only a few hours, depending on the definition of the year.
Considering a year to be of this 6,940-day cycle gives a year length of 365 +  +  days (the unrounded cycle is much more accurate), which is about 11 days more than 12 synodic months. To keep a 12-month lunar year in pace with the solar year, an intercalary 13th month would have to be added on seven occasions during the nineteen-year period (235 = 19 × 12 + 7). When Meton introduced the cycle around 432 BC, it was already known by Babylonian astronomers.
A mechanical computation of the cycle is built into the Antikythera mechanism.
The cycle was used in the Babylonian calendar, ancient Chinese calendar systems (the 'Rule Cycle' 章) and the medieval computus (i.e. the calculation of the date of Easter). It regulates the 19-year cycle of intercalary months of the modern Hebrew calendar.
Mathematical basis.
At the time of Meton, axial precession had not yet been discovered, and he could not distinguish between sidereal years (currently: 365.256363 days) and tropical years (currently: 365.242190 days). Most calendars, like the commonly used Gregorian calendar, are based on the tropical year and maintain the seasons at the same calendar times each year. Nineteen tropical years are about two hours shorter than 235 synodic months. The Metonic cycle's error is, therefore, one full day every 219 years, or 12.4 parts per million.
Note that the 19-year cycle is also close (to somewhat more than half a day) to 255 draconic months, so it is also an eclipse cycle, which lasts only for about 4 or 5 recurrences of eclipses. The Octon is of a Metonic cycle (47 synodic months, 3.8 years), and it recurs about 20 to 25 cycles.
This cycle seems to be a coincidence. The periods of the Moon's orbit around the Earth and the Earth's orbit around the Sun are believed to be independent, and not to have any known physical resonance. An example of a non-coincidental cycle is the orbit of Mercury, with its 3:2 spin-orbit resonance.
A lunar year of 12 synodic months is about 354 days, approximately 11 days short of the "365-day" solar year. Therefore, for a lunisolar calendar, every 2 to 3 years there is a difference of more than a full lunar month between the lunar and solar years, and an extra ("embolismic") month needs to be inserted (intercalation). The Athenians initially seem not to have had a regular means of intercalating a 13th month; instead, the question of when to add a month was decided by an official. Meton's discovery made it possible to propose a regular intercalation scheme. The Babylonians seem to have introduced this scheme around 500 BC, thus well before Meton.
Application in traditional calendars.
Traditionally, for the Babylonian and Hebrew lunisolar calendars, the years 3, 6, 8, 11, 14, 17, and 19 are the long (13-month) years of the Metonic cycle. This cycle, which can be used to predict eclipses, forms the basis of the Greek and Hebrew calendars, and is used for the computation of the date of Easter each year.
The Babylonians applied the 19-year cycle since the late sixth century BC. As they measured the moon's motion against the stars, the 235:19 relationship may originally have referred to sidereal years, instead of tropical years as it has been used for various calendars.
Apollo was said to have visited the Hyperboreans once every 19 years, presumably at the high point of the cycle. 
The Runic calendar is a perpetual calendar based on the 19-year-long Metonic cycle. Also known as a Rune staff or Runic Almanac, it appears to have been a medieval Swedish invention. This calendar does not rely on knowledge of the duration of the tropical year or of the occurrence of leap years. It is set at the beginning of each year by observing the first full moon after the winter solstice. The oldest one known, and the only one from the Middle Ages, is the Nyköping staff, which is believed to date from the 13th century.
The Bahá'í calendar, established during the middle of the 19th century, is also based on cycles of 19 years.
Further details.
The Metonic cycle is related to two less accurate subcycles:
By combining appropriate numbers of 11-year and 19-year periods, it is possible to generate ever more accurate cycles. For example, simple arithmetic shows that:
This gives an error of only about half an hour in 687 years (2.5 seconds a year), although this is subject to secular variation in the length of the tropical year and the lunation.
Meton of Athens approximated the cycle to a whole number (6,940) of days, obtained by 125 long months of 30 days and 110 short months of 29 days. During the next century, Callippus developed the Callippic cycle of four 19-year periods for a 76-year cycle with a mean year of exactly 365.25 days.

</doc>
<doc id="20427" url="https://en.wikipedia.org/wiki?curid=20427" title="March 26">
March 26


</doc>
<doc id="20428" url="https://en.wikipedia.org/wiki?curid=20428" title="Marcello Malpighi">
Marcello Malpighi

Marcello Malpighi (10 March 1628 – 29 November 1694) was an Italian biologist and physician, who is referred to as the "Father of microscopical anatomy, histology, physiology and embryology". Malpighi's name bears to several physiological features related to the biological excretory system, such as the Malpighian corpuscles and Malpighian pyramids of the kidneys and the Malpighian tubule system of insects. The splenic lymphoid nodules are often called the "Malpighian bodies of the spleen" or Malpighian corpuscles. The botanical family Malpighiaceae is also named after him. He was the first person to see capillaries in animals, and he discovered the link between arteries and veins that had eluded William Harvey and Malpighi has been the earliest person to observe red blood cells under a microscope. His treatise "De polypo cordis" (1666) was important for understanding blood composition, as well as how blood clots. In it, Malpighi described how the form of a blood clot differed in the right against the left sides of the heart. 
The use of the microscope enabled Malpighi to discover that invertebrates do not use lungs to breathe, but small holes in their skin called tracheae. Malpighi also studied the anatomy of the brain and concluded this organ is a gland. In terms of modern endocrinology, this deduction is correct because the hypothalamus of the brain has long been recognized for its hormone-secreting capacity. 
Because Malpighi had a wide knowledge of both plants and animals, he made contributions to the scientific study of both. The Royal Society of London published two volumes of his botanical and zoological works in 1675 and 1679. Another edition followed in 1687, and a supplementary volume in 1697. In his autobiography, Malpighi speaks of his "Anatome Plantarum", decorated with the engravings of Robert White as "the most elegant format in the whole literate world."
His study of plants led him to conclude that plants had tubules similar to those he saw in insects like the silk worm (using his microscope, he probably saw the stomata, through which plants exchange carbon dioxide with oxygen). Malpighi observed that when a ring-like portion of bark was removed on a trunk a swelling occurred in the tissues above the ring, and he correctly interpreted this as growth stimulated by food coming down from the leaves, and being blocked above the ring.
Early years.
Malpighi was born on 10 March 1628 at Crevalcore near Bologna, Italy. The son of well-to-do parents, Malpighi was educated in his native city, entering the University of Bologna at the age of 17. In a posthumous work delivered and dedicated to the Royal Society in London in 1697, Malpighi says he completed his grammatical studies in 1645, at which point he began to apply himself to the study of peripatetic philosophy. He completed these studies about 1649, where at the persuasion of his mother Frances Natalis, he began to study physics. When his parents and grandmother became ill, he returned to his family home near Bologna to care for them.
Malpighi studied Aristotelian philosophy at the University of Bologna while he was very young. 
Despite opposition from the university authorities because he was non-Bolognese by birth, in 1653 he was granted doctorates in both medicine and philosophy. He later graduated as a medical doctor at the age of 25. Subsequently, he was appointed as a teacher, whereupon he immediately dedicated himself to further study in anatomy and medicine. For most of his career, Malpighi combined an intense interest in scientific research with a fond love of teaching. He was invited to correspond with the Royal Society in 1667 by Henry Oldenburg, and became a fellow of the society the next year.
In 1656, Ferdinand II of Tuscany invited him to the professorship of theoretical medicine at the University of Pisa. There Malpighi began his lifelong friendship with Giovanni Borelli, mathematician and naturalist, who was a prominent supporter of the Accademia del Cimento, one of the first scientific societies. Malpighi questioned the prevailing medical teachings at Pisa, tried experiments on colour changes in blood, and attempted to recast anatomical, physiological, and medical problems of the day. Family responsibilities and poor health prompted Malpighi’s return in 1659 to the University of Bologna, where he continued to teach and do research with his microscopes. In 1661 he identified and described the pulmonary and capillary network connecting small arteries with small veins, one of the major discoveries in the history of science. Malpighi’s views evoked increasing controversy and dissent, mainly from envy and lack of understanding on the part of his colleagues.
Career.
In 1653, his father, mother, and grandmother being dead, Malpighi left his family villa and returned to the University of Bologna to study anatomy. In 1656, he was made a reader at Bologna, and then a professor of physics at Pisa, where he began to abandon the disputative method of learning and apply himself to a more experimental method of research. Based on this research, he wrote some "Dialogues against the Peripatetics and Galenists" (those who followed the precepts of Galen), which were destroyed when his house burned down. Weary of philosophical disputation, in 1660, Malpighi returned to Bologna and dedicated himself to the study of anatomy. He subsequently discovered a new structure of the lungs which led him to several disputes with the learned medical men of the times. In 1662, he was made a professor of Physics at the Academy of Messina.
Retiring from university life to his villa in the country near Bologna in 1663, he worked as a physician while continuing to conduct experiments on the plants and insects he found on his estate. There he made discoveries of the structure of plants which he published in his "Observations". At the end of 1666, Malpighi was invited to return to the public academy at Messina, which he did in 1667. Although he accepted temporary chairs at the universities of Pisa and Messina, throughout his life he continuously returned to Bologna to practice medicine, a city that repaid him by erecting a monument in his memory after his death.
In 1668, Malpighi received a letter from Mr. Oldenburg of the Royal Society in London, inviting him to correspond. Malpighi wrote his history of the silkworm in 1668, and sent the manuscript to Mr. Oldenburg. As a result, Malpighi was made a member of the Royal Society in 1669. In 1671, Malpighi’s "Anatomy of Plants" was published in London by the Royal Society, and he simultaneously wrote to Mr. Oldenburg, telling him of his recent discoveries regarding the lungs, fibers of the spleen and testicles, and several other discoveries involving the brain and sensory organs. He also shared more information regarding his research on plants. At that time, he related his disputes with some younger physicians who were strenuous supporters of the Galenic principles and opposed to all new discoveries. Following many other discoveries and publications, in 1691, Malpighi was uprooted from his beloved home in Bologna and summoned to Rome by Pope Innocent XII
Marcello Malpighi is buried in the church of the Santi Gregorio e Siro, in Bologna, where nowadays can be seen a marble monument to the scientist with an inscription in Latin remembering – among other things – his "SUMMUM INGENIUM / INTEGERRIMAM VITAM / FORTEM STRENUAMQUE MENTEM / AUDACEM SALUTARIS ARTIS AMOREM" (great genius, honest life, strong and tough mind, daring love for the medical art).
Research.
Around the age of 38, and with a remarkable academic career behind him, Malpighi decided to dedicate his free time to anatomical studies. Although he conducted some of his studies using vivisection and others through the dissection of corpses, his most illustrative efforts appear to have been based on the use of the microscope. Because of this work, many microscopic anatomical structures are named after Malpighi, including a skin layer (Malpighi layer) and two different Malpighian corpuscles in the kidneys and the spleen, as well as the Malpighian tubules in the excretory system of insects.
See Timeline of microscope technology for more information.
Although a Dutch spectacle maker created the compound lens and inserted it in a microscope around the turn of the 17th century, and Galileo had applied the principle of the compound lens to the making of his microscope patented in 1609, its possibilities as a microscope had remained unexploited for half a century, until Robert Hooke improved the instrument. Following this, Marcello Malpighi, Hooke, and two other early investigators associated with the Royal Society, Nehemiah Grew and Antoine van Leeuwenhoek were fortunate to have a virtually untried tool in their hands as they began their investigations.
Working on frogs and extrapolating to humans, Malpighi demonstrated the structure of the lungs, previously thought to be a homogeneous mass of flesh, and he offered an explanation for how air and blood mixed in the lungs. Malpighi also used the microscope for his studies of the skin, kidneys, and liver. For example, after he dissected a black male, Malpighi made some groundbreaking headway into the discovery of the origin of black skin. He found that the black pigment was associated with a layer of mucus just beneath the skin.
A talented sketch artist, Malpighi seems to have been the first author to have made detailed drawings of individual organs of flowers. In his "Anatome plantarum" is a longitudinal section of a flower of "Nigella" (his Melanthi, literally honey-flower) with details of the nectariferous organs. He adds that it is strange that nature has produced on the leaves of the flower shell-like organs in which honey is produced.
Malpighi had success in tracing the ontogeny of plant organs, and the serial development of the shoot owing to his instinct shaped in the sphere of animal embryology. He specialized in seedling development, and in 1679, he published a volume containing a series of exquisitely drawn and engraved images of the stages of development of Leguminosae (beans) and Cucurbitaceae (squash, melons). Later, he published material depicting the development of the date palm. The great Swedish botanist Linnaeus named the genus "Malpighia" in honor of Malpighi’s work with plants; "Malpighia" is the type genus for the Malpighiaceae, a family of tropical and subtropical flowering plants.
Because Malpighi was concerned with teratology (the scientific study of the visible conditions caused by the interruption or alteration of normal development) he expressed grave misgivings about the view of his contemporaries that the galls of trees and herbs gave birth to insects. He conjectured that the creatures in question arose from eggs previously laid in the plant tissue.
Malpighi’s investigations of the lifecycle of plants and animals led him into the topic of reproduction. He created detailed drawings of his studies of chick embryo development, seed development in plants (such as the lemon tree), and the transformation of caterpillars into insects. His discoveries helped to illuminate philosophical arguments surrounding the topics of "emboîtment", pre-existence, preformation, epigenesis, and metamorphosis.
Years in Rome.
In 1691 Pope Innocent XII invited him to Rome as papal physician. He taught medicine in the Papal Medical School and wrote a long treatise about his studies which he donated to the Royal Society of London.
Marcello Malpighi died of apoplexy (an old-fashioned term for a stroke or stroke-like symptoms) in Rome on 29 September 1694, at the age of 66. In accordance with his wishes, an autopsy was performed. The Royal Society published his studies in 1696.
Memories of Malpighi in Bologna.
Malpighi is buried in the church of the Santi Gregorio e Siro, in Bologna, where nowadays can be seen a marble monument to the scientist with an inscription in Latin remembering – among other things – his "SUMMUM INGENIUM / INTEGERRIMAM VITAM / FORTEM STRENUAMQUE MENTEM / AUDACEM SALUTARIS ARTIS AMOREM" (great genius, honest life, strong and tough mind, daring love for the medical art).

</doc>
<doc id="20431" url="https://en.wikipedia.org/wiki?curid=20431" title="Momentum">
Momentum

In classical mechanics, linear momentum or translational momentum (pl. momenta; SI unit kg m/s, or equivalently, newton second) is the product of the mass and velocity of an object. For example, a heavy truck moving rapidly has a large momentum—it takes a large or prolonged force to get the truck up to this speed, and it takes a large or prolonged force to bring it to a stop afterwards. If the truck were lighter, or moving more slowly, then it would have less momentum.
Like velocity, linear momentum is a vector quantity, possessing a direction as well as a magnitude:
where is the three-dimensional vector stating the object's momentum in the three directions of three-dimensional space, is the three-dimensional velocity vector giving the object's rate of movement in each direction, and is the object's mass.
Linear momentum is also a "conserved" quantity, meaning that if a closed system is not affected by external forces, its total linear momentum cannot change.
In classical mechanics, conservation of linear momentum is implied by Newton's laws. It also holds in special relativity (with a modified formula) and, with appropriate definitions, a (generalized) linear momentum conservation law holds in electrodynamics, quantum mechanics, quantum field theory, and general relativity. It is ultimately an expression of one of the fundamental symmetries of space and time, that of translational symmetry.
Newtonian mechanics.
Momentum has a direction as well as magnitude. Quantities that have both a magnitude and a direction are known as vector quantities. Because momentum has a direction, it can be used to predict the resulting direction of objects after they collide, as well as their speeds. Below, the basic properties of momentum are described in one dimension. The vector equations are almost identical to the scalar equations (see multiple dimensions).
Single particle.
The momentum of a particle is traditionally represented by the letter . It is the product of two quantities, the mass (represented by the letter ) and velocity ():
The units of momentum are the product of the units of mass and velocity. In SI units, if the mass is in kilograms and the velocity in meters per second then the momentum is in kilogram meters/second (kg m/s). An equivalent derived unit is the newton second (1 N s = 1 kg m/s). In cgs units, if the mass is in grams and the velocity in centimeters per second then the momentum is in gram centimeters/second (g cm/s) or dyne seconds (1 dyne s = 1 g m/s).
Being a vector, momentum has magnitude and direction. For example, a 1 kg model airplane, traveling due north at 1 m/s in straight and level flight, has a momentum of 1 kg m/s due north measured from the ground.
Many particles.
The momentum of a system of particles is the sum of their momenta. If two particles have masses and , and velocities and , the total momentum is
The momenta of more than two particles can be added in the same way.
A system of particles has a center of mass, a point determined by the weighted sum of their positions:
If all the particles are moving, the center of mass will generally be moving as well (unless the system is in pure rotation around it). If the center of mass is moving at velocity , the momentum is:
This is known as Euler's first law.
Relation to force.
If a force is applied to a particle for a time interval , the momentum of the particle changes by an amount
In differential form, this is Newton's second law; the rate of change of the momentum of a particle is proportional to the force acting on it,
If the force depends on time, the change in momentum (or impulse) between times and is
Under the assumption of constant mass , it is equivalent to write
so the force is equal to mass times acceleration.
"Example": A model airplane of 1 kg accelerates from rest to a velocity of 6 m/s due north in 2 s. The net force required to produce this acceleration is 3 newtons due north. The change in momentum is 6 kg m/s. The rate of change of momentum is 3 (kg m/s)/s = 3 N.
Conservation.
In a closed system (one that does not exchange any matter with its surroundings and is not acted on by external forces) the total momentum is constant. This fact, known as the "law of conservation of momentum", is implied by Newton's laws of motion. Suppose, for example, that two particles interact. Because of the third law, the forces between them are equal and opposite. If the particles are numbered 1 and 2, the second law states that and . Therefore,
with the negative sign indicating that the forces oppose. Equivalently,
If the velocities of the particles are and before the interaction, and afterwards they are and , then
This law holds no matter how complicated the force is between particles. Similarly, if there are several particles, the momentum exchanged between each pair of particles adds up to zero, so the total change in momentum is zero. This conservation law applies to all interactions, including collisions and separations caused by explosive forces. It can also be generalized to situations where Newton's laws do not hold, for example in the theory of relativity and in electrodynamics.
Dependence on reference frame.
Momentum is a measurable quantity, and the measurement depends on the motion of the observer. For example: if an apple is sitting in a glass elevator that is descending, an outside observer, looking into the elevator, sees the apple moving, so, to that observer, the apple has a non-zero momentum. To someone inside the elevator, the apple does not move, so, it has zero momentum. The two observers each have a frame of reference, in which, they observe motions, and, if the elevator is descending steadily, they will see behavior that is consistent with those same physical laws.
Suppose a particle has position in a stationary frame of reference. From the point of view of another frame of reference, moving at a uniform speed , the position (represented by a primed coordinate) changes with time as
This is called a Galilean transformation. If the particle is moving at speed in the first frame of reference, in the second, it is moving at speed
Since does not change, the accelerations are the same:
Thus, momentum is conserved in both reference frames. Moreover, as long as the force has the same form, in both frames, Newton's second law is unchanged. Forces such as Newtonian gravity, which depend only on the scalar distance between objects, satisfy this criterion. This independence of reference frame is called Newtonian relativity or Galilean invariance.
A change of reference frame, can, often, simplify calculations of motion. For example, in a collision of two particles, a reference frame can be chosen, where, one particle begins at rest. Another, commonly used reference frame, is the center of mass frame - one that is moving with the center of mass. In this frame,
the total momentum is zero.
Application to collisions.
By itself, the law of conservation of momentum is not enough to determine the motion of particles after a collision. Another property of the motion, kinetic energy, must be known. This is not necessarily conserved. If it is conserved, the collision is called an "elastic collision"; if not, it is an "inelastic collision".
Elastic collisions.
An elastic collision is one in which no kinetic energy is lost. Perfectly elastic "collisions" can occur when the objects do not touch each other, as for example in atomic or nuclear scattering where electric repulsion keeps them apart. A slingshot maneuver of a satellite around a planet can also be viewed as a perfectly elastic collision from a distance. A collision between two pool balls is a good example of an "almost" totally elastic collision, due to their high rigidity; but when bodies come in contact there is always some dissipation.
A head-on elastic collision between two bodies can be represented by velocities in one dimension, along a line passing through the bodies. If the velocities are and before the collision and and after, the equations expressing conservation of momentum and kinetic energy are:
A change of reference frame can often simplify the analysis of a collision. For example, suppose there are two bodies of equal mass , one stationary and one approaching the other at a speed (as in the figure). The center of mass is moving at speed and both bodies are moving towards it at speed . Because of the symmetry, after the collision both must be moving away from the center of mass at the same speed. Adding the speed of the center of mass to both, we find that the body that was moving is now stopped and the other is moving away at speed . The bodies have exchanged their velocities. Regardless of the velocities of the bodies, a switch to the center of mass frame leads us to the same conclusion. Therefore, the final velocities are given by
In general, when the initial velocities are known, the final velocities are given by
If one body has much greater mass than the other, its velocity will be little affected by a collision while the other body will experience a large change.
Inelastic collisions.
In an inelastic collision, some of the kinetic energy of the colliding bodies is converted into other forms of energy such as heat or sound. Examples include traffic collisions, in which the effect of lost kinetic energy can be seen in the damage to the vehicles; electrons losing some of their energy to atoms (as in the Franck–Hertz experiment); and particle accelerators in which the kinetic energy is converted into mass in the form of new particles.
In a perfectly inelastic collision (such as a bug hitting a windshield), both bodies have the same motion afterwards. If one body is motionless to begin with, the equation for conservation of momentum is
so
In a frame of reference moving at the speed , the objects are brought to rest by the collision and 100% of the kinetic energy is converted.
One measure of the inelasticity of the collision is the coefficient of restitution , defined as the ratio of relative velocity of separation to relative velocity of approach. In applying this measure to ball sports, this can be easily measured using the following formula:
The momentum and energy equations also apply to the motions of objects that begin together and then move apart. For example, an explosion is the result of a chain reaction that transforms potential energy stored in chemical, mechanical, or nuclear form into kinetic energy, acoustic energy, and electromagnetic radiation. Rockets also make use of conservation of momentum: propellant is thrust outward, gaining momentum, and an equal and opposite momentum is imparted to the rocket.
Multiple dimensions.
Real motion has both direction and velocity and must be represented by a vector. In a coordinate system with axes, velocity has components in the direction, in the direction, in the direction. The vector is represented by a boldface symbol:
Similarly, the momentum is a vector quantity and is represented by a boldface symbol:
The equations in the previous sections, work in vector form if the scalars and are replaced by vectors and . Each vector equation represents three scalar equations. For example,
represents three equations:
The kinetic energy equations are exceptions to the above replacement rule. The equations are still one-dimensional, but each scalar represents the magnitude of the vector, for example,
Each vector equation represents three scalar equations. Often coordinates can be chosen so that only two components are needed, as in the figure. Each component can be obtained separately and the results combined to produce a vector result.
A simple construction involving the center of mass frame can be used to show that if a stationary elastic sphere is struck by a moving sphere, the two will head off at right angles after the collision (as in the figure).
Objects of variable mass.
The concept of momentum plays a fundamental role in explaining the behavior of variable-mass objects such as a rocket ejecting fuel or a star accreting gas. In analyzing such an object, one treats the object's mass as a function that varies with time: . The momentum of the object at time is therefore . One might then try to invoke Newton's second law of motion by saying that the external force on the object is related to its momentum by , but this is incorrect, as is the related expression found by applying the product rule to :
This equation does not correctly describe the motion of variable-mass objects. The correct equation is
where is the velocity of the ejected/accreted mass "as seen in the object's rest frame". This is distinct from , which is the velocity of the object itself as seen in an inertial frame.
This equation is derived by keeping track of both the momentum of the object as well as the momentum of the ejected/accreted mass (dm). When considered together, the object and the mass (dm) constitute a closed system in which total momentum is conserved.
Relativistic mechanics.
Lorentz invariance.
Newtonian physics assumes that absolute time and space exist outside of any observer; this gives rise to the Galilean invariance described earlier. It also results in a prediction that the speed of light can vary from one reference frame to another. This is contrary to observation. In the special theory of relativity, Einstein keeps the postulate that the equations of motion do not depend on the reference frame, but assumes that the speed of light is invariant. As a result, position and time in two reference frames are related by the Lorentz transformation instead of the Galilean transformation.
Consider, for example, a reference frame moving relative to another at velocity in the direction. The Galilean transformation gives the coordinates of the moving frame as
while the Lorentz transformation gives
where is the Lorentz factor:
Newton's second law, with mass fixed, is not invariant under a Lorentz transformation. However, it can be made invariant by making the "inertial mass" of an object a function of velocity:
The modified momentum,
obeys Newton's second law:
Within the domain of classical mechanics, relativistic momentum closely approximates Newtonian momentum: at low velocity, is approximately equal to , the Newtonian expression for momentum.
Four-vector formulation.
In the theory of special relativity, physical quantities are expressed in terms of four-vectors that include time as a fourth coordinate along with the three space coordinates. These vectors are generally represented by capital letters, for example for position. The expression for the "four-momentum" depends on how the coordinates are expressed. Time may be given in its normal units or multiplied by the speed of light so that all the components of the four-vector have dimensions of length. If the latter scaling is used, an interval of proper time, , defined by
is invariant under Lorentz transformations (in this expression and in what follows the metric signature has been used, different authors use different conventions). Mathematically this invariance can be ensured in one of two ways: by treating the four-vectors as Euclidean vectors and multiplying time by Imaginary unit; or by keeping time a real quantity and embedding the vectors in a Minkowski space. In a Minkowski space, the scalar product of two four-vectors and is defined as
In all the coordinate systems, the (contravariant) relativistic four-velocity is defined by
and the (contravariant) four-momentum is
where is the invariant mass. If (in Minkowski space), then
Using Einstein's mass-energy equivalence, , this can be rewritten as
Thus, conservation of four-momentum is Lorentz-invariant and implies conservation of both mass and energy.
The magnitude of the momentum four-vector is equal to :
and is invariant across all reference frames.
The relativistic energy–momentum relationship holds even for massless particles such as photons; by setting it follows that
In a game of relativistic "billiards", if a stationary particle is hit by a moving particle in an elastic collision, the paths formed by the two afterwards will form an acute angle. This is unlike the non-relativistic case where they travel at right angles.
Generalized coordinates.
Newton's laws can be difficult to apply to many kinds of motion because the motion is limited by "constraints". For example, a bead on an abacus is constrained to move along its wire and a pendulum bob is constrained to swing at a fixed distance from the pivot. Many such constraints can be incorporated by changing the normal Cartesian coordinates to a set of "generalized coordinates" that may be fewer in number. Refined mathematical methods have been developed for solving mechanics problems in generalized coordinates. They introduce a "generalized momentum", also known as the "canonical" or "conjugate momentum", that extends the concepts of both linear momentum and angular momentum. To distinguish it from generalized momentum, the product of mass and velocity is also referred to as "mechanical", "kinetic" or "kinematic momentum". The two main methods are described below.
Lagrangian mechanics.
In Lagrangian mechanics, a Lagrangian is defined as the difference between the kinetic energy and the potential energy :
If the generalized coordinates are represented as a vector and time differentiation is represented by a dot over the variable, then the equations of motion (known as the Lagrange or Euler–Lagrange equations) are a set of equations:
If a coordinate is not a Cartesian coordinate, the associated generalized momentum component does not necessarily have the dimensions of linear momentum. Even if is a Cartesian coordinate, will not be the same as the mechanical momentum if the potential depends on velocity. Some sources represent the kinematic momentum by the symbol .
In this mathematical framework, a generalized momentum is associated with the generalized coordinates. Its components are defined as
Each component is said to be the "conjugate momentum" for the coordinate .
Now if a given coordinate does not appear in the Lagrangian (although its time derivative might appear), then
This is the generalization of the conservation of momentum.
Even if the generalized coordinates are just the ordinary spatial coordinates, the conjugate momenta are not necessarily the ordinary momentum coordinates. An example is found in the section on electromagnetism.
Hamiltonian mechanics.
In Hamiltonian mechanics, the Lagrangian (a function of generalized coordinates and their derivatives) is replaced by a Hamiltonian that is a function of generalized coordinates and momentum. The Hamiltonian is defined as
where the momentum is obtained by differentiating the Lagrangian as above. The Hamiltonian equations of motion are
As in Lagrangian mechanics, if a generalized coordinate does not appear in the Hamiltonian, its conjugate momentum component is conserved.
Symmetry and conservation.
Conservation of momentum is a mathematical consequence of the homogeneity (shift symmetry) of space (position in space is the canonical conjugate quantity to momentum). That is, conservation of momentum is a consequence of the fact that the laws of physics do not depend on position; this is a special case of Noether's theorem.
Electromagnetism.
In Newtonian mechanics, the law of conservation of momentum can be derived from the law of action and reaction, which states that every force has a reciprocating equal and opposite force. Under some circumstances, moving charged particles can exert forces on each other in non-opposite directions. Moreover, Maxwell's equations, the foundation of classical electrodynamics, are Lorentz-invariant. Nevertheless, the combined momentum of the particles and the electromagnetic field is conserved.
Vacuum.
In Maxwell's equations, the forces between particles are mediated by electric and magnetic fields. The electromagnetic force ("Lorentz force") on a particle with charge due to a combination of electric field and magnetic field (as given by the "B-field" ) is
This force imparts a momentum to the particle, so by Newton's second law the particle must impart a momentum to the electromagnetic fields.
In a vacuum, the momentum per unit volume is
where is the vacuum permeability and is the speed of light. The momentum density is proportional to the Poynting vector which gives the directional rate of energy transfer per unit area:
If momentum is to be conserved over the volume over a region , changes in the momentum of matter through the Lorentz force must be balanced by changes in the momentum of the electromagnetic field and outflow of momentum. If is the momentum of all the particles in , and the particles are treated as a continuum, then Newton's second law gives
The electromagnetic momentum is
and the equation for conservation of each component of the momentum is
The term on the right is an integral over the surface area of the surface representing momentum flow into and out of the volume, and is a component of the surface normal of . The quantity is called the Maxwell stress tensor, defined as
Media.
The above results are for the "microscopic" Maxwell equations, applicable to electromagnetic forces in a vacuum (or on a very small scale in media). It is more difficult to define momentum density in media because the division into electromagnetic and mechanical is arbitrary. The definition of electromagnetic momentum density is modified to
where the H-field is related to the B-field and the magnetization by
The electromagnetic stress tensor depends on the properties of the media.
Particle in field.
If a charged particle moves in an electromagnetic field, neither its kinetic momentum nor its canonical momentum is conserved.
Lagrangian and Hamiltonian formulation.
The "kinetic momentum" is different from the "canonical momentum" (synonymous with the generalized momentum) conjugate to the ordinary position coordinates , because includes a contribution from the electric potential and vector potential :
where is the velocity (see time derivative), is the electric charge of the particle and is the Lorentz factor. See also Electromagnetism (momentum). If neither nor depends on position, is conserved.
The classical Hamiltonian for a particle in any field equals the total energy of the system – the kinetic energy (where , see dot product) plus the potential energy . For a particle in an electromagnetic field, the potential energy is , and since the kinetic energy always corresponds to the kinetic momentum , replacing the kinetic momentum by the above equation () leads to the Hamiltonian in the table.
These Lagrangian and Hamiltonian expressions can derive the Lorentz force.
Canonical commutation relations.
The kinetic momentum ( above) satisfies the commutation relation:
where: , , are indices labelling vector components, is a component of the magnetic field, and is the Levi-Civita symbol, here in 3 dimensions.
Quantum mechanics.
In quantum mechanics, momentum is defined as a self-adjoint operator on the wave function. The Heisenberg uncertainty principle defines limits on how accurately the momentum and position of a single observable system can be known at once. In quantum mechanics, position and momentum are conjugate variables.
For a single particle described in the position basis the momentum operator can be written as
where is the gradient operator, is the reduced Planck constant, and is the imaginary unit. This is a commonly encountered form of the momentum operator, though the momentum operator in other bases can take other forms. For example, in momentum space the momentum operator is represented as
where the operator acting on a wave function yields that wave function multiplied by the value , in an analogous fashion to the way that the position operator acting on a wave function yields that wave function multiplied by the value "x".
For both massive and massless objects, relativistic momentum is related to the phase constant formula_63 by 
Electromagnetic radiation (including visible light, ultraviolet light, and radio waves) is carried by photons.Even though photons (the particle aspect of light) have no mass, they still carry momentum. This leads to applications such as the solar sail. The calculation of the momentum of light within dielectric media is somewhat controversial (see Abraham–Minkowski controversy).
Deformable bodies and fluids.
Conservation in a continuum.
In fields such as fluid dynamics and solid mechanics, it is not feasible to follow the motion of individual atoms or molecules. Instead, the materials must be approximated by a continuum in which there is a particle or fluid parcel at each point that is assigned the average of the properties of atoms in a small region nearby. In particular, it has a density and velocity that depend on time and position . The momentum per unit volume is .
Consider a column of water in hydrostatic equilibrium. All the forces on the water are in balance and the water is motionless. On any given drop of water, two forces are balanced. The first is gravity, which acts directly on each atom and molecule inside. The gravitational force per unit volume is , where is the gravitational acceleration. The second force is the sum of all the forces exerted on its surface by the surrounding water. The force from below is greater than the force from above by just the amount needed to balance gravity. The normal force per unit area is the pressure . The average force per unit volume inside the droplet is the gradient of the pressure, so the force balance equation is
If the forces are not balanced, the droplet accelerates. This acceleration is not simply the partial derivative because the fluid in a given volume changes with time. Instead, the material derivative is needed:
Applied to any physical quantity, the material derivative includes the rate of change at a point and the changes due to advection as fluid is carried past the point. Per unit volume, the rate of change in momentum is equal to . This is equal to the net force on the droplet.
Forces that can change the momentum of a droplet include the gradient of the pressure and gravity, as above. In addition, surface forces can deform the droplet. In the simplest case, a shear stress , exerted by a force parallel to the surface of the droplet, is proportional to the rate of deformation or strain rate. Such a shear stress occurs if the fluid has a velocity gradient because the fluid is moving faster on one side than another. If the speed in the direction varies with , the tangential force in direction per unit area normal to the direction is
where is the viscosity. This is also a flux, or flow per unit area, of x-momentum through the surface.
Including the effect of viscosity, the momentum balance equations for the incompressible flow of a Newtonian fluid are
These are known as the Navier–Stokes equations.
The momentum balance equations can be extended to more general materials, including solids. For each surface with normal in direction and force in direction , there is a stress component . The nine components make up the Cauchy stress tensor , which includes both pressure and shear. The local conservation of momentum is expressed by the Cauchy momentum equation:
where is the body force.
The Cauchy momentum equation is broadly applicable to deformations of solids and liquids. The relationship between the stresses and the strain rate depends on the properties of the material (see Types of viscosity).
Acoustic waves.
A disturbance in a medium gives rise to oscillations, or waves, that propagate away from their source. In a fluid, small changes in pressure can often be described by the acoustic wave equation:
where is the speed of sound. In a solid, similar equations can be obtained for propagation of pressure (P-waves) and shear (S-waves).
The flux, or transport per unit area, of a momentum component by a velocity is equal to . In the linear approximation that leads to the above acoustic equation, the time average of this flux is zero. However, nonlinear effects can give rise to a nonzero average. It is possible for momentum flux to occur even though the wave itself does not have a mean momentum.
History of the concept.
In about 530 A.D., working in Alexandria, Byzantine philosopher John Philoponus developed a concept of momentum in his commentary to Aristotle's "Physics".
Aristotle claimed that everything that is moving must be kept moving by something. For example, a thrown ball must be kept moving by motions of the air. Most writers continued to accept Aristotle's theory until the time of Galileo, but a few were skeptical. Philoponus pointed out the absurdity in Aristotle's claim that motion of an object is promoted by the same air that is resisting its passage. He proposed instead that an impetus was imparted to the object in the act of throwing it. Ibn Sīnā (also known by his Latinized name Avicenna) read Philoponus and published his own theory of motion in "The Book of Healing" in 1020. He agreed that an impetus is imparted to a projectile by the thrower; but unlike Philoponus, who believed that it was a temporary virtue that would decline even in a vacuum, he viewed it as a persistent, requiring external forces such as air resistance to dissipate it. 
The work of Philoponus, and possibly that of Ibn Sīnā, was read and refined by the European philosophers Peter Olivi and Jean Buridan. Buridan, who in about 1350 was made rector of the University of Paris, referred to impetus being proportional to the weight times the speed. Moreover, Buridan's theory was different from his predecessor's in that he did not consider impetus to be self-dissipating, asserting that a body would be arrested by the forces of air resistance and gravity which might be opposing its impetus.
René Descartes believed that the total "quantity of motion" in the universe is conserved, where the quantity of motion is understood as the product of size and speed. This should not be read as a statement of the modern law of momentum, since he had no concept of mass as distinct from weight and size, and more importantly he believed that it is speed rather than velocity that is conserved. So for Descartes if a moving object were to bounce off a surface, changing its direction but not its speed, there would be no change in its quantity of motion. Galileo, later, in his "Two New Sciences", used the Italian word "impeto".
Leibniz, in his "Discourse on Metaphysics", gave an argument against Descartes' construction of the conservation of the "quantity of motion" using an example of dropping blocks of different sizes different distances. He points out that force is conserved but quantity of motion, construed as the product of size and speed of an object, is not conserved.
The first correct statement of the law of conservation of momentum was by English mathematician John Wallis in his 1670 work, "Mechanica sive De Motu, Tractatus Geometricus": "the initial state of the body, either of rest or of motion, will persist" and "If the force is greater than the resistance, motion will result". Wallis uses "momentum" and "vis" for force. Newton's "Philosophiæ Naturalis Principia Mathematica", when it was first published in 1687, showed a similar casting around for words to use for the mathematical momentum. His Definition II defines "quantitas motus", "quantity of motion", as "arising from the velocity and quantity of matter conjointly", which identifies it as momentum. Thus when in Law II he refers to "mutatio motus", "change of motion", being proportional to the force impressed, he is generally taken to mean momentum and not motion. It remained only to assign a standard term to the quantity of motion. The first use of "momentum" in its proper mathematical sense is not clear but by the time of Jenning's "Miscellanea" in 1721, four years before the final edition of Newton's "Principia Mathematica", momentum or "quantity of motion" was being defined for students as "a rectangle", the product of and , where is "quantity of material" and is "velocity", .

</doc>
<doc id="20432" url="https://en.wikipedia.org/wiki?curid=20432" title="Mood stabilizer">
Mood stabilizer

A mood stabilizer is a psychiatric pharmaceutical drug used to treat mood disorders characterized by intense and sustained mood shifts, typically bipolar disorder type I or type II or schizophrenia.
Uses.
Used to treat bipolar disorder, mood stabilizers suppress swings between mania and depression. Mood-stabilizing drugs are also used in borderline personality disorder and schizoaffective disorder.
Examples.
The term "mood stabilizer" does not describe a mechanism, but rather an effect. More precise terminology is used to classify these agents.
Drugs commonly classed as mood stabilizers include:
Anticonvulsants.
Many agents described as "mood stabilizers" are also categorized as anticonvulsants. The term "anticonvulsant mood stabilizers" is sometimes used to describe these as a class. Although this group is also defined by effect rather than mechanism, there is at least a preliminary understanding of the mechanism of most of the anticonvulsants used in the treatment of mood disorders.
There is insufficient evidence to support the use of various other anticonvulsants, such as gabapentin and topiramate, as mood stabilizers.
Combination therapy.
In routine practice, monotherapy is often not sufficiently effective for acute and/or maintenance therapy and thus most patients are given combination therapies. Combination therapy (atypical antipsychotic with lithium or valproate) shows better efficacy over monotherapy in the manic phase in terms of efficacy and prevention of relapse. However, side effects are more frequent and discontinuation rates due to adverse events are higher with combination therapy than with monotherapy.
Relationship to antidepressants.
Most mood stabilizers are primarily antimanic agents, meaning that they are effective at treating mania and mood cycling and shifting, but are not effective at treating acute depression. The principal exceptions to that rule, because they treat both manic and depressive symptoms, are lamotrigine, lithium carbonate and quetiapine.
Nevertheless, antidepressants are still often prescribed in addition to mood stabilizers during depressive phases. This brings some risks, however, as antidepressants can induce mania, psychosis, and other disturbing problems in people with bipolar disorder—in particular, when taken alone. The risk of antidepressant-induced mania when given to patients concomitantly on antimanic agents is not known for certain but may still exist. The majority of antidepressants appear ineffective in treating bipolar depression.
Antidepressants cause several risks when given to bipolar patients. They are ineffective in treating acute bipolar depression, preventing relapse, and can cause rapid cycling. Studies have been shown that antidepressants have no benefit versus a placebo or other treatment. Antidepressants can also lead to a higher rate of non-lethal suicidal behavior. Relapse can also be related to treatment with antidepressants. This is less likely to occur if a mood stabilizer is combined with an antidepressant, rather than an antidepressant being used alone. Evidence from previous studies shows that rapid cycling is linked to use of antidepressants. Rapid cycling is defined as the presence of four or more mood episodes within a year's time. Evidence suggests that rapid cycling and mixed symptoms have become more common since antidepressant medication has come into widespread use. There is a need for caution when treating bipolar patients with antidepressant medication due to the risks that they pose. 
Use of mood stabilizers and anticonvulsants such as lamotrigine, carbamazapine, valproate and others may lead to chronic folate deficiency, potentiating depression. Also, "Folate deficiency may increase the risk of depression and reduce the action of antidepressants." L-methylfolate (also formally known as 5-MTHF or Levofolinic acid), a centrally acting trimonoamine modulator, boosts the synthesis of three CNS neurotransmitters: dopamine, norepinephrine and serotonin. Mood stabilizers and anticonvulsants may interfere with folic acid absorption and L-methylfolate formation. Augmentation with the medical food L-methylfolate may improve antidepressant effects of these medicines, including lithium and antidepressants themselves, by boosting the synthesis of antidepressant neurotransmitters. However, the U.S. National Institutes of Health issued a warning caution about the use of L-methylfolate for patients with bipolar disease.
Mechanism.
The precise mechanism of action of lithium is still unknown, and it is suspected that it acts at various points of the neuron between the nucleus and the synapse. Lithium is known to inhibit the enzyme GSK-3B. This has the effect relieving pressure on the circadian clock - which is thought to be often malfunctioning in people with bipolar disorder - and positively modulates gene transcription of brain-derived neurotrophic factor (BDNF). The resulting increase in neural plasticity may be central to lithium's therapeutic effects. Lithium may also increase the synthesis of serotonin.
All of the anticonvulsants routinely used to treat bipolar disorder are blockers of voltage-gated sodium channels, affecting the brain's glutamate system. For valproic acid, carbamazepine and oxcarbazepine, however, their mood-stabilizing effects may be more related to effects on the GABAergic system. Lamotrigine is known to decrease the patient's cortisol response to stress.
One possible downstream target of several mood stabilizers such as lithium, valproate, and carbamazepine is the arachidonic acid cascade.

</doc>
<doc id="20433" url="https://en.wikipedia.org/wiki?curid=20433" title="Mere Christianity">
Mere Christianity

Mere Christianity is a theological book by C. S. Lewis, adapted from a series of BBC radio talks made between 1942 and 1944, while Lewis was at Oxford during World War II. Considered a classic of Christian apologetics, the transcripts of the broadcasts originally appeared in print as three separate pamphlets: "The Case for Christianity" (1942), "Christian Behaviour" (1943), and "Beyond Personality" (1944). Lewis was invited to give the talks by Rev. James Welch, the BBC Director of Religious Broadcasting, who had read his 1940 book, "The Problem of Pain".
Thesis.
Lewis, an Anglican, intended to describe the Christian common ground. In "Mere Christianity", he aims at avoiding controversies to explain fundamental teachings of Christianity, for the sake of those basically educated as well as the intellectuals of his generation, for whom the jargon of formal Christian theology did not retain its original meaning.
"The Case for Christianity" ("Broadcast Talks" in UK).
Lewis spends most of his defense of the Christian faith on an argument from morality, a point which persuaded him from atheism to Christianity. He bases his case on a moral law, a "rule about right and wrong" commonly known to all human beings, citing the example of Nazism; both Christians and atheists believed that Hitler's actions were morally wrong. On a more mundane level, it is generally accepted that stealing is violating this moral law. Lewis argues that the moral law is like the law of nature in that it was not contrived by humans. However, it is unlike natural laws in that it can be broken or ignored, and it is known intuitively, rather than through observation. After introducing the moral law, Lewis argues that thirst reflects the fact that people naturally need water, and there is no other substance which satisfies that need. Lewis points out that earthly experience does not satisfy the human craving for "joy" and that only God could fit the bill; humans cannot know to yearn for something if it does not exist.
After providing reasons for his conversion to theism, Lewis goes over rival conceptions of God to Christianity. Pantheism, he argues, is incoherent, and atheism too simple. Eventually he arrives at Jesus Christ, and invokes a well-known argument now known as the "Lewis trilemma". Lewis, arguing that Jesus was claiming to be God, uses logic to advance three possibilities: either he really was God, was deliberately lying, or was not God but thought himself to be (which would make him delusional and likely insane). The book goes on to say that the latter two possibilities are not consistent with Jesus' character and it was most likely that he was being truthful.
Lewis claims that to understand Christianity, one must understand the moral law, which is the underlying structure of the universe and is "hard as nails." Unless one grasps the dismay which comes from humanity's failure to keep the moral law, one cannot understand the coming of Christ and his work. The eternal God who is the law's source takes primacy over the created Satan whose rebellion undergirds all evil. The death and resurrection of Christ is introduced as the only way in which our inadequate human attempts to redeem humanity's sins could be made adequate in God's eyes.
God "became a man" in Christ, Lewis says, so that mankind could be "amalgamated with God's nature" and make full atonement possible. Lewis offers several analogies to explain this abstract concept: that of Jesus "paying the penalty" for a crime, "paying a debt," or helping humanity out of a hole. His main point, however, is that redemption is so incomprehensible that it cannot be fully appreciated, and he attempts to explain that the method by which God atones for the sins of humanity is not nearly as important as the fact that he does so.
"Christian Behaviour".
The next third of the book explores the ethics resulting from Christian belief. He cites the four cardinal virtues: prudence, justice, temperance, and fortitude. After touching on these, he goes into the three theological virtues: hope, faith, and charity. Lewis also explains morality as being composed of three "layers": relationships between man and man, the motivations and attitudes of the man himself, and contrasting worldviews.
Lewis also covers such topics as social relations and forgiveness, sexual ethics and the tenets of Christian marriage, and the relationship between morality and psychoanalysis. He also writes about "the great sin": pride, which he argues to be the root cause of all evil and rebellion.
His most important point is that Christianity mandates that one "love your neighbor as yourself." He points out that all persons unconditionally love themselves. Even if one does not "like" oneself, one would still love oneself. Christians, he writes, must also apply this attitude to others, even if they do not like them. Lewis calls this one of the "great secrets": when one acts as if he loves others, he will presently come to love them.
Cultural impact.
In 2006, "Mere Christianity" was placed third in "Christianity Today"'s list of the most influential books amongst evangelicals since 1945. The title has influenced "Touchstone Magazine: A Journal of Mere Christianity" and William Dembski's book "Mere Creation". Charles Colson's conversion to Christianity resulted from his reading this book, as did the conversions of Francis Collins, Jonathan Aitken, Josh Caterer and the philosopher C. E. M. Joad.
A passage in the book also influenced the name of contemporary Christian Texan Grammy-nominated pop/rock group Sixpence None the Richer. The phrase, "the hammering process" was used by Christian metal band Living Sacrifice for the name of their album "The Hammering Process". Metalcore band, Norma Jean, derived the title of their song "No Passenger: No Parasite" from the section in the book in which Lewis describes a fully Christian society as having "No passengers or parasites".

</doc>
<doc id="20434" url="https://en.wikipedia.org/wiki?curid=20434" title="Mathematical game">
Mathematical game

A mathematical game is a game whose rules, strategies, and outcomes are defined by clear mathematical parameters. Often, such games have simple rules and match procedures, such as Tic-tac-toe and Dots and Boxes. Generally, mathematical games need not be conceptually intricate to involve deeper computational underpinnings. For example, even though the rules of Mancala are relatively basic, the game can be rigorously analyzed through the lens of combinatorial game theory.
Mathematical games differ sharply from mathematical puzzles in that mathematical puzzles require specific mathematical expertise to complete, whereas mathematical games do not require a deep knowledge of mathematics to play. Often, the arithmetic core of mathematical games is not readily apparent to players untrained to note the statistical or mathematical aspects.
Some mathematical games are of deep interest in the field of recreational mathematics.
When studying a game's core mathematics, arithmetic theory is generally of higher utility than actively playing or observing the game itself. To analyze a game numerically, it is particularly useful to study the rules of the game insofar as they can yield equations or relevant formulas. This is frequently done to determine winning strategies or to distinguish if the game has a solution.
Specific mathematical games and puzzles.
Abstract Strategy Games (No chance involved).
Sometimes it is not immediately obvious that a particular game involves chance. Often a card game is described as "pure strategy" and such, but a game with any sort of random shuffling or face-down dealing of cards should not be considered to be "no chance". Several abstract strategy games are listed below:

</doc>
<doc id="20435" url="https://en.wikipedia.org/wiki?curid=20435" title="Martin Gardner">
Martin Gardner

Martin Gardner (October 21, 1914May 22, 2010) was an American popular mathematics and popular science writer, with interests also encompassing micromagic, scientific skepticism, philosophy, religion, and literature—especially the writings of Lewis Carroll, L. Frank Baum, and G. K. Chesterton.
Gardner was best known for creating and sustaining general interest in recreational mathematics for a large part of the 20th century, principally through his "Scientific American" "Mathematical Games" columns from 1956 to 1981 and his subsequent books collecting them. He was an uncompromising critic of fringe science and was a founding member of CSICOP, an organization devoted to debunking pseudoscience, and wrote a monthly column ("Notes of a Fringe Watcher") from 1983 to 2002 in "Skeptical Inquirer", that organization's monthly magazine. He also wrote a "Puzzle Tale" column for "Asimov's Science Fiction" magazine from 1977 to 1986 and altogether published more than 100 books.
Gardner was considered a leading authority on Lewis Carroll. "The Annotated Alice: The Definitive Edition" (Norton, 1999) has been Gardner's most successful, selling over half a million copies.
Gardner believed in a personal God, an afterlife, and prayer, but rejected established religion. He considered himself a philosophical theist and fideist.
Biography.
Youth and education.
Gardner, son of a petroleum geologist, grew up in and around Tulsa, Oklahoma. He showed an early interest in puzzles and games, and his closest childhood friend, John Bennett Shaw, later became "the greatest of all collectors of Sherlockian memorabilia". He attended the University of Chicago, where he earned his bachelor's degree in philosophy in 1936. Early jobs included reporter on the "Tulsa Tribune", writer at the University of Chicago Office of Press Relations, and case worker in Chicago's Black Belt for the city's Relief Administration. During World War II, he served for four years in the U.S. Navy as a yeoman on board the destroyer escort USS "Pope" in the Atlantic. His ship was still in the Atlantic when the war came to an end with the surrender of Japan in August 1945.
After the war, Gardner returned to the University of Chicago. He attended graduate school for a year there, but he did not earn an advanced degree. In 1950 he published an article in the "Antioch Review" entitled "The Hermit Scientist", a pioneering work on what would later come to be called the pseudoscientist. It was Gardner's first publication of a skeptical nature, and two years later it was published in a much-expanded book version: "In the Name of Science", his first book.
Early career.
In the late 1940s, Gardner moved to New York City and became a writer and designer at "Humpty Dumpty" magazine where for eight years he wrote features and stories for it and several other children's magazines. His paper-folding puzzles at that magazine (sister publication to "Children's Digest" at the time, and now sister publication to "Jack and Jill" magazine) led to his first work at "Scientific American." For many decades, Gardner, his wife Charlotte, and their two sons lived in Hastings-on-Hudson, New York, where he earned his living as an independent author, publishing books with several different publishers, and also publishing hundreds of magazine and newspaper articles. Appropriately enough – given his interest in logic and mathematics – they lived on Euclid Avenue. The year 1960 saw the original edition of his best-selling book ever, "The Annotated Alice", various editions of which have sold over a million copies worldwide in several languages.
Gatherings for Gardner.
Gardner was famously shy and declined many honors when he learned that a public appearance would be required if he accepted. (He once told Colm Mulcahy that he "never gave a lecture in his life and that he wouldn't know how to.") However, in 1993 Atlanta puzzle collector Tom Rodgers persuaded Gardner to attend an evening devoted to Gardner's puzzle-solving efforts, called "Gathering for Gardner". The event was repeated in 1996, again with Gardner in attendance, which convinced Rodgers and his friends to make the gathering a regular event. It has been held since then in even-numbered years near Atlanta, and the program consists of any topic which could have been touched by Gardner during his writing career. The event's name is abbreviated to "G4G"n"", with "n" being replaced by the number of the event (the 2010 event thus was "G4G9"). Gardner attended the 1993 and 1996 events.
Retirement and death.
In 1979, Gardner and his wife Charlotte semi-retired and moved to Hendersonville, North Carolina. Gardner never really retired as an author, but rather he continued to do literature research and to write, especially in updating many of his older books, such as "Origami, Eleusis, and the Soma Cube", ISBN 978-0-521-73524-7, published 2008. Charlotte died in 2000 and two years later Gardner returned to Norman, Oklahoma, where his son, James Gardner, was a professor of education at the University of Oklahoma. He died there on May 22, 2010. An autobiography — "Undiluted Hocus-Pocus: The Autobiography of Martin Gardner" — was published posthumously.
Recreational mathematics and "Mathematical Games".
For over a quarter century Gardner wrote a monthly column on the subject of "recreational mathematics" for "Scientific American". It all began with his free-standing article on hexaflexagons which ran in the December 1956 issue. Flexagons became a bit of a fad and soon people all over New York City were making them. Gerry Piel, the "SA" publisher at the time asked Gardner, "Is there enough similar material to this to make a regular feature?" Gardner said he thought so. The January 1957 issue contained his first column, entitled "Mathematical Games". Almost 300 more columns were to follow.
The "Mathematical Games" column ran from 1956 to 1981 and was the first introduction of many subjects to a wider audience, notably:
Ironically, Gardner had problems learning calculus and never took a mathematics course after high school. While editing "Humpty Dumpty's Magazine" he constructed many paper folding puzzles, and this led to his interest in the flexagons invented by British mathematician Arthur H Stone. The subsequent article he wrote on hexaflexagons led directly to the column.
In the 1980s the "Mathematical Games" column began to appear only irregularly. Other authors began to share the column and the June 1986 issue saw the final installment under that title. In 1981, on Gardner's retirement from "Scientific American", the column was replaced by Douglas Hofstadter's "Metamagical Themas", a name that is an anagram of "Mathematical Games".
Many of the games columns were collected in book form starting in 1959 with "The Scientific American Book of Mathematical Puzzles & Diversions". Over the next four decades fourteen more books followed. Donald Knuth called them the .
Pseudoscience and skepticism.
Gardner's uncompromising attitude toward pseudoscience made him one of the foremost anti-pseudoscience polemicists of the 20th century. The evolutionary biologist Stephen Jay Gould called Gardner "the single brightest beacon defending rationality and good science against the mysticism and anti-intellectualism that surround us." His book "Fads and Fallacies in the Name of Science" (1952, revised 1957) is a classic and seminal work of the skeptical movement. It explored and debunked myriad dubious movements and theories, including Fletcherism, creationism, food faddism, Charles Fort, Rudolf Steiner, Scientology, Dianetics, UFOs, dowsing, extra-sensory perception, the Bates method, and psychokinesis. This book and his subsequent efforts ("Science: Good, Bad and Bogus", 1981; "Order and Surprise", 1983, "Gardner's Whys & Wherefores", 1989, etc.) earned him a wealth of detractors and antagonists in the fields of "fringe science" and New Age philosophy, with many of whom he kept up running dialogues (both public and private) for decades.
Gardner was a relentless critic of self-proclaimed Israeli psychic Uri Geller and wrote two satirical exposes of him in the 1970s using the pen name "Uriah Fuller". In two booklets he wrote showing how purported psychics — such as Uri Geller do their "seemingly impossible paranormal feats" (mentally bending spoons, "reading minds", etc.). The booklets are:
both published by Karl Fulves of Teaneck, New Jersey.
In 1976 Gardner joined with Carl Sagan, Isaac Asimov and others in founding the Committee for the Scientific Investigation of Claims of the Paranormal (CSICOP). He wrote a column called "Notes of a Fringe Watcher" (originally "Notes of a Psi-Watcher") from 1983 to 2002 for that organization's periodical "Skeptical Inquirer". These have been collected in five books:
Gardner was a senior CSICOP fellow and prominent skeptic of the paranormal.
On August 21, 2010, Gardner was posthumously honored with an award recognizing his contributions in the skeptical field from the Independent Investigations Group during its 10th Anniversary Gala.
Micromagic.
Gardner's personal interest in magic focused mainly upon micromagic (table or close-up magic) and, from the 1930s on, he published a significant number of original contributions to this secretive field. He was well known for his innovative tapping and spelling effects, with and without playing cards, and was most proud of the effect he called the "Wink Change". 
In 1999, "Magic" magazine named Gardner one of the "100 Most Influential Magicians of the Twentieth Century". The last publication in his lifetime was a magic trick that Gardner contributed to the May 2010 issue of "".
Theism and religion.
Gardner had an abiding fascination with religious belief. He was a fideistic theist, professing belief in one God as Creator, but critical of organized religion. In his autobiography, Gardner stated: "When many of my fans discovered that I believed in God and even hoped for an afterlife, they were shocked and dismayed... I do not mean the God of the Bible, especially the God of the Old Testament, or any other book that claims to be divinely inspired. For me God is a "Wholly Other" transcendent intelligence, impossible for us to understand. He or she is somehow responsible for our universe and capable of providing, how I have no inkling, an afterlife."
He described his own belief as philosophical theism inspired by the theology of the philosopher Miguel de Unamuno. While eschewing systematic religious doctrine, Gardner believed in God, asserting that this belief cannot be confirmed or disconfirmed by reason or science. At the same time, he was skeptical of claims that any god has communicated with human beings through spoken or telepathic revelation or through miracles in the natural world.
He has been quoted as saying that he regarded parapsychology and other research into the paranormal as tantamount to "tempting God" and seeking "signs and wonders". He stated that while he would expect tests on the efficacy of prayers to be negative, he would not rule out "a priori" the possibility that as yet unknown paranormal forces may allow prayers to influence the physical world.
Gardner wrote repeatedly about what public figures such as Robert Maynard Hutchins, Mortimer Adler, and William F. Buckley, Jr. believed and whether their beliefs were logically consistent. In some cases, he attacked prominent religious figures such as Mary Baker Eddy on the grounds that their claims are unsupportable. His semi-autobiographical novel "The Flight of Peter Fromm" depicts a traditionally Protestant Christian man struggling with his faith, examining 20th century scholarship and intellectual movements and ultimately rejecting Christianity while remaining a theist.
Gardner said that he suspected that the fundamental nature of human consciousness may not be knowable or discoverable, unless perhaps a physics more profound than ("underlying") quantum mechanics is some day developed. In this regard, he said, he was an adherent of the "New Mysterianism".
Literary criticism and fiction.
Gardner was considered a leading authority on Lewis Carroll. His annotated version of "Alice's Adventures in Wonderland" and "Through the Looking Glass", explaining the many mathematical riddles, wordplay, and literary references found in the Alice books, was first published as "The Annotated Alice" (Clarkson Potter, 1960), a sequel published with new annotations as "More Annotated Alice" (Random House, 1990), and finally as "The Annotated Alice: The Definitive Edition" (Norton, 1999) combining notes from the earlier editions and new material. The book arose when Gardner, who found the Alice books 'sort of frightening' when he was young but found them fascinating as an adult, felt that someone ought to annotate them and suggested to a publisher that Bertrand Russell be asked; when the publisher did not manage to get past Russell's secretary, Gardner was asked to take the project.
Gardner's interest in wordplay led him to conceive of a magazine on recreational linguistics. In 1967 he pitched the idea to Greenwood Periodicals and nominated Dmitri Borgmann as editor. The resulting journal, "Word Ways", carried many articles from Gardner; as of 2013 it was still publishing his submissions posthumously.
In addition to the 'Alice' books, Gardner produced "Annotated" editions of G. K. Chesterton’s "The Innocence Of Father Brown" and "The Man Who Was Thursday" as well as of celebrated poems including "The Rime of the Ancient Mariner", "Casey at the Bat", "The Night Before Christmas", and "The Hunting of the Snark"; the last also written by Lewis Carroll.
Gardner occasionally tried his hand at fiction of a kind always closely associated with his non-fictional preoccupations. His "roman à clef" novel was "The Flight of Peter Fromm" (1973) and his short stories were collected in "The No-Sided Professor and Other Tales of Fantasy, Humor, Mystery, and Philosophy" (1987). Gardner published stories about an imaginary numerologist named Dr. Matrix and "Visitors from Oz" (1998), based on L. Frank Baum's Oz books, which reflected his love of Oz. (He was a founding member of the International Wizard of Oz Club, and winner of its 1971 L. Frank Baum Memorial Award.)
Gardner was a member of the all-male literary banqueting club, the Trap Door Spiders, which served as the basis of Isaac Asimov's fictional group of mystery solvers, the Black Widowers.
Philosophy of mathematics.
Gardner was known for his sometimes controversial philosophy of mathematics. He wrote negative reviews of "The Mathematical Experience" by Philip J. Davis and Reuben Hersh and "What Is Mathematics, Really?" by Hersh, both of which were critical of aspects of mathematical Platonism, and the first of which was well received by the mathematical community. While Gardner was often perceived as a hard-core Platonist, his reviews demonstrated some formalist tendencies. Gardner maintained that his views are widespread among mathematicians, but Hersh has countered that in his experience as a professional mathematician and speaker, this is not the case.
Other views.
Over the years Gardner held forth on many contemporary issues, arguing for his points of view in a wide range of fields, from general semantics to fuzzy logic to watching TV (he once wrote a negative review of Jerry Mander's book "Four Arguments for the Elimination of Television"). His philosophical views are described and defended in his book "The Whys of a Philosophical Scrivener" (1983, revised 1999). Under the pseudonym "George Groth", Gardner panned his own book for the "New York Review of Books".

</doc>
<doc id="20436" url="https://en.wikipedia.org/wiki?curid=20436" title="MIDI timecode">
MIDI timecode

MIDI time code (MTC), or MIDI time division, embeds the same timing information as standard SMPTE timecode as a series of small 'quarter-frame' MIDI messages. There is no provision for the user bits in the standard MIDI time code messages, and SysEx messages are used to carry this information instead. The quarter-frame messages are transmitted in a sequence of eight messages, thus a complete timecode value is specified every two frames. If the MIDI data stream is running close to capacity, the MTC data may arrive a little behind schedule which has the effect of introducing a small amount of jitter. In order to avoid this it is ideal to use a completely separate MIDI port for MTC data. Larger full-frame messages, which encapsulate a frame worth of timecode in a single message, are used to locate to a time while timecode is not running.
Unlike standard SMPTE timecode, MIDI timecode's quarter-frame and full-frame messages carry a two-bit flag value that identifies the rate of the timecode, specifying it as either:
MTC distinguishes between film speed and video speed only by the rate at which timecode advances, not by the information contained in the timecode messages; thus, 29.97 frame/s dropframe is represented as 30 frame/s dropframe at 0.1% pulldown.
MTC allows the synchronisation of a sequencer or DAW with other devices that can synchronise to MTC or for these devices to 'slave' to a tape machine that is striped with SMPTE. For this to happen a SMPTE to MTC converter needs to be employed. It is possible for a tape machine to synchronise to an MTC signal (if converted to SMPTE), if the tape machine is able to 'slave' to incoming timecode via motor control, which is a rare feature.
Time code format.
The MIDI time code is 32 bits long, of which 24 are used, while 8 bits are unused and always zero. Because the full-time code messages requires that the most significant bits of each byte are zero (valid MIDI data bytes), there are really only 28 available bits and 4 spare bits.
Like most audiovisual timecodes such as SMPTE time code, it encodes only time of day, repeating each 24 hours. Time is given in units of hours, minutes, seconds, and frames. There may be 24, 25, or 30 frames per second.
Each component is assigned one byte:
Full time code.
When there is a jump in the time code, a single full-time code is sent to synchronize attached equipment. This takes the form of a special global system exclusive message:
The manufacturer ID of codice_10 indicates a real-time universal message, the channel of codice_10 indicates it is a global broadcast. The following ID of codice_12 identifies this is a time code type message, and the second codice_12 indicates it is a full-time code message. The 4 bytes of time code follow. Although MIDI is generally little-endian, the 4 time code bytes follow in big-endian order, followed by a codice_14 "end of exclusive" byte.
After a jump, the time clock stops until the first following quarter-frame message is received.
Quarter-frame messages.
When the time is running continuously, the 32-bit time code is broken into 8 4-bit pieces, and one piece is transmitted each quarter frame. I.e. 96—120 times per second, depending on the frame rate. A quarter-frame messages consists of a status byte of 0xF1, followed by a single 7-bit data value: 3 bits to identify the piece, and 4 bits of partial time code. When time is running forward, the piece numbers increment from 0–7; with the time that piece 0 is transmitted is the coded instant, and the remaining pieces are transmitted later.
If the MIDI data stream is being rewound, the time codes count backward. Again, piece 0 is transmitted at the coded moment.
The time code is divided little-endian as follows:

</doc>
<doc id="20437" url="https://en.wikipedia.org/wiki?curid=20437" title="Mass transfer">
Mass transfer

Mass transfer is the net movement of mass from one location, usually meaning stream, phase, fraction or component, to another. Mass transfer occurs in many processes, such as absorption, evaporation, adsorption, drying, precipitation, membrane filtration, and distillation. Mass transfer is used by different scientific disciplines for different processes and mechanisms. The phrase is commonly used in engineering for physical processes that involve diffusive and convective transport of chemical species within physical systems.
Some common examples of mass transfer processes are the evaporation of water from a pond to the atmosphere, the purification of blood in the kidneys and liver, and the distillation of alcohol. In industrial processes, mass transfer operations include separation of chemical components in distillation columns, absorbers such as scrubbers, adsorbers such as activated carbon beds, and liquid-liquid extraction. Mass transfer is often coupled to additional transport processes, for instance in industrial cooling towers. These towers couple heat transfer to mass transfer by allowing hot water to flow in contact with hotter air and evaporate as it absorbs heat from the air.
Astrophysics.
In astrophysics, mass transfer is the process by which matter gravitationally bound to a body, usually a star, fills its Roche lobe and becomes gravitationally bound to a second body, usually a compact object (white dwarf, neutron star or black hole), and is eventually accreted onto it. It is a common phenomenon in binary systems, and may play an important role in some types of supernovae and pulsars.
Chemical engineering.
Mass transfer finds extensive application in chemical engineering problems. It is used in reaction engineering, separations engineering, heat transfer engineering, and many other sub-disciplines of chemical engineering.
The driving force for mass transfer is typically a difference in chemical potential, when it can be defined, though other thermodynamic gradients may couple to the flow of mass and drive it as well. A chemical species moves from areas of high chemical potential to areas of low chemical potential. Thus, the maximum theoretical extent of a given mass transfer is typically determined by the point at which the chemical potential is uniform. For single phase-systems, this usually translates to uniform concentration throughout the phase, while for multiphase systems chemical species will often prefer one phase over the others and reach a uniform chemical potential only when most of the chemical species has been absorbed into the preferred phase, as in liquid-liquid extraction.
While thermodynamic equilibrium determines the theoretical extent of a given mass transfer operation, the actual rate of mass transfer will depend on additional factors including the flow patterns within the system and the diffusivities of the species in each phase. This rate can be quantified through the calculation and application of mass transfer coefficients for an overall process. These mass transfer coefficients are typically published in terms of dimensionless numbers, often including Péclet numbers, Reynolds numbers, Sherwood numbers and Schmidt numbers, among others.
Analogies between heat, mass, and momentum transfer.
There are notable similarities in the commonly used approximate differential equations for momentum, heat, and mass transfer. The molecular transfer equations of Newton's law for fluid momentum at low Reynolds number (Stokes flow), Fourier's law for heat, and Fick's law for mass are very similar, since they are all linear approximations to transport of conserved quantities in a flow field. 
At higher Reynolds number, the analogy between mass and heat transfer and momentum transfer becomes less useful due to the nonlinearity of the Navier-Stokes equation (or more fundamentally, the general momentum conservation equation), but the analogy between heat and mass transfer remains good. A great deal of effort has been devoted to developing analogies among these three transport processes so as to allow prediction of one from any of the others.

</doc>
<doc id="20448" url="https://en.wikipedia.org/wiki?curid=20448" title="Museum of Jurassic Technology">
Museum of Jurassic Technology

The Museum of Jurassic Technology is a museum located at 9341 Venice Boulevard in the Palms district of Los Angeles, California (although it has a postal address of Culver City because it is served by that city's post office). It was founded by David Hildebrand Wilson and Diana Drake Wilson (husband and wife) in 1988.
The museum calls itself "an educational institution dedicated to the advancement of knowledge and the public appreciation of the Lower Jurassic"; the relevance of the term "Lower Jurassic" to the museum's collections is left uncertain and unexplained. The museum's collection includes a mixture of artistic, scientific, ethnographic, and historic, as well as some unclassifiable exhibits, and the diversity of its offerings evokes the cabinets of curiosities that were the 16th-century predecessors of modern natural history museums. The factual claims of many of the museum's exhibits strain credibility, provoking an array of interpretations from commentators. The museum was the subject of a 1995 book by Lawrence Weschler entitled "Mr. Wilson's Cabinet of Wonder: Pronged Ants, Horned Humans, Mice on Toast, and Other Marvels of Jurassic Technology", which describes in detail many of its exhibits. David Hildebrand Wilson received a MacArthur Foundation fellowship in 2001. The museum is also mentioned in the novel "The Museum of Innocence", by Turkish Nobel-laureate Orhan Pamuk.
Overview.
The museum contains an unusual collection of exhibits and objects with varying and uncertain degrees of authenticity. "New York Times" critic Edward Rothstein described it as a "museum about museums", "where the persistent question is: what kind of place is this?" "Smithsonian" magazine called it "a witty, self-conscious homage to private museums of yore . . . when natural history was only barely charted by science, and museums were closer to Renaissance cabinets of curiosity." In a similar vein, "The Economist" said the museum "captures a time chronicled in Richard Holmes's recent book "The Age of Wonder", when science mingled with poetry in its pursuit of answers to life's mysterious questions."
Lawrence Weschler's book, "Mr. Wilson's Cabinet of Wonder: Pronged Ants, Horned Humans, Mice on Toast, And Other Marvels of Jurassic Technology", attempts to explain the mystery of the Museum of Jurassic Technology. Weschler deeply explores the museum through conversations with its founder, David Wilson, and through outside research on several exhibitions. His investigations into the history of certain exhibits led to various results of authenticity; some exhibits seem to have been created by Wilson's imagination while other exhibits might just be displayed in the Natural History Museum. The Museum of Jurassic Technology at its heart, according to Wilson, is "a museum interested in presenting phenomena that other natural history museums are unwilling to present."
The museum's introductory slideshow recounts that, "In its original sense, the term, 'museum' meant '"a spot dedicated to the Muses, a place where man's mind could attain a mood of aloofness above everyday affairs"'". In this spirit, the dimly lit atmosphere, wood and glass vitrines, and labyrinthine floorplan lead visitors through an eclectic range of exhibits on art, natural history, history of science, philosophy, and anthropology, with a special focus on the history of museums and the variety of paths to knowledge. The museum attracts approximately 25,000 visitors per year.
Over the years, the museum has expanded both its exhibitions and other public offerings. In 2005, the museum opened its Tula Tea Room, a Russian-style tea room where Georgian tea, cookies, and crackers are served to patrons. This room is a miniature reconstruction of the study of Tsar Nicolas II from the Winter Palace in St. Petersburg, Russia. The Borzoi Kabinet Theater screens a series of poetic documentaries produced by the Museum of Jurassic Technology in collaboration with the St. Petersburg–based arts and science collective Kabinet. The series of films, entitled "A Chain of Flowers", draws its name from the quote by Charles Willson Peale: "The Learner must be led always from familiar objects toward the unfamiliar, guided along, as it were, a chain of flowers into the mysteries of life". The titles of the films are "Levsha: The Cross-eyed Lefty from Tula and the Steel Flea" (2001), "Obshee Delo: The Common Task" (2005), "Bol'shoe Sovietskaia Zatmenie: The Great Soviet Eclipse" (2008), "The Book of Wisdom and Lies" (2011), and "Language of the Birds" (2012).
Publications.
The museum produces a series of leaflets and books about museum exhibits, including:
Many of these books are published in conjunction with the Society for the Diffusion of Useful Information.
Exhibitions.
The museum maintains over 30 permanent exhibits, including:
From 1992 to 2006, the museum's Foundation Collection was on display in its Tochtermuseum at the Karl Ernst Osthaus-Museum in Hagen, Germany. This exhibition was part of the Museum of Museums wing at the KEOM, which came into being under the stewardship of then-director Michael Fehr.

</doc>
<doc id="20451" url="https://en.wikipedia.org/wiki?curid=20451" title="Men at Work">
Men at Work

Men at Work were an Australian rock band, which formed in 1978. Their founding mainstay was Colin Hay on lead vocals; he formed the group with Jerry Speiser on drums and Ron Strykert on lead guitar. They were joined by Greg Ham on flute and keyboards and then John Rees on bass guitar. This line-up achieved national and international success in the early 1980s. In January 1983, they were the first Australian artists to have a simultaneous No. 1 album and No. 1 single in the United States "Billboard" charts – "Business as Usual" (released on 9 November 1981) and "Down Under" (1981), respectively. With the same works, they achieved the same distinction of a simultaneous No. 1 album and No. 1 single on the Australian, New Zealand and United Kingdom charts. Their second album, "Cargo" (2 May 1983) was also No. 1 in Australia, No. 2 in New Zealand, No. 3 in the US, and No. 8 in the UK. Their third album, "Two Hearts" (3 April 1985), reached the top 20 in Australia and top 50 in the US.
At the Grammy Awards of 1983 they won the Best New Artist category; while at the ARIA Music Awards of 1994 they were inducted into the related Hall of Fame. Men at Work have sold over 30 million albums worldwide. According to Australian musicologist, Ian McFarlane, "of the band's fairytale rise to prominence, [their phenomenal success inextricably created worldwide interest in Australia and Australian music ... simply opened the floodgates with little more than a clutch of great songs" The group disbanded in 1986 and reformed in 1996 to disband again by 2002.
In May 2001 "Down Under" was listed at No. 4 on the APRA Top 30 Australian songs and "Business as Usual" appeared in the book, "100 Best Australian Albums" (October 2010). In February 2010 Larrikin Music Publishing won a case against Hay and Strykert, their record label (Sony BMG Music Entertainment) and music publishing company (EMI Songs Australia) arising from the uncredited appropriation of "Kookaburra" for the flute line in "Down Under". On 19 April 2012 Greg Ham was found dead at his home of an apparent heart attack.
History.
Origins.
Men at Work formed in Melbourne in 1978 by Colin Hay on lead vocals; Jerry Speiser on drums; and Ron Strykert on lead guitar; they were soon joined by Greg Ham on flute and keyboards; and then John Rees on bass guitar. Hay had emigrated to Australia in 1967 from Scotland with his family. In 1978, he formed a duo with Strykert, which expanded by mid-1979 with the addition of Speiser and progressive rocker Greg Sneddon on keyboards (ex-Alroy Band). They formed an unnamed four-piece group. The band's first experience in the recording studio was recording the music to "Riff Raff", a low-budget stage musical on which Sneddon had worked.
Sneddon left and was replaced in late 1979 by Ham, and when Rees joined they adopted the name Men at Work from a construction zone sign near an early venue, The Cricketer's Arms Hotel, Richmond. The band built a "grass roots" reputation as a pub rock band. In 1980 the group issued their debut single, "Keypunch Operator" backed by "Down Under", with both tracks co-written by Hay and Strykert. It was "self-financed" and appeared on their own independent, M. A. W. label. Australian musicologist, Ian McFarlane, felt the A-side was "a fast-paced country-styled rocker with a clean sound and quirky rhythm". Despite not appearing in the top 100 on the Australian Kent Music Report Singles Chart, by the end of that year the group had "grown in stature to become the most in-demand and highly paid, unsigned band of the year".
International success (1981–83).
Early in 1981 Men at Work signed with the Australian branch of Columbia Records on the recommendation of Peter Karpin, the label's A&R person. Fran of the "Woroni" caught their performance at the Refectory in Canberra in April, she noted that they provided "some reggae-ish type music and the minimum of audience attention. From what I saw of them they were perhaps a little bit boring but quite competent and probably deserving of more notice". The group's second single, "Who Can It Be Now?", was released in June 1981 which reached No. 2 and remained in the chart for 24 weeks. It had been produced by United States-based, Peter McIan, who was also working on their debut album, "Business as Usual".
Their next single was a re-worked version of "Down Under", Ham added an improvised flute solo, and the group had revisited its tempo and arrangement with McIan. It appeared in October that year and reached No. 1 in November, where it remained for six weeks. "Business as Usual" was also released in October and went to No. 1 on the Australian Kent Music Report Albums Chart, spending a total of nine weeks at the top spot. "The Canberra Times" Garry Raffaele opined that it "generally stays at a high level, tight and jerky ... There is a delicacy about this music — and that is not a thing you can say about too many rock groups. The flute and reeds of Greg Ham do much to further that". McFarlane noted that "side from the strength of the music, part of the album's appeal was its economy. The production sound was low-key, but clean and uncluttered. Indeed, the songs stood by themselves with little embellishment save for a bright, melodic, singalong quality".
By February the following year both "Down Under" and "Business as Usual" had reached No. 1 on the respective Official New Zealand Music Charts – the latter was the first Australian album to reach that peak in New Zealand. Despite its strong Australian and New Zealand showing, and having an American producer (McIan), "Business as Usual" was twice rejected by Columbia's US parent company. Thanks to the persistence of the band's management and Karpin, the album was finally released in the US and the United Kingdom in April 1982 – six months after its Australian release. Their next single, "Be Good Johnny", was issued in Australia in April 1982 and reached No. 8 in Australia, and No. 3 in New Zealand.
Men at Work initially broke through to North American audiences in the western provinces of Canada with "Who Can It Be Now?" hitting top 10 on radio stations in Winnipeg by May 1982. It peaked at No. 8 on the Canadian "RPM" Top Singles Chart in July. In August the group toured Canada and the US to promote the album and related singles, supporting Fleetwood Mac. The band became more popular on Canadian radio in the following months and also started receiving top 40 US airplay by August. In October "Who Can It Be Now?" reached No. 1 on the US "Billboard" Hot 100, while Canada was one single ahead with "Down Under" topping the Canadian charts that same month. In the following month "Business as Usual" began a 15-week run at No. 1 on the "Billboard" 200.
While "Who Can It Be Now?" was still in the top ten in the US, "Down Under" was finally released in that market. It entered the US charts at No. 79 and ten weeks later, it was No. 1. By January 1983 Men at Work had the top album and single in both the US and the UK – never previously achieved by an Australian act. "Be Good Johnny" received moderate airplay in the US; it reached the top 20 in Canada.
The band released their second album, "Cargo", in April 1983, which also peaked at No. 1 – for two weeks – on the Australian charts. In New Zealand it reached No.2. It had been finished in mid-1982 with McIan producing again, but was held back due to the success of their debut album on the international market, where "Business as Usual" was still riding high. "Cargo" appeared at No. 3 on the "Billboard" 200, and No. 8 in the UK. The lead single, "Overkill", was issued in Australia ahead of the album in October 1982 and reached No. 6, it peaked at No. 3 in the US. "Dr. Heckyll and Mr. Jive" followed in March 1983 made it to No. 5 in Australia, and No. 28 in the US. "It's a Mistake" reached No. 6 in the US. The band toured the world extensively in 1983.
"Two Hearts" to first break-up (1984–86).
During 1984 the band took a break as members pursued other interests. Upon reconvening later that year, tensions during rehearsals between Hay and Speiser over songwriting and the band's management led to a split in the band. Both Rees and Speiser were told they were "not required", as Hay, Ham and Strykert used session musicians to record their third album, "Two Hearts" (23 April 1985). Studio musicians included Jeremy Alsop on bass guitar (ex-Ram Band, Pyramid, Broderick Smith Band); and Mark Kennedy on drums (Spectrum, Ayers Rock, Marcia Hines Band). "Two Hearts" was produced by Hay and Ham, and peaked at No. 16 in Australia, and No. 50 on the US chart. Strykert had left during its production.
Four tracks were released as singles, "Everything I Need" (May 1985), "Man with Two Hearts", "Maria" (August), and "Hard Luck Story" (October); only the lead single charted in Australia (No. 37) and the US (No. 47). The album relied heavily on drum machines and synthesisers, and reduced the presence of Ham's saxophone, giving it a more contemporary feel compared to its predecessors. Hay and Ham hired new band mates, to tour in support of "Two Hearts", with Alsop and Kennedy joined by James Black on guitar and keyboards (Mondo Rock, The Black Sorrows). Soon after a third guitarist, Colin Bayley (Mi-Sex), was added and Kennedy was replaced on drums by Chad Wackerman (Frank Zappa). Australian singers Kate Ceberano and Renée Geyer had also worked on the album and performed live as guest vocalists.
On 13 July 1985 Men at Work performed three tracks for the Oz for Africa concert (part of the global Live Aid program)—"Maria", "Overkill", and an unreleased one, "The Longest Night". They were broadcast in Australia (on both Seven Network and Nine Network) and on MTV in the US. "Maria" and "Overkill" were also broadcast by American Broadcasting Company (ABC) during their Live Aid telecast. Ham left during the band's time touring behind the album. The final Men at Work performances during 1985 had jazz saxophonist Paul Williamson (The Black Sorrows), replacing Ham. By early 1986 Hay disbanded Men at Work and then he started recording his first solo album, "Looking for Jack" (January 1987), which had Alsop and Wackerman as session musicians.
Reunion to second break-up (1996–2002).
By mid-1996, after a ten-year absence, Hay and Ham reformed Men at Work to tour South America. They had enjoyed strong fan support there during their earlier career and demands for a reunion had persisted. The 1996 line up had Stephen Hadley on bass guitar and backing vocals (ex-The Black Sorrows, Paul Kelly Band); Simon Hosford on guitar and backing vocals (Colin Hay backing band); and John Watson on drums (The Black Sorrows). The tour culminated in a performance in São Paulo, which was recorded for the Brazilian release of a live album, "Brazil '96", in 1997, which was co-produced by Hay and Ham for Sony Records. It was re-released worldwide in 1998 as "Brazil" with a bonus track, "The Longest Night", the first new studio track since "Two Hearts".
The band toured Australia, South America, Europe and the US from 1998 to 2000. Other than Hay and Ham the line up for these tours varied, including Rick Grossman of the Hoodoo Gurus on bass guitar for a Brazilian tour, among other touring musicians. Men at Work performed "Down Under" at the closing ceremony of the 2000 Summer Olympics in Sydney, alongside Paul Hogan of ""Crocodile" Dundee" (1986).
One of their European tours for mid-2000 was cancelled and the group had disbanded by 2002, although Hay and Ham periodically reunited Men at Work with guest musicians (including an appearance in February 2009, when they performed "Down Under" at the Australia Unites Victorian Bushfire Appeal Telethon) until Ham's death; his body was found at his home on April 19, 2012. He had reportedly suffered a fatal heart attack.
Copyright lawsuit and controversy.
In February 2010 Larrikin Music Publishing won a case against Hay and Strykert, their record label (Sony BMG Music Entertainment) and music publishing company (EMI Songs Australia) arising from the uncredited appropriation of "Kookaburra", originally written in 1934 by Marion Sinclair and for which Larrikin owned the publishing rights, as the flute line in the Men at Work song, "Down Under". Back in early 2009 the Australian music-themed TV quiz, "Spicks and Specks", had posed a question which suggested that "Down Under" contained elements of "Kookaburra".
Larrikin, headed by Norman Lurie (now retired), then filed suit after Larrikin was sold to another company and had demanded between 40% and 60% of the previous six years of earnings from the song. In February 2010 the judge ruled that "Down Under" did contain a flute riff based on "Kookaburra" but stipulated that neither was it necessarily the hook nor a substantial part of the hit song (Hay and Strykert had written the track years before the flute riff was added by Ham). In July 2010 a judge ruled that Larrikin should be paid 5% of past (since 2002) and future profits.
Other projects.
Hay maintained a solo career and played with Ringo Starr & His All-Starr Band. Strykert relocated to Hobart in 2009 from Los Angeles, and continued to play music and released his first solo album, "Paradise", in September that year. He expressed resentment towards Hay, mainly over royalties. Ham remained musically active and played sax with the Melbourne-based group The Nudist Funk Orchestra until his death. Rees was a music teacher in Melbourne and also played the violin and bass guitar for the band Beggs 2 Differ. Speiser played drums for the band, The Afterburner.
Awards and nominations.
The group won the 1983 Grammy Award for Best New Artist; the other nominees were Asia, Jennifer Holliday, The Human League and Stray Cats. In August 1983 they ware given a Crystal Globe Award for $100 million worth of record business by their US label. That same year in Canada they were awarded a Juno Award for "International LP of the Year". Men at Work have sold over 30 million albums worldwide.
At the ARIA Music Awards of 1994 they were inducted into the related Hall of Fame. On 28 May 2001 "Down Under" was listed at No. 4 on the APRA Top 30 Australian songs. In October 2010, "Business as Usual" was listed in the book, "100 Best Australian Albums".
Members.
Colin Hay has been the only constant member in all configurations.

</doc>
<doc id="20452" url="https://en.wikipedia.org/wiki?curid=20452" title="Meconium aspiration syndrome">
Meconium aspiration syndrome

Meconium aspiration syndrome (MAS) also known as neonatal aspiration of meconium is a medical condition affecting newborn infants. It occurs when meconium is present in their lungs during or before delivery. Meconium is the first stool of an infant, composed of materials ingested during the time the infant spends in the uterus.
Meconium is normally stored in the infant's intestines until after birth, but sometimes (often in response to fetal distress and hypoxia) it is expelled into the amniotic fluid prior to birth, or during labor. If the baby then inhales the contaminated fluid, respiratory problems may occur.
Signs and symptoms.
The most obvious sign that meconium has been passed during or before labor is the greenish or yellowish appearance of the amniotic fluid. The infant's skin, umbilical cord, or nailbeds may be stained green if the meconium was passed a considerable amount of time before birth. These symptoms alone do not necessarily indicate that the baby has inhaled in the fluid by gasping in utero or after birth. After birth, rapid or labored breathing, cyanosis, slow heartbeat, a barrel-shaped chest or low Apgar score are all signs of the syndrome. Inhalation can be confirmed by one or more tests such as using a stethoscope to listen for abnormal lung sounds (diffuse 'wet' crackles and rhonchi), performing blood gas tests to confirm a severe loss of lung function (respiratory acidosis as a consequence of hypercapnia), and using chest X-rays to look for patchy or streaked areas on the lungs. Infants who have inhaled meconium may develop respiratory distress syndrome often requiring ventilatory support. Complications of MAS include pneumothorax and persistent pulmonary hypertension of the newborn.
Causes.
Fetal distress during labor causes intestinal contractions, as well as relaxation of the anal sphincter, which allows meconium to pass into the amniotic fluid and contaminate the amniotic fluid. Meconium passage into the amniotic fluid occurs in about 5–20 percent of all births and is more common in overdue births. Of the cases where meconium is found in the amniotic fluid, meconium aspiration syndrome develops less than 5 percent of the time. Amniotic fluid is normally clear, but becomes greenish if it is tinted with meconium.
Maternal risk factors can include: preeclampsia, maternal hypertension, oligohydramnios, maternal infections, maternal drug use, placental insufficiency, and/or intrauterine growth restriction.
The risk of MAS increases after the 40th week of pregnancy.
Mechanism.
The pathophysiology of MAS is due to a combination of primary surfactant deficiency and surfactant inactivation as a result of plasma proteins leaking into the airways from areas of epithelial disruption and injury.
The leading three causes of MAS are
If an infant inhales this mixture before, during, or after birth, it may be sucked deep into the lungs. Three main problems occur if this happens:
These can lead to significant morbidity and mortality if severe enough.
Diagnosis.
High risk infants may be identified by fetal tachycardia, bradycardia or absence of fetal accelerations upon CTG in utero, at birth the infant may look cachexic and show signs of yellowish meconium staining on skin, nail and the umbillical cord, these infants usually progress onto Infant Respiratory distress syndrome within 4 hours. Investigations which can confirm the diagnosis are fetal chest x-ray, which will show hyperinflation, diaphragmatic flattening, cardiomegaly, patchy atelectasis and consolidation, and ABG samples, which will show decreased oxygen levels.
Prevention.
MAS is difficult to prevent. Amnioinfusion, a method of thinning thick meconium that has passed into the amniotic fluid through pumping of sterile fluid into the amniotic fluid, has not shown a benefit.
Treatment.
Surfactant appears to improve outcomes when given to infants follow meconium aspiration.
It has been recommended that the throat and nose of the baby be suctioned as soon as the head is delivered. However, this is not really useful and the revised Neonatal Resuscitation Guidelines no longer recommend it. When meconium staining of the amniotic fluid is present and the baby is born depressed, it is recommended that an individual trained in neonatal intubation use a laryngoscope and endotracheal tube to suction meconium from below the vocal cords. If the condition worsens, extracorporeal membrane oxygenation (ECMO) can be useful.
Albumin-lavage has not demonstrated to benefit outcomes of MAS. Steroid use has not demonstrated to benefit the outcomes of MAS.
Prognosis.
The mortality rate of meconium-stained infants is considerably higher than that of non-stained infants; meconium aspiration used to account for a significant proportion of neonatal deaths. Residual lung problems are rare but include symptomatic cough, wheezing, and persistent hyperinflation for up to five to ten years. The ultimate prognosis depends on the extent of CNS injury from asphyxia and the presence of associated problems such as pulmonary hypertension. Fifty percent of newborns affected by meconium aspiration would die fifteen years ago; however, today the percent has dropped to about twenty.
Epidemiology.
In a study conducted between 1995 and 2002, MAS occurred in 1,061 of 2,490,862 live births, reflecting an incidence of 0.43 of 1,000. MAS requiring intubation occurs at higher rates in pregnancies beyond 40 weeks. 34% of all MAS cases born after 40 weeks required intubation compared to 16% prior to 40 weeks.
References.
^Edmonds, P.(2014). An Introduction to Meconium."Midwifery Today."(111), 32-33. PMID 25975056

</doc>
<doc id="20453" url="https://en.wikipedia.org/wiki?curid=20453" title="Meconium">
Meconium

Meconium is the earliest stool of a mammalian infant. Unlike later feces, meconium is composed of materials ingested during the time the infant spends in the uterus: intestinal epithelial cells, lanugo, mucus, amniotic fluid, bile, and water. Meconium, unlike later feces, is viscous and sticky like tar, its color usually being a very dark olive green; it is almost odorless. When diluted in amniotic fluid, it may appear in various shades of green, brown, or yellow. It should be completely passed by the end of the first few days after birth, with the stools progressing toward yellow (digested milk).
Meconium is normally retained in the infant's bowel until after birth, but sometimes it is expelled into the amniotic fluid (also called "amniotic liquor") prior to birth or during labor and delivery. The stained amniotic fluid (called "meconium liquor" or "meconium stained liquor") is recognised by medical staff that this may be a sign of fetal distress. Some post-dates pregnancies (where the woman is more than 40 weeks pregnant) may also have meconium stained liquor without fetal distress. Medical staff may aspirate the meconium from the nose and mouth of a newborn immediately after delivery in the event the baby shows signs of respiratory distress to decrease the risk of meconium aspiration syndrome.
Meconium had been thought to be sterile until the team of researchers from the University of Valencia in Spain found bacterial communities in it so developed that they seemed to fall into two categories. Around half of the samples appeared to be dominated by bacteria that produce lactic acid, such as lactobacillus, while the other half mostly contained a family of so-called enteric bacteria, such as "Escherichia coli".
The Latin term "meconium" derives from the Greek , "mēkōnion", a diminutive of , "mēkōn", i.e. poppy, in reference either to its tarry appearance that may resemble some raw opium preparations, or to Aristotle's belief that it induces sleep in the fetus.
A symptom of both Hirschsprung's disease and cystic fibrosis is the failure to pass meconium.
Meconium can be tested for various drugs, to check for "in utero" exposure. Using meconium, a Canadian research group at the Hospital for Sick Children, University of Toronto, showed that by measuring a by-product of alcohol (FAEE) they could objectively detect babies exposed to excessive maternal drinking of alcohol in pregnancy. In the USA, the results of meconium testing may be used by child protective services and other law enforcement agencies to determine the eligibility of the parents to keep the newborn.
Terminal meconium.
Most of the time that the amniotic fluid is stained with meconium it will be homogeneously distributed throughout the fluid making it brown. This indicates that the fetus passed the meconium some time ago such that sufficient mixing occurred as to establish the homogeneous mixture. Terminal meconium occurs when the fetus passes the meconium a short enough time before birth/caesarean section that the amniotic fluid remains clear, but individual clumps of meconium are in the fluid.
Meconium ileus.
The meconium sometimes becomes thickened and congested in the intestines, a condition known as meconium ileus. Meconium ileus is often the first sign of cystic fibrosis. In cystic fibrosis, the meconium can form a bituminous black-green mechanical obstruction in a segment of the ileum. Beyond this there may be a few separate grey-white globular pellets. Below this level, the bowel is a narrow and empty micro-colon. Above the level of the obstruction, there are several loops of hypertrophied bowel distended with fluid. No meconium is passed, and abdominal distension and vomiting appear soon after birth. About 20% of cases of cystic fibrosis present with meconium ileus, while approximately 20% of one series of cases of meconium ileus did not have cystic fibrosis. The presence of meconium ileus is not related to the severity of the cystic fibrosis. The obstruction can be relieved in a number of different ways.
Meconium ileus should be distinguished from meconium plug syndrome, in which a tenacious mass of mucus prevents the meconium from passing and there is no risk of intestinal perforation. Meconium ileus has a significant risk of intestinal perforation. In barium enema, meconium plug syndrome rather shows normal or dilated colon as compared to micro-colon in meconium ileus.

</doc>
<doc id="20454" url="https://en.wikipedia.org/wiki?curid=20454" title="Montreux Convention Regarding the Regime of the Straits">
Montreux Convention Regarding the Regime of the Straits

The Montreux Convention Regarding the Regime of the Straits is a 1936 agreement that gives Turkey control over the Bosporus Straits and the Dardanelles and regulates the transit of naval warships. The Convention gives Turkey full control over the Straits and guarantees the free passage of civilian vessels in peacetime. It restricts the passage of naval ships not belonging to Black Sea states. The terms of the convention have been the source of controversy over the years, most notably concerning the Soviet Union's military access to the Mediterranean Sea.
Signed on 20 July 1936 at the Montreux Palace in Switzerland, it permitted Turkey to remilitarise the Straits. It went into effect on 9 November 1936 and was registered in "League of Nations Treaty Series" on 11 December 1936. It is still in force today, with some amendments.
The proposed 21st century Kanal Istanbul project may constitute a possible by-pass to the Montreux Convention and force greater Turkish autonomy with respect to the passage of military ships from the Black Sea to the Sea of Marmara.
Background.
The convention was one of a series of agreements in the 19th and 20th centuries that sought to address the long-running "Straits Question" of who should control the strategically vital link between the Black Sea and Mediterranean Sea. In 1923 the Treaty of Lausanne had demilitarised the Dardanelles and opened the Straits to unrestricted civilian and military traffic, under the supervision of the International Straits Commission of the League of Nations.
By the late 1930s, the strategic situation in the Mediterranean had altered with the rise of Fascist Italy, which controlled the Greek-inhabited Dodecanese islands off the west coast of Turkey and had constructed fortifications on Rhodes, Leros and Kos. The Turks feared that Italy would seek to exploit access to the Straits to expand its power into Anatolia and the Black Sea region. There were also fears of Bulgarian rearmament. Although Turkey was not permitted to refortify the Straits, it nonetheless did so secretly.
In April 1935, the Turkish government dispatched a lengthy diplomatic note to the signatories of the Treaty of Lausanne proposing a conference on the agreement of a new regime for the Straits and requested that the League of Nations authorise the reconstruction of the Dardanelles forts. In the note, Turkish foreign minister Tevfik Rüştü Aras explained that the international situation had changed greatly since 1923. At that time, Europe had been moving towards disarmament and an international guarantee to defend the Straits. The Abyssinia Crisis of 1934–35, the denunciation by Germany of the Treaty of Versailles and international moves towards rearmament meant that "the only guarantee intended to guard against the total insecurity of the Straits has just disappeared in its turn." Indeed, Aras said, "the Powers most closely concerned are proclaiming the existence of a threat of general conflagration." The key weaknesses of the present regime were that the machinery for collective guarantees were too slow and ineffective, there was no contingency for a general threat of war and no provision for Turkey to defend itself. Turkey was therefore prepared
The response to the note was generally favourable, and Australia, Bulgaria, France, Germany, Greece, Japan, Romania, the Soviet Union, Turkey, the United Kingdom and Yugoslavia agreed to attend negotiations at Montreux in Switzerland, which began on 22 June 1936. Two major powers were not represented: Italy, whose aggressively expansionist policies had prompted the conference in the first place, refused to attend and the United States declined even to send an observer.
Turkey, the UK and the Soviet Union each put forward their own set of proposals, aimed chiefly at protecting their own interests. The British favoured the continuation of a relatively restrictive approach, while the Turks sought a more liberal regime that reasserted their own control over the Straits and the Soviets proposed a regime that would guarantee absolute freedom of passage. The British, supported by France, sought to exclude the Soviet fleet from the Mediterranean Sea, where it might have threatened the vital shipping lanes to India, Egypt and the Far East. In the end, the British conceded some of their requests while the Soviets succeeded in ensuring that the Black Sea countries – including the USSR – were given some exemptions from the military restrictions imposed on non-Black Sea nations. The agreement was ratified by all of the conference attendees with the exception of Germany, which had not been a signatory to the Treaty of Lausanne, and with reservations by Japan, and came into force on 9 November 1936.
Britain's willingness to make concessions has been attributed to a desire to avoid Turkey being driven to ally itself with, or fall under the influence of Adolf Hitler or Benito Mussolini. It was thus the first in a series of steps by Britain and France to ensure that Turkey would either remain neutral or tilt towards the Western Allies in the event of any future conflict with the Axis.
Terms and consequences of the Convention.
The Convention consists of 29 Articles, four annexes and one protocol. Articles 2–7 consider the passage of merchant ships. Articles 8–22 consider the passage of war vessels. The key principle of freedom of passage and navigation is stated in articles 1 and 2. Article 1 provides that "The High Contracting Parties recognise and affirm the principle of freedom of passage and navigation by sea in the Straits". Article 2 states that "In time of peace, merchant vessels shall enjoy complete freedom of passage and navigation in the Straits, by day and by night, under any flag with any kind of cargo."
The International Straits Commission was abolished, authorising the full resumption of Turkish military control over the Straits and the refortification of the Dardanelles. Turkey was authorised to close the Straits to all foreign warships in wartime or when it was threatened by aggression; additionally, it was authorised to refuse transit from merchant ships belonging to countries at war with Turkey. A number of highly specific restrictions were imposed on what type of warships are allowed passage. Non-Black Sea state warships in the Straits must be under 15,000 tons. No more than nine non-Black Sea state warships, with a total aggregate tonnage of no more than 30,000 tons, may pass at any one time, and they are permitted to stay in the Black Sea for no longer than twenty-one days. 
Although the treaty is often cited as prohibiting aircraft carriers in the straits, there is no explicit prohibition on aircraft carriers in the treaty. However, the tonnage limits in Article 14, which apply to all non-Black Sea powers, would preclude the transit of modern aircraft carrying ships. In the case of non-Black Sea powers, these terms make it impossible for transit any modern ships carrying aircraft through the straits without violating the terms of the convention.
By contrast, Black Sea powers such as the USSR were able to transit aircraft carrying cruisers through the straits under other terms of the convention. As with non-Black Seas powers, the Montreux convention does not explicitly forbid a Black Sea power from transiting aircraft carriers through the straits, and the tonnage limits in Article 14 also apply to Black Sea powers as well as non-Black Sea powers. However, under Article 11, Black Sea states are permitted to transit capital ships of any tonnage through the straits. Annex II specifically excludes aircraft carriers from the definition of capital ships, but limits the definition of carriers to ships that are designed primarily for carrying and operating aircraft at sea and specifically excludes other ships that merely are able to operate aircraft.
The result of this is that by designing its aircraft carrying ships such as "Kiev" and "Admiral Kuznetsov" to have roles other than aircraft operation and by designating those ships as "aircraft carrying cruisers" rather than "aircraft carriers" the Soviet Union was able to transit its aircraft carrying ships through the straits in compliance with the convention, while at the same time the Convention denied access to NATO aircraft carriers, which are not covered by the exemption in Article 11. 
Under Article 12, Black Sea states are also allowed to send submarines through the Straits, with prior notice, as long as the vessels have been constructed, purchased or sent for repair outside the Black Sea. The less restrictive rules applicable to Black Sea states were agreed as, effectively, a concession to the Soviet Union, the only Black Sea state other than Turkey with any significant number of capital ships or submarines. The passage of civil aircraft between the Mediterranean and Black Seas is permitted, but only along routes authorised by the Turkish government.
The terms of the Convention were largely a reflection of the international situation in the mid-1930s. They largely served Turkish and Soviet interests, enabling Turkey to regain military control of the Straits and assuring Soviet dominance of the Black Sea. Although the Convention restricted the Soviets' ability to send naval forces into the Mediterranean sea – thereby satisfying British concerns about Soviet intrusion into what was considered a British sphere of influence – it also ensured that outside powers could not exploit the Straits to threaten the Soviet Union. This was to have significant repercussions during World War II when the Montreux regime prevented the Axis powers from sending naval forces through the Straits to attack the Soviet Union. The Axis powers were thus severely limited in naval capability in their Black Sea campaigns, relying principally on small vessels that had been transported overland by rail and canal networks. Auxiliary vessels and armed merchant ships occupied a grey area, however, and the transit of such vessels through the straits led to friction between the Allies and Turkey. Repeated protests from Moscow and London led to the Turkish government banning the movements of "suspicious" Axis ships with effect from June 1944 after a number of German auxiliary ships were permitted to transit the Straits.
Development of the Convention since 1936.
The Convention remains in force today, with amendments, though not without dispute. It was repeatedly challenged by the Soviet Union during World War II and the Cold War. As early as 1939, Joseph Stalin sought to reopen the Straits Question and proposed joint Turkish and Soviet control of the Straits, complaining that "a small state Turkey supported by Great Britain held a great state by the throat and gave it no outlet." After the Molotov–Ribbentrop Pact was signed by the Soviet Union and Nazi Germany, the Soviet Foreign Minister Vyacheslav Molotov informed his German counterparts that the USSR wished to take military control of the Straits and establish its own military base there. The Soviets returned to the issue in 1945 and 1946, demanding a revision of the Montreux Convention at a conference excluding most of the Montreux signatories, a permanent Soviet military presence and joint control of the Straits. This was firmly rejected by Turkey, despite an ongoing Soviet "strategy of tension". For several years after World War II, the Soviets exploited the restriction on the number of foreign warships by ensuring that one of theirs was always in the Straits, thus effectively blocking any nation other than Turkey from sending warships through the Straits. Soviet pressure expanded into full on demands to revise the Montreux Convention, which led to the Turkish Straits crisis of 1946, which led to Turkey abandoning its policy of neutrality. In 1947 it became the recipient of US military and economic assistance under the Truman Doctrine of "containment" and joined NATO, along with Greece, in 1952.
The passage of US warships through the Straits also raised controversy, as the convention forbids the transit of non-Black Sea nations' warships with guns of a calibre larger than eight inches (203 mm). In the 1960s, the US sent warships carrying 420 mm calibre ASROC missiles through the Straits, prompting Soviet protests. The Turkish government rejected the Soviet complaints, pointing out that guided missiles were not guns and that such weapons had not even existed at the time of the Convention's agreement so were not restricted.
The United Nations Convention on the Law of the Sea (UNCLOS), which entered into force in November 1994, has prompted calls for the Montreux Convention to be revised and adapted to make it compatible with UNCLOS's regime governing straits used for international navigation. However, Turkey's long-standing refusal to sign UNCLOS has meant that Montreux remains in force without further amendments.
The safety of vessels passing through the Bosporus has become a major concern in recent years as the volume of traffic has increased greatly since the Convention was signed – from 4,500 in 1934 to 49,304 by 1998. As well as obvious environmental concerns, the Straits bisect the city of Istanbul with over 14 million people living on its shores; maritime incidents in the Straits therefore pose a considerable risk to public safety. The Convention does not, however, make any provision for the regulation of shipping for the purposes of safety and environmental protection. In January 1994 the Turkish government adopted new "Maritime Traffic Regulations for the Turkish Straits and the Marmara Region". This introduced a new regulatory regime "in order to ensure the safety of navigation, life and property and to protect the environment in the region" but without violating the Montreux principle of free passage. The new regulations provoked some controversy when Russia, Greece, Cyprus, Romania, Ukraine and Bulgaria raised objections. However, they were approved by the International Maritime Organisation on the grounds that they were not intended to prejudice "the rights of any ship using the Straits under international law". The regulations were revised in November 1998 to address Russian concerns.

</doc>
<doc id="20455" url="https://en.wikipedia.org/wiki?curid=20455" title="Michael Jordan">
Michael Jordan

Michael Jeffrey Jordan (born February 17, 1963), also known by his initials, MJ, is an American retired professional basketball player. He is also a businessman, and principal owner and chairman of the Charlotte Hornets. Jordan played 15 seasons in the National Basketball Association (NBA) for the Chicago Bulls and Washington Wizards. His biography on the NBA website states: "By acclamation, Michael Jordan is the greatest basketball player of all time." Jordan was one of the most effectively marketed athletes of his generation and was considered instrumental in popularizing the NBA around the world in the 1980s and 1990s.
Jordan played three seasons for coach Dean Smith at the University of North Carolina. He was a member of the Tar Heels' national championship team in 1982. Jordan joined the NBA's Chicago Bulls in 1984 as the third overall draft pick. He quickly emerged as a league star, entertaining crowds with his prolific scoring. His leaping ability, illustrated by performing slam dunks from the free throw line in slam dunk contests, earned him the nicknames "Air Jordan" and "His Airness". He also gained a reputation for being one of the best defensive players in basketball. In 1991, he won his first NBA championship with the Bulls, and followed that achievement with titles in 1992 and 1993, securing a "three-peat". Although Jordan abruptly retired from basketball before the beginning of the 1993–94 NBA season to pursue a career in baseball, he returned to the Bulls in March 1995 and led them to three additional championships in 1996, 1997, and 1998, as well as a then-record 72 regular-season wins in the 1995–96 NBA season. Jordan retired for a second time in January 1999, but returned for two more NBA seasons from 2001 to 2003 as a member of the Wizards.
Jordan's individual accolades and accomplishments include five Most Valuable Player (MVP) Awards, ten All-NBA First Team designations, nine All-Defensive First Team honors, fourteen NBA All-Star Game appearances, three All-Star Game MVP Awards, ten scoring titles, three steals titles, six NBA Finals MVP Awards, and the 1988 NBA Defensive Player of the Year Award. Among his numerous accomplishments, Jordan holds the NBA records for highest career regular season scoring average (30.12 points per game) and highest career playoff scoring average (33.45 points per game). In 1999, he was named the greatest North American athlete of the 20th century by ESPN, and was second to Babe Ruth on the Associated Press's list of athletes of the century. Jordan is a two-time inductee into the Basketball Hall of Fame, having been enshrined in 2009 for his individual career, and again in 2010 as part of the group induction of the 1992 United States men's Olympic basketball team ("The Dream Team").
Jordan is also known for his product endorsements. He fueled the success of Nike's Air Jordan sneakers, which were introduced in 1985 and remain popular today. Jordan also starred in the 1996 feature film "Space Jam" as himself. In 2006, he became part-owner and head of basketball operations for the then-Charlotte Bobcats, buying a controlling interest in 2010. In 2015, Jordan became the first billionaire NBA player in history.
Early years.
Jordan was born in Brooklyn, New York, the son of Deloris (née Peoples), who worked in banking, and James R. Jordan, Sr., an equipment supervisor. His family moved to Wilmington, North Carolina, when he was a toddler.
Jordan is the fourth of five children. He has two older brothers, Larry Jordan and James R. Jordan, Jr., one older sister, Deloris, and a younger sister, Roslyn. Jordan's brother James retired in 2006 as the Command Sergeant Major of the 35th Signal Brigade of the XVIII Airborne Corps in the U.S. Army.
High school career.
Jordan attended Emsley A. Laney High School in Wilmington, where he anchored his athletic career by playing baseball, football, and basketball. He tried out for the varsity basketball team during his sophomore year, but at 5'11" (1.80 m), he was deemed too short to play at that level. His taller friend, Harvest Leroy Smith, was the only sophomore to make the team.
Motivated to prove his worth, Jordan became the star of Laney's junior varsity squad, and tallied several 40-point games. The following summer, he grew four inches (10 cm) and trained rigorously. Upon earning a spot on the varsity roster, Jordan averaged about 20 points per game over his final two seasons of high school play. As a senior, he was selected to the McDonald's All-American Team after averaging a triple-double: 29.2 points, 11.6 rebounds, and 10.1 assists.
Jordan was recruited by numerous college basketball programs, including Duke, North Carolina, South Carolina, Syracuse, and Virginia. In 1981, Jordan accepted a basketball scholarship to North Carolina, where he majored in cultural geography.
College career.
As a freshman in coach Dean Smith's team-oriented system, he was named ACC Freshman of the Year after he averaged 13.4 points per game (ppg) on 53.4% shooting (field goal percentage). He made the game-winning jump shot in the 1982 NCAA Championship game against Georgetown, which was led by future NBA rival Patrick Ewing. Jordan later described this shot as the major turning point in his basketball career. During his three seasons at North Carolina, he averaged 17.7 ppg on 54.0% shooting, and added 5.0 rebounds per game (rpg). He was selected by consensus to the NCAA All-American First Team in both his sophomore (1983) and junior (1984) seasons. After winning the Naismith and the Wooden College Player of the Year awards in 1984, Jordan left North Carolina one year before his scheduled graduation to enter the 1984 NBA draft. The Chicago Bulls selected Jordan with the third overall pick, after Hakeem Olajuwon (Houston Rockets) and Sam Bowie (Portland Trail Blazers). One of the primary reasons why Jordan was not drafted sooner was because the first two teams were in need of a center. However, the Trail Blazers general manager Stu Inman contended that it was not a matter of drafting a center, but more a matter of taking Sam Bowie over Jordan, in part because Portland already had a guard with similar skills to Jordan, Clyde Drexler. ESPN, citing Bowie's injury-laden college career, named the Blazers' choice of Bowie as the worst draft pick in North American professional sports history. Jordan returned to North Carolina to complete his degree in 1986.
Professional career.
Early NBA years (1984–1987).
During his first season in the NBA, Jordan averaged 28.2 ppg on 51.5% shooting. He quickly became a fan favorite even in opposing arenas, and appeared on the cover of "Sports Illustrated" with the heading "A Star Is Born" just over a month into his professional career. Jordan was also voted in as an All-Star starter by the fans in his rookie season. Controversy arose before the All-Star game when word surfaced that several veteran players, led by Isiah Thomas, were upset by the amount of attention Jordan was receiving. This led to a so-called "freeze-out" on Jordan, where players refused to pass him the ball throughout the game. The controversy left Jordan relatively unaffected when he returned to regular season play, and he would go on to be voted Rookie of the Year. The Bulls finished the season 38–44, and lost in the first round of the playoffs in four games to the Milwaukee Bucks.
Jordan's second season was cut short by a broken foot in the third game of the season, which caused him to miss 64 games. Despite Jordan's injury and a 30–52 record (at the time it was fifth worst record of any team to qualify for the playoffs in NBA history), the Bulls made the playoffs. Jordan recovered in time to participate in the playoffs and performed well upon his return. Against a 1985–86 Boston Celtics team that is often considered one of the greatest in NBA history, Jordan set the still-unbroken record for points in a playoff game with 63 in Game 2. The Celtics, however, managed to sweep the series.
Jordan had recovered completely by the 1986–87 season, and had one of the most prolific scoring seasons in NBA history. He became the only player other than Wilt Chamberlain to score 3,000 points in a season, averaging a league high 37.1 points on 48.2% shooting. In addition, Jordan demonstrated his defensive prowess, as he became the first player in NBA history to record 200 steals and 100 blocks in a season. Despite Jordan's success, Magic Johnson won the league's Most Valuable Player Award. The Bulls reached 40 wins, and advanced to the playoffs for the third consecutive year. However, they were again swept by the Celtics.
Pistons roadblock (1987–1990).
Jordan led the league in scoring again in the 1987–88 season, averaging 35.0 ppg on 53.5% shooting and won his first league MVP Award. He was also named the Defensive Player of the Year, as he had averaged 1.6 blocks and a league high 3.16 steals per game. The Bulls finished 50–32, and made it out of the first round of the playoffs for the first time in Jordan's career, as they defeated the Cleveland Cavaliers in five games. However, the Bulls then lost in five games to the more experienced Detroit Pistons, who were led by Isiah Thomas and a group of physical players known as the "".
In the 1988–89 season, Jordan again led the league in scoring, averaging 32.5 ppg on 53.8% shooting from the field, along with 8 rpg and 8 assists per game (apg). The Bulls finished with a 47–35 record, and advanced to the Eastern Conference Finals, defeating the Cavaliers and New York Knicks along the way. The Cavaliers series included a career highlight for Jordan when he hit "The Shot" over Craig Ehlo at the buzzer in the fifth and final game of the series. However, the Pistons again defeated the Bulls, this time in six games, by utilizing their "Jordan Rules" method of guarding Jordan, which consisted of double and triple teaming him every time he touched the ball.
The Bulls entered the 1989–90 season as a team on the rise, with their core group of Jordan and young improving players like Scottie Pippen and Horace Grant, and under the guidance of new coach Phil Jackson. Jordan averaged a league leading 33.6 ppg on 52.6% shooting, to go with 6.9 rpg and 6.3 apg in leading the Bulls to a 55–27 record. They again advanced to the Eastern Conference Finals beating the Bucks and Philadelphia 76ers. However, despite pushing the series to seven games, the Bulls lost to the Pistons for the third consecutive season.
First three-peat (1991–1993).
In the 1990–91 season, Jordan won his second MVP award after averaging 31.5 ppg on 53.9% shooting, 6.0 rpg, and 5.5 apg for the regular season. The Bulls finished in first place in their division for the first time in 16 years and set a franchise record with 61 wins in the regular season. With Scottie Pippen developing into an All-Star, the Bulls had elevated their play. The Bulls defeated the New York Knicks and the Philadelphia 76ers in the opening two rounds of the playoffs. They advanced to the Eastern Conference Finals where their rival, the Detroit Pistons, awaited them. However, this time the Bulls beat the Pistons in a four-game sweep. In an unusual ending to the fourth and final game, Isiah Thomas led his team off the court before the final seconds had concluded. Most of the Pistons went directly to their locker room instead of shaking hands with the Bulls.
The Bulls advanced to the NBA Finals for the first time in franchise history to face Magic Johnson and James Worthy and beat the Los Angeles Lakers four games to one, compiling an outstanding 15–2 playoff record along the way. Perhaps the best known moment of the series came in Game 2 when, attempting a dunk, Jordan avoided a potential Sam Perkins block by switching the ball from his right hand to his left in mid-air to lay the shot in. In his first Finals appearance, Jordan posted per game averages of 31.2 points on 56% shooting from the field, 11.4 assists, 6.6 rebounds, 2.8 steals and 1.4 blocks. Jordan won his first NBA Finals MVP award, and he cried while holding the NBA Finals trophy.
Jordan and the Bulls continued their dominance in the 1991–92 season, establishing a 67–15 record, topping their franchise record from 1990 to 91. Jordan won his second consecutive MVP award with averages of 30.1 points, 6.4 rebounds and 6.1 assists per game on 52% shooting. After winning a physical 7-game series over the New York Knicks in the second round of the playoffs and finishing off the Cleveland Cavaliers in the Conference Finals in 6 games, the Bulls met Clyde Drexler and the Portland Trail Blazers in the Finals. The media, hoping to recreate a Magic–Bird rivalry, highlighted the similarities between "Air" Jordan and Clyde "The Glide" during the pre-Finals hype. In the first game, Jordan scored a Finals-record 35 points in the first half, including a record-setting six three-point field goals. After the sixth three-pointer, he jogged down the court shrugging as he looked courtside. Marv Albert, who broadcast the game, later stated that it was as if Jordan was saying, "I can't believe I'm doing this." The Bulls went on to win Game 1, and defeat the Blazers in six games. Jordan was named Finals MVP for the second year in a row and finished the series averaging 35.8 ppg, 4.8 rpg, and 6.5 apg, while shooting 53% from the floor.
In the 1992–93 season, despite a 32.6 ppg, 6.7 rpg and 5.5 apg campaign, Jordan's streak of consecutive MVP seasons ended as he lost the award to his friend Charles Barkley. Coincidentally, Jordan and the Bulls met Barkley and his Phoenix Suns in the 1993 NBA Finals. The Bulls won their third NBA championship on a game-winning shot by John Paxson and a last-second block by Horace Grant, but Jordan was once again Chicago's leader. He averaged a Finals-record 41.0 ppg during the six-game series, and became the first player in NBA history to win three straight Finals MVP awards. He scored more than 30 points in every game of the series, including 40 or more points in 4 consecutive games. With his third Finals triumph, Jordan capped off a seven-year run where he attained seven scoring titles and three championships, but there were signs that Jordan was tiring of his massive celebrity and all of the non-basketball hassles in his life.
Gambling controversy.
During the Bulls' playoff run in 1993, controversy arose when Jordan was seen gambling in Atlantic City, New Jersey, the night before a game against the New York Knicks. In that same year, he admitted to having to cover $57,000 in gambling losses, and author Richard Esquinas wrote a book claiming he had won $1.25 million from Jordan on the golf course. In 2005, Jordan talked to Ed Bradley of the CBS evening show "60 Minutes" about his gambling and admitted that he made some reckless decisions. Jordan stated, "Yeah, I've gotten myself into situations where I would not walk away and I've pushed the envelope. Is that compulsive? Yeah, it depends on how you look at it. If you're willing to jeopardize your livelihood and your family, then yeah." When Bradley asked him if his gambling ever got to the level where it jeopardized his livelihood or family, Jordan replied, "No."
First retirement and baseball career (1993–1994).
On October 6, 1993, Jordan announced his retirement, citing a loss of desire to play the game. Jordan later stated that the murder of his father earlier in the year also shaped his decision. Jordan's father was murdered on July 23, 1993, at a highway rest area in Lumberton, North Carolina, by two teenagers, Daniel Green and Larry Martin Demery. The assailants were traced from calls they made on James Jordan's cellular phone, caught, convicted, and sentenced to life in prison. Jordan was close to his father; as a child he had imitated his father's proclivity to stick out his tongue while absorbed in work. He later adopted it as his own signature, displaying it each time he drove to the basket. In 1996, he founded a Chicago area Boys & Girls Club and dedicated it to his father.
In his 1998 autobiography "For the Love of the Game", Jordan wrote that he had been preparing for retirement as early as the summer of 1992. The added exhaustion due to the Dream Team run in the 1992 Olympics solidified Jordan's feelings about the game and his ever-growing celebrity status. Jordan's announcement sent shock waves throughout the NBA and appeared on the front pages of newspapers around the world.
Jordan then further surprised the sports world by signing a minor league baseball contract with the Chicago White Sox on February 7, 1994. He reported to spring training in Sarasota, Florida, and was assigned to the team's minor league system on March 31, 1994. Jordan has stated this decision was made to pursue the dream of his late father, who had always envisioned his son as a Major League Baseball player. The White Sox were another team owned by Bulls owner Jerry Reinsdorf, who continued to honor Jordan's basketball contract during the years he played baseball. 
In 1994, Jordan played for the Birmingham Barons, a Double-A minor league affiliate of the Chicago White Sox, batting .202 with three home runs, 51 runs batted in, 30 stolen bases, 114 strikeouts, 51 base on balls, and 11 errors. He also appeared for the Scottsdale Scorpions in the 1994 Arizona Fall League, batting .252 against the top prospects in baseball. On November 1, 1994, his number 23 was retired by the Bulls in a ceremony that included the erection of a permanent sculpture known as "The Spirit" outside the new United Center.
"I'm back": Return to the NBA (1995).
In the 1993–94 season, the Bulls, without Jordan, achieved a 55–27 record, and lost to the New York Knicks in the second round of the playoffs. But the 1994–95 Bulls were a shell of the championship team of just two years earlier. Struggling at mid-season to ensure a spot in the playoffs, Chicago was 31–31 at one point in mid-March. The team received help, however, when Jordan decided to return to the NBA for the Bulls.
In March 1995, Jordan decided to quit baseball due to the ongoing Major League Baseball strike, as he wanted to avoid becoming a potential replacement player. On March 18, 1995, Jordan announced his return to the NBA through a two-word press release: "I'm back." The next day, Jordan wore jersey number 45 (his number with the Barons), as his familiar 23 had been retired in his honor following his first retirement. He took to the court with the Bulls to face the Indiana Pacers in Indianapolis, scoring 19 points. The game had the highest Nielsen rating of a regular season NBA game since 1975.
Although he had not played an NBA game in a year and a half, Jordan played well upon his return, making a game-winning jump shot against Atlanta in his fourth game back. He then scored 55 points in the next game against the Knicks at Madison Square Garden on March 28, 1995. Boosted by Jordan's comeback, the Bulls went 13–4 to make the playoffs and advanced to the Eastern Conference Semifinals against the Orlando Magic. At the end of Game 1, Orlando's Nick Anderson stripped Jordan from behind, leading to the game-winning basket for the Magic; he would later comment that Jordan "didn't look like the old Michael Jordan" and that "No. 45 doesn't explode like No. 23 used to." Jordan then returned to wearing his old number in the next game, scoring 38 points in a Bulls win. The Bulls were fined $30,000 for the game: $25,000 for failing to report the impromptu number change to the NBA and $5,000 for Jordan wearing different shoes. Jordan averaged 31 points per game in the series, but Orlando won the series in 6 games.
Second three-peat (1995–1998).
Freshly motivated by the playoff defeat, Jordan trained aggressively for the 1995–96 season. Strengthened by the addition of rebound specialist Dennis Rodman, the Bulls dominated the league, starting the season 41–3, and eventually finishing with the second-best regular season record in NBA history: 72–10. Jordan led the league in scoring with 30.4 ppg, and won the league's regular season and All-Star Game MVP awards. 
In the playoffs, the Bulls lost only three games in four series (Miami Heat 3-0, New York Knicks 4-1, Orlando Magic 4-0). They defeated the Seattle SuperSonics 4-2 in the NBA Finals to win their fourth championship. Jordan was named Finals MVP for a record fourth time, surpassing Magic Johnson's three Finals MVP awards. He also achieved only the second sweep of the MVP Awards in the All-Star Game, regular season and NBA Finals, Willis Reed having achieved the first, during the 1969–70 season. Because this was Jordan's first championship since his father's murder, and it was won on Father's Day, Jordan reacted very emotionally upon winning the title, including a memorable scene of him crying on the locker room floor with the game ball.
In the 1996–97 season, the Bulls started out 69–11, but missed out on a second consecutive 70-win season by losing their final two games to finish 69–13. However, this year Jordan was beaten for the NBA MVP Award by Karl Malone. The Bulls again advanced to the Finals, where they faced Malone and the Utah Jazz. The series against the Jazz featured two of the more memorable clutch moments of Jordan's career. He won Game 1 for the Bulls with a buzzer-beating jump shot. In Game 5, with the series tied at 2, Jordan played despite being feverish and dehydrated from a stomach virus. In what is known as the "Flu Game", Jordan scored 38 points, including the game-deciding 3-pointer with 25 seconds remaining. The Bulls won 90–88 and went on to win the series in six games. For the fifth time in as many Finals appearances, Jordan received the Finals MVP award. During the 1997 NBA All-Star Game, Jordan posted the first triple double in All-Star Game history in a victorious effort; however, he did not receive the MVP award.
Jordan and the Bulls compiled a 62–20 record in the 1997–98 season. Jordan led the league with 28.7 points per game, securing his fifth regular-season MVP award, plus honors for All-NBA First Team, First Defensive Team and the All-Star Game MVP. The Bulls won the Eastern Conference Championship for a third straight season, including surviving a seven-game series with Reggie Miller's Indiana Pacers in the Eastern Conference Finals; it was the first time Jordan had played in a Game 7 since the 1992 Eastern Conference Semifinals with the Knicks. After winning, they moved on for a rematch with the Jazz in the Finals.
The Bulls returned to the Delta Center for Game 6 on June 14, 1998, leading the series 3–2. Jordan executed a series of plays, considered to be one of the greatest clutch performances in NBA Finals history. With the Bulls trailing 86–83 with 41.9 seconds remaining, Phil Jackson called a timeout. When play resumed, Jordan received the inbound pass, drove to the basket, and hit a shot over several Jazz defenders, cutting the Utah lead to 86–85. The Jazz brought the ball upcourt and passed the ball to forward Karl Malone, who was set up in the low post and was being guarded by Rodman. Malone jostled with Rodman and caught the pass, but Jordan cut behind him and stole the ball out of his hands for a steal. Jordan then dribbled down the court and paused, eyeing his defender, Jazz guard Bryon Russell. With 10 seconds remaining, Jordan started to dribble right, then crossed over to his left, possibly pushing off Russell, although the officials did not call a foul. With 5.2 seconds left, Jordan gave Chicago an 87–86 lead with a game-winning jumper, the climactic shot of his Bulls career. Afterwards, John Stockton missed a go-ahead three-pointer. Jordan and the Bulls won their sixth NBA championship and second three-peat. Once again, Jordan was voted the Finals MVP, having led all scorers averaging 33.5 points per game, including 45 in the deciding Game 6. Jordan's six Finals MVPs is a record; Shaquille O'Neal, Magic Johnson, and Tim Duncan are tied for second place with three apiece. The 1998 Finals holds the highest television rating of any Finals series in history. Game 6 also holds the highest television rating of any game in NBA history.
Second retirement (1999–2001).
With Phil Jackson's contract expiring, the pending departures of Scottie Pippen (who stated his desire to be traded during the offseason) and Dennis Rodman (who would sign with the Los Angeles Lakers as a free agent) looming, and being in the latter stages of an owner-induced lockout of NBA players, Jordan retired for the second time on January 13, 1999.
On January 19, 2000, Jordan returned to the NBA not as a player, but as part owner and President of Basketball Operations for the Washington Wizards. Jordan's responsibilities with the Wizards were comprehensive. He controlled all aspects of the Wizards' basketball operations, and had the final say in all personnel matters. Opinions of Jordan as a basketball executive were mixed. He managed to purge the team of several highly paid, unpopular players (such as forward Juwan Howard and point guard Rod Strickland), but used the first pick in the 2001 NBA draft to select high schooler Kwame Brown, who did not live up to expectations and was traded away after four seasons.
Despite his January 1999 claim that he was "99.9% certain" that he would never play another NBA game, in the summer of 2001 Jordan expressed interest in making another comeback, this time with his new team. Inspired by the NHL comeback of his friend Mario Lemieux the previous winter, Jordan spent much of the spring and summer of 2001 in training, holding several invitation-only camps for NBA players in Chicago. In addition, Jordan hired his old Chicago Bulls head coach, Doug Collins, as Washington's coach for the upcoming season, a decision that many saw as foreshadowing another Jordan return.
Washington Wizards comeback (2001–2003).
On September 25, 2001, Jordan announced his return to the NBA to play for the Washington Wizards, indicating his intention to donate his salary as a player to a relief effort for the victims of the September 11, 2001 attacks. In an injury-plagued 2001–02 season, he led the team in scoring (22.9 ppg), assists (5.2 apg), and steals (1.42 spg). However, torn cartilage in his right knee ended Jordan's season after only 60 games, the fewest he had played in a regular season since playing 17 games after returning from his first retirement during the 1994–95 season. Jordan started 53 of his 60 games for the season, averaging 24.3 points, 5.4 assists, and 6.0 rebounds, and shooting 41.9% from the field in his 53 starts. His last seven appearances were in a reserve role, in which he averaged just over 20 minutes per game.
Playing in his 14th and final NBA All-Star Game in 2003, Jordan passed Kareem Abdul-Jabbar as the all-time leading scorer in All-Star Game history (a record since broken by Kobe Bryant). That year, Jordan was the only Washington player to play in all 82 games, starting in 67 of them. He averaged 20.0 points, 6.1 rebounds, 3.8 assists, and 1.5 steals per game. He also shot 45% from the field, and 82% from the free throw line. Even though he turned 40 during the season, he scored 20 or more points 42 times, 30 or more points nine times, and 40 or more points three times. On February 21, 2003, Jordan became the first 40-year-old to tally 43 points in an NBA game. During his stint with the Wizards, all of Jordan's home games at the MCI Center were sold out, and the Wizards were the second most-watched team in the NBA, averaging 20,172 fans a game at home and 19,311 on the road. However, neither of Jordan's final two seasons resulted in a playoff appearance for the Wizards, and Jordan was often unsatisfied with the play of those around him. At several points he openly criticized his teammates to the media, citing their lack of focus and intensity, notably that of the number one draft pick in the 2001 NBA draft, Kwame Brown.
With the recognition that 2002–03 would be Jordan's final season, tributes were paid to him throughout the NBA. In his final game at his old home court, the United Center in Chicago, Jordan received a four-minute standing ovation. The Miami Heat retired the number 23 jersey on April 11, 2003, even though Jordan never played for the team. At the 2003 All-Star Game, Jordan was offered a starting spot from Tracy McGrady and Allen Iverson, but refused both. In the end he accepted the spot of Vince Carter, who decided to give it up under great public pressure.
Jordan's final NBA game was on April 16, 2003 in Philadelphia. After scoring only 13 points in the game, Jordan went to the bench with 4 minutes and 13 seconds remaining in the third quarter and with his team trailing the Philadelphia 76ers, 75–56. Just after the start of the fourth quarter, the First Union Center crowd began chanting "We want Mike!". After much encouragement from coach Doug Collins, Jordan finally rose from the bench and re-entered the game, replacing Larry Hughes with 2:35 remaining. At 1:45, Jordan was intentionally fouled by the 76ers' Eric Snow, and stepped to the line to make both free throws. After the second foul shot, the 76ers in-bounded the ball to rookie John Salmons, who in turn was intentionally fouled by Bobby Simmons one second later, stopping time so that Jordan could return to the bench. Jordan received a three-minute standing ovation from his teammates, his opponents, the officials and the crowd of 21,257 fans.
Olympic career.
Jordan played on two Olympic gold medal-winning American basketball teams. As a college player he participated, and won the gold, in the 1984 Summer Olympics. The team was coached by Bob Knight and featured players such as Patrick Ewing, Sam Perkins, Chris Mullin, Steve Alford, and Wayman Tisdale. Jordan led the team in scoring, averaging 17.1 ppg for the tournament.
In the 1992 Summer Olympics, he was a member of the star-studded squad that included Magic Johnson, Larry Bird, and David Robinson and was dubbed the "Dream Team". Jordan was the only player to start all 8 games in the Olympics. Playing limited minutes due to the frequent blowouts, Jordan averaged 14.9 ppg, finishing second on the team in scoring. Jordan and fellow Dream Team members Patrick Ewing and Chris Mullin are the only American men's basketball players to win Olympic gold as amateurs and professionals.
Post-retirement.
After his third retirement, Jordan assumed that he would be able to return to his front office position of Director of Basketball Operations with the Wizards. However, his previous tenure in the Wizards' front office had produced the aforementioned mixed results and may have also influenced the trade of Richard "Rip" Hamilton for Jerry Stackhouse (although Jordan was not technically Director of Basketball Operations in 2002). On May 7, 2003, Wizards owner Abe Pollin fired Jordan as Washington's President of Basketball Operations. Jordan later stated that he felt betrayed, and that if he knew he would be fired upon retiring he never would have come back to play for the Wizards.
Jordan kept busy over the next few years by staying in shape, playing golf in celebrity charity tournaments, spending time with his family in Chicago, promoting his Jordan Brand clothing line, and riding motorcycles. Since 2004, Jordan has owned Michael Jordan Motorsports, a professional closed-course motorcycle road racing team that competed with two Suzukis in the premier Superbike championship sanctioned by the American Motorcyclist Association (AMA) until the end of the 2013 season. Jordan and his then-wife Juanita pledged $5 million to Chicago's Hales Franciscan High School in 2006, and the Jordan Brand has made donations to Habitat for Humanity and a Louisiana branch of the Boys & Girls Clubs of America.
Charlotte Bobcats/Hornets.
On June 15, 2006, Jordan bought a minority stake in the Charlotte Bobcats, becoming the team's second-largest shareholder behind majority owner Robert L. Johnson. As part of the deal, Jordan took full control over the basketball side of the operation, with the title "Managing Member of Basketball Operations." Despite Jordan's previous success as an endorser, he has made an effort not to be included in Charlotte's marketing campaigns. A decade earlier, Jordan had made a bid to become part-owner of Charlotte's original NBA team, the Charlotte Hornets, but talks collapsed when owner George Shinn refused to give Jordan complete control of basketball operations.
In February 2010, it was reported that Jordan was seeking majority ownership of the Bobcats. As February wore on, it emerged that the leading contenders for the team were Jordan and former Houston Rockets president George Postolos. On February 27, the Bobcats announced that Johnson had reached an agreement with Jordan and his group, MJ Basketball Holdings, to buy the team pending NBA approval. On March 17, the NBA Board of Governors unanimously approved Jordan's purchase, making him the first former player ever to become the majority owner of an NBA team. It also made him the league's only African-American majority owner.
During the 2011 NBA lockout, "The New York Times" wrote that Jordan led a group of 10 to 14 hardline owners wanting to cap the players' share of basketball-related income at 50 percent and as low as 47. Journalists observed that, during the labor dispute in 1998, Jordan had told Washington Wizards then-owner Abe Pollin, "If you can't make a profit, you should sell your team." Jason Whitlock of "FoxSports.com" called Jordan a "sellout" wanting "current players to pay for his incompetence." He cited Jordan's executive decisions to draft disappointing players Kwame Brown and Adam Morrison.
During the 2011–12 NBA season, which was shortened to 66 games, the Bobcats posted a 7–59 record. Their .106 winning percentage was the worst in NBA history. "I'm not real happy about the record book scenario last year. It's very, very frustrating", Jordan said later that year.
On May 21, 2013, Jordan filed papers to change the Bobcats' name to the Hornets, effective with the 2014–15 season. The Hornets name had become available when the original Hornets, who had moved to New Orleans in 2002, changed their name to the New Orleans Pelicans for the 2013–14 season. The NBA approved the change on July 18. The name change became official on May 20, 2014. On the same day, the team announced that it had reclaimed the history and records of the original 1988–2002 Hornets.
Player profile.
Jordan was a shooting guard who was also capable of playing as a small forward (the position he would primarily play during his second return to professional basketball with the Washington Wizards), and as a point guard. Jordan was known throughout his career for being a strong clutch performer. With the Bulls, he decided 25 games with field goals or free throws in the last 30 seconds, including two NBA Finals games and five other playoff contests. His competitiveness was visible in his prolific trash-talk and well-known work ethic. As the Bulls organization built the franchise around Jordan, management had to trade away players who were not "tough enough" to compete with him in practice. To help improve his defense, he spent extra hours studying film of opponents. On offense, he relied more upon instinct and improvisation at game time. Noted as a durable player, Jordan did not miss four or more games while active for a full season from 1986–87 to 2001–02, when he injured his right knee. He played all 82 games nine times. Jordan has frequently cited David Thompson, Walter Davis, and Jerry West as influences. From the start of his career, Jordan was unique among NBA players in that he had a special "Love of the Game Clause" written into his contract, which allowed him to play basketball against anyone at any time, anywhere.
Jordan had a versatile offensive game. He was capable of aggressively driving to the basket, as well as drawing fouls from his opponents at a high rate; his 8,772 free throw attempts are the ninth-highest total of all time. As his career progressed, Jordan also developed the ability to post up his opponents and score with his trademark fadeaway jump shot, using his leaping ability to "fade away" from block attempts. According to Hubie Brown, this move alone made him nearly unstoppable. Despite media criticism as a "selfish" player early in his career, Jordan's 5.3 assists per game also indicate his willingness to defer to his teammates. In later years, the NBA shortened its three-point line to 22 feet (from 23 feet, 9 inches), which coupled with Jordan's extended shooting range to make him a long-range threat as well—his 3-point stroke developed from a low 9/52 rate (.173) in his rookie year into a stellar 111/260 (.427) shooter in the 1995–96 season. For a guard, Jordan was also a good rebounder (6.2 per game).
In 1988, Jordan was honored with the NBA's Defensive Player of the Year Award and became the first NBA player to win both the Defensive Player of the Year and MVP awards in a career (since equaled by Hakeem Olajuwon, David Robinson, and Kevin Garnett; Olajuwon is the only player other than Jordan to win both during the same season). In addition he set both seasonal and career records for blocked shots by a guard, and combined this with his ball-thieving ability to become a standout defensive player. He ranks third in NBA history in total steals with 2,514, trailing John Stockton and Jason Kidd. Jerry West often stated that he was more impressed with Jordan's defensive contributions than his offensive ones. He was also known to have strong eyesight; broadcaster Al Michaels said that he was able to read baseball box scores on a 27-inch television clearly from about 50 feet away.
Legacy.
Jordan's marked talent was clear from his rookie season. In his first game in Madison Square Garden against the New York Knicks, Jordan received a prolonged standing ovation, a rarity for an opposing player. After Jordan scored a playoff record 63 points against the Boston Celtics on April 20, 1986, Celtics star Larry Bird described him as "God disguised as Michael Jordan."
Jordan led the NBA in scoring in 10 seasons (NBA record) and tied Wilt Chamberlain's record of seven consecutive scoring titles. He was also a fixture on the NBA All-Defensive First Team, making the roster nine times (NBA record shared with Gary Payton, Kevin Garnett and Kobe Bryant). Jordan also holds the top career regular season and playoff scoring averages of 30.1 and 33.4 points per game, respectively. By 1998, the season of his Finals-winning shot against the Jazz, he was well known throughout the league as a clutch performer. In the regular season, Jordan was the Bulls' primary threat in the final seconds of a close game and in the playoffs, Jordan would always demand the ball at crunch time. Jordan's total of 5,987 points in the playoffs is the highest in NBA history. He retired with 32,292 points in regular season play, placing him fourth on the NBA's all-time scoring list behind Kareem Abdul-Jabbar, Karl Malone, and Kobe Bryant.
With five regular-season MVPs (tied for second place with Bill Russell; only Kareem Abdul-Jabbar has won more, six), six Finals MVPs (NBA record), and three All-Star MVPs, Jordan is the most decorated player ever to play in the NBA. Jordan finished among the top three in regular-season MVP voting a record 10 times, and was named one of the 50 Greatest Players in NBA History in 1996. He is one of only seven players in history to win an NCAA championship, an NBA championship, and an Olympic gold medal (doing so twice with the 1984 and 1992 U.S. men's basketball teams).
Many of Jordan's contemporaries say that Jordan is the greatest basketball player of all time. In 1999, an ESPN survey of journalists, athletes and other sports figures ranked Jordan the greatest North American athlete of the 20th century, above such luminaries as Babe Ruth and Muhammad Ali. Jordan placed second to Babe Ruth in the Associated Press's December 1999 list of 20th century athletes. In addition, the Associated Press voted him as the basketball player of the 20th century. Jordan has also appeared on the front cover of "Sports Illustrated" a record 50 times. In the September 1996 issue of "Sport", which was the publication's 50th anniversary issue, Jordan was named the greatest athlete of the past 50 years.
Jordan's athletic leaping ability, highlighted in his back-to-back slam dunk contest championships in 1987 and 1988, is credited by many with having influenced a generation of young players. Several current NBA All-Stars have stated that they considered Jordan their role model while growing up, including LeBron James and Dwyane Wade. In addition, commentators have dubbed a number of next-generation players "the next Michael Jordan" upon their entry to the NBA, including Anfernee "Penny" Hardaway, Grant Hill, Allen Iverson, Kobe Bryant, LeBron James, Vince Carter, and Dwyane Wade. Although Jordan was a well-rounded player, his "Air Jordan" image is also often credited with inadvertently decreasing the jump shooting skills, defense, and fundamentals of young players, a fact Jordan himself has lamented. 
Although Jordan has done much to increase the status of the game, some of his impact on the game's popularity in America appears to be fleeting. Television ratings in particular increased only during his time in the league and have subsequently lowered each time he left the game.
In August 2009, the Basketball Hall of Fame in Springfield, Massachusetts, opened a Michael Jordan exhibit containing items from his college and NBA careers, as well as from the 1992 "Dream Team". The exhibit also has a batting glove to signify Jordan's short career in baseball. After Jordan received word of his being accepted into the Hall of Fame, he selected Class of 1996 member David Thompson to present him. As Jordan would later explain during his induction speech in September 2009, growing up in North Carolina, he was not a fan of the Tar Heels, and greatly admired Thompson, who played at rival North Carolina State. He was inducted into the Hall in September, with several former Bulls teammates in attendance, including Scottie Pippen, Dennis Rodman, Charles Oakley, Ron Harper, Steve Kerr, and Toni Kukoč. Former coaches of Jordan's, Dean Smith and Doug Collins, were also among those present. His emotional reaction during his speech, when he began to cry, was captured by Associated Press photographer Stephan Savoia and would later become widely shared on social media as the Crying Jordan Internet meme.
Personal life.
He married Juanita Vanoy in September 1989, and they have two sons, Jeffrey Michael and Marcus James, and a daughter, Jasmine. Jordan and Vanoy filed for divorce on January 4, 2002, citing irreconcilable differences, but reconciled shortly thereafter. They again filed for divorce and were granted a final decree of dissolution of marriage on December 29, 2006, commenting that the decision was made "mutually and amicably". It is reported that Juanita received a $168 million settlement, making it the largest celebrity divorce settlement in history at the time on public record.
In 1991, Jordan purchased a lot in Highland Park, Illinois, to build a 56,000 square foot mansion, which was completed four years later. Both of his sons attended Loyola Academy, a private Roman Catholic high school located in Wilmette, Illinois. Jeffrey graduated as a member of the 2007 graduating class and played his first collegiate basketball game on November 11, 2007, for the University of Illinois. After two seasons, Jeffrey left the Illinois basketball team in 2009. He later rejoined the team for a third season, then received a release to transfer to the University of Central Florida, where Marcus was attending. Marcus transferred to Whitney Young High School after his sophomore year at Loyola Academy and graduated in 2009. He began attending UCF in the fall of 2009, and played three seasons of basketball for the school.
On July 21, 2006, a judge in Cook County, Illinois, determined that Jordan did not owe his alleged former lover Karla Knafel $5 million in a breach of contract claim. Jordan had allegedly paid Knafel $250,000 to keep their relationship a secret. Knafel claimed Jordan promised her $5 million for remaining silent and agreeing not to file a paternity suit after Knafel learned she was pregnant in 1991. A DNA test showed Jordan was not the father of the child.
He proposed to his longtime girlfriend, Cuban-American model Yvette Prieto, on Christmas Eve, 2011, and they were married on April 27, 2013, at Bethesda-by-the-Sea Episcopal Church. It was announced on November 30, 2013, that the two were expecting their first child together. Jordan listed his Highland Park mansion for sale in 2012. On February 11, 2014, Prieto gave birth to identical twin daughters named Victoria and Ysabel.
Jordan's private jet features a stripe in Carolina blue, the "Air Jordan" logo on the tail, and references to his career in the identification number.
Media figure and business interests.
Jordan is one of the most marketed sports figures in history. He has been a major spokesman for such brands as Nike, Coca-Cola, Chevrolet, Gatorade, McDonald's, Ball Park Franks, Rayovac, Wheaties, Hanes, and MCI. Jordan has had a long relationship with Gatorade, appearing in over 20 commercials for the company since 1991, including the "Be Like Mike" commercials in which a song was sung by children wishing to be like Jordan.
Nike created a signature shoe for him, called the "Air Jordan". One of Jordan's more popular commercials for the shoe involved Spike Lee playing the part of Mars Blackmon. In the commercials Lee, as Blackmon, attempted to find the source of Jordan's abilities and became convinced that "it's gotta be the shoes". The hype and demand for the shoes even brought on a spate of "shoe-jackings" where people were robbed of their sneakers at gunpoint. Subsequently, Nike spun off the Jordan line into its own division named the "Jordan Brand". The company features an impressive list of athletes and celebrities as endorsers. The brand has also sponsored college sports programs such as those of North Carolina, Cal, Georgetown, and Marquette.
Jordan also has been associated with the Looney Tunes cartoon characters. A Nike commercial shown during 1992's Super Bowl XXVI featured Jordan and Bugs Bunny playing basketball. The Super Bowl commercial inspired the 1996 live action/animated film "Space Jam", which starred Jordan and Bugs in a fictional story set during the former's first retirement from basketball. They have subsequently appeared together in several commercials for MCI. Jordan also made an appearance in the music video of Michael Jackson's "Jam" (1992).
Jordan's yearly income from the endorsements is estimated to be over forty million dollars. In addition, when Jordan's power at the ticket gates was at its highest point, the Bulls regularly sold out both their home and road games. Due to this, Jordan set records in player salary by signing annual contracts worth in excess of US $30 million per season. An academic study found that Jordan's first NBA comeback resulted in an increase in the market capitalization of his client firms of more than $1 billion.
Most of Jordan's endorsement deals, including his first deal with Nike, were engineered by his agent, David Falk. Jordan has described Falk as "the best at what he does" and that "marketing-wise, he's great. He's the one who came up with the concept of 'Air Jordan.'"
In June 2010, Jordan was ranked by "Forbes" magazine as the 20th-most powerful celebrity in the world with $55 million earned between June 2009 and June 2010. According to the Forbes article, Jordan Brand generates $1 billion in sales for Nike. In June 2014, Jordan was named the first NBA player to become a billionaire, after he increased his stake in the Charlotte Hornets from 80% to 89.5%. On January 20, 2015, Jordan was honored with the "Charlotte Business Journal"'s Business Person of the Year for 2014. As of November 2015, his current net worth is estimated at $1.1 billion by "Forbes".

</doc>
<doc id="20458" url="https://en.wikipedia.org/wiki?curid=20458" title="Musicology">
Musicology

Musicology () is the scholarly analysis of, and research on, music, a part of humanities. A person who studies music is a musicologist.
Traditionally, historical musicology (commonly termed "music history") has been the most prominent sub-discipline of musicology. In the 2010s, historical musicology is one of several large musicology sub-disciplines. Historical musicology, ethnomusicology, and systematic musicology are approximately equal in size. Ethnomusicology is the study of music in its cultural context. Systematic musicology includes music acoustics, the science and technology of acoustical musical instruments, and the musical implications of physiology, psychology, sociology, philosophy and computing. Cognitive musicology is the set of phenomena surrounding the computational modeling of music. In some countries, music education is a prominent sub-field of musicology, while in others it is regarded as a distinct academic field, or one more closely affiliated with teacher education, educational research, and related fields.
Parent disciplines.
The parent disciplines of musicology include:
Musicology also has two central, practically oriented sub-disciplines with no parent discipline: performance practice and research (sometimes viewed as a form of artistic research), and the theory, analysis and composition of music. The disciplinary neighbors of musicology address other forms of art, performance, ritual and communication, including the history and theory of the visual and plastic arts and of architecture; linguistics, literature and theater; religion and theology; and sport. Musical knowledge is applied in medicine, education, and music therapy—which, effectively, are parent disciplines of applied musicology.
Subdisciplines.
Historical musicology.
Music history or historical musicology is concerned with the composition, performance, reception, and criticism of music over time. Historical studies of music are for example concerned with a composer's life and works, the developments of styles and genres (e. g. baroque concertos), the social function of music for a particular group of people (e. g. court music), or modes of performance at a particular place and time (e. g. Johann Sebastian Bach's choir in Leipzig). Like the comparable field of art history, different branches and schools of historical musicology emphasize different types of musical works and approaches to music. There are also national differences in various definitions of historical musicology. In theory, "music history" could refer to the study of the history of any type or genre of music (e.g., the history of Indian music or the history of rock). In practice, these research topics are more often considered within ethnomusicology (see below) and "historical musicology" is typically assumed to imply Western Art music of the European tradition.
The methods of historical musicology include source studies (especially manuscript studies), paleography, philology (especially textual criticism), style criticism, historiography (the choice of historical method), musical analysis (analysis of music to find "inner coherence"), and iconography. The application of musical analysis to further these goals is often a part of music history, though pure analysis or the development of new tools of music analysis is more likely to be seen in the field of music theory. Music historians create a number of written products, ranging from journal articles describing their current research, new editions of musical works, biographies of composers and other musicians, book-length studies or university textbook chapters or entire textbooks. Music historians may examine issues in a close focus, as in the case of scholars who examine the relationship between words and music for a given composer's art songs. On the other hand, some scholars take a broader view, and assess the place of a given type of music, such as the symphony in society using techniques drawn from other fields, such as economics, sociology, or philosophy.
"New musicology" is a term applied since the late 1980s to a wide body of work emphasizing cultural study, analysis, and criticism of music. Such work may be based on feminist, gender studies, queer theory, or postcolonial theory, or the work of Theodor Adorno. Although New Musicology emerged from within historical musicology, the emphasis on cultural study within the Western art music tradition places New Musicology at the junction between historical, ethnological and sociological research in music.
New musicology was a reaction against traditional historical musicology, which according to Susan McClary, "fastidiously declares issues of musical signification off-limits to those engaged in legitimate scholarship." Charles Rosen, however, retorts that McClary, "sets up, like so many of the 'new musicologists', a straw man to knock down, the dogma that music has no meaning, and no political or social significance". Today, many musicologists no longer distinguish between musicology and new musicology, since many of the scholarly concerns once associated with new musicology have now become mainstream, and they feel the term "new" no longer applies.
Cultural musicology or ethnomusicology.
Ethnomusicology, formerly comparative musicology, is the study of music in its cultural context. It is often considered the anthropology or ethnography of music. Jeff Todd Titon has called it the study of "people making music". Although it is most often concerned with the study of non-Western musics, it also includes the study of Western music from an anthropological or sociological perspective, cultural studies and sociology as well as other disciplines in the social sciences and humanities. Some ethnomusicologists primarily conduct historical studies, but the majority are involved in long-term participant observation, or combine ethnographic and historical approaches in their fieldwork. Therefore, ethnomusiological scholarship can be characterized as featuring a substantial, intensive fieldwork component, often involving long-term residence within the community studied.
Closely related to ethnomusiology is the emerging branch of sociomusicology. For instance, Ko (2011) proposed the hypothesis of "Biliterate and Trimusical" in Hong Kong sociomusicology.
Popular music studies.
Popular music studies, known, "misleadingly," as "popular musicology", emerged in the 1980s as an increasing number of musicologists, ethnomusicologists, and other varieties of historians of American and European culture began to write about popular musics past and present. The first journal focusing on popular music studies was Popular Music, which began publication in 1981. It was not until 1994 that an academic society solely devoted to the topic was formed, the International Association for the Study of Popular Music. The Association's founding was partly motivated by the interdisciplinary agenda of popular musicology though the group has been characterized by a polarized 'musicological' and 'sociological' approach also typical of popular musicology.
Music theory, analysis and composition.
Music theory is a field of study that describes the elements of music and includes the development and application of methods for composing and for analyzing music through both notation and, on occasion, musical sound itself. Broadly, theory may include any statement, belief, or conception of or about music (Boretz, 1995). A person who studies or practices music theory is a music theorist.
Some music theorists attempt to explain the techniques composers use by establishing rules and patterns. Others model the experience of listening to or performing music. Though extremely diverse in their interests and commitments, many Western music theorists are united in their belief that the acts of composing, performing, and listening to music may be explicated to a high degree of detail (this, as opposed to a conception of musical expression as fundamentally ineffable except in musical sounds). Generally, works of music theory are both descriptive and prescriptive, attempting both to define practice and to influence later practice. Thus, music theory generally lags behind practice but also points towards future exploration, composition, and performance.
Musicians study music theory to understand the structural relationships in the (nearly always notated) music. Composers study music theory to understand how to produce effects and structure their own works. Composers may study music theory to guide their precompositional and compositional decisions. Broadly speaking, music theory in the Western tradition focuses on harmony and counterpoint, and then uses these to explain large scale structure and the creation of melody.
Music psychology.
Music psychology applies the content and methods of all subdisciplines of psychology (perception, cognition, motivation, etc.) to understand how music is created, perceived, responded to, and incorporated into individuals' and societies' daily lives. Its primary branches include cognitive musicology, which emphasizes the use of computational models for human musical abilities and cognition, and the cognitive neuroscience of music, which studies the way that music perception and production manifests in the brain using the methodologies of cognitive neuroscience. While aspects of the field can be highly theoretical, much of modern music psychology seeks to optimize the practices and professions of music performance, composition, education, and therapy.
Performance practice and research.
Performance practice draws on many of the tools of historical musicology to answer the specific question of how music was performed in various places at various times in the past. Although previously confined to early music, recent research in performance practice has embraced questions such as how the early history of recording affected the use of vibrato in classical music, or instruments in Klezmer.
Within the rubric of musicology, performance practice tends to emphasize the collection and synthesis of evidence about how music should be performed. The important other side, learning how to sing authentically or perform a historical instrument is usually part of conservatory or other performance training. However, many top researchers in performance practice are also excellent musicians.
Music performance research (or music performance science) is strongly associated with music psychology. It aims to document and explain the psychological, physiological, sociological and cultural details of how music is actually performed (rather than how it should be performed). The approach to research tends to be systematic and empirical, and to involve the collection and analysis of both quantitative and qualitative data. The findings of music performance research can often be applied in music education.
Education and careers.
Musicologists in tenure track professor positions typically hold a Ph.D in musicology. In the 1960s and 1970s, some musicologists obtained professor positions with an M.A. as their highest degree, but in the 2010s, the Ph.D is the standard minimum credential for tenure track professor positions. As part of their initial training, musicologists typically complete a B.Mus or a B.A. in music (or a related field such as history) and in many cases an M.A. in musicology. Some individuals apply directly from a bachelor's degree to a Ph.D, and in these cases, they may not receive an M.A. In the 2010s, given the increasingly interdisciplinary nature of university graduate programs, some applicants for musicology Ph.D programs may have academic training both in music and outside of music (e.g., a student may apply with a B.Mus and an M.A. in psychology). In music education, individuals may hold an M.Ed and an Ed.D.
Most musicologists work as instructors, lecturers or professors in colleges, universities or conservatories. The job market for tenure track professor positions is very competitive. Entry-level applicants must hold a completed Ph.D or the equivalent degree and applicants to more senior professor positions must have a strong record of publishing in peer-reviewed journals. Some Ph.D-holding musicologists are only able to find insecure positions as sessional lecturers. The job tasks of a musicologist are the same as those of a professor in any other humanities discipline: she teaches undergraduate and/or graduate classes in her area of specialization and, in many cases some general courses (such as Music Appreciation or Introduction to Music History), conducts research in her area of expertise, publishes articles about her research in peer-reviewed journals, authors book chapters, books or textbooks, travels to conferences to give talks on her research and learn about research in her field, and, if her program includes a graduate school, supervises M.A. and Ph.D students and gives them guidance on the preparation of their theses and dissertations. Some musicology professors may take on senior administrative positions in their institution, such as Dean or Chair of the School of Music.
Role of women.
The vast majority of major musicologists and music historians from past generations have been men, as in the 19th century and early 20th century, women's involvement in teaching music was mainly in elementary and secondary music teaching. Nevertheless, some women musicologists have reached the top ranks of the profession. Carolyn Abbate (born 1956) is an American musicologist who did her PhD at Princeton University. She has been described by the "Harvard Gazette" as "one of the world's most accomplished and admired music historians". Susan McClary (born 1946) is a musicologist associated with the "New Musicology" who incorporates feminist music criticism in her work. McClary holds a PhD from Harvard University. One of her best known works is "Feminine Endings" (1991), which covers musical constructions of gender and sexuality, gendered aspects of traditional music theory, gendered sexuality in musical narrative, music as a gendered discourse and issues affecting women musicians. In the book, McClary suggests that the sonata form (used in symphonies and string quartets) may be a sexist or misogynistic procedure that constructs of gender and sexual identity. McClary's "Conventional Wisdom" (2000) argues that the traditional musicological assumption of the existence of "purely musical" elements, divorced from culture and meaning, the social and the body, is a conceit used to veil the social and political imperatives of the worldview that produces the classical canon most prized by supposedly objective musicologists.
Other notable women scholars include:
External links.
Although many musicology journals are not available on-line, or are only available through pay-for-access portals, a sampling of peer reviewed journals in various subfields gives some idea of musicological writings:
The following musicology journals can be accessed on-line through JSTOR (requires subscription for full access). Many of them have their latest issues available on-line via publisher portals (usually requiring a fee for access).

</doc>
<doc id="20460" url="https://en.wikipedia.org/wiki?curid=20460" title="Film promotion">
Film promotion

Film promotion is the practice of promotion specifically in the film industry, and usually occurs in coordination with the process of film distribution. Sometimes called the press junket or film junket, film promotion generally includes press releases, advertising campaigns, merchandising, franchising and media, and interviews with the key people involved with the making of the film, like actors and directors. As with all business, it is an important part of any release because of the inherent high financial risk; film studios will invest in expensive marketing campaigns to maximize revenue early in the release cycle. Marketing budgets tend to equal about half the production budget. Publicity is generally handled by the distributor and exhibitors.
Techniques.
Promotional tours and interviews.
Film actors, directors, and producers appear for television, cable, radio, print, and online media interviews, which can be conducted in person or remotely. During film production, these can take place "on set". After the film's premiere, key personnel make appearances in major market cities or participate remotely via satellite videoconference or telephone. The purpose of interviews is to encourage journalists to publish stories about their "exclusive interviews" with the film's stars, thereby creating "marketing buzz" around the film and stimulating audience interest in watching the film.
When it comes to feature films picked up by a major film studio for international distribution, promotional tours are notoriously grueling. Key cast and crew are often contracted to travel to several major cities around the world to promote the film and sit for dozens of interviews. In every interview they are supposed to stay "on message" by energetically expressing their enthusiasm for the film in a way that appears candid, fun, and fresh, even though it may be their fifth or sixth interview that day. They are expected to disclose just enough juicy "behind-the-scenes" information about the filmmaking process or the filmmakers' artistic vision to make each journalist feel like he or she got a nice scoop, while at the same time tactfully avoiding disclosure of anything truly negative or embarrassing.
Audience research.
There are seven distinct types of research conducted by film distributors in connection with domestic theatrical releases, according to "Marketing to Moviegoers: Second Edition." Such audience research can cost $1 million per film, especially when scores of TV advertisements are tested and re-tested. The bulk of research is done by major studios for the roughly 170 major releases they mount each year that are supported by tens of millions of advertising buys for each film. Independent film distributors, which typically spend less than $10 million in media buys per film, don’t have the budget or breadth of advertising materials to analyze, so they spend little or nothing on pre-release audience research.
When audience research is conducted for domestic theatrical release, it involves these areas:
Marketing can play a big role in whether or not a film gets the green light. Audience research is a strong factor in determining the ability of a film to sell in theaters, which is ultimately how films make their money. As part of a movie's Marketing strategy, audience research comes into account as producers create promotional materials. These promotional materials consistently change and evolve as a direct consequence of audience research up until the film opens in theaters.

</doc>
<doc id="20463" url="https://en.wikipedia.org/wiki?curid=20463" title="Miltiades (disambiguation)">
Miltiades (disambiguation)

Miltiades the Younger (c. 550 – 489 BC) was tyrant of the Thracian Chersonese and the Athenian commanding general in the Battle of Marathon.
Miltiades may also refer to:

</doc>
<doc id="20468" url="https://en.wikipedia.org/wiki?curid=20468" title="Maggie Out">
Maggie Out

"Maggie Out" was a chant popular during the Miners' Strike, student grant protests, Poll Tax protests and other public demonstrations that fell within the time when Margaret Thatcher was the Prime Minister of the United Kingdom.
The chant called for her to be removed from that role. It was referred to, in that context, during a parliamentary session in 1984.
When Margaret Thatcher felt compelled to resign some people had memories of chanting it for thirteen years. People were passionate about this group activity and associated it with varied political struggles from that time.
It is a variant of the "Oggy Oggy Oggy, Oi Oi Oi" chant. When used in that format, the lyrics were:
The chorus of the chant became the title of a compilation album from Anagram Records (Catalog#:GRAM 28) released in 1987.
The Larks produced a track called "Maggie, Maggie, Maggie (Out, Out, Out)" which was included on the Miners’ Benefit LP “Here We Go” on Sterile Records.
Comedian Alexei Sayle remarked humorously that he couldn't find his way around London unless he walked down the middle of the streets shouting the words.
Since 1990 two variants of this song have been heard - adapted for both her successors; replacing 'Major' for 'Maggie' during the tenure of John Major and 'Tony' for 'Maggie' since Tony Blair's plan for the Iraq War in 2003.
The song has occasionally been revived to "greet" Thatcher's public outings following her resignation as Prime Minister, but with the word "gone" substituted for "out".
Following the death of Thatcher on 8 April 2013, this chant was revived in the format of "Maggie, Maggie Maggie (Dead, Dead, Dead)" at celebratory parties held in Glasgow and Brixton.

</doc>
<doc id="20469" url="https://en.wikipedia.org/wiki?curid=20469" title="M25 motorway">
M25 motorway

The M25 or London Orbital Motorway is a motorway that almost encircles Greater London, England (with the exception of North Ockendon), in the United Kingdom. A narrower concept was first mooted in the 1960s as part of the plan to build four ring roads around London. A few sections, based on the abandoned London Ringways plan, were constructed in the early 1970s and it was completed in 1986.
It is one of the busiest of the British motorway network: 196,000 vehicles were recorded on a busy day near London Heathrow Airport in 2003 and the western half experienced an average daily flow of 147,000 vehicles in 2007.
Although technically not an entire orbital motorway – a short non-motorway stretch forms the Dartford Crossing (A282) of the River Thames – the M25, at is Europe's second longest orbital road after the Berliner Ring, which is .
Description.
Originally built almost wholly as a dual three-lane motorway, much of the motorway has been widened: to dual four-lanes for almost half, to a dual five-lanes section between junctions 12 and 14 and a dual six-lane section between junctions 14 and 15. Further widening is in progress of minor sections with plans for managed motorways in many others.
To the east of London the two ends of the M25 are joined to complete a loop by the non-motorway A282 Dartford Crossing of the River Thames between Thurrock and Dartford. This crossing, which consists of twin two-lane tunnels and the four-lane QE2 (Queen Elizabeth II) bridge, is named "Canterbury Way". Passage across the bridge or through the tunnels is subject to a toll, its level depending on the kind of vehicle. This stretch being non-motorway allows traffic, including that not permitted to use motorways, to cross the River Thames east of the Woolwich Ferry; the only crossing further to the east is a passenger ferry between Gravesend, in Kent, and Tilbury, in Essex.
At Junction 5, the clockwise carriageway of the M25 is routed off the main north–south dual carriageway onto the main east–west dual carriageway with the main north–south carriageway becoming the A21. In the opposite direction, to the east of the point where the M25 diverges from the main east–west carriageway, that carriageway become the M26 motorway.
The radial distance from London (taken as Charing Cross) varies from in Potters Bar to in Byfleet. Three Greater London Boroughs (Enfield, Hillingdon and Havering) have realigned their boundaries to the M25 for minor stretches; while in others, most notably in Essex and Surrey, the radial gap between Greater London and the motorway reaches , neither of which coincide with the Metropolitan Green Belt. Major towns listed as destinations (right), in various counties, adjoin the M25. North Ockendon is the only settlement of Greater London situated outside the M25. In 2004, following an opinion poll, the London Assembly mooted for consultation alignment of the Greater London boundary with the M25. "Inside the M25" and "outside/beyond the M25" are colloquial, looser alternatives to "Greater London" sometimes used in haulage. The Communications Act 2003 explicitly uses the M25 as the boundary in requiring a proportion of television programmes to be made outside the London area.
Two Motorway service areas are on the M25, and two others are directly accessible from it. Those on the M25 are Clacket Lane between junctions 5 and 6 (in the south-east) and Cobham between junctions 9 and 10 (in the south-west). Those directly accessible from it are South Mimms off junction 23 (to the north of London) and Thurrock off junction 31 (to the east of London). Cobham services opened on 13 September 2012.
Originally, the M25 was unlit until except for sections around Heathrow, major interchanges and Junctions 23–30. Originally, low pressure sodium (SOX) lighting was the most prominent technology used, but widening projects from the 1990s onwards have all used high-pressure sodium (SON) lighting and this has diminished the original installations. By 2014 only one significant stretch was still SOX-lit (Junction 25–26) and the units were removed the same year.
The motorway passes through five counties. Junctions 1A–5 are in Kent, 6–14 are in Surrey, 15–16 are in Buckinghamshire, 17–25 are in Hertfordshire, and 26–31 are in Essex. Policing of the road is carried out by an integrated policing group made up of the Metropolitan, Thames Valley, Essex, Kent, Hertfordshire and Surrey forces.
The M25 is one of Europe's busiest motorways. In 2003, a maximum of 196,000 vehicles a day were recorded on the motorway just south of London Heathrow Airport between junctions 13 and 14.
History.
Plans.
A precursor of the M25 was the "North Orbital Road" (see A414 road).
The idea of an orbital road around London was first proposed early in the 20th century and then re-examined in Sir Charles Bressey's and Sir Edwin Lutyens' "The Highway Development Survey, 1937". Sir Patrick Abercrombie's "County of London Plan, 1943" and "Greater London Plan, 1944" proposed a series of five roads encircling the capital. The northern sections of the M25 follow a similar route to the World War II Outer London Defence Ring.
Little was done to progress these plans until the 1960s when the Greater London Council developed their London Ringways plan consisting of four "rings" around the capital. Sections of the two outer rings – Ringway 3 (the 'M16 motorway') and Ringway 4 – were constructed in the early 1970s and were integrated into the single M25 orbital motorway. But the Ringways plan was hugely controversial owing to the destruction required for the inner two ring roads, (Ringway 1 and Ringway 2). Parts of Ringway 1 were constructed (including West Cross Route and Westway), against stiff opposition, before the overall plan was abandoned in 1973 following pressure from residents in the threatened areas.
Construction.
Construction of parts of the two outer ring roads, Ringways 3 and 4, began in 1973. The first section, between South Mimms and Potters Bar in Hertfordshire (junction 23 to junction 24) opened in September 1975 and was given the temporary general purpose road designation A1178 (a section of motorway-standard-road, originally the M16, which eventually was incorporated into the M25) was completed and operational before this. A Watford-avoiding route between the M1 and the A40 between north Watford and Denham was locally known as the Croxley Green/Rickmansworth bypass, and was operational about 1973/4; a section south of London (junction 6 to junction 8) opened in 1976. A section of Ringway 3 south of the river between Dartford and Swanley (junction 1 to junction 3) was constructed between 1974 and 1977. In 1975 the plans for Ringway 3 were modified to combine it with Ringway 4, the outermost Ringway. The M25 as a component of ringway 4, was first conceived to be an east-west road south of London to relieve the A25, and running parallel to it, with its eastern end following the route of what is now the M26. However, it was subsequently routed northwards towards the Dartford Tunnel to form, in conjunction with similar roads, including the M16 planned to the north of London, part of the London Orbital. The combined motorway was given the designation M25 which had originally been intended for the southern and western part of Ringway 4 and the M16 designation was dropped. The section of Ringway 3 west of South Mimms anti-clockwise around London to Swanley in Kent was cancelled. The stages were not constructed contiguously but in small sections. As the orbital road developed the sections were linked. Each section was presented to planning authorities in its own right and was individually justified, with almost 40 public inquiries relating to sections of the route. Maps at this time depicting these short sections named the route as the M16 but this changed before completion.
The section from Potters Bar to the Dartford Tunnel was constructed between 1979 and 1982. Construction of the M25 continued in stages until its completion in 1986. Prime Minister Margaret Thatcher officially opened the M25 on 29 October 1986, with a ceremony in the section between J22 and J23 (London Colney and South Mimms). The initial tenders for the construction of the M25 totalled £631.9 million. This did not include compulsory purchase of land and subsequent upgrades and repairs.
Operational history.
Soon after the motorway opened in 1986 traffic levels exceeded the maximum design capacity and in 1990 the Secretary of State for Transport announced plans to widen the whole of the M25 to four lanes. By 1993 the motorway, which was designed for a maximum of 88,000 vehicles per day, was carrying 200,000 vehicles per day. 15% of UK motorway traffic volume was on the M25 and there were plans to add six lanes to the section from Junctions 12 to 15 as well as widening the rest of the motorway to four lanes.
In parts, particularly the western third this plan went ahead, due to consistent congestion. Again, however, plans to widen further sections to eight lanes (four each way) were scaled back in 2009 in response to rising costs. The plans were reinstated in the agreed Highways Agency 2013-14 business plan.
In 1995 a contract was awarded to widen the section between Junctions 8 and 10 from six to eight lanes for a cost of £93.4 million and a Motorway Incident Detection and Automatic Signalling (MIDAS) system was introduced to the M25 from Junction 10 to Junction 15 at a cost of £13.5m in 1995. This was then extended to Junction 16 at a cost of £11.7m in 2002. This consists of a distributed network of traffic and weather sensors, speed cameras and variable-speed signs that control traffic speeds with little human supervision, and has improved traffic flow slightly, reducing the amount of start-stop driving.
In 1995 there was a proposal to widen the section close to Heathrow Airport to fourteen lanes. This attracted fierce opposition from road protesters opposing the Newbury Bypass and other schemes and it was cancelled shortly afterwards. In 1997, however, the Department of Transport announced new proposals to widen the section between Junction 12 (M3) and Junction 15 (M4) to twelve lanes. At the Terminal Five public inquiry a Highways Agency official said that the widening was needed to accommodate traffic to the proposed new terminal, however the transport minister said that no such evidence had been given. Environmental groups objected to the decision to go ahead with a scheme that would create the widest motorways in the UK without holding a public inquiry. The decision was again deferred. A decision to go-ahead was given for a ten-lane scheme in 1998 and the £148 million 'M25 Jct 12 to 15 Widening' contract was awarded to Balfour Beatty in 2003. The scheme was completed in 2005 as dual-five lanes between Junctions 12 and 14 and dual-six lanes from Junctions 14 to 15.
In 2007 capacity at Junction 25 (A10/Waltham Cross) was increased and the Holmesdale Tunnel was widened to three lanes in an easterly direction at a cost of £75 million.
Work to widen the exit slip-roads in both directions at Junction 28 (A12 road/A1023) was completed in 2008. It was designed to reduce the amount of traffic queueing on the slip roads at busy periods, particularly traffic from the clockwise M25 joining the northbound A12 where the queue extended onto the inside lane of the Motorway.
Design, Build, Finance and Operate (DBFO) contract.
In 2006 the Highways Agency proposed to widen of M25 from six to eight lanes, between junctions 5–6 and 16–30 as part of a Design, Build, Finance and Operate (DBFO) project. A shortlist of contractors was announced in October 2006 for the project which was expected to cost £4.5 billion. Contractors were asked to resubmit their bids in January 2008 and in June 2009 the new transport minister indicated that the cost had risen to £5.5 billion and the benefit to cost ratio had dropped considerably. In January 2009 the government announced that plans to widen the sections from Junction 5–7 and from 23–27 had been 'scrapped' and that hard shoulder running would be introduced instead. However widening was reinstated to four lanes in the 2013–14 Highways Agency Business Plan.
In 2009 a £6.2 billion M25 DBFO private finance initiative contract was awarded to Connect Plus to widen the sections between junctions 16 and 23 and between junctions 27 and 30 and maintain the M25 and the Dartford Crossing for a 30-year period.
Works to widen the section between Junctions 16 (M40) and 23 (A1(M)) to dual four lanes started in July 2009 at an estimated cost of £580 million. The Junction 16 to 21 (M1) section was completed by July 2011 and the Junction 21 to 23 by June 2012. Works to widen the Junctions 27 (M11) to 30 (A13) section to dual four lanes also started in July 2009. The Junction 27 to 28 (A12) section was completed in July 2010, the Junction 28 to 29 (A127) in June 2011 and finally the Junction 29 to 30 (A13) section opened in May 2012.
Works to introduce managed motorway technology and permanent hard shoulder running on two sections of the M25 began in 2013. The first section between Junctions 5 (A21/M26) and 7 (M23) started construction in May 2013 with the scheme being completed and opened in April 2014. The second section, between Junctions 23 (A1/A1(M)) and 27 (M11), began construction in February 2013 and was completed and opened in November 2014.
Developments under construction.
Junction 30 improvement.
Preparation work to increase capacity at Junction 30 (Thurrock) as part of the Thames Gateway Delivery Plan is underway although there is no guarantee of delivery. Plans were announced in 2007. An early estimate on the start of major works was given as 2013/2014. Philip Hammond has confirmed that there will be no funding for the J30 improvements in this spending period, but has announced that preparation work would continue so that the scheme could be funded at a later date.
Conditional Proposals.
Lower Thames Crossing.
In 2009 the Department for Transport published options for a new Lower Thames Crossing to add capacity to the Dartford Crossing or create a new road and crossing linking to the M2 and M20 motorways.
Comparisons.
The M25 is the second-longest ring road in Europe, after the Berlin Ring (A 10), which is longer.
Other cities in the UK encircled by motorways include: Birmingham, using parts of the M5, M6 and M42, and Manchester, using the M60. Additionally, from 2011 Glasgow has an orbital motorway made of the M8, M73 and M74, although one section of the route passes through the centre of the city. 
The M25 is one of the busiest motorways in Europe. Here are some comparisons:
Popular culture.
Iain Sinclair's 2002 book and film "London Orbital" is based on a year-long journey around the M25 on foot.
The M25 (including the A282 Dartford Crossing) is known for its frequent traffic jams. These have been the subject of so much comment from such an early stage that even at the official opening ceremony Margaret Thatcher complained about "those who carp and criticise". The jams have inspired jokes (e.g., "the world's first circular car park", "the London Orbital Car Park") and songs (e.g., Chris Rea's "The Road to Hell").
The M25 plays a role in the comedy-fantasy novel "Good Omens", as "evidence for the hidden hand of Satan in the affairs of Man". The demon character, Crowley, had manipulated the design of the M25 to resemble a Satanic sigil.
The M25 enjoyed a more positive reputation among ravers in the late 1980s, when this new orbital motorway became a popular route to the parties that took place around the outskirts of London. This use of the M25 for these raves inspired the name of electronic duo Orbital.
Racing.
The orbital nature of the motorway, in common with racetracks, lent itself to unofficial, and illegal, motor racing. At the end of the 1980s, before the advent of speed enforcement devices, owners of supercars, many employed in the financial service industry in the City and in Docklands, would meet at night at service stations such as South Mimms and conduct time trials. Times below 1 hour were achieved - an average speed of over 117 mph (188 km/h), which included coming to a halt at the Dartford Tunnel road user charge payment booths.
Junctions.
Data from driver location signs provide carriageway identifier information. The numbers on the signs are kilometres from a point near the River Thames, east of London, when travelling clockwise on the motorway. The table below gives details of each junction, including the roads interchanged and the destinations that are signed from the motorway on the blue advance direction signs. Figures in kilometres are from the driver location signs; figures in miles are derived from them.
!scope=col| miles
!scope=col| km
!scope=col| Clockwise exits (A carriageway)
!scope=col| Junction
!scope=col| Anti-clockwise exits (B carriageway)
!scope=col| European Route

</doc>
<doc id="20474" url="https://en.wikipedia.org/wiki?curid=20474" title="Mohs scale of mineral hardness">
Mohs scale of mineral hardness

The Mohs scale of mineral hardness is a qualitative ordinal scale that characterizes the scratch resistance of various minerals through the ability of a harder material to scratch a softer material. It was created in 1812 by the German geologist and mineralogist Friedrich Mohs and is one of several definitions of hardness in materials science, some of which are more quantitative. The method of comparing hardness by seeing which minerals can visibly scratch others, however, is of great antiquity, having been mentioned by Theophrastus in his treatise "On Stones", c. 300 BC, followed by Pliny the Elder in his "Naturalis Historia", c. 77 AD. While greatly facilitating the identification of minerals in the field, the Mohs scale does not show how well hard materials perform in an industrial setting.
Usage.
Despite its simplicity and lack of precision, the Mohs scale is highly relevant for field geologists, who use the scale to roughly identify minerals using scratch kits. The Mohs scale hardness of minerals can be commonly found in reference sheets. Reference materials may be expected to have a uniform Mohs hardness.
Minerals.
The Mohs scale of mineral hardness is based on the ability of one natural sample of mineral to scratch another mineral visibly. The samples of matter used by Mohs are all different minerals. Minerals are pure substances found in nature. Rocks are made up of one or more minerals. As the hardest known naturally occurring substance when the scale was designed, diamonds are at the top of the scale. The hardness of a material is measured against the scale by finding the hardest material that the given material can scratch, and/or the softest material that can scratch the given material. For example, if some material is scratched by apatite but not by fluorite, its hardness on the Mohs scale would fall between 4 and 5. "Scratching" a material for the purposes of the Mohs scale means creating non-elastic dislocations visible to the naked eye. Frequently, materials that are lower on the Mohs scale can create microscopic, non-elastic dislocations on materials that have a higher Mohs number. While these microscopic dislocations are permanent and sometimes detrimental to the harder material's structural integrity, they are not considered "scratches" for the determination of a Mohs scale number.
The Mohs scale is a purely ordinal scale. For example, corundum (9) is twice as hard as topaz (8), but diamond (10) is four times as hard as corundum. The table below shows the comparison with the absolute hardness measured by a sclerometer, with pictorial examples.
On the Mohs scale, a streak plate (unglazed porcelain) has a hardness of 7.0. Using these ordinary materials of known hardness can be a simple way to approximate the position of a mineral on the scale.
Intermediate hardness.
The table below incorporates additional substances that may fall between levels:
Hardness (Vickers).
Comparison between Hardness (Mohs) and Hardness (Vickers):

</doc>
<doc id="20476" url="https://en.wikipedia.org/wiki?curid=20476" title="Murray Gell-Mann">
Murray Gell-Mann

Murray Gell-Mann (; born September 15, 1929) is an American physicist who received the 1969 Nobel Prize in physics for his work on the theory of elementary particles. He is the Robert Andrews Millikan Professor of Theoretical Physics Emeritus at the California Institute of Technology, a Distinguished Fellow and co-founder of the Santa Fe Institute, Professor in the Physics and Astronomy Department of the University of New Mexico, and the Presidential Professor of Physics and Medicine at the University of Southern California.
Gell-Mann has spent several periods at CERN, among others as a John Simon Guggenheim Memorial Foundation Fellow in 1972.
He introduced, independently of George Zweig, the quark—constituents of all hadrons—having first identified the SU(3) flavor symmetry of hadrons. This symmetry is now understood to underlie the light quarks, extending isospin to include strangeness, a quantum number which he also discovered.
He developed the V−A theory of the weak interaction in collaboration with Richard Feynman. In the 1960s, he introduced current algebra as a method of systematically exploiting symmetries to extract predictions from quark models, in the absence of reliable dynamical theory. This method led to model-independent sum rules confirmed by experiment and provided starting points underpinning the development of the standard theory of elementary particles.
Gell-Mann, along with Maurice Lévy, developed the sigma model of pions, which describes low-energy pion interactions. Modifying the integer-charged quark model of Moo-Young Han and Yoichiro Nambu, Harald Fritzsch and Gell-Mann were the first to write down the modern accepted theory of quantum chromodynamics, although they did not anticipate asymptotic freedom. In 1969 he received the Nobel Prize in physics for his contributions and discoveries concerning the classification of elementary particles and their interactions.
Gell-Mann is responsible, together with Pierre Ramond and Richard Slansky, and independently of Peter Minkowski, Rabindra Mohapatra, Goran Senjanovic, Sheldon Lee Glashow, and Tsutomu Yanagida, for the see-saw theory of neutrino masses, that produces masses at the large scale in any theory with a right-handed neutrino. He is also known to have played a large role in keeping string theory alive through the 1970s and early 1980s, supporting that line of research at a time when it was unpopular.
Gell-Mann is a proponent of the consistent histories approach to understanding quantum mechanics.
Early life and education.
Gell-Mann was born in lower Manhattan into a family of Jewish immigrants from the Austro-Hungarian Empire. His parents were Pauline (née Reichstein) and Arthur Isidore Gell-Mann, who taught English as a Second Language (ESL).
Propelled by an intense boyhood curiosity and love for nature and mathematics, he graduated valedictorian from the Columbia Grammar & Preparatory School and subsequently entered Yale at the age of 15 as a member of Jonathan Edwards College. At Yale, he participated in the William Lowell Putnam Mathematical Competition and was on the team representing Yale University (along with Murray Gerstenhaber and Henry O. Pollak) that won the second prize in 1947. Gell-Mann earned a bachelor's degree in physics from Yale in 1948, and a PhD in physics from Massachusetts Institute of Technology (MIT) in 1951. His supervisor at MIT was Victor Weisskopf.
Career and research.
In 1958, Gell-Mann and Richard Feynman, in parallel with the independent team of George Sudarshan and Robert Marshak, discovered the chiral structures of the weak interaction in physics. This work followed the experimental discovery of the violation of parity by Chien-Shiung Wu, as suggested by Chen Ning Yang and Tsung-Dao Lee, theoretically.
Gell-Mann's work in the 1950s involved recently discovered cosmic ray particles that came to be called kaons and hyperons. Classifying these particles led him to propose that a quantum number called strangeness would be conserved by the strong and the electromagnetic interactions, but not by the weak interactions. Another of Gell-Mann's ideas is the Gell-Mann-Okubo formula, which was, initially, a formula based on empirical results, but was later explained by his quark model. Gell-Mann and Abraham Pais were involved in explaining several puzzling aspects of the physics of these particles.
In 1961, this led him (and Kazuhiko Nishijima) to introduce a classification scheme for hadrons, elementary particles that participate in the strong interaction. (This scheme had been independently proposed by Yuval Ne'eman.) This scheme is now explained by the quark model. Gell-Mann referred to the scheme as the "Eightfold Way", because of the "octets" of particles in the classification. (The term is a reference to the eightfold way of Buddhism.)
In 1964, Gell-Mann and, independently, George Zweig went on to postulate the existence of quarks, particles of which the hadrons of this scheme are composed. The name was coined by Gell-Mann and is a reference to the novel "Finnegans Wake", by James Joyce ("Three quarks for Muster Mark!" book 2, episode 4.) Zweig had referred to the particles as "aces", but Gell-Mann's name caught on. Quarks, antiquarks, and gluons were soon established as the underlying elementary objects in the study of the structure of hadrons. He was awarded a Nobel Prize in physics in 1969 for his contributions and discoveries concerning the classification of elementary particles and their interactions.
In 1972 he and Harald Fritzsch introduced the conserved quantum number "color charge", and later, together with Heinrich Leutwyler, they coined the term quantum chromodynamics (QCD) as the gauge theory of the strong interaction. The quark model is a part of QCD, and it has been robust enough naturally to accommodate the discovery of new "flavors" of quarks, which superseded the eightfold way scheme.
During the 1990s, Gell-Mann's interest turned to the emerging study of complexity. He played a central role in the founding of the Santa Fe Institute, where he continues to work as a distinguished professor.
He wrote a popular science book about these matters, "The Quark and the Jaguar: Adventures in the Simple and the Complex". The title of the book is taken from a line of a poem by Arthur Sze: "The world of the quark has everything to do with a jaguar circling in the night".
The author George Johnson has written a biography of Gell-Mann, "Strange Beauty: Murray Gell-Mann, and the Revolution in 20th-Century Physics", which was shortlisted for the Royal Society Book Prize. Dr. Gell-Mann has criticized it as inaccurate. The Nobel Prize–winning physicist Philip Anderson, in his chapter on Gell-Mann, says that Johnson's biography is excellent. Both Anderson and Johnson say that Gell-Mann is a perfectionist and that his semibiographical, "The Quark and the Jaguar" is consequently incomplete.
Personal life.
Gell-Mann married Marcia Southwick in 1992, after the death of his first wife, J. Margaret Dow (d. 1981), whom he married in 1955. His children are Elizabeth Sarah Gell-Mann (b. 1956) and Nicholas Webster Gell-Mann (b. 1963); and he has a stepson, Nicholas Southwick Levis (b. 1978).
Gell-Mann has interests in birdwatching, collecting antiques, ranching, historical linguistics, archaeology, natural history, the psychology of creative thinking, other subjects connected with biological, and cultural evolution and with learning. Along with S. A. Starostin, he established the "Evolution of Human Languages project" at the Santa Fe Institute.
He is currently the Robert Andrews Millikan Professor of Theoretical Physics Emeritus at California Institute of Technology as well as a University Professor in the Physics and Astronomy Department of the University of New Mexico in Albuquerque, New Mexico, and the Presidential Professor of Physics and Medicine at the University of Southern California. He is a member of the editorial board of the "Encyclopædia Britannica". In 1984 Gell-Mann co-founded the Santa Fe Institute—a non-profit theoretical research institute in Santa Fe, New Mexico—to study complex systems and disseminate the notion of a separate interdisciplinary study of complexity theory.
He was a postdoctoral fellow at the Institute for Advanced Study in 1951, and a visiting research professor at the University of Illinois at Urbana–Champaign from 1952 to 1953. He was a visiting associate professor at Columbia University and an associate professor at the University of Chicago in 1954–55 before moving to the California Institute of Technology, where he taught from 1955 until he retired in 1993.
As a humanist and an agnostic, Gell-Mann is a Humanist Laureate in the International Academy of Humanism.
Gell-Mann endorsed Barack Obama for the United States presidency in October 2008.
Awards and honors.
Gell-Mann was won numerous awards and honours including

</doc>
<doc id="20478" url="https://en.wikipedia.org/wiki?curid=20478" title="Magnetopause">
Magnetopause

The magnetopause is the abrupt boundary between a magnetosphere and the surrounding plasma. For planetary science, the magnetopause is the boundary between the planet’s magnetic field and the solar wind. The location of the magnetopause is determined by the balance between the pressure of the dynamic planetary magnetic field and the dynamic pressure of the solar wind. As the solar wind pressure increases and decreases, the magnetopause moves inward and outward in response. Waves (ripples and flapping motion) along the magnetopause move in the direction of the solar wind flow in response to small scale variations in the solar wind pressure and to Kelvin-Helmholtz instability.
The solar wind is supersonic and passes through a bow shock where the direction of flow is changed so that most of the solar wind plasma is deflected to either side of the magnetopause, much like water is deflected before the bow of a ship. The zone of shocked solar wind plasma is the magnetosheath. At Earth and all the other planets with intrinsic magnetic fields, some solar wind plasma succeeds in entering and becoming trapped within the magnetosphere. At Earth, the solar wind plasma which enters the magnetosphere forms the plasma sheet. The amount of solar wind plasma and energy that enters the magnetosphere is regulated by the orientation of the interplanetary magnetic field, which is embedded in the solar wind.
The Sun and other stars with magnetic fields and stellar winds have a solar magnetopause or heliopause where the stellar environment is bounded by the interstellar environment.
Characteristics.
Prior to the age of space exploration, interplanetary space was considered to be a vacuum. The coincidence of the Carrington super flare and the super geomagnetic event of 1859 was evidence that plasma was ejected from the Sun during a flare event. Chapman and Ferraro proposed that a plasma was emitted by the Sun in a burst as part of a flare event which disturbed the planet's magnetic field in a manner known as a geomagnetic storm. The collision frequency of particles in the plasma in the interplanetary medium is very low and the electrical conductivity is so high that it could be approximated to an infinite conductor. A magnetic field in a vacuum cannot penetrate a volume with infinite conductivity. Chapman and Bartels (1940) illustrated this concept by postulating a plate with infinite conductivity placed on the dayside of a planet’s dipole as shown in the schematic. The field lines on the dayside are bent. At low latitudes, the magnetic field lines are pushed inward. At high latitudes, the magnetic field lines are pushed backwards and over the polar regions. The boundary between the region dominated by the planet’s magnetic field (i.e., the magnetosphere) and the plasma in the interplanetary medium is the magnetopause. The configuration equivalent to a flat, infinitely conductive plate is achieved by placing an image dipole (green arrow at left of schematic) at twice the distance from the planet’s dipole to the magnetopause along the planet-Sun line. Since the solar wind is continuously flowing outward, the magnetopause above, below and to the sides of the planet are swept backward into the geomagnetic tail as shown in the artist’s concept. The region (shown in pink in the schematic) which separates field lines from the planet which are pushed inward from those which are pushed backward over the poles is an area of weak magnetic field or day-side cusp. Solar wind particles can enter the planet’s magnetosphere through the cusp region. Because the solar wind exists at all times and not just times of solar flares, the magnetopause is a permanent feature of the space near any planet with a magnetic field.
The magnetic field lines of the planet’s magnetic field are not stationary. They are continuously joining or merging with magnetic field lines of the interplanetary magnetic field. The joined field lines are swept back over the poles into the planetary magnetic tail. In the tail, the field lines from the planet’s magnetic field are re-joined and start moving toward night-side of the planet. The physics of this process was first explain by Dungey (1961) .
If one assumed that magnetopause was just a boundary between a magnetic field in a vacuum and a plasma with a weak magnetic field embedded in it, then the magnetopause would be defined by electrons and ions penetrating one gyroradius into the magnetic field domain. Since the gyro-motion of electrons and ions is in opposite directions, an electric current flows along the boundary. The actual magnetopause is much more complex.
Estimating the standoff distance to the magnetopause.
If the pressure from particles within the magnetosphere is neglected, it is possible to estimate the distance to the part of the magnetosphere that faces the Sun. The condition governing this position is that the dynamic ram pressure from the solar wind is equal to the magnetic pressure from the Earth's magnetic field.
"B"("r") is the Magnetic field strength of the planet in SI units ("B" in T, μ0 in H/m)
Since the dipole magnetic field strength varies with distance as formula_4 the magnetic field strength can be written as formula_5, where formula_6 is the planet's magnetic moment, expressed in formula_7.
Solving this equation for r leads to an estimate of the distance
The distance from Earth to the subsolar magnetopause varies over time due to solar activity, but typical distances range from 6 - 15 Rformula_10. Empirical models using real-time solar wind data can provide a real-time estimate of the magnetopause location. A bow shock stands upstream from the magnetopause. It serves to decelerate and deflect the solar wind flow before it reaches the magnetopause 
Solar System magnetopauses.
Research on the magnetopause is conducted using the LMN coordinate system (which is set of axes like XYZ). N points normal to the magnetopause outward to the magnetosheath, L lies along the projection of the dipole axis onto the magnetopause (positive northward), and M completes the triad by pointing dawnward.
Venus and Mars do not have a planetary magnetic field and do not have a magnetopause. The solar wind interacts with the planet’s atmosphere and a void is created behind the planet. In the case of the Earth’s moon and other bodies without a magnetic field or atmosphere, the body’s surface interacts with the solar wind and a void is created behind the body.

</doc>
<doc id="20479" url="https://en.wikipedia.org/wiki?curid=20479" title="Magnetosphere">
Magnetosphere

A magnetosphere is the region of space surrounding an astronomical object in which charged particles are controlled by that object's magnetic field. The magnetic field near the surface of many astronomical objects resembles that of a dipole. The field lines farther away from the surface can be significantly distorted by the flow of electrically conducting plasma emitted from a nearby star (e.g., the solar wind from the Sun).
History.
Study of Earth's magnetosphere began in 1600, when William Gilbert discovered that the magnetic field on the surface of Earth resembled that on a terrella, a small, magnetized sphere. In the 1940s, Walter M. Elsasser proposed the model of dynamo theory, which attributes Earth's magnetic field to the motion of Earth's iron outer core. Through the use of magnetometers, scientists were able to study the variations in Earth's magnetic field as functions of both time and latitude and longitude. Beginning in the late 1940s, rockets were used to study cosmic rays. In 1958, Explorer 1, the first of the Explorer series of space missions, was launched to study the intensity of cosmic rays above the atmosphere and measure the fluctuations in this activity. This mission observed the existence of the Van Allen radiation belt (located in the inner region of Earth's magnetosphere), with the Explorer 3 mission later that year definitively proving its existence. Also in 1958, Eugene Parker proposed the idea of the solar wind. The term 'magnetosphere' was proposed by Thomas Gold in 1959. The Explorer 12 mission (1961) led to the observation by Cahill and Amazeen in 1963 of a sudden decrease in the strength of the magnetic field near the noon meridian, later named the magnetopause. In 1983, the International Cometary Explorer observed the magnetotail, or the distant magnetic field.
Types.
The structure and behavior of magnetospheres are dependent on several variables: the type of astronomical object, the nature of sources of plasma and momentum, the period of the object's spin, the nature of the axis about which the object spins, the axis of the magnetic dipole, and the magnitude and direction of the flow of solar wind.
The distance at which a planet can withstand the solar wind pressure is called the Chapman–Ferraro distance. This is modeled by a formula wherein formula_1 represents the radius of the planet, formula_2 represents the magnetic field on the surface of the planet at the equator, and formula_3 represents the velocity of the solar wind:
formula_4
A magnetosphere is classified as "intrinsic" when formula_5, or when the primary opposition to the flow of solar wind is the magnetic field of the object. Mercury, Earth, Jupiter, Ganymede, Saturn, Uranus, and Neptune exhibit intrinsic magnetospheres. A magnetosphere is classified as "induced" when formula_6, or when the solar wind is not opposed by the object's magnetic field. In this case, the solar wind interacts with the atmosphere or ionosphere of the planet (or surface of the planet, if the planet has no atmosphere). Venus has an induced magnetic field, which means that because Venus appears to have no internal dynamo effect, the only magnetic field present is that formed by the solar wind's wrapping around the physical obstacle of Venus (see also Venus' induced magnetosphere). When formula_7, the planet itself and its magnetic field both contribute. It is possible that Mars is of this type.
Structure.
Bow shock.
The bow shock forms the outermost layer of the magnetosphere; the boundary between the magnetosphere and the ambient medium. For stars, this is usually the boundary between the stellar wind and interstellar medium; for planets, the speed of the solar wind there decreases as it approaches the magnetopause.
Magnetosheath.
The magnetosheath is the region of the magnetosphere between the bow shock and the magnetopause. It is formed mainly from shocked solar wind, though it contains a small amount of plasma from the magnetosphere. It is an area exhibiting high particle energy flux, where the direction and magnitude of the magnetic field varies erratically. This is caused by the collection of solar wind gas that has effectively undergone thermalization. It acts as a cushion that transmits the pressure from the flow of the solar wind and the barrier of the magnetic field from the object.
Magnetopause.
The magnetopause is the area of the magnetosphere wherein the pressure from the planetary magnetic field is balanced with the pressure from the solar wind. It is the convergence of the shocked solar wind from the magnetosheath with the magnetic field of the object and plasma from the magnetosphere. Because both sides of this convergence contain magnetized plasma, the interactions between them are complex. The structure of the magnetopause depends upon the Mach number and beta of the plasma, as well as the magnetic field. The magnetopause changes size and shape as the pressure from the solar wind fluctuates.
Magnetotail.
Opposite the compressed magnetic field is the magnetotail, where the magnetosphere extends far beyond the astronomical object. It contains two lobes, referred to as the northern and southern tail lobes. The northern tail lobe points towards the object and the southern tail lobe points away. The tail lobes are almost empty, with few charged particles opposing the flow of the solar wind. The two lobes are separated by a plasma sheet, an area where the magnetic field is weaker and the density of charged particles is higher.
Earth's magnetosphere.
Over Earth's equator, the magnetic field lines become almost horizontal, then return to reconnect at high latitudes. However, at high altitudes, the magnetic field is significantly distorted by the solar wind and its solar magnetic field. On the dayside of Earth, the magnetic field is significantly compressed by the solar wind to a distance of approximately . Earth's bow shock is about thick and located about from Earth. The magnetopause exists at a distance of several hundred kilometers above Earth's surface. Earth's magnetopause has been compared to a sieve because it allows solar wind particles to enter. Kelvin–Helmholtz instabilities occur when large swirls of plasma travel along the edge of the magnetosphere at a different velocity from the magnetosphere, causing the plasma to slip past. This results in magnetic reconnection, and as the magnetic field lines break and reconnect, solar wind particles are able to enter the magnetosphere. On Earth's nightside, the magnetic field extends in the magnetotail, which lengthwise exceeds . Earth's magnetotail is the primary source of the polar aurora. Also, NASA scientists have suggested that Earth's magnetotail might cause "dust storms" on the Moon by creating a potential difference between the day side and the night side.
Other objects.
The magnetosphere of Jupiter is the largest planetary magnetosphere in the Solar System, extending up to on the dayside and almost to the orbit of Saturn on the nightside. Jupiter's magnetosphere is stronger than Earth's by an order of magnitude, and its magnetic moment is approximately 18,000 times larger.

</doc>
<doc id="20481" url="https://en.wikipedia.org/wiki?curid=20481" title="Manama">
Manama

Manama ( "") is the capital and largest city of Bahrain, with an approximate population of 157,000 people. Long an important trading center in the Persian Gulf, Manama is home to a very diverse population. After periods of Portuguese and Persian control and invasions from the ruling dynasties of Saudi Arabia and Oman, Bahrain established itself as an independent nation during the 19th century period of British hegemony.
Although the current twin cities of Manama and Muharraq appear to have been founded simultaneously in the 1800s, Muharraq took prominence due to its defensive location and was thus the capital of Bahrain until 1921. Manama became the mercantile capital, and was the gateway to the main Bahrain Island. In the 20th century, Bahrain's oil wealth helped spur fast growth and in the 1990s a concerted diversification effort led to expansion in other industries and helped transform Manama into an important financial hub in the Middle East. Manama was designated as the capital of Arab culture for the year 2012 by the Arab League.
Etymology.
The name is derived from the Arabic word المنامة (transliterated:"al-manãma") meaning "the place of rest" or "the place of dreams".
History.
Pre-modern history.
There is evidence of human settlement on the northern coastline of Bahrain dating back to the Bronze Age. The Dilmun civilisation inhabited the area in 3000 BC, serving as a key regional trading hub between Mesopotamia, Magan and the Indus Valley civilisation. Approximately 100,000 Dilmun burial mounds were found across the north and central regions of the country, some originating 5,000 years ago. Despite the discovery of the mounds, there is no significant evidence to suggest heavy urbanisation took place during the Dilmun era. It is believed that the majority of the population lived in rural areas, numbering several thousands. Evidence of an ancient large rural population was confirmed by one of Alexander the Great's ship captains, during voyages in the Persian Gulf. A vast system of aqueducts in northern Bahrain helped facilitate ancient horticulture and agriculture. 
The commercial network of Dilmun lasted for almost 2,000 years, after which the Assyrians took control of the island in 700 BC for more than a century. This was followed by Babylonian and Achaemenid rule, which later gave way to Greek influence during the time of Alexander the Great's conquests. In the first century AD, the Roman writer Pliny the Elder wrote of Tylos, the Hellenic name of Bahrain in the classical era, and its pearls and cotton fields. The island came under the control of the Parthian and Sassanid empires respectively, by which time Nestorian Christianity started to spread in Bahrain. By 410-420 AD, a Nestorian bishopric and monastery was established in Al Dair, on the neighbouring island of Muharraq. Following the conversion of Bahrain to Islam in 628 AD, work on one of the earliest mosques in the region, the Khamis Mosque, began as early as the seventh century AD. During this time, Bahrain was engaged in long distance marine trading, evident from the discovery of Chinese coins dating between 600-1200 AD, in Manama.
In 1330, under the Jarwanid dynasty, the island became a tributary of the Kingdom of Hormuz. The town of Manama was mentioned by name for the first time in a manuscript dating to 1345 AD. Bahrain, particularly Manama and the nearby settlement of Bilad Al Qadeem, became a centre of Shia scholarship and training for the ulema, it would remain so for centuries. The ulema would help fund pearling expeditions and finance grain production in the rural areas surrounding the city. In 1521, Bahrain fell to the expanding Portuguese Empire in the Persian Gulf, having already defeated Hormuz. The Portuguese consolidated their hold on the island by constructing the Bahrain Fort, on the outskirts of Manama. After numerous revolts and an expanding Safavid empire in Persia, the Portuguese were expelled from Bahrain and the Safavids took control in 1602.
Early modern history.
The Safavids, sidelining Manama, designated the nearby town of Bilad Al Qadeem as the provincial capital. The town was also the seat of the Persian governor and the Shaikh al-Islam of the islands. The position of Shaikh al-Islam lied under jurisdiction of the central Safavid government and as such, candidates were carefully vetted by the Isfahan courts. During the Safavid era, the islands continued to be a centre for Twelver Shi'ism scholarship, producing clerics for use in mainland Persia. Additionally, the rich agricultural northern region of Bahrain continued to flourish due to an abundance of date palm farms and orchards. The Portuguese traveler Pedro Teixeira commented on the extensive cultivation of crops like barley and wheat. The opening of Persian markets to Bahraini exports, especially pearls, boosted the islands' export economy. The yearly income of exported Bahraini pearls was 600,000 ducats, collected by around 2,000 pearling dhows. Another factor that contributed to Bahrain's agricultural wealth was the migration of Shia cultivators from Ottoman-occupied Qatif and Al-Hasa, fearing religious persecution, in 1537. Some time after 1736, Nader Shah constructed a fort on the southern outskirts of Manama (likely the Diwan Fort).
Persian control over the Persian Gulf waned during the later half of the 18th century. At this time, Bahrain archipelago was a dependency of the emirate of Bushehr, itself a part of Persia. In 1783, the Bani Utbah tribal confederation invaded Bahrain and expelled the resident governor Nasr Al-Madhkur. As a result, the Al Khalifa family became the rulers of the country, and all political relations with Bushehr and Persia/Iran were terminated. Ahmed ibn Muhammad ibn Khalifa (later called Ahmed al-Fateh, lit. "Ahmed the conqueror") become the dynasty's first Hakim of Bahrain. Political instability in the 19th century had disastrous effects on Manama's economy; Invasions by the Omanis in 1800 and by the Wahhabis in 1810–11, in addition to a civil war in 1842 between Bahrain's co-rulers saw the town being a major battleground. The instability paralysed commercial trade in Manama; the town's port was closed, most merchants fled abroad to Kuwait and the Persian coast until hostilities ceased. The English scholar William Gifford Palgrave, on a visit to Manama in 1862, described the town as having a few ruined stone buildings, with a landscape dominated with the huts of poor fishermen and pearl-divers.
The Pax Britannica of the 19th century resulted in British consolidation of trade routes, particularly those close to the British Raj. In response to piracy in the Persian Gulf region, the British deployed warships and forced much of the Persian Gulf States at the time (including Bahrain) to sign the General Maritime Treaty of 1820, which prohibited piracy and slavery. In 1861, the Perpetual Truce of Peace and Friendship was signed between Britain and Bahrain, which placed the British in charge of defending Bahrain in exchange for British control over Bahraini foreign affairs. With the ascension of Isa ibn Ali Al Khalifa as the Hakim of Bahrain in 1869, Manama became the centre of British activity in the Persian Gulf, though its interests were initially strictly commercial. Trading recovered fully by 1873 and the country's earnings from pearl exports increased by sevenfold between 1873 and 1900. Representing the British were native agents, usually from minorities such as Persians or Huwala who regularly reported back to British India and the British political residency in Bushehr. The position of native agent was later replaced by a British political agent, following the construction of the British political residency (locally referred to in ) in 1900, which further solidified Britain's position in Manama.
Modern history.
Following the outbreak of World War I in 1914, the British Raj used Manama as a military base of operations during the Mesopotamian campaign. Prompted by the presence of oil in the region, the British political agency in Bushire concluded an oil agreement with the Hakim to prohibit the exploration and exploitation of oil for a five-year period. In 1919, Bahrain was officially integrated into the British empire as an overseas imperial territory following the Bahrain order-in-council decree, issued in 1913. The decree gave the resident political agent greater powers and placed Bahrain under the residency of Bushire and therefore under the governance of the British Raj. The British pressured a series of administrative reforms in Bahrain during the 1920s (a move met with opposition from tribal leaders), during which the aging Hakim Isa ibn Ali Al Khalifa was forced to abdicate in favour of his reform-minded son Hamad ibn Isa Al Khalifa. A municipal government was established in Manama in 1919, the customs office was reorganised in 1923 and placed under the supervision of an English businessman, the pearling industry was later reformed in 1924. Earnings from the customs office would be kept in the newly created state treasury. Civil courts were established for the first time in 1923, followed by the establishment of the Department of Land Registration in 1924. Charles Belgrave, from the Colonial office, was appointed in 1926 by the British to carry on further reforms and manage administration as a financial advisor to the King. He later organised the State Police and was in charge of the Finance and Land departments of the government. 
In 1927, the country's pearling economy collapsed due to the introduction of Japanese cultured pearls in the world market. It is estimated that between 1929 and 1931, pearling entrepreneurs lost more than two-thirds of their income. Further aggravated by the Great Depression, many leading Bahraini businessmen, shopkeepers and pearl-divers fell into debt. With the discovery of oil in 1932 and the subsequent production of oil exports in 1934, the country gained a greater significance in geopolitics. The security of oil supplies in the Middle East was a priority of the British, especially in the run-up to the Second World War. The discovery of oil led to a gradual employment of bankrupt divers from the pearling industry in the 1930s, eventually causing the pearling industry to disappear. During the war, the country served as a strategic airbase between Britain and India as well as hosting RAF Muharraq and a naval base in Juffair. Bahrain was bombed by the Italian Air Force in 1940. In 1947, following the end of the war and subsequent Indian independence, the British residency of the Persian Gulf moved to Manama from Bushire.
Following the rise of Arab nationalism across the Middle East and sparked by the Suez Crisis in 1956, anti-British unrest broke out in Manama, organised by the National Union Committee. Though the NUC advocated peaceful demonstrations, buildings and enterprises belonging to Europeans (the British in particular) as well as the main Catholic church in the city and petrol stations, were targeted and set ablaze. Demonstrations held in front of the British political residency called for the dismissal of Charles Belgrave, who was later dismissed by direct intervention of the Foreign Office the following year. A subsequent crackdown on the NUC led to the dissolution of the body. Another anti-British uprising erupted in March 1965, though predominately led by students aspiring for independence rather than by Arab nationalists. In 1968, the British announced their withdrawal from Bahrain by 1971. The newly independent State of Bahrain designated Manama as the capital city.
Post-independence Manama was characterised by the rapid urbanisation of the city and the swallowing-up of neighboring villages and hamlets into a single urbanised area, incorporating new neighbourhoods such as Adliya and Salmaniya. The construction boom attracted large numbers of foreigners from the Indian subcontinent and by 1981, foreigners outnumbered Bahrainis two-to-one. The construction of the Diplomatic Area district in the city's northeast helped facilitate diversification of the country's economy from oil by exploiting the lucrative financial industry. Financial institutions in the district numbered 187 by 1986. Scarcity of land suitable for construction led to land reclamation. Religious activism migrated from Manama to the suburban districts of Bani Jamra, Diraz and Bilad Al Qadeem, hotspots of unrest in the 1990s uprising that called for the reinstatement of an elected parliament. In 2001, the National Action Charter, presented by King Hamad bin Isa al-Khalifa was approved by Bahrainis. The charter led to the first parliamentary and municipal elections in decades. Further elections in 2006 and 2010 led to the election of Islamist parties, Al Wefaq, Al Menbar, and Al Asalah, as well as independent candidates. In 2011, a month-long uprising led to the intervention of GCC forces and the proclamation of a three-month state of emergency. The Bahrain Independent Commission of Inquiry published a 500-page report on the events of 2011.
Government.
Historically, Manama has been restricted to what is now known as the Manama Souq and the Manama Fort (now the Ministry of Interior) to its south. However the city has now grown to include a number of newer suburban developments as well as older neighboring villages that have been engulfed by the growth of the city. The neighborhoods of Manama today include:
Manama is part of the Capital Governorate, one of five Governorates of Bahrain. Until 2002 it was part of the municipality of Al-Manamah. Councils exist within the governorates; eight constituencies are voted upon within Capital Governorate in 2006.
Economy.
Manama is the focal point of the Bahraini economy. While petroleum has decreased in importance in recent years due to depleting reserves and growth in other industries, it is still the mainstay of the economy. Heavy industry (e.g. aluminium smelting, ship repair), banking and finance, and tourism are among the industries which have experienced recent growth. Several multinationals have facilities and offices in and around Manama. The primary industry in Manama itself is financial services, with over two hundred financial institutions and banks based in the CBD and the Diplomatic Area. Manama is a financial hub for the Persian Gulf region and a center of Islamic banking. There is also a large retail sector in the shopping malls around Seef, while the center of Manama is dominated by small workshops and traders.
Manama's economy in the early 20th century relied heavily on pearling; in 1907, the pearling industry was estimated to include 917 boats providing employment for up to 18,000 people. Shipbuilding also employed several hundreds in both Manama and Muharraq. The estimated income earned from pearling in 1926 and subsequent years prior to the Great Depression was £1.5 million annually. Custom duties and tariffs served as the prime source of revenue for the government. With the onset of the Great Depression, the collapse of the pearling industry and the discovery of oil in 1932, the country's economy began to shift towards oil.
Historically, the ports at Manama were of poor reputation. The British described the ports importing systems as being "very bad - goods were exposed to the weather and there were long delays in delivery", in 1911. Indians began maintaining the ports and new resources were built on site, improving the situation. As of 1920, Manama was one of the main exporters of Bahrain pearls, attracting steamships from India. During this time, they also imported goods from India and from other regional countries. They imported rice, textiles, ghee, coffee, dates, tea, tobacco, fuel, and livestock. They exported less of a variety, with focus on pearls, oysters, and sailcloth. For the year of 1911–12, Manama was visited by 52 steamships, the majority being British and the rest Turkish-Arabian.
Demographics.
The role of Manama as a regional port city in the Persian Gulf made it a hub for migrant workers in search of a better living. As a result, Manama has often been described, both in the pre-oil and post-oil era, as a cosmopolitan city. In 1904, it was estimated that Manama's population numbered 25,000, out of which half were believed to have been foreigners from Basra, Najd, Al Hasa and Iran, as well as from India and Europe.
The two main branches of Islam, Shia Islam and Sunni Islam, coexisted in Manama for centuries and are represented by distinct ethnic groups. The Shia community is represented by the native Arab Baharna, the Hasawis and Qatifis of mainland Arabia and the Persian Ajam. The Sunni community is represented by Arab Bedouin tribes who migrated in the eighteenth century along with the Bani Utbah and the Huwala, Arabic-speaking Persians. There is also a sizable native Bahraini Christian population in the country, numbering more than a thousand, in addition to immigrant Hindus and a small native Jewish community numbering 37.
Transport.
Road network.
Manama is the main hub of the country's road network. At the moment the city's road network is undergoing substantial development to ameliorate the situation of traffic in the city. Due to the fact that it is the capital and the main city in the country, where most of the government and the commercial offices and facilities are established, along with the entertainment centers, and the country's fast growth, vehicle population is increasing rapidly.
The widening of roads in the old districts of Manama and the development of a national network linking the capital to other settlements commenced as early as the arrival of the first car in 1914. The continuous increase in the number of cars from 395 in 1944, to 3,379 in 1954 and to 18,372 cars in 1970 caused urban development to primarily focus on expanding the road network, widening carriageways and the establishment of more parking spaces. Many tracks previously laid in the pre-oil era (prior to the 1930s) were resurfaced and widened, turning them into 'road arteries'. Initial widening of the roads started in the Manama Souq district, widening its main roads by demolishing encroaching houses.
A series of ring roads were constructed (Isa al Kabeer avenue in the 1930s, Exhibition avenue in the 1960s and Al Fateh highway in the 1980s), to push back the coastline and extend the city area in belt-like forms. To the north, the foreshore used to be around "Government Avenue" in the 1920s but it shifted to a new road, "King Faisal Road", in the early 1930s which became the coastal road. To the east, a bridge connected Manama to Muharraq since 1929, a new causeway was built in 1941 which replaced the old wooden bridge. Transits between the two islands peaked after the construction of the Bahrain International Airport in 1932.
To the south of Manama, roads connected groves, lagoons and marshes of Hoora, Adliya, Gudaibiya and Juffair. Villages such as Mahooz, Ghuraifa, Seqaya served as the end of these roads. To the west, a major highway was built that linked Manama to the isolated village port of Budaiya, this highway crossed through the 'green belt' villages of Sanabis, Jidhafs and Duraz. To the south, a road was built that connected Manama to Riffa. The discovery of oil accelerated the growth of the city's road network.
The four main islands and all the towns and villages are linked by well-constructed roads. There were of roadways in 2002, of which were paved. A causeway stretching over , connect Manama with Muharraq Island, and another bridge joins Sitra to the main island. A four-lane highway atop a causeway, linking Bahrain with the Saudi Arabian mainland via the island of Umm an-Nasan was completed in December, 1986, and financed by Saudi Arabia. In 2000, there were 172,684 passenger vehicles and 41,820 commercial vehicles.
Bahrain's port of Mina Salman can accommodate 16 oceangoing vessels drawing up to . In 2001, Bahrain had a merchant fleet of eight ships of 1,000 GRT or over, totaling 270,784 GRT. Private vehicles and taxis are the primary means of transportation in the city.
Buses.
Manama has a recently reformed comprehensive bus service that launched on 1 April 2015, with a fleet of 141 MAN buses. Regulated by the Ministry of Transportation, bus routes extend across Bahrain and around Manama with fares of a minimum 200 Fils (BD0.200) (around $0.50(USD); £0.30).
Air transport.
Bahrain International Airport is located on the nearby Muharraq Island, approximately from the CBD. It is a premier hub airport in the Middle East. Strategically located in the Northern Persian Gulf between the major markets of Saudi Arabia and Iran, the airport has one of the widest range and highest frequency of regional services with connections to major international destinations in Europe, Asia, Africa, and North America.
Education.
Quranic schools were the only source of education in Bahrain prior to the 20th century; such schools were primarily dedicated to the study of the Qur'an. The first modern school to open in the country was a missionary elementary school setup in 1892 (according to one account) in Manama by the American Dutch Reformed Church, with the school's syllabus comprising English, Mathematics and the study of Christianity. Leading merchants in the country sent their children to the school until it was closed down in 1933 due to financial difficulties. The school reopened some years later under the name of Al Raja School where it operates till the present day. In addition to the American Mission School, another foreign private school was opened in 1910; Al-Ittihad school, funded by the Persian community of Bahrain.
Following the end of the First World War, Western ideas became more widespread in the country, culminating in the opening of the first public school of Bahrain, Al-Hidaya Al-Khalifia Boys school, in the island of Muharraq in 1919. The school was founded by prominent citizens of Muharraq and was endorsed by the Bahraini royal family. The country's first Education Committee was established by several leading Bahraini merchants, headed by Shaikh Abdulla bin Isa Al-Khalifa, the son of the then-ruler of Bahrain Isa ibn Ali Al Khalifa, who acted as the de facto Minister of Education. The Education Committee was also responsible for managing the Al-Hidaya Boys school. The school was in fact the brainchild of Shaikh Abdulla, who suggested the idea after returning from post-WWI celebrations in England.
In 1926, a second public school for boys opened up in the capital city, Manama. Two years later, in 1928, the first public school for girls was established. Due to financial constraints suffered by the Education Committee, the Bahraini government took control of the schools in 1930.
Presently, Manama has a wide range of private and public universities and colleges such as Ahlia University, Applied Science University, Arab Open University, Arabian Gulf University, Bahrain Institute of Banking and Finance, Delmon University. Other notable primary and secondary schools situated in the city include the Bahrain School, The Indian School, Bahrain amongst others.
Geography.
The city is located in the north-eastern corner of Bahrain on a small peninsula. As in the rest of Bahrain, the land is generally flat (or gently rolling) and arid.
Climate.
Manama has an arid climate. In common with the rest of Bahrain, Manama experiences extreme climatic conditions, with summer temperatures up to , and winter as low as with even hail at rare occasions. Average temperatures of the summer and winter seasons are generally from to about . The most pleasant time in Bahrain is autumn when sunshine is comparatively low, coupled with warm temperatures tempered by soft breezes.
Culture.
The country attracts a large number of foreigners and foreign influences, with just under one third of the population hailing from abroad. Alcohol is legal in the country, with bars and nightclubs operating in the city. Bahrain gave women the right to vote in elections for the first time in 2002. Football is the most popular sport in Manama (and the rest of the country), with 3 teams from Manama participating in the Bahraini Premier League.
The central areas of Manama are the main location for Muharram processions in the country, attracting hundreds of thousands of people annually.

</doc>
<doc id="20484" url="https://en.wikipedia.org/wiki?curid=20484" title="Mance Lipscomb">
Mance Lipscomb

Mance Lipscomb (April 9, 1895 – January 30, 1976) was an American blues singer, guitarist and songster. Born Beau De Glen Lipscomb near Navasota, Texas, United States, he as a youth took the name of 'Mance' from a friend of his oldest brother Charlie ("Mance" being short for emancipation).
Biography.
Lipscomb was born April 9, 1895, to an ex-slave father from Alabama and a half Native American (Choctaw) mother. Lipscomb spent most of his life working as a tenant farmer in Texas and was discovered and recorded by Mack McCormick and Chris Strachwitz in 1960 during the country blues revival. He released many albums of blues, ragtime, Tin Pan Alley and folk music (most of them on Strachwitz' Arhoolie label), singing and accompanying himself on acoustic guitar. He had a "dead-thumb" finger-picking guitar technique, and an expressive voice. Lipscomb often honed his skills by playing in nearby Brenham, Texas, with a blind musician, Sam Rogers. His debut release was "Texas Songster" (1960). Lipscomb performed old songs like "Sugar Babe," the first song he ever learned, to pop numbers like "Shine On, Harvest Moon" and "It's a Long Way to Tipperary".
"Trouble in Mind" was recorded in 1961 and released by Reprise Records. In May 1963, Lipscomb appeared at the first Monterey Folk Festival in California.
Unlike many of his contemporaries, he did not record in the early blues era, but his life is well documented thanks to his autobiography, "I Say Me for a Parable: The Oral Autobiography of Mance Lipscomb, Texas Bluesman", narrated to Glen Alyn, which was published posthumously, and also a short 1971 documentary by Les Blank, "A Well Spent Life".
He began playing guitar early on and played regularly for years at local gatherings, mostly what he called "Saturday Night Suppers" hosted by someone in the area. These gatherings were hosted regularly for a while by himself and his wife. The majority of his musical activity took place within what he called his "precinct", meaning the local area around Navasota, until around 1960.
Following his discovery by McCormick and Strachwitz, Lipscomb became an important figure in the folk music revival of the 1960s. He was a regular performer at folk festivals and folk-blues clubs around the United States, notably the Ash Grove in Los Angeles, California.
He died in Navasota in 1976, two years after suffering a stroke.
Honors.
An annual Navasota Blues Festival is held in his honor, and on August 12, 2011, a bronze sculpture of him was unveiled in Mance Lipscomb Park in Navasota. The statue was sculpted by artist Sid Henderson of California and weighs almost 300 pounds. It portrays Lipscomb playing his guitar whilst seated on a bench, with room for fans to sit beside him and play their own guitars "with" him.

</doc>
<doc id="20485" url="https://en.wikipedia.org/wiki?curid=20485" title="Melbourne Cup">
Melbourne Cup

The Melbourne Cup is one of Australia's most prestigious Thoroughbred horse races. It is a 3,200 metre race for three-year-olds and over. It is the richest "two-mile" handicap in the world, and one of the richest turf races. Conducted annually by the Victoria Racing Club on the Flemington Racecourse in Melbourne, Victoria, the event starts at 3pm on the first Tuesday in November.
The first race was held in 1861 over but was shortened to in 1972 when Australia adopted the metric system. This reduced the distance by , and Rain Lover's 1968 race record of 3:19.1 was accordingly adjusted to 3:17.9. The present record holder is the 1990 winner Kingston Rule with a time of 3:16.3.
Qualifying and race conditions.
The race is a quality handicap for horses 3 years old and over, run over a distance of 3,200 metres, on the first Tuesday in November at Flemington Racecourse. The minimum handicap weight is 50 kg. There is no maximum weight, but the top allocated weight must not be less than 57 kg. The weight allocated to each horse is declared by the VRC Handicapper in early September.
The Melbourne Cup race is a handicap contest in which the weight of the jockey and riding gear is adjusted with ballast to a nominated figure. Older horses carry more weight than younger ones, and weights are adjusted further according to the horse's previous results.
Weights were theoretically calculated to give each horse an equal winning chance in the past, but in recent years the rules were adjusted to a "quality handicap" formula where superior horses are given less severe weight penalties than under pure handicap rules.
Weight penalties.
After the declaration of weights for the Melbourne Cup, the winner of any handicap flat race of the advertised value of A$55,000 or over to the winner, or an internationally recognised Listed, Group, or Graded handicap flat race, shall carry such additional weight (if any), for each win, as the VRC Handicapper shall determine.
Fees.
Entries for the Melbourne Cup usually close during the first week of August. The initial entry fee is $600 per horse. Around 300 to 400 horses are nominated each year, but the final field is limited to 24 starters. Following the allocation of weights, the owner of each horse must on four occasions before the race in November, declare the horse as an acceptor and pay a fee. First acceptance is $960, second acceptance is $1,450 and third acceptance is $2,420. The final acceptance fee, on the Saturday prior to the race, is $45,375. Should a horse be balloted out of the final field, the final declaration fee is refunded.
Balloting conditions.
The race directors retain the absolute discretion to exclude any horse from the race, or exempt any horse from the ballot on the race, but in order to reduce the field to the safety limit of 24, horses are balloted out based on a number of factors which include:
The winner of the following races are exempt from any ballot:
The limitation of 24 starters is stated explicitly to be for safety reasons. However, in the past far larger numbers were allowed - the largest field ever raced was 39 runners in 1890.
Quarantine.
International horses (New Zealand not included) that are entered for the Melbourne Cup must undergo quarantine in an approved premises in their own country for a minimum period of 14 days before travelling to Australia. The premises must meet the Australian Government Standards. The Werribee International Horse Centre at Werribee racecourse is the Victorian quarantine station for international horses competing in the Melbourne Spring Racing Carnival. The facility has stabling for up to 24 horses in five separate stable complexes and is located 32 km from the Melbourne CBD.
Prize money and trophies.
Prize money.
The total prize money for the 2015 race is A$6,200,000, plus trophies valued at $175,000. The first 10 past the post receive prizemoney, with the winner being paid $3.6 million, and tenth place $125,000. Prizemoney is distributed to the connections of each horse in the ratio of 85 percent to the owner, 10 percent to the trainer and 5 percent to the jockey.
The 1985 Melbourne Cup, won by "What a Nuisance", was the first race run in Australia with prize money of $1 million.
The Cup currently has a $500,000 bonus for the owner of the winner if it has also won the group one Irish St. Leger run the previous September.
Trophies.
The winner of the first Melbourne Cup in 1861 received a gold watch. The first Melbourne Cup trophy was awarded in 1865 and was an elaborate silver bowl on a stand that had been manufactured in England. The first existing and un-altered Melbourne Cup is from 1866, presented to the owners of The Barb; as of 2013, it is in the National Museum of Australia. The silver trophy presented in 1867, now also in the National Museum of Australia, was also made in England but jewellers in Victoria complained to the Victorian Racing Club that the trophy should have been made locally. They believed the work of Melbournian, William Edwards, to be superior in both design and workmanship to the English made trophy. No trophy was awarded to the Melbourne Cup winner for the next eight years.
In 1876 Edward Fischer, an immigrant from Austria, produced the first Australian-made trophy. It was an Etruscan shape with two handles. One side depicted a horse race with the grandstand and hill of Flemington in the background. The opposite side had the words "Melbourne Cup, 1876" and the name of the winning horse. A silver plated base sporting three silver horses was added in 1888, but in 1891 the prize changed to being a , trophy showing a Victory figure offering an olive wreath to a jockey. From 1899 the trophy was in the form of silver galloping horse embossed on a plaque, although it was said to look like a greyhound by some people.
The last Melbourne Cup trophy manufactured in England was made for the 1914 event. It was a chalice centred on a long base which had a horse at each end. The trophy awarded in 1916, the first gold trophy, was a three-legged, three-armed rose bowl. The three-handled loving cup design was first awarded in 1919. In that year the Victorian Racing Club had commissioned James Steeth to design a trophy that would be in keeping with the prestige of the race, little realising that it would become the iconic Melbourne Cup still presented today. In the Second World War years (1942, 1943 and 1944) the winning owner received war bonds valued at 200 pounds.
A new trophy is struck each year and becomes the property of the winning owner. In the event of a dead heat a second cup is on hand. The present trophy is made by Hardy Brothers from 34 pieces of gold metal hand beaten for over 200 hours. Close inspection of the inside of the Cup will reveal small hammer imprints. As of 2008, the trophy values were increased and the Cup now contains 1.65 kg of 18-carat gold valuing the trophy at $125,000. The winning trainer and jockey also receive a miniature replica of the cup (since 1973) and the strapper is awarded the Tommy Woodcock Trophy, named after the strapper of Phar Lap.
Melbourne Cup Tour.
In 2003 an annual tour of the Melbourne Cup trophy was initiated to provide communities across Australia and New Zealand with an opportunity to view the Cup trophy and highlight the contribution the Melbourne Cup has made to the Australia's social, sporting and racing culture. Each year, communities in Australia and New Zealand apply for the cup to tour to their community and the tour also takes in cities around the world as part of the Victoria Racing Club’s strategy to promote the Melbourne Cup and the Melbourne Cup Carnival internationally.
The Tour has visited schools and aged-care and hospital facilities, and participated in community events and celebrations including race days across Australia and New Zealand.
History.
Early years.
Frederick Standish, member of the Victorian Turf Club and steward on the day of the first Cup, was credited with forming the idea to hold a horse race and calling it the "Melbourne Cup".
Seventeen horses contested the first Melbourne Cup on Thursday 7 November 1861, racing for the modest prize of 710 gold sovereigns (£710) cash and a hand-beaten gold watch, winner takes all. The prize was not, as some have suggested, the largest purse up to that time.
In order to attract a bigger crowd to the fledgling Cup, the first secretary of the Victorian Racing Club, Robert Bagot (c. 1828–1881) decided to issue members with two ladies tickets, calculating that "where ladies went, men would follow". A large crowd of 4,000 men and women watched the race, although it has been suggested this was less than expected because of news reaching Melbourne of the death of explorers Burke and Wills five days earlier on 2 November. Nevertheless, the attendance was the largest at Flemington on any day for the past two years, with the exception of the recently run Two Thousand Guinea Stakes.
The winner of this first Melbourne Cup race was a 16.3 hand bay stallion by the name of Archer in a time of 3.52.00, ridden by John Cutts, trained by Etienne de Mestre, and leased (and consequently raced in his own name) by de Mestre. As a lessee de Mestre "owned" and was fully responsible for Archer during the lease. Archer was leased from the "Exeter Farm" of Jembaicumbene near Braidwood, New South Wales. His owners were Thomas John "Tom" Roberts (a good school-friend of de Mestre's), Rowland H. Hassall (Roberts' brother-in-law), and Edmund Molyneux Royds and William Edward Royds (Roberts' nephews).
The inaugural Melbourne Cup of 1861 was an eventful affair when one horse bolted before the start, and three of the seventeen starters fell during the race, two of which died. Archer, a Sydney "outsider" who drew scant favour in the betting, spread-eagled the field and defeated the favourite, and Victorian champion, Mormon by six lengths. Dismissed by the bookies, Archer took a lot of money away from Melbourne, 'refuelling interstate rivalry' and adding to the excitement of the Cup. The next day, Archer was raced in and won another 2 mile long distance race, the Melbourne Town Plate.
It has become legend that Archer walked over 800 km (over 500 miles) to Flemington from de Mestre's stable at "Terara" near Nowra, New South Wales. However, newspaper archives of the day reveal that he had travelled south from Sydney to Melbourne on the steamboat "City Of Melbourne", together with de Mestre, and two of de Mestre's other horses Exeter and Inheritor. Before being winched aboard the steamboat for the trip to Melbourne, the horses had arrived in Sydney in September 1861.
Archer travelled to Melbourne by steamboat again the following year (1862) to run in the second Melbourne Cup. This time he won 810 gold sovereigns (£810) cash and a gold watch before a crowd of 7,000, nearly twice the size of the previous years large crowd in a time of 3.47.00, taking to two the number of Melbourne Cup wins by this horse. Archer had already won the 1862 AJC Queen Elizabeth Stakes in Randwick, Sydney, and returned to win his second Melbourne Cup carrying 10 stone 2 pounds. He defeated a field of twenty starters by eight lengths, a record that has never been beaten, and that was not matched for over 100 years. Mormon again running second. Winning the Melbourne Cup twice was a feat not repeated until more than seventy years later when Peter Pan won the race in 1932 and 1934, and winning the Melbourne Cup two years in a row was a feat not repeated until more than 30 years later when Rain Lover won in 1968 and 1969.
Archer travelled to Melbourne by steamboat yet again the next year (1863). Despite his weight of 11 stone 4 pounds, Archer would have contested the third cup in 1863, but due to a Victorian public holiday trainer Etienne de Mestre's telegraphed acceptance form arrived late, and Archer was scratched on a technicality. In protest of this decision and in a show of solidarity, many of de Mestre's owners boycotted the third race and scratched their horses in sympathy. As a result, the Melbourne Cup of that year ran with only 7 starters, the smallest number in the history of the Cup.
In 1865, Adam Lindsay Gordon wrote a verse in which the Melbourne Cup winner was called Tim Whiffler. Two years later in 1867 two horses with the name Tim Whiffler ran in the Melbourne Cup. (The year before in 1866 two horses with the same name, Falcon, also ran in the Melbourne Cup.) To distinguish between the two Tim Whifflers they were called "Sydney" Tim Whiffler and "Melbourne" Tim Whiffler. "Sydney" Tim Whiffler actually won the Cup. He was trained by Etienne de Mestre, and like Archer before him raced in de Mestre's name but was leased from the "Exeter Farm".
As early as 1865, Cup day was a half-holiday in Melbourne for public servants and bank officials. Various businesses also closed at lunchtime.
It took some years before the purpose of the declared holiday was acknowledged in the Victoria Government Gazette. The Gazette of 31 October 1873 announced that the following Thursday (Cup Day) be observed as a bank and civil (public) service holiday.
The Melbourne Cup was first run on a Tuesday in 1875, the first Tuesday in that month.
On 7 November 1876, the running of the Melbourne Cup on the first Tuesday in November saw the three-year-old filly, Briseis, owned and trained by James Wilson Snr., win in a time of 3.36.25. Briseis then went on to creat a record that is never likely to be equalled, winning the VRC Derby, the Melbourne Cup and the VRC Oaks in the space of six days. She was ridden in the Melbourne Cup by the tiny featherweight figure of jockey Peter St. Albans. In 1876 at the recorded age thirteen (he was actually twelve, being 8 days short of his thirteenth birthday), Peter St. Albans is also the youngest person ever to win a Melbourne Cup. Before 75,000 at Flemington Briseis, with St Albans in the saddle, comfortably won by 1 length in the biggest field of all time. "At 4 o'clock the starter released the 33 runners and they swept down the long Flemington straight in a thundering rush. Briseis, ridden by what one writer termed a mere child, (in the Cup) captured a rare double, the Victoria Race Club Derby and the Melbourne Cup. Shouts and hurrahs were heard, hats were thrown in the air and one excited individual fell on his back in the attempt to do a somersault. The boy who rode the winner was carried around the pack and is the hero of the day," reported the "Australasian Sketcher" in 1876. Both Peter St. Albans and Briseis have now become racing legends, and Briseis is regarded as one of the greatest mares foaled in Australia.
Briseis wasn't the only sensation surrounding the 1876 Melbourne Cup. Two months before the event, on Saturday 9 September, the "City Of Melbourne" sailed for Melbourne from Sydney with a cargo including 13 racehorses, many of whom were considered serious contenders for the Melbourne Cup. The following day the ship ran into a savage storm and was hit by several rogue waves, with Nemesis (the winner of the 1876 AJC Metropolitan Handicap in Randwick, Sydney and favourite for the Cup, owned by John Moffat) and Robin Hood (another favourite, owned by Etienne de Mestre) being among the 11 horses that were killed. Betting on the big race was paralysed. To the dismay and anger of the public, bookmakers, showing no feeling, presented a purse (loaded with coins) to the captain as token of their appreciation for his part in saving them many thousands of pounds in bets already laid on the favourites who had perished. Perhaps they should have kept their money, however. The outsider Briseis comfortably won by 1 length in the biggest field of all time, and in an extremely good time, so it is unlikely that the horses who perished could have beaten her.
1877 is also the year that the trainer Etienne de Mestre won his fourth Melbourne Cup with Chester owned by Hon. James White. In 1878, as in previous years De Mestre fielded more than one horse. He entered the favourite Firebell (owned by W.S. Cox) who finished last, Chester (owned by Hon. James White) the previous year's winner who fell, and Calamia (owned by de Mestre) who, though less fancied, won easily by two lengths. First prize was £1,790, the crowd was 80,000 and there were 30 starters. De Mestre's 1878 win with Calamia brought to 5 the number of Melbourne Cups he had won. This record was not to be matched for nearly 100 years when the trainer Bart Cummings won his fifth Melbourne Cup in 1975. Bart Cummings, regarded as the best Australian horse trainer of all time, went on to win 12 Melbourne Cups to 2008, and is still training horses.
In 1883, the hardy New Zealand bred, Martini Henry won the VRC Derby, the Melbourne Cup and on the following Monday retained his undefeated record by winning Mares' Produce Stakes.
Phar Lap, the most famous horse in the world of his day, won the 1930 Melbourne Cup at 11/8 odds on, the shortest priced favourite in the history of the race. He had to be hidden away at Geelong before the race after an attempt was made to shoot him and only emerged an hour before the race time of the Cup. Phar Lap also competed in 1929 and 1931, but came 3rd and 8th respectively, despite heavy favouritism in both years.
There are a few legends of the first Aboriginal jockey to ride a Melbourne Cup. It was believed to be John Cutts who won the first and second cups in 1861 and 1862 riding Archer. He was reputedly an Aboriginal stockman born in the area where Archer was trained, but was actually John 'Cutts' Dillon, the son of a Sydney clerk, a jockey who rode for many trainers in his long career, and who was one of the best known, best liked and most respected jockeys in New South Wales. It is thought that Peter St. Albans was the first Aboriginal jockey to win the cup, on Briseis in 1876. Because St. Albans was not quite 13 years old, the jockey was too young to ride in the cup. Thus, to allow him to race Briseis in the Cup, it was argued his birthdate and parents were unknown, and from this the legend of him being Aboriginal grew. Both these legends, however, can definitely be disproved, and history had to wait nearly another 100 years. The first jockey of Indigenous heritage to ride a Melbourne Cup winner was Frank Reys in 1973 on Gala Supreme, who had a Filipino father and a half-Aboriginal mother.
Recent years.
The race has undergone several alterations in recent years, the most visible being the entry of many foreign-trained horses. Most have failed to cope with the conditions; the three successful "foreign raids" include two by Irish trainer Dermot K. Weld successful in 1993 and 2002, and one in 2006 by Katsumi Yoshida of Japan's renowned Yoshida racing and breeding family. The attraction for foreigners to compete was, primarily, the low-profile change to the new "quality handicap" weighting system.
The 1910 Melbourne Cup was won by Comedy King, the first foreign bred horse to do so. Subsequent foreign bred horses to win Cup were Backwood 1924; Phar Lap 1930; Wotan 1936 Belldale Ball 1980; At Talaq 1986; Kingston Rule 1990; Vintage Crop 1993; Jeune 1994; Media Puzzle 2002; Makybe Diva 2003, 2004, 2005; Americain 2010 and Dunaden 2011.
The 1938 Melbourne Cup was won by trainer Mrs. Allan McDonald, who conditioned Catalogue. Mrs McDonald was a successful trainer in New Zealand, however, at the time women were not allowed to compete as trainers in Australia so her husband's name was officially recorded as the winning trainer. The 2001 edition was won by New Zealand mare Ethereal, trained by Sheila Laxon, the first woman to formally train a Melbourne Cup winner. She also won the Caulfield Cup, a 2,400 metre race also held in Melbourne, and therefore has won the "Cups Double".
Maree Lyndon became the first female to ride in the Melbourne Cup, when she partnered Argonaut Style in 1987, in which she ran second last in the 21 horse field.
In 2004, Makybe Diva became the first mare to win two cups, and also the first horse to win with different trainers, after David Hall moved to Hong Kong and transferred her to the Lee Freedman stables.
The 2005 Melbourne Cup was held before a crowd of 106,479. Makybe Diva made history by becoming the only horse to win the race three times. Trainer Lee Freedman said after the race, "Go and find the youngest child on the course, because that's the only person here who will have a chance of seeing this happen again in their lifetime."
Due to the 2007 Australian Equine influenza outbreak, believed to have been started by a horse brought into Australia from Japan, neither Delta Blues nor Pop Rock participated in the 2007 Melbourne Cup. Both horses had been stabled in Japan. Corowa, NSW trained "Leica Falcon" also was not be permitted to race in Victoria, despite Corowa being close to the Victorian border. Leica Falcon was ordained as the new staying star of Australian racing in 2005 when he ran fourth in both the Caulfield Cup and in Makybe Diva's famous third Melbourne Cup victory. But serious leg injuries saw the horse not race for another 20 months. Efficient, the previous year's VRC Derby winner, won the race.
In 2013, Damien Oliver returned from an eight-month ban, after betting against his own mount at a previous race meet, to win his 3rd Melbourne cup.
Public holiday.
Melbourne Cup day is a public holiday for all working within metropolitan Melbourne and some parts of regional Victoria, but not for some country Victorian cities and towns which hold their own spring carnivals. For Federal Public Servants it is also observed as a holiday in the entire state of Victoria, and from 2007 to 2009 also in the Australian Capital Territory known as Family and Community Day replacing Picnic Day.
As early as 1865, Cup day was a half-holiday in Melbourne for public servants and bank officials. Various businesses also closed at lunchtime.
It took some years before the purpose of the declared holiday was acknowledged in the Victoria Government Gazette. The Gazette of 31 October 1873 announced that the following Thursday (Cup Day) be observed as a bank and civil (public) service holiday.
Attendance.
The event is one of the most popular spectator events in Australia, with sometimes over 110,000 people, some dressed in traditional formal raceday wear and others in all manner of exotic and amusing costumes, attending the race. The record crowd was 122,736 in 2003. The 1926 running of the Cup was the first time the 100,000 mark had been passed. Today the record at Flemington is held by the 2006 Victoria Derby when almost 130,000 attended.
In 2007, a limit was placed on the Spring Carnival attendance at Flemington Racecourse and race-goers are now required to pre-purchase tickets.
Off the track.
'Fashions On The Field' is a major focus of the day, with substantial prizes awarded for the best-dressed man and woman. The requirement for elegant hats, and more recently the alternative of a fascinator, almost single-handedly keeps Melbourne's milliners in business. Raceday fashion has occasionally drawn almost as much attention as the race itself, The miniskirt received worldwide publicity when model Jean Shrimpton wore a white shift version of one on Derby Day during Melbourne Cup week in 1965.
Flowers, especially roses are an important component of the week's racing at Flemington. The racecourse has around 12,000 roses within its large expanse. Over 200 varieties of the fragrant flower are nurtured by a team of up to 12 gardeners. Each of the major racedays at Flemington has an official flower. Victoria Derby Day has the Corn Flower, Melbourne Cup Day is for the Yellow Rose, Oaks Day highlights the Pink Rose and Stakes Day goes to the Red Rose.
In the Melbourne metropolitan area, the race day has been a gazetted public holiday since 1877, but around both Australia and New Zealand a majority of people watch the race on television and gamble, either through direct betting or participating in workplace cup "sweeps". As of April 2007, the ACT also recognises Melbourne Cup Race Day as a holiday. In 2000, a betting agency claimed that 80 percent of the adult Australian population placed a bet on the race that year. In 2010 it was predicted that $183 million would be spent by 83,000 tourists during the Spring Racing Carnival. In New Zealand, the Melbourne Cup is the country's single biggest betting event, with carnival race-days held at several of the country's top tracks showing the cup live on big screens.
It is commonly billed as "The race that stops a nation", but it is more accurately "The race that stops two nations", as many people in New Zealand, as well as Australia, pause to watch the race.

</doc>
<doc id="20486" url="https://en.wikipedia.org/wiki?curid=20486" title="Messerschmitt Me 163 Komet">
Messerschmitt Me 163 Komet

The Messerschmitt Me 163 Komet, designed by Alexander Lippisch, was a German rocket-powered fighter aircraft. It is the only rocket-powered fighter aircraft ever to have been operational. Its design was revolutionary, and had performance unrivaled at the time. German test pilot Heini Dittmar in early July 1944 reached , an unofficial flight airspeed record unmatched by turbojet-powered aircraft for almost a decade. Over 300 aircraft were built, but the Komet proved ineffective as a fighter and was responsible for the destruction of only about nine Allied aircraft. (Sixteen air victories for 10 losses, according to other sources.)
Development.
Work on the design started under the aegis of the "Deutsche Forschungsanstalt für Segelflug" (DFS)—the German Institute for the Study of sailplane flight. Their first design was a conversion of the earlier Lippisch Delta IV known as the DFS 39 and used purely as a glider testbed of the airframe. A larger follow-on version with a small propeller engine started as the DFS 194. This version used wingtip-mounted rudders, which Lippisch felt would cause problems at high speed. Lippisch changed the system of vertical stabilization for the DFS 194's airframe from the earlier DFS 39's wingtip rudders, to a conventional vertical stabilizer at the rear of the aircraft. The design included a number of features from its origins as a glider, notably a skid used for landings, which could be retracted into the aircraft's keel in flight. For takeoff, a pair of wheels, each mounted onto the ends of a specially designed cross-axle, were needed due to the weight of the fuel, but the wheels, forming a takeoff "dolly" under the landing skid, were released shortly after takeoff.
The designers planned to use the forthcoming Walter R-1-203 "cold engine" of thrust, which used a monopropellant consisting of stabilized HTP known by the name "T-Stoff". Heinkel had also been working with Hellmuth Walter on his rocket engines, mounting them in the He 112's tail for testing, and later in the first purpose-designed, liquid-fueled rocket aircraft, the He 176. Heinkel had also been selected to produce the fuselage for the DFS 194 when it entered production, as it was felt that the highly volatile monopropellant "fuel's" reactivity with organic matter would be too dangerous in a wooden fuselage structure. Work continued under the code name "Projekt X".
The division of work between DFS and Heinkel led to problems, notably that DFS seemed incapable of building even a prototype fuselage. Lippisch eventually asked to leave DFS and join Messerschmitt instead. On 2 January 1939, he moved with his team and the partly completed DFS 194 to the Messerschmitt works at Augsburg. The delays caused by this move allowed the engine development to "catch up". Once at Messerschmitt, the team decided to abandon the propeller-powered version and move directly to rocket-power. The airframe was completed in Augsburg and in early 1940 was shipped to receive its engine at Peenemünde-West, one of the quartet of "Erprobungsstelle"-designated military aviation test facilities of the Reich. Although the engine proved to be extremely unreliable, the aircraft had excellent performance, reaching a speed of in one test.
In the Me 163B and -C subtypes, a windmill generator on the extreme nose of the fuselage, and the backup lead-acid battery inside the fuselage that it charged, provided the electrical power for the radio, the Revi16B, -C, or -D reflector gunsight, the direction finder, the compass, the firing circuits of the cannons, and some of the lighting in the cockpit instrumentation.
There was an onboard lead/acid battery, but its capacity was limited, as was its endurance, no more than 10 minutes, hence the fitted generator.
The airspeed indicator averaged readings from two sources: the pitot tube on the leading edge of the port wing, and a small pitot inlet in the nose, just above the top edge of the underskid channel. There was a further tapping-off of pressure-ducted air from the pitot tube which also provided the rate of climb indicator with its source.
Me 163A.
Production of a prototype series started in early 1941, known as the "Me 163". Secrecy was such that the RLM's "GL/C" airframe number, "8-163", was actually that of the earlier, pre-July 1938 Messerschmitt Bf 163. It was thought that intelligence services would conclude any reference to the number "163" would be for that earlier design. The Me 163A V4 was shipped to Peenemünde to receive the HWK RII-203 engine in May 1941. By 2 October 1941, the Me 163A V4, bearing the radio call sign letters, or "Stammkennzeichen", "KE+SW", set a new world speed record of , piloted by Heini Dittmar, with no apparent damage to the aircraft during the attempt. Some postwar aviation history publications stated that the Me 163A V3 was thought to have set the record.
The 1,004 km/h record figure would not be officially approached until the postwar period by the new jet fighters of the British and U.S., and was not surpassed (except by the later Me 163B V18 in 1944, but seriously damaged by the attempt) until the American Douglas Skystreak turbojet-powered research aircraft did so on 20 August 1947 with no damage. Five prototype Me 163A V-series aircraft were built, adding to the original DFS 194 (V1), followed by eight pre-production examples designated as "Me 163 A-0".
Landing gear and ground handling procedures.
During testing, the jettisonable main landing gear arrangement, of a differing design to that used on the later B-series production aircraft, was a serious problem. The A-series "dolly" landing gear caused many aircraft to be damaged on takeoff when the wheels rebounded and crashed into the aircraft due to the sizable springs and shock absorbers on the A-series "dolly" devices which possessed well-sprung independent suspension systems for each main wheel, not used on the much simpler, crossbeam-axled B-series aircraft dollies. The landing skid on the B-series "Komet" design possessed a pneumatic cylinder for the extendable skid, which had to remain extended after attachment of the dolly to absorb ground-running impacts during the takeoff run, as well as a hydraulic cylinder for shock absorption on landing. If the hydraulic cylinder was malfunctioning—or if the pilot simply forgot to release the hydraulic pressure on the skid before landing, after extending it for touchdown to absorb the force of the landing itself—the resulting unbuffered impact of a hard touchdown on the skid could cause back injuries to the pilot when landing.
Once on the ground, the aircraft had to be retrieved by a "Scheuch-Schlepper", a converted small agricultural vehicle, originally based on the concept of the two-wheel tractor, carrying a detachable third swiveling wheel at the extreme rear of its design for stability in normal use—this swiveling third wheel was replaced with a pivoting, special retrieval trailer that rolled on a pair of short, triple-wheeled continuous track setups (one per side) for military service wherever the "Komet" was based. This retrieval trailer usually possessed twin trailing lifting arms, that lifted the stationary aircraft off the ground from under each wing. Another form of trailer, known also to have been trialled with the later B-series examples, was tried during the "Komet"s test phase, which used a pair of sausage-shaped air bags in place of the lifting arms and could also be towed by the "Scheuch-Schlepper" tractor, inflating the air bags to lift the aircraft. The three-wheeled "Scheuch-Schlepper" tractor used for the task was originally meant for farm use, but such a vehicle with a specialized trailer was required as the "Komet" was unpowered after exhausting its rocket propellants, and lacked main wheels after landing, from the jettisoning of its "dolly" main gear at takeoff. The slightly larger Sd Kfz 2 "Kettenkrad" half-track motorcycle, known to be used with the Me 262 jet fighter for ground handling needs, and documented as also being used with the Arado Ar 234B jet recon-bomber, was not known to have ever been used for ground handling operations with the "Komet" at any time.
During flight testing, the superior gliding capability of the "Komet" proved detrimental to safe landing. As the now un-powered aircraft completed its final descent, it could rise back into the air with the slightest updraft. Since the approach was unpowered, there was no opportunity to make another landing pass. For production models, a set of landing flaps allowed somewhat more controlled landings. This issue remained a problem throughout the program. Nevertheless, the overall performance was tremendous, and plans were made to put Me 163 squadrons all over Germany in around any potential target. Development of an operational version was given the highest priority.
Me 163B.
A simplified construction format for the Me 163 fighter's airframe was deemed necessary, as the Me 163A version was not truly optimized for large-scale production, with design work starting in December 1941. The result was the Me 163B subtype, which had the desired, more mass-producible fuselage, wing panel, retractable landing skid and tailwheel designs with the previously mentioned unsprung "dolly" takeoff gear, and a generally one-piece conical nose for the forward fuselage which could incorporate a "windmill" generator for supplementary electrical power while in flight, as well as a one-piece, perimeter frame-only hinged canopy for ease of production.
Meanwhile, Walter had started work on the newer HWK 109-509 bipropellant "hot engine", which added a true fuel of hydrazine hydrate and methanol, designated "C-Stoff", that burned with the oxygen-rich exhaust from the "T-Stoff", used as the oxidizer, for added thrust (see: List of Stoffs). The new powerplant and numerous detail design changes meant to simplify production over the general A-series airframe design resulted in the significantly modified Me 163B of late 1941. Due to the "Reichsluftfahrtministerium" (RLM) requirement that it should be possible to throttle the engine, the original power plant grew complicated and lost reliability.
The fuel system was particularly troublesome, as leaks incurred during hard landings easily caused fires and explosions. Metal fuel lines and fittings, which failed in unpredictable ways, were used as this was the best technology available. Both fuel and oxidizer were toxic and required extreme care when loading in the aircraft, yet there were occasions when "Komets" exploded on the tarmac from the propellants' hypergolic nature. Both propellants were clear fluids, and different tanker trucks were used for delivering each propellant to a particular "Komet" aircraft, usually the "C-Stoff" hydrazine/methanol-base fuel first. For safety purposes, it left the immediate area of the aircraft following its delivery and capping off of the "Komet"s fuel tanks from a rear located dorsal fuselage filling point just ahead of the "Komet"s vertical stabilizer. Then, the other tanker truck carrying the very reactive "T-Stoff" hydrogen peroxide oxidizer would deliver its load through a different filling point on the "Komet"s dorsal fuselage surface, located not far behind the rear edge of the canopy.
The corrosive nature of the liquids, especially for the "T-Stoff" oxidizer, required special protective gear for the pilots. To help prevent explosions, the engine and the propellant storage and delivery systems were frequently and thoroughly hosed down and flushed with water run through the propellant tanks and the rocket engine's propellant systems before and after flights, to clean out any remnants. The relative "closeness" to the pilot of some 120 litres (31.7 US gal) of the chemically active T-Stoff oxidizer, split between two auxiliary oxidizer tanks of equal volume to either side within the lower flanks of the cockpit area—besides the main oxidizer tank of some 1,040 litre (275 US gal) volume just behind the cockpit's rear wall, could present a serious or even fatal hazard to a pilot in a fuel-caused mishap.
Two prototypes were followed by 30 Me 163 B-0 pre-production aircraft armed with two 20 mm MG 151/20 cannon and some 400 Me 163 B-1 production aircraft armed with two 30 mm (1.18 inch) MK 108 cannons, but which were otherwise similar to the B-0. Early in the war, when German aircraft firms created versions of their aircraft for export purposes, the a was added to export ("ausland") variants (B-1a) or to foreign-built variants (Ba-1) but for the Me 163, there were neither export nor a foreign-built version. Later in the war, the "a" and successive letters were used for aircraft using different engine types: as Me 262 A-1a with Jumo engines, Me 262 A-1b with BMW engines. As the Me 163 was planned with an alternative BMW P3330A rocket engine, it is likely the "a" was used for this purpose on early examples. Only one Me 163, the V10, was tested with the BMW engine, so this designation suffix was soon dropped. The Me 163 B-1a did not have any wingtip "washout" built into it, and as a result, it had a much higher critical Mach number than the Me 163 B-1.
The Me 163B had very docile landing characteristics, mostly due to its integrated leading edge slots, located directly forward of the elevon control surfaces, and just behind and at the same angle as the wing's leading edge. It would neither stall nor spin. One could fly the "Komet" with the stick full back, and have it in a turn and then use the rudder to take it out of the turn, and not fear it snapping into a spin. It would also slip well. Because the Me 163B's airframe design was derived from glider design concepts, it had excellent gliding qualities, and the tendency to continue flying above the ground due to ground effect. On the other hand, making a too close turn from base onto final, the sink rate would increase, and one could quickly lose altitude and come in short. Another main difference from a propeller-driven aircraft is that there was no slipstream over the rudder. On takeoff, one had to attain the speed at which the aerodynamic controls become effective—about —and that was always a critical factor. Pilots accustomed to flying propeller-driven aircraft had to be careful the control stick was not somewhere in the corner when the control surfaces began working. These, like many other specific Me 163 problems, would be resolved by specific training.
The performance of the Me 163 far exceeded that of contemporary piston engine fighters. At a speed of over the aircraft would take off, in a so-called "scharfen start" ("sharp start", with "start" being the German word for "take-off") from the ground, from its two-wheeled dolly. The aircraft would be kept at level flight at low altitude until the best climbing speed of around was reached, at which point it would jettison the dolly, retract its extendable skid using a knob-topped release lever just forward of the throttle (as both levers were located atop the cockpit's portside 120 litre "T-Stoff" oxidizer tank) that engaged the aforementioned pneumatic cylinder, and then pull up into a 70° angle of climb, to a bomber's altitude. It could go higher if required, reaching in an unheard of three minutes. Once there, it would level off and quickly accelerate to speeds around or faster, which no Allied fighter could match. The usable Mach number was similar to that of the Me 262, but because of the high thrust-to-drag ratio, it was much easier for the pilot to lose track of the onset of severe compressibility and risk loss of control. A Mach warning system was installed as a result. The aircraft was remarkably agile and docile to fly at high speed. According to Rudolf Opitz, chief test pilot of the Me 163, it could "fly circles around any other fighter of its time".
By this point, Messerschmitt was completely overloaded with production of the Messerschmitt Bf 109 and attempts to bring the Me 210 into service. Production in a dispersed network was handed over to Klemm, but quality control problems were such that the work was later given to Junkers, who were, at that time, underworked. As with many German designs of World War II's later years, parts of the airframe (especially the wings) were made of wood by furniture manufacturers. The older Me 163A and first Me 163B prototypes were used for training. It was planned to introduce the Me 163S, which removed the rocket engine and tank capacity and placed a second seat for the instructor above and behind the pilot, with his own canopy. The Me 163S would be used for glider landing training, which as explained above, was essential to operate the Me 163. It appears the 163Ss were converted from the earlier Me 163B series prototypes.
In service, the Me 163 turned out to be difficult to use against enemy aircraft. Its tremendous speed and climb rate meant a target was reached and passed in a matter of seconds. Although the Me 163 was a stable gun platform, it required excellent marksmanship to bring down an enemy bomber. The "Komet" was equipped with two 30 mm (1.18 inch) MK 108 cannons which had a relatively low muzzle velocity of 540 meters per second (1,772 feet/sec), and were accurate only at short range, making it almost impossible to hit a slow moving bomber. Four or five hits were typically needed to take down a B-17.
A number of innovative solutions were implemented to ensure kills by less experienced pilots. The most promising was a unique weapon called the "Sondergerät 500 Jägerfaust". This consisted of a series of single-shot, short-barreled 50 mm (2 inch) guns pointing upwards, akin to "Schräge Musik". Five were mounted in the wing roots on each side of the aircraft. The trigger was tied to a photocell in the upper surface of the aircraft, and when the "Komet" flew under the bomber, the resulting change in brightness caused by the underside of the aircraft could cause the rounds to be fired. As each shell shot upwards, the disposable gun barrel that fired it was ejected downwards, thus making the weapon recoilless. It appears that this weapon was used in combat only once, resulting in the destruction of a bomber.
Later versions.
The biggest concern about the design was the short flight time, which never met the projections made by Walter. With only seven and a half minutes of powered flight, the fighter truly was a dedicated point defense interceptor. To improve this, the Walter firm began developing two more advanced versions of the 509A rocket engine, the 509B and C, each with two separate combustion chambers of differing sizes, one above the other, for greater efficiency. The B-version possessed a main combustion chamber—usually termed in German as a "Hauptofen" on these dual-chamber subtypes—with an exterior shape much like that on the single chamber 509A version, with the C-version having a forward chamber shape of a more cylindrical nature, designed for a higher top thrust level of some 2,000 kg (4,410 lb) of thrust, while simultaneously dropping the use of the cubic-shape "frame" for the forward engine propellant flow/turbopump mechanisms as used by the earlier -A and -B versions. The 509B and 509C rocket motors' main combustion chambers were supported by the "thrust tube" exactly as the 509A motor's single chamber had been. They were tuned for "high power" for takeoff and climb. The added, smaller volume "lower" chamber on the two later models, nicknamed the "Marschofen" with approximately of thrust at its top performance level, was intended for more efficient, lower power cruise flight. These HWK 109–509B and C motors would improve endurance by as much as 50%. Two 163 Bs, models V6 and V18, were experimentally fitted with the lower-thrust B-version of the new twin-chamber engine (mandating twin combustion chamber pressure gauges on the instrument panel of any "Komet" equipped with them), a retractable tailwheel, and tested in spring 1944.
The main combustion chamber of the 509B engine used for the B V6 and V18 occupied the same location as the A-series' engine did, with the lower "Marschofen" "cruise chamber" housed within the retractable tailwheel's appropriately widened ventral tail fairing. On 6 July 1944, the Me 163B V18 (VA+SP), like the B V6 basically a standard production Me 163B airframe outfitted with the new-twin chamber "cruiser" rocket motor with the aforementioned modifications beneath the original rocket motor orifice to accept the extra combustion chamber, set a new unofficial world speed record of , piloted by Heini Dittmar, and landed with almost all of the vertical rudder surface broken away from flutter. This record was not broken in terms of absolute speed until 6 November 1947 by Chuck Yeager in flight number 58 that was part of the Bell X-1 test program, with a , or Mach 1.35 supersonic speed, recorded at an altitude of nearly .
The X-1 never exceeded Dittmar's speed from a normal runway "scharfen-Start" liftoff. Heini Dittmar had reached the performance, after a normal "sharp start" ground takeoff, without an air drop from a mother ship. Neville Duke exceeded Heini Dittmar's record mark on 31 August 1953, with the Hawker Hunter F Mk3 at a speed of , after a normal ground start. Postwar experimental aircraft of the aerodynamic configuration that the Me 163 used, were found to have serious stability problems when entering transonic flight, like the similarly configured, and turbojet powered, Northrop X-4 Bantam and de Havilland DH 108, which made the V18's record with the Walter 509B "cruiser" rocket motor more remarkable.
Waldemar Voigt of Messerschmitt's "Oberammergau" project and development offices started a redesign of the 163 to incorporate the new twin-chamber Walter rocket engine, as well as fix other problems. The resulting Me 163C design featured a larger wing through the addition of an insert at the wing root, an extended fuselage with extra tank capacity through the addition of a "plug" insert behind the wing, and a new pressurized cockpit topped with a bubble canopy for improved visibility, on a fuselage that had dispensed with the earlier B-version's dorsal fairing. The additional tank capacity and cockpit pressurization allowed the maximum altitude to increase to , as well as improving powered time to about 12 minutes, almost doubling combat time (from about five minutes to nine). Three Me 163 C-1a prototypes were planned, but it appears only one was flown, but without its intended engine.
By this time the project was moved to Junkers. There, a new design effort under the direction of Heinrich Hertel at Dessau attempted to improve the "Komet". The Hertel team had to compete with the Lippisch team and their Me 163C. Hertel investigated the Me 163 and found it was not well suited for mass production and not optimized as a fighter aircraft, with the most glaring deficiency being the lack of retractable landing gear. To accommodate this, what would eventually become the Me 263 V1 prototype would be fitted with the desired tricycle gear, also accommodating the twin-chamber Walter rocket from the start—later it was assigned to the Ju 248 program.
The resulting "Junkers Ju 248" used a three-section fuselage to ease construction. The V1 prototype was completed for testing in August 1944, and was glider-tested behind a Junkers Ju 188. Some sources state that the Walter 109–509C engine was fitted in September, but it was probably never tested under power. At this point the RLM reassigned the project to Messerschmitt, where it became the Messerschmitt Me 263. This appears to have been a formality only, with Junkers continuing the work and planning production. By the time the design was ready to go into production, the plant where it was to be built was overrun by Soviet forces. While it did not reach operational status, the work was briefly continued by the Soviet Mikoyan-Gurevich (MiG) design bureau as the Mikoyan-Gurevich I-270.
Operational history.
The initial test deployment of the Me 163A, to acquaint prospective pilots with the world's first rocket-powered fighter, occurred with "Erprobungskommando 16", led by Luftwaffe Major Wolfgang Späte and first established in late 1942, receiving their eight A-model service test aircraft by July 1943. Their initial base was as the "Erprobungsstelle" test facility located at the Peenemünde-West field, then departed permanently following an RAF bombing raid on the area on 17 August 1943. The next day the unit moved out, southwards to the base at Anklam, near the Baltic coast. Their stay was brief, as a few weeks later they were placed in northwest Germany, based at the military airfield at Bad Zwischenahn (at ) from August 1943 to August 1944. EK 16 received their first B-series armed Komets in January 1944, and was ready for action by May while at Bad Zwischenahn, first seeing combat flights on the 13th of the month.
As EK 16 commenced small-scale combat operations with the Me 163B in May 1944, the Me 163B's unsurpassed velocity was something that the Allied fighter pilots were at a loss to counter. The "Komets" attacked singly or in pairs, often even faster than the intercepting fighters could dive. A typical Me 163 tactic was to fly vertically upward through the bombers flying at , rise up to an altitude of , then dive through the formation again firing as they went. This approach afforded the pilot two brief chances to fire a few rounds from his cannons before gliding back to his airfield. The pilots reported that it was possible to make four passes on a bomber, but only if it was flying alone. As the cockpit was unpressurized, the operational ceiling was limited by what the pilot could endure for several minutes while breathing oxygen from a mask, without losing consciousness. Pilots underwent altitude-chamber training to harden them against the rigors of operating in the thin air of the stratosphere without a pressure suit. Special low fiber diets were prepared for pilots, as gas in the gastrointestinal tract would expand rapidly during ascent.
Following the initial combat trials of the Me 163B with EK 16, during the winter and spring of 1944 Major Wolfgang Späte formed the first dedicated Me 163 fighter wing, "Jagdgeschwader" 400 (JG 400), in Brandis near Leipzig. JG 400's purpose was to provide additional protection for the Leuna synthetic gasoline works which were raided frequently during almost all of 1944. A further group was stationed at Stargard near Stettin to protect the large synthetic fuel plant at Pölitz (today Police, Poland). Further defensive units of rocket fighters were planned for Berlin, the Ruhr and the German Bight.
The first actions involving the Me 163 occurred on 28 July 1944, from I./JG 400's base at Brandis, when two USAAF B-17 Flying Fortress were attacked without confirmed kills. Combat operations continued from May 1944 to spring 1945. During this time, there were nine confirmed kills with 14 Me 163s lost. Feldwebel Siegfried Schubert was the most successful pilot, with three bombers to his credit. Allied fighter pilots soon noted the short duration of the powered flight. They would wait, and when the engine exhausted its propellant supply, pounce on the unpowered "Komet". However, the "Komet" was extremely manoeuvrable in gliding flight. Another Allied method was to attack the fields the Komets operated from and strafe them after the Me 163s landed. Due to the skid-based landing gear system, the Komet was immobile until the "Scheuch-Schlepper" tractor could back the trailer up to the nose of the aircraft, place its two rear arms under the wing panels, and jack up the trailer's arms to hoist the aircraft off the ground or place it back on its take-off dolly to tow it back to its maintenance area.
Establishing a defensive perimeter with anti-aircraft guns ensured that Allied fighters avoided these bases. At the end of 1944, 91 aircraft had been delivered to JG 400 but lack of fuel had kept most of them grounded. It was clear that the original plan for a huge network of Me 163 bases would never to be realized. Up to that point, JG 400 had lost only six aircraft due to enemy action. Nine were lost to other causes, remarkably few for such a revolutionary and technically advanced aircraft. In the last days of the Third Reich the Me 163 was given up in favor of the more successful Me 262. In May 1945, Me 163 operations were stopped, the JG 400 disbanded, and many of its pilots sent to fly Me 262s. In any operational sense, the "Komet" was a failure. Although it shot down 16 aircraft, mainly four-engined bombers, it did not warrant the effort put into the project. Due to fuel shortages late in the war, few went into combat, and it took an experienced pilot with excellent shooting skills to achieve "kills". The "Komet" also spawned later weapons like the vertical-launch Bachem Ba 349 Natter, and the postwar, American turbojet-powered Convair XF-92 delta wing interceptor. Ultimately, the point defense role that the Me 163 played would be taken over by the surface-to-air missile (SAM), Messerschmitt's own example being the Enzian.
Flying the Me 163.
Captain Eric Brown RN, Chief Naval Test Pilot and commanding officer of the Captured Enemy Aircraft Flight, who tested the Me 163 at the Royal Aircraft Establishment (RAE) at Farnborough, said, "The Me 163 was an aeroplane that you could not afford to just step into the aircraft and say 'You know, I'm going to fly it to the limit.' You had very much to familiarise yourself with it because it was state-of-the-art and the technology used." Acting unofficially, after a spate of accidents involving Allied personnel flying captured German aircraft resulted in official disapproval of such flights, Brown was determined to fly a powered Komet. On around 17 May 1945, he flew an Me 163B at Husum with the help of a cooperative German ground crew, after initial towed flights in an Me 163A to familiarise himself with the handling.
The day before the flight, Brown and his ground crew had performed an engine run on the chosen Me 163B to ensure that everything was running correctly, the German crew being apprehensive should an accident befall Brown, until being given a disclaimer signed by him to the effect that they were acting under his orders. On the rocket-powered "scharfen-start" takeoff the next day, after dropping the takeoff dolly and retracting the skid, Brown later described the resultant climb as "like being in charge of a runaway train", the aircraft reaching 32,000 ft (9.76 km) altitude in 2 minutes, 45 seconds. During the flight, while practicing attacking passes at an imaginary bomber, he was surprised at how well the Komet accelerated in the dive with the engine shut down. When the flight was over Brown had no problems on the approach to the airfield, apart from the rather restricted view from the cockpit due to the flat angle of glide, the aircraft touching down at . Once down safely, Brown and his much-relieved ground crew celebrated with a drink.
Beyond Brown's unauthorised flight, the British never tested the Me 163 under power themselves; due to the danger of its hypergolic propellants it was only flown unpowered. Brown himself piloted RAE's Komet "VF241" on a number of occasions, the rocket motor being replaced with test instrumentation. When interviewed for a 1990s television programme, Brown said he had flown five tailless aircraft in his career (including the British de Havilland DH 108). Referring to the Komet, he said "this is the only one that had good flight characteristics"; he called the other four "killers".
Surviving aircraft.
It has been claimed that at least 29 "Komets" were shipped out of Germany after the war and that of those at least 10 have been known to survive the war to be put on display in museums around the world. Most of the 10 surviving Me 163s were part of JG 400, and were captured by the British at Husum, the squadron's base at the time of Germany's surrender in 1945. According to the RAF museum, 48 aircraft were captured intact and 24 were shipped to the United Kingdom for evaluation, although only one, "VF241", was test flown (unpowered).
Germany.
Eventually an elderly German woman came forward with Me 163 instruments that her late husband had collected after the war, and the engine was reproduced by a machine shop owned by Me 163 enthusiast Reinhold Opitz. The factory closed in the early 1990s and the "Yellow 25" was moved to a small museum created on the site. The museum contained aircraft that had once served as gate guards, monuments and other damaged aircraft previously located on the air base. In 1997 "Yellow 25" was moved to the official Luftwaffe Museum located at the former RAF base at Berlin-Gatow, where it is displayed today alongside a restored Walter HWK 109–509 rocket engine. This particular Me 163B is one of the very few World War II–era German military aircraft, restored and preserved in a German aviation museum, to have a swastika national marking of the Third Reich, in a "low visibility" white outline form, currently displayed on the tailfin.
United Kingdom.
Of the 21 aircraft that were captured by the British, at least three have survived. They were assigned the British serial numbers AM200 to AM220.
Japanese versions.
As part of their alliance, Germany provided the Japanese Empire with plans and an example of the Me 163. One of the two submarines carrying Me 163 parts did not arrive in Japan, so at the time, the Japanese lacked a few important parts, including the turbopump which they could not make themselves. The Japanese Me 163 crashed on its first flight and was completely destroyed. The Japanese versions were designed as trainers, fighters, and interceptors. Differences between the versions were fairly minor. The Mitsubishi Ki-200 "Shusui" ("Shu" means "autumn", "sui" means "water" in Japanese) was the equivalent of the 163 B, armed with two 30 mm (1.18 in) Ho 155-II cannon.
The Navy version, the Mitsubishi J8M1 "Shusui", replaced the Ho 155 cannon with the Navy's 30 mm (1.18 in) Type 5. Mitsubishi also planned on producing a version of the 163 C for the Navy, known as the J8M2 "Shusui" Model 21. A version of the 163 D/263 was known as the J8M3 "Shusui" for the Navy with the Type 5 cannon, and a Ki-202 "Shusui-kai" ("kai" means "modified" in Japanese) with the Ho 155-II for the Army. Trainers were planned, roughly the equivalent of the Me 163 A-0/S. These were known as the Yokoi Ku-13 "Akigusa" ("Aki" means also "autumn" and "gusa" ("kusa") means "grass" in Japanese) or Ki-200 "Shusui" Rocket Interceptor practice glider.
Other trainer variants included:
One complete example of the Japanese aircraft survives at the Planes of Fame Air Museum in California. The fuselage of a second aircraft is displayed at the Mitsubishi company's Komaki Plant Museum, at Komaki, Aichi in Japan.
Replicas.
A flying replica Me 163 was constructed between 1994 and 1996 by Joseph Kurtz, a former "Luftwaffe" pilot who trained to fly Me 163s, but who never flew in combat. He subsequently sold the aircraft to EADS. The replica is an unpowered glider whose shape matches that of an Me 163, although its construction completely differs - the glider is built of wood with an empty weight of , a fraction of the weight of a wartime aircraft. Reportedly, it has excellent flying characteristics. The glider is painted red to represent the Me 163 flown by Wolfgang Späte. As of 2011, it was still flying with the civil registration D-1636.
In the early 2000s, a rocket-powered airworthy replica, the "Komet II", was proposed by XCOR Aerospace, an aerospace company that had previously built the XCOR EZ-Rocket rocket-plane. Although outwardly the same as a wartime aircraft, the "Komet II" design would have differed considerably for safety reasons. It would have been partially constructed with composite materials, powered by one of XCOR's own simpler and safer, pressure fed, liquid oxygen/alcohol engines, and retractable undercarriage would have been used instead of a takeoff dolly and landing skid. The project is no longer discussed on the company's website, and it appears work has ceased on this project.
Several static replica Me 163s are exhibited in museums.

</doc>
<doc id="20487" url="https://en.wikipedia.org/wiki?curid=20487" title="Mohamed Atta">
Mohamed Atta

Mohamed Mohamed el-Amir Awad el-Sayed Atta ( ""  ; September 1, 1968 – September 11, 2001) was an Egyptian hijacker and one of the ringleaders of the September 11 attacks who served as the hijacker-pilot of American Airlines Flight 11, crashing the plane into the North Tower of the World Trade Center as part of the coordinated attacks. At 33 years of age, he was the oldest hijacker to participate in the attacks.
Born in 1968 in a small town in Egypt's Nile Delta, Atta moved with his family to the Abdeen section of Cairo at the age of 10. Atta studied architecture at Cairo University, graduating in 1990, and continued his studies in Hamburg, Germany at the Technical University of Hamburg. In Hamburg, Atta became involved with the al-Quds Mosque, where he met Marwan al-Shehhi, Ramzi bin al-Shibh, and Ziad Jarrah, together forming the Hamburg cell. Atta disappeared from Germany for periods of time, spending some time in Afghanistan, including several months in late 1999 and early 2000 when he met Osama bin Laden and other top al-Qaeda leaders. Atta and the other Hamburg cell members were recruited by bin Laden and Khalid Sheikh Mohammed for the "planes operation" in the United States. Atta returned to Hamburg in February 2000, and began inquiring about flight training in the United States.
Atta arrived in the United States, together with Marwan al-Shehhi, in June 2000. Both ended up in Venice, Florida at Huffman Aviation where they entered the Accelerated Pilot Program. Atta and Shehhi obtained instrument ratings in November 2000, and continued training on simulators and flight training. Beginning in May 2001, Atta assisted with the arrival of the muscle hijackers. In July 2001, Atta traveled to Spain where he met with bin al-Shibh to exchange information and finalize the plot. In August 2001, Atta traveled as a passenger on several "surveillance" flights, to establish in detail how the attacks could be carried out.
In early September 2001, Atta traveled to Prince George's County, Maryland, where fellow hijacker Hani Hanjour was at the time. Atta then traveled to Boston, and on September 10, with Abdulaziz al-Omari to Portland, Maine. They spent the night at the Comfort Inn in South Portland. On the morning of September 11, Atta and Omari traveled on Colgan Air back to Boston, where they boarded American Airlines Flight 11. Fifteen minutes into the flight, the team of hijackers attacked and Atta took over control of the aircraft. At 8:46 a.m., Atta crashed the Boeing 767 into the North Tower of the World Trade Center, killing everyone on board and an additional 1,366 civilians at or above the floors of impact in the North Tower.
Aliases.
Mohamed Atta varied his name on documents, also using "Mehan Atta", "Mohammad El Amir", "Muhammad Atta", "Mohamed El Sayed", "Mohamed Elsayed", "Muhammad al-Amir", "Awag Al Sayyid Atta", "Muhammad al-Amir", and "Awad Al Sayad". In Germany, he registered his name as "Mohamed el-Amir Awad el-Sayed Atta", and went by the name Mohamed el-Amir at the Technical University of Hamburg. In his will, written in 1996, Atta gives his name as "Mohamed the son of Mohamed Elamir awad Elsayed." When he came to the United States, he used the name Mohamed Atta. Atta also claimed different nationalities, sometimes Egyptian and other times telling people he was from the United Arab Emirates.
Early life.
Atta was born on September 1, 1968 in Kafr el-Sheikh, located in Egypt's Nile Delta region. His father, Mohamed el-Amir Awad el-Sayed Atta, was a lawyer, educated in both sharia and civil law. His mother, Bouthayna Mohamed Mustapha Sheraqi, came from a wealthy farming and trading family and was also educated. Bouthayna and Mohamed married when she was 14, via an arranged marriage. The family had few relatives on the father's side and kept a distance from Bouthayna's family. In-laws characterized Atta's father as "austere, strict, and private," and neighbors considered the family reclusive. Atta was the only son, but he had two older sisters who are both well-educated and successful in their careers—one as a medical doctor and the other as a professor.
When Atta was ten, his family moved to the Cairo neighborhood of Abdeen, located near the center of the city. Atta's father continued to keep the family private, and did not allow Atta to socialize with other neighborhood children. Atta spent most of his time at home studying, excelling in school. In 1985, Atta entered Cairo University, where he studied engineering. As one of the highest-scoring students, Atta was admitted into the very selective architecture program during his senior year. In 1990, Atta graduated with a degree in architecture, and joined the Muslim Brotherhood-affiliated Engineers Syndicate organization. For several months after graduating, Atta worked at the Urban Development Center in Cairo, where he worked on architectural, planning, and building design. In 1990, Atta's family moved into an 11th floor apartment in Giza.
Germany.
Upon graduating from Cairo University, Atta's marks were average and insufficient to be accepted into the University's graduate program. His father insisted he go abroad for graduate studies, and had Atta enroll in a German language program at the Goethe Institute in Cairo. In 1992, Atta's father invited a German couple over for dinner while they were visiting Cairo. The German couple ran an exchange program between Germany and Egypt, and suggested that Atta continue his studies in Germany. They offered him a temporary place to live at their house in the city. Mohamed Atta ended up in Germany two weeks later, in July 1992.
In Germany, Atta enrolled in the urban planning graduate program at the Technical University of Hamburg-Harburg. Atta initially stayed with the two high school teachers, but they found Atta frustrating owing to his closed-mindedness and intensely introverted personality. Atta also began adhering to a strictly Islamic diet, frequented the mosque, seldom socialized, and was unfriendly towards the couple's unmarried daughter who had a young child. After six months, they asked him to move out.
By early 1993, Atta had moved into university housing, sharing an apartment in Centrumshaus with two roommates. He remained at Centrumshaus until 1998. During that time, his roommates became frustrated with Atta, who seldom cleaned, and kept to himself to the extent that he would walk in and out of a room without acknowledging others. Beyond anything else, they could not deal with Atta's personality, described as "complete, almost aggressive insularity".
Academic studies.
At the Technical University of Hamburg-Harburg, Atta studied under the guidance of the department chair, Dittmar Machule, who specialized in the Middle East. Atta was concerned about modern development and the construction of high-rise buildings in Cairo and other ancient cities in the Middle East. He believed that the large, impersonal, and often ugly apartment blocks built in the 1960s and 1970s had ruined old neighborhoods, and took away privacy and dignity from people. Atta's own family moved into such an apartment block in 1990, which to him was "a shabby symbol of Egypt's haphazard attempts to modernize and its shameless embrace of the West." For his thesis, Atta focused his studies on the ancient city of Aleppo in Syria. He explored the history of Aleppo's urban landscapes and the general themes of the conflict between Arab civilization and modernity. Atta criticized how the modern skyscrapers and development projects in Aleppo were disrupting the fabric of that city by blocking community streets and altering the skyline.
In 1994, Atta's professor, Dittmar Machule, invited him to Aleppo for a three-day archaeological visit. Atta ended up spending several weeks in Aleppo during August 1994, and visited again that December. While in Syria, he met Amal, a young Palestinian woman, who worked there in the planning bureau. Volker Hauth, who was traveling with Atta, described Amal as "attractive and self-confident. She observed the Muslim niceties, taking taxis to and from the office so as not to come into close physical contact with men on the buses. But, she was 'emancipated' and 'challenging'." They appeared to be attracted to one another, but Atta regretfully explained to Hauth that, "she had a quite different orientation and that the emancipation of the young lady did not fit." This was the closest thing to romance for Atta. In mid-1995, Atta spent three months in Cairo with fellow students Volker Hauth and Ralph Bodenstein, on a grant from the Carl Duisberg Society. They looked at the effects of redevelopment in the Islamic Cairo old quarter which the government wanted to develop for tourism. Atta remained in Cairo with his family, after Hauth and Bodenstein returned to Germany.
While in Hamburg, Atta held several jobs, including a part-time position at Plankontor, an urban planning firm, from 1992 until mid-1997 when he was laid off. The firm's business had declined, and when it bought a CAD system, "his draughtsmanship was not needed." Atta also worked at a cleaning firm, and buying and selling cars to earn extra money. After studying in Hamburg, Atta had wanted to return to Cairo to work, but there were few job prospects, as his family did not have the "right connections." Atta was also concerned about actions of the Egyptian government in arresting political activists, and feared that he too would be a target due to his social and political beliefs.
Islamic revival.
After coming to Hamburg in 1992, Atta became more religious, frequently attending the mosque. Atta's friends in Germany described him as an intelligent man with religious beliefs, along with political motivations, including anger at U.S. policy toward the Middle East, particularly the Oslo Accords and the Gulf War. Atta was also enraged by Egypt's ruling elite, and the Egyptian government's crackdown on Muslim Brotherhood members.
Atta went back to Egypt on August 1, 1995, for three months of study. Before going to Egypt, he grew a beard, which is a sign of a devout Muslim, but was also seen as a political gesture. Atta returned to Hamburg on October 31, 1995, and joined the pilgrimage to Mecca that fall.
In Hamburg, Atta was drawn to al-Quds Mosque, which adhered to a "harsh, uncompromisingly fundamentalist, and resoundingly militant" version of Sunni Islam. He made acquaintances at al-Quds, some of whom visited him at Centrumshaus. He also began teaching classes at Al-Quds, as well as at a Turkish mosque near Harburg. Atta also formed a prayer group, which Ahmed Maklat and Mounir El Motassadeq joined. Ramzi bin al-Shibh was also there teaching occasional classes, and became Atta's friend.
On April 11, 1996, Atta signed his last will and testament at the mosque, officially declaring his Muslim beliefs and giving 18 instructions regarding his burial. This was the day that Israel attacked Lebanon in Operation Grapes of Wrath, which outraged Atta. Signing the will, "offering his life" was Atta's response. The instructions in his last will and testament reflect both Sunni funeral practices, along with some more puritanical demands from Salafism, including asking people not "to weep and cry" or show emotion. The will was signed by el-Motassadeq and a second individual at the mosque.
After leaving Plankontor in the summer of 1997, Atta disappeared again and did not return until 1998. Atta phoned his graduate advisor in 1998, after a year of doing nothing for his thesis, telling Machule that he had family problems at home and said, “Please understand, I don’t want to talk about this.” At the winter break in 1997, Atta left and did not return to Hamburg for three months. He said that he went on pilgrimage to Mecca again, just 18 months after his first time. Terry McDermott explained in "Perfect Soldiers" that it is highly unusual and unlikely for someone, especially a young student, to go on Hajj again that soon. Also, three months is an exceptionally long time, much longer than what Hajj requires. When Atta returned, he claimed that his passport was lost and got a new one, which is a common tactic to erase evidence of travel to places such as Afghanistan. When he returned in spring 1998, after disappearing for several months, he had grown a thick long beard, and "seemed more serious and aloof" to those who knew him.
By mid-1998, Atta was no longer eligible for university housing in Centrumshaus. He moved into a nearby apartment in Wilhelmsburg, where he lived with Said Bahaji and Ramzi bin al-Shibh. By early 1999, Atta had completed his thesis, and formally defended it in August 1999.
In mid-1998, Atta worked alongside Shehhi, bin al-Shibh, and Belfas, at a warehouse, packing computers in crates for shipping. The Hamburg group did not stay in Wilhelmsburg for long. The next winter, they moved into an apartment at Marienstrasse 54 in the borough of Harburg, near the Technical University of Hamburg, at which they enrolled. It was here that the Hamburg cell developed and acted more as a group. They met three or four times a week to discuss their anti-American feelings and to plot possible attacks. Many al-Qaeda members lived in this apartment at various times, including hijacker Marwan al-Shehhi, Zakariya Essabar, and others.
In late 1999, Atta, Shehhi, Jarrah, Bahaji, and bin al-Shibh decided to travel to Chechnya to fight against the Russians, but were convinced by Khalid al-Masri and Mohamedou Ould Slahi at the last minute to change their plans. They instead traveled to Afghanistan over a two-week period in late November. On November 29, 1999, Mohamed Atta boarded Turkish Airlines Flight TK1662 from Hamburg to Istanbul, where he changed to flight TK1056 to Karachi, Pakistan. After they arrived, they were selected by Al Qaeda leader Abu Hafs as suitable candidates for the "planes operation" plot. They were all well-educated, had experience of living in western society, along with some English skills, and would be able to obtain visas. Even before bin al-Shibh had arrived, Atta, Shehhi, and Jarrah were sent to the House of Ghamdi near bin Laden’s home in Kandahar, where he was waiting to meet them. Bin Laden asked them to pledge loyalty and commit to suicide missions, which Atta and the other three Hamburg men all accepted. Bin Laden sent them to see Mohammed Atef to get a general overview of the mission, and then they were sent to Karachi to see Khalid Sheikh Mohammed to go over specifics.
German investigators said that they had evidence that Mohamed Atta trained at al-Qaeda camps in Afghanistan from late 1999 to early 2000. The timing of the Afghanistan training was outlined on August 23, 2002 by a senior investigator. The investigator, Klaus Ulrich Kersten, director of Germany's federal anticrime agency, the Bundeskriminalamt, provided the first official confirmation that Atta and two other pilots had been in Afghanistan and the first dates of the training. Kersten said in an interview at the agency's headquarters in Wiesbaden, Germany, that Atta was in Afghanistan from late 1999 until early 2000, and that there was evidence that Atta met with Osama bin Laden there.
A video surfaced in October 2006 which showed bin Laden at Tarnak Farms on January 8, 2000, and also showed Atta together with Ziad Jarrah reading their wills ten days later on January 18, 2000. On his return journey, Atta left Karachi on February 24, 2000 by flight TK1057 to Istanbul where he changed to flight TK1661 to Hamburg. Immediately after returning to Germany, Atta, al-Shehhi, and Jarrah reported their passports stolen, possibly to discard travel visas to Afghanistan.
In the United States.
On March 22, 2000, while still in Germany, Mohamed Atta sent an e-mail to the Academy of Lakeland in Florida, inquiring about flight training, "Dear sir, we are a small group of young men from different Arab countries. Now, we are living in Germany since a while for study purposes. We would like to start training for the career of airline professional pilots. In this field, we haven't yet any knowledge but we are ready to undergo an intensive training program (up to ATP and eventually higher)." Atta sent 50–60 similar e-mails to other flight training schools in the United States.
On May 17, Mohamed Atta applied for a United States visa, receiving a five-year B-1/B-2 (tourist/business) visa the next day from the United States embassy in Berlin. Because Atta had lived in Germany for approximately five years, along with his "strong record as a student", he was treated favorably and not scrutinized. After obtaining his visa, Atta took a bus on June 2 from Germany to Prague where he stayed overnight before traveling on to the United States the next day. Bin al-Shibh later explained that they believed it would contribute to operational security for Atta to fly out of Prague instead of Hamburg, where he traveled from previously. Likewise, Shehhi traveled from a different location, in his case via Brussels.
On June 6, 2002, ABC's "World News Tonight" broadcast an interview with Johnelle Bryant, former loan officer at the U.S. Department of Agriculture in South Florida, who told about her encounter with Mohamed Atta. This encounter took place "around the third week of April to the third week of May of 2000", before Atta's official entry date into the United States (see below). According to Bryant, Atta wanted to finance the purchase of a crop-duster. "He wanted to finance a twin-engine, six-passenger aircraft and remove the seats," Bryant told ABC's "World News Tonight". He insisted that she write his name as ATTA, that he originally was from Egypt but had moved to Afghanistan, that he was an engineer and that his dream was to go to a flight school. He asked about the Pentagon and the White House. He said he wanted to visit the World Trade Center and asked Bryant about the security there. He mentioned Al Qaeda and said the organization "could use memberships from Americans". He mentioned Osama bin Laden and said "this man would someday be known as the world's greatest leader." Bryant said "the picture that came out in the newspaper, that's exactly what that man looked like." Bryant contacted the authorities after recognising Atta in news reports. Law-enforcement officials said Bryant passed a lie-detector exam.
According to official reports, Atta arrived on June 3, 2000, at Newark International Airport from Prague. That month, Atta and Shehhi stayed in hotels and rented rooms in New York City on a short-term basis. They continued to inquire about flight schools and personally visited some, including Airman Flight School in Norman, Oklahoma, which they visited on July 3, 2000. Days later, Shehhi and Atta ended up in Venice, Florida (On the Gulf Coast of South Florida). Atta and Shehhi established accounts at SunTrust Bank and received wire transfers from Ali Abdul Aziz Ali, Khalid Sheikh Mohammed's nephew in the United Arab Emirates. On July 6, 2000, Atta and Shehhi enrolled at Huffman Aviation in Venice, Florida, where they entered the Accelerated Pilot Program, while Ziad Jarrah took flight training from a different school also based in Venice. When Atta and Shehhi arrived in Florida, they initially stayed with Huffman's bookkeeper and his wife in a spare room of their house. After a week, they were asked to leave because they were rude. Atta and Shehhi then moved into a small house nearby in Nokomis where they stayed for six months.
Atta began flight training on July 7, 2000, and continued training nearly every day. By the end of July, both Atta and Shehhi did solo flights. Atta earned his private pilot certificate in September, and then he and Shehhi decided to switch flight schools. Both enrolled at Jones Aviation in Sarasota and took training there for a brief time. They had problems following instructions and were both very upset when they failed their Stage 1 exam at Jones Aviation. They inquired about multi-engine planes and told the instructor that "they wanted to move quickly, because they had a job waiting in their country upon completion of their training in the U.S." In mid-October, Atta and Shehhi returned to Huffman Aviation to continue training. In November 2000, Atta earned his instrument rating, and then a commercial pilot's license in December from the Federal Aviation Administration.
Atta continued with flight training, including solo flights and simulator time. On December 22, Atta and Shehhi applied to Eagle International for large jet and simulator training for McDonnell Douglas DC-9 and Boeing 737–300 models. On December 26, Atta and Shehhi needed a tow for their rented Piper Cherokee on a taxiway of Miami International Airport after the engine shut down. On December 29 and 30, Atta and Marwan went to the Opa-locka Airport where they practiced on a Boeing 727 simulator, and they obtained Boeing 767 simulator training from Pan Am International on December 31. Atta purchased flight deck videos for Boeing 747–200, Boeing 757–200, Airbus A320 and Boeing 767-300ER models via mail-order from Sporty's Pilot Shop in Batavia, Ohio in November and December 2000.
Atta's cellphone was recorded phoning the Moroccan embassy in Washington on January 2, just before Shehhi flew to the country. Atta flew to Spain on January 4, 2001 to coordinate with bin al-Shibh and returned to the United States on January 10. While in the United States he traveled to Lawrenceville, Georgia, where he and Shehhi attended a LA Fitness Health Club. During that time Atta flew out of Briscoe Field in Lawrenceville with a pilot, and Atta and either the pilot or Shehhi flew around the Atlanta area. They lived in the area for several months. On April 3, Atta and Shehhi rented a postal box in Virginia Beach, Virginia.
On April 11, Atta and Shehhi rented an apartment at 10001 Atlantic Blvd, Apt. 122 in Coral Springs, Florida for $840 per month, and assisted with the arrival of the muscle hijackers. On April 16 Atta was given a citation for not having a valid driver's license, and began steps to get one. On May 2, Atta received his driver's license in Lauderdale Lakes, Florida. While in the United States, Atta owned a red 1989 Pontiac Grand Prix.
On June 27, Atta flew from Fort Lauderdale to Boston, Massachusetts, where he spent a day, and then continued to San Francisco for a short time, and from there to Las Vegas. On June 28, Atta arrived at McCarran International Airport in Las Vegas to meet with the three other pilots. He rented a Chevrolet Malibu from an Alamo Rent A Car agency. It is not known where he stayed that night, but on the 29th he registered at the Econo Lodge at 1150 South Las Vegas Boulevard. Here he presented an AAA membership for a discount, and paid cash for the $49.50/night room. During his trip to Las Vegas, he is thought to have used a video camera that he had rented from a Select Photo outlet back in Delray Beach, Florida.
July 2001 summit in Spain.
Atta left again in July 2001 for Spain to meet with bin al-Shibh for the last time. On July 7, 2001, Atta flew on Swissair Flight 117 from Miami to Zürich, where he had a stopover. On July 8, Atta was recorded withdrawing 1700 Swiss francs from an ATM, and using his credit card to purchase two Swiss Army knives and some chocolate in an airport shop in Zurich. After the stopover in Zurich, he arrived in Madrid at 4:45 pm on Swissair Flight 656, and spent several hours at the airport. Then at 8:50 pm, he checked into the Hotel Diana Cazadora in Barajas, a town near the airport. That night and twice the next morning, he called Bashar Ahmad Ali Musleh, a Jordanian student in Hamburg who served as a liaison for bin al-Shibh.
On the morning of July 9, Mohamed Atta rented a silver Hyundai Accent, which he booked from SIXT Rent-A-Car for July 9 to 16, and later extended to the 19th. He drove east out of Madrid towards the Mediterranean beach area of Tarragona. On the way, Atta stopped in Reus to pick up Ramzi bin al-Shibh at the airport. They drove to Cambrils, where they spent a night at the Hotel Monica. They checked out the next morning, and spent the next few days at an unknown location in Tarragona. The absence of other hotel stays, signed receipts or credit card stubs has led investigators to believe that the men may have met in a safe house provided by other al-Qaeda operatives in Spain. There, Atta and bin al-Shibh held a meeting to complete the planning of the attacks. Several clues have been found to link their stay in Spain to Syrian-born Imad Eddin Barakat Yarkas (Abu Dahdah), and Amer el Azizi, a Moroccan in Spain. They may have helped arrange and host the meeting in Tarragona. Yosri Fouda, who interviewed bin al-Shibh and Khalid Sheikh Mohammed (KSM) before the arrest, believes that Said Bahaji and KSM may have also been present at the meeting. Spanish investigators have said that Marwan al-Shehhi and two others later joined the meeting. Bin al-Shibh would not discuss this meeting with Fouda.
During the Spain meetings, Atta and bin al-Shibh had coordinated the details of the attacks. The 9/11 Commission obtained details about the meeting, based on interrogations of bin al-Shibh in the weeks after his arrest in September 2002. Bin al-Shibh explained that he passed along instructions from Osama bin Laden, including his desire for the attacks to be carried out as soon as possible. Bin Laden was concerned about having so many operatives in the United States. Atta confirmed that all the muscle hijackers had arrived in the United States, without any problems, but said that he needed five to six more weeks to work out details. Bin Laden also asked that other operatives not be informed of the specific data until the last minute. During the meeting, Atta and bin al-Shibh also decided on the targets to be hit, ruling out a strike on a nuclear plant. Bin al-Shibh passed along bin Laden's list of targets; bin Laden wanted the U.S. Congress, the Pentagon, and the World Trade Center to be attacked, as they were deemed "symbols of America." They also discussed the personal difficulties Atta was having with fellow hijacker Ziad Jarrah. Bin al-Shibh was worried that Jarrah might even abandon the plan. The 9/11 Commission Report speculated that the now-convicted terrorist conspirator Zacarias Moussaoui was being trained as a possible replacement for Jarrah.
From July 13 to 16, Atta stayed at the Hotel Sant Jordi in Tarragona. After bin al-Shibh returned to Germany on July 16, 2001, Atta had three more days in Spain. He spent two nights in Salou at the beachside Casablanca Playa Hotel, then spent the last two nights at the Hotel Residencia Montsant. On July 19, Atta returned to the United States, flying on Delta Air Lines from Madrid to Fort Lauderdale, via Atlanta.
August 2001 final plans in U.S..
On July 22, 2001, Mohamed Atta rented a Mitsubishi Galant from Alamo Rent A Car, putting 3,836 miles on the vehicle before returning it on July 26. On July 25, Atta dropped Ziad Jarrah off at Miami International Airport for a flight back to Germany. On July 26, Atta traveled via Continental Airlines to Newark, New Jersey, checked into the Kings Inn Hotel in Wayne, New Jersey and stayed there until July 30 when he took a flight from Newark back to Fort Lauderdale.
On August 4, Atta is believed to have been at Orlando International Airport waiting to pick up suspected "20th Hijacker" Mohammed al-Qahtani from Dubai, who ended up being held by immigration as "suspicious." Atta was believed to have used a payphone at the airport to phone a number "linked to al-Qaeda" after Qahtani was denied entry.
On August 6, Atta and Shehhi rented a 1995 white, four door Ford Escort from Warrick's Rent-A-Car, which was returned on August 13. On August 6, Atta booked a flight on Spirit Airlines from Fort Lauderdale to Newark, leaving on August 7 and returning on August 9. The reservation was not used and canceled on August 9 with the reason "Family Medical Emergency". Instead, he went to Central Office & Travel in Pompano Beach to purchase a ticket for a flight to Newark, leaving on the evening of August 7 and schedule to return in the evening on August 9. Atta did not take the return flight. On August 7, Atta checked into the Wayne Inn in Wayne, New Jersey and checked out on August 9. The same day, he booked a one-way first class ticket via the Internet on America West Flight 244 from Ronald Reagan Washington National Airport to Las Vegas. Atta traveled twice to Las Vegas on "surveillance flights" rehearsing how the 9/11 attacks would be carried out. Other hijackers traveled to Las Vegas at different times in the summer of 2001.
Throughout the summer, Atta met with Nawaf al-Hazmi to discuss the status of the operation on a monthly basis.
On August 23, Atta's driver license was revoked "in absentia" after he failed to show up in traffic court to answer the earlier citation for driving without a license. On the same day, Israeli Mossad reportedly gave his name to the CIA as part of a list of 19 names they said were planning an attack in the near future. Only four of the names are known for certain, the others being Marwan al-Shehhi, Khalid al-Mihdhar and Nawaf al-Hazmi. On August 30 he was recorded purchasing a utility knife from a Wal-Mart store near the hotel where he stayed prior to 9/11.
Attacks.
On September 10, 2001, Atta picked up Omari from the Milner Hotel in Boston, Massachusetts, and the two drove their rented Nissan Altima to a Comfort Inn in South Portland, Maine; on the way they were seen getting gasoline at an Exxon Gas Station. They arrived at 5:43 pm and spent the night in room 232. While in South Portland, they were seen making two ATM withdrawals, and stopping at Wal-Mart. FBI also reported that "two middle-eastern men" were seen in the parking lot of a Pizza Hut, where Atta is known to have eaten that day.
Atta and Omari arrived early the next morning, at 5:40 am, at the Portland International Jetport, where they left their rental car in the parking lot and boarded a 6:00 am Colgan Air (US Airways Express) BE-1900C flight to Boston's Logan International Airport. In Portland, Mohamed Atta was selected by the Computer Assisted Passenger Prescreening System (CAPPS), which required his checked bags to undergo extra screening for explosives but involved no extra screening at the passenger security checkpoint.
The connection between the two flights at Logan International Airport was within Terminal B, but the two gates were not connected within security. Passengers must leave the secured area, go outdoors, cross a covered roadway, and enter another building before going through security once again. There are two separate concourses in Terminal B; the south concourse is mainly used by US Airways and the north one is mostly used by American Airlines. It had been overlooked that there would still be a security screen to pass in Boston because of this distinct detail of the terminal's arrangement. At 6:45 am, while at the Boston airport, Atta took a call from Flight 175 hijacker Marwan al-Shehhi. This call was apparently to confirm that the attacks were ready to begin. Atta checked in for American Airlines Flight 11, passed through security again, and boarded the flight. Atta was seated in business class, in seat 8D. At 7:59 am, the plane departed from Boston, carrying 81 passengers.
The hijacking began at 8:14 am—15 minutes after the flight departed—when beverage service would be starting. At this time, the pilots stopped responding to air traffic control, and the aircraft began deviating from the planned route. At 8:18 am, flight attendants Betty Ong and Madeline Amy Sweeney began making phone calls to American Airlines to report what was happening. Ong provided information about lack of communication with the cockpit, lack of access to the cockpit, and passenger injuries. At 8:24:38 am, a voice believed to be Atta's was heard by air traffic controllers, saying: "We have some planes. Just stay quiet and you will be OK. We are returning to the airport." "Nobody move, everything will be OK. If you try to make any moves you'll endanger yourself and the airplane. Just stay quiet." "Nobody move, please. We are going back to the airport. Don't try to make any stupid moves." The plane's transponder was turned off at 8:28 am. At 8:46:40 am, Atta flew the plane into the North Tower.
Because the flight from Portland to Boston had been delayed, his bags did not make it onto Flight 11. Atta's bags were later recovered in Logan International Airport, and they contained airline uniforms, flight manuals, and other items. The luggage included a copy of Atta's will, written in Arabic, as well as a list of instructions, also in Arabic, such as "make an oath to die and renew your intentions", "you should feel complete tranquility, because the time between you and your marriage in heaven is very short", and "check your weapon before you leave and long before you leave. You must make your knife sharp and you must not discomfort your animal during the slaughter".
Martyrdom video.
On October 1, 2006, "The Sunday Times" released a video it had obtained "through a previously tested channel", purporting to show Mohamed Atta and Ziad Jarrah recording a martyrdom message six years earlier at a training camp in Afghanistan. The video, bearing the date of January 18, 2000, is of good resolution but contains no sound track. Lip readers have failed to decipher it. Atta and Jarrah appear in high spirits, laughing and smiling in front of the camera. They had never been pictured together before. Unidentified sources from both Al-Qaeda and the United States confirmed to The Times the video's authenticity. A separate section of the video shows Osama bin Laden addressing his followers at a complex near Kandahar. Ramzi bin al-Shibh is also identified in the video. According to "The Sunday Times", "American and German investigators have struggled to find evidence of Atta’s whereabouts in January 2000 after he disappeared from Hamburg. The hour-long tape places him in Afghanistan at a decisive moment in the development of the conspiracy when he was given operational command. Months later both he and Jarrah enrolled at flying schools in America."
Mistaken identity.
In the aftermath of the September 11, 2001 attacks, names of the hijackers were released. There was some confusion regarding who Mohamed Atta was, and cases of mistaken identity. Initially, Mohamed Atta's identity was confused with that of a native Jordanian, Mahmoud Mahmoud Atta, who bombed an Israeli bus in the West Bank in 1986, killing one and severely injuring three. Mahmoud Atta was 14 years older than Atta. Mahmoud Atta, a naturalized U.S. citizen, was subsequently deported from Venezuela to the United States, extradited to Israel, tried and sentenced to life in prison. The Israeli Supreme Court later overturned his extradition and set him free. After 9/11, there also were reports stating that Mohamed Atta had attended International Officers School at Maxwell Air Force Base in Montgomery, Alabama. "The Washington Post" quoted a United States Air Force official who explained, "discrepancies in their biographical data, such as birth dates 20 years off, indicate we are probably not talking about the same people."
There were numerous reports in the media of Atta and Shehhi going to Shuckum's Oyster Pub and Seafood Grill, a sports bar in Hollywood, Florida, on September 8, 2001. Atta, Shehhi, and a third unidentified man reportedly drank heavily and played the Golden Tee '97 arcade game there. The bartender said "Atta drank vodka and orange juice, while Shehhi preferred rum and cokes, five drinks apiece. They were wasted intoxicated." Manager Tony Amos described, "The guy Mohamed was drunk, his voice was slurred and he had a thick accent." Bartender Patricia Idrissi said the men argued over the bill, and when she asked if there was a problem, "Mohamed said he worked for American Airlines and he could pay his bill." Atta said, "I have plenty of money. I'm a pilot." And he hauled a wad of $50 and $100 bills from his pocket, eventually leaving a $3 tip. However, Atta flew on US Airways Flight 2719 to Baltimore on September 7 from Fort Lauderdale. On September 8, Atta was in Laurel, Maryland, where he went to a Safeway grocery store to wire $2850 to Mustafa Muhammad Ahmad in Dubai, and sent another $5000 to Ahmed from a Giant Food store in Laurel on the same day. On September 9, Atta flew on to Boston.
Prague controversy.
In the months following the September 11 attacks, officials at the Czech Interior Ministry asserted that Atta made a trip to Prague on April 8, 2001 to meet with an Iraqi intelligence agent named Ahmed Khalil Ibrahim Samir al-Ani. This piece of information was passed on to the FBI as "unevaluated raw intelligence". Intelligence officials have concluded that such a meeting did not occur. A Pakistani businessman named Mohammed had come to Prague from Saudi Arabia on May 31, 2000, with this second Atta possibly contributing to confusion. The Egyptian Mohamed Atta arrived at the Florenc bus terminal in Prague, from Germany, on June 2, 2000. He left Prague the next day, flying on Czech Airlines to Newark, New Jersey. In the Czech Republic, some intelligence officials say the source of the purported meeting was an Arab informant who approached the Czech intelligence service with his sighting of Atta only after Atta's photograph had appeared in newspapers all over the world. U.S. and Czech intelligence officials have since concluded that the person seen with Ani, was mistakenly identified as Atta, and the consensus of investigators has concluded that Atta never attended a meeting in Prague.
Able Danger.
In 2005, Army Lt. Col. Anthony Shaffer and Congressman Curt Weldon alleged that the Defense Department data mining project Able Danger produced a chart that identified Atta, along with Nawaf al-Hazmi, Khalid al-Mihdhar, and Marwan al-Shehhi, as members of a Brooklyn-based al-Qaeda cell in early 2000. Shaffer largely based his allegations on the recollections of Navy Captain Scott Phillpott, who later recanted his recollection, telling investigators that he was "convinced that Atta was not on the chart that we had." Phillpott said that Shaffer was "relying on my recollection 100 percent," and the Defense Department Inspector General's report indicated that Philpott "may have exaggerated knowing Atta's identity because he supported using Able Danger's techniques to fight terrorism."
Five witnesses who had worked on Able Danger and had been questioned by the Defense Department's Inspector General later told investigative journalists that their statements to the IG were distorted by investigators in the final IG's report, or the report omitted essential information that they had provided. The alleged distortions of the IG report centered around excluding any evidence that Able Danger had identified and tracked Atta years before 9/11.
Lt Col Shaffer's book also clearly indicates direct identification of the Brooklyn cell, and Mohammed Atta.
Family reaction and denial.
Atta's father, Mohamed el-Amir Awad el-Sayed Atta, a retired lawyer in Egypt vehemently rejected allegations his son was involved in the September 11 attacks, and instead accused Mossad and the U.S. government of having a hand in framing his son. Atta Sr. rejected media reports his son was drinking wildly, and instead described his son as a quiet boy uninvolved in politics, shy and devoted to studying architecture. The elder Mr. Atta said he had spoken with Mohamed by phone the day after the September 11 crashes. He held interviews with the German news magazine "Bild am Sonntag" in late 2002, saying his son was alive and in hiding in fear for his life, and that American Christians were responsible for the September 11 attacks. In a subsequent interview in 2005, Atta Sr. stated "My son is gone. He is now with God. The Mossad killed him."
Motivation.
There are multiple, conflicting explanations for Atta's behavior and motivation. Political psychologist Jerrold Post has suggested that Atta and his fellow hijackers were just following orders from Al Qaeda leadership, "and whatever their destructive, charismatic leader, Osama bin Laden said was the right thing to do for the sake of the cause was what they would do." In turn, political scientist Robert Pape has claimed that Atta was motivated by his commitment to the political cause, that he was psychologically normal, and that he was “not readily characterized as depressed, not unable to enjoy life, not detached from friends and society.” By contrast, criminal justice professor Adam Lankford has found evidence that Atta was clinically suicidal, and that his struggles with social isolation, depression, guilt, shame, hopelessness, and rage were extraordinarily similar to the struggles of those who commit conventional suicide and murder-suicide. By this view, Atta’s political and religious beliefs affected the method of his suicide and his choice of target, but they were not the underlying causes of his behavior.

</doc>
<doc id="20488" url="https://en.wikipedia.org/wiki?curid=20488" title="Messerschmitt Me 262">
Messerschmitt Me 262

The Messerschmitt Me 262, nicknamed Schwalbe (German: "Swallow") in fighter versions, or Sturmvogel (German: "Storm Bird") in attack versions, was the world's first operational jet-powered fighter aircraft. Design work started before World War II began, but engine problems and top-level interference kept the aircraft from operational status with the Luftwaffe until mid-1944. The Me 262 was faster, and more heavily-armed than any Allied fighter, including the British jet-powered Gloster Meteor. One of the most advanced aviation designs in operational use during World War II, the Me 262 was used in a variety of roles, including light bomber, reconnaissance, and even experimental night fighter versions.
Me 262 pilots claimed a total of 542 Allied kills, although higher claims are sometimes made. The Allies countered its potential effectiveness in the air by attacking the aircraft on the ground and during takeoff and landing. Engine reliability problems, from the pioneering nature of its Junkers Jumo 004 axial-flow turbojet engines—the first ever placed in mass production—and attacks by Allied forces on fuel supplies during the deteriorating late-war situation also reduced the effectiveness of the aircraft as a fighting force. In the end, the Me 262 had a negligible impact on the course of the war as a result of its late introduction and the consequently small numbers put in operational service.
While German use of the aircraft ended with the close of the Second World War, a small number were operated by the Czechoslovak Air Force until 1951. Captured Me 262s were studied and flight tested by the major powers, and ultimately influenced the designs of a number of post-war aircraft such as the North American F-86 Sabre and Boeing B-47 Stratojet. A number of aircraft have survived on static display in museums, and there have also been several privately built flying reproductions.
Design and development.
Origins.
Several years before World War II, the Germans foresaw the great potential for aircraft that used the jet engine constructed by Hans Joachim Pabst von Ohain in 1936. After the successful test flights of the world's first jet aircraft—the Heinkel He 178—within a week of the Invasion of Poland to start the war, they adopted the jet engine for an advanced fighter aircraft. As a result, the Me 262 was already under development as "Projekt" 1065 (P.1065) before the start of World War II. The project originated with a request by the "Reichsluftfahrtministerium" (RLM, Ministry of Aviation) for a jet aircraft capable of one hour's endurance and a speed of at least . Dr Waldemar Voigt headed the design team, with Messerschmitt's chief of development, Robert Lusser, overseeing.
Plans were first drawn up in April 1939, and the original design was very different from the aircraft that eventually entered service, with wing root-mounted engines, rather than podded ones, when submitted in June 1939. The progression of the original design was delayed greatly by technical issues involving the new jet engine. Because the engines were slow to arrive, Messerschmitt moved the engines from the wing roots to underwing pods, allowing them to be changed more readily if needed; this would turn out to be important, both for availability and maintenance. Since the BMW 003 jets proved heavier than anticipated, the wing was swept slightly, by 18.5°, to accommodate a change in the center of gravity. Funding for the jet engine program was also initially lacking as many high-ranking officials thought the war could easily be won with conventional aircraft. Among those were Hermann Göring, head of the Luftwaffe, who cut the engine development program to just 35 engineers in February 1940 (the month before the first wooden mock-up was completed); Willy Messerschmitt, who desired to maintain mass production of the piston-powered, 1935-origin Bf 109 and the projected Me 209; and Major General Adolf Galland, who had initially supported Messerschmitt through the early development years, flying the Me 262 himself on 22 April 1943. By that time, problems with engine development had slowed production of the aircraft considerably. One particularly acute problem arose with the lack of an alloy with a melting point high enough to endure the high temperatures involved, a problem that by the end of the war had not been adequately resolved. The aircraft made its first successful flight entirely on jet power on 18 July 1942, powered by a pair of Jumo 004 engines, after a November 1941 flight (with BMW 003s) ended in a double flameout.
The project aerodynamicist on the design of the Me 262 was Ludwig Bölkow. He initially designed the wing using NACA airfoils modified with an elliptical nose section. Later in the design process, these were changed to AVL derivatives of NACA airfoils, the NACA 00011-0.825-35 being used at the root and the NACA 00009-1.1-40 at the tip. The elliptical nose derivatives of the NACA airfoils were used on the horizontal and vertical tail surfaces. Wings were of single-spar cantilever construction, with stressed skins, varying from thick at the root to at the tip. As a conservation measure, late in the war, wing interiors would not be painted. The wings were fastened to the fuselage at four points, using a pair of and forty-two bolts.
In mid-1943, Adolf Hitler envisioned the Me 262 as a ground-attack/bomber aircraft rather than a defensive interceptor. The configuration of a high-speed, light-payload "Schnellbomber" ("fast bomber") was intended to penetrate enemy airspace during the expected Allied invasion of France. His edict resulted in the development of (and concentration on) the "Sturmvogel" variant. It is debatable to what extent Hitler's interference extended the delay in bringing the "Schwalbe" into operation; it appears engine vibration issues were at least as costly, if not more so. Albert Speer, then Minister of Armaments and War Production, claimed in his memoirs that Hitler originally had blocked mass production of the Me 262 before agreeing in early 1944. He rejected arguments that the aircraft would be more effective as a fighter against the Allied bombers that were destroying large parts of Germany, and wanted it as a bomber for revenge attacks. According to Speer, Hitler felt its superior speed compared to other fighters of the era meant it could not be attacked, and so preferred it for high altitude straight flying.
Although the Me 262 is often referred to as a "swept wing" design, the production Me 262 had a leading edge sweep of only 18.5°, too slight to achieve any significant advantage in increasing the critical Mach number. Sweep was added after the initial design of the aircraft, when the engines proved heavier than originally expected, primarily to position the center of lift properly relative to the center of mass. (The original 35° sweep, proposed by Adolph Busemann, was not adopted.) On 1 March 1940, instead of moving the wing backward on its mount, the outer wing was repositioned slightly aft; the trailing edge of the midsection of the wing remained unswept. Based on data from the AVA Göttingen and wind tunnel results, the middle section's leading edge was later swept to the same angle as the outer panels, from the "V6" sixth prototype onwards throughout volume production.
Test flights.
The first test flights began on 18 April 1941, with the Me 262 V1 example, bearing its "Stammkennzeichen" radio code letters of PC+UA, but since its intended BMW 003 turbojets were not ready for fitting, a conventional Junkers Jumo 210 engine was mounted in the V1 prototype's nose, driving a propeller, to test the Me 262 V1 airframe. When the BMW 003 engines were installed, the Jumo was retained for safety, which proved wise as both 003s failed during the first flight and the pilot had to land using the nose-mounted engine alone. The V1 through V4 prototype airframes all possessed what would become an uncharacteristic feature for most later jet aircraft designs, a fully retracting conventional gear setup with a retracting tailwheel — indeed, the very first prospective German "jet fighter" airframe design ever flown, the Heinkel He 280, used a retractable tricycle landing gear from its beginnings, and flying on jet power alone as early as the end of March 1941.
The V3 third prototype airframe, with the code PC+UC, became a true jet when it flew on 18 July 1942 in Leipheim near Günzburg, Germany, piloted by Fritz Wendel. This was almost nine months ahead of the British Gloster Meteor's first flight on 5 March 1943. Its retracting conventional gear, a feature shared with the first four Me 262 V-series airframes, caused its jet exhaust to deflect off the runway, with the wing's turbulence negating the effects of the elevators, and the first takeoff attempt was cut short.
On the second attempt, Wendel solved the problem by tapping the aircraft's brakes at takeoff speed, lifting the horizontal tail out of the wing's turbulence. The aforementioned initial four prototypes (V1-V4) were built with the conventional gear configuration. Changing to a tricycle arrangement — a permanently fixed undercarriage on the fifth prototype (V5, code PC+UE), with the definitive fully retractable nosewheel gear on the V6 (with "Stammkennzeichen" code VI+AA, from a new code block) and subsequent aircraft corrected this problem.
Test flights continued over the next year, but engine problems continued to plague the project, the Jumo 004 being only marginally more reliable than the BMW 003. Airframe modifications were complete by 1942 but, hampered by the lack of engines, serial production did not begin until 1944, and deliveries were low, with 28 Me 262s in June, 59 in July, but only 20 in August. This delay in engine availability was in part due to the shortage of strategic materials, especially metals and alloys able to handle the extreme temperatures produced by the jet engine.
Even when the engines were completed, they had an expected operational lifetime of approximately 50 continuous flight hours; most 004s lasted just 12 hours, even with adequate maintenance. A pilot familiar with the Me 262 and its engines could expect approximately 20–25 hours of life from the 004s. Changing a 004 engine was intended to require three hours, but this typically took eight to nine due to poorly made parts and inadequate training of ground crews. With one engine out, the Me 262 still flew well, with speeds of , but pilots were warned never to fly slower than on one engine, as the asymmetrical thrust would cause serious handling problems.
Due to the high speed jet stream, turbojet engines develop much less thrust at low speed than propeller powered aircraft, resulting in poor low-speed acceleration. This was particularly noticeable in the Me 262, since early jet engines had relatively low power and responded slowly to throttle changes. The introduction of a primitive autothrottle late in the war helped only slightly. Conversely, the superior power of jet engines at higher speeds meant the Me 262 enjoyed a much greater rate of climb. Used tactically, this gave the jet fighter an even larger speed advantage in climb than in level flight at top speed.
Operationally, carrying of fuel in two tanks, one each fore and aft the cockpit, and a tank beneath, the Me 262 would have a total flight endurance of 60 to 90 minutes. Fuel was usually brown coal-derived J2, with the option of diesel oil or a mixture of oil and high octane B4 aviation petrol. Fuel consumption was double the rate of typical twin-engine fighter aircraft of the era, which led to the installation of a low-fuel warning indicator in the cockpit that notified pilots when remaining fuel fell below .
Unit cost for an Me 262 airframe, less engines, armament, and electronics, was "RM"87,400. To build one airframe took around 6,400 man-hours.
Operational history.
Introduction.
On 19 April 1944, "Erprobungskommando" 262 was formed at Lechfeld just south of Augsburg, as a test unit ("Jäger Erprobungskommando Thierfelder", commanded by "Hauptmann" Werner Thierfelder) to introduce the 262 into service and train a corps of pilots to fly it. On 26 July 1944, "Leutnant" Alfred Schreiber with the 262 A-1a W.Nr. 130 017 damaged a Mosquito reconnaissance aircraft of No. 540 Squadron RAF PR Squadron, which was allegedly lost in a crash upon landing at an air base in Italy. Other sources state the aircraft was damaged during evasive manoeuvres and escaped.
Major Walter Nowotny was assigned as commander after the death of Thierfelder in July 1944, and the unit redesignated "Kommando Nowotny". Essentially a trials and development unit, it holds the distinction of having mounted the world's first jet fighter operations. Trials continued slowly, with initial operational missions against the Allies in August 1944 allegedly downing 19 Allied aircraft for six Me 262s lost, although these claims have never been verified by cross-checking with USAAF records. The RAF Museum holds no intelligence reports of RAF aircraft engaging in combat with Me 262s in August, although there is a report of an unarmed encounter between an Me 262 and a Mosquito.
Despite orders to stay grounded, Nowotny chose to fly a mission against an enemy bomber formation flying some 30,000 feet above, on 8 November 1944. He claimed two P-51Ds destroyed before suffering engine failure at high altitude. Then, while diving and trying desperately to restart his engines, he was attacked by other Mustangs, and forced to bail out. Historians Morgan and Weal proposed Nowotny's victor was P-51D pilot Lt. Robert W. Stevens of the 364th Fighter Group. The exact circumstances surrounding the death of Walter Nowotny remain uncertain to this day. It is also possible he was hit by "friendly" flak. The "Kommando" was then withdrawn for further training and a revision of combat tactics to optimise the 262's strengths.
On 26 November 1944, a Me 262A-2a Sturmvogel of III/KG51 based at Hopsten/Rheine near Osnabruck was the first confirmed ground-to-air kill of a jet combat aircraft. The 262 was shot down by a 40/L60 40mm Bofors gun of B.11 Detachment of 2875 Squadron RAF Regiment at the RAF forward airfield of Helmond, near Eindhoven. Others were lost to ground fire on 17 and 18 December when the same airfield was attacked at intervals by a total of eighteen 262s and the guns of 2873 and 2875 Squadrons RAF Regiment damaged several, causing at least two of them to crash within a few miles of the airfield. In February 1945, Sergeant Pollards's B.6 gun detachment of 2809 Squadron RAF Regiment shot down another Me 262 over the airfield of Vokel. The final appearance of 262s over Vokel was in 1945, when yet another fell to 2809's guns.
By January 1945, "Jagdgeschwader" 7 (JG 7) had been formed as a pure jet fighter wing, although it was several weeks before it was operational. In the meantime, a bomber unit—I "Gruppe", "Kampfgeschwader" 54 (KG 54)—had re-equipped with the Me 262 A-2a fighter-bomber for use in a ground-attack role. However, the unit lost 12 jets in action in two weeks for minimal returns. "Jagdverband 44" (JV 44) was another Me 262 fighter unit, of "Staffel" (squadron) size given the low numbers of available personnel, formed in February 1945 by Lieutenant General Adolf Galland, who had recently been dismissed as Inspector of Fighters. Galland was able to draw into the unit many of the most experienced and decorated Luftwaffe fighter pilots from other units grounded by lack of fuel.
During March, Me 262 fighter units were able, for the first time, to mount large-scale attacks on Allied bomber formations. On 18 March 1945, 37 Me 262s of JG 7 intercepted a force of 1,221 bombers and 632 escorting fighters. They shot down 12 bombers and one fighter for the loss of three Me 262s. Although a 4:1 ratio was exactly what the Luftwaffe would have needed to make an impact on the war, the absolute scale of their success was minor, as it represented only one per cent of the attacking force. In 1943 and early 1944, regardless of the presence of the small numbers of Me 262s, the USAAF was able to keep up offensive operations at loss ratios of roughly 5%.
Several two-seat trainer variants of the Me 262, the Me 262 B-1a, had been adapted through the "Umrüst-Bausatz 1" factory refit package as night fighters, complete with on-board FuG 218 "Neptun" high-VHF band radar, using "Hirschgeweih" ("stag's antlers") antennae with a set of shorter dipole elements than the "Lichtenstein SN-2" had used, as the B-1a/U1 version. Serving with 10 "Staffel", "Nachtjagdgeschwader" 11, near Berlin, these few aircraft (alongside several single-seat examples) accounted for most of the 13 Mosquitoes lost over Berlin in the first three months of 1945. However, actual intercepts were generally or entirely made using "Wilde Sau" methods, rather than AI radar-controlled interception. As the two-seat trainer was largely unavailable, many pilots made their first jet flight in a single-seater without an instructor.
Despite its deficiencies, the Me 262 clearly signaled the beginning of the end of piston-engined aircraft as effective fighting machines. Once airborne, it could accelerate to speeds over , about faster than any Allied fighter operational in the European Theater of Operations.
The Me 262's top ace was probably "Hauptmann" Franz Schall with 17 kills, which included six four-engine bombers and 10 P-51 Mustang fighters, although night fighter ace "Oberleutnant" Kurt Welter claimed 25 Mosquitos and two four-engine bombers shot down by night and two further Mosquitos by day flying the Me 262. Most of Welter's claimed night kills were achieved in standard radar-less aircraft, even though Welter had tested a prototype Me 262 fitted with FuG 218 "Neptun" radar. Another candidate for top ace on the aircraft was "Oberstleutnant" Heinrich Bär, who claimed 16 enemy aircraft while flying the Me 262.
Anti-bomber tactics.
The Me 262 was so fast that German pilots needed new tactics to attack Allied bombers. In the head-on attack, the closing speed, of about , was too high for accurate shooting. Even from astern, the closing speed was too great to use the short-ranged 30 mm cannon to maximum effect. Therefore, a roller-coaster attack was devised. The 262s approached from astern and about than the bombers. From about , they went into a shallow dive that took them through the escort fighters with little risk of interception. When they were about and below the bombers, they pulled up sharply to reduce their excess speed. On levelling off, they were and overtaking the bombers at about , well placed to attack them.
Since the 30mm MK 108 cannon's short barrels and low muzzle velocity of rendered it inaccurate beyond , coupled with the jet's velocity, which required breaking off at to avoid colliding with the target, Me 262 pilots normally commenced firing at . Turret gunners of Allied bomber aircraft found that their manned electrically powered gun turrets had problems tracking the jets. Target acquisition was difficult because the jets closed into firing range quickly and remained in firing position only briefly, using their standard attack profile, which proved more effective.
Captain Eric Brown, Chief Naval Test Pilot and C.O. Captured Enemy Aircraft Flight Royal Aircraft Establishment, who tested the Me 262 noted: "This was a Blitzkrieg aircraft. You whack in at your bomber. It was never meant to be a dogfighter, it was meant to be a destroyer of bombers... The great problem with it was it did not have dive brakes. For example, if you want to fight and destroy a B-17, you come in on a dive. The 30mm cannon were not so accurate beyond 600 meters. So you normally came in at 600 yards and would open fire on your B-17. And your closing speed was still high and since you had to break away at 200 meters to avoid a collision, you only had two seconds firing time. Now, in two seconds, you can't sight. You can fire randomly and hope for the best. If you want to sight and fire, you need to double that time to four seconds. And with dive brakes, you could have done that."
Eventually, German pilots developed new combat tactics to counter Allied bombers' defenses. Me 262s, equipped with up to 24 unguided folding-fin R4M rockets — twelve in each of two underwing racks, outboard of the engine nacelle — approached from the side of a bomber formation, where their silhouettes were widest, and while still out of range of the bombers' machine guns, fired a salvo of rockets with strongly brisant Hexogen-filled warheads, exactly the same explosive in the shells fired by the Me 262A's quartet of MK 108 cannon. One or two of these rockets could down even the famously rugged B-17 Flying Fortress, from the "metal-shattering" brisant effect of the R4M rockets' explosive warheads, weighing only per projectile out of a total launch weight of .
Though this tactic was effective, it came too late to have a real effect on the war, and only small numbers of Me 262s were equipped with the rocket packs. Most of those so equipped were Me 262A-1as, members of "Jagdgeschwader" 7. This method of attacking bombers became the standard, and mass deployment of Ruhrstahl X-4 guided missiles was cancelled. Some nicknamed this tactic the Luftwaffe's "Wolf Pack", as the fighters often made runs in groups of two or three, fired their rockets, then returned to base. On 1 September 1944, USAAF General Carl Spaatz expressed the fear that if greater numbers of German jets appeared, they could inflict losses heavy enough to force cancellation of the Allied bombing offensive by daylight.
Counter-jet tactics.
The Me 262 was difficult for its opponents to counter because its high speed and rate of climb made it extremely hard to intercept. As with all other early jets, the Me 262's engines did not provide a lot of thrust at low air speeds (a key criterion for good turn performance at low speeds), and throttle response was slow. Another disadvantage all early jet engines shared was a relatively high risk of flameout if the pilot used the throttle too aggressively (as is common in a dogfight). Pilots were instructed to operate the throttle gently and avoid quick changes. German engineers introduced an automatic throttle regulator later in the war but it only partly alleviated the problem. On the plus side, thrust at high speed was much greater than on propeller-driven aircraft.
The plane had, by contemporary standards, quite a high wing loading (294.0 kg/m2, 60.2 lbs/ft2) and its turn radius at low speeds was therefore correspondingly wide. This, coupled with the low thrust at slow speeds and high chance of a flameout if the throttle was worked too aggressively, resulted in Me 262 pilots being told to avoid low speed dogfights with the Allied piston-engine fighters. The high speed of the Me 262 also presented problems when engaging enemy aircraft, the high-speed convergence allowing Me 262 pilots little time to line up their targets or acquire the appropriate amount of deflection. This problem faces any aircraft that approaches another from behind at much higher speed, as the slower aircraft in front can always pull a tighter turn, forcing the faster aircraft to overshoot. The Me 262 faced this problem frequently as its cruising speed alone was up to faster than that of any piston-engine fighter of the period.
Luftwaffe pilots eventually learned how to handle the Me 262's higher speed, and the Me 262 soon proved a formidable air superiority fighter, with pilots such as Franz Schall managing to shoot down 12 enemy fighters in the Me 262, 10 of them American P-51 Mustangs. Other notable Me 262 aces included Georg-Peter Eder, also with 12 enemy fighters to his credit (including nine P-51s), Erich Rudorffer also with 12 enemy fighters to his credit, Walther Dahl with 11 (including three Lavochkin La-7s and six P-51s) and Heinz-Helmut Baudach with six (including one Spitfire and two P-51s) amongst many others.
Pilots soon learned that the Me 262 was quite maneuverable, despite its high wing loading and lack of low-speed thrust, especially if attention was drawn to its effective maneuvering speeds. The controls were light and effective right up to the maximum permissible speed and perfectly harmonized. The inclusion of full span automatic leading-edge slats, something of a "tradition" on Messerschmitt fighters dating back to the original Bf 109's outer wing slots of a similar type, helped increase the overall lift produced by the wing by as much as 35% in tight turns or at low speeds, greatly improving the aircraft's turn performance as well as its landing and take off characteristics. As many pilots soon found out, the Me 262's clean design also meant that it, like all jets, held its speed in tight turns much better than conventional propeller-driven fighters, which was a great potential advantage in a dogfight as it meant better energy retention in maneuvers. Luftwaffe test pilot and flight instructor Hans Fey stated, "The 262 will turn much better at high than at slow speeds and, due to its clean design, will keep its speed in tight turns much longer than conventional type aircraft."
Too fast to catch for the escorting Allied fighters, the Me 262s were almost impossible to head off. As a result, Me 262 pilots were relatively safe from the Allied fighters, as long as they did not allow themselves to get drawn into low-speed turning contests and saved their maneuvering for higher speeds. Combating the Allied fighters could be effectively done the same way as the U.S. fighters fought the more nimble, but slower, Japanese fighters in the Pacific.
Allied pilots soon found that the only reliable way to destroy the jets, as with the even faster Me 163 "Komet" rocket fighters, was to attack them on the ground or during takeoff or landing. Luftwaffe airfields identified as jet bases were frequently bombed by medium bombers, and Allied fighters patrolled over the fields to attack jets trying to land. The Luftwaffe countered by installing extensive "flak" alleys of anti-aircraft guns along the approach lines to protect the Me 262s from the ground—and by providing top cover during the jets' takeoff and landing with the most advanced Luftwaffe single-engined fighters, the Focke-Wulf Fw 190D and (just becoming available in 1945) Focke-Wulf Ta 152H. Nevertheless, in March–April 1945, Allied fighter patrol patterns over Me 262 airfields resulted in numerous jet losses.
The British Hawker Tempest scored a number of kills against the new German jets, including the Messerschmitt Me 262. Hubert Lange, a Me 262 pilot, said: "the Messerschmitt Me 262's most dangerous opponent was the British Hawker Tempest — extremely fast at low altitudes, highly manoeuvrable and heavily armed." Some were destroyed with a tactic known to the Tempest 135 Wing as the "Rat Scramble": Tempests on immediate alert took off when an Me 262 was reported airborne. They did not intercept the jet, but instead flew towards the Me 262 and Ar 234 base at Rheine-Hopsten. The aim was to attack jets on their landing approach, when they were at their most vulnerable, travelling slowly, with flaps down and incapable of rapid acceleration. The German response was the construction of a "flak lane" of over 150 of the infamous "Flakvierling" 20 mm (.79 in) autocannon batteries at Rheine-Hopsten to protect the approaches. After seven Tempests were lost to flak at Rheine-Hopsten in a single week, the "Rat Scramble" was discontinued.
High-speed research.
Adolf Busemann had proposed swept wings as early as 1935. Messerschmitt researched the topic from 1940. In April 1941, Busemann proposed fitting a 35° swept wing ("Pfeilflügel II", literally "arrow wing II") to the Me 262, the same wing sweep angle later used on both the American F-86 Sabre and Soviet MiG-15 Fagot fighter jets. Though this was not implemented, he continued with the projected HG II and HG III ("Hochgeschwindigkeit", "high-speed") derivatives in 1944, which were designed with a 35° and 45° wing sweep, respectively.
Interest in high-speed flight, which led him to initiate work on swept wings starting in 1940, is evident from the advanced developments Messerschmitt had on his drawing board in 1944. While the Me 262 V9 "Hochgeschwindigkeit I" (HG I) actually flight tested in 1944 had only small changes compared to combat aircraft, most notably a low-profile canopy — tried as the "Rennkabine" (literally "racing cabin") on the ninth Me 262 prototype for a short time — to reduce drag, the HG II and HG III designs were far more radical. The projected HG II combined the low-drag canopy with a 35° wing sweep and a butterfly tail. The HG III had a conventional tail, but a 45° wing sweep and turbines embedded in the wing roots.
Messerschmitt also conducted a series of flight tests with the series production Me 262. In dive tests, they determined that the Me 262 went out of control in a dive at Mach 0.86, and that higher Mach numbers would cause a nose-down trim that the pilot could not counter. The resulting steepening of the dive would lead to even higher speeds and the airframe would disintegrate from excessive negative g loads.
The HG series of Me 262 derivatives was believed capable of reaching transonic Mach numbers in level flight, with the top speed of the HG III being projected as Mach 0.96 at altitude. Despite the necessity to gain experience in high-speed flight for the HG II and III designs, Messerschmitt made no attempt to exceed the Mach 0.86 limit for the Me 262. After the war, the Royal Aircraft Establishment, at that time one of the leading institutions in high-speed research, re-tested the Me 262 to help with British attempts at exceeding Mach 1. The RAE achieved speeds of up to Mach 0.84 and confirmed the results from the Messerschmitt dive tests. The Soviets ran similar tests.
After Willy Messerschmitt's death in 1978, the former Me 262 pilot Hans Guido Mutke claimed to have exceeded Mach 1, on 9 April 1945 in a Me 262 in a "straight-down" 90° dive. This claim is disputed because it is only based on Mutke's memory of the incident, which recalls effects other Me 262 pilots observed below the speed of sound at high indicated airspeed, but with no altitude reading required to determine the actual speed. Furthermore, the pitot tube used to measure airspeed in aircraft can give falsely elevated readings as the pressure builds up inside the tube at high speeds. Finally, the Me 262 wing had only a slight sweep, incorporated for trim (center of gravity) reasons and likely would have suffered structural failure due to divergence at high transonic speeds. One airframe — the aforementioned Me 262 V9, Werknummer 130 004, with "Stammkennzeichen" of VI+AD, was prepared as the HG I test airframe with the low-profile "Rennkabine" racing canopy and may have achieved an unofficial record speed for a turbojet-powered aircraft of , altitude unspecified, even with the recorded wartime airspeed record being set on 6 July 1944, by another Messerschmitt design — the Me 163B V18 rocket fighter setting a record, but landing with a nearly disintegrated rudder surface.
Production.
About 1,400 Me 262s were produced, but a maximum of 200 were operational at the same time. According to sources they destroyed from 300 to 450 enemy planes, with the Allies destroying about 100 Me 262s in the air. While Germany was bombed intensively, production of the Me 262 was dispersed into low-profile production facilities, sometimes little more than clearings in the forests of Germany and occupied countries. Through the end of February to the end of March 1945, approximately 60 Me 262s were destroyed in attacks on Obertraubling and 30 at Leipheim; the Neuburg jet plant itself was bombed on 19 March 1945.
Large, heavily protected underground factories were constructed to take up production of the Me 262, safe from bomb attacks, but the war ended before they could be completed. Wings were produced in Germany's oldest motorway tunnel at Engelberg to the west of Stuttgart. At "B8 Bergkristall-Esche II" at St. Georgen/Gusen, Austria, forced laborers of "Concentration Camp Gusen II" produced fully equipped fuselages for the Me 262 at a monthly rate of 450 units on large assembly lines from early 1945.
Postwar history.
After the end of the war, the Me 262 and other advanced German technologies were quickly swept up by the Soviets and Americans (as part of the USAAF's Operation Lusty). Many Me 262s were found in readily repairable condition and were confiscated. Both the Soviets and Americans wished to evaluate the technology, particularly the engines.
During testing, the Me 262 was faster and had better visibility to the sides and rear (mostly due to the canopy frames and the discoloration caused by the plastics used in the Meteor's construction), and was a superior gun platform to the Gloster Meteor F.1 which had a tendency to snake at high speed and exhibited "weak" aileron response. The Me 262 had a shorter range than the Meteor and had less reliable engines.
The USAAF compared the P-80 Shooting Star and Me 262 concluding, "Despite a difference in gross weight of nearly , the Me 262 was superior to the P-80 in acceleration, speed and approximately the same in climb performance. The Me 262 apparently has a higher critical Mach number, from a drag standpoint, than any current Army Air Force fighter."
The Army Air Force also tested a Me 262A-1a/U3 (US flight evaluation serial FE-4012), an unarmed photo reconnaissance version, which was fitted with a fighter nose and given an overall smooth finish. It was used for performance comparisons against the P-80. During testing between May and August 1946, the aircraft completed eight flights, lasting four hours and 40 minutes. Testing was discontinued after four engine changes were required during the course of the tests, culminating in two single-engine landings. These aircraft were extensively studied, aiding development of early U.S. and Soviet jet fighters. The F-86, designed by engineer Edgar Schmued, used a slat design based on the Me 262's.
The Czechoslovak aircraft industry continued to produce single-seat (Avia S-92) and two-seat (Avia CS-92) variants of the Me 262 after World War II. From August 1946, a total of nine S-92s and three two-seater CS-92s were completed and test flown. They were introduced in 1947 and in 1950 were supplied to the 5th Fighter Squadron, becoming the first jet fighters to serve in the Czechoslovak Air Force. These were kept flying until 1951, when they were replaced in service by Soviet jet fighters. Both versions are on display at the Prague Aviation museum in Kbely.
Flyable reproductions.
In January 2003, the American Me 262 Project, based in Everett, Washington, completed flight testing to allow the delivery of near-exact reproductions of several versions of the Me 262 including at least two B-1c two-seater variants, one A-1c single seater and two "convertibles" that could be switched between the A-1c and B-1c configurations. All are powered by General Electric CJ610 (civil J85) engines and feature additional safety features, such as upgraded brakes and strengthened landing gear. The "c" suffix refers to the new CJ610 powerplant and has been informally assigned with the approval of the Messerschmitt Foundation in Germany (the Werk Number of the reproductions picked up where the last wartime produced Me 262 left off – a continuous airframe serial number run with a 50-year production break).
Flight testing of the first newly manufactured Me 262 A-1c (single-seat) variant (Werk Number 501244) was completed in August 2005. The first of these machines (Werk Number 501241) went to a private owner in the southwestern United States, while the second (Werk Number 501244) was delivered to the Messerschmitt Foundation at Manching, Germany. This aircraft conducted a private test flight in late April 2006, and made its public debut in May at the ILA 2006. The new Me 262 flew during the public flight demonstrations. Me 262 Werk Number 501241 was delivered to the Collings Foundation as White 1 of JG 7; this aircraft offered ride-along flights starting in 2008. The third replica, a non-flyable Me 262 A-1c, was delivered to the Evergreen Aviation & Space Museum in May 2010.
Variants.
"Note:"- U = " Umrüst-Bausatz" - conversion kit installed at factory level, denoted as a suffix in the form /U"n".
Rüstsätze (field modification kits).
Rüstsatze may be applied to various sub-types of their respective aircraft type, denoted as a suffix in the form /R"n".
"Data from:"'Messerschmitt Me 262A Schwalbe
Reproductions.
These reproductions are constructed by Legend Flyers (later Me 262 Project) of Everett, Washington. The Jumo 004 engines of the original are replaced by more reliable General Electric CJ610 engines. The Jumo 004 was hampered by poor workmanship and a lack of high-temperature super alloys. The first Me 262 reproduction (a two-seater) took off for the first time in December 2002 and the second one in August 2005. This one was delivered to the Messerschmitt Foundation and was presented at the ILA airshow in 2006.

</doc>
<doc id="20493" url="https://en.wikipedia.org/wiki?curid=20493" title="Masuria">
Masuria

Masuria (, ) is a region in northern Poland famous for its 2,000 lakes. It has been the part of East Prussia inhabited by Polish-speaking, Lutheran Masurians. Masuria occupies much of the Masurian Lake District (). Administratively, it belongs to Warmian-Masurian Voivodeship (). Its biggest city is Ełk, often regarded as its capital. It has territory of about 10,000 km2 and population of 500,000.
History.
Old Prussians.
Before 13th century, the territory was inhabited by the Old Prussians also called Baltic Prussians, a Baltic ethnic group that lived in Prussia (the lands of the southeastern coastal region of the Baltic Sea around the Vistula Lagoon and the Curonian Lagoon). The territory of later Masuria used to be called that time Galindia and was probably peripheral, very much forested and little populated area. Its inhabitants spoke a language now known as Old Prussian and followed pagan Prussian mythology. Although a 19th-century German political entity bore their name, they were not Germans. They were converted to Roman Catholicism in the 13th century, after conquest by the Knights of the Teutonic Order.
Estimates range from about 170,000 to 220,000 Old Prussians living in the whole Prussia around 1200. The wilderness was their natural barrier against the attacks by would-be invaders. During the Northern Crusades of the early 13th century, the Old Prussians used this wide forest as a line of defence. They did it again against the Knights of the Teutonic Order invited to Poland by Konrad Mazowiecki in 1226. The order's goal was to convert the native population to Christianity and baptise it by force if necessary. In the subsequent conquest which lasted over 50 years, the original population was nearly exterminated especially during the major Prussian rebellion of 1261–83.
Teutonic Order.
After the Order's acquisition of Prussia, Poles (or more specifically, Mazurs, that is inhabitants of the adjacent region of Mazovia) began to settle in the southeastern part of the conquered region. German, Dutch, Flemish, and Danish colonists entered the area afterward, from the northwest. The number of Polish settlers grew significantly again in the beginning of 15th century, especially after the first and the second treaties of Thorn, in 1411 and 1466 respectively, following the Thirteen Years' War and the final defeat of the order. Later assimilation of the German settlers as well as the Polish immigrants and native inhabitants created the new Prussian identity, although the subregional difference between the German- and Slavic-speaking part remained. Western half of the province was ceded to Poland, and the grand master (still ruling, among others, also Masuria) became a vassal of the Polish crown.
Ducal Prussia.
The secularization of the Teutonic Order in Prussia and the conversion of Albert of Prussia to Lutheranism in 1525 brought Prussia including the later called Masuria area to Protestantism. The Polish language predominated due to the many immigrants from Mazovia, who additionally settled the eastern, till then virgin part of (later Masuria) in the 16th century. While the countryside was inhabited by Protestant Polish-speakers, who took refuge, the cities constituted German mixed with Polish-speaking population. The ancient Old Prussian language survived in parts of the countryside until the early 18th century. Areas that had many Polish language speakers were known as the Polish Departments.
In 1656, during the Battle of Prostki, the forces of Polish-Lithuanian Commonwealth, including 2,000 Tatar raiders, beat the allied Swedish and Brandenburg army capturing Bogusław Radziwiłł. The war resulted in the destruction of most towns, 249 villages and settlements and 37 churches were destroyed. Over 50% of the population of southern Prussia (later Masuria) died within the years 1656–1657, 23,000 killed, another 80,000 died of diseases and famine, 3,400 people were enslaved.^ Jacek Płosiński, "Potop szwedzki na Podlasiu 1655-1657", Inforteditions Publishing, 2006. ISBN 83-89943-07-7</ref> From 1709–1711, in all of East Prussia between 200,000 and 245,000 out of 600,000 inhabitants died from the Black Death. in Masuria the death toll varied regionally, while 6,789 people died in the district of Rhein (Rhyn) only 677 died in Seehesten (Szestno). In Lötzen (Giżycko) 800 out of 919 people died. Losses in population were compensated by migration of Protestant settlers or refugees from Scotland, Salzburg (expulsion of Protestants 1731), France (Huguenot refugees after the Edict of Fontainebleau in 1685), and especially from the Polish-Lithuanian Commonwealth, including Polish brethren expelled from Poland in 1657. The last group of refugees to emigrate to Masuria were the Russian Philipons in 1830, when King Frederick William III of Prussia granted them asylum.
Kingdom of Prussia.
After the death of Albert Frederick, Duke of Prussia in 1618, his son-in-law John Sigismund, Margrave of Brandenburg, inherited the duchy (including Masuria), combining the two territories under a single dynasty and forming Brandenburg-Prussia. The Treaty of Wehlau revoked the sovereignty of the King of Poland in 1657. The region became part of the Kingdom of Prussia with the coronation of King Frederick I of Prussia in 1701. Masuria became part of the newly created administrative province of East Prussia upon its creation in 1773. The name "Masuria" began to be used officially after new administrative reforms in the Kingdom after 1818. Masurians referred to themselves during that period as "Polish Prussians" or as "Staroprusaki" (Old Prussians)
Masurians showed considerable support for the Polish uprising in 1831, and maintained many contacts with Russian-held areas of Poland beyond the border of Prussia, the areas being connected by common culture and language; before the uprising people visited each other's country fairs and much trade took place, with smuggling also widespread Some early writers about Masurians - like Max Toeppen - postulated them as mediators between German and Slav cultures.
Germanisation policies in Masuria included various strategies, first and foremost they included attempts to propagate the German language and to eradicate the Polish language as much as possible; German became the obligatory language in schools in 1834.
German Empire.
After the Unification of Germany into the German Empire in 1871, the Polish language was removed from schools in 1872, as part of Otto von Bismarck's Culture War. He also sought to eradicate the use of the Polish language and culture in the German Empire. After 1871 Masurians who expressed sympathy for Poland were deemed "national traitors" by German nationalists (this increased especially after 1918) According to Stefan Berger after 1871 the Masurians in the German Empire were seen in a view that while acknowledging their "objective" Polishness (in terms of culture and language) they felt "subjectively" German and thus should be tightly integrated into German nation-state; to Berger this argument went directly against the German nationalist demands in Alsace where Alsatians were declared German despite their "subjective" choice. Berger concludes that such the arguments of German nationalists were simply aimed at gaining as much territory as possible into German Reich.
During the period of German Empire the Germanisation policies in Masuria became more effective; children using Polish in playgrounds and classrooms were widely punished by corporal punishment, authorities tried to appoint Protestant pastors who would use German instead of Polish which resulted in protests of local population. According to Jerzy Mazurek the native Polish-speaking population, like in other areas with Polish inhabitants, faced severe discrimination from Germanised local administration, in this climate first resistance defending the rights of rural population was formed; according to Jerzy Mazurek usually teachers engaged in publishing Polish language newspapers.
Despite anti-Polish policies, such Polish language newspapers as the "Pruski Przyjaciel Ludu" (Prussian Friend of People) or the "Kalendarz Królewsko-Pruski Ewangelicki" (Royal Prussian Evangelical Calendar) or bilingual journals like the "Oletzkoer Kreisblatt - Tygodnik Obwodu Oleckiego" continued to be published in Masuria. In contrast to the Prussian-oriented periodicals, in the late 19th century such newspapers as "Przyjaciel Ludu Łecki" and "Mazur" were founded by members of the Warsaw-based "Komitet Centralny dla Śląska, Kaszub i Mazur" (Central Committee for Silesia, Kashubia and Masuria), influenced by Polish politicians like Antoni Osuchowski or Juliusz Bursche, to strengthen the Polish identity in Masuria. The "Gazeta Ludowa" was published in Lyck in 1896–1902, with 2,500 copies in 1897 and the "Mazur" in Ortelsburg after 1906 with 500 copies in 1908 and 2,000 prior to World War I.
Polish activists started to regard Masurians as "Polish brothers" after Wojciech Kętrzyński had published his pamphlet "O Mazurach" in 1872 and Polish activists engaged in active self-help against repressions by German state Kętrzyński fought against attempts to Germanise Masuria
The attempts to create a Masurian Polish national consciousness, largely originating from nationalist circles of Provinz Posen, however faced the resistance of the Masurians, who, despite having similar folk traditions and linguistics to Poles, regarded themselves Prussians and later Germans. and were loyal to the Hohenzollern dynasty, the Prussian and German state. After World War I the editor of the Polish language "Mazur" described the Masurians as "not nationally conscious, on the contrary, the most loyal subjects of the Prussian king". However, a minority of Masurians did exist which expressed Polish identity
After 1871 there appeared resistance among the Masurians towards Germanisation efforts, the so-called Gromadki movement was formed which supported use of Polish language and came into conflict with German authorities; while most of its members viewed themselves as loyal to Prussian state, a part of them joined the Pro-Polish faction of Masurians. German actions like Kulturkampf, the programme of Germanisation started to unite and mobilise Polish people in Polish inhabited territories held by Germany including Masuria A Polish-oriented party, the "Mazurska Partia Ludowa" ("People's Party of Masuria"), was founded in 1897. The eastern area of German Empire was systematically Germanised with changing of names and public signs, and the German state fostered cultural imperialism, in addition to giving financial and other support to German farmers, officials and teachers to settle in the east.
The German authorities in their efforts of Germanisation tried somewhat artificially to separate Masurian language from Polish by classifying it as non-Slavic language different from Polish one, this was reflected in official census
Thus the Masurian population in 1890, 143,397 were reported by German officials as having German as their language (either primary or secondary), 152,186 Polish and 94,961 Masurian. In 1910, the German language was reported by German authorities as used by 197,060, Polish by 30,121 and Masurian by 171,413. In 1925, German authorities reported 40,869 as giving Masurian as their native tongue and 2,297  as Polish. However, the last result may have been a result of politics at the time and a desire to present the province as purely German; in reality the Masurian dialect was still in use. 
Throughout industrialisation in the late 19th century about 10 percent of the Masurian populace emigrated to the Ruhr Area, where about 180,000 Masurians lived in 1914. Wattenscheid, Wanne and Gelsenkirchen were the centres of Masurian emigration and Gelsenkirchen-Schalke was even called Klein (little)-Ortelsburg before 1914. Masurian newspapers like the "Przyjaciel Ewangeliczny" and the "Gazeta Polska dla Ludu staropruskiego w Westfalii i na Mazurach" but also the German language "Altpreußische Zeitung" were published.
During World War I, the Battle of Tannenberg and the First and Second Battle of the Masurian Lakes between Imperial Germany and the Russian Empire took place within the borders of Masuria in 1914. After the war, the League of Nations held the East Prussian plebiscite on 11 July 1920 to determine if the people of the southern districts of East Prussia wanted to remain within East Prussia or to join the Second Polish Republic. The German side terrorised the local population before the plebiscite using violence, Polish organisations and activists were harassed by German militias, and those actions included attacks and murder of Polish activists; Masurs who supported voting for Poland were singled out and subjected to terror and repressions
Names of those Masurs supporting the Polish side were published in German newspapers, and their photos presented in German shops; afterwards a regular hunts were organised after them by German militias which terrorized Polish population At least 3,000 Warmian and Masurian activists who were engaged for Polish side had to flee the region out of fear of their lives At the same time German police engaged in active surveillance of the Polish minority and attacks against Polish activists Before the plebiscite Poles started to flee the region to escape the German harassment and terror
The results determined that 99.32% of the voters in Masuria proper chose to remain with East Prussia. However, the contemporary Polish ethnographer Adam Chętnik accused the German authorities of abuses and falsifications during the plebiscite. Moreover, the plebiscite took place during the time when Polish–Soviet War threatened to erase the Polish state. As a result, even many Poles of the region voted for Germany out of fear that if the area was allocated to Poland it would fall under Soviet rule. After the plebiscite in German areas of Masuria attacks on Polish population commenced by German mobs, and Polish priests and politicians were driven from their homes After the plebiscite at least 10,000 Poles had to flee German held Masuria to Poland
Polish Masuria — the Działdowo county.
The region of Działdowo (Soldau), where according to the official German census of 1910 ethnic Germans formed a minority of 37.3%, was excluded from the plebiscite and became part of Poland. This was reasoned with placing the railway connection between Warsaw and Danzig (Gdańsk), of vital importance to Poland as it connected central Poland with its seacoast, completely under Polish sovereignty. Działdowo itself counted about 24,000 people of which 18,000 were Masurians
According to the municipal administration of Rybno, after World War I Poles in Działdowo believed that they will be quickly joined with Poland, they organised secret gatherings during which the issue of rejoining Polish state with help of Polish military was discussed. According to the Rybno administration most active Poles in that subregion included Jóżwiakowscy, Wojnowscy, Grzeszczowscy families working under the guidance of politician Leon Wojnowski who protested German attempts to remain Działdowo a part of Germany after the war; other local pro-Polish activists were Alfred Wellenger, Paczyński, Tadeusz Bogdański, Jóźwiakowski.
The historian Andreas Kossert describes that the incorporation happened despite protests of the local populace, the municipal authorities and the German Government, According to Kossert 6,000 inhabitants of the region soon left the area.
In 1920 the candidate of the German Party, Ernst Barczewski, was elected to the Sejm with 74.6 percent of votes and to the Polish Senate with 34.6% of votes for the Bloc of National Minorities in 1928. During the Polish–Soviet War Działdowo was briefly occupied by the Red Army which was cheered as liberators by the local German populace, which hoisted the German flag, but it was soon recovered by the Polish Army.
During the interwar period many native inhabitants of Działdowo subregion left it and migrated to Germany.
With the start of the German war against Poland on 1 September 1939, the German minority in the parts of Masuria attached to Poland after World War I, organised in paramilitary formation called Selbstschutz begun to engage in massacres of local Polish population; Poles were imprisoned, tortured and murdered while Masurians were sometimes forcefully placed on Volksliste
The Soldau concentration camp was established in winter 1939, where 13,000 people were murdered by the Nazi German state during the war. Notable victims included the Polish bishops Antoni Julian Nowowiejski and Leon Wetmański, as well as the nun Mieczysława Kowalska. Additionally, almost 1,900 mentally ill patients from East Prussia and annexed areas of Poland were murdered there as well, in what was known as Action T4.
Polish resistance in Masuria was organised by Paweł Nowakowski "Leśnik" commander of the Home Army's Działdowo district
Weimar Republic and Nazi Germany.
Masuria was the only region of Germany directly affected by the battles of World War I. Damaged towns and villages were reconstructed with the aid of several twin towns from western Germany like Cologne to Neidenburg, Frankfurt to Lötzen and even Vienna to Ortelsburg. However Masuria was still largely agrarian-oriented and suffered from the economic decline after World War I, additionally badly affected by the creation of the Polish Corridor, which raised freight costs to the traditional markets in Germany.
The later implemented Osthilfe had only a minor influence on Masuria as it privileged larger estates, while Masurian farms were generally small.
The interwar period was characterised by ongoing Germanisation policies, intensified especially under the Nazis
In the 1920s Masuria remained a heartland of conservatism with the German National People's Party as strongest party. The Nazi Party became the strongest party in the Masurian constituencies in the elections of 1930 and received its best results in the poorest areas of Masuria with the highest rate of Polish speakers. Especially in the elections of 1932 and 1933 they reached up to 81 percent of votes in the district of Neidenburg and 80 percent in the district of Lyck. The Nazis used the economic crisis, which had significant effects in far-off Masuria, as well as traditional anti-Polish sentiments while at the same time Nazi political rallies were organized in the Masurian dialect during the campaigning.
In 1938, the Nazi government (1933–1945) changed thousands of toponyms (especially names of cities and villages) of Old Prussian and Polish origin to newly created German names; about 50% of the existing names were changed in 1938 alone, despite resistance by the Prussian people, who continued to use their traditional place names.
According to German author Andreas Kossert, Polish parties were financed and aided by the Polish government in Warsaw, and remained splintergroups without any political influence, e.g. in the 1932 elections the Polish Party received 147 votes in Masuria proper. According to Wojciech Wrzesiński (1963), the Polish organisations in Masuria had decided to lower their activity in order to escape acts of terror performed against Polish minority activists and organisations by Nazi activists. Jerzy Lanc, a teacher and Polish national who had moved to Masuria in 1931 to establish a Polish school in Piassutten (Piasutno), died in his home of carbon monoxide poisoning, most likely murdered by local German nationalists.
Before the war the Nazi German state sent undercover operatives to spy on Polish organisations and created lists of people that were to be executed and sent to concentration camps. Information was gathered on who sent children to Polish schools, bought Polish press or took part in Polish ceremonies and organised repressions against these people were executed by Nazi militias. Polish schools, printing presses and headquarters of Polish institutions were attacked as well as homes of the most active Poles; shops owned by Poles were demolished. Polish masses were dispersed, and Polish teachers were intimidated as members of the SS gathered under their locals performing songs like "Wenn das Polenblut vom Messer spritzt, dann geht’s noch mal so gut"("When Polish blood spurts from the knife, everything will be better").
The anti-Polish activities intensified in 1939. Those Poles were most active in politics were evicted from their own homes, while Polish newspapers and cultural houses were closed down in the region. Polish masses were banned between June and July in Warmia and Mazury.
In the final moments of August 1939 all remains of political and cultural life of Polish minority was eradicated by the Nazis, with imprisonment of Polish activists and liquidation of Polish institutions. Seweryn Pieniężny, the chief editor of "Gazeta Olsztyńska", who opposed Germanisation of Masuria, was interned. Others included Juliusz Malewski (director of Bank Ludowy of Olsztyn), Stefan Różycki, Leon Włodarczyk (activist of Polonia Warmińsko-Mazurska).
Directors of Polish schools and teachers were imprisoned, as was the staff of Polish pre-schools in the Masuria region. They were often forced to destroy Polish signs, emblems and symbols of Polish institutions.
World War II.
The Nazis believed that in future, the Masurs, as a separate non-German entity, would disappear, while those who would cling to their "foreigness" as one Nazi report mentioned, would be deported. Poles and Jews were considered by the Nazis to be "untermenschen", subject to slavery and extermination, and Nazi authorities murdered Polish activists in Masuria, those who were not killed were arrested and sent to concentration camps,
In August 1943 the Uderzeniowe Bataliony Kadrowe attacked the village of Mittenheide (Turośl) in southern Masuria
In 1943 "Związek Mazurski" was reactivated secretly by Masurian activists of the Polish Underground State in Warsaw and led by Karol Małłek. Związek Mazurski opposed Nazi Germany and asked Polish authorities during the war to liquidate German property after victory over Nazi Germany to help in agricultural reform and settlement of Masurian population, Masurians opposed to Nazi Germany requested to remove German heritage sites "regardless of their cultural value". Additionally a Masurian Institute was founded by Masurian activists in Radość near Warsaw in 1943
In the final stages of World War II, Masuria was partially devastated by the retreating German and advancing Soviet armies during the Vistula-Oder Offensive. The region came under Polish rule at the war's end in the Potsdam Conference. Most of the population fled to Germany or was killed during or after the war, while the rest was subject to a "nationality verification", organised by the communist government of Poland. As a result, the number of native Masurians remaining in Masuria was initially relatively high, while most of the population was subsequently expelled. Poles from central Poland and the Polish areas annexed by the Soviet Union as well as Ukrainians expelled from southern Poland throughout the Operation Vistula, were resettled in Masuria.
Masuria after World War II.
According to the Masurian Institute the Masurian members of resistance against Nazi Germany who survived the war, became active in 1945 in the region, working in Olsztyn in cooperation with new state authorities in administration, education and cultural affairs
German author Andreas Kossert describes the post-war process of "national verification" as based on an ethnic racism which categorised the local populace according to their alleged ethnic background. A Polish-sounding last name or a Polish-speaking ancestor was sufficient to be regarded as "autochthonous" Polish.
In October 1946 37,736 persons were "verified" as Polish citizens while 30,804 remained "unverified". A centre of such "unverified" Masurians was the district of Mragowo (Sensburg), where in early 1946 out of 28,280 persons, 20,580 were "unverified", while in October, 16,385 still refused to adopt Polish citizenship. However even those who complied with the often used pressure by Polish authorities were in fact treated as Germans because of their Lutheran faith and their often rudimentary knowledge of Polish. Names were "Polonised" and the usage of the German language in public was forbidden. In the late 1940s the pressure to sign the "verification documents" grew and in February 1949 the former chief of the stalinist secret Police (UB) of Łódź, Mieczyslaw Moczar, started the "Great verification" campaign. Many unverified Masurians were imprisoned and accused of pro-Nazi or pro-American propaganda, even former pro-Polish activists and inmates of Nazi concentration camps were jailed and tortured. After the end of this campaign in the district of Mragowo (Sensburg) only 166 Masurians were still "unverified".
In 1950 1,600 Masurians left the country and in 1951, 35,000 people from Masuria and Warmia managed to obtain a declaration of their German nationality by the embassies of the USA and Great Britain in Warsaw. Sixty-three percent of the Masurians in the district of Mragowo (Sensburg) received such a document. In December 1956 Masurian pro-Polish activists signed a memorandum to the Communist Party leadership:
"The history of the people of Warmia and Masuria is full of tragedy and suffering. Injustice, hardship and pain often pressed on the shoulders of Warmians and Masurians... Dislike, injustice and violence surrounds us...They (Warmians and Masurians) demand respect for their differentness, grown in the course of seven centuries and for freedom to maintain their traditions".
Soon after the political reforms of 1956, Masurians were given the opportunity to join their families in West Germany. The majority (over 100 thousand) gradually left and after the improvement of Germano-Polish relations by the German Ostpolitik of the 1970s, 55,227 persons from Warmia and Masuria moved to West Germany in between 1971 and 1988, today approximately between 5,000 and 6,000 Masurians still live in the area, about 50 percent of them members of the German minority in Poland, the remaining half is ethnic Polish. As the Polish journalist Andrzej K. Wróblewski stated, the Polish post-war policy succeeded in what the Prussian state never managed: the creation of a German national consciousness among the Masurians.
Most of the originally Protestant churches in Masuria are now used by the Polish Roman Catholic Church as the number of Lutherans in Masuria declined from 68,500 in 1950 to 21,174 in 1961 and further to 3,536 in 1981. Sometimes, like on 23 September 1979 in the village of Spychowo (Puppen), the Lutheran Parish was even forcefully driven out of their church while liturgy was held.
Modern Masuria.
In modern Masuria the native population has virtually disappeared. Masuria was incorporated into the voivodeship system of administration in 1945. In 1999 Masuria was constituted with neighbouring Warmia as a single administrative province through the creation of the Warmian-Masurian Voivodeship.
The Masurian Szczytno-Szymany International Airport gained international attention as press reports alleged the airport to be a so-called ""black site"" involved in the CIA's network of extraordinary renditions.
Economy.
Important branch of economy is agrotourism. The lakes for which the region is best known offer varieties of water sports, recreation and vacation activities. As a result of collapse of PGRs after 1989, the unemployment rate is very high and currently stands at about 20 percent.
Landscape.
Masuria and the Masurian Lake District are known in Polish as "Kraina Tysiąca Jezior" and in German as "Land der Tausend Seen", meaning "land of a thousand lakes." These lakes were ground out of the land by glaciers during the Pleistocene ice age around 14,000 - 15,000 years ago, when ice covered northeastern Europe. From that period originates the horn of a reindeer found in the vicinity of Giżycko. By 10,000 BC this ice started to melt. Great geological changes took place and even in the last 500 years the maps showing the lagoons and peninsulas on the Baltic Sea have greatly altered in appearance. More than in other parts of northern Poland, such as from Pomerania (from the River Oder to the River Vistula), this continuous stretch of lakes is popular among tourists. The terrain is rather hilly, with connecting lakes, rivers and streams. Forests account for about 30% of the area. The northern part of Masuria is covered mostly by the broadleaved forest, while the southern part is dominated by pine and mixed forests.

</doc>
