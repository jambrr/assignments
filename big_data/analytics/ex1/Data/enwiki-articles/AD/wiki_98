<doc id="24355" url="https://en.wikipedia.org/wiki?curid=24355" title="Perth">
Perth

Perth () is the capital and largest city of the Australian state of Western Australia. It is the fourth-most populous city in Australia, with an estimated population of 2.02 million () living in Greater Perth. Perth is part of the South West Land Division of Western Australia, with the majority of the metropolitan area of Perth located on the Swan Coastal Plain, a narrow strip between the Indian Ocean and the Darling Scarp, a low coastal escarpment. The first areas settled were on the Swan River, with the city's central business district and port (Fremantle) both located on its shores. The Perth Metropolitan Region includes 30 local government areas, which themselves consist of a large number of suburbs, extending from Two Rocks in the north to Rockingham in the south, and east inland to The Lakes.
Perth was originally founded by Captain James Stirling in 1829 as the administrative centre of the Swan River Colony. It gained city status (currently vested in the smaller City of Perth) in 1856, and was promoted to the status of a Lord Mayorality in 1929. The city is named after Perth, Scotland, due to the influence of Sir George Murray, Member of Parliament for Perthshire and Secretary of State for War and the Colonies. The city's population increased substantially as a result of the Western Australian gold rushes in the late 19th century, largely as a result of emigration from the eastern colonies of Australia. During Australia's involvement in World War II, Fremantle served as a base for submarines operating in the Pacific Theatre, and a US Navy Catalina flying boat fleet was based at Matilda Bay. An influx of immigrants after the war, predominantly from Britain, Greece, Italy and Yugoslavia, led to rapid population growth. This was followed by a surge in economic activity flowing from several mining booms in the late 20th and early 21st centuries that saw Perth become the regional headquarters for a number of large mining operations located around the state.
As part of Perth's role as the capital of Western Australia, the state's Parliament and Supreme Court are located within the city, as is Government House, the residence of the Governor of Western Australia. Perth became known worldwide as the "City of Light" when city residents lit their house lights and streetlights as American astronaut John Glenn passed overhead while orbiting the earth on Friendship 7 in 1962. The city repeated the act as Glenn passed overhead on the Space Shuttle in 1998. Perth came 8th in the Economist Intelligence Unit's August 2015 list of the world's most liveable cities, and was classified by the Globalization and World Cities Research Network in 2010 as a world city.
History.
Indigenous history.
Before European colonisation, the area had been inhabited by the Whadjuk Noongar people for over 40,000 years, as evidenced by archaeological findings on the Upper Swan River. These Noongar people occupied the southwest corner of Western Australia and lived as hunter-gatherers. The wetlands on the Swan Coastal Plain were particularly important to them, both spiritually, featuring in local mythology, and as a source of food. Rottnest, Carnac and Garden Islands were also important to the Noongar people. 
The area where Perth now stands is also known as Boorloo by the Noongar people. Boorloo formed part of "Mooro", the tribal lands of Yellagonga's group, one of several based around the Swan River and known collectively as the Whadjuk. The Whadjuk were part of a larger group of fourteen tribes that formed the south-west socio-linguistic block known as the Noongar (meaning "the people" in their language), also sometimes called the "Bibbulmun". On 19 September 2006, the Federal Court of Australia brought down a judgment recognising Noongar native title over the Perth metropolitan area, in the case of "Bennell v State of Western Australia" FCA 1243. The judgment was overturned on appeal.
Early European sightings.
The first documented sighting of the region was made by the Dutch Captain Willem de Vlamingh and his crew on 10 January 1697. Subsequent sightings between this date and 1829 were made by other Europeans, but as in the case of the sighting and observations made by Vlamingh, the area was considered to be inhospitable and unsuitable for the agriculture that would be needed to sustain a settlement.
Swan River Colony.
Although the Colony of New South Wales had established a convict-supported settlement at King George's Sound (later Albany) on the south coast of Western Australia in 1826 in response to rumours that the area would be annexed by France, Perth was the first full-scale settlement by Europeans in the western third of the continent. The British colony would be officially designated Western Australia in 1832, but was known informally for many years as the Swan River Colony after the area's major watercourse.
On 4 June 1829, newly arriving British colonists had their first view of the mainland, and Western Australia's founding has since been recognised by a public holiday on the first Monday in June each year. Captain James Stirling, aboard "Parmelia", said that Perth was "as beautiful as anything of this kind I had ever witnessed". On 12 August that year, Helen Dance, wife of the captain of the second ship, "Sulphur", cut down a tree to mark the founding of the town.
It is clear that Stirling had already selected the name "Perth" for the capital well before the town was proclaimed, as his proclamation of the colony, read in Fremantle on 18 June 1829, ended "given under my hand and Seal at Perth this 18th Day of June 1829. James Stirling Lieutenant Governor". The only contemporary information on the source of the name comes from Fremantle's diary entry for 12 August, which records that they "named the town Perth according to the wishes of Sir George Murray". Murray was born in Perth, Scotland, and was in 1829 Secretary of State for the Colonies and Member for Perthshire in the British House of Commons. The town was named after the Scottish Perth, in Murray's honour.
Beginning in 1831, hostile encounters between the British settlers and the Noongar people – both large-scale land users with conflicting land value systems – increased considerably as the colony grew. The hostile encounters between the two groups of people resulted in a number of events, including the execution of the Whadjuk elder Midgegooroo, the death of his son Yagan in 1833, and the Pinjarra massacre in 1834.
The racial relations between the Noongar people and the Europeans were strained due to these happenings. Because of the large amount of building in and around "Boorloo", the local Whadjuk Noongar people were slowly dispossessed of their country. They were forced to camp around prescribed areas, including the swamps and lakes north of the settlement area including Third Swamp, known to them as "Boodjamooling". Boodjamooling continued to be a main camp-site for the remaining Noongar people in the Perth region, and was also used by travellers, itinerants, and homeless people. By the gold-rush days of the 1890s they were joined by miners who were en route to the goldfields.
In 1850, Western Australia was opened to convicts at the request of farming and business people looking for cheap labour. Queen Victoria announced the city status of Perth in 1856.
Federation and beyond.
After a referendum in 1900, Western Australia joined the Federation of Australia in 1901. It was the last of the Australian colonies to agree to join the Federation, and did so only after the other colonies had offered several concessions, including the construction of a transcontinental railway line from Port Augusta in South Australia to Kalgoorlie to link Perth with the eastern states.
In 1933, Western Australia voted in a referendum to leave the Australian Federation, with a majority of two to one in favour of secession. However, an election held shortly before the referendum had voted out the incumbent "pro-independence" government, replacing it with a government that did not support the independence movement. Respecting the result of the referendum, the new government nonetheless petitioned the Agent General of the United Kingdom for independence, where the request was simply ignored.
Perth's growth and relative prosperity, especially since the mid-1960s, has resulted from its role as the main service centre for the state's resource industries, which extract gold, iron ore, nickel, alumina, diamonds, mineral sands, coal, oil, and natural gas. Whilst most mineral and petroleum production takes place elsewhere in the state, the non-base services provide most of the employment and income to the people of Perth.
Geography.
Central business district.
The central business district of Perth is bounded by the Swan River to the south and east, with Kings Park on the western end, while the railway reserve formed a northern border. A state and federally funded project named Perth City Link sunk a section of the railway line, to link Northbridge and the CBD for the first time in 100 years. The Perth Arena is a building in the city link area that has received a number of architecture awards. St Georges Terrace is the prominent street of the area with 1.3 million m2 of office space in the CBD. Hay Street and Murray Street have most of the retail and entertainment facilities. The tallest building in the city is Central Park, which is the seventh tallest building in Australia. The CBD has recently been the centre of a mining-induced boom, with several commercial and residential projects being built, including Brookfield Place, a office building for Anglo-Australian mining company BHP Billiton.
Geology and landforms.
Perth is set on the Swan River, named for the native black swans by Willem de Vlamingh, captain of a Dutch expedition and namer of WA's Rottnest Island who discovered the birds while exploring the area in 1697. Traditionally, this water body had been known by Aboriginal inhabitants as "Derbarl Yerrigan". The city centre and most of the suburbs are located on the sandy and relatively flat Swan Coastal Plain, which lies between the Darling Scarp and the Indian Ocean. The soils of this area are quite infertile. The metropolitan area extends along the coast to Two Rocks in the north and Singleton to the south, a total distance of approximately . From the coast in the west to Mundaring in the east is a total distance of approximately . The Perth metropolitan area covers .
Much of Perth was originally built on a series of freshwater wetlands running from Herdsman Lake in the west through to Claisebrook Cove in the east.
To the east, the city is bordered by a low escarpment called the Darling Scarp. Perth is on generally flat, rolling land – largely due to the high amount of sandy soils and deep bedrock. The Perth metropolitan area has two major river systems: the first is made up of the Swan and Canning Rivers; the second is that of the Serpentine and Murray Rivers, which discharge into the Peel Inlet at Mandurah.
Climate.
Perth receives moderate though highly seasonal rainfall, making it the fourth wettest Australian capital city after Darwin, Sydney and Brisbane. Summers are generally very hot and dry, lasting from December to late March, with February generally being the hottest month of the year. Winters are relatively mild and wet, making Perth a classic example of a Mediterranean climate (Köppen climate classification "Csa"). Perth is a particularly sunny city for this type of climate; it has an average of 8.8 hours of sunshine per day, which equates to around 3200 hours of annual sunshine, and 138.7 clear days annually, making it the sunniest capital city in Australia.
Summer is not completely devoid of rain and humidity, with sporadic rainfall in the form of short-lived thunderstorms, weak cold fronts and on occasions decaying tropical cyclones from Western Australia's north-west, which can bring significant rainfall. Winters are also known to be clear and sunny. The highest temperature recorded in Perth was on 23 February 1991, although Perth Airport recorded on the same day. On most summer afternoons a sea breeze, known locally as the "Fremantle Doctor", blows from the southwest, providing relief from the hot north-easterly winds. Temperatures often fall below a few hours after the arrival of the wind change. In the summer, the 3 pm dewpoint averages at around .
Winters are wet but mild, with most of Perth's annual rainfall being between May and September. The lowest temperature recorded in Perth was on 17 June 2006. The lowest temperature within the Perth metropolitan area was on the same day at Jandakot Airport. However, temperatures at or below zero are very rare occurrences and it seldom gets cold enough for frost to form.
The rainfall pattern has changed in Perth and southwest Western Australia since the mid-1970s. A significant reduction in winter rainfall has been observed with a greater number of extreme rainfall events in the summer months, such as the slow-moving storms on 8 February 1992 that brought of rain, the highest recorded in Perth, and a severe thunderstorm on 22 March 2010, which brought of rain and caused significant damage in the metropolitan area.
Isolation.
Perth is one of the most isolated major cities in the world. The nearest city with a population of more than 100,000 is Adelaide, away. Only Honolulu (population 953,000), from San Francisco, is more isolated.
Perth is geographically closer to both Dili, East Timor (), and Jakarta, Indonesia (), than to Sydney (), Brisbane (), or Canberra ().
Demographics.
Perth is Australia's fourth-most populous city, having overtaken Adelaide's population in the early 1980s. At the 2006 Census 1,445,079 residents in the Perth statistical area were counted. In 2014 there were approximately 2.02 million residents in the metropolitan area.
Ethnic groups.
In 2006, the largest ancestry groups in the Perth metropolitan areas were: English (534,555 or 28.6%), Australian (479,174 or 25.6%), Irish (115,384 or 6.2%), Scottish (113,846 or 6.1%), Italian (84,331 or 4.5%) and Chinese (53,390 or 2.9%). There were 26,486 Indigenous Australians in the city.
Perth's population is notable for the high proportion of British and Irish born residents. At the 2006 Census, 142,424 England-born Perth residents were counted, narrowly behind Sydney (145,261), despite the fact that Perth had just 35% of the overall population of Sydney.
The ethnic make-up of Perth changed in the second part of the 20th century, when significant numbers of continental European immigrants arrived in the city. Prior to this, Perth's population had been almost completely Anglo-Celtic in ethnic origin. As Fremantle was the first landfall in Australia for many migrant ships coming from Europe in the 1950s and 1960s, Perth started to experience a diverse influx of people, including Italians, Greeks, Dutch, Germans, Croats. The Italian influence in the Perth and Fremantle area has been substantial, evident in places like the "Cappuccino strip" in Fremantle featuring many Italian eateries and shops. In Fremantle the traditional Italian blessing of the fleet festival is held every year at the start of the fishing season. In Northbridge every December is the San Nicola (Saint Nicholas) Festival, which involves a pageant followed by a concert, predominantly in Italian. Suburbs surrounding the Fremantle area, such as Spearwood and Hamilton Hill, also contain high concentrations of Italians, Croatians and Portuguese. Perth also has a small Jewish community – numbering 5,082 in 2006 – who have emigrated primarily from Eastern Europe and more recently from South Africa.
Another more recent wave of arrivals includes white minorities from Southern Africa. South African residents overtook those born in Italy as the fourth largest foreign group in 2001. By 2006, there were 18,825 South Africans residing in Perth, accounting for 1.3% of the city's population. Many Afrikaners and Anglo-Africans emigrated to Perth during the 1980s and 1990s, with the phrase "packing for Perth" becoming associated with South Africans who choose to emigrate abroad, sometimes regardless of the destination. As a result, the city has been described as "the Australian capital of South Africans in exile". The reason for Perth being so popular among white South Africans has often been the location, the vast amount of land, and the slightly warmer climate compared to other large Australian cities – Perth has a Mediterranean climate reminiscent of Cape Town.
Since the late 1970s, Southeast Asia has become an increasingly important source of migrants, with communities from Vietnam, Malaysia, Indonesia, Thailand, Singapore, Hong Kong, Mainland China, and India all now well-established. There were 53,390 persons of Chinese descent in Perth in 2006 – 2.9% of the city's population. These are supported by the Australian Eurasian Association of Western Australia, which also serves a community of Portuguese-Malacca Eurasian or Kristang immigrants.
The Indian community includes a substantial number of Parsees who emigrated from Bombay – Perth being the closest Australian city to India – and the India-born population of the city at the time of the 2006 census was 14,094 or 0.8%. Perth is also home to the largest population of Anglo-Burmese in the world; many settled here following the independence of Burma in 1948 and the city is now the cultural hub for Anglo-Burmese worldwide. There is also a substantial Anglo-Indian population in Perth, who also settled in the city following the independence of India.
Religion.
Protestants, predominantly Anglican, make up approximately 28% of the population. Perth is the seat of the Anglican Diocese of Perth and of the Roman Catholic Archdiocese of Perth. Roman Catholics make up about 23% of the population, and Catholicism is the most common single denomination. Perth is also home to 12,000 Latter-day Saints and the Perth Australia Temple of The Church of Jesus Christ of Latter-day Saints. Perth is also home of the seat of the Personal Ordinariate of Our Lady of the Southern Cross as the Church of St Ninian and St Chad in Perth was named the principal church of the ordinariate.
Buddhism and Islam each claim more than 20,000 adherents. Perth has one of the larger Jewish populations in Australia, numbering approximately 20,000, with both Orthodox and Progressive synagogues and a Jewish Day School. The Bahá'í community in Perth numbers around 1,500. Hinduism has over 20,000 adherents in Perth; the Diwali (festival of lights) celebration in 2009 attracted over 20,000 visitors. There are Hindu temples in Canning Vale, Anketell and a Swaminarayan temple north of the Swan River.
Approximately one in five people from Perth profess to having no religion, with 11% of people not specific as to their beliefs.
Governance.
Perth houses the Parliament of Western Australia and the Governor of Western Australia. , 42 of the Legislative Assembly's 59 seats and 18 of the Legislative Council's 36 seats are based in Perth's metropolitan area. Perth is represented by 9 full seats and significant parts of three others in the Federal House of Representatives, with the seats of Canning, Pearce and Brand including some areas outside the metropolitan area. The metropolitan area is divided into over 30 local government bodies, including the City of Perth which administers Perth's central business district.
The state's highest court, the Supreme Court, is located in Perth, along with the District and Family Courts. The Magistrates' Court has six metropolitan locations. The Federal Court of Australia and the Federal Circuit Court of Australia (previously the Federal Magistrates Court) occupy the Commonwealth Law Courts building on Victoria Avenue, which is also the location for annual Perth sittings of Australia's High Court.
The administrative region of Perth includes 30 local governments, with the outer extent being the City of Wanneroo and the City of Swan to the north, the Shire of Mundaring, Shire of Kalamunda and the City of Armadale to the east, the Shire of Serpentine-Jarrahdale to the southeast and the City of Rockingham to the southwest, and including the islands of Rottnest Island and Garden Island off the west coast; this also correlates with the Metropolitan Region Scheme. Perth can also be defined by its wider extent of Greater Perth.
Economy.
By virtue of its population and role as the administrative centre for business and government, Perth dominates the Western Australian economy, despite the major mining, petroleum, and agricultural export industries located elsewhere in the state. Perth's function as the state's capital city, its economic base and population size have also created development opportunities for many other businesses oriented to local or more diversified markets.
Perth's economy has been changing in favour of the service industries since the 1950s. Although one of the major sets of services it provides is related to the resources industry and, to a lesser extent, agriculture, most people in Perth are not connected to either; they have jobs that provide services to other people in Perth.
As a result of Perth's relative geographical isolation, it has never had the necessary conditions to develop significant manufacturing industries other than those serving the immediate needs of its residents, mining, agriculture and some specialised areas, such as, in recent times, niche ship building and maintenance. It was simply cheaper to import all the needed manufactured goods from either the eastern states or overseas.
Industrial employment influenced the economic geography of Perth. After WWII, Perth experienced suburban expansion aided by high levels of car ownership. Workforce decentralisation and transport improvements made it possible for the establishment of small-scale manufacturing in the suburbs. Many firms took advantage of relatively cheap land to build spacious, single-storey plants in suburban locations with plentiful parking, easy access and minimal traffic congestion. "The former close ties of manufacturing with near-central and/or rail-side locations were loosened."
Industrial estates such as Kwinana, Welshpool and Kewdale were post-war additions contributing to the growth of manufacturing south of the river. The establishment of the Kwinana industrial area was supported by standardisation of the east-west rail gauge linking Perth with eastern Australia. Since the 1950s the area has been dominated by heavy industry, including an oil refinery, steel-rolling mill with a blast furnace, alumina refinery, power station and a nickel refinery. Another development, also linked with rail standardisation, was in 1968 when the Kewdale Freight Terminal was developed adjacent to the Welshpool industrial area, replacing the former Perth railway yards.
With significant population growth post-WWII, employment growth occurred not in manufacturing but in retail and wholesale trade, business services, health, education, community and personal services and in public administration. Increasingly it was these services sectors, concentrated around the Perth metropolitan area, that provided jobs.
Education.
Education is compulsory in Western Australia between the ages of six and seventeen, corresponding to primary and secondary school. Tertiary education is available through a number of universities and technical and further education (TAFE) colleges.
Primary and secondary education.
Students may attend either public schools, run by the state government's Department of Education, or private schools, usually associated with a religion.
The Western Australian Certificate of Education (WACE) is the credential given to students who have completed Years 11 and 12 of their secondary schooling.
In 2012 the minimum requirements for students to receive their WACE changed.
Tertiary education.
Perth is home to four public universities: the University of Western Australia, Curtin University, Murdoch University, and Edith Cowan University. There is also one private university, the University of Notre Dame.
The University of Western Australia, which was founded in 1911, is renowned as one of Australia's leading research institutions. The university's monumental neo-classical architecture, most of which is carved from white limestone, is a notable tourist destination in the city. It is the only university in the state to be a member of the Group of Eight, as well as the Sandstone universities. It is also the state's only university to have produced a Nobel Laureate Barry Marshall who graduated with a Bachelor of Medicine, Bachelor of Surgery in 1975 and was awarded a joint Nobel Prize for physiology or medicine in 2005, together with Robin Warren.
Curtin University (previously known as Western Australian Institute of Technology (1966-1986) and Curtin University of Technology (1986-2010) is Western Australia's largest university by student population.
Murdoch University was founded in the 1973, and incorporates Western Australia's only veterinary school.
Edith Cowan University was established in the early 1990s from the existing Western Australian College of Advanced Education (WACAE) which itself was formed in the 1970s from the existing Teachers Colleges at Claremont, Churchlands, and Mount Lawley. It incorporates the Western Australian Academy of Performing Arts (WAAPA).
The University of Notre Dame Australia was established in 1990. Notre Dame was established as a Catholic university with its lead campus in Fremantle and a large campus in Sydney. Its campus is set in the west end of Fremantle, using historic port buildings built in the 1890s, giving Notre Dame a distinct European university atmosphere. Though Notre Dame shares its name with the University of Notre Dame in Indiana USA, it is a separate institution, claiming only "strong ties" with its American namesake.
Colleges of TAFE provide trade and vocational training, including certificate- and diploma-level courses. TAFE began as a system of technical colleges and schools under the Education Department, from which they were separated in the 1980s and ultimately formed into regional colleges. Four exist in the Perth metropolitan area: Central Institute of Technology (formerly Central TAFE); West Coast Institute of Training (northern suburbs); Polytechnic West (eastern and south-eastern suburbs; formerly Swan TAFE); and Challenger Institute of Technology (Fremantle/Peel).
Media.
Perth is served by twenty-eight digital free-to-air television channels:
ABC, SBS, Seven, Nine and Ten were also broadcast in an analogue format until 16 April 2013, when the analogue transmission was switched off. Community station Access 31 closed in August 2008. In April 2010 a new community station, West TV, began transmission (in digital format only).
Foxtel provides a subscription-based satellite and cable television service. Perth has its own local newsreaders on ABC (James McHale), Seven (Rick Ardon, Susannah Carr), Nine (Emmy Kubainski, Tim McMillan) and Ten (Narelda Jacobs).
Television shows produced in Perth include local editions of the current affair program "Today Tonight", and other types of programming such as "".
An annual telethon has been broadcast since 1968 to raise funds for charities including Princess Margaret Hospital for Children. The 24-hour Perth Telethon claims to be "the most successful fundraising event per capita in the world" and raised more than A$20 million in 2013, with a combined total of over A$153 million since 1968.
The main newspapers for Perth are "The West Australian" and "The Sunday Times". Localised free community papers cater for each local government area. There are also many advertising newspapers, such as "The Quokka". The local business paper is "Western Australian Business News".
Radio stations are on AM, FM and DAB+ frequencies. ABC stations include News Radio (585AM), 720 ABC Perth, Radio National (810AM), Classic FM (97.7FM) and Triple J (99.3FM). The six local commercial stations are: 92.9, Nova 93.7, Mix 94.5, 96fm, on FM and 882 6PR and 1080 6IX on AM. DAB+ has mostly the same as both FM and AM plus national stations from the ABC/SBS, Radar Radio and Novanation, along with local stations My Perth Digital and HotCountry Perth. Major community radio stations include RTRFM (92.1FM), Sonshine FM (98.5FM), SportFM (91.3FM) and Curtin FM (100.1FM).
Online news media covering the Perth area include TheWest.com.au backed by "The West Australian", Perth Now from the newsroom of The Sunday Times, WAToday from Fairfax Media and other outlets like TweetPerth on social media.
Culture.
Arts and entertainment.
The Perth Cultural Centre is the location of the city's major arts, cultural and educational institutions, including the Art Gallery of Western Australia, Western Australian Museum, State Library of Western Australia, State Records Office, and Perth Institute of Contemporary Arts (PICA). The State Theatre Centre of Western Australia is also located there, and is the home of the Black Swan State Theatre Company and the Perth Theatre Company. Other performing arts companies based in Perth include the West Australian Ballet, the West Australian Opera and the West Australian Symphony Orchestra, all of which present regular programmes. The Western Australian Youth Orchestras provide young musicians with performance opportunities in orchestral and other musical ensembles.
Perth is also home to the internationally regarded Western Australian Academy of Performing Arts at Edith Cowan University, from which many successful actors and broadcasters have launched their careers. The city's main performance venues include the Riverside Theatre within the Perth Convention Exhibition Centre, the Perth Concert Hall, the historic His Majesty's Theatre, the Regal Theatre in Subiaco and the Astor Theatre in Mount Lawley. Perth Arena can be configured as an entertainment or sporting arena, and concerts are also hosted at other sporting venues, including Subiaco Oval, HBF Stadium, and nib Stadium. Outdoor concert venues include Quarry Amphitheatre, Supreme Court Gardens, Kings Park and Russell Square.
A number of annual events are held in Perth. The Perth International Arts Festival is a large cultural festival that has been held annually since 1953, and has since been joined by the Winter Arts festival, Perth Fringe Festival, and Perth Writers Festival. Perth also hosts annual music festivals including Future Music, Stereosonic and Soundwave. The Perth International Comedy Festival features a variety of local and international comedic talent, with performances held at the Astor Theatre and nearby venues in Mount Lawley, and regular night food markets throughout the summer months across Perth and its surrounding suburbs. Sculpture by the Sea showcases a range of local and international sculptors' creations along Cottesloe Beach. There is also a wide variety of public art and sculptures on display across the city, throughout the year.
Perth has featured in a variety of artistic works in various mediums. An early novel, "Moondyne", set in the Swan River Colony, was written by a former Fenian convict, John Boyle O'Reilly, and a "A Faithful Picture", edited by Peter Cowan, gives a good idea of the early days of the colony. Songs that refer to the city include "I Love Perth" (1996) by Pavement, and "Perth" (2011) by Bon Iver, while a number of films feature Perth: "Last Train to Freo", "Two Fists, One Heart", "Thunderstruck", "Bran Nue Dae", "Japanese Story" and "Nickel Queen". The industrial metal band Fear Factory recorded the video for their single "Cyberwaste" in South Fremantle.
Because of Perth's relative isolation from other Australian cities, overseas artists often exclude it from their Australian tour schedules. This isolation, however, has developed a strong local music scene, and the development of local music groups such as John Butler Trio, The Triffids, Pendulum, Eskimo Joe, Pond, Tame Impala, Karnivool, Gyroscope, Jebediah, Little Birdy, The Panics and Birds of Tokyo. Celebrity musical performers from Perth have included the late AC/DC lead singer Bon Scott, who has been remembered with a statue in Fremantle, and veteran performer and artist Rolf Harris, given the nickname "The Boy From Bassendean". The largest performance area within the State Theatre Centre, the Heath Ledger Theatre, is named in honour of Perth-born film actor Heath Ledger.
Tourism and recreation.
Tourism in Perth is an important part of the state's economy, with approximately 2.8 million domestic visitors and 0.7 million international visitors in the year ending March 2012. Tourist attractions are generally focused around the city centre, Fremantle, the coast, and the Swan River.
In addition to the Perth Cultural Centre, there are a number of museums across the city. The Scitech Discovery Centre in is an interactive science museum, with regularly changing exhibitions on a large range of science and technology based subjects. Scitech also conducts live science demonstration shows, and operates the adjacent "Horizon" planetarium. The Western Australian Maritime Museum in Fremantle displays maritime objects from all eras. It houses "Australia II", the yacht that won the 1983 America's Cup, as well as a former Royal Australian Navy submarine. Also located in Fremantle is the Army Museum of Western Australia, situated within a historic artillery barracks. The museum consists of several galleries which reflect the Army's involvement in Western Australia, and the military service of Western Australians. The museum holds numerous items of significance, including three Victoria Crosses. Aviation history is represented by the Aviation Heritage Museum in Bull Creek, with its significant collection of aircraft, including a Lancaster bomber and a Catalina of the type operated from the Swan River during WWII. There are many heritage sites in Perth's CBD, Fremantle, and other parts of the metropolitan areas. Some of the oldest remaining building, dating back to the 1830s, include the Round House in Fremantle, the Old Mill in South Perth, and the Old Court House in the city centre. Registers of important buildings are maintained by the Heritage Council of Western Australia and local governments. A late heritage building is the Perth Mint.
Retail shopping in the Perth CBD is focused around Murray Street and Hay Street. Both of these streets are pedestrian malls between William Street and Barrack Street. Forrest Place is another pedestrian mall, connecting the Murray Street mall to Wellington Street and the Perth railway station. A number of arcades run between Hay Street and Murray Street, including the Piccadilly Arcade, which housed the Piccadilly Cinema until it closed in late 2013. Other shopping precincts include Harbour Town in West Perth, featuring factory outlets for major brands, the historically significant Fremantle Markets, which date back to 1897, and the Midland townsite on Great Eastern Highway, combining historic development around the Town Hall and Post Office buildings with the modern Midland Gate shopping centre further east. Joondalup's central business district is largely a shopping and retail area lined with townhouses and apartments, and also features Lakeside Joondalup Shopping City. Joondalup was granted the status of "tourism precinct" by the State Government in 2009, allowing for extended retail trading hours.
The Swan Valley, with fertile soil, uncommon in the Perth region, features numerous wineries such as the large complex at Houghtons, the state's biggest producer, Sandalfords and many smaller operators, including microbreweries and rum distilleries. The Swan Valley also contains specialised food producers, many restaurants and cafes, and roadside local-produce stalls that sell seasonal fruit throughout the year. Tourist Drive 203 is a circular route in the Swan Valley, passing by many attractions on West Swan Road and Great Northern Highway.
Kings Park, located in central Perth between the CBD and the University of Western Australia, is one of the world's largest inner-city parks, at . There are many landmarks and attractions within Kings Park, including the State War Memorial Precinct on Mount Eliza, Western Australian Botanic Garden, and children's playgrounds. Other features include DNA Tower, a 15m high double helix staircase that resembles the deoxyribonucleic acid (DNA) molecule, and Jacob's Ladder, comprising 242 steps that lead down to Mounts Bay Road.
Hyde Park is another inner-city park located north of the CBD. It was gazetted as a public park in 1897, created from of a chain of wetlands known as Third Swamp. Avon Valley, John Forrest and Yanchep national parks are areas of protected bushland at the northern and eastern edges of the metropolitan area. Within the city's northern suburbs is Whiteman Park, a bushland area, with bushwalking trails, bike paths, sports facilities, playgrounds, a tram on a circular track, motor and tractor museums, and Caversham Wildlife Park.
Perth Zoo, located in South Perth, houses a variety of Australian and exotic animals from around the globe. The zoo is home to highly successful breeding programs for orangutans and giraffes, and participates in captive breeding and reintroduction efforts for a number of Western Australian species, including the numbat, the dibbler, the chuditch, and the western swamp tortoise.
More wildlife can be observed at the Aquarium of Western Australia in Hillarys, which is Australia's largest aquarium, specialising in marine animals that inhabit the western coast of Australia. The northern Perth section of the coastline is known as Sunset Coast; it includes numerous beaches and the Marmion Marine Park, a protected area inhabited by tropical fish, Australian sea lions and bottlenose dolphins, and traversed by humpback whales. Tourist Drive 204, also known as Sunset Coast Tourist Drive, is a designated route from North Fremantle to Iluka along coastal roads.
Sport.
The climate of Perth allows for extensive outdoor sporting activity, and this is reflected in the wide variety of sports available to residents of the city. Perth was host to the 1962 Commonwealth Games and the 1987 America's Cup defence (based at Fremantle). Australian rules football is the most popular spectator sport in Perth – nearly 23% of Western Australians attended a match at least once in 2009–2010. The two Australian Football League teams located in Perth, the West Coast Eagles and the Fremantle Football Club, have two of the largest fan bases in the country. The Eagles, the older club, is one of the most successful teams in the league, and one of the largest sporting clubs in Australia.The next level of football is the Western Australian Football League, comprising nine clubs each having a League, Reserves and Colts team. Each of these clubs has a junior football system for all genders, and ages from 7 up to 17. The next level of football is the Western Australian Amateur Football League, comprising 68 clubs servicing senior footballers within the metropolitan area. Other popular sports include cricket, basketball, association football (soccer), and rugby union.
Current sport teams.
Perth has hosted numerous state and international sporting events. Ongoing international events include the Hopman Cup during the first week of January at the Perth Arena. In addition to these Perth has hosted international Rugby Union games, including qualifying matches for 2003 Rugby World Cup. The 1991 and 1998 FINA World Championships were held in Perth.
Four races (2006, 2007, 2008 and 2010) in the Red Bull Air Race World Championship have been held on a stretch of the Swan River called Perth Water, using Langley Park as a temporary air field. Several motorsport facilities exist in Perth including Perth Motorplex, catering to drag racing and speedway, and Barbagallo Raceway for circuit racing and drifting. Perth also has two thoroughbred racing facilities: Ascot, home of the Railway Stakes and Perth Cup; and Belmont Park.
The WACA Ground opened in the 1890s and has hosted Test cricket since 1970. The Western Australian Athletics Stadium opened in 2009.
Infrastructure.
Health.
Perth has ten large hospitals with emergency departments. , Royal Perth Hospital in the city centre is the largest, with others spread around the metropolitan area: Armadale Kelmscott District Memorial Hospital, Joondalup Health Campus, King Edward Memorial Hospital for Women in Subiaco, Rockingham General Hospital, Sir Charles Gairdner Hospital in Nedlands, St John of God Murdoch Hospital, Swan District Hospital in Middle Swan, and Fiona Stanley Hospital in Murdoch. Princess Margaret Hospital for Children is the state's only specialist children's hospital, and Graylands Hospital is the only public stand-alone psychiatric teaching hospital. Most of these are public hospitals, with some operating under public-private partnerships. St John of God Murdoch Hospital is privately owned and operated.
New hospitals are under construction to replace ageing facilities. A new children's hospital, due to open in 2015, is being constructed next to Sir Charles Gairdner Hospital, and will replace Princess Margaret Hospital. Midland Health Campus, a public and a private hospital, is under construction in Midland. St John of God Health Care will build and operate the new hospitals under a public-private partnership with the state government. Midland Health Campus will open in late 2015, and replace the nearby Swan District Hospital.
A number of other public and private hospitals operate in Perth.
Transport.
Perth is served by Perth Airport in the city's east for regional, domestic and international flights and Jandakot Airport in the city's southern suburbs for general aviation and charter flights.
Perth has a road network with three freeways and nine metropolitan highways. The Northbridge tunnel, part of the Graham Farmer Freeway, is the only significant road tunnel in Perth.
Perth metropolitan public transport, including trains, buses and ferries, are provided by Transperth, with links to rural areas provided by Transwa. There are 70 railway stations and 15 bus stations in the metropolitan area. 
Perth provides zero-fare bus and train trips around the city centre (the "Free Transit Zone"), including four high-frequency CAT bus routes.
The "Indian Pacific" passenger rail service connects Perth with Adelaide and Sydney once per week in each direction. The "Prospector" passenger rail service connects Perth with Kalgoorlie via several Wheatbelt towns, while the "Australind" connects to Bunbury, and the "AvonLink" connects to Northam.
Rail freight terminates at the Kewdale Rail Terminal, south-east of the city centre.
Perth's main container and passenger port is at Fremantle, south west at the mouth of the Swan River. A second port complex is planned to be developed in Cockburn Sound primarily for the export of bulk commodities.
Utilities.
Perth's electricity is generated, supplied, and retailed by three Western Australian Government corporations. Verve Energy operates coal and gas power generation stations, as well as wind farms and other power sources. The physical network is maintained by Western Power, while Synergy, the state's largest energy retailer, sells electricity to residential and business customers.
Alinta Energy, which was previously a government owned company, had a monopoly in the domestic gas market since the 1990s. However, in 2013 Kleenheat Gas began operating in the market, allowing consumers to choose their gas retailer.
The Water Corporation is the dominant supplier of water, as well as wastewater and drainage services, in Perth and throughout the Western Australia. It is also owned by the state government.
Water supply.
Reduced rainfall in the region in recent years has lowered inflow to reservoirs by two-thirds over the last 30 years, and affected groundwater levels. Coupled with the city's relatively high growth rate, this had led to concerns that Perth could run out of water in the near future. The Western Australian State Government responded by introducing mandatory household sprinkler restrictions in the city. The Kwinana Desalination Plant was opened in November 2006, able to supply over 45 gigalitres9) per year --> (10 billion imperial or 12 billion US gallons) of potable water per year; its power requirements were met by the construction of the Emu Downs Wind Farm near Cervantes. Consideration was given to piping water from the Kimberley region, but the idea was rejected in May 2006 due primarily to its high cost. Other proposals under consideration included the controversial extraction of an extra 45 gigalitres of water a year from the Yarragadee Aquifer in the south-west of the state. However, in May 2007, the state government announced that a second desalination plant will be built at Binningup, on the coast between Mandurah and Bunbury. A trial winter (1 June – 31 August) sprinkler ban was introduced in 2009 by the State Government, a move which the Government later announced would be made permanent. In September 2009 Western Australia's dams reached 50% overall capacity for the first time since 2000.

</doc>
<doc id="24358" url="https://en.wikipedia.org/wiki?curid=24358" title="Human pathogen">
Human pathogen

A human pathogen is a pathogen (microbe or microorganism such as a virus, bacterium, prion, or fungus) that causes disease in humans.
The human physiological defense against common pathogens (such as "Pneumocystis") is mainly the responsibility of the immune system with help by some of the body's normal flora and fauna. However, if the immune system or "good" microbiota are damaged in any way (such as by chemotherapy, human immunodeficiency virus (HIV), or antibiotics being taken to kill other pathogens), pathogenic bacteria that were being held at bay can proliferate and cause harm to the host. Such cases are called opportunistic infection.
Some pathogens (such as the bacterium "Yersinia pestis", which may have caused the Black Plague, the "Variola" virus, and the Malaria protozoa) have been responsible for massive numbers of casualties and have had numerous effects on afflicted groups. Of particular note in modern times is HIV, which is known to have infected several million humans globally, along with the Influenza virus. Today, while many medical advances have been made to safeguard against infection by pathogens, through the use of vaccination, antibiotics, and fungicide, pathogens continue to threaten human life. Social advances such as food safety, hygiene, and water treatment have reduced the threat from some pathogens.
Types.
Viral.
Pathogenic viruses are mainly those of the families of: Adenoviridae, Picornaviridae, Herpesviridae, Hepadnaviridae, Flaviviridae, Retroviridae, Orthomyxoviridae, Paramyxoviridae, Papovaviridae, Polyomavirus, Rhabdoviridae, Togaviridae. Some notable pathogenic viruses cause smallpox, influenza, mumps, measles, chickenpox, ebola, and rubella. Viruses typically range between 20-300 nanometers in length.
Bacterial.
Although the vast majority of bacteria are harmless or beneficial to ones body, a few pathogenic bacteria can cause infectious diseases. The most common bacterial disease is tuberculosis, caused by the bacterium "Mycobacterium tuberculosis", which affects just about 2 million people mostly in sub-Saharan Africa. Pathogenic bacteria contribute to other globally important diseases, such as pneumonia, which can be caused by bacteria such as "Streptococcus" and "Pseudomonas", and foodborne illnesses, which can be caused by bacteria such as "Shigella", "Campylobacter" and "Salmonella". Pathogenic bacteria also cause infections such as tetanus, typhoid fever, diphtheria, syphilis and Hansen's disease. They typically range between 1 and 5 micrometers in length.
Fungal.
Fungi comprise a eukaryotic kingdom of microbes that are usually saprophytes but can cause diseases in humans. Life-threatening fungal infections in humans most often occur in immunocompromised patients or vulnerable people with a weakened immune system, although fungi are common problems in the immunocompetent population as the causative agents of skin, nail or yeast infections. Most antibiotics that function on bacterial pathogens cannot be used to treat fungal infections because fungi and their hosts both have eukaryotic cells. Most clinical fungicides belong to the azole group. The typical fungal spore size is 1-40 micrometer in length.
Other parasites.
Some eukaryotic organisms, such as protists and helminths, cause disease. One of the best known diseases caused by protists in the genus "Plasmodium" is malaria. These can range from 3-200 micrometers in length.
Prionic.
Prions are infectious pathogens that do not contain nucleic acids. Prions are abnormal proteins whose presence causes some diseases such as scrapie, bovine spongiform encephalopathy (mad cow disease) and Creutzfeldt–Jakob disease. The discovery of prion as a new class of pathogen led Stanley B. Prusiner to receive the Nobel Prize in Physiology or Medicine in 1997.
Animal pathogens.
Animal pathogens are disease-causing agents of wild and domestic animal species, at times including humans.
Virulence.
Virulence (the tendency of a pathogen to cause damage to a host's fitness) evolves when that pathogen can spread from a diseased host, despite that host being very debilitated. An example is the malaria parasite which can spread from a person near death, by hitching a ride to a healthy person on a mosquito that has bitten the diseased person. This is called horizontal transmission in contrast to vertical transmission, which tends to evolve symbiosis (after a period of high morbidity and mortality in the population) by linking the pathogen's evolutionary success to the evolutionary success of the host organism.
Evolutionary medicine has found that under horizontal transmission, the host population might never develop tolerance to the pathogen.
Transmission.
Transmission of pathogens occurs through many different routes, including airborne, direct or indirect contact, sexual contact, through blood, breast milk, or other body fluids, and through the fecal-oral route. One of the primary pathways by which food or water become contaminated is from the release of untreated sewage into a drinking water supply or onto cropland, with the result that people who eat or drink contaminated sources become infected. In developing countries most sewage is discharged into the environment or on cropland; even in developed countries there are periodic system failures resulting in a sanitary sewer overflow.

</doc>
<doc id="24363" url="https://en.wikipedia.org/wiki?curid=24363" title="People Against Gangsterism and Drugs">
People Against Gangsterism and Drugs

People Against Gangsterism and Drugs (PAGAD) was a vigilante group formed in 1996 in the Cape Flats area of Cape Town, South Africa.
Origins.
PAGAD was originally initiated by a handful of Qiblah, PAC and community members from a Cape Town townships who decided to organize public demonstrations to pressure the government to fight the illegal drug trade and gangsterism more effectively. However, PAGAD increasingly took matters into their own hands, believing the police were not taking enough action against gangs. Initially the community and police were hesitant to act against PAGAD activities, recognising the need for community action against crime in the gang-ridden communities of the Cape Flats.
Notorious gangsters were initially asked by PAGAD members to stop their criminal activities or be subject to 'popular justice'. A common PAGAD modus operandi was to set fire to drug dealers houses and kill gangsters. PAGAD's campaign came to prominence in 1996 when a local gang leader, Rashaad Staggie, was beaten and burnt to death by a mob during a march to his home in Salt River. South Africa's police quickly came to regard PAGAD as part of the problem, rather than a partner in the fight against crime and they were eventually designated a terrorist organization by the South African government.
Changes within the organisation following the incidences of 1996 increased the influence of more highly politicised and organisationally experienced people within it associated with radical Islamic groups such as Qibla. This caused a series of changes such as the emergence of new leadership and the development of tighter organisational structures. This succeeded in transforming PAGAD from a relatively non-religious popular mass movement into a smaller, better organised but also a religiously radical isolated group.
The threat of growing vigilantism in 2000 led the Western Cape provincial government to declare a 'war on gangs' that became a key priority of the ANC provincial government at the time.
Cape Town bombings.
Although PAGAD's leadership denied involvement, PAGAD's G-Force, operating in small cells, was believed responsible for killing a large number of gang leaders, and also for a bout of urban terrorism — particularly bombings — in Cape Town. The bombings started in 1998, and included nine bombings in 2000. In addition to targeting gang leaders, bombing targets included South African authorities, moderate Muslims, synagogues, gay nightclubs, tourist attractions, and Western-associated restaurants. The most prominent attack during this time was the bombing on 25 August 1998 of the Cape Town Planet Hollywood.
In September 2000, magistrate Pieter Theron, who was presiding in a case involving PAGAD members, was murdered in a drive-by shooting.
PAGAD's leaders have become known for making anti-semitic statements. A 1997 incendiary bomb attack on a Jewish bookshop owner was found by police to have been committed with the same material PAGAD has used in other attacks. In 1998, Ebrahim Moosa, a University of Cape Town academic who had been critical of PAGAD, decided to take a post in the United States after his home was bombed.
Violent acts such as bombings and vigilantism in Cape Town subsided in 2002, and the police have not attributed any such acts to PAGAD since the November 2002 bombing of the Bishop Lavis offices of the Serious Crimes Unit in the Western Cape. In 2002, PAGAD leader Abdus Salaam Ebrahim was convicted of public violence and imprisoned for seven years. Although a number of other PAGAD members were arrested and convicted of related crimes, none were convicted of the Cape Town bombings.
Current activities.
Today, PAGAD maintains a small and less visible presence in the Cape Town Cape Muslim community.
However, in the run up to the 2014 South African general elections it has been reported that the organisation has been growing in strength. Hosting more motorcades and marches in Mitchell’s Plain in February–March 2014 than it has had in the whole of 2013. One of PAGAD's largest marches in 2014 was joined by the EFF, a far left political party who expressed their support for the organisation.

</doc>
<doc id="24364" url="https://en.wikipedia.org/wiki?curid=24364" title="PDP-8">
PDP-8

The 12-bit PDP-8, produced by Digital Equipment Corporation (DEC), is the first successful commercial minicomputer. DEC introduced it on March 22, 1965 for a price of US$18,500, and eventually sold more than 50,000 systems, the most of any computer up to that time. It was the first widely sold computer in the DEC PDP series of computers (the PDP-5 was not originally intended to be a general-purpose computer). The chief engineer who designed the initial version of the PDP-8 was Edson de Castro, who later founded Data General.
The earliest PDP-8 model (informally known as a "Straight-8") used diode-transistor logic, packaged on flip chip cards, and was about the size of a small household refrigerator.
This was followed in 1966 by the PDP-8/S, available in desktop and rack-mount models. By using a one-bit serial arithmetic logic unit (ALU) implementation, the PDP-8/S was smaller, less expensive and slower than the original PDP-8. The PDP-8/S was about 20% of the cost and about 10% of the performance of the PDP-8. The only mass storage peripheral available for the PDP-8/S was the DF32 disk.
Later systems (the PDP-8/I and /L, the PDP-8/E, /F, and /M, and the PDP-8/A) returned to a faster, fully parallel implementation but used much less costly transistor-transistor logic (TTL) MSI logic. Most surviving PDP-8s are from this era. The PDP-8/E is common, and well-regarded because so many types of I/O devices were available for it. It was often configured as a general-purpose computer.
In 1975, early personal computers based on inexpensive microprocessors, such as the MITS Altair 8800 and later TRS-80, Apple II and others began to dominate the market for small general purpose computers.
The last commercial PDP-8 models in 1979 were called "CMOS-8s" and used custom complementary metal-oxide-semiconductor (CMOS) microprocessors. They were not priced competitively, and the offering failed. The IBM PC in 1981 cemented the doom of the CMOS-8s by making a legitimate, well-supported small microprocessor computer.
Intersil sold the integrated circuits commercially through to 1982 as the Intersil 6100 family. By virtue of their CMOS technology they had low power requirements and were used in some embedded military systems.
Architectural significance.
The PDP-8 combined low cost, simplicity, expandability, and careful engineering for value. The greatest historical significance was that the PDP-8's low cost and high volume made a computer available to many new people for many new uses. Its continuing significance is as a historical example of value-engineered computer design.
The low complexity brought other costs. It made programming cumbersome, as is seen in the examples in this article and from the discussion of "pages" and "fields". Some ambitious programming projects failed to fit in memory or developed design defects that could not be solved.
As design advances reduced the costs of logic and memory, the programmer's time became more important. Subsequent computer designs emphasized ease of programming, typically using a larger and more intuitive instruction set.
Eventually, most machine-language programming came to be generated by compilers and report generators. The reduced instruction set computer returned full-circle to the PDP-8's emphasis on a simple instruction set and achieving multiple actions in a single instruction cycle, in order to maximize execution speed, although the newer computers had much longer instruction words.
Description.
The PDP-8 used ideas from several 12-bit predecessors, most notably the LINC designed by W.A. Clark and C.E. Molnar who were inspired by Seymour Cray's CDC 160 minicomputer.
The architecture had a simple programmed I/O bus, plus a DMA channel. The programmed I/O bus would typically run low to medium-speed peripherals, such as printers, teletypes, paper tape punches and readers, while DMA was used for cathode ray tube screens with a light pen, analog-to-digital converters, digital-to-analog converters, tape drives, disk drives.
The word size, 12 bits, is large enough to handle unsigned integers from 0 to 4095 – wide enough for controlling simple machinery. This is also enough to handle signed numbers from -2048 to +2047. This is higher precision than a slide rule or most analog computers. Twelve bits could also store two six-bit ASCII subset characters.
To save money, the design uses inexpensive main memory for many purposes that are served by more expensive flip-flop registers in other computers.
The PDP-8's basic configuration had a main memory of 4,096 twelve-bit words. An optional memory-expansion unit could switch banks of such memories using the IOT instruction.
At its inception, the programmer's view of the PDP-8 had only eight instructions and two registers (a 12-bit accumulator, AC, and a carry bit called the "link register", L). The machine used magnetic core memory with a cycle time of 1.5 microseconds, so that a typical two-cycle (Fetch, Execute) memory-reference instruction ran at a speed of 0.333 MIPS. The 1974 Pocket Reference Card for the PDP-8/E gave a basic instruction time of 1.2 microseconds, or 2.6 microseconds for instructions that referenced memory.
A second register, the Multiplier/Quotient or MQ register, along with multiply and divide instructions, were included in an optional extended arithmetic element (EAE). The EAE was an option on the original PDP-8 as well as the 8/I and 8/E, and it was an integral part of the Intersil 6100 microprocessor.
The PDP-8 was optimized for simplicity of design. The CPU of the serial model, the PDP-8/S, had only about 519 logic gates, while small microcontrollers (as of 2008) usually have 15,000 or more. Compared to more complex machines, unnecessary features were removed, and logic was shared when possible. Instructions used autoincrement, autoclear and indirect access to increase the software's speed, reduce memory use and substitute inexpensive memory for expensive registers. A basic PDP-8 CPU has only four 12-bit registers: the accumulator, program counter, memory-buffer register, and memory-address register. To save money, these were all used for multiple purposes at different points in the operating cycle. For example, the memory buffer register provided arithmetic operands, was part of the instruction register, and stored data to rewrite the core memory. (This restores the core data destroyed by the read.) In spite of its basic simplicity, the use of very small modules made it an expensive machine, with a large part of the cost sunk into the small modules, the thousands of gold-plated connectors, and the complex wire-wrapped backplane. In the 8/S two different logic voltages were used, an inexpensive way to increase the fan-out of the inexpensive diode-transistor logic. The PDP-8/E employed transistor-transistor logic, a redesigned core memory, a more integrated hardware design and the OMNIBUS as a replacement of the former wire-wrapped backplane, saving cost and allowing for flexible expansion with additional modules. A personal account of the development of the PDP-8/E can be read on the IEEE Global History Network.
Versions of the PDP-8.
The total sales figure for the PDP-8 family has been estimated at over 300,000 machines. The following models were manufactured:
Latter-day implementations.
The PDP-8 is readily emulated, as its instruction set is much simpler than modern architectures. Enthusiasts have created entire PDP-8s using single FPGA devices.
Several software simulations of a PDP-8 are available on the Internet. The best of these correctly execute DEC's operating systems and diagnostic software. The software simulations often simulate late-model PDP-8s with all possible peripherals. Even these use only a tiny fraction of the capacity of a modern personal computer.
Input/Output.
The I/O systems underwent huge changes during the PDP-8 era. Early PDP-8 models used a front panel interface, a paper-tape reader and a teletype printer with an optional paper-tape punch. Over time I/O systems such as magnetic tape, RS-232 and current loop dumb terminals, punched card readers, and fixed-head disks were added. Toward the end of the PDP-8 era, floppy disks and moving-head cartridge disk drives were popular I/O devices. Modern enthusiasts have created standard PC style IDE hard disk adapters for real and simulated PDP-8 computers.
I/O was supported through several different methods:
A simplified, inexpensive form of DMA called "three-cycle data break" was supported; this required the assistance of the processor. The "data break" method moved some of common logic needed to implement DMA I/O from each I/O device into one common copy of the logic within the processor. "Data break" placed the processor in charge of maintaining the DMA address and word count registers. In three successive memory cycles, the processor would update the word count, update the transfer address, and store or retrieve the actual I/O data word.
One cycle data break effectively tripled the DMA transfer rate because only the target data needed to be transferred to and from the core memory. However, the I/O devices needed more electronic logic to manage their own word count and transfer address registers. By the time the PDP-8/E was introduced, electronic logic had become less expensive and "one-cycle data break" became more popular.
Programming facilities.
Early PDP-8 systems did not have an operating system, just a front panel and run and halt switches. Software development systems for the PDP-8 series began with the most basic front panel entry of raw binary machine code (booting entry).
In the middle era, various paper tape "operating systems" were developed. Many utility programs became available on paper tape. PAL-8 assembly language source code was often stored on paper tape, read into memory, and saved to paper tape. PAL assembled from paper tape into memory. Paper tape versions of a number of programming languages were available, including DEC's FOCAL interpreter and a 4K FORTRAN compiler and runtime.
Toward the end of the PDP-8 era, operating systems such as OS/8 and COS-310 allowed a traditional line mode editor and command-line compiler development system using languages such as PAL-III assembly language, FORTRAN, BASIC, and DIBOL.
Fairly modern and advanced real-time operating system (RTOS) and preemptive multitasking multi-user systems were available: a real-time system (RTS-8) was available as were multiuser commercial systems (COS-300 and COS-310) and a dedicated single-user word-processing system (WPS-8).
A time-sharing system, TSS-8, was also available. TSS-8 allowed multiple users to log in to the system via 110-baud terminals, and edit, compile and debug programs. Languages included a special version of BASIC, a FORTRAN subset similar to FORTRAN-1 (no user-written subroutines or functions), an ALGOL subset, FOCAL, and an assembler called PAL-D.
A fair amount of user-donated software for the PDP-8 was available from DECUS, the Digital Equipment Corporation User Society, and often came with full source listings and documentation.
Instruction set.
The three high-order bits of the 12-bit instruction word (labelled bits 0 through 2) are the operation code. For the six operations that refer to memory, bits 5 through 11 provide a 7-bit address. Bit 4, if set, says to complete the address using the 5 high-order bits of the PC; if clear, zeroes are used. Bit 3 specifies indirection; if set, the address obtained as described so far points to a 12-bit value in memory that gives the actual effective address for the instruction. (The JMP instruction does not operate on a memory word, except if indirection is specified, but has the same bit fields.)
This use of the instruction word divides the 4,096-word memory into 128-word pages; bit 4 of the instruction selects either the current page or page 0 (addresses 0000–0177 in octal). Memory in page 0 is at a premium, since variables placed here can be addressed directly from any page. (Moreover, address 0000 is where any interrupt service routine must start, and addresses 0010–0017 have the special property of auto-incrementing preceding any indirect reference through them.)
The standard assembler places constant values for arithmetic in the current page. Likewise, cross-page jumps and subroutine calls use an indirect address in the current page.
It was important to write routines to fit within 128-word pages, or to arrange routines to minimize page transitions, as references and jumps outside the current page required an extra word. Consequently, much time was spent cleverly conserving one or several words. Programmers deliberately placed code at the end of a page to achieve a free transition to the next page as PC was incremented.
IOT (Input-Output Transfer) instructions.
The PDP-8 processor defined few of the IOT instructions, but simply provided a framework. Most IOT instructions were defined by the individual I/O devices.
Bits 3 through 8 of an IOT instruction selected an I/O device. Some of these device addresses were standardized by convention:
Instructions for device 0 affected the processor as a whole. For example, ION (6001) enabled interrupt processing, and IOFF (6002) disabled it.
Bits 9 through 11 of an IOT instruction selected the function(s) the device would perform. Simple devices (such as the paper tape reader and punch and the console keyboard and printer) would use the bits in standard ways:
These operations took place in a well-defined order that gave useful results if more than one bit was set.
More complicated devices, such as disk drives, used these 3 bits in device-specific fashions. Typically, a device decoded the 3 bits to give 8 possible function codes.
OPR (OPeRate).
Many operations were achieved using OPR, including most of the conditionals. OPR does not address a memory location; conditional execution is achieved by conditionally skipping one instruction, which was typically a JMP.
The OPR instruction was said to be "microcoded." This did not mean what the word means today (that a lower-level program fetched and interpreted the OPR instruction), but meant that each bit of the instruction word specified a certain action, and the programmer could achieve several actions in a single instruction cycle by setting multiple bits. In use, a programmer would write several instruction mnemonics alongside one another, and the assembler would combine them with OR to devise the actual instruction word. Many I/O devices supported "microcoded" IOT instructions.
Microcoded actions took place in a well-defined sequence designed to maximize the utility of many combinations.
The OPR instructions came in Groups. Bits 3, 8 and 11 identify the Group of an OPR instruction, so it was impossible to combine the microcoded actions from different groups.
Group 1.
In most cases, the operations are sequenced so that they can be combined in the most useful ways. For example, combining CLA (CLear Accumulator), CLL (CLear Link), and IAC (Increment ACcumulator) first clears the AC and Link, then increments the accumulator, leaving it set to 1. Adding RAL to the mix (so CLA CLL IAC RAL) causes the accumulator to be cleared, incremented, then rotated left, leaving it set to 2. In this way, small integer constants were placed in the accumulator with a single instruction.
The combination CMA IAC, which the assembler let you abbreviate as CIA, produced the arithmetic inverse of AC: the twos-complement negation. Since there was no subtraction instruction, only the twos-complement add (TAD), computing the difference of two operands required first negating the subtrahend.
A Group 1 OPR instruction that has none of the microprogrammed bits set performs no action. The programmer can write NOP (No Operation) to assemble such an instruction.
Group 2, Or Group.
When bit 8 is clear, a skip is performed if any of the specified conditions are true. For example, "SMA SZA", opcode 7540, skips if AC ≤ 0.
A Group 2 OPR instruction that has none of the microprogrammed bits set is another No-Op instruction.
Group 2, And Group.
When bit 8 is set, the Group 2, Or skip condition is inverted: the skip is "not" performed if any of the group 2, Or conditions are true, meaning that "all" of the specified skip conditions must be true. For example, "SPA SNA", opcode 7550, skips if AC > 0. If none of bits 5–7 are set, then the skip is unconditional.
Group 3.
Unused bit combinations of OPR were defined as a third Group of microprogrammed actions mostly affecting the MQ (Multiplier/Quotient) register.
Typically CLA and MQA were combined to transfer MQ into AC. Another useful combination is MQA and MQL, to exchange the two registers.
Three bits specified a multiply/divide instruction to perform:
Memory control.
A 12-bit word can have 4,096 different values, and this was the maximum number of words the original PDP-8 could address indirectly through a word pointer. As programs became more complex and the price of memory fell, it became desirable to expand this limit.
To maintain compatibility with pre-existing programs, new hardware outside the original design added high-order bits to the effective addresses generated by the program. The Memory Extension Controller expanded the addressable memory by a factor of 8, to a total of 32,768 words. This expansion was thought sufficient because, with core memory then costing about 50 cents a word, a full 32K of memory would equal the cost of the CPU.
Each 4K of memory was called a field. The Memory Extension Controller contained two three-bit registers: the DF (Data Field) and the IF (Instruction Field). These registers specified a field for each memory reference of the CPU, allowing a total of 15 bits of address. The IF register specified the field for instruction fetches and direct memory references; the DF register specified the field for indirect data accesses. A program running in one field could reference data in the same field by direct addressing, and reference data in another field by indirect addressing.
A set of IO instructions in the range 6200 through 6277 was handled by the Memory Extension Controller and gave access to the DF and IF registers. The 62X1 instruction (CDF, Change Data Field) set the data field to X. Similarly 62X2 (CIF) set the instruction field, and 62X3 set both. Pre-existing programs would never execute CIF or CDF; the DF and IF registers would both point to the same field, a single field to which these programs were limited. The effect of the CIF instruction was deferred to coincide with the next JMP or JMS instruction, so that executing CIF would not cause a jump.
It was more complicated for multiple-field programs to deal with field boundaries and the DF and IF registers than it would have been if they could simply generate 15-bit addresses, but the design provided backward compatibility and was consistent with the 12-bit architecture used throughout the PDP-8. Compare the later Intel 8086, whose 16-bit memory addresses are expanded to 20 bits by combining them with the contents of a specified or implied segment register.
The extended memory scheme let existing programs handle increased memory with minimal changes. For example, 4K FOCAL normally had about 3K of code with only 1K left over for user program and data. With a few patches, FOCAL could use a second 4K field for user program and data. Moreover, additional 4K fields could be allocated to separate users, turning 4K FOCAL into a multi-user timesharing system.
On the PDP-8/E and later models, the Memory Extension Controller was enhanced to enable machine virtualization. A program written to use a PDP-8's entire resources could coexist with other such programs on the same PDP-8 under the control of a virtual machine manager. The manager could make all IO instructions (including those that operated on the Memory Extension Controller) cause a trap (an interrupt handled by the manager). In this way, the manager could map memory references, map data or instruction fields, and redirect IO to different devices. Each original program had complete access to a "virtual machine" provided by the manager.
New IO instructions to the Memory Extension Controller retrieved the current value of the data and instruction fields, letting software save and restore most of the machine state across a trap. However, a program could not sense whether the CPU was in the process of deferring the effect of a CIF instruction (whether it had executed a CIF and not yet executed the matching jump instruction). The manager had to include a complete PDP-8 emulator (not difficult for an 8-instruction machine). Whenever a CIF instruction trapped to the manager, it had to emulate the instructions up to the next jump. Fortunately, as a jump usually was the next instruction after CIF, this emulation did not slow programs down much, but it is a large workaround to a seemingly small design deficiency.
By the time of the PDP-8/A, memory prices had fallen enough that memory exceeding 32K was desirable. The 8/A added a new set of instructions for handling more than eight fields of memory. The field number could now be placed in the AC, rather than hard-coded into the instruction. However, by this time, the PDP-8 was in decline, so very little standard software was modified to use these new features.
Examples.
The following examples show code in PDP-8 assembly language as one might write for the PAL-III assembler.
Comparing two numbers.
The following piece of code shows what is needed just to compare two numbers:
As shown, much of the text of a typical PDP-8 program focuses not on the author's intended algorithm but on low-level mechanics. An additional readability problem is that in conditional jumps such as the one shown above, the conditional instruction (which skips around the JMP) highlights the opposite of the condition of interest.
String output.
This complete PDP-8 assembly language program outputs "Hello, world!" to the teleprinter.
Subroutines.
The PDP-8 processor did not implement a stack upon which to store registers or other context when a subroutine was called or an interrupt occurred. (A stack could be implemented in software, as demonstrated in the next section.) Instead, the JMS instruction simply stored the updated PC (pointing past JMS, to the return address) at the effective address and jumped to the effective address plus one. The subroutine returned to its caller using an indirect JMP instruction that addressed the subroutine's first word.
For example, here is "Hello, World!" re-written to use a subroutine. When the JMS instruction jumps to the subroutine, it modifies the 0 coded at location OUT1:
The fact that the JMS instruction used the word just before the code of the subroutine to deposit the return address prevented reentrancy and recursion without additional work by the programmer. It also made it difficult to use ROM with the PDP-8 because read-write return-address storage was commingled with read-only code storage in the address space. Programs intended to be placed into ROMs approached this problem in several ways:
The use of the JMS instruction made debugging difficult. If a programmer made the mistake of having a subroutine call itself, directly or by an intermediate subroutine, then the return address for the outer call would be destroyed by the return address of the subsequent call, leading to an infinite loop. If one module was coded with an incorrect or obsolete address for a subroutine, it would not just fail to execute the entire code sequence of the subroutine, it might modify a word of the subroutine's code, depositing a return address that the processor might interpret as an instruction during a subsequent correct call to the subroutine. Both types of error might become evident during the execution of code that was written correctly.
Software stack.
Though the PDP-8 did not have a hardware stack, it could be implemented in software.
Here are example PUSH and POP subroutines, simplified to omit issues such as testing for stack overflow and underflow:
And here is "Hello World" with this "stack" implemented, and "OUT" subroutine:
Linked list.
Another possible subroutine for the PDP-8 was a linked list.
Interrupts.
There was a single interrupt line on the PDP-8 I/O bus. The processor handled any interrupt by disabling further interrupts and executing a codice_1 to location 0000. As it was difficult to write reentrant subroutines, it was difficult to nest interrupts and this was usually not done; each interrupt ran to completion and re-enabled interrupts just before executing the codice_2 instruction that returned from the interrupt.
Because there was only a single interrupt line on the I/O bus, the occurrence of an interrupt did not inform the processor of the source of the interrupt. Instead, the interrupt service routine had to serially poll each active I/O device to see if it was the source. The code that did this was called a "skip chain" because it consisted of a series of PDP-8 "test and skip if flag set" I/O instructions. (It was not unheard-of for a skip chain to reach its end without finding any device in need of service.) The relative interrupt priority of the I/O devices was determined by their position in the skip chain: If several devices interrupted, the device tested earlier in the skip chain would be serviced first.
Books.
An engineering textbook popular in the 1980s, "The Art of Digital Design" by David Winkel and Franklin Prosser, contains an example problem spanning several chapters in which the authors demonstrate the process of designing a computer that is compatible with the PDP-8/I. The function of every component is explained. Although it is not a production design, the exercise provides a detailed description of the computer's operation.

</doc>
<doc id="24365" url="https://en.wikipedia.org/wiki?curid=24365" title="Porsche">
Porsche

Dr.-Ing. h.c. F. Porsche AG, usually shortened to Porsche AG (), is a German automobile manufacturer specializing in high-performance sports cars, SUVs and sedans. Porsche AG is headquartered in Stuttgart, and is owned by Volkswagen AG, which is itself majority-owned by Porsche Automobil Holding SE. Porsche's current lineup includes the 911, Boxster, Cayman, Panamera, Cayenne and the Macan.
History.
Origin.
Ferdinand Porsche founded the company called "Dr. Ing. h. c. F. Porsche GmbH" in 1931, with main offices at Kronenstraße 24 in the centre of Stuttgart. Initially, the company offered motor vehicle development work and consulting, but did not build any cars under its own name. One of the first assignments the new company received was from the German government to design a car for the people, that is a "Volkswagen". This resulted in the Volkswagen Beetle, one of the most successful car designs of all time. The Porsche 64 was developed in 1939 using many components from the Beetle.
During World War II, Volkswagen production turned to the military version of the Volkswagen Beetle, the Kübelwagen, 52,000 produced, and Schwimmwagen, 15,584 produced. Porsche produced several designs for heavy tanks during the war, losing out to Henschel & Son in both contracts that ultimately led to the Tiger I and the Tiger II. However, not all this work was wasted, as the chassis Porsche designed for the Tiger I was used as the base for the Elefant tank destroyer. Porsche also developed the Maus super-heavy tank in the closing stages of the war, producing two prototypes.
At the end of World War II in 1945, the Volkswagen factory at KdF-Stadt fell to the British. Ferdinand lost his position as Chairman of the Board of Management of Volkswagen, and Ivan Hirst, a British Army Major, was put in charge of the factory (in Wolfsburg, the Volkswagen company magazine dubbed him "The British Major who saved Volkswagen"). On 15 December of that year, Ferdinand was arrested for war crimes, but not tried. During his 20-month imprisonment, Ferdinand Porsche's son, Ferry Porsche, decided to build his own car, because he could not find an existing one that he wanted to buy. He also had to steer the company through some of its most difficult days until his father's release in August 1947. The first models of what was to become the 356 were built in a small sawmill in Gmünd, Austria. The prototype car was shown to German auto dealers, and when pre-orders reached a set threshold, production (with Aluminium body) was begun by Porsche Konstruktionen GesmbH founded by Ferry and Louise. Many regard the 356 as the first Porsche simply because it was the first model "sold" by the fledgling company along with Porsche 360. After the production of 356 was taken over by the father's Dr. Ing. h.c. F. Porsche GmbH in Stuttgart in 1950, Porsche commissioned a Zuffenhausen-based company, "Reutter Karosserie", which had previously collaborated with the firm on Volkswagen Beetle prototypes, to produce the 356's steel body. In 1952, Porsche constructed an assembly plant (Werk 2) across the street from "Reutter Karosserie"; the main road in front of Werk 1, the oldest Porsche building, is now known as Porschestrasse. The 356 was road certified in 1948.
Company logo.
Porsche's company logo was based on the coat of arms of the Free People's State of Württemberg of former Weimar Germany, which had Stuttgart as its capital (the same arms were used by Württemberg-Hohenzollern from 1945-1952, while Stuttgart during these years was the capital of adjacent Württemberg-Baden). The arms of Stuttgart was placed in the middle as an inescutcheon, since the cars were made in Stuttgart. The heraldic symbols were combined with the texts "Porsche" and "Stuttgart", which shows that it is not a coat of arms since heraldic achievements never spell out the name of the armiger nor the armigers home town in the shield.
Württemberg-Baden and Württemberg-Hohenzollern became part of the present land of Baden-Württemberg in 1952 after the political consolidation of West Germany in 1949, and the old design of the arms of Württemberg now only lives on in the Porsche logo. On 30 January 1951, not long before the creation of Baden-Württemberg, Ferdinand Porsche died from complications following a stroke.
Developments.
In 1964, after a fair amount of success in motor-racing with various models including the 550 Spyder, and with the 356 needing a major re-design, the company launched the Porsche 911: another air-cooled, rear-engined sports car, this time with a six-cylinder "boxer" engine. The team to lay out the body shell design was led by Ferry Porsche's eldest son, Ferdinand Alexander Porsche (F. A.). The design phase for the 911 caused internal problems with Erwin Komenda, who led the body design department until then. F. A. Porsche complained Komenda made unauthorized changes to the design. Company leader Ferry Porsche took his son's drawings to neighboring chassis manufacturer Reuter. Reuter's workshop was later acquired by Porsche (so-called Werk 2). Afterward Reuter became a seat manufacturer, today known as Keiper-Recaro.
The design office gave sequential numbers to every project (See Porsche type numbers), but the designated 901 nomenclature contravened Peugeot's trademarks on all 'x0x' names, so it was adjusted to 911. Racing models adhered to the "correct" numbering sequence: 904, 906, 908. The 911 has become Porsche's most well-known and iconic model – successful on the race-track, in rallies, and in terms of road car sales. Far more than any other model, the Porsche brand is defined by the 911. It remains in production; however, after several generations of revision, current-model 911s share only the basic mechanical configuration of a rear-engined, six-cylinder coupé, and basic styling cues with the original car. A cost-reduced model with the same body, but with 356-derived four-cylinder engine, was sold as the 912.
In 1972, the company's legal form was changed from "Kommanditgesellschaft" (KG), or limited partnership, to Aktiengesellschaft (AG), or public limited company, because Ferry Porsche came to believe the scale of the company outgrew a "family operation", after learning about Soichiro Honda's "no family members in the company" policy at Honda. This led to the establishment of an Executive Board with members from outside the Porsche family, and a Supervisory Board consisting largely of family members. With this change, most family members in the operation of the company including F. A. Porsche and Ferdinand Piëch departed from the company.
F. A. Porsche founded his own design company, Porsche Design, which is renowned for exclusive sunglasses, watches, furniture, and many other luxury articles. Louise's son and Ferry's nephew Ferdinand Piëch, who was responsible for mechanical development of Porsche's production and racing cars (including the very successful 911, 908 and 917 models), formed his own engineering bureau, and developed a five-cylinder-inline diesel engine for Mercedes-Benz. A short time later he moved to Audi (used to be a division, then a subsidiary, of Volkswagen), and pursued his career through the entire company, ultimately becoming the Chairman of Volkswagen Group.
The first Chief Executive Officer (CEO) of Porsche AG was Dr. Ernst Fuhrmann, who had been working in the company's engine development division. Fuhrmann was responsible for the so-called Fuhrmann-engine, used in the 356 Carrera models as well as the 550 Spyder, having four overhead camshafts instead of a central camshaft with pushrods, as in the Volkswagen-derived serial engines. He planned to cease the 911 during the 1970s, and replace it with the V8-front engined grand sportswagon 928. As we know today, the 911 outlived the 928 by far. Fuhrmann was replaced in the early 1980s by Peter W. Schutz, an American manager and self-proclaimed 911 aficionado. He was then replaced in 1988 by the former manager of German computer company Nixdorf Computer AG, Arno Bohn, who made some costly miscalculations that led to his dismissal soon after, along with that of the development director, Dr. Ulrich Bez, who was formerly responsible for BMW's Z1 model, and is today the CEO of Aston Martin.
In 1990, Porsche drew up a memorandum of understanding with Toyota to learn and benefit from Japanese lean manufacturing methods. In 2004 it was reported that Toyota was assisting Porsche with hybrid technology.
Following the dismissal of Bohn, Heinz Branitzki, a longtime Porsche employee, was appointed as interim CEO. Branitzki served in that position until Wendelin Wiedeking became CEO in 1993. Wiedeking took over the chairmanship of the board at a time when Porsche appeared vulnerable to a takeover by a larger company. During his long tenure, Wiedeking transformed Porsche into a very efficient and profitable company.
Ferdinand Porsche's nephew, Ferdinand Piëch, was chairman and CEO of the Volkswagen Group from 1993 to 2002, and is chairman of the Volkswagen AG Supervisory Board since. With 12.8 percent of the Porsche SE voting shares, he also remains the second largest individual shareholder of Porsche SE after his cousin, F. A. Porsche, (13.6 percent).
Porsche's 2002 introduction of the Cayenne also marked the unveiling of a new production facility in Leipzig, Saxony, which once accounted for nearly half of Porsche's annual output. In 2004, production of the Carrera GT commenced in Leipzig, and at EUR 450,000 ($440,000 in the United States) it was the most expensive production model Porsche ever built.
In mid-2006, after years of the Boxster (and later the Cayenne) as the best selling Porsche in North America, the 911 regained its position as Porsche's best-seller in the region. The Cayenne and 911 have cycled as the top-selling model since. In Germany, the 911 outsells the Boxster/Cayman and Cayenne.
In May 2011, Porsche Cars North America announced plans to spend $80–$100 million, but will receive about $15 million in economic incentives to move their North American headquarters from Sandy Springs, a suburb of Atlanta, to Aerotropolis, Atlanta, a new mixed-use development on the site of the old Ford Hapeville plant adjacent to Atlanta's airport. Designed by architectural firm HOK, the headquarters will include a new office building and test track. The facility will be known by its new address, One Porsche Drive.
Relationship with Volkswagen.
The company has always had a close relationship with, initially, the Volkswagen (VW) marque, and later, the Volkswagen Group (which also owns Audi AG), because the first Volkswagen Beetle was designed by Ferdinand Porsche.
The two companies collaborated in 1969 to make the VW-Porsche 914 and 914-6, whereby the 914-6 had a Porsche engine, and the 914 had a Volkswagen engine, in 1976 with the Porsche 912E (USA only) and the Porsche 924, which used many Audi components, and was built at Audi's Neckarsulm factory. Porsche 944s were also built there, although they used far fewer Volkswagen components. The Cayenne, introduced in 2002, shares its chassis with the Volkswagen Touareg and the Audi Q7, which is built at the Volkswagen Group factory in Bratislava, Slovakia.
Corporate restructuring.
Porsche SE was created in June 2007 by renaming the old Dr. Ing. h.c. F. Porsche AG, and became a holding company for the families' stake in Porsche Zwischenholding GmbH (50.1%) (which in turn held 100% of the old Porsche AG) and Volkswagen AG (50.7%). At the same time, the new Dr. Ing. h.c. F. Porsche AG (Porsche AG) was created for the car manufacturing business.
In August 2009, Porsche SE and Volkswagen AG reached an agreement that the car manufacturing operations of the two companies would merge in 2011, to form an "Integrated Automotive Group". The management of Volkswagen AG agreed to 50.76% of Volkswagen AG being owned by Porsche SE in return for Volkswagen AG management taking Porsche SE management positions (in order for Volkswagen management to remain in control), and for Volkswagen AG acquiring ownership of Porsche AG.
As of the end of 2013, the 50.76% control interest in VW AG is the predominant investment by Porsche SE, and Volkswagen AG in turn controls brands and companies such as Volkswagen, Audi, SEAT, Škoda, Bentley, Bugatti, Lamborghini, Porsche AG, Ducati, VW Commercial Vehicles, Scania, MAN, as well as Volkswagen Financial Services.
Dr. Ing. h.c. F. Porsche AG (which stands for "Doktor Ingenieur honoris causa Ferdinand Porsche Aktiengesellschaft"), as a 100% subsidiary of VW AG, is responsible for the actual production and manufacture of the Porsche automobile line. The company currently produces Porsche 911, Boxster and Cayman sports cars, the Cayenne and Macan sport utility vehicles, the four-door Panamera, and the 918 Spyder super car.
Subsidiaries.
Porsche AG has a 29% share in German engineering and design consultancy Bertrandt AG and 81.8% of Mieschke Hofmann und Partner.
Wholly owned subsidiaries of Porsche AG include Porsche Consulting GmbH.
Production and sales.
The headquarters and main factory are located in Zuffenhausen, a district in Stuttgart, but the Cayenne and Panamera models are manufactured in Leipzig, Germany, and parts for the SUV are also assembled in the Volkswagen Touareg factory in Bratislava, Slovakia. Boxster and Cayman production was outsourced to Valmet Automotive in Finland from 1997 to 2011, and in 2012 production moved to Germany.
In 2008, Porsche reported selling a total of 98,652 cars, 13,524 (13.7%) as domestic German sales, and 85,128 (86.3%) internationally.
The company has been highly successful in recent times, and indeed claims to have the highest profit per unit sold of any car company in the world. Table of profits (in millions of euros) and number of cars produced. Figures from 2008/9 onwards were not reported as part of Porsche SE.
Production composition.
Of the 165,808 cars produced in the 2013 financial year, 29,751 (17.9%) were 911 models, 28,996 (17.5%) were Boxster and Cayman cars, 81,916 (49.4%) were Cayennes, 24,798 (15.0%) were Panameras. There were 312 Macan and 35 918 Spyder models also reported. The production figures of sports cars were quite similar to the 2001/2 totals when 33,061 Porsche 911 and 21,989 Boxsters were produced.
Models.
The current Porsche model range includes sports cars from the Boxster roadster to their most famous product, the 911. The Cayman is a coupé otherwise similar to the Boxster. The Cayenne is Porsche's mid-size luxury sport utility vehicle (SUV). A high performance luxury saloon/sedan, the Panamera, was launched in 2009.
Hybrid and electric vehicles.
In 2010 Porsche launched the Cayenne S Hybrid and announced the Panamera S Hybrid, and launched the Porsche 918 hypercar in 2014, which also features a hybrid system. Also a plug-in hybrid model called the Panamera S E-Hybrid was released in October 2013 in the United States, and during the fourth quarter of 2013 in several European countries.
Porsche developed a prototype electric Porsche Boxster called the Boxster E in 2011 and a hybrid version of the 911 called the GT3 R Hybrid, developed with Williams Grand Prix Engineering in 2010.
In July 2014 Porsche announced the launch by the end of 2014 of the Porsche Cayenne S E-Hybrid a plug-in hybrid, which will displaced the Cayenne S Hybrid from the line up. The S E-Hybrid will be the first plug-in hybrid in the premium SUV segment and will allow Porsche to become the first automaker with three production plug-in hybrid models.
Aircraft engines.
See Porsche PFM 3200.
Motorsport.
Porsche is the most successful brand in motorsport, scoring a total of more than 28,000 victories, including a record 16 constructor wins at the 24 Hours of Le Mans. Porsche is currently the world's largest race car manufacturer. In 2006, Porsche built 195 race cars for various international motor sports events. In 2007, Porsche is expected to construct no fewer than 275 dedicated race cars (7 RS Spyder LMP2 prototypes, 37 GT2 spec 911 GT3-RSRs, and 231 911 GT3 Cup vehicles).
Pronunciation of "Porsche".
In keeping with the family name of founder Ferdinand Porsche, the company's name is pronounced in German, which corresponds to in English, homophonous with the feminine name "Portia". However, in English it is often pronounced as a single syllable —without a final . In German orthography, word-final is not silent but is instead an unstressed schwa.
Reputation.
In a survey conducted by the Luxury Institute in New York, Porsche was awarded the title of "the most prestigious automobile brand". 500 households with a gross annual income of at least $200,000 and a net worth of at least $720,000 participated.
Porsche won the J.D. Power and Associates Initial Quality Study (IQS) in 2006, 2009, 2010, and 2014.
Reliability.
A Canadian study in 2011 revealed that 97.4 percent of Porsches from the last 25 years are still on the road.
In 2014, the Cayman and Boxster made the Consumer Reports list for most reliable vehicles on the road.
Porsche's 911 has been officially named by the Technischer Überwachungsverein (Technical Inspection Association) as Germany's most reliable car.
SUV reception.
According to CNBC, even an at-the-time questionable foray into the SUV market with the Cayenne in 2003, could not damage Porsche credibility. In 2009, "The Times" journalist Andrew Frankel says on one level, it is the world's best 4x4, on another, it is the cynical exploitation of a glorious brand that risks long-term damage to that brand's very identity in the pursuit of easy money with his verdict being "Great car, if only it wasn't a Porsche".
In 2015, US News ranked the Macan as the best luxury compact SUV in its class.

</doc>
<doc id="24366" url="https://en.wikipedia.org/wiki?curid=24366" title="Porsche 924">
Porsche 924

The Porsche 924 is a luxury sports car which was produced by Porsche AG of Germany from 1976 to 1988. A two-door, 2+2 coupé, the 924 replaced the 914 as the company's entry-level model, and was the model that finally retired the 912. In production terms, the 924 was the first Porsche model powered by a water-cooled, front-mounted engine although the similarly configured 928 was designed first. The front-engine, rear wheel drive arrangement was normal for most other manufacturers, but it was unusual for Porsche having previously only used mid- or rear-mounted engines of a boxer configuration, all of which had been air-cooled. It was the first Porsche to be offered with a fully automatic transmission.
The first official appearance of the 924 took place in November 1975 (as a press launch rather than a motorshow appearance) at the harbour at La Grande Motte, Camargue in the south of France. The model was a qualified success with just over 150,000 produced (from 1977-1988), and it helped to take Porsche out of financial ruin. The 924 was meant to be replaced by the closely related 944 in 1983 in the U.S. market, but it returned in 1986 and continued to be produced until 1988.
For the 1986 to 1988 model years, the car acquired the powerplant from the 944 model and became the Porsche 924S.
History.
The 924 was originally a joint project of Volkswagen and Porsche created by the Vertriebsgesellschaft (VG), the joint sales and marketing company funded by Porsche and VW to market and sell sports cars (Ludvigsen: "Porsche, Excellence was Expected"). For Volkswagen, it was intended to be that company's flagship coupé sports car and was dubbed "Project 425" during its development. For Porsche, it was to be its entry-level sports car replacing the 914. At the time, Volkswagen lacked a significant internal research and design division for developing sports cars; further, Porsche had been doing the bulk of the company's development work anyway, per a deal that went back to the 1940s. In keeping with this history, Porsche was contracted to develop a new sporting vehicle with the caveat that this vehicle must work with an existing VW/Audi inline-four engine. Porsche chose a rear-wheel drive layout and a rear-mounted transaxle for the design to help provide 48/52 front/rear weight distribution; this slight rear weight bias aided both traction and brake balance.
The 1973 oil crisis, a series of automobile-related regulatory changes enacted during the 1970s and a change of directors at Volkswagen made the case for a Volkswagen sports car less striking and the 425 project was put on hold. After serious deliberation at VW, the project was scrapped entirely after a decision was made to move forward with the cheaper, more practical, Golf-based Scirocco model instead. Porsche, which needed a model to replace the 914, made a deal with Volkswagen leadership to buy the design back.
The deal specified that the car would be built at the ex-NSU factory in Neckarsulm located north of the Porsche headquarters in Stuttgart, Volkswagen becoming the subcontractor. Hence, Volkswagen employees would do the actual production line work (supervised by Porsche's own production specialists) and that Porsche would own the design. It became one of Porsche's best-selling models, and the relative cheapness of building the car made it both profitable and fairly easy for Porsche to finance.
The original design used an Audi-sourced four-speed manual transmission from a front wheel drive car but now placed and used as a rear transaxle. It was mated to VW's EA831 2.0 L I4 engine, subsequently used in the Audi 100 and the Volkswagen LT van (common belief is that 'the engine originated in the LT van', but it first appeared in the Audi car and in 924 form has a Porsche-designed cylinder head). The Audi engine, equipped with a Weber/Holley carburetor, was optional in the 1977-1979 AMC Gremlin, Concord, and Spirit. The 924 engine used Bosch K-Jetronic fuel injection, producing in North American trim. This was brought up to in mid-1977 with the introduction of a catalytic converter, which reduced the need for power-robbing smog equipment. The four-speed manual was the only transmission available for the initial 1976 model, later this was replaced by a five-speed dog-leg unit. An Audi three-speed automatic was offered starting with the 1977.5 model. In 1980 the five-speed transmission was changed to a conventional H-pattern, with reverse now on the right beneath fifth gear.
In 1980, the model received some minor fettling including a three-way catalyst and slightly higher compression, which brought power up to . Nonetheless, the strong Deutschemark and US inflation severely hampered sales, as a well equipped 924 now easily could cost twice as much as the considerably more powerful Nissan 280ZX.
European models, which did not require any emissions equipment, made . They also differed visually from the US spec model by not having the US cars' low-speed impact bumpers and the round reflectors plus side-marker lamps on each end of the body.
The 924 was sold in Japan at Yanase dealerships that specialize in North American and European vehicles, with right hand drive for its entire generation. Sales were helped by the fact that it was in compliance with Japanese Government dimension regulations with regards to its engine displacement and exterior dimensions.
A five-speed transmission, available in normally aspirated cars (type 016) starting in 1979 and standard on all turbos (type G31), was a dog-leg shift pattern Porsche unit, with first gear below reverse on the left side. This was robust, but expensive due to some 915 internal parts, and was replaced for 1980 with a normal H-pattern Audi five-speed on all non-turbo cars. This lighter duty design was originally not used on the more powerful 924 Turbo. The brakes were solid discs at the front and drums at the rear. The car was criticized in "Car and Driver" magazine for this braking arrangement, which was viewed as a step backward from the 914's standard four-wheel disc brakes. However, four-wheel disc brakes, five stud hubs and alloys from the 924 Turbo were available on the base 924 as an "S" package starting with the 1980 model year. Also, standard brakes could be optioned on the turbo as a cost-saving measure.
The overall styling was created by Dutchman Harm Lagaay, a member of the Porsche styling team, with the folding headlights, sloping bonnet line and grille-less nose giving the car its popular wedge shape. The car went on sale in the USA in July 1976 as a 1977 model with a base price of $9,395. Porsche made small improvements to the 924 each model year between 1977 and 1985, but nothing major was changed on non-turbo cars. Turbo charged variants received many different, non-VW sourced parts, throughout the drive train, and when optioned with the M471 disc brake package and forged 16" wheels, the car was twice as expensive as a standard model. Its appearance has been credited as the inspiration for the second generation Mazda RX-7.
J. Pasha, writing in "Excellence" magazine, at the time, described the 924 as "the best handling Porsche in stock form".
While the car was praised for its styling, handling, fuel economy, and reliability, it was harshly written up in the automotive press for its very poor performance, especially with the US spec cars. With only 95-110 hp, rapid acceleration was simply not an option, but the Porsche name carried with it higher expectations. When the 924 turbo models came out, "Car and Driver" magazine proclaimed the car "Fast...at Last!" The later 924S had performance on par with the turbo, but with much improved reliability, and at less cost. The '81 and '82 Turbos and the associated special variants are garnering interest in collector circles; and while many still exist, excellent examples of the cars are quite scarce as of 2015.
Production.
924.
† includes 1002 special edition "Le Mans" cars
<br>
‡ includes 1015 special edition "50 Jahre Porsche/Weissach" cars.
924 Turbo (931 LHD, 932 RHD).
† For year 1979 porsche 924 turbo. Too few if them got made and due to the age of the vehicle they became very very rare. At 2009 there were only less than 10 right hand drive 1979 porsche 924 turbo S1 reported world wide.
^ cars brought only into Italy
924S (946 LHD, 947 RHD).
There was also a sport package for the 924S, available for the ROW and U.S. market for which production data is stated below.
924 Turbo.
Porsche executives soon recognized the need for a higher-performance version of the 924 that could bridge the gap between the basic 924 and the 911s. Having already found the benefits of turbochargers on several race cars and the 1975 911 Turbo, Porsche chose to use this technology for the 924, eventually introducing the 924 Turbo as a 1978 model.
Porsche started with the same Audi-sourced VW EA831 2.0 L I4, designed an all new cylinder head (which was hand assembled at Stuttgart), dropped the compression to 7.5:1 and engineered a KKK K-26 turbocharger for it. With boost, output increased to . The 924 Turbo engine assembly weighed about more, so front spring rates and anti-roll bars were revised. Weight distribution was now 49/51 compared to the original 924 figure of 48/52 front to rear.
In order to help make the car more functional, as well as to distinguish it from the naturally aspirated version, Porsche added a NACA duct in the hood and air intakes in the badge panel in the nose, 15-inch spoke-style alloy wheels, four-wheel disc brakes with 5 stud hubs and a five-speed transmission. Forged 16" flat wheels of the style used on the 928 were optional, but fitment specification was that of the 911 which the 924 shared wheel offsets with. Internally, Porsche called it 931 (left hand drive) and 932 (right hand drive), much like the 911 Carrera Turbo, which had been "Type 930". These designations are commonly used by 924 aficionados.
The turbocharged VW EA831 engine allowed the 924's performance to come surprisingly close to that of the 911 SC (180 bhp), thanks in part to a lighter curb weight, but it also brought reliability problems. This was in part due to the fact that the general public did not know how to operate, or care for, what is by today's standards a primitive turbo setup.
A turbocharger cooled only by engine oil led to short component life and turbo-related seal and seat problems. To fix the problems, Porsche released a revised 924 Turbo series 2 (although badging still read 924 Turbo) in 1979. By using a smaller turbocharger running at increased boost, slightly higher compression of 8:1 and an improved fuel injection system with DITC ignition triggered by the flywheel, reliability improved and power rose to .
In North America, the 924 Turbo arrived in late 1979 for the 1980 model year. It was saddled with extra weight, due to the federally mandated large bumpers and other safety equipment, and less power due to stringent emissions controls. Power was , nearly twenty percent down on the European model. For the 1981 model year, power increased slightly to and the transmission was switched to one with a regular H-pattern layout.
Carrera GT.
After a successful sales run of both naturally aspirated and turbo models in 1980, Porsche surprised everyone and release the 924 Carrera GT, making clear their intention to enter the 924 in competition. By adding an intercooler, increasing compression to 8.5:1 as well as various other little changes, Porsche was able to develop the 924 Turbo into the race car they had wanted, dubbing it the 924 Carrera GT.
Visually it differed to the 931 in that it had polyurethane plastic front and rear flared guards, a polyurethane plastic front spoiler, a top mounted air scoop for the intercooler, a much larger rubber rear spoiler and a flush mounted front windscreen. It lost the 931's NACA duct in the hood but retained the air intakes in the badge panel. This more aggressive styling was later used for as motivation for the 944.
In order to comply with the homologation regulations, the 924 Carrera GT and later 924 Carrera GTS were offered as road cars as well, producing 210 and 245 hp (157 and 183 kW) respectively. Clubsport versions of the GTS were also available with , and factory included Matter rollcage and race seats. 924 Carrera GT variations were known by model numbers 937 (left hand drive) and 938 (right hand drive).
The ultimate development of the 924 in its race trim was the 924 Carrera GTR race car, which produced from a highly modified version of the 2.0 L I4 used in all 924s. In 1980 Porsche entered three 924 GTRs at the 24hrs of Le Mans, which went on to finish 6th, 12th and 13th overall. Also building a 924GTR Rally race car, and 2 other GTR's (Miller & BF Goodrich).
Lastly, in 1981, Porsche entered 1 of 2 specially built 924 Carrera GTP's (Aka: 944GTP Le Mans) which Porsche Motorsports introduced a new prototype highly modified 2.5 liter I4 engine. This engine sported 4 valves per cylinder, dual over head camshafts, twin balance shafts and a single turbocharger K28 to produce . This last variant managed a 7th place overall finish and spent the least time out of any other car in the pits. This new 2.5 liter configuration engine is the predecessor of the 944 platform(s) and the later quick 1987-88 944S 16V M44/40 power-plant.
Production of the 924 Turbo ceased in 1982 except for the Italian market which lasted until 1984. This is due to the restrictions on engines larger than 2 liters, putting the forthcoming 2.5 liter 944 into a much higher tax category.
924S.
In 1984, VW decided to stop manufacturing the engine blocks used in the 2.0 924, leaving Porsche with a predicament. The 924 was considerably cheaper than its 944 stablemate, and dropping the model left Porsche without an affordable entry-level option. The decision was made to equip the narrower bodied 924 with a slightly detuned version of the 944's 163 bhp 2.5 litre straight four, upgrading the suspension but retaining the 924's early interior. The result was 1986's 150 bhp 924S. Porsche also decided to re-introduce the 924 to the American market with an initial price tag of under $20,000.
In 1988, the 924S' final year of production, power increased to matching that of the previous year's Le Mans spec cars and the base model 944 (itself detuned by for 1988). This was achieved using different pistons which raised the S' compression ratio from 9.7:1 to 10.2:1, the knock-on effect being an increase in the octane rating, up from 91 RON to 95. This made the 924S slightly faster than the base 944 due to its lighter weight and more aerodynamic body.
With unfavourable exchange rates in the late 1980s, Porsche decided to focus its efforts on its more upmarket models, dropping the 924S for 1989 and the base 944 later that same year.
924S Special models.
The 1988 924S SE (USA) and "Le Mans" (ROW) were Club Sport editions aimed at autocross (US term for autotests to UK readers) and club racers. 
The final 924S RHD 'run-out' versions in 1988 for the UK (just 37 white and 37 black vehicles) had "Le Mans" logos with stripes on their flanks. Officially known at Porsche as the "Sportliches Sondermodell" (loosely translates as Sporting Special Model) their options package list M-755 was more complete than the Special Edition M-756 for the USA.
Only 980 Club Sport option cars were built in total.
500 units M-756 for USA black only, 
250 GER 200 black and 50 white cars, 
230 ROW 113 black and 117 white; totalling 480 units M-755.
ROW "Le Mans" Edition M-755:<br>
Only on the final 74, GB supplied, RHD cars were the exterior side stripes broken by scripted ‘Le Mans’ logos on the lower part of the door, while the rims of the holes in each wheel were either in the Ochre (white cars) or Turquoise (black cars). Inside, all the cars featured cloth-upholstered "Turbo" sports seats, with the cloth door panels also colour-coded. They had the 360 mm (14 in) steering wheel and all the 74 British M-755 cars came with a 160 BHP engine plus an electric tilt/removable sunroof fitted as standard. They were lowered 10 mm (0.39 in) at the front and 15 mm (0.59 in) at the rear, and fitted with stiffer springs and gas-filled shock absorbers all round. They also had 'Sport' anti-roll bars with diameters of 21.5 mm (0.85 in) at the front but 20 mm (0.79 in), (rather than 14 mm (0.55 in)), at the rear. Wheels were ‘telephone dial’ cast alloy 6J x 15s at the front and 7J x 15s (at the rear).
ROW M-755 Paint finishes and interiors were also only offered in two colour choices - Alpine White with Ochre/Grey detailing and upholstery - or Black with Turquoise detailing and grey/turquoise upholstery. 
On ROW cars there was no Le Mans logo, nor striping and the phone dial wheels in white or black matching color had outer rims of respectively ochre or turquoise. ROW Upholstery was the grey/ochre striped flannel cloth with ochre piping for Alpine White cars, or grey/turquoise flannel with turquoise piping for Black cars.
US market SE:<br>
Black only paint scheme with optional SE Edition decal. Equipped with manual steering, manual windows and door locks, sunroof delete, radio delete, AC delete, cruise delete, passenger side door mirror delete, wider 15x7 phone dial alloys for the rear while retaining 15x6 in front, and the M030 package which included stiffer springs and Koni shocks. The cars had a unique lightweight gray knit cloth upholstery (which deteriorated very quickly) with maroon pinstriping, and maroon carpeting. The sunroof, A/C, cruise, power steering, passenger door mirror, and radio could be added back optionally.
The Porsche 924 and motorsport.
The 924 has its own racing series in the UK run by the BRSCC and Porsche Racing Drivers Association. The Porsche 924 Championship was started in 1992 by Jeff May who was championship coordinator until his death on 10 November 2003. Jeff May was also one of the founding members of Porsche Club Great Britain.
In the United States, the 924S is also eligible to race in the 944-Spec racing class.

</doc>
<doc id="24373" url="https://en.wikipedia.org/wiki?curid=24373" title="Pain">
Pain

Pain is a distressing feeling often caused by intense or damaging stimuli, such as stubbing a toe, burning a finger, putting alcohol on a cut, and bumping the "funny bone". Because it is a complex, subjective phenomenon, defining pain has been a challenge. The International Association for the Study of Pain's widely used definition states: "Pain is an unpleasant sensory and emotional experience associated with actual or potential tissue damage, or described in terms of such damage." In medical diagnosis, pain is a symptom.
Pain motivates the individual to withdraw from damaging situations, to protect a damaged body part while it heals, and to avoid similar experiences in the future. Most pain resolves once the noxious stimulus is removed and the body has healed, but it may persist despite removal of the stimulus and apparent healing of the body. Sometimes pain arises in the absence of any detectable stimulus, damage or disease. Simple pain medications are useful in 20% to 70% of cases.
Pain is the most common reason for physician consultation in most developed countries. It is a major symptom in many medical conditions, and can interfere with a person's quality of life and general functioning. Psychological factors such as social support, hypnotic suggestion, excitement, or distraction can significantly affect pain's intensity or unpleasantness. In some arguments put forth in physician-assisted suicide or euthanasia debates, pain has been used as an argument to permit terminally ill patients to end their lives.
Classification.
In 1994, responding to the need for a more useful system for describing chronic pain, the International Association for the Study of Pain (IASP) classified pain according to specific characteristics: (1) region of the body involved (e.g. abdomen, lower limbs), (2) system whose dysfunction may be causing the pain (e.g., nervous, gastrointestinal), (3) duration and pattern of occurrence, (4) intensity and time since onset, and (5) etiology. However, this system has been criticized by Clifford J. Woolf and others as inadequate for guiding research and treatment. Woolf suggests three classes of pain : (1) nociceptive pain, (2) inflammatory pain which is associated with tissue damage and the infiltration of immune cells, and (3) pathological pain which is a disease state caused by damage to the nervous system or by its abnormal function (e.g. fibromyalgia, irritable bowel syndrome, tension type headache, etc.).
Duration.
Pain is usually transitory, lasting only until the noxious stimulus is removed or the underlying damage or pathology has healed, but some painful conditions, such as rheumatoid arthritis, peripheral neuropathy, cancer and idiopathic pain, may persist for years. Pain that lasts a long time is called "chronic" or persistent, and pain that resolves quickly is called "acute". Traditionally, the distinction between "acute" and "chronic" pain has relied upon an arbitrary interval of time from onset; the two most commonly used markers being 3 months and 6 months since the onset of pain, though some theorists and researchers have placed the transition from acute to chronic pain at 12 months. Others apply "acute" to pain that lasts less than 30 days, "chronic" to pain of more than six months' duration, and "subacute" to pain that lasts from one to six months. A popular alternative definition of "chronic pain", involving no arbitrarily fixed durations, is "pain that extends beyond the expected period of healing". Chronic pain may be classified as cancer pain or else as benign.
Nociceptive.
Nociceptive pain is caused by stimulation of peripheral nerve fibers that respond to stimuli approaching or exceeding harmful intensity (nociceptors), and may be classified according to the mode of noxious stimulation. The most common categories are "thermal" (e.g. heat or cold), "mechanical" (e.g. crushing, tearing, shearing, etc.) and "chemical" (e.g. iodine in a cut or chemicals released during inflammation). Some nociceptors respond to more than one of these modalities and are consequently designated polymodal.
Nociceptive pain may also be divided into "visceral", "deep somatic" and "superficial somatic" pain. Visceral structures are highly sensitive to stretch, ischemia and inflammation, but relatively insensitive to other stimuli that normally evoke pain in other structures, such as burning and cutting. "Visceral pain" is diffuse, difficult to locate and often referred to a distant, usually superficial, structure. It may be accompanied by nausea and vomiting and may be described as sickening, deep, squeezing, and dull. "Deep somatic" pain is initiated by stimulation of nociceptors in ligaments, tendons, bones, blood vessels, fasciae and muscles, and is dull, aching, poorly-localized pain. Examples include sprains and broken bones. "Superficial" pain is initiated by activation of nociceptors in the skin or other superficial tissue, and is sharp, well-defined and clearly located. Examples of injuries that produce superficial somatic pain include minor wounds and minor (first degree) burns.
Neuropathic.
Neuropathic pain is caused by damage or disease affecting any part of the nervous system involved in bodily feelings (the somatosensory system). Peripheral neuropathic pain is often described as "burning", "tingling", "electrical", "stabbing", or "pins and needles". Bumping the "funny bone" elicits acute peripheral neuropathic pain.
Phantom.
Phantom pain is pain felt in a part of the body that has been lost or from which the brain no longer receives signals. It is a type of neuropathic pain. Phantom limb pain is a common experience of amputees.
The prevalence of phantom pain in upper limb amputees is nearly 82%, and in lower limb amputees is 54%. One study found that eight days after amputation, 72 percent of patients had phantom limb pain, and six months later, 65 percent reported it. Some amputees experience continuous pain that varies in intensity or quality; others experience several bouts a day, or it may occur only once every week or two. It is often described as shooting, crushing, burning or cramping. If the pain is continuous for a long period, parts of the intact body may become sensitized, so that touching them evokes pain in the phantom limb, or phantom limb pain may accompany urination or defecation.
Local anesthetic injections into the nerves or sensitive areas of the stump may relieve pain for days, weeks, or sometimes permanently, despite the drug wearing off in a matter of hours; and small injections of hypertonic saline into the soft tissue between vertebrae produces local pain that radiates into the phantom limb for ten minutes or so and may be followed by hours, weeks or even longer of partial or total relief from phantom pain. Vigorous vibration or electrical stimulation of the stump, or current from electrodes surgically implanted onto the spinal cord, all produce relief in some patients.
Work by Vilayanur S. Ramachandran using mirror box therapy allows for illusions of movement and touch in a phantom limb which in turn cause a reduction in pain.
Paraplegia, the loss of sensation and voluntary motor control after serious spinal cord damage, may be accompanied by girdle pain at the level of the spinal cord damage, visceral pain evoked by a filling bladder or bowel, or, in five to ten per cent of paraplegics, phantom body pain in areas of complete sensory loss. This phantom body pain is initially described as burning or tingling but may evolve into severe crushing or pinching pain, or the sensation of fire running down the legs or of a knife twisting in the flesh. Onset may be immediate or may not occur until years after the disabling injury. Surgical treatment rarely provides lasting relief.
Psychogenic.
Psychogenic pain, also called "psychalgia" or "somatoform pain", is pain caused, increased, or prolonged by mental, emotional, or behavioral factors. Headache, back pain, and stomach pain are sometimes diagnosed as psychogenic. Sufferers are often stigmatized, because both medical professionals and the general public tend to think that pain from a psychological source is not "real". However, specialists consider that it is no less actual or hurtful than pain from any other source.
People with long-term pain frequently display psychological disturbance, with elevated scores on the Minnesota Multiphasic Personality Inventory scales of hysteria, depression and hypochondriasis (the "neurotic triad"). Some investigators have argued that it is this neuroticism that causes acute pain to turn chronic, but clinical evidence points the other way, to chronic pain causing neuroticism. When long-term pain is relieved by therapeutic intervention, scores on the neurotic triad and anxiety fall, often to normal levels. Self-esteem, often low in chronic pain patients, also shows improvement once pain has resolved.
Breakthrough pain.
Breakthrough pain is transitory acute pain that comes on suddenly and is not alleviated by the patient's normal pain management. It is common in cancer patients who often have background pain that is generally well-controlled by medications, but who also sometimes experience bouts of severe pain that from time to time "breaks through" the medication. The characteristics of breakthrough cancer pain vary from person to person and according to the cause. Management of breakthrough pain can entail intensive use of opioids, including fentanyl.
Incident pain.
Incident pain is pain that arises as a result of activity, such as movement of an arthritic joint, stretching a wound, etc.
Pain asymbolia and insensitivity.
The ability to experience pain is essential for protection from injury, and recognition of the presence of injury. Episodic analgesia may occur under special circumstances, such as in the excitement of sport or war: a soldier on the battlefield may feel no pain for many hours from a traumatic amputation or other severe injury.
Although unpleasantness is an essential part of the IASP definition of pain, it is possible to induce a state described as intense pain devoid of unpleasantness in some patients, with morphine injection or psychosurgery. Such patients report that they have pain but are not bothered by it; they recognize the sensation of pain but suffer little, or not at all. Indifference to pain can also rarely be present from birth; these people have normal nerves on medical investigations, and find pain unpleasant, but do not avoid repetition of the pain stimulus.
Insensitivity to pain may also result from abnormalities in the nervous system. This is usually the result of acquired damage to the nerves, such as spinal cord injury, diabetes mellitus (diabetic neuropathy), or leprosy in countries where that disease is prevalent. These individuals are at risk of tissue damage and infection due to undiscovered injuries. People with diabetes-related nerve damage, for instance, sustain poorly-healing foot ulcers as a result of decreased sensation.
A much smaller number of people are insensitive to pain due to an inborn abnormality of the nervous system, known as "congenital insensitivity to pain". Children with this condition incur carelessly-repeated damage to their tongues, eyes, joints, skin, and muscles. Some die before adulthood, and others have a reduced life expectancy. Most people with congenital insensitivity to pain have one of five hereditary sensory and autonomic neuropathies (which includes familial dysautonomia and congenital insensitivity to pain with anhidrosis). These conditions feature decreased sensitivity to pain together with other neurological abnormalities, particularly of the autonomic nervous system. A very rare syndrome with isolated congenital insensitivity to pain has been linked with mutations in the "SCN9A" gene, which codes for a sodium channel (Nav1.7) necessary in conducting pain nerve stimuli.
Effect on functioning.
Experimental subjects challenged by acute pain and patients in chronic pain experience impairments in attention control, working memory, mental flexibility, problem solving, and information processing speed. Acute and chronic pain are also associated with increased depression, anxiety, fear, and anger.
Theory.
Historical theories.
Before the relatively recent discovery of neurons and their role in pain, various different body functions were proposed to account for pain. There were several competing early theories of pain among the ancient Greeks: Hippocrates believed that it was due to an imbalance in vital fluids. In the 11th century, Avicenna theorized that there were a number of feeling senses including touch, pain and titillation.
In 1644, René Descartes theorized that pain was a disturbance that passed down along nerve fibers until the disturbance reached the brain, a development that transformed the perception of pain from a spiritual, mystical experience to a physical, mechanical sensation . Descartes's work, along with Avicenna's, prefigured the 19th-century development of specificity theory. Specificity theory saw pain as "a specific sensation, with its own sensory apparatus independent of touch and other senses". Another theory that came to prominence in the 18th and 19th centuries was intensive theory, which conceived of pain not as a unique sensory modality, but an emotional state produced by stronger than normal stimuli such as intense light, pressure or temperature. By the mid-1890s, specificity was backed mostly by physiologists and physicians, and the intensive theory was mostly backed by psychologists. However, after a series of clinical observations by Henry Head and experiments by Max von Frey, the psychologists migrated to specificity almost en masse, and by century's end, most textbooks on physiology and psychology were presenting pain specificity as fact.
In 1955, DC Sinclair and G Weddell developed peripheral pattern theory, based on a 1934 suggestion by John Paul Nafe. They proposed that all skin fiber endings (with the exception of those innervating hair cells) are identical, and that pain is produced by intense stimulation of these fibers. Another 20th-century theory was gate control theory, introduced by Ronald Melzack and Patrick Wall in the 1965 "Science" article "Pain Mechanisms: A New Theory". The authors proposed that both thin (pain) and large diameter (touch, pressure, vibration) nerve fibers carry information from the site of injury to two destinations in the dorsal horn of the spinal cord, and that the more large fiber activity relative to thin fiber activity at the inhibitory cell, the less pain is felt. Both peripheral pattern theory and gate control theory have been superseded by more modern theories of pain.
Three dimensions of pain.
In 1968 Ronald Melzack and Kenneth Casey described pain in terms of its three dimensions: "sensory-discriminative" (sense of the intensity, location, quality and duration of the pain), "affective-motivational" (unpleasantness and urge to escape the unpleasantness), and "cognitive-evaluative" (cognitions such as appraisal, cultural values, distraction and hypnotic suggestion). They theorized that pain intensity (the sensory discriminative dimension) and unpleasantness (the affective-motivational dimension) are not simply determined by the magnitude of the painful stimulus, but "higher" cognitive activities can influence perceived intensity and unpleasantness. Cognitive activities "may affect both sensory and affective experience or they may modify primarily the affective-motivational dimension. Thus, excitement in games or war appears to block both dimensions of pain, while suggestion and placebos may modulate the affective-motivational dimension and leave the sensory-discriminative dimension relatively undisturbed." (p. 432) The paper ends with a call to action: "Pain can be treated not only by trying to cut down the sensory input by anesthetic block, surgical intervention and the like, but also by influencing the motivational-affective and cognitive factors as well." (p. 435)
Theory today.
The pain signal travels from the periphery to the spinal cord along an A-delta or C fiber. Because the A-delta fiber is thicker than the C fiber, and is thinly sheathed in an electrically insulating material (myelin), it carries its signal faster (5–30 m/s) than the unmyelinated C fiber (0.5–2 m/s). Pain evoked by the (faster) A-delta fibers is described as sharp and is felt first. This is followed by a duller pain, often described as burning, carried by the C fibers. These first order neurons enter the spinal cord via Lissauer's tract.
A-delta and C fibers synapse on second order neurons in substantia gelatinosa (laminae II and III of the dorsal horns). These second order fibers then cross the cord via the anterior white commissure and ascend in the spinothalamic tract. Before reaching the brain, the spinothalamic tract splits into the lateral neospinothalamic tract and the medial paleospinothalamic tract.
Second order neospinothalamic tract neurons carry information from A-delta fibers and terminate at the ventral posterolateral nucleus of the thalamus, where they synapse on third order neurons (dendrites of the somatosensory cortex). Paleospinothalamic neurons carry information from C fibers and terminate throughout the brain stem, a tenth of them in the thalamus and the rest in the medulla, pons and periaqueductal gray matter.
Spinal cord fibers dedicated to carrying A-delta fiber pain signals, and others that carry both A-delta and C fiber pain signals up the spinal cord to the thalamus in the brain have been identified. Other spinal cord fibers, known as wide dynamic range neurons, respond to A-delta and C fibers, but also to the large A-beta fibers that carry touch, pressure and vibration signals. Pain-related activity in the thalamus spreads to the insular cortex (thought to embody, among other things, the feeling that distinguishes pain from other homeostatic emotions such as itch and nausea) and anterior cingulate cortex (thought to embody, among other things, the motivational element of pain); and pain that is distinctly located also activates the primary and secondary somatosensory cortices. Melzack and Casey's 1968 picture of the dimensions of pain is as influential today as ever, firmly framing theory and guiding research in the functional neuroanatomy and psychology of pain.
A. D. (Bud) Craig and Derek Denton include pain in a class of feelings they name, respectively, "homeostatic" or "primordial" emotions. These are feelings such as hunger, thirst and fatigue, evoked by internal body states, which motivate behavior aimed at maintaining the body in its ideal state. Craig and Denton distinguish these feelings from the "classical emotions" such as love, fear and anger, which are elicited by environmental stimuli sensed through the nose, eyes and ears.
Evolutionary and behavioral role.
Pain is part of the body's defense system, producing a reflexive retraction from the painful stimulus, and tendencies to protect the affected body part while it heals, and avoid that harmful situation in the future. It is an important part of animal life, vital to healthy survival. People with congenital insensitivity to pain have reduced life expectancy.
In his book, "", biologist Richard Dawkins grapples with the question of why pain has to be so very painful. He describes the alternative as a simple, mental raising of a "red flag". To argue why that red flag might be insufficient, Dawkins explains that drives must compete with each other within living beings. The most fit creature would be the one whose pains are well balanced. Those pains which mean certain death when ignored will become the most powerfully felt. The relative intensities of pain, then, may resemble the relative importance of that risk to our ancestors (lack of food, too much cold, or serious injuries are felt as agony, whereas minor damage is felt as mere discomfort). This resemblance will not be perfect, however, because natural selection can be a poor designer. The result is often glitches in animals, including supernormal stimuli. Such glitches help explain pains which are not, or at least no longer directly adaptive (e.g. perhaps some forms of toothache, or injury to fingernails).
Idiopathic pain (pain that persists after the trauma or pathology has healed, or that arises without any apparent cause), may be an exception to the idea that pain is helpful to survival, although some psychodynamic psychologists argue that such pain is psychogenic, enlisted as a protective distraction to keep dangerous emotions unconscious.
Thresholds.
In pain science, thresholds are measured by gradually increasing the intensity of a stimulus such as electric current or heat applied to the body. The pain perception threshold is the point at which the stimulus begins to hurt, and the pain tolerance threshold is reached when the subject acts to stop the pain.
Differences in pain perception and tolerance thresholds are associated with, among other factors, ethnicity, genetics, and sex. People of Mediterranean origin report as painful some radiant heat intensities that northern Europeans describe as nonpainful. And Italian women tolerate less intense electric shock than Jewish or Native American women. Some individuals in all cultures have significantly higher than normal pain perception and tolerance thresholds. For instance, patients who experience painless heart attacks have higher pain thresholds for electric shock, muscle cramp and heat.
Assessment.
A person's self-report is the most reliable measure of pain, with health care professionals tending to underestimate severity. A definition of pain widely employed in nursing, emphasizing its subjective nature and the importance of believing patient reports, was introduced by Margo McCaffery in 1968: "Pain is whatever the experiencing person says it is, existing whenever he says it does".More recently, McCaffery defined pain as "whatever the experiencing person says it is, existing whenever the experiencing person says it does." </ref> To assess intensity, the patient may be asked to locate their pain on a scale of 0 to 10, with 0 being no pain at all, and 10 the worst pain they have ever felt. Quality can be established by having the patient complete the McGill Pain Questionnaire indicating which words best describe their pain.
Multidimensional pain inventory.
The Multidimensional Pain Inventory (MPI) is a questionnaire designed to assess the psychosocial state of a person with chronic pain. Analysis of MPI results by Turk and Rudy (1988) found three classes of chronic pain patient: "(a) dysfunctional, people who perceived the severity of their pain to be high, reported that pain interfered with much of their lives, reported a higher degree of psychological distress caused by pain, and reported low levels of activity; (b) interpersonally distressed, people with a common perception that significant others were not very supportive of their pain problems; and (c) adaptive copers, patients who reported high levels of social support, relatively low levels of pain and perceived interference, and relatively high levels of activity." Combining the MPI characterization of the person with their IASP five-category pain profile is recommended for deriving the most useful case description.
People who are non-verbal.
When a person is non-verbal and cannot self-report pain, observation becomes critical, and specific behaviors can be monitored as pain indicators. Behaviors such as facial grimacing and guarding indicate pain, as well as an increase or decrease in vocalizations, changes in routine behavior patterns and mental status changes. Patients experiencing pain may exhibit withdrawn social behavior and possibly experience a decreased appetite and decreased nutritional intake. A change in condition that deviates from baseline such as moaning with movement or when manipulating a body part, and limited range of motion are also potential pain indicators. In patients who possess language but are incapable of expressing themselves effectively, such as those with dementia, an increase in confusion or display of aggressive behaviors or agitation may signal that discomfort exists, and further assessment is necessary.
Infants feel pain but they lack the language needed to report it, so communicate distress by crying. A non-verbal pain assessment should be conducted involving the parents, who will notice changes in the infant not obvious to the health care provider. Pre-term babies are more sensitive to painful stimuli than full term babies.
Other barriers to reporting.
The experience of pain has many cultural dimensions. For instance, the way in which one experiences and responds to pain is related to sociocultural characteristics, such as gender, ethnicity, and age. An aging adult may not respond to pain in the way that a younger person would. Their ability to recognize pain may be blunted by illness or the use of multiple prescription drugs. Depression may also keep the older adult from reporting they are in pain. The older adult may also quit doing activities they love because it hurts too much. Decline in self-care activities (dressing, grooming, walking, etc.) may also be indicators that the older adult is experiencing pain. The older adult may refrain from reporting pain because they are afraid they will have to have surgery or will be put on a drug they might become addicted to. They may not want others to see them as weak, or may feel there is something impolite or shameful in complaining about pain, or they may feel the pain is deserved punishment for past transgressions.
Cultural barriers can also keep a person from telling someone they are in pain. Religious beliefs may prevent the individual from seeking help. They may feel certain pain treatment is against their religion. They may not report pain because they feel it is a sign that death is near. Many people fear the stigma of addiction and avoid pain treatment so as not to be prescribed potentially addicting drugs. Many Asians do not want to lose respect in society by admitting they are in pain and need help, believing the pain should be borne in silence, while other cultures feel they should report pain right away and get immediate relief. Gender can also be a factor in reporting pain. Sexual differences can be the result of social and cultural expectations, with women expected to be emotional and show pain and men stoic, keeping pain to themselves.
As an aid to diagnosis.
Pain is a symptom of many medical conditions. Knowing the time of onset, location, intensity, pattern of occurrence (continuous, intermittent, etc.), exacerbating and relieving factors, and quality (burning, sharp, etc.) of the pain will help the examining physician to accurately diagnose the problem. For example, chest pain described as extreme heaviness may indicate myocardial infarction, while chest pain described as tearing may indicate aortic dissection.
Physiological measurement of pain.
fMRI brain scanning has been used to measure pain, giving good correlations with self-reported pain.
Hedonic adaptation.
Hedonic adaptation means that actual long-term suffering due to physical illness is often much lower than expected.
Legal awards for pain and suffering.
One area where assessments of pain are effectively required to be made is in legal awards for pain and suffering. In the Western world these are typically discretionary awards made by juries and are regarded as difficult to predict, variable and subjective, for instance in the US, UK, Australia and New Zealand.
Management.
Inadequate treatment of pain is widespread throughout surgical wards, intensive care units, accident and emergency departments, in general practice, in the management of all forms of chronic pain including cancer pain, and in end of life care. This neglect is extended to all ages, from neonates to the frail elderly. African and Hispanic Americans are more likely than others to suffer needlessly in the hands of a physician; and women's pain is more likely to be undertreated than men's.
The International Association for the Study of Pain advocates that the relief of pain should be recognized as a human right, that chronic pain should be considered a disease in its own right, and that pain medicine should have the full status of a specialty. It is a specialty only in China and Australia at this time. Elsewhere, pain medicine is a subspecialty under disciplines such as anesthesiology, physiatry, neurology, palliative medicine and psychiatry. In 2011, Human Rights Watch alerted that tens of millions of people worldwide are still denied access to inexpensive medications for severe pain.
Medication.
Acute pain is usually managed with medications such as analgesics and anesthetics. Caffeine when added to pain medications provides some additional benefit. Management of chronic pain, however, is much more difficult and may require the coordinated efforts of a pain management team, which typically includes medical practitioners, clinical psychologists, physiotherapists, occupational therapists, physician assistants, and nurse practitioners.
Sugar taken orally reduces the total crying time but not the duration of the first cry in newborns undergoing a painful procedure (a single lancing of the heel). It does not moderate the effect of pain on heart rate and a recent single study found that sugar did not significantly affect pain-related electrical activity in the brains of newborns one second after the heel lance procedure. Sweet oral liquid moderately reduces the incidence and duration of crying caused by immunization injection in children between one and twelve months of age.
Psychological.
Individuals with more social support experience less cancer pain, take less pain medication, report less labor pain and are less likely to use epidural anesthesia during childbirth or suffer from chest pain after coronary artery bypass surgery.
Suggestion can significantly affect pain intensity. About 35% of people report marked relief after receiving a saline injection they believe to have been morphine. This "placebo" effect is more pronounced in people who are prone to anxiety, so anxiety reduction may account for some of the effect, but it does not account for all of it. Placebos are more effective in intense pain than mild pain; and they produce progressively weaker effects with repeated administration.
It is possible for many chronic pain sufferers to become so absorbed in an activity or entertainment that the pain is no longer felt, or is greatly diminished.
Cognitive behavioral therapy (CBT) has been shown effective for improving quality of life in those with chronic pain but the reduction in suffering is quite modest, and the CBT method employed seems to have no effect on outcome. Acceptance and Commitment Therapy (ACT) is likely also effective in the treatment of chronic pain.
A number of meta-analyses have found clinical hypnosis to be effective in controlling pain associated with diagnostic and surgical procedures in both adults and children, as well as pain associated with cancer and childbirth. A 2007 review of 13 studies found evidence for the efficacy of hypnosis in the reduction of chronic pain in some conditions, though the number of patients enrolled in the studies was low, bringing up issues of power to detect group differences, and most lacked credible controls for placebo and/or expectation. The authors concluded that "although the findings provide support for the general applicability of hypnosis in the treatment of chronic pain, considerably more research will be needed to fully determine the effects of hypnosis for different chronic-pain conditions."
Alternative medicine.
Pain is the most common reason for people to use complementary and alternative medicine. An analysis of the 13 highest quality studies of pain treatment with acupuncture, published in January 2009, concluded there is little difference in the effect of real, sham and no acupuncture. However other reviews have found benefit. Additionally, there is tentative evidence for a few herbal medicine. There is interest in the relationship between vitamin D and pain, but the evidence so far from controlled trials for such a relationship, other than in osteomalacia, is unconvincing.
A 2003 meta-analysis of randomized clinical trials found that spinal manipulation was "more effective than sham therapy but was no more or less effective than general practitioner care, analgesics, physical therapy, exercise, or back school" in the treatment of low back pain.
Epidemiology.
Pain is the main reason for visiting the emergency department in more than 50% of cases and is present in 30% of family practice visits. Several epidemiological studies from different countries have reported widely varying prevalence rates for chronic pain, ranging from 12 to 80% of the population. It becomes more common as people approach death. A study of 4,703 patients found that 26% had pain in the last two years of life, increasing to 46% in the last month.
A survey of 6,636 children (0–18 years of age) found that, of the 5,424 respondents, 54% had experienced pain in the preceding three months. A quarter reported having experienced recurrent or continuous pain for three months or more, and a third of these reported frequent and intense pain. The intensity of chronic pain was higher for girls, and girls' reports of chronic pain increased markedly between ages 12 and 14.
Society and culture.
The nature or meaning of physical pain has been diversely understood by religious or secular traditions from antiquity to modern times.
Physical pain is an important political topic in relation to various issues, including pain management policy, drug control, animal rights or animal welfare, torture, and pain compliance. In various contexts, the deliberate infliction of pain in the form of corporal punishment is used as retribution for an offence, or for the purpose of disciplining or reforming a wrongdoer, or to deter attitudes or behaviour deemed unacceptable. In some cultures, extreme practices such as mortification of the flesh or painful rites of passage are highly regarded.
Philosophy of pain is a branch of philosophy of mind that deals essentially with physical pain, especially in connection with such views as dualism, identity theory, and functionalism.
More generally, it is often as a part of pain in the broad sense, i.e. suffering, that physical pain is dealt with in culture, religion, philosophy, or society.
Other animals.
The most reliable method for assessing pain in most humans is by asking a question: a person may report pain that cannot be detected by any known physiological measure. However, like infants (Latin "infans" meaning "unable to speak"), animals cannot answer questions about whether they feel pain; thus the defining criterion for pain in humans cannot be applied to them. Philosophers and scientists have responded to this difficulty in a variety of ways. René Descartes for example argued that animals lack consciousness and therefore do not experience pain and suffering in the way that humans do. Bernard Rollin of Colorado State University, the principal author of two U.S. federal laws regulating pain relief for animals, writes that researchers remained unsure into the 1980s as to whether animals experience pain, and that veterinarians trained in the U.S. before 1989 were simply taught to ignore animal pain. In his interactions with scientists and other veterinarians, he was regularly asked to "prove" that animals are conscious, and to provide "scientifically acceptable" grounds for claiming that they feel pain. Carbone writes that the view that animals feel pain differently is now a minority view. Academic reviews of the topic are more equivocal, noting that although the argument that animals have at least simple conscious thoughts and feelings has strong support, some critics continue to question how reliably animal mental states can be determined. The ability of invertebrate species of animals, such as insects, to feel pain and suffering is also unclear.
The presence of pain in an animal cannot be known for certain, but it can be inferred through physical and behavioral reactions. Specialists currently believe that all vertebrates can feel pain, and that certain invertebrates, like the octopus, might too. As for other animals, plants, or other entities, their ability to feel physical pain is at present a question beyond scientific reach, since no mechanism is known by which they could have such a feeling. In particular, there are no known nociceptors in groups such as plants, fungi, and most insects, except for instance in fruit flies.
In vertebrates, endogenous opioids are neuromodulators that moderate pain by interacting with opioid receptors. Opioids and opioid receptors occur naturally in crustaceans and, although at present no certain conclusion can be drawn, their presence indicates that lobsters may be able to experience pain. Opioids may mediate their pain in the same way as in vertebrates. Veterinary medicine uses, for actual or potential animal pain, the same analgesics and anesthetics as used in humans.
Etymology.
First attested in English in 1297, the word "peyn" comes from the Old French "peine", in turn from Latin "poena" meaning "punishment, penalty" (in L.L. also meaning "torment, hardship, suffering") and that from Greek ποινή ("poine"), generally meaning "price paid, penalty, punishment".

</doc>
<doc id="24374" url="https://en.wikipedia.org/wiki?curid=24374" title="Pacifist organisation">
Pacifist organisation

A Pacifist organization promotes the pacifist principle of renouncing war and violence for political ends. They are distinguished from organizations concerned only with removing nuclear weapons from war, though those organization may call for suspension of hostilities as well. Other organizations include those that deal with other concerns, but have a strong pacifist element.
Pacifist organizations:
"Nuclear pacifist" organizations :
Organizations that cite pacifism as an aim :

</doc>
<doc id="24375" url="https://en.wikipedia.org/wiki?curid=24375" title="Porsche 944">
Porsche 944

The Porsche 944 is a luxury sports car that was built by Porsche from 1982 to 1991. It used the 924 platform that remained in production until 1988. The 944 was intended to last into the 1990s, but major revisions planned for a 944 "S3" model were eventually combined into the 968 instead, which replaced the 944. The 944 was available in coupé or cabriolet body styles, with either naturally aspirated or turbocharged engines.
History and model overview.
The Porsche 924 had originally been a project of VW-Porsche a joint Porsche/Volkswagen company created to develop and produce the 914 which was sold in Europe as both a Porsche and a Volkswagen. In 1972, a replacement for the Volkswagen version of the 914, code named EA-425 began development. The model was to be sold as an Audi as part of the VW-Audi-Porsche marketing arrangement. Porsche was to have its own version. At one point, VW head Rudolf Leidig, declared the EX-425 was going to be a VW exclusively, thus denying Porsche of a 914 replacement. Although testing had begun in the Spring of 1974, Volkswagen cancelled the EX-425 program because of significant financial losses due to declining sales and rising development costs for new vehicles, as well as the departure of Leidig. The recently released Volkswagen Scirocco was expected to fill the sports coupé market segment. 
This led Porsche to market an entry level car to replace the Porsche 912E, which was a US-only stop-gap model for 1976, and their model of 914, which was discontinued in 1975. Porsche purchased the design and the finished development with a mechanical fuel injection system. The vehicle received positive reviews, but was criticized by Porsche enthusiasts for its Audi-sourced 2 L engine. In 1979, Porsche introduced a Turbocharged 924 to increase performance, but this model carried a high price. Rather than scrap the design, Porsche decided to develop the 924, as they had with generations of the 911; although model numbers would change, the 924 would provide the basis for its replacement.
Porsche re-worked the platform and a new all-alloy 2.5 L inline-four engine, bore , stroke , that was, in essence, half of the 928's 5.0 L V8, although very few parts were actually interchangeable. Not typical in luxury sports cars, the four-cylinder engine was chosen for fuel efficiency and size, because it had to be fitted from below on the Neckarsulm production line. To overcome roughness caused by the unbalanced secondary forces that are typical of four-cylinder engines, Porsche included two counter-rotating balance shafts running at twice engine speed. Invented in 1904 by British engineer Frederick Lanchester, and further developed and patented in 1975 by Mitsubishi Motors, balance shafts carry eccentric weights which produce inertial forces that balance out the unbalanced secondary forces, making a four-cylinder engine feel as smooth as a six-cylinder. The engine was factory-rated at in its U.S. configuration. Revised bodywork with wider wheel arches, similar to that of the 924 Carrera GT, a fresh interior and upgrades to the braking and suspension systems rounded out the major changes. 
Porsche introduced the 944 for MY 1982. It was slightly faster (despite having a poorer drag co-efficient than the 924), the 944 was better equipped and more refined than the 924; it had better handling and stopping power, and was more comfortable to drive. The factory-claimed 0-60 mph time of less than 9 seconds (8.3 seconds according to "Porsche the Ultimate Guide" By Scott Faragher). The factory-claimed top speed of was also pessimistic, Autocar having verified a top speed of . The car had nearly even front to rear weight distribution (50.7% front/49.3% rear) thanks to the rear transaxle balancing out the engine in the front.
In mid-1985, the 944 underwent its first significant changes. These included : a new dash and door panels, embedded radio antenna, upgraded alternator (from 90 amp to 115 amp), increased oil sump capacity, new front and rear cast alloy control arms and semi-trailing arms, larger fuel tank, optional heated and powered seats, Porsche HiFi sound system, and revisions in the mounting of the transaxle to reduce noise and vibration. The "cookie cutter" style wheels used in the early 944s were upgraded to new "phone dial" style wheels (Fuchs wheels remained an option). 1985 model year cars incorporating these changes are sometimes referred to as "1985B", "85.5" or "1985½" cars.
For the 1987 model year, the 944 Motronic DME was updated, and newly incorporated anti-lock braking and air bags. Because of the ABS system, the wheel offset changed to and Fuchs wheels were no longer an option.
In early 1989 before the release of the 944S2, Porsche upgraded the 944 from the 2.5 to a 2.7 L engine, bore , stroke , with a rated (versus for the 1988 2.5 L version) and a significant increase in torque. In addition to the increase in displacement, the new motor featured a siamesed-cylinder block design and a different cylinder head which incorporated larger valves.
944 Turbo (951/952).
For the 1985 model year, Porsche introduced the 944 Turbo, known internally as the 951. This had a turbocharged and intercooled version of the standard car's engine that produced ( in the US) at 6000 rpm. In 1987, "Car and Driver" tested the 944 Turbo and achieved a 0-60 mph time of 5.9 seconds. The turbo was the first car using a ceramic port liner to retain exhaust gas temperature and new forged pistons and was also the first vehicle to produce identical power output with or without a catalytic converter. The Turbo also featured several other changes, such as improved aerodynamics, notably an integrated front bumper. This featured the widest turn signals (indicators) fitted to any production car, a strengthened gearbox with a different final drive ratio, standard external oil coolers for both the engine and transmission, standard 16 inch wheels (optional forged Fuchs wheels), and a slightly stiffer suspension (progressive springs) to handle the extra weight. The Turbo's front and rear brakes were borrowed from the Porsche 911, with Brembo 4-piston fixed calipers and 12-inch discs as ABS also came standard. Engine component revisions, more than thirty in all, were made to the 951 to compensate for increased internal loads and heat.
Changes occurred for the 1987 model year. On the interior, the 1987 944 Turbo for North America became the first production car in the world to be equipped with driver and passenger side air bags as standard equipment. A low oil level light was added to the dash as well as a speedometer as opposed to the speedometer on the 1986 model Turbos. Also included is the deletion of the transmission oil cooler, and a change in suspension control arms to reduce the car's scrub radius. The engine remained the same M44/51 as in the 1986 model.
In 1988, Porsche introduced the Turbo S. The 944 Turbo S had a more powerful engine (designation number M44/52) with and torque (standard 944 Turbo and ). This higher output was achieved by using a larger K26-8 turbine housing and revised engine mapping which allowed maintaining maximum boost until 5800 rpm, compared to the standard 944 Turbo the boost would decrease from at 3000 rpm to at 5800 rpm. In June 1988, "Car and Driver" tested the 944 Turbo S (with the advantage of shorter final drive gear) and achieved a 0- time of 5.5 seconds and a quarter-mile time of 13.9 seconds at . Top speed was factory rated at .
The 944 Turbo S's suspension had the "M030" option consisting of Koni adjustable shocks front and rear, with ride height adjusting threaded collars on the front struts, progressive rate springs, larger hollow rear anti-roll/torsion bars, harder durometer suspension bushings, larger hollow anti-roll/torsion bars at the front, and chassis stiffening brackets in the front frame rails. The air conditioning dryer lines are routed so as to clear the front frame brace on the driver's side. The 944 Turbo S wheels, known as the Club Sport design, were 16-inch Fuchs forged and flat-dished, similar to the Design 90 wheel. Wheel widths were in the front, and in the rear with offset; sizes of the Z-rated tires were 225/50 in the front and 245/45 in the rear. The front and rear fender edges were rolled to accommodate the larger wheels. The manual transmission (case code designation: AOR) featured a higher friction clutch disc setup, an external cooler, and a limited slip differential with a 40% lockup setting. The Turbo S front brakes were borrowed from the Porsche 928 S4, with larger Brembo GT 4-piston fixed calipers and 12-inch discs; rear Brembo brakes remained the same as a standard Turbo. ABS also came standard.
The 944 Turbo S interior featured power seats for both driver and passenger, where the majority of the factory-built Turbo S models sported a "Burgundy plaid" (Silver Rose edition) but other interior/exterior colors were available. A 10-speaker sound system and equalizer + amp was a common option with the Turbo S and S/SE prototypes. Only the earlier 1986, prototypes featured a "special wishes custom interior" options package.
In 1989 and later production, the 'S' designation was dropped from the 944 Turbo S, and all 944 Turbos featured the Turbo S enhancements as standard, however the "M030" suspension and the Club Sport wheels were not part of that standard. The 944 Turbo S was the fastest production four cylinder car of its time.
944 S.
For the 1987 model year, the 944S "Super" was introduced. The 944S featured a high performance normally aspirated, dual-overhead-cam 16-valve version of the 2.5 L engine (M44/40) featuring a self-adjusting timing belt tensioner. This marked the first use of four-valve-per-cylinder heads and DOHC in the 944 series, derived from the 928 S4 featuring a redesigned camshaft drive, a magnesium intake tract/passages, magnesium valve cover, larger capacity oil sump, and revised exhaust system. The alternator capacity was 115 amps. The wheel bearings were also strengthened and the brake servo action was made more powerful. Floating 944 calipers were standard, but the rear wheel brake circuit pressure regulator from the 944 turbo was used. Small '16 Ventiler' script badges were added on the sides in front of the body protection mouldings. Performance was quoted as 0 – in 6.5 seconds (Best) and a top speed due to a 2857 lb weight. It also featured an improved programmed Bosch Digital Motronic 2 Computer/DME with dual knock sensors for improved fuel performance for the higher 10.9:1 compression ratio cylinder head. Like the 944 Turbo, the 944S received progressive springs for greater handling, Larger front and rear anti-roll bars, revised transmission and gearing to better suit the 2.5 L DOHC higher 6800 rpm rev limit. Dual safety air bags, limited-slip differential, and ABS braking system were optional on the 944S.
A Club Sport touring package (M637) was available as was the lightweight 16 inch CS/Sport Fuch 16x7 and 16x9 forged alloy wheels. This SC version car was raced in Canada, Europe and in the U.S. IMSA Firehawk Cup Series. Production was only during 1987 and 1988. It was superseded in 1989 by the 'S2' 944 edition. The 1987 944S power-to-weight ratio was such that it was able to accelerate from 0 to 62 mph in 6.5 seconds thus matching the acceleration of its newer larger displacement 3.0 L 944 S2 sibling.
944 S2.
In 1989 the 944S2 was introduced, powered by a normally aspirated, dual-overhead-cam 16-valve 3.0 L version of the 944S engine. With a bore of and a stroke of , it was the largest production 4-cylinder engine of its time. The 944S2 also received a revised transmission and gearing to better suit the 3.0 L M44/41 powerplant. The 944S2 had the same rounded nose and a rear valance found on the Turbo model. This was the first example of the use of an integrated front bumper, where the fender and hood profiles would merge smoothly with the bumper, a design feature that has only now seen widespread adoption on the 1990 onward production cars. Performance was quoted as 0-60 mph in 6.0 seconds (0–100 km/h 6.8 s), with a top speed of via manual transmission. A Club Sport touring package (M637) was also available. Dual air bags (left hand drive models), limited-slip differential and ABS were optional. Series 90 16-inch cast alloy wheels were standard equipment.
In 1989, Porsche released the 944 S2 Cabriolet, a first for the 944 line that featured the cabriolet body built by ASC-American Sunroof Company at Weinsberg Germany. The first year of production included sixteen 944 S2 Cabriolet for the U.S. market. For the 1990 model year, Porsche produced 3,938 944 S2 Cabriolets for all markets including right-hand drive units for the United Kingdom, Australia and South Africa.
This car was raced, including the British championship that was called the Porsche Motorsport Championship. Production was during 1989, 1990, and 1991. The 944 S2 power-to-weight ratio was such that it was able to accelerate from 0 to 60 mph in 6.5 seconds.
944 Turbo Cabriolet.
In February 1991, Porsche released the 944 Turbo Cabriolet, which combined the Turbo S's engine with the cabriolet body built by ASC-American Sunroof Company at Weinsberg Germany. Porsche initially announced that 600 would be made; ultimately 625 were built, 100 of which were right-hand drive for the United Kingdom, Japanese, Australian, and South African market. None were imported to the U.S. and The Americas.
End of the line.
In early 1990, Porsche engineers began working on what they had intended to be the third evolution of the 944, the S3. As they progressed with the development process, they realized that so many parts were being changed that they had produced an almost entirely new vehicle. Porsche consequently shifted development from the 944 S/S2 to the car that would replace the 944 entirely, the 968. The 944's final year of production was 1991 with over 4,000 cars built and sold. In 1992 the 968 debuted and was sold alongside the 928 through 1995, when both water-cooled front engine models were discontinued.
Awards.
The 944 was on "Car and Driver's" Ten Best list from 1983 through 1985, and the "Turbo" made the list for 1986.
In 1984, "Car and Driver" named the 944 the Best Handling Production Car in America.
Production.
A grand total 163,192 cars in the 944 family were produced between 1982 and 1991. This made it the most successful car line in Porsche's history until the introductions of the Boxster and 997 Carrera.
944.
A total of 113,070 944s were made between 1982 and 1989, with 56,921 being exported to the United States. A project joint venture with Porsche and Callaway produced 16 specially built turbo 944's for the U.S. in 1983.
944 Turbo (951/952).
A total of 25,245 944 Turbos were made, with 13,982 being exported to the United States.
<br>
<br>
† - Includes 251 Turbo Cabriolet. A different source, Jerry Sloniger's
article in the October 1991 issue of "Excellence", indicates that the
factory built 525, of which 255 were exported to markets outside Germany.
< >"CUP" designates a cup car
special edition race car.
944S.
A total of 12,936 944S models were produced from 1987 to 1988, with 8,815 being exported to the United States. In 1985 a Prototype 944S Cabriolet 'Studie' built by Braun was powered by the 2.5 L 16 valve which developed 185 hp, forerunner of the later production 944S and S2 Cabriolet models.
944S2.
A total of around 14,071 944S2s were made between 1989 and 1991, with 3,650 being exported to the United States.

</doc>
<doc id="24376" url="https://en.wikipedia.org/wiki?curid=24376" title="Porsche 968">
Porsche 968

The Porsche 968 is a luxury sports car that was sold by Porsche AG from 1992 to 1995. It took over the entry-level position in Porsche's lineup from the 944, with which it shared about 20% of its parts. The 968 became the final model in an evolving line, starting almost 20 years earlier with the introduction of the Porsche 924.
Introduction.
Porsche's 944 model debuted for the 1982 model year, was updated as "944S" in 1987 and as "944S2" in 1989. Shortly after the start of production of the S2 variant, Porsche engineers began working on another set of significant upgrades for the model, as executives were planning a final "S3" variant of the 944. During the development phase, 80% of the 944's mechanical components were either significantly modified or completely replaced by the engineers, leaving so little of the outgoing S2 behind that Porsche management chose to introduce the variant as a new model, calling it the 968. In addition to the numerous mechanical upgrades, the new model also received significantly evolved styling both inside and out, with a more modern, streamlined look and more standard luxury than on the 944. Production was moved from the Audi plant in Neckarsulm (where the 924 and 944 had been manufactured under contract to Porsche), to Porsche's own factory in Zuffenhausen.
The 968 was powered by an updated version of the 944's straight-four engine, now displacing 3.0 L with 104 mm bore, 88 mm stroke and producing . Changes to the 968's powertrain also included the addition of Porsche's then-new VarioCam variable valve timing system, newly optimized induction and exhaust systems, a dual-mass flywheel, and updated engine management electronics among other more minor revisions. The 968's engine was the second-largest four-cylinder ever offered in a production car up to that time. A new 6-speed manual transmission replaced the 944's old 5-speed, and Porsche's dual-mode Tiptronic automatic became an available option. Both the VarioCam timing system and Tiptronic transmission were very recent developments for Porsche. The Tiptronic transmission had debuted for the first time ever only 3 years prior to the debut of the 968, on the 1989 Type 964 911. The VarioCam timing system was first introduced on the 968 and would later become a feature of the Type 993 air-cooled six-cylinder engine.
The 968's styling was an evolution on that of the outgoing 944, itself styled evolutionarily from the earlier 924, but elements were borrowed from the more expensive 928 model in an attempt to create a "family resemblance" between models, and the swooping headlamp design, inspired by those of the 959, previewed similar units found later on the Type 993 911. Along with the new styling, the 968 featured numerous small equipment and detail upgrades, including a Fuba roof-mounted antenna, updated single lens tail lamps, "Cup" style 16" alloy wheels, a wider selection of interior and exterior colors, and a slightly updated "B" pillar and rear quarter window to accommodate adhesive installation to replace the older rubber gasket installation. Because some parts are interchangeable between the 968, 944 and 924, some enthusiasts purchase those parts from Porsche parts warehouses as "upgrades" for their older models.
Like the 944, the 968 was sold as both a coupe and a convertible. Much of the 968's chassis was carried over from the 944 S2, which in itself shared many components with the 944 Turbo (internally numbered "951"). Borrowed components include the Brembo-sourced four-piston brake calipers on all four wheels, aluminium semi-trailing arms and aluminum front A-arms, used in a Macpherson strut arrangement. The steel unibody structure was also very similar to that of the previous models. Porsche maintained that 80% of the car was new.
968 Clubsport.
From 1993 through 1995, Porsche offered a lighter-weight "Club Sport" version of the 968 designed for enthusiasts seeking increased track performance. Much of the 968's luxury-oriented equipment was removed or taken off the options list; less sound deadening material was used, electrical windows were replaced with crank-driven units, upgraded stereo systems, A/C and sunroof were still optional as on the standard Coupe and Convertible models. In addition, Porsche installed manually adjustable lightweight Recaro racing seats rather than the standard power-operated leather buckets (also manufactured by Recaro), a revised suspension system optimized and lowered by 20 mm for possible track use, 17-inch wheels (also slightly wider to accommodate wider tires) rather than the 16-inch as found on the Coupe and wider tires, 225 front and 255 rears rather than 205 and 225 respectively. The four-spoke airbag steering wheel was replaced with a thicker-rimmed three-spoke steering wheel with no airbag, heated washer jets were replaced with non heated, vanity covers in the engine bay were deleted, as was the rear wiper. The Club Sport has no rear seats, unlike the 2+2 Coupé.
Club Sports were only available in Grand Prix White, black, Speed yellow, Guards red, Riviera blue or Maritime blue. Seat backs were colour-coded to the body. Club Sport decals were standard in either black, red or white but there was a 'delete' option.
All Club Sports had black interiors with the 944 S2 door cards. Due to the reduction in the number of electrical items the wiring loom was reduced in complexity which saved weight and also the battery was replaced with a smaller one, again reducing weight. With the no frills approach meaning less weight, as well as the optimising of the suspension, Porsche could focus media attention on the Club Sport variants fast road and track abilities. This helped to slightly bolster the flagging sales figures in the mid-1990s. The Club Sport variant achieved a 'Performance Car Of The Year' award in 1993 from Performance Car magazine in the UK. Club Sport models were only officially available in the UK, Europe, Japan & Australia, although "grey market" cars found their way elsewhere. The declared weight of the 968 CS is 1320 kg, ~100 kg lighter than the regular 968. Acceleration from standstill to is 6.3 seconds and a top speed is .
A UK-only version called "968 Sport", was offered in 1994 and 1995, and was essentially a Club Sport model (and was produced on the same production line with similar chassis numbers) with electric windows, electric release boot, central locking, cloth "comfort seats" (different from both the standard and the Club Sport). With the added electrics the larger wiring loom was used. The Sport Variant also got back the two rear seats, again in the cloth material specific to the Sport. At £29,975, the 968 Sport was priced £5,500 lower than the standard 968, but had most of the latter's desirable "luxuries" and consequently outsold it by a large margin (306 of the 968 Sport models compared to 40 standard 968 coupés).
A 968 CS driven by Peter Fitzgerald and Brett Peters won the 1993 Sandown 6 Hour race in the Australian competition debut of the model.
968 Turbo S.
In 1993, Porsche Motorsports at Weissach briefly produced a turbocharged 968 Turbo S, a fairly odd naming choice for Porsche which usually reserves the added "S" moniker for models that have been tuned for more power over a "lesser" counterpart, such as with the 911 Turbo. The 968 Turbo S shared the same body and interior as the Club Sport and visually can be identified by the NACA bonnet hood scoops, adjustable rear wing and deeper front spoiler. Powered by a large 8 valve SOHC cylinder head (944 Turbo S) with 3.0 Liter 944S2 style engine block. Tests conducted in 1993 produced a 0 to of 4.7 seconds and a top speed of , performance comparable to the much newer Type 996 911. It generated at 5600 rpm with a maximum torque of at 3000rpm. Only 16 were produced in total and only for sale in mainland Europe.
968 Turbo RS.
Between 1992 and 1994, Porsche Motorsports Research and Development built and provided a full "Race" version (stripped out 968 Turbo S) for Porsche's customer race teams. The 968 Turbo RS was available in two variations; a version using the K27 turbocharger from the Turbo S, which was built to the German ADAC GT specification (ballast added to bring the car up to the 1350 kg minimum weight limit), and an international spec version which used a KKK L41 turbocharger producing and was reduced to 1212 kg in weight.
Only 4 were ever produced ; 1 Guards Red (WPOZZZ96ZNS820065), 1 Speed Yellow (WPOZZZ96ZPS896061), 1 Black (WPOZZZ96ZPS896062) and 1 White (WPOZZZ96ZPS896063). These are the rarest 968s ever produced.
Historical significance.
The 968 was Porsche's last new front-engined vehicle (of any type) before the introduction of the Cayenne SUV. Its discontinuation in 1995 coincided with that of the 928, Porsche's only other front-engined car at the time.
In 2010, Porsche released the Panamera a front-engined, 4-door sports-touring sedan. A successor to the 928, based on that sedan's architecture, is predicted to follow.
The 968 was also the last Porsche sold with a four-cylinder engine prior to the introduction of the 718 Boxster in 2016.
Collectability.
While lacking the wider ranging appeal of the 911, the 968 has nonetheless carved out its own niche in the hearts of enthusiasts. This is likely due to the 968's unique combination of speed and practicality, and low production numbers. Clean, low mileage examples have become extremely scarce and expensive by 2013. In April 2012, when the Gooding & Company bidding ended and the auctioneer's hammer finally fell, the only 1 Red 968 Turbo RS 350 hp machine had fetched a hefty $346,000 price.

</doc>
<doc id="24377" url="https://en.wikipedia.org/wiki?curid=24377" title="Porsche 912">
Porsche 912

The Porsche 912 is a sports car that was manufactured by Porsche of Germany between 1965 and 1969 as their entry-level model. The 912 is a nimble-handling compact performance four-seat vehicle, delivering 90 SAE horsepower at 5800 rpm. It is capable of up to fuel economy. This combination is possible because of a high-efficiency petrol engine, low weight, and low drag. A variant of the Type 911, one of the most famous and successful sports cars of all time, the Type 912 initially outsold the 911, boosting the manufacturer's total production until success of the 911 was assured.
History.
Concerned that the considerable price increase of a Type 911 with "flat" six-cylinder powerplant over the Type 356 would cost the company sales and narrow brand appeal, in 1963 Porsche executives decided to introduce a new four-cylinder entry-level model. Like the 911 (original internal factory designation "901"), the four-cylinder 912 was originally known at Zuffenhausen by a number with a zero in the middle, but the "902" designation was never used publicly. ("912" as project number was used after 1968 to indicate the 12 cylinder flat-engine developed for Porsche 917 racing car)
In 1963, Porsche assigned Dan Schwartz, later Chief Departmental Manager for Development, Mechanics, a project to oversee design and construction of a new horizontally-opposed four-cylinder engine for the 902, utilizing components from the new 901 six-cylinder engine, that would produce higher performance than their 356SC engine, and be less costly and complex than their Carrera 2 engine. Another option explored by Claus von Rücker was to increase displacement of the 356 Type 616 engine to 1.8 liters, add Kugelfischer fuel injection, and modify both valve and cooling systems. Considering performance, cost, and scheduling, Porsche discontinued both of these design projects, and instead developed a third option, to tailor the 1.6 liter Type 616 engine to the 902.
Before 911 production commenced in 1964, the Porsche Vehicle Research Department had set aside chassis numbers 13328, 13329, 13330, 13352, and 13386 through 13397 for research testing of the 902; research vehicle Serial Number 13394 is the oldest 902 known to exist today. In production form, the Type 912 combined a 911 chassis / bodyshell with the 1.6L, four-cylinder, push-rod Type 616/36 engine, based upon the Type 616/16 engine used in the Type 356SC of 1964-1965. With a lower compression ratio and new Solex carburetors, the Type 616/36 engine produced five less horsepower than the 616/16, but delivered about the same maximum torque at 3,500 rpm versus 4,200 rpm for the 616/16. Compared to the 911, the resulting production Type 912 vehicle demonstrated superior weight distribution, handling, and range. To bring 912 pricing close to the 356, Porsche also deleted some features standard on the 911.
As production of the 356 model concluded in 1965, on April 5, 1965 Porsche officially began production of the 912 coupé. Styling, performance, quality construction, reliability, and price made the 912 a very attractive buy to both new and old customers, and it substantially outsold the 911 during the first few years of production. Porsche produced nearly 30,000 912 coupé units and about 2500 912 Targa body style units (Porsche's patented variation of a cabriolet) during a five-year manufacturing run. Production of the Targa, complete with removable roof and heavy transparent plastic rear windows openable with a zipper (later called 'Version I' by Porsche and the 'soft-window Targa' by enthusiasts), commenced in December 1966 as a 1967 model. In January 1968, Porsche also made available a Targa 'Version II' option ('hard window Targa') with fixed glass rear window, transforming the Targa into a coupé with removable roof.
The Type 912 was also made in a special version for the German autobahn police (polizei); the 100,000th Porsche car was a 912 Targa for the police of Baden-Württemberg, the home state of Porsche. In the April 1967 edition, the Porsche factory's Christophorus Magazine noted: "On 21 December 1966, Porsche celebrated a particularly proud anniversary. The 100,000th Porsche, a 912 Targa outfitted for the police, was delivered."
Porsche executives decided that after the 1969 model year, continuation of 912 production would not be viable, due to both internal and external factors. First, production facilities used for the 912 were reallocated to a new 914-6, a six-cylinder high performance version of the 914 Porsche-Volkswagen joint effort vehicle. Second, the 911 platform had returned to Porsche's traditional three performance-level ladder, including a most powerful 911S, a fuel-injected 911E, and a base model 911T, with pricing largely in line with market expectations. Third, more stringent United States engine emission control regulations also had a bearing on the decision; Ferry Porsche stated "It would have taken some trouble to prepare the 912 for the new exhaust rules, and with the arrival of the 914 we would have had three different engines to keep current. That was too many." 
Porsche had constructed more than 32,000 of the Type 912 from April 1965 to July 1969. For the 1970 model year the four-cylinder 914 superseded the 912 as Porsche's entry-level model, which Porsche had thought would be less expensive for them to manufacture and sell than the 912. In practice, a deterioration in relationships between Porsche and Volkswagen - who had designed and planned to manufacture the 914 - severely curtailed the intended cost reduction, and 914 production was discontinued in early 1976.
Porsche 912E (1975-1976).
After a six-year absence, the 912 was re-introduced to North America as the 1976 model year 912E (internal factory designation 923) which shared the "G-Series" bodywork with the 911S. The "Prototyp" Museum collection in Hamburg, Germany includes a 912E preseries vehicle constructed utilizing a 911 Chassis No. 911 520 1617 and four-cylinder VW-Porsche 90HP 2.0L Type 4 similar to the late-model 2.0L 914/4. Once in production, the 912E was powered by an 86 bhp 2.0 L Volkswagen air-cooled engine, refined with a new Bosch L-Jetronic (Air Flow Controlled) fuel injection system. The 912E occupied the entry-level position left vacant by the discontinuation of the 914, while the new 924 – another Porsche-Volkswagen joint effort vehicle and the 914's official replacement – was being finalized and put into production. During the production run of May 1975 to July 1976, Porsche manufactured nearly 2,100 of the 912E, targeted to the United States market.
Motorsport.
Sold to the public for street use, the Porsche 912 has also proven successful as a race car, from production years to current vintage events. In 1967 the 912 contributed to Porsche factory rally history when independent Polish driver Sobiesław Zasada drove a factory-loaned 912, bearing Polish plate 6177 KR, to capture the European Rally Championship for Group 1 series touring cars. In the 1967 Rally of Poland, the second oldest rally in the world and one of the oldest motorsport events in the world, Zasada drove his 912 race No. 47 to finish first overall out of a starting field of 50 entries.
As a vintage rally car, on January 29, 2012 Hayden Burvill, Alastair Caldwell, and their #35 1968 Porsche 912 finished first in class, and 7th overall in the 2012 London to Cape Town World Cup Rally; a 14 country, three continent, 14,000 kilometre, 26 driving-days event.

</doc>
<doc id="24378" url="https://en.wikipedia.org/wiki?curid=24378" title="PVC (disambiguation)">
PVC (disambiguation)

PVC is the plastic polyvinyl chloride.
PVC also may refer to:

</doc>
<doc id="24380" url="https://en.wikipedia.org/wiki?curid=24380" title="Pope Zachary">
Pope Zachary

Pope Zachary (; 679 – 15 March 752) reigned from 3 December or 5 December 741 to his death in 752. A Greek from Santa Severina, Calabria, he was the last pope of the Byzantine Papacy. Most probably he was a deacon of the Roman Church and as such signed the decrees of the Roman council of 732 and was on intimate terms with Gregory III, whom he succeeded on 5 December 741.
Life.
Zachary was a wise and subtle diplomat. His predecessor's alliance with the Lombard Duke of Spoleto put papal cities at risk when the Dukes of Spoleto and Benevento rebelled. Zachary turned to King Liutprand the Lombard directly. Out of respect for Zachary the king restored to the church of Rome all the territory seized by the Lombards and sent back the captives without ransom. The contemporary history ("Liber pontificalis") dwells chiefly on Zachary's great personal influence with Liutprand, and with his successor Ratchis. His tact in dealing with these princes in a variety of emergencies contributed to save the Exarchate of Ravenna from the Lombard attacks.
A correspondence of considerable extent and of great interest between Zachary and Saint Boniface, the apostle of Germany, survives, and shows how great was the influence of this pope on events in France and Germany. He encouraged the deposition of the last Merovingian king of the Franks, Childeric III, and it was with his sanction that Boniface crowned Pepin the Short as King of the Franks at Soissons in 752. Zachary is stated to have remonstrated with the Byzantine emperor Constantine V Copronymus on the part he had taken in the iconoclastic controversy.
In the effort to Christianize Rome, Zachary built the original church of Santa Maria sopra Minerva over an ancient temple to Minerva near the Pantheon. He also restored the Lateran Palace, moving the relic of the head of Saint George to the church of San Giorgio al Velabro. Also in Rome, some Venetian merchants bought many slaves in the city to sell to the Muslims of Africa; however, Zachary forbade such traffic and then paid the merchants their price, giving the slaves their freedom.
Pope Zachary died on 15 March 752 and was buried in St. Peter's Basilica. His successor was Stephen, who died soon before his consecration and is not considered a valid pope. He was then succeeded by another Stephen who became Stephen II.
The letters and decrees of Zachary are published in Jacques Paul Migne, "Patrolog. lat." lxxxix. p. 917–960.

</doc>
<doc id="24381" url="https://en.wikipedia.org/wiki?curid=24381" title="Pope Valentine">
Pope Valentine

Pope Valentine (in Latin: "Valentinus"; 800 – 10 October 827) was Pope for two months in 827.
Biography.
Born in Rome in the region of the Via Lata, Valentine was the son of a Roman noble called Leontius. Showing an early aptitude for learning, he was moved from the school attached to the Lateran Palace and, according to the "Liber Pontificalis", was made a Deacon by Pope Paschal I (817–824). Paschal grew attached to the young man, and soon raised him rank of Archdeacon. He also was clearly favoured by Paschal’s successor, Pope Eugenius II, to the point where rumours were circulated that Valentine was really the son of Eugenius. Other rumours declared that Valentine and Eugenius were involved in an illicit relationship.
With the death of Eugenius, the Roman clergy, nobility and people all acclaimed Valentine as being the most worthy to occupy the Apostolic See. They took him from the Basilica di Santa Maria Maggiore, and installed him in the Lateran Palace, ignoring his protests. In their haste, they enthroned him before he was consecrated a priest; this was an unusual reversal of the normal proceedings, and in fact was the first time it had happened in the recorded history of the papacy, although it would be repeated during the pontificate of Pope Benedict III. On the following Sunday, he was formally consecrated bishop at St. Peter's Basilica. There were no imperial representatives present during the election, and Valentine had no opportunity to ratify his election with the emperor, as he was dead within five weeks, dying on 10 October 827.
The election of Valentine was another sign of the increased influence the Roman nobility was having in the papal electoral process. Not only had they managed to get one of their own elected, but they also took part in the election itself. The Lateran Council of 769, under Pope Stephen III, had mandated that the election of the pope was to be the responsibility of the Roman clergy only, and that the nobility could only offer their respects after the pope had been chosen and enthroned. This gradual encroachment into the papal electoral process would reach its nadir during the tenth century, when the papacy became the plaything of the Roman aristocracy.

</doc>
<doc id="24382" url="https://en.wikipedia.org/wiki?curid=24382" title="Pope Victor I">
Pope Victor I

Pope Victor I (died 199) was a bishop of Rome, and hence a pope, in the late second century. The dates of his tenure are uncertain, but one source states he became pope in 189 and gives the year of his death as 199. He was the first bishop of Rome born in the Roman Province of Africa—probably in Leptis Magna (or Tripolitania). He was later considered a saint. His feast day is celebrated on 28 July as "St Victor I, Pope and Martyr".
Biography.
The primary sources vary over the dates assigned to Victor’s episcopate, but indicate it included the last decade of the second century. Eusebius puts his accession in the tenth year of Commodus (i.e. AD 189), which is accepted by Lipsius as the correct date. Jerome’s version of the Chronicle puts his accession in the reign of Pertinax, or the first year of Septimius Severus (i.e. 193), while the Armenian version puts it in the seventh year of Commodus (186). The "Liber Pontificalis" dates his accession to the consulate of Commodus and Glabrio (i.e. 186), while the "Liberian Catalogue", a surviving copy of the source the "Liber Pontificalis" drew upon for its chronology, is damaged at this point Concerning the duration of his episcopate, Eusebius, in his "History", does not state directly the duration of his episcopate, but the Armenian version of Eusebius' Chronicle gives it as twelve years. The Liberian Catalogue gives his episcopate a length of nine years two months and ten days, while the "Liber Pontificalis" states it was ten years and the same number of months and days; the Felician Catalogue something over ten. Finally, Eusebius in his "History" (5.28) states Zephyrinus succeeded him "about the ninth year of Severus", (201), while the "Liber Pontificalis" dates it to the consulate of Laternus and Rufinus (197). Lipsius, considering Victor in connection with his successors, concludes that he held office between nine and ten years, and therefore gives as his dates 189–198 or 199.
According to an anonymous writer quoted by Eusebius, Victor excommunicated Theodotus of Byzantium for teaching that Christ was a mere man. However, he is best known for his role in the Quartodeciman controversy. Prior to his elevation, a difference in dating the celebration of the Christian Passover/Easter between Rome and the bishops of Asia Minor had been tolerated by both the Roman and Eastern churches. The churches in Asia Minor celebrated it on the 14th of the Jewish month of Nisan, the day before Jewish Passover, regardless of what day of the week it fell on, as the Crucifixion had occurred on the Friday before Passover, justifying this as the custom they had learned from the apostles; for this the Latins called them "Quartodecimans". Synods were held on the subject in various parts—in Palestine under Theophilus of Caesarea and Narcissus of Jerusalem, in Pontus under Palmas, in Gaul under Irenaeus, in Corinth under its bishop, Bachillus, at Osrhoene in Mesopotamia, and elsewhere—all of which disapproved of this practice and consequently issued by synodical letters declaring that "on the Lord's Day only the mystery of the resurrection of the Lord from the dead was accomplished, and that on that day only we keep the close of the paschal fast" (Eusebius H. E. v. 23). Despite this disapproval, the general feeling was that this divergent tradition was not sufficient grounds for excommunication. Victor alone was intolerant of this difference, and severed ties with these ancient churches, whose bishops included such luminaries as Polycrates of Ephesus; in response he was rebuked by Irenaeus and others, according to Eusebius.

</doc>
<doc id="24383" url="https://en.wikipedia.org/wiki?curid=24383" title="Pope Victor II">
Pope Victor II

Pope Victor II (c. 1018 – 28 July 1057), born Gebhard, Count of Calw, Tollenstein, and Hirschberg, was Pope from 13 April 1055 to his death in 1057. He was one of a series of German reform popes.
Life.
He was born Gebhard of Calw, a son of the Swabian Count Hartwig of Calw and a kinsman of Emperor Henry III. At the insistence of Gebhard, Bishop of Ratisbon, the 24-year-old Gebhard was appointed Bishop of Eichstätt. In this position, he supported the Emperor's interests and eventually became one of his closest advisors.
After the death of Pope Leo IX, a Roman delegation headed by Hildebrand, later Pope Gregory VII, travelled to Mainz and asked the Emperor for the nomination of a successor. He suggested Gebhard, who was duly nominated in September 1054. Gebhard, taking the name Victor II, moved to Rome and was enthroned in St. Peter's Basilica on 13 April 1055.
Victor excommunicated both Ramon Berenguer I, count of Barcelona, and Almodis, countess of Limoges, for adultery, at the behest of Ermesinde of Carcassonne, in 1055.
In June 1055, Victor met the Emperor at Florence and held a council, which reinforced Pope Leo IX's condemnation of clerical marriage, simony, and the loss of the church's properties. In the following year, he was summoned to the Emperor's side, and was with Henry III when he died at Bodfeld in the Harz on 5 October 1056. As guardian of Henry III's infant son Henry and adviser of the Empress Agnes, Henry IV's mother and regent, Victor II now wielded enormous power, which he used to maintain peace throughout the empire and to strengthen the papacy against the aggressions of the barons. He died shortly after his return to Italy, at Arezzo, on 28 July 1057.
Victor II's retinue wished to bring his remains to the cathedral at Eichstätt for burial. Before they reached the city, however, the remains were seized by some citizens of Ravenna and buried there in the Church of Santa Maria Rotonda, the burial place of Theodoric the Great.
Although there have been nine German Popes, Victor II is one of only three Popes from the territory of present-day Germany, the others being Pope Clement II (1046–47) and Benedict XVI (2005–13).

</doc>
<doc id="24384" url="https://en.wikipedia.org/wiki?curid=24384" title="Pope Victor III">
Pope Victor III

Pope Victor III (c. 1026 – 16 September 1087), born Dauferio, was Pope from 24 May 1086 to his death in 1087. He was the successor of Pope Gregory VII, yet his pontificate is far less impressive in history than his time as Desiderius, the great Abbot of Montecassino.
His failing health was the factor that made him so reluctant to accept his pontifical election and his health was so poor that he fell to illness during his papal coronation. The only literary work of his that remains is his "Dialogues" on the miracles performed by Saint Benedict of Nursia and other saints at Montecassino.
Pope Leo XIII beatified him on 23 July 1887.
Biography.
Early life and abbacy.
He was born in 1026 to a branch of the Lombard dukes of Benevento as the only son of Prince Landulf V of Benevento.
After his father died in battle with the Normans in 1047, he fled from an arranged marriage and, though brought back by force, eventually fled again. He went to Cava de' Tirreni, where he obtained permission to enter the monastery of S. Sophia at Benevento, where he changed his name from Dauferius to Desiderius. It was a decision that his mother vehemently opposed, owing to his being the only son and the only child. The life at S. Sophia was not strict enough for the young monk, who betook himself first to the island monastery of Tremite San Nicolo in the Adriatic and in 1053 to some hermits at Majella in the Abruzzi. About this time he was brought to the notice of St. Leo IX, and it is probable that the pope employed him at Benevento to negotiate peace with the Normans after the fatal battle of Civitate.
Somewhat later Desiderius attached himself to the court of Pope Victor II at Florence. There he met two monks of the renowned Benedictine monastery of Monte Cassino, with whom he returned in 1055. He joined the community and was shortly afterwards appointed superior of the dependent house at Capua. In 1057 Pope Stephen IX, who had retained the abbacy of Monte Cassino, came to visit and at Christmas, believing himself to be dying, ordered the monks to elect a new abbot. Their choice fell on Desiderius. The pope recovered, and, desiring to retain the abbacy during his lifetime, appointed the abbot-designate his legate for Constantinople. It was at Bari, when about to sail for the East, that the news of the pope's death reached Desiderius. Having obtained a safe-conduct from Robert Guiscard, the Norman Count (later Duke) of Apulia, he returned to his monastery and was duly installed by Cardinal Humbert on Easter Day 1058.
Pope Nicholas II elevated him into the cardinalate the Cardinal-Deacon of Santi Sergio e Bacco on 6 March 1058. He opted to be the Cardinal-Priest of Santa Cecilia in 1059.
Desiderius rebuilt the church and conventual buildings, perfected the products of the "scriptorium" and re-established monastic discipline, so that there were 200 monks in the monastery in his day. On 1 October 1071, the new Basilica of Monte Cassino was consecrated by Pope Alexander II. Desiderius' reputation brought gifts and exemptions to the abbey. The money was spent on church ornaments, including a great golden altar front from Constantinople adorned with gems and enamels and "nearly all the church ornaments of Victor II, which had been pawned here and there throughout the city". Peter the Deacon gives a list of some seventy books Desiderius had copied at Monte Cassino, including works of Saint Augustine, Saint Ambrose, Saint Bede, Saint Basil, Saint Jerome, Saint Gregory of Nazianzus and Cassian, the histories of Josephus, Paul Warnfrid, Jordanes and Saint Gregory of Tours, the "Institutes" and "Novels" of Justinian, the works of Terence, Virgil and Seneca, Cicero's "De natura deorum", and Ovid's "Fasti".
Desiderius had been appointed papal vicar for Campania, Apulia, Calabria and the Principality of Beneventum with special powers for the reform of monasteries. So great was his reputation with the Holy See that he "...was allowed by the Roman Pontiff to appoint Bishops and Abbots from among his brethren in whatever churches or monasteries he desired, of those that had lost their patron".
Within two years of the consecration of the Cassinese Basilica, Alexander II died and was succeeded by Hildebrand as Pope Gregory VII. Desiderius was able to call forth the help of the Normans of southern Italy repeatedly in favour of the Holy See. Already in 1059 he had persuaded Robert Guiscard and Richard of Capua to become vassals of St. Peter for their newly conquered territories: now Gregory VII immediately after his election sent for him to give an account of the state of Norman Italy and entrusted him with the negotiation of an interview with Robert Guiscard on 2 August 1073, at Benevento. In 1074 and 1075 he acted as intermediary, probably as Gregory's agent, between the Norman princes themselves, and even when the latter were at open war with the pope, they still maintained the best relations with Monte Cassino. At the end of 1080 Desiderius obtained Norman troops for Gregory. In 1082 he visited the Italian king and future Holy Roman Emperor Henry IV at Albano, while the troops of the Imperialist antipope were harassing the pope from Tivoli. In 1083 the peace-loving abbot joined Hugh of Cluny in an attempt to reconcile pope and emperor, and his proceedings seem to have aroused some suspicion in Gregory's entourage. In 1084, when Rome was in Henry's hands and the pope besieged in Castel Sant'Angelo, Desiderius announced the approach of Guiscard's army to both emperor and pope.He is the last pope named Victor
Papacy.
Though certainly a strong partisan of the Hildebrandine reforms, Desiderius belonged to the moderate party and could not always see eye-to-eye with Pope Gregory VII in his most intransigent proceedings. Yet when the latter lay dying at Salerno on 25 May 1085, the Abbot of Monte Cassino was one of those whom he recommended to the cardinals of southern Italy as fittest to succeed him. The Roman people had expelled the antipope Clement III from the city, and hither Desiderius hastened to consult with the cardinals on the approaching election. Finding, however, that they were bent on forcing the papal dignity upon him, he fled to Monte Cassino, where he busied himself in exhorting the Normans and Lombards to rally to the support of the Holy See. When autumn came, Desiderius accompanied the Norman army on its march to Rome. However, when he became aware of the plot between the cardinals and the Norman princes to force the papal tiara on him, he would not enter Rome unless they swore to abandon their design. They refused to do that, and the election was postponed. At about Easter the bishops and cardinals assembled at Rome summoned Desiderius and the cardinals who were with him at Monte Cassino to come to Rome to treat concerning the election.
On 23 May a great meeting was held in the deaconry of St. Lucy, and Desiderius was again importuned to accept the papacy but persisted in his refusal, threatening to return to his monastery in case of violence. On the next day, the feast of Pentecost, the same scene was repeated very early in the morning. The Roman consul Cencius now suggested the election of Odo, Cardinal-Bishop of Ostia (afterwards pope Urban II), but this was rejected by some of the cardinals on the grounds that the translation of a bishop was contrary to ecclesiastical law.
Cardinal Desiderio, O.S.B., abbot of Montecassino, was elected successor to Gregory VII on May 24, 1086 in the deaconry of S. Lucia in Septisolis and took the name Victor III. Four days later, pope and cardinals had to flee from Rome before the imperial prefect of the Eternal City, and at Terracina, in spite of all protests, Victor laid aside the papal insignia and once more retired to Monte Cassino, where he remained nearly a whole year. In the middle of Lent 1087, the pope-elect assisted at a council of cardinals and bishops held at Capua as "Papal vicar of those parts" (letter of Hugh of Lyons) together with the Norman princes, Cencius the Consul and the Roman nobles. Here, Victor finally yielded and "by the assumption of the cross and purple confirmed the past election". How much his obstinacy had irritated some of the prelates is evidenced in the letter of Hugh of Lyons preserved by Hugh of Flavigny.
Under pressure from Prince Jordan I of Capua, to whom he had also rendered important service, he was elected on 24 May 1086, taking the throne name of Victor III, but his consecration did not take place until 9 May 1087 owing to the presence of the Antipope Clement III in Rome. After celebrating Easter of 1087 in his monastery, Victor proceeded to Rome, and when the Normans had driven the soldiers of the Antipope Clement III (Guibert of Ravenna) out of St. Peter's, he was consecrated and enthroned on 9 May 1087. He only remained eight days in Rome and then returned to Monte Cassino, though with the help of Matilda and Jordan, he took back the Vatican Hill. Before May was out he was once more in Rome in answer to a summons for the countess Matilda of Tuscany, whose troops held the Leonine City and Trastevere, but when at the end of June the antipope once more gained possession of St. Peter's, Victor again withdrew at once to his Monte Cassino abbey. In August a council or synod of some importance was held at Benevento, which renewed the excommunication of the antipope Clement III and the condemnation of lay investiture, proclaimed a kind of crusade against the Saracens in northern Africa and anathematised Hugh of Lyons and Richard, Abbot of Marseilles.
When the council had lasted three days, Victor became seriously ill and retired to Monte Cassino to die. He had himself carried into the chapter-house, issued various decrees for the benefit of the abbey, appointed with the consent of the monks the prior, Cardinal Oderisius, to succeed him in the abbacy, just as he himself had been appointed by Stephen IX, and proposed Odo of Ostia to the assembled cardinals and bishops as the next pope. He died on 16 September 1087 and was buried in the tomb he had prepared for himself in the abbey's chapter-house. His successor was Pope Urban II.
Beatification.
In the sixteenth century his body was removed to the abbey church, and again translated in 1890. The cult of Blessed Victor III seems to have begun not later than the pontificate of Pope Anastasius IV, about six decades after his death (Acta Sanctorum, Loc. cit.).
In 1727 the Abbot of Monte Cassino obtained from Pope Benedict XIII permission to keep his feast (Tosti, I, 393).
Pope Leo XIII beatified Victor III.
Writings.
Victor's only existing literary work "Dialogues," is on the miracles wrought by St. Benedict and other saints at Monte Cassino. There is also a letter to the bishops of Sardinia, where (since circa 1050 brought under Pisan and Genoan control) he sent monks while still abbot of Monte Cassino.
In his "De Viris Illustribus Casinensibus", Peter the Deacon ascribes to him the composition of a "Cantus ad B. Maurum" and letters to King Philip I of France and to Hugh of Cluny, which no longer exist.

</doc>
<doc id="24387" url="https://en.wikipedia.org/wiki?curid=24387" title="Pribislav-Henry">
Pribislav-Henry

Pribislav-Henry (; d. 1150) was a Slavic Christian prince and the last ruler of the Hevelli (Stodorani) tribe in the Northern March of Brandenburg. His reign started, probably supported by the Ascanians, after the prior Hevelli prince Meinfried had been murdered in 1127. Having no sons of his own, he around 1129 gave the area between Brandenburg and Lehnin to his son-in-law, who was the oldest son of Albert the Bear. Emperor Lothair III approved the gift and made Albert margrave of the Northern March in 1134. In 1150, Pribislav Henry died and was succeeded, after a short war of succession, by Albert the Bear.

</doc>
<doc id="24388" url="https://en.wikipedia.org/wiki?curid=24388" title="Political science">
Political science

Political science is a social science discipline that deals with systems of government and the analysis of political activity and political behavior. It deals extensively with the theory and practice of politics which is commonly thought of as the determining of the distribution of power and resources. Political scientists "see themselves engaged in revealing the relationships underlying political events and conditions, and from these revelations they attempt to construct general principles about the way the world of politics works."
Political science is related to and draws upon the fields of economics, law, sociology, history, philosophy, geography, psychology, and anthropology.
Although it was codified in the 19th century, when the contemporary form of the academic social sciences was established, the study of political science has ancient roots that can be traced back to the works of Aristotle, Plato, and Chanakya which were written nearly 2,500 years ago. Political science is commonly divided into distinct sub-disciplines which together constitute the field:
Comparative politics is the science of comparison and teaching of different types of constitutions, political actors, legislature and associated fields, all of them from an intrastate perspective. International relations deals with the interaction between nation-states as well as intergovernmental and transnational organizations. Political theory is more concerned with contributions of various classical and contemporary thinkers and philosophers.
Political science is methodologically diverse and appropriates many methods originating in social research. Approaches include positivism, interpretivism, rational choice theory, behavioralism, structuralism, post-structuralism, realism, institutionalism, and pluralism. Political science, as one of the social sciences, uses methods and techniques that relate to the kinds of inquiries sought: primary sources such as historical documents and official records, secondary sources such as scholarly journal articles, survey research, statistical analysis, case studies, experimental research and model building.
Overview.
Political scientists study matters concerning the allocation and transfer of power in decision making, the roles and systems of governance including governments and international organizations, political behavior and public policies. They measure the success of governance and specific policies by examining many factors, including stability, justice, material wealth, peace and public health. Some political scientists seek to advance positive (attempt to describe how things are, as opposed to how they should be) theses by analyzing politics. Others advance normative theses, by making specific policy recommendations.
Political scientists provide the frameworks from which journalists, special interest groups, politicians, and the electorate analyze issues. According to Chaturvedy, "...Political scientists may serve as advisers to specific politicians, or even run for office as politicians themselves. Political scientists can be found working in governments, in political parties or as civil servants. They may be involved with non-governmental organizations (NGOs) or political movements. In a variety of capacities, people educated and trained in political science can add value and expertise to corporations. Private enterprises such as think tanks, research institutes, polling and public relations firms often employ political scientists." In the United States, political scientists known as "Americanists" look at a variety of data including constitutional development, elections, public opinion and public policy such as Social Security reform, foreign policy, US Congressional committees, and the US Supreme Court — to name only a few issues.
Political science, possibly like the social sciences as a whole, "as a discipline lives on the fault line between the 'two cultures' in the academy, the sciences and the humanities." Thus, in some American colleges where there is no separate School or College of Arts and Sciences per se, political science may be a separate department housed as part of a division or school of Humanities or Liberal Arts. Whereas classical political philosophy is primarily defined by a concern for Hellenic and Enlightenment thought, political scientists are also marked by a great concern for "modernity" and the contemporary nation state, along with the study of classical thought, and as such share a greater deal of terminology with sociologists (e.g. structure and agency).
Most United States colleges and universities offer B.A. programs in political science. M.A. or M.A.T. and Ph.D. or Ed.D. programs are common at larger universities. The term "political science" is more popular in North America than elsewhere; other institutions, especially those outside the United States, see political science as part of a broader discipline of "political studies," "politics," or "government." While "political science" implies use of the scientific method, "political studies" implies a broader approach, although the naming of degree courses does not necessarily reflect their content. Separate degree granting programs in international relations and public policy are not uncommon at both the undergraduate and graduate levels. Master's level programs in political science are common when political scientists engage in public administration.
The national honor society for college and university students of government and politics in the United States is Pi Sigma Alpha.
Modern political science.
Modern political science was founded by Niccolò Machiavelli. Because political science is essentially a study of human behavior, in all aspects of politics, observations in controlled environments are often challenging to reproduce or duplicate, though experimental methods are increasingly common (see experimental political science). Citing this difficulty, former American Political Science Association President Lawrence Lowell once said "We are limited by the impossibility of experiment. Politics is an observational, not an experimental science." Because of this, political scientists have historically observed political elites, institutions, and individual or group behavior in order to identify patterns, draw generalizations, and build theories of politics.
Like all social sciences, political science faces the difficulty of observing human actors that can only be partially observed and who have the capacity for making conscious choices unlike other subjects such as non-human organisms in biology or inanimate objects as in physics. Despite the complexities, contemporary political science has progressed by adopting a variety of methods and theoretical approaches to understanding politics and methodological pluralism is a defining feature of contemporary political science.
The advent of political science as a university discipline was marked by the creation of university departments and chairs with the title of political science arising in the late 19th century. In fact, the designation "political scientist" is typically for those with a doctorate in the field, but can also apply to those with a master's in the subject. Integrating political studies of the past into a unified discipline is ongoing, and the history of political science has provided a rich field for the growth of both normative and positive political science, with each part of the discipline sharing some historical predecessors. The American Political Science Association was founded in 1903 and the American Political Science Review was founded in 1906 in an effort to distinguish the study of politics from economics and other social phenomena.
Behavioral revolution and new institutionalism.
In the 1950s and the 1960s, a behavioral revolution stressing the systematic and rigorously scientific study of individual and group behavior swept the discipline. A focus on studying political behavior, rather than institutions or interpretation of legal texts, characterized early behavioral political science, including work by Robert Dahl, Philip Converse, and in the collaboration between sociologist Paul Lazarsfeld and public opinion scholar Bernard Berelson.
The late 1960s and early 1970s witnessed a take off in the use of deductive, game theoretic formal modeling techniques aimed at generating a more analytical corpus of knowledge in the discipline. This period saw a surge of research that borrowed theory and methods from economics to study political institutions, such as the United States Congress, as well as political behavior, such as voting. William H. Riker and his colleagues and students at the University of Rochester were the main proponents of this shift.
Despite considerable research progress in the discipline based on all the kinds of scholarship discussed above, it has been observed that progress toward systematic theory has been modest and uneven.
Political science in the Soviet Union.
In the Soviet Union, political studies were carried out under the guise of some other disciplines like theory of state and law, area studies, international relations, studies of labor movement, "critique of bourgeois theories", etc. Soviet scholars were represented at the International Political Science Association (IPSA) since 1955 (since 1960 by the Soviet Association of Political and State Studies).
In 1979, the 11th World Congress of IPSA took place in Moscow. Until the late years of the Soviet Union, political science as a field was subjected to tight control of the Communist Party of the Soviet Union and was thus subjected to distrust. Anti-communists accused political scientists of being "false" scientists and of having served the old regime.
After the fall of the Soviet Union, two of the major institutions dealing with political science, the Institute of Contemporary Social Theories and the Institute of International Affairs, were disbanded, and most of their members were left without jobs. These institutes were victims of the first wave of anticommunist opinion and ideological attacks. Today, the Russian Political Science Association unites professional political scientists from all around Russia.
Recent developments.
In 2000, the Perestroika Movement in political science was introduced as a reaction against what supporters of the movement called the mathematicization of political science. Those who identified with the movement argued for a plurality of methodologies and approaches in political science and for more relevance of the discipline to those outside of it.
Evolutionary psychology theories argue that humans have evolved a highly developed set of psychological mechanisms for dealing with politics. However, these mechanisms evolved for dealing with the small group politics that characterized the ancestral environment and not the much larger political structures in today's world. This is argued to explain many important features and systematic cognitive biases of current politics.
Cognate Fields.
Most political scientists work broadly in one or more of the following five areas:
Some political science departments also classify methodology as well as scholarship on the domestic politics of a particular country as distinct fields. In the United States, American politics is often treated as a separate subfield.
In contrast to this traditional classification, some academic departments organize scholarship into thematic categories, including political philosophy, political behavior (including public opinion, collective action, and identity), and political institutions (including legislatures and international organizations). Political science conferences and journals often emphasize scholarship in more specific categories. The American Political Science Association, for example, has 42 organized sections that address various methods and topics of political inquiry.
History.
Political science as a separate field is a rather late arrival in terms of social sciences. However, the term "political science" was not always distinguished from political philosophy, and the modern discipline has a clear set of antecedents including also moral philosophy, political economy, political theology, history, and other fields concerned with normative determinations of what ought to be and with deducing the characteristics and functions of the ideal state.

</doc>
<doc id="24389" url="https://en.wikipedia.org/wiki?curid=24389" title="Public relations">
Public relations

Public relations (PR) is the practice of managing the spread of information between an individual or an organization (such as a business, government agency, or a nonprofit organization) and the public. Public relations may include an organization or individual gaining exposure to their audiences using topics of public interest and news items that do not require direct payment. This differentiates it from advertising as a form of marketing communications. Public relations is the idea of creating coverage for clients for free, rather than marketing or advertising. An example of good public relations would be generating an article featuring a client, rather than paying for the client to be advertised next to the article. The aim of public relations is to inform the public, prospective customers, investors, partners, employees and other stakeholders and ultimately persuade them to maintain a certain view about the organization, its leadership, products, or political decisions. Public relations professionals typically work for PR and marketing firms, businesses and companies, government, government agencies and public officials as PIOs and nongovernmental organizations and nonprofit organizations. Jobs central to Public Relations include account coordinator, account executive, account supervisor and media relations manager.
Public relations specialists establish and maintain relationships with an organization's target audience, the media and other opinion leaders. Common responsibilities include designing communications campaigns, writing news releases and other content for news, working with the press, arranging interviews for company spokespeople, writing speeches for company leaders, acting as an organization's spokesperson, preparing clients for press conferences, media interviews and speeches, writing website and social media content, managing company reputation (crisis management), managing internal communications, and marketing activities like brand awareness and event management Success in the field of public relations requires a deep understanding of the interests and concerns of each of the client's many publics. The public relations professional must know how to effectively address those concerns using the most powerful tool of the public relations trade, which is publicity.
Definition.
Ivy Lee, the man who turned around the Rockefeller name and image, and his friend, Edward Louis Bernays, established the first definition of public relations in the early 1900s as follows: "a management function, which tabulates public attitudes, defines the policies, procedures and interests of an organization... followed by executing a program of action to earn public understanding and acceptance." However, when PR pioneer Ivy Lee was later asked about his role in a hearing with the United Transit Commission, he said "I have never been able to find a satisfactory phrase to describe what I do." In 1948, historian Eric Goldman noted that the definition of public relations in Webster's would be "disputed by both practitioners and critics in the field."
According to Edward Bernays, the public relations counsel is the agent working with both modern media of communications and group formations of society in order to provide ideas to the public’s consciousness. Furthermore, he is also concerned with ideologies and courses of actions as well as material goods and services and public utilities and industrial associations and large trade groups for which it secures popular support.
In August 1978, the World Assembly of Public Relations Associations defined the field as "the art and social science of analyzing trends, predicting their consequences, counseling organizational leaders and implementing planned programs of action, which will serve both the organization and the public interest."
Public Relations Society of America, a professional trade association, defined public relations in 1982 as: "Public relations helps an organization and its publics adapt mutually to each other."
In 2011 and 2012, the PRSA developed a crowd-sourced definition:
"Public relations is a strategic communication process that builds mutually beneficial relationships between organizations and their publics."
Public relations can also be defined as the practice of managing communication between an organization and its publics.
History.
Most textbooks consider the establishment of the Publicity Bureau in 1900 to be the founding of the public relations profession. However academics have found early forms of public influence and communications management in ancient civilizations, during the settling of the New World and during the movement to abolish slavery in England. Basil Clark is considered the founder of public relations in the United Kingdom for his establishment of Editorial Services in 1924.
Propaganda was used by the United States, the United Kingdom, Germany and others to rally for domestic support and demonize enemies during the World Wars, which led to more sophisticated commercial publicity efforts as public relations talent entered the private sector. Most historians believe public relations became established first in the US by Ivy Lee or Edward Bernays, then spread internationally. Many American companies with PR departments spread the practice to Europe when they created European subsidiaries as a result of the Marshall plan.
The second half of the 1900s is considered the professional development building era of public relations. Trade associations, PR news magazines, international PR agencies and academic principles for the profession were established. In the early 2000s, press release services began offering social media press releases. The Cluetrain Manifesto, which predicted the impact of social media in 1999, was controversial in its time, but by 2006 the effect of social media and new internet technologies became broadly accepted.
Salaries and growth.
The U.S Bureau of Labor Statistics reported that in 2014, the median annual salary for public relations practitioners was $55,680. The top ten percent in the field made around $105,720 and the bottom ten percent made around $31,190.
For public relations managers, however, the median annual wage in 2011 was $93,310. Workers in the 90th percentile earned around $176,400, and workers in the 10th percentile earned $50,360, according to the U.S. Department of Labor.
The U.S. Bureau of Labor Statistics also projects an employment growth of 12 percent between 2012 and 2022 for the profession, where an additional 27,400 jobs will need to be filled. The public relations profession has claimed the No. 75 spot on the 2014 U.S. News & World Report list of Best Jobs because of its promising direction.
In the United States, public relations professionals earn an average annual salary of $49,800 which compares with £40,000 ($68,880) for a practitioner with a similar job in the UK. Top earners make around $89,220 annually, while entry-level public relations specialists earn around $28,080. Corporate, or in-house communications is generally more profitable, and communications executives can earn salaries in the mid six-figures, though this only applies to a fraction of the sector's workforce.
According to the 2015 PRWeek/Bloom, Gross & Associates Salary Survey, the median salary at PR firms was $90,000, a 5.9% increase from $85,000 in the 2014 survey.
The role of public relations professionals is changing because of the shift from traditional to online media. Many PR professionals are finding it necessary to learn new skills and to understand how social media can impact upon a brand's reputation.
Tactics.
Public relations professionals present the face of an organization or individual, usually to articulate its objectives and official views on issues of relevance, primarily to the media. Public relations contributes to the way an organization is perceived by influencing the media and maintaining relationships with stakeholders. According to Dr. Jacquie L’Etang from Queen Margaret University, public relations professionals can be viewed as "discourse workers specializing in communication and the presentation of argument and employing rhetorical strategies to achieve managerial aims."
Specific public relations disciplines include:
Building and managing relationships with those who influence an organization or individual’s audiences has a central role in doing public relations. After a public relations practitioner has been working in the field, they accumulate a list of relationships that become an asset, especially for those in media relations.
Within each discipline, typical activities include publicity events, speaking opportunities, press releases, newsletters, blogs, social media, press kits and outbound communication to members of the press. Video and audio news releases (VNRs and ANRs) are often produced and distributed to TV outlets in hopes they will be used as regular program content.
Audience targeting.
A fundamental technique used in public relations is to identify the target audience and to tailor messages to be relevant to each audience. Sometimes the interests of differing audiences and stakeholders common to a public relations effort necessitate the creation of several distinct but complementary messages. These messages however should be relevant to each other, thus creating a consistency to the overall message and theme. Audience targeting tactics are important for public relations practitioners because they face all kinds of problems: low visibility, lack of public understanding, opposition from critics and insufficient support from funding sources.
On the other hand, stakeholder theory identifies people who have a stake in a given institution or issue. All audiences are stakeholders (or presumptive stakeholders), but not all stakeholders are audiences. For example, if a charity commissions a public relations agency to create an advertising campaign to raise money to find a cure for a disease, the charity and the people with the disease are stakeholders, but the audience is anyone who is likely to donate money. Public relations experts possess deep skills in media relations, market positioning and branding. They are powerful agents that help clients deliver clear, unambiguous information to a target audience that matters to them.
Messaging.
Messaging is the process of creating a consistent story around a product, person, company or service. Messaging aims to avoid having readers receive contradictory or confusing information that will instill doubt in their purchasing choice or other decisions that have an impact on the company. Brands aim to have the same problem statement, industry viewpoint or brand perception shared across sources and media.
Social media marketing.
Digital marketing is the use of Internet tools and technologies such as search engines, Web 2.0 social bookmarking, new media relations, blogging and social media marketing. Interactive PR allows companies and organizations to disseminate information without relying solely on mainstream publications and communicate directly with the public, customers and prospects.
PR practitioners have always relied on the media such as TV, radio and magazines, to promote their ideas and messages tailored specifically to a target audience. Social media marketing is not only a new way to achieve that goal, it is also a continuation of a strategy that existed for decades. Lister et al. said that "Digital media can be seen as a continuation and extension of a principal or technique that was already in place". 
PR professionals are well aware of the fact that digital technology is used in a practically different way than before. For instance, cellphones are no longer just devices we use to talk to one another. They are also used for online shopping, dating, learning and getting the most up to date news around the world.
As digital technology has evolved, the methods to measure effective online public relations effectiveness have improved. The Public Relations Society of America, which has been developing PR strategies since 1947, identified 5 steps to measure online public relations effectiveness. 
Other techniques.
Litigation public relations is the management of the communication process during the course of any legal dispute or adjudicatory processing so as to affect the outcome or its impact on the client’s overall reputation (Haggerty, 2003).
Ethics.
Public Relations professionals both serve the public's interest and private interests of businesses, associations, non-profit organizations and governments. This dual obligation gave rise to heated debates among scholars of the discipline and practitioners over its fundamental values. This conflict represents the main ethical predicament of public relations. In 2000, the Public Relations Society of America (PRSA) responded to the controversy by acknowledging in its new code of ethics "advocacy" – for the first time – as a core value of the discipline.
The field of public relations is generally highly un-regulated, but many professionals voluntarily adhere to the code of conduct of one or more professional bodies to avoid exposure for ethical violations. The Chartered Institute of Public Relations, the Public Relations Society of America and The Institute of Public Relations are a few organizations that publish an ethical code. Still, Edelman's 2003 semi-annual trust survey found that only 20 percent of survey respondents from the public believed paid communicators within a company were credible. Public relations people are growing increasingly concerned with their company’s marketing practices, questioning whether they agree with the company’s social responsibility. They seek more influence over marketing and more of a counseling and policy-making role. On the other hand, marketing people are increasingly interested in incorporating publicity as a tool within the realm marketing.
According to Scott Cutlip, the social justification for public relations is the right for an organization to have a fair hearing of their point of view in the public forum, but to obtain such a hearing for their ideas requires a skilled advocate.
Spin.
Spin has been interpreted historically to mean overt deceit meant to manipulate the public, but since the 1990s has shifted to describing a "polishing of the truth." Today spin refers to providing a certain interpretation of information meant to sway public opinion. Companies may use spin to create the appearance of the company or other events are going in a slightly different direction than they actually are. Within the field of public relations, spin is seen as a derogatory term, interpreted by professionals as meaning blatant deceit and manipulation. Skilled practitioners of spin are sometimes called "spin doctors."
In Stuart Ewen’s "PR! A Social History of Spin", he argues that public relations can be a real menace to democracy as it renders the public discourse powerless. Corporations are able to hire public relations professionals and transmit their messages through the media channels and exercise a huge amount of influence upon the individual who is defenseless against such a powerful force. He claims that public relations is a weapon for capitalist deception and the best way to resist is to become media literate and use critical thinking when interpreting the various mediated messages.
The techniques of spin include selectively presenting facts and quotes that support ideal positions (cherry picking), the so-called "non-denial denial," phrasing that in a way presumes unproven truths, euphemisms for drawing attention away from items considered distasteful, and ambiguity in public statements. Another spin technique involves careful choice of timing in the release of certain news so it can take advantage of prominent events in the news.
Negative.
Negative public relations, also called dark public relations (DPR) and in some earlier writing "Black PR", is a process of destroying the target's reputation and/or corporate identity. The objective in DPR is to discredit someone else, who may pose a threat to the client's business or be a political rival. DPR may rely on IT security, industrial espionage, social engineering and competitive intelligence. Common techniques include using dirty secrets from the target, producing misleading facts to fool a competitor. In politics, a decision to use negative PR is also known as negative campaigning.
Politics and civil society.
In "Propaganda" (1928), Bernays argued that the manipulation of public opinion was a necessary part of democracy. In public relations, lobby groups are created to influence government policy, corporate policy or public opinion, typically in a way that benefits the sponsoring organization.
In fact, Edward Bernays stresses that we are in fact dominated in almost every aspect of our lives, by a relatively small number of persons who have mastered the ‘mental processes and social patterns of the masses,’ which include our behavior, political and economic spheres or our morals. In theory, each individual chooses his own opinion on behavior and public issues. However, in practice, it is impossible for one to study all variables and approaches of a particular question and come to a conclusion without any external influence. This is the reason why the society has agreed upon an ‘invisible government’ to interpret on our behalf information and narrow the choice field to a more practical scale.
When a lobby group hides its true purpose and support base, it is known as a front group. Front groups are a form of astroturfing, because they intend to sway the public or the government without disclosing their financial connection to corporate or political interests. They create a fake grass-roots movement by giving the appearance of a trusted organization that serves the public, when they actually serve their sponsors.
Politicians also employ public relations professionals to help project their views, policies and even personalities to their best advantages.

</doc>
<doc id="24390" url="https://en.wikipedia.org/wiki?curid=24390" title="Paradox">
Paradox

A paradox is a statement that contradicts itself and yet might be true (or wrong at the same time). Some logical paradoxes are known to be invalid arguments but are still valuable in promoting critical thinking.
Some paradoxes have revealed errors in definitions assumed to be rigorous, and have caused axioms of mathematics and logic to be re-examined. One example is Russell's paradox, which questions whether a "list of all lists that do not contain themselves" would include itself, and showed that attempts to found set theory on the identification of sets with properties or predicates were flawed. Others, such as Curry's paradox, are not yet resolved.
Examples outside logic include the Ship of Theseus from philosophy (questioning whether a ship repaired over time by replacing each of its wooden parts would remain the same ship). Paradoxes can also take the form of images or other media. For example, M.C. Escher featured perspective-based paradoxes in many of his drawings, with walls that are regarded as floors from other points of view, and staircases that appear to climb endlessly.
In common usage, the word "paradox" often refers to statements that are ironic or unexpected, such as "the paradox that standing is more tiring than walking".
Logical paradox.
Common themes in paradoxes include self-reference, infinite regress, circular definitions, and confusion between different levels of abstraction.
Patrick Hughes outlines three laws of the paradox:
Other paradoxes involve false statements ("impossible is not a word in my vocabulary", a simple paradox) or half-truths and the resulting biased assumptions. This form is common in howlers.
For example, consider a situation in which a father and his son are driving down the road. The car crashes into a tree and the father is killed. The boy is rushed to the nearest hospital where he is prepared for emergency surgery. On entering the surgery suite, the surgeon says, "I can't operate on this boy. He's my son."
The apparent paradox is caused by a hasty generalization, for if the surgeon is the boy's father, the statement cannot be true. The paradox is resolved if it is revealed that the surgeon is a woman — the boy's mother.
Paradoxes which are not based on a hidden error generally occur at the fringes of context or language, and require extending the context or language in order to lose their paradoxical quality. Paradoxes that arise from apparently intelligible uses of language are often of interest to logicians and philosophers. "This sentence is false" is an example of the well-known liar paradox: it is a sentence which cannot be consistently interpreted as either true or false, because if it is known to be false, then it is known that it must be true, and if it is known to be true, then it is known that it must be false. Russell's paradox, which shows that the notion of "the set of all those sets that do not contain themselves" leads to a contradiction, was instrumental in the development of modern logic and set theory.
Thought experiments can also yield interesting paradoxes. The grandfather paradox, for example, would arise if a time traveller were to kill his own grandfather before his mother or father had been conceived, thereby preventing his own birth. This is a specific example of the more general observation of the butterfly effect, or that a time-traveller's interaction with the past — however slight — would entail making changes that would, in turn, change the future in which the time-travel was yet to occur, and would thus change the circumstances of the time-travel itself.
Often a seemingly paradoxical conclusion arises from an inconsistent or inherently contradictory definition of the initial premise. In the case of that apparent paradox of a time traveler killing his own grandfather it is the inconsistency of defining the past to which he returns as being somehow different from the one which leads up to the future from which he begins his trip but also insisting that he must have come to that past from the same future as the one that it leads up to.
Quine's classification.
W. V. Quine (1962) distinguished between three classes of paradoxes:
A fourth kind has sometimes been described since Quine's work.
In philosophy.
A taste for paradox is central to the philosophies of Laozi, Zhuangzi, Heraclitus, Bhartrhari, Meister Eckhart, Hegel, Kierkegaard, Nietzsche, and G.K. Chesterton, among many others. Søren Kierkegaard, for example, writes, in the "Philosophical Fragments", that
But one must not think ill of the paradox, for the paradox is the passion of thought, and the thinker without the paradox is like the lover without passion: a mediocre fellow. But the ultimate potentiation of every passion is always to will its own downfall, and so it is also the ultimate passion of the understanding to will the collision, although in one way or another the collision must become its downfall. This, then, is the ultimate paradox of thought: to want to discover something that thought itself cannot think.
In medicine.
A paradoxical reaction to a drug is the opposite of what one would expect, such as becoming agitated by a sedative or sedated by a stimulant. Some are common and are used regularly in medicine, such as the use of stimulants such as Adderall and Ritalin in the treatment of attention deficit hyperactivity disorder (also known as ADHD or ADD,) while others are rare and can be dangerous as they are not expected, such as severe agitation from a benzodiazepine.

</doc>
<doc id="24393" url="https://en.wikipedia.org/wiki?curid=24393" title="Parousia">
Parousia

Parousia (; ) is an ancient Greek word meaning presence, arrival, or official visit.
Classical usage.
From the Ptolemaic period to the second century of the common era "parousia' was used in the East as a technical expression to denote the arrival or visit of a king or emperor, and celebrated the glory of the sovereign publicly. In memory of the visit of Emperor Nero to the cities of Patras and Corinth, advent coins were struck that carried the legend "Adventus Augusti Corinth". The Greek word parousia here corresponded to the Latin word advent. The numerous journeyings of the Emperor Hadrian were celebrated by many advent coins, and often new eras were reckoned from date of the parousia.
Septuagint.
The term occurs only twice in the Septuagint (2 Maccabees 8:12 and 15:21) in its ordinary meaning of arrival.
New Testament.
The word is used 24 times in the New Testament. Of these, six uses refer to the coming of individuals: Stephanas, Fortunatus, and Achaicus (1Co.16:17), Titus (2Co.7:6 & 7) the physical "presence" of Paul himself (2Co.10:10, Php.1:26, 2:12), and a 7th use to the "coming of the lawless one" (2Thess.2:9). The other seventeen uses refer to the Second Coming of Christ, except the one case in which it refers to the coming of the "Day of God" (2Pe.3:12, see also The Day of the Lord).
The word parousia is found in the following verses: Matthew 24:3, 27, 37, 39; 1 Corinthians 15:23; 1 Thessalonians 2:19; 3:13; 4:15; 5:23; 2 Thessalonians 2:1, 8, 9; James 5:7, 8; 2 Peter 1:16; 3:4, 12; 1 John 2:28.
Theological usage.
The word "parousia" is mainly used in Christian theology to refer to the second coming of Christ. 
Twentieth-century theologian Karl Barth suggested that the parousia includes not only Resurrection Sunday but also Pentecost as well. As such, Barth concluded that the New Testament parousia is not limited to Christ's final return.
Related terms.
The following Greek-English words may be related to, and can be distinguished from, "parousia":

</doc>
<doc id="24397" url="https://en.wikipedia.org/wiki?curid=24397" title="Paul J. McAuley">
Paul J. McAuley

Paul J. McAuley (born 23 April 1955) is a British botanist and science fiction author.
A biologist by training, McAuley writes mostly hard science fiction, dealing with themes such as biotechnology, alternate history/alternate reality, and space travel.
McAuley began with far-future space opera "Four Hundred Billion Stars", its sequel "Eternal Light", and the planetary-colony adventure "Of the Fall". "Red Dust", set on a far-future Mars colonized by the Chinese, is a planetary romance featuring many emerging technologies and SF motifs: nanotechnology, biotechnology, artificial intelligence, personality downloads, virtual reality. The Confluence series, set in an even more distant future (about ten million years from now), is one of a number of novels to use Frank J. Tipler's Omega Point Theory (that the universe seems to be evolving toward a maximum degree of complexity and consciousness) as one of its themes. 
About the same time, he published "Pasquale's Angel", set in an alternate Italian Renaissance and featuring Niccolò Machiavegli (Machiavelli) and Leonardo da Vinci as major characters.
McAuley has also used biotechnology and nanotechnology themes in near-future settings: "Fairyland" describes a dystopian, war-torn Europe where genetically engineered "dolls" are used as disposable slaves. Since 2001 he has produced several SF-based techno-thrillers such as "The Secret of Life", "Whole Wide World", and "White Devils".
"Four Hundred Billion Stars", his first novel, won the Philip K. Dick Award in 1988. "Fairyland" won the 1996 Arthur C. Clarke Award and the 1997 John W. Campbell Memorial Award for Best SF Novel. "The Temptation of Dr. Stein", won the British Fantasy Award. "Pasquale's Angel" won the Sidewise Award for Alternate History (Long Form).

</doc>
<doc id="24399" url="https://en.wikipedia.org/wiki?curid=24399" title="PDP-11">
PDP-11

The PDP-11 is a series of 16-bit minicomputers sold by Digital Equipment Corporation (DEC) from 1970 into the 1990s, one of a succession of products in the PDP series. The PDP-11 had several uniquely innovative features, and was easier to program than its predecessors through the additional general-purpose registers. The PDP-11 replaced the PDP-8 in many real-time applications, although both product lines lived in parallel for more than 10 years. In total, around 600,000 PDP-11s of all models were sold, making it one of DEC's most successful product lines. Its successor in the mid-range minicomputer niche was the 32-bit VAX-11, named as a nod to the PDP-11's popularity.
The PDP-11 is considered by some experts to be the most popular minicomputer ever.
Design features of the PDP-11 influenced the design of most late-1970s computer systems including the Intel x86 and the Motorola 68000.
Design features of PDP-11 operating systems, as well as other operating systems from Digital Equipment, influenced the design of other operating systems such as CP/M and hence also MS-DOS. For a decade PDP-11 was the smallest system that could run Unix; the first officially named version ran on the PDP-11/20 in 1970. It is commonly stated that the C programming language took advantage of several low-level PDP-11–dependent programming features, albeit not originally by design.
History.
In 1967–68, DEC engineers designed a 16-bit, word-addressed machine. Management cancelled the project and some of the engineers later left DEC and produced it as the Data General Nova. A subsequent effort, code-named "Desk Calculator", looked at a variety of options before choosing what became the 16-bit PDP-11; DEC's previous PDP-8 and PDP-9 had 12- and 18-bit words, respectively. The PDP-11 family was announced in January 1970 and shipments began early that year. DEC sold over 170,000 PDP-11s in the 1970s. Initially manufactured of small-scale transistor–transistor logic, a single-board large scale integration version of the processor was developed in 1975. A single-chip processor, the J-11 was developed in 1979. The last models of the PDP-11 line were the PDP-11/94 and -11/93 introduced in 1990.
Innovative features.
Instruction set orthogonality.
The PDP-11 processor architecture has a mostly orthogonal instruction set. For example, instead of instructions such as "load" and "store", the PDP-11 has a "move" instruction for which either operand (source and destination) can be memory or register. There are no specific "input" or "output" instructions; the PDP-11 uses memory-mapped I/O and so the same "move" instruction is used; orthogonality even enables moving data directly from an input device to an output device. More complex instructions such as "add" likewise can have memory, register, input, or output as source or destination.
Most operands can apply any of eight addressing modes to eight registers. The addressing modes provide register, immediate, absolute, relative, deferred (indirect), and indexed addressing, and can specify autoincrementation and autodecrementation of a register by one (byte instructions) or two (word instructions). Use of relative addressing lets a machine-language program be position-independent.
No dedicated I/O instructions.
Early models of the PDP-11 had no dedicated bus for input/output, but only a system bus called the Unibus, as input and output devices were mapped to memory addresses.
An input/output device determined the memory addresses to which it would respond, and specified its own interrupt vector and interrupt priority. This flexible framework provided by the processor architecture made it unusually easy to invent new bus devices, including devices to control hardware that had not been contemplated when the processor was originally designed. DEC openly published the basic Unibus specifications, even offering prototyping bus interface circuit boards, and encouraging customers to develop their own Unibus-compatible hardware.
The Unibus made the PDP-11 suitable for custom peripherals. One of the predecessors of Alcatel-Lucent, the Bell Telephone Manufacturing Company, developed the BTMC DPS-1500 packet-switching (X.25) network and used PDP-11s in the regional and national network management system, with the Unibus directly connected to the DPS-1500 hardware.
Higher-performance members of the PDP-11 family, starting with the PDP-11/45 Unibus and 11/83 Q-bus systems, departed from the single-bus approach. Instead, memory was interfaced by dedicated circuitry and space in the CPU cabinet, while the Unibus continued to be used for I/O only. In the PDP-11/70, this was taken a step further, with the addition of a dedicated interface between disks and tapes and memory, via the Massbus. Although input/output devices continued to be mapped into memory addresses, some additional programming was necessary to set up the added bus interfaces.
Interrupts.
The PDP-11 supports hardware interrupts at four priority levels. Interrupts are serviced by software service routines, which could specify whether they themselves could be interrupted (achieving interrupt nesting). The event that causes the interrupt is indicated by the device itself, as it informs the processor of the address of its own interrupt vector.
Interrupt vectors are blocks of two 16-bit words in low kernel address space (which normally corresponded to low physical memory) between 0 and 776. The first word of the interrupt vector contains the address of the interrupt service routine and the second word the value to be loaded into the PSW (priority level) on entry to the service routine.
The article on PDP-11 architecture provides more details on interrupts.
Designed for mass production.
The PDP-11 was designed for ease of manufacture by semiskilled labor. The dimensions of its pieces were relatively non-critical. It used a wire-wrapped backplane. That is, the printed circuit boards plugged into a backplane connector. The backplane connectors had square pins that could be connected to by wrapping wires around them. The corners of the pins would bite into the wire to form a gas-tight (i.e. corrosion-proof, therefore reliable) connection.
LSI-11.
The LSI-11 (PDP-11/03), introduced in February 1975 is the first PDP-11 model produced using large-scale integration; the entire CPU is contained on four LSI chips made by Western Digital (the MCP-1600 chip set; a fifth chip can be added to extend the instruction set, as pictured on the right). It uses a bus which is a close variant of the Unibus called the LSI Bus or Q-Bus; it differs from the Unibus primarily in that addresses and data are multiplexed onto a shared set of wires, as opposed to having separate sets of wires, as in the Unibus. It also differs slightly in how it addresses I/O devices and it eventually allowed a 22-bit physical address (whereas the Unibus only allows an 18-bit physical address) and block-mode operations for significantly improved bandwidth (which the Unibus does not support).
The CPU microcode includes a debugger: firmware with a direct serial interface (RS-232 or current loop) to a terminal. This let the operator do debugging by typing commands and reading octal numbers, rather than operating switches and reading lights, the typical debugging method at the time. The operator can thus examine and modify the computer's registers, memory, and input/output devices, diagnosing and perhaps correcting failures in software and peripherals (unless a failure disables the microcode itself). The operator can also specify which disk to boot from.
Both innovations increased the reliability and decreased the cost of the LSI-11.
Later Q-Bus based systems such as the LSI-11/23, /73, and /83 are based upon chip sets designed in house by Digital Equipment Corporation. Later PDP-11 Unibus systems were designed to use similar Q-Bus processor cards, using a Unibus adapter to support existing Unibus peripherals, sometimes with a special memory bus for improved speed.
There were other significant innovations in the Q-Bus lineup. For example, a system variant of the PDP-11/03 introduced full system Power-on self-test (POST).
Decline.
The basic design of the PDP-11 was flexible, and was continually updated to use newer technologies. However, the limited throughput of the Unibus and Q-bus started to become a system-performance bottleneck, and the 16-bit logical address limitation hampered the development of larger software applications. The article on PDP-11 architecture describes the hardware and software techniques used to work around address-space limitations.
DEC's 32-bit successor to the PDP-11, the VAX (for "Virtual Address eXtension") overcame the 16-bit limitation, but was initially a superminicomputer aimed at the high-end time-sharing market. The early VAXes provided a PDP-11 compatibility mode under which much existing software could be immediately used, in parallel with newer 32-bit software.
In the 1980s, the IBM PC and its clones largely took over the small computer market; "BYTE" in 1984 reported that Venix on the PC's Intel 8088 microprocessor outperformed the same operating system on the PDP-11/23. Newer microprocessors such as the Motorola 68000 (1979) and Intel 80386 (1985) also included 32-bit logical addressing. The mass-production of those chips eliminated any cost advantage for the 16-bit PDP-11. A line of personal computers based on the PDP-11, the DEC Professional series, failed commercially, along with other non-PDP-11 PC offerings from DEC.
In 1994 DEC sold the PDP-11 system-software rights to Mentec Inc., an Irish producer of LSI-11 based boards for Q-Bus and ISA architecture personal computers, and in 1997 discontinued PDP-11 production. For several years, Mentec produced new PDP-11 processors. Other companies found a niche market for replacements for legacy PDP-11 processors, disk subsystems, etc.
By the late 1990s, not only DEC but most of the New England computer industry which had been built around minicomputers similar to the PDP-11 collapsed in the face of microcomputer-based workstations and servers.
Models.
The PDP-11 processors tend to fall into several natural groups depending on the original design upon which they are based and which I/O bus they use. Within each group, most models were offered in two versions, one intended for OEMs and one intended for end-users. Although all models share the same instruction set, later models added new instructions and interpreted certain instructions slightly differently. As the architecture evolved, there were also variations in handling of some processor status and control registers.
Unibus models.
The following models use the Unibus as their principal bus:
Q-bus models.
The following models use the Q-Bus as their principal bus:
Models without standard bus.
The PDT series were desktop systems marketed as "smart terminals". The /110 and /130 were housed in a VT100 terminal enclosure. The /150 was housed in a table-top unit which included two 8-inch floppy drives, three asynchronous serial ports, one printer port, one modem port and one synchronous serial port and required an external terminal. All three employed the same chipset as used on the LSI-11/03 and LSI-11/2 in four "microm"s. There is an option which combines two of the microms into one dual carrier, freeing one socket for an EIS/FIS chip. The /150 in combination with a VT105 terminal was also sold as MiniMINC, a budget version of the MINC-11.
The DEC Professional series are desktop PCs intended to compete with IBM's earlier 8088 and 80286 based personal computers. The models are equipped with 5¼ inch floppy disk drives and hard disks, except the 325 which has no hard disk. The original operating system was P/OS, which was essentially RSX-11M+ with a menu system on top. As the design was intended to avoid software exchange with existing PDP-11 models, their ill fate in the market was no surprise for anyone except DEC. The RT-11 operating system was eventually ported to the PRO series. A port of RSTS/E to the PRO series was also done internal to DEC, but it was not released. The PRO-325 and -350 units are based on the DCF-11 ("Fonz") chipset, the same as found in the 11/23, 11/23+ and 11/24. The PRO-380 is based on the DCJ-11 ("Jaws") chipset, the same as found in the 11/53,73,83 and others, though running only at 10 MHz because of limitations in the support chipset.
Unauthorized clones.
The PDP-11 was sufficiently popular that many unauthorized PDP-11-compatible minicomputers and microcomputers were produced in Eastern Bloc countries. Some were pin-compatible with the PDP-11 and could use its peripherals and system software. These include:
Operating systems.
Several operating systems were available for the PDP-11
Peripherals.
A wide range of peripherals were available; some of them were also used in other DEC systems like the PDP-8 or PDP-10.
The following are some of the more common PDP-11 peripherals.
Use.
The PDP-11 family of computers was used for many purposes. It was used as a standard minicomputer for general-purpose computing, such as timesharing, scientific, educational, medical, or business computing. Another common application was real-time process control and factory automation.
Some OEM models were also frequently used as embedded systems to control complex systems like traffic-light systems, medical systems, numerical controlled machining, or for network-management. An example of such use of PDP-11s was the management of the packet switched network Datanet 1. In the 1980s, the UK's air traffic control radar processing was conducted on a PDP 11/34 system known as PRDS – Processed Radar Display System at RAF West Drayton. The software for the Therac-25 medical linear particle accelerator also ran on a 32K PDP 11/23.
In 2013, it was reported that PDP-11 programmers would be needed to control nuclear power plants through 2050.
Another use was for storage of test programs for Teradyne ATE equipment, in a system known as the TSD (Test System Director). As such, they were in use until their software was rendered inoperable by the Year 2000 problem. The U.S. Navy used a PDP-11/34 to control its Multi-station Spatial Disorientation Device, a simulator used in pilot training, until 2007, when it was replaced by a PC-based emulator that could run the original PDP-11 software and interface with custom Unibus controller cards.
A PDP-11/45 was used for the experiment that discovered the J/ψ meson at the Brookhaven National Laboratory. In 1976, Samuel C. C. Ting received the Nobel Prize for this discovery.

</doc>
<doc id="24400" url="https://en.wikipedia.org/wiki?curid=24400" title="Pair programming">
Pair programming

Pair programming is an agile software development technique in which two programmers work together at one workstation. One, the "driver", writes code while the other, the "observer" or "navigator", reviews each line of code as it is typed in. The two programmers switch roles frequently.
While reviewing, the observer also considers the "strategic" direction of the work, coming up with ideas for improvements and likely future problems to address. This frees the driver to focus all of his or her attention on the "tactical" aspects of completing the current task, using the observer as a safety net and guide.
Advantages.
Economics.
Pairs spend about 15% more time on programs than individuals. However, the resulting code has about 15% fewer defects. Along with code development time, other factors like field support costs and quality assurance also affect the expenses. IBM reported spending about “$250 million repairing and reinstalling fixes to 30,000 customer-reported problems”. Pair programming significantly reduces these expenses by reducing the defects in the programs.
Design quality.
A system with two programmers possesses greater potential for the generation of more diverse solutions to problems for three reasons: (1) the programmers bring different prior experiences to the task; (2) they may access information relevant to the task in different ways; (3) they stand in different relationships to the problem by virtue of their functional roles. In an attempt to share goals and plans, the programmers must overtly negotiate a shared course of action when a conflict arises between them. In doing so, they consider a larger number of ways of solving the problem than a single programmer alone might do. This significantly improves the design quality of the program as it reduces the chances of selecting a poor method.
Satisfaction.
In an online survey of pair programmers, 96% of them stated that they enjoyed their work more than when they programmed alone. Additionally, 95% of the surveyed programmers stated that they were more confident in their solutions when they pair programmed. A correlation exists between satisfaction among programmers and their confidence in the code i.e. the pairs enjoy their work more because they are more confident in it.
Learning.
Knowledge is constantly shared between pair programmers, whether in the industry or in a classroom, many sources suggest that students show higher confidence when programming in pairs, and many learn whether it be from tips on programming language rules to overall design skill. In "promiscuous pairing", each programmer communicates and works with all the other programmers on the team rather than pairing only with one partner, which causes knowledge of the system to spread throughout the whole team. Pair programming allows the programmers to examine their partner's code and provide feedback which is necessary to increase their own ability to develop monitoring mechanisms for their own learning activities.
Team-building and communication.
Pair programming allows team members to share problems and solutions quickly making them less likely to have hidden agendas from each other. This helps pair programmers to learn to communicate more easily. “This raises the communication bandwidth and frequency within the project, increasing overall information flow within the team.”
Studies.
There are both empirical studies and meta-analyses of pair programming. The empirical studies tend to examine the level of productivity and the quality of the code, while meta-analyses may focus on biases introduced by the process of testing and publishing.
Empirical studies.
Pairs typically consider more design alternatives than programmers working alone, and arrive at simpler, more maintainable designs; they also catch design defects early.
Although pair programmers may complete a task faster than a solo programmer, the total number of man-hours increases. A manager would have to balance faster completion of the work and reduced testing and debugging time against the higher cost of coding. The relative weight of these factors can vary by project and task. The benefit of pairing is greatest on tasks that the programmers do not fully understand before they begin: that is, challenging tasks that call for creativity and sophistication. On simple tasks, which the pair already fully understands, pairing results in a net drop in productivity.
Productivity can also drop when novice–novice pairing is used without sufficient availability of a mentor to coach them.
The matter of the superiority of pair programming over solo programming involves various factors like the experience and skill of the programmers and the complexity of the task. Pair programming could be helpful for attaining high quality and correctness on complex programming tasks, but it would also increase the development effort (cost) significantly. Pair programming may reduce the code development time but also risks reducing the quality of the program. Thus the meta-analysis concluded that "pair programming is not uniformly beneficial or effective".
Indicators of non-performance.
There are indicators that a pair is not performing well:
Remote pair programming.
Remote pair programming, also known as virtual pair programming or distributed pair programming, is pair programming in which the two programmers are in different locations, working via a collaborative real-time editor, shared desktop, or a remote pair programming IDE plugin. Remote pairing introduces difficulties not present in face-to-face pairing, such as extra delays for coordination, depending more on "heavyweight" task-tracking tools instead of "lightweight" ones like index cards, and loss of verbal communication resulting in confusion and conflicts over such things as who "has the keyboard".
Tool support could be provided by:

</doc>
<doc id="24401" url="https://en.wikipedia.org/wiki?curid=24401" title="Psychology of torture">
Psychology of torture

Torture, whether physical, psychological, or both, depends on complicated interpersonal relationships between victims, aggressors, bystanders, and others. Torture also involves deeply personal processes in those involved. These interacting psychological relationships, processes, and dynamics form the basis for the psychology of torture. Torture is about reprogramming the victim to succumb to an alternative exegesis of the world, proffered by the abuser. It is an act of deep, indelible, traumatic indoctrination.
The torture process to the torturer.
Motivation to torture.
Research over the past 50 years, starting with the Milgram experiment, suggests that under the right circumstances and with the appropriate encouragement and setting, most people can be encouraged to actively torture others.
John Conroy:
Confidence in the efficacy of torture is based upon the behaviorist theory of human behavior.
Stages of torture mentality include:
One of the apparent ringleaders of the Abu Ghraib prison torture incident, Charles Graner Jr., exemplified some of these when he was reported to have said, "The Christian in me says it's wrong, but the corrections officer in me says, 'I love to make a grown man piss himself.'"
As P. Saliya Sumanatilake concludes:Whether it be for securing a "justifiable" or reprehensible end, torture cannot be effectuated without invoking and focussing one’s diffused innate cruelty. Accordingly, it is the prevalence of this "congenital trait of heinousness" that renders every human being a potential torturer: hence, the existence of torture! Moreover, it is the natural occurrence of such nascent evil within each successive generation of human beings that serves to propagate torture!
Psychological effects of Torture.
Victims.
There is always a question about applying diagnostic categories and descriptions of symptoms or behavior developed in Western societies to people from the developing countries with very different personal, political, or religious beliefs and perspectives. One of the most marked differences is between individualist societies where realization of personal goals often takes priority over the needs of kin and societal expectations, and collectivist societies in which the needs of family and prescribed roles take precedence over personal preferences. Another evident difference is the belief in a subsequent life in which suffering in this life is rewarded, and this has emerged in some studies of torture survivors in South East Asia.
On a different level, the development of the diagnosis of PTSD for American veterans of the Vietnam War can be understood as a political act which labeled the collective distress of a defeated USA as individual psychopathology. Proponents of this view point to the depoliticisation of the distress of torture survivors by describing their distress, disturbance, and profound sense of injustice in psychiatric terms. These are not only conceptual issues but affect treatment, since recovery is associated with reconstruction of social and cultural networks, economic supports, and respect for human rights.
The rich research on treatment of PTSDs in veterans has substantially informed treatment offered to torture survivors. It is more appropriate than extrapolation from work with civilian survivors of single events as individuals (assault, accidents) or as communities or groups (natural or man-made disasters). Some literature distinguishes between single-event trauma (type 1) and prolonged and repeated trauma, such as torture (type 2). There is no doubt that (disregarding concerns about the diagnosis) rates of PTSD are much higher in refugees than among people of a similar age in the countries where the refugees settle, and that, among refugees, rates of PTSD are even higher among those seeking asylum.
The argument that torture causes unique problems waxes and wanes, and is often associated with claims to particular expertise in treatment, and therefore claims on funding, but Gurr et al. describe how torture targets the person as a whole – physically, emotionally, and socially – so that PTSD is an inadequate description of the magnitude and complexity of the effects of torture. When the diagnosis of PTSD is applied, some survivors of torture who have very severe symptoms related to trauma may still not reach the criteria for diagnosis. Categories such as ‘complex trauma’ have been proposed, and it may be that the next iterations of the diagnostic compendia may modify the criteria.
Torture has profound and long lasting physical and psychological effects. Torture is a form of collective suffering. It does not limit to the victim. The victims’ family members and friends are also affected due to adjustment problems including outbreaks of anger and violence directed towards family members. Based on new research psychological and physical torture have similar mental effects. Often torture victims suffer from elevated rates of Anxiety, Depression, Adjustment Disorder, PTSD, DESNOS (Disorders of Extreme Stress Not Otherwise Specified), Somatoform Disorders and sometimes psychotic manifestations, nightmares, intrusions, insomnia, decreased libido, memory lapses, reduced capacity to learn, sexual dysfunction, social withdrawal, emotional flatness and periodic headaches. Based on new research psychological and physical torture have similar mental effects. Symptoms should always be understood in the context above. No diagnostic terminology encapsulates the deep distrust of others which many torture survivors have developed, nor the destruction of all that gave their lives meaning. Guilt and shame about humiliation during torture, and about the survivor’s inability to withstand it, as well as guilt at surviving, are common problems which discourage disclosure. On top of this, uncertainty about the future, including the possibility of being sent back to the country in which the survivor was tortured, and the lack of any close confidant or even of any social support, compound the stress. Some current conditions are identifiable as additional risk factors: social isolation, poverty, unemployment, institutional accommodation, and pain can all predict higher levels of emotional distress in torture survivors.
Perpetrator.
Torture is a doubled edged sword that can harm not only the victim but the perpetrators as well. Many people who engage in torture have various psychological deviations and often they derive sadistic satisfaction. For a considerable degree, torture fulfills the emotional needs of the perpetrator when they willingly engage in these activities. They lack empathy and their victim’s agonized painful reactions, screaming and pleading give them a sense of authority and feelings of superiority. After the fact, perpetrators will often experience failing mental health, PTSD, suicidal tendencies, substance dependency and a myriad of other mental defects associated with inducing physical or mental trauma upon their victims.
The perpetrator has flashbacks of torture, intense rage, suicidal and homicidal ideas, alienation, impulse deregulation, alterations in attention and consciousness, alterations in self-perception, alterations in relationships with others, inability to trust and inability to maintain long-term relationships, or even mere intimacy.
Torture survivors in healthcare settings.
For the clinician, in medicine rather than in psychiatry, it is useful to recognise that symptoms of post-traumatic stress can complicate presentation and treatment. Pain predicts greater severity of both PTSD symptoms and major depression, and intrusive memories and flashbacks can exacerbate existing pain. While under-recognition and undertreatment of torture survivors is common, there are useful guidelines for good medical practice, although not specifically concerned with pain, and for good psychological practice.
Some people die during torture; many survivors are too disabled and destitute to find their way to safety. A large element of chance, and, to a lesser extent, resources and resilience, enable a minority to arrive in developed countries. Nevertheless, they often present multiple and complex problems, which the clinician can find overwhelming. For all these reasons, an interdisciplinary approach to assessment and treatment is therefore recommended, guarding against either disregarding significant psychological distress as inevitable in torture survivors or discounting physical symptoms by attributing them to psychological origin.
Rehabilitation and reparation are part of the rights of the torture survivor under the United Nations Convention, yet far less attention is paid to health needs on a national or international basis than to legal and civil claims. Collaborative efforts are needed, involving survivors themselves, to understand better the usefulness and limitations of existing assessment instruments and treatment methods. Some excellent studies exist, such as that by Elsass et al. who interviewed Tibetan Lamas on the quantification of suffering in scales used to evaluate intervention with Tibetan torture survivors.
Education of medical and other healthcare personnel needs to address issues concerning treatment of torture survivors, who will be seen in all possible settings but not necessarily recognised or treated adequately. Teaching on ethics is also important, since medical students can have worryingly tolerant views of torture, and medical and healthcare staff complicity in torture continues in many countries. Medical staff are often in a key position to try to prevent torture, and to help those who have survived.

</doc>
<doc id="24402" url="https://en.wikipedia.org/wiki?curid=24402" title="Pongo de Manseriche">
Pongo de Manseriche

The Pongo de Manseriche is a gorge in northwest Peru. The Marañón River runs through this gorge (and water gap) before it reaches the Amazon Basin.
The Pongo de Manseriche is 3 miles (4.8 km) long, located at 4° 27' 30" south latitude and 77° 34' 51" west longitude, just below the mouth of the Rio Santiago, and between it and the old missionary station of Borja.
According to Captain Carvajal, who descended the Pongo in the little steamer "Napo," in 1868, it is a vast rent in the Andes about 2000 ft (600 m) deep, narrowing in places to a width of only 100 ft (30 m), the precipices "seeming to close in at the top." Through this dark canyon the Marañón leaps along, at times, at the rate of 12 miles an hour (20 km/h).
The Pongo de Manseriche was first discovered by the "Adelantado" Joan de Salinas. He fitted out an expedition at Loja in Ecuador, descended the Rio Santiago to the Marañón, passed through the Pongo in 1557 and invaded the country of the Mayna Indians. Later, the missionaries of Cajamarca and Cusco established many missions in the Maynas, and made extensive use of the Pongo de Manseriche as an avenue of communication with their several convents on the Andean plateau. According to their accounts, the huge rent in the Andes, the Pongo, is about five or six miles (10 km) long, and in places not more than 80 feet (25 m) wide, and is a frightful series of torrents and whirlpools interspersed with rocks. There is an ancient tradition of the indigenous people of the vicinity that one of their gods descended the Marañón and another ascended the Amazon to communicate with him. They opened the pass called the Pongo de Manseriche.

</doc>
<doc id="24403" url="https://en.wikipedia.org/wiki?curid=24403" title="Presbyterianism">
Presbyterianism

Presbyterianism is a part of the Reformed tradition within Protestantism which traces its origins to the British Isles. Presbyterian churches derive their name from the presbyterian form of church government, which is governed by representative assemblies of elders. Many Reformed churches are organized this way, but the word "Presbyterian," when capitalized, is often applied uniquely to the churches that trace their roots to the Scottish and English churches that bore that name and English political groups that formed during the English Civil War. Presbyterian theology typically emphasizes the sovereignty of God, the authority of the Scriptures, and the necessity of grace through faith in Christ. Presbyterian church government was ensured in Scotland by the Acts of Union in 1707 which created the kingdom of Great Britain. In fact, most Presbyterians found in England can trace a Scottish connection, and the Presbyterian denomination was also taken to North America mostly by Scots and Scots-Irish immigrants. The Presbyterian denominations in Scotland hold to the theology of John Calvin and his immediate successors, although there are a range of theological views within contemporary Presbyterianism. Local congregations of churches which use presbyterian polity are governed by sessions made up of representatives of the congregation (elders); a conciliar approach which is found at other levels of decision-making (presbytery, synod and general assembly).
The roots of Presbyterianism lie in the European Reformation of the 16th century; the example of John Calvin's Geneva being particularly influential. Most Reformed churches which trace their history back to Scotland are either presbyterian or congregationalist in government. In the twentieth century, some Presbyterians played an important role in the ecumenical movement, including the World Council of Churches. Many Presbyterian denominations have found ways of working together with other Reformed denominations and Christians of other traditions, especially in the World Communion of Reformed Churches. Some Presbyterian churches have entered into unions with other churches, such as Congregationalists, Lutherans, Anglicans, and Methodists. Presbyterians in the United States came largely from Scotch-Irish immigrants communities, and also from New England Yankee communities that had originally been Congregational but changed because of an agreed-upon "Plan of Union of 1801" for frontier areas.
History.
Presbyterian history is part of the history of Christianity, but the beginning of Presbyterianism as a distinct movement occurred during the 16th-century Protestant Reformation. As the Catholic Church resisted the reformers, several different theological movements splintered from the Church and bore different denominations. Presbyterianism was especially influenced by the French theologian John Calvin, who is credited with the development of Reformed theology, and the work of John Knox, a Scotsman who studied with Calvin in Geneva, Switzerland and brought his teachings back to Scotland. The Presbyterian church traces its ancestry back primarily to England and Scotland. In August 1560 the Parliament of Scotland adopted the "Scots Confession" as the creed of the Scottish Kingdom. In December 1560, the "First Book of Discipline" was published, outlining important doctrinal issues but also establishing regulations for church government, including the creation of ten ecclesiastical districts with appointed superintendents which later became known as presbyteries.
In time, the Scots Confession would be supplanted by the Westminster Confession of Faith, and the Larger and Shorter Catechisms, which were formulated by the Westminster Assembly between 1643 and 1649.
Characteristics.
Presbyterians distinguish themselves from other denominations by doctrine, institutional organization (or "church order") and worship; often using a "Book of Order" to regulate common practice and order. The origins of the Presbyterian churches are in Calvinism. Many branches of Presbyterianism are remnants of previous splits from larger groups. Some of the splits have been due to doctrinal controversy, while some have been caused by disagreement concerning the degree to which those ordained to church office should be required to agree with the Westminster Confession of Faith, which historically serves as an important confessional document – second only to the Bible, yet directing particularities in the standardization and translation of the Bible – in Presbyterian churches.
Presbyterians place great importance upon education and lifelong learning. Continuous study of the scriptures, theological writings, and understanding and interpretation of church doctrine are embodied in several statements of faith and catechisms formally adopted by various branches of the church, often referred to as "subordinate standards". It is generally considered that the point of such learning is to enable one to put one's faith into practice; some Presbyterians generally exhibit their faith in action as well as words, by generosity, hospitality, as well as proclaiming the gospel of Christ.
Government.
Presbyterian government is by councils (known as "courts") of elders. Teaching and ruling elders are ordained and convene in the lowest council known as a "session" or "consistory" responsible for the discipline, nurture, and mission of the local . Teaching elders (pastors) have responsibility for teaching, worship, and performing sacraments. Pastors are called by individual congregations. A congregation issues a call for the pastor's service, but this call must be ratified by the local presbytery.
Ruling elders are usually laymen (and laywomen in some denominations) who are elected by the congregation and ordained to serve with the teaching elders, assuming responsibility for nurture and leadership of the congregation. Often, especially in larger congregations, the elders delegate the practicalities of buildings, finance, and temporal ministry to the needy in the congregation to a distinct group of officers (sometimes called deacons, which are ordained in some denominations). This group may variously be known as a "Deacon Board", "Board of Deacons" "Diaconate", or "Deacons' Court". These are sometimes known as "presbyters" to the full congregation.
Above the sessions exist presbyteries, which have area responsibilities. These are composed of teaching elders and ruling elders from each of the constituent congregations. The presbytery sends representatives to a broader regional or national assembly, generally known as the General Assembly, although an intermediate level of a "synod" sometimes exists. This congregation / presbytery / synod / general assembly schema is based on the historical structure of the larger Presbyterian churches, such as the Church of Scotland or the Presbyterian Church (U.S.A.); some bodies, such as the Presbyterian Church in America and the Presbyterian Church in Ireland, skip one of the steps between congregation and General Assembly, and usually the step skipped is the Synod. The Church of Scotland has now abolished the Synod.
Presbyterian governance is practised by Presbyterian denominations and also by many other Reformed churches.
Doctrine.
Presbyterianism is historically a confessional tradition. This has two implications. The obvious one is that confessional churches express their faith in the form of "confessions of faith," which have some level of authoritative status. However this is based on a more subtle point: In confessional churches, theology is not solely an individual matter. While individuals are encouraged to understand Scripture, and may challenge the current institutional understanding, theology is carried out by the community as a whole. It is this community understanding of theology that is expressed in confessions.
However, there has arisen a spectrum of approaches to confessionalism. The manner of subscription, or the degree to which the official standards establish the actual doctrine of the church, turns out to be a practical matter. That is, the decisions rendered in ordination and in the courts of the church largely determine what the church means, representing the whole, by its adherence to the doctrinal standard.
Some Presbyterian traditions adopt only the Westminster Confession of Faith as the doctrinal standard to which teaching elders are required to subscribe, in contrast to the Larger and Shorter catechisms, which are approved for use in instruction. Many Presbyterian denominations, especially in North America, have adopted all of the Westminster Standards as their standard of doctrine which is subordinate to the Bible. These documents are Calvinistic in their doctrinal orientation. The Presbyterian Church in Canada retains the Westminster Confession of Faith in its original form, while admitting the historical period in which it was written should be understood when it is read.
The Westminster Confession is "The principal subordinate standard of the Church of Scotland" but "with due regard to liberty of opinion in points which do not enter into the substance of the Faith" (V). This formulation represents many years of struggle over the extent to which the confession reflects the Word of God and the struggle of conscience of those who came to believe it did not fully do so (e.g. William Robertson Smith). Some Presbyterian Churches, such as the Free Church of Scotland, have no such "conscience clause".
The Presbyterian Church (U.S.A.) has adopted the "Book of Confessions", which reflects the inclusion of other Reformed confessions in addition to the Westminster Standards. These other documents include ancient creedal statements (the Nicene Creed, the Apostles' Creed), 16th-century Reformed confessions (the Scots Confession, the Heidelberg Catechism, the Second Helvetic Confession), and 20th century documents (The Theological Declaration of Barmen, Confession of 1967 and A Brief Statement of Faith).
The Presbyterian Church in Canada developed the confessional document "Living Faith" (1984) and retains it as a subordinate standard of the denomination. It is confessional in format, yet like the Westminster Confession, draws attention back to original Bible text.
Presbyterians in Ireland who rejected Calvinism and the Westminster Confessions formed the Non-subscribing Presbyterian Church of Ireland.
Worship and Sacraments.
Worship.
Presbyterian denominations that trace their heritage to the British Isles usually organise their church services inspired by the principles in the Directory of Public Worship, developed by the Westminster Assembly in the 1640s. This directory documented Reformed worship practices and theology adopted and developed over the preceding century by British Puritans, initially guided by John Calvin and John Knox. It was enacted as law by the Scottish Parliament, and became one of the foundational documents of Presbyterian church legislation elsewhere.
Historically, the driving principle in the development of the standards of Presbyterian worship is the Regulative principle of worship, which specifies that (in worship), what is not commanded is forbidden.
Over subsequent centuries, many Presbyterian churches modified these prescriptions by introducing hymnody, instrumental accompaniment, and ceremonial vestments into worship. However, there is not one fixed "Presbyterian" worship style. Although there are set services for the "Lord's Day", one can find a service to be evangelical and even revivalist in tone (especially in some conservative denominations), or strongly liturgical, approximating the practices of Lutheranism or Anglicanism (especially where Scottish tradition is esteemed), or semi-formal, allowing for a balance of hymns, preaching, and congregational participation (favored by probably most American Presbyterians). Most Presbyterian churches follow the traditional liturgical year and observe the traditional holidays, holy seasons, such as Advent, Christmas, Ash Wednesday, Holy Week, Easter, Pentecost, etc. They also make use of the appropriate seasonal liturgical colors, etc. Many, incorporate ancient liturgical prayers and responses into the communion services and follow a daily, seasonal, and festival lectionary. Other Presbyterians, however, such as the Reformed Presbyterians, would practice a cappella exclusive psalmody, as well as eschew the celebration of holy days.
Among the paleo-orthodox and emerging church movements in Protestant and evangelical churches, in which some Presbyterians are involved, clergy are moving away from the traditional black Geneva gown to such vestments as the alb and chasuble, but also cassock and surplice (typically a full length Old English style surplice which resembles the Celtic alb, an ungirdled liturgical tunic of the old Gallican Rite), which some, particularly those identifying with the Liturgical Renewal Movement, hold to be more ancient and representative of a more ecumenical past.
Sacraments.
Presbyterians traditionally have held the Worship position that there are only two sacraments:
Architecture.
Early Presbyterians were careful to distinguish between the "church," which referred the "members", and the "meeting house," which was the building in which the church met. Until the late 19th century, very few Presbyterians ever referred to their buildings as "churches." Presbyterians believed that meeting-houses (now called churches) are buildings to support the worship of God. The decor in some instances was austere so as not to detract from worship. Early Presbyterian meeting-houses were extremely plain. No stained glass, no elaborate furnishings, and no images were to be found in the meeting-house. The pulpit, often raised so as only to be accessible by a staircase, was the centerpiece of the building.
In the late 19th century a gradual shift began to occur. Prosperous congregations built imposing churches, such as Fourth Presbyterian in Chicago, Madison Avenue Presbyterian and Fifth Avenue Presbyterian in New York City, Shadyside Presbyterian Church in Pittsburgh, PA, East Liberty Presbyterian Church in Pittsburgh, PA, First Presbyterian in Dallas, House of Hope Presbyterian Church in Saint Paul, Minnesota Independent Presbyterian Church (Birmingham, Alabama) plus many others.
Usually a Presbyterian church will not have statues of saints, nor the ornate altar more typical of a Roman Catholic church. Instead, one will find a "communion table," usually on the same level as the congregation. There may be a rail between the communion table and the "Chancel" behind it, which may contain a more decorative altar-type table, choir loft, or choir stalls, lectern and clergy area. The altar is called the communion table and the altar area is called the Chancel by Presbyterians. In a Presbyterian (Reformed Church) there may be an altar cross, either on the communion table or on a table in the chancel. By using the "empty" cross, or cross of the resurrection, Presbyterians emphasize the resurrection and that Christ is not continually dying, but died once and is alive for all eternity. Some Presbyterian church buildings are often decorated with a cross that has a circle around the center, or Celtic cross. This not only emphasized the resurrection, but also acknowledges historical aspects of Presbyterianism. A baptismal font will be located either at the entrance or near the chancel area. Presbyterian architecture generally makes significant use of symbolism. You may also find decorative and ornate stained glass windows depicting scenes from the bible. Some Presbyterian churches will also have ornate statues of Christ or Graven Scenes from the Last Supper located behind the Chancel. St. Giles Cathedral ( Church Of Scotland- The Mother Church of Presbyterians) does have a Crucifix next to one of the Pulpits that hangs alongside. The image of Christ is more of faint image and more modern design.
Regions.
France.
There is a Church of Scotland (Presbyterian) in central Paris The Scots Kirk, Paris which is English-speaking, and is attended by many nationalities. It maintains close links with the Church of Scotland in Scotland itself, as well as with the Reformed Church of France.
Scotland.
John Knox (1505–1572), a Scot who had spent time studying under Calvin in Geneva, returned to Scotland and urged his countrymen to reform the Church in line with Calvinist doctrines. After a period of religious convulsion and political conflict culminating in a victory for the Protestant party at the Siege of Leith the authority of the Church of Rome was abolished in favour of Reformation by the legislation of the Scottish Reformation Parliament in 1560. The Church was eventually organised by Andrew Melville along Presbyterian lines to become the national Church of Scotland. King James VI and I moved the Church of Scotland towards an episcopal form of government, and in 1637, James' successor, Charles I and William Laud, the Archbishop of Canterbury, attempted to force the Church of Scotland to use the Book of Common Prayer. What resulted was an armed insurrection, with many Scots signing the "Solemn League and Covenant". The Covenanters would serve as the government of Scotland for nearly a decade, and would also send military support to the Parliamentarians during the English Civil War. Following the restoration of the monarchy in 1660, Charles II, despite the initial support that he received from the Covenanters, reinstated an episcopal form of government on the church.
However, with the Glorious Revolution of 1688 the Church of Scotland was finally unequivocally recognised as a Presbyterian institution by the monarch due to Scottish Presbyterian support for the aforementioned revolution and the Acts of Union 1707 between Scotland and England guaranteed the Church of Scotland's form of government. However, legislation by the United Kingdom parliament allowing patronage led to splits in the Church. In 1733, a group of ministers seceded from the Church of Scotland to form the Associate Presbytery, another group seceded in 1761 to form the Relief Church and the Disruption of 1843 led to the formation of the Free Church of Scotland. Further splits took place, especially over theological issues, but most Presbyterians in Scotland were reunited by 1929 union of the established Church of Scotland and the United Free Church of Scotland.
The Presbyterian denominations in Scotland today are the Church of Scotland, the Free Church of Scotland, the United Free Church of Scotland, the Free Church of Scotland (Continuing), the Free Presbyterian Church of Scotland, the Associated Presbyterian Church (Associated Presbyterian Churches), and the Reformed Presbyterian Church of Scotland.
Within Scotland the term kirk is usually used to refer to a local Presbyterian church. Informally the term 'The Kirk' refers to the
Church of Scotland.
England.
In England, Presbyterianism was established in secret in 1592. Thomas Cartwright is thought to be the first Presbyterian in England. Cartwright's controversial lectures at Cambridge University condemning the episcopal hierarchy of the Elizabethan Church led to his deprivation of his post by Archbishop John Whitgift and his emigration abroad. Between 1645 and 1648, a series of ordinances of the Long Parliament established Presbyterianism as the polity of the Church of England. Presbyterian government was established in London and Lancashire and in a few other places in England, although Presbyterian hostility to the execution of Charles I and the establishment of the republican Commonwealth of England meant that Parliament never enforced the Presbyterian system in England. The re-establishment of the monarchy in 1660 brought the return of Episcopal church government in England (and in Scotland for a short time); but the Presbyterian church in England continued in Non-Conformity, outside of the established church. In 1719 a major split, the Salter's Hall controversy, occurred; with the majority siding with nontrinitarian views. Thomas Bradbury published several sermons bearing on the controversy, and in 1719, "An answer to the reproaches cast on the dissenting ministers who subscribed their belief of the Eternal Trinity.". By the 18th century many English Presbyterian congregations had become Unitarian in doctrine.
A number of new Presbyterian Churches were founded by Scottish immigrants to England in the 19th century and later. Following the 'Disruption' in 1843 many of those linked to the Church of Scotland eventually joined what became the Presbyterian Church of England in 1876. Some, that is Crown Court (Covent Garden, London), St Andrew's (Stepney, London) and Swallow Street (London), did not join the English denomination, which is why there are Church of Scotland congregations in England such as those at Crown Court, and St Columba's, Pont Street (Knightsbridge) in London. There is also a congregation in the heart of London's financial district called London City Presbyterian Church that is also affiliated with Free Church of Scotland.
In 1972, the Presbyterian Church of England (PCofE) united with the Congregational Church in England and Wales to form the United Reformed Church (URC). Among the congregations the PCofE brought to the URC were Tunley (Lancashire), Aston Tirrold (Oxfordshire) and John Knox Presbyterian Church, Stepney, London (now part of Stepney Meeting House URC) – these are among the sole survivors today of the English Presbyterian churches of the 17th century. The URC also has a presence in Scotland, mostly of former Congregationalist Churches. Two former Presbyterian congregations, St Columba's, Cambridge (founded in 1879), and St Columba's, Oxford (founded as a chaplaincy by the PCofE and the Church of Scotland in 1908 and as a congregation of the PCofE in 1929), continue as congregations of the URC and university chaplaincies of the Church of Scotland.
In recent years a number of smaller denominations adopting Presbyterian forms of church government have organised in England, including the International Presbyterian Church planted by evangelical theologian Francis Schaeffer of L'Abri Fellowship in the 1970s, and the Evangelical Presbyterian Church in England and Wales founded in the North of England in the late 1980s.
Wales.
In Wales, Presbyterianism is represented by the Presbyterian Church of Wales, which was originally composed largely of Calvinistic Methodists who accepted Calvinist theology rather than the Arminianism of the Wesleyan Methodists. They broke off from the Church of England in 1811, ordaining their own ministers. They were originally known as the Calvinist Methodist connexion and in the 1920s it became alternatively known as the Presbyterian Church of Wales.
Ireland.
Presbyterianism is the largest Protestant denomination in Northern Ireland and the second largest on the island of Ireland (after the Anglican Church of Ireland), and was brought by Scottish plantation settlers to Ulster who had been strongly encouraged to emigrate by James VI of Scotland, later James I of England. An estimated 100,000 Scottish Presbyterians moved to the northern counties of Ireland between 1607 and the Battle of the Boyne in 1690. The Presbytery of Ulster was formed in 1642 separately from the established Anglican Church. Presbyterians, along with Roman Catholics in Ulster and the rest of Ireland, suffered under the discriminatory Penal Laws until they were revoked in the early 19th century. Presbyterianism is represented in Ireland by the Presbyterian Church in Ireland, the Free Presbyterian Church of Ulster, the Non-subscribing Presbyterian Church of Ireland, the Reformed Presbyterian Church of Ireland and the Evangelical Presbyterian Church.
Italy.
Further information: Waldensian
The Waldensian Evangelical Church (Chiesa Evangelica Valdese, CEV) is an Italian Protestant denomination.
The church was founded in the 12th century, and centuries later, after the Protestant Reformation, it adhered to Calvinist theology and became the Italian branch of the Presbyterian churches. As such, the church is a member of the World Communion of Reformed Churches.
North America.
Even before Presbyterianism spread with immigrants abroad from Scotland, there were divisions in the larger Presbyterian family. Some later rejoined only to separate again. In what some interpret as rueful self-reproach, some Presbyterians refer to the divided Presbyterian churches as the "Split P's".
United States.
Presbyterianism first officially arrived in Colonial America in 1703 with the establishment of the first Presbytery in Philadelphia. In time, the presbytery would be joined by two more to form a synod (1717) and would eventually evolve into the Presbyterian Church in the United States of America in 1789. The nation's largest Presbyterian denomination, the Presbyterian Church (U.S.A.) – PC (USA) – can trace their heritage back to the original PCUSA, as can the Presbyterian Church in America (PCA), the Orthodox Presbyterian Church (OPC), the Bible Presbyterian Church (BPC), the Cumberland Presbyterian Church (CPC), the Cumberland Presbyterian Church in America the Evangelical Presbyterian Church (EPC) and the Evangelical Covenant Order of Presbyterians (ECO).
Other Presbyterian bodies in the United States include the Reformed Presbyterian Church of North America (RPCNA), the Associate Reformed Presbyterian Church (ARP), the Reformed Presbyterian Church in the United States (RPCUS), the Reformed Presbyterian Church General Assembly, the Reformed Presbyterian Church – Hanover Presbytery, the Covenant Presbyterian Church, the Presbyterian Reformed Church, the Westminster Presbyterian Church in the United States, the Korean American Presbyterian Church, and the Free Presbyterian Church of North America.
The territory within about a radius of Charlotte, North Carolina, is historically the greatest concentration of Presbyterianism in the Southern United States, while an almost identical geographic area around Pittsburgh, Pennsylvania, contains probably the largest number of Presbyterians in the entire nation.
The PC (USA), beginning with its predecessor bodies, has, in common with other so-called "mainline" Protestant denominations, experienced a significant decline in members in recent years. Some estimates have placed that loss at nearly half in the last forty years.
Presbyterian influence, especially through Princeton theology can be traced in modern Evangelicalism. Balmer says that:
In the late 1800s, Presbyterian missionaries established a presence in what is now northern New Mexico. This provided an alternative to the Catholicism, which was brought to the area by the Spanish Conquistadors and had remained unchanged. The area experienced a "mini" reformation, in that many converts were made to Presbyterianism, prompting persecution. In some cases, the converts left towns and villages to establish their own neighboring villages. The arrival of the United States to the area prompted the Catholic church to modernize and make efforts at winning the converts back, many of which did return. However, there are still stalwart Presbyterians and Presbyterian churches in the area.
Canada.
In Canada, the largest Presbyterian denomination – and indeed the largest Protestant denomination – was the Presbyterian Church in Canada, formed in 1875 with the merger of four regional groups. In 1925, the United Church of Canada was formed by the majority of Presbyterians combining with the Methodist Church, Canada, and the Congregational Union of Canada. A sizable minority of Canadian Presbyterians, primarily in southern Ontario but also throughout the entire nation, withdrew, and reconstituted themselves as a non-concurring continuing Presbyterian body. They regained use of the original name in 1939.
Latin America.
Presbyterianism arrived in Latin America in the 19th century.
Mexico.
The biggest Presbyterian church is the National Presbyterian Church in Mexico ("Iglesia Nacional Presbiteriana de México"), which has around 2,500,000 members and associates and 3000 congregations, but there are other small denominations like the Associate Reformed Presbyterian Church in Mexico which was founded in 1875 by the Associate Reformed Church in North America. The Independent Presbyterian Church and the Presbyterian Reformed Church in Mexico, the National Conservative Presbyterian Church in Mexico are existing churches in the Reformed tradition.
Brazil.
In Brazil, the Presbyterian Church of Brazil ("Igreja Presbiteriana do Brasil") totals approximately 1,011,300 members; other Presbyterian churches (Independents, United, Conservatives, Renovated, etc.) in this nation have around 350,000 members. The Renewed Presbyterian Church in Brazil was influenced by the charismatic movement and has about 131 000 members as of 2011. The Conservative Presbyterian Church was founded in 1940 and has eight presbyteries. The Fundamentalist Presbyterian church in Brazil was influenced by Karl McIntosh and the Bible Presbyterian church USA and has around 1 800 members. The Independent Presbyterian Church in Brasil was founded in 1903 by pastor Pereira, has 500 congregations and 75 000 members. The United Presbyterian Church in Brazil has around 4 000 members. There are also ethnic Korean Presbyterian churches in the country. The Evangelical Reformed Church in Brazil has Dutch origin. The Reformed Churches in Brazil were recently founded by the Canadian Reformed Churches with the Reformed Church in the Netherlands (liberated).
Congregational churches present in the country are also part of the Calvinistic tradition in Latin America.
Other Latin American states.
There are probably more than four million members of Presbyterian churches in all of Latin America. Presbyterian churches are also present in Peru, Bolivia, Cuba, Trinidad and Tobago, Venezuela, Colombia, Chile, Paraguay, Costa Rica, Nicaragua, Argentina and others, but with few members. The Presbyterian Church in Belize has 14 churches and church plants and there is a Reformed Seminary founded in 2004. Some Latin Americans in North America are active in the Presbyterian Cursillo Movement.
Africa.
Presbyterianism arrived in Africa in the 19th century through the work of Scottish missionaries and founded churches such as St Michael and All Angels Church, Blantyre, Malawi. The church has grown extensively and now has a presence in at least 23 countries in the region.
African Presbyterian churches often incorporate diaconal ministries, including social services, emergency relief, and the operation of mission hospitals. A number of partnerships exist between presbyteries in Africa and the PC(USA), including specific connections with Lesotho, Malawi, South Africa, Ghana and Zambia. For example, the Lackawanna Presbytery, located in Northeastern Pennsylvania, has a partnership with a presbytery in Ghana. Also the Southminster Presbyterian Church, located near Pittsburgh, has partnerships with churches in Malawi and Kenya. The Presbyterian Church of Nigeria, western Africa is also healthy and strong in mostly the southern states of this nation, strong density in the south-eastern states of this country. Beginning from Cross River state, the nearby coastal states, Rivers state, Lagos state to Ebonyi and Abia States. The missionary expedition of Mary Slessor and Hope Waddel and their group in the mid 18th century in this coastal regions of the ten British colony has brought about the beginning and the flourishing of this church in these areas.
Kenya.
The Presbyterian Church of East Africa, based in Kenya, is particularly strong, with 500 clergy and 4 million members.
Malawi.
The Reformed Presbyterian Church in Malawi has 150 congregations and 17 000–20 000 members. It was a mission of the Free Presbyterian church of Scotland. The Restored Reformed Church works with RPCM. Evangelical Presbyterian Church in Malawi is an existing small church. Part of the Presbyterian Church in Malawi and Zambia is known as CCAP, Church of Central Africa-Presbyterian. Often the churches there have one main congregation and a number of Prayer Houses develop. education, health ministries as well as worship and spiritual development are important.
Southern Africa.
Southern Africa is a major base of Reformed and Presbyterian Churches.
Northern Africa.
In addition also there are a number of Presbyterian Churches in north Africa, the most known is the Nile Synod in Egypt and a recently founded synod for Sudan.
Asia.
Hong Kong.
Cumberland Presbyterian Church Yao Dao Secondary School is a Presbyterian school in Yuen Long, New Territories. The Cumberland Presbyterian Church also have a church on the island of Cheung Chau. There are also Korean Christians resident in Hong Kong who are Presbyterians.
South Korea.
Presbyterian Churches are the biggest and by far the most influential Protestant denominations in South Korea, with close to 20,000 churches affiliated with the two largest Presbyterian denominations in the country. In South Korea there are 15 million Protestants and about 9 million are Presbyterians. In South Korea there are 100 different Presbyterian denominations.
Most of the Korean Presbyterian denominations share the same name in Korean, 대한예수교장로회 (literally means the Presbyterian Church of Korea or PCK), tracing its roots to the United Presbyterian Assembly before its long history of disputes and schisms. The Presbyterian schism began with the controversy in relation to the Japanese shrine worship enforced during the Japanese colonial period and the establishment of a minor division (Koryu-pa, 고려파, later The Koshin Presbyterian Church in Korea, Koshin 고신) in 1952. And in 1953 the second schism happened when the theological orientation of the Chosun Seminary (later Hanshin University) founded in 1947 could not be tolerated in the PCK and another minor group (The Presbyterian Church in the Republic of Korea, Kijang, 기장) was separated. The last major schism had to do with the issue of whether the PCK should join the WCC. The controversy divided the PCK into two denominations, The Presbyterian Church of Korea (Tonghap, 통합) and The General Assembly of Presbyterian Church in Korea (Hapdong, 합동) in 1959. All major seminaries associated with each denomination claim heritage from the Pyung Yang Theological Seminary, therefore, not only Presbyterian University and Theological Seminary and Chongsin University which are related to PCK but also Hanshin University of PROK all celebrated the 100th class in 2007, 100 years from the first graduates of Pyung Yang Theological Seminary.
Korean Presbyterian denominations are active in evangelism and many of its missionaries are being sent overseas, being the second biggest missionary sender in the world after the United States. GSM, the missionary body of the "Hapdong" General Assembly of Presbyterian Churches of Korea, is the single largest Presbyterian missionary organization in Korea.
In addition there are many Korean-American Presbyterians in the United States, either with their own church sites or sharing space in pre-existing churches as is the case in Australia, New Zealand and even Muslim countries such as Saudi Arabia with Korean immigration.
The Korean Presbyterian Church started through the mission of the Presbyterian Church (USA) and the Australian Presbyterian theological tradition is central to the United States. But after independence, the 'Presbyterian Church in Korea (KoRyuPa)' advocated a Dutch Reformed position. In the 21st century, a new General Assembly of the Orthodox Presbyterian Church of Korea (president Seung-moo Ha) in 2012 declared itself an authentic historical succession of Scottish Presbyterian John Knox.
Taiwan.
The Presbyterian Church in Taiwan (PCT) is by far the largest Protestant denomination in Taiwan, with some 238,372 members as of 2009 (including a majority of the island's aborigines). English Presbyterian missionary James Laidlaw Maxwell established the first Presbyterian church in Tainan in 1865. His colleague George Leslie Mackay, of the Canadian Presbyterian Mission, was active in Danshui and north Taiwan from 1872 to 1901; he founded the island's first university and hospital, and created a written script for Taiwanese Minnan. The English and Canadian missions joined together as the PCT in 1912. One of the few churches permitted to operate in Taiwan through the era of Japanese rule (1895–1945), the PCT experienced rapid growth during the era of Guomindang-imposed martial law (1949–1987), in part due to its support for democracy, human rights, and Taiwan independence. Former ROC president Lee Teng-hui (in office 1988–2000) is a Presbyterian.
India.
In the mainly Christian Indian state of Mizoram, the Presbyterian denomination is the largest denomination; it was brought to the region with missionaries from Wales in 1894. Prior to Mizoram, the Welsh Presbyterians (missionaries) started venturing into the north-east of India through the Khasi Hills (presently located within the state of Meghalaya in India) and established Presbyterian churches all over the Khasi Hills from the 1840s onwards. Hence there is a strong presence of Presbyterians in Shillong (the present capital of Meghalaya) and the areas adjoining it. The Welsh missionaries built their first church in Sohra (aka Cherrapunji) in 1846. Presbyterians participated in the mergers that resulted in the Church of North India and the Church of South India.Sohra
Oceania.
Australia.
In 1977, two thirds of the Presbyterian Church of Australia, along with most of the Congregational Union of Australia and all the Methodist Church of Australasia, combined to form the Uniting Church in Australia. The third who did not unite had various reasons for so acting, often cultural attachment but often conservative theological or social views. The permission for the ordination of women given in 1974 was rescinded in 1991 without affecting the two or three existing woman ministers. The approval of women elders given in the 1960s has been rescinded in all states except New South Wales, which has the largest membership. The theology of the church is now generally conservative and Reformed. A number of small Presbyterian denominations have arisen since the 1950s through migration or schism.
New Zealand.
In New Zealand, Presbyterian is the dominant denomination in Otago and Southland due largely to the rich Scottish and to a lesser extent Ulster-Scots heritage in the region. The area around Christchurch, Canterbury, is dominated philosophically by the Anglican (Episcopalian) denomination.
Originally there were two branches of Presbyterianism in New Zealand, the northern Presbyterian church which existed in the North Island and the parts of the South Island north of the Waitaki River, and the Synod of Otago and Southland, founded by Free Church settlers in southern South Island. The two churches merged in 1901, forming what is now the Presbyterian Church of Aotearoa New Zealand.
In addition to the Presbyterian Church of Aotearoa New Zealand, there is also a more conservative Presbyterian church called Grace Presbyterian Church of New Zealand. Many of its members left the largely liberal PCANZ because they were seeking a more Biblical church. It has 17 churches throughout New Zealand.
Vanuatu.
The Presbyterian Church in Vanuatu is the largest denomination in the country, with approximately one-third of the population of Vanuatu members of the church. The PCV was taken to Vanuatu by missionaries from Scotland. The PCV (Presbyterian Church of Vanuatu) is headed by a moderator with offices in Port Vila. The PCV is particularly strong in the provinces of Tafea, Shefa, and Malampa. The Province of Sanma is mainly Presbyterian with a strong Roman Catholic minority in the Francophone areas of the province. There are some Presbyterian people, but no organised Presbyterian churches in Penama and Torba, both of which are traditionally Anglican. Vanuatu is the only country in the South Pacific with a significant Presbyterian heritage and membership. The PCV is a founding member of the Vanuatu Christian Council (VCC). The PCV runs many primary schools and Onesua secondary school. The church is strong in the rural villages.

</doc>
<doc id="24406" url="https://en.wikipedia.org/wiki?curid=24406" title="Parliament">
Parliament

In modern politics and history, a parliament is a legislative, elected body of government. Generally a modern parliament has three functions: representing the electorate, making laws, and overseeing the government ("i.e.", hearings, inquiries).
Although some restrict the use of the word "parliament" to parliamentary systems, it is also commonly used to describe the legislature in presidential systems ("i.e." the French parliament), even where it is not in the official name.
Historically, parliaments included various kinds of deliberative, consultative, and judicial assemblies ("i.e." the mediaeval parlements).
Etymology.
The term is derived from Anglo-Norman , from the verb "parler" 'talk'. The meaning evolved over time: originally any discussion, conversation, or negotiation (attested around 1100), through various kinds of deliberative or judicial groups, often summoned by the monarch. By 1400, it had come to mean in Britain specifically the British supreme legislature.
Origins.
Various parliaments are claimed to be the oldest in the world, under varying definitions:
Proto-parliamentarian institutions.
Since ancient times, when societies were tribal, there were councils or a headman whose decisions were assessed by village elders. This is called tribalism. Some scholars suggest that in ancient Mesopotamia there was a primitive democratic government where the kings were assessed by council. The same has been said about ancient India, where some form of deliberative assemblies existed, and therefore there was some form of democracy. However, these claims are not accepted by most scholars, who see these forms of government as oligarchies.
Ancient Athens was the cradle of democracy. The Athenian assembly (, "ekklesia") was the most important institution, and every citizen could take part in the discussions. However, Athenian democracy was not representative, but rather direct, and therefore the "ekklesia" was different from the parliamentary system.
The Roman Republic had legislative assemblies, who had the final say regarding the election of magistrates, the enactment of new statutes, the carrying out of capital punishment, the declaration of war and peace, and the creation (or dissolution) of alliances. The Roman Senate controlled money, administration, and the details of foreign policy.
Some Muslim scholars argue that the Islamic shura (a method of taking decisions in Islamic societies) is analogous to the parliament. However, others highlight what they consider fundamental differences between the shura system and the parliamentary system.
In Anglo-Saxon England, the Witenagamot was an important political institution. The name derives from the Old English ƿitena ȝemōt, or witena gemōt, for "meeting of wise men". The first recorded act of a witenagemot was the law code issued by King Æthelberht of Kent ca. 600, the earliest document which survives in sustained Old English prose; however, the witan was certainly in existence long before this time. The Witan, along with the folkmoots(local assemblies), is an important ancestor of the modern English parliament.
England.
Early forms of assembly.
England has long had a tradition of a body of men who would assist and advise the king on important matters. Under the Anglo-Saxon kings, there was an advisory council, the Witenagemot ("meeting of wise men"). As part of the Norman Conquest of England, the new king, William I, did away with the Witenagemot, replacing it with a Curia Regis ("King's Council"). Membership of the Curia was largely restricted to the tenants in chief, the few nobles who "rented" great estates directly from the king, along with certain senior ecclesiastics.
Most historians date the emergence of a parliament with some degree of power to which the throne had to defer no later than the rule of Edward I. (Kaeuper, Richard W., War Justice and Public Order: England and France in the Later Middle Ages, Oxford U. Press, 1988.) Like previous kings, Edward called leading nobles and church leaders to discuss government matters, especially finance. A meeting in 1295 became known as the Model Parliament because it set the pattern for later Parliaments. The significant difference between the Model Parliament and the earlier Curia Regis was the addition of the Commons; that is, the inclusion of elected representatives of rural landowners and of townsmen. In 1307, Edward I agreed not to collect certain taxes without the consent of the realm. He also enlarged the court system.
William the Conqueror brought to England the feudal system of his native Normandy, and sought the advice of the curia regis before making laws. This is the original body from which the Parliament, the higher courts of law, and the Privy Council and Cabinet descend. Of these, the legislature is formally the High Court of Parliament; judges sit in the Supreme Court of Judicature. Only the executive government is no longer conducted in a royal court.
Magna Carta and the Model Parliament.
The tenants-in-chief often struggled with their spiritual counterparts and with the king for power. In 1215, they secured from John the Magna Carta, which established that the king may not levy or collect any taxes (except the feudal taxes to which they were hitherto accustomed), save with the consent of a council. It was also established that the most important tenants-in-chief and ecclesiastics be summoned to the council by personal writs from the sovereign, and that all others be summoned to the council by general writs from the sheriffs of their counties. Modern government has its origins in the Curia Regis; parliament descends from the Great Council later known as the "parliamentum" established by Magna Carta.
During the reign of King Henry III, 13th-Century English Parliaments incorporated elected representatives from shires and towns. These parliaments are, as such, considered forerunners of the modern parliament.
In 1265, Simon de Montfort, then in rebellion against Henry III, summoned a parliament of his supporters without royal authorization. The archbishops, bishops, abbots, earls, and barons were summoned, as were two knights from each shire and two burgesses from each borough. Knights had been summoned to previous councils, but it was unprecedented for the boroughs to receive any representation. Come 1295, Edward I later adopted de Montfort's ideas for representation and election in the so-called "Model Parliament". At first, each estate debated independently; by the reign of Edward III, however, Parliament recognisably assumed its modern form, with authorities dividing the legislative body into two separate chambers.
Parliament under Henry VIII and Edward VI.
The purpose and structure of Parliament in Tudor England underwent a significant transformation under the reign of Henry VIII. Originally its methods were primarily medieval, and the monarch still possessed a form of inarguable dominion over its decisions. According to Elton, it was Thomas Cromwell, 1st Earl of Essex, then chief minister to Henry VIII, who initiated still other changes within parliament.
The Reformation Acts supplied Parliament with unlimited power over the country. This included authority over virtually every matter, whether social, economic, political, or religious ; it legalised the Reformation, officially and indisputably. The king had to rule through the council, not over it, and all sides needed to reach a mutual agreement when creating or passing laws, adjusting or implementing taxes, or changing religious doctrines. This was significant: the monarch no longer had sole control over the country. For instance, during the later years of Mary, Parliament exercised its authority in originally rejecting Mary's bid to revive Catholicism in the realm. Later on, the legislative body even denied Elizabeth her request to marry . If Parliament had possessed this power before Cromwell, such as when Wolsey's served as secretary, the Reformation may never have happened, as the king would have had to gain the consent of all parliament members before so drastically changing the country's religious laws and fundamental identity .
The power of Parliament increased considerably after Cromwell's adjustments. It also provided the country with unprecedented stability. More stability, in turn, helped assure more effective management, organisation, and efficiency. Parliament printed statutes and devised a more coherent parliamentary procedure.
The rise of Parliament proved especially important in the sense that it limited the repercussions of dynastic complications that had so often plunged England into civil war. Parliament still ran the country even in the absence of suitable heirs to throne, and its legitimacy as a decision-making body reduced the royal prerogatives of kings like Henry VIII and the importance of their whims. For example, Henry VIII could not simply establish supremacy by proclamation; he required Parliament to enforce statutes and add felonies and treasons. An important liberty for Parliament was its freedom of speech; Henry allowed anything to be spoken openly within Parliament and speakers could not face arrest — a fact which they exploited incessantly. Nevertheless, Parliament in Henry VIII's time offered up very little objection to the monarch's desires. Under his and Edward's reign, the legislative body complied willingly with the majority of the kings' decisions.
Much of this compliance stemmed from how the English viewed and traditionally understood authority. As Williams described it, "King and parliament were not separate entities, but a single body, of which the monarch was the senior partner and the Lords and the Commons the lesser, but still essential, members.".
The importance of the Commonwealth years.
Although its role in government expanded significantly during the reigns of Henry VIII and Edward VI, the Parliament of England saw some of its most important gains in the 17th century. A series of conflicts between the Crown and Parliament culminated in the execution of King Charles I in 1649. Afterward, England became a commonwealth, with Oliver Cromwell, its lord protector, the de facto ruler. Frustrated with its decisions, Cromwell purged and suspended Parliament on several occasions.
A controversial figure accused of despotism, war crimes, and even genocide, Cromwell is nonetheless regarded as essential to the growth of Parliament in England. The years of his Commonwealth. coupled with the restoration of the monarchy in 1660 and the subsequent Glorious Revolution of 1688, helped reinforce and strengthen Parliament as an institution separate from the Crown. The BBC even went so far as to dub Cromwell the "father of British democracy".
Acts of Union.
The Parliament of England met until it merged with the Parliament of Scotland under the Acts of Union. This union created the new Parliament of Great Britain in 1707.
France.
Originally, there was only the Parliament of Paris, born out of the Curia Regis in 1307, and located inside the medieval royal palace, now the Paris Hall of Justice. The jurisdiction of the "Parliament" of Paris covered the entire kingdom. In the thirteenth century, judicial functions were added. In 1443, following the turmoil of the Hundred Years' War, King Charles VII of France granted Languedoc its own "parliament" by establishing the "Parliament" of Toulouse, the first "parliament" outside of Paris, whose jurisdiction extended over the most part of southern France. From 1443 until the French Revolution several other "parliaments" were created in some provinces of France (Grenoble, Bordeaux).
All the "parliaments" could issue regulatory decrees for the application of royal edicts or of customary practices; they could also refuse to register laws that they judged contrary to fundamental law or simply as being untimely. Parliamentary power in France was suppressed more so than in England as a result of absolutism, and parliaments were eventually overshadowed by the larger Estates General, up until the French Revolution, when the National Assembly became the lower house of France's bicameral legislature.
Scotland.
From the 10th century the Kingdom of Alba was ruled by chiefs ("toisechs") and subkings ("mormaers") under the suzerainty, real or nominal, of a High King. Popular assemblies, as in Ireland, were involved in law-making, and sometimes in king-making, although the introduction of tanistry—naming a successor in the lifetime of a king—made the second less than common. These early assemblies cannot be considered "parliaments" in the later sense of the word, and were entirely separate from the later, Norman-influenced, institution.
The Parliament of Scotland evolved during the Middle Ages from the King's Council of Bishops and Earls. The unicameral parliament is first found on record, referred to as a "colloquium", in 1235 at Kirkliston (a village now in Edinburgh).
By the early fourteenth century the attendance of knights and freeholders had become important, and from 1326 burgh commissioners attended. Consisting of the Three Estates; of clerics, lay tenants-in-chief and burgh commissioners sitting in a single chamber, the Scottish parliament acquired significant powers over particular issues. Most obviously it was needed for consent for taxation (although taxation was only raised irregularly in Scotland in the medieval period), but it also had a strong influence over justice, foreign policy, war, and all manner of other legislation, whether political, ecclesiastical, social or economic. Parliamentary business was also carried out by "sister" institutions, before c. 1500 by General Council and thereafter by the Convention of Estates. These could carry out much business also dealt with by Parliament – taxation, legislation and policy-making – but lacked the ultimate authority of a full parliament.
The parliament, which is also referred to as the Estates of Scotland, the Three Estates, the Scots Parliament or the auld Scots Parliament (Eng: "old"), met until the Acts of Union merged the Parliament of Scotland and the Parliament of England, creating the new Parliament of Great Britain in 1707.
Following the Scottish devolution referendum, 1997, and the passing of the Scotland Act 1998 by the Parliament of the United Kingdom, the Scottish Parliament was reconvened on 1 July 1999, although with much more limited powers than its 18th century predecessor. The parliament has sat since 2004 at its newly constructed Scottish Parliament Building in Edinburgh, situated at the foot of the Royal Mile, next to the royal palace of Holyroodhouse.
Nordic and Germanic development.
A "thing" or "ting" (Old Norse and ; other modern Scandinavian: "ting", "ding" in Dutch) was the governing assembly in Germanic societies, made up of the free men of the community and presided by lawspeakers. Today the term lives on in the official names of national legislatures, political and judicial institutions in the North-Germanic countries. In the Yorkshire and former Danelaw areas of England, which were subject to much Norse invasion and settlement, the wapentake was another name for the same institution.
The thing was the assembly of the free men of a country, province or a hundred "(hundare/härad/herred)". There were consequently, hierarchies of things, so that the local things were represented at the thing for a larger area, for a province or land. At the thing, disputes were solved and political decisions were made. The place for the thing was often also the place for public religious rites and for commerce.
The thing met at regular intervals, legislated, elected chieftains and kings, and judged according to the law, which was memorised and recited by the "law speaker" (the judge).
Later national diets with chambers for different estates developed, e.g. in Sweden and in Finland (which was part of Sweden until 1809), each with a House of Knights for the nobility. In both these countries, the national parliaments are now called riksdag (in Finland also "eduskunta"), a word used since the Middle Ages and equivalent of the German word Reichstag.
Poland.
According to the "Chronicles" of Gallus Anonymus, the first legendary Polish ruler, Siemowit, who began the Piast Dynasty, was chosen by a "wiec". The "veche" (, ) was a popular assembly in medieval Slavic countries, and in late medieval period, a parliament. The idea of the "wiec" led in 1182 to the development of the Polish parliament, the "Sejm".
The term "sejm" comes from an old Polish expression denoting a meeting of the populace. The power of early sejms grew between 1146–1295, when the power of individual rulers waned and various councils and wiece grew stronger. The history of the national Sejm dates back to 1182. Since the 14th century irregular sejms (described in various Latin sources as "contentio generalis, conventio magna, conventio solemna, parlamentum, parlamentum generale, dieta" or Polish "sejm walny") have been called by Polish kings. From 1374, the king had to receive sejm permission to raise taxes. The General Sejm (Polish "Sejm Generalny" or "Sejm Walny"), first convoked by the king John I Olbracht in 1493 near Piotrków, evolved from earlier regional and provincial meetings ("sejmiks"). It followed most closely the "sejmik generally", which arose from the 1454 Nieszawa Statutes, granted to the szlachta (nobles) by King Casimir IV the Jagiellonian. From 1493 forward, indirect elections were repeated every two years. With the development of the unique Polish Golden Liberty the Sejm's powers increased.
The Commonwealth's general parliament consisted of three estates: the King of Poland (who also acted as the Grand Duke of Lithuania, Russia/Ruthenia, Prussia, Mazovia, etc.), the Senat (consisting of Ministers, Palatines, Castellans and Bishops) and the Chamber of Envoys—circa 170 nobles (szlachta) acting on behalf of their Lands and sent by Land Parliaments. Also representatives of selected cities but without any voting powers. Since 1573 at a royal election all peers of the Commonwealth could participate in the Parliament and become the King's electors.
Ukraine.
Cossack Rada was the legislative body of a military republic of the Ukrainian Cossacks that grew rapidly in the 15th century from serfs fleeing the more controlled parts of the Polish Lithuanian Commonwealth. The republic did not regard social origin/nobility and accepted all people who declared to be Orthodox Christians.
Originally established at the Zaporizhian Sich, the rada (council) was an institution of Cossack administration in Ukraine from the 16th to the 18th century. With the establishment of the Hetman state in 1648, it was officially known as the General Military Council until 1750.
Russia.
The zemsky sobor (Russian: зе́мский собо́р) was the first Russian parliament of the feudal Estates type, in the 16th and 17th centuries. The term roughly means assembly of the land.
It could be summoned either by tsar, or patriarch, or the Boyar Duma. Three categories of population, comparable to the Estates-General of France but with the numbering of the first two Estates reversed, participated in the assembly:
Nobility and high bureaucracy, including the Boyar Duma
The Holy Sobor of high Orthodox clergy
Representatives of merchants and townspeople (third estate)
The name of the parliament of nowadays Russian Federation is the Federal Assembly of Russia. The term for its lower house, State Duma (which is better known than the Federal Assembly itself, and is often mistaken for the entirety of the parliament) comes from the Russian word "думать" ("dumat"), "to think". The Boyar Duma was an advisory council to the grand princes and tsars of Muscovy. The Duma was discontinued by Peter the Great, who transferred its functions to the Governing Senate in 1711.
Novgorod and Pskov.
The "veche" was the highest legislature and judicial authority in the republic of Novgorod until 1478. In its sister state, Pskov, a separate veche operated until 1510.
Since the Novgorod revolution of 1137 ousted the ruling grand prince, the veche became the supreme state authority. After the reforms of 1410, the veche was restructured on a model similar to that of Venice, becoming the Commons chamber of the parliament. Аn upper Senate-like Council of Lords was also created, with title membership for all former city magistrates. Some sources indicate that veche membership may have become full-time, and parliament deputies were now called "vechniks". It is recounted that the Novgorod assembly could be summoned by anyone who rung the veche bell, although it is more likely that the common procedure was more complex. This bell was a symbol of republican sovereignty and independence. The whole population of the city—boyars, merchants, and common citizens—then gathered at Yaroslav's Court. Separate assemblies could be held in the districts of Novgorod. In Pskov the veche assembled in the court of the Trinity cathedral.
Spain.
Although there are documented councils held in 873, 1020, 1050 and 1063, there was no representation of commoners. What is considered to be the first Spanish parliament (with the presence of commoners), the Cortes, was held in the Kingdom of León in 1188. Prelates, nobles and commoners met separately in the three estates of the Cortes. In this meeting new laws were approved to protect commoners against the arbitrarities of nobles, prelates and the king. This important set of laws is known as the "Carta Magna Leonesa".
Following this event, new Cortes would appear in the other different territories that would make up Spain: Catalonia in 1218, the Kingdom of Castile in 1250, Kingdom of Aragon in 1274, Kingdom of Valencia in 1283 and Kingdom of Navarre in 1300.
After the union of the Kingdoms of Leon and Castile under the Crown of Castile, their Cortes were united as well in 1258. The Castilian Cortes had representatives from Burgos, Toledo, León, Seville, Córdoba, Murcia, Jaén, Zamora, Segovia, Ávila, Salamanca, Cuenca, Toro, Valladolid, Soria, Madrid, Guadalajara and Granada (after 1492). The Cortes' assent was required to pass new taxes, and could also advise the king on other matters. The comunero rebels intended a stronger role for the Cortes, but were defeated by the forces of Habsburg Emperor Charles V in 1521. The Cortes maintained some power, however, though it became more of a consultative entity. However, by the time of King Philip II, Charles's son, the Castilian Cortes had come under functionally complete royal control, with its delegates dependent on the Crown for their income.
The Cortes of the Crown of Aragon kingdoms retained their power to control the king's spending with regard to the finances of those kingdoms. But after the War of the Spanish Succession and the arrival of another royal house – the Bourbons – in 1714 with Philip V, their Cortes were suppressed (as were those of Aragon and Valencia in 1707, Catalonia and Balearic islands in 1714).
The Roman Catholic Church.
"Conciliarism" or the "conciliar movement", was a reform movement in the 14th and 15th century Roman Catholic Church which held that final authority in spiritual matters resided with the Roman Church as corporation of Christians, embodied by a general church council, not with the pope. In effect, the movement sought – ultimately, in vain – to create an All-Catholic Parliament. Its struggle with the Papacy had many points in common with the struggle of parliaments in specific countries against the authority of Kings and other secular rulers.
Development of modern parliaments.
The development of the modern concept of parliamentary government dates back to the Kingdom of Great Britain (1707–1800) and the parliamentary system in Sweden during the Age of Liberty (1718–1772).
Parliaments of the United Kingdom.
The British Parliament is often referred to as the "Mother of Parliaments" (in fact a misquotation of John Bright, who remarked in 1865 that "England is the Mother of Parliaments") because the British Parliament has been the model for most other parliamentary systems, and its Acts have created many other parliaments. Many nations with parliaments have to some degree emulated the British "three-tier" model. Most countries in Europe and the Commonwealth have similarly organised parliaments with a largely ceremonial head of state who formally opens and closes parliament, a large elected lower house and a smaller, upper house.
The Parliament of Great Britain was formed in 1707 by the Acts of Union that replaced the former parliaments of England and Scotland. A further union in 1801 united the Parliament of Great Britain and the Parliament of Ireland into a Parliament of the United Kingdom.
In the United Kingdom, Parliament consists of the House of Commons, the House of Lords, and the Monarch. The House of Commons is composed of 650 members who are directly elected by British citizens to represent single-member constituencies. The leader of a Party that wins more than half the seats or less than half but can count on support of smaller parties to achieve enough support to pass law is invited by the Queen to form a government. The House of Lords is a body of long-serving, unelected members: Lords Temporal - 92 of whom inherit their titles (and of whom 90 are elected internally by members of the House to lifetime seats), 588 of whom have been appointed to lifetime seats, and Lords Spiritual - 26 bishops while they remain in office.
Legislation can originate from either the Lords or the Commons. It is voted on in several distinct stages, called readings, in each house. First reading is merely a formality. Second reading is where the bill as a whole is considered. Third reading is detailed consideration of clauses of the bill.
In addition to the three readings a bill also goes through a committee stage where it is considered in great detail. Once the bill has been passed by one house it goes to the other and essentially repeats the process. If after the two sets of readings there are disagreements between the versions that the two houses passed it is returned to the first house for consideration of the amendments made by the second. If it passes through the amendment stage Royal Assent is granted and the bill becomes law as an Act of Parliament.
The House of Lords is the less powerful of the two houses as a result of the Parliament Acts 1911 and 1949. These Acts removed the veto power of the Lords over a great deal of legislation. If a bill is certified by the Speaker of the House of Commons as a money bill (i.e. acts raising taxes and similar) then the Lords can only block it for a month. If an ordinary bill originates in the Commons the Lords can only block it for a maximum of one session of Parliament. The exceptions to this rule are things like bills to prolong the life of a Parliament beyond five years.
In addition to functioning as the second chamber of Parliament, the House of Lords was also the final court of appeal for much of the law of the United Kingdom—a combination of judicial and legislative function that recalls its origin in the Curia Regis. This changed in October 2009 when the Supreme Court of the United Kingdom opened and acquired the former jurisdiction of the House of Lords.
Since 1999, there has been a Scottish Parliament in Edinburgh, which is a national, unicameral legislature for Scotland. However, the Scottish Parliament does not have complete power over Scottish Politics, as it only holds the powers which were devolved to it by Westminster in 1997. It cannot legislate on defence issues, currency, or national taxation (e.g. VAT, or Income Tax). Additionally, the Scottish Parliament can be dissolved at any given time by the British Parliament without the consent of the devolved government. This applies to all devolved governments within the United Kingdom, a limit on the sovereignty of the devolved governments.
Parliament of Sweden.
In Sweden, the half-century period of parliamentary government beginning with Charles XII's death in 1718 and ending with Gustav III's self-coup in 1772 is known as the Age of Liberty. During this period, civil rights were expanded and power shifted from the monarch to parliament.
While suffrage did not become universal, the taxed peasantry was represented in Parliament, although with little influence and commoners without taxed property had no suffrage at all.
Parliamentary system.
Many parliaments are part of a parliamentary system of government, in which the executive is constitutionally answerable to the parliament. Some restrict the use of the word "parliament" to parliamentary systems, while others use the word for any elected legislative body. Parliaments usually consist of "chambers" or "houses", and are usually either bicameral or unicameral although more complex models exist, or have existed ("see Tricameralism).
In some parliamentary systems, the prime minister (PM) is a member of parliament (Britain), whereas in others he is not (Netherlands). He or she is commonly the leader of the majority party in the lower house of parliament, but he or she only holds his or her office as long as the "confidence of the house" is maintained. If members of the lower house lose faith in the leader for whatever reason, they can call a vote of no confidence and force the PM to resign.
This can be particularly dangerous to a government when the distribution of seats among different parties is relatively even, in which case a new election is often called shortly thereafter. However, in case of general discontent with the head of government, his replacement can be made very smoothly without all the complications that it represents in the case of a presidential system.
The parliamentary system can be contrasted with a presidential system, such as the American congressional system, which operates under a stricter separation of powers, whereby the executive does not form part of, nor is it appointed by, the parliamentary or legislative body. In such a system, congresses do not select or dismiss heads of governments, and governments cannot request an early dissolution as may be the case for parliaments. Some states have a semi-presidential system which falls between parliamentary and congressional systems, combining a powerful head of state (president) with a head of government (PM) responsible to parliament.
List of subnational parliamentary governments.
Belgium.
In the federal (bicameral) kingdom of Belgium, there is a curious asymmetrical constellation serving as directly elected legislatures for three "territorial" "regions"—Flanders (Dutch), Brussels (bilingual, certain peculiarities of competence, also the only region not comprising any of the 10 provinces) and Wallonia (French)—and three cultural "communities"—Flemish (Dutch, competent in Flanders and for the Dutch-speaking inhabitants of Brussels), Francophone (French, for Wallonia and for Francophones in Brussels) and German (for speakers of that language in a few designated municipalities in the east of the Walloon Region, living alongside Francophones but under two different regimes)
Canada.
Canada's provinces and territories:

</doc>
<doc id="24408" url="https://en.wikipedia.org/wiki?curid=24408" title="Polar bear">
Polar bear

The polar bear ("Ursus maritimus") is a carnivorous bear whose native range lies largely within the Arctic Circle, encompassing the Arctic Ocean, its surrounding seas and surrounding land masses. It is a large bear, approximately the same size as the omnivorous Kodiak bear ("Ursus arctos middendorffi"). A boar (adult male) weighs around , while a sow (adult female) is about half that size. Although it is the sister species of the brown bear, it has evolved to occupy a narrower ecological niche, with many body characteristics adapted for cold temperatures, for moving across snow, ice, and open water, and for hunting seals, which make up most of its diet. Although most polar bears are born on land, they spend most of their time on the sea ice. Their scientific name means "maritime bear", and derives from this fact. Polar bears hunt their preferred food of seals from the edge of sea ice, often living off fat reserves when no sea ice is present. Because of their dependence on the sea ice, polar bears are classified as marine mammals.
Because of expected habitat loss caused by climate change, the polar bear is classified as a vulnerable species, and at least three of the nineteen polar bear subpopulations are currently in decline. For decades, large-scale hunting raised international concern for the future of the species, but populations rebounded after controls and quotas began to take effect. For thousands of years, the polar bear has been a key figure in the material, spiritual, and cultural life of circumpolar peoples, and polar bears remain important in their cultures.
Naming and etymology.
Constantine John Phipps was the first to describe the polar bear as a distinct species in 1774. He chose the scientific name "Ursus maritimus", the Latin for 'maritime bear', due to the animal's native habitat. The Inuit refer to the animal as "nanook" (transliterated as "nanuq" in the Inupiat language). The Yupik also refer to the bear as "nanuuk" in Siberian Yupik. The bear is "umka" in the Chukchi language. In Russian, it is usually called бе́лый медве́дь ("bélyj medvédj", the white bear), though an older word still in use is ошку́й ("Oshkúj", which comes from the Komi "oski", "bear"). In French, the polar bear is referred to as "ours blanc" ("white bear") or "ours polaire" ("polar bear"). In the Norwegian-administered Svalbard archipelago, the polar bear is referred to as "Isbjørn" ("ice bear").
The polar bear was previously considered to be in its own genus, "Thalarctos". However, evidence of hybrids between polar bears and brown bears, and of the recent evolutionary divergence of the two species, does not support the establishment of this separate genus, and the accepted scientific name is now therefore "Ursus maritimus", as Phipps originally proposed.
Taxonomy and evolution.
The bear family, Ursidae, is thought to have split off from other carnivorans about 38 million years ago. The Ursinae subfamily originated approximately 4.2 million years ago. The oldest known polar bear fossil is a 130,000 to 110,000-year-old jaw bone, found on Prince Charles Foreland in 2004. Fossils show that between 10,000 and 20,000 years ago, the polar bear's molar teeth changed significantly from those of the brown bear. Polar bears are thought to have diverged from a population of brown bears that became isolated during a period of glaciation in the Pleistocene or from the eastern part of Siberia, (from Kamchatka and the Kolym Peninsula).
The evidence from DNA analysis is more complex. The mitochondrial DNA (mtDNA) of the polar bear diverged from the brown bear, "Ursus arctos", roughly 150,000 years ago. Further, some clades of brown bear, as assessed by their mtDNA, are more closely related to polar bears than to other brown bears, meaning that the polar bear might not be considered a species under some species concepts. The mtDNA of extinct Irish brown bears is particularly close to polar bears. A comparison of the nuclear genome of polar bears with that of brown bears revealed a different pattern, the two forming genetically distinct clades that diverged approximately 603,000 years ago, although the latest research is based on analysis of the complete genomes (rather than just the mitochondria or partial nuclear genomes) of polar and brown bears, and establishes the divergence of polar and brown bears at 400,000 years ago.
However, the two species have mated intermittently for all that time, most likely coming into contact with each other during warming periods, when polar bears were driven onto land and brown bears migrated northward. Most brown bears have about 2 percent genetic material from polar bears, but one population, the ABC Islands bears has between 5 percent and 10 percent polar bear genes, indicating more frequent and recent mating. Polar bears can breed with brown bears to produce fertile grizzly–polar bear hybrids, rather than indicating that they have only recently diverged, the new evidence suggests more frequent mating has continued over a longer period of time, and thus the two bears remain genetically similar. However, because neither species can survive long in the other's ecological niche, and because they have different morphology, metabolism, social and feeding behaviours, and other phenotypic characteristics, the two bears are generally classified as separate species.
When the polar bear was originally documented, two subspecies were identified: "Ursus maritimus maritimus" by Constantine J. Phipps in 1774, and "Ursus maritimus marinus" by Peter Simon Pallas in 1776. This distinction has since been invalidated. One alleged fossil subspecies has been identified: "Ursus maritimus tyrannus" became extinct during the Pleistocene. "U.m. tyrannus" was significantly larger than the living subspecies. However, recent reanalysis of the fossil suggests that it was actually a type of brown bear.
Population and distribution.
The polar bear is found in the Arctic Circle and adjacent land masses as far south as Newfoundland. Due to the absence of human development in its remote habitat, it retains more of its original range than any other extant carnivore. While they are rare north of 88°, there is evidence that they range all the way across the Arctic, and as far south as James Bay in Canada. Their southernmost range is near the boundary between the subarctic and humid continental climate zones. They can occasionally drift widely with the sea ice, and there have been anecdotal sightings as far south as Berlevåg on the Norwegian mainland and the Kuril Islands in the Sea of Okhotsk. It is difficult to estimate a global population of polar bears as much of the range has been poorly studied; however, biologists use a working estimate of about 20–25,000 or 22–31,000 polar bears worldwide.
There are 19 generally recognized, discrete subpopulations, though polar bears are thought to exist only in low densities in the area of the Arctic Basin. The subpopulations display seasonal fidelity to particular areas, but DNA studies show that they are not reproductively isolated. The thirteen North American subpopulations range from the Beaufort Sea south to Hudson Bay and east to Baffin Bay in western Greenland and account for about 54% of the global population. The Eurasian population is broken up into the eastern Greenland, Barents Sea, Kara Sea, Laptev Sea, and Chukchi Sea subpopulations, though there is considerable uncertainty about the structure of these populations due to limited mark and recapture data.
The range includes the territory of five nations: Denmark (Greenland), Norway (Svalbard), Russia, the United States (Alaska) and Canada. These five nations are the signatories of the International Agreement on the Conservation of Polar Bears, which mandates cooperation on research and conservation efforts throughout the polar bear's range.
Modern methods of tracking polar bear populations have been implemented only since the mid-1980s, and are expensive to perform consistently over a large area. The most accurate counts require flying a helicopter in the Arctic climate to find polar bears, shooting a tranquilizer dart at the bear to sedate it, and then tagging the bear. In Nunavut, some Inuit have reported increases in bear sightings around human settlements in recent years, leading to a belief that populations are increasing. Scientists have responded by noting that hungry bears may be congregating around human settlements, leading to the illusion that populations are higher than they actually are. The Polar Bear Specialist Group of the IUCN Species Survival Commission takes the position that "estimates of subpopulation size or sustainable harvest levels should not be made solely on the basis of traditional ecological knowledge without supporting scientific studies."
Of the 19 recognized polar bear subpopulations, three are declining, six are stable, one is increasing, and nine have insufficient data, as of 2014.
Habitat.
The polar bear is a marine mammal because it spends many months of the year at sea. However, it is the only living marine mammal with powerful, large limbs and feet that allow them to cover miles on foot and run on land. Its preferred habitat is the annual sea ice covering the waters over the continental shelf and the Arctic inter-island archipelagos. These areas, known as the "Arctic ring of life", have high biological productivity in comparison to the deep waters of the high Arctic. The polar bear tends to frequent areas where sea ice meets water, such as polynyas and leads (temporary stretches of open water in Arctic ice), to hunt the seals that make up most of its diet. Freshwater is limited in these environments because it is either locked up in snow or saline. Polar bears are able to produce water through the metabolism of fats found in seal blubber. Polar bears are therefore found primarily along the perimeter of the polar ice pack, rather than in the Polar Basin close to the North Pole where the density of seals is low.
Annual ice contains areas of water that appear and disappear throughout the year as the weather changes. Seals migrate in response to these changes, and polar bears must follow their prey. In Hudson Bay, James Bay, and some other areas, the ice melts completely each summer (an event often referred to as "ice-floe breakup"), forcing polar bears to go onto land and wait through the months until the next freeze-up. In the Chukchi and Beaufort seas, polar bears retreat each summer to the ice further north that remains frozen year-round.
Biology and behaviour.
Physical characteristics.
The only other bear of a similar size to the polar bear is the Kodiak bear, which is a subspecies of brown bear. Adult male polar bears weigh and measure in total length. The Guinness Book of World Records listed the average male as having a body mass of and a shoulder height of , slightly smaller than the average male Kodiak bears. Around the Beaufort Sea, however, mature males reportedly average . Adult females are roughly half the size of males and normally weigh , measuring in length. Elsewhere, a slightly larger estimated average weight of was claimed for adult females. When pregnant, however, females can weigh as much as . The polar bear is among the most sexually dimorphic of mammals, surpassed only by the pinnipeds such as elephant seals. The largest polar bear on record, reportedly weighing , was a male shot at Kotzebue Sound in northwestern Alaska in 1960. This specimen, when mounted, stood tall on its hindlegs. The shoulder height of an adult polar bear is . While all bears are short-tailed, the polar bear's tail is relatively the shortest amongst living bears, ranging from in length.
Compared with its closest relative, the brown bear, the polar bear has a more elongated body build and a longer skull and nose. As predicted by Allen's rule for a northerly animal, the legs are stocky and the ears and tail are small. However, the feet are very large to distribute load when walking on snow or thin ice and to provide propulsion when swimming; they may measure across in an adult. The pads of the paws are covered with small, soft papillae (dermal bumps), which provide traction on the ice. The polar bear's claws are short and stocky compared to those of the brown bear, perhaps to serve the former's need to grip heavy prey and ice. The claws are deeply scooped on the underside to assist in digging in the ice of the natural habitat. Research of injury patterns in polar bear forelimbs found injuries to the right forelimb to be more frequent than those to the left, suggesting, perhaps, right-handedness. Unlike the brown bear, polar bears in captivity are rarely overweight or particularly large, possibly as a reaction to the warm conditions of most zoos.
The 42 teeth of a polar bear reflect its highly carnivorous diet. The cheek teeth are smaller and more jagged than in the brown bear, and the canines are larger and sharper. The dental formula is 
Polar bears are superbly insulated by up to of adipose tissue, their hide and their fur; they overheat at temperatures above , and are nearly invisible under infrared photography. Polar bear fur consists of a layer of dense underfur and an outer layer of guard hairs, which appear white to tan but are actually transparent. The guard hair is over most of the body. Polar bears gradually moult from May to August, but, unlike other Arctic mammals, they do not shed their coat for a darker shade to camouflage themselves in the summer conditions. The hollow guard hairs of a polar bear coat were once thought to act as fiber-optic tubes to conduct light to its black skin, where it could be absorbed; however, this hypothesis was disproved by a study in 1998.
The white coat usually yellows with age. When kept in captivity in warm, humid conditions, the fur may turn a pale shade of green due to algae growing inside the guard hairs. Males have significantly longer hairs on their forelegs, which increase in length until the bear reaches 14 years of age. The male's ornamental foreleg hair is thought to attract females, serving a similar function to the lion's mane.
The polar bear has an extremely well developed sense of smell, being able to detect seals nearly away and buried under of snow. Its hearing is about as acute as that of a human, and its vision is also good at long distances.
The polar bear is an excellent swimmer and often will swim for days. One bear swam continuously for 9 days in the frigid Bering Sea for to reach ice far from land. She then travelled another . During the swim, the bear lost 22% of her body mass and her yearling cub died. With its body fat providing buoyancy, the bear swims in a dog paddle fashion using its large forepaws for propulsion. Polar bears can swim . When walking, the polar bear tends to have a lumbering gait and maintains an average speed of around . When sprinting, they can reach up to .
Hunting and diet.
The polar bear is the most carnivorous member of the bear family, and throughout most of its range, its diet primarily consists of ringed ("Pusa hispida") and bearded seals ("Erignathus barbatus"). The Arctic is home to millions of seals, which become prey when they surface in holes in the ice in order to breathe, or when they haul out on the ice to rest. Polar bears hunt primarily at the interface between ice, water, and air; they only rarely catch seals on land or in open water.
The polar bear's most common hunting method is called "still-hunting": The bear uses its excellent sense of smell to locate a seal breathing hole, and crouches nearby in silence for a seal to appear. The bear may lay in wait for several hours. When the seal exhales, the bear smells its breath, reaches into the hole with a forepaw, and drags it out onto the ice. The polar bear kills the seal by biting its head to crush its skull. The polar bear also hunts by stalking seals resting on the ice: Upon spotting a seal, it walks to within , and then crouches. If the seal does not notice, the bear creeps to within of the seal and then suddenly rushes forth to attack. A third hunting method is to raid the birth lairs that female seals create in the snow.
A widespread legend tells that polar bears cover their black noses with their paws when hunting. This behaviour, if it happens, is rare – although the story exists in the oral history of northern peoples and in accounts by early Arctic explorers, there is no record of an eyewitness account of the behaviour in recent decades.
Mature bears tend to eat only the calorie-rich skin and blubber of the seal, which are highly digestible, whereas younger bears consume the protein-rich red meat. Studies have also photographed polar bears scaling near-vertical cliffs, to eat birds' chicks and eggs. For subadult bears, which are independent of their mother but have not yet gained enough experience and body size to successfully hunt seals, scavenging the carcasses from other bears' kills is an important source of nutrition. Subadults may also be forced to accept a half-eaten carcass if they kill a seal but cannot defend it from larger polar bears. After feeding, polar bears wash themselves with water or snow.
Although polar bears are extraordinarily powerful, its primary prey species, the ringed seal, is much smaller than itself, and many of the seals hunted are pups rather than adults. Ringed seals are born weighing and grown to an estimated average weight of only . They also in places prey heavily upon the harp seal ("Pusa groenlandica") or the harbor seal. The bearded seal, on the other hand, can be nearly the same size as the bear itself, averaging . Adult male bearded seals, at are too large for a female bear to overtake, and so are potential prey only for mature male bears. Large males also occasionally attempt to hunt and kill even larger prey items. It can kill an adult walrus ("Odobenus rosmarus"), although this is rarely attempted. At up to and a typical adult mass range of , a walrus can be more than twice the bear's weight, and has up to -long ivory tusks that can be used as formidable weapons. A polar bear may charge a group of walruses, with the goal of separating a young, infirm, or injured walrus from the pod. They will even attack adult walruses when their diving holes have frozen over or intercept them before they can get back to the diving hole in the ice. Yet, polar bears will very seldom attack full-grown adult walruses, with the largest male walrus probably invulnerable unless otherwise injured or incapacitated. Since an attack on a walrus tends to be an extremely protracted and exhausting venture, bears have been known to back down from the attack after making the initial injury to the walrus. Polar bears have also been seen to prey on beluga whales ("Delphinapterus leucas") and narwhals ("Monodon monoceros"), by swiping at them at breathing holes. The whales are of similar size to the walrus and nearly as difficult for the bear to subdue. Most terrestrial animals in the Arctic can outrun the polar bear on land as polar bears overheat quickly, and most marine animals the bear encounters can outswim it. In some areas, the polar bear's diet is supplemented by walrus calves and by the carcasses of dead adult walruses or whales, whose blubber is readily devoured even when rotten.
Polar bears sometimes like to go fishing where they swim underwater to catch fish like the Arctic charr or the fourhorn sculpin.
With the exception of pregnant females, polar bears are active year-round, although they have a vestigial hibernation induction trigger in their blood. Unlike brown and black bears, polar bears are capable of fasting for up to several months during late summer and early fall, when they cannot hunt for seals because the sea is unfrozen. When sea ice is unavailable during summer and early autumn, some populations live off fat reserves for months at a time, as polar bears do not 'hibernate' any time of the year.
Being both curious animals and scavengers, polar bears investigate and consume garbage where they come into contact with humans. Polar bears may attempt to consume almost anything they can find, including hazardous substances such as styrofoam, plastic, car batteries, ethylene glycol, hydraulic fluid, and motor oil. The dump in Churchill, Manitoba was closed in 2006 to protect bears, and waste is now recycled or transported to Thompson, Manitoba.
Dietary flexibility.
Although seal predation is the primary and an indispensable way of life for most polar bears, when alternatives are present they are quite flexible. Polar bears will consume a wide variety of other wild foods, including muskox ("Ovibos moschatus"), reindeer ("Rangifer tarandus"), birds, eggs, rodents, crabs, other crustaceans and other polar bears. They may also eat plants, including berries, roots, and kelp, however none of these are a significant part of their diet, except that beachcast marine mammal carcasses are an exception. When stalking land animals, such as muskox, reindeer, and even willow ptarmigan ("Lagopus lagopus"), polar bears appear to make use of vegetative cover and wind direction to bring them as close to their prey as possible before attacking. Polar bears have been observed to hunt the small Svalbard reindeer ("R. t. platyrhynchus"), which weigh only as adults, as well as the barren-ground caribou ("R. t. groenlandicus"), which is about twice as heavy as that. Adult muskox, which can weigh or more, are a more formidable quarry. Although ungulates are not typical prey, the killing of one during the summer months can greatly increase the odds of survival during that lean period. Like the brown bear, most ungulate prey of polar bears is likely to be young, sickly or injured specimens rather than healthy adults. The polar bear's biology is specialized to require large amounts of fat from marine mammals, and it cannot derive sufficient caloric intake from terrestrial food.
In their southern range, especially near Hudson Bay and James Bay, Canadian polar bears endure all summer without sea ice to hunt from. Here, their food ecology shows their dietary flexibility. They still manage to consume some seals, but they are food-deprived in summer as only marine mammal carcasses are an important alternative without sea ice, especially carcasses of the beluga whale. These alternatives may reduce the rate of weight loss of bears when on land. One scientist found that 71% of the Hudson Bay bears had fed on seaweed (marine algae) and that about half were feeding on birds like sea ducks, especially the oldsquaw (53%), common eider, long-tailed duck or dovekie by swimming underwater to catch them. They were also diving to feed on blue mussels and other underwater food sources like the green sea urchin. 24% had eaten moss recently, 19% had consumed grass, 34% had eaten black crowberry and about half had consumed willows. This study illustrates the polar bear's dietary flexibility but it does not represent its life history elsewhere. Most polar bears elsewhere will never have access to these alternatives, except for the marine mammal carcasses that are important wherever they occur.
In Svalbard, polar bears were observed to kill white-beaked dolphins during spring, when the dolphins were trapped in the sea ice. The bears then proceeded to cache the carcasses, which remained and were eaten during the ice-free summer and autumn.
Behaviour.
Unlike grizzly bears, polar bears are not territorial. Although stereotyped as being voraciously aggressive, they are normally cautious in confrontations, and often choose to escape rather than fight. Satiated polar bears rarely attack humans unless severely provoked. However, due to their lack of prior human interaction, hungry polar bears are extremely unpredictable, fearless towards people and are known to kill and sometimes eat humans. Many attacks by brown bears are the result of surprising the animal, which is not the case with the polar bear. Polar bears are stealth hunters, and the victim is often unaware of the bear's presence until the attack is underway. Whereas brown bears often maul a person and then leave, polar bear attacks are more likely to be predatory and are almost always fatal. However, due to the very small human population around the Arctic, such attacks are rare. Michio Hoshino, a Japanese wildlife photographer, was once pursued briefly by a hungry male polar bear in northern Alaska. According to Hoshino, the bear started running but Hoshino made it to his truck. The bear was able to reach the truck and tore one of the doors off the truck before Hoshino was able to drive off.
In general, adult polar bears live solitary lives. Yet, they have often been seen playing together for hours at a time and even sleeping in an embrace, and polar bear zoologist Nikita Ovsianikov has described adult males as having "well-developed friendships." Cubs are especially playful as well. Among young males in particular, play-fighting may be a means of practicing for serious competition during mating seasons later in life. Polar bears have a wide range of vocalisations, including bellows, roars, growls, chuffs and purrs.
In 1992, a photographer near Churchill took a now widely circulated set of photographs of a polar bear playing with a Canadian Eskimo Dog ("Canis lupus familiaris") a tenth of its size. The pair wrestled harmlessly together each afternoon for ten days in a row for no apparent reason, although the bear may have been trying to demonstrate its friendliness in the hope of sharing the kennel's food. This kind of social interaction is uncommon; it is far more typical for polar bears to behave aggressively towards dogs.
Reproduction and lifecycle.
Courtship and mating take place on the sea ice in April and May, when polar bears congregate in the best seal hunting areas. A male may follow the tracks of a breeding female for or more, and after finding her engage in intense fighting with other males over mating rights, fights that often result in scars and broken teeth. Polar bears have a generally polygynous mating system; recent genetic testing of mothers and cubs, however, has uncovered cases of litters in which cubs have different fathers. Partners stay together and mate repeatedly for an entire week; the mating ritual induces ovulation in the female.
After mating, the fertilized egg remains in a suspended state until August or September. During these four months, the pregnant female eats prodigious amounts of food, gaining at least and often more than doubling her body weight.
Maternity denning and early life.
When the ice floes break up in the fall, ending the possibility of hunting, each pregnant female digs a "maternity den" consisting of a narrow entrance tunnel leading to one to three chambers. Most maternity dens are in snowdrifts, but may also be made underground in permafrost if it is not sufficiently cold yet for snow. In most subpopulations, maternity dens are situated on land a few kilometers from the coast, and the individuals in a subpopulation tend to reuse the same denning areas each year. The polar bears that do not den on land make their dens on the sea ice. In the den, she enters a dormant state similar to hibernation. This hibernation-like state does not consist of continuous sleeping; however, the bear's heart rate slows from 46 to 27 beats per minute. Her body temperature does not decrease during this period as it would for a typical mammal in hibernation.
Between November and February, cubs are born blind, covered with a light down fur, and weighing less than , but in captivity they might be delivered in the earlier months. The earliest recorded birth of polar bears in captivity was on 11 October 2011 in the Toronto Zoo. On average, each litter has two cubs. The family remains in the den until mid-February to mid-April, with the mother maintaining her fast while nursing her cubs on a fat-rich milk. By the time the mother breaks open the entrance to the den, her cubs weigh about . For about 12 to 15 days, the family spends time outside the den while remaining in its vicinity, the mother grazing on vegetation while the cubs become used to walking and playing. Then they begin the long walk from the denning area to the sea ice, where the mother can once again catch seals. Depending on the timing of ice-floe breakup in the fall, she may have fasted for up to eight months. During this time, cubs playfully imitate the mother's hunting methods in preparation for later life.
Female polar bears are noted for both their affection towards their offspring, and their valor in protecting them. Multiple cases of adoption of wild cubs have been confirmed by genetic testing. Adult male bears occasionally kill and eat polar bear cubs. As of 2006, in Alaska, 42% of cubs were reaching 12 months of age, down from 65% in 1991. In most areas, cubs are weaned at two and a half years of age, when the mother chases them away or abandons them. The Western Hudson Bay subpopulation is unusual in that its female polar bears sometimes wean their cubs at only one and a half years. This was the case for 40% of cubs there in the early 1980s; however by the 1990s, fewer than 20% of cubs were weaned this young. After the mother leaves, sibling cubs sometimes travel and share food together for weeks or months.
Later life.
Females begin to breed at the age of four years in most areas, and five years in the Beaufort Sea area. Males usually reach sexual maturity at six years; however, as competition for females is fierce, many do not breed until the age of eight or ten. A study in Hudson Bay indicated that both the reproductive success and the maternal weight of females peaked in their mid-teens.
Polar bears appear to be less affected by infectious diseases and parasites than most terrestrial mammals. Polar bears are especially susceptible to "Trichinella", a parasitic roundworm they contract through cannibalism, although infections are usually not fatal. Only one case of a polar bear with rabies has been documented, even though polar bears frequently interact with Arctic foxes, which often carry rabies. Bacterial leptospirosis and "Morbillivirus" have been recorded. Polar bears sometimes have problems with various skin diseases that may be caused by mites or other parasites.
Life expectancy.
Polar bears rarely live beyond 25 years. The oldest wild bears on record died at age 32, whereas the oldest captive was a female who died in 1991, age 43. The causes of death in wild adult polar bears are poorly understood, as carcasses are rarely found in the species's frigid habitat. In the wild, old polar bears eventually become too weak to catch food, and gradually starve to death. Polar bears injured in fights or accidents may either die from their injuries or become unable to hunt effectively, leading to starvation.
Ecological role.
The polar bear is the apex predator within its range, and is a keystone species for the Arctic. Several animal species, particularly Arctic foxes ("Vulpes lagopus") and glaucous gulls ("Larus hyperboreus"), routinely scavenge polar bear kills.
The relationship between ringed seals and polar bears is so close that the abundance of ringed seals in some areas appears to regulate the density of polar bears, while polar bear predation in turn regulates density and reproductive success of ringed seals. The evolutionary pressure of polar bear predation on seals probably accounts for some significant differences between Arctic and Antarctic seals. Compared to the Antarctic, where there is no major surface predator, Arctic seals use more breathing holes per individual, appear more restless when hauled out on the ice, and rarely defecate on the ice. The baby fur of most Arctic seal species is white, presumably to provide camouflage from predators, whereas Antarctic seals all have dark fur at birth.
Brown bears tend to dominate polar bears in disputes over carcasses, and dead polar bear cubs have been found in brown bear dens. Wolves are rarely encountered by polar bears, though there are two records of Arctic wolf ("Canis lupus arctos") packs killing polar bear cubs. A rather unlikely killer of a grown polar bear has reportedly included a wolverine ("Gulo gulo"), anecedotely reported to have suffocated a bear in a zoo with a bite to the throat during a conflict. Polar bears are sometimes the host of arctic mites such as "Alaskozetes antarcticus".
Long-distance swimming and diving.
Researchers tracked 52 sows in the southern Beaufort Sea off Alaska with GPS system collars; no boars were involved in the study due to males' necks being too thick for the GPS-equipped collars. Fifty long-distance swims were recorded; the longest at , with an average of . The length of these swims ranged from most of a day to ten days. Ten of the sows had a cub swim with them and after a year, six cubs survived. The study did not determine if the others lost their cubs before, during, or some time after their long swims. Researchers do not know whether or not this is a new behaviour; before polar ice shrinkage, they opined that there was probably neither the need nor opportunity to swim such long distances.
The polar bear may swim underwater for up to three minutes to approach seals on shore or on ice floes.
Hunting.
Indigenous people.
Polar bears have long provided important raw materials for Arctic peoples, including the Inuit, Yupik, Chukchi, Nenets, Russian Pomors and others. Hunters commonly used teams of dogs to distract the bear, allowing the hunter to spear the bear or shoot it with arrows at closer range. Almost all parts of captured animals had a use. The fur was used in particular to make trousers and, by the Nenets, to make galoshes-like outer footwear called "tobok"; the meat is edible, despite some risk of trichinosis; the fat was used in food and as a fuel for lighting homes, alongside seal and whale blubber; sinews were used as thread for sewing clothes; the gallbladder and sometimes heart were dried and powdered for medicinal purposes; the large canine teeth were highly valued as talismans. Only the liver was not used, as its high concentration of vitamin A is poisonous. Hunters make sure to either toss the liver into the sea or bury it in order to spare their dogs from potential poisoning. Traditional subsistence hunting was on a small enough scale to not significantly affect polar bear populations, mostly because of the sparseness of the human population in polar bear habitat.
History of commercial harvest.
In Russia, polar bear furs were already being commercially traded in the 14th century, though it was of low value compared to Arctic fox or even reindeer fur. The growth of the human population in the Eurasian Arctic in the 16th and 17th century, together with the advent of firearms and increasing trade, dramatically increased the harvest of polar bears. However, since polar bear fur has always played a marginal commercial role, data on the historical harvest is fragmentary. It is known, for example, that already in the winter of 1784/1785 Russian Pomors on Spitsbergen harvested 150 polar bears in Magdalenefjorden. In the early 20th century, Norwegian hunters were harvesting 300 bears per year at the same location. Estimates of total historical harvest suggest that from the beginning of the 18th century, roughly 400 to 500 animals were being harvested annually in northern Eurasia, reaching a peak of 1,300 to 1,500 animals in the early 20th century, and falling off as the numbers began dwindling.
In the first half of the 20th century, mechanized and overpoweringly efficient methods of hunting and trapping came into use in North America as well. Polar bears were chased from snowmobiles, icebreakers, and airplanes, the latter practice described in a 1965 "New York Times" editorial as being "about as sporting as machine gunning a cow." Norwegians used "self-killing guns", comprising a loaded rifle in a baited box that was placed at the level of a bear's head, and which fired when the string attached to the bait was pulled. The numbers taken grew rapidly in the 1960s, peaking around 1968 with a global total of 1,250 bears that year.
Contemporary regulations.
Concerns over the future survival of the species led to the development of national regulations on polar bear hunting, beginning in the mid-1950s. The Soviet Union banned all hunting in 1956. Canada began imposing hunting quotas in 1968. Norway passed a series of increasingly strict regulations from 1965 to 1973, and has completely banned hunting since then. The United States began regulating hunting in 1971 and adopted the Marine Mammal Protection Act in 1972. In 1973, the International Agreement on the Conservation of Polar Bears was signed by all five nations whose territory is inhabited by polar bears: Canada, Denmark, Norway, the Soviet Union, and the United States. Member countries agreed to place restrictions on recreational and commercial hunting, ban hunting from aircraft and icebreakers, and conduct further research. The treaty allows hunting "by local people using traditional methods". Norway is the only country of the five in which all harvest of polar bears is banned. The agreement was a rare case of international cooperation during the Cold War. Biologist Ian Stirling commented, "For many years, the conservation of polar bears was the only subject in the entire Arctic that nations from both sides of the Iron Curtain could agree upon sufficiently to sign an agreement. Such was the intensity of human fascination with this magnificent predator, the only marine bear."
Agreements have been made between countries to co-manage their shared polar bear subpopulations. After several years of negotiations, Russia and the United States signed an agreement in October 2000 to jointly set quotas for indigenous subsistence hunting in Alaska and Chukotka. The treaty was ratified in October 2007. In September 2015, the polar bear range states agreed upon a "circumpolar action plan" describing their conservation strategy for polar bears.
Although the United States government has proposed that polar bears be transferred to Appendix I of CITES, which would ban all international trade in polar bear parts, polar bears currently remain listed under Appendix II. This decision was approved of by members of the IUCN and TRAFFIC, who determined that such an uplisting was unlikely to confer a conservation benefit.
Canada.
Polar bears were designated "Not at Risk" in April 1986 and uplisted to "Special Concern" in April 1991. This status was re-evaluated and confirmed in April 1999, November 2002, and April 2008. Polar bears continue to be listed as a species of special concern in Canada because of their sensitivity to overharvest and because of an expected range contraction caused by loss of Arctic sea ice.
More than 600 bears are killed per year by humans across Canada, a rate calculated by scientists to be unsustainable for some areas, notably Baffin Bay. Canada has allowed sport hunters accompanied by local guides and dog-sled teams since 1970, but the practice was not common until the 1980s. The guiding of sport hunters provides meaningful employment and an important source of income for northern communities in which economic opportunities are few. Sport hunting can bring CDN$20,000 to $35,000 per bear into northern communities, which until recently has been mostly from American hunters.
The territory of Nunavut accounts for the location 80% of annual kills in Canada. In 2005, the government of Nunavut increased the quota from 400 to 518 bears, despite protests from the IUCN Polar Bear Specialist Group. In two areas where harvest levels have been increased based on increased sightings, science-based studies have indicated declining populations, and a third area is considered data-deficient. While most of that quota is hunted by the indigenous Inuit people, a growing share is sold to recreational hunters. (0.8% in the 1970s, 7.1% in the 1980s, and 14.6% in the 1990s) Nunavut polar bear biologist, Mitchell Taylor, who was formerly responsible for polar bear conservation in the territory, has insisted that bear numbers are being sustained under current hunting limits. In 2010, the 2005 increase was partially reversed. Government of Nunavut officials announced that the polar bear quota for the Baffin Bay region would be gradually reduced from 105 per year to 65 by the year 2013. The Government of the Northwest Territories maintain their own quota of 72 to 103 bears within the Inuvialuit communities of which some are set aside for sports hunters. Environment Canada also banned the export from Canada of fur, claws, skulls and other products from polar bears harvested in Baffin Bay as of 1 January 2010.
Because of the way polar bear hunting quotas are managed in Canada, attempts to discourage sport hunting would actually increase the number of bears killed in the short term. Canada allocates a certain number of permits each year to sport and subsistence hunting, and those that are not used for sport hunting are re-allocated to indigenous subsistence hunting. Whereas northern communities kill all the polar bears they are permitted to take each year, only half of sport hunters with permits actually manage to kill a polar bear. If a sport hunter does not kill a polar bear before his or her permit expires, the permit cannot be transferred to another hunter.
In August 2011, Environment Canada published a national polar bear conservation strategy.
Greenland.
In Greenland, hunting restrictions were first introduced in 1994 and expanded by executive order in 2005. Until 2005 Greenland placed no limit on hunting by indigenous people. However, in 2006 it imposed a limit of 150, while also allowed recreational hunting for the first time. Other provisions included year-round protection of cubs and mothers, restrictions on weapons used, and various administrative requirements to catalogue kills.
Russia.
The Soviet Union banned the harvest of polar bears in 1956; however, poaching continued and is estimated to pose a serious threat to the polar bear population. In recent years, polar bears have approached coastal villages in Chukotka more frequently due to the shrinking of the sea ice, endangering humans and raising concerns that illegal hunting would become even more prevalent. In 2007, the Russian government made subsistence hunting legal for indigenous Chukotkan peoples only, a move supported by Russia's most prominent bear researchers and the World Wide Fund for Nature as a means to curb poaching.
Polar bears are currently listed as "Rare", of "Uncertain Status", or "Rehabilitated and rehabilitating" in the Red Data Book of Russia, depending on population. In 2010, the Ministry of Natural Resources and Environment published a strategy for polar bear conservation in Russia.
United States.
The Marine Mammal Protection Act of 1972 afforded polar bears some protection in the United States. It banned hunting (except by indigenous subsistence hunters), banned importing of polar bear parts (except polar bear pelts taken legally in Canada), and banned the harassment of polar bears. On 15 May 2008, the United States Department of the Interior listed the polar bear as a threatened species under the Endangered Species Act, citing the melting of Arctic sea ice as the primary threat to the polar bear. It banned all importing of polar bear trophies. Importing products made from polar bears had been prohibited from 1972 to 1994 under the Marine Mammal Protection Act, and restricted between 1994 and 2008. Under those restrictions, permits from the United States Fish and Wildlife Service were required to import sport-hunted polar bear trophies taken in hunting expeditions in Canada. The permit process required that the bear be taken from an area with quotas based on sound management principles. Since 1994, hundreds of sport-hunted polar bear trophies have been imported into the U.S. In 2015, the U.S. Fish and Wildlife Service published a draft conservation management plan for polar bears to improve their status under the Endangered Species Act and the Marine Mammal Protection Act.
Conservation status, threats and controversies.
Polar bear population sizes and trends are difficult to estimate accurately because they occupy remote home ranges and exist at low population densities. Polar bear fieldwork can also be hazardous to researchers. As of 2015, the International Union for Conservation of Nature (IUCN) reports that the global population of polar bears is 22,000 to 31,000, and the current population trend is unknown. Nevertheless, polar bears are listed as "Vulnerable" under criterion A3c, which indicates an expected population decrease of ≥30% over the next three generations (~34.5 years) due to "decline in area of occupancy, extent of occurrence and/or quality of habitat". Risks to the polar bear include climate change, pollution in the form of toxic contaminants, conflicts with shipping, oil and gas exploration and development, and human-bear interactions including harvesting and possible stresses from recreational polar-bear watching.
According to the World Wildlife Fund, the polar bear is important as an indicator of Arctic ecosystem health. Polar bears are studied to gain understanding of what is happening throughout the Arctic, because at-risk polar bears are often a sign of something wrong with the Arctic marine ecosystem.
Climate change.
The International Union for Conservation of Nature, Arctic Climate Impact Assessment, United States Geological Survey and many leading polar bear biologists have expressed grave concerns about the impact of climate change, including the belief that the current warming trend imperils the survival of the polar bear.
The key danger posed by climate change is malnutrition or starvation due to habitat loss. Polar bears hunt seals from a platform of sea ice. Rising temperatures cause the sea ice to melt earlier in the year, driving the bears to shore before they have built sufficient fat reserves to survive the period of scarce food in the late summer and early fall. Reduction in sea-ice cover also forces bears to swim longer distances, which further depletes their energy stores and occasionally leads to drowning. Thinner sea ice tends to deform more easily, which appears to make it more difficult for polar bears to access seals. Insufficient nourishment leads to lower reproductive rates in adult females and lower survival rates in cubs and juvenile bears, in addition to poorer body condition in bears of all ages.
In addition to creating nutritional stress, a warming climate is expected to affect various other aspects of polar bear life: Changes in sea ice affect the ability of pregnant females to build suitable maternity dens. As the distance increases between the pack ice and the coast, females must swim longer distances to reach favored denning areas on land. Thawing of permafrost would affect the bears who traditionally den underground, and warm winters could result in den roofs collapsing or having reduced insulative value. For the polar bears that currently den on multi-year ice, increased ice mobility may result in longer distances for mothers and young cubs to walk when they return to seal-hunting areas in the spring. Disease-causing bacteria and parasites would flourish more readily in a warmer climate.
Problematic interactions between polar bears and humans, such as foraging by bears in garbage dumps, have historically been more prevalent in years when ice-floe breakup occurred early and local polar bears were relatively thin. Increased human-bear interactions, including fatal attacks on humans, are likely to increase as the sea ice shrinks and hungry bears try to find food on land.
The effects of climate change are most profound in the southern part of the polar bear's range, and this is indeed where significant degradation of local populations has been observed. The Western Hudson Bay subpopulation, in a southern part of the range, also happens to be one of the best-studied polar bear subpopulations. This subpopulation feeds heavily on ringed seals in late spring, when newly weaned and easily hunted seal pups are abundant. The late spring hunting season ends for polar bears when the ice begins to melt and break up, and they fast or eat little during the summer until the sea freezes again.
Due to warming air temperatures, ice-floe breakup in western Hudson Bay is currently occurring three weeks earlier than it did 30 years ago, reducing the duration of the polar bear feeding season. The body condition of polar bears has declined during this period; the average weight of lone (and likely pregnant) female polar bears was approximately in 1980 and in 2004. Between 1987 and 2004, the Western Hudson Bay population declined by 22%, although the population is currently listed as "stable".
In Alaska, the effects of sea ice shrinkage have contributed to higher mortality rates in polar bear cubs, and have led to changes in the denning locations of pregnant females. In recent years, polar bears in the Arctic have undertaken longer than usual swims to find prey, possibly resulting in four recorded drownings in the unusually large ice pack regression of 2005.
A new development is that polar bears have begun ranging to new territory. While not unheard of but still uncommon, polar bears have been sighted increasingly in larger numbers ashore, staying on the mainland for longer periods of time during the summer months, particularly in North Canada, traveling farther inland. This may cause an increased reliance on terrestrial diets, such as goose eggs, waterfowl and caribou, as well as increased human–bear conflict.
Pollution.
Polar bears accumulate high levels of persistent organic pollutants such as polychlorinated biphenyl (PCBs) and chlorinated pesticides. Due to their position at the top of the ecological pyramid, with a diet heavy in blubber in which halocarbons concentrate, their bodies are among the most contaminated of Arctic mammals. Halocarbons are known to be toxic to other animals, because they mimic hormone chemistry, and biomarkers such as immunoglobulin G and retinol suggest similar effects on polar bears. PCBs have received the most study, and they have been associated with birth defects and immune system deficiency.
Many chemicals, such as PCBs and DDT, have been internationally banned due to the recognition of their harm on the environment. Their concentrations in polar bear tissues continued to rise for decades after being banned as these chemicals spread through the food chain. Since then, the trend seems to have discontinued, with tissue concentrations of PCBs declining between studies performed from 1989 to 1993 and studies performed from 1996 to 2002. During the same time periods, DDT was notably lower in the Western Hudson Bay population only.
Oil and gas development.
Oil and gas development in polar bear habitat can affect the bears in a variety of ways. An oil spill in the Arctic would most likely concentrate in the areas where polar bears and their prey are also concentrated, such as sea ice leads. Because polar bears rely partly on their fur for insulation and soiling of the fur by oil reduces its insulative value, oil spills put bears at risk of dying from hypothermia. Polar bears exposed to oil spill conditions have been observed to lick the oil from their fur, leading to fatal kidney failure. Maternity dens, used by pregnant females and by females with infants, can also be disturbed by nearby oil exploration and development. Disturbance of these sensitive sites may trigger the mother to abandon her den prematurely, or abandon her litter altogether.
Predictions.
Steven Amstrup and other U.S. Geological Survey scientists have predicted two-thirds of the world's polar bears may disappear by 2050, based on moderate projections for the shrinking of summer sea ice caused by climate change, though the validity of this study has been debated. The bears could disappear from Europe, Asia, and Alaska, and be depleted from the Canadian Arctic Archipelago and areas off the northern Greenland coast. By 2080, they could disappear from Greenland entirely and from the northern Canadian coast, leaving only dwindling numbers in the interior Arctic Archipelago. However, in the short term, some polar bear populations in historically colder regions of the Arctic may temporarily benefit from a milder climate, as multiyear ice that is too thick for seals to create breathing holes is replaced by thinner annual ice.
Polar bears diverged from brown bears 400,000–600,000 years ago and have survived past periods of climate fluctuation. It has been claimed that polar bears will be able to adapt to terrestrial food sources as the sea ice they use to hunt seals disappears. However, most polar bear biologists think that polar bears will be unable to completely offset the loss of calorie-rich seal blubber with terrestrial foods, and that they will be outcompeted by brown bears in this terrestrial niche, ultimately leading to a population decline.
Controversy over species protection.
Warnings about the future of the polar bear are often contrasted with the fact that worldwide population estimates have increased over the past 50 years and are relatively stable today. Some estimates of the global population are around 5,000 to 10,000 in the early 1970s; other estimates were 20,000 to 40,000 during the 1980s. Current estimates put the global population at between 20,000 and 25,000 or 22,000 and 31,000.
There are several reasons for the apparent discordance between past and projected population trends: estimates from the 1950s and 1960s were based on stories from explorers and hunters rather than on scientific surveys. Second, controls of harvesting were introduced that allowed this previously overhunted species to recover. Third, the recent effects of climate change have affected sea ice abundance in different areas to varying degrees.
Debate over the listing of the polar bear under endangered species legislation has put conservation groups and Canada's Inuit at opposing positions; the Nunavut government and many northern residents have condemned the U.S. initiative to list the polar bear under the Endangered Species Act. Many Inuit believe the polar bear population is increasing, and restrictions on commercial sport-hunting are likely to lead to a loss of income to their communities.
In culture.
Indigenous folklore.
For the indigenous peoples of the Arctic, polar bears have long played an important cultural and material role. Polar bear remains have been found at hunting sites dating to 2,500 to 3,000 years ago and 1,500-year-old cave paintings of polar bears have been found in the Chukchi Peninsula. Indeed, it has been suggested that Arctic peoples' skills in seal hunting and igloo construction has been in part acquired from the polar bears themselves.
The Inuit and Alaska Natives have many folk tales featuring the bears including legends in which bears are humans when inside their own houses and put on bear hides when going outside, and stories of how the constellation that is said to resemble a great bear surrounded by dogs came into being. These legends reveal a deep respect for the polar bear, which is portrayed as both spiritually powerful and closely akin to humans. The human-like posture of bears when standing and sitting, and the resemblance of a skinned bear carcass to the human body, have probably contributed to the belief that the spirits of humans and bears were interchangeable. Eskimo legends tell of humans learning to hunt from the polar bear.
Among the Chukchi and Yupik of eastern Siberia, there was a longstanding shamanistic ritual of "thanksgiving" to the hunted polar bear. After killing the animal, its head and skin were removed and cleaned and brought into the home, a feast was held in the hunting camp in its honor. In order to appease the spirit of the bear, there were traditional song and drum music and the skull would be ceremonially fed and offered a pipe. Only once the spirit was appeased would the skull be separated from the skin, taken beyond the bounds of the homestead, and placed in the ground, facing north. Many of these traditions have faded somewhat in time, especially in light of the total hunting ban in the Soviet Union (and now Russia) since 1955.
The Nenets of north-central Siberia placed particular value on the talismanic power of the prominent canine teeth. They were traded in the villages of the lower Yenisei and Khatanga rivers to the forest-dwelling peoples further south, who would sew them into their hats as protection against brown bears. It was believed that the "little nephew" (the brown bear) would not dare to attack a man wearing the tooth of its powerful "big uncle" (the polar bear). The skulls of killed polar bears were buried at specific sacred sites and altars, called "sedyangi", were constructed out of the skulls. Several such sites have been preserved on the Yamal Peninsula.
Symbols and mascots.
Their distinctive appearance and their association with the Arctic have made polar bears popular icons, especially in those areas where they are native. The Canadian Toonie (two-dollar coin) features the image of a polar bear and both the Northwest Territories and Nunavut license plates in Canada are in the shape of a polar bear. The polar bear is the mascot of Bowdoin College in Maine and the University of Alaska Fairbanks (see also Alaska Nanooks) and was chosen as mascot for the 1988 Winter Olympics held in Calgary. The Eisbären Berlin professional hockey team, playing in the DEL top-level pro hockey league of Germany uses a roaring polar bear (seen head-on) as their team logo.
Companies such as Coca-Cola, Polar Beverages, Nelvana, Bundaberg Rum, and Good Humor-Breyers have used images of the polar bear in advertising, while Fox's Glacier Mints have featured a polar bear named Peppy as the brand mascot since 1922. This has supported the popularity of the polar bear, and it has since become one of a collection of creatures who are associated with Christmas, including penguins, reindeer and the European robin.
Fiction.
Polar bears are also popular in fiction, particularly in books aimed at children or teenagers. For example, "The Polar Bear Son" is adapted from a traditional Inuit tale. The animated television series "Noah's Island" features a polar bear named Noah as the protagonist. Polar bears feature prominently in "East" (also released as "North Child") by Edith Pattou, "The Bear" by Raymond Briggs (adapted into an animated short in 1998), and Chris d'Lacey's "The Fire Within" series. The "panserbjørne" of Philip Pullman's fantasy trilogy "His Dark Materials" are sapient, dignified polar bears who exhibit anthropomorphic qualities, and feature prominently in the 2007 film adaptation of "The Golden Compass". The television series "Lost" features polar bears living on the tropical island setting.

</doc>
<doc id="24411" url="https://en.wikipedia.org/wiki?curid=24411" title="Pagan (disambiguation)">
Pagan (disambiguation)

Pagan may refer to
Pagan may also refer to:

</doc>
<doc id="24412" url="https://en.wikipedia.org/wiki?curid=24412" title="Phalanx (disambiguation)">
Phalanx (disambiguation)

The phalanx is a rectangular mass military formation.
Phalanx may also refer to:

</doc>
<doc id="24413" url="https://en.wikipedia.org/wiki?curid=24413" title="Penguin Island">
Penguin Island

Penguin Island may refer to:

</doc>
<doc id="24416" url="https://en.wikipedia.org/wiki?curid=24416" title="Pommern (disambiguation)">
Pommern (disambiguation)

Pommern is the German language name for Pomerania, a historical region divided between Germany and Poland.
Pommern may also refer to:

</doc>
<doc id="24417" url="https://en.wikipedia.org/wiki?curid=24417" title="Punic Wars">
Punic Wars

The Punic Wars were a series of three wars fought between Rome and Carthage from 264 BC to 146 BC. At the time, they were probably the largest wars that had ever taken place. The term "Punic" comes from the Latin word "Punicus" (or "Poenicus"), meaning "Carthaginian", with reference to the Carthaginians' Phoenician ancestry.
The main cause of the Punic Wars was the conflicts of interest between the existing Carthaginian Empire and the expanding Roman Republic. The Romans were initially interested in expansion via Sicily (which at that time was a cultural melting pot), part of which lay under Carthaginian control. At the start of the first Punic War, Carthage was the dominant power of the Western Mediterranean, with an extensive maritime empire. Rome was a rapidly ascending power in Italy, but it lacked the naval power of Carthage. By the end of the third war, after more than a hundred years and the loss of many hundreds of thousands of soldiers from both sides, Rome had conquered Carthage's empire, completely destroyed the city, and become the most powerful state of the Western Mediterranean.
With the end of the Macedonian Wars – which ran concurrently with the Punic Wars – and the defeat of the Seleucid King Antiochus III the Great in the Roman–Seleucid War (Treaty of Apamea, 188 BC) in the eastern sea, Rome emerged as the dominant Mediterranean power and one of the most powerful cities in classical antiquity. The Roman victories over Carthage in these wars gave Rome a preeminent status it would retain until the 5th century AD.
Background.
During the mid-3rd century BC, Carthage was a large city located on the coast of modern Tunisia. Founded by the Phoenicians in the mid-9th century BC, it was a powerful thalassocratic city-state with a vast commercial network. Of the great city-states in the western Mediterranean, only Rome rivaled it in power, wealth, and population. While Carthage's navy was the largest in the ancient world at the time, it did not maintain a large, permanent, standing army. Instead, Carthage relied mostly on mercenaries, especially the indigenous Numidians, to fight its wars. However, most of the officers who commanded the armies were Carthaginian citizens. The Carthaginians were famed for their abilities as sailors, and unlike their armies, many Carthaginians from the lower classes served in their navy, which provided them with a stable income and career.
In 200 BC the Roman Republic had gained control of the Italian peninsula south of the Po river. Unlike Carthage, Rome had large disciplined armed forces. On the other hand, at the start of the First Punic War the Romans had no navy, and were thus at a disadvantage until they began to construct their own large fleets during the war.
First Punic War (264–241 BC).
The First Punic War (264–241 BC) was fought partly on land in Sicily and Africa, but was largely a naval war. It began as a local conflict in Sicily between Hiero II of Syracuse and the Mamertines of Messina. The Mamertines enlisted the aid of the Carthaginian navy, and then subsequently betrayed them by entreating the Roman Senate for aid against Carthage. The Romans sent a garrison to secure Messina, so the outraged Carthaginians then lent aid to Syracuse. With the two powers now embroiled in the conflict, tensions quickly escalated into a full-scale war between Carthage and Rome for the control of Sicily. After a harsh defeat at the Battle of Agrigentum in 262 BC, the Carthaginian leadership resolved to avoid further direct land-based engagements with the powerful Roman legions, and concentrate on the sea where they believed Carthage's large navy had the advantage. Initially the Carthaginian navy prevailed. In 260 BC they defeated the fledgling Roman navy at the Battle of the Lipari Islands. Rome responded by drastically expanding its navy in a very short time. Within two months the Romans had a fleet of over one hundred warships. Because they knew that they could not defeat the Carthaginians in the traditional tactics of ramming and sinking enemy ships, the Romans added the "corvus", an assault bridge, to Roman ships. The hinged bridge would swing onto enemy vessels with a sharp spike and stop them. Roman legionaries could then board and capture Carthaginian ships. This innovative Roman tactic reduced the Carthaginian navy's advantage in ship-to-ship engagements, and allowed Rome's superior infantry to be brought to bear in naval conflicts. However, the "corvus" was also cumbersome and dangerous, and was eventually phased out as the Roman navy became more experienced and tactically proficient. Save for the disastrous defeat at the Battle of Tunis in Africa, and two naval engagements, the First Punic War was a nearly unbroken string of Roman victories. In 241 BC, Carthage signed a peace treaty under the terms of which they evacuated Sicily and paid Rome a large war indemnity. The long war was costly to both powers, but Carthage was more seriously destabilized. In 238 BC, Carthage was plunged into the Mercenary War, during which Rome seized Sardinia and Corsica. Rome was now the most powerful state in the western Mediterranean: its large navy able to prevent seaborne invasion of Italy, control important sea trade routes, and invade foreign shores.
Aftermath.
Carthage spent the years following the war improving its finances and expanding its colonial empire in Hispania under the militaristic Barcid family. Rome's attention was mostly concentrated on the Illyrian Wars. In 219 BC Hannibal, the son of Hamilcar Barca, attacked Saguntum in Hispania, a city allied to Rome, starting the second Punic War.
Interval between the First and Second Punic Wars.
According to Polybius there had been several trade agreements between Rome and Carthage, even a mutual alliance against king Pyrrhus of Epirus. When Rome and Carthage made peace in 241 BC, Rome secured the release of all 8,000 prisoners of war without ransom and, furthermore, received a considerable amount of silver as a war indemnity. However, Carthage refused to deliver to Rome the Roman deserters serving among their troops. A first issue for dispute was that the initial treaty, agreed upon by Hamilcar Barca and the Roman commander in Sicily, had a clause stipulating that the Roman popular assembly had to accept the treaty in order for it to be valid. The assembly not only rejected the treaty but increased the indemnity Carthage had to pay.
Carthage had a liquidity problem and attempted to gain financial help from Egypt, a mutual ally of Rome and Carthage, but failed. This resulted in delay of payments owed to the mercenary troops that had served Carthage in Sicily, leading to a climate of mutual mistrust and, finally, a revolt supported by the Libyan natives, known as the Mercenary War (240–238 BC). During this war, Rome and Syracuse both aided Carthage, although traders from Italy seem to have done business with the insurgents. Some of them were caught and punished by Carthage, aggravating the political climate which had started to improve in recognition of the old alliance and treaties.
During the uprising in the Punic mainland, the mercenary troops in Corsica and Sardinia toppled Punic rule and briefly established their own, but were expelled by a native uprising. After securing aid from Rome, the exiled mercenaries then regained authority on the island of Sicily. For several years a brutal campaign was fought to quell the insurgent natives. Like many Sicilians, they would ultimately rise again in support of Carthage during the Second Punic War.
Eventually, Rome annexed Corsica and Sardinia by revisiting the terms of the treaty that ended the first Punic War. As Carthage was under siege and engaged in a difficult civil war, they grudgingly accepted the loss of these islands and the subsequent Roman conditions for ongoing peace, which also increased the war indemnity levied against Carthage after the first Punic War. This eventually plunged relations between the two powers to a new low point.
After Carthage emerged victorious from the Mercenary War there were two opposing factions: the reformist party was led by Hamilcar Barca while the other, more conservative, faction was represented by Hanno the Great and the old Carthaginian aristocracy. Hamilcar had led the initial Carthaginian peace negotiations and was blamed for the clause that allowed the Roman popular assembly to increase the war indemnity and annex Corsica and Sardinia, but his superlative generalship was instrumental in enabling Carthage to ultimately quell the mercenary uprising, ironically fought against many of the same mercenary troops he had trained. Hamilcar ultimately left Carthage for the Iberian peninsula where he captured rich silver mines and subdued many tribes who fortified his army with levies of native troops.
Hanno had lost many elephants and soldiers when he became complacent after a victory in the Mercenary War. Further, when he and Hamilcar were supreme commanders of Carthage's field armies, the soldiers had supported Hamilcar when his and Hamilcar's personalities clashed. On the other hand, he was responsible for the greatest territorial expansion of Carthage's hinterland during his rule as "strategus" and wanted to continue such expansion. However, the Numidian king of the relevant area was now a son-in-law of Hamilcar and had supported Carthage during a crucial moment in the Mercenary War. While Hamilcar was able to obtain the resources for his aim, the Numidians in the Atlas Mountains were not conquered, like Hanno suggested, but became vassals of Carthage.
The Iberian conquest was begun by Hamilcar Barca and his other son-in-law, Hasdrubal the Fair, who ruled relatively independently of Carthage and signed the Ebro Treaty with Rome. Hamilcar died in battle in 228 BC. Around this time, Hasdrubal became Carthaginian commander in Iberia (229 BC). He maintained this post for some eight years until 221 BC. Soon the Romans became aware of a burgeoning alliance between Carthage and the Celts of the Po river valley in northern Italy. The latter were amassing forces to invade Italy, presumably with Carthaginian backing. Thus, the Romans preemptively invaded the Po region in 225 BC. By 220 BC, the Romans had annexed the area as Gallia Cisalpina. Hasdrubal was assassinated around the same time (221 BC), bringing Hannibal to the fore. It seems that, having apparently dealt with the threat of a Gallo-Carthaginian invasion of Italy (and perhaps with the original Carthaginian commander killed), the Romans lulled themselves into a false sense of security. Thus, Hannibal took the Romans by surprise a mere two years later (218 BC) by merely reviving and adapting the original Gallo-Carthaginian invasion plan of his brother-in-law Hasdrubal.
After Hasdrubal's assassination by a Celtic assassin, Hamilcar's young sons took over, with Hannibal becoming the "strategus" of Iberia, although this decision was not undisputed in Carthage. The output of the Iberian silver mines allowed for the financing of a standing army and the payment of the war indemnity to Rome. The mines also served as a tool for political influence, creating a faction in Carthage's magistrate that was called the "Barcino".
In 219 BC Hannibal attacked the town of Saguntum, which stood under the special protection of Rome. According to Roman tradition, Hannibal had been made to swear by his father never to be a friend of Rome, and he certainly did not take a conciliatory attitude when the Romans berated him for crossing the river Iberus (Ebro) which Carthage was bound by treaty not to cross. Hannibal did not cross the Ebro River (Saguntum was near modern Valencia – well south of the river) in arms, and the Saguntines provoked his attack by attacking their neighboring tribes who were Carthaginian protectorates and by massacring pro-Punic factions in their city. Rome had no legal protection pact with any tribe south of the Ebro River. Nonetheless, they asked Carthage to hand Hannibal over, and when the Carthaginian oligarchy refused, Rome declared war on Carthage.
The Barcid Empire.
The 'Barcid Empire' consisted of the Punic territories in Iberia. According to the historian Pedro Barceló, it can be described as a private military-economic hegemony backed by the two independent powers, Carthage and Gades (modern Cádiz). These shared the profits of the silver mines in southern Iberia with the Barcas family and closely followed Hellenistic diplomatic customs. Gades played a supporting role in this field, but Hannibal visited the local temple to conduct ceremonies before launching his campaign against Rome. The Barcid Empire was strongly influenced by the Hellenistic kingdoms of the time and for example, contrary to Carthage, it minted silver coins in its short time of existence.
Second Punic War (218–201 BC).
The Second Punic War (218 BC – 201 BC) is most remembered for the Carthaginian Hannibal's crossing of the Alps. His army invaded Italy from the north and resoundingly defeated the Roman army in several battles, but never achieved the ultimate goal of causing a political break between Rome and its allies.
While fighting Hannibal in Italy, Hispania, and Sicily, Rome simultaneously fought against Macedon in the First Macedonian War. Eventually, the war was taken to Africa, where Carthage was defeated at the Battle of Zama (201 BC) by Scipio Africanus. The end of the war saw Carthage's control reduced to only the city itself.
There were three military theaters in this war: Italy, where Hannibal defeated the Roman legions repeatedly; Hispania, where Hasdrubal, a younger brother of Hannibal, defended the Carthaginian colonial cities with mixed success until eventually retreating into Italy; and Sicily, where the Romans held military supremacy.
Hannibal.
After assaulting Saguntum in Hispania (219 BC), Hannibal attacked Italy in 218 BC by leading the Iberians and three dozen elephants through the Alps. Although Hannibal surprised the Romans and thoroughly beat them on the battlefields of Italy, he lost his only siege engines and most of his elephants to the cold temperatures and icy mountain paths. In the end he could defeat the Romans in the field, but not in the strategically crucial city of Rome itself, thus leaving him unable to win the war.
Hannibal defeated the Roman legions in several major engagements, including the Battle of the Trebia (December 218 BC), the Battle of Lake Trasimene (217 BC) and most famously the Battle of Cannae (216 BC), but his long-term strategy failed. Lacking siege engines and sufficient manpower to take the city of Rome itself, he had planned to turn the Italian allies against Rome and to starve the city out through a siege. However, with the exception of a few of the southern city-states, the majority of the Roman allies remained loyal and continued to fight alongside Rome, despite Hannibal's near-invincible army devastating the Italian countryside. Rome also exhibited an impressive ability to draft army after army of conscripts after each crushing defeat by Hannibal, allowing them to recover from the defeats at Cannae and elsewhere and to keep Hannibal cut off from aid.
Hannibal never successfully received any significant reinforcements from Carthage. Despite his many pleas, Carthage only ever sent reinforcements successfully to Hispania. This lack of reinforcements prevented Hannibal from decisively ending the conflict by conquering Rome through force of arms.
The Roman army under Quintus Fabius Maximus intentionally deprived Hannibal of open battle in Italy for the rest of the war, while making it difficult for Hannibal to forage for supplies. Nevertheless, Rome was also incapable of bringing the conflict in the Italian theatre to a decisive close. Not only did Roman legions contend with Hannibal in Italy and with Hannibal's brother Hasdrubal in Hispania, but Rome had embroiled itself in yet another foreign war, the first of its Macedonian wars against Carthage's ally Philip V, at the same time.
Through Hannibal's inability to take strategically important Italian cities, through the general loyalty Italian allies showed to Rome, and through Rome's own inability to counter Hannibal as a master general, Hannibal's campaign continued in Italy inconclusively for sixteen years. Though he managed to sustain his forces for 15 years, Hannibal did so only by ravaging farm-lands, keeping his army healthy, which brought anger among the Romans' subject states. Realizing that Hannibal's army was outrunning its supply lines quickly, Rome took countermeasures against Hannibal's home base in Africa by sea command and stopped the flow of supplies. Hannibal quickly turned back and rushed to home defense, but suffered defeat in the Battle of Zama (202 BC).
Hasdrubal's campaign to reinforce Hannibal.
In Hispania, a young Roman commander, Publius Cornelius Scipio (later to be given the agnomen "Africanus" because of his feats during this war), eventually defeated the larger but divided Carthaginian forces under Hasdrubal and two other Carthaginian generals. Abandoning Hispania, Hasdrubal moved to bring his mercenary army into Italy to reinforce Hannibal but never made it and was defeated by Roman forces near the Alps.
Third Punic War (149–146 BC).
The Third Punic War (149–146 BC) involved an extended siege of Carthage, ending in the city's thorough destruction. The resurgence of the struggle can be explained by growing anti-Roman agitations in Hispania and Greece, and the visible improvement of Carthaginian wealth and martial power in the fifty years since the Second War.
With no military, Carthage suffered raids from its neighbor Numidia. Under the terms of the treaty with Rome, such disputes were arbitrated by the Roman Senate. Because Numidia was a favored client state of Rome, Roman rulings were slanted heavily to favor the Numidians. After some fifty years of this condition, Carthage had managed to discharge its war indemnity to Rome, and considered itself no longer bound by the restrictions of the treaty, although Rome believed otherwise. Carthage mustered an army to repel Numidian forces. It immediately lost the war with Numidia, placing itself in debt yet again, this time to Numidia.
This new-found Punic militarism alarmed many Romans, including Cato the Elder who, after a voyage to Carthage, ended all his speeches, no matter what the topic, by saying: "Ceterum censeo Carthaginem esse delendam" – "And I also think that Carthage must be destroyed".
In 149 BC, in an attempt to draw Carthage into open conflict, Rome made a series of escalating demands, one being the surrender of three hundred children of the nobility as hostages, and finally ending with the near-impossible demand that the city be demolished and rebuilt away from the coast, deeper into Africa. When the Carthaginians refused this last demand, Rome declared the Third Punic War. Having previously relied on mercenaries to fight their wars for them, the Carthaginians were now forced into a more active role in the defense of their city. They made thousands of makeshift weapons in a short time, even using women's hair for catapult strings, and were able to hold off the initial Roman attack. A second offensive under the command of Scipio Aemilianus resulted in a three-year siege before he breached the walls, sacked the city, and systematically burned Carthage to the ground in 146 BC. When the war ended, the remaining 50,000 Carthaginians, a small part of the original pre-war population, were, as was the normal fate in antiquity of inhabitants of sacked cities, sold into slavery by the victors. Carthage was systematically burned for 17 days; the city's walls and buildings were utterly destroyed. The remaining Carthaginian territories were annexed by Rome and reconstituted to become the Roman province of Africa.
After Rome emerged as victorious, significant Carthaginian settlements, such as those in Mauretania and Corinth were taken over and aggrandized by the Romans. Volubilis, for example, was an important Roman town situated near the westernmost border of Roman conquests. It was built on the site of the previous Carthaginian settlement that overlies an earlier neolithic habitation.

</doc>
<doc id="24419" url="https://en.wikipedia.org/wiki?curid=24419" title="Peter Carey (novelist)">
Peter Carey (novelist)

Peter Philip Carey AO (born 7 May 1943) is an Australian novelist. Carey has won the Miles Franklin Award three times and is frequently named as Australia's next contender for the Nobel Prize in Literature. Carey is one of only four writers to have won the Booker Prize twice—the others being J. G. Farrell, J. M. Coetzee and Hilary Mantel. In May 2008 he was nominated for the Best of the Booker Prize.
In addition to writing fiction, he collaborated on the screenplay of the film "Until the End of the World" with Wim Wenders and is executive director of the Master of Fine Arts in Creative Writing program at Hunter College, part of the City University of New York.
Early life and career: 1943–1970.
Peter Carey was born in Bacchus Marsh, Victoria, in 1943. His parents ran a General Motors dealership, Carey Motors. He attended Bacchus Marsh State School from 1948 to 1953, then boarded at Geelong Grammar School between 1954 and 1960. In 1961, Carey enrolled in a science degree at the new Monash University in Melbourne, majoring in chemistry and zoology, but cut his studies short because of a car accident and a lack of interest. It was at university that he met his first wife, Leigh Weetman, who was studying German and philosophy, and who also dropped out.
In 1962, he began to work in advertising. He was employed by various Melbourne agencies between 1962 and 1967, including on campaigns for Volkswagen and Lindeman's Wine. His advertising work brought him into contact with older writers who introduced him to recent European and American fiction: "I didn't really start getting an education until I worked in advertising with people like Barry Oakley and Morris Lurie—and Bruce Petty had an office next door."
During this time, he read widely, particularly the works of Samuel Beckett, William Faulkner, James Joyce, Franz Kafka, and Gabriel García Márquez, and began writing on his own, receiving his first rejection slip in 1964, the same year he married Weetman. Over the next few years he wrote five novels—"Contacts" (1964–1965), "Starts Here, Ends Here" (1965–1967), "The Futility Machine" (1966–1967), "Wog" (1969), and "Adventures on Board the Marie" "Celeste" (1971). None of them was published. Sun Books accepted "The Futility Machine" but did not proceed with publication, and "Adventures on Board the Marie Celeste" was accepted by Outback Press before being withdrawn by Carey himself. These and other unpublished manuscripts from the period—including twenty-one short stories—are now held by the Fryer Library at the University of Queensland.
Carey's only publications during the 1960s were "Contacts" (a short extract from the unpublished novel of the same name, in "Under Twenty-Five: An Anthology", 1966) and "She Wakes" (a short story, in "Australian Letters", 1967). Towards the end of the decade, Carey and Weetman abandoned Australia with "a certain degree of self-hatred", travelling through Europe and Iran before settling in London in 1968, where Carey continued to write highly regarded advertising copy and unpublished fiction.
Middle career: 1970–1990.
Returning to Australia in 1970, Carey once again did advertising work in Melbourne and Sydney. He also kept writing, and gradually broke through with editors, publishing short stories in magazines and newspapers such as "Meanjin" and "Nation Review". Most of these were collected in his first book, "The Fat Man In History", which appeared in 1974. In the same year Carey moved to Balmain in Sydney to work for Grey Advertising. Carey also taught for a while during this period as a "lecteur" in English at Université de Paris IV (Paris-Sorbonne).
In 1976, Carey moved to Queensland and joined an alternative community named Starlight in Yandina, north of Brisbane, with his new partner, the painter Margot Hutcheson, with whom he lived in the 1970s and 1980s. He remained with Grey, writing in Yandina for three weeks, then spending the fourth week at the agency in Sydney. It was during this time that he produced most of the stories collected in "War Crimes" (1979), as well as "Bliss" (1981), his first published novel.
Carey started his own advertising agency in 1980, the Sydney-based McSpedden Carey Advertising Consultants, in partnership with Bani McSpedden. After many years of separation, Leigh Weetman asked for a divorce in 1980 so that she could remarry and Peter agreed. In 1981, he moved to Bellingen in northern New South Wales. There he wrote "Illywhacker", published in 1985. In the same year he married theatre director Alison Summers. "Illusion", a stage musical Carey wrote with Mike Mullins and composer Martin Armiger, was performed at the 1986 Adelaide Festival of the Arts and a studio cast recording of the musical was nominated for a 1987 ARIA Award (for which Carey as lyricist was nominated).
The decade—and the Australian phase of Carey's career—culminated with the publication of "Oscar and Lucinda" (1988), which won the Booker McConnell Prize (as it was then known) and brought the author international recognition. Carey explained that the novel was inspired, in part, by his time in Bellingen:
Move to New York: 1990–present.
Carey sold his share of McSpedden Carey and in 1990 moved with Alison Summers and their son to New York, where he took a job teaching creative writing at New York University. He later said that New York would not have been his first choice of place to live, and that moving there was his wife's idea. Carey and Summers divorced in 2005 after a four-year separation. Carey now lives with the British-born publisher Frances Coady.
"The Tax Inspector" (1991), begun in Australia, was the first book he completed in the United States. It was followed by "The Unusual Life of Tristan Smith" (1994), a fable in which he explored the relationship between Australia and America, disguised in the novel as "Efica" and "Voorstand". This is a relationship that has preoccupied him throughout his career, going back to "Bliss" (1981), "Illywhacker" (1985), and the early short stories. Nevertheless, Carey continued to set his fiction primarily in Australia and remained diffident about writing explicitly on American themes. In a piece on "True History of the Kelly Gang" (2001), Mel Gussow reported that:
It was only after nearly two decades in the United States that he embarked on "Parrot and Olivier in America" (2010), loosely based on events in the life of Alexis de Tocqueville. Carey says “Tocqueville opened a door I could enter. I saw the present in the past. It was accessible, imaginable." Carey continues to extend his canvas; in his most recent novel, "The Chemistry of Tears" (2012), "contemporary London is brought intimately in touch with ... a 19th-century Germany redolent of the Brothers Grimm".
Controversies.
In 1998, Carey was accused of snubbing Queen Elizabeth II by declining an invitation to meet her after winning the Commonwealth Writers Prize for "Jack Maggs" (1997). While Carey is a republican, he insists that no offence was intended:
The meeting did eventually take place, with the Queen remarking, according to Carey, "I believe you had a little trouble getting here."
The unhappy circumstances of Carey's break-up with Alison Summers received wide publicity in 2006 when "" appeared, depicting the toxic relationship between its protagonist, Butcher Bones, and his ex-wife, known only as "the Plaintiff".
In April 2015 he, alongside Michael Ondaatje, Francine Prose, Teju Cole, Rachel Kushner and Taiye Selasi, withdrew from the PEN American Center gala honouring the French satirical magazine Charlie Hebdo with its "Freedom of Expression Courage" award. He stated that one of his reasons for doing so was "PEN’s seeming blindness to the cultural arrogance of the French nation, which does not recognise its moral obligation to a large and disempowered segment of their population."
Awards and distinctions.
Carey has been awarded three honorary degrees. He has been elected a Fellow of the Royal Society of Literature (1989), an Honorary Fellow of the Australian Academy of the Humanities (2001), a Member of the American Academy of Arts and Sciences (2003), and a Member of the American Academy of Arts and Letters (2016), which has also awarded him its Harold D Vursell Memorial Award (2012). In 2010, he appeared on two Australian postage stamps in a series dedicated to "Australian Legends". On 11 June 2012, Carey was named an Officer of the Order of Australia for "distinguished service to literature as a novelist, through international promotion of the Australian identity, as a mentor to emerging writers." And in 2014, Carey was awarded an honorary Doctor of Letters (honoris causa) by Sydney University.
Carey has won numerous literary awards, including:
Bibliography.
Short story collections.
Stories from Carey's first two collections have been repackaged in "The Fat Man in History and Other Stories" (1980), "Exotic Pleasures" (1990), and "Collected Stories" (1994); the last also includes three previously uncollected stories: "Joe" ("Australian New Writing", 1973), "A Million Dollars Worth of Amphetamines" ("Nation Review", 1975), and "Concerning the Greek Tyrant" ("The Tabloid Story Pocket Book", 1978).

</doc>
<doc id="24420" url="https://en.wikipedia.org/wiki?curid=24420" title="Punched card">
Punched card

A punched card or punch card is a piece of stiff paper that contains digital information represented by the presence or absence of holes in predefined positions. The information might be data for data processing applications or, in earlier examples, used to directly controlling automated machinery. The terms IBM card, or Hollerith card specifically refer to punched cards used in semiautomatic data processing. 
Punched cards were widely used through much of the 20th century in what became known as the data processing industry, where specialized and increasingly complex unit record machines, organized into data processing systems, used punched cards for data input, output, and storage. Many early digital computers used punched cards, often prepared using keypunch machines, as the primary medium for input of both computer programs and data.
While now obsolete as a recording medium, as of 2012, some voting machines still use punched cards to record votes.
History.
Semen Korsakov was reputedly the first to use the punched cards in informatics for information store and search. Korsakov announced his new method and machines in September 1832; rather than seeking patents, he offered the machines for public use.
Charles Babbage proposed the use of "Number Cards", "pierced with certain holes and stand opposite levers connected with a set of figure wheels ... advanced they push in those levers opposite to which there are no holes on the card and thus transfer that number" in his description of the Calculating Engine's Store.
Herman Hollerith invented the recording of data on a medium that could then be read by a machine. Prior uses of machine readable media, such as those above (other than Korsakov), had been for control, not data. "After some initial trials with paper tape, he settled on punched cards...", developing punched card data processing technology for the 1890 US census.
Hollerith founded "The Tabulating Machine Company" (1896) which was one of four companies that were consolidated to form Computing-Tabulating-Recording Company (CTR), later renamed the International Business Machines Corporation (IBM). IBM manufactured and marketed a variety of unit record machines for creating, sorting, and tabulating punched cards, even after expanding into electronic computers in the late 1950s. IBM developed punched card technology into a powerful tool for business data-processing and produced an extensive line of general purpose unit record machines. By 1950, the IBM card and IBM unit record machines had become ubiquitous in industry and government. "Do not fold, spindle or mutilate," a generalized version of the warning that appeared on some punched cards (generally on those distributed as paper documents to be later returned for further machine processing, checks for example), became a motto for the post-World War II era. 
From the 1900s, into the 1950s, punched cards were the primary medium for data entry, data storage, and processing in institutional computing. According to the IBM Archives: "By 1937... IBM had 32 presses at work in Endicott, N.Y., printing, cutting and stacking five to 10 million punched cards every day." Punched cards were even used as legal documents, such as U.S. Government checks and savings bonds. The UNITYPER introduced magnetic tape for data entry in the 1950s. During the 1960s, the punched card was gradually replaced as the primary means for data storage by magnetic tape, as better, more capable computers became available. Mohawk Data Sciences introduced a magnetic tape encoder in 1965, a system marketed as a keypunch replacement which was somewhat successful, but punched cards were still commonly used for data entry and programming until the mid-1980s when the combination of lower cost magnetic disk storage, and affordable interactive terminals on less expensive minicomputers made punched cards obsolete for this role as well. However, their influence lives on through many standard conventions and file formats. The terminals that replaced the punched cards, the IBM 3270 for example, displayed 80 columns of text in text mode, for compatibility with existing software. Some programs still operate on the convention of 80 text columns, although fewer and fewer do as newer systems employ graphical user interfaces with variable-width type fonts.
Today punched cards are mostly obsolete and replaced with other storage methods, except for a few legacy systems and specialized applications.
Nomenclature.
The terms "punched card", "punch card", and "punchcard" were all commonly used, as were "IBM card" and "Hollerith card" (after Herman Hollerith). IBM used "IBM card" or, later, "punched card" at first mention in its documentation and thereafter simply "card" or "cards". Specific formats were often indicated by the number of character positions available, e.g. "80-column card". A sequence of cards that is input to or output from some step in an application's processing is called a "card deck" or simply "deck". The rectangular, round, or oval bits of paper punched out were called chad ("chads") or "chips" (in IBM usage). Adjacent card columns allocated for a specific use, such as names, addresses, multi-digit numbers, etc., are known as a "field".
Card formats.
The Hollerith punched cards used for the US 1890 census were blank. Following that, cards commonly had printing such that the row and column position of a hole could be easily seen. Printing could include having fields named and marked by vertical lines, logos, and more. "General purpose" layouts (see, for example, the IBM 5081 below) were also available. Some cards had one upper corner cut so that cards not oriented correctly, or cards with different corner cuts, could be identified.
Hollerith's punched card formats.
Herman Hollerith was awarded a series of patents in 1889 for electromechanical tabulating machines. These patents described both paper tape and rectangular cards as possible recording media. The card shown in of June 8 was printed with a template and had hole positions arranged close to the edges so they could be reached by a railroad conductor's ticket punch, with the center reserved for written descriptions. Hollerith was originally inspired by railroad tickets that let the conductor encode a rough description of the passenger:
Use of the ticket punch proved tiring and error prone, so Hollerith invented a pantograph "keyboard punch". It featured an enlarged diagram of the card, indicating the positions of the holes to be punched. A printed reading board could be placed under a card that was to be read manually.
Hollerith envisioned a number of card sizes. In an article he wrote describing his proposed system for tabulating the 1890 U.S. Census, Hollerith suggested a card 3 inches by 5½ inches of Manila stock "would be sufficient to answer all ordinary purposes." The cards used in the 1890 census had round holes, 12 rows and 24 columns. A reading board for these cards can be seen at the Columbia University Computing History site. At some point, became the standard card size. 
Hollerith's original system used an ad-hoc coding system for each application, with groups of holes assigned specific meanings, e.g. sex or marital status. His tabulating machine had up to 40 counters, each with a dial divided into 100 divisions, with two indicator hands; one which stepped one unit with each counting pulse, the other which advanced one unit every time the other dial made a complete revolution. This arrangement allowed a count up to 10,000. During a given tabulating run, each counter was typically assigned a specific hole. Hollerith also used relay logic to allow counts of combination of holes, e.g. to count married females.
Later designs standardized the coding. These cards had ten rows, each row assigned a digit value, 0 through 9, and 45 columns.
This provided for a field (adjacent columns) to represent multi-digit numbers that tabulators could sum, instead of their simply counting cards. Hollerith's 45 column punched cards are illustrated in Comrie's "The application of the Hollerith Tabulating Machine to Brown's Tables of the Moon".
IBM 80-column punched card formats and character codes.
This IBM card format, designed in 1928,
has rectangular holes, 80 columns with 12 punch locations each, one character to each column. Card size is exactly by inches (187.325 mm × 82.55 mm). The cards are made of smooth stock, thick. There are about 143 cards to the inch (/cm). In 1964, IBM changed from square to round corners. They come typically in boxes of 2000 cards or as continuous form cards. Continuous form cards could be both pre-numbered and pre-punched for document control (checks, for example).
The lower ten positions represented (from top to bottom) the digits 0 through 9. The top two positions of a column were called zone punches, 12 (top) and 11. Originally only numeric information was punched, with 1 punch per column indicating the digit. Signs could be added to a field by overpunching the least significant digit with a zone punch: 12 for plus and 11 for minus. Zone punches had other uses in processing as well, such as indicating a master record.
　Reference: Note: The X and Y zones were also called the 11 and 12 zones, respectively.
In 1931 IBM began introducing multiple punches for upper-case letters and special characters. A letter has two punches (zone + digit [1–9); most special characters have two or three punches (zone none + digit + 8); a few special characters were exceptions (in EBCDIC "&" is 12 only, "-" is 11 only, and "/" is 0 + 1). With these changes, the information represented in a column by a combination of zones [12, 11 and digits is dependent on the use of that column. For example, the combination "12-1" is the letter "A" in an alphabetic column, a plus signed digit "1" in a signed numeric column, or an unsigned digit "1" in a column where the "12" have some other use. The introduction of EBCDIC in 1964 allowed columns with as many as six punches (zones [12,11,0,8,9 + digit [1–7]). IBM and other manufacturers used many different 80-column card character encodings. A 1969 American National Standard defined the punches for 128 characters and was named the "Hollerith Punched Card Code" (often referred to simply as "Hollerith Card Code"), honoring Hollerith.
For some computer applications, binary formats were used, where each hole represented a single binary digit (or "bit"), every column (or row) is treated as a simple bitfield, and every combination of holes is permitted. For example, the IBM 711 card reader used with the 704/709/7090/7094 series scientific computers treated every row as two 36-bit words, ignoring 8 columns. (The specific 72 columns used were selectable using a plugboard control panel, which is almost always wired to select columns 1–72.) Sometimes the ignored columns (usually 73–80) were used to contain a sequence number for each card, so the card deck could be sorted to the correct order in case it was dropped. An alternative format, used by the IBM 704's IBM 714 native card reader, is referred to as Column Binary or Chinese Binary, and used 3 columns for each 36-bit word. Later computers, such as the IBM 1130 or System/360, used every column. The IBM 1401's card reader could be used in Column Binary mode, which stored two characters in every column, or one 36-bit word in three columns when used as input device for other computers. However, most of the older card punches were not intended to punch more than 3 holes in a column, so they could not be used to produce binary cards.
As a prank, in binary mode, cards could be punched where every possible punch position had a hole. Such "lace cards" lacked structural strength, and would frequently buckle and jam inside the machine.
The 80-column card format dominated the industry, becoming known as just IBM cards, even though other companies made cards and equipment to process them.
One of the most common printed punched cards is the IBM 5081 card format, a general purpose layout with no field divisions. This format has digits printed on it corresponding to the punch positions of the digits in each of the 80 columns. Other card vendors manufactured cards with this same layout and number.
IBM "Stub cards" or "Short cards".
The 80-column card could be scored, on either end, creating a stub that could be torn off, leaving a "stub card" or "short card". A common length for stub cards was 51-columns. Stub cards were used in applications requiring tags, labels, or carbon copies.
IBM Port-A-Punch.
According to the IBM Archive: "IBM's Supplies Division introduced the Port-A-Punch in 1958 as a fast, accurate means of manually punching holes in specially scored IBM punched cards. Designed to fit in the pocket, Port-A-Punch made it possible to create punched card documents anywhere. The product was intended for "on-the-spot" recording operations—such as physical inventories, job tickets and statistical surveys—because it eliminated the need for preliminary writing or typing of source documents.".
IBM 96-column punched card format.
In the early 1970s, IBM introduced a new, smaller, round-hole, 96-column card format along with the IBM System/3 computer. These cards have tiny (1 mm), circular holes, smaller than those in paper tape. Data is stored in six-bit binary-coded decimal code, with three rows of 32 characters each, or 8-bit EBCDIC. In this format, each column of the top tiers are combined with two punch rows from the bottom tier to form an 8-bit byte, and the middle tier is combined with two more punch rows, so that each card contains 64 bytes of 8-bit-per-byte binary coded data.
Powers/Remington Rand UNIVAC card formats.
The Powers/Remington Rand card format was initially the same as Hollerith's; 45 columns and round holes. In 1930, Remington Rand leap-frogged IBM's 80 column format from 1928 by coding two characters in each of the 45 columns – producing what is now commonly called the 90-column card. There are two sets of six rows across each card. The rows in each set are labeled 0, 1/2, 3/4, 5/6, 7/8 and 9. The even numbers in a pair are formed by combining that punch with a 9 punch. Alphabetic and special characters use 3 or more punches
IBM punched card manufacturing.
IBM's Fred M. Carroll developed a series of rotary type presses that were used to produce the well-known standard tabulating cards, including a 1921 model that operated at 400 cards per minute (cpm). Later, he developed a completely different press capable of operating at speeds in excess of 800 cpm, and it was introduced in 1936. Carroll's high-speed press, containing a printing cylinder, revolutionized the manufacture of punched tabulating cards. It is estimated that between 1930 and 1950, the Carroll press accounted for as much as 25 percent of the company's profits
Discarded printing plates from these card presses, each printing plate the size of an IBM card and formed into a cylinder, often found use as desk pen/pencil holders, and even today are collectible IBM artifacts (every card layout had its own printing plate).
IBM initially required that its customers use only IBM manufactured cards with IBM machines, which were leased, not sold. IBM viewed its business as providing a service and that the cards were part of the machine. In 1932, the US government took IBM to court on this issue. IBM fought all the way to the Supreme Court and lost in 1936; the court ruling that IBM could only set card specifications. In another case, heard in 1955, IBM signed a consent decree requiring, amongst other things, that IBM would by 1962 have no more than one-half of the punched card manufacturing capacity in the United States. Tom Watson Jr.'s decision to sign this decree, where IBM saw the punched card provisions as the most significant point, completed the transfer of power to him from Thomas Watson, Sr.
Cultural impact.
While punched cards have not been widely used for a generation, the impact was so great for most of the 20th century that they still appear from time to time in popular culture. For example:
metaphor... symbol of the "system"—first the registration system and then bureaucratic systems more generally ... a symbol of alienation ... Punched cards were the symbol of information machines, and so they became the symbolic point of attack. Punched cards, used for class registration, were first and foremost a symbol of uniformity. ... A student might feel "he is one of out of 27,500 IBM cards" ... The president of the Undergraduate Association criticized the University as "a machine ... IBM pattern of education."... Robert Blaumer explicated the symbolism: he referred to the "sense of impersonality... symbolized by the IBM technology."...
––Steven Lubar
In Arthur C. Clarke's early short story "Rescue Party", the alien explorers find a "... wonderful battery of almost human Hollerith analyzers and the five thousand million punched cards holding all that could be recorded on each man, woman and child on the planet". Writing in 1946, Clarke, like almost all sci-fi authors, had not then foreseen the development and eventual ubiquity of the computer.
Card handling equipment.
Creation and processing of punched cards was handled by a variety of devices, including:

</doc>
<doc id="24421" url="https://en.wikipedia.org/wiki?curid=24421" title="Profiler">
Profiler

Profiler may refer to:

</doc>
<doc id="24422" url="https://en.wikipedia.org/wiki?curid=24422" title="Pasteur (disambiguation)">
Pasteur (disambiguation)

Pasteur may refer to:

</doc>
<doc id="24423" url="https://en.wikipedia.org/wiki?curid=24423" title="Pope Innocent I">
Pope Innocent I

Pope Innocent I (; died 12 March 417) served as the Catholic Church Pope from 401 to his death in 417.
Biography.
According to his biographer in the "Liber Pontificalis", Innocent was a native of Albano Laziale and the son of a man called Innocentius, but his contemporary Jerome referred to him as the son of the previous pope, Anastasius I, probably a unique case of a son succeeding his father in the papacy. According to Urbano Cerri, Pope Innocent was native of Albania.
Innocent I lost no opportunity in maintaining and extending the authority of the Roman apostolic See, which was seen as the ultimate resort for the settlement of all ecclesiastical disputes. His communications with Victricius of Rouen, Exuperius of Toulouse, Alexander of Antioch and others, as well as his actions on the appeal made to him by John Chrysostom against Theophilus of Alexandria, show that opportunities of this kind were numerous and varied. He took a decided view on the Pelagian controversy, confirming the decisions of the synod of the province of proconsular Africa, held in Carthage in 416, which had been sent to him, and also writing in the same year in a similar sense to the fathers of the Numidian synod of Mileve who had addressed him (Augustine of Hippo among them). In addition he acted as metropolitan over the bishops of Italia Suburbicaria.
The historian Zosimus in his "Historia Nova" suggests that during the sack of Rome in 410 by Alaric I, Innocent I was willing to permit private pagan practices as a temporary measure. However, Zosimus also suggests that this attempt by pagans to restore public worship failed due to lack of public interest, suggesting that Rome had been successfully Christianized in the last century.
Among Innocent I's letters is one to Jerome and another to John II, Bishop of Jerusalem, regarding annoyances to which the former had been subjected by the Pelagians at Bethlehem.
He died on 12 March 417. Accordingly, his feast day is now celebrated on 12 March, though from the thirteenth to the twentieth century he was commemorated on 28 July. His successor was Zosimus.
Role in establishing Bible Canon.
It is accepted that the canon of the Bible was closed c. 405 AD by Pope Innocent, when he sent a list of the sacred books to a Gallic bishop, Exsuperius of Toulouse, identical with that of Trent, except for some uncertainty in the manuscript tradition about whether the letters ascribed to Paul were 14 or only 13, in the latter case possibly implying omission of the Letter to the Hebrews.
Relics.
In 846, Pope Sergius II gave approval for the relics of St. Innocent to be moved by Duke Liudolf of Saxony, along with those of his father and predecessor Anastasius, to the crypt of the former collegiate church of Gandersheim, now Gandersheim Abbey, where they rest until this day.

</doc>
<doc id="24425" url="https://en.wikipedia.org/wiki?curid=24425" title="Philippi">
Philippi

Philippi (; , "Philippoi") was a city in eastern Macedonia, established by Philip II of Macedon in 356 BC and abandoned in the 14th century after the Ottoman conquest. The present municipality, Filippoi, is located near the ruins of the ancient city and is part of the region of East Macedonia and Thrace in Kavalla Greece.
History.
Philippi was established by the king of Macedon, Philip II, on the site of the Thasian colony of Krinides or Crenides (, "Fountains"). He sited it near the head of the Aegean Sea and at the foot of Mt. Orbelos, now called Mt. Lekani, about north-west of Kavalla, on the northern border of the marsh that, in antiquity covered, the entire plain separating it from the Pangaion hills to the south of Greece.
The objective of founding the town was to take control of the neighbouring gold mines and to establish a garrison at a strategic passage: the site controlled the route between Amphipolis and Neapolis, part of the great royal route which crosses Macedonia from the east to the west and which was reconstructed later by the Roman Empire as the "Via Egnatia". Philip II endowed the new city with important fortifications, which partially blocked the passage between the swamp and Mt. Orbelos, and sent colonists to occupy it. Philip also had the marsh partially drained, as is attested by the writer Theophrastus. Philippi preserved its autonomy within the kingdom of Macedon and had its own political institutions (the "Assembly" of the "demos"). The discovery of new gold mines near the city, at Asyla, contributed to the wealth of the kingdom and Philip established a mint there. The city was fully integrated into the kingdom under Philip V.
The city contained 2,000 people.
When the Romans destroyed the Antigonid dynasty of Macedon in 168 BC, they divided the city-state into four separate states ("merides"). It was Amphipolis and not Philippi that became the capital of the eastern Macedonian state.
Almost nothing is known about the city in this period, aside from the walls, the Greek theatre, the foundations of a house under the Roman forum and a little temple dedicated to a hero cult. This monument covers the tomb of a certain Exekestos, is possibly situated on the agora and is dedicated to the κτίστης ("ktistès"), the foundation hero of the city.
The Roman era.
The city reappears in the sources during the Roman civil war that followed the assassination of Julius Caesar. His heirs Mark Antony and Octavian confronted the assassins of Caesar, Marcus Junius Brutus and Gaius Cassius Longinus, at the Battle of Philippi on the plain to the west of the city during October in 42 BC. Antony and Octavian were victorious in this final battle against the partisans of the Republic. They released some of their veteran soldiers, probably from Legion XXVIII and colonized them in the city, which was refounded as "Colonia Victrix Philippensium". In 30 BC, Octavian became Roman emperor, reorganized the colony, and established more settlers there, veterans possibly from the Praetorian Guard and other Italians. The city was renamed "Colonia Iulia Philippensis", and then "Colonia Augusta Iulia Philippensis" after January, 27 BC, when Octavian received the title Augustus from the Roman Senate.
Following this second renaming, and perhaps after the first, the territory of Philippi was centuriated (divided into squares of land) and distributed to the colonists. The city kept its Macedonian walls, and its general plan was modified only partially by the construction of a forum, a little to the east of the site of Greek agora. It was a "miniature Rome," under the municipal law of Rome and governed by two military officers, the "duumviri", who were appointed directly from Rome.
The colony recognized its dependence on the mines that brought it its privileged position on the "Via Egnatia". This wealth was shown by the many monuments that were particularly imposing considering the relatively small size of the urban area: the forum, laid out in two terraces on both sides of the main road, was constructed in several phases between the reigns of Claudius and Antoninus Pius, and the theatre was enlarged and expanded in order to hold Roman games. There is an abundance of Latin inscriptions testifying to the prosperity of the city.
The early Christian era.
According to the New Testament, in AD 49 or 50, the city was visited by the apostle Paul (). From the Acts of the Apostles () and the letter to the Philippians (), early Christians concluded that Paul had founded their community. Accompanied by Silas, Timothy and possibly Luke, the author of the Acts of the Apostles, Paul is believed to have preached for the first time on European soil in Philippi (). According to the New Testament, Paul visited the city on two other occasions, in 56 and 57. The Epistle to the Philippians dates from around 61-62 and is believed to show the immediate effects of Paul's instruction.
The development of Christianity in Philippi is indicated by a letter from Polycarp of Smyrna addressed to the community in Philippi around AD 160 and by funerary inscriptions.
The first church described in the city is a small building that was probably originally a small prayer house. This "Basilica of Paul", identified by a mosaic inscription on the pavement, is dated around 343 from a mention by the bishop Porphyrios, who was present at the Council of Serdica that year.
The prosperity of the city in the 5th and 6th centuries was attributed to Paul and to his ministry. As in other cities, many new ecclesiastical buildings were constructed at this time. Seven different churches were constructed in Philippi between the mid-4th century and the end of the 6th, some of which competed in size and decoration with the most beautiful buildings in Thessalonica, or those of Constantinople. The relationship of the plan and of the architectural decoration of Basilica B with Hagia Sophia and Saint Irene in Constantinople accorded a privileged place to this church in the history of early Christian art. The complex cathedral which took the place of the Basilica of Paul at the end of the 5th century, constructed around an octagonal church, also rivaled the churches of Constantinople.
In the same age, the fortifications of the city were rebuilt in order to better defend against the growing instability in the Balkans. In 473, the city was besieged by the Ostrogoths, who were unable to take it but burned down the surrounding villages.
The Byzantine and Ottoman era.
Already weakened by the Slavic invasions at the end of the 6th century, which ruined the agrarian economy of Macedonia and probably also by the Plague of Justinian in 547, the city was almost totally destroyed by an earthquake around 619, from which it never recovered. There was a small amount of activity there in the 7th century, but the city was now hardly more than a village.
The Byzantine Empire possibly maintained a garrison there, but in 838 the city was taken by the Bulgars under "kavhan" Isbul, who celebrated their victory with a monumental inscription on the stylobate in Basilica B, now partially in ruins. The site of Philippi was so strategically sound that the Byzantines attempted very soon to recapture it around 850. Several seals of civil servants and other Byzantine officials, dated to the first half of the 9th century, prove the presence of Byzantine armies in the city.
Around 969, Emperor Nicephorus II Phocas rebuilt the fortifications on the acropolis and in part of the city. These gradually helped weaken Bulgar power and strengthen the Byzantine presence in the area. In 1077, Bishop Basil Kartzimopoulos rebuilt part of the defenses inside the city. The city began to prosper once more, as witnessed by the Arab geographer Al Idrisi, who mentions it as a centre of business and wine production around 1150.
After a brief occupation by the Franks after the Fourth Crusade and the capture of Constantinople in 1204, the city was captured by the Serbs. Still, it remained a notable fortification on the route of the ancient "Via Egnatia"; in 1354, the pretender to the Byzantine throne, Matthew Cantacuzenus, was captured there by the Serbs.
The city was abandoned at an unknown date, but when the French traveller Pierre Belon visited it in the 16th century, there were nothing but ruins, used by the Turks as a quarry. The name of the city was preserved at first by a Turkish village on the nearby plain, Philibedjik (Filibecik, "Little Filibe" in Turkish), which has since disappeared and then by a Greek village in the mountains.
Archaeological excavation of the site.
Noted or briefly described by 16th century travellers, the first archaeological description of the city was made in 1856 by Perrot, then in 1861 by Léon Heuzey and Henri Daumet in their famous "Mission archéologique de Macédoine". The first excavations did not begin until the summer of 1914, and were soon interrupted by the First World War. The excavations, carried out by the École française d'Athènes, were renewed in 1920 and continued until 1937. During this time the Greek theatre, the forum, Basilicas A and B, the baths and the walls were excavated. After the Second World War, Greek archaeologists returned to the site. From 1958 to 1978, the Société Archéologique, then the Service archéologique and the University of Thessalonica uncovered the bishop's quarter and the octagonal church, large private residences, a new basilica near the Museum and two others in the necropolis to the east of the city.
References.
Translated from the French Wikipedia article, retrieved February 11, 2005. That article, in turn, gives the following references:

</doc>
<doc id="24427" url="https://en.wikipedia.org/wiki?curid=24427" title="Victoria, Crown Princess of Sweden">
Victoria, Crown Princess of Sweden

Victoria, Crown Princess of Sweden, Duchess of Västergötland (; Victoria Ingrid Alice Désirée; born 14 July 1977), is the eldest child and heir apparent of King Carl XVI Gustaf. If she ascends to the throne as expected, she will be Sweden's fourth queen regnant (after Margaret, Christina and Ulrika Eleonora) and the first since 1720.
Early life.
Victoria was born on 14 July 1977 at 21:45 CET at the Karolinska University Hospital in Solna, Stockholm County, Sweden, and is the oldest child of King Carl XVI Gustaf and Queen Silvia. She is a member of the Royal House of Bernadotte. Born as a Princess of Sweden, she was designated Crown Princess in 1979 (SFS 1979:932) ahead of her younger brother. Her place as first in the line of succession formally went into effect on 1 January 1980 with the parliamentary change to the Act of Succession that introduced absolute primogeniture.
Her given names honour various relatives. Her first name comes primarily from her great-great-grandmother, Victoria of Baden, the queen-consort of Sweden as wife of King Gustaf V, and her great-great-great-grandmother Victoria, queen of the United Kingdom (the Queen's granddaughter, Margaret of Connaught, Crown Princess of Sweden, was Victoria's great-grandmother). Her other names honour her great-aunt Ingrid of Denmark; her maternal grandmother, the Brazilian Alice Sommerlath (born Alice Soares de Toledo); and her ancestor Désirée Clary, the queen-consort of Charles XIV John and a former fiancée of Napoleon I of France as well as her paternal aunt and godmother, Princess Désirée.
She was christened at The Royal Palace Church on 27 September 1977. Her godparents are King Harald V of Norway, her maternal uncle, Ralf Sommerlath, Princess Beatrix of the Netherlands, and her aunt Princess Désirée, Baroness Silfverschiöld. The Crown Princess was confirmed in the summer of 1992 at Räpplinge church on the island of Öland.
Education.
Victoria studied for a year (1996/97) at the Université Catholique de l'Ouest at Angers in France, and in the fall term of 1997 participated in a special program following the work of the "Riksdag". From 1998 to 2000, Victoria resided in the United States, where she studied various subjects at Yale University, New Haven, Connecticut.
In May 1999, she was an intern at the Swedish Embassy in Washington, D.C. Victoria completed a study program at the Government Offices in 2001. In 2003, Victoria's education continued with visits to Swedish businesses, a study and intern program in agriculture and forestry, as well as completion of the basic soldier training at SWEDINT (the Swedish Armed Forces International Centre).
In 2006, Victoria enrolled in the Ministry for Foreign Affairs' Diplomat Program, running from September 2006 to June 2007. The program is a training program for young future diplomats and gives an insight to the ministry's work, Swedish foreign and security policies and Sweden's relations with the rest of the world. In June 2009, she graduated with a Bachelor of Arts degree from Uppsala University.
She speaks Swedish, English, French and German.
Change in status.
She was made Crown Princess and heir apparent on 1 January 1980 by the 1979 change to the Act of Succession of 1810 ("Successionsordningen"). This constitutional reform meant that the throne would be inherited by the monarch's eldest child without regard to gender. King Carl XVI Gustaf objected to the reform after it occurred because he favoured tradition.
When she became heir, she also was made titular Duchess of Västergötland, one of the historical provinces of Sweden.
Prior to this constitutional change, the heir apparent to the throne was her younger brother, the then-Crown Prince Carl Philip, Duke of Värmland. He is now fourth in line to the throne, behind the Crown Princess's daughter and son.
She is one of only three female heirs apparent in the worldthe other two being her goddaughter Catharina-Amalia, Princess of Orange, and Princess Elisabeth of Belgium. 
Declaration of majority.
Victoria's declaration of majority took place in the Hall of State at the Royal Palace of Stockholm on 14 July 1995. As of the day she turned 18, she became eligible to act as Head of State when the King is not in country. Victoria made her first public speech on this occasion. Located on the dais in the background was the same silver throne on which her father was seated at his enthronement, in actual use from 1650 and up until this ceremony.
Royal duties.
As heir apparent to the throne, Victoria is a working member of the Swedish Royal Family with her own agenda of official engagements. Victoria attends the regular Advisory Council on Foreign Affairs and the information councils with Government ministers headed by the King, and steps in as a temporary regent (Riksföreståndare) when needed. Victoria also takes part in the regular official dinners hosted by the King and Queen, state visits to Sweden, high level and official visits from foreign dignitaries, the opening of the "Riksdag" (Parliament), celebrations of the Swedish National Day and the annual Nobel Prize festivities.
Victoria has made many official trips abroad as a representative of Sweden. Her first major official visit on her own was to Japan in 2001, where she promoted Swedish tourism, design, music, gastronomy and environmental sustainability during the "Swedish Style" event. That same year, Victoria also travelled to the West Coast of the United States, where she participated in the celebrations of the Nobel centenary.
In 2002, she paid official visits to United States, Spain, Uganda, Ethiopia, and Kosovo where she visited Camp Victoria. In 2003, she made official visits to Egypt and the United States. In early 2004, she paid an official visit to Saudi Arabia, as a part of a large official business delegation from Sweden, and in October 2004, she travelled to Hungary.
In January 2005, Victoria made a long official visit to Australia, promoting Swedish Style and businesses, and in April she visited Bangladesh and Sri Lanka to follow aid work and become informed about the work in the aftermath of the tsunami. In April 2005, Victoria made an official visit to Japan where she visited the Expo 2005 in Aichi, laid the foundation for a new IKEA store in Yokohama together with Princess Takamado and met with Emperor Akihito, Empress Michiko, Crown Prince Naruhito and Sayako Kuroda. In June 2005, Victoria travelled to Turkey on an official visit where she participated in the Swedish Business Seminar and Sweden Day celebrations in Ankara during a historic visit, which was organised by the Swedish Embassy in Ankara and Swedish Trade Council in Istanbul. Victoria also visited the historic sights such as the Blue Mosque, Topkapı Palace and Hagia Sophia. This was the first official Royal visit from Sweden to Turkey since 1934. In September 2005, she made an official visit to China.
In March 2006, Victoria made an official visit to Brazil where she followed the Volvo Ocean Race and visited projects supported by the World Childhood Foundation, such as the Abrigo Rainha Sílvia. In December, she paid a four-day official visit to Paris where she attended a French-Swedish soirée arranged by the Swedish Chamber of Commerce, the Swedish Trade Council and the Swedish Embassy, during which she also awarded the Prix d’Excellence 2006. The visit to Paris also included events with the Swedish Club in Paris, attendance at a church service in the Sofia Church (the Swedish church in Paris), a study visit to the OECD headquarters and meetings with the Secretary-General José Ángel Gurría, the Swedish Ambassador to the OECD, Gun-Britt Andersson, and other senior officials. She also attended a gala dinner hosted by La Fondation Pour L’Enfance at Versailles.
State visits, in which she has participated in Sweden are Austria 1997, South Africa 1999, France 2000, Germany 2003, Jordan 2003, Latvia 2005, Malaysia 2005, Republic of Botswana 2006, China 2007, Brazil 2007, Bulgaria 2007, Serbia 2008 ; abroad Finland 1996 (her first), Belgium 2001, Finland 2003, Iceland 2004, Denmark 2007.
She is a member of the Honorary Board of the International Paralympic Committee.
In 2011, it was announced that Victoria would continue working throughout her pregnancy. In 2012, she took her maternity leave one day prior to the birth of her daughter Estelle and her husband Daniel revealed that he would take his paternity leave and switch parental roles with Victoria when Estelle began preschool.
The Crown Princess's household.
Crown Princess Victoria was given her own household in October 2004. The Crown Princess's household is headed by the Marshal of the Court, and serves to coordinate the official engagements of The Crown Princess.
The Crown Princess Victoria Fund.
The Crown Princess Victorias Fund was set up in 1997 and is run as a part of Radiohjälpen, the fundraising branch of Sveriges Television and Sveriges Radio. The fund’s aim is to provide support for leisure and recreational activities for children and young people with functional disabilities or chronic illnesses. Applications can be addressed to the fund year round and the use of grants can cover everything from compensations to assistants at recreational trips to leisure activities such as horseback riding, skiing, wheelchair floorball, camps and outings.
Every summer, Sveriges Television carries out fundraising drives for the fund via messages on television, these are especially concentrated around the Swedish national holiday on 6 June and the Crown Princess's birthday, Victoriadagen, on 14 July. On the Crown Princess's birthday, when a long televised entertainment program is aired from Borgholm where the people and the Royal Family celebrate Victoria, the public is also able to call in and donate money at the same time as they compete for prizes.
The Crown Princess Victoria Fund’s means mainly derive from donations by the public, but large companies such as Arla Foods, Swedbank and AB Svenska Returpack are constant sponsor partners. Additional support comes from The Association of Swedish Bakers & Confectioners who every year arrange a national “princess cake week” during which the participating cafés and bakeries give 2,50 SEK per sold princess pastry and 10 SEK per sold princess cake to the fund. The result of this fund-raising drive is usually presented to Victoria herself on her name day on 12 March every year; in 2007, the total amount was 200,000 SEK. Congratulatory and memorial cards are also issued by Radiohjälpen benefitting the fund, a simple way to pay respects and do a good deed in one act. In 2006, The Crown Princess Victoria Fund raised a total of 5,5 million SEK.
Every year Victoria visits one or several clubs or projects that have been granted money. These visits are not announced via the official royal diary but kept private, instead Sveriges Television often accompanies her and airs short programs from these visits at some time during the year.
Personal life.
Victoria’s first boyfriend was Daniel Collert. They socialized in the same circles, went to the same school and were already friends when their romance developed in the mid-1990s. When Victoria moved to the United States in 1998 to study and recover from her eating disorders, Collert moved with her across the Atlantic and settled in New York. In September 2000, Victoria's relationship with Collert was confirmed in an interview with her at Expo 2000, and later by then-Director of the Press and Information Department at the Royal Court Elisabeth Tarras-Wahlberg. They broke up in 2001.
In May 2002, Swedish newspaper "Expressen" reported that Victoria had a new boyfriend, her personal trainer at Master Training, Daniel Westling. When the news broke and the media turned its attention on him, it was obvious that he did not like being in the public eye. Once Westling was photographed crossing a street against a red light in order to avoid a camera. In July 2002, Victoria and Daniel Westling were pictured kissing for the first time at a birthday party for Caroline Kreuger, a close friend of Victoria's.
In a popular personal report called "Tre dagar med Victoria", which profiled her work during a three-day period that aired on TV4 in December 2004, Victoria commented on criticism directed at Westling, “Many unfair things are written. I understand that there is speculation, but some day justice will be done there, too.” Victoria also gave her opinion that happiness is important, and that these days it is not so much about background and pedigree but about two people who have to live with each other. She said that if they are not happy and comfortable with each other, it is impossible to do a good job.
During her April 2005 visit to Expo 2005 in Nagakute, Victoria was interviewed by Mikio Yikuma of the Japanese newspaper "Yomiuri Shinbun". Yikuma brought up the subject of royals marrying commoners, to which the princess responded, "I think the general idea with the Swedes is that the modern way is to marry someone you love, not necessarily based on where she or he comes from." Though she did not mention Westling by name, Victoria did admit, "There is someone in my life", but that marriage was not on her mind then. The interview was conducted at the Swedish embassy in Tokyo and published in the paper on 18 April 2005.
Engagement.
Swedish media often speculated about upcoming engagements and marriages for Victoria. On 24 February 2009, rumours that wedding plans were imminent became particularly intense preceding an information council between the King and Prime Minister Fredrik Reinfeldt. Under the terms of the Swedish Act of Succession, the Government, upon the request of the King, gives the final consent for a dynastic marriage of a Prince or Princess of Sweden. The prince or princess otherwise loses their right to the throne. Later that day, it was confirmed that permission had been granted and that Victoria would marry Daniel Westling in the summer of 2010. The wedding date was set in Stockholm Cathedral for 19 June 2010, the 34th anniversary of her parents' marriage. Her engagement ring features a solitaire round brilliant-cut diamond mounted on white gold.
Wedding.
The wedding took place on 19 June 2010. More than 1200 guests including royalty and ambassadors from various countries were invited to the wedding ceremony which took place at Stockholm Cathedral. After the wedding the newlyweds were driven through Stockholm in a coach and then rowed in the antique royal barge "Vasaorden" to the royal palace where the wedding banquet was held. On the evening before the wedding, there was a gala concert dedicated to the couple in the Stockholm Concert Hall.
More than half a million Swedes waved with Swedish flags and cheered the couple from in their cortege, from the church to the castle. The popularity of the monarchy exploded after the wedding, and a SIFO showed that more than 70% of the Swedes supported the monarchy and only 16% wanted to abandon it. Following their wedding the Crown Princess and Prince moved into Haga Palace. Prior to the wedding, the Crown Princess resided at Drottningholm Palace.
Motherhood and Children.
On 17 August 2011, the Swedish royal court announced that Crown Princess Victoria was pregnant and expecting the couple's first child in March 2012. On 23 February 2012 at 04:26 CET, Victoria gave birth to Princess Estelle, Duchess of Östergötland, in the Karolinska University Hospital. Princess Estelle is second-in-line to the Swedish throne.
On 4 September 2015, the royal court announced that Crown Princess Victoria was expecting her second child in March 2016. On 2 March 2016, she gave birth to a son, Prince Oscar, Duke of Skåne.
Godchildren.
The Crown Princess is godmother to:
Health.
In 1996, it was established that Victoria suffered from anorexia, it was however not confirmed until the next year. Already at that time she was getting professional help, but given her public position in Sweden it was getting increasingly difficult to handle the situation. Victoria had planned to study at Uppsala University, but after intense media speculation and public discussion when pictures of an evidently emaciated Victoria in sleeveless dresses at the Order of the Innocence’s ball and the gala dinner for the incoming state visit from Austria surfaced in April 1997, the Royal Court decided to confirm what was feared.
After a press release from the Royal Court in November 1997 announced that Victoria had eating disorders, plans changed for her and she moved to the United States where she received professional help and studied at Yale University. By making this drastic decision, Victoria lived an anonymous life while getting professional help and recovering without having to worry about media speculations or if people were recognizing her on the streets.
In an interview with Björn Carlgren for SVT2 in June 1999, Victoria said, "It was a really hard time. This kind of illness is hard, not only for the individual but also for the people close to him or her. Today I'm fine."
In November 2002, the book “Victoria, Victoria!” came out, speaking further about her eating disorder. Victoria said: “I felt like an accelerating train, going right down... during the whole period. I had eating disorders and was aware of it, my anguish was enormous. I really hated how I looked like, how I was... I, Victoria, didn’t exist. It felt like everything in my life and around me was controlled by others. The one thing I could control was the food I put in me”. She further said that “What happened cost and I was the one who stood for the payments. Now I’m feeling well and with the insights I’ve acquired through this I can hopefully help someone else”.
Princess Victoria made her first public comment about her anorexia at a conference on bullying held at the University of Örebro. In 2008, she also spoke about her face blindness.
Titles, styles and honours.
Honours.
Swedish honours.
"See also List of honours of the Swedish Royal Family by country"

</doc>
<doc id="24428" url="https://en.wikipedia.org/wiki?curid=24428" title="Pope Innocent II">
Pope Innocent II

Pope Innocent II (; died 23 September 1143), born Gregorio Papareschi, was Pope from 14 February 1130 to his death in 1143. His election was controversial and the first eight years of his reign were marked by a struggle for recognition against the supporters of Antipope Anacletus II. He reached an understanding with Lothair II, Holy Roman Emperor who supported him against Anacletus and whom he crowned King of the Romans. Innocent went on to preside over the Second Lateran council.
Early years.
Papareschi came from a Roman family, probably of the "rione" Trastevere. He was probably one of the clergy in personal attendance on the Antipope Clement III (Guibert of Ravenna).
Pope Paschal II made him a cardinal deacon. In this capacity, he accompanied Pope Gelasius II when he was driven into France. He was selected by Pope Callixtus II for various important and difficult missions, such as the one to Worms for the conclusion of the Concordat of Worms, the peace accord made with Holy Roman Emperor Henry V in 1122, and also the one to France in 1123 that made peace with King Louis VI.
Election as Pope.
In 1130, as Pope Honorius II lay dying, the cardinals decided to entrust the election to a commission of eight men led by papal chancellor Haimeric, who had his candidate Cardinal Gregory Papareschi hastily elected as Pope Innocent II. He was consecrated on 14 February, the day after Honorius' death. The other cardinals announced that Innocent had not been canonically elected and chose Cardinal Pietro Pierleoni, a Roman whose family were the enemy of Haimeric's supporters, the Frangipani; Pierleoni took the name Pope Anacletus II. Anacletus' mixed group of supporters were powerful enough to take control of Rome while Innocent was forced to flee north. Based on a simple majority of the entire college of cardinals, Anacletus was the canonically elected pope, and Innocent was the anti-Pope. However, the legislation of Pope Nicholas II pre-empted the choice of the majority of the cardinal priests and cardinal deacons. This rule was changed by the Second Lateran council of 1139.
Papacy.
Anacletus had control of Rome, so Innocent II took ship for Pisa, and thence sailed by way of Genoa to France, where the influence of Bernard of Clairvaux readily secured his cordial recognition by the clergy and the court. In October of the same year he was duly acknowledged by Holy Roman Emperor Lothair III and his bishops at the synod of Würzburg. In January 1131, he had also a favourable interview with Henry I of England, and in August 1132 Lothar III undertook an expedition to Italy for the double purpose of setting aside Anacletus as antipope and of being crowned by Innocent. Anacletus and his supporters being in secure control of St. Peter's Basilica, the coronation ultimately took place in the Lateran Church (4 June 1133), but otherwise the expedition proved abortive. At the investiture of Lothair as Emperor he gained the territories belonging to Matilda of Tuscany in return for an annuity to be paid to the pope, in consequence of which the curial party based the contention that the Emperor was a vassal of the Papal see.
A second expedition by Lothar III in 1136 was not more decisive in its results, and the protracted struggle between the rival pontiffs was terminated only by the death of Anacletus II on 25 January 1138.
Innocent took as cardinal-nephew first his nephew, Gregorio Papareschi, whom he elevated to cardinal in 1134, and then his brother Pietro Papareschi, whom he elevated to cardinal in 1142. Another nephew, Cinzio Papareschi (died 1182), was also a cardinal, raised to the cardinalate in 1158, after Innocent's death.
Second Lateran Council.
By the Second Lateran council of 1139, at which King Roger II of Sicily, Innocent II's most uncompromising foe, was excommunicated, peace was at last restored to the Church. Aside from the complete rebuilding of the ancient church of Santa Maria in Trastevere, which boldly features Ionic capitals from former colonnades in the Baths of Caracalla and other richly detailed "spolia" from Roman monuments, the remaining years of this Pope's life were almost as barren of permanent political results as the first had been. His efforts to undo the mischief wrought in Rome by the long schism were almost entirely neutralized by a quarrel with his erstwhile supporter, Louis VII of France over the candidate for archbishop of Bourges, in the course of which that kingdom was laid under an interdict to press for the papal candidate, and by a struggle with the town of Tivoli in which he became involved. As a result, Roman factions that wished Tivoli annihilated took up arms against Innocent.
It was also in 1139 that, in the "Omne Datum Optimum", Innocent II declared that the Knights Templar -- a religious and military organization then twenty-one years old -- should in the future be answerable only to the papacy. An unfortunate consequence of this ruling was the growing arrogance of the Templars, which brought about their violent suppression in October of 1307.
Treaty of Mignano.
On 22 July 1139, at Galluccio, Roger II's son Roger III, Duke of Apulia, ambushed the papal troops with a thousand knights and captured Innocent. On 25 July 1139, Innocent was forced to acknowledge the kingship and possessions of Roger with the Treaty of Mignano. In 1143, Innocent refused to recognise the Treaty of Mignano with Roger of Sicily, who sent Robert of Selby to march on papal Benevento. The terms agreed upon at Mignano were then recognised. Innocent II died on 24 September 1143 and was succeeded by Pope Celestine II.
The doctrinal questions which he was called on to decide were those that condemned the opinions of Pierre Abélard and of Arnold of Brescia.
In 1143, as the Pope lay dying, the Commune of Rome, to resist papal power, began deliberations that officially reinstated the Roman Senate the following year. The Pope was interred in a porphyry sarcophagus that contemporary tradition asserted had been the Emperor Hadrian's.

</doc>
<doc id="24429" url="https://en.wikipedia.org/wiki?curid=24429" title="Pope Zosimus">
Pope Zosimus

Pope Zosimus (died 26 December 418) reigned from 18 March 417 to his death in 418. He was born in Mesoraca, Calabria.
He succeeded Innocent I and was followed by Boniface I. Zosimus took a decided part in the protracted dispute in Gaul as to the jurisdiction of the See of Arles over that of Vienne, giving energetic decisions in favour of the former, but without settling the controversy. His fractious temper coloured all the controversies in which he took part, in Gaul, Africa and Italy, including Rome, where at his death the clergy were very much divided.
Biography.
According to the "Liber Pontificalis", Zosimus was a Greek and his father's name was Abram. Historian Adolf von Harnack deduced from this that the family was of Jewish origin, but this cannot be certain.
Nothing is known of the life of Zosimus before his elevation to the Papal See. His consecration as Bishop of Rome took place on 18 March 417. The festival was attended by Patroclus, Bishop of Arles, who had been raised to that See in place of Bishop Heros of Arles, who had been deposed by Constantius III. Patroclus gained the confidence of the new pope at once; as early as 22 March he received a papal letter which conferred upon him the rights of a metropolitan over all the bishops of the Gallic provinces of Viennensis and Narbonensis I and II. In addition, he was made a kind of papal vicar for the whole of Gaul; no Gallic ecclesiastic being permitted to journey to Rome without bringing with him a certificate of identity from Patroclus.
In the year 400, Arles had been substituted for Trier as the residence of the chief government official of the civil Diocese of Gaul, the "Prefectus Praetorio Galliarum". Patroclus, who enjoyed the support of the commander Constantine, used this opportunity to procure for himself the position of supremacy above mentioned, by winning over Zosimus to his ideas. The bishops of Vienne, Narbonne and Marseille regarded this elevation of the See of Arles as an infringement of their rights, and raised objections which occasioned several letters from Zosimus. The dispute, however, was not settled until the pontificate of Pope Leo I.
Confrontation with Pelagianism.
Not long after the election of Zosimus Caelestius, a proponent of Pelagianism who had been condemned by Innocent I, the preceding pope, came to Rome to appeal to the new pope, having been expelled from Constantinople. In the summer of 417, Zosimus held a meeting of the Roman clergy in the Basilica of St. Clement before which Caelestius appeared. The propositions drawn up by the deacon Paulinus of Milan, on account of which Caelestius had been condemned at Carthage in 411, were laid before him. Caelestius refused to condemn these propositions, at the same time declaring in general that he accepted the doctrine expounded in the letters of Pope Innocent and making a confession of faith which was approved. The pope was won over by the calculated conduct of Caelestius, and said that it was not certain whether he had really maintained the false doctrine rejected by Innocent, and therefore Zosimus considered the action of the African bishops against Caelestius too hasty. He wrote at once in this sense to the bishops of the African province, and called upon those who had anything to bring against Caelestius to appear at Rome within two months.
Soon after this, Zosimus received from Pelagius a confession of faith, together with a new treatise on free will. The pope held a new synod of the Roman clergy, before which both these writings were read; the assembly held the statements to be orthodox, and Zosimus again wrote to the African bishops defending Pelagius and reproving his accusers, among whom were the Gallic bishops Hero and Lazarus. Archbishop Aurelius of Carthage quickly called a synod, which sent a reply to Zosimus in which it was proved that the pope had been deceived by the heretics. In his answer Zosimus declared that he had settled nothing definitely, and wished to settle nothing without consulting the African bishops. After the new synodal letter of the African council of 1 May 418 to the pope, and after the steps taken by the emperor Honorius against the Pelagians, Zosimus issued his "Tractoria", in which Pelagianism and its authors were finally condemned.
Shortly after this, Zosimus became involved in a dispute with the African bishops in regard to the right of appeal to the Roman See clerics who had been condemned by their bishops. When the priest Apiarius of Sicca had been excommunicated by his bishop on account of his crimes, he appealed directly to the pope, without regard to the regular course of appeal in Africa which was exactly prescribed. The pope at once accepted the appeal, and sent legates with letters to Africa to investigate the matter. Another, potentially wiser, course would have been to have first referred Apiarius to the ordinary course of appeal in Africa itself. Zosimus next made the further mistake of basing his action on a reputed canon of the First Council of Nicaea, which was in reality a canon of the Council of Sardica. In the Roman manuscripts the canons of Sardica followed those of Nicaea immediately, without an independent title, while the African manuscripts contained only the genuine canons of Nicaea, so that the canon appealed to by Zosimus was not contained in the African copies of the Nicene canons. This mistake ignited a serious disagreement over the appeal, which continued after the death of Zosimus.
Besides the writings of the pope already mentioned, there are extant other letters to the bishops of the Byzantine province in Africa, in regard to a deposed bishop, and to the bishops of Gaul and Spain in respect to Priscillianism and ordination to the different grades of the clergy. The "Liber Pontificalis" attributes to Zosimus a decree on the wearing of the maniple by deacons and on the dedication of Easter candles in the country parishes; also a decree forbidding clerics to visit taverns. Zosimus was buried in the sepulchral Church of St. Laurence in Agro Verano.

</doc>
