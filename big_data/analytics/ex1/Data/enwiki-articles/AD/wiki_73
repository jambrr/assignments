<doc id="22997" url="https://en.wikipedia.org/wiki?curid=22997" title="Panama">
Panama

Panama ( ; ), officially called the Republic of Panama (), is a country in Central America situated between North and South America. It is bordered by Costa Rica to the west, Colombia to the southeast, the Caribbean to the north and the Pacific Ocean to the south. The capital and largest city is Panama City, whose metropolitan area is home to nearly half of the country's 3.9 million people.
Panama was inhabited by several indigenous tribes prior to settlement by the Spanish in the 16th century. Panama broke away from Spain in 1821 and joined a union of Nueva Granada, Ecuador, and Venezuela named the Republic of Gran Colombia. When Gran Colombia dissolved in 1831, Panama and Nueva Granada remained joined, eventually becoming the Republic of Colombia. With the backing of the United States, Panama seceded from Colombia in 1903, allowing the Panama Canal to be built by the U.S. Army Corps of Engineers between 1904 and 1914. In 1977, an agreement was signed for the total transfer of the Canal from the United States to Panama by the end of the 20th century, which culminated on 31 December 2000.
Revenue from canal tolls continues to represent a significant portion of Panama's GDP, although commerce, banking, and tourism are major and growing sectors. Panama has the second largest economy in Central America and is also the fastest growing economy and largest per capita consumer in Central America. In 2013, Panama ranked 5th among Latin American countries in terms of the Human Development Index, and 59th in the world. Since 2010, Panama remains the second most competitive economy in Latin America, according to the World Economic Forum's Global Competitiveness Index. Covering around 40 percent of its land area, Panama's jungles are home to an abundance of tropical plants and animals – some of them to be found nowhere else on the planet.
Etymology.
There are several theories about the origin of the name "Panama". Some believe that the country was named after a commonly found species of tree ("Sterculia apetala", the Panama tree). Others believe that the first settlers arrived in Panama in August, when butterflies abound, and that the name means "many butterflies" in an indigenous language.
The best-known version is that a fishing village and its nearby beach bore the name "Panamá", which meant "an abundance of fish". Captain Antonio Tello de Guzmán, while exploring the Pacific side in 1515, stopped in the small indigenous fishing town. This was communicated to the Crown and in 1517 Don Gaspar De Espinosa, a Spanish lieutenant, decided to settle a post there. In 1519, Pedrarias Dávila decided to establish the Empire's Pacific city in this site. The new settlement replaced Santa María La Antigua del Darién, which had lost its function within the Crown's global plan after the beginning of the Spanish exploitation of the riches in the Pacific.
Blending all of the above together, Panamanians believe in general that the word Panama means "abundance of fish, trees and butterflies". This is the official definition given in social studies textbooks approved by the Ministry of Education in Panama. However, others believe the word "Panama" comes from the Kuna word "bannaba" which means "distant" or "far away".
History.
At the time of the arrival of the Spanish in the 16th century, the known inhabitants of Panama included the Cuevas and the Coclé tribes. These people have nearly disappeared, as they had no immunity from European infectious diseases.
Pre-Colombian period.
The Isthmus of Panama was formed about 3 million years ago when the land bridge between North and South America finally closed, after which plants and animals gradually crossed it in both directions. The existence of the isthmus had an impact on the dispersal of people, agriculture and technology throughout the American continent from the appearance of the first hunters and collectors to the era of villages and cities.
The earliest artifacts discovered of indigenous peoples in Panama have included Paleo-Indians projectile points. Later central Panama was home to some of the first pottery-making in the Americas, such as the Monagrillo cultures dating to about 2500–1700 BC. These evolved into significant populations that are best known through the spectacular burials (dating to c. 500–900 AD) at the Monagrillo archaeological site, and the beautiful polychrome pottery of the Gran Coclé style. The monumental monolithic sculptures at the Barriles (Chiriqui) site are other important evidence of the ancient isthmian cultures.
Prior to the arrival of Europeans, Panama was widely settled by Chibchan, Chocoan, and Cueva peoples, among whom the largest group were the Cueva (whose specific language affiliation is poorly documented). There is no accurate knowledge of the size of the indigenous population of the isthmus at the time of the European conquest. Estimates range as high as two million people, but more recent studies place that number closer to 200,000. Archaeological finds as well as testimonials by early European explorers describe diverse native isthmian groups exhibiting cultural variety and suggesting people already conditioned by regular regional routes of commerce.
When Panama was colonized, the indigenous peoples fled into the forest and nearby islands. Scholars believe that, among the various contributing factors, infectious disease was the main cause of the population decline of the American natives. The indigenous peoples had no acquired immunity to such diseases, which had been chronic in Eurasian populations for centuries.
Conquest to 1799.
Rodrigo de Bastidas, sailing westward from Venezuela in 1501 in search of gold, was the first European to explore the isthmus of Panama. A year later, Christopher Columbus visited the isthmus and established a short-lived settlement in the Darien. Vasco Núñez de Balboa's tortuous trek from the Atlantic to the Pacific in 1513 demonstrated that the isthmus was, indeed, the path between the seas, and Panama quickly became the crossroads and marketplace of Spain's empire in the New World. Gold and silver were brought by ship from South America, hauled across the isthmus, and loaded aboard ships for Spain. The route became known as the Camino Real, or Royal Road, although it was more commonly known as Camino de Cruces (Road of the Crosses) because of the abundance of gravesites along the way.
Panama, under Spanish rule for almost 300 years (1538–1821) became part of the Viceroyalty of Peru, along with all other Spanish possessions in South America. From the outset, Panamanian identity was based on a sense of "geographic destiny", and Panamanian fortunes fluctuated with the geopolitical importance of the isthmus. The colonial experience also spawned Panamanian nationalism as well as a racially complex and highly stratified society, the source of internal conflicts that ran counter to the unifying force of nationalism.
In 1538, the Real Audiencia de Panama was established, initially with jurisdiction from Nicaragua to Cape Horn before the conquest of Peru. A Real Audiencia (royal audiency) was a judicial district that functioned as an appeals court. Each audiencia had oidores (Spanish: hearer, a judge).
Spanish authorities exercised little control over much of the territory of Panama, large sections managing to resist conquest until very late in the colonial era. Because of this, indigenous people of the area were often referred to as "indios de guerra" (war Indians) and resisted Spanish attempts to conquer them or missionize them. However, Panama was enormously important to Spain strategically because it was the easiest way to transship silver mined in Peru to Europe. Silver cargos were landed at Panama and then taken overland to Portobello or Nombre de Dios on the Caribbean side of the isthmus for further shipment.
Because of the incomplete Spanish control, the Panama route was vulnerable to attack from pirates (mostly Dutch and English) and from 'new world' Africans called cimarrons who had freed themselves from enslavement and lived in communes or palenques around the Camino Real in Panama's Interior, and on some of the islands off Panama's Pacific coast. One such famous community amounted to a small kingdom under Bayano, which emerged in the 1552 to 1558. Sir Francis Drake's famous raids on Panama in 1572–73 were aided by Panama cimarrons, and Spanish authorities were only able to bring them under control by making an alliance with them that guaranteed their freedom in exchange for military support in 1582.
The prosperity enjoyed during the first two centuries (1540–1740) while contributing to colonial growth; the placing of extensive regional judicial authority (Real Audiencia) as part of its jurisdiction; and the pivotal role it played at the height of the Spanish Empire – the first modern global empire – helped define a distinctive sense of autonomy and of regional or national identity within Panama well before the rest of the colonies.
The end of the encomienda system in Azuero, however, sparked the conquest of Veraguas in that same year. Under the leadership of Francisco Vázquez, the region of Veraguas passed into Castillan rule in 1558. In the newly conquered region, the old system of encomienda was imposed. On the other hand, the Panamanian movement for independence can be indirectly attributed to the abolishment of the encomienda system in the Azuero Peninsula, set forth by the Spanish Crown, in 1558 because of repeated protests by locals against the mistreatment of the native population. In its stead, a system of medium and smaller-sized landownership was promoted, thus taking away the power from the large landowners and into the hands of medium and small sized proprietors.
Panama was the site of the ill-fated Darien scheme, which set up a Scottish colony in the region in 1698. This failed for a number of reasons, and the ensuing debt contributed to the union of England and Scotland in 1707.
In 1671, the privateer Henry Morgan, licensed by the English government, sacked and burned the city of Panama – the second most important city in the Spanish New World at the time. In 1717, the viceroyalty of New Granada (northern South America) was created in response to other Europeans trying to take Spanish territory in the Caribbean region. The Isthmus of Panama was placed under its jurisdiction. However, the remoteness of New Granada's capital, Santa Fe de Bogotá (the modern capital of Colombia) proved a greater obstacle than the Spanish crown anticipated as the authority of New Granada was contested by the seniority, closer proximity, and previous ties to the viceroyalty of Lima and even by Panama's own initiative. This uneasy relationship between Panama and Bogotá would persist for centuries.
In 1744, Bishop Francisco Javier de Luna Victoria DeCastro established the College of San Ignacio de Loyola and on June 3, 1749, founded La Real y Pontificia Universidad de San Javier. By this time, however, Panama's importance and influence had become insignificant as Spain's power dwindled in Europe and advances in navigation technique increasingly permitted to round Cape Horn in order to reach the Pacific. While the Panama route was short it was also labor-intensive and expensive because of the loading and unloading and laden-down trek required to get from the one coast to the other.
During the last half of the 18th century and the first half of the 19th century, migrations to the countryside decreased Panama City's population and the isthmus' economy shifted from the tertiary to the primary sector.
1800s.
As the Spanish American wars of independence were heating up all across Latin America, Panama City was preparing for independence; however, their plans were accelerated by the unilateral Grito de La Villa de Los Santos (Cry From the Town of Saints), issued on November 10, 1821 by the residents of Azuero without backing from Panama City to declare their separation from the Spanish Empire. In both Veraguas and the capital this act was met with disdain, although on differing levels. To Veraguas, it was the ultimate act of treason, while to the capital, it was seen as inefficient and irregular, and furthermore forced them to accelerate their plans.
Nevertheless, the Grito was an event that shook the isthmus to its very core. It was a sign, on the part of the residents of Azuero, of their antagonism toward the independence movement in the capital. Those in the capital region in turn regarded the Azueran movement with contempt, since the separatists in Panama City believed that their counterparts in Azuero were fighting not only for independence from Spain, but also for their right to self-rule apart from Panama City once the Spaniards were gone.
It was an incredibly brave move on the part of Azuero, which lived in fear of Colonel José Pedro Antonio de Fábrega y de las Cuevas (1774–1841), and with good reason; the Colonel was a staunch loyalist, and had the entirety of the isthmus' military supplies in his hands. They feared quick retaliation and swift retribution against the separatists.
What they had counted on, however, was the influence of the separatists in the capital. Ever since October 1821, when the former Governor General, Juan de la Cruz Murgeón, left the isthmus on a campaign in Quito and left the Veraguan colonel in charge, the separatists had been slowly converting Fábrega to the separatist side. As such, by November 10, Fábrega was now a supporter of the independence movement. Soon after the separatist declaration of Los Santos, Fábrega convened every organization in the capital with separatist interests and formally declared the city's support for independence. No military repercussions occurred because of the skillful bribing of royalist troops.
Post-colonial Panama.
In the first eighty years following independence from Spain, Panama was a department of Colombia, since voluntarily becoming part of it at the end of 1821. The people of the isthmus made several attempts to secede and came close to success in 1831, and again during the Thousand Days' War of 1899–1902. When the Senate of Colombia rejected the Hay–Herrán Treaty, the United States decided to support the Panamanian independence movement.
In November 1903 Panama proclaimed its independence and concluded the Hay–Bunau-Varilla Treaty with the United States. The treaty granted rights to the United States "as if it were sovereign" in a zone roughly wide and long. In that zone, the U.S. would build a canal, then administer, fortify, and defend it "in perpetuity". In 1914, the United States completed the existing 83 km (52 mi) canal. The early 1960s saw the beginning of sustained pressure in Panama for the renegotiation of this treaty.
The United States of America's intentions to influence the area (especially the Panama Canal construction and control) led to the separation of Panama from Colombia in 1903 and the establishment of it as a nation (the United States intensively encouraged the Panamanian separatist movement). From 1903 until 1968, Panama was a constitutional democracy dominated by a commercially oriented oligarchy. During the 1950s, the Panamanian military began to challenge the oligarchy's political hegemony.
Amid negotiations for the Robles–Johnson treaty, Panama held elections in 1968. The candidates were Dr. Arnulfo Arias Madrid, Antonio González Revilla, and engineer David Samudio, who had the government's support. Samudio was the candidate of Alianza del Pueblo ("People's Alliance"), Arias Madrid was the candidate of Unión Nacional ("National Union"), and González Revilla was the candidate of Democracia Cristiana ("Christian Democrats") (see Pizzurno Gelós and Araúz, Estudios sobre el Panamá republicano 508).
Arias Madrid was declared the winner of elections that were marked by violence and accusations of fraud against Alianza del Pueblo. On October 1, 1968, Arias Madrid took office as president of Panama, promising to lead a government of "national union" that would end the reigning corruption and pave the way for a new Panama. A week and a half later, on October 11, 1968, the National Guard (Guardia Nacional) ousted Arias and initiated the downward spiral that would culminate with the United States' invasion in 1989. Arias, who had promised to respect the hierarchy of the National Guard, broke the pact and started a large restructuring of the Guard. To preserve the Guard's interests, Lieutenant Colonel Omar Torrijos Herrera and Major Boris Martínez commanded the first coup of a military force against a civilian government in Panamanian republican history.
The military justified itself by declaring that Arias Madrid was trying to install a dictatorship, and promised a return to constitutional rule. In the meantime, the Guard began a series of populist measures that would gain support for the coup. Among them were the freezing of prices on food, medicine and other goods until January 31, 1969, the freezing of renting prices, and the legalization of the permanence of squatting families in boroughs surrounding the historic site of Panama Viejo. Parallel to this, the military began a policy of repression against the opposition, who were labeled communists. The military appointed a Provisional Government Junta that would arrange new elections. However, the National Guard would prove to be very reluctant to abandon power and soon began calling itself El Gobierno Revolucionario ("The Revolutionary Government").
Post-1970.
During Omar Torrijos's control, the military regime transformed the political and economic structure of the country by initiating massive coverage of social security services and expanding public education. The constitution was changed in 1972. For the reform to the constitution, the military created a new organization, the Assembly of Corregimiento Representatives, which replaced the National Assembly. The new assembly, also known as the Poder Popular ("Power of the People"), was composed of 505 members selected by the military without the participation of political parties, which had been eliminated by the military. The new constitution proclaimed Omar Torrijos the "Maximum Leader of the Panamanian Revolution", and conceded him unlimited power for six years, although, to keep a façade of constitutionality, Demetrio B. Lakas was appointed president for the same period (Pizzurno Gelós and Araúz, "Estudios sobre el Panamá republicano" 541).
In 1981, Torrijos died in a plane crash. Torrijos' death altered the tone of Panama's political evolution. Despite the 1983 constitutional amendments, which proscribed a political role for the military, the Panama Defense Forces (PDF), as they were then known, continued to dominate Panamanian political life. By this time, General Manuel Noriega was firmly in control of both the PDF and the civilian government.
In the 1984 elections, the candidates were Nicolás Ardito Barletta Vallarino, supported by the military in a union called UNADE; Dr. Arnulfo Arias Madrid, for the opposition union ADO; the ex-General Rubén Darío Paredes, who had been forced to an early retirement by Noriega, running for Partido Nacionalista Popular PNP ("Popular Nationalist Party"), and Carlos Iván Zúñiga, running for Partido Acción Popular (PAPO) meaning "Popular Action Party". Nicolás Ardito Barletta was declared the winner of elections that had been clearly won by Arnulfo Arias Madrid. Ardito Barletta inherited a country in economic ruin and hugely indebted to the IMF and the World Bank. Amid the economic crisis and Barletta's efforts to calm the country's creditors, street protests arose, and so did military repression.
Meanwhile, Noriega's regime had fostered the development of a well-hidden criminal economy that operated as a parallel source of income for the military and their allies, providing revenues from drugs and money laundering. Toward the end of the military dictatorship, a new wave of Chinese migrants arrived on the isthmus in the hope of migrating to the United States. The smuggling of Chinese became an enormous business, with revenues of up to 200 million dollars for Noriega's regime (see Mon 167).
The military dictatorship, at that time supported by the United States, perpetrated the assassination and torture of more than one hundred Panamanians and forced into exile at least another hundred dissidents (see Zárate 15). Noriega also began playing a double role in Central America under the supervision of the CIA. While the Contadora group conducted diplomatic efforts to achieve peace in the region, Noriega supplied the Nicaraguan Contras and other guerrillas in the region with weapons and ammunition.
On June 6, 1987, the recently retired Colonel Roberto Díaz Herrera, resentful for Noriega's violation of the "Torrijos Plan" of succession that would turn him into the chief of the military after Noriega, decided to denounce the regime. He revealed details of the electoral fraud, accused Noriega of planning Torrijos's death, declared that Torrijos had received 12 million dollars from the Shah of Iran so that Panama would give the exiled Iranian leader asylum, and blamed Noriega for the assassination by decapitation of opposition leader Dr. Hugo Spadafora.
On the night of June 9, 1987, the Cruzada Civilista ("Civic Crusade") was created and began organizing actions of civil disobedience. The Crusade called for a general strike. In response, the military suspended constitutional rights and declared a state of emergency in the country. On July 10, the Civic Crusade called for a massive demonstration that was violently repressed by the "Dobermans", the military's special riot control unit. That day, later known as El Viernes Negro ("Black Friday"), left six hundred people injured and another six hundred detained, many of whom were later tortured and raped.
United States President Ronald Reagan began a series of sanctions against the military regime. The United States froze economic and military assistance to Panama in the summer of 1987 in response to the domestic political crisis in Panama and an attack on the U.S. Embassy. Yet these sanctions did little to overthrow Noriega but instead severely damaged Panama's economy. The sanctions hit the Panamanian population hard and caused the Gross Domestic Product (GDP) to decline almost 25% between 1987–1989 (see Acosta n.p.).
On February 5, 1988, General Manuel Antonio Noriega was accused of drug trafficking by federal juries in Tampa and Miami.
In April 1988, the U.S. President Ronald Reagan invoked the International Emergency Economic Powers Act, freezing Panamanian government assets in all U.S. organizations. In May 1989 Panamanians voted overwhelmingly for the anti-Noriega candidates. The Noriega regime promptly annulled the election and embarked on a new round of repression.
U.S. invasion (1989).
The United States government justified Operation Just Cause, which commenced on December 20, 1989 as necessary to safeguard the lives of U.S. citizens in Panama, defend democracy and human rights, combat drug trafficking, and secure the neutrality of the Canal as required by the Torrijos–Carter Treaties ("New York Times", A Transcript of President Bush's Address n.p.). Human Rights Watch wrote in the 1989 report: "Washington turned a blind eye to abuses in Panama for many years until concern over drug trafficking prompted indictments of the general by two grand juries in Florida in February 1988". The U.S. reported 23 servicemen killed and 324 wounded, with estimated Panamanian casualties around 450. Described as a surgical maneuver, the action led to civilian deaths whose estimated numbers range from 400 to 4,000 during the two weeks of armed activities. This surgical maneuver represented the largest United States military operation to that date since the end of the Vietnam War (Cajar Páez 22) The United Nations put the Panamanian civilian death toll at 500, while other sources had higher statistics. The number of U.S. civilians (and their dependents), who had worked for the Panama Canal Commission and the U.S. Military, and were killed by the Panamanian Defense Forces, has never been fully disclosed.
On December 29, the UN General Assembly approved a resolution calling the intervention in Panama a "flagrant violation of international law and of the independence, sovereignty and territorial integrity of the States". The resolution was vetoed by the United States, United Kingdom, and France.
The urban population, with many living below the poverty level, was greatly affected by the 1989 intervention. As pointed out in 1995 by a UN Technical Assistance Mission to Panama, the bombardments during the invasion caused the displacement of 20,000 people. The most heavily affected district was impoverished El Chorrillo, where several blocks of apartments were completely destroyed; El Chorrillo had been since Canal construction days a series of wooden barracks; these easily caught fire under the United States attack. The economic damage caused by the intervention has been estimated to be between 1.5 and 2 billion dollars. n.p. Many Panamanians supported the intervention.
Post-intervention era.
Panama's Electoral Tribunal moved quickly to restore the civilian constitutional government, reinstated the results of the May 1989 election on December 27, 1989, and confirmed the victory of President Guillermo Endara and Vice Presidents Guillermo Ford and Ricardo Arias Calderon.
During its five-year term, the often-fractious government struggled to meet the public's high expectations. Its new police force was a major improvement over its predecessor but was not fully able to deter crime. Ernesto Pérez Balladares was sworn in as President on September 1, 1994, after an internationally monitored election campaign.
Perez Balladares ran as the candidate for a three-party coalition dominated by the Democratic Revolutionary Party (PRD), the erstwhile political arm of military dictatorships. Perez Balladares worked skillfully during the campaign to rehabilitate the PRD's image, emphasizing the party's populist Torrijos roots rather than its association with Noriega. He won the election with only 33% of the vote when the major non-PRD forces splintered into competing factions. His administration carried out economic reforms and often worked closely with the U.S. on implementation of the Canal treaties.
On September 1, 1999, Mireya Moscoso, the widow of former President Arnulfo Arias Madrid, took office after defeating PRD candidate Martin Torrijos, son of Omar Torrijos, in a free and fair election. During her administration, Moscoso attempted to strengthen social programs, especially for child and youth development, protection, and general welfare. Moscoso's administration successfully handled the Panama Canal transfer and was effective in the administration of the Canal.
The PRD's Martin Torrijos won the presidency and a legislative majority in the National Assembly in 2004. Torrijos ran his campaign on a platform of, among other pledges, a "zero tolerance" for corruption, a problem endemic to the Moscoso and Perez Balladares administrations. After taking office, Torrijos passed a number of laws which made the government more transparent. He formed a National Anti-Corruption Council whose members represented the highest levels of government, as well as civil society, labor organizations, and religious leadership. In addition, many of his closest Cabinet ministers were non-political technocrats known for their support for the Torrijos government's anti-corruption aims. Despite the Torrijos administration's public stance on corruption, many high-profile cases, particularly involving political or business elites, were never acted upon.
Conservative supermarket magnate Ricardo Martinelli was elected to succeed Martin Torrijos with a landslide victory at the May 2009 presidential election. Mr. Martinelli's business credentials drew voters worried by slowing growth due to the world financial crisis. Standing for the four-party opposition Alliance for Change, Mr. Martinelli gained 60% of the vote, against 37% for the candidate of the governing left-wing Democratic Revolutionary Party.
On May 4, 2014, Juan Carlos Varela won the 2014 presidential election with over 39% of the votes, against the party of his former political partner Ricardo Martinelli, Cambio Democrático, and their candidate José Domingo Arias. He was sworn in on 1 July 2014.
Geography.
Panama is located in Central America, bordering both the Caribbean Sea and the Pacific Ocean, between Colombia and Costa Rica. It mostly lies between latitudes 7° and 10°N, and longitudes 77° and 83°W (a small area lies west of 83°).
Its location on the Isthmus of Panama is strategic. By 2000, Panama controlled the Panama Canal which connects the Atlantic Ocean and the Caribbean Sea to the North of the Pacific Ocean. Panama's total area is 74,177.3 km2.
The dominant feature of Panama's geography is the central spine of mountains and hills that forms the continental divide. The divide does not form part of the great mountain chains of North America, and only near the Colombian border are there highlands related to the Andean system of South America. The spine that forms the divide is the highly eroded arch of an uplift from the sea bottom, in which peaks were formed by volcanic intrusions.
The mountain range of the divide is called the Cordillera de Talamanca near the Costa Rican border. Farther east it becomes the Serranía de Tabasará, and the portion of it closer to the lower saddle of the isthmus, where the Panama Canal is located, is often called the Sierra de Veraguas. As a whole, the range between Costa Rica and the canal is generally referred to by geographers as the Cordillera Central.
The highest point in the country is the Volcán Barú, which rises to 3,475 metres (11,401 ft). A nearly impenetrable jungle forms the Darién Gap between Panama and Colombia where Colombian guerrilla and drug dealers are operating with hostage-taking. This and forest protection movements create a break in the Pan-American Highway, which otherwise forms a complete road from Alaska to Patagonia.
Panama's wildlife holds the most diversity of all the countries in Central America. It is home to many South American species as well as North American wildlife.
Waterways.
Nearly 500 rivers lace Panama's rugged landscape. Mostly unnavigable, many originate as swift highland streams, meander in valleys, and form coastal deltas. However, the Río Chagres ("Rio Chagres"), located in central Panama, is one of the few wide rivers and a source of enormous hydroelectric power. The central part of the river is dammed by the Gatun Dam and forms Gatun Lake, an artificial lake that constitutes part of the Panama Canal. The lake was created between 1907 and 1913 by the building of the Gatun Dam across the Chagres River. When it was created, Gatun Lake was the largest man-made lake in the world, and the dam was the largest earth dam. The river drains northwest into the Caribbean. The Kampia and Madden Lakes (also filled from the Río Chagres) provide hydroelectricity for the area of the former Canal Zone.
The Río Chepo, another source of hydroelectric power, is one of the more than 300 rivers emptying into the Pacific. These Pacific-oriented rivers are longer and slower running than those of the Caribbean side. Their basins are also more extensive. One of the longest is the Río Tuira, which flows into the Golfo de San Miguel and is the nation's only river navigable by larger vessels.
Harbors.
The Caribbean coastline is marked by several good natural harbors. However, Cristóbal, at the Caribbean terminus of the canal, had the only important port facilities in the late 1980s. The numerous islands of the Archipiélago de Bocas del Toro, near the Beaches of Costa Rica, provide an extensive natural roadstead and shield the banana port of Almirante. The over 350 San Blas Islands, near Colombia, are strung out for more than 160 km along the sheltered Caribbean coastline.
Currently, the terminal ports located at each end of the Panama Canal, namely the Port of Cristobal and the Port of Balboa, are ranked second and third respectively in Latin America in terms of numbers of containers units (TEU) handled. The Port of Balboa covers 182 hectares and contains four berths for containers and two multi-purpose berths. In total, the berths are over 2,400 meters long with alongside depth of 15 meters. The Port of Balboa has 18 super post-Panamax and Panamax quay cranes and 44 gantry cranes. The Port of Balboa also contains 2,100 square meters of warehouse space.
The Ports of Cristobal (encompassing the container terminals of Panama Ports Cristobal, Manzanillo International Terminal and Colon Container Terminal) handled 2,210,720 TEU in 2009, second only to the Port of Santos, Brazil, in Latin America.
Excellent deep water ports capable of accommodating large VLCC (Very Large Crude Oil Carriers) are located at Charco Azul, Chiriquí (Pacific) and Chiriquí Grande, Bocas del Toro (Atlantic) near Panama's western border with Costa Rica. The Trans-Panama pipeline, running across the isthmus with a length of 131 km, has been operating between Charco Azul and Chiriquí Grande since 1979.
Climate.
Panama has a tropical climate. Temperatures are uniformly high—as is the relative humidity—and there is little seasonal variation. Diurnal ranges are low; on a typical dry-season day in the capital city, the early morning minimum may be and the afternoon maximum . The temperature seldom exceeds for more than a short time. Temperatures on the Pacific side of the isthmus are somewhat lower than on the Caribbean, and breezes tend to rise after dusk in most parts of the country. Temperatures are markedly cooler in the higher parts of the mountain ranges, and frosts occur in the Cordillera de Talamanca in western Panama.
Climatic regions are determined less on the basis of temperature than on rainfall, which varies regionally from less than to more than per year. Almost all of the rain falls during the rainy season, which is usually from April to December, but varies in length from seven to nine months. In general, rainfall is much heavier on the Caribbean than on the Pacific side of the continental divide. The annual average in Panama City is little more than half of that in Colón. Although rainy-season thunderstorms are common, the country is outside the hurricane belt.
Panama's tropical environment supports an abundance of plants. Forests dominate, interrupted in places by grasslands, scrub, and crops. Although nearly 40% of Panama is still wooded, deforestation is a continuing threat to the rain-drenched woodlands. Tree cover has been reduced by more than 50% since the 1940s. Subsistence farming, widely practiced from the northeastern jungles to the southwestern grasslands, consists largely of corn, bean, and tuber plots. Mangrove swamps occur along parts of both coasts, with banana plantations occupying deltas near Costa Rica. In many places, a multi-canopied rain forest abuts the swamp on one side of the country and extends to the lower reaches of slopes in the other.
Politics.
Panama's politics take place in a framework of a presidential representative democratic republic, whereby the President of Panama is both head of state and head of government, and of a multi-party system. Executive power is exercised by the government. Legislative power is vested in both the government and the National Assembly. The judiciary is independent of the executive and the legislature.
For all people national elections are universal and mandatory for all citizens 18 years and older. National elections for the executive and legislative branches take place every five years. Members of the judicial branch (justices) are appointed by the head of state. Panama's National Assembly is elected by proportional representation in fixed electoral districts, so many smaller parties are represented. Presidential elections do not require a simple majority; out of the four last presidents only one, incumbent president Ricardo Martinelli, was elected with over 50% of the popular vote.
Political culture.
In December 1989 the United States invaded Panama to depose the dictator Manuel Noriega. Since the U.S. invasion, and resulting end to the 21-year military dictatorship, Panama has successfully completed four peaceful transfers of power to opposing political factions. The political landscape is dominated by two major parties and many smaller parties, many of which are driven by individual leaders more than ideologies. Former President Martin Torrijos is the son of dictator Omar Torrijos. He succeeded Mireya Moscoso, the widow of Arnulfo Arias. Panama's most recent national elections occurred on May 4, 2014 with Incumbent Vice-President Juan Carlos Varela declared the victor.
Foreign relations.
The United States cooperates with the Panamanian government in promoting economic, political, security, and social development through U.S. and international agencies. Cultural ties between the two countries are strong, and many Panamanians come to the United States for higher education and advanced training.
Military.
The Panamanian Public Forces are the national security forces of Panama. Panama is the second country in Latin America (the other being Costa Rica) to permanently abolish standing armies. Panama maintains armed police and security forces, and small air and maritime forces. They are tasked with law enforcement and can perform limited military actions.
Administrative divisions.
Panama is divided into ten provinces with their respective local authorities (governors), which are divided into districts and "corregimientos" (townships) and has a total of ten cities. Also, there are five "Comarcas" (literally: "Shires") populated by a variety of indigenous groups.
Economy.
According to the CIA World Factbook, Panama had an unemployment rate of 2.7%. A food surplus was registered in August 2008. On the Human Development Index, Panama ranked 58th in 2012. In recent years, Panama's economy has experienced a boom, with growth in real gross domestic product (GDP) averaging over 10.4% from 2006–2008. Panama's economy has been among the fastest growing and best managed in Latin America. Latin Business Chronicle had predicted that Panama would be the fastest growing economy in Latin America during the five years 2010–14, matching Brazil's 10% rate.
The expansion project of the Panama Canal, combined with the conclusion of a free trade agreement with the United States, are expected to boost and extend economic expansion for some time.
Despite Panama's upper-middle per capita GDP, it remains a country of stark contrasts. Perpetuated by dramatic educational disparities, over 25% of Panama's population lived in national poverty in 2013 and 3% of the population lives in extreme poverty, according to latest reports by the World Bank.
Economic sectors.
Panama's economy, because of its key geographic location, is mainly based on a well developed service sector especially commerce, tourism, and trading. The handover of the Canal and military installations by the United States has given rise to large construction projects.
A project to build of a third set of locks for the Panama Canal A was overwhelmingly approved in referendum (with low voter turnout, however) on October 22, 2006. The official estimated cost of the project is US$5.25 billion. The canal is of major economic importance because it provides millions of dollars of toll revenue to the national economy and provides massive employment. Transfer of control of the Canal to the Panamanian government completed in 1999, after being controlled by the US for 85 years.
Copper and gold deposits are being developed by foreign investors, to the dismay of some environmental groups, as all of the projects are located within protected areas.
Transportation.
Panama is home to Tocumen International Airport, Central America's largest airport. Additionally there are more than 20 smaller airfields in the country. See list of airports in Panama.
Panama's roads, traffic and transportation systems are generally safe, though night driving is difficult and in many cases, restricted by local authorities. This usually occurs in informal settlements. Traffic in Panama moves on the right, and Panamanian law requires that drivers and passengers wear seat belts. Highways are generally well-developed for a Latin American country.
Currently, Panama has modern buses known as Metrobuses, along with a Metro line. Formerly, the system was dominated by colorfully painted "diablos rojos", with some remaining. A "" is usually "customized" or painted with bright colors, usually depicting famous actors, politicians or singers. Panama City's streets experience frequent traffic jams due to poor planning for the now extensive private vehicle fleet.
Tourism.
Tourism in Panama is rapidly growing. It has maintained its growth over the past five years due to government tax and price discounts to foreign guests and retirees. These economic incentives have caused Panama to be regarded as a relatively good place to retire in the world. Real estate developers in Panama have increased the number of tourism destinations in the past five years because of the interest for these visitor incentives. 2,200,000 tourists arrived in 2012.
The number of tourists from Europe grew by 23.1% during the first nine months of 2008. According to the Tourism Authority of Panama (ATP), from January to September, 71,154 tourists from Europe entered Panama, which is 13,373 more than figures for same period the previous year. Most of the European tourists were Spaniards (14,820), followed by Italians (13,216), French (10,174) and British (8,833). There were 6997 from Germany, the most populous country in the European Union. Europe has become one of the key markets to promote Panama as a tourist destination.
In 2012, 4.345.5 million entered into the Panamanian economy as a result of tourism. This accounted for 9.5% of gross domestic product in the country, surpassing other productive sectors.
Panama enacted Law No. 80 in 2012 for the promotion of foreign investment in tourism. Law 80 replaced an older Law 8 of 1994. Law 80 provides 100% exemption from income tax and real estate taxes for 15 years, duty-free imports for construction materials and equipment for five years, and capital gains tax exemption for five years.
Currency.
The Panamanian currency is officially the balboa, fixed at a rate of 1:1 with the United States dollar since independence in 1903. In practice Panama is dollarized: US dollars are legal tender and used for all paper currency, while Panama has its own coinage. Because of the tie to US dollars, Panama has traditionally had low inflation. According to the Economic Commission for Latin American and the Caribbean, Panama's inflation in 2006 was 2.0% as measured by weight Consumer Price Index (CPI).
The balboa replaced the Colombian peso in 1904 after Panama's independence. Balboa banknotes were printed in 1941 by President Arnulfo Arias. They were recalled several days later, giving them the name "The Seven Day Dollar". The notes were burned after the seven days but occasionally balboa notes can be found with collectors. These were the only banknotes issued by Panama and U.S. notes have circulated both before and since.
International trade.
The high levels of Panamanian trade are in large part from the Colón Free Trade Zone, the largest free trade zone in the Western Hemisphere. Last year the zone accounted for 92% of Panama's exports and 64% of its imports, according to an analysis of figures from the Colon zone management and estimates of Panama's trade by the United Nations Economic Commission for Latin America and the Caribbean. Panama's economy is also very much supported by the trade and export of coffee and other agricultural products.
The Bilateral Investment Treaty (BIT) between the governments of the United States and Panama was signed on October 27, 1982. The treaty protects US investment and assists Panama in its efforts to develop its economy by creating conditions more favorable for US private investment and thereby strengthening the development of its private sector. The BIT was the first such treaty signed by the US in the Western Hemisphere. A Panama - United States Trade Promotion Agreement (TPA) was signed in 2007, approved by Panama on July 11, 2007 and by US President Obama on October 21, 2011, and the agreement entered into force on October 31, 2012.
Society.
Demographics.
Panama recorded a population of 3,405,813 in its 2010 census. The proportion of the population aged below 15 in 2010 was 29%. 64.5% of the population were aged between 15 and 65, with 6.6% of the population being 65 years or older.
More than half the population lives in the Panama City–Colón metropolitan corridor, which spans several cities. Panama's urban population exceeds 70%, making Panama's population the most urbanized in Central America.
Ethnic groups.
In 2010 the population was 65% Mestizo (mixed white, Native American), 12.3% Native Americans, 9.2% Black/mulattoes and 6.7% White.
Ethnic groups in Panama include Mestizo people, who are a mix of European and native ancestry. Black, or Afro-Panamanians account for 15-20% of the population. Most Afro-Panamanians live on the Panama-Colón metropolitan area, the Darien Province, La Palma, and Bocas Del Toro. Neighborhoods in Panama City that have large black populations include; Curundu, El Chorrillo, Rio Abajo, San Joaquín, El Marañón, San Miguelito, Colón, and Santa Ana. Black Panamanians are descendents of African slaves stolen from Africa and brought to the Americas on the 1500 Atlantic Slave Trade. The second wave of black people brought to Panama came from the Caribbean during the construction of the Panama Canal. Panama also has a considerable Chinese and Indian (India) population. They were brought to work on the canal during its construction. Most Chinese-Panamanians reside in the province of Chiriquí. Europeans and white-Panamanians are a minority in Panama. They are descendents of the people who colonized Panama, worked on the canal, and who moved to the country. Panama is also home to a small Arab community that have Mosques to practice Islam.
The Amerindian population includes seven ethnic groups: the Ngäbe, Kuna (Guna), Emberá, Buglé, Wounaan, Naso Tjerdi (Teribe), and Bri Bri.
Languages.
Spanish is the official and dominant language. The Spanish spoken in Panama is known as Panamanian Spanish. About 93% of the population speak Spanish as their first language, though many citizens who hold jobs at international levels, or who are a part of business corporations speak both English and Spanish. Native languages, such as Ngäbere are spoken throughout the country, mostly in their native grounds. Over 400,000 Panamanians hold their native languages and customs. Some new statistics show that as second language, English is spoken by 8%, French by 4% and Arabic by 1%.
Largest cities.
These are the 10 largest Panamanian cities and towns. Most of Panama's largest cities are part of the Panama City Metropolitan Area.
Religion.
The government of Panama does not collect statistics on the religious affiliation of citizens, but various sources estimate that 75% to 85% of the population identifies itself as Roman Catholic and 15%–25% as Protestant. The Bahá'í Faith community of Panama is estimated at 2.00% of the national population, or about 60,000 including about 10% of the Guaymí population.
The Church of Jesus Christ of Latter-day Saints (LDS Church) claim more than 40,000 members. Smaller religious groups include Seventh-day Adventists, Jehovah's Witnesses, Episcopalians with between 7,000 and 10,000 members, Jewish and Muslim communities with approximately 10,000 members each, Hindus, Buddhists, and other Christians. Indigenous religions include Ibeorgun (among Kuna) and Mamatata (among Ngobe). There are also a small number of Rastafarians.
Education.
Originally, during the 16th century, education in Panama was provided by Jesuit priests. Public education, as a national and governmental institution, began in 1903. The principles underlying this early education system were that children should receive different types of education in accordance with their social class and therefore the position they were expected to occupy in society.
Public education began in Panama soon after the separation from Colombia in 1903. The first efforts were guided by an extremely paternalistic view of the goals of education, as evidenced in comments made in a 1913 meeting of the First Panamanian Educational Assembly, "The cultural heritage given to the child should be determined by the social position he will or should occupy. For this reason education should be different in accordance with the social class to which the student should be related." This elitist focus changed rapidly under United States influence.
In 2010, it was estimated that 94.1% of the population was literate (94.7% of males and 93.5% of females). Education in Panama is compulsory for the children of age group between 6 and 18. In recent decades, school enrollment at all levels, but especially at upper levels, has increased significantly. Panama used to participate in the PISA exams but due to debts and unsatisfactory exam results is postponing participation until 2018.
Culture.
The culture of Panama derived from European music, art and traditions that were brought over by the Spanish to Panama. Hegemonic forces have created hybrid forms of this by blending African and Native American culture with European culture. For example, the "tamborito" is a Spanish dance that was blended with African rhythms, themes and dance moves.
Dance is a symbol of the diverse cultures that have coupled in Panama. The local folklore can be experienced through a multitude of festivals, dances and traditions that have been handed down from generation to generation. Local cities host live "reggae en español", "reggaeton", "haitiano (compas)", jazz, blues, "salsa", reggae, and rock music performances.
Handicraft.
Outside Panama City, regional festivals take place throughout the year featuring local musicians and dancers. Another example of Panama's blended culture is reflected in the traditional products, such as woodcarvings, ceremonial masks and pottery, as well as in its architecture, cuisine and festivals. In earlier times, baskets were woven for utilitarian uses, but now many villages rely almost exclusively on the baskets they produce for tourists.
An example of undisturbed, unique culture in Panama is that of the Guna who are known for "molas". "Mola" is the Guna word for blouse, but the term "mola" has come to mean the elaborate embroidered panels made by Guna women, that make up the front and back of a Guna woman's blouse. They are several layers of cloth, varying in color, that are loosely stitched together, made using an appliqué process referred to as "reverse appliqué".
Holidays and festivities.
The Christmas parade, known as "El desfile de Navidad", is celebrated in the capital, Panama City. This holiday is celebrated on December 25. The floats in the parade are decorated with the Panamanian colors, and the women dress in dresses called "Pollera" while the men dress in the traditional "Montuno". In addition, the marching band in the parade, consisting of drummers, keeps the crowds entertained. In the city, a big Christmas tree is lit with Christmas lights, and everybody surrounds the tree and sings Christmas carols.
Traditional cuisine.
Panamanian Cuisine is a mix of African, Spanish, and Native American techniques, dishes, and ingredients, reflecting its diverse population. Since Panama is a land bridge between two continents, it has a large variety of tropical fruits, vegetables and herbs that are used in native cooking.
Typical Panamanian foods are mildly flavored, without the pungency of some of Panama's Latin American and Caribbean neighbors. Common ingredients are maize, rice, wheat flour, plantains, "yuca" (cassava), beef, chicken, pork and seafood.
Traditional clothing.
Panamanian men's traditional clothing consists of white cotton shirts, trousers and woven straw hat.
The traditional women's clothing is the "pollera". It originated in Spain in the 16th century, and by the early 1800s it was a typical in Panama, worn by women servants, especially wet nurses ("De Zarate" 5). Later, it was adopted by upper-class women.
A "pollera" is made of "cambric" or "fine linen" (Baker 177). It is white, and is usually about 13 yards of material.
The original "pollera" consists of a ruffled blouse worn off the shoulders and a skirt is on the waistline with gold buttons. The skirt is also ruffled, so that when it is lifted up, it looks like a peacock's tail or a "mantilla" fan. The designs on the skirt and blouse are usually flowers or birds. Two large matching pom poms ("mota") are on the front and back, four ribbons hang from the front and back on the waist line, five gold chains ("caberstrillos") hang from the neck to the waist, a gold cross or medallion on a black ribbon is worn as a choker, and a silk purse is worn on the waistline. Earrings ("zaricillos") are usually gold or coral. Slippers usually match the color of the "pollera". Hair is usually worn in a bun, held by three large gold combs that have pearls ("tembleques") worn like a crown. Quality "pollera" can cost up to $10,000, and may take a year to complete.
Today, there are different types of "polleras"; the "pollera de gala" consists of a short-sleeved ruffle skirt blouse, two full-length skirts and a petticoat. Girls wear "tembleques" in their hair. Gold coins and jewelry are added to the outfit. The "pollera montuna" is a daily dress, with a blouse, a skirt with a solid color, a single gold chain, and pendant earrings and a natural flower in the hair. Instead of an off-the-shoulder blouse is a fitted white jacket with, shoulder pleats, and a flared hem.
Traditional clothing in Panama can be worn in parades, where the females and males do a traditional dance. Females do a gentle sway and twirl their skirts, while the men hold their hats in their hands and dance behind the females.
Literature.
According to Professor Rodrigo Miró, the first story about Panama was written by Gonzalo Fernández de Oviedo y Valdés and published as part of the "Historia General y Natural de Las Indias" in 1535. Some poets and novelists born in Panamá are Manuel María Ayala (1785–1824), Amelia Denis de Icaza (1836–1911), Darío Herrera (1870–1914), Ricardo Miró (1883–1940), Gaspar Octavio Hernández (1893–1918), Demetrio Korsi (1899–1957), Ricardo Bermúdez (1914–2000), Mario Augusto Rodriguez (1917–2008), José María Sánchez (1918–1973), Ramón H. Jurado (1922–1978), Carlos Francisco Changmarín (1922– ), Joaquín Beleño (1922–1988), Tristán Solarte (1924– ), Pedro Rivera (1939– ), Moravia Ochoa López (1941– ), Gloria Guardia (1940– ), Dimas Lidio Pitty (1941– ), Roberto Fernández Iglesias (1941– ), Jarl Ricardo Babot (1946– ), Manuel Orestes Nieto (1951– ), Moisés Pascual (1955– ), Héctor Miguel Collado (1960– ), David Robinson Orobio (1960– ), Katia Chiari (1969– ), Carlos Oriel Wynter Melo (1971– ), José Luis Rodríguez Pittí (1971– ) and Sofía Santim (1982– ).
Sports.
The U.S. influence in Panama can be seen in the country's sports. Baseball is Panama's national sport and the country has regional teams and a national team that represents it in international events. At least 140 Panamanian players have played professional baseball in the United States, more than any other Central American country. Notable players include Bruce Chen, Rod Carew, Mariano Rivera, Carlos Lee, Manny Sanguillén, and Carlos Ruiz.
In boxing, four Panamanians are in the International Boxing Hall of Fame: Roberto Durán, Eusebio Pedroza, Ismael Laguna and Panama Al Brown. Panama presently has two reigning world boxing champions: Guillermo Jones and Anselmo Moreno.
Since finals of 20th century, the Football is becoming a popular sport for the panamanians, the progress of the national league and the national team are notorious, with legendary players as Luis Ernesto Tapia, Rommel Fernández, the Dely Valdes Brothers: Armando, Julio and Jorge; and recent players as Jaime Penedo, Felipe Baloy, Luis Tejada, Blas Perez, Roman Torres and Harold Cummings.
Basketball is popular in Panama. There are regional teams as well as a squad that competes internationally. Two of Panama's prominent basketball players are Rolando Blackman, a four-time NBA All-Star, and Kevin Daley, a 10-year captain and showman of the Harlem Globetrotters.
Other popular sports include volleyball, taekwondo, golf, and tennis. A long-distance hiking trail called the TransPanama Trail is being built from Colombia to Costa Rica.
Other non-traditional sports in the country have had great importance such as the triathlon that has captured the attention of many athletes nationwide and the country has hosted international competitions. Flag football has also been growing in popularity in both men and women and with international participation in world of this discipline being among the best teams in the world, the sport was introduced by Americans residing in the Canal Zone for veterans and retirees who even had a festival called the Turkey Ball. Other popular sports are American football, rugby, hockey, softball and other amateur sports including skateboarding, BMX and surfing, because the many beaches of Panama such as Santa Catalina and Venao that have hosted events the likes of ISA World Surfing Games.
Long jumper Irving Saladino became the first Panamanian Olympic gold medalist in 2008. In 2012 eight different athletes represented Panama in the London 2012 Olympics: Irving Saladino in the long jump, Alonso Edward and Andrea Ferris in track and field, Diego Castillo in swimming, and the youngest on the team, Carolena Carstens who was 16 competing in taekwondo. She was the first representative to compete for Panama in that sport.
Nature & Environment.
Climate Change.
Panama was one of the few countries that didn't enter an INDC at COP21.

</doc>
<doc id="23000" url="https://en.wikipedia.org/wiki?curid=23000" title="Polynomial">
Polynomial

In mathematics, a polynomial is an expression consisting of variables and coefficients which only employs the operations of addition, subtraction, multiplication, and non-negative integer exponents. An example of a polynomial of a single variable is . An example in three variables is .
Polynomials appear in a wide variety of areas of mathematics and science. For example, they are used to form polynomial equations, which encode a wide range of problems, from elementary word problems to complicated problems in the sciences; they are used to define polynomial functions, which appear in settings ranging from basic chemistry and physics to economics and social science; they are used in calculus and numerical analysis to approximate other functions. In advanced mathematics, polynomials are used to construct polynomial rings and algebraic varieties, central concepts in algebra and algebraic geometry.
Etymology.
The word "polynomial" joins two diverse roots, the Greek "poly", meaning "many," and the Latin "nomen", or name. It was derived from the term "binomial" by replacing the Latin root "bi-" with the Greek "poly-". The word "polynomial" was first used in the 17th century.
Notation and terminology.
The terms variable, indeterminate, or "unknown" are often used interchangeably, though they refer to slightly different aspects of unknown values.
Definition.
A polynomial is an expression that can be built from constants and symbols called indeterminates or variables by means of addition, multiplication and exponentiation to a non-negative power. Two such expressions that may be transformed, one to the other, by applying the usual properties of commutativity, associativity and distributivity of addition and multiplication are considered as defining the same polynomial.
A polynomial in a single indeterminate "x" can always be written (or rewritten) in the form
where formula_2 are constants and formula_3 is
the indeterminate. The word "indeterminate" means that formula_3 does not represent any value, although any value may be substituted for it. The mapping that associates the result of this substitution to the substituted value is a function, called a "polynomial function".
This can be expressed more concisely by using summation notation:
That is, a polynomial can either be zero or can be written as the sum of a finite number of non-zero terms. Each term consists of the product of a number—called the coefficient of the term—and a finite number of indeterminates, raised to nonnegative integer powers. The exponent on an indeterminate in a term is called the degree of that indeterminate in that term; the degree of the term is the sum of the degrees of the indeterminates in that term, and the degree of a polynomial is the largest degree of any one term with nonzero coefficient. Because , the degree of an indeterminate without a written exponent is one. A term and a polynomial with no indeterminates are called respectively a constant term and a constant polynomial; the degree of a constant term and of a nonzero constant polynomial is 0. The degree of the zero polynomial (which has no term) is generally treated as not defined (but see below).
For example:
is a term. The coefficient is , the indeterminates are and , the degree of is two, while the degree of is one. The degree of the entire term is the sum of the degrees of each indeterminate in it, so in this example the degree is .
Forming a sum of several terms produces a polynomial. For example, the following is a polynomial:
It consists of three terms: the first is degree two, the second is degree one, and the third is degree zero.
Polynomials of small degree have been given specific names. A polynomial of degree zero is a "constant polynomial" or simply a "constant". Polynomials of degree one, two or three are respectively "linear polynomials," "quadratic polynomials" and "cubic polynomials". For higher degrees the specific names are not commonly used, although "quartic polynomial" (for degree four) and "quintic polynomial" (for degree five) are sometimes used. The names for the degrees may be applied to the polynomial or to its terms. For example, in the term is a linear term in a quadratic polynomial.
The polynomial 0, which may be considered to have no terms at all, is called the zero polynomial. Unlike other constant polynomials, its degree is not zero. Rather the degree of the zero polynomial is either left explicitly undefined, or defined as negative (either −1 or −∞). These conventions are useful when defining Euclidean division of polynomials. The zero polynomial is also unique in that it is the only polynomial having an infinite number of roots. In the case of polynomials in more than one indeterminate, a polynomial is called "homogeneous" of if "all" its non-zero terms have . The zero polynomial is homogeneous, and, as homogeneous polynomial, its degree is undefined. For example, is homogeneous of degree 5. For more details, see homogeneous polynomial.
The commutative law of addition can be used to rearrange terms into any preferred order. In polynomials with one indeterminate, the terms are usually ordered according to degree, either in "descending powers of ", with the term of largest degree first, or in "ascending powers of ". The polynomial in the example above is written in descending powers of . The first term has coefficient , indeterminate , and exponent . In the second term, the coefficient . The third term is a constant. Because the "degree" of a non-zero polynomial is the largest degree of any one term, this polynomial has degree two.
Two terms with the same indeterminates raised to the same powers are called "similar terms" or "like terms", and they can be combined, using the distributive law, into a single term whose coefficient is the sum of the coefficients of the terms that were combined. It may happen that this makes the coefficient 0. Polynomials can be classified by the number of terms with nonzero coefficients, so that a one-term polynomial is called a monomial, a two-term polynomial is called a binomial, and a three-term polynomial is called a "trinomial". The term "quadrinomial" is occasionally used for a four-term polynomial.
A polynomial in one indeterminate is called a "univariate polynomial", a polynomial in more than one indeterminate is called a multivariate polynomial. A polynomial with two indeterminates is called a bivariate polynomial. These notions refer more to the kind of polynomials one is generally working with than to individual polynomials; for instance when working with univariate polynomials one does not exclude constant polynomials (which may result, for instance, from the subtraction of non-constant polynomials), although strictly speaking constant polynomials do not contain any indeterminates at all. It is possible to further classify multivariate polynomials as "bivariate", "trivariate", and so on, according to the maximum number of indeterminates allowed. Again, so that the set of objects under consideration be closed under subtraction, a study of trivariate polynomials usually allows bivariate polynomials, and so on. It is common, also, to say simply "polynomials in , and ", listing the indeterminates allowed.
The "evaluation of a polynomial" consists of substituting a numerical value to each indeterminate and carrying out the indicated multiplications and additions. For polynomials in one indeterminate, the evaluation is usually more efficient (lower number of arithmetic operations to perform) using Horner's method:
Arithmetic.
Polynomials can be added using the associative law of addition (grouping all their terms together into a single sum), possibly followed by reordering, and combining of like terms. For example, if
then
which can be simplified to
To work out the product of two polynomials into a sum of terms, the distributive law is repeatedly applied, which results in each term of one polynomial being multiplied by every term of the other. For example, if
then
which can be simplified to
Polynomial evaluation can be used to compute the remainder of polynomial division by a polynomial of degree one, because the remainder of the division of by is ; see the polynomial remainder theorem. This is more efficient than the usual algorithm of division when the quotient is not needed.
As for the integers, two kinds of divisions are considered for the polynomials. The "Euclidean division of polynomials" that generalizes the Euclidean division of the integers. It results in two polynomials, a "quotient" and a "remainder" that are characterized by the following property of the polynomials: given two polynomials "a" and "b" such that "b" ≠ 0, there exists a unique pair of polynomials, "q", the quotient, and "r", the remainder, such that and (here the polynomial zero is supposed to have a negative degree). By hand as well as with a computer, this division can be computed by the polynomial long division algorithm.
All polynomials with coefficients in a unique factorization domain (for example, the integers or a field) also have a factored form in which the polynomial is written as a product of irreducible polynomials and a constant. This factored form is unique up to the order of the factors and their multiplication by an invertible constant. In the case of the field of complex numbers, the irreducible factors are linear. Over the real numbers, they have the degree either one or two. Over the integers and the rational numbers the irreducible factors may have any degree. For example, the factored form of
is
over the integers and the reals and
over the complex numbers.
The computation of the factored form, called "factorization" is, in general, too difficult to be done by hand-written computation. However, efficient polynomial factorization algorithms are available in most computer algebra systems.
A formal quotient of polynomials, that is, an algebraic fraction wherein the numerator and denominator are polynomials, is called a "rational expression" or "rational fraction" and is not, in general, a polynomial. Division of a polynomial by a number, however, yields another polynomial. For example, is considered a valid term in a polynomial (and a polynomial by itself) because it is equivalent to and is just a constant. When this expression is used as a term, its coefficient is therefore . For similar reasons, if complex coefficients are allowed, one may have a single term like ; even though it looks like it should be expanded to two terms, the complex number is one complex number, and is the coefficient of that term. The expression is not a polynomial because it includes division by a non-constant polynomial. The expression is not a polynomial, because it contains an indeterminate used as exponent.
Because subtraction can be replaced by addition of the opposite quantity, and because positive integer exponents can be replaced by repeated multiplication, all polynomials can be constructed from constants and indeterminates using only addition and multiplication.
Polynomial functions.
A "polynomial function" is a function that can be defined by evaluating a polynomial. A function of one argument is thus a polynomial function if it satisfies.
for all arguments , where is a non-negative integer and are constant coefficients.
For example, the function , taking real numbers to real numbers, defined by
is a polynomial function of one variable. Polynomial functions of multiple variables are similarly defined, using polynomials in multiple indeterminates, as in
An example is also the function formula_21 which, although it doesn't look like a polynomial, is a polynomial function on formula_22 because for every formula_3 from formula_22 it is true that formula_25 (see Chebyshev polynomials).
Polynomial functions are a class of functions having many important properties. They are all continuous, smooth, entire, computable, etc.
Graphs.
A polynomial function in one real variable can be represented by a graph.
The graph of a non-constant (univariate) polynomial always tends to infinity when the variable increases indefinitely (in absolute value).
Polynomial graphs are analyzed in calculus using intercepts, slopes, concavity, and end behavior.
Equations.
A "polynomial equation", also called "algebraic equation", is an equation of the form
For example,
is a polynomial equation.
In case of a univariate polynomial equation, the variable is considered an unknown, and one seeks to find the possible values for which both members of the equation evaluate to the same value (in general more than one solution may exist). A polynomial equation stands in contrast to a "polynomial identity" like , where both expressions represent the same polynomial in different forms, and as a consequence any evaluation of both members gives a valid equality.
In elementary algebra, methods such as the quadratic formula are given for solving all first degree and second degree polynomial equations in one variable. There are also formulas for the cubic and quartic equations. For higher degrees, the Abel–Ruffini theorem asserts that there can not exist a general formula in radicals. However, root-finding algorithms may be used to find numerical approximations of the roots of a polynomial expression of any degree.
The number of real solutions of a polynomial equation with real coefficients may not exceed the degree, and equals the degree when the complex solutions are counted with their multiplicity. This fact is called the fundamental theorem of algebra.
Solving equations.
Every polynomial in corresponds to a function, (where the occurrences of in are interpreted as the argument of ), called the "polynomial function" of ; the equation in setting is the "polynomial equation" corresponding to . The solutions of this equation are called the "roots" of the polynomial; they are the "zeroes" of the function (corresponding to the points where the graph of meets the -axis). A number is a root of if and only if the polynomial (of degree one in ) divides . It may happen that divides more than once: if divides then is called a "multiple root" of , and otherwise is called a "simple root" of . If is a nonzero polynomial, there is a highest power such that divides , which is called the "multiplicity" of the root in . When is the zero polynomial, the corresponding polynomial equation is trivial, and this case is usually excluded when considering roots: with the above definitions every number would be a root of the zero polynomial, with undefined (or infinite) multiplicity. With this exception made, the number of roots of , even counted with their respective multiplicities, cannot exceed the degree of . The relation between the roots of a polynomial and its coefficients is described by Vieta's formulas.
Some polynomials, such as , do not have any roots among the real numbers. If, however, the set of allowed candidates is expanded to the complex numbers, every non-constant polynomial has at least one root; this is the fundamental theorem of algebra. By successively dividing out factors , one sees that any polynomial with complex coefficients can be written as a constant (its leading coefficient) times a product of such polynomial factors of degree 1; as a consequence, the number of (complex) roots counted with their multiplicities is exactly equal to the degree of the polynomial.
There is a difference between approximating roots and finding exact expressions for roots. Formulas for expressing the roots of polynomials of degree 2 in terms of square roots have been known since ancient times (see quadratic equation), and for polynomials of degree 3 or 4 similar formulas (using cube roots in addition to square roots) were found in the 16th century (see cubic function and quartic function for the formulas and Niccolò Fontana Tartaglia, Lodovico Ferrari, Gerolamo Cardano, and François Viète for historical details). But formulas for degree 5 eluded researchers for several centuries. In 1824, Niels Henrik Abel proved the striking result that there can be no general (finite) formula, involving only arithmetic operations and radicals, that expresses the roots of a polynomial of degree 5 or greater in terms of its coefficients (see Abel–Ruffini theorem). In 1830, Évariste Galois, studying the permutations of the roots of a polynomial, extended the Abel–Ruffini theorem by showing that, given a polynomial equation, one may decide whether it is solvable by radicals, and, if it is, solve it. This result marked the start of Galois theory and group theory, two important branches of modern mathematics. Galois himself noted that the computations implied by his method were impracticable. Nevertheless, formulas for solvable equations of degrees 5 and 6 have been published (see quintic function and sextic equation).
Numerical approximation of roots of polynomials in one unknown is easily done on a computer by the Jenkins–Traub method, Laguerre's method, Durand–Kerner method, or by some other root-finding algorithm.
For polynomials in more than one indeterminate the notion of root does not exist, and there are usually infinitely many combinations of values for the variables for which the polynomial function takes the value zero. However, for certain "sets" of such polynomials it may happen that for only finitely many combinations all polynomial functions take the value zero.
For a set of polynomial equations in several unknowns, there are algorithms to decide whether they have a finite number of complex solutions. If the number of solutions is finite, there are algorithms to compute the solutions. The methods underlying these algorithms are described in the article System of polynomial equations.
The special case where all the polynomials are of degree one is called a system of linear equations, for which another range of different solution methods exist, including the classical Gaussian elimination.
A polynomial equation for which one is interested only in the solutions which are integers is called a Diophantine equation. Solving Diophantine equations is a very hard task. It has been proved that there cannot be any general algorithm for solving them, and even for deciding is the set of solutions is empty (see Hilbert's tenth problem). Some of the most famous problems that has been solved during the fifty last years are related to Diophantine equations, such as Fermat's Last Theorem.
Generalizations.
There are at least two ways to generalize polynomials:
Trigonometric polynomials.
A trigonometric polynomial is a finite linear combination of functions sin("nx") and cos("nx") with "n" taking on the values of one or more natural numbers. The coefficients may be taken as real numbers, for real-valued functions. For complex coefficients, there is no difference between such a function and a finite Fourier series.
Trigonometric polynomials are widely used, for example in trigonometric interpolation applied to the interpolation of periodic functions. They are used also in the discrete Fourier transform.
The term "trigonometric polynomial" for the real-valued case can be seen as using the analogy: the functions sin("nx") and cos("nx") are similar to the monomial basis for polynomials. In the complex case the trigonometric polynomials are spanned by the positive and negative powers of "e""ix".
Matrix polynomials.
A matrix polynomial is a polynomial with matrices as variables. Given an ordinary, scalar-valued polynomial
this polynomial evaluated at a matrix "A" is
where "I" is the identity matrix.
A matrix polynomial equation is an equality between two matrix polynomials, which holds for the specific matrices in question. A matrix polynomial identity is a matrix polynomial equation which holds for all matrices "A" in a specified matrix ring "Mn"("R").
Laurent polynomials.
Laurent polynomials are like polynomials, but allow negative powers of the variable(s) to occur.
Rational functions.
A rational fraction is the quotient (algebraic fraction) of two polynomials. Any algebraic expression that can be rewritten as a rational fraction is a rational function.
While polynomial functions are defined for all values of the variables, a rational function is defined only for the values of the variables for which the denominator is not zero.
The rational fractions include the Laurent polynomials, but do not limit denominators to powers of an indeterminate.
Power series.
Formal power series are like polynomials, but allow infinitely many non-zero terms to occur, so that they do not have finite degree. Unlike polynomials they cannot in general be explicitly and fully written down (just like irrational numbers cannot), but the rules for manipulating their terms are the same as for polynomials. Non-formal power series also generalize polynomials, but the multiplication of two power series may not converge.
Applications.
Calculus.
The simple structure of polynomial functions makes them quite useful in analyzing general functions using polynomial approximations. An important example in calculus is Taylor's theorem, which roughly states that every differentiable function locally looks like a polynomial function, and the Stone–Weierstrass theorem, which states that every continuous function defined on a compact interval of the real axis can be approximated on the whole interval as closely as desired by a polynomial function.
Calculating derivatives and integrals of polynomial functions is particularly simple. For the polynomial function
the derivative with respect to "x" is
and the indefinite integral is
Abstract algebra.
In abstract algebra, one distinguishes between "polynomials" and "polynomial functions". A "polynomial" in one indeterminate over a ring is defined as a formal expression of the form
where is a natural number, the coefficients are elements of , and is a formal symbol, whose powers are just placeholders for the corresponding coefficients , so that the given formal expression is just a way to encode the sequence , where there is an such that for all . Two polynomials sharing the same value of "n" are considered equal if and only if the sequences of their coefficients are equal; furthermore any polynomial is equal to any polynomial with greater value of obtained from it by adding terms in front whose coefficient is zero. These polynomials can be added by simply adding corresponding coefficients (the rule for extending by terms with zero coefficients can be used to make sure such coefficients exist). Thus each polynomial is actually equal to the sum of the terms used in its formal expression, if such a term is interpreted as a polynomial that has zero coefficients at all powers of other than . Then to define multiplication, it suffices by the distributive law to describe the product of any two such terms, which is given by the rule
Thus the set of all polynomials with coefficients in the ring forms itself a ring, the "ring of polynomials" over , which is denoted by . The map from to sending to is an injective homomorphism of rings, by which is viewed as a subring of . If is commutative, then is an algebra over .
One can think of the ring as arising from by adding one new element "x" to "R", and extending in a minimal way to a ring in which satisfies no other relations than the obligatory ones, plus commutation with all elements of (that is ). To do this, one must add all powers of and their linear combinations as well.
Formation of the polynomial ring, together with forming factor rings by factoring out ideals, are important tools for constructing new rings out of known ones. For instance, the ring (in fact field) of complex numbers, which can be constructed from the polynomial ring over the real numbers by factoring out the ideal of multiples of the polynomial . Another example is the construction of finite fields, which proceeds similarly, starting out with the field of integers modulo some prime number as the coefficient ring (see modular arithmetic).
If is commutative, then one can associate to every polynomial in , a "polynomial function" with domain and range equal to (more generally one can take domain and range to be the same unital associative algebra over ). One obtains the value by substitution of the value for the symbol in . One reason to distinguish between polynomials and polynomial functions is that over some rings different polynomials may give rise to the same polynomial function (see Fermat's little theorem for an example where is the integers modulo ). This is not the case when is the real or complex numbers, whence the two concepts are not always distinguished in analysis. An even more important reason to distinguish between polynomials and polynomial functions is that many operations on polynomials (like Euclidean division) require looking at what a polynomial is composed of as an expression rather than evaluating it at some constant value for .
Divisibility.
In commutative algebra, one major focus of study is "divisibility" among polynomials. If is an integral domain and and are polynomials in , it is said that "divides" or is a divisor of if there exists a polynomial in such that . One can show that every zero gives rise to a linear divisor, or more formally, if is a polynomial in and is an element of such that , then the polynomial () divides . The converse is also true. The quotient can be computed using the polynomial long division.
If is a field and and are polynomials in with , then there exist unique polynomials and in with
and such that the degree of is smaller than the degree of (using the convention that the polynomial 0 has a negative degree). The polynomials and are uniquely determined by and . This is called "Euclidean division, division with remainder" or "polynomial long division" and shows that the ring is a Euclidean domain.
Analogously, "prime polynomials" (more correctly, "irreducible polynomials") can be defined as "non-zero polynomials which cannot be factorized into the product of two non constant polynomials". In the case of coefficients in a ring, ""non constant"" must be replaced by ""non constant or non unit"" (both definitions agree in the case of coefficients in a field). Any polynomial may be decomposed into the product of an invertible constant by a product of irreducible polynomials. If the coefficients belong to a field or a unique factorization domain this decomposition is unique up to the order of the factors and the multiplication of any non unit factor by a unit (and division of the unit factor by the same unit). When the coefficients belong to integers, rational numbers or a finite field, there are algorithms to test irreducibility and to compute the factorization into irreducible polynomials (see Factorization of polynomials). These algorithms are not practicable for hand written computation, but are available in any computer algebra system. Eisenstein's criterion can also be used in some cases to determine irreducibility.
Other applications.
Polynomials serve to approximate other functions, such as the use of splines.
Polynomials are frequently used to encode information about some other object. The characteristic polynomial of a matrix or linear operator contains information about the operator's eigenvalues. The minimal polynomial of an algebraic element records the simplest algebraic relation satisfied by that element. The chromatic polynomial of a graph counts the number of proper colourings of that graph.
The term "polynomial", as an adjective, can also be used for quantities or functions that can be written in polynomial form. For example, in computational complexity theory the phrase "polynomial time" means that the time it takes to complete an algorithm is bounded by a polynomial function of some variable, such as the size of the input.
History.
Determining the roots of polynomials, or "solving algebraic equations", is among the oldest problems in mathematics. However, the elegant and practical notation we use today only developed beginning in the 15th century. Before that, equations were written out in words. For example, an algebra problem from the Chinese Arithmetic in Nine Sections, circa 200 BCE, begins "Three sheafs of good crop, two sheafs of mediocre crop, and one sheaf of bad crop are sold for 29 dou." We would write .
History of the notation.
The earliest known use of the equal sign is in Robert Recorde's "The Whetstone of Witte", 1557. The signs + for addition, − for subtraction, and the use of a letter for an unknown appear in Michael Stifel's "Arithemetica integra", 1544. René Descartes, in "La géometrie", 1637, introduced the concept of the graph of a polynomial equation. He popularized the use of letters from the beginning of the alphabet to denote constants and letters from the end of the alphabet to denote variables, as can be seen above, in the general formula for a polynomial in one variable, where the 's denote constants and denotes a variable. Descartes introduced the use of superscripts to denote exponents as well.

</doc>
<doc id="23001" url="https://en.wikipedia.org/wiki?curid=23001" title="Polymer">
Polymer

A polymer (; Greek "poly-", "many" + "-mer", "parts") is a large molecule, or macromolecule, composed of many repeated subunits. Because of their broad range of properties, both synthetic and natural polymers play an essential and ubiquitous role in everyday life. Polymers range from familiar synthetic plastics such as polystyrene to natural biopolymers such as DNA and proteins that are fundamental to biological structure and function. Polymers, both natural and synthetic, are created via polymerization of many small molecules, known as monomers. Their consequently large molecular mass relative to small molecule compounds produces unique physical properties, including toughness, viscoelasticity, and a tendency to form glasses and semicrystalline structures rather than crystals.
The term "polymer" derives from the ancient Greek word πολύς ("polus", meaning "many, much") and μέρος ("meros", meaning "parts"), and refers to a molecule whose structure is composed of multiple repeating units, from which originates a characteristic of high relative molecular mass and attendant properties. The units composing polymers derive, actually or conceptually, from molecules of low relative molecular mass. The term was coined in 1833 by Jöns Jacob Berzelius, though with a definition distinct from the modern IUPAC definition. The modern concept of polymers as covalently bonded macromolecular structures was proposed in 1920 by Hermann Staudinger, who spent the next decade finding experimental evidence for this hypothesis.
Polymers are studied in the fields of biophysics and macromolecular science, and polymer science (which includes polymer chemistry and polymer physics). Historically, products arising from the linkage of repeating units by covalent chemical bonds have been the primary focus of polymer science; emerging important areas of the science now focus on non-covalent links. Polyisoprene of latex rubber and the polystyrene of styrofoam are examples of polymeric natural/biological and synthetic polymers, respectively. In biological contexts, essentially all biological macromolecules—i.e., proteins (polyamides), nucleic acids (polynucleotides), and polysaccharides—are purely polymeric, or are composed in large part of polymeric components—e.g., isoprenylated/lipid-modified glycoproteins, where small lipidic molecules and oligosaccharide modifications occur on the polyamide backbone of the protein.
The simplest theoretical models for polymers are ideal chains.
Common examples.
Polymers are of two types:
Most commonly, the continuously linked backbone of a polymer used for the preparation of plastics consists mainly of carbon atoms. A simple example is polyethylene ('polythene' in British English), whose repeating unit is based on ethylene monomer. However, other structures do exist; for example, elements such as silicon form familiar materials such as silicones, examples being Silly Putty and waterproof plumbing sealant. Oxygen is also commonly present in polymer backbones, such as those of polyethylene glycol, polysaccharides (in glycosidic bonds), and DNA (in phosphodiester bonds).
Polymer synthesis.
Polymerization is the process of combining many small molecules known as monomers into a covalently bonded chain or network. During the polymerization process, some chemical groups may be lost from each monomer. This is the case, for example, in the polymerization of PET polyester. The monomers are terephthalic acid (HOOC-C6H4-COOH) and ethylene glycol (HO-CH2- CH2-OH) but the repeating unit is -OC-C6H4-COO-CH2-CH2-O-, which corresponds to the combination of the two monomers with the loss of two water molecules. The distinct piece of each monomer that is incorporated into the polymer is known as a repeat unit or monomer residue.
Laboratory synthetic methods are generally divided into two categories, step-growth polymerization and chain-growth polymerization. The essential difference between the two is that in chain growth polymerization, monomers are added to the chain one at a time only, such as in polyethylene, whereas in step-growth polymerization chains of monomers may combine with one another directly, such as in polyester. However, some newer methods such as plasma polymerization do not fit neatly into either category. Synthetic polymerization reactions may be carried out with or without a catalyst. Laboratory synthesis of biopolymers, especially of proteins, is an area of intensive research.
Biological synthesis.
There are three main classes of biopolymers: polysaccharides, polypeptides, and polynucleotides.
In living cells, they may be synthesized by enzyme-mediated processes, such as the formation of DNA catalyzed by DNA polymerase. The synthesis of proteins involves multiple enzyme-mediated processes to transcribe genetic information from the DNA to RNA and subsequently translate that information to synthesize the specified protein from amino acids. The protein may be modified further following translation in order to provide appropriate structure and functioning. There are other biopolymers such as rubber, suberin, melanin and lignin.
Modification of natural polymers.
Naturally occurring polymers such as cotton, starch and rubber were familiar materials for years before synthetic polymers such as polyethene and perspex appeared on the market. 
Many commercially important polymers are synthesized by chemical modification of naturally occurring polymers. Prominent examples include the reaction of nitric acid and cellulose to form nitrocellulose and the formation of vulcanized rubber by heating natural rubber in the presence of sulfur.
Ways in which polymers can be modified include oxidation, cross-linking and end-capping.
Especially in the production of polymers, the gas separation by membranes has acquired increasing importance in the petrochemical industry and is now a relatively well-established unit operation.
The process of polymer degassing is necessary to suit polymer for extrusion and pelletizing, increasing safety, environmental, and product quality aspects. Nitrogen is generally used for this purpose, resulting in a vent gas primarily composed of monomers and nitrogen.
Polymer properties.
Polymer properties are broadly divided into several classes based on the scale at which the property is defined as well as upon its physical basis. The most basic property of a polymer is the identity of its constituent monomers. A second set of properties, known as microstructure, essentially describe the arrangement of these monomers within the polymer at the scale of a single chain. These basic structural properties play a major role in determining bulk physical properties of the polymer, which describe how the polymer behaves as a continuous macroscopic material. Chemical properties, at the nano-scale, describe how the chains interact through various physical forces. At the macro-scale, they describe how the bulk polymer interacts with other chemicals and solvents.
Monomers and repeat units.
The identity of the repeat units (monomer residues, also known as "mers") comprising a polymer is its first and most important attribute. Polymer nomenclature is generally based upon the type of monomer residues comprising the polymer. Polymers that contain only a single type of repeat unit are known as homopolymers, while polymers containing a mixture of repeat units are known as copolymers. Poly(styrene), for example, is composed only of styrene monomer residues, and is therefore classified as a homopolymer. Ethylene-vinyl acetate, on the other hand, contains more than one variety of repeat unit and is thus a copolymer. Some biological polymers are composed of a variety of different but structurally related monomer residues; for example, polynucleotides such as DNA are composed of a variety of nucleotide subunits.
A polymer molecule containing ionizable subunits is known as a polyelectrolyte or ionomer.
Microstructure.
The microstructure of a polymer (sometimes called configuration) relates to the physical arrangement of monomer residues along the backbone of the chain. These are the elements of polymer structure that require the breaking of a covalent bond in order to change. Structure has a strong influence on the other properties of a polymer. For example, two samples of natural rubber may exhibit different durability, even though their molecules comprise the same monomers.
Polymer architecture.
An important microstructural feature of a polymer is its architecture and shape, which relates to the way branch points lead to a deviation from a simple linear chain. A branched polymer molecule is composed of a main chain with one or more substituent side chains or branches. Types of branched polymers include star polymers, comb polymers, brush polymers, dendronized polymers, "ladders", and dendrimers. There exist also two-dimensional polymers which are composed of topologically planar repeat units. A polymer's architecture affects many of its physical properties including, but not limited to, solution viscosity, melt viscosity, solubility in various solvents, glass transition temperature and the size of individual polymer coils in solution. A variety of techniques may be employed for the synthesis of a polymeric material with a range of architectures, for example Living polymerization.
Chain length.
The physical properties of a polymer are strongly dependent on the size or length of the polymer chain. For example, as chain length is increased, melting and boiling temperatures increase quickly. Impact resistance also tends to increase with chain length, as does the viscosity, or resistance to flow, of the polymer in its molten state. Melt viscosity formula_1 is related to polymer chain length Z roughly as formula_1 ~ Z3.2, so that a tenfold increase in polymer chain length results in a viscosity increase of over 1000 times. Increasing chain length furthermore tends to decrease chain mobility, increase strength and toughness, and increase the glass transition temperature (Tg). This is a result of the increase in chain interactions such as Van der Waals attractions and entanglements that come with increased chain length. These interactions tend to fix the individual chains more strongly in position and resist deformations and matrix breakup, both at higher stresses and higher temperatures.
A common means of expressing the length of a chain is the degree of polymerization, which quantifies the number of monomers incorporated into the chain. As with other molecules, a polymer's size may also be expressed in terms of molecular weight. Since synthetic polymerization techniques typically yield a polymer product including a range of molecular weights, the weight is often expressed statistically to describe the distribution of chain lengths present in the same. Common examples are the number average molecular weight and weight average molecular weight. The ratio of these two values is the polydispersity index, commonly used to express the "width" of the molecular weight distribution. A final measurement is contour length, which can be understood as the length of the chain backbone in its fully extended state.
The flexibility of an unbranched chain polymer is characterized by its persistence length.
Monomer arrangement in copolymers.
Monomers within a copolymer may be organized along the backbone in a variety of ways.
Tacticity.
Tacticity describes the relative stereochemistry of chiral centers in neighboring structural units within a macromolecule. There are three types: isotactic (all substituents on the same side), atactic (random placement of substituents), and syndiotactic (alternating placement of substituents).
Polymer morphology.
Polymer morphology generally describes the arrangement and microscale ordering of polymer chains in space.
Crystallinity.
When applied to polymers, the term "crystalline" has a somewhat ambiguous usage. In some cases, the term "crystalline" finds identical usage to that used in conventional crystallography. For example, the structure of a crystalline protein or polynucleotide, such as a sample prepared for x-ray crystallography, may be defined in terms of a conventional unit cell composed of one or more polymer molecules with cell dimensions of hundreds of angstroms or more.
A synthetic polymer may be loosely described as crystalline if it contains regions of three-dimensional ordering on atomic (rather than macromolecular) length scales, usually arising from intramolecular folding and/or stacking of adjacent chains. Synthetic polymers may consist of both crystalline and amorphous regions; the degree of crystallinity may be expressed in terms of a weight fraction or volume fraction of crystalline material. Few synthetic polymers are entirely crystalline.
The crystallinity of polymers is characterized by their degree of crystallinity, ranging from zero for a completely non-crystalline polymer to one for a theoretical completely crystalline polymer. 
Polymers with microcrystalline regions are generally tougher (can be bent more without breaking) and more impact-resistant than totally amorphous polymers.
Polymers with a degree of crystallinity approaching zero or one will tend to be transparent, while polymers with intermediate degrees of crystallinity will tend to be opaque due to light scattering by crystalline or glassy regions. Thus for many polymers, reduced crystallinity may also be associated with increased transparency.
Chain conformation.
The space occupied by a polymer molecule is generally expressed in terms of radius of gyration, which is an average distance from the center of mass of the chain to the chain itself. Alternatively, it may be expressed in terms of pervaded volume, which is the volume of solution spanned by the polymer chain and scales with the cube of the radius of gyration.
Mechanical properties.
The bulk properties of a polymer are those most often of end-use interest. These are the properties that dictate how the polymer actually behaves on a macroscopic scale.
Tensile strength.
The tensile strength of a material quantifies how much elongating stress the material will endure before failure. This is very important in applications that rely upon a polymer's physical strength or durability. For example, a rubber band with a higher tensile strength will hold a greater weight before snapping. In general, tensile strength increases with polymer chain length and crosslinking of polymer chains.
Young's modulus of elasticity.
Young's Modulus quantifies the elasticity of the polymer. It is defined, for small strains, as the ratio of rate of change of stress to strain. Like tensile strength, this is highly relevant in polymer applications involving the physical properties of polymers, such as rubber bands. The modulus is strongly dependent on temperature. Viscoelasticity describes a complex time-dependent elastic response, which will exhibit hysteresis in the stress-strain curve when the load is removed. Dynamic mechanical analysis or DMA measures this complex modulus by oscillating the load and measuring the resulting strain as a function of time.
Transport properties.
Transport properties such as diffusivity relate to how rapidly molecules move through the polymer matrix. These are very important in many applications of polymers for films and membranes.
Phase behavior.
Melting point.
The term "melting point", when applied to polymers, suggests not a solid–liquid phase transition but a transition from a crystalline or semi-crystalline phase to a solid amorphous phase. Though abbreviated as simply "Tm", the property in question is more properly called the crystalline melting temperature. Among synthetic polymers, crystalline melting is only discussed with regards to thermoplastics, as thermosetting polymers will decompose at high temperatures rather than melt.
Glass transition temperature.
A parameter of particular interest in synthetic polymer manufacturing is the glass transition temperature (Tg), at which amorphous polymers undergo a transition from a rubbery, viscous liquid, to a brittle, glassy amorphous solid on cooling. The glass transition temperature may be engineered by altering the degree of branching or crosslinking in the polymer or by the addition of plasticizer.
Mixing behavior.
In general, polymeric mixtures are far less miscible than mixtures of small molecule materials. This effect results from the fact that the driving force for mixing is usually entropy, not interaction energy. In other words, miscible materials usually form a solution not because their interaction with each other is more favorable than their self-interaction, but because of an increase in entropy and hence free energy associated with increasing the amount of volume available to each component. This increase in entropy scales with the number of particles (or moles) being mixed. Since polymeric molecules are much larger and hence generally have much higher specific volumes than small molecules, the number of molecules involved in a polymeric mixture is far smaller than the number in a small molecule mixture of equal volume. The energetics of mixing, on the other hand, is comparable on a per volume basis for polymeric and small molecule mixtures. This tends to increase the free energy of mixing for polymer solutions and thus make solvation less favorable. Thus, concentrated solutions of polymers are far rarer than those of small molecules.
Furthermore, the phase behavior of polymer solutions and mixtures is more complex than that of small molecule mixtures. Whereas most small molecule solutions exhibit only an upper critical solution temperature phase transition, at which phase separation occurs with cooling, polymer mixtures commonly exhibit a lower critical solution temperature phase transition, at which phase separation occurs with heating.
In dilute solution, the properties of the polymer are characterized by the interaction between the solvent and the polymer. In a good solvent, the polymer appears swollen and occupies a large volume. In this scenario, intermolecular forces between the solvent and monomer subunits dominate over intramolecular interactions. In a bad solvent or poor solvent, intramolecular forces dominate and the chain contracts. In the theta solvent, or the state of the polymer solution where the value of the second virial coefficient becomes 0, the intermolecular polymer-solvent repulsion balances exactly the intramolecular monomer-monomer attraction. Under the theta condition (also called the Flory condition), the polymer behaves like an ideal random coil. The transition between the states is known as a coil-globule transition.
Inclusion of plasticizers.
Inclusion of plasticizers tends to lower Tg and increase polymer flexibility. Plasticizers are generally small molecules that are chemically similar to the polymer and create gaps between polymer chains for greater mobility and reduced interchain interactions. A good example of the action of plasticizers is related to polyvinylchlorides or PVCs. An uPVC, or unplasticized polyvinylchloride, is used for things such as pipes. A pipe has no plasticizers in it, because it needs to remain strong and heat-resistant. Plasticized PVC is used in clothing for a flexible quality. Plasticizers are also put in some types of cling film to make the polymer more flexible.
Chemical properties.
The attractive forces between polymer chains play a large part in determining polymer's properties. Because polymer chains are so long, these interchain forces are amplified far beyond the attractions between conventional molecules. Different side groups on the polymer can lend the polymer to ionic bonding or hydrogen bonding between its own chains. These stronger forces typically result in higher tensile strength and higher crystalline melting points.
The intermolecular forces in polymers can be affected by dipoles in the monomer units. Polymers containing amide or carbonyl groups can form hydrogen bonds between adjacent chains; the partially positively charged hydrogen atoms in N-H groups of one chain are strongly attracted to the partially negatively charged oxygen atoms in C=O groups on another. These strong hydrogen bonds, for example, result in the high tensile strength and melting point of polymers containing urethane or urea linkages. Polyesters have dipole-dipole bonding between the oxygen atoms in C=O groups and the hydrogen atoms in H-C groups. Dipole bonding is not as strong as hydrogen bonding, so a polyester's melting point and strength are lower than Kevlar's (Twaron), but polyesters have greater flexibility.
Ethene, however, has no permanent dipole. The attractive forces between polyethylene chains arise from weak van der Waals forces. Molecules can be thought of as being surrounded by a cloud of negative electrons. As two polymer chains approach, their electron clouds repel one another. This has the effect of lowering the electron density on one side of a polymer chain, creating a slight positive dipole on this side. This charge is enough to attract the second polymer chain. Van der Waals forces are quite weak, however, so polyethylene can have a lower melting temperature compared to other polymers.
Optical properties.
Polymers such as PMMA and HEMA:MMA are used as matrices in the gain medium of solid-state dye lasers that are also known as polymer lasers. These polymers have a high surface quality and are also highly transparent so that the laser properties are dominated by the laser dye used to dope the polymer matrix. These type of lasers, that also belong to the class of organic lasers, are known to yield very narrow linewidths which is useful for spectroscopy and analytical applications. An important optical parameter in the polymer used in laser applications is the change in refractive index with temperature 
also known as dn/dT. For the polymers mentioned here the (dn/dT) ~ −1.4 × 10−4 in units of K−1 in the 297 ≤ T ≤ 337 K range.
Standardized polymer nomenclature.
There are multiple conventions for naming polymer substances. Many commonly used polymers, such as those found in consumer products, are referred to by a common or trivial name. The trivial name is assigned based on historical precedent or popular usage rather than a standardized naming convention. Both the American Chemical Society (ACS) and IUPAC have proposed standardized naming conventions; the ACS and IUPAC conventions are similar but not identical. Examples of the differences between the various naming conventions are given in the table below:
In both standardized conventions, the polymers' names are intended to reflect the monomer(s) from which they are synthesized rather than the precise nature of the repeating subunit. For example, the polymer synthesized from the simple alkene ethene is called polyethylene, retaining the "-ene" suffix even though the double bond is removed during the polymerization process:
Polymer characterization.
The characterization of a polymer requires several parameters which need to be specified. This is because a polymer actually consists of a statistical distribution of chains of varying lengths, and each chain consists of monomer residues which affect its properties.
A variety of lab techniques are used to determine the properties of polymers. Techniques such as wide angle X-ray scattering, small angle X-ray scattering, and small angle neutron scattering are used to determine the crystalline structure of polymers. Gel permeation chromatography is used to determine the number average molecular weight, weight average molecular weight, and polydispersity. FTIR, Raman and NMR can be used to determine composition. Thermal properties such as the glass transition temperature and melting point can be determined by differential scanning calorimetry and dynamic mechanical analysis. Pyrolysis followed by analysis of the fragments is one more technique for determining the possible structure of the polymer. Thermogravimetry is a useful technique to evaluate the thermal stability of the polymer. Detailed analysis of TG curves also allow us to know a bit of the phase segregation in polymers. Rheological properties are also commonly used to help determine molecular architecture (molecular weight, molecular weight distribution and branching) as well as to understand how the polymer will process, through measurements of the polymer in the melt phase. Another polymer characterization technique is Automatic Continuous Online Monitoring of Polymerization Reactions (ACOMP) which provides real-time characterization of polymerization reactions. It can be used as an analytical method in R&D, as a tool for reaction optimization at the bench and pilot plant level and, eventually, for feedback control of full-scale reactors. ACOMP measures in a model-independent fashion the evolution of average molar mass and intrinsic viscosity, monomer conversion kinetics and, in the case of copolymers, also the average composition drift and distribution. It is applicable in the areas of free radical and controlled radical homo- and copolymerization, polyelectrolyte synthesis, heterogeneous phase reactions, including emulsion polymerization, adaptation to batch and continuous reactors, and modifications of polymers.
Polymer degradation.
Polymer degradation is a change in the properties—tensile strength, color, shape, or molecular weight—of a polymer or polymer-based product under the influence of one or more environmental factors, such as heat, light, chemicals and, in some cases, galvanic action. It is often due to the scission of polymer chain bonds via hydrolysis, leading to a decrease in the molecular mass of the polymer.
Although such changes are frequently undesirable, in some cases, such as biodegradation and recycling, they may be intended to prevent environmental pollution. Degradation can also be useful in biomedical settings. For example, a copolymer of polylactic acid and polyglycolic acid is employed in hydrolysable stitches that slowly degrade after they are applied to a wound.
The susceptibility of a polymer to degradation depends on its structure. Epoxies and chains containing aromatic functionalities are especially susceptible to UV degradation while polyesters are susceptible to degradation by hydrolysis, while polymers containing an unsaturated backbone are especially susceptible to ozone cracking. Carbon based polymers are more susceptible to thermal degradation than inorganic polymers such as polydimethylsiloxane and are therefore not ideal for most high-temperature applications. High-temperature matrices such as bismaleimides (BMI), condensation polyimides (with an O-C-N bond), triazines (with a nitrogen (N) containing ring), and blends thereof are susceptible to polymer degradation in the form of galvanic corrosion when bare carbon fiber reinforced polymer CFRP is in contact with an active metal such as aluminium in salt water environments.
The degradation of polymers to form smaller molecules may proceed by random scission or specific scission. The degradation of polyethylene occurs by random scission—a random breakage of the bonds that hold the atoms of the polymer together. When heated above 450 °C, polyethylene degrades to form a mixture of hydrocarbons. Other polymers, such as poly(alpha-methylstyrene), undergo specific chain scission with breakage occurring only at the ends. They literally unzip or depolymerize back to the constituent monomer.
The sorting of polymer waste for recycling purposes may be facilitated by the use of the Resin identification codes developed by the Society of the Plastics Industry to identify the type of plastic.
Product failure.
In a finished product, such a change is to be prevented or delayed. Failure of safety-critical polymer components can cause serious accidents, such as fire in the case of cracked and degraded polymer fuel lines. Chlorine-induced cracking of acetal resin plumbing joints and polybutylene pipes has caused many serious floods in domestic properties, especially in the USA in the 1990s. Traces of chlorine in the water supply attacked vulnerable polymers in the plastic plumbing, a problem which occurs faster if any of the parts have been poorly extruded or injection molded. Attack of the acetal joint occurred because of faulty molding, leading to cracking along the threads of the fitting which is a serious stress concentration.
Polymer oxidation has caused accidents involving medical devices. One of the oldest known failure modes is ozone cracking caused by chain scission when ozone gas attacks susceptible elastomers, such as natural rubber and nitrile rubber. They possess double bonds in their repeat units which are cleaved during ozonolysis. Cracks in fuel lines can penetrate the bore of the tube and cause fuel leakage. If cracking occurs in the engine compartment, electric sparks can ignite the gasoline and can cause a serious fire. In medical use degradation of polymers can lead to changes of physical and chemical characteristics of implantable devices.
Fuel lines can also be attacked by another form of degradation: hydrolysis. Nylon 6,6 is susceptible to acid hydrolysis, and in one accident, a fractured fuel line led to a spillage of diesel into the road. If diesel fuel leaks onto the road, accidents to following cars can be caused by the slippery nature of the deposit, which is like black ice.

</doc>
<doc id="23002" url="https://en.wikipedia.org/wiki?curid=23002" title="Perfect competition">
Perfect competition

In economics and general equilibrium theory, a perfect market is defined by several conditions, collectively called perfect competition. These conditions are:
When conditions of perfect competition hold, it has been proven that a market will reach an equilibrium in which the quantity supplied for every product or service, including labor, equals the quantity demanded at the current price. This equilibrium will be a Pareto optimum, meaning that nobody can be made better off by exchange without making someone else worse off.
Such markets are allocatively efficient, as output will always occur where marginal cost is equal to marginal revenue (MC = MR). But perfectly competitive markets are not necessarily productively efficient as output will not always occur where marginal cost is equal to average cost (MC = AC). 
In perfect competition, any profit-maximizing producer faces a market price equal to its marginal cost (P = MC). This implies that a factor's price equals the factor's marginal revenue product. It allows for derivation of the supply curve on which the neoclassical approach is based. This is also the reason why "a monopoly does not have a supply curve". The abandonment of price taking creates considerable difficulties for the demonstration of a general equilibrium except under other, very specific conditions such as that of monopolistic competition.
Real markets are never perfect, but range from close-to-perfect to very imperfect. Share and foreign exchange markets are commonly said to be the most similar to the perfect market. The real estate market is an example of a very imperfect market.
Normal profit.
In a perfect market tho sellers operate at zero economic surplus, sellers do make what's called normal profits.
"Normal" profit is a component of (implicit) costs and not a component of business profit at all. It represents the opportunity cost, as the time that the owner spends running the firm could be spent on running a different firm. The enterprise component of normal profit is thus the profit that a business owner considers necessary to make running the business worth her or his while i.e. it is comparable to the next best amount the entrepreneur could earn doing another job. Particularly if enterprise is not included as a factor of production, it can also be viewed a return to capital for investors including the entrepreneur, equivalent to the return the capital owner could have expected (in a safe investment), plus compensation for risk. In other words, the cost of normal profit varies both within and across industries; it is commensurate with the riskiness associated with each type of investment, as per the risk-return spectrum.
Only normal profits arise in circumstances of perfect competition when long run economic equilibrium is reached; there is no incentive for firms to either enter or leave the industry.
In competitive and contestable markets.
Economic profit does not occur in perfect competition in long run equilibrium; if it did, there would be an incentive for new firms to enter the industry, aided by a lack of barriers to entry until there was no longer any economic profit. As new firms enter the industry, they increase the supply of the product available in the market, and these new firms are forced to charge a lower price to entice consumers to buy the additional supply these new firms are supplying as the firms all compete for customers (See "Persistence" in the "Monopoly Profit" discussion). Incumbent firms within the industry face losing their existing customers to the new firms entering the industry, and are therefore forced to lower their prices to match the lower prices set by the new firms. New firms will continue to enter the industry until the price of the product is lowered to the point that it is the same as the average cost of producing the product, and all of the economic profit disappears. When this happens, economic agents outside of the industry find no advantage to forming new firms that enter into the industry, the supply of the product stops increasing, and the price charged for the product stabilizes, settling into an equilibrium.
The same is likewise true of the long run equilibria of monopolistically competitive industries and, more generally, any market which is held to be contestable. Normally, a firm that introduces a differentiated product can initially secure a "temporary" market power for a "short while" (See "Persistence" in "Monopoly Profit"). At this stage, the initial price the consumer must pay for the product is high, and the demand for, as well as the availability of the product in the market, will be limited. In the long run, however, when the profitability of the product is well established, and because there are few barriers to entry, the number of firms that produce this product will increase until the available supply of the product eventually becomes relatively large, the price of the product shrinks down to the level of the average cost of producing the product. When this finally occurs, all monopoly profit associated with producing and selling the product disappears, and the initial monopoly turns into a competitive industry. In the case of contestable markets, the cycle is often ended with the departure of the former "hit and run" entrants to the market, returning the industry to its previous state, just with a lower price and no economic profit for the incumbent firms.
Profit can, however, occur in competitive and contestable markets in the short run, as firms jostle for market position. Once risk is accounted for, long-lasting economic profit in a competitive market is thus viewed as the result of constant cost-cutting and performance improvement ahead of industry competitors, allowing costs to be below the market-set price.
In uncompetitive markets.
Economic profit is, however, much more prevalent in uncompetitive markets such as in a perfect monopoly or oligopoly situation. In these scenarios, individual firms have some element of market power: Though monopolists are constrained by consumer demand, they are not price takers, but instead either price-setters or quantity setters. This allows the firm to set a price which is higher than that which would be found in a similar but more competitive industry, allowing them economic profit in both the long and short run.
The existence of economic profits depends on the prevalence of barriers to entry: these stop other firms from entering into the industry and sapping away profits, like they would in a more competitive market. In cases where barriers are present, but more than one firm, firms can collude to limit production, thereby restricting supply in order to ensure the price of the product remains high enough to ensure all of the firms in the industry achieve an economic profit.
However, some economists, for instance Steve Keen, a professor at the University of Western Sydney, argue that even an infinitesimal amount of market power can allow a firm to produce a profit and that the absence of economic profit in an industry, or even merely that some production occurs at a loss, in and of itself constitutes a barrier to entry.
In a single-goods case, a positive economic profit happens when the firm's average cost is less than the price of the product or service at the profit-maximizing output. The economic profit is equal to the quantity of output multiplied by the difference between the average cost and the price.
Government intervention.
Often, governments will try to intervene in uncompetitive markets to make them more competitive. Antitrust (US) or competition (elsewhere) laws were created to prevent powerful firms from using their economic power to artificially create the barriers to entry they need to protect their economic profits. This includes the use of predatory pricing toward smaller competitors. For example, in the United States, Microsoft Corporation was initially convicted of breaking Anti-Trust Law and engaging in anti-competitive behavior in order to form one such barrier in "United States v. Microsoft"; after a successful appeal on technical grounds, Microsoft agreed to a settlement with the Department of Justice in which they were faced with stringent oversight procedures and explicit requirements designed to prevent this predatory behaviour. With lower barriers, new firms can enter the market again, making the long run equilibrium much more like that of a competitive industry, with no economic profit for firms.
If a government feels it is impractical to have a competitive market – such as in the case of a natural monopoly – it will sometimes try to regulate the existing uncompetitive market by controlling the price firms charge for their product. For example, the old AT&T (regulated) monopoly, which existed before the courts ordered its breakup, had to get government approval to raise its prices. The government examined the monopoly's costs, and determined whether or not the monopoly should be able raise its price and if the government felt that the cost did not justify a higher price, it rejected the monopoly's application for a higher price. Though a regulated firm will not have an economic profit as large as it would in an unregulated situation, it can still make profits well above a competitive firm in a truly competitive market.
Results.
In a perfectly competitive market, the demand curve facing a firm is perfectly elastic.
As mentioned above, the perfect competition model, if interpreted as applying also to short-period or very-short-period behaviour, is approximated only by markets of homogeneous products produced and purchased by very many sellers and buyers, usually organized markets for agricultural products or raw materials. In real-world markets, assumptions such as perfect information cannot be verified and are only approximated in organized double-auction markets where most agents wait and observe the behaviour of prices before deciding to exchange (but in the long-period interpretation perfect information is not necessary, the analysis only aims at determining the average around which market prices gravitate, and for gravitation to operate one does not need perfect information).
In the absence of externalities and public goods, perfectly competitive equilibria are Pareto-efficient, i.e. no improvement in the utility of a consumer is possible without a worsening of the utility of some other consumer. This is called the First Theorem of Welfare Economics. The basic reason is that no productive factor with a non-zero marginal product is left unutilized, and the units of each factor are so allocated as to yield the same indirect marginal utility in all uses, a basic efficiency condition (if this indirect marginal utility were higher in one use than in other ones, a Pareto improvement could be achieved by transferring a small amount of the factor to the use where it yields a higher marginal utility).
A simple proof assuming differentiable utility functions and production functions is the following. Let wj be the 'price' (the rental) of a certain factor j, let MPj1 and MPj2 be its marginal product in the production of goods 1 and 2, and let p1 and p2 be these goods' prices. In equilibrium these prices must equal the respective marginal costs MC1 and MC2; remember that marginal cost equals factor 'price' divided by factor marginal productivity (because increasing the production of good by one very small unit through an increase of the employment of factor j requires increasing the factor employment by 1/MPji and thus increasing the cost by wj/MPji, and through the condition of cost minimization that marginal products must be proportional to factor 'prices' it can be shown that the cost increase is the same if the output increase is obtained by optimally varying all factors). Optimal factor employment by a price-taking firm requires equality of factor rental and factor marginal revenue product, wj=piMPji, so we obtain p1=MC1=wj/MPj1, p2=MCj2=wj/MPj2.
Now choose any consumer purchasing both goods, and measure his utility in such units that in equilibrium his marginal utility of money (the increase in utility due to the last unit of money spent on each good), MU1/p1=MU2/p2, is 1. Then p1=MU1, p2=MU2. The indirect marginal utility of the factor is the increase in the utility of our consumer achieved by an increase in the employment of the factor by one (very small) unit; this increase in utility through allocating the small increase in factor utilization to good 1 is MPj1MU1=MPj1p1=wj, and through allocating it to good 2 it is MPj2MU2=MPj2p2=wj again. With our choice of units the marginal utility of the amount of the factor consumed directly by the optimizing consumer is again w, so the amount supplied of the factor too satisfies the condition of optimal allocation.
Monopoly violates this optimal allocation condition, because in a monopolized industry market price is above marginal cost, and this means that factors are underutilized in the monopolized industry, they have a higher indirect marginal utility than in their uses in competitive industries. Of course this theorem is considered irrelevant by economists who do not believe that general equilibrium theory correctly predicts the functioning of market economies; but it is given great importance by neoclassical economists and it is the theoretical reason given by them for combating monopolies and for antitrust legislation.
Profit.
In contrast to a monopoly or oligopoly, in perfect competition it is impossible for a firm to earn economic profit in the long run, which is to say that a firm cannot make any more money than is necessary to cover its economic costs. In order not to misinterpret this zero-long-run-profits thesis, it must be remembered that the term 'profit' is used in different ways: 
Thus, if one leaves aside risk coverage for simplicity, the neoclassical zero-long-run-profit thesis would be re-expressed in classical parlance as profits coinciding with interest in the long period (i.e. the rate of profit tending to coincide with the rate of interest). Profits in the classical meaning do not necessarily disappear in the long period but tend to normal profit. 
With this terminology, if a firm is earning abnormal profit in the short term, this will act as a trigger for other firms to enter the market. As other firms enter the market, the market supply curve will shift out, causing prices to fall. Existing firms will react to this lower price by adjusting their capital stock downward. This adjustment will cause their marginal cost to shift to the left causing the market supply curve to shift inward. However, the net effect of entry by new firms and adjustment by existing firms will be to shift the supply curve outward. The market price will be driven down until all firms are earning normal profit only.
It is important to note that perfect competition is a sufficient condition for allocative and productive efficiency, but it is not a necessary condition. Laboratory experiments in which participants have significant price setting power and little or no information about their counterparts consistently produce efficient results given the proper trading institutions.
The shutdown point.
In the short run, a firm operating at a loss < TC (revenue less than total cost) or P < ATC (price less than unit cost) must decide whether to continue to operate or temporarily shutdown. The shutdown rule states "in the short run a firm should continue to operate if price exceeds average variable costs." Restated, the rule is that for a firm to continue producing in the short run it must earn sufficient revenue to cover its variable costs. The rationale for the rule is straightforward. By shutting down a firm avoids all variable costs. However, the firm must still pay fixed costs. Because fixed cost must be paid regardless of whether a firm operates they should not be considered in deciding whether to produce or shutdown.
Thus in determining whether to shut down a firm should compare total revenue to total variable costs (VC) rather than total costs (FC + VC). If the revenue the firm is receiving is greater than its total variable cost (R > VC) then the firm is covering all variable cost plus there is additional revenue ("contribution"), which can be applied to fixed costs. (The size of the fixed costs is irrelevant as it is a sunk cost. The same consideration is used whether fixed costs are one dollar or one million dollars.) On the other hand, if VC > R then the firm is not even covering its production costs and it should immediately shut down. The rule is conventionally stated in terms of price (average revenue) and average variable costs. The rules are equivalent (If you divide both sides of inequality TR > TVC by Q gives P > AVC). If the firm decides to operate, the firm will continue to produce where marginal revenue equals marginal costs because these conditions insure not only profit maximization (loss minimization) but also maximum contribution.
Another way to state the rule is that a firm should compare the profits from operating to those realized if it shutdown and select the option that produces the greater profit. A firm that is shutdown is generating zero revenue and incurring no variable costs. However, the firm still has to pay fixed cost. So the firm's profit equals fixed costs or −FC. An operating firm is generating revenue, incurring variable costs and paying fixed costs. The operating firm's profit is R − VC − FC. The firm should continue to operate if R − VC − FC ≥ −FC, which simplified is R ≥ VC. The difference between revenue, R, and variable costs, VC, is the contribution to fixed costs and any contribution is better than none. Thus, if R ≥ VC then firm should operate. If R < VC the firm should shut down.
A decision to shut down means that the firm is temporarily suspending production. It does not mean that the firm is going out of business (exiting the industry). If market conditions improve, and prices increase, the firm can resume production. Shutting down is a short-run decision. A firm that has shut down is not producing. The firm still retains its capital assets; however, the firm cannot leave the industry or avoid its fixed costs in the short run. Exit is a long-term decision. A firm that has exited an industry has avoided all commitments and freed all capital for use in more profitable enterprises.
However, a firm cannot continue to incur losses indefinitely. In the long run, the firm will have to earn sufficient revenue to cover all its expenses and must decide whether to continue in business or to leave the industry and pursue profits elsewhere. The long-run decision is based on the relationship of the price and long-run average costs. If P ≥ AC then the firm will not exit the industry. If P < AC, then the firm will exit the industry. These comparisons will be made after the firm has made the necessary and feasible long-term adjustments. In the long run a firm operates where marginal revenue equals long-run marginal costs.
Short-run supply curve.
The short run supply curve for a perfectly competitive firm is the marginal cost (MC) curve at and above the shutdown point. Portions of the marginal cost curve below the shut down point are not part of the SR supply curve because the firm is not producing in that range. Technically the SR supply curve is a discontinuous function composed of the segment of the MC curve at and above minimum of the average variable cost curve and a segment that runs with the vertical axis from the origin to but not including a point "parallel" to minimum average variable costs.
Examples.
Though there is no actual perfectly competitive market in the real world, a number of approximations exist:
An example is that of a large action of identical goods with all potential buyers and sellers present. By design, a stock exchange resembles this, not as a complete description (for no markets may satisfy all requirements of the model) but as an approximation. The flaw in considering the stock exchange as an example of Perfect Competition is the fact that large institutional investors (e.g. investment banks) may solely influence the market price. This, of course, violates the condition that "no one seller can influence market price".
Horse betting is also quite a close approximation. When placing bets, consumers can just look down the line to see who is offering the best odds, and so no one bookie can offer worse odds than those being offered by the market as a whole, since consumers will just go to another bookie. This makes the bookies price-takers. Furthermore, the product on offer is very homogeneous, with the only differences between individual bets being the pay-off and the horse. Of course, there are not an infinite amount of bookies, and some barriers to entry exist, such as a license and the capital required to set up.
Free software works along lines that approximate perfect competition as well. Anyone is free to enter and leave the market at no cost. All code is freely accessible and modifiable, and individuals are free to behave independently. Free software may be bought or sold at whatever price that the market may allow.
Some believe that one of the prime examples of a perfectly competitive market anywhere in the world is street food in developing countries. This is so since relatively few barriers to entry/exit exist for street vendors. Furthermore, there are often numerous buyers and sellers of a given street food, in addition to consumers/sellers possessing perfect information of the product in question. It is often the case that street vendors may serve a homogenous product, in which little to no variations in the product's nature exist.
Another very near example of perfect competition would be the fish market and the vegetable or fruit vendors who sell at the same place, the bars in "Le Carré" (Liège, Belgium) or the "kebab street" near the Grand Place in Brussels:
Criticisms.
The use of the assumption of perfect competition as the foundation of price theory for product markets is often criticized as representing all agents as passive, thus removing the active attempts to increase one's welfare or profits by price undercutting, product design, advertising, innovation, activities that - the critics argue – characterize most industries and markets. These criticisms point to the frequent lack of realism of the assumptions of product homogeneity and impossibility to differentiate it, but apart from this the accusation of passivity appears correct only for short-period or very-short-period analyses, in long-period analyses the inability of price to diverge from the natural or long-period price is due to active reactions of entry or exit.
Some economists have a different kind of criticism concerning perfect competition model. They are not criticizing the price taker assumption because it makes economic agents too "passive", but because it then raises the question of who sets the prices. Indeed, if everyone is price taker, there is the need for a benevolent planner who gives and sets the prices, in other word, there is a need for a "price maker". Therefore, it makes the perfect competition model appropriate not to describe a decentralize "market" economy but a centralized one. This in turn means that such kind of model has more to do with communism than capitalism.
Another frequent criticism is that it is often not true that in the short run differences between supply and demand cause changes in price; especially in manufacturing, the more common behaviour is alteration of production without nearly any alteration of price.
The critics of the assumption of perfect competition in product markets seldom question the basic neoclassical view of the working of market economies for this reason. The Austrian School insists strongly on this criticism, and yet the neoclassical view of the working of market economies as fundamentally efficient, reflecting consumer choices and assigning to each agent his contribution to social welfare, is esteemed to be fundamentally correct. Some non-neoclassical schools, like Post-Keynesians, reject the neoclassical approach to value and distribution, but not because of their rejection of perfect competition as a reasonable approximation to the working of most product markets; the reasons for rejection of the neoclassical 'vision' are different views of the determinants of income distribution and of aggregated demand.
In particular, the rejection of perfect competition does not generally entail the rejection of free competition as characterizing most product markets; indeed it has been argued that competition is stronger nowadays than in 19th century capitalism, owing to the increasing capacity of big conglomerate firms to enter any industry: therefore the classical idea of a tendency toward a uniform rate of return on investment in all industries owing to free entry is even more valid today; and the reason why General Motors, Exxon or Nestlé do not enter the computers or pharmaceutical industries is not insurmountable barriers to entry but rather that the rate of return in the latter industries is already sufficiently in line with the average rate of return elsewhere as not to justify entry. On this few economists, it would seem, would disagree, even among the neoclassical ones. Thus when the issue is normal, or long-period, product prices, differences on the validity of the perfect competition assumption do not appear to imply important differences on the existence or not of a tendency of rates of return toward uniformity as long as entry is possible, and what is found fundamentally lacking in the perfect competition model is the absence of marketing expenses and innovation as causes of costs that do enter normal average cost.
The issue is different with respect to factor markets. Here the acceptance or denial of perfect competition in labour markets does make a big difference to the view of the working of market economies. One must distinguish neoclassical from non-neoclassical economists. For the former, absence of perfect competition in labour markets, e.g. due to the existence of trade unions, impedes the smooth working of competition, which if left free to operate would cause a decrease of wages as long as there were unemployment, and would finally ensure the full employment of labour: labour unemployment is due to absence of perfect competition in labour markets. Most non-neoclassical economists deny that a full flexibility of wages would ensure the full employment of labour and find a stickiness of wages an indispensable component of a market economy, without which the economy would lack the regularity and persistence indispensable to its smooth working. This was, for example, John Maynard Keynes's opinion.
Particularly radical is the view of the Sraffian school on this issue: the labour demand curve cannot be determined hence a level of wages ensuring the equality between supply and demand for labour does not exist, and economics should resume the viewpoint of the classical economists, according to whom competition in labour markets does not and cannot mean indefinite price flexibility as long as supply and demand are unequal, it only means a tendency to equality of wages for similar work, but the level of wages is necessarily determined by complex sociopolitical elements; custom, feelings of justice, informal allegiances to classes, as well as overt coalitions such as trade unions, far from being impediments to a smooth working of labour markets that would be able to determine wages even without these elements, are on the contrary indispensable because without them there would be no way to determine wages.
Equilibrium in perfect competition.
Equilibrium in perfect competition is the point where market demands will be equal to market supply. A firm's price will be determined at this point. In the short run, equilibrium will be affected by demand. In the long run, both demand and supply of a product will affect the equilibrium in perfect competition. A firm will receive only normal profit in the long run at the equilibrium point.

</doc>
<doc id="23003" url="https://en.wikipedia.org/wiki?curid=23003" title="Philosophy of religion">
Philosophy of religion

Philosophy of religion according to the "Stanford Encyclopedia of Philosophy" is, "the philosophical examination of the central themes and concepts involved in religious traditions." It is an ancient discipline, being found in the earliest known manuscripts concerning philosophy, and relates to many other branches of philosophy and general thought, including metaphysics, logic, and history.
The philosophy of religion differs from religious philosophy in that it seeks to discuss questions regarding the nature of religion as a whole, rather than examining the problems brought forth by a particular belief system. It is designed such that it can be carried out dispassionately by those who identify as believers or non-believers.
As a part of metaphysics.
Philosophy of religion has classically been regarded as a part of metaphysics. In Aristotle's "Metaphysics", the necessarily prior cause of eternal motion was an unmoved mover, who, like the object of desire, or of thought, inspires motion without itself being moved. This, according to Aristotle, is God, the subject of study in theology. Today, however, philosophers have adopted the term "philosophy of religion" for the subject, and typically it is regarded as a separate field of specialization, although it is also still treated by some, particularly Catholic philosophers, as a part of metaphysics.
History.
Although the term did not come into general use until the nineteenth century, perhaps the earliest strictly philosophical writings about religion can be found in the Hindu Upanishads. Around the same time, the works of Daoism and Confucianism also dealt, in part, with reasoning about religious concepts. The Buddhist writing in the Pali canon "contains acute philosophical thinking", and "we have in Buddhism a very shrewd grasp of the nature of religion as philosophy illuminates it."
Field of study.
Philosophy of religion covers alternative beliefs about God, the varieties of religious experience, the interplay between science and religion, the nature and scope of good and evil, and religious treatments of birth, history, and death. The field also includes the ethical implications of religious commitments, the relation between faith, reason, experience and tradition, concepts of the miraculous, the sacred revelation, mysticism, power, and salvation.
The philosophy of religion has been distinguished from theology by pointing out that, for theology, "its critical reflections are based on religious convictions". Also, "theology is responsible to an authority that initiates its thinking, speaking, and witnessing ... philosophy bases its arguments on the ground of timeless evidence."
Basic themes and problems.
Three considerations that are basic to the philosophy of religion concerning deities are: the existence of God, the nature of God, and the knowledge of God.
Existence of God.
There are several main positions with regard to the existence of God that one might take:
These are not mutually exclusive positions. For example, agnostic theists choose to believe God exists while asserting that knowledge of God's existence is inherently unknowable. Similarly, agnostic atheists reject belief in the existence of all deities, while asserting that whether any such entities exist or not is inherently unknowable.
Natural theology.
The attempt to provide proofs or arguments for the existence of God is one aspect of what is known as natural theology or the natural theistic project. This strand of natural theology attempts to justify belief in God by independent grounds. There is plenty of philosophical literature on faith (especially fideism) and other subjects generally considered to be outside the realm of natural theology. Perhaps most of philosophy of religion is predicated on natural theology's assumption that the existence of God can be justified or warranted on rational grounds. There has been considerable philosophical and theological debate about the kinds of proofs, justifications and arguments that are appropriate for this discourse.
The philosopher Alvin Plantinga has shifted his focus to justifying belief in God (that is, those who believe in God, for whatever reasons, are rational in doing so) through Reformed epistemology, in the context of a theory of warrant and proper cognitive function.
Other reactions to natural theology are those of Wittgensteinian philosophers of religion, most notably D. Z. Phillips. Phillips rejects "natural theology" and its evidentialist approach as confused, in favor of a grammatical approach which investigates the meaning of belief in God. For Phillips, belief in God is not a proposition with a particular truth value, but a form of life. Consequently, the question of whether God exists confuses the logical categories which govern theistic language with those that govern other forms of discourse (most notably, scientific discourse). According to Phillips, the question of whether or not God exists cannot be "objectively" answered by philosophy because the categories of truth and falsity, which are necessary for asking the question, have no application in the religious contexts wherein religious belief has its sense and meaning. In other words, the question cannot be answered because it cannot be asked without entering into confusion. As Phillips sees things, the job of the philosopher is not to investigate the "rationality" of belief in God but to elucidate its meaning.
Problem of evil.
The problem of evil is the question of how to reconcile the existence of evil with that of a deity who is, in either absolute or relative terms, omnipotent, omniscient, and omnibenevolent. An argument from evil attempts to show that the co-existence of evil and such a deity is unlikely or impossible if placed in absolute terms. Attempts to show the contrary have traditionally been discussed under the heading of theodicy.
The nature of God.
There exists many understandings of the term "God". It typically differs not only from religion to religion, but also from person to person who share the same religious believes. It is therefore hard to define "God" or list a complete array of characteristics (nature) of God that is applicable to all religions.
For the sake of simplicity, the concept of "God" is often described by philosophers of religion to be an " 1. Omniscient, 2. Omnipotent, 3. Omnibenevolent 4. Being". Any "God" that is referred to in most contexts of philosophy of religion must have the above four characteristics, including being a "Being". Note that this list is not exhaustive. There exists other characteristics of certain "God"s that are not included, for example, "omnipresent". 
Above, a simple if relationship exists between "God" and those four characteristics. That is to say. if X is a "God", X must possess all four characteristics. Yet, according to the above statement, the existence of those four characteristics together on Y might not be sufficient to lead to the conclusion that Y is a "God". However, the non-existence of one or more of the four characteristics on an object Z is sufficient to lead to the argument that Z is not a "God" that we have defined above.
Analytic philosophy of religion.
In "Analytic Philosophy of Religion", James Franklin Harris noted that
As with the study of ethics, early analytic philosophy tended to avoid the study of philosophy of religion, largely dismissing (as per the logical positivists view) the subject as part of metaphysics and therefore meaningless. The collapse of logical positivism renewed interest in philosophy of religion, prompting philosophers like William Alston, John Mackie, Alvin Plantinga, Robert Merrihew Adams, Richard Swinburne, and Antony Flew not only to introduce new problems, but to re-open classical topics such as the nature of miracles, theistic arguments, the problem of evil, (see existence of God) the rationality of belief in God, concepts of the nature of God, and many more.
Plantinga, Mackie and Flew debated the logical validity of the "free will defense" as a way to solve the problem of evil. Alston, grappling with the consequences of analytic philosophy of language, worked on the nature of religious language. Adams worked on the relationship of faith and morality. Analytic epistemology and metaphysics has formed the basis for a number of philosophically-sophisticated theistic arguments, like those of the reformed epistemologists like Plantinga.
Analytic philosophy of religion has also been preoccupied with Ludwig Wittgenstein, as well as his interpretation of Søren Kierkegaard's philosophy of religion. Using first-hand remarks (which would later be published in "Philosophical Investigations", "Culture and Value", and other works), philosophers such as Peter Winch and Norman Malcolm developed what has come to be known as "contemplative philosophy", a Wittgensteinian school of thought rooted in the "Swansea tradition" and which includes Wittgensteinians such as Rush Rhees, Peter Winch and D. Z. Phillips, among others. The name "contemplative philosophy" was first coined by D. Z. Phillips in "Philosophy's Cool Place", which rests on an interpretation of a passage from Wittgenstein's "Culture and Value." This interpretation was first labeled, "Wittgensteinian Fideism," by Kai Nielsen but those who consider themselves Wittgensteinians in the Swansea tradition have relentlessly and repeatedly rejected this construal as a caricature of Wittgenstein's considered position; this is especially true of D. Z. Phillips. Responding to this interpretation, Kai Nielsen and D.Z. Phillips became two of the most prominent philosophers on Wittgenstein's philosophy of religion.

</doc>
<doc id="23004" url="https://en.wikipedia.org/wiki?curid=23004" title="Precedent">
Precedent

In common law legal systems, a precedent or authority is a principle or rule established in a previous legal case that is either binding on or persuasive for a court or other tribunal when deciding subsequent cases with similar issues or facts. Common law legal systems place great value on deciding cases according to consistent principled rules so that similar facts will yield similar and predictable outcomes, and observance of precedent is the mechanism by which that goal is attained. Black's Law Dictionary defines "precedent" as a "rule of law established for the first time by a court for a particular type of case and thereafter referred to in deciding similar cases." Common law precedent is a third kind of law, on equal footing with statutory law (statutes and codes enacted by legislative bodies), and Delegated legislation (in U.K. parlance) or regulatory law (in U.S. parlance) (regulations promulgated by executive branch agencies).
Case law or common law is the set of decisions of adjudicatory tribunals that can be cited as precedent. In most countries, including most European countries, the term is applied to any set of rulings on law which is guided by previous rulings, for example, previous decisions of a government agency.
Precedential (whether strongly binding or weakly persuasive) case law can arise from a ruling by either a judicial court, or by an executive branch agency. Cases, trials, and hearings that do not result in written decisions, decisions from tribunals that are not in the "chain of command" that binds the later court, written decisions that are designated "nonprecedential" by the tribunal, or written decisions of agencies that are not issued and indexed with sufficient formality to gain precedential effect, do not create binding precedent for future court decisions.
Principle.
Stare decisis (Anglo-Latin pronunciation: ) is a legal principle by which judges are obligated to respect the precedent established by prior decisions. The words originate from the phrasing of the principle in the Latin maxim "Stare decisis et non quieta movere": "to stand by decisions and not disturb the undisturbed." In a legal context, this is understood to mean that courts should generally abide by precedent and not disturb settled matters. The principle of "stare decisis" can be divided into two components.
The first is the rule that a decision made by a superior court, or by the same court in an earlier decision, is binding precedent that the court itself and all its inferior courts are obligated to follow. The second is the principle that a court should not overturn its own precedent unless there is a strong reason to do so and should be guided by principles from lateral and inferior courts. The second principle, regarding persuasive precedent, is an advisory one that courts can and do ignore occasionally.
Case law in common law systems.
In the common law tradition, courts decide the law applicable to a case by interpreting statutes and applying precedent which record how and why prior cases have been decided. Unlike most civil law systems, common law systems follow the doctrine of "stare decisis", by which most courts are bound by their own previous decisions in similar cases, and all lower courts should make decisions consistent with previous decisions of higher courts. For example, in England, the High Court and the Court of Appeal are each bound by their own previous decisions, but the Supreme Court of the United Kingdom is able to deviate from its earlier decisions, although in practice it rarely does so.
Generally speaking, higher courts do not have direct oversight over day-to-day proceedings in lower courts, in that they cannot reach out on their own initiative ("sua sponte") at any time to reverse or overrule judgments of the lower courts. Normally, the burden rests with litigants to appeal rulings (including those in clear violation of established case law) to the higher courts. If a judge acts against precedent and the case is not appealed, the decision will stand.
A lower court may not rule against a binding precedent, even if the lower court feels that the precedent is unjust; the lower court may only express the hope that a higher court or the legislature will reform the rule in question. If the court believes that developments or trends in legal reasoning render the precedent unhelpful, and wishes to evade it and help the law evolve, the court may either hold that the precedent is inconsistent with subsequent authority, or that the precedent should be "distinguished" by some material difference between the facts of the cases. If that judgment goes to appeal, the appellate court will have the opportunity to review both the precedent and the case under appeal, perhaps overruling the previous case law by setting a new precedent of higher authority. This may happen several times as the case works its way through successive appeals. Lord Denning, first of the High Court of Justice, later of the Court of Appeal, provided a famous example of this evolutionary process in his development of the concept of estoppel starting in the "High Trees" case: "Central London Property Trust Ltd v. High Trees House Ltd" K.B. 130.
Judges may refer to various types of persuasive authority to reach a decision in a case. Widely cited non-binding sources include legal encyclopedias such as "Corpus Juris Secundum" and "Halsbury's Laws of England", or the published work of the Law Commission or the American Law Institute. Some bodies are given statutory powers to issue Guidance with persuasive authority or similar statutory effect, such as the Highway Code.
In federal or multi-jurisdictional law systems there may exist conflicts between the various lower appellate courts. Sometimes these differences may not be resolved and it may be necessary to distinguish how the law is applied in one district, province, division or appellate department. Usually only an appeal accepted by the court of last resort will resolve such differences and, for many reasons, such appeals are often not granted.
Any court may seek to distinguish its present case from that of a binding precedent, in order to reach a different conclusion. The validity of such a distinction may or may not be accepted on appeal. An appellate court may also propound an entirely new and different analysis from that of junior courts, and may or may not be bound by its own previous decisions, or in any case may distinguish the decisions based on significant differences in the facts applicable to each case. Or, a court may view the matter before it as one of "first impression," not governed by any controlling precedent.
Where there are several members of a court, there may be one or more judgments given; only the ratio decidendi of the majority can constitute a binding precedent, but all may be cited as persuasive, or their reasoning may be adopted in argument. Quite apart from the rules of precedent, the weight actually given to any reported judgment may depend on the reputation of both the court and the judges.
Type of precedent.
Verticality.
Generally, a common law court system has trial courts, intermediate appellate courts and a supreme court. The inferior courts conduct almost all trial proceedings. The inferior courts are bound to obey precedent established by the appellate court for their jurisdiction, and all supreme court precedent.
The Supreme Court of California's explanation of this principle is that
An Intermediate state appellate court is generally bound to follow the decisions of the highest court of that state.
The application of the doctrine of "stare decisis" from a superior court to an inferior court is sometimes called "vertical stare decisis".
Horizontality.
The idea that a judge is bound by (or at least should respect) decisions of earlier judges of similar or coordinate level is called horizontal "stare decisis".
In the United States federal court system, the intermediate appellate courts are divided into thirteen "circuits," each covering some range of territory ranging in size from the District of Columbia alone up to seven states. Each panel of judges on the court of appeals for a circuit is bound to obey the prior appellate decisions of the same circuit. Precedent of a United States court of appeals may be overruled only by the court "en banc," that is, a session of all the active appellate judges of the circuit, or by the United States Supreme Court, not simply by a different three-judge panel.
When a court binds itself, this application of the doctrine of precedent is sometimes called "horizontal stare decisis". The state of New York has a similar appellate structure as it is divided into four appellate departments supervised by the final New York Court of Appeals. Decisions of one appellate department are not binding upon another, and in some cases the departments differ considerably on interpretations of law.
Federalism and parallel state and federal courts.
In federal systems the division between federal and state law may result in complex interactions. In the United States, state courts are not considered inferior to federal courts but rather constitute a parallel court system.
In practice, however, judges in one system will almost always choose to follow relevant case law in the other system to prevent divergent results and to minimize forum shopping.
Binding precedent.
Precedent that must be applied or followed is known as "binding precedent" (alternately "metaphorically precedent", "mandatory" or "binding authority", etc.). Under the doctrine of "stare decisis", a lower court must honor findings of law made by a higher court that is within the appeals path of cases the court hears. In state and federal courts in the United States of America, jurisdiction is often divided geographically among local trial courts, several of which fall under the territory of a regional appeals court. All appellate courts fall under a highest court (sometimes but not always called a "supreme court"). By definition, decisions of lower courts are not binding on courts higher in the system, nor are appeals court decisions binding on local courts that fall under a different appeals court. Further, courts must follow their own proclamations of law made earlier on other cases, and honor rulings made by other courts in disputes among the parties before them pertaining to the same pattern of facts or events, unless they have a strong reason to change these rulings (see Law of the case re: a court's previous holding being binding precedent for that court).
In law, a binding precedent (also mandatory precedent or binding authority) is a precedent which must be followed by all lower courts under common law legal systems. In English law it is usually created by the decision of a higher court, such as the Supreme Court of the United Kingdom, which took over the judicial functions of the House of Lords in 2009. In Civil law and pluralist systems precedent is not binding but case law is taken into account by the courts.
Binding precedent relies on the legal principle of "stare decisis". "Stare decisis" means to stand by things decided. It ensures certainty and consistency in the application of law. Existing binding precedent from past cases are applied in principle to new situations by analogy.
One law professor has described mandatory precedent as follows:
In extraordinary circumstances a higher court may overturn or overrule mandatory precedent, but will often attempt to distinguish the precedent before overturning it, thereby limiting the scope of the precedent.
Under the U.S. legal system, courts are set up in a hierarchy. At the top of the federal or national system is the Supreme Court, and underneath are lower federal courts. The state court systems have hierarchy structures similar to that of the federal system.
The U.S. Supreme Court has final authority on questions about the meaning of federal law, including the U.S. Constitution. For example, when the Supreme Court says that the First Amendment applies in a specific way to suits for slander, then every court is bound by that precedent in its interpretation of the First Amendment as it applies to suits for slander. If a lower court judge disagrees with a higher court precedent on what the First Amendment should mean, the lower court judge must rule according to the binding precedent. Until the higher court changes the ruling (or the law itself is changed), the binding precedent is authoritative on the meaning of the law.
Lower courts are bound by the precedent set by higher courts within their region. Thus, a federal district court that falls within the geographic boundaries of the Third Circuit Court of Appeals (the mid-level appeals court that hears appeals from district court decisions from Delaware, New Jersey, Pennsylvania, and the Virgin Islands) is bound by rulings of the Third Circuit Court, but not by rulings in the Ninth Circuit (Alaska, Arizona, California, Guam, Hawaii, Idaho, Montana, Nevada, Northern Mariana Islands, Oregon, and Washington), since the Circuit Courts of Appeals have jurisdiction defined by geography. The Circuit Courts of Appeals can interpret the law how they want, so long as there is no binding Supreme Court precedent. One of the common reasons the Supreme Court grants certiorari (that is, they agree to hear a case) is if there is a conflict among the circuit courts as to the meaning of a federal law.
There are three elements needed for a precedent to work. Firstly, the hierarchy of the courts needs to be accepted, and an efficient system of law reporting. 'A balance must be struck between the need on one side for the legal certainty resulting from the binding effect of previous decisions, and on the other side the avoidance of undue restriction on the proper development of the law (1966 Practice Statement (Judicial Precedent) by Lord Gardiner L.C.)'.
Binding precedent in English law.
Judges are bound by the law of binding precedent in England and Wales and other common law jurisdictions. This is a distinctive feature of the English legal system. In Scotland and many countries throughout the world, particularly in mainland Europe, civil law means that judges take case law into account in a similar way, but are not obliged to do so and are required to consider the precedent in terms of principle. Their fellow judges' decisions may be persuasive but are not binding. Under the English legal system, judges are not necessarily entitled to make their own decisions about the development or interpretations of the law. They may be bound by a decision reached in a previous case. Two facts are crucial to determining whether a precedent is binding:
Super "stare decisis".
Super-"stare decisis" is a term used for important precedent that is resistant or immune from being overturned, without regard to whether correctly decided in the first place. It may be viewed as one extreme in a range of precedential power, or alternatively, to express a belief, or a critique of that belief, that some decisions should not be overturned.
In 1976, Richard Posner and William Landes coined the term "super-precedent," in an article they wrote about testing theories of precedent by counting citations. Posner and Landes used this term to describe the influential effect of a cited decision. The term "super-precedent" later became associated with different issue: the difficulty of overturning a decision. In 1992, Rutgers professor Earl Maltz criticized the Supreme Court's decision in "Planned Parenthood v. Casey" for endorsing the idea that if one side can take control of the Court on an issue of major national importance (as in "Roe v. Wade"), that side can protect its position from being reversed "by a kind of super-stare decisis." The controversial idea that some decisions are virtually immune from being overturned, regardless of whether they were decided correctly in the first place, is the idea to which the term "super "stare decisis"" now usually refers.
The concept of super-"stare decisis" (or "super-precedent") was mentioned during the interrogations of Chief Justice John Roberts and Justice Samuel Alito before the Senate Judiciary Committee. Prior to the commencement of the Roberts hearings, the chair of that committee, Senator Arlen Specter of Pennsylvania, wrote an op/ed in the "New York Times" referring to "Roe" as a "super-precedent." He revisited this concept during the hearings, but neither Roberts nor Alito endorsed the term or the concept.
Persuasive precedent.
Persuasive precedent (also persuasive authority) is precedent or other legal writing that is not binding precedent but that is useful or relevant and that may guide the judge in making the decision in a current case. Persuasive precedent includes cases decided by lower courts, by peer or higher courts from other geographic jurisdictions, cases made in other parallel systems (for example, military courts, administrative courts, indigenous/tribal courts, state courts versus federal courts in the United States), statements made in dicta, treatises or academic law reviews, and in some exceptional circumstances, cases of other nations, treaties, world judicial bodies, etc.
In a "case of first impression", courts often rely on persuasive precedent from courts in other jurisdictions that have previously dealt with similar issues. Persuasive precedent may become binding through its adoption by a higher court.
In civil law and pluralist systems, as under Scots law, precedent is not binding but case law is taken into account by the courts.
Lower courts.
A lower court's opinion may be considered as persuasive authority if the judge believes they have applied the correct legal principle and reasoning.
Higher courts in other circuits.
A court may consider the ruling of a higher court that is not binding. For example, a district court in the United States First Circuit could consider a ruling made by the United States Court of Appeals for the Ninth Circuit as persuasive authority.
Horizontal courts.
Courts may consider rulings made in other courts that are of equivalent authority in the legal system. For example, an appellate court for one district could consider a ruling issued by an appeals court in another district.
Statements made in "obiter dicta".
Courts may consider "obiter dicta" in opinions of higher courts. Dicta of a higher court, though not binding, will often be persuasive to lower courts. The phrase "obiter dicta" is usually translated as "other things said", but due to the high number of judges and individual concurring opinions, it is often hard to distinguish from the "ratio decidendi" (reason for the decision). For these reasons, the obiter dicta may often be taken into consideration by a court. A litigant may also consider "obiter dicta" if a court has previously signaled that a particular legal argument is weak and may even warrant sanctions if repeated.
Dissenting opinions.
A case decided by a multi-judge panel could result in a split decision. While only the majority opinion is considered precedential, an outvoted judge can still publish a dissenting opinion. Common patterns for dissenting opinions include:
A judge in a subsequent case, particularly in a different jurisdiction, could find the dissenting judge's reasoning persuasive. In the jurisdiction of the original decision, however, a judge should only overturn the holding of a court lower or equivalent in the hierarchy. A district court, for example, could not rely on a Supreme Court dissent as a basis to depart from the reasoning of the majority opinion. However, lower courts occasionally cite dissents, either for either a limiting principle on the majority, or for propositions that are not stated in the majority opinion and not inconsistent with that majority, or to explain a disagreement with the majority and to urge reform (while following the majority in the outcome).
Treatises, restatements, law review articles.
Courts may consider the writings of eminent legal scholars in treatises, restatements of the law, and law reviews. The extent to which judges find these types of writings persuasive will vary widely with elements such as the reputation of the author and the relevance of the argument.
Persuasive effect of decisions from other jurisdictions.
The courts of England and Wales are free to consider decisions of other jurisdictions, and give them whatever persuasive weight the English court sees fit, even though these other decisions are not binding precedent. Jurisdictions that are closer to modern English common law are more likely to be given persuasive weight, for example Commonwealth states (for example Canada, Australia, or New Zealand). Persuasive weight might be given to other common law courts, such as from the United States, most often where the American courts have been particularly innovative, e.g. in product liability and certain areas of contract law.
In the United States, in the late 20th and early 21st centuries, the concept of a U.S. court considering foreign law or precedent has been considered controversial by some parties. The Supreme Court splits on this issue. In "Atkins v. Virginia", for example, the majority cited as part of their reasoning the fact that the European Union forbids the death penalty. But, Chief Justice Rehnquist opposed the "Court's decision to place weight on foreign laws." The House of Representatives passed a nonbinding resolution criticizing the citing of foreign law and "reaffirming American independence." This critique is recent, as in the early history of the United States, citation of English authority was ubiquitous. One of the first acts of many of the new state legislatures was to adopt the body of English common law into the law of the state. See here. Citation to English cases was common through the 19th and well into the 20th centuries. Even in the late 20th and early 21st centuries, it is relatively uncontroversial for American state courts to rely on English decisions for matters of pure common (i.e. judge-made) law.
Within the federal legal systems of several common-law countries, and most especially the United States, it is relatively common for the distinct lower-level judicial systems (e.g. state courts in the United States and Australia, provincial courts in Canada) to regard the decisions of other jurisdictions within the same country as persuasive precedent. Particularly in the United States, the adoption of a legal doctrine by a large number of other state judiciaries is regarded as highly persuasive evidence that such doctrine is preferred. A good example is the adoption in Tennessee of comparative negligence (replacing contributory negligence as a complete bar to recovery) by the 1992 Tennessee Supreme Court decision "McIntyre v. Balentine" (by this point all US jurisdictions save Tennessee, five other states, and the District of Columbia had adopted comparative negligence schemes). Moreover, in American law, the "Erie" doctrine requires federal courts sitting in diversity actions to apply state substantive law, but in a manner consistent with how the court believes the state's highest court would rule in that case. Since such decisions are not binding on state courts, but are often very well-reasoned and useful, state courts cite federal interpretations of state law fairly often as persuasive precedent, although it is also fairly common for a state high court to reject a federal court's interpretation of its jurisprudence.
Nonprecedential decisions: non-publication and depublication, noncitation rules.
Non-publication of opinions, or unpublished opinions, are those decisions of courts that are not available for citation as precedent because the judges making the opinion deem the case as having less precedential value. Selective publication is the legal process which a judge or justices of a court decide whether a decision is to be or not published in a reporter. "Unpublished" federal appellate decisions are published in the Federal Appendix. Depublication is the power of a court to make a previously published order or opinion unpublished.
Litigation that is settled out of court generates no written decision, and thus has no precedential effect. As one practical effect, the U.S. Department of Justice settles many cases against the federal government simply to avoid creating adverse precedent.
"Res judicata", claim preclusion, collateral estoppel, issue preclusion, law of the case.
Several rules may cause a decision to apply as narrow "precedent" to preclude future legal positions of the specific parties to a case, even if a decision is non-precedential with respect to all other parties.
"Res judicata", claim preclusion.
Once a case is decided, the same plaintiff cannot sue the same defendant again on any claim arising out of the same facts. The law requires plaintiffs to put all issues on the table in a single case, not split the case. For example, in a case of an auto accident, the plaintiff cannot sue first for property damage, and then personal injury in a separate case. This is called "res judicata" or claim preclusion ("'Res judicata'" is the traditional name going back centuries; the name shifted to "claim preclusion" in the United States over the late 20th century). Claim preclusion applies whether the plaintiff wins or loses the earlier case, even if the later case raises a different legal theory, even the second claim is unknown at the time of the first case. Exceptions are extremely limited, for example if the two claims for relief must necessarily be brought in different courts (for example, one claim might be exclusively federal, and the other exclusively state).
collateral estoppel, issue preclusion.
Once a case is finally decided, any issues decided in the previous case may be binding against the party that lost the issue in later cases, even in cases involving other parties. For example, if a first case decides that a party was negligent, then other plaintiffs may rely on that earlier determination in later cases, and need not re-prove the issue of negligence. For another example, if a patent is shown to be invalid in a case against one accused infringer, that same patent is invalid against all other accused infringers—invalidity need not be re-proved. Again, there are limits and exceptions on this principle. The principle is called collateral estoppel or issue preclusion.
law of the case.
Within a single case, once there's been a first appeal, both the lower court and the appellate court itself will not further review the same issue, and will not re-review an issue that could have been appealed in the first appeal. Exceptions are limited to three "exceptional circumstances:" (1) when substantially different evidence is raised at a subsequent trial, (2) when the law changes after the first appeal, for example by a decision of a higher court, or (3) when a decision is clearly erroneous and would result in a manifest injustice. This principle is called "law of the case."
Splits, tensions.
On many questions, reasonable people may differ. When two of those people are judges, the tension among two lines of precedent may be resolved as follows.
Jurisdictional splits: disagreements among different geographical regions or levels of federalism.
If the two courts are in separate, parallel jurisdictions, there is no conflict, and two lines of precedent may persist. Courts in one jurisdiction are influenced by decisions in others, and notably better rules may be adopted over time.
Splits among different areas of law.
Courts try to formulate the common law as a "seamless web" so that principles in one area of the law apply to other areas. However, this principle does not apply uniformly. Thus, a word may have different definitions in different areas of the law, or different rules may apply so that a question has different answers in different legal contexts. Judges try to minimize these conflicts, but they arise from time to time, and under principles of 'stare decisis', may persist for some time.
Matter of first impression.
First impression (known as "primae impressionis" in Latin) is a legal case in which there is no binding authority on the matter presented. Such a case can set forth a completely original issue of law for decision by the courts. A first impression case may be a first impression in only a particular jurisdiction. In that situation, courts will look to holdings of other jurisdictions for persuasive authority.
In the latter meaning, the case in question cannot be decided through referring to and/or relying on precedent. Since the legal issue under consideration has never been decided by an appeals court and, therefore, there is no precedent for the court to follow, the court uses analogies from prior rulings by appeals courts, refers to commentaries and articles by legal scholars, and applies its own logic. In cases of first impression, the trial judge will often ask both sides' attorneys for legal briefs.
In some situations, a case of first impression may exist in a jurisdiction until a reported appellate court decision is rendered.
Contrasting role of case law in common law, civil law, and mixed systems.
The different roles of case law in civil law and common law traditions create differences in the way that courts render decisions. Common law courts generally explain in detail the legal rationale behind their decisions, with citations of both legislation and previous relevant judgments, and often an exegesis of the wider legal principles. The necessary analysis, called "ratio decidendi", then constitutes a precedent binding on other courts; further analyses not strictly necessary to the determination of the current case are called "obiter dicta", which constitute persuasive authority but are not technically binding. By contrast, decisions in civil law jurisdictions are generally very short, referring only to statutes. The reason for this difference is that these civil law jurisdictions adhere to a tradition that the reader should be able to deduce the logic from the decision and the statutes, so that, in some cases, it is somewhat difficult to apply previous decisions to the facts presented in future cases.
Civil law systems.
"Stare decisis" is not usually a doctrine used in civil law systems, because it violates the principle that only the legislature may make law. However, the civil law system does have jurisprudence constante, which is similar to "stare decisis" and dictates that the Court's decision condone a cohesive and predictable result. In theory, inferior courts are generally not bound to precedent established by superior courts. In practice, the need for predictability means that inferior courts generally defer to precedent by superior courts. In a sense, the most superior courts in civil law jurisdictions, such as the "Cour de cassation" and the "Conseil d'État" in France are recognized as being bodies of a quasi-legislative nature.
The doctrine of jurisprudence constante also influences how court decisions are structured. In general, court decisions of common law jurisdictions give a sufficient statement of rationale as to guide future courts. This occurs to justify a court decision on the basis of previous case law as well as to make it easier to use the decision as a precedent for future cases.
By contrast, court decisions in some civil law jurisdictions (most prominently France) tend to be extremely brief, mentioning only the relevant legislation and not going into great detail about how a decision was reached. This is the result of the theoretical view that the court is only interpreting the view of the legislature and that detailed exposition is unnecessary. Because of this, much more of the exposition of the law is done by academic jurists which provide the explanations that in common law nations would be provided by the judges themselves.
In other civil law jurisdictions, such as the German-speaking countries, court opinions tend to be much longer than in France, and courts will frequently cite previous cases and academic writing. However, some courts (such as German courts) have less emphasis on the particular facts of the case than common law courts, but have more emphasis on the discussion of various doctrinal arguments and on finding what the correct interpretation of the law is.
The legal systems of the Nordic countries are sometimes included among the civil law systems, but as a separate branch, and sometimes counted as separate from the civil law tradition. In Sweden, for instance, case law arguably plays a more important role than in some of the Continental civil law systems. The two highest courts, the Supreme Court ("Högsta domstolen") and the Supreme Administrative Court ("Högsta förvaltningsdomstolen"), have the right to set precedent which is in practice (however not formally) binding on all future application of the law. Courts of appeal, both general courts ("hovrätter") and administrative courts ("kammarrätter") may also issue decisions that act as guides for the application of the law, but these decisions may be overturned by higher courts.
Mixed or bijuridical systems.
Some pluralist systems, such as Scots law in Scotland and so-called civil law jurisdictions in Quebec and Louisiana, do not precisely fit into the dual "common-civil" law system classifications. Such systems may have been heavily influenced by the Anglo-American common law tradition; however, their substantive law is firmly rooted in the civil law tradition. Because of their position between the two main systems of law, these types of legal systems are sometimes referred to as "mixed" systems of law.
The role of academics in compiling and interpreting case law in civil law jurisdictions.
Law professors in common law traditions play a much smaller role in developing case law than professors in civil law traditions. Because court decisions in civil law traditions are brief and not amenable to establishing precedent, much of the exposition of the law in civil law traditions is done by academics rather than by judges; this is called doctrine and may be published in treatises or in journals such as "Recueil Dalloz" in France. Historically, common law courts relied little on legal scholarship; thus, at the turn of the twentieth century, it was very rare to see an academic writer quoted in a legal decision (except perhaps for the academic writings of prominent judges such as Coke and Blackstone). Today academic writers are often cited in legal argument and decisions as persuasive authority; often, they are cited when judges are attempting to implement reasoning that other courts have not yet adopted, or when the judge believes the academic's restatement of the law is more compelling than can be found in precedent. Thus common law systems are adopting one of the approaches long common in civil law jurisdictions.
Critical analysis of precedent.
Court formulations.
Justice Louis Brandeis, in a heavily-footnoted dissent to "Burnet v. Coronado Oil & Gas Co.", 285 U.S. 393, 405-411 (1932), explained (citations and quotations omitted):
The United States Court of Appeals for the Third Circuit has stated:
The United States Court of Appeals for the Ninth Circuit has stated:
Justice McHugh of the High Court of Australia in relation to precedents remarked in "Perre v Apand":
Academic study.
Precedent viewed against passing time can serve to establish trends, thus indicating the next logical step in evolving interpretations of the law. For instance, if immigration has become more and more restricted under the law, then the next legal decision on that subject may serve to restrict it further still. The existence of submerged precedent (reasoned opinions not made available through conventional legal research sources) has been identified as a potentially distorting force in the evolution of law.
Scholars have recently attempted to apply network theory to precedent in order to establish which precedent is most important or authoritative, and how the court's interpretations and priorities have changed over time.
Application.
Development.
Early English common law did not have or require the "stare decisis" doctrine for a range of legal and technological reasons:
These features changed over time, opening the door to the doctrine of "stare decisis":
United States legal system.
"Stare decisis" applies to the holding of a case, rather than to obiter dicta ("things said by the way"). As the United States Supreme Court has put it: "dicta may be followed if sufficiently persuasive but are not binding."
In the United States Supreme Court, the principle of stare decisis is most flexible in constitutional cases:
For example, in the years 1946–1992, the U.S. Supreme Court reversed itself in about 130 cases. The U.S. Supreme Court has further explained as follows:
The United States Supreme Court has stated that where a court gives multiple reasons for a given result, each alternative reason that is "explicitly" labeled by the court as an "independent" ground for the decision is not treated as "simply a dictum."
English legal system.
The doctrine of binding precedent or "stare decisis" is basic to the English legal system, as described in the rest of this article. Special features of the English legal system include the following:
Last resort and strict "stare decisis" in the House of Lords and UK Supreme Court.
The British House of Lords, as the court of last appeal outside Scotland before the creation of the UK Supreme Court, was not strictly bound to always follow its own decisions until the case "London Street Tramways v London County Council AC 375". After this case, once the Lords had given a ruling on a point of law, the matter was closed unless and until Parliament made a change by statute. This is the most strict form of the doctrine of "stare decisis" (one not applied, previously, in common law jurisdictions, where there was somewhat greater flexibility for a court of last resort to review its own precedent).
This situation changed, however, after the issuance of the Practice Statement of 1966. It enabled the House of Lords to adapt English law to meet changing social conditions. In "R v G & R" 2003, the House of Lords overruled its decision in "Caldwell" 1981, which had allowed the Lords to establish mens rea ("guilty mind") by measuring a defendant's conduct against that of a "reasonable person," regardless of the defendant's actual state of mind.
However, the Practice Statement has been seldom applied by the House of Lords, usually only as a last resort. As of 2005, the House of Lords has rejected its past decisions no more than 20 times. They are reluctant to use it because they fear to introduce uncertainty into the law. In particular, the Practice Statement stated that the Lords would be especially reluctant to overrule themselves in criminal cases because of the importance of certainty of that law. The first case involving criminal law to be overruled with the Practice Statement was "Anderton v Ryan" (1985), which was overruled by "R v Shivpuri" (1986), two decades after the Practice Statement. Remarkably, the precedent overruled had been made only a year before, but it had been criticised by several academic lawyers. As a result, Lord Bridge stated he was "undeterred by the consideration that the decision in "Anderton v Ryan" was so recent. The Practice Statement is an effective abandonment of our pretention to infallibility. If a serious error embodied in a decision of this House has distorted the law, the sooner it is corrected the better." Still, the House of Lords has remained reluctant to overrule itself in some cases; in "R v Kansal" (2002), the majority of House members adopted the opinion that "R v Lambert" had been wrongly decided and agreed to depart from their earlier decision.
Distinguishing precedent on legal (rather than fact) grounds.
A precedent does not bind a court if it finds there was a lack of care in the original "Per Incuriam". For example, if a statutory provision or precedent had not been brought to the previous court's attention before its decision, the precedent would not be binding.
Rules of Statutory Interpretation.
Statutory Interpretation in the U.K..
Judges and barristers in the U.K use three primary rules for interpreting the law.
The normal aids that a judge has include access to all previous cases in which a precedent has been set, and a good English dictionary.
Under the literal rule, the judge should do what the actual legislation states rather than trying to do what the judge thinks that it means. The judge should use the plain everyday ordinary meaning of the words, even if this produces an unjust or undesirable outcome. A good example of problems with this method is "R v Maginnis" (1987) [http://www.bailii.org/uk/cases/UKHL/1987/4.html] in which several judges in separate opinions found several different dictionary meanings of the word "supply." Another example might be "Fisher v Bell", where it was held that a shopkeeper who placed an illegal item in a shop window with a price tag did not make an offer to sell it, because of the specific meaning of "offer for sale" in contract law. As a result of this case, Parliament amended the statute concerned to end this discrepancy.
The golden rule is used when use of the literal rule would obviously create an absurd result. The court must find genuine difficulties before it declines to use the literal rule. There are two ways in which the Golden Rule can be applied: the narrow method, and the broad method. Under the narrow method, when there are apparently two contradictory meanings to a word used in a legislative provision or it is ambiguous, the least absurd is to be used. For example, in "Adler v George" (1964), the defendant was found guilty under the Official Secrets Act of 1920. The act said it was an offence to obstruct HM Forces in the vicinity of a prohibited place. Mr. Adler argued that he was not in the "vicinity" of a prohibited place but was actually "in" a prohibited place. The court chose not to accept the wording literally. Under the broad method, the court may reinterpret the law at will when it is clear that there is only one way to read the statute. This occurred in "Re Sigsworth" (1935) where a man who murdered his mother was forbidden from inheriting her estate, despite a statute to the contrary.
The mischief rule is the most flexible of the interpretation methods. Stemming from "Heydon's Case" (1584), it allows the court to enforce what the statute is intended to remedy rather than what the words actually say. For example, in "Corkery v Carpenter" (1950), a man was found guilty of being drunk in charge of a carriage, although in fact he only had a bicycle.
Statutory Interpretation in the United States.
In the United States, the courts have stated consistently that the text of the statute is read as it is written, using the ordinary meaning of the words of the statute.
Practical application.
Although inferior courts are bound in theory by superior court precedent, in practice a judge may believe that justice requires an outcome at some variance with precedent, and may distinguish the facts of the individual case on reasoning that does not appear in the binding precedent. On appeal, the appellate court may either adopt the new reasoning, or reverse on the basis of precedent. On the other hand, if the losing party does not appeal (typically because of the cost of the appeal), the lower court decision may remain in effect, at least as to the individual parties.
Judicial resistance.
Occasionally, a lower court judge explicitly states personal disagreement with the judgment he or she has rendered, but that he or she is required to do so by binding precedent. Note that inferior courts cannot evade binding precedent of superior courts, but a court can depart from its own prior decisions.
Structural considerations.
In the United States, "stare decisis" can interact in counterintuitive ways with the federal and state court systems. On an issue of federal law, a state court is not bound by an interpretation of federal law at the district or circuit level, but is bound by an interpretation by the United States Supreme Court. On an interpretation of state law, whether common law or statutory law, the federal courts are bound by the interpretation of a state court of last resort, and are required normally to defer to the precedent of intermediate state courts as well.
Courts may choose to obey precedent of international jurisdictions, but this is not an application of the doctrine of "stare decisis", because foreign decisions are not binding. Rather, a foreign decision that is obeyed on the basis of the soundness of its reasoning will be called "persuasive authority" — indicating that its effect is limited to the persuasiveness of the reasons it provides.
Originalism.
Originalism — the doctrine that holds that the meaning of a written text must be applied — is in tension with "stare decisis", but is not necessarily opposed irrevocably. As noted above, ""Stare decisis" is not usually a doctrine used in civil law systems, because it violates the principle that only the legislature may make law"; Justice Antonin Scalia argues in "A Matter of Interpretation" that America is a civil law nation, not a common law nation. By principle, originalists are generally unwilling to defer to precedent when precedent seems to come into conflict with the Constitution. However, there is still room within an originalist paradigm for "stare decisis"; whenever the plain meaning of the text has alternative constructions, past precedent is generally considered a valid guide, with the qualifier being that it cannot change what the text actually says.
Some originalists may be even more extreme. In his confirmation hearings, Justice Clarence Thomas answered a question from Senator Strom Thurmond, qualifying his willingness to change precedent in this way:
Possibly he has changed his mind, or there are a very large body of cases which merit "the additional step" of ignoring the doctrine; according to Scalia, "Clarence Thomas doesn't believe in stare decisis, period. If a constitutional line of authority is wrong, he would say, let's get it right."
Professor Caleb Nelson, a former clerk for Justice Thomas and law professor at the University of Virginia, has elaborated on the role of "stare decisis" in originalist jurisprudence:
Advantages and disadvantages.
There are disadvantages and advantages of binding precedent, as noted by scholars and jurists.
Criticism of precedent.
In a 1997 book, attorney Michael Trotter blamed over-reliance by American lawyers on binding and persuasive authority, rather than the merits of the case at hand, as a major factor behind the escalation of legal costs during the 20th century. He argued that courts should ban the citation of persuasive precedent from outside their jurisdiction, with two exceptions:
The disadvantages of "stare decisis" include its rigidity, the complexity of learning law, the differences between some cases may be very small and appear illogical, and the slow growth or incremental changes to the law that are in need of major overhaul.
An argument often used against the system is that it is undemocratic as it allows judges, which may or may not be elected, to make law.
Regarding constitutional interpretations, there is concern that over-reliance on the doctrine of "stare decisis" can be subversive. An erroneous precedent may at first be only slightly inconsistent with the Constitution, and then this error in interpretation can be propagated and increased by further precedent until a result is obtained that is greatly different from the original understanding of the Constitution. "Stare decisis" is not mandated by the Constitution, and if it causes unconstitutional results then the historical evidence of original understanding can be re-examined. In this opinion, predictable fidelity to the Constitution is more important than fidelity to unconstitutional precedent. See also the living tree doctrine.
Agreement with precedent.
A counter-argument (in favor of the advantages of "stare decisis") is that if the legislature wishes to alter the case law (other than constitutional interpretations) by statute, the legislature is empowered to do so. Critics sometimes accuse particular judges of applying the doctrine selectively, invoking it to support precedent that the judge supported anyway, but ignoring it in order to change precedent with which the judge disagreed.
There is much discussion about the virtue of using "stare decisis". Supporters of the system, such as minimalists, argue that obeying precedent makes decisions "predictable." For example, a business person can be reasonably assured of predicting a decision where the facts of his or her case are sufficiently similar to a case decided previously. This parallels the arguments against retroactive (ex post facto) laws banned by the U.S. Constitution.

</doc>
<doc id="23005" url="https://en.wikipedia.org/wiki?curid=23005" title="Philip K. Dick">
Philip K. Dick

Philip Kindred Dick (December 16, 1928March 2, 1982) was an American writer, whose published works mainly belong to the genre of science fiction. 
Dick explored philosophical, sociological and political themes in novels with plots dominated by monopolistic corporations, authoritarian governments, and altered states of consciousness. In his later works, Dick's thematic focus tended to reflect his personal interest in metaphysics and theology.
He often drew upon his life experiences in addressing the nature of drug abuse, paranoia, schizophrenia, and transcendental experiences in novels such as "A Scanner Darkly" and "VALIS". Later in life, he wrote non-fiction on philosophy, theology, the nature of reality, and science. This material was published posthumously as "The Exegesis".
The novel "The Man in the High Castle" bridged the genres of alternate history and science fiction, earning Dick a Hugo Award for Best Novel in 1963. "Flow My Tears, the Policeman Said", a novel about a celebrity who awakens one day to find that he is unknown, won the John W. Campbell Memorial Award for best novel in 1975. "I want to write about people I love, and put them into a fictional world spun out of my own mind, not the world we actually have, because the world we actually have does not meet my standards," Dick wrote of these stories. "In my writing I even question the universe; I wonder out loud if it is real, and I wonder out loud if all of us are real."
In addition to 44 published novels, Dick wrote approximately 121 short stories, most of which appeared in science fiction magazines during his lifetime. Although Dick spent most of his career as a writer in near-poverty, eleven popular films based on his works have been produced, including "Blade Runner", "Total Recall", "A Scanner Darkly", "Minority Report", "Paycheck", "Next", "Screamers", "The Adjustment Bureau" and "Impostor". In 2005, "Time" magazine named "Ubik" one of the hundred greatest English-language novels published since 1923. In 2007, Dick became the first science fiction writer to be included in The Library of America series.
Personal life.
Philip Kindred Dick and his twin sister, Jane Charlotte Dick, were born six weeks prematurely on December 16, 1928, in Chicago, Illinois, to Dorothy Kindred Dick and Joseph Edgar Dick, who worked for the United States Department of Agriculture. The death of Jane six weeks later, on January 26, 1929, profoundly affected Philip's life, leading to the recurrent motif of the "phantom twin" in his books.
His family later moved to the San Francisco Bay Area. When Philip was five, his father was transferred to Reno, Nevada; when Dorothy refused to move, she and Joseph divorced. Both parents fought for custody of Philip, which was awarded to the mother. Dorothy, determined to raise Philip alone, took a job in Washington, D.C., and moved there with her son. Philip was enrolled at John Eaton Elementary School (1936–38), completing the second through fourth grades. His lowest grade was a "C" in Written Composition, although a teacher remarked that he "shows interest and ability in story telling." He was educated in Quaker schools. In June 1938, Dorothy and Philip returned to California, and it was around this time that he became interested in science fiction. Dick stated that he read his first science fiction magazine, "Stirring Science Stories" in 1940 at the age of twelve.
Dick attended Berkeley High School in Berkeley, California. He and fellow science fiction author Ursula K. Le Guin were members of the same graduating class (1947) but did not know each other at the time. After graduation, he briefly attended the University of California, Berkeley, (September 1949 to November 11, 1949) with an honorable dismissal granted January 1, 1950. Dick did not declare a major and took classes in history, psychology, philosophy, and zoology. Through his studies in philosophy, he believed that existence is based on the internal-based perception of a human, which does not necessarily correspond to external reality; he described himself as "an acosmic panentheist," believing in the universe only as an extension of God. After reading the works of Plato and pondering the possibilities of metaphysical realms, Dick came to the conclusion that, in a certain sense, the world is not entirely real and there is no way to confirm whether it is truly there. This question from his early studies persisted as a theme in many of his novels. Dick dropped out because of ongoing anxiety problems, according to his third wife Anne's memoir. She also says he disliked the mandatory ROTC training. At Berkeley, Dick befriended poet Robert Duncan and poet and linguist Jack Spicer, who gave Dick ideas for a Martian language. Dick claimed to have been host of a classical music program on KSMO Radio in 1947.
From 1948 to 1952, Dick worked at Art Music Company, a record store on Telegraph Avenue. In 1955, he and his second wife, Kleo Apostolides, received a visit from the FBI, which they believed to be the result of Kleo's socialist views and left-wing activities. The couple briefly befriended one of the FBI agents.
Dick was married five times:
Dick had three children, Laura Archer (February 25, 1960), Isolde Freya (now Isa Dick Hackett) (March 15, 1967), and Christopher Kenneth (July 25, 1973).
Dick tried to stay out of the political scene because of high societal turmoil from the Vietnam War; however, he did show some anti-Vietnam War and anti-governmental sentiments. In 1968, he joined the "Writers and Editors War Tax Protest", an anti-war pledge to pay no U.S. federal income tax, which resulted in the confiscation of his car by the IRS.
Career.
Dick sold his first story in 1951, and from then on wrote full-time. During 1952 his first speculative fiction publications appeared in July and September numbers of "Planet Stories", edited by Jack O'Sullivan, and in "If" and "The Magazine of Fantasy and Science Fiction" that fall. His debut novel was "Solar Lottery", published in 1955 as half of Ace Double #D-103 alongside "The Big Jump" by Leigh Brackett. The 1950s were a difficult and impoverished time for Dick, who once lamented, "We couldn't even pay the late fees on a library book." He published almost exclusively within the science fiction genre, but dreamed of a career in mainstream American literature. During the 1950s he produced a series of non-genre, relatively conventional novels. In 1960 he wrote that he was willing to "take twenty to thirty years to succeed as a literary writer." The dream of mainstream success formally died in January 1963 when the Scott Meredith Literary Agency returned all of his unsold mainstream novels. Only one of these works, "Confessions of a Crap Artist", was published during Dick's lifetime.
In 1963, Dick won the Hugo Award for "The Man in the High Castle". Although he was hailed as a genius in the science fiction world, the mainstream literary world was unappreciative, and he could publish books only through low-paying science fiction publishers such as Ace. Even in his later years, he continued to have financial troubles. In the introduction to the 1980 short story collection "The Golden Man", Dick wrote:
In 1972, Dick donated manuscripts, papers and other materials to the Special Collections Library at California State University, Fullerton where they are archived in the Philip K. Dick Science Fiction Collection in the Pollak Library. It was in Fullerton that Philip K. Dick befriended budding science-fiction writers K. W. Jeter, James Blaylock, and Tim Powers. The last novel Dick wrote was "The Transmigration of Timothy Archer"; it was published shortly after his death in 1982.
Flight to Canada and suicide attempt.
In 1971, Dick's marriage to Nancy Hackett broke down, and she moved out of their shared home. Dick descended into amphetamine abuse, eventually allowing a number of other drug users to move into the house with him. One day in November of that year, Dick returned to his home in San Rafael to discover that it had been burgled, with his safe blown open and personal papers missing. The police were unable to determine the culprit, and even suspected Dick of having done so himself. Shortly afterwards, he was invited to be guest of honor at the Vancouver Science Fiction Convention in February 1972. Within a day of arriving at the conference and giving his speech "The Android and the Human", he informed people that he had fallen in love with a woman that he had met there, called Janis, and announced that he would be remaining in Vancouver. An attendee of the conference, Michael Walsh, movie critic for local newspaper The Province, invited Dick to stay in his home, but had to ask him to leave two weeks later due to his erratic behavior. This was followed by Janis ending her and Dick's relationship and moving away. On the 23rd of March 1972, Dick attempted to commit suicide by consuming an overdose of the sedative potassium bromide. Subsequently, after deciding to seek help, Dick became a participant in X-Kalay (a Canadian Synanon-type recovery program), and was well enough by April that he was able to return to California.
Dick returned to the events of these months while writing his 1977 novel "A Scanner Darkly", which contains fictionalized depictions of the burglary of his home, his time using amphetamines and living with addicts, and his experiences of X-Kalay (portrayed in the novel as "New-Path"). A factual account of Dick's recovery program participation was portrayed in his posthumously released book "The Dark Haired Girl", a collection of letters and journals from the period.
Paranormal experiences and mental health issues.
On February 20, 1974, while recovering from the effects of sodium pentothal administered for the extraction of an impacted wisdom tooth, Dick received a home delivery of Darvon from a young woman. When he opened the door, he was struck by the beauty of the dark-haired girl and was especially drawn to her golden necklace. He asked her about its curious fish-shaped design. "This is a sign used by the early Christians," she said, and then left. Dick called the symbol the "vesicle pisces". This name seems to have been based on his conflation of two related symbols, the Christian ichthys symbol (two intersecting arcs delineating a fish in profile) which the woman was wearing, and the vesica piscis.
Dick recounted that as the sun glinted off the gold pendant, the reflection caused the generation of a "pink beam" of light that mesmerized him. He came to believe the beam imparted wisdom and clairvoyance, and also believed it to be intelligent. On one occasion, Dick was startled by a separate recurrence of the pink beam. It imparted the information to him that his infant son was ill. The Dicks rushed the child to the hospital, where his suspicion was confirmed by professional diagnosis.
After the woman's departure, Dick began experiencing strange hallucinations. Although initially attributing them to side effects from medication, he considered this explanation implausible after weeks of continued hallucinations. "I experienced an invasion of my mind by a transcendentally rational mind, as if I had been insane all my life and suddenly I had become sane," Dick told Charles Platt.
Throughout February and March 1974, Dick experienced a series of hallucinations, which he referred to as "2-3-74", shorthand for February–March 1974. Aside from the "pink beam", Dick described the initial hallucinations as geometric patterns, and, occasionally, brief pictures of Jesus and ancient Rome. As the hallucinations increased in length and frequency, Dick claimed he began to live two parallel lives, one as himself, "Philip K. Dick", and one as "Thomas", a Christian persecuted by Romans in the first century AD. He referred to the "transcendentally rational mind" as "Zebra", "God" and "VALIS". Dick wrote about the experiences, first in the semi-autobiographical novel "Radio Free Albemuth" and then in "VALIS", "The Divine Invasion" and the unfinished "The Owl in Daylight" (the VALIS trilogy).
At one point Dick felt that he had been taken over by the spirit of the prophet Elijah. He believed that an episode in his novel "Flow My Tears, the Policeman Said" was a detailed retelling of a biblical story from the Book of Acts, which he had never read. Dick documented and discussed his experiences and faith in a private journal, later published as "The Exegesis of Philip K. Dick".
Pen names.
Dick had two professional stories published under the pen names Richard Phillipps and Jack Dowland. "Some Kinds of Life" was published in October 1953 in Fantastic Universe under byline Richard Phillipps, apparently because the magazine had a policy against publishing multiple stories by the same author in the same issue; "Planet for Transients" was published in the same issue under his own name.
The short story "Orpheus with Clay Feet" was published under the pen name Jack Dowland. The protagonist desires to be the muse for fictional author Jack Dowland, considered the greatest science fiction author of the 20th century. In the story, Dowland publishes a short story titled "Orpheus with Clay Feet" under the pen name Philip K. Dick.
The surname Dowland refers to Renaissance composer John Dowland, who is featured in several works. The title "Flow My Tears, the Policeman Said" directly refers to Dowland's best-known composition, "Flow My Tears". In the novel "The Divine Invasion", the character Linda Fox, created specifically with Linda Ronstadt in mind, is an intergalactically famous singer whose entire body of work consists of recordings of John Dowland compositions. Also, some protagonists in Dick's short fiction are named Dowland.
Style and works.
Themes.
Dick's stories typically focus on the fragile nature of what is "real" and the construction of personal identity. His stories often become surreal fantasies, as the main characters slowly discover that their everyday world is actually an illusion constructed by powerful external entities (such as in "Ubik"), vast political conspiracies, or simply from the vicissitudes of an unreliable narrator. "All of his work starts with the basic assumption that there cannot be one, single, objective reality", writes science fiction author Charles Platt. "Everything is a matter of perception. The ground is liable to shift under your feet. A protagonist may find himself living out another person's dream, or he may enter a drug-induced state that actually makes better sense than the real world, or he may cross into a different universe completely."
Alternate universes and simulacra were common plot devices, with fictional worlds inhabited by common, working people, rather than galactic elites. "There are no heroes in Dick's books", Ursula K. Le Guin wrote, "but there are heroics. One is reminded of Dickens: what counts is the honesty, constancy, kindness and patience of ordinary people." Dick made no secret that much of his thinking and work was heavily influenced by the writings of Carl Jung. The Jungian constructs and models that most concerned Dick seem to be the archetypes of the collective unconscious, group projection/hallucination, synchronicities, and personality theory. Many of Dick's protagonists overtly analyze reality and their perceptions in Jungian terms (see "Lies Inc."). Dick's self-named "Exegesis" also contained many notes on Jung in relation to theology and mysticism.
Dick identified one major theme of his work as the question, "What constitutes the authentic human being?" In works such as "Do Androids Dream of Electric Sheep?", beings can appear totally human in every respect while lacking soul or compassion, while completely alien beings such as Glimmung in "Galactic Pot-Healer" may be more humane and complex than Dick's human characters.
Mental illness was a constant interest of Dick's, and themes of mental illness permeate his work. The character Jack Bohlen in the 1964 novel "Martian Time-Slip" is an "ex-schizophrenic". The novel "Clans of the Alphane Moon" centers on an entire society made up of descendants of lunatic asylum inmates. In 1965 he wrote the essay titled "Schizophrenia and the Book of Changes".
Drug use (including religious, recreational, and abuse) was also a theme in many of Dick's works, such as "A Scanner Darkly" and "The Three Stigmata of Palmer Eldritch". Dick himself was a drug user for much of his life. According to a 1975 interview in "Rolling Stone", Dick wrote all of his books published before 1970 while on amphetamines. ""A Scanner Darkly" (1977) was the first complete novel I had written without speed", said Dick in the interview. He also experimented briefly with psychedelics, but wrote "The Three Stigmata of Palmer Eldritch", which "Rolling Stone" dubs "the classic LSD novel of all time", before he had ever tried them. Despite his heavy amphetamine use, however, Dick later said that doctors told him the amphetamines never actually affected him, that his liver had processed them before they reached his brain.
Summing up all these themes in "Understanding Philip K. Dick", Eric Carl Link discussed eight themes or 'ideas and motifs': Epistemology and the Nature of Reality, Know Thyself, The Android and the Human, Entropy and Pot Healing, The Theodicy Problem, Warfare and Power Politics, The Evolved Human, and 'Technology, Media, Drugs and Madness'.
Selected works.
"The Man in the High Castle" (1962) is set in an alternative history in which the United States is ruled by the victorious Axis powers. It is the only Dick novel to win a Hugo Award.
"The Three Stigmata of Palmer Eldritch" (1965) utilizes an array of science fiction concepts and features several layers of reality and unreality. It is also one of Dick's first works to explore religious themes. The novel takes place in the 21st century, when, under UN authority, mankind has colonized the Solar System's every habitable planet and moon. Life is physically daunting and psychologically monotonous for most colonists, so the UN must draft people to go to the colonies. Most entertain themselves using "Perky Pat" dolls and accessories manufactured by Earth-based "P.P. Layouts". The company also secretly creates "Can-D", an illegal but widely available hallucinogenic drug allowing the user to "translate" into Perky Pat (if the drug user is a woman) or Pat's boyfriend, Walt (if the drug user is a man). This recreational use of Can-D allows colonists to experience a few minutes of an idealized life on Earth by participating in a collective hallucination.
"Do Androids Dream of Electric Sheep?" (1968) is the story of a bounty hunter policing the local android population. It occurs on a dying, poisoned Earth de-populated of all "successful" humans; the only remaining inhabitants of the planet are people with no prospects off-world. The 1968 novel is the literary source of the film "Blade Runner" (1982). It is both a conflation and an intensification of the pivotally Dickian question, What is real, what is fake? What crucial factor defines humanity as distinctly 'alive', versus those merely alive only in their outward appearance?
"Ubik" (1969) uses extensive networks of psychics and a suspended state after death in creating a state of eroding reality. A group of psychics is sent to investigate a group of rival psychics, but several of them are apparently killed by a saboteur's bomb. Much of the novel flicks between a number of equally plausible realities; the "real" reality, a state of half-life and psychically manipulated realities. In 2005, "Time" magazine listed it among the "All-TIME 100 Greatest Novels" published since 1923.
"Flow My Tears, the Policeman Said" (1974) concerns Jason Taverner, a television star living in a dystopian near-future police state. After being attacked by an angry ex-girlfriend, Taverner awakens in a dingy Los Angeles hotel room. He still has his money in his wallet, but his identification cards are missing. This is no minor inconvenience, as security checkpoints (manned by "pols" and "nats", the police and National Guard) are set up throughout the city to stop and arrest anyone without valid ID. Jason at first thinks that he was robbed, but soon discovers that his entire identity has been erased. There is no record of him in any official database, and even his closest associates do not recognize or remember him. For the first time in many years, Jason has no fame or reputation to rely on. He has only his innate charisma to help him as he tries to find out what happened to his past while avoiding the attention of the pols. The novel was Dick's first published novel after years of silence, during which time his critical reputation had grown, and this novel was awarded the John W. Campbell Memorial Award for Best Science Fiction Novel. It is the only Philip K. Dick novel nominated for both a Hugo and a Nebula Award.
In an essay written two years before dying, Dick described how he learned from his Episcopalian priest that an important scene in "Flow My Tears, the Policeman Said" – involving its other main character, Police General Felix Buckman, the policeman of the title – was very similar to a scene in "Acts of the Apostles", a book of the Christian New Testament. Film director Richard Linklater discusses this novel in his film "Waking Life", which begins with a scene reminiscent of another Dick novel, "Time Out of Joint".
"A Scanner Darkly" (1977) is a bleak mixture of science fiction and police procedural novels; in its story, an undercover narcotics police detective begins to lose touch with reality after falling victim to the same permanently mind-altering drug, Substance D, he was enlisted to help fight. Substance D is instantly addictive, beginning with a pleasant euphoria which is quickly replaced with increasing confusion, hallucinations and eventually total psychosis. In this novel, as with all Dick novels, there is an underlying thread of paranoia and dissociation with multiple realities perceived simultaneously. It was adapted to film by Richard Linklater.
"The Philip K. Dick Reader" is an introduction to the variety of Dick's short fiction.
"VALIS" (1980) is perhaps Dick's most postmodern and autobiographical novel, examining his own unexplained experiences. It may also be his most academically studied work, and was adapted as an opera by Tod Machover. Later works like the VALIS trilogy were heavily autobiographical, many with "two-three-seventy-four" (2-3-74) references and influences. The word VALIS is the acronym for "Vast Active Living Intelligence System". Later, Dick theorized that VALIS was both a "reality generator" and a means of extraterrestrial communication. A fourth VALIS manuscript, "Radio Free Albemuth", although composed in 1976, was posthumously published in 1985. This work is described by the publisher (Arbor House) as "an introduction and key to his magnificent VALIS trilogy."
Regardless of the feeling that he was somehow experiencing a divine communication, Dick was never fully able to rationalize the events. For the rest of his life, he struggled to comprehend what was occurring, questioning his own sanity and perception of reality. He transcribed what thoughts he could into an eight-thousand-page, one-million-word journal dubbed the "Exegesis". From 1974 until his death in 1982, Dick spent many nights writing in this journal. A recurring theme in "Exegesis" is Dick's hypothesis that history had been stopped in the first century CE, and that "the Empire never ended". He saw Rome as the pinnacle of materialism and despotism, which, after forcing the Gnostics underground, had kept the population of Earth enslaved to worldly possessions. Dick believed that VALIS had communicated with him, and anonymous others, to induce the impeachment of U.S. President Richard Nixon, whom Dick believed to be the current Emperor of Rome incarnate.
In a 1968 essay titled "Self Portrait", collected in the 1995 book "The Shifting Realities of Philip K. Dick", Dick reflects on his work and lists which books he feels "might escape World War Three": "Eye in the Sky", "The Man in the High Castle", "Martian Time-Slip", "Dr. Bloodmoney, or How We Got Along After the Bomb", "The Zap Gun", "The Penultimate Truth", "The Simulacra", "The Three Stigmata of Palmer Eldritch" (which he refers to as "the most vital of them all"), "Do Androids Dream of Electric Sheep?", and "Ubik". In a 1976 interview, Dick cited "A Scanner Darkly" as his best work, feeling that he "had finally written a true masterpiece, after 25 years of writing".
Adaptations.
Films.
A number of Dick's stories have been made into films. Dick himself wrote a screenplay for an intended film adaptation of "Ubik" in 1974, but the film was never made. Many film adaptations have not used Dick's original titles. When asked why this was, Dick's ex-wife Tessa said, "Actually, the books rarely carry Phil's original titles, as the editors usually wrote new titles after reading his manuscripts. Phil often commented that he couldn't write good titles. If he could, he would have been an advertising writer instead of a novelist." Films based on Dick's writing have accumulated a total revenue of over US $1 billion as of 2009.
Future films based on Dick's writing include an animated adaptation of "The King of the Elves" from Walt Disney Animation Studios, set to be released in the spring of 2016; and a film adaptation of "Ubik" which, according to Dick's daughter, Isa Dick Hackett, is in advanced negotiation. Ubik is set to be made into a film by Michel Gondry.
The "Terminator" series also uses the theme of humanoid assassination machines portrayed in "Second Variety". The Halcyon Company, known for developing the "Terminator" franchise, acquired right of first refusal to film adaptations of the works of Philip K. Dick in 2007. In May 2009, they announced plans for an adaptation of "Flow My Tears, the Policeman Said".
Television.
It was reported in 2010 that Ridley Scott would produce an adaptation of "The Man in the High Castle" for the BBC, in the form of a mini-series. A pilot episode was released on Amazon Prime in January 2015 and Season 1 was fully released in ten episodes of about 60 minutes each on 20 Nov 2015.
In late 2015, Fox aired "The Minority Report", a sequel adaptation to the 2002 film of the same name based on Dick's 1956 short story "The Minority Report".
Stage and radio.
Four of Dick's works have been adapted for the stage.
One was the opera "VALIS", composed and with libretto by Tod Machover, which premiered at the Pompidou Center in Paris on December 1, 1987, with a French libretto. It was subsequently revised and readapted into English, and was recorded and released on CD (Bridge Records BCD9007) in 1988.
Another was "Flow My Tears, the Policeman Said", adapted by Linda Hartinian and produced by the New York-based avant-garde company Mabou Mines. It premiered in Boston at the Boston Shakespeare Theatre (June 18–30, 1985) and was subsequently staged in New York and Chicago. Productions of "Flow My Tears, the Policeman Said" were also staged by the Evidence Room in Los Angeles in 1999 and by the Fifth Column Theatre Company at the Oval House Theatre in London in the same year.
A play based on "Radio Free Albemuth" also had a brief run in the 1980s.
In November 2010, a production of "Do Androids Dream of Electric Sheep?", adapted by Edward Einhorn, premiered at the 3LD Art and Technology Center in Manhattan.
A radio drama adaptation of Dick's short story "Mr. Spaceship" was aired by the Finnish Broadcasting Company (Yleisradio) in 1996 under the name "Menolippu Paratiisiin". Radio dramatizations of Dick's short stories "Colony" and "The Defenders" were aired by NBC in 1956 as part of the series "X Minus One".
Comics.
Marvel Comics adapted Dick's short story "The Electric Ant" as a limited series which was released in 2009. The comic was produced by writer David Mack ("Daredevil") and artist Pascal Alixe ("Ultimate X-Men"), with covers provided by artist Paul Pope. "The Electric Ant" had earlier been loosely adapted by Frank Miller and Geof Darrow in their 3-issue mini-series "Hard Boiled" published by Dark Horse Comics in 1990-1992.
In 2009, BOOM! Studios started publishing a 24-issue miniseries comic book adaptation of "Do Androids Dream of Electric Sheep?" "Blade Runner", the 1982 film adapted from "Do Androids Dream of Electric Sheep?", had previously been adapted to comics as "".
In 2011, Dynamite Entertainment published a 4-issue miniseries "Total Recall," a sequel to the 1990 film "Total Recall", inspired by Philip K. Dick's short story "We Can Remember It for You Wholesale". In 1990, DC Comics published the official adaptation of the original film as a "".
Alternate formats.
In response to a 1975 request from the National Library for the Blind for permission to make use of "The Man in the High Castle", Dick responded, "I also grant you a general permission to transcribe any of my former, present or future work, so indeed you can add my name to your 'general permission' list." A number of his books and stories are available in braille and other specialized formats through the NLS.
As of December 2012, thirteen of Philip K. Dick's early works in the public domain in the United States are available in ebook form from Project Gutenberg. As of April 4, 2012, Wikisource has one of Philip K. Dick's early works in the public domain in the United States available in ebook form which is not from Project Gutenberg. See .
Death.
On February 17, 1982, after completing an interview, Dick contacted his therapist, complaining of failing eyesight, and was advised to go to a hospital immediately; but he did not. The next day, he was found unconscious on the floor of his Santa Ana, California, home, having suffered a stroke. In the hospital, he suffered another stroke, after which his brain activity ceased. Five days later, on March 2, 1982, he was disconnected from life support and died. After his death, Dick's father, Joseph, took his son's ashes to Riverside Cemetery in Fort Morgan, Colorado, (section K, block 1, lot 56) where they were buried next to his twin sister Jane, whose tombstone had been inscribed with both their names when she died 53 years earlier.
Influence and legacy.
Lawrence Sutin's 1989 biography of Dick, "Divine Invasions: A Life of Philip K. Dick", is considered the standard biographical treatment of Dick's life.
In 1993, French writer Emmanuel Carrère published "Je suis vivant et vous êtes morts" which was first translated and published in English in 2004 as "I Am Alive and You Are Dead: A Journey Into the Mind of Philip K. Dick", which the author describes in his preface in this way:The book you hold in your hands is a very peculiar book. I have tried to depict the life of Philip K. Dick from the inside, in other words, with the same freedom and empathy – indeed with the same truth – with which he depicted his own characters. Critics of the book have complained about the lack of fact checking, sourcing, notes and index, "the usual evidence of deep research that gives a biography the solid stamp of authority." It can be considered a non-fiction novel about his life.
Dick has influenced many writers, including Jonathan Lethem, and Ursula K. Le Guin. The prominent literary critic Fredric Jameson proclaimed Dick the "Shakespeare of Science Fiction", and praised his work as "one of the most powerful expressions of the society of spectacle and pseudo-event". The author Roberto Bolaño also praised Dick, describing him as "Thoreau plus the death of the American dream". Dick has also influenced filmmakers, his work being compared to films such as the Wachowskis' "The Matrix", David Cronenberg's "Videodrome", "eXistenZ", and "Spider", Spike Jonze's "Being John Malkovich", "Adaptation", Michel Gondry's "Eternal Sunshine of the Spotless Mind", Alex Proyas's "Dark City", Peter Weir's "The Truman Show", Andrew Niccol's "Gattaca", "In Time", Terry Gilliam's "12 Monkeys", Wes Craven's "A Nightmare on Elm Street", David Lynch's "Mulholland Drive", Alejandro Amenábar's "Open Your Eyes", David Fincher's "Fight Club", Cameron Crowe's "Vanilla Sky", Darren Aronofsky's "Pi", Richard Kelly's "Donnie Darko" and "Southland Tales", Rian Johnson's "Looper", and Christopher Nolan's "Memento" and "Inception".
The Philip K. Dick Society was an organization dedicated to promoting the literary works of Dick and was led by Dick's longtime friend and music journalist Paul Williams. Williams also served as Dick's literary executor for several years after Dick's death and wrote one of the first biographies of Dick, entitled "Only Apparently Real: The World of Philip K. Dick".
The Philip K. Dick estate owns and operates the production company Electric Shepherd Productions, which has produced the films "Adjustment Bureau" (2011) and the upcoming Walt Disney Company film "King of the Elves", the TV series "The Man in the High Castle" and also a Marvel Comics 5-issue adaptation of "Electric Ant".
Dick was recreated by his fans in the form of a simulacrum or remote-controlled android designed in his likeness. Such simulacra had been themes of many of Dick's works. The Philip K. Dick simulacrum was included on a discussion panel in a San Diego Comic Con presentation about the film adaptation of the novel, "A Scanner Darkly". In February 2006, an America West Airlines employee misplaced the android's head, and it has not yet been found. In January 2011, it was announced that Hanson Robotics had built a replacement.
Contemporary philosophy.
Postmodernists such as Jean Baudrillard, Fredric Jameson, Laurence Rickels and Slavoj Žižek have commented on Dick's writing's foreshadowing of postmodernity. Jean Baudrillard offers this interpretation:
It is hyperreal. It is a universe of simulation, which is something altogether different. And this is so not because Dick speaks specifically of simulacra. SF has always done so, but it has always played upon the double, on artificial replication or imaginary duplication, whereas here the double has disappeared. There is no more double; one is always already in the other world, an other world which is not another, without mirrors or projection or utopias as means for reflection. The simulation is impassable, unsurpassable, checkmated, without exteriority. We can no longer move "through the mirror" to the other side, as we could during the golden age of transcendence.
For his anti-government skepticism, Philip K. Dick was afforded minor mention in "Mythmakers and Lawbreakers", a collection of interviews about fiction by anarchist authors. Noting his early authorship of "The Last of the Masters", an anarchist-themed novelette, author Margaret Killjoy expressed that while Dick never fully sided with anarchism, his opposition to government centralization and organized religion has influenced anarchist interpretations of gnosticism.
Awards and honors.
The Science Fiction Hall of Fame inducted Dick in 2005.
During his lifetime he received numerous annual literary awards and nominations for particular works.
Philip K. Dick Award.
The Philip K. Dick Award is a science fiction award that annually recognizes the previous year's best SF paperback original published in the U.S. It is conferred at Norwescon, sponsored by the Philadelphia Science Fiction Society, and since 2005 supported by the Philip K. Dick Trust. Winning works are identified on their covers as "Best Original SF Paperback". It is currently administered by David G. Hartwell and Gordon Van Gelder.
The award was inaugurated in 1983, the year after Dick's death. It was founded by Thomas Disch with assistance from David G. Hartwell, Paul S. Williams, and Charles N. Brown. Past administrators include Algis J. Budrys and David Alexander Smith.

</doc>
<doc id="23006" url="https://en.wikipedia.org/wiki?curid=23006" title="Penélope Cruz">
Penélope Cruz

Penélope Cruz Sánchez (; born 28 April 1974), known professionally as Penélope Cruz, is a Spanish actress and model. Signed by an agent at age 15, she made her acting debut at 16 on television and her feature film debut the following year in "Jamón, jamón" (1992) to critical acclaim. Her subsequent roles in the 1990s and 2000s included "Open Your Eyes" (1997), "The Hi-Lo Country" (1999), "The Girl of Your Dreams" (2000) and "Woman on Top" (2000). Cruz achieved recognition for her lead roles in the 2001 films "Vanilla Sky", "All the Pretty Horses", "Captain Corelli's Mandolin" and "Blow".
She has since appeared in films in a range of genres, including the comedy "Waking Up in Reno" (2002), the thriller "Gothika" (2003), the Christmas film "Noel" (2004), and the action adventure "Sahara" (2005). She was critically acclaimed for her roles in "Volver" (2006) and "Nine" (2009), receiving Golden Globe and Academy Award nominations for each. She won the Academy Award for Best Supporting Actress in 2008 for playing María Elena in "Vicky Cristina Barcelona". She was the first Spanish actress in history to receive an Academy Award and the first Spanish actress to receive a star at the Hollywood Walk of Fame.
Cruz has modelled for Mango, Ralph Lauren and L'Oréal. Penélope and her younger sister Mónica Cruz have designed clothing for Mango. Cruz has volunteered in Uganda and India, where she spent one week working with Mother Teresa; she donated her salary from "The Hi-Lo Country" to help fund the late nun's mission.
Early life.
She was born in the working-class town of Alcobendas, Madrid, Spain, to Encarna (née Sánchez), a hairdresser and personal manager, and Eduardo Cruz, a retailer and car mechanic. She has two siblings, Monica, an actress, and Eduardo, Jr. She also has a paternal half-sister, Salma. She was raised as a Roman Catholic. Cruz grew up in Alcobendas, and spent long hours at her grandmother's apartment. She says she had a happy childhood. Cruz remembers "playing with some friends and being aware that I was acting as I was playing with them. I would think of a character and pretend to be someone else."
Initially, Cruz focused on dance, having studied classical ballet for nine years at Spain's National Conservatory. She took three years of Spanish ballet training and four years of theatre at Cristina Rota's New York school. She says that ballet instilled in her discipline that would be important in her future acting career. When she became a cinephile at 10 or 11, her father bought a Betamax machine, which was then a very rare thing to own in her neighborhood.
As a teenager, Cruz became interested in acting after seeing the film "Tie Me Up! Tie Me Down!" by Spanish director Pedro Almodóvar. She did casting calls for an agent but was rejected multiple times because the agent felt that she was too young. Cruz commented on the experience, "I was very extroverted as a kid... I was studying when I was in high school at night, I was in ballet and I was doing castings. I looked for an agent and she sent me away three times because I was a little girl but I kept coming back. I'm still with her after all these years." In 1989, at the age of 15, Cruz won an audition at a talent agency over more than 300 other girls. In 1999, Katrina Bayonas, Cruz's agent, commented, "She was absolutely magic the audition. It was obvious there was something very impressive about this kid... She was very green, but there was a presence. There was just something coming from within."
Her father Eduardo died at his home in Spain in 2015, aged 62, due to heart attack.
Acting career.
Early work, 1989–1996.
In 1989, 15-year-old Cruz made her acting debut in a music video for the Spanish pop group Mecano's song "La Fuerza del Destino". Between 1990 and 1997, she hosted the Spanish TV channel Telecinco's talk show "La Quinta Marcha", a programme that was hosted by teenagers, aimed at a teenage audience. She also played in the ""Elle et lui"" episode of an erotic French TV series called "Série rose" in 1991, where she appeared nude. In 1992, Cruz made her feature film debut at 18 as the lead female role in the comedy drama art house film, "Jamón, jamón". In the film, she portrayed Silvia, a young woman who is expecting her first child with a man whose mother does not approve of the relationship and attempts to sabotage it by paying Javier Bardem's character to seduce her. "People" magazine noted that after Cruz appeared topless in the film, she became "a major sex symbol". In an interview with the "Los Angeles Daily News" in 1999, Cruz commented that "it was a great part, but...I wasn't really ready for the nudity. [...] But I have no regrets because I wanted to start working and it changed my life." Charlie Rose of "60 Minutes" noted that Cruz "became an overnight sensation as much for her nude scenes as for her talent". When Rose asked Cruz if she was concerned about how she would be perceived after her role in the film, Cruz replied, "I just knew I had to do the complete opposite."
"Jamón, jamón" received favorable reviews, with Chris Hicks of the "Deseret News" describing Cruz's portrayal of Silvia as "enchanting". Writing for the "Chicago Sun-Times", film critic Roger Ebert wrote "it stars actors of considerable physical appeal, most particularly Penélope Cruz as Silvia". For her performance, Cruz was nominated for a Spanish Actors Union Newcomer Award and a Goya Award for Best Actress. The same year she appeared in the Academy-Award winning "Belle Epoque" as the virginal Luz. "People" magazine noted that Cruz's role as Luz showed that she was versatile. From 1993 to 1996, Cruz appeared in ten Spanish and Italian films. At 20, she went to live in New York for two years at Christopher and Greenwich to study ballet and English between films. She recalls learning English "kind of late" only knowing the dialogue she had learned for the casting beyond that, she could only say, "How are you?" and "Thank you."
Early critical success, 1997–2000.
Cruz's agent is Hylda Queally, shared with Cate Blanchett and Kate Winslet. In 1997, Cruz appeared in the Spanish comedy film "Love Can Seriously Damage Your Health". She portrays Diana, a fan of the Beatles band member John Lennon; she tries unsuccessfully to meet him. Years later, after multiple failed relationships, Diana re-unites with an acquaintance under unusual circumstances. Also in 1997, she appeared in the opening scene of Pedro Almodóvar's "Live Flesh" as a prostitute who gives birth on a bus and in "Et hjørne af paradis" (A Corner of Paradise) as Doña Helena. Cruz's final appearance in 1997 was the Amenabar-directed Spanish sci-fi drama, "Abre Los Ojos"/ "Open Your Eyes". She plays Sofia, the love interest of Eduardo Noriega's lead character. "Open Your Eyes" received positive reviews, and was later remade by U.S. director Cameron Crowe as "Vanilla Sky" (who cast Cruz in the same role and Tom Cruise in Noriega's role), but "Open Your Eyes" was not commercially successful. Kevin N. Laforest of the Montreal Film Journal commented in his September 2002 review that Cruz "has been getting some really bad reviews for her recent American work, but I personally think that she's a more than decent actress, especially here, where she's charming, moving and always believable. [...] There's one shot in particular, where Cruz enters a room in a greenish glow, which is right out of Hitchcock's picture ["Vertigo"]."
The following year, Cruz appeared in her first American film as Billy Crudup's consolation-prize Mexican girlfriend in Stephen Frears' western film, "The Hi-Lo Country". Cruz stated that she had difficulties understanding people speaking English while she was filming "The Hi-Lo Country". The film was critically and commercially unsuccessful. Kevin Lally of the "Film Journal International" commented in his review for the film that "in an ironic casting twist, the Spanish actress Penélope Cruz [...] is much more appealing as Josepha in her previous roles". For her performance in the film, she was nominated for an ALMA Award for Best Actress. Also in 1998 Cruz appeared in "Don Juan" and "The Girl of Your Dreams".
In the period drama "The Girl of Your Dreams" (La niña de tus ojos), Cruz portrayed Macarena Granada, a singer who is in an on-and-off relationship with Antonio Resines's character, Blas. They are part of a Francoist film troupe that travels from Spain during the Spanish Civil War to Nazi Germany for a joint production with UFA. Cruz's performance in the film was praised by film critics, with Jonathan Holloland of "Variety" magazine writing "if confirmation is still needed that Cruz is an actress first and a pretty face second, then here it is". A writer for "Film4" commented that "Cruz herself is the inevitable focus of the film" but noted that overall the film "looks great". Cruz's role as Macerna has been viewed as her "largest role to date". For her performance, Cruz received a Goya Award and Spanish Actors' Union Award, and was nominated for a European Film Award. In 1999, Cruz worked with Almodóvar again in "All About My Mother", playing Sister María Rosa Sanz, a pregnant nun with AIDS. The film received favorable reviews, and was commercially successful, grossing over $67 million worldwide, although it performed better at the box office internationally than domestically.
In 2000, she appeared in "Woman on Top" in the lead female role as Isabelle, a world-class chef who has suffered from motion sickness since birth, her first American lead role. Lisa Nesselson of "Variety" magazine praised the performances of both Cruz and her co-star, Harold Perrineau, saying they "burst off the screen", and added that Cruz has a charming accent. BBC News film critic Jane Crowther said that "Cruz is wonderfully ditzy as the innocent abroad" but remarked that "it's Harold Perrineau Jr as Monica who pockets the movie". Annlee Ellingson of "Box Office" magazine wrote "Cruz is stunning in the role—innocent and vulnerable yet possessing a mature grace and determined strength, all while sizzling with unchecked sensuality." Also in 2000, she played Alejandra Villarreal, who is Matt Damon's love interest in Billy Bob Thornton's film adaptation of the western bestselling novel, "All the Pretty Horses". Susan Stark of the "Detroit News" commented that in the film Thornton was able to guide Damon, Henry Thomas and Cruz to "their most impressive performances in a major movie yet". However, Bob Longigo of "The Atlanta Journal Constitution" was less enthusiastic about Cruz and Damon's performance, saying that their "resulting onscreen chemistry would hardly warm a can of beans".
Breakthrough acting, 2001–2005.
2001 marked a turning point year when Cruz starred in the feature films "Vanilla Sky" and "Blow". In "Vanilla Sky", Cameron Crowe's interpretation of "Open Your Eyes", she played Sofia Serrano, the love interest of Tom Cruise's character. The film received mixed reviews but made $200 million worldwide. Her performance was well received by critics, with BBC film critic Brandon Graydon saying that Cruz "is an enchanting screen presence", and Ethan Alter of the "Film Journal International" noting that Cruz and her co-star Cruise were "able to generate some actual chemistry". Her next film was "Blow", adapted from Bruce Porter's 1993 book "Blow: How a Small Town Boy Made $100 million with the Medellín Cocaine Cartel and Lost It All". She had a supporting role as Mirtha Jung, the wife of Johnny Depp's character. The film received mixed reviews, but made $80 million worldwide. Nina Willdorf of the "Boston Phoenix" described Cruz as "multi-talented" and Mark Salvo of "The Austin Chronicle" wrote "I may be one of the last male holdouts to join the Cruz-Rules camp, but her tour de force performance here sucks you right in."
In 2001, she also appeared in "Don't Tempt Me", playing Carmen Ramos. The film received negative reviews. Jeff Vice of the "Deseret News" commented that "unfortunately, casting Cruz as a tough girl is a hilariously bad " and Michael Miller of the "Village Voice" writing that "as Satan's helper Carmen, Penélope Cruz doesn't hold a candle to her cocaine-huffing enabler in "Blow"". Cruz's last film in 2001 was "Captain Corelli's Mandolin", film adaption of the novel of the same name. She played Pelagia, who falls in love with another man while her fiancé is in battle during the Second World War. "Captain Corelli's Mandolin" was not well received by critics, but made $62 million worldwide. In 2002, she had a minor role in "Waking Up in Reno". It had negative reviews and was a box office failure, making $267,000 worldwide. The following year, Cruz had a supporting role in the horror film "Gothika", as Chloe Sava, a patient at a mental hospital. David Rooney of "Variety" wrote that Cruz "adds a serviceably malevolent edge to Chole's apparent madness". Cruz's performance in "Fanfan la Tulipe", also in 2003, was not well received, Peter Bradshaw of "The Guardian" commenting that Cruz "deserves a special Cannes Razzie for a performance of purest teak".
In 2004, Cruz appeared in the Christmas film "Noel" as Nina, the girlfriend of Paul Walker's character and as Mia in the romantic drama, "Head in the Clouds", set in the 1930s. "Head in the Clouds" performed poorly at the box office. For "Head in the Clouds", Bruce Birkland of "Jam! Canoe" said, "The story feels forced and the performances dreary, with the notable exception of Cruz, who seems to be in a different film from the rest of the cast." Desson Thompson of "The Washington Post" was more critical; his comment about the character's "pronounced limp" was that "Cruz (hardly the world's greatest actress) can't even perform without looking fake". She also starred in Sergio Castellitto's melodrama "Don't Move". Cruz, who learned Italian for the role, earned critical acclaim for her performance and won the David di Donatello. She was also awarded the European Film Award for Best Actress for the film in 2004.
In 2005, Cruz appeared as Dr. Eva Rojas in the action adventure "Sahara". She earned $1.6 million for her supporting role. The film grossed $110 million worldwide but did not recoup its $160 million budget. Moviefone dubbed the film "one of the most famous flops in history" and in 2007, listed it at 24 on its list of "Biggest Box-Office Turkeys of All Time". Lori Hoffman of the "Atlantic City Weekly" felt Cruz put her "considerable skills on cruise control as Dr Eva Rojas" and James Berardnelli of ReelViews described Cruz's performance as a "black hole", that she "lacks screen presence". Also in 2005, Cruz appeared in "Chromophobia", screened at the 2005 Cannes Film Festival and released the following year. Mathew Turner of "View London" said Cruz's character Gloria, a cancer-riddled prostitute, is "actually more interesting than the main storyline" while Time Evan's of "Sky Movies" wrote, "The Cruz/Ifans storyline—featuring the only two remotely sympathetic characters—never really fuses with the main plot." Her final 2005 film was "Don't Move" playing Italia. Eric Harrison of the "Houston Chronicle" noted that Cruz "goes all out" with her appearance and Patrick Peters of "Empire" magazine commented that the film's director, who also appears in the film, was able to draw a "sensitive performance" from Cruz.
Worldwide recognition, 2006–2009.
Cruz appeared alongside her good friend Salma Hayek in the 2006 Western comedy film, "Bandidas". Randy Cordova of the "Arizona Republic" said the film "sports" Cruz and her co-star Salma Hayek as the "lusty dream team" and that they were the "marketing fantasy" for the film. Also in 2006, Cruz received favourable reviews for her performance as Raimunda in Pedro Almodóvar's "Volver". Carina Chocano of "The Los Angeles Times" wrote, "Cruz, who has remarked that in Hollywood she's rarely allowed to be anything more than pretty, instills her with an awesome resoluteness and strength of character." She shared a Best Actress award at the 2006 Cannes Film Festival with five of her co-stars, as well as receiving a Goya Award and European Film Award, and was nominated for the Golden Globe, the Screen Actors Guild Award, the BAFTA Award, and the Academy Award for Best Actress in a leading role. She was the first Spaniard to ever be nominated for an Academy Award for Best Actress.
In 2007, Cruz appeared in the lead female role in "Manolete", a biopic of bullfighter Manuel Laureano Rodríguez Sánchez, playing Antoñita "Lupe" Sino. She also appeared in "The Good Night", playing two characters, Anna and Melody. TV Guide film critic Maitland McDonagh noted that in the film Cruz "expertly mines the contrast between chic, compliant, white-clad Anna and funky, street-wise Melody, who treats Freeman's character Gary like the world-class drag he is". In 2008, Cruz appeared in Isabel Coixet's film "Elegy", which was based on the Philip Roth story "The Dying Animal", as the lead female role, Consuela Castillo. Ray Bennett of "The Hollywood Reporter" described Cruz's performance as being "outstanding in an otherwise lame male fantasy ".
Later that year, she starred in Woody Allen's "Vicky Cristina Barcelona" as María Elena, a mentally unstable woman, which was praised. Peter Bradshaw of "The Guardian" praised Cruz's performance in the film. Cruz received a Goya Award and her first Academy Award and BAFTA Award for Best Supporting Actress. She also received a Golden Globe and SAG nomination. Cruz was the first Spanish actress to ever be awarded an Academy Award in that category and the sixth Hispanic person to ever receive the award.
After being shelved since 2007, Cruz's film "Manolete" (originally shot in 2005) released on demand via cable, satellite, telco and online in June 7, 2011 under the title "A Matador's Mistress".
Cruz's next film was the kid-friendly "G-Force" voicing a guinea pig spy named Juarez. "G-Force" was a commercial success, making over $290 million worldwide. Also in 2009, she appeared in the film "Broken Embraces" as Lena. Stephanie Zacharek of Salon.com noted in her review for the film that Cruz "doesn't coast on her beauty in "Broken Embraces", and she has the kind of role that can be difficult to flesh out". Cruz received nominations from the Satellite Awards and European Film Awards for her performance in "Broken Embraces". Cruz's final 2009 film was the film version of the musical "Nine", playing the character Carla Albanese, the lead character's mistress. "Variety" reported that Cruz had originally auditioned for the role of the film within a film's star, Claudia, which eventually went to Nicole Kidman. Cruz said that she trained for three months for the dance routine in the film. Claudia Puig of "USA Today" commented that while Cruz "does a steamy song and dance", her "acting is strangely caricatured". Cruz's performance as Carla garnered her nominations for Best Supporting Actress from the Academy Awards, Golden Globes and SAG Awards.
2010s.
In 2012, Cruz appeared in the first ever Nintendo commercial to promote "New Super Mario Bros. 2" and the Nintendo 3DS XL in which she played the role of Mario in the ad. She spoke Italian again, this time in Woody Allen's romantic ensemble comedy film "To Rome with Love", in which she portrayed a street-smart prostitute who agrees to pretend to be the wife of a newlywed. Fond to work with her again, Allen compared Cruz's play in the film with that of Italian icons Anna Magnani and Sophia Loren. While the film garnered mixed reviews in general, Cruz received favourable reviews for her "exuberantly, cartoonishly sexy" performance, which reviewers such as "The Week" cited as a stand out. The same year, Cruz also reunited with Italian director Sergio Castellitto in his war tale "Twice Born" about an infertile Italian woman who returns to relive her past in Sarajevo. An adaptation of Castellitto's wife Margaret Mazzantini's same-titled bestseller, Cruz portrayed the transitional character at different phases in her life, ranging from her early twenties to her late forties. Despite receiving little praise from critics, Cruz's performance opposite Emile Hirsch earned positive reviews.
In 2013, Cruz appeared in Ridley Scott's "The Counselor", featuring an ensemble cast consisting of Michael Fassbender, Cameron Diaz, Brad Pitt and husband Javier Bardem. The crime thriller follows a lawyer who, tempted by the lure of quick money, finds himself involved in drug dealing with ruthless Mexican cartels. Cruz plays his girlfriend, Laura, the only innocent character in the story. The film received mostly negative reviews from critics and became a moderate commercial success at the international box offices. The same year, Cruz along with Antonio Banderas made a cameo appearance in Pedro Almodóvar's farcical comedy "I'm So Excited", which marked a return to the director's light, campy comedies of the 1980s and 1990s."
In 2015, Cruz co-produced and starred in the Spanish drama film "Ma Ma", directed by Julio Medem. In it, she plays Magda, a gutsy mother and unemployed teacher, who is diagnosed with breast cancer, a role which Cruz later cited as "one of the most complex, beautiful characters I've ever been offered, the most difficult." The melodrama was screened in the Special Presentations section of the 2015 Toronto International Film Festival, where it garnered generally negative reviews for its weepie story line. Cruz however was praised for her "aces performance," which earned her an eighth Goya nomination at the 30th awards ceremony.
Cruz's first film of 2016 was the American comedy "Zoolander 2", co-starring and directed by Ben Stiller. In the sequel film, Cruz portrayed a secret Interpol agent who enlists models Derek Zoolander (Stiller) and Hansel McDonald, played by Owen Wilson, to help find out who is killing the world’s most beautiful people. Specifically written for her persona, Cruz, a fan of the original 2001 film, was one of the first actors to be cast in their parts. Upon its release, the film received generally negative reviews from critics, who felt that it had "more celebrity cameos than laughs." Cruz's other film that year was Louis Leterrier's British spy comedy "Grimsby", in which she played a powerful philanthropist, opposite Sacha Baron Cohen and Mark Strong. Cruz was reportedly offered $400,000 for her appearance in the film, which was released to generally mixed reviews from critics, who felt that the actress was highly underused and "looking even less invested here than she did in "Zoolander 2"."
As of March 2016, Cruz has various film projects in different states of production. She will reteam with Fernando Trueba on his Spanish-language period pic "The Queen of Spain", in which she plays a famous movie star who flees the glitz and glamour of 1950s Hollywood to return to her roots in Madrid. Cruz will also co-produce the project. She has also signed on to appear in Fernando León de Aranoa's Pablo Escobar biopic "Escobar", also starring husband Javier Bardem, and will start work with Iranian director Asghar Farhadi's on his first foreign film in fall 2016.
Public image.
In 2006, Cruz became spokesmodel for French cosmetics company L'Oréal to promote products such as the L'Oréal Paris hair dye Natural Match and L'Oreal mascara products. She receives $2 million a year for her work for the company. Cruz has appeared in print ads for Mango and had a contract with Ralph Lauren in 2001. Cruz and her sister designed their second collection for Mango in 2007. It was inspired by Brigitte Bardot and summers in St Tropez.
Cruz ranked as No. 58 in "Maxim" "Hot 100" of 2007 list, and was chosen by "Empire" magazine as being one of the 100 Sexiest Movie Stars in the world. Cruz was also ranked on Askmen.com's Most Desirable Women of 2008 at No. 26, in 2009 at No. 25, and in 2010 at No. 7. In April 2010, she replaced Kate Winslet as the new face and ambassador of Lancôme's Trésor fragrance. Lancôme has signed Cruz as the brand's third superstar spokesmodel, along with Julia Roberts and Winslet. The campaign was shot by Mario Testino at Paris's Hotel de Crillon and debuted in the autumn of 2010.
In 2010, Cruz was a guest editor for the French "Vogue" magazine, focusing on larger-size models in a provocative photo shoot. Almodóvar described her as his muse. On the cover of Spanish Vogue's December 2010 issue, she agreed to be photographed by fashion photographer Peter Lindbergh only if her pregnancy was not shown. In 2011, "The Telegraph" reported the most sought after body parts of the rich and famous revealed by two Hollywood plastic surgeons who carried out a survey among their patients to build up the picture of the perfect woman. Under the category of the most sought after body shape, Penélope Cruz, known for her voluptuous figure, was voted as having the top body. "Men's Health" ranked her at No. 32. on their "100 Hottest Women of All-Time" list.
"Esquire" named her the "Sexiest Woman Alive" in 2014.
During the 2014 Israel–Gaza conflict, Cruz along with her husband signed an open letter, denouncing Israel's actions as a "genocide". a letter that was criticized by several Hollywood personalities including actor Jon Voight.
Philanthropy.
Cruz has donated money and time to charity. In addition to work in Nepal, she has volunteered in Uganda and India, where she spent a week working with Mother Teresa that included assisting in a leprosy clinic. That trip inspired Cruz to help start a foundation to support homeless girls in India, where she sponsors two young women. She donated her salary from her first Hollywood film, "The Hi-Lo Country", to Mother Teresa's mission. In the early 2000s, she spent time in Nepal photographing Tibetan children for an exhibition attended by the Dalai Lama. She also photographed residents at the Pacific Lodge Boys' Home, most of whom are former gang members and recovering substance abusers. She said: "These kids break my heart. I have to control myself not to cry. Not out of pity, but seeing how tricky life is and how hard it is to make the right choices." A pregnant Cruz showed her support for the battle against AIDS by lighting up the Empire State Building with red lights in New York City on 1 December 2010 on International AIDS Day, as part of (RED)'s new awareness campaign, 'An AIDS Free Generation is Due in 2015,' which aims to eradicate the HIV virus from pregnant mothers to their babies. In 2012, she posed for an ad supporting PETA's anti-fur campaign.
Personal life.
Cruz is married to Spanish actor Javier Bardem. Bardem was her co-star in her first breakthrough role as Silvia in "Jamon, Jamon" as well as starring alongside her in "Vicky Cristina Barcelona". 
They were also both in the 2013 film "The Counselor".
Cruz began dating Bardem in 2007 and they married in early July 2010 in a private ceremony at a friend's home in the Bahamas. They have a son born in 2011 in Los Angeles, and a daughter born in 2013. She has become a public advocate of breastfeeding since the birth of her children.
Cruz had a three-year relationship with Tom Cruise after they appeared together in "Vanilla Sky". It ended in January 2004. In April 2003, she filed a lawsuit against the Australian magazine "New Idea" for defamation over an article about her relationship with Cruise.
Cruz is a friend of Spanish director Pedro Almodóvar, whom she has known for more than two decades and with whom she has worked on films. She is known to friends as "Pe". Cruz owns a clothing store in Madrid and designed jewelry and handbags with her younger sister for a company in Japan.

</doc>
<doc id="23008" url="https://en.wikipedia.org/wiki?curid=23008" title="Preliminary hearing">
Preliminary hearing

Within some criminal justice systems, a preliminary hearing, preliminary examination, evidentiary hearing or probable cause hearing is a proceeding, after a criminal complaint has been filed by the prosecutor, to determine whether there is enough evidence to require a trial. In the United States, the judge must find that such evidence provides probable cause to believe that the crime was committed by the defendant.
In Scotland, a preliminary hearing is a non-evidential diet in cases to be tried before the High Court of Justiciary. It is a pre-trial diet to enable the court to be advised whether both parties, the prosecution and the defence, are ready to proceed to trial and may also deal with ancillary procedural matters.
At such a hearing, the defendant may be assisted by counsel; in U.S. jurisdictions, there is a right to counsel at the preliminary hearing. A preliminary hearing is not always required, and its requirement varies by jurisdiction. In the U.S., for example, some states hold these hearings in every criminal case; in others, they are held upon request by the defense, and still others, they are only held in felony cases. If, on the other hand, the defendant is charged with a felony under Federal law, he has the right to an indictment by a grand jury pursuant to the Fifth Amendment to the Constitution. At grand jury proceedings, the defendant is not entitled to counsel, and indeed may not even know that a grand jury is considering his or her case.
The conduct of the preliminary hearing as well as the specific rules regarding the admissibility of evidence vary from jurisdiction to jurisdiction. Hearsay is typically allowed. Should the court decide that there is probable cause, a formal charging instrument (called the information in some jurisdictions) will issue; and the prosecution will continue. If the court should find that there is no probable cause, then typically the prosecution will cease. Many jurisdictions, however, allow the prosecution to seek a new preliminary hearing, or even seek a bill of indictment from a grand jury.
Some important questions that such a hearing generally addresses are:
If a judge determines that there is sufficient evidence to believe that the defendant committed the crime, it is said that the defendant is "held to answer" or "bound over" (in U.S. jurisdictions).
In some jurisdictions, after the court holds a defendant to answer, the court schedules an arraignment, while in other jurisdictions the arraignment precedes the preliminary hearing. The prosecutor files a new pleading with the court (sometimes called an "information") and the defendant can enter a plea at the arraignment. If that plea is not guilty, a trial normally follows and the court sets a trial date at the arraignment or preliminary hearing, depending on which comes later.

</doc>
<doc id="23010" url="https://en.wikipedia.org/wiki?curid=23010" title="Paul Ehrlich">
Paul Ehrlich

His laboratory discovered arsphenamine (Salvarsan), the first effective medicinal treatment for syphilis, thereby initiating and also naming the concept of chemotherapy. Ehrlich popularized the concept of a magic bullet. He also made a decisive contribution to the development of an antiserum to combat diphtheria and conceived a method for standardizing therapeutic serums.
In 1908, he received the Nobel Prize in Physiology or Medicine for his contributions to immunology. He was the founder and first director of what is now known as the Paul Ehrlich Institute.
Life and career.
Born 14 March 1854 in Strehlen in Silesia in what is now south-west Poland.
Paul Ehrlich was the second child of Rosa (Weigert) and Ismar Ehrlich. His father was an innkeeper and distiller of liqueurs and the royal lottery collector in Strehelen, a town of some 5,000 inhabitants in the province of Lower Silesia, now in Poland. His grandfather, Heymann Ehrlich, had been a fairly successful distiller and tavern manager. Ismar Ehrlich was the leader of the local Jewish community.
After elementary school, Paul attended the time-honored secondary school Maria-Magdalenen-Gymnasium in Breslau, where he met Albert Neisser, who later became a professional colleague. As a schoolboy (inspired by his cousin Karl Weigert who owned one of the first microtomes), he became fascinated by the process of staining microscopic tissue substances. He retained that interest during his subsequent medical studies at the universities of Breslau, Strasbourg, Freiburg im Breisgau and Leipzig. After obtaining his doctorate in 1882, he worked at the Charité in Berlin as an assistant medical director under Theodor Frerichs, the founder of experimental clinical medicine, focusing on histology, hematology and color chemistry (dyes).
He married Hedwig Pinkus (then aged 19) in 1883. The couple had two daughters, Stephanie and Marianne.
After completing his clinical education and habilitation at the prominent Charité medical school and teaching hospital in Berlin in 1886, Ehrlich traveled to Egypt and other countries in 1888 and 1889, in part to cure a case of tuberculosis which he had contracted in the laboratory. Upon his return he established a private medical practice and small laboratory in Berlin-Steglitz. In 1891, Robert Koch invited Ehrlich to join the staff at his Berlin Institute of Infectious Diseases, where in 1896 a new branch, the Institute for Serum Research and Testing ("Institut für Serumforschung und Serumprüfung"), was established for Ehrlich’s specialization. Ehrlich was named its founding director.
In 1899 his institute moved to Frankfurt am Main and was renamed the Institute of Experimental Therapy ("Institut für experimentelle Therapie"). One of his important collaborators there was Max Neisser. In 1906 Ehrlich became the director of the Georg Speyer House in Frankfurt, a private research foundation affiliated with his institute. Here he discovered in 1909 the first drug to be targeted against a specific pathogen: Salvarsan, a treatment for syphilis, which was at that time one of the most lethal and infectious diseases in Europe. Among the foreign guest scientists working with Ehrlich were two Nobel Prize winners, Henry Hallett Dale and Paul Karrer. The institute was renamed Paul Ehrlich Institute in Ehrlich's honour in 1947.
In 1914 Ehrlich signed the controversial Manifesto of the Ninety-Three which was a defense of Germany’s World War I politics and militarism. On 17 August 1915 Ehrlich suffered a heart attack and died on 20 August in Bad Homburg vor der Höhe. Wilhelm II the German emperor, wrote in a telegram of condolence, “I, along with the entire civilized world, mourn the death of this meritorious researcher for his great service to medical science and suffering humanity; his life’s work ensures undying fame and the gratitude of both his contemporaries and posterity”.
Paul Ehrlich was buried at the Old Jewish Cemetery, Frankfurt (Block 114 N).
Research.
Hematological staining.
In the early 1870s, Ehrlich’s cousin Karl Weigert was the first person to stain bacteria with dyes and to introduce aniline pigments for histological studies and bacterial diagnostics. During his studies in Strassburg under the anatomist Heinrich Wilhelm Waldeyer, Ehrlich continued the research started by his cousin in pigments and staining tissues for microscopic study. He spent his eighth university semester in Freiburg im Breisgau investigating primarily the red dye dahlia (monophenylrosanilin), giving rise to his first publication.
In 1878 he followed his dissertation supervisor Julius Friedrich Cohnheim to Leipzig, and that year obtained a doctorate with a dissertation entitled "Contributions to the Theory and Practice of Histological Staining" ("Beiträge zur Theorie und Praxis der histologischen Färbung").
One of the most outstanding results of his dissertation investigations was the discovery of a new cell type. Ehrlich discovered in the protoplasm of supposed plasma cells a granulate which could be made visible with the help of an alkaline dye. He thought this granulate was a sign of good nourishment, and accordingly named these cells mast cells, (from the German word for an animal-fattening feed, "Mast"). This focus on chemistry was unusual for a medical dissertation. In it, Ehrlich presented the entire spectrum of known staining techniques and the chemistry of the pigments employed.
While he was at the Charité, Ehrlich elaborated upon the differentiation of white blood cells according to their different granules. A precondition was a dry specimen technique, which he also developed. A drop of blood placed between two glass slides and heated over a Bunsen burner fixed the blood cells while still allowing them to be stained. Ehrlich used both alkaline and acid dyes, and also created new “neutral” dyes. For the first time this made it possible to differentiate the lymphocytes among the leucocytes (white blood cells). By studying their granulation he could distinguish between nongranular lymphocytes, mono- and poly-nuclear leucocytes, eosinophil granulocytes, and mast cells.
Starting in 1880, Ehrlich also studied red blood cells. He demonstrated the existence of nucleated red blood cells, which he subdivided into normoblasts, megaloblasts, microblasts and poikiloblasts; he had discovered the precursors of erythrocytes. Ehrlich thus also laid the basis for the analysis of anemias, after he had created the basis for systematizing leukemias with his investigation of white blood cells.
His duties at the Charité included analyzing patients’ blood and urine specimens. In 1881 he published a new urine test which could be used to distinguish various types of typhoid from simple cases of diarrhea. The intensity of staining made possible a disease prognosis. The pigment solution he used is known today as Ehrlich’s reagent.
Ehrlich’s great achievement, but also a source of problems during his further career, was that he had initiated a new field of study interrelating chemistry, biology and medicine. Much of his work was rejected by the medical profession, which lacked the requisite chemical knowledge. It also meant that there was no suitable professorship in sight for Ehrlich.
Serum research.
Friendship with Robert Koch.
When a student in Breslau, Ehrlich was given an opportunity by the pathologist Julius Friedrich Cohnheim to conduct extensive research and was also introduced to Robert Koch, who was at the time a district physician in Wollstein, Posen Province. In his spare time, Koch had clarified the life cycle of the anthrax pathogen and had contacted Ferdinand Cohn, who was quickly convinced by Koch’s work and introduced him to his Breslau colleagues. From 30 April to 2 May 1876, Koch presented his investigations in Breslau, which the student Paul Ehrlich was able to attend 
On 24 March, 1882, Ehrlich was present when Robert Koch, working since 1880 at the Imperial Public Health Office ("Kaiserliches Gesundheitsamt") in Berlin, presented the lecture in which he reported how he was able to identify the tuberculosis pathogen. Ehrlich later described this lecture as his “greatest experience in science.” The day after Koch’s lecture, Ehrlich had already made an improvement to Koch’s staining method, which Koch unreservedly welcomed. From this date on, the two men were bound in friendship.
In 1887 Ehrlich became an unsalaried lecturer in internal medicine ("Privatdozent für Innere Medizin") at Berlin University, and in 1890 took over the tuberculosis station at a public hospital in Berlin-Moabit at Koch’s request. This was where Koch’s hoped-for tuberculosis therapeutic agent tuberculin was under study; and Ehrlich had even injected himself with it. In the ensuing tuberculin scandal, Ehrlich tried to support Koch and stressed the value of tuberculin for diagnostic purposes. In 1891 Koch invited Ehrlich to work at the newly founded Institute of Infectious Diseases ("Institut für Infektionskrankheiten" – now the Robert Koch Institute) at "Friedrich-Wilhelms-Universität" (now Humboldt University) in Berlin. Koch was unable to give him any remuneration, but did offer him full access to laboratory staff, patients, chemicals and laboratory animals, which Ehrlich always remembered with gratitude.
First work on immunity.
Ehrlich had started his first experiments on immunization already in his private laboratory. He accustomed mice to the poisons ricin and abrin. After feeding them with small but increasing dosages of ricin he ascertained that they had become "ricin-proof." Ehrlich interpreted this as immunization and observed that it was abruptly initiated after a few days and was still in existence after several months, but mice immunized against ricin were just as sensitive to abrin as untreated animals.
This was followed by investigations on the "inheritance" of acquired immunity. It was already known that in some cases after a smallpox or syphilis infection, specific immunity was transmitted from the parents to their offspring. Ehrlich rejected inheritance in the genetic sense because the offspring of a male mouse immunized against abrin and an untreated female mouse were not immune to abrin. He concluded that the fetus was supplied with antibodies via the pulmonary circulation of the mother. This idea was supported by the fact that this “inherited immunity” decreased after a few months. In another experiment he exchanged the offspring of treated and untreated female mice. The mice which were nursed by the treated females were protected from the poison, providing the proof that antibodies can also be conveyed in breast milk.
Ehrlich also researched autoimmunity, but he specifically rejected the possibility that an organism's immune system could attack the organism's own tissue calling it "horror autotoxicus." Ironically it was Ehrlich's student, Ernest Witebsky, who demonstrated that autoimmunity could cause disease in humans. 
Work with Behring on a diphtheria serum.
Emil Behring had worked at the Berlin Institute of Infectious Diseases until 1893 on developing an antiserum for treating diphtheria and tetanus but with inconsistent results. Koch suggested that Behring and Ehrlich cooperate on the project. This joint work was successful to the extent that Ehrlich was quickly able to increase the level of immunity of the laboratory animals based on his experience with mice. Clinical tests with diphtheria serum early in 1894 were successful and in August the chemical company Hoechst started to market Behring’s “Diphtheria Remedy synthesized by Behring-Ehrlich.” The two discoverers had originally agreed to share any profits after the Hoechst share had been subtracted. Their contract was changed several times and finally Ehrlich was eventually pressured into accepting a profit share of only eight percent. Ehrlich resented what he considered as unfair treatment, and his relationship with Behring was thereafter problematic, a situation which later escalated over the issue of the valency of tetanus serum. Ehrlich recognized that the principle of serum therapy had been developed by Behring and Kitasato. But he was of the opinion that he had been the first to develop a serum which could also be used on humans, and that his role in developing the diphtheria serum had been insufficiently acknowledged. Behring, for his part, schemed against Ehrlich at the Prussian Ministry of Culture, and from 1900 on Ehrlich refused to collaborate with him. von Behring was the sole recipient of the first Nobel Prize in Medicine, in 1901, for contributions to research on diphtheria.
The valency of serums.
Since antiserums were an entirely new type of medicine whose quality was highly variable, a government system was established to guarantee their safety and effectiveness. Beginning 1 April 1895, only government-approved serum could be sold in the German Reich. The testing station for diphtheria serum was provisionally housed at the Institute of Infectious Diseases. At the initiative of Friedrich Althoff, an Institute of Serum Research and Testing ("Institut für Serumforschung und Serumprüfung") was established in 1896 in Berlin-Steglitz, with Paul Ehrlich as director (which required him to cancel all his contracts with Hoechst). In this function and as honorary professor at Berliner University he had annual earnings of 6,000 marks, approximately the salary of a university professor. In addition to a testing department the institute also had a research department.
In order to determine the effectiveness of diphtheria antiserum, a stable concentration of diphtheria toxin was required. Ehrlich discovered that the toxin being used was perishable, in contrast to what had been assumed, which for him led to two consequences: He did not use the toxin as a standard, but instead a serum powder developed by Behring, which had to be dissolved in liquid shortly before use. The strength of a test toxin was first determined in comparison with this standard. The test toxin could then be used as a reference for testing other serums. For the test itself, toxin and serum were mixed in a ratio so that their effects just cancelled each other when injected into a guinea pig. But since there was a large margin in determining whether symptoms of illness were present, Ehrlich established an unambiguous target: the death of the animal. The mixture was to be such that the test animal would die after four days. If it died earlier, the serum was too weak and was rejected. Ehrlich claimed to have made the determination of the valency of serum as accurate as it would be with chemical titration. This again demonstrates his tendency to quantify the life sciences.
Influenced by the mayor of Frankfurt am Main, Franz Adickes, who endeavored to establish science institutions in Frankfurt in preparation of the founding of a university, Ehrlich’s institute moved to Frankfurt In 1899 and was renamed the Royal Prussian Institute of Experimental Therapy ("Königlich Preußisches Institut für Experimentelle Therapie"). The German quality-control methodology was copied by government serum institutes all over the world, and they also obtained the standard serum from Frankfurt. After diphtheria antiserum, tetanus serum and various bactericide serums for use in veterinary medicine were developed in rapid sequence. These were also evaluated at the institute, as was tuberculin and later on various vaccines. Ehrlich’s most important colleague at the institute was the Jewish doctor and biologist Julius Morgenroth.
Ehrlich’s side-chain theory.
This research inspired Ehrlich in 1897 to develop his famous side-chain theory. As he saw it, the reaction between a toxin and the operative components of a serum is a chemical reaction. He explained the toxic effect using the example of tetanus toxin.
He postulated that cell protoplasm contains special structures which have chemical "side chains" (today’s term is macromolecules) to which the toxin binds, affecting function. If the organism survives the effects of the toxin, the blocked side-chains are replaced by new ones. This regeneration can be trained, the name for this phenomenon being "immunization." If the cell produces a surplus of side chains, these might also be released into the blood as antibodies.
In the following years Ehrlich expanded his side chain theory using concepts (“amboceptors,” “receptors of the first, second and third order,” etc.) which are no longer customary. Between the antigen and the antibody he assumed there was an additional immune molecule, which he called an “additive” or a “complement.” For him, the side chain contained at least two functional groups.
For providing a theoretical basis for immunology as well as for his work on serum valency, Ehrlich was awarded the Nobel Prize for Physiology or Medicine in 1908 together with Élie Metchnikoff. Metchnikoff, who had researched the cellular branch of immunity, Phagocytosis, at the Pasteur Institute had previously sharply attacked Ehrlich.
Cancer research.
In 1901, the Prussian Ministry of Finance criticized Ehrlich for exceeding his budget and as a consequence reduced his income. In this situation Althoff arranged a contact with Georg Speyer, a Jewish philanthropist and joint owner of the bank house Lazard Speyer-Ellissen. The cancerous disease of Princess Victoria, the widow of the German Emperor Friedrich II, had received much public attention and prompted a collection among wealthy Frankfurt citizens, including Speyer, in support of cancer research. Ehrlich had also received from the German Emperor Wilhelm II a personal request to devote all his energy to cancer research. Such efforts led to the founding of a department for cancer research affiliated with the Institute of Experimental Therapy. The chemist Gustav Embden, among others, worked there. Ehrlich informed his sponsors that cancer research meant basic research, and that a cure could not be expected soon.
Among the results achieved by Ehrlich and his research colleagues was the insight that when tumors are cultivated by transplanting tumor cells, their malignancy increases from generation to generation. If the primary tumor is removed, then metastasis precipitously increases. Ehrlich applied bacteriological methods to cancer research. In analogy to vaccination, he attempted to generate immunity to cancer by injecting weakened cancer cells. Both in cancer research and chemotherapy research (see below) he introduced the methodologies of Big Science.
Chemotherapy.
In vivo staining.
In 1885 Ehrlich‘s monograph "The Need of the Organism for Oxygen," ("Das Sauerstoffbedürfnis des Organismus- Eine farbenanalytische Studie") appeared, which he also submitted as a habilitation thesis. In it he introduced the new technology of in vivo staining. One of his findings was that pigments can only be easily assimilated by living organisms if they are in granular form. He injected the dyes alizarin blue and indophenol blue into laboratory animals and established after their death that various organs had been colored to different degrees. In organs with high oxygen saturation, indophenol was retained; in organs with medium saturation, indophenol was reduced, but not alizarin blue. And in areas with low oxygen saturation, both pigments were reduced. With this work, Ehrlich also formulated the conviction which guided his research: that all life processes can be traced to processes of physical chemistry occurring in the cell.
Methylene blue.
In the course of his investigations Ehrlich came across methylene blue, which he regarded as particularly suitable for staining bacteria. Later, Robert Koch also used methylene blue as a dye in his research on the tuberculosis pathogen. In Ehrlich’s view, an added benefit was that methylene blue also stained the long appendages of nerve cells, the axons. He initiated a doctoral dissertation on the subject, but did not follow up the topic himself. It was the opinion of the neurologist Ludwig Edinger that Ehrlich had thereby opened up a major new topic in the field of neurology.
After mid-1889, when Ehrlich was unemployed, he privately continued his research on methylene blue. His work on in vivo staining gave him the idea of using it therapeutically. Since the parasite family of "Plasmodiidae" – which includes the malaria pathogen – can be stained with methylene blue, he thought it could possibly be used in the treatment of malaria. In the case of two patients so treated at the city hospital in Berlin-Moabit, their fever indeed subsided and the malaria plasmodia disappeared from their blood. Ehrlich obtained methylene blue from the company Meister Lucius & Brüning AG (later renamed Hoechst AG), which started a long collaboration with this company.
The search for a “Chemotherapia specifica”.
Before the Institute of Experimental Therapy had moved to Frankfurt, Ehrlich had already resumed work on methylene blue. After the death of Georg Speyer, his widow Franziska Speyer endowed the Georg-Speyer House in his memory which was erected next door to Ehrlich’s institute. As director of the Georg-Speyer House, Ehrlich transferred his chemotherapeutic research there. He was looking for an agent which was as effective as methylene blue, but without its side effects. His model was on the one hand the impact of quinine on malaria, and on the other hand, in analogy to serum therapy, he thought there must also be chemical pharmaceuticals which would have just as specific an effect on individual diseases. His goal was to find a "Therapia sterilisans magna," in other words a treatment that could kill all disease pathogens.
As a model for experimental therapy Ehrlich used a guinea pig disease trypanosoma and tested out various chemical substances on laboratory animals. The trypanosomes could indeed be successfully killed with the dye trypan red. Beginning in 1906, he intensively investigated atoxyl and had it tested by Robert Koch along with other arsenic compounds during Koch's sleeping sickness expedition of 1906/07. Although the name literally means “nonpoisonous,” atoxyl does cause damage, especially to the optic nerve. Ehrlich elaborated the systematic testing of chemical compounds in the sense of screening as now practiced in the pharmaceutical industry. He discovered that Compound 418 - Arsenophenylglycine - had an impressive therapeutic effect and had it tested in Africa.
With the support of his assistant Sahachiro Hata Ehrlich discovered in 1909 that Compound 606, Arsphenamine effectively combatted "spirillum" spirochaetes bacteria, one of whose subspecies causes syphilis. The compound proved to have few side effects in human trials, and the spirochetes disappeared in seven syphilis patients after this treatment.
After extensive clinical testing (all the research participants had the negative example of tuberculin in mind) the Hoechst company began to market the compound toward the end of 1910 under the name Salvarsan. This was the first agent with a specific therapeutic effect to be created on the basis of theoretical considerations. Salvarsan proved to be amazingly effective, particularly when compared with the conventional therapy of mercury salts. Manufactured by Hoechst AG, Salvarsan became the most widely prescribed drug in the world. It was the most effective drug for treating syphilis until penicillin became available in the 1940s. Salvarsan required improvement as to side effects and solubility and was replaced in 1911 with Neosalvarsan. Ehrlich's work illuminated the existence of the blood-brain barrier.
The medication triggered the so-called "Salvarsan war." On one side there was hostility on the part of those who feared a resulting moral breakdown of sexual inhibitions. Ehrlich was also accused, with clearly anti-Semitic undertones, of excessively enriching himself. In addition, Ehrlich's associate, Paul Uhlenhuth claimed priority in discovering the drug.
Because some people died during the clinical testing, Ehrlich was accused of "stopping at nothing." In 1914, one of the most prominent accusers was convicted of criminal libel at a trial for which Ehrlich was called to testify. Though Ehrlich was thereby exonerated, the ordeal threw him into a depression from which he never fully recovered.
Magic bullet.
Ehrlich reasoned that if a compound could be made that selectively targeted a disease-causing organism, then a toxin for that organism could be delivered along with the agent of selectivity. Hence, a "magic bullet" ("magische Kugel", his term for an ideal therapeutic agent) would be created that killed only the organism targeted. The concept of a "magic bullet" has to some extent been realized by the development of antibody-drug conjugates (a monoclonal antibody linked to a cytotoxic biologically active drug), as they enable cytotoxic drugs to be selectively delivered to their designated targets (e.g. cancer cells).
Legacy.
In 1910, a street was named after Ehrlich in Frankfurt-Sachsenhausen. During the Third Reich, Ehrlich's achievements were ignored while Emil Adolf von Behring was stylized as the ideal Aryan scientist, and the street named after Ehrlich was given another name. Shortly after the end of the war the name Paul-Ehrlich-Strasse was reinstated, and today numerous German cities have streets named after Paul Ehrlich.
West Germany issued a postage stamp in 1954 on the 100th anniversary of the births of Paul Ehrlich (14 March 1854) and Emil von Behring (15 March 1854).
A 200 Deutsche Mark bank note featured Paul Ehrlich.
The German Paul Ehrlich Institute, the successor to the Steglitz Institute for Serum Research and Serum Testing and the Frankfurt Royal Institute for Experimental Therapy, was named in 1947 after its first director, Paul Ehrlich.
His name is also borne by many schools and pharmacies, by the Paul-Ehrlich-Gesellschaft für Chemotherapie e. V. (PEG) in Frankfurt am Main, and the Paul-Ehrlich-Klinik in Bad Homburg vor der Höhe. The Paul Ehrlich and Ludwig Darmstaedter Prize is the most distinguished German award for biomedical research. A European network of PhD studies in Medicinal Chemistry has been named after him (Paul Ehrlich MedChem Euro PhD Network).
The Anti-Defamation League awards a Paul Ehrlich–Günther K. Schwerin Human Rights Prize.
A crater of the moon was named after Paul Ehrlich in 1970.
Ehrlich’s life and work was featured in the 1940 U.S. film "Dr. Ehrlich's Magic Bullet" with Edward G. Robinson in the title role. It focused on Salvarsan (arsphenamine, "compound 606"), his cure for syphilis. Since the Nazi government was opposed to this tribute to a Jewish scientist, attempts were made to keep the film a secret in Germany.

</doc>
<doc id="23012" url="https://en.wikipedia.org/wiki?curid=23012" title="Philosophical methodology">
Philosophical methodology

Philosophical method (or philosophical methodology) is the study of how to do philosophy. A common view among philosophers is that philosophy is distinguished by the ways that philosophers follow in addressing philosophical questions. There is not just one method that philosophers use to answer philosophical questions.
Methodology process.
Systematic philosophy is a generic term that applies to philosophical methods and approaches that attempt to provide a framework in reason that can explain all questions and problems related to human life. Examples of systematic philosophers include Plato, Aristotle, Descartes, Spinoza, and Hegel. In a meaningful sense, all of western philosophy from Plato to the modern schools of theoretical metaphysics. In many ways, any attempts to formulate a philosophical method that provides the ultimate constituents of reality, a metaphysics, can be considered systematic philosophy. In modern philosophy the reaction to systematic philosophy began with Kierkegaard and continued in various forms through analytic philosophy, existentialism, hermeneutics, and deconstructionism.
Some common features of the methods that philosophers follow (and discuss when discussing philosophical method) include:
Doubt and the sense of wonder.
Plato said that "philosophy begins in wonder", a view which is echoed by Aristotle: "It was their wonder, astonishment, that first led men to philosophize and still leads them." Philosophizing may begin with some simple doubts about accepted beliefs. The initial impulse to philosophize may arise from suspicion, for example that we do not fully understand, and have not fully justified, even our most basic beliefs about the world.
Formulate questions and problems.
Another element of philosophical method is to formulate questions to be answered or problems to be solved. The working assumption is that the more clearly the question or problem is stated, the easier it is to identify critical issues.
A relatively small number of major philosophers prefer not to be quick, but to spend more time trying to get extremely clear on what the problem is all about.
Enunciate a solution.
Another approach is to enunciate a theory, or to offer a definition or analysis, which constitutes an attempt to solve a philosophical problem. Sometimes a philosophical theory by itself can be stated quite briefly. All the supporting philosophical text is offered by way of hedging, explanation, and argument.
Not all proposed solutions to philosophical problems consist of definitions or generalizations. Sometimes what is called for is a certain sort of explanation — not a causal explanation, but an explanation for example of how two different views, which seem to be contrary to one another, can be held at the same time, consistently. One can call this a philosophical explanation.
Justify the solution.
A argument is a set of statements, one of which (the conclusion), it is said or implied, follows from the others (the premises). One might think of arguments as bundles of reasons — often not just a list, but logically interconnected statements — followed by the claim they are reasons for. The reasons are the premises, the claim they support is the conclusion; together they make an argument.
Philosophical arguments and justifications are another important part of philosophical method. It is rare to find a philosopher, particularly in the Western philosophical tradition, who lacks many arguments. Philosophers are, or at least are expected to be, very good at giving arguments. They constantly demand and offer arguments for different claims they make. This therefore indicates that philosophy is a quest for arguments.
A good argument — a clear, organized, and sound statement of reasons — may ultimately cure the original doubts that motivated us to take up philosophy. If one is willing to be satisfied without any good supporting reasons, then a Western philosophical approach may not be what one actually requires.
Philosophical criticism.
In philosophy, which concerns the most fundamental aspects of the universe, the experts all disagree. It follows that another element of philosophical method, common in the work of nearly all philosophers, is philosophical criticism. It is this that makes much philosophizing a social endeavor.
Philosophers offer definitions and explanations in solution to problems; they argue for those solutions; and then other philosophers provide counter arguments, expecting to eventually come up with better solutions. This exchange and resulting revision of views is called dialectic. Dialectic (in one sense of this history-laden word) is simply philosophical conversation amongst people who do not always agree with each other about everything.
One can do this sort of harsh criticism on one's own, but others can help greatly, if important assumptions are shared with the person offering the criticisms. Others are able to think of criticisms from another perspective.
Some philosophers and ordinary people dive right in and start trying to solve the problem. They immediately start giving arguments, pro and con, on different sides of the issue. Doing philosophy is different from this. It is about questioning assumptions, digging for deeper understanding. Doing philosophy is about the journey, the process, as much as it is about the destination, the conclusion. Its method differs from other disciplines, in which the experts can agree about most of the fundamentals.
Motivation.
Method in philosophy is in some sense rooted in motivation, only by understanding why people take up philosophy can one properly understand what philosophy is. People often find themselves believing things that they do not understand. For example, about God, themselves, the natural world, human society, morality and human productions. Often, people fail to understand what it is they believe, and fail to understand the reasons they believe in what they do. Some people have questions about the meaning of their beliefs and questions about the justification (or rationality) of their beliefs. A lack of these things shows a lack of understanding, and some dislike not having this understanding.
These questions about are only the tip of the philosophical iceberg. There are many other things about this universe about which people are also fundamentally ignorant. Philosophers are in the business of investigating all sorts of those areas of ignorance.
A bewilderingly huge number of basic concepts are poorly understood. For example:
One might also consider some of the many questions about justification. Human lives are deeply informed with many basic assumptions. Different assumptions, would lead to different ways of living.

</doc>
<doc id="23013" url="https://en.wikipedia.org/wiki?curid=23013" title="Punch and Judy">
Punch and Judy

Punch and Judy is a traditional, popular, and usually very violent puppet show featuring Pulcinella (Mr. Punch) and his wife, Judy. The performance consists of a sequence of short scenes, each depicting an interaction between two characters, most typically Mr. Punch and one other character (who usually falls victim to Mr. Punch's club). It is often associated with traditional British seaside culture. The various episodes of Punch and Judy are performed in the spirit of outrageous comedy — often provoking shocked laughter — and are dominated by the clowning of Mr. Punch.
The show is performed by a single puppeteer inside the booth, known since Victorian times as a "professor" or "punchman", and assisted sometimes by a "bottler", who corrals the audience outside the booth, introduces the performance, and collects the money ("the bottle"). The bottler might also play accompanying music or sound effects on a drum or guitar, and engage in back chat with the puppets, sometimes repeating lines that may have been difficult for the audience to understand. In Victorian times the drum and pan pipes were the instruments of choice. Today, the audience is also encouraged to participate, calling out to the characters on the stage to warn them of danger, or clue them into what is going on behind their backs. Also nowadays, most professors work solo, since the need for a bottler became less important when busking with the show gave way to paid engagements at private parties or public events.
History.
The Punch and Judy show has roots in the 16th-century Italian commedia dell'arte. The figure of Punch derives from the Neapolitan stock character of Pulcinella, which was anglicized to "Punchinello". He is a manifestation of the Lord of Misrule and Trickster figures of deep-rooted mythologies. Punch's wife was originally called "Joan."
The figure who later became Mr. Punch made his first recorded appearance in England on 9 May 1662, which is traditionally reckoned as Punch's UK birthday. The diarist Samuel Pepys observed a marionette show featuring an early version of the Punch character in Covent Garden in London. It was performed by an Italian puppet showman, Pietro Gimonde, a.k.a. "Signor Bologna." Pepys described the event in his diary as "an Italian puppet play, that is within the rails there, which is very pretty."
In the British Punch and Judy show, Punch wears a brightly coloured jester's motley and sugarloaf hat with a tassel. He is a hunchback whose hooked nose almost meets his curved, jutting chin. He carries a stick (called a slapstick) as large as himself, which he freely uses upon most of the other characters in the show. He speaks in a distinctive squawking voice, produced by a contrivance known as a "swazzle" or "swatchel" which the professor holds in his mouth, transmitting his gleeful cackle. This gives Punch a vocal quality as though he were speaking through a kazoo. So important is Punch's signature sound that it is a matter of some controversy within Punch and Judy circles as to whether a "non-swazzled" show can be considered a true Punch and Judy Show. Other characters do not use the swazzle, so the Punchman has to switch back and forth while still holding the device in his mouth.
In the early 18th century, the marionette theatre starring Punch was at its height, with showman Martin Powell attracting sizable crowds at both his "Punch's Theatre" at Covent Garden and earlier in provincial Bath, Somerset. Powell has been credited with being "largely responsible for the form taken by the drama of Punch and Judy". In 1721, a puppet theatre that would run for decades opened in Dublin. The cross-dressing actress Charlotte Charke ran the successful but short-lived Punch's Theatre in the Old Tennis Court at St. James's, Westminster, presenting adaptations of Shakespeare as well as plays by herself, her father Colley Cibber, and her friend Henry Fielding. Fielding eventually ran his own puppet theatre under the pseudonym Madame de la Nash to avoid the censorship concomitant with the Theatre Licensing Act of 1737.
Punch was extremely popular in Paris, and, by the end of the 18th century, he was also playing in Britain's American colonies, where even George Washington bought tickets for a show. However, marionette productions presented in empty halls, the back rooms of taverns, or within large tents at England's yearly agricultural events at Bartholomew Fair and Mayfair were expensive and cumbersome to mount and transport. In the latter half of the 18th century, marionette companies began to give way to glove-puppet shows, performed from within a narrow, lightweight booth by one puppeteer, usually with an assistant, or "bottler," to gather a crowd and collect money. These shows might travel through country towns or move from corner to corner along busy London streets, giving many performances in a single day. The character of Punch adapted to the new format, going from a stringed comedian who might say outrageous things to a more aggressive glove-puppet who could do outrageous—and often violent—things to the other characters. About this time, Punch's wife's name changed from "Joan" to "Judy."
The mobile puppet booth of the late 18th- and early 19th-century Punch and Judy glove-puppet show was originally covered in checked bed ticking or whatever inexpensive cloth might come to hand. Later Victorian booths, particularly those used for Christmas parties and other indoor performances, were gaudier affairs. In the 20th century, however, red-and-white-striped puppet booths became iconic features on the beaches of many English seaside and summer holiday resorts. Such striped cloth is the most common covering today, wherever the show might be performed.
A more substantial change came over time to the show's target audience. Originally intended for adults, the show evolved into primarily a children's entertainment in the late Victorian era. Ancient members of the show's cast, like the Devil and Punch's mistress "Pretty Polly," ceased to be included when they came to be seen as inappropriate for young audiences. The term "pleased as Punch" is derived from Punch and Judy; specifically, Mr. Punch's characteristic sense of gleeful self-satisfaction.
The story changes, but some phrases remain the same for decades or even centuries: for example, Punch, after dispatching his foes each in turn, still squeaks his famous catchphrase: ""That's" the way to do it!" Modern British performances of Punch and Judy are no longer exclusively the traditional seaside children's entertainments they had become. They can now be seen at carnivals, festivals, birthday parties, and other celebratory occasions.
Characters.
The characters in a Punch and Judy show are not fixed as in a Shakespeare play, for instance. They are similar to the cast of a soap opera or a folk tale like Robin Hood. While the principal characters must appear, the lesser characters are included at the discretion of the performer. New characters may be added as the tradition evolves, and older characters dropped.
Along with Punch and Judy, the cast of characters usually includes their baby, a hungry crocodile, a clown, an officious policeman, and a prop string of sausages. The devil and the generic hangman Jack Ketch may still make their appearances but, if so, Punch will always get the better of them. The cast of a typical Punch and Judy show today will include:
Characters once regular but now occasional include:
Other characters included Boxers, Chinese Plate Spinners, topical figures, a trick puppet with an extending neck (the "Courtier") and a monkey. A live Dog Toby which sat on the playboard and performed 'with' the puppets was once a regular featured novelty routine.
Story.
Glyn Edwards (2011, p. 19) has likened the story of Punch and Judy to the story of Cinderella. He points out there are parts of the story everyone knows, namely, the cruel step sisters, the invitation to the ball, the handsome prince, the fairy godmother, Cinderella's dress turning to rags at midnight, the glass slipper left behind, the prince searching for its owner and the happy ending. None of these elements can be omitted and the famous story still told. The same principle applies to Punch and Judy. Everyone knows that Punch mishandles the baby, that Punch and Judy quarrel and fight, that a policeman comes for Punch and gets a taste of his stick, that Punch has a gleeful run-in with a variety of other figures and takes his stick to them all, that eventually he faces his final foe (which might be a hangman, the devil, a crocodile, or a ghost). Edwards contends that a proper Punch and Judy show requires these elements or the audience will feel let down.
Peter Fraser writes (1970, p. 8), "the drama developed as a succession of incidents which the audience could join or leave at any time, and much of the show was impromptu." This was elaborated by George Speaight (1970, p. 78), who explained that the plotline "is like a story compiled in a parlour game of Consequences ... the show should, indeed, not be regarded as a story at all but a succession of encounters." Robert Leach makes it clear that "the story is a conceptual entity, not a set text: the means of telling it, therefore, are always variable." Rosalind Crone (2006, p. 1058) asserts the story needed to be episodic so that passers by on the street could easily join or leave the audience during a performance.
Much emphasis is often placed on the first printed script of Punch and Judy (1827). Based on a show by travelling performer Giovanni Piccini, it was illustrated by George Cruikshank and written by John Payne Collier. While this is the only surviving script of a performance, its accuracy is questioned. The performance was stopped frequently to allow Collier and Cruikshank to write and sketch, and Collier, in the words of Speaight (1970, p. 82), is someone of whom "the full list of his forgeries has not yet been reckoned, and the myths he propagated are still being repeated. (His) 'Punch and Judy' is to be warmly welcomed as the first history of puppets in England, but it is also sadly to be examined as the first experiment of a literary criminal."
The tale of Punch and Judy, as previously with Punchinello and Joan, varies from puppeteer to puppeteer and has changed over time. Nonetheless, the skeletal outline is often recognizable. It typically involves Punch behaving outrageously, again, struggling with his wife Judy and the baby and then triumphing in a series of encounters with the forces of law and order (and often the supernatural), interspersed with jokes and songs.
As performed currently in the UK a typical show will start with the arrival of Mr. Punch followed by the introduction of Judy. They may well kiss and dance before Judy requests Mr. Punch to look after the baby. Punch will fail to carry this task out appropriately. It is rare for Punch to hit his baby these days, but he may well sit on it in a failed attempt to "babysit", or drop it, or even let it go through a sausage machine. In any event Judy will return, will be outraged, will fetch a stick and the knockabout will commence. A policeman will arrive in response to the mayhem and will himself be felled by Punch's slapstick. All this is carried out at breakneck farcical speed with much involvement from a gleefully shouting audience. From here on anything goes. Joey the Clown might appear and suggest that "It's dinner time." This will lead to the production of a string of sausages, which Mr. Punch must look after, although the audience will know this really signals the arrival of a crocodile whom Mr. Punch might not see until the audience shouts out and lets him know. Punch's subsequent comic struggle with the crocodile might then leave him in need of a Doctor who will arrive and attempt to treat Punch by walloping him with a stick until Punch turns the tables on him. Punch may next pause to count his "victims" by laying puppets on the stage only for Joey the Clown to move them about behind his back in order to frustrate him. A ghost might then appear and give Mr. Punch a fright before it too is chased off with a slapstick. In less squeamish times a hangman would arrive to punish Mr. Punch, only to himself be tricked into sticking his head in the noose. "Do you do the hanging?" is a question often asked of performers. Some will include it where circumstances warrant (such as for an adult audience) but most do not. Some will choose to include it whatever the circumstances and will face down any critics. Finally the show will often end with the Devil arriving for Mr. Punch (and possibly to threaten his audience as well). Punch — in his final gleefully triumphant moment — will win his fight with the Devil and bring the show to a rousing conclusion and earn a round of applause.
While Punch and Judy, as with the tale of Robin Hood, might follow no one fixed storyline, there are nevertheless episodes common to many recorded versions. It is these set piece encounters or "routines" which are used by performers to construct their own Punch and Judy shows. A visit to a Punch and Judy Festival at Punch's "birthplace" in London's Covent Garden will reveal a whole variety of changes that are wrung by puppeteers from this basic material and although scripts have been published at different times since the early 19th century, none can be claimed as being the definitive traditional script of Punch and Judy. Each printed script reflects the era in which it was performed and the circumstances under which it was printed.
The various episodes of the show are performed in the spirit of outrageous comedy — often provoking shocked laughter — and are dominated by the anarchic clowning of Mr. Punch. While the Victorian version of the show drew on the morality of its day, the Punch & Judy College of Professors considers that the 20th- and 21st-century versions of the tale have evolved into something more akin to a primitive version of The Simpsons, in which a bizarre family is used as vehicle for grotesque visual comedy and a sideways look at contemporary society.
While censorious political correctness threatened Punch and Judy performances in the UK and other English speaking countries for a time, the show is having one of its cyclical recurrences and can now be seen not only in England, Wales, and Ireland, but also in Canada, the United States, the Caribbean and Puerto Rico, Australia, New Zealand and South Africa. In 2001, the characters were honoured in the UK with a set of British commemorative postage stamps, issued by the Royal Mail. In a 2006 UK poll, the public voted Punch and Judy onto the list of icons of England.
Comedy.
Despite Punch’s unapologetic murder throughout the performances, it is still a comedy. The humour is aided by a few things. Rosalind Crone (2006, p. 1065) suggests that since the puppets are carved from wood, their facial expressions cannot change, but are stuck in the same exaggerated pose, which helps to deter any sense of realism and to distance the audience. The use of the swazzle also helps to create humour. It was suggested to Proschan (1981, p. 546) the swazzled sound of Punch’s voice takes the cruelty out of Punch. According to Crone, a third aspect that helped make the violence humorous was that Punch’s violence toward his wife was prompted by her own violence toward him. In this aspect, he retains some of his previous hen-pecked persona. This would suggest that since Punch was merely acting violently out of self-defence, it was okay. This is a possible explanation for the humour of his violence toward his wife, and even towards others who may have somehow "had it coming." . This suggestion better explains the humour of the violence toward the baby. Other characters that had to incur the wrath of Punch varied depending on the punchman, but the most common were the foreigner, the blind man, the publican, the constable, and the devil.
Published scripts.
In 1828, the critic John Payne Collier published a Punch and Judy script under the title "The Tragical Comedy or Comical Tragedy of Punch and Judy". The script was illustrated by the well-known caricaturist George Cruikshank. Collier said his script was based on the version performed by the "professor" Giovanni Piccini in the early 19th century, and Piccini himself had begun performing in the streets of London in the late 18th century. The Collier/Cruickshank "Punch" has been republished in facsimile several times. Collier's later career as a literary forger has cast some doubt on the authenticity of the script, which is rather literary in style and may well have been tidied up from the rough-and-tumble street-theatre original. Punch is primarily an oral tradition, adapted by a succession of exponents from live performances rather than authentic scripts, and in constant evolution. A transcript of a typical Punch and Judy show in London of the 1840s can be found in Henry Mayhew's "London Labour and the London Poor".
Origin of the characters.
In 1996 David Bryson (a British scientist) in the "European Journal of Internal Medicine" suggested that Punch's rages and facial features may have been copied from someone suffering from acromegaly. In late 2015 researchers at the University of Zurich including Frank Rühli suggested that Punch's hunchback and bad temper may have been copied from someone suffering from tuberculous spondylitis.

</doc>
<doc id="23014" url="https://en.wikipedia.org/wiki?curid=23014" title="Poker">
Poker

Poker is a family of gambling card games. All poker variants involve betting as an intrinsic part of play, and determine the winner of each hand according to the combinations of players' cards, at least some of which remain hidden until the end of the hand. Poker games vary in the number of cards dealt, the number of shared or "community" cards, the number of cards that remain hidden, and the betting procedures.
In most modern poker games, the first round of betting begins with one or more of the players making some form of a forced bet (the "blind" and/or "ante"). In standard poker, each player bets according to the rank he believes his hand is worth as compared to the other players. The action then proceeds clockwise as each player in turn must either match, or "call", the maximum previous bet or fold, losing the amount bet so far and all further interest in the hand. A player who matches a bet may also "raise", or increase the bet. The betting round ends when all players have either matched the last bet or folded. If all but one player folds on any round, the remaining player collects the pot without being required to reveal their hand. If more than one player remains in contention after the final betting round, the hands are revealed, and the player with the winning hand takes the pot.
With the exception of initial forced bets, money is only placed into the pot voluntarily by a player who either believes the bet has positive expected value or who is trying to bluff other players for various strategic reasons. Thus, while the outcome of any particular hand significantly involves chance, the long-run expectations of the players are determined by their actions chosen on the basis of probability, psychology, and game theory.
Poker has gained in popularity since the beginning of the twentieth century and has gone from being primarily a recreational activity confined to small groups of enthusiasts to a widely popular activity, both for participants and spectators, including online, with many professional players and multimillion-dollar tournament prizes.
History.
English actor Joseph Cowell reported in his memoirs that the game was played in New Orleans, Louisiana in 1829, with a deck of 20 cards, and four players betting on which player's hand was the most valuable. Jonathan H. Green's book, "An Exposure of the Arts and Miseries of Gambling" (G. B. Zieber, Philadelphia, 1843), described the spread of the game from there to the rest of the country by Mississippi riverboats, on which gambling was a common pastime. As it spread north along the Mississippi River and to the West during the gold rush, it is thought to have become a part of the frontier pioneer ethos.
Soon after this spread, the full 52-card French deck was used and the flush was introduced. The draw was added prior to 1850 (when it was first mentioned in print in a handbook of games). During the American Civil War, many additions were made including stud poker (specifically five-card stud) and the straight. Further American developments followed, such as the wild card (around 1875), lowball and split-pot poker (around 1900), and community card poker games (around 1925).
Modern tournament play became popular in American casinos after the World Series of Poker (WSOP) began in 1970. Among the champions from these early WSOP tournaments were Johnny Moss, Amarillo Slim, Bobby Baldwin, Doyle Brunson, and Puggy Pearson. Later in the 1970s, the first serious poker strategy books appeared, including "Super/System" by Doyle Brunson and "Caro's Book of Poker Tells" by Mike Caro , followed later by "The Theory of Poker" by David Sklansky. By the 1980s, poker was being depicted in popular culture as a commonplace recreational activity. For example, it was featured in at least 10 episodes of "" as a weekly event of the senior staff of the fictional ship's crew. In the 1990s, poker and casino gambling began to expand across the United States.
Poker's popularity experienced an unprecedented spike at the beginning of the 21st century, largely because of the introduction of online poker and hole-card cameras, which turned the game into a spectator sport. Not only could viewers now follow the action and drama of the game on television, but they could also play the game in the comfort of their own homes. In the 2003 World Series of Poker, accountant Chris Moneymaker, who had never played professional poker, won the main event. He won his seat into the $10,000 tournament via a $40 multi-table satellite and turned his $40 into $2.5 million. This helped popularize the game further, which became known as the "Moneymaker effect".
Following the surge in popularity, new poker tours soon emerged, including the World Poker Tour and European Poker Tour, both televised, and the latter sponsored by online poker company PokerStars. Subsequent tours have since been created by PokerStars, such as Latin American Poker Tour and Asia Pacific Poker Tour, as well as other national tours.
In 2009 the International Federation of Poker was founded in Lausanne, Switzerland, becoming the official governing body for poker and promoting the game as a mind sport. In 2011 it announced plans for two new events: The Nations Cup, a duplicate poker team event, to be staged on the London Eye on the banks of the River Thames and “The Table”, the invitation-only IFP World Championship, featuring roughly 130 of the world’s best poker players, in an event to find the 2011 official "World Champion".
Gameplay.
In casual play, the right to deal a hand typically rotates among the players and is marked by a token called a "dealer button" (or "buck"). In a casino, a house dealer handles the cards for each hand, but the button (typically a white plastic disk) is rotated clockwise among the players to indicate a nominal dealer to determine the order of betting. The cards are dealt clockwise around the poker table, one at a time.
One or more players are usually required to make forced bets, usually either an "ante" or a "blind bet" (sometimes both). The dealer shuffles the cards, the player on the chair to his right cuts, and the dealer deals the appropriate number of cards to the players one at a time, beginning with the player to his left. Cards may be dealt either face-up or face-down, depending on the variant of poker being played. After the initial deal, the first of what may be several betting rounds begins. Between rounds, the players' hands develop in some way, often by being dealt additional cards or replacing cards previously dealt. At the end of each round, all bets are gathered into the central pot.
At any time during a betting round, if one player bets, no opponents choose to "call" (match) the bet, and all opponents instead "fold", the hand ends immediately, the bettor is awarded the pot, no cards are required to be shown, and the next hand begins. This is what makes bluffing possible. Bluffing is a primary feature of poker, one that distinguishes it from other vying games and from other games that make use of poker hand rankings.
At the end of the last betting round, if more than one player remains, there is a showdown, in which the players reveal their previously hidden cards and evaluate their hands. The player with the best hand according to the poker variant being played wins the pot. A poker hand comprises five cards; in variants where a player has more than five cards available to them, only the best five-card combination counts.
Variants.
Poker has many variations, all following a similar pattern of play and generally using the same hand ranking hierarchy. There are four main families of variants, largely grouped by the protocol of card-dealing and betting:
Other games that use poker hand rankings may likewise be referred to as "poker". Video poker is a single-player video game that functions much like a slot machine; most video poker machines play draw poker, where the player bets, a hand is dealt, and the player can discard and replace cards. Payout is dependent on the hand resulting after the draw and the player's initial bet.
Strip poker is a traditional poker variation where players remove clothing when they lose bets. Since it depends only on the basic mechanic of betting in rounds, strip poker can be played with any form of poker; however, it is usually based on simple variants with few betting rounds, like five card draw.
Another game with the "poker" name, but with a vastly different mode of play, is called "Acey-Deucey" or "Red Dog" poker. This game is more similar to Blackjack in its layout and betting; each player bets against the house, and then is dealt two cards. For the player to win, the third card dealt (after an opportunity to raise the bet) must have a value in-between the first two. Payout is based on the odds that this is possible, based on the difference in values of the first two cards. Other poker-like games played at casinos against the house include three card poker and pai gow poker.
Computer programs.
In a January 2015 article published in Science, a group of researchers mostly from the University of Alberta announced that they "essentially weakly solved" heads-up limit "Texas hold 'em" with their development of their Cepheus poker bot. The authors claimed that Cepheus, in 1000 games, would lose at most 1 big blind on average against its worst-case opponent, a strategy that is so "close to optimal" that "it can't be beaten with statistical significance within a lifetime of human poker playing".
Less autonomous poker programs exist whose primary purpose is not to play poker by themselves, but is instead to calculate the odds of certain hand outcomes. For example, one might input a hand which contains three 7s and two unrelated low cards, the program in question would then return that holding just the 7s results in a 10.37% chance of an improved hand being drawn.

</doc>
<doc id="23015" url="https://en.wikipedia.org/wiki?curid=23015" title="Programming language">
Programming language

A programming language is a formal constructed language designed to communicate instructions to a machine, particularly a computer. Programming languages can be used to create programs to control the behavior of a machine or to express algorithms.
The earliest known programmable machine preceded the invention of the digital computer and is the automatic flute player described in the 9th century by the brothers Musa in Baghdad, at the time a major centre of knowledge. From the early 1800s, "programs" were used to direct the behavior of machines such as Jacquard looms and player pianos. Thousands of different programming languages have been created, mainly in the computer field, and many more still are being created every year. Many programming languages require computation to be specified in an imperative form (i.e., as a sequence of operations to perform), while other languages use other forms of program specification such as the declarative form (i.e. the desired result is specified, not how to achieve it).
The description of a programming language is usually split into the two components of syntax (form) and semantics (meaning). Some languages are defined by a specification document (for example, the C programming language is specified by an ISO Standard), while other languages (such as Perl) have a dominant implementation that is treated as a reference. Some languages have both, with the basic language defined by a standard and extensions taken from the dominant implementation being common.
Definitions.
A programming language is a notation for writing programs, which are specifications of a computation or algorithm. Some, but not all, authors restrict the term "programming language" to those languages that can express "all" possible algorithms. Traits often considered important for what constitutes a programming language include:
Markup languages like XML, HTML or troff, which define structured data, are not usually considered programming languages. Programming languages may, however, share the syntax with markup languages if a computational semantics is defined. XSLT, for example, is a Turing complete XML dialect. Moreover, LaTeX, which is mostly used for structuring documents, also contains a Turing complete subset.
The term "computer language" is sometimes used interchangeably with programming language. However, the usage of both terms varies among authors, including the exact scope of each. One usage describes programming languages as a subset of computer languages. In this vein, languages used in computing that have a different goal than expressing computer programs are generically designated computer languages. For instance, markup languages are sometimes referred to as computer languages to emphasize that they are not meant to be used for programming.
Another usage regards programming languages as theoretical constructs for programming abstract machines, and computer languages as the subset thereof that runs on physical computers, which have finite hardware resources. John C. Reynolds emphasizes that formal specification languages are just as much programming languages as are the languages intended for execution. He also argues that textual and even graphical input formats that affect the behavior of a computer are programming languages, despite the fact they are commonly not Turing-complete, and remarks that ignorance of programming language concepts is the reason for many flaws in input formats.
History.
Early developments.
The earliest computers were often programmed without the help of a programming language, by writing programs in absolute machine language. The programs, in decimal or binary form, were read in from punched cards or magnetic tape, or toggled in on switches on the front panel of the computer. Absolute machine languages were later termed "first-generation programming languages" (1GL).
The next step was development of so-called "second-generation programming languages" (2GL) or assembly languages, which were still closely tied to the instruction set architecture of the specific computer. These served to make the program much more human-readable, and relieved the programmer of tedious and error-prone address calculations.
The first "high-level programming languages", or "third-generation programming languages" (3GL), were written in the 1950s. An early high-level programming language to be designed for a computer was Plankalkül, developed for the German Z3 by Konrad Zuse between 1943 and 1945. However, it was not implemented until 1998 and 2000.
John Mauchly's Short Code, proposed in 1949, was one of the first high-level languages ever developed for an electronic computer. Unlike machine code, Short Code statements represented mathematical expressions in understandable form. However, the program had to be translated into machine code every time it ran, making the process much slower than running the equivalent machine code.
At the University of Manchester, Alick Glennie developed Autocode in the early 1950s. A programming language, it used a compiler to automatically convert the language into machine code. The first code and compiler was developed in 1952 for the Mark 1 computer at the University of Manchester and is considered to be the first compiled high-level programming language.
The second autocode was developed for the Mark 1 by R. A. Brooker in 1954 and was called the "Mark 1 Autocode". Brooker also developed an autocode for the Ferranti Mercury in the 1950s in conjunction with the University of Manchester. The version for the EDSAC 2 was devised by D. F. Hartley of University of Cambridge Mathematical Laboratory in 1961. Known as EDSAC 2 Autocode, it was a straight development from Mercury Autocode adapted for local circumstances, and was noted for its object code optimisation and source-language diagnostics which were advanced for the time. A contemporary but separate thread of development, Atlas Autocode was developed for the University of Manchester Atlas 1 machine.
In 1954, language FORTRAN was invented at IBM by John Backus; it was the first widely used high level general purpose programming language to have a functional implementation, as opposed to just a design on paper. It is still popular language for high-performance computing and is used for programs that benchmark and rank the world's fastest supercomputers.
Another early programming language was devised by Grace Hopper in the US, called FLOW-MATIC. It was developed for the UNIVAC I at Remington Rand during the period from 1955 until 1959. Hopper found that business data processing customers were uncomfortable with mathematical notation, and in early 1955, she and her team wrote a specification for an English programming language and implemented a prototype. The FLOW-MATIC compiler became publicly available in early 1958 and was substantially complete in 1959. Flow-Matic was a major influence in the design of COBOL, since only it and its direct descendant AIMACO were in actual use at the time.
Refinement.
The increased use of high-level languages introduced a requirement for "low-level programming languages" or "system programming languages". These languages, to varying degrees, provide facilities between assembly languages and high-level languages, and can be used to perform tasks which require direct access to hardware facilities but still provide higher-level control structures and error-checking.
The period from the 1960s to the late 1970s brought the development of the major language paradigms now in use:
Each of these languages spawned descendants, and most modern programming languages count at least one of them in their ancestry.
The 1960s and 1970s also saw considerable debate over the merits of "structured programming", and whether programming languages should be designed to support it. Edsger Dijkstra, in a famous 1968 letter published in the Communications of the ACM, argued that GOTO statements should be eliminated from all "higher level" programming languages.
Consolidation and growth.
The 1980s were years of relative consolidation. C++ combined object-oriented and systems programming. The United States government standardized Ada, a systems programming language derived from Pascal and intended for use by defense contractors. In Japan and elsewhere, vast sums were spent investigating so-called "fifth generation" languages that incorporated logic programming constructs. The functional languages community moved to standardize ML and Lisp. Rather than inventing new paradigms, all of these movements elaborated upon the ideas invented in the previous decades.
One important trend in language design for programming large-scale systems during the 1980s was an increased focus on the use of "modules", or large-scale organizational units of code. Modula-2, Ada, and ML all developed notable module systems in the 1980s, which were often wedded to generic programming constructs.
The rapid growth of the Internet in the mid-1990s created opportunities for new languages. Perl, originally a Unix scripting tool first released in 1987, became common in dynamic websites. Java came to be used for server-side programming, and bytecode virtual machines became popular again in commercial settings with their promise of "Write once, run anywhere" (UCSD Pascal had been popular for a time in the early 1980s). These developments were not fundamentally novel, rather they were refinements of many existing languages and paradigms (although their syntax was often based on the C family of programming languages).
Programming language evolution continues, in both industry and research. Current directions include security and reliability verification, new kinds of modularity (mixins, delegates, aspects), and database integration such as Microsoft's LINQ.
"Fourth-generation programming languages" (4GL) are a computer programming languages which aim to provide a higher level of abstraction of the internal computer hardware details than 3GLs. "Fifth generation programming languages" (5GL) are programming languages based on solving problems using constraints given to the program, rather than using an algorithm written by a programmer.
Elements.
All programming languages have some primitive building blocks for the description of data and the processes or transformations applied to them (like the addition of two numbers or the selection of an item from a collection). These primitives are defined by syntactic and semantic rules which describe their structure and meaning respectively.
Syntax.
A programming language's surface form is known as its syntax. Most programming languages are purely textual; they use sequences of text including words, numbers, and punctuation, much like written natural languages. On the other hand, there are some programming languages which are more graphical in nature, using visual relationships between symbols to specify a program.
The syntax of a language describes the possible combinations of symbols that form a syntactically correct program. The meaning given to a combination of symbols is handled by semantics (either formal or hard-coded in a reference implementation). Since most languages are textual, this article discusses textual syntax.
Programming language syntax is usually defined using a combination of regular expressions (for lexical structure) and Backus–Naur Form (for grammatical structure). Below is a simple grammar, based on Lisp:
This grammar specifies the following:
The following are examples of well-formed token sequences in this grammar: codice_1, codice_2 and codice_3.
Not all syntactically correct programs are semantically correct. Many syntactically correct programs are nonetheless ill-formed, per the language's rules; and may (depending on the language specification and the soundness of the implementation) result in an error on translation or execution. In some cases, such programs may exhibit undefined behavior. Even when a program is well-defined within a language, it may still have a meaning that is not intended by the person who wrote it.
Using natural language as an example, it may not be possible to assign a meaning to a grammatically correct sentence or the sentence may be false:
The following C language fragment is syntactically correct, but performs operations that are not semantically defined (the operation *p Â» 4 has no meaning for a value having a complex type and p->im is not defined because the value of p is the null pointer):
If the type declaration on the first line were omitted, the program would trigger an error on compilation, as the variable "p" would not be defined. But the program would still be syntactically correct, since type declarations provide only semantic information.
The grammar needed to specify a programming language can be classified by its position in the Chomsky hierarchy. The syntax of most programming languages can be specified using a Type-2 grammar, i.e., they are context-free grammars. Some languages, including Perl and Lisp, contain constructs that allow execution during the parsing phase. Languages that have constructs that allow the programmer to alter the behavior of the parser make syntax analysis an undecidable problem, and generally blur the distinction between parsing and execution. In contrast to Lisp's macro system and Perl's codice_4 blocks, which may contain general computations, C macros are merely string replacements, and do not require code execution.
Semantics.
The term Semantics refers to the meaning of languages, as opposed to their form (syntax).
Static semantics.
The static semantics defines restrictions on the structure of valid texts that are hard or impossible to express in standard syntactic formalisms. For compiled languages, static semantics essentially include those semantic rules that can be checked at compile time. Examples include checking that every identifier is declared before it is used (in languages that require such declarations) or that the labels on the arms of a case statement are distinct. Many important restrictions of this type, like checking that identifiers are used in the appropriate context (e.g. not adding an integer to a function name), or that subroutine calls have the appropriate number and type of arguments, can be enforced by defining them as rules in a logic called a type system. Other forms of static analyses like data flow analysis may also be part of static semantics. Newer programming languages like Java and C# have definite assignment analysis, a form of data flow analysis, as part of their static semantics.
Dynamic semantics.
Once data has been specified, the machine must be instructed to perform operations on the data. For example, the semantics may define the strategy by which expressions are evaluated to values, or the manner in which control structures conditionally execute statements. The "dynamic semantics" (also known as "execution semantics") of a language defines how and when the various constructs of a language should produce a program behavior. There are many ways of defining execution semantics. Natural language is often used to specify the execution semantics of languages commonly used in practice. A significant amount of academic research went into formal semantics of programming languages, which allow execution semantics to be specified in a formal manner. Results from this field of research have seen limited application to programming language design and implementation outside academia.
Type system.
A type system defines how a programming language classifies values and expressions into "types", how it can manipulate those types and how they interact. The goal of a type system is to verify and usually enforce a certain level of correctness in programs written in that language by detecting certain incorrect operations. Any decidable type system involves a trade-off: while it rejects many incorrect programs, it can also prohibit some correct, albeit unusual programs. In order to bypass this downside, a number of languages have "type loopholes", usually unchecked casts that may be used by the programmer to explicitly allow a normally disallowed operation between different types. In most typed languages, the type system is used only to type check programs, but a number of languages, usually functional ones, infer types, relieving the programmer from the need to write type annotations. The formal design and study of type systems is known as "type theory".
Typed versus untyped languages.
A language is "typed" if the specification of every operation defines types of data to which the operation is applicable, with the implication that it is not applicable to other types. For example, the data represented by codice_5 is a string, and in many programming languages dividing a number by a string has no meaning and will be rejected by the compilers. The invalid operation may be detected when the program is compiled ("static" type checking) and will be rejected by the compiler with a compilation error message, or it may be detected when the program is run ("dynamic" type checking), resulting in a run-time exception. Many languages allow a function called an exception handler to be written to handle this exception and, for example, always return "-1" as the result.
A special case of typed languages are the "single-type" languages. These are often scripting or markup languages, such as REXX or SGML, and have only one data type—most commonly character strings which are used for both symbolic and numeric data.
In contrast, an "untyped language", such as most assembly languages, allows any operation to be performed on any data, which are generally considered to be sequences of bits of various lengths. High-level languages which are untyped include BCPL, Tcl, and some varieties of Forth.
In practice, while few languages are considered typed from the point of view of type theory (verifying or rejecting "all" operations), most modern languages offer a degree of typing. Many production languages provide means to bypass or subvert the type system, trading type-safety for finer control over the program's execution (see casting).
Static versus dynamic typing.
In "static typing", all expressions have their types determined prior to when the program is executed, typically at compile-time. For example, 1 and (2+2) are integer expressions; they cannot be passed to a function that expects a string, or stored in a variable that is defined to hold dates.
Statically typed languages can be either "manifestly typed" or "type-inferred". In the first case, the programmer must explicitly write types at certain textual positions (for example, at variable declarations). In the second case, the compiler "infers" the types of expressions and declarations based on context. Most mainstream statically typed languages, such as C++, C# and Java, are manifestly typed. Complete type inference has traditionally been associated with less mainstream languages, such as Haskell and ML. However, many manifestly typed languages support partial type inference; for example, Java and C# both infer types in certain limited cases. Additionally, some programming languages allow for some types to be automatically converted to other types; for example, an int can be used where the program expects a float.
"Dynamic typing", also called "latent typing", determines the type-safety of operations at run time; in other words, types are associated with "run-time values" rather than "textual expressions". As with type-inferred languages, dynamically typed languages do not require the programmer to write explicit type annotations on expressions. Among other things, this may permit a single variable to refer to values of different types at different points in the program execution. However, type errors cannot be automatically detected until a piece of code is actually executed, potentially making debugging more difficult. Lisp, Smalltalk, Perl, Python, JavaScript, and Ruby are dynamically typed.
Weak and strong typing.
"Weak typing" allows a value of one type to be treated as another, for example treating a string as a number. This can occasionally be useful, but it can also allow some kinds of program faults to go undetected at compile time and even at run time.
"Strong typing" prevents the above. An attempt to perform an operation on the wrong type of value raises an error. Strongly typed languages are often termed "type-safe" or "safe".
An alternative definition for "weakly typed" refers to languages, such as Perl and JavaScript, which permit a large number of implicit type conversions. In JavaScript, for example, the expression codice_6 implicitly converts codice_7 to a number, and this conversion succeeds even if codice_7 is codice_9, codice_10, an codice_11, or a string of letters. Such implicit conversions are often useful, but they can mask programming errors.
"Strong" and "static" are now generally considered orthogonal concepts, but usage in the literature differs. Some use the term "strongly typed" to mean "strongly, statically typed", or, even more confusingly, to mean simply "statically typed". Thus C has been called both strongly typed and weakly, statically typed.
It may seem odd to some professional programmers that C could be "weakly, statically typed". However, notice that the use of the generic pointer, the void* pointer, does allow for casting of pointers to other pointers without needing to do an explicit cast. This is extremely similar to somehow casting an array of bytes to any kind of datatype in C without using an explicit cast, such as codice_12 or codice_13.
Standard library and run-time system.
Most programming languages have an associated core library (sometimes known as the 'standard library', especially if it is included as part of the published language standard), which is conventionally made available by all implementations of the language. Core libraries typically include definitions for commonly used algorithms, data structures, and mechanisms for input and output.
The line between a language and its core library differs from language to language. In some cases, the language designers may treat the library as a separate entity from the language. However, a language's core library is often treated as part of the language by its users, and some language specifications even require that this library be made available in all implementations. Indeed, some languages are designed so that the meanings of certain syntactic constructs cannot even be described without referring to the core library. For example, in Java, a string literal is defined as an instance of the java.lang.String class; similarly, in Smalltalk, an anonymous function expression (a "block") constructs an instance of the library's BlockContext class. Conversely, Scheme contains multiple coherent subsets that suffice to construct the rest of the language as library macros, and so the language designers do not even bother to say which portions of the language must be implemented as language constructs, and which must be implemented as parts of a library.
Design and implementation.
Programming languages share properties with natural languages related to their purpose as vehicles for communication, having a syntactic form separate from its semantics, and showing "language families" of related languages branching one from another. But as artificial constructs, they also differ in fundamental ways from languages that have evolved through usage. A significant difference is that a programming language can be fully described and studied in its entirety, since it has a precise and finite definition. By contrast, natural languages have changing meanings given by their users in different communities. While constructed languages are also artificial languages designed from the ground up with a specific purpose, they lack the precise and complete semantic definition that a programming language has.
Many programming languages have been designed from scratch, altered to meet new needs, and combined with other languages. Many have eventually fallen into disuse. Although there have been attempts to design one "universal" programming language that serves all purposes, all of them have failed to be generally accepted as filling this role. The need for diverse programming languages arises from the diversity of contexts in which languages are used:
One common trend in the development of programming languages has been to add more ability to solve problems using a higher level of abstraction. The earliest programming languages were tied very closely to the underlying hardware of the computer. As new programming languages have developed, features have been added that let programmers express ideas that are more remote from simple translation into underlying hardware instructions. Because programmers are less tied to the complexity of the computer, their programs can do more computing with less effort from the programmer. This lets them write more functionality per time unit.
Natural language programming has been proposed as a way to eliminate the need for a specialized language for programming. However, this goal remains distant and its benefits are open to debate. Edsger W. Dijkstra took the position that the use of a formal language is essential to prevent the introduction of meaningless constructs, and dismissed natural language programming as "foolish". Alan Perlis was similarly dismissive of the idea. Hybrid approaches have been taken in Structured English and SQL.
A language's designers and users must construct a number of artifacts that govern and enable the practice of programming. The most important of these artifacts are the language "specification" and "implementation".
Specification.
The specification of a programming language is an artifact that the language users and the implementors can use to agree upon whether a piece of source code is a valid program in that language, and if so what its behavior shall be.
A programming language specification can take several forms, including the following:
Implementation.
An "implementation" of a programming language provides a way to write programs in that language and execute them on one or more configurations of hardware and software. There are, broadly, two approaches to programming language implementation: "compilation" and "interpretation". It is generally possible to implement a language using either technique.
The output of a compiler may be executed by hardware or a program called an interpreter. In some implementations that make use of the interpreter approach there is no distinct boundary between compiling and interpreting. For instance, some implementations of BASIC compile and then execute the source a line at a time.
Programs that are executed directly on the hardware usually run several orders of magnitude faster than those that are interpreted in software.
One technique for improving the performance of interpreted programs is just-in-time compilation. Here the virtual machine, just before execution, translates the blocks of bytecode which are going to be used to machine code, for direct execution on the hardware.
Proprietary languages.
Although most of the most commonly used programming languages have fully open specifications and implementations, many programming languages exist only as proprietary programming languages with the implementation available only from a single vendor, which may claim that such a proprietary language is their intellectual property. Proprietary programming languages are commonly domain specific languages or internal scripting languages for a single product; some proprietary languages are used only internally within a vendor, while others are available to external users.
Some programming languages exist on the border between proprietary and open; for example, Oracle Corporation asserts proprietary rights to some aspects of the Java programming language, and Microsoft's C# programming language, which has open implementations of most parts of the system, also has Common Language Runtime (CLR) as a closed environment.
Many proprietary languages are widely used, in spite of their proprietary nature; examples include MATLAB and VBScript. Some languages may make the transition from closed to open; for example, Erlang was originally an Ericsson's internal programming language.
Usage.
Thousands of different programming languages have been created, mainly in the computing field.
Software is commonly built with 5 programming languages or more.
Programming languages differ from most other forms of human expression in that they require a greater degree of precision and completeness. When using a natural language to communicate with other people, human authors and speakers can be ambiguous and make small errors, and still expect their intent to be understood. However, figuratively speaking, computers "do exactly what they are told to do", and cannot "understand" what code the programmer intended to write. The combination of the language definition, a program, and the program's inputs must fully specify the external behavior that occurs when the program is executed, within the domain of control of that program. On the other hand, ideas about an algorithm can be communicated to humans without the precision required for execution by using pseudocode, which interleaves natural language with code written in a programming language.
A programming language provides a structured mechanism for defining pieces of data, and the operations or transformations that may be carried out automatically on that data. A programmer uses the abstractions present in the language to represent the concepts involved in a computation. These concepts are represented as a collection of the simplest elements available (called primitives). "Programming" is the process by which programmers combine these primitives to compose new programs, or adapt existing ones to new uses or a changing environment.
Programs for a computer might be executed in a batch process without human interaction, or a user might type commands in an interactive session of an interpreter. In this case the "commands" are simply programs, whose execution is chained together. When a language can run its commands through an interpreter (such as a Unix shell or other command-line interface), without compiling, it is called a scripting language.
Measuring language usage.
It is difficult to determine which programming languages are most widely used, and what usage means varies by context. One language may occupy the greater number of programmer hours, a different one have more lines of code, and a third may consume the most CPU time. Some languages are very popular for particular kinds of applications. For example, COBOL is still strong in the corporate data center, often on large mainframes; Fortran in scientific and engineering applications; Ada in aerospace, transportation, military, real-time and embedded applications; and C in embedded applications and operating systems. Other languages are regularly used to write many different kinds of applications.
Various methods of measuring language popularity, each subject to a different bias over what is measured, have been proposed:
Combining and averaging information from various internet sites, langpop.com claims that in 2013 the ten most popular programming languages are (in descending order by overall popularity): C, Java, PHP, JavaScript, C++, Python, Shell, Ruby, Objective-C and C#.
Taxonomies.
There is no overarching classification scheme for programming languages. A given programming language does not usually have a single ancestor language. Languages commonly arise by combining the elements of several predecessor languages with new ideas in circulation at the time. Ideas that originate in one language will diffuse throughout a family of related languages, and then leap suddenly across familial gaps to appear in an entirely different family.
The task is further complicated by the fact that languages can be classified along multiple axes. For example, Java is both an object-oriented language (because it encourages object-oriented organization) and a concurrent language (because it contains built-in constructs for running multiple threads in parallel). Python is an object-oriented scripting language.
In broad strokes, programming languages divide into "programming paradigms" and a classification by "intended domain of use," with general-purpose programming languages distinguished from domain-specific programming languages. Traditionally, programming languages have been regarded as describing computation in terms of imperative sentences, i.e. issuing commands. These are generally called imperative programming languages. A great deal of research in programming languages has been aimed at blurring the distinction between a program as a set of instructions and a program as an assertion about the desired answer, which is the main feature of declarative programming. More refined paradigms include procedural programming, object-oriented programming, functional programming, and logic programming; some languages are hybrids of paradigms or multi-paradigmatic. An assembly language is not so much a paradigm as a direct model of an underlying machine architecture. By purpose, programming languages might be considered general purpose, system programming languages, scripting languages, domain-specific languages, or concurrent/distributed languages (or a combination of these). Some general purpose languages were designed largely with educational goals.
A programming language may also be classified by factors unrelated to programming paradigm. For instance, most programming languages use English language keywords, while a minority do not. Other languages may be classified as being deliberately esoteric or not.

</doc>
<doc id="23019" url="https://en.wikipedia.org/wiki?curid=23019" title="Economy of Poland">
Economy of Poland

The Economy of Poland is the largest economy in Central Europe, sixth-largest in the EU and the largest among the ex-communist members of the European Union. Before the late-2000s recession its economy grew a yearly growth rate of over 6.0% .
Poland is ranked 20th worldwide in terms of GDP and classified as high-income economy by World Bank. The largest component of its economy is the service sector. With the economic reform of 1989 the Polish external debt increased from $42.2 billion in 1989 to $365.2 billion in 2014
According to the Central Statistical Office of Poland, in 2010 the Polish economic growth rate was 3.9%, which was one of the best results in Europe. In Q1 2014 its economy grew by 3.4% and is expected to grow by 3.4% in 2014, 3.7% in 2015 and 3.9% in 2016.
History.
Poland has seen the largest increase GDP per capita (more than 100%) both among the former Soviet-bloc countries, and compared to the EU-15 (around 45%). It has had uninterrupted economic growth since 1992, even after the 2007 financial crisis.
Before 1989.
This article discusses the economy of the current Poland, post-1989. For historical overview of past Polish economies, see:
1990-2009.
The Polish state steadfastly pursued a policy of economic liberalization throughout the 1990s, with positive results for economic growth but negative results for some sectors of the population. The privatization of small and medium state-owned companies and a liberal law on establishing new firms has encouraged the development of the private business sector, which has been the main drive for Poland's economic growth. The agricultural sector remains handicapped by structural problems, surplus labor, inefficient small farms, and a lack of investment. Restructuring and privatization of "sensitive sectors" (e.g. coal), has also been slow, but recent foreign investments in energy and steel have begun to turn the tide. Recent reforms in health care, education, the pension system, and state administration have resulted in larger than expected fiscal pressures. Improving this account deficit and tightening monetary policy, with focus on inflation, are priorities for the Polish government. Further progress in public finance depends mainly on the reduction of public sector employment, and an overhaul of the tax code to incorporate farmers, who currently pay significantly lower taxes than other people with similar income levels.
Since the 2009 financial crisis.
Since the global recession of 2009, Poland's GDP continued to grow. In 2009, at the high point of the crisis, the GDP for the European Union as a whole dropped by 4.5% while Polish GDP increased by 1.6%. As of November 2013, the size of EU's economy remains below the pre-crisis level, while Poland's economy increased by a cumulative 16%. The major reasons for its success appear to be a large internal market (in terms of population is sixth in EU) and a business friendly political climate. The economic reforms implemented after the fall of communism in the 1990s have also played a role; between 1989 and 2007 Poland's economy grew by 177%, faster than other countries in Eastern and Central Europe, while at the same time millions were left without work.
Another factor which allowed the Polish economy to avoid the financial crisis was its low level of public debt, at about 50% of GDP, below the EU average (around 90%). Strict financial regulation also helped to keep household and corporate debt low. Furthermore, unlike many other European countries, Poland did not implement austerity but rather boosted domestic demand through Keynesian policy of tax cut, and foreign-assistance funded public spending. An additional reason for its success lay in the fact that Poland is outside the Euro zone. The depreciation of the currency, the złoty, increased international competitiveness and boosted the value of Poland's exports (in złotys).
However, the economic fluctuations of the business cycle did have an impact on Poland's unemployment rate, which by early 2013 reached almost 11%. This level was still below European average and has begun falling subsequently. As of February 2014, Poland's unemployment rate stood at 7% according to Eurostat.
Labour market and wages.
Unemployment in Poland appeared after the fall of communism, although the economy previously had high levels of hidden unemployment. The unemployment rate then fell to 10% by the late 1990s and then increased again in the first few years of the 21st century, reaching a peak of 20% in 2002. It has since decreased, although unevenly. Since 2008 the unemployment rate in Poland has consistently been below European average.
The rate fell below 8% in 2015,
Foreign trade and FDI.
With the collapse of the rouble-based COMECON trading bloc in 1991, Poland reoriented its trade. As early as 1996, 70% of its trade was with EU members. Neighboring Germany is Poland's main trading partner today. Poland joined the European Union in May 2004. Before that, it fostered regional integration and trade through the Central European Free Trade Agreement (CEFTA), which included Hungary, the Czech Republic, Slovakia and Slovenia.
Poland is a founding member of the World Trade Organization. As a member of the European Union, it applies the common external tariff to goods from other countries including the United States. Poland's Major imports are capital goods needed for industrial retooling and for manufacturing inputs. The country's exports also include machinery, but are highly diversified. The most successful exports are furniture, foods, motor boats, light planes, hardwood products, casual clothing, shoes and cosmetics. Germany is by far the biggest importer of Poland's exports as of 2013. In the agricultural sector, the biggest money-makers abroad include smoked and fresh fish, fine chocolate, and dairy products, meats and specialty breads, with the exchange rate conducive to export growth. Food exports amounted to 62 billion złoty in 2011, increasing by 17% from 2010. Most Polish exports to the U.S. receive tariff benefits under the Generalized System of Preferences (GSP) program.
Poland is less dependent on external trade than most other Central and Eastern European countries, but its volume of trade with Europe is still substantial. In 2011 the volume of trade (exports plus imports) with the Euro area as share of GDP was 40%, a doubling from the mid 1990s. 30% of Poland's exports are to Germany and another 30% to the rest of Europe. There has been substantial increase in Poland's exports to Russia. However, in August 2014, exports of fruits and vegetables to Russia fell dramatically following its politically motivated ban by Moscow.
Foreign direct investment (FDI) was at 40% of GDP in 2010, a doubling over the level in 2000. Most FDI into Poland comes from France, Germany and Netherlands. Polish firms in turn have foreign investments primarily in Italy and Luxembourg. Most of the internal FDI is in manufacturing, which makes it susceptible to economic fluctuations in the source countries.
The UAE has become Poland's largest trading partner in the Arab world, Roman Chalaczkiewicz, Polish Ambassador to the UAE, told Gulf News.
Polish law is rather favourable to foreign entrepreneurs. The government offers investors various forms of state aid, such as: CIT tax at the level of 19% and investment incentives in 14 Special Economic Zones (among others: income tax exemption, real estate tax exemption, competitive land prices), several industrial and technology parks, the possibility to benefit from the EU structural funds, brownfield and greenfield locations. According to the National Bank of Poland (NBP) the level of FDI inflow into Poland in 2006 amounted to €13.9 billion.
According to an Ernst & Young report, Poland ranks 7th in the world in terms of investment attractiveness. However, Ernst & Young's 2010 European attractiveness survey reported that Poland saw a 52% decrease in FDI job creation and a 42% decrease in number of FDI projects since 2008. According to the OECD (www.oecd.org) report, in 2004 Poles were one of the hardest working nations in Europe. Yet, the ability to establish and conduct business easily has been cause for economic hardship; the 2010 the World Economic Forum ranked Poland near the bottom of OECD countries in terms of the clarity, efficiency and neutrality of the legal framework used by firms to settle disputes.
Sectors.
Primary.
Agriculture.
Agriculture employs 12.7% of the work force but contributes 3.8% to the gross domestic product (GDP), reflecting relatively low productivity. Unlike the industrial sector, Poland's agricultural sector remained largely in private hands during the decades of communist rule. Most of the former state farms are now leased to farmer tenants. Lack of credit is hampering efforts to sell former state farmland. Currently, Poland's 2 million private farms occupy 90% of all farmland and account for roughly the same percentage of total agricultural production. Farms are small—8 hectares on average—and often fragmented. Farms with an area exceeding 15 ha accounted for 9% of the total number of farms but cover 45% of total agricultural area. Over half of all farm households in Poland produce only for their own needs with little, if any, commercial sales.
Poland is a net exporter of processed fruit and vegetables, meat, and dairy products. Processors often rely on imports to supplement domestic supplies of wheat, feed grains, vegetable oil, and protein meals, which are generally insufficient to meet domestic demand. However, Poland is the leading EU producer of potatoes and rye and is one of the world's largest producers of sugar beets and triticale. Poland also is a significant producer of rapeseed, grains, hogs, and cattle. 
Poland is the sixth largest producer and exporter of apples in the entire world.
Mining.
Historically, mining industry in Poland had been extensive, particularly in Silesia.
Secondary.
Production industries.
Before World War II, Poland's industrial base was concentrated in the coal, textile, chemical, machinery, iron, and steel sectors. Today it extends to fertilizers, petrochemicals, machine tools, electrical machinery, electronics, car manufacture and shipbuilding.
Poland's industrial base suffered greatly during World War II, and many resources were directed toward reconstruction. The socialist economic system imposed in the late 1940s created large and unwieldy economic structures operated under a tight central command. In part because of this systemic rigidity, the economy performed poorly even in comparison with other economies in Central Europe.
Tertiary.
Financial.
The Polish banking sector is regulated by the Polish Financial Supervision Authority (PFSA).
While transforming the country to a market-oriented economy during 1992–97, the government privatized some banks, recapitalized the rest and introduced legal reforms that made the sector competitive. These reforms, and the health and relative stability of the sector, attracted a number of strategic foreign investors. At the beginning of 2009, Poland's banking sector had 51 domestic banks, a network of 578 cooperative banks and 18 branches of foreign-owned banks. In addition, foreign investors had controlling stakes in nearly 40 commercial banks, which made up 68% of the banking capital. Banks in Poland reacted to the financial crisis of 2009 by restraining lending, raising interest rates, and strengthening balance sheets. Subsequently, the sector started lending again, with an increase of more than 4% expected in 2011.
Major Polish companies.
Selection from the list of 500 largest companies in Poland compiled by magazine Polityka.
Production and sales data.
Retail sales in Poland
Investment (gross fixed):
18.4% of GDP (2004 est.)
Household income or consumption by percentage share:
Distribution of family income – Gini index:
30.6 (2004)
Agriculture – products:
potatoes, fruits, vegetables, wheat, poultry, eggs, pork
Industrial production growth rate:
17.8% (2006)
Electricity:
Electricity – production by source:
Oil:
Natural gas:
Households with access to fixed and mobile telephone access
Broadband penetration rate
Individuals using computer and internet
Exports – commodities:
machinery and transport equipment 37.8%, intermediate manufactured goods 23.7%, miscellaneous manufactured goods 17.1%, food and live animals 7.6% (2003)
Imports – commodities:
machinery and transport equipment 38%, intermediate manufactured goods 21%, chemicals 14.8%, minerals, fuels, lubricants, and related materials 9.1% (2003)
Currency exchange rates:
Unemployment:
Average gross monthly pay: 3403.07 PLN (~€830) (~$1202) December 2009
Budget and debt.
The public and private debt levels of Poland are below the European average (2015).
Polish state budget expenditure by division (2008):
Source: Concise Statistical Yearbook of Poland (2008/9)
Reserves of foreign exchange & gold:
$70.08 billion (2004 est.)
State Treasury Debt – foreign:
$55.4 billion (2008 est.)
Current account balance:
$−6.7 billion [−1.5% of GDP] (2009 est.)
GDP growth in Poland.
Recent GDP growth (comparing to the same quarter of previous year): 
Historical annual data

</doc>
<doc id="23021" url="https://en.wikipedia.org/wiki?curid=23021" title="Telecommunications in Poland">
Telecommunications in Poland

Telecommunications in Poland include radio, television, fixed and mobile telephones, and the Internet.
Telephones.
From the communist era Poland inherited an underdeveloped and outmoded system of telephones, with some areas (e.g. in the extreme South East) being served by manual exchanges. In December 2005 the last analog exchange was shut down. All telephone lines are now served by modern fully computerized exchanges (Siemens EWSD, Alcatel S12, Lucent 5ESS, Alcatel E10). The former state owned telephone monopoly (TPSA) has been mostly privatized, with France Telecom buying the largest share. Various other companies have entered the fixed phone market, but generally aiming for niches (e.g. Sferia with fixed wireless, Netia covering primarily business). Whilst prices have reduced and availability has increased considerably since the introduction of competition, there is little sign of TPSA's market share being seriously reduced.
The long waiting list for fixed line telephones helped in a boom in mobile cellular telephone use and all mobile phone operators in Poland use GSM. There are three competing networks with similar market share, T-Mobile (T-Mobile and Heyah brands), Orange Polska (Orange and POP brands) and Plus (Plus and Sami Swoi brands). The fourth network, Play, owned by Netia and Novator Telecom, started offering UMTS network services in early 2007. All mobile operators have UMTS services in the major cities, with nationwide coverage planned.

</doc>
<doc id="23022" url="https://en.wikipedia.org/wiki?curid=23022" title="Transport in Poland">
Transport in Poland

Transport in Poland involves air traffic, waterways, roads and railroads.
As a country located at the 'cross-roads' of Europe, Poland, with its highly developed economy, is a nation with a large and increasingly modern network of transport infrastructure.
The country's most important waterway is the river Vistula. The largest seaports are the Port of Szczecin and Port of Gdańsk. Air travel is generally used for international travel, with many flights originating at Warsaw Chopin Airport. Railways connect all of Poland's major cities and the state-owned Polish State Railways (PKP) corporation, through its subsidiaries, runs a great number of domestic and international services of varying speed and comfort. In addition to this, five out of sixteen Polish voivodeships have their own provincial rail service providers. Many major Polish cities have rapid transit systems (typically tram networks) and public transport is available in nearly all areas throughout the country.
Roads.
Polish public roads are grouped into categories related to administrative division. Poland has of public roads, of which are unsurfaced (2011):
According to national roads state report by GDDKiA in 2008 1/4 of national roads were capable of handling 11.5 tonnes per axle loads.
In recent years, the network has been improving and government spending on road construction recently saw a huge increase, due to rapid development of the country and the inflow of European Union funds for infrastructure projects.
Motorways and expressways.
Polish motorways and expressways are part of national roads network. As of December 2012, there are of motorways ("autostrady", singular - "autostrada") and of expressways ("drogi ekspresowe", singular - "droga ekspresowa").
A1 | A2 | A4 | A6 | A8 | A18
S1 | S2 | S3 | S5 |
S6 | S7 | S8 | S10 |
S11 | S12 | S14 |
S17 | S19 | S22 |
S51 | S61 | S69 | S74 | S79 | S86
Air transport.
The most important airport in Poland is Warsaw 'Frederic Chopin' International Airport. Warsaw's airport is the main international hub for LOT Polish Airlines.
In addition to Warsaw Chopin, Wrocław, Gdańsk, Katowice, Kraków and Poznań all have international airports.
In preparation for the Euro 2012 football championships jointly hosted by Poland and Ukraine, a number of airports around the country were renovated and redeveloped. This included the building of new terminals with an increased number of jetways and stands at both Copernicus Airport in Wrocław and Lech Wałęsa Airport in Gdańsk.
Airports.
The Polish airline market was until 2004 a closed market, with bilateral agreements between countries served from the national hub – Warsaw. The regional airports were mostly serving as spokes, and were controlled by PPL, the state-owned airport authority. However, in the 1990s it was decided to deregulate the airport market and abolish the dominant position of PPL. Nearly all local airports (apart from Zielona Góra airport) became separate companies, with local governments involved in their management, which led to the partial decentralisation. Soon after opening of Polish sky for competition, flights “avoiding” the Warsaw hub became more common.
There are twelve passenger airports in operation, and there is also an airport Heringsdorf in German village Garz, 7 kilometers from Polish seaside spa Świnoujście.
International airports.
List of airports in Poland
The following are the largest airports in Poland (In descending order for 2013):
Domestic:
Airports with paved runways:
Total: 84 (2005)
Airports – with unpaved runways:
Total: 39 (2005)
Heliports: 2 (2005)
Marine.
Marine transport in Poland has two main sub-groups, riverine and seaborne. On the Baltic Sea coast, a number of large seaports exist to serve the international freight and passenger trade; these are typically deep water ports and are able to serve very large ships, including the ro-ro ferries of Unity Line, Polferries and Stena Line which operate the Poland – Scandinavia passenger lines.
Riverine services operate on almost all major Polish rivers and canals (such as the Danube–Oder and Elbląg canals) as well as on domestic coastal routes.
Waterways.
Poland has of navigable rivers and canals (as of 2009).
Merchant marine.
Total: 57 ships (1,000 GRT or over) totaling 1,120,165 GRT/
Ships by type:
bulk 50, cargo 2, chemical tanker 2, roll-on/roll-off 1, short-sea passenger 2
Railways.
Poland is served by an extensive network of railways. In most cities the main railway station is located near a city centre and is well connected to the local transportation system. The infrastructure is operated by PKP PLK ( PKP-Polskie Linie Kolejowe : PKP-Polish Rail Lines), part of state-run PKP Group. The rail network is very dense in western and northern Poland, while eastern part of the country is less developed.
The only high-speed rail line (though by most definitions, real high-speed rail only includes speeds over 200 km/h) in central-eastern Europe is the Central Rail Line (Poland), "Centralna Magistrala Kolejowa" (CMK). It has a length of , and was built in 1971–1977; it links Warsaw with Kraków and Katowice. Most trains on the CMK operate at speeds up to , but since December 2014 new Alstom Pendolino ED250 trains operate on a 90 km section of the CMK at , and improvements under way should raise the authorized speed to on most of the line. In test runs on the CMK in November 2013 a new Pendolino ED250 train set a new Polish speed record of .
Other high-speed lines:
"Projects"
The Warsaw–Łódź line is being upgraded to allow speed up to 160 km/h (in order to bind together the Warsaw–Łódź agglomeration).
Plans were made to construct a new high-speed line (350 km/h) from Warsaw to Poznań and Wrocław with forks in Łódź and Kalisz., but the project was cancelled in November 2011 due to its high cost.
The PKP Group is the fourth largest railway throughout Europe. Trains are run by its different subsidiaries.
Passenger transport operators.
The following companies operate in Poland:
Narrow gauge railways.
There are hundreds of kilometres of , , , and narrow gauge lines in Poland.
These railways are mostly in decline, some surviva as a museum or tourist railways.
Freight transport market.
Broad gauge railways.
Except for Linia Hutnicza Szerokotorowa, and a few very short stretches near border crossings, Poland uses the standard gauge for its railways. Therefore, Linia Hutnicza Szerokotorowa (known by its acronym "LHS", English: "Broad gauge steelworks line") in Sławków is the longest broad gauge railway line in Poland. The line runs on a single track for almost from the Polish-Ukrainian border, crossing it just east of Hrubieszów. It is the westernmost broad gauge railway line in Europe that is connected to the broad gauge rail system of the countries of the former Soviet Union.
Rail system.
Total: 
As of December 2002 narrow gauge railways were no longer owned or operated by PKP. They were transferred to regional authorities or became independent companies.
Municipal transport.
Bus.
Most Polish towns and cities have well developed municipal bus services. Typically, a city possesses its own local bus company, however, in some cases they have private competitors operating on certain lines upon the agreement with local authorities.
Until the 1990s, interurban connections were operated by a single, state-owned company PKS. Since then, it has been broken into a number of independent national and municipal enterprises. In addition, several private operators emerged. There are two classes of service distinguished by vehicle length:
While they often use the same bus stops, they tend to use different stations.
Tram.
Bigger cities run dense tram networks, which are the primary mean of public transport. Currently, there are 14 systems serving over 30 cities including Bydgoszcz, Gdańsk, Katowice, Kraków, Łódź, Poznań, Szczecin, Warsaw and Wrocław, with the total track length varying from (Silesian Interurbans) to less than (Tramways in Grudziądz). A new network has been constructed in Olsztyn in 2015. See the list of town tramway systems in Poland
Since the 1990s, a number of cities attempts to upgrade certain parts of their networks to the light rail standard (called "szybkie tramwaje", En. "fast trams"). The most notable investments are Poznań Fast Tram and Kraków Fast Tram with the underground premetro section.
Trolleybus.
Trolleybuses can be found in three cities: Gdynia (with some lines reaching Sopot), Lublin and Tychy.
Metro.
The first metro line was opened in Warsaw in 1995. The second line was opened in 2015. There are no official plans to build metro in other cities due to the lack of funds, but there is an ongoing debate whether they should be built, especially in Kraków and Wrocław.

</doc>
<doc id="23024" url="https://en.wikipedia.org/wiki?curid=23024" title="Foreign relations of Poland">
Foreign relations of Poland

The Republic of Poland is a Central European country and member of the European Union and NATO, among others. In recent years, Poland has extended its responsibilities and position in European and Western affairs, supporting and establishing friendly foreign relations with both the West and with numerous European countries.
Integration with the West and Europe.
After regaining independence in 1989, Poland has forged ahead on its economic reintegration with the Western world. Poland also has been an active nation in advocating European integration.
In 1994, Poland became an associate member of the European Union (EU) and its defensive arm, the Western European Union (WEU). In 1996, Poland achieved full OECD membership and submitted preliminary documentation for full EU membership. In 1997, Poland was invited in the first wave of NATO policy enlargement at the July 1997 NATO Summit in Madrid, Spain. In March 1999, Poland became a full member of NATO. Poland promoted its NATO candidacy through energetic participation in the Partnership for Peace (PfP) program and through intensified individual dialogue with NATO. Poland formally joined the European Union in May 2004, along with the other members of the Visegrád group.
Poland was a part of the multinational force in Iraq.
Establishing relationships with European countries.
The collapse of the Soviet Union led to the establishment of seven new sovereign states in Poland's immediate neighborhood (Latvia, Lithuania, Estonia, Belarus, Ukraine, and Russia), of which Lithuania, Belarus, Ukraine, and Russia (through the Kaliningrad Oblast) border Poland. Poland has actively pursued good relations with all its neighboring countries, signing friendship treaties replacing links severed by the collapse of the Warsaw Pact. The Poles have forged special relationships with Lithuania and particularly Ukraine in an effort to firmly anchor these states to the West.
Due to its tragic historical experience with a repeating pattern of disloyal allies and simultaneous aggression of powerful neighbors (e.g., Partitions of Poland, Second World War), Polish foreign policy pursues close cooperation with a strong partner, one apt enough to give strong military support in times of critical situations. This creates the background of Poland's tight relations with the USA and their sensitivity in relations towards its partner within the European Union, Germany. At the same time, the equally burdened attitude towards Russia results in very tense diplomatic relations, which have been constantly worsening since Vladimir Putin's rise to power. This is an important factor for the special attention Poland pays to the political emancipation of all its Eastern neighbors: Lithuania, Belarus and Ukraine.

</doc>
<doc id="23028" url="https://en.wikipedia.org/wiki?curid=23028" title="Media of Poland">
Media of Poland

The Media of Poland consist of several different types of communications media including television, radio, cinema, newspapers, magazines, and Internet. Many of the media are controlled by large for-profit corporations who reap revenue from advertising, subscriptions, and sale of copyrighted material.
Media and politics.
During the communist regime in Poland the Stalinist press doctrine dominated and controlled Polish media. This doctrine aimed at getting the support of people and making Polish people "Soviets". The country instituted freedom of press since the fall of communism. However, public TV and radio are still politically controlled, via a state regulatory body called "Krajowa Rada Radiofonii i Telewizji" ("The National Radio and Television Committee"), which is similar to CRTC in Canada. It is said that both public and private media are not impartial, and used as means for political propaganda. Various irregularities have been exposed during the investigation by a special parliamentary committee into the "Lew Rywin affair".
TV stations.
TVP – public broadcaster
Polsat – private
Grupa ITI ("International Trading and Investments Holdings SA Luxembourg")
"*TTV belongs to Stavka (51% - TVN, 49% - Besta Film)"
Polcast Television
Minor players and joint-ventures:
Many major players are also present on the market, among them:
Canal+ Polska, Canal+ Sport, Canal+ Film, Canal+ Sport2, HBO, HBO2, EuroSport, EuroSport2, Discovery Channel, Discovery Travel & Living, Discovery Science, Discovery World, MTV Poland, VIVA Poland, VH1 Poland
Radio stations.
Privately owned stations.
Broker FM group:
Eurozet group:
Agora group:
Time group:
other:

</doc>
